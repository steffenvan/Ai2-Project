<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.9973005">
Learning Continuous Phrase Representations for
Translation Modeling
</title>
<author confidence="0.979497">
Jianfeng Gao Xiaodong He Wen-tau Yih Li Deng
</author>
<affiliation confidence="0.957335">
Microsoft Research
</affiliation>
<address confidence="0.9298985">
One Microsoft Way
Redmond, WA 98052, USA
</address>
<email confidence="0.999314">
{jfgao,xiaohe,scottyih,deng}@microsoft.com
</email>
<sectionHeader confidence="0.994812" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99663825">
This paper tackles the sparsity problem in
estimating phrase translation probabilities
by learning continuous phrase representa-
tions, whose distributed nature enables the
sharing of related phrases in their represen-
tations. A pair of source and target phrases
are projected into continuous-valued vec-
tor representations in a low-dimensional
latent space, where their translation score
is computed by the distance between the
pair in this new space. The projection is
performed by a neural network whose
weights are learned on parallel training
data. Experimental evaluation has been
performed on two WMT translation tasks.
Our best result improves the performance
of a state-of-the-art phrase-based statistical
machine translation system trained on
WMT 2012 French-English data by up to
1.3 BLEU points.
</bodyText>
<sectionHeader confidence="0.998884" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954754385965">
The phrase translation model, also known as the
phrase table, is one of the core components of
phrase-based statistical machine translation (SMT)
systems. The most common method of construct-
ing the phrase table takes a two-phase approach
(Koehn et al. 2003). First, the bilingual phrase
pairs are extracted heuristically from an automat-
ically word-aligned training data. The second
phase, which is the focus of this paper, is parame-
ter estimation where each phrase pair is assigned
with some scores that are estimated based on
counting these phrases or their words using the
same word-aligned training data.
Phrase-based SMT systems have achieved
state-of-the-art performance largely due to the fact
that long phrases, rather than single words, are
used as translation units so that useful context in-
formation can be captured in selecting translations.
However, longer phrases occur less often in train-
ing data, leading to a severe data sparseness prob-
lem in parameter estimation. There has been a
plethora of research reported in the literature on
improving parameter estimation for the phrase
translation model (e.g., DeNero et al. 2006;
Wuebker et al. 2010; He and Deng 2012; Gao and
He 2013).
This paper revisits the problem of scoring a
phrase translation pair by developing a Continu-
ous-space Phrase Translation Model (CPTM).
The translation score of a phrase pair in this model
is computed as follows. First, we represent each
phrase as a bag-of-words vector, called word vec-
tor henceforth. We then project the word vector,
in either the source language or the target lan-
guage, into a respective continuous feature vector
in a common low-dimensional space that is lan-
guage independent. The projection is performed
by a multi-layer neural network. The projected
feature vector forms the continuous representa-
tion of a phrase. Finally, the translation score of a
source-target phrase pair is computed by the dis-
tance between their feature vectors.
The main motivation behind the CPTM is to
alleviate the data sparseness problem associated
with the traditional counting-based methods by
grouping phrases with a similar meaning across
different languages. This style of grouping is
made possible because of the distributed nature of
the continuous-space representations for phrases.
No such sharing was possible in the original sym-
bolic space for representing words or phrases. In
this model, semantically or grammatically related
phrases, in both the source and the target lan-
guages, would tend to have similar (close) feature
vectors in the continuous space, guided by the
training objective. Since the translation score is a
smooth function of these feature vectors, a small
</bodyText>
<page confidence="0.998337">
699
</page>
<bodyText confidence="0.999910450980392">
change in the features should only lead to a small
change in the translation score.
The primary research task in developing the
CPTM is learning the continuous representation
of a phrase that is effective for SMT. Motivated
by recent studies on continuous-space language
models (e.g., Bengio et al. 2003; Mikolov et al.
2011; Schwenk et al., 2012), we use a neural net-
work to project a word vector to a feature vector.
Ideally, the projection would discover those latent
features that are useful to differentiate good trans-
lations from bad ones, for a given source phrase.
However, there is no training data with explicit
annotation on the quality of phrase translations.
The phrase translation pairs are hidden in the par-
allel source-target sentence pairs, which are used
to train the traditional translation models. The
quality of a phrase translation can only be judged
implicitly through the translation quality of the
sentences, as measured by BLEU, which contain
the phrase pair. In order to overcome this chal-
lenge and let the BLEU metric guide the projec-
tion learning, we propose a new method to learn
the parameters of a neural network. This new
method, via the choice of an appropriate objective
function in training, automatically forces the fea-
ture vector of a source phrase to be closer to the
feature vectors of its candidate translations. As a
result, the BLEU score is improved when these
translations are selected by an SMT decoder to
produce final, sentence-level translations. The
new learning method makes use of the L-BFGS
algorithm and the expected BLEU as the objective
function defined on N-best lists.
To the best of our knowledge, the CPTM pro-
posed in this paper is the first continuous-space
phrase translation model that makes use of joint
representations of a phrase in the source language
and its translation in the target language (to be de-
tailed in Section 4) and that is shown to lead to
significant improvement over a standard phrase-
based SMT system (to be detailed in Section 6).
Like the traditional phrase translation model,
the translation score of each bilingual phrase pair
is modeled explicitly in our model. However, in-
stead of estimating the phrase translation score on
aligned parallel data, our model intends to capture
the grammatical and semantic similarity between
a source phrase and its paired target phrase by pro-
jecting them into a common, continuous space
that is language independent.
</bodyText>
<footnote confidence="0.516664666666667">
1 Niehues et al. (2011) use different translation units in order
to integrate the n-gram translation model into the phrase-
based approach. However, it is not clear how a continuous
</footnote>
<bodyText confidence="0.999897">
The rest of the paper is organized as follows.
Section 2 reviews previous work. Section 3 re-
views the log-linear model for phrase-based SMT
and Sections 4 presents the CPTM. Section 5 de-
scribes the way the model parameters are esti-
mated, followed by the experimental results in
Section 6. Finally, Section 7 concludes the paper.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998101818181818">
Representations of words or documents as contin-
uous vectors have a long history. Most of the ear-
lier latent semantic models for learning such vec-
tors are designed for information retrieval
(Deerwester et al. 1990; Hofmann 1999; Blei et al.
2003). In contrast, recent work on continuous
space language models, which estimate the prob-
ability of a word sequence in a continuous space
(Bengio et al. 2003; Mikolov et al. 2010), have ad-
vanced the state of the art in language modeling,
outperforming the traditional n-gram model on
speech recognition (Mikolov et al. 2012; Sunder-
meyer et al. 2013) and machine translation
(Mikolov 2012; Auli et al. 2013).
Because these models are developed for mono-
lingual settings, word embedding from these mod-
els is not directly applicable to translation. As a
result, variants of such models for cross-lingual
scenarios have been proposed so that words in dif-
ferent languages are projected into the shared la-
tent vector space (Dumais et al. 1997; Platt et al.
2010; Vinokourov et al. 2002; Yih et al. 2011;
Gao et al. 2011; Huang et al. 2013; Zou et al.
2013). In principle, a phrase table can be derived
using any of these cross-lingual models, although
decoupling the derivation from the SMT training
often results in suboptimal performance (e.g.,
measured in BLEU), as we will show in Section 6.
Recently, there is growing interest in applying
continuous-space models for translation. The
most related to this study is the work of continu-
ous space n-gram translation models (Schwenk et
al. 2007; Schwenk 2012; Son et al. 2012), where
the feed-forward neural network language model
is extended to represent translation probabilities.
However, these earlier studies focused on the n-
gram translation models, where the translation
probability of a phrase or a sentence is decom-
posed as a product of n-gram probabilities as in a
standard n-gram language model. Therefore, it is
not clear how their approaches can be applied to
the phrase translation model1, which is much more
version of such a model can be trained efficiently because the
factor models used by Son et al. cannot be applied directly.
</bodyText>
<page confidence="0.989219">
700
</page>
<bodyText confidence="0.9997285">
widely used in modern SMT systems. In contrast,
our model learns jointly the representations of a
phrase in the source language as well as its trans-
lation in the target language. The recurrent contin-
uous translation models proposed by Kalchbren-
ner and Blunsom (2013) also adopt the recurrent
language model (Mikolov et al. 2010). But unlike
the n-gram translation models above, they make
no Markov assumptions about the dependency of
the words in the target sentence. Continuous space
models have also been used for generating trans-
lations for new words (Mikolov et al. 2013a) and
ITG reordering (Li et al. 2013).
There has been a lot of research on improving
the phrase table in phrase-based SMT (Marcu and
Wong 2002; Lamber and Banchs 2005; Denero et
al. 2006; Wuebker et al. 2010; Zhang et al., 2011;
He and Deng 2012; Gao and He 2013). Among
them, (Gao and He 2013) is most relevant to the
work described in this paper. They estimate
phrase translation probabilities using a discrimi-
native training method under the N-best reranking
framework of SMT. In this study we use the same
objective function to learn the continuous repre-
sentations of phrases, integrating the strengths as-
sociated with these earlier studies.
</bodyText>
<sectionHeader confidence="0.991886" genericHeader="method">
3 The Log-Linear Model for SMT
</sectionHeader>
<bodyText confidence="0.942624">
Phrase-based SMT is based on a log-linear model
which requires learning a mapping between input
F ∈ ℱ to output E ∈ ℰ. We are given
</bodyText>
<listItem confidence="0.981816">
• Training samples (Fi, Ei) for 𝑖 = 1 ... 𝑁,
where each source sentence Fi is paired with
a reference translation in target language Ei;
• A procedure GEN to generate a list of N-best
candidates GEN(Fi) for an input Fi, where
GEN in this study is the baseline phrase-
based SMT system, i.e., an in-house
implementation of the Moses system (Koehn
et al. 2007) that does not use the CPTM, and
each E ∈ GEN(Fi) is labeled by the
sentence-level BLEU score (He and Deng
2012), denoted by sBleu(Ei, E) , which
measures the quality of E with respect to its
reference translation Ei;
• A vector of features 𝐡 ∈ R𝑀 that maps each
(Fi, E) to a vector of feature values2; and
• A parameter vector I ∈ R𝑀, which assigns a
real-valued weight to each feature.
</listItem>
<footnote confidence="0.917491">
2 Our baseline system uses a set of standard features sug-
gested in Koehn et al. (2007), which is also detailed in Sec-
tion 6.
</footnote>
<bodyText confidence="0.999142333333333">
The components GEN(.), 𝐡 and I define a log-
linear model that maps Fi to an output sentence as
follows:
</bodyText>
<equation confidence="0.998719">
E∗ = argmax
(𝐸,𝐴)∈GEN(𝐹𝑖)
</equation>
<bodyText confidence="0.999591166666667">
which states that given I and 𝐡, argmax returns
the highest scoring translation E∗, maximizing
over correspondences A. In phrase-based SMT, A
consists of a segmentation of the source and target
sentences into phrases and an alignment between
source and target phrases. Since computing the
argmax exactly is intractable, it is commonly
performed approximatedly by beam search (Och
and Ney 2004). Following Liang et al. (2006), we
assume that every translation candidate is always
coupled with a corresponding A, called the Viterbi
derivation, generated by (1).
</bodyText>
<sectionHeader confidence="0.980895" genericHeader="method">
4 A Continuous-Space Phrase Transla-
tion Model (CPTM)
</sectionHeader>
<bodyText confidence="0.999787333333333">
The architecture of the CPTM is shown in Figures
1 and 2, where for each pair of source and target
phrases (𝑓i, 𝑒𝑗) in a source-target sentence pair,
we first project them into feature vectors y𝑓𝑖 and
y𝑒𝑗 in a latent, continuous space via a neural net-
work with one hidden layer (as shown in Figure
2), and then compute the translation score,
score(𝑓i, 𝑒𝑗), by the distance of their feature vec-
tors in that space.
We start with a bag-of-words representation of
a phrase 𝐱 ∈ R𝑑, where 𝐱 is a word vector and 𝑑
is the size of the vocabulary consisting of words
in both source and target languages, which is set
to 200K in our experiments. We then learn to pro-
ject 𝐱 to a low-dimensional continuous space Rk:
</bodyText>
<equation confidence="0.665956">
𝜙 (𝐱): R𝑑 → Rk
</equation>
<bodyText confidence="0.999948166666667">
The projection is performed using a fully con-
nected neural network with one hidden layer and
tanh activation functions. Let W1 be the projec-
tion matrix from the input layer to the hidden layer
and W2 the projection matrix from the hidden
layer to the output layer, we have
</bodyText>
<equation confidence="0.9985285">
y ≡ 𝜙(𝐱) = tanh (W2T(tanh(W1T𝐱))) (2)
IT𝐡(Fi,E,A) (1)
</equation>
<page confidence="0.995032">
701
</page>
<figure confidence="0.969737176470588">
... (le processus de)
on
𝑓𝑖−1
... (the process of) (machine translation)
𝑒𝑗−1 𝑒𝑗 𝑒𝑗+1
𝑓𝑖
te ) 𝑓𝑖+1
𝐲𝑓𝑖
𝐲𝑒𝑗
Continuous representations of
source phrases
Target phrases
Continuous representations of
target phrases
Source phrases
score(𝑓𝑖, 𝑒𝑗) = 𝐲𝑓𝑖 T 𝐲𝑒𝑗 Translation score as dot product of
feature vectors in the continuous space
</figure>
<figureCaption confidence="0.9965185">
Figure 1. The architecture of the CPTM, where the mapping from a phrase to its continuous repre-
sentation is shown in Figure 2.
</figureCaption>
<equation confidence="0.81734625">
100 (𝑘)
𝐖2
100
𝐖1
</equation>
<bodyText confidence="0.982434">
introducing a new feature ℎ𝑀+1 and a new feature
weight 𝜆𝑀+1. The new feature is defined as
</bodyText>
<equation confidence="0.89904375">
ℎ𝑀+1(𝐹𝑖, 𝐸, 𝐴) = ∑(𝑓,𝑒 )∈𝐴 sim𝛉 (𝐱𝑓, 𝐱𝑒) (4)
𝐲
𝐱
Raw phrase (𝑤1 ... 𝑤𝑛) 𝑒 or 𝑓
</equation>
<figureCaption confidence="0.97109">
Figure 2. A neural network model for phrases
</figureCaption>
<bodyText confidence="0.986754714285714">
giving rise to their continuous representations.
The model with the same form is used for both
source and target languages.
The translation score of a source phrase f and a
target phrase a can be measured as the similarity
(or distance) between their feature vectors. We
choose the dot product as the similarity function3:
</bodyText>
<equation confidence="0.994823">
score(𝑓,𝑒) ≡ sim𝛉(𝐱𝑓,𝐱𝑒) = 𝐲𝑓T𝐲𝑒 (3)
</equation>
<bodyText confidence="0.989629523809524">
According to (2), we see that the value of the scor-
ing function is determined by the projection ma-
trices 𝛉 = {𝐖1, 𝐖2}.
The CPTM of (2) and (3) can be incorporated
into the log-linear model for SMT (1) by
3 In our experiments, we compare dot product and the cosine
similarity functions and find that the former works better for
nonlinear multi-layer neural networks, and the latter works
better for linear neural networks. For the sake of clarity, we
choose dot product when we describe the CPTM and its train-
ing in Sections 4 and 5, respectively.
4 The baseline SMT needs to be reasonably good in the
sense that the oracle BLEU score on the generated n-best
Thus, the phrase-based SMT system, into which
the CPTM is incorporated, is parameterized by
(𝛌, 𝛉), where 𝛌 is a vector of a handful of param-
eters used in the log-linear model of (1), with one
weight for each feature; and 𝛉 is the projection
matrices used in the CPTM defined by (2) and (3).
In our experiments we take three steps to learn
(𝛌, 𝛉):
</bodyText>
<listItem confidence="0.616589888888889">
1. We use a baseline phrase-based SMT sys-
tem to generate for each source sentence in
training data an N-best list of translation hy-
potheses4.
2. We set 𝛌 to that of the baseline system and
let 𝜆𝑀+1 = 1, and optimize 𝛉 w.r.t. a loss
function on training datas.
3. We fix 𝛉, and optimize 𝛌 using MERT
(Och 2003) to maximize BLEU on dev data.
</listItem>
<bodyText confidence="0.970173">
In the next section, we will describe Step 2 in de-
tail as it is directly related to the CPTM training.
lists needs to be significantly higher than that of the top-1
translations so that the CPTM can be effectively trained.
5 The initial value of 𝜆𝑀+1 can also be tuned using the dev
set. However, we find in a pilot study that it is good enough
to set it to 1 when the values of all the baseline feature
weights, used in the log-linear model of (1), are properly nor-
malized, such as by setting 𝜆𝑚 = 𝜆𝑚/𝐶 for 𝑚 = 1... 𝑀 ,
where 𝐶 is the unnormalized weight value of the target lan-
guage model.
</bodyText>
<figure confidence="0.990084">
200K (d)
Feature vector
Neural network
Word vector
</figure>
<page confidence="0.980646">
702
</page>
<sectionHeader confidence="0.985331" genericHeader="method">
5 Training CPTM
</sectionHeader>
<bodyText confidence="0.998819875">
This section describes the loss function we em-
ploy with the CPTM and the algorithm to train the
neural network weights.
We define the loss function ℒ(𝛉) as the nega-
tive of the N-best list based expected BLEU, de-
noted by xBleu(𝛉). In the reranking framework of
SMT outlined in Section 3, xBleu(𝛉) over one
training sample (𝐹𝑖, 𝐸𝑖) is defined as
</bodyText>
<equation confidence="0.973057">
xBleu(𝛉) = ∑𝐸∈GEN(𝐹𝑖) 𝑃(𝐸|𝐹𝑖)sBleu(𝐸𝑖, 𝐸) (5)
</equation>
<bodyText confidence="0.99977">
where sBleu(𝐸𝑖, 𝐸) is the sentence-level BLEU
score, and 𝑃 (𝐸 |𝐹𝑖) is the translation probability
from 𝐹𝑖 to 𝐸 computed using softmax as
</bodyText>
<equation confidence="0.989326333333333">
exp(𝛾𝛌T𝐡(𝐹𝑖,𝐸,𝐴))
𝑃(𝐸|𝐹𝑖) = (6)
∑ 𝐸′∈GEN(𝐹𝑖) exp(𝛾𝛌T𝐡(𝐹𝑖,𝐸′,𝐴))
</equation>
<bodyText confidence="0.9999112">
where 𝛌T𝐡 is the log-linear model of (1), which
also includes the feature derived from the CPTM
as defined by (4), and 𝛾 is a tuned smoothing fac-
tor.
Let ℒ(𝛉) be a loss function which is differen-
tiable w.r.t. the parameters of the CPTM, 𝛉. We
can compute the gradient of the loss and learn 𝛉
using gradient-based numerical optimization al-
gorithms, such as L-BFGS or stochastic gradient
descent (SGD).
</bodyText>
<subsectionHeader confidence="0.996686">
5.1 Computing the Gradient
</subsectionHeader>
<bodyText confidence="0.999456">
Since the loss does not explicitly depend on 𝛉, we
use the chain rule for differentiation:
</bodyText>
<equation confidence="0.886107333333333">
𝜕ℒ(𝛉) 𝜕ℒ(𝛉) 𝜕sim𝛉(𝐱𝑓,𝐱𝑒)
𝜕𝛉 = ∑𝜕sim𝛉(𝐱𝑓,𝐱𝑒) 𝜕𝛉
(𝑓,𝑒 )
𝜕sim𝛉 (𝐱𝑓, 𝐱𝑒)
= ∑ −𝛿(𝑓,𝑒)𝜕𝛉
(𝑓,𝑒 )
</equation>
<bodyText confidence="0.9998122">
which takes the form of summation over all phrase
pairs occurring either in a training sample (sto-
chastic mode) or in the entire training data (batch
mode). 𝛿(𝑓,𝑒) in (7) is known as the error term of
the phrase pair (𝑓, 𝑒), and is defined as
</bodyText>
<equation confidence="0.991516666666667">
𝜕ℒ(𝛉)
𝛿(𝑓,𝑒) = − (8)
𝜕sim𝛉(𝐱𝑓,𝐱𝑒)
</equation>
<bodyText confidence="0.9992776">
It describes how the overall loss changes with the
translation score of the phrase pair (𝑓, 𝑒). We will
leave the derivation of 𝛿(𝑓,𝑒) to Section 5.1.2, and
will first describe how the gradient of
sim𝛉(𝐱𝑓,𝐱𝑒) w.r.t. 𝛉 is computed.
</bodyText>
<subsubsectionHeader confidence="0.973431">
5.1.1 Computing 𝝏𝐬𝐢𝐦𝛉(𝐱 𝒇, 𝐱𝒆)/𝝏𝛉
</subsubsectionHeader>
<bodyText confidence="0.995772">
Without loss of generality, we use the following
notations to describe a neural network:
</bodyText>
<listItem confidence="0.940017666666667">
• 𝐖𝑙 is the projection matrix for the l-th layer
of the neural network;
• 𝐱 is the input word vector of a phrase;
• 𝐳𝑙 is the sum vector of the l-th layer; and
• 𝐲𝑙 = 𝜎(𝐳𝑙) is the output vector of the l-th
layer, where 𝜎 is an activation function;
</listItem>
<bodyText confidence="0.990661">
Thus, the CPTM defined by (2) and (3) can be rep-
resented as
</bodyText>
<equation confidence="0.9964866">
𝐳1 = 𝐖1T𝐱
𝐲1 = 𝜎(𝐳1)
𝐳2 = 𝐖2T𝐲1
𝐲2 = 𝜎(𝐳2)
sim𝛉 (𝐱𝑓, 𝐱𝑒) = (𝐲𝑓 2)T𝐲𝑒2
</equation>
<bodyText confidence="0.9932985">
The gradient of the matrix 𝐖2 which projects the
hidden vector to the output vector is computed as:
</bodyText>
<equation confidence="0.978356333333333">
T
∂(𝐲𝑓2) = 𝐲𝑒2 + (𝐲𝑓 2)T ∂𝐲𝑒2
∂𝐖2 ∂𝐖2 ∂𝐖2
T T
= 𝐲𝑓 1 (𝐲𝑒2 ∘ 𝜎′(𝐳𝑓 2)) + 𝐲𝑒1 (𝐲𝑓 2 ∘ 𝜎′(𝐳𝑒2))
(9)
</equation>
<bodyText confidence="0.9996126">
where ∘ is the element-wise multiplication (Hada-
mard product). Applying the back propagation
principle, the gradient of the projection matrix
mapping the input vector to the hidden vector 𝐖1
is computed as
</bodyText>
<equation confidence="0.898243666666667">
∂sim𝛉(𝐱𝑓,𝐱𝑒)
∂𝐖1
= 𝐱𝑓 (𝐖2 (𝐲𝑒2 ∘ 𝜎′(𝐳𝑓 2)) ∘ 𝜎′(𝐳𝑓 1) )T
</equation>
<bodyText confidence="0.923198333333333">
+𝐱𝑒 (𝐖2 (𝐲𝑓 2 ∘ 𝜎′(𝐳𝑒2)) ∘ 𝜎′(𝐳𝑒1)
The derivation can be easily extended to a neural
network with multiple hidden layers.
</bodyText>
<subsubsectionHeader confidence="0.983808">
5.1.2 Computing 𝜹(𝒇,𝒆)
</subsubsectionHeader>
<bodyText confidence="0.999861">
To simplify the notation, we rewrite our loss func-
tion of (5) and (6) over one training sample as
</bodyText>
<equation confidence="0.955307666666667">
(7)
∂sim𝛉(𝐱𝑓,𝐱𝑒)
T
)
(10)
703
G(𝛉)
ℒ(𝛉) = −xBleu(𝛉) = − (11)
Z(𝛉)
</equation>
<bodyText confidence="0.70159">
where
</bodyText>
<equation confidence="0.997851909090909">
G(𝛉) = ∑𝐸 sBleu(𝐸, 𝐸𝑖) exp(𝛌T𝐡(𝐹𝑖, 𝐸, 𝐴))
Z(𝛉) = ∑𝐸 exp(𝛌T𝐡(𝐹𝑖, 𝐸, 𝐴))
Combining (8) and (11), we have
𝛿(𝑓,𝑒) =
= 1 𝜕G(𝛉) 𝜕Z(𝛉) xBleu(𝛉)
Z(𝛉) ( 𝜕sim𝛉 (𝐱𝑓, 𝐱𝑒) 𝜕sim𝛉 (𝐱𝑓, 𝐱𝑒)
Because 𝛉 is only relevant to ℎ𝑀+1 which is de-
fined in (4), we have
𝜕ℎ𝑀+1(𝐹𝑖, 𝐸, 𝐴)
𝜕sim𝛉(𝐱𝑓, 𝐱𝑒)
= 𝜆𝑀+1𝑁(𝑓,𝑒;𝐴) (13)
</equation>
<bodyText confidence="0.999587">
where 𝑁(𝑓, 𝑒; 𝐴) is the number of times the
phrase pair (𝑓, 𝑒) occurs in 𝐴. Combining (12)
and (13), we end up with the following equation
</bodyText>
<equation confidence="0.985601">
𝛿(𝑓,𝑒)
= ∑ U(𝛉,𝐸)𝑃(𝐸|𝐹𝑖)𝜆𝑀+1𝑁(𝑓,𝑒; 𝐴)
(𝐸,𝐴)∈𝐺𝐸𝑁(𝐹𝑖)
where (14)
U(𝛉, 𝐸) = sBleu(𝐸𝑖, 𝐸) − xBleu(𝛉).
</equation>
<subsectionHeader confidence="0.998665">
5.2 The Training Algorithm
</subsectionHeader>
<bodyText confidence="0.999892595744681">
In our experiments we train the parameters of the
CPTM, 𝛉, using the L-BFGS optimizer described
in Andrew and Gao (2007), together with the loss
function described in (5). The gradient is com-
puted as described in Sections 5.1. Although SGD
has been advocated for neural network training
due to its simplicity and its robustness to local
minima (Bengio 2009), we find that in our task
that the L-BFGS minimizes the loss in a desirable
fashion empirically when iterating over the com-
plete training data (batch mode). For example, the
convergence of the algorithm was found to be
smooth, despite the non-convexity in our loss. An-
other merit of batch training is that the gradient
over all training data can be computed efficiently.
As shown in Section 5.1, computing
𝜕simθ(x𝑓, x𝑒)/𝜕θ requires large-scale matrix
multiplications, and is expensive for multi-layer
neural networks. Eq. (7) suggests that
𝜕simθ(x𝑓,x𝑒)/𝜕θ and 𝛿(𝑓,𝑒) can be computed
separately, thus making the computation cost of
the former term only depends on the number of
phrase pairs in the phrase table, but not the size of
training data. Therefore, the training method de-
scribed here can be used on larger amounts of
training data with little difficulty.
As described in Section 4, we take three steps
to learn the parameters for both the log-linear
model of SMT and the CPTM. While steps 1 and
3 can be easily parallelized on a computer cluster,
the CPTM training is performed on a single ma-
chine. For example, given a phrase table contain-
ing 16M pairs and a 1M-sentence training set, it
takes a couple of hours to generate the N-best lists
on a cluster, and about 10 hours to train the CPTM
on a Xeon E5-2670 2.60GHz machine.
For a non-convex problem, model initialization
is important. In our experiments we always initial-
ize 𝐖1 using a bilingual topic model trained on
parallel data (see detail in Section 6.2), and 𝐖2 as
an identity matrix. In principle, the loss function
of (5) can be further regularized (e.g. by adding a
term of 𝐿2 norm) to deal with overfitting. How-
ever, we did not find clear empirical advantage
over the simpler early stop approach in a pilot
study, which is adopted in the experiments in this
paper.
</bodyText>
<sectionHeader confidence="0.999509" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999878666666667">
This section evaluates the CPTM presented on
two translation tasks using WMT data sets. We
first describe the data sets and baseline setup.
Then we present experiments where we compare
different versions of the CPTM and previous
models.
</bodyText>
<subsectionHeader confidence="0.994731">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.996792428571429">
Baseline. We experiment with an in-house
phrase-based system similar to Moses (Koehn et
al. 2007), where the translation candidates are
scored by a set of common features including
maximum likelihood estimates of source given
target phrase mappings 𝑃𝑀𝐿𝐸 (𝑒 |𝑓) and vice versa
𝑃𝑀𝐿𝐸 (𝑓 |𝑒), as well as lexical weighting estimates
𝑃𝐿𝑊 (𝑒 |𝑓) and 𝑃𝐿𝑊 (𝑓 |𝑒), word and phrase penal-
ties, a linear distortion feature, and a lexicalized
reordering feature. The baseline includes a stand-
ard 5-gram modified Kneser-Ney language model
trained on the target side of the parallel corpora
described below. Log-linear weights are estimated
with the MERT algorithm (Och 2003).
</bodyText>
<equation confidence="0.9971974">
𝜕𝛌T𝐡(𝐹𝑖, 𝐸, 𝐴) = 𝜆𝑀+1
𝜕sim𝛉 (𝐱𝑓, 𝐱𝑒)
𝜕xBleu(𝛉)
(12)
𝜕sim𝛉(𝐱𝑓, 𝐱𝑒)
</equation>
<page confidence="0.963152">
704
</page>
<bodyText confidence="0.999973433333333">
Evaluation. We test our models on two different
data sets. First, we train an English to French sys-
tem based on the data of WMT 2006 shared task
(Koehn and Monz 2006). The parallel corpus in-
cludes 688K sentence pairs of parliamentary pro-
ceedings for training. The development set con-
tains 2000 sentences, and the test set contains
other 2000 sentences, all from the official WMT
2006 shared task.
Second, we experiment with a French to Eng-
lish system developed using 2.1M sentence pairs
of training data, which amounts to 102M words,
from the WMT 2012 campaign. The majority of
the training data set is parliamentary proceedings
except for 5M words which are newswire. We use
the 2009 newswire data set, comprising 2525 sen-
tences, as the development set. We evaluate on
four newswire domain test sets from 2008, 2010
and 2011 as well as the 2010 system combination
test set, containing 2034 to 3003 sentences.
In this study we perform a detailed empirical
comparison using the WMT 2006 data set, and
verify our best models and results using the larger
WMT 2012 data set.
The metric used for evaluation is case insensi-
tive BLEU score (Papineni et al. 2002). We also
perform a significance test using the Wilcoxon
signed rank test. Differences are considered statis-
tically significant when the p-value is less than
0.05.
</bodyText>
<subsectionHeader confidence="0.997481">
6.2 Results of the CPTM
</subsectionHeader>
<bodyText confidence="0.999935052631579">
Table 1 shows the results measured in BLEU eval-
uated on the WMT 2006 data set, where Row 1 is
the baseline system. Rows 2 to 4 are the systems
enhanced by integrating different versions of the
CPTM. Rows 5 to 7 present the results of previous
models. Row 8 is our best system. Table 2 shows
the main results on the WMT 2012 data set.
CPTM is the model described in Sections 4.
As illustrated in Figure 2, the number of the nodes
in the input layer is the vocabulary size 𝑑. Both
the hidden layer and the output layer have 100
nodes6. That is, 𝐖1 is a 𝑑 × 100 matrix and 𝐖2
a 100 × 100 matrix. The result shows that
CPTM leads to a substantial improvement over
the baseline system with a statistically significant
margin of 1.0 BLEU points as in Table 1.
We have developed a set of variants of CPTM
to investigate two design choices we made in de-
veloping the CPTM: (1) whether to use a linear
</bodyText>
<footnote confidence="0.7068635">
6 We can achieve slightly better results using more nodes in
the hidden and output layers, say 500 nodes. But the model
</footnote>
<table confidence="0.658600222222222">
# Systems WMT test2006
1 Baseline 33.06
2 CPTM 34.10α
3 CPTML 33.60αβ
4 CPTMW 33.25β
5 BLTMPR 33.15β
6 DPM 33.29β
7 MRFP 33.91α
8 Comb (2 + 7) 34.39αβ
</table>
<tableCaption confidence="0.93642">
Table 1: BLEU results for the English to French
</tableCaption>
<bodyText confidence="0.984190964285714">
task using translation models and systems built
on the WMT 2006 data set. The superscripts α
and β indicate statistically significant difference
(p &lt; 0.05) from Baseline and CPTM, respec-
tively.
projection or a multi-layer nonlinear projection;
and (2) whether to compute the phrase similarity
using word-word similarities as suggested by e.g.,
the lexical weighting model (Koehn et al. 2003).
We compare these variants on the WMT 2006
data set, as shown in Table 1.
CPTML (Row 3 in Table 1) uses a linear neural
network to project a word vector of a phrase 𝐱 to
a feature vector 𝐲: 𝐲 ≡ 𝜙(𝐱) = 𝐖T𝐱, where 𝐖 is
a 𝑑 × 100 projection matrix. The translation
score of a source phrase f and a target phrase a is
measured as the similarity of their feature vectors.
We choose cosine similarity because it works bet-
ter than dot product for linear projection.
CPTMW (Row 4 in Table 1) computes the phrase
similarity using word-word similarity scores. This
follows the common smoothing strategy of ad-
dressing the data sparseness problem in modeling
phrase translations, such as the lexical weighting
model (Koehn et al. 2003) and the word factored
n-gram translation model (Son et al. 2012). Let 𝑤
denote a word, and 𝑓 and 𝑒 the source and target
phrases, respectively. We define
</bodyText>
<equation confidence="0.99379325">
sim(𝑓, 𝑒) = 1
|𝑓 |∑𝑤∈𝑓 sim𝜏(𝑤, 𝑒) +
1
|𝑒 |∑𝑤∈𝑒 sim𝜏(𝑤,𝑓)
</equation>
<bodyText confidence="0.999952333333333">
where sim𝜏 (𝑤, 𝑒) (or sim𝜏 (𝑤, 𝑓 )) is the word-
phrase similarity, and is defined as a smooth ap-
proximation of the maximum function
</bodyText>
<equation confidence="0.796128">
sim𝜏(𝑤, 𝑒)
= ∑𝑤′∈𝑒 sim(𝑤, 𝑤′) exp(𝜏sim(𝑤, 𝑤′))
∑𝑤′∈𝑒 exp(𝜏sim(𝑤, 𝑤′))
</equation>
<footnote confidence="0.659294333333333">
training is too slow to perform a detailed study within a rea-
sonable time. Therefore, all the models reported in this paper
use 100 nodes.
</footnote>
<page confidence="0.990223">
705
</page>
<table confidence="0.9995502">
# Systems dev news2011 news2010 news2008 newssyscomb2010
1 Baseline 23.58 25.24 24.35 20.36 24.14
2 MRFP 24.07α 26.00α 24.90 20.84α 25.05α
3 CPTM 24.12α 26.25α 25.05α 21.15αβ 24.91α
4 Comb (2 + 3) 24.46αβ 26.56αβ 25.52αβ 21.64αβ 25.22α
</table>
<tableCaption confidence="0.707016666666667">
Table 2: BLEU results for the French to English task using translation models and systems built on
the WMT 2012 data set. The superscripts α and β indicate statistically significant difference (p &lt;
0.05) from Baseline and MRFp, respectively.
</tableCaption>
<bodyText confidence="0.9999655">
where simT (w, e) (or simT (w, f )) is the word-
phrase similarity, and is defined as a smooth ap-
proximation of the maximum function
where i is the tuned smoothing parameter.
Similar to CPTM, CPTMW also uses a nonlin-
ear projection to map each word (not a phrase vec-
tor as in CPTM) to a feature vector.
Two observations can be made by comparing
CPTM in Row 2 to its variants in Table 1. First of
all, it is more effective to model the phrase trans-
lation directly than decomposing it into word-
word translations in the CPTMs. Second, we see
that the nonlinear projection is able to generate
more effective features, leading to better results
than the linear projection.
We also compare the best version of the CPTM
i.e., CPTM, with three related models proposed
previously. We start the discussion with the re-
sults on the WMT 2006 data set in Table 1.
Rows 5 and 6 in Table 1 are two state-of-the-
art latent semantic models that are originally
trained on clicked query-document pairs (i.e.,
clickthrough data extracted from search logs) for
query-document matching (Gao et al. 2011). To
adopt these models for SMT, we view source-tar-
get sentence pairs as clicked query-document
pairs, and trained both models using the same
methods as in Gao et al. (2011) on the parallel bi-
lingual training data described earlier. Specifi-
cally, BTLMPR is an extension to PLSA, and is
the best performer among different versions of the
Bi-Lingual Topic Model (BLTM) described in
Gao et al. (2011). BLTM with Posterior Regular-
ization (BLTMPR) is trained on parallel training
data using the EM algorithm with a constraint en-
forcing a source sentence and its paralleled target
sentence to not only share the same prior topic dis-
tribution, but to also have similar fractions of
words assigned to each topic. We incorporated the
model into the log-linear model for SMT (1) as
</bodyText>
<footnote confidence="0.7519355">
7 Gao and He (2013) reported results of MRF models with
different feature sets. We picked the MRF using phrase fea-
tures only (MRFP) for comparison since we are mainly inter-
ested in phrase representation.
</footnote>
<bodyText confidence="0.9997248">
follows. First of all, the topic distribution of a
source sentence Fi , denoted by P (z |Fi), is in-
duced from the learned topic-word distributions
using EM. Then, each translation candidate E in
the N-best list GEN(Fi) is scored as
</bodyText>
<equation confidence="0.995635">
P(E|Fi) = II,cEZ.P(w|Z)P(Z|Fi)
</equation>
<bodyText confidence="0.998624419354838">
P (Fi |E) can be similarly computed. Finally, the
logarithms of the two probabilities are incorpo-
rated into the log-linear model of (1) as two addi-
tional features. DPM is the Discriminative Projec-
tion Model described in Gao et al. (2011), which
is an extension of LSA. DPM uses a matrix to pro-
ject a word vector of a sentence to a feature vector.
The projection matrix is learned on parallel train-
ing data using the S2Net algorithm (Yih et al.
2011). DPM can be incorporated into the log-lin-
ear model for SMT (1) by introducing a new fea-
ture hmr+1 for each phrase pair, which is defined
as the cosine similarity of the phrases in the pro-
ject space.
As we see from Table 1, both latent semantic
models, although leading to some slight improve-
ment over Baseline, are much less effective than
CPTM.
Finally, we compare the CPTM with the Mar-
kov Random Field model using phrase features
(MRFP in Tables 1 and 2), proposed by Gao and
He (2013)7, on both the WMT 2006 and WMT
2012 datasets. MRFp is a state-of-the-art large
scale discriminative training model that uses the
same expected BLEU training criterion, which
has proven to give superior performance across a
range of MT tasks recently (He and Deng 2012,
Setiawan and Zhou 2013, Gao and He 2013).
Unlike CPTM, MRFp is a linear model that
simply treats each phrase pair as a single feature.
Therefore, although both are trained using the
</bodyText>
<page confidence="0.996005">
706
</page>
<bodyText confidence="0.999908233333333">
same expected BLEU based objective function,
CPTM and MRFp model the translation relation-
ship between two phrases from different angles.
MRFp estimates one translation score for each
phrase pair explicitly without parameter sharing,
while in CPTM, all phrases share the same neural
network that projects raw phrases to the continu-
ous space, providing a more smoothed estimation
of the translation score for each phrase pair.
The results in Tables 1 and 2 show that CPTM
outperforms MRFP on most of the test sets across
the two WMT data sets, but the difference be-
tween them is often not significant. Our interpre-
tation is that although CPTM provides a better
smoothed estimation for low-frequent phrase
pairs, which otherwise suffer the data sparsity is-
sue, MRFp provides a more precise estimation for
those high-frequent phrase pairs. That is, CPTM
and MRFp capture complementary information
for translation. We thus combine CPTM and
MRFP (Comb in Tables 1 and 2) by incorporating
two features, each for one model, into the log-lin-
ear model of SMT (1). We observe that for both
translation tasks, accuracy improves by up to 0.8
BLEU over MRFP alone (e.g., on the news2008
test set in Table 2). The results confirm that
CPTM captures complementary translation infor-
mation to MRFp. Overall, we improve accuracy
by up to 1.3 BLEU over the baseline on both
WMT data sets.
</bodyText>
<sectionHeader confidence="0.999123" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999988681818182">
The work presented in this paper makes two major
contributions. First, we develop a novel phrase
translation model for SMT, where joint represen-
tations are exploited of a phrase in the source lan-
guage and of its translation in the target language,
and where the translation score of the pair of
source-target phrases are represented as the dis-
tance between their feature vectors in a low-di-
mensional, continuous space. The space is derived
from the representations generated using a multi-
layer neural network. Second, we present a new
learning method to train the weights in the multi-
layer neural network for the end-to-end BLEU
metric directly. The training method is based on
L-BFGS. We describe in detail how the gradient
in closed form, as required for efficient optimiza-
tion, is derived. The objective function, which
takes the form of the expected BLEU computed
from N-best lists, is very different from the usual
objective functions used in most existing architec-
tures of neural networks, e.g., cross entropy (Hin-
ton et al. 2012) or mean square error (Deng et al.
2012). We hence have provided details in the der-
ivation of the gradient, which can serve as an ex-
ample to guide the derivation of neural network
learning with other non-standard objective func-
tions in the future.
Our evaluation on two WMT data sets show
that incorporating the continuous-space phrase
translation model into the log-linear framework
significantly improves the accuracy of a state-of-
the-art phrase-based SMT system, leading to a
gain up to 1.3 BLEU. Careful implementation of
the L-BFGS optimization based on the BLEU-
centric objective function, together with the asso-
ciated closed-form gradient, is a key to the suc-
cess.
A natural extension of this work is to expand
the model and learning algorithm from shallow to
deep neural networks. The deep models are ex-
pected to produce more powerful and flexible se-
mantic representations (e.g., Tur et al., 2012), and
thus greater performance gain than what is pre-
sented in this paper.
</bodyText>
<sectionHeader confidence="0.997193" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999935">
We thank Michael Auli for providing a dataset
and for helpful discussions. We also thank the four
anonymous reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.977689" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.81553845">
Andrew, G. and Gao, J. 2007. Scalable training
of L1-regularized log-linear models. In
ICML.
Auli, M., Galley, M., Quirk, C. and Zweig, G.
2013 Joint language and translation modeling
with recurrent neural networks. In EMNLP.
Bengio, Y. 2009. Learning deep architectures for
AI. Fundamental Trends Machine Learning,
vol. 2, no. 1, pp. 1–127.
Bengio, Y., Duharme, R., Vincent, P., and Janvin,
C. 2003. A neural probabilistic language
model. JMLR, 3:1137-1155.
Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003.
Latent Dirichlet allocation. Journal of Ma-
chine Learning Research, 3: 993-1022.
Collobert, R., Weston, J., Bottou, L., Karlen, M.,
Kavukcuoglu, K., and Kuksa, P. 2011. Natural
language processing (almost) from scratch.
Journal of Machine Learning Research, vol.
12.
</reference>
<page confidence="0.987245">
707
</page>
<reference confidence="0.99670068367347">
Deerwester, S., Dumais, S. T., Furnas, G. W.,
Landauer, T., and Harshman, R. 1990. Index-
ing by latent semantic analysis. Journal of the
American Society for Information Science,
41(6): 391-407
DeNero, J., Gillick, D., Zhang, J., and Klein, D.
2006. Why generative phrase models underper-
form surface heuristics. In Workshop on Statis-
tical Machine Translation, pp. 31-38.
Deng, L., Yu, D., and Platt, J. 2012. Scalable
stacking and learning for building deep archi-
tectures. In ICASSP.
Diamantaras, K. I., and Kung, S. Y. 1996. Princi-
ple Component Ieural Ietworks: Theory and
Applications. Wiley-Interscience.
Dumais S., Letsche T., Littman M. and Landauer
T. 1997. Automatic cross-language retrieval us-
ing latent semantic indexing. In AAAI-97
Spring Symposium Series: Cross-Language
Text and Speech Retrieval.
Ganchev, K., Graca, J., Gillenwater, J., and
Taskar, B. 2010. Posterior regularization for
structured latent variable models. Journal of
Machine Learning Research, 11 (2010): 2001-
2049.
Gao, J., and He, X. 2013. Training MRF-based
translation models using gradient ascent. In
IAACL-HLT, pp. 450-459.
Gao, J., Toutanova, K., Yih., W-T. 2011. Click-
through-based latent semantic models for web
search. In SIGIR, pp. 675-684.
He, X., and Deng, L. 2012. Maximum expected
bleu training of phrase and lexicon translation
models. In ACL, pp. 292-301.
Hinton, G., and Salakhutdinov, R., 2010. Discov-
ering Binary Codes for Documents by Learn-
ing Deep Generative Models. Topics in Cogni-
tive Science, pp. 1-18.
Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed,
A., Jaitly, N., Senior, A., Vanhoucke, V., Ngu-
yen, P., Sainath, T., and Kingsbury, B., 2012.
Deep neural networks for acoustic modeling in
speech recognition. IEEE Signal Processing
Magazine, vol. 29, no. 6, pp. 82-97.
Hofmann, T. 1999. Probabilistic latent semantic
indexing. In SIGIR, pp. 50-57.
Huang, P-S., He, X., Gao, J., Deng, L., Acero, A.
and Heck, L. 2013. Learning deep structured se-
mantic models for web search using click-
through data. In CIKM.
Kalchbrenner, N. and Blunsom, P. 2013. Recur-
rent continuous translation models. In EMILP.
Koehn, P., Hoang, H., Birch, A., Callison-Burch,
C., Federico, M., Bertoldi, N., Cowan, B., Shen,
W., Moran, C., Zens, R., Dyer, C., Bojar, O.,
Constantin, A., and Herbst, E. 2007. Moses:
open source toolkit for statistical machine trans-
lation. In ACL 2007, demonstration session.
Koehn, P. and Monz, C. 2006. Manual and auto-
matic evaluation of machine translation be-
tween European languages. In Workshop on
Statistical Machine Translation, pp. 102-121.
Koehn, P., Och, F., and Marcu, D. 2003. Statisti-
cal phrase-based translation. In HLT-IAACL,
pp. 127-133.
Lambert, P. and Banchs, R. E. 2005. Data inferred
multi-word expressions for statistical machine
translation. In MT Summit X, Phuket, Thailand.
Li, P., Liu, Y., and Sun, M. 2013. Recursive auto-
encoders for ITG-based translation. In EMILP.
Liang,P., Bouchard-Cote,A., Klein, D. and
Taskar, B. 2006. An end-to-end discriminative
approach to machine translation. In COLIIG-
ACL.
Marcu, D., and Wong, W. 2002. A phrase-based,
joint probability model for statistical machine
translation. In EMILP.
Mikolov, T., Karafiat, M., Burget, L., Cernocky,
J., and Khudanpur, S. 2010. Recurrent neural
network based language model. In IITER-
SPEECH, pp. 1045-1048.
Mikolov, T., Kombrink, S., Burget, L., Cernocky,
J., and Khudanpur, S. 2011. Extensions of re-
current neural network language model. In
ICASSP, pp. 5528-5531.
Mikolov, T. 2012. Statistical Language Model
based on Neural Networks. Ph.D. thesis, Brno
University of Technology.
Mikolov, T., Le, Q. V., and Sutskever, H. 2013a.
Exploiting similarities among languages for
machine translation. CoRR. 2013;
abs/1309.4148.
Mikolov, T., Yih, W. and Zweig, G. 2013b. Lin-
guistic Regularities in Continuous Space Word
Representations. In IAACL-HLT.
Mimno, D., Wallach, H., Naradowsky, J., Smith,
D. and McCallum, A. 2009. Polylingual topic
models. In EMILP.
</reference>
<page confidence="0.975652">
708
</page>
<reference confidence="0.997976844155845">
Niehues J., Herrmann, T., Vogel, S., and Waibel,
A. 2011. Wider context by using bilingual lan-
guage models in machine translation.
Och, F. 2003. Minimum error rate training in sta-
tistical machine translation. In ACL, pp. 160-
167.
Och, F., and Ney, H. 2004. The alignment tem-
plate approach to statistical machine translation.
Computational Linguistics, 29(1): 19-51.
Papineni, K., Roukos, S., Ward, T., and Zhu W-J.
2002. BLEU: a method for automatic evaluation
of machine translation. In ACL.
Platt, J., Toutanova, K., and Yih, W. 2010.
Translingual Document Representations from
Discriminative Projections. In EMILP.
Rosti, A-V., Hang, B., Matsoukas, S., and
Schwartz, R. S. 2011. Expected BLEU training
for graphs: bbn system description for WMT
system combination task. In Workshop on Sta-
tistical Machine Translation.
Schwenk, H., Costa-Jussa, M. R. and Fonollosa, J.
A. R. 2007. Smooth bilingual n-gram transla-
tion. In EMILP-CoILL, pp. 430-438.
Schwenk, H. 2012. Continuous space translation
models for phrase-based statistical machine
translation. In COLIIG.
Schwenk, H., Rousseau, A., and Mohammed A.
2012. Large, pruned or continuous space lan-
guage models on a GPU for statistical machine
translation. In IAACL-HLT Workshop on the
future of language modeling for HLT, pp. 11-
19.
Setiawan, H. and Zhou, B., 2013. Discriminative
training of 150 million translation parameters
and its application to pruning. In IAACL.
Socher, R., Huval, B., Manning, C., Ng, A., 2012.
Semantic Compositionality through Recursive
Matrix-Vector Spaces. In EMILP.
Socher, R., Lin, C., Ng, A. Y., and Manning, C. D.
2011. Parsing natural scenes and natural lan-
guage with recursive neural networks. In ICML.
Son, L. H., Allauzen, A., and Yvon, F. 2012. Con-
tinuous space translation models with neural
networks. In IAACL-HLT, pp. 29-48.
Sundermeyer, M., Oparin, I., Gauvain, J-L.
Freiberg, B., Schluter, R. and Ney, H. 2013.
Comparison of feed forward and recurrent neu-
ral network language models. In ICASSP, pp.
8430–8434.
Tur, G, Deng, L., Hakkani-Tur, D., and He, X.,
2012. Towards deeper understanding: deep con-
vex networks for semantic utterance classifica-
tion. In ICASSP.
Vinokourov,A., Shawe-Taylor,J. and Cristia-
nini,N. 2002. Inferring a semantic representa-
tion of text via cross-language correlation anal-
ysis. In IIPS.
Weston, J., Bengio, S., and Usunier, N. 2011.
Large scale image annotation: learning to rank
with joint word-image embeddings. In IJCAI.
Wuebker, J., Mauser, A., and Ney, H. 2010. Train-
ing phrase translation models with leaving-one-
out. In ACL, pp. 475-484.
Yih, W., Toutanova, K., Platt, J., and Meek, C.
2011. Learning discriminative projections for
text similarity measures. In CoILL.
Zhang, Y., Deng, L., He, X., and Acero, A. 2011.
A novel decision function and the associated de-
cision-feedback learning for speech translation.
In ICASSP.
Zhila, A., Yih, W., Meek, C., Zweig, G. and
Mikolov, T. 2013. Combining heterogeneous
models for measuring relational similarity. In
IAACL-HLT.
Zou, W. Y., Socher, R., Cer, D., and Manning, C.
D. 2013. Bilingual word embeddings for
phrase-based machine translation. In EMILP.
</reference>
<page confidence="0.998668">
709
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.608342">
<title confidence="0.9849025">Learning Continuous Phrase Representations Translation Modeling</title>
<author confidence="0.970885">Jianfeng Gao Xiaodong He Wen-tau Yih Li</author>
<affiliation confidence="0.976121">Microsoft</affiliation>
<address confidence="0.891905">One Microsoft Redmond, WA 98052,</address>
<email confidence="0.999703">jfgao@microsoft.com</email>
<email confidence="0.999703">xiaohe@microsoft.com</email>
<email confidence="0.999703">scottyih@microsoft.com</email>
<email confidence="0.999703">deng@microsoft.com</email>
<abstract confidence="0.990992714285714">This paper tackles the sparsity problem in estimating phrase translation probabilities by learning continuous phrase representations, whose distributed nature enables the sharing of related phrases in their representations. A pair of source and target phrases are projected into continuous-valued vector representations in a low-dimensional latent space, where their translation score is computed by the distance between the pair in this new space. The projection is performed by a neural network whose weights are learned on parallel training data. Experimental evaluation has been performed on two WMT translation tasks. Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.3 BLEU points.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Andrew</author>
<author>J Gao</author>
</authors>
<title>Scalable training of L1-regularized log-linear models.</title>
<date>2007</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="19593" citStr="Andrew and Gao (2007)" startWordPosition="3370" endWordPosition="3373">(𝛌T𝐡(𝐹𝑖, 𝐸, 𝐴)) Combining (8) and (11), we have 𝛿(𝑓,𝑒) = = 1 𝜕G(𝛉) 𝜕Z(𝛉) xBleu(𝛉) Z(𝛉) ( 𝜕sim𝛉 (𝐱𝑓, 𝐱𝑒) 𝜕sim𝛉 (𝐱𝑓, 𝐱𝑒) Because 𝛉 is only relevant to ℎ𝑀+1 which is defined in (4), we have 𝜕ℎ𝑀+1(𝐹𝑖, 𝐸, 𝐴) 𝜕sim𝛉(𝐱𝑓, 𝐱𝑒) = 𝜆𝑀+1𝑁(𝑓,𝑒;𝐴) (13) where 𝑁(𝑓, 𝑒; 𝐴) is the number of times the phrase pair (𝑓, 𝑒) occurs in 𝐴. Combining (12) and (13), we end up with the following equation 𝛿(𝑓,𝑒) = ∑ U(𝛉,𝐸)𝑃(𝐸|𝐹𝑖)𝜆𝑀+1𝑁(𝑓,𝑒; 𝐴) (𝐸,𝐴)∈𝐺𝐸𝑁(𝐹𝑖) where (14) U(𝛉, 𝐸) = sBleu(𝐸𝑖, 𝐸) − xBleu(𝛉). 5.2 The Training Algorithm In our experiments we train the parameters of the CPTM, 𝛉, using the L-BFGS optimizer described in Andrew and Gao (2007), together with the loss function described in (5). The gradient is computed as described in Sections 5.1. Although SGD has been advocated for neural network training due to its simplicity and its robustness to local minima (Bengio 2009), we find that in our task that the L-BFGS minimizes the loss in a desirable fashion empirically when iterating over the complete training data (batch mode). For example, the convergence of the algorithm was found to be smooth, despite the non-convexity in our loss. Another merit of batch training is that the gradient over all training data can be computed effi</context>
</contexts>
<marker>Andrew, Gao, 2007</marker>
<rawString>Andrew, G. and Gao, J. 2007. Scalable training of L1-regularized log-linear models. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Auli</author>
<author>M Galley</author>
<author>C Quirk</author>
<author>G Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="7341" citStr="Auli et al. 2013" startWordPosition="1162" endWordPosition="1165">ments as continuous vectors have a long history. Most of the earlier latent semantic models for learning such vectors are designed for information retrieval (Deerwester et al. 1990; Hofmann 1999; Blei et al. 2003). In contrast, recent work on continuous space language models, which estimate the probability of a word sequence in a continuous space (Bengio et al. 2003; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Auli, M., Galley, M., Quirk, C. and Zweig, G. 2013 Joint language and translation modeling with recurrent neural networks. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
</authors>
<title>Learning deep architectures for AI. Fundamental Trends</title>
<date>2009</date>
<booktitle>Machine Learning,</booktitle>
<volume>2</volume>
<pages>1--127</pages>
<contexts>
<context position="19830" citStr="Bengio 2009" startWordPosition="3411" endWordPosition="3412">, 𝑒; 𝐴) is the number of times the phrase pair (𝑓, 𝑒) occurs in 𝐴. Combining (12) and (13), we end up with the following equation 𝛿(𝑓,𝑒) = ∑ U(𝛉,𝐸)𝑃(𝐸|𝐹𝑖)𝜆𝑀+1𝑁(𝑓,𝑒; 𝐴) (𝐸,𝐴)∈𝐺𝐸𝑁(𝐹𝑖) where (14) U(𝛉, 𝐸) = sBleu(𝐸𝑖, 𝐸) − xBleu(𝛉). 5.2 The Training Algorithm In our experiments we train the parameters of the CPTM, 𝛉, using the L-BFGS optimizer described in Andrew and Gao (2007), together with the loss function described in (5). The gradient is computed as described in Sections 5.1. Although SGD has been advocated for neural network training due to its simplicity and its robustness to local minima (Bengio 2009), we find that in our task that the L-BFGS minimizes the loss in a desirable fashion empirically when iterating over the complete training data (batch mode). For example, the convergence of the algorithm was found to be smooth, despite the non-convexity in our loss. Another merit of batch training is that the gradient over all training data can be computed efficiently. As shown in Section 5.1, computing 𝜕simθ(x𝑓, x𝑒)/𝜕θ requires large-scale matrix multiplications, and is expensive for multi-layer neural networks. Eq. (7) suggests that 𝜕simθ(x𝑓,x𝑒)/𝜕θ and 𝛿(𝑓,𝑒) can be computed separately, thus</context>
</contexts>
<marker>Bengio, 2009</marker>
<rawString>Bengio, Y. 2009. Learning deep architectures for AI. Fundamental Trends Machine Learning, vol. 2, no. 1, pp. 1–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bengio</author>
<author>R Duharme</author>
<author>P Vincent</author>
<author>C Janvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>JMLR,</journal>
<pages>3--1137</pages>
<contexts>
<context position="4032" citStr="Bengio et al. 2003" startWordPosition="612" endWordPosition="615">r phrases. In this model, semantically or grammatically related phrases, in both the source and the target languages, would tend to have similar (close) feature vectors in the continuous space, guided by the training objective. Since the translation score is a smooth function of these feature vectors, a small 699 change in the features should only lead to a small change in the translation score. The primary research task in developing the CPTM is learning the continuous representation of a phrase that is effective for SMT. Motivated by recent studies on continuous-space language models (e.g., Bengio et al. 2003; Mikolov et al. 2011; Schwenk et al., 2012), we use a neural network to project a word vector to a feature vector. Ideally, the projection would discover those latent features that are useful to differentiate good translations from bad ones, for a given source phrase. However, there is no training data with explicit annotation on the quality of phrase translations. The phrase translation pairs are hidden in the parallel source-target sentence pairs, which are used to train the traditional translation models. The quality of a phrase translation can only be judged implicitly through the transla</context>
<context position="7092" citStr="Bengio et al. 2003" startWordPosition="1121" endWordPosition="1124">ase-based SMT and Sections 4 presents the CPTM. Section 5 describes the way the model parameters are estimated, followed by the experimental results in Section 6. Finally, Section 7 concludes the paper. 2 Related Work Representations of words or documents as continuous vectors have a long history. Most of the earlier latent semantic models for learning such vectors are designed for information retrieval (Deerwester et al. 1990; Hofmann 1999; Blei et al. 2003). In contrast, recent work on continuous space language models, which estimate the probability of a word sequence in a continuous space (Bengio et al. 2003; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; V</context>
</contexts>
<marker>Bengio, Duharme, Vincent, Janvin, 2003</marker>
<rawString>Bengio, Y., Duharme, R., Vincent, P., and Janvin, C. 2003. A neural probabilistic language model. JMLR, 3:1137-1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M J Jordan</author>
</authors>
<title>Latent Dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="6937" citStr="Blei et al. 2003" startWordPosition="1095" endWordPosition="1098"> not clear how a continuous The rest of the paper is organized as follows. Section 2 reviews previous work. Section 3 reviews the log-linear model for phrase-based SMT and Sections 4 presents the CPTM. Section 5 describes the way the model parameters are estimated, followed by the experimental results in Section 6. Finally, Section 7 concludes the paper. 2 Related Work Representations of words or documents as continuous vectors have a long history. Most of the earlier latent semantic models for learning such vectors are designed for information retrieval (Deerwester et al. 1990; Hofmann 1999; Blei et al. 2003). In contrast, recent work on continuous space language models, which estimate the probability of a word sequence in a continuous space (Bengio et al. 2003; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual sc</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D. M., Ng, A. Y., and Jordan, M. J. 2003. Latent Dirichlet allocation. Journal of Machine Learning Research, 3: 993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Collobert</author>
<author>J Weston</author>
<author>L Bottou</author>
<author>M Karlen</author>
<author>K Kavukcuoglu</author>
<author>P Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. 2011. Natural language processing (almost) from scratch. Journal of Machine Learning Research, vol. 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<pages>391--407</pages>
<contexts>
<context position="6904" citStr="Deerwester et al. 1990" startWordPosition="1089" endWordPosition="1092">e phrasebased approach. However, it is not clear how a continuous The rest of the paper is organized as follows. Section 2 reviews previous work. Section 3 reviews the log-linear model for phrase-based SMT and Sections 4 presents the CPTM. Section 5 describes the way the model parameters are estimated, followed by the experimental results in Section 6. Finally, Section 7 concludes the paper. 2 Related Work Representations of words or documents as continuous vectors have a long history. Most of the earlier latent semantic models for learning such vectors are designed for information retrieval (Deerwester et al. 1990; Hofmann 1999; Blei et al. 2003). In contrast, recent work on continuous space language models, which estimate the probability of a word sequence in a continuous space (Bengio et al. 2003; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T., and Harshman, R. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6): 391-407</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Gillick</author>
<author>J Zhang</author>
<author>D Klein</author>
</authors>
<title>Why generative phrase models underperform surface heuristics.</title>
<date>2006</date>
<booktitle>In Workshop on Statistical Machine Translation,</booktitle>
<pages>31--38</pages>
<contexts>
<context position="2182" citStr="DeNero et al. 2006" startWordPosition="319" endWordPosition="322">sed on counting these phrases or their words using the same word-aligned training data. Phrase-based SMT systems have achieved state-of-the-art performance largely due to the fact that long phrases, rather than single words, are used as translation units so that useful context information can be captured in selecting translations. However, longer phrases occur less often in training data, leading to a severe data sparseness problem in parameter estimation. There has been a plethora of research reported in the literature on improving parameter estimation for the phrase translation model (e.g., DeNero et al. 2006; Wuebker et al. 2010; He and Deng 2012; Gao and He 2013). This paper revisits the problem of scoring a phrase translation pair by developing a Continuous-space Phrase Translation Model (CPTM). The translation score of a phrase pair in this model is computed as follows. First, we represent each phrase as a bag-of-words vector, called word vector henceforth. We then project the word vector, in either the source language or the target language, into a respective continuous feature vector in a common low-dimensional space that is language independent. The projection is performed by a multi-layer </context>
</contexts>
<marker>DeNero, Gillick, Zhang, Klein, 2006</marker>
<rawString>DeNero, J., Gillick, D., Zhang, J., and Klein, D. 2006. Why generative phrase models underperform surface heuristics. In Workshop on Statistical Machine Translation, pp. 31-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Deng</author>
<author>D Yu</author>
<author>J Platt</author>
</authors>
<title>Scalable stacking and learning for building deep architectures.</title>
<date>2012</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="33544" citStr="Deng et al. 2012" startWordPosition="5783" endWordPosition="5786">tions generated using a multilayer neural network. Second, we present a new learning method to train the weights in the multilayer neural network for the end-to-end BLEU metric directly. The training method is based on L-BFGS. We describe in detail how the gradient in closed form, as required for efficient optimization, is derived. The objective function, which takes the form of the expected BLEU computed from N-best lists, is very different from the usual objective functions used in most existing architectures of neural networks, e.g., cross entropy (Hinton et al. 2012) or mean square error (Deng et al. 2012). We hence have provided details in the derivation of the gradient, which can serve as an example to guide the derivation of neural network learning with other non-standard objective functions in the future. Our evaluation on two WMT data sets show that incorporating the continuous-space phrase translation model into the log-linear framework significantly improves the accuracy of a state-ofthe-art phrase-based SMT system, leading to a gain up to 1.3 BLEU. Careful implementation of the L-BFGS optimization based on the BLEUcentric objective function, together with the associated closed-form grad</context>
</contexts>
<marker>Deng, Yu, Platt, 2012</marker>
<rawString>Deng, L., Yu, D., and Platt, J. 2012. Scalable stacking and learning for building deep architectures. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K I Diamantaras</author>
<author>S Y Kung</author>
</authors>
<title>Principle Component Ieural Ietworks: Theory and Applications.</title>
<date>1996</date>
<publisher>Wiley-Interscience.</publisher>
<marker>Diamantaras, Kung, 1996</marker>
<rawString>Diamantaras, K. I., and Kung, S. Y. 1996. Principle Component Ieural Ietworks: Theory and Applications. Wiley-Interscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>T Letsche</author>
<author>M Littman</author>
<author>T Landauer</author>
</authors>
<title>Automatic cross-language retrieval using latent semantic indexing.</title>
<date>1997</date>
<booktitle>In AAAI-97 Spring Symposium Series: Cross-Language Text and Speech Retrieval.</booktitle>
<contexts>
<context position="7670" citStr="Dumais et al. 1997" startWordPosition="1216" endWordPosition="1219">n a continuous space (Bengio et al. 2003; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed</context>
</contexts>
<marker>Dumais, Letsche, Littman, Landauer, 1997</marker>
<rawString>Dumais S., Letsche T., Littman M. and Landauer T. 1997. Automatic cross-language retrieval using latent semantic indexing. In AAAI-97 Spring Symposium Series: Cross-Language Text and Speech Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>J Graca</author>
<author>J Gillenwater</author>
<author>B Taskar</author>
</authors>
<title>Posterior regularization for structured latent variable models.</title>
<date>2010</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>11</volume>
<pages>2001--2049</pages>
<marker>Ganchev, Graca, Gillenwater, Taskar, 2010</marker>
<rawString>Ganchev, K., Graca, J., Gillenwater, J., and Taskar, B. 2010. Posterior regularization for structured latent variable models. Journal of Machine Learning Research, 11 (2010): 2001-2049.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>X He</author>
</authors>
<title>Training MRF-based translation models using gradient ascent.</title>
<date>2013</date>
<booktitle>In IAACL-HLT,</booktitle>
<pages>450--459</pages>
<contexts>
<context position="2239" citStr="Gao and He 2013" startWordPosition="331" endWordPosition="334"> word-aligned training data. Phrase-based SMT systems have achieved state-of-the-art performance largely due to the fact that long phrases, rather than single words, are used as translation units so that useful context information can be captured in selecting translations. However, longer phrases occur less often in training data, leading to a severe data sparseness problem in parameter estimation. There has been a plethora of research reported in the literature on improving parameter estimation for the phrase translation model (e.g., DeNero et al. 2006; Wuebker et al. 2010; He and Deng 2012; Gao and He 2013). This paper revisits the problem of scoring a phrase translation pair by developing a Continuous-space Phrase Translation Model (CPTM). The translation score of a phrase pair in this model is computed as follows. First, we represent each phrase as a bag-of-words vector, called word vector henceforth. We then project the word vector, in either the source language or the target language, into a respective continuous feature vector in a common low-dimensional space that is language independent. The projection is performed by a multi-layer neural network. The projected feature vector forms the co</context>
<context position="9666" citStr="Gao and He 2013" startWordPosition="1552" endWordPosition="1555"> proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires learning a mapping between input F ∈ ℱ to output E ∈ ℰ. We are given • Training samples (Fi, Ei) for 𝑖 = 1 ... 𝑁, where eac</context>
<context position="29244" citStr="Gao and He (2013)" startWordPosition="5052" endWordPosition="5055">on the parallel bilingual training data described earlier. Specifically, BTLMPR is an extension to PLSA, and is the best performer among different versions of the Bi-Lingual Topic Model (BLTM) described in Gao et al. (2011). BLTM with Posterior Regularization (BLTMPR) is trained on parallel training data using the EM algorithm with a constraint enforcing a source sentence and its paralleled target sentence to not only share the same prior topic distribution, but to also have similar fractions of words assigned to each topic. We incorporated the model into the log-linear model for SMT (1) as 7 Gao and He (2013) reported results of MRF models with different feature sets. We picked the MRF using phrase features only (MRFP) for comparison since we are mainly interested in phrase representation. follows. First of all, the topic distribution of a source sentence Fi , denoted by P (z |Fi), is induced from the learned topic-word distributions using EM. Then, each translation candidate E in the N-best list GEN(Fi) is scored as P(E|Fi) = II,cEZ.P(w|Z)P(Z|Fi) P (Fi |E) can be similarly computed. Finally, the logarithms of the two probabilities are incorporated into the log-linear model of (1) as two additiona</context>
<context position="30629" citStr="Gao and He (2013)" startWordPosition="5298" endWordPosition="5301">tence to a feature vector. The projection matrix is learned on parallel training data using the S2Net algorithm (Yih et al. 2011). DPM can be incorporated into the log-linear model for SMT (1) by introducing a new feature hmr+1 for each phrase pair, which is defined as the cosine similarity of the phrases in the project space. As we see from Table 1, both latent semantic models, although leading to some slight improvement over Baseline, are much less effective than CPTM. Finally, we compare the CPTM with the Markov Random Field model using phrase features (MRFP in Tables 1 and 2), proposed by Gao and He (2013)7, on both the WMT 2006 and WMT 2012 datasets. MRFp is a state-of-the-art large scale discriminative training model that uses the same expected BLEU training criterion, which has proven to give superior performance across a range of MT tasks recently (He and Deng 2012, Setiawan and Zhou 2013, Gao and He 2013). Unlike CPTM, MRFp is a linear model that simply treats each phrase pair as a single feature. Therefore, although both are trained using the 706 same expected BLEU based objective function, CPTM and MRFp model the translation relationship between two phrases from different angles. MRFp es</context>
</contexts>
<marker>Gao, He, 2013</marker>
<rawString>Gao, J., and He, X. 2013. Training MRF-based translation models using gradient ascent. In IAACL-HLT, pp. 450-459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gao</author>
<author>K Toutanova</author>
<author>W-T Yih</author>
</authors>
<title>Clickthrough-based latent semantic models for web search.</title>
<date>2011</date>
<booktitle>In SIGIR,</booktitle>
<pages>675--684</pages>
<contexts>
<context position="7747" citStr="Gao et al. 2011" startWordPosition="1232" endWordPosition="1235">e state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation p</context>
<context position="28453" citStr="Gao et al. 2011" startWordPosition="4917" endWordPosition="4920">than decomposing it into wordword translations in the CPTMs. Second, we see that the nonlinear projection is able to generate more effective features, leading to better results than the linear projection. We also compare the best version of the CPTM i.e., CPTM, with three related models proposed previously. We start the discussion with the results on the WMT 2006 data set in Table 1. Rows 5 and 6 in Table 1 are two state-of-theart latent semantic models that are originally trained on clicked query-document pairs (i.e., clickthrough data extracted from search logs) for query-document matching (Gao et al. 2011). To adopt these models for SMT, we view source-target sentence pairs as clicked query-document pairs, and trained both models using the same methods as in Gao et al. (2011) on the parallel bilingual training data described earlier. Specifically, BTLMPR is an extension to PLSA, and is the best performer among different versions of the Bi-Lingual Topic Model (BLTM) described in Gao et al. (2011). BLTM with Posterior Regularization (BLTMPR) is trained on parallel training data using the EM algorithm with a constraint enforcing a source sentence and its paralleled target sentence to not only shar</context>
<context position="29929" citStr="Gao et al. (2011)" startWordPosition="5167" endWordPosition="5170">ed the MRF using phrase features only (MRFP) for comparison since we are mainly interested in phrase representation. follows. First of all, the topic distribution of a source sentence Fi , denoted by P (z |Fi), is induced from the learned topic-word distributions using EM. Then, each translation candidate E in the N-best list GEN(Fi) is scored as P(E|Fi) = II,cEZ.P(w|Z)P(Z|Fi) P (Fi |E) can be similarly computed. Finally, the logarithms of the two probabilities are incorporated into the log-linear model of (1) as two additional features. DPM is the Discriminative Projection Model described in Gao et al. (2011), which is an extension of LSA. DPM uses a matrix to project a word vector of a sentence to a feature vector. The projection matrix is learned on parallel training data using the S2Net algorithm (Yih et al. 2011). DPM can be incorporated into the log-linear model for SMT (1) by introducing a new feature hmr+1 for each phrase pair, which is defined as the cosine similarity of the phrases in the project space. As we see from Table 1, both latent semantic models, although leading to some slight improvement over Baseline, are much less effective than CPTM. Finally, we compare the CPTM with the Mar</context>
</contexts>
<marker>Gao, Toutanova, Yih, 2011</marker>
<rawString>Gao, J., Toutanova, K., Yih., W-T. 2011. Clickthrough-based latent semantic models for web search. In SIGIR, pp. 675-684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X He</author>
<author>L Deng</author>
</authors>
<title>Maximum expected bleu training of phrase and lexicon translation models.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>292--301</pages>
<contexts>
<context position="2221" citStr="He and Deng 2012" startWordPosition="327" endWordPosition="330">rds using the same word-aligned training data. Phrase-based SMT systems have achieved state-of-the-art performance largely due to the fact that long phrases, rather than single words, are used as translation units so that useful context information can be captured in selecting translations. However, longer phrases occur less often in training data, leading to a severe data sparseness problem in parameter estimation. There has been a plethora of research reported in the literature on improving parameter estimation for the phrase translation model (e.g., DeNero et al. 2006; Wuebker et al. 2010; He and Deng 2012; Gao and He 2013). This paper revisits the problem of scoring a phrase translation pair by developing a Continuous-space Phrase Translation Model (CPTM). The translation score of a phrase pair in this model is computed as follows. First, we represent each phrase as a bag-of-words vector, called word vector henceforth. We then project the word vector, in either the source language or the target language, into a respective continuous feature vector in a common low-dimensional space that is language independent. The projection is performed by a multi-layer neural network. The projected feature v</context>
<context position="9648" citStr="He and Deng 2012" startWordPosition="1548" endWordPosition="1551">translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires learning a mapping between input F ∈ ℱ to output E ∈ ℰ. We are given • Training samples (Fi, Ei) for 𝑖 = </context>
<context position="30897" citStr="He and Deng 2012" startWordPosition="5342" endWordPosition="5345"> the cosine similarity of the phrases in the project space. As we see from Table 1, both latent semantic models, although leading to some slight improvement over Baseline, are much less effective than CPTM. Finally, we compare the CPTM with the Markov Random Field model using phrase features (MRFP in Tables 1 and 2), proposed by Gao and He (2013)7, on both the WMT 2006 and WMT 2012 datasets. MRFp is a state-of-the-art large scale discriminative training model that uses the same expected BLEU training criterion, which has proven to give superior performance across a range of MT tasks recently (He and Deng 2012, Setiawan and Zhou 2013, Gao and He 2013). Unlike CPTM, MRFp is a linear model that simply treats each phrase pair as a single feature. Therefore, although both are trained using the 706 same expected BLEU based objective function, CPTM and MRFp model the translation relationship between two phrases from different angles. MRFp estimates one translation score for each phrase pair explicitly without parameter sharing, while in CPTM, all phrases share the same neural network that projects raw phrases to the continuous space, providing a more smoothed estimation of the translation score for each </context>
</contexts>
<marker>He, Deng, 2012</marker>
<rawString>He, X., and Deng, L. 2012. Maximum expected bleu training of phrase and lexicon translation models. In ACL, pp. 292-301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hinton</author>
<author>R Salakhutdinov</author>
</authors>
<title>Discovering Binary Codes for Documents by Learning Deep Generative Models. Topics in Cognitive Science,</title>
<date>2010</date>
<pages>1--18</pages>
<marker>Hinton, Salakhutdinov, 2010</marker>
<rawString>Hinton, G., and Salakhutdinov, R., 2010. Discovering Binary Codes for Documents by Learning Deep Generative Models. Topics in Cognitive Science, pp. 1-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hinton</author>
<author>L Deng</author>
<author>D Yu</author>
<author>G Dahl</author>
<author>A Mohamed</author>
<author>N Jaitly</author>
<author>A Senior</author>
<author>V Vanhoucke</author>
<author>P Nguyen</author>
<author>T Sainath</author>
<author>B Kingsbury</author>
</authors>
<title>Deep neural networks for acoustic modeling in speech recognition.</title>
<date>2012</date>
<journal>IEEE Signal Processing Magazine,</journal>
<volume>29</volume>
<pages>82--97</pages>
<contexts>
<context position="33504" citStr="Hinton et al. 2012" startWordPosition="5774" endWordPosition="5778">. The space is derived from the representations generated using a multilayer neural network. Second, we present a new learning method to train the weights in the multilayer neural network for the end-to-end BLEU metric directly. The training method is based on L-BFGS. We describe in detail how the gradient in closed form, as required for efficient optimization, is derived. The objective function, which takes the form of the expected BLEU computed from N-best lists, is very different from the usual objective functions used in most existing architectures of neural networks, e.g., cross entropy (Hinton et al. 2012) or mean square error (Deng et al. 2012). We hence have provided details in the derivation of the gradient, which can serve as an example to guide the derivation of neural network learning with other non-standard objective functions in the future. Our evaluation on two WMT data sets show that incorporating the continuous-space phrase translation model into the log-linear framework significantly improves the accuracy of a state-ofthe-art phrase-based SMT system, leading to a gain up to 1.3 BLEU. Careful implementation of the L-BFGS optimization based on the BLEUcentric objective function, toget</context>
</contexts>
<marker>Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior, Vanhoucke, Nguyen, Sainath, Kingsbury, 2012</marker>
<rawString>Hinton, G., Deng, L., Yu, D., Dahl, G., Mohamed, A., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B., 2012. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Probabilistic latent semantic indexing.</title>
<date>1999</date>
<booktitle>In SIGIR,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="6918" citStr="Hofmann 1999" startWordPosition="1093" endWordPosition="1094">However, it is not clear how a continuous The rest of the paper is organized as follows. Section 2 reviews previous work. Section 3 reviews the log-linear model for phrase-based SMT and Sections 4 presents the CPTM. Section 5 describes the way the model parameters are estimated, followed by the experimental results in Section 6. Finally, Section 7 concludes the paper. 2 Related Work Representations of words or documents as continuous vectors have a long history. Most of the earlier latent semantic models for learning such vectors are designed for information retrieval (Deerwester et al. 1990; Hofmann 1999; Blei et al. 2003). In contrast, recent work on continuous space language models, which estimate the probability of a word sequence in a continuous space (Bengio et al. 2003; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models f</context>
</contexts>
<marker>Hofmann, 1999</marker>
<rawString>Hofmann, T. 1999. Probabilistic latent semantic indexing. In SIGIR, pp. 50-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P-S Huang</author>
<author>X He</author>
<author>J Gao</author>
<author>L Deng</author>
<author>A Acero</author>
<author>L Heck</author>
</authors>
<title>Learning deep structured semantic models for web search using clickthrough data. In CIKM.</title>
<date>2013</date>
<contexts>
<context position="7766" citStr="Huang et al. 2013" startWordPosition="1236" endWordPosition="1239">t in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. Howev</context>
</contexts>
<marker>Huang, He, Gao, Deng, Acero, Heck, 2013</marker>
<rawString>Huang, P-S., He, X., Gao, J., Deng, L., Acero, A. and Heck, L. 2013. Learning deep structured semantic models for web search using clickthrough data. In CIKM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kalchbrenner</author>
<author>P Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In EMILP.</booktitle>
<contexts>
<context position="9094" citStr="Kalchbrenner and Blunsom (2013)" startWordPosition="1451" endWordPosition="1455">ity of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more version of such a model can be trained efficiently because the factor models used by Son et al. cannot be applied directly. 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 20</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Kalchbrenner, N. and Blunsom, P. 2013. Recurrent continuous translation models. In EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
<author>C Dyer</author>
<author>O Bojar</author>
<author>A Constantin</author>
<author>E Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="10568" citStr="Koehn et al. 2007" startWordPosition="1710" endWordPosition="1713">nuous representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires learning a mapping between input F ∈ ℱ to output E ∈ ℰ. We are given • Training samples (Fi, Ei) for 𝑖 = 1 ... 𝑁, where each source sentence Fi is paired with a reference translation in target language Ei; • A procedure GEN to generate a list of N-best candidates GEN(Fi) for an input Fi, where GEN in this study is the baseline phrasebased SMT system, i.e., an in-house implementation of the Moses system (Koehn et al. 2007) that does not use the CPTM, and each E ∈ GEN(Fi) is labeled by the sentence-level BLEU score (He and Deng 2012), denoted by sBleu(Ei, E) , which measures the quality of E with respect to its reference translation Ei; • A vector of features 𝐡 ∈ R𝑀 that maps each (Fi, E) to a vector of feature values2; and • A parameter vector I ∈ R𝑀, which assigns a real-valued weight to each feature. 2 Our baseline system uses a set of standard features suggested in Koehn et al. (2007), which is also detailed in Section 6. The components GEN(.), 𝐡 and I define a loglinear model that maps Fi to an output sente</context>
<context position="22040" citStr="Koehn et al. 2007" startWordPosition="3783" endWordPosition="3786">) can be further regularized (e.g. by adding a term of 𝐿2 norm) to deal with overfitting. However, we did not find clear empirical advantage over the simpler early stop approach in a pilot study, which is adopted in the experiments in this paper. 6 Experiments This section evaluates the CPTM presented on two translation tasks using WMT data sets. We first describe the data sets and baseline setup. Then we present experiments where we compare different versions of the CPTM and previous models. 6.1 Experimental Setup Baseline. We experiment with an in-house phrase-based system similar to Moses (Koehn et al. 2007), where the translation candidates are scored by a set of common features including maximum likelihood estimates of source given target phrase mappings 𝑃𝑀𝐿𝐸 (𝑒 |𝑓) and vice versa 𝑃𝑀𝐿𝐸 (𝑓 |𝑒), as well as lexical weighting estimates 𝑃𝐿𝑊 (𝑒 |𝑓) and 𝑃𝐿𝑊 (𝑓 |𝑒), word and phrase penalties, a linear distortion feature, and a lexicalized reordering feature. The baseline includes a standard 5-gram modified Kneser-Ney language model trained on the target side of the parallel corpora described below. Log-linear weights are estimated with the MERT algorithm (Och 2003). 𝜕𝛌T𝐡(𝐹𝑖, 𝐸, 𝐴) = 𝜆𝑀+1 𝜕sim𝛉 (𝐱𝑓, 𝐱𝑒)</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B., Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and Herbst, E. 2007. Moses: open source toolkit for statistical machine translation. In ACL 2007, demonstration session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>C Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between European languages.</title>
<date>2006</date>
<booktitle>In Workshop on Statistical Machine Translation,</booktitle>
<pages>102--121</pages>
<contexts>
<context position="22840" citStr="Koehn and Monz 2006" startWordPosition="3918" endWordPosition="3921">𝑀𝐿𝐸 (𝑓 |𝑒), as well as lexical weighting estimates 𝑃𝐿𝑊 (𝑒 |𝑓) and 𝑃𝐿𝑊 (𝑓 |𝑒), word and phrase penalties, a linear distortion feature, and a lexicalized reordering feature. The baseline includes a standard 5-gram modified Kneser-Ney language model trained on the target side of the parallel corpora described below. Log-linear weights are estimated with the MERT algorithm (Och 2003). 𝜕𝛌T𝐡(𝐹𝑖, 𝐸, 𝐴) = 𝜆𝑀+1 𝜕sim𝛉 (𝐱𝑓, 𝐱𝑒) 𝜕xBleu(𝛉) (12) 𝜕sim𝛉(𝐱𝑓, 𝐱𝑒) 704 Evaluation. We test our models on two different data sets. First, we train an English to French system based on the data of WMT 2006 shared task (Koehn and Monz 2006). The parallel corpus includes 688K sentence pairs of parliamentary proceedings for training. The development set contains 2000 sentences, and the test set contains other 2000 sentences, all from the official WMT 2006 shared task. Second, we experiment with a French to English system developed using 2.1M sentence pairs of training data, which amounts to 102M words, from the WMT 2012 campaign. The majority of the training data set is parliamentary proceedings except for 5M words which are newswire. We use the 2009 newswire data set, comprising 2525 sentences, as the development set. We evaluate</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Koehn, P. and Monz, C. 2006. Manual and automatic evaluation of machine translation between European languages. In Workshop on Statistical Machine Translation, pp. 102-121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>F Och</author>
<author>D Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In HLT-IAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="1301" citStr="Koehn et al. 2003" startWordPosition="182" endWordPosition="185">ction is performed by a neural network whose weights are learned on parallel training data. Experimental evaluation has been performed on two WMT translation tasks. Our best result improves the performance of a state-of-the-art phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.3 BLEU points. 1 Introduction The phrase translation model, also known as the phrase table, is one of the core components of phrase-based statistical machine translation (SMT) systems. The most common method of constructing the phrase table takes a two-phase approach (Koehn et al. 2003). First, the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data. The second phase, which is the focus of this paper, is parameter estimation where each phrase pair is assigned with some scores that are estimated based on counting these phrases or their words using the same word-aligned training data. Phrase-based SMT systems have achieved state-of-the-art performance largely due to the fact that long phrases, rather than single words, are used as translation units so that useful context information can be captured in selecting translations. Howe</context>
<context position="25619" citStr="Koehn et al. 2003" startWordPosition="4412" endWordPosition="4415">ay 500 nodes. But the model # Systems WMT test2006 1 Baseline 33.06 2 CPTM 34.10α 3 CPTML 33.60αβ 4 CPTMW 33.25β 5 BLTMPR 33.15β 6 DPM 33.29β 7 MRFP 33.91α 8 Comb (2 + 7) 34.39αβ Table 1: BLEU results for the English to French task using translation models and systems built on the WMT 2006 data set. The superscripts α and β indicate statistically significant difference (p &lt; 0.05) from Baseline and CPTM, respectively. projection or a multi-layer nonlinear projection; and (2) whether to compute the phrase similarity using word-word similarities as suggested by e.g., the lexical weighting model (Koehn et al. 2003). We compare these variants on the WMT 2006 data set, as shown in Table 1. CPTML (Row 3 in Table 1) uses a linear neural network to project a word vector of a phrase 𝐱 to a feature vector 𝐲: 𝐲 ≡ 𝜙(𝐱) = 𝐖T𝐱, where 𝐖 is a 𝑑 × 100 projection matrix. The translation score of a source phrase f and a target phrase a is measured as the similarity of their feature vectors. We choose cosine similarity because it works better than dot product for linear projection. CPTMW (Row 4 in Table 1) computes the phrase similarity using word-word similarity scores. This follows the common smoothing strategy of add</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Koehn, P., Och, F., and Marcu, D. 2003. Statistical phrase-based translation. In HLT-IAACL, pp. 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Lambert</author>
<author>R E Banchs</author>
</authors>
<title>Data inferred multi-word expressions for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit X,</booktitle>
<location>Phuket, Thailand.</location>
<marker>Lambert, Banchs, 2005</marker>
<rawString>Lambert, P. and Banchs, R. E. 2005. Data inferred multi-word expressions for statistical machine translation. In MT Summit X, Phuket, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Li</author>
<author>Y Liu</author>
<author>M Sun</author>
</authors>
<title>Recursive autoencoders for ITG-based translation.</title>
<date>2013</date>
<booktitle>In EMILP.</booktitle>
<contexts>
<context position="9440" citStr="Li et al. 2013" startWordPosition="1509" endWordPosition="1512">00 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier </context>
</contexts>
<marker>Li, Liu, Sun, 2013</marker>
<rawString>Li, P., Liu, Y., and Sun, M. 2013. Recursive autoencoders for ITG-based translation. In EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Liang</author>
<author>A Bouchard-Cote</author>
<author>D Klein</author>
<author>B Taskar</author>
</authors>
<title>An end-to-end discriminative approach to machine translation.</title>
<date>2006</date>
<booktitle>In COLIIGACL.</booktitle>
<contexts>
<context position="11634" citStr="Liang et al. (2006)" startWordPosition="1900" endWordPosition="1903">sted in Koehn et al. (2007), which is also detailed in Section 6. The components GEN(.), 𝐡 and I define a loglinear model that maps Fi to an output sentence as follows: E∗ = argmax (𝐸,𝐴)∈GEN(𝐹𝑖) which states that given I and 𝐡, argmax returns the highest scoring translation E∗, maximizing over correspondences A. In phrase-based SMT, A consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phrases. Since computing the argmax exactly is intractable, it is commonly performed approximatedly by beam search (Och and Ney 2004). Following Liang et al. (2006), we assume that every translation candidate is always coupled with a corresponding A, called the Viterbi derivation, generated by (1). 4 A Continuous-Space Phrase Translation Model (CPTM) The architecture of the CPTM is shown in Figures 1 and 2, where for each pair of source and target phrases (𝑓i, 𝑒𝑗) in a source-target sentence pair, we first project them into feature vectors y𝑓𝑖 and y𝑒𝑗 in a latent, continuous space via a neural network with one hidden layer (as shown in Figure 2), and then compute the translation score, score(𝑓i, 𝑒𝑗), by the distance of their feature vectors in that space</context>
</contexts>
<marker>Liang, Bouchard-Cote, Klein, Taskar, 2006</marker>
<rawString>Liang,P., Bouchard-Cote,A., Klein, D. and Taskar, B. 2006. An end-to-end discriminative approach to machine translation. In COLIIGACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>W Wong</author>
</authors>
<title>A phrase-based, joint probability model for statistical machine translation.</title>
<date>2002</date>
<booktitle>In EMILP.</booktitle>
<contexts>
<context position="9545" citStr="Marcu and Wong 2002" startWordPosition="1528" endWordPosition="1531">phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires le</context>
</contexts>
<marker>Marcu, Wong, 2002</marker>
<rawString>Marcu, D., and Wong, W. 2002. A phrase-based, joint probability model for statistical machine translation. In EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>M Karafiat</author>
<author>L Burget</author>
<author>J Cernocky</author>
<author>S Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In IITERSPEECH,</booktitle>
<pages>1045--1048</pages>
<contexts>
<context position="7114" citStr="Mikolov et al. 2010" startWordPosition="1125" endWordPosition="1128">ctions 4 presents the CPTM. Section 5 describes the way the model parameters are estimated, followed by the experimental results in Section 6. Finally, Section 7 concludes the paper. 2 Related Work Representations of words or documents as continuous vectors have a long history. Most of the earlier latent semantic models for learning such vectors are designed for information retrieval (Deerwester et al. 1990; Hofmann 1999; Blei et al. 2003). In contrast, recent work on continuous space language models, which estimate the probability of a word sequence in a continuous space (Bengio et al. 2003; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002;</context>
<context position="9156" citStr="Mikolov et al. 2010" startWordPosition="1462" endWordPosition="1465">ilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more version of such a model can be trained efficiently because the factor models used by Son et al. cannot be applied directly. 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They</context>
</contexts>
<marker>Mikolov, Karafiat, Burget, Cernocky, Khudanpur, 2010</marker>
<rawString>Mikolov, T., Karafiat, M., Burget, L., Cernocky, J., and Khudanpur, S. 2010. Recurrent neural network based language model. In IITERSPEECH, pp. 1045-1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>S Kombrink</author>
<author>L Burget</author>
<author>J Cernocky</author>
<author>S Khudanpur</author>
</authors>
<title>Extensions of recurrent neural network language model.</title>
<date>2011</date>
<booktitle>In ICASSP,</booktitle>
<pages>5528--5531</pages>
<contexts>
<context position="4053" citStr="Mikolov et al. 2011" startWordPosition="616" endWordPosition="619">odel, semantically or grammatically related phrases, in both the source and the target languages, would tend to have similar (close) feature vectors in the continuous space, guided by the training objective. Since the translation score is a smooth function of these feature vectors, a small 699 change in the features should only lead to a small change in the translation score. The primary research task in developing the CPTM is learning the continuous representation of a phrase that is effective for SMT. Motivated by recent studies on continuous-space language models (e.g., Bengio et al. 2003; Mikolov et al. 2011; Schwenk et al., 2012), we use a neural network to project a word vector to a feature vector. Ideally, the projection would discover those latent features that are useful to differentiate good translations from bad ones, for a given source phrase. However, there is no training data with explicit annotation on the quality of phrase translations. The phrase translation pairs are hidden in the parallel source-target sentence pairs, which are used to train the traditional translation models. The quality of a phrase translation can only be judged implicitly through the translation quality of the s</context>
</contexts>
<marker>Mikolov, Kombrink, Burget, Cernocky, Khudanpur, 2011</marker>
<rawString>Mikolov, T., Kombrink, S., Burget, L., Cernocky, J., and Khudanpur, S. 2011. Extensions of recurrent neural network language model. In ICASSP, pp. 5528-5531.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
</authors>
<title>Statistical Language Model based on Neural Networks.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Brno University of Technology.</institution>
<contexts>
<context position="7322" citStr="Mikolov 2012" startWordPosition="1160" endWordPosition="1161"> words or documents as continuous vectors have a long history. Most of the earlier latent semantic models for learning such vectors are designed for information retrieval (Deerwester et al. 1990; Hofmann 1999; Blei et al. 2003). In contrast, recent work on continuous space language models, which estimate the probability of a word sequence in a continuous space (Bengio et al. 2003; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT tra</context>
</contexts>
<marker>Mikolov, 2012</marker>
<rawString>Mikolov, T. 2012. Statistical Language Model based on Neural Networks. Ph.D. thesis, Brno University of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>Q V Le</author>
<author>H Sutskever</author>
</authors>
<title>Exploiting similarities among languages for machine translation.</title>
<date>2013</date>
<pages>1309--4148</pages>
<location>CoRR.</location>
<contexts>
<context position="9402" citStr="Mikolov et al. 2013" startWordPosition="1502" endWordPosition="1505">y Son et al. cannot be applied directly. 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the st</context>
</contexts>
<marker>Mikolov, Le, Sutskever, 2013</marker>
<rawString>Mikolov, T., Le, Q. V., and Sutskever, H. 2013a. Exploiting similarities among languages for machine translation. CoRR. 2013; abs/1309.4148.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mikolov</author>
<author>W Yih</author>
<author>G Zweig</author>
</authors>
<title>Linguistic Regularities in Continuous Space Word Representations.</title>
<date>2013</date>
<booktitle>In IAACL-HLT.</booktitle>
<contexts>
<context position="9402" citStr="Mikolov et al. 2013" startWordPosition="1502" endWordPosition="1505">y Son et al. cannot be applied directly. 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the st</context>
</contexts>
<marker>Mikolov, Yih, Zweig, 2013</marker>
<rawString>Mikolov, T., Yih, W. and Zweig, G. 2013b. Linguistic Regularities in Continuous Space Word Representations. In IAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Mimno</author>
<author>H Wallach</author>
<author>J Naradowsky</author>
<author>D Smith</author>
<author>A McCallum</author>
</authors>
<title>Polylingual topic models.</title>
<date>2009</date>
<booktitle>In EMILP.</booktitle>
<marker>Mimno, Wallach, Naradowsky, Smith, McCallum, 2009</marker>
<rawString>Mimno, D., Wallach, H., Naradowsky, J., Smith, D. and McCallum, A. 2009. Polylingual topic models. In EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Niehues</author>
<author>T Herrmann</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Wider context by using bilingual language models in machine translation.</title>
<date>2011</date>
<contexts>
<context position="6191" citStr="Niehues et al. (2011)" startWordPosition="970" endWordPosition="973"> the target language (to be detailed in Section 4) and that is shown to lead to significant improvement over a standard phrasebased SMT system (to be detailed in Section 6). Like the traditional phrase translation model, the translation score of each bilingual phrase pair is modeled explicitly in our model. However, instead of estimating the phrase translation score on aligned parallel data, our model intends to capture the grammatical and semantic similarity between a source phrase and its paired target phrase by projecting them into a common, continuous space that is language independent. 1 Niehues et al. (2011) use different translation units in order to integrate the n-gram translation model into the phrasebased approach. However, it is not clear how a continuous The rest of the paper is organized as follows. Section 2 reviews previous work. Section 3 reviews the log-linear model for phrase-based SMT and Sections 4 presents the CPTM. Section 5 describes the way the model parameters are estimated, followed by the experimental results in Section 6. Finally, Section 7 concludes the paper. 2 Related Work Representations of words or documents as continuous vectors have a long history. Most of the earlie</context>
</contexts>
<marker>Niehues, Herrmann, Vogel, Waibel, 2011</marker>
<rawString>Niehues J., Herrmann, T., Vogel, S., and Waibel, A. 2011. Wider context by using bilingual language models in machine translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="15239" citStr="Och 2003" startWordPosition="2559" endWordPosition="2560">e CPTM is incorporated, is parameterized by (𝛌, 𝛉), where 𝛌 is a vector of a handful of parameters used in the log-linear model of (1), with one weight for each feature; and 𝛉 is the projection matrices used in the CPTM defined by (2) and (3). In our experiments we take three steps to learn (𝛌, 𝛉): 1. We use a baseline phrase-based SMT system to generate for each source sentence in training data an N-best list of translation hypotheses4. 2. We set 𝛌 to that of the baseline system and let 𝜆𝑀+1 = 1, and optimize 𝛉 w.r.t. a loss function on training datas. 3. We fix 𝛉, and optimize 𝛌 using MERT (Och 2003) to maximize BLEU on dev data. In the next section, we will describe Step 2 in detail as it is directly related to the CPTM training. lists needs to be significantly higher than that of the top-1 translations so that the CPTM can be effectively trained. 5 The initial value of 𝜆𝑀+1 can also be tuned using the dev set. However, we find in a pilot study that it is good enough to set it to 1 when the values of all the baseline feature weights, used in the log-linear model of (1), are properly normalized, such as by setting 𝜆𝑚 = 𝜆𝑚/𝐶 for 𝑚 = 1... 𝑀 , where 𝐶 is the unnormalized weight value of the </context>
<context position="22602" citStr="Och 2003" startWordPosition="3875" endWordPosition="3876">sed system similar to Moses (Koehn et al. 2007), where the translation candidates are scored by a set of common features including maximum likelihood estimates of source given target phrase mappings 𝑃𝑀𝐿𝐸 (𝑒 |𝑓) and vice versa 𝑃𝑀𝐿𝐸 (𝑓 |𝑒), as well as lexical weighting estimates 𝑃𝐿𝑊 (𝑒 |𝑓) and 𝑃𝐿𝑊 (𝑓 |𝑒), word and phrase penalties, a linear distortion feature, and a lexicalized reordering feature. The baseline includes a standard 5-gram modified Kneser-Ney language model trained on the target side of the parallel corpora described below. Log-linear weights are estimated with the MERT algorithm (Och 2003). 𝜕𝛌T𝐡(𝐹𝑖, 𝐸, 𝐴) = 𝜆𝑀+1 𝜕sim𝛉 (𝐱𝑓, 𝐱𝑒) 𝜕xBleu(𝛉) (12) 𝜕sim𝛉(𝐱𝑓, 𝐱𝑒) 704 Evaluation. We test our models on two different data sets. First, we train an English to French system based on the data of WMT 2006 shared task (Koehn and Monz 2006). The parallel corpus includes 688K sentence pairs of parliamentary proceedings for training. The development set contains 2000 sentences, and the test set contains other 2000 sentences, all from the official WMT 2006 shared task. Second, we experiment with a French to English system developed using 2.1M sentence pairs of training data, which amounts to 102M w</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Och, F. 2003. Minimum error rate training in statistical machine translation. In ACL, pp. 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="11603" citStr="Och and Ney 2004" startWordPosition="1895" endWordPosition="1898">et of standard features suggested in Koehn et al. (2007), which is also detailed in Section 6. The components GEN(.), 𝐡 and I define a loglinear model that maps Fi to an output sentence as follows: E∗ = argmax (𝐸,𝐴)∈GEN(𝐹𝑖) which states that given I and 𝐡, argmax returns the highest scoring translation E∗, maximizing over correspondences A. In phrase-based SMT, A consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phrases. Since computing the argmax exactly is intractable, it is commonly performed approximatedly by beam search (Och and Ney 2004). Following Liang et al. (2006), we assume that every translation candidate is always coupled with a corresponding A, called the Viterbi derivation, generated by (1). 4 A Continuous-Space Phrase Translation Model (CPTM) The architecture of the CPTM is shown in Figures 1 and 2, where for each pair of source and target phrases (𝑓i, 𝑒𝑗) in a source-target sentence pair, we first project them into feature vectors y𝑓𝑖 and y𝑒𝑗 in a latent, continuous space via a neural network with one hidden layer (as shown in Figure 2), and then compute the translation score, score(𝑓i, 𝑒𝑗), by the distance of thei</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>Och, F., and Ney, H. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 29(1): 19-51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W-J Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="23829" citStr="Papineni et al. 2002" startWordPosition="4086" endWordPosition="4089">from the WMT 2012 campaign. The majority of the training data set is parliamentary proceedings except for 5M words which are newswire. We use the 2009 newswire data set, comprising 2525 sentences, as the development set. We evaluate on four newswire domain test sets from 2008, 2010 and 2011 as well as the 2010 system combination test set, containing 2034 to 3003 sentences. In this study we perform a detailed empirical comparison using the WMT 2006 data set, and verify our best models and results using the larger WMT 2012 data set. The metric used for evaluation is case insensitive BLEU score (Papineni et al. 2002). We also perform a significance test using the Wilcoxon signed rank test. Differences are considered statistically significant when the p-value is less than 0.05. 6.2 Results of the CPTM Table 1 shows the results measured in BLEU evaluated on the WMT 2006 data set, where Row 1 is the baseline system. Rows 2 to 4 are the systems enhanced by integrating different versions of the CPTM. Rows 5 to 7 present the results of previous models. Row 8 is our best system. Table 2 shows the main results on the WMT 2012 data set. CPTM is the model described in Sections 4. As illustrated in Figure 2, the num</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Papineni, K., Roukos, S., Ward, T., and Zhu W-J. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Platt</author>
<author>K Toutanova</author>
<author>W Yih</author>
</authors>
<title>Translingual Document Representations from Discriminative Projections.</title>
<date>2010</date>
<booktitle>In EMILP.</booktitle>
<contexts>
<context position="7689" citStr="Platt et al. 2010" startWordPosition="1220" endWordPosition="1223"> (Bengio et al. 2003; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural net</context>
</contexts>
<marker>Platt, Toutanova, Yih, 2010</marker>
<rawString>Platt, J., Toutanova, K., and Yih, W. 2010. Translingual Document Representations from Discriminative Projections. In EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A-V Rosti</author>
<author>B Hang</author>
<author>S Matsoukas</author>
<author>R S Schwartz</author>
</authors>
<title>Expected BLEU training for graphs: bbn system description for WMT system combination task.</title>
<date>2011</date>
<booktitle>In Workshop on Statistical Machine Translation.</booktitle>
<marker>Rosti, Hang, Matsoukas, Schwartz, 2011</marker>
<rawString>Rosti, A-V., Hang, B., Matsoukas, S., and Schwartz, R. S. 2011. Expected BLEU training for graphs: bbn system description for WMT system combination task. In Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
<author>M R Costa-Jussa</author>
<author>J A R Fonollosa</author>
</authors>
<title>Smooth bilingual n-gram translation.</title>
<date>2007</date>
<booktitle>In EMILP-CoILL,</booktitle>
<pages>430--438</pages>
<contexts>
<context position="8222" citStr="Schwenk et al. 2007" startWordPosition="1309" endWordPosition="1312">e projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more version of such a model can be trained efficiently because the factor models used by Son et al. cannot be applied directly</context>
</contexts>
<marker>Schwenk, Costa-Jussa, Fonollosa, 2007</marker>
<rawString>Schwenk, H., Costa-Jussa, M. R. and Fonollosa, J. A. R. 2007. Smooth bilingual n-gram translation. In EMILP-CoILL, pp. 430-438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
</authors>
<title>Continuous space translation models for phrase-based statistical machine translation.</title>
<date>2012</date>
<booktitle>In COLIIG.</booktitle>
<contexts>
<context position="8236" citStr="Schwenk 2012" startWordPosition="1313" endWordPosition="1314">shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more version of such a model can be trained efficiently because the factor models used by Son et al. cannot be applied directly. 700 widely u</context>
</contexts>
<marker>Schwenk, 2012</marker>
<rawString>Schwenk, H. 2012. Continuous space translation models for phrase-based statistical machine translation. In COLIIG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schwenk</author>
<author>A Rousseau</author>
<author>A Mohammed</author>
</authors>
<title>Large, pruned or continuous space language models on a GPU for statistical machine translation.</title>
<date>2012</date>
<booktitle>In IAACL-HLT Workshop on the future of language modeling for HLT,</booktitle>
<pages>11--19</pages>
<contexts>
<context position="4076" citStr="Schwenk et al., 2012" startWordPosition="620" endWordPosition="623"> grammatically related phrases, in both the source and the target languages, would tend to have similar (close) feature vectors in the continuous space, guided by the training objective. Since the translation score is a smooth function of these feature vectors, a small 699 change in the features should only lead to a small change in the translation score. The primary research task in developing the CPTM is learning the continuous representation of a phrase that is effective for SMT. Motivated by recent studies on continuous-space language models (e.g., Bengio et al. 2003; Mikolov et al. 2011; Schwenk et al., 2012), we use a neural network to project a word vector to a feature vector. Ideally, the projection would discover those latent features that are useful to differentiate good translations from bad ones, for a given source phrase. However, there is no training data with explicit annotation on the quality of phrase translations. The phrase translation pairs are hidden in the parallel source-target sentence pairs, which are used to train the traditional translation models. The quality of a phrase translation can only be judged implicitly through the translation quality of the sentences, as measured b</context>
</contexts>
<marker>Schwenk, Rousseau, Mohammed, 2012</marker>
<rawString>Schwenk, H., Rousseau, A., and Mohammed A. 2012. Large, pruned or continuous space language models on a GPU for statistical machine translation. In IAACL-HLT Workshop on the future of language modeling for HLT, pp. 11-19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Setiawan</author>
<author>B Zhou</author>
</authors>
<title>Discriminative training of 150 million translation parameters and its application to pruning.</title>
<date>2013</date>
<booktitle>In IAACL.</booktitle>
<contexts>
<context position="30921" citStr="Setiawan and Zhou 2013" startWordPosition="5346" endWordPosition="5349">rity of the phrases in the project space. As we see from Table 1, both latent semantic models, although leading to some slight improvement over Baseline, are much less effective than CPTM. Finally, we compare the CPTM with the Markov Random Field model using phrase features (MRFP in Tables 1 and 2), proposed by Gao and He (2013)7, on both the WMT 2006 and WMT 2012 datasets. MRFp is a state-of-the-art large scale discriminative training model that uses the same expected BLEU training criterion, which has proven to give superior performance across a range of MT tasks recently (He and Deng 2012, Setiawan and Zhou 2013, Gao and He 2013). Unlike CPTM, MRFp is a linear model that simply treats each phrase pair as a single feature. Therefore, although both are trained using the 706 same expected BLEU based objective function, CPTM and MRFp model the translation relationship between two phrases from different angles. MRFp estimates one translation score for each phrase pair explicitly without parameter sharing, while in CPTM, all phrases share the same neural network that projects raw phrases to the continuous space, providing a more smoothed estimation of the translation score for each phrase pair. The results</context>
</contexts>
<marker>Setiawan, Zhou, 2013</marker>
<rawString>Setiawan, H. and Zhou, B., 2013. Discriminative training of 150 million translation parameters and its application to pruning. In IAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>B Huval</author>
<author>C Manning</author>
<author>A Ng</author>
</authors>
<title>Semantic Compositionality through Recursive Matrix-Vector Spaces.</title>
<date>2012</date>
<booktitle>In EMILP.</booktitle>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Socher, R., Huval, B., Manning, C., Ng, A., 2012. Semantic Compositionality through Recursive Matrix-Vector Spaces. In EMILP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Socher</author>
<author>C Lin</author>
<author>A Y Ng</author>
<author>C D Manning</author>
</authors>
<title>Parsing natural scenes and natural language with recursive neural networks.</title>
<date>2011</date>
<booktitle>In ICML.</booktitle>
<marker>Socher, Lin, Ng, Manning, 2011</marker>
<rawString>Socher, R., Lin, C., Ng, A. Y., and Manning, C. D. 2011. Parsing natural scenes and natural language with recursive neural networks. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L H Son</author>
<author>A Allauzen</author>
<author>F Yvon</author>
</authors>
<title>Continuous space translation models with neural networks.</title>
<date>2012</date>
<booktitle>In IAACL-HLT,</booktitle>
<pages>29--48</pages>
<contexts>
<context position="8254" citStr="Son et al. 2012" startWordPosition="1315" endWordPosition="1318">vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more version of such a model can be trained efficiently because the factor models used by Son et al. cannot be applied directly. 700 widely used in modern SMT </context>
<context position="26408" citStr="Son et al. 2012" startWordPosition="4555" endWordPosition="4558">feature vector 𝐲: 𝐲 ≡ 𝜙(𝐱) = 𝐖T𝐱, where 𝐖 is a 𝑑 × 100 projection matrix. The translation score of a source phrase f and a target phrase a is measured as the similarity of their feature vectors. We choose cosine similarity because it works better than dot product for linear projection. CPTMW (Row 4 in Table 1) computes the phrase similarity using word-word similarity scores. This follows the common smoothing strategy of addressing the data sparseness problem in modeling phrase translations, such as the lexical weighting model (Koehn et al. 2003) and the word factored n-gram translation model (Son et al. 2012). Let 𝑤 denote a word, and 𝑓 and 𝑒 the source and target phrases, respectively. We define sim(𝑓, 𝑒) = 1 |𝑓 |∑𝑤∈𝑓 sim𝜏(𝑤, 𝑒) + 1 |𝑒 |∑𝑤∈𝑒 sim𝜏(𝑤,𝑓) where sim𝜏 (𝑤, 𝑒) (or sim𝜏 (𝑤, 𝑓 )) is the wordphrase similarity, and is defined as a smooth approximation of the maximum function sim𝜏(𝑤, 𝑒) = ∑𝑤′∈𝑒 sim(𝑤, 𝑤′) exp(𝜏sim(𝑤, 𝑤′)) ∑𝑤′∈𝑒 exp(𝜏sim(𝑤, 𝑤′)) training is too slow to perform a detailed study within a reasonable time. Therefore, all the models reported in this paper use 100 nodes. 705 # Systems dev news2011 news2010 news2008 newssyscomb2010 1 Baseline 23.58 25.24 24.35 20.36 24.14 2 MRFP 24.0</context>
</contexts>
<marker>Son, Allauzen, Yvon, 2012</marker>
<rawString>Son, L. H., Allauzen, A., and Yvon, F. 2012. Continuous space translation models with neural networks. In IAACL-HLT, pp. 29-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sundermeyer</author>
<author>I Oparin</author>
<author>J-L Freiberg Gauvain</author>
<author>B Schluter</author>
<author>R</author>
<author>H Ney</author>
</authors>
<title>Comparison of feed forward and recurrent neural network language models.</title>
<date>2013</date>
<booktitle>In ICASSP,</booktitle>
<pages>8430--8434</pages>
<contexts>
<context position="7284" citStr="Sundermeyer et al. 2013" startWordPosition="1152" endWordPosition="1156">ludes the paper. 2 Related Work Representations of words or documents as continuous vectors have a long history. Most of the earlier latent semantic models for learning such vectors are designed for information retrieval (Deerwester et al. 1990; Hofmann 1999; Blei et al. 2003). In contrast, recent work on continuous space language models, which estimate the probability of a word sequence in a continuous space (Bengio et al. 2003; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although deco</context>
</contexts>
<marker>Sundermeyer, Oparin, Gauvain, Schluter, R, Ney, 2013</marker>
<rawString>Sundermeyer, M., Oparin, I., Gauvain, J-L. Freiberg, B., Schluter, R. and Ney, H. 2013. Comparison of feed forward and recurrent neural network language models. In ICASSP, pp. 8430–8434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Tur</author>
<author>L Deng</author>
<author>D Hakkani-Tur</author>
<author>X He</author>
</authors>
<title>Towards deeper understanding: deep convex networks for semantic utterance classification.</title>
<date>2012</date>
<booktitle>In ICASSP.</booktitle>
<marker>Tur, Deng, Hakkani-Tur, He, 2012</marker>
<rawString>Tur, G, Deng, L., Hakkani-Tur, D., and He, X., 2012. Towards deeper understanding: deep convex networks for semantic utterance classification. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vinokourov</author>
<author>J Shawe-Taylor</author>
<author>N Cristianini</author>
</authors>
<title>Inferring a semantic representation of text via cross-language correlation analysis.</title>
<date>2002</date>
<booktitle>In IIPS.</booktitle>
<contexts>
<context position="7713" citStr="Vinokourov et al. 2002" startWordPosition="1224" endWordPosition="1227">3; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is e</context>
</contexts>
<marker>Vinokourov, Shawe-Taylor, Cristianini, 2002</marker>
<rawString>Vinokourov,A., Shawe-Taylor,J. and Cristianini,N. 2002. Inferring a semantic representation of text via cross-language correlation analysis. In IIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Weston</author>
<author>S Bengio</author>
<author>N Usunier</author>
</authors>
<title>Large scale image annotation: learning to rank with joint word-image embeddings.</title>
<date>2011</date>
<booktitle>In IJCAI.</booktitle>
<marker>Weston, Bengio, Usunier, 2011</marker>
<rawString>Weston, J., Bengio, S., and Usunier, N. 2011. Large scale image annotation: learning to rank with joint word-image embeddings. In IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wuebker</author>
<author>A Mauser</author>
<author>H Ney</author>
</authors>
<title>Training phrase translation models with leaving-oneout.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>475--484</pages>
<contexts>
<context position="2203" citStr="Wuebker et al. 2010" startWordPosition="323" endWordPosition="326">e phrases or their words using the same word-aligned training data. Phrase-based SMT systems have achieved state-of-the-art performance largely due to the fact that long phrases, rather than single words, are used as translation units so that useful context information can be captured in selecting translations. However, longer phrases occur less often in training data, leading to a severe data sparseness problem in parameter estimation. There has been a plethora of research reported in the literature on improving parameter estimation for the phrase translation model (e.g., DeNero et al. 2006; Wuebker et al. 2010; He and Deng 2012; Gao and He 2013). This paper revisits the problem of scoring a phrase translation pair by developing a Continuous-space Phrase Translation Model (CPTM). The translation score of a phrase pair in this model is computed as follows. First, we represent each phrase as a bag-of-words vector, called word vector henceforth. We then project the word vector, in either the source language or the target language, into a respective continuous feature vector in a common low-dimensional space that is language independent. The projection is performed by a multi-layer neural network. The p</context>
<context position="9610" citStr="Wuebker et al. 2010" startWordPosition="1540" endWordPosition="1543">arget language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires learning a mapping between input F ∈ ℱ to output E ∈ ℰ. We are give</context>
</contexts>
<marker>Wuebker, Mauser, Ney, 2010</marker>
<rawString>Wuebker, J., Mauser, A., and Ney, H. 2010. Training phrase translation models with leaving-oneout. In ACL, pp. 475-484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Yih</author>
<author>K Toutanova</author>
<author>J Platt</author>
<author>C Meek</author>
</authors>
<title>Learning discriminative projections for text similarity measures.</title>
<date>2011</date>
<booktitle>In CoILL.</booktitle>
<contexts>
<context position="7730" citStr="Yih et al. 2011" startWordPosition="1228" endWordPosition="1231"> have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to repres</context>
<context position="30141" citStr="Yih et al. 2011" startWordPosition="5208" endWordPosition="5211"> induced from the learned topic-word distributions using EM. Then, each translation candidate E in the N-best list GEN(Fi) is scored as P(E|Fi) = II,cEZ.P(w|Z)P(Z|Fi) P (Fi |E) can be similarly computed. Finally, the logarithms of the two probabilities are incorporated into the log-linear model of (1) as two additional features. DPM is the Discriminative Projection Model described in Gao et al. (2011), which is an extension of LSA. DPM uses a matrix to project a word vector of a sentence to a feature vector. The projection matrix is learned on parallel training data using the S2Net algorithm (Yih et al. 2011). DPM can be incorporated into the log-linear model for SMT (1) by introducing a new feature hmr+1 for each phrase pair, which is defined as the cosine similarity of the phrases in the project space. As we see from Table 1, both latent semantic models, although leading to some slight improvement over Baseline, are much less effective than CPTM. Finally, we compare the CPTM with the Markov Random Field model using phrase features (MRFP in Tables 1 and 2), proposed by Gao and He (2013)7, on both the WMT 2006 and WMT 2012 datasets. MRFp is a state-of-the-art large scale discriminative training mo</context>
</contexts>
<marker>Yih, Toutanova, Platt, Meek, 2011</marker>
<rawString>Yih, W., Toutanova, K., Platt, J., and Meek, C. 2011. Learning discriminative projections for text similarity measures. In CoILL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>L Deng</author>
<author>X He</author>
<author>A Acero</author>
</authors>
<title>A novel decision function and the associated decision-feedback learning for speech translation.</title>
<date>2011</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="9630" citStr="Zhang et al., 2011" startWordPosition="1544" endWordPosition="1547">ecurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires learning a mapping between input F ∈ ℱ to output E ∈ ℰ. We are given • Training samples</context>
</contexts>
<marker>Zhang, Deng, He, Acero, 2011</marker>
<rawString>Zhang, Y., Deng, L., He, X., and Acero, A. 2011. A novel decision function and the associated decision-feedback learning for speech translation. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zhila</author>
<author>W Yih</author>
<author>C Meek</author>
<author>G Zweig</author>
<author>T Mikolov</author>
</authors>
<title>Combining heterogeneous models for measuring relational similarity.</title>
<date>2013</date>
<booktitle>In IAACL-HLT.</booktitle>
<marker>Zhila, Yih, Meek, Zweig, Mikolov, 2013</marker>
<rawString>Zhila, A., Yih, W., Meek, C., Zweig, G. and Mikolov, T. 2013. Combining heterogeneous models for measuring relational similarity. In IAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Y Zou</author>
<author>R Socher</author>
<author>D Cer</author>
<author>C D Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In EMILP.</booktitle>
<contexts>
<context position="7784" citStr="Zou et al. 2013" startWordPosition="1240" endWordPosition="1243">ing, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier </context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Zou, W. Y., Socher, R., Cer, D., and Manning, C. D. 2013. Bilingual word embeddings for phrase-based machine translation. In EMILP.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>