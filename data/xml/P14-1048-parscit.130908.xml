<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.999409">
A Linear-Time Bottom-Up Discourse Parser
with Constraints and Post-Editing
</title>
<author confidence="0.997616">
Vanessa Wei Feng
</author>
<affiliation confidence="0.9985155">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.888518">
Toronto, ON, Canada
</address>
<email confidence="0.999395">
weifeng@cs.toronto.edu
</email>
<sectionHeader confidence="0.994814" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948904761905">
Text-level discourse parsing remains a
challenge. The current state-of-the-art
overall accuracy in relation assignment is
55.73%, achieved by Joty et al. (2013).
However, their model has a high order of
time complexity, and thus cannot be ap-
plied in practice. In this work, we develop
a much faster model whose time complex-
ity is linear in the number of sentences.
Our model adopts a greedy bottom-up ap-
proach, with two linear-chain CRFs ap-
plied in cascade as local classifiers. To en-
hance the accuracy of the pipeline, we add
additional constraints in the Viterbi decod-
ing of the first CRF. In addition to effi-
ciency, our parser also significantly out-
performs the state of the art. Moreover,
our novel approach of post-editing, which
modifies a fully-built tree by considering
information from constituents on upper
levels, can further improve the accuracy.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9994011875">
Discourse parsing is the task of identifying the
presence and the type of the discourse relations
between discourse units. While research in dis-
course parsing can be partitioned into several di-
rections according to different theories and frame-
works, Rhetorical Structure Theory (RST) (Mann
and Thompson, 1988) is probably the most am-
bitious one, because it aims to identify not only
the discourse relations in a small local context, but
also the hierarchical tree structure for the full text:
from the relations relating the smallest discourse
units (called elementary discourse units, EDUs),
to the ones connecting paragraphs.
For example, Figure 1 shows a text fragment
consisting of two sentences with four EDUs in
total (e1-e4). Its discourse tree representation is
</bodyText>
<author confidence="0.694231">
Graeme Hirst
</author>
<affiliation confidence="0.857055666666667">
Department of Computer Science
University of Toronto
Toronto, ON, Canada
</affiliation>
<email confidence="0.993046">
gh@cs.toronto.edu
</email>
<bodyText confidence="0.999630048780488">
shown below the text, following the notation con-
vention of RST: the two EDUs e1 and e2 are re-
lated by a mononuclear relation CONSEQUENCE,
where e2 is the more salient span (called nucleus,
and e1 is called satellite); e3 and e4 are related by
another mononuclear relation CIRCUMSTANCE,
with e4 as the nucleus; the two spans e1:2 and e3:4
are further related by a multi-nuclear relation SE-
QUENCE, with both spans as the nucleus.
Conventionally, there are two major sub-tasks
related to text-level discourse parsing: (1) EDU
segmentation: to segment the raw text into EDUs,
and (2) tree-building: to build a discourse tree
from EDUs, representing the discourse relations in
the text. Since the first sub-task is considered rela-
tively easy, with the state-of-art accuracy at above
90% (Joty et al., 2012), the recent research focus
is on the second sub-task, and often uses manual
EDU segmentation.
The current state-of-the-art overall accuracy of
the tree-building sub-task, evaluated on the RST
Discourse Treebank (RST-DT, to be introduced in
Section 8), is 55.73% by Joty et al. (2013). How-
ever, as an optimal discourse parser, Joty et al.’s
model is highly inefficient in practice, with re-
spect to both their DCRF-based local classifiers,
and their CKY-like bottom-up parsing algorithm.
DCRF (Dynamic Conditional Random Fields) is
a generalization of linear-chain CRFs, in which
each time slice contains a set of state variables
and edges (Sutton et al., 2007). CKY parsing is
a bottom-up parsing algorithm which searches all
possible parsing paths by dynamic programming.
Therefore, despite its superior performance, their
model is infeasible in most realistic situations.
The main objective of this work is to develop
a more efficient discourse parser, with similar or
even better performance with respect to Joty et
al.’s optimal parser, but able to produce parsing re-
sults in real time.
Our contribution is three-fold. First, with a
</bodyText>
<page confidence="0.96322">
511
</page>
<note confidence="0.844716">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 511–521,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.936449">
[On Aug. 1, the state tore up its controls,]e1
[and food prices leaped]e2 [Without buffer
stocks,]e3 [inflation exploded.]e4
</bodyText>
<equation confidence="0.97694725">
wsj 1146
e1:4
sequence
e1:2 e3:4
</equation>
<bodyText confidence="0.999837333333333">
strategy is its efficiency in practice. Also, the em-
ployment of SVM classifiers allows the incorpora-
tion of rich features for better data representation
(Feng and Hirst, 2012). However, HILDA’s ap-
proach also has obvious weakness: the greedy al-
gorithm may lead to poor performance due to local
optima, and more importantly, the SVM classifiers
are not well-suited for solving structural problems
due to the difficulty of taking context into account.
</bodyText>
<figure confidence="0.6958515">
consequence circumstance
e1 e2 e3 e4
</figure>
<figureCaption confidence="0.967041">
Figure 1: An example text fragment composed of
two sentences and four EDUs, with its RST dis-
course tree representation shown below.
</figureCaption>
<bodyText confidence="0.999454578947369">
greedy bottom-up strategy, we develop a discourse
parser with a time complexity linear in the total
number of sentences in the document. As a re-
sult of successfully avoiding the expensive non-
greedy parsing algorithms, our discourse parser is
very efficient in practice. Second, by using two
linear-chain CRFs to label a sequence of discourse
constituents, we can incorporate contextual infor-
mation in a more natural way, compared to us-
ing traditional discriminative classifiers, such as
SVMs. Specifically, in the Viterbi decoding of
the first CRF, we include additional constraints
elicited from common sense, to make more ef-
fective local decisions. Third, after a discourse
(sub)tree is fully built from bottom up, we perform
a novel post-editing process by considering infor-
mation from the constituents on upper levels. We
show that this post-editing can further improve the
overall parsing performance.
</bodyText>
<sectionHeader confidence="0.999927" genericHeader="introduction">
2 Related work
</sectionHeader>
<subsectionHeader confidence="0.998032">
2.1 HILDA discourse parser
</subsectionHeader>
<bodyText confidence="0.99916875">
The HILDA discourse parser by Hernault et al.
(2010) is the first attempt at RST-style text-level
discourse parsing. It adopts a pipeline framework,
and greedily builds the discourse tree from the bot-
tom up. In particular, starting from EDUs, at each
step of the tree-building, a binary SVM classifier
is first applied to determine which pair of adjacent
discourse constituents should be merged to form a
larger span, and another multi-class SVM classi-
fier is then applied to assign the type of discourse
relation that holds between the chosen pair.
The strength of HILDA’s greedy tree-building
</bodyText>
<subsectionHeader confidence="0.99951">
2.2 Joty et al.’s joint model
</subsectionHeader>
<bodyText confidence="0.999967025">
Joty et al. (2013) approach the problem of text-
level discourse parsing using a model trained by
Conditional Random Fields (CRF). Their model
has two distinct features.
First, they decomposed the problem of text-
level discourse parsing into two stages: intra-
sentential parsing to produce a discourse tree for
each sentence, followed by multi-sentential pars-
ing to combine the sentence-level discourse trees
and produce the text-level discourse tree. Specif-
ically, they employed two separate models for
intra- and multi-sentential parsing. Their choice
of two-stage parsing is well motivated for two rea-
sons: (1) it has been shown that sentence bound-
aries correlate very well with discourse bound-
aries, and (2) the scalability issue of their CRF-
based models can be overcome by this decompo-
sition.
Second, they jointly modeled the structure and
the relation for a given pair of discourse units.
For example, Figure 2 shows their intra-sentential
model, in which they use the bottom layer to rep-
resent discourse units; the middle layer of binary
nodes to predict the connection of adjacent dis-
course units; and the top layer of multi-class nodes
to predict the type of the relation between two
units. Their model assigns a probability to each
possible constituent, and a CKY-like parsing al-
gorithm finds the globally optimal discourse tree,
given the computed probabilities.
The strength of Joty et al.’s model is their joint
modeling of the structure and the relation, such
that information from each aspect can interact with
the other. However, their model has a major defect
in its inefficiency, or even infeasibility, for appli-
cation in practice. The inefficiency lies in both
their DCRF-based joint model, on which infer-
ence is usually slow, and their CKY-like parsing
algorithm, whose issue is more prominent. Due to
the O(n3) time complexity, where n is the number
</bodyText>
<page confidence="0.997242">
512
</page>
<figureCaption confidence="0.9889765">
Figure 2: Joty et al. (2013)’s intra-sentential Con-
dition Random Fields.
</figureCaption>
<bodyText confidence="0.99485">
of input discourse units, for large documents, the
parsing simply takes too long1.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="method">
3 Overall work flow
</sectionHeader>
<bodyText confidence="0.999343884615385">
Figure 3 demonstrates the overall work flow of our
discourse parser. The general idea is that, similar
to Joty et al. (2013), we perform a sentence-level
parsing for each sentence first, followed by a text-
level parsing to generate a full discourse tree for
the whole document. However, in addition to effi-
ciency (to be shown in Section 6), our discourse
parser has a distinct feature, which is the post-
editing component (to be introduced in Section 5),
as outlined in dashes.
Our discourse parser works as follows. A doc-
ument D is first segmented into a list of sen-
tences. Each sentence Si, after being segmented
into EDUs (not shown in the figure), goes through
an intra-sentential bottom-up tree-building model
Mintra, to form a sentence-level discourse tree TSi,
with the EDUs as leaf nodes. After that, we ap-
ply the intra-sentential post-editing model Pintra to
modify the generated tree TSi to TSip , by considering
upper-level information.
We then combine all sentence-level discourse
tree TSip’s using our multi-sentential bottom-up
tree-building model Mmulti to generate the text-
level discourse tree TD. Similar to sentence-level
parsing, we also post-edit TD using Pmulti to pro-
duce the final discourse tree TD p .
</bodyText>
<footnote confidence="0.9911346">
1The largest document in the RST-DT contains over 180
sentences, i.e., n &gt; 180 for their multi-sentential CKY pars-
ing. Intuitively, suppose the average time to compute the
probability of each constituent is 0.01 second, then in total,
the CKY-like parsing takes over 16 hours. It is possible to op-
timize Joty et al.’s CKY-like parsing by replacing their CRF-
based computation for upper-level constituents with some lo-
cal computation based on the probabilities of lower-level con-
stituents. However, such optimization is beyond the scope of
this paper.
</footnote>
<sectionHeader confidence="0.698294" genericHeader="method">
4 Bottom-up tree-building
</sectionHeader>
<bodyText confidence="0.9998075">
For both intra- and multi-sentential parsing, our
bottom-up tree-building process adopts a similar
greedy pipeline framework like the HILDA dis-
course parser (discussed in Section 2.1), to guar-
antee efficiency for large documents. In partic-
ular, starting from the constituents on the bot-
tom level (EDUs for intra-sentential parsing and
sentence-level discourse trees for multi-sentential
parsing), at each step of the tree-building, we
greedily merge a pair of adjacent discourse con-
stituents such that the merged constituent has the
highest probability as predicted by our structure
model. The relation model is then applied to as-
sign the relation to the new constituent.
</bodyText>
<subsectionHeader confidence="0.995761">
4.1 Linear-chain CRFs as Local models
</subsectionHeader>
<bodyText confidence="0.999986333333333">
Now we describe the local models we use to make
decisions for a given pair of adjacent discourse
constituents in the bottom-up tree-building. There
are two dimensions for our local models: (1) scope
of the model: intra- or multi-sentential, and (2)
purpose of the model: for determining structures
</bodyText>
<equation confidence="0.8668266">
or relations. So we have four local models, Mstruct
intra ,
Mrel
intra, Mstruct
multi ,
</equation>
<bodyText confidence="0.9997865">
While our bottom-up tree-building shares the
greedy framework with HILDA, unlike HILDA,
our local models are implemented using CRFs.
In this way, we are able to take into account the
sequential information from contextual discourse
constituents, which cannot be naturally repre-
sented in HILDA with SVMs as local classifiers.
Therefore, our model incorporates the strengths
of both HILDA and Joty et al.’s model, i.e., the
efficiency of a greedy parsing algorithm, and the
ability to incorporate sequential information with
CRFs.
As shown by Feng and Hirst (2012), for a pair
of discourse constituents of interest, the sequential
information from contextual constituents is cru-
cial for determining structures. Therefore, it is
well motivated to use Conditional Random Fields
(CRFs) (Lafferty et al., 2001), which is a discrimi-
native probabilistic graphical model, to make pre-
dictions for a sequence of constituents surround-
ing the pair of interest.
In this sense, our local models appear similar
to Joty et al.’s non-greedy parsing models. How-
ever, the major distinction between our models
and theirs is that we do not jointly model the struc-
ture and the relation; rather, we use two linear-
</bodyText>
<figure confidence="0.99438612">
Relation
sequence
Structure
sequence
Unit sequence
at level i
U1 U2
R2
S2
U3
R3
S3
Uj
Rj
Sj
Ut-1
Rt-1
St-1
and Mrel
multi.
513
D
S1
...
...
Sn
Si
Mintra
Mintra
Mintra
...
...
TSn TpSn
TSi
TS,
...
Pintra
Pintra
Pintra
...
...
T p
Si
TP
S1
Mmulti
TD
TP
D
Pmulti
</figure>
<figureCaption confidence="0.871038666666667">
Figure 3: The work flow of our proposed discourse parser. In the figure, Mintra and Mmulti stand for the
intra- and multi-sentential bottom-up tree-building models, and Pintra and Pmulti stand for the intra- and
multi-sentential post-editing models.
</figureCaption>
<bodyText confidence="0.99997872">
chain CRFs to model the structure and the relation
separately. Although joint modeling has shown to
be effective in various NLP and computer vision
applications (Sutton et al., 2007; Yang et al., 2009;
Wojek and Schiele, 2008), our choice of using two
separate models is for the following reasons:
First, it is not entirely appropriate to model the
structure and the relation at the same time. For
example, with respect to Figure 2, it is unclear
how the relation node Rj is represented for a train-
ing instance whose structure node Sj = 0, i.e., the
units Uj−1 and Uj are disjoint. Assume a special
relation NO-REL is assigned for Rj. Then, in the
tree-building process, we will have to deal with the
situations where the joint model yields conflicting
predictions: it is possible that the model predicts
Sj = 1 and Rj = NO-REL, or vice versa, and we
will have to decide which node to trust (and thus
in some sense, the structure and the relation is no
longer jointly modeled).
Secondly, as a joint model, it is mandatory to
use a dynamic CRF, for which exact inference is
usually intractable or slow. In contrast, for linear-
chain CRFs, efficient algorithms and implementa-
tions for exact inference exist.
</bodyText>
<subsectionHeader confidence="0.957786">
4.2 Structure models
4.2.1 Intra-sentential structure model
</subsectionHeader>
<bodyText confidence="0.9790465">
Figure 4a shows our intra-sentential structure
model Mstruct
intra in the form of a linear-chain CRF.
Similar to Joty et al.’s intra-sentential model, the
first layer of the chain is composed of discourse
constituents Uj’s, and the second layer is com-
posed of binary nodes Sj’s to indicate the proba-
bility of merging adjacent discourse constituents.
</bodyText>
<figure confidence="0.78604">
(b) Multi-sentential structure model Mstruct
multi . C1, C2, and C3
denote the three chains for predicting Uj and Uj+1.
</figure>
<figureCaption confidence="0.999623">
Figure 4: Local structure models.
</figureCaption>
<bodyText confidence="0.999713166666667">
At each step in the bottom-up tree-building pro-
cess, we generate a single sequence E, consisting
of U1,U2,...,Uj,...,Ut, which are all the current
discourse constituents in the sentence that need
to be processed. For instance, initially, we have
the sequence E1 = {e1,e2,...,em}, which are the
EDUs of the sentence; after merging e1 and e2 on
the second level, we have E2 = {e1:2,e3,...,em};
after merging e4 and e5 on the third level, we have
E3 = {e1:2,e3,e4:5,...,em}, and so on.
Because the structure model is the first com-
ponent in our pipeline of local models, its accu-
racy is crucial. Therefore, to improve its accuracy,
we enforce additional commonsense constraints in
its Viterbi decoding. In particular, we disallow 1-
1 transitions between adjacent labels (a discourse
unit can be merged with at most one adjacent unit),
and we disallow all-zero sequences (at least one
</bodyText>
<figure confidence="0.995915411764706">
Structure
sequence
All units in
sentence
at level i
(a) Intra-sentential structure model Mstruct
intra .
U1
U2
S2
U3
S3
Uj
Sj
Ut
St
Structure
sequence
Adjacent
units at
level i
Uj-3
Uj-2 Uj-1
Sj-1
C1
Sj-1
C2
Uj
Sj
C3
Uj+1
Sj+1
Uj+2
Sj+2
</figure>
<page confidence="0.989414">
514
</page>
<bodyText confidence="0.999618941176471">
pair must be merged).
Since the computation of Ei does not depend
on a particular pair of constituents, we can use the
same sequence Ei to compute structural probabili-
ties for all adjacent constituents. In contrast, Joty
et al.’s computation of intra-sentential sequences
depends on the particular pair of constituents: the
sequence is composed of the pair in question, with
other EDUs in the sentence, even if those EDUs
have already been merged. Thus, different CRF
chains have to be formed for different pairs of con-
stituents. In addition to efficiency, our use of a
single CRF chain for all constituents can better
capture the sequential dependencies among con-
text, by taking into account the information from
partially built discourse constituents, rather than
bottom-level EDUs only.
</bodyText>
<subsectionHeader confidence="0.554762">
4.2.2 Multi-sentential structure model
</subsectionHeader>
<bodyText confidence="0.999946466666667">
For multi-sentential parsing, where the smallest
discourse units are single sentences, as argued by
Joty et al. (2013), it is not feasible to use a long
chain to represent all constituents, due to the fact
that it takes O(TM2) time to perform the forward-
backward exact inference on a chain with T units
and an output vocabulary size of M, thus the over-
all complexity for all possible sequences in their
model is O(M2n3)2.
Instead, we choose to take a sliding-window
approach to form CRF chains for a particular pair
of constituents, as shown in Figure 4b. For exam-
ple, suppose we wish to compute the structural
probability for the pair Uj−1 and Uj, we form three
chains, each of which contains two contextual
</bodyText>
<equation confidence="0.8463215">
constituents: C1 = {Uj−3,Uj−2,Uj−1,Uj},
C2 = {Uj−2,Uj−1,Uj,Uj+1}, and C3 =
</equation>
<bodyText confidence="0.850381727272727">
{Uj−1,Uj,Uj+1,Uj+2}. We then find the chain
Ct,1 &lt; t &lt; 3, with the highest joint probability
over the entire sequence, and assign its marginal
probability P(Stj = 1) to P(Sj = 1).
Similar to Mstruct
intra , for Mstruct
multi , we also include
additional constraints in the Viterbi decoding, by
disallowing transitions between two ones, and dis-
allowing the sequence to be all zeros if it contains
all the remaining constituents in the document.
</bodyText>
<subsectionHeader confidence="0.97229">
4.3 Relation models
</subsectionHeader>
<subsubsectionHeader confidence="0.490873">
4.3.1 Intra-sentential relation model
</subsubsectionHeader>
<bodyText confidence="0.8693485">
The intra-sentential relation model Mrel
intra, shown
in Figure 5a, works in a similar way to Mstruct
intra , as
</bodyText>
<footnote confidence="0.807642333333333">
2The time complexity will be reduced to O(M2n2), if we
use the same chain for all constituents as in our Mstruct
intra .
</footnote>
<bodyText confidence="0.9995099">
described in Section 4.2.1. The linear-chain CRF
contains a first layer of all discourse constituents
Uj’s in the sentence on level i, and a second layer
of relation nodes Rj’s to represent the relation be-
tween a pair of discourse constituents.
However, unlike the structure model, adjacent
relation nodes do not share discourse constituents
on the first layer. Rather, each relation node Rj
attempts to model the relation of one single con-
stituent Uj, by taking Uj’s left and right subtrees
Uj,L and Uj,R as its first-layer nodes; if Uj is a sin-
gle EDU, then the first-layer node of Rj is simply
Uj, and Rj is a special relation symbol LEAF3.
Since we know, a priori, that the constituents in the
chains are either leaf nodes or the ones that have
been merged by our structure model, we never
need to worry about the NO-REL issue as out-
lined in Section 4.1.
In the bottom-up tree-building process, after
merging a pair of adjacent constituents using
</bodyText>
<subsubsectionHeader confidence="0.741458">
Mstruct
</subsubsectionHeader>
<bodyText confidence="0.999903357142857">
intra into a new constituent, say Uj, we form a
chain consisting of all current constituents in the
sentence to decide the relation label for Uj, i.e.,
the Rj node in the chain. In fact, by perform-
ing inference on this chain, we produce predic-
tions not only for Rj, but also for all other R nodes
in the chain, which correspond to all other con-
stituents in the sentence. Since those non-leaf con-
stituents are already labeled in previous steps in
the tree-building, we can now re-assign their rela-
tions if the model predicts differently in this step.
Therefore, this re-labeling procedure can compen-
sate for the loss of accuracy caused by our greedy
bottom-up strategy to some extent.
</bodyText>
<subsectionHeader confidence="0.622831">
4.3.2 Multi-sentential relation model
</subsectionHeader>
<bodyText confidence="0.872031133333333">
Figure 5b shows our multi-sentential relation
model. Like Mrel
intra, the first layer consists of adja-
cent discourse units, and the relation nodes on the
second layer model the relation of each constituent
separately.
Similar to Mstruct
multi introduced in Section 4.2.2,
Mrel
multi also takes a sliding-window approach to
predict labels for constituents in a local context.
For a constituent Uj to be predicted, we form three
chains, and use the chain with the highest joint
probability to assign or re-assign relations to con-
stituents in that chain.
</bodyText>
<footnote confidence="0.999634333333333">
3These leaf constituents are represented using a special
feature vector is leaf = True; thus the CRF never labels
them with relations other than LEAF.
</footnote>
<page confidence="0.993752">
515
</page>
<figure confidence="0.999595666666667">
(a) Intra-sentential relation model Mrel
intra.
(b) Multi-sentential relation model Mrel
</figure>
<figureCaption confidence="0.671552666666667">
multi. C1, C2, and C3
denote the three sliding windows for predicting Uj,L and Uj,R.
Figure 5: Local relation models.
</figureCaption>
<sectionHeader confidence="0.916499" genericHeader="method">
5 Post-editing
</sectionHeader>
<bodyText confidence="0.999922911764706">
After an intra- or multi-sentential discourse tree
is fully built, we perform a post-editing to con-
sider possible modifications to the current tree, by
considering useful information from the discourse
constituents on upper levels, which is unavailable
in the bottom-up tree-building process.
The motivation for post-editing is that, some
particular discourse relations, such as TEXTUAL-
ORGANIZATION, tend to occur on the top levels
of the discourse tree; thus, information such as the
depth of the discourse constituent can be quite in-
dicative. However, the exact depth of a discourse
constituent is usually unknown in the bottom-up
tree-building process; therefore, it might be ben-
eficial to modify the tree by including top-down
information after the tree is fully built.
The process of post-editing is shown in Algo-
rithm 1. For each input discourse tree T, which
is already fully built by bottom-up tree-building
models, we do the following:
Lines 3 – 9: Identify the lowest level of T on
which the constituents can be modified according
to the post-editing structure component, Pstruct. To
do so, we maintain a list L to store the discourse
constituents that need to be examined. Initially, L
consists of all the bottom-level constituents in T.
At each step of the loop, we consider merging the
pair of adjacent units in L with the highest proba-
bility predicted by Pstruct. If the predicted pair is
not merged in the original tree T, then a possible
modification is located; otherwise, we merge the
pair, and proceed to the next iteration.
Lines 10 – 12: If modifications have been pro-
posed in the previous step, we build a new tree
</bodyText>
<figure confidence="0.467766875">
Algorithm 1 Post-editing algorithm.
Input: A fully built discourse tree T.
1: if |T |= 1 then
2: return T &gt; Do nothing if it is a single
EDU.
3: L ← [U1,U2,...,Ut] &gt; The bottom-level
constituents in T.
4: while |L |&gt; 2 do
5: i ← PREDICTMERGING(L,Pstruct)
6: p ← PARENT(L[i],L[i + 1],T)
7: if p = NULL then
8: break
9: Replace L[i] and L[i + 1] with p
10: if |L |= 2 then 11: L ← [U1,U2,...,Ut]
12: Tp ← BUILDTREE(L,Pstruct,Prel,T)
Output: Tp
</figure>
<bodyText confidence="0.979429285714286">
T p using Pstruct as the structure model, and Prel
as the relation model, from the constituents on
which modifications are proposed. Otherwise, Tp
is built from the bottom-level constituents of T.
The upper-level information, such as the depth of
a discourse constituent, is derived from the initial
tree T.
</bodyText>
<subsectionHeader confidence="0.99438">
5.1 Local models
</subsectionHeader>
<bodyText confidence="0.991498857142857">
The local models, P{struct|rel}
{intra|multi}, for post-editing
is almost identical to their counterparts of the
bottom-up tree-building, except that the linear-
chain CRFs in post-editing includes additional
features to represent information from constituents
on higher levels (to be introduced in Section 7).
</bodyText>
<sectionHeader confidence="0.943196" genericHeader="method">
6 Linear time complexity
</sectionHeader>
<bodyText confidence="0.9999956">
Here we analyze the time complexity of each com-
ponent in our discourse parser, to quantitatively
demonstrate the time efficiency of our model. The
following analysis is focused on the bottom-up
tree-building process, but a similar analysis can be
carried out for the post-editing process. Since the
number of operations in the post-editing process is
roughly the same (1.5 times in the worst case) as
in the bottom-up tree-building, post-editing shares
the same complexity as the tree-building.
</bodyText>
<subsectionHeader confidence="0.963565">
6.1 Intra-sentential parsing
</subsectionHeader>
<bodyText confidence="0.999706666666667">
Suppose the input document is segmented into
n sentences, and each sentence Sk contains mk
EDUs. For each sentence Sk with mk EDUs, the
</bodyText>
<figure confidence="0.999431756756757">
Relation
sequence
All units in
sentence
at level i
U1,L
R1
U1,R
U2
R2
Uj,L
Rj
Uj,R
Ut,L
Rt
Ut,R
Uj_2,L
R1
Uj_2,R
C1
Uj_1
Rj_1
Uj,L
Rj
Uj,R
C2
Uj.1,L
C3
Rj.1
Uj.1,R
Uj.2
Rj.2
Relation
sequence
Adjacent
units
at level i
</figure>
<page confidence="0.996368">
516
</page>
<bodyText confidence="0.999654615384615">
overall time complexity to perform intra-sentential
parsing is O(m2k). The reason is the following. On
level i of the bottom-up tree-building, we generate
a single chain to represent the structure or relation
for all the mk − i constituents that are currently in
the sentence. The time complexity for performing
forward-backward inference on the single chain is
O((mk −i)xM2) = O(mk −i), where the constant
M is the size of the output vocabulary. Starting
from the EDUs on the bottom level, we need to
perform inference for one chain on each level dur-
ing the bottom-up tree-building, and thus the total
time complexity is Σmk
</bodyText>
<equation confidence="0.969975">
i=1O(mk − i) = O(m2k).
</equation>
<bodyText confidence="0.98344575">
The total time to generate sentence-level dis-
course trees for n sentences is Σnk=1O(m2k). It is
fairly safe to assume that each mk is a constant,
in the sense that mk is independent of the total
number of sentences in the document. There-
fore, the total time complexity Σnk=1O(m2k) :5; n x
O(max1:5;j:5;n(m2j)) = n x O(1) = O(n), i.e., linear
in the total number of sentences.
</bodyText>
<subsectionHeader confidence="0.999367">
6.2 Multi-sentential parsing
</subsectionHeader>
<bodyText confidence="0.874757">
For multi-sentential models, Mstruct
</bodyText>
<subsubsectionHeader confidence="0.58851">
multi and Mrel
</subsubsectionHeader>
<bodyText confidence="0.9999515">
multi, as
shown in Figures 4b and 5b, for a pair of con-
stituents of interest, we generate multiple chains
to predict the structure or the relation.
By including a constant number k of discourse
units in each chain, and considering a constant
number l of such chains for computing each ad-
jacent pair of discourse constituents (k = 4 for
</bodyText>
<subsubsectionHeader confidence="0.4167755">
Mstruct
multi
</subsubsectionHeader>
<bodyText confidence="0.999791333333333">
overall time complexity of O(n). The reason is
that it takes l xO(kM2) = O(1) time, where l,k,M
are all constants, to perform exact inference for a
given pair of adjacent constituents, and we need
to perform such computation for all n −1 pairs of
adjacent sentences on the first level of the tree-
building. Adopting a greedy approach, on an ar-
bitrary level during the tree-building, once we de-
cide to merge a certain pair of constituents, say
Uj and Uj+1, we only need to recompute a small
number of chains, i.e., the chains which originally
include Uj or Uj+1, and inference on each chain
takes O(1). Therefore, the total time complexity
is (n−1)xO(1)+(n−1)xO(1) = O(n), where
the first term in the summation is the complexity
of computing all chains on the bottom level, and
the second term is the complexity of computing
the constant number of chains on higher levels.
We have thus showed that the time complexity
is linear in n, which is the number of sentences in
the document. In fact, under the assumption that
the number of EDUs in each sentence is indepen-
dent of n, it can be shown that the time complexity
is also linear in the total number of EDUs4.
</bodyText>
<sectionHeader confidence="0.995095" genericHeader="method">
7 Features
</sectionHeader>
<bodyText confidence="0.996500022727273">
In our local models, to encode two adjacent units,
Uj and Uj+1, within a CRF chain, we use the fol-
lowing 10 sets of features, some of which are mod-
ified from Joty et al.’s model.
Organization features: Whether Uj (or Uj+1) is
the first (or last) constituent in the sentence (for
intra-sentential models) or in the document (for
multi-sentential models); whether Uj (or Uj+1) is
a bottom-level constituent.
Textual structure features: Whether Uj con-
tains more sentences (or paragraphs) than Uj+1.
N-gram features: The beginning (or end) lexi-
cal n-grams in each unit; the beginning (or end)
POS n-grams in each unit, where n E 11,2,3}.
Dominance features: The PoS tags of the head
node and the attachment node; the lexical heads of
the head node and the attachment node; the domi-
nance relationship between the two units.
Contextual features: The feature vector of the
previous and the next constituent in the chain.
Substructure features: The root node of the left
and right discourse subtrees of each unit.
Syntactic features: whether each unit corre-
sponds to a single syntactic subtree, and if so, the
top PoS tag of the subtree; the distance of each
unit to their lowest common ancestor in the syntax
tree (intra-sentential only).
Entity transition features: The type and the
number of entity transitions across the two units.
We adopt Barzilay and Lapata (2008)’s entity-
based local coherence model to represent a doc-
ument by an entity grid, and extract local transi-
tions among entities in continuous discourse con-
stituents. We use bigram and trigram transitions
with syntactic roles attached to each entity.
4We implicitly made an assumption that the parsing time
is dominated by the time to perform inference on CRF chains.
However, for complex features, the time required for fea-
ture computation might be dominant. Nevertheless, a care-
ful caching strategy can accelerate feature computation, since
a large number of multi-sentential chains overlap with each
other.
and k = 3 for Mrel
multi; l = 3), we have an
</bodyText>
<page confidence="0.989971">
517
</page>
<bodyText confidence="0.9915125">
Cue phrase features: Whether a cue phrase oc-
curs in the first or last EDU of each unit. The cue
phrase list is based on the connectives collected by
Knott and Dale (1994)
Post-editing features: The depth of each unit in
the initial tree.
</bodyText>
<sectionHeader confidence="0.995881" genericHeader="method">
8 Experiments
</sectionHeader>
<bodyText confidence="0.99989">
For pre-processing, we use the Stanford CoreNLP
(Klein and Manning, 2003; de Marneffe et al.,
2006; Recasens et al., 2013) to syntactically parse
the texts and extract coreference relations, and we
use Penn2Malt5 to lexicalize syntactic trees to ex-
tract dominance features.
For local models, our structure models are
trained using MALLET (McCallum, 2002) to in-
clude constraints over transitions between adja-
cent labels, and our relation models are trained
using CRFSuite (Okazaki, 2007), which is a fast
implementation of linear-chain CRFs.
The data that we use to develop and evaluate
our discourse parser is the RST Discourse Tree-
bank (RST-DT) (Carlson et al., 2001), which is a
large corpus annotated in the framework of RST.
The RST-DT consists of 385 documents (347 for
training and 38 for testing) from the Wall Street
Journal. Following previous work on the RST-DT
(Hernault et al., 2010; Feng and Hirst, 2012; Joty
et al., 2012; Joty et al., 2013), we use 18 coarse-
grained relation classes, and with nuclearity at-
tached, we have a total set of 41 distinct relations.
Non-binary relations are converted into a cascade
of right-branching binary relations.
</bodyText>
<sectionHeader confidence="0.990854" genericHeader="evaluation">
9 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.999882">
9.1 Parsing accuracy
</subsectionHeader>
<bodyText confidence="0.999901166666667">
We compare four different models using manual
EDU segmentation. In Table 1, the jCRF model
in the first row is the optimal CRF model proposed
by Joty et al. (2013). gSVMFH in the second row
is our implementation of HILDA’s greedy parsing
algorithm using Feng and Hirst (2012)’s enhanced
feature set. The third model, gCRF, represents our
greedy CRF-based discourse parser, and the last
row, gCRFPE, represents our parser with the post-
editing component included.
In order to conduct a direct comparison with
Joty et al.’s model, we use the same set of eval-
</bodyText>
<footnote confidence="0.995762">
5http://stp.lingfil.uu.se/˜nivre/
research/Penn2Malt.html.
</footnote>
<table confidence="0.916543222222222">
Model Span Nuc Relation
Acc MAFS
jCRF 82.5 68.4 55.7 N/A
gSVMFH 82.8 67.1 52.0 27.4/23.3
gCRF 84.9* 69.9* 57.2* 35.3/31.3
gCRFPE 85.7*† 71.0*† 58.2*† 36.2/32.3
Human 88.7 77.7 65.8 N/A
*: significantly better than gSVMFH (p &lt; .01)
†: significantly better than gCRF (p &lt; .01)
</table>
<tableCaption confidence="0.957962">
Table 1: Performance of different models using
</tableCaption>
<bodyText confidence="0.981752575757576">
gold-standard EDU segmentation, evaluated us-
ing the constituent accuracy (%) for span, nucle-
arity, and relation. For relation, we also report the
macro-averaged F1-score (MAFS) for correctly
retrieved constituents (before the slash) and for
all constituents (after the slash). Statistical sig-
nificance is verified using Wilcoxon’s signed-rank
test.
uation metrics, i.e., the unlabeled and labeled pre-
cision, recall, and F-score6 as defined by Marcu
(2000). For evaluating relations, since there is a
skewed distribution of different relation types in
the corpus, we also include the macro-averaged
F1-score (MAFS)7 as another metric, to empha-
size the performance of infrequent relation types.
We report the MAFS separately for the correctly
retrieved constituents (i.e., the span boundary is
correct) and all constituents in the reference tree.
As demonstrated by Table 1, our greedy CRF
models perform significantly better than the other
two models. Since we do not have the actual out-
put of Joty et al.’s model, we are unable to con-
duct significance testing between our models and
theirs. But in terms of overall accuracy, our gCRF
model outperforms their model by 1.5%. More-
over, with post-editing enabled, gCRFPE signif-
icantly (p &lt; .01) outperforms our initial model
gCRF by another 1% in relation assignment, and
this overall accuracy of 58.2% is close to 90% of
human performance. With respect to the macro-
averaged F1-scores, adding the post-editing com-
ponent also obtains about 1% improvement.
However, the overall MAFS is still at the lower
</bodyText>
<footnote confidence="0.9994392">
6For manual segmentation, precision, recall, and F-score
are the same.
7MAFS is the F1-score averaged among all relation
classes by equally weighting each class. Therefore, we can-
not conduct significance test between different MAFS.
</footnote>
<page confidence="0.97152">
518
</page>
<table confidence="0.993022">
Avg Min Max
# of EDUs 61.74 4 304
# of Sentences 26.11 2 187
# of EDUs per sentence 2.36 1 10
</table>
<tableCaption confidence="0.9271395">
Table 2: Characteristics of the 38 documents in the
test data.
</tableCaption>
<bodyText confidence="0.998936666666667">
end of 30% for all constituents. Our error anal-
ysis shows that, for two relation classes, TOPIC-
CHANGE and TEXTUAL-ORGANIZATION, our
model fails to retrieve any instance, and for
TOPIC-COMMENT and EVALUATION, our model
scores a class-wise F1 score lower than 5%. These
four relation classes, apart from their infrequency
in the corpus, are more abstractly defined, and thus
are particularly challenging.
</bodyText>
<subsectionHeader confidence="0.998725">
9.2 Parsing efficiency
</subsectionHeader>
<bodyText confidence="0.999992448275862">
We further illustrate the efficiency of our parser by
demonstrating the time consumption of different
models.
First, as shown in Table 2, the average number
of sentences in a document is 26.11, which is al-
ready too large for optimal parsing models, e.g.,
the CKY-like parsing algorithm in jCRF, let alone
the fact that the largest document contains sev-
eral hundred of EDUs and sentences. Therefore,
it should be seen that non-optimal models are re-
quired in most cases.
In Table 3, we report the parsing time8 for the
last three models, since we do not know the time of
jCRF. Note that the parsing time excludes the time
cost for any necessary pre-processing. As can be
seen, our gCRF model is considerably faster than
gSVMFH, because, on one hand, feature compu-
tation is expensive in gSVMFH, since gSVMFH
utilizes a rich set of features; on the other hand,
in gCRF, we are able to accelerate decoding by
multi-threading MALLET (we use four threads).
Even for the largest document with 187 sentences,
gCRF is able to produce the final tree after about
40 seconds, while jCRF would take over 16 hours
assuming each DCRF decoding takes only 0.01
second. Although enabling post-editing doubles
the time consumption, the overall time is still ac-
ceptable in practice, and the loss of efficiency can
be compensated by the improvement in accuracy.
</bodyText>
<footnote confidence="0.8054025">
8Tested on a Linux system with four duo-core 3.0GHz
processors and 16G memory.
</footnote>
<table confidence="0.9994798">
Model Parsing Time (seconds)
Avg Min Max
gSVMFH 11.19 0.42 124.86
gCRF 5.52 0.05 40.57
gCRFPE 10.71 0.12 84.72
</table>
<tableCaption confidence="0.875451666666667">
Table 3: The parsing time (in seconds) for the 38
documents in the test set of RST-DT. Time cost of
any pre-processing is excluded from the analysis.
</tableCaption>
<sectionHeader confidence="0.989769" genericHeader="conclusions">
10 Conclusions
</sectionHeader>
<bodyText confidence="0.999986133333333">
In this paper, we presented an efficient text-level
discourse parser with time complexity linear in
the total number of sentences in the document.
Our approach was to adopt a greedy bottom-
up tree-building, with two linear-chain CRFs as
local probabilistic models, and enforce reason-
able constraints in the first CRF’s Viterbi decod-
ing. While significantly outperforming the state-
of-the-art model by Joty et al. (2013), our parser
is much faster in practice. In addition, we pro-
pose a novel idea of post-editing, which modifies a
fully-built discourse tree by considering informa-
tion from upper-level constituents. We show that,
although doubling the time consumption, post-
editing can further boost the parsing performance
to close to 90% of human performance.
In future work, we wish to further explore the
idea of post-editing, since currently we use only
the depth of the subtrees as upper-level informa-
tion. Moreover, we wish to study whether we can
incorporate constraints into the relation models, as
we do to the structure models. For example, it
might be helpful to train the relation models us-
ing additional criteria, such as Generalized Ex-
pectation (Mann and McCallum, 2008), to better
take into account some prior knowledge about the
relations. Last but not least, as reflected by the
low MAFS in our experiments, some particularly
difficult relation types might need specifically de-
signed features for better recognition.
</bodyText>
<sectionHeader confidence="0.998822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999388">
We thank Professor Gerald Penn and the review-
ers for their valuable advice and comments. This
work was financially supported by the Natural
Sciences and Engineering Research Council of
Canada and by the University of Toronto.
</bodyText>
<page confidence="0.997449">
519
</page>
<sectionHeader confidence="0.993877" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902541284403">
Jason Baldridge and Alex Lascarides. 2005. Proba-
bilistic head-driven parsing for discourse structure.
In Proceedings of the Ninth Conference on Compu-
tational Natural Language Learning (CoNLL-2005),
pages 96–103, Ann Arbor, Michigan, June. Associ-
ation for Computational Linguistics.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: an entity-based approach. Compu-
tational Linguistics, 34(1):1–34.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2001. Building a discourse-tagged cor-
pus in the framework of Rhetorical Structure The-
ory. In Proceedings of Second SIGDial Workshop
on Discourse and Dialogue (SIGDial 2001), pages
1–10.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
Proceedings of the 5th International Conference on
Language Resources and Evaluation (LREC 2006).
Vanessa Wei Feng and Graeme Hirst. 2012. Text-level
discourse parsing with rich linguistic features. In
Proceedings of the 50th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies (ACL 2012), pages 60–68, Jeju,
Korea.
Hugo Hernault, Helmut Prendinger, David A. duVerle,
and Mitsuru Ishizuka. 2010. HILDA: A discourse
parser using support vector machine classification.
Dialogue and Discourse, 1(3):1–33.
Shafiq Joty, Giuseppe Carenini, and Raymond T.
Ng. 2012. A novel discriminative framework
for sentence-level discourse analysis. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL 2012, pages 904–915.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining intra- and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2013), pages 486–496, Sofia, Bul-
garia, August.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics (ACL 2003), ACL 2003, pages
423–430, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Alistair Knott and Robert Dale. 1994. Using linguistic
phenomena to motivate a set of coherence relations.
Discourse Processes, 18(1):35–64.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In Proceedings of the Eighteenth Inter-
national Conference on Machine Learning, ICML
2001, pages 282–289, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Gideon S. Mann and Andrew McCallum. 2008. Gen-
eralized Expectation Criteria for semi-supervised
learning of Conditional Random Fields. In Proceed-
ings of the 46th Annual Meeting of the Association
for Computational Linguistics: Human Language
Technologies (ACL 2008), pages 870–878, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
William Mann and Sandra Thompson. 1988. Rhetor-
ical structure theory: Toward a functional theory of
text organization. Text, 8(3):243–281.
Daniel Marcu. 2000. The Theory and Practice of Dis-
course Parsing and Summarization. The MIT Press.
Andrew Kachites McCallum. 2002. MAL-
LET: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Philippe Muller, Stergos Afantenos, Pascal Denis, and
Nicholas Asher. 2012. Constrained decoding for
text-level discourse parsing. In Proceedings of
COLING 2012, pages 1883–1900, Mumbai, India,
December. The COLING 2012 Organizing Commit-
tee.
Naoaki Okazaki. 2007. CRFsuite: a fast im-
plementation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The life and death of dis-
course entities: Identifying singleton mentions. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 627–633, Atlanta, Georgia, June. Association
for Computational Linguistics.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic infor-
mation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, pages 566–574, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic conditional random
fields: Factorized probabilistic models for labeling
and segmenting sequence data. The Journal of Ma-
chine Learning Research, 8:693–723, May.
Christian Wojek and Bernt Schiele. 2008. A dynamic
conditional random field model for joint labeling of
object and scene classes. In European Conference
</reference>
<page confidence="0.962027">
520
</page>
<reference confidence="0.998998090909091">
on Computer Vision (ECCV 2008), pages 733–747,
Marseille, France.
Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oon-
ishi, Masanobu Nakamura, and Sadaoki Furui.
2009. Combining a two-step conditional random
field model and a joint source channel model for
machine transliteration. In Proceedings of the 2009
Named Entities Workshop: Shared Task on Translit-
eration (NEWS 2009), pages 72–75, Suntec, Singa-
pore, August. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.997864">
521
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.984421">
<title confidence="0.999622">A Linear-Time Bottom-Up Discourse with Constraints and Post-Editing</title>
<author confidence="0.999865">Vanessa Wei Feng</author>
<affiliation confidence="0.999974">Department of Computer Science University of Toronto</affiliation>
<address confidence="0.998539">Toronto, ON, Canada</address>
<email confidence="0.999928">weifeng@cs.toronto.edu</email>
<abstract confidence="0.999393227272727">Text-level discourse parsing remains a challenge. The current state-of-the-art overall accuracy in relation assignment is 55.73%, achieved by Joty et al. (2013). However, their model has a high order of time complexity, and thus cannot be applied in practice. In this work, we develop a much faster model whose time complexity is linear in the number of sentences. Our model adopts a greedy bottom-up approach, with two linear-chain CRFs applied in cascade as local classifiers. To enhance the accuracy of the pipeline, we add additional constraints in the Viterbi decoding of the first CRF. In addition to efficiency, our parser also significantly outperforms the state of the art. Moreover, our novel approach of post-editing, which modifies a fully-built tree by considering information from constituents on upper levels, can further improve the accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Alex Lascarides</author>
</authors>
<title>Probabilistic head-driven parsing for discourse structure.</title>
<date>2005</date>
<booktitle>In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005),</booktitle>
<pages>96--103</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<marker>Baldridge, Lascarides, 2005</marker>
<rawString>Jason Baldridge and Alex Lascarides. 2005. Probabilistic head-driven parsing for discourse structure. In Proceedings of the Ninth Conference on Computational Natural Language Learning (CoNLL-2005), pages 96–103, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Mirella Lapata</author>
</authors>
<title>Modeling local coherence: an entity-based approach.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="28535" citStr="Barzilay and Lapata (2008)" startWordPosition="4691" endWordPosition="4694">d node and the attachment node; the dominance relationship between the two units. Contextual features: The feature vector of the previous and the next constituent in the chain. Substructure features: The root node of the left and right discourse subtrees of each unit. Syntactic features: whether each unit corresponds to a single syntactic subtree, and if so, the top PoS tag of the subtree; the distance of each unit to their lowest common ancestor in the syntax tree (intra-sentential only). Entity transition features: The type and the number of entity transitions across the two units. We adopt Barzilay and Lapata (2008)’s entitybased local coherence model to represent a document by an entity grid, and extract local transitions among entities in continuous discourse constituents. We use bigram and trigram transitions with syntactic roles attached to each entity. 4We implicitly made an assumption that the parsing time is dominated by the time to perform inference on CRF chains. However, for complex features, the time required for feature computation might be dominant. Nevertheless, a careful caching strategy can accelerate feature computation, since a large number of multi-sentential chains overlap with each o</context>
</contexts>
<marker>Barzilay, Lapata, 2008</marker>
<rawString>Regina Barzilay and Mirella Lapata. 2008. Modeling local coherence: an entity-based approach. Computational Linguistics, 34(1):1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
<author>Mary Ellen Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory.</title>
<date>2001</date>
<booktitle>In Proceedings of Second SIGDial Workshop on Discourse and Dialogue</booktitle>
<pages>1--10</pages>
<contexts>
<context position="30110" citStr="Carlson et al., 2001" startWordPosition="4950" endWordPosition="4953"> (Klein and Manning, 2003; de Marneffe et al., 2006; Recasens et al., 2013) to syntactically parse the texts and extract coreference relations, and we use Penn2Malt5 to lexicalize syntactic trees to extract dominance features. For local models, our structure models are trained using MALLET (McCallum, 2002) to include constraints over transitions between adjacent labels, and our relation models are trained using CRFSuite (Okazaki, 2007), which is a fast implementation of linear-chain CRFs. The data that we use to develop and evaluate our discourse parser is the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), which is a large corpus annotated in the framework of RST. The RST-DT consists of 385 documents (347 for training and 38 for testing) from the Wall Street Journal. Following previous work on the RST-DT (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013), we use 18 coarsegrained relation classes, and with nuclearity attached, we have a total set of 41 distinct relations. Non-binary relations are converted into a cascade of right-branching binary relations. 9 Results and Discussion 9.1 Parsing accuracy We compare four different models using manual EDU segmentati</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2001</marker>
<rawString>Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory. In Proceedings of Second SIGDial Workshop on Discourse and Dialogue (SIGDial 2001), pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Text-level discourse parsing with rich linguistic features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL 2012),</booktitle>
<pages>60--68</pages>
<location>Jeju,</location>
<contexts>
<context position="4416" citStr="Feng and Hirst, 2012" startWordPosition="683" endWordPosition="686"> to produce parsing results in real time. Our contribution is three-fold. First, with a 511 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 511–521, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics [On Aug. 1, the state tore up its controls,]e1 [and food prices leaped]e2 [Without buffer stocks,]e3 [inflation exploded.]e4 wsj 1146 e1:4 sequence e1:2 e3:4 strategy is its efficiency in practice. Also, the employment of SVM classifiers allows the incorporation of rich features for better data representation (Feng and Hirst, 2012). However, HILDA’s approach also has obvious weakness: the greedy algorithm may lead to poor performance due to local optima, and more importantly, the SVM classifiers are not well-suited for solving structural problems due to the difficulty of taking context into account. consequence circumstance e1 e2 e3 e4 Figure 1: An example text fragment composed of two sentences and four EDUs, with its RST discourse tree representation shown below. greedy bottom-up strategy, we develop a discourse parser with a time complexity linear in the total number of sentences in the document. As a result of succe</context>
<context position="11938" citStr="Feng and Hirst (2012)" startWordPosition="1884" endWordPosition="1887"> local models, Mstruct intra , Mrel intra, Mstruct multi , While our bottom-up tree-building shares the greedy framework with HILDA, unlike HILDA, our local models are implemented using CRFs. In this way, we are able to take into account the sequential information from contextual discourse constituents, which cannot be naturally represented in HILDA with SVMs as local classifiers. Therefore, our model incorporates the strengths of both HILDA and Joty et al.’s model, i.e., the efficiency of a greedy parsing algorithm, and the ability to incorporate sequential information with CRFs. As shown by Feng and Hirst (2012), for a pair of discourse constituents of interest, the sequential information from contextual constituents is crucial for determining structures. Therefore, it is well motivated to use Conditional Random Fields (CRFs) (Lafferty et al., 2001), which is a discriminative probabilistic graphical model, to make predictions for a sequence of constituents surrounding the pair of interest. In this sense, our local models appear similar to Joty et al.’s non-greedy parsing models. However, the major distinction between our models and theirs is that we do not jointly model the structure and the relation</context>
<context position="30358" citStr="Feng and Hirst, 2012" startWordPosition="4993" endWordPosition="4996">structure models are trained using MALLET (McCallum, 2002) to include constraints over transitions between adjacent labels, and our relation models are trained using CRFSuite (Okazaki, 2007), which is a fast implementation of linear-chain CRFs. The data that we use to develop and evaluate our discourse parser is the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), which is a large corpus annotated in the framework of RST. The RST-DT consists of 385 documents (347 for training and 38 for testing) from the Wall Street Journal. Following previous work on the RST-DT (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013), we use 18 coarsegrained relation classes, and with nuclearity attached, we have a total set of 41 distinct relations. Non-binary relations are converted into a cascade of right-branching binary relations. 9 Results and Discussion 9.1 Parsing accuracy We compare four different models using manual EDU segmentation. In Table 1, the jCRF model in the first row is the optimal CRF model proposed by Joty et al. (2013). gSVMFH in the second row is our implementation of HILDA’s greedy parsing algorithm using Feng and Hirst (2012)’s enhanced feature set. The thir</context>
</contexts>
<marker>Feng, Hirst, 2012</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2012. Text-level discourse parsing with rich linguistic features. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL 2012), pages 60–68, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
<author>Helmut Prendinger</author>
<author>David A duVerle</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>HILDA: A discourse parser using support vector machine classification. Dialogue and Discourse,</title>
<date>2010</date>
<contexts>
<context position="5860" citStr="Hernault et al. (2010)" startWordPosition="910" endWordPosition="913">extual information in a more natural way, compared to using traditional discriminative classifiers, such as SVMs. Specifically, in the Viterbi decoding of the first CRF, we include additional constraints elicited from common sense, to make more effective local decisions. Third, after a discourse (sub)tree is fully built from bottom up, we perform a novel post-editing process by considering information from the constituents on upper levels. We show that this post-editing can further improve the overall parsing performance. 2 Related work 2.1 HILDA discourse parser The HILDA discourse parser by Hernault et al. (2010) is the first attempt at RST-style text-level discourse parsing. It adopts a pipeline framework, and greedily builds the discourse tree from the bottom up. In particular, starting from EDUs, at each step of the tree-building, a binary SVM classifier is first applied to determine which pair of adjacent discourse constituents should be merged to form a larger span, and another multi-class SVM classifier is then applied to assign the type of discourse relation that holds between the chosen pair. The strength of HILDA’s greedy tree-building 2.2 Joty et al.’s joint model Joty et al. (2013) approach</context>
<context position="30336" citStr="Hernault et al., 2010" startWordPosition="4989" endWordPosition="4992"> For local models, our structure models are trained using MALLET (McCallum, 2002) to include constraints over transitions between adjacent labels, and our relation models are trained using CRFSuite (Okazaki, 2007), which is a fast implementation of linear-chain CRFs. The data that we use to develop and evaluate our discourse parser is the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), which is a large corpus annotated in the framework of RST. The RST-DT consists of 385 documents (347 for training and 38 for testing) from the Wall Street Journal. Following previous work on the RST-DT (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013), we use 18 coarsegrained relation classes, and with nuclearity attached, we have a total set of 41 distinct relations. Non-binary relations are converted into a cascade of right-branching binary relations. 9 Results and Discussion 9.1 Parsing accuracy We compare four different models using manual EDU segmentation. In Table 1, the jCRF model in the first row is the optimal CRF model proposed by Joty et al. (2013). gSVMFH in the second row is our implementation of HILDA’s greedy parsing algorithm using Feng and Hirst (2012)’s enhanced</context>
</contexts>
<marker>Hernault, Prendinger, duVerle, Ishizuka, 2010</marker>
<rawString>Hugo Hernault, Helmut Prendinger, David A. duVerle, and Mitsuru Ishizuka. 2010. HILDA: A discourse parser using support vector machine classification. Dialogue and Discourse, 1(3):1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
</authors>
<title>A novel discriminative framework for sentence-level discourse analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL 2012,</booktitle>
<pages>904--915</pages>
<contexts>
<context position="2746" citStr="Joty et al., 2012" startWordPosition="428" endWordPosition="431">leus, and e1 is called satellite); e3 and e4 are related by another mononuclear relation CIRCUMSTANCE, with e4 as the nucleus; the two spans e1:2 and e3:4 are further related by a multi-nuclear relation SEQUENCE, with both spans as the nucleus. Conventionally, there are two major sub-tasks related to text-level discourse parsing: (1) EDU segmentation: to segment the raw text into EDUs, and (2) tree-building: to build a discourse tree from EDUs, representing the discourse relations in the text. Since the first sub-task is considered relatively easy, with the state-of-art accuracy at above 90% (Joty et al., 2012), the recent research focus is on the second sub-task, and often uses manual EDU segmentation. The current state-of-the-art overall accuracy of the tree-building sub-task, evaluated on the RST Discourse Treebank (RST-DT, to be introduced in Section 8), is 55.73% by Joty et al. (2013). However, as an optimal discourse parser, Joty et al.’s model is highly inefficient in practice, with respect to both their DCRF-based local classifiers, and their CKY-like bottom-up parsing algorithm. DCRF (Dynamic Conditional Random Fields) is a generalization of linear-chain CRFs, in which each time slice conta</context>
<context position="30377" citStr="Joty et al., 2012" startWordPosition="4997" endWordPosition="5000">rained using MALLET (McCallum, 2002) to include constraints over transitions between adjacent labels, and our relation models are trained using CRFSuite (Okazaki, 2007), which is a fast implementation of linear-chain CRFs. The data that we use to develop and evaluate our discourse parser is the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), which is a large corpus annotated in the framework of RST. The RST-DT consists of 385 documents (347 for training and 38 for testing) from the Wall Street Journal. Following previous work on the RST-DT (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013), we use 18 coarsegrained relation classes, and with nuclearity attached, we have a total set of 41 distinct relations. Non-binary relations are converted into a cascade of right-branching binary relations. 9 Results and Discussion 9.1 Parsing accuracy We compare four different models using manual EDU segmentation. In Table 1, the jCRF model in the first row is the optimal CRF model proposed by Joty et al. (2013). gSVMFH in the second row is our implementation of HILDA’s greedy parsing algorithm using Feng and Hirst (2012)’s enhanced feature set. The third model, gCRF, repr</context>
</contexts>
<marker>Joty, Carenini, Ng, 2012</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, and Raymond T. Ng. 2012. A novel discriminative framework for sentence-level discourse analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL 2012, pages 904–915.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond Ng</author>
<author>Yashar Mehdad</author>
</authors>
<title>Combining intra- and multisentential rhetorical parsing for document-level discourse analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>486--496</pages>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="3030" citStr="Joty et al. (2013)" startWordPosition="472" endWordPosition="475">-tasks related to text-level discourse parsing: (1) EDU segmentation: to segment the raw text into EDUs, and (2) tree-building: to build a discourse tree from EDUs, representing the discourse relations in the text. Since the first sub-task is considered relatively easy, with the state-of-art accuracy at above 90% (Joty et al., 2012), the recent research focus is on the second sub-task, and often uses manual EDU segmentation. The current state-of-the-art overall accuracy of the tree-building sub-task, evaluated on the RST Discourse Treebank (RST-DT, to be introduced in Section 8), is 55.73% by Joty et al. (2013). However, as an optimal discourse parser, Joty et al.’s model is highly inefficient in practice, with respect to both their DCRF-based local classifiers, and their CKY-like bottom-up parsing algorithm. DCRF (Dynamic Conditional Random Fields) is a generalization of linear-chain CRFs, in which each time slice contains a set of state variables and edges (Sutton et al., 2007). CKY parsing is a bottom-up parsing algorithm which searches all possible parsing paths by dynamic programming. Therefore, despite its superior performance, their model is infeasible in most realistic situations. The main o</context>
<context position="6451" citStr="Joty et al. (2013)" startWordPosition="1007" endWordPosition="1010">r by Hernault et al. (2010) is the first attempt at RST-style text-level discourse parsing. It adopts a pipeline framework, and greedily builds the discourse tree from the bottom up. In particular, starting from EDUs, at each step of the tree-building, a binary SVM classifier is first applied to determine which pair of adjacent discourse constituents should be merged to form a larger span, and another multi-class SVM classifier is then applied to assign the type of discourse relation that holds between the chosen pair. The strength of HILDA’s greedy tree-building 2.2 Joty et al.’s joint model Joty et al. (2013) approach the problem of textlevel discourse parsing using a model trained by Conditional Random Fields (CRF). Their model has two distinct features. First, they decomposed the problem of textlevel discourse parsing into two stages: intrasentential parsing to produce a discourse tree for each sentence, followed by multi-sentential parsing to combine the sentence-level discourse trees and produce the text-level discourse tree. Specifically, they employed two separate models for intra- and multi-sentential parsing. Their choice of two-stage parsing is well motivated for two reasons: (1) it has b</context>
<context position="8332" citStr="Joty et al. (2013)" startWordPosition="1311" endWordPosition="1314">g algorithm finds the globally optimal discourse tree, given the computed probabilities. The strength of Joty et al.’s model is their joint modeling of the structure and the relation, such that information from each aspect can interact with the other. However, their model has a major defect in its inefficiency, or even infeasibility, for application in practice. The inefficiency lies in both their DCRF-based joint model, on which inference is usually slow, and their CKY-like parsing algorithm, whose issue is more prominent. Due to the O(n3) time complexity, where n is the number 512 Figure 2: Joty et al. (2013)’s intra-sentential Condition Random Fields. of input discourse units, for large documents, the parsing simply takes too long1. 3 Overall work flow Figure 3 demonstrates the overall work flow of our discourse parser. The general idea is that, similar to Joty et al. (2013), we perform a sentence-level parsing for each sentence first, followed by a textlevel parsing to generate a full discourse tree for the whole document. However, in addition to efficiency (to be shown in Section 6), our discourse parser has a distinct feature, which is the postediting component (to be introduced in Section 5),</context>
<context position="16913" citStr="Joty et al. (2013)" startWordPosition="2717" endWordPosition="2720">nce is composed of the pair in question, with other EDUs in the sentence, even if those EDUs have already been merged. Thus, different CRF chains have to be formed for different pairs of constituents. In addition to efficiency, our use of a single CRF chain for all constituents can better capture the sequential dependencies among context, by taking into account the information from partially built discourse constituents, rather than bottom-level EDUs only. 4.2.2 Multi-sentential structure model For multi-sentential parsing, where the smallest discourse units are single sentences, as argued by Joty et al. (2013), it is not feasible to use a long chain to represent all constituents, due to the fact that it takes O(TM2) time to perform the forwardbackward exact inference on a chain with T units and an output vocabulary size of M, thus the overall complexity for all possible sequences in their model is O(M2n3)2. Instead, we choose to take a sliding-window approach to form CRF chains for a particular pair of constituents, as shown in Figure 4b. For example, suppose we wish to compute the structural probability for the pair Uj−1 and Uj, we form three chains, each of which contains two contextual constitue</context>
<context position="30397" citStr="Joty et al., 2013" startWordPosition="5001" endWordPosition="5004"> (McCallum, 2002) to include constraints over transitions between adjacent labels, and our relation models are trained using CRFSuite (Okazaki, 2007), which is a fast implementation of linear-chain CRFs. The data that we use to develop and evaluate our discourse parser is the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), which is a large corpus annotated in the framework of RST. The RST-DT consists of 385 documents (347 for training and 38 for testing) from the Wall Street Journal. Following previous work on the RST-DT (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013), we use 18 coarsegrained relation classes, and with nuclearity attached, we have a total set of 41 distinct relations. Non-binary relations are converted into a cascade of right-branching binary relations. 9 Results and Discussion 9.1 Parsing accuracy We compare four different models using manual EDU segmentation. In Table 1, the jCRF model in the first row is the optimal CRF model proposed by Joty et al. (2013). gSVMFH in the second row is our implementation of HILDA’s greedy parsing algorithm using Feng and Hirst (2012)’s enhanced feature set. The third model, gCRF, represents our greedy CR</context>
<context position="36067" citStr="Joty et al. (2013)" startWordPosition="5920" endWordPosition="5923"> 0.05 40.57 gCRFPE 10.71 0.12 84.72 Table 3: The parsing time (in seconds) for the 38 documents in the test set of RST-DT. Time cost of any pre-processing is excluded from the analysis. 10 Conclusions In this paper, we presented an efficient text-level discourse parser with time complexity linear in the total number of sentences in the document. Our approach was to adopt a greedy bottomup tree-building, with two linear-chain CRFs as local probabilistic models, and enforce reasonable constraints in the first CRF’s Viterbi decoding. While significantly outperforming the stateof-the-art model by Joty et al. (2013), our parser is much faster in practice. In addition, we propose a novel idea of post-editing, which modifies a fully-built discourse tree by considering information from upper-level constituents. We show that, although doubling the time consumption, postediting can further boost the parsing performance to close to 90% of human performance. In future work, we wish to further explore the idea of post-editing, since currently we use only the depth of the subtrees as upper-level information. Moreover, we wish to study whether we can incorporate constraints into the relation models, as we do to th</context>
</contexts>
<marker>Joty, Carenini, Ng, Mehdad, 2013</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, Raymond Ng, and Yashar Mehdad. 2013. Combining intra- and multisentential rhetorical parsing for document-level discourse analysis. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 486–496, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL 2003), ACL 2003,</booktitle>
<pages>423--430</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="29514" citStr="Klein and Manning, 2003" startWordPosition="4856" endWordPosition="4859">chains. However, for complex features, the time required for feature computation might be dominant. Nevertheless, a careful caching strategy can accelerate feature computation, since a large number of multi-sentential chains overlap with each other. and k = 3 for Mrel multi; l = 3), we have an 517 Cue phrase features: Whether a cue phrase occurs in the first or last EDU of each unit. The cue phrase list is based on the connectives collected by Knott and Dale (1994) Post-editing features: The depth of each unit in the initial tree. 8 Experiments For pre-processing, we use the Stanford CoreNLP (Klein and Manning, 2003; de Marneffe et al., 2006; Recasens et al., 2013) to syntactically parse the texts and extract coreference relations, and we use Penn2Malt5 to lexicalize syntactic trees to extract dominance features. For local models, our structure models are trained using MALLET (McCallum, 2002) to include constraints over transitions between adjacent labels, and our relation models are trained using CRFSuite (Okazaki, 2007), which is a fast implementation of linear-chain CRFs. The data that we use to develop and evaluate our discourse parser is the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), wh</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics (ACL 2003), ACL 2003, pages 423–430, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
<author>Robert Dale</author>
</authors>
<title>Using linguistic phenomena to motivate a set of coherence relations.</title>
<date>1994</date>
<booktitle>Discourse Processes,</booktitle>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="29360" citStr="Knott and Dale (1994)" startWordPosition="4832" endWordPosition="4835">h syntactic roles attached to each entity. 4We implicitly made an assumption that the parsing time is dominated by the time to perform inference on CRF chains. However, for complex features, the time required for feature computation might be dominant. Nevertheless, a careful caching strategy can accelerate feature computation, since a large number of multi-sentential chains overlap with each other. and k = 3 for Mrel multi; l = 3), we have an 517 Cue phrase features: Whether a cue phrase occurs in the first or last EDU of each unit. The cue phrase list is based on the connectives collected by Knott and Dale (1994) Post-editing features: The depth of each unit in the initial tree. 8 Experiments For pre-processing, we use the Stanford CoreNLP (Klein and Manning, 2003; de Marneffe et al., 2006; Recasens et al., 2013) to syntactically parse the texts and extract coreference relations, and we use Penn2Malt5 to lexicalize syntactic trees to extract dominance features. For local models, our structure models are trained using MALLET (McCallum, 2002) to include constraints over transitions between adjacent labels, and our relation models are trained using CRFSuite (Okazaki, 2007), which is a fast implementation</context>
</contexts>
<marker>Knott, Dale, 1994</marker>
<rawString>Alistair Knott and Robert Dale. 1994. Using linguistic phenomena to motivate a set of coherence relations. Discourse Processes, 18(1):35–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the Eighteenth International Conference on Machine Learning, ICML</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="12180" citStr="Lafferty et al., 2001" startWordPosition="1919" endWordPosition="1922"> sequential information from contextual discourse constituents, which cannot be naturally represented in HILDA with SVMs as local classifiers. Therefore, our model incorporates the strengths of both HILDA and Joty et al.’s model, i.e., the efficiency of a greedy parsing algorithm, and the ability to incorporate sequential information with CRFs. As shown by Feng and Hirst (2012), for a pair of discourse constituents of interest, the sequential information from contextual constituents is crucial for determining structures. Therefore, it is well motivated to use Conditional Random Fields (CRFs) (Lafferty et al., 2001), which is a discriminative probabilistic graphical model, to make predictions for a sequence of constituents surrounding the pair of interest. In this sense, our local models appear similar to Joty et al.’s non-greedy parsing models. However, the major distinction between our models and theirs is that we do not jointly model the structure and the relation; rather, we use two linearRelation sequence Structure sequence Unit sequence at level i U1 U2 R2 S2 U3 R3 S3 Uj Rj Sj Ut-1 Rt-1 St-1 and Mrel multi. 513 D S1 ... ... Sn Si Mintra Mintra Mintra ... ... TSn TpSn TSi TS, ... Pintra Pintra Pintr</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the Eighteenth International Conference on Machine Learning, ICML 2001, pages 282–289, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>Andrew McCallum</author>
</authors>
<title>Generalized Expectation Criteria for semi-supervised learning of Conditional Random Fields.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL</booktitle>
<pages>870--878</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<marker>Mann, McCallum, 2008</marker>
<rawString>Gideon S. Mann and Andrew McCallum. 2008. Generalized Expectation Criteria for semi-supervised learning of Conditional Random Fields. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL 2008), pages 870–878, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Mann</author>
<author>Sandra Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="1380" citStr="Mann and Thompson, 1988" startWordPosition="209" endWordPosition="212">terbi decoding of the first CRF. In addition to efficiency, our parser also significantly outperforms the state of the art. Moreover, our novel approach of post-editing, which modifies a fully-built tree by considering information from constituents on upper levels, can further improve the accuracy. 1 Introduction Discourse parsing is the task of identifying the presence and the type of the discourse relations between discourse units. While research in discourse parsing can be partitioned into several directions according to different theories and frameworks, Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is probably the most ambitious one, because it aims to identify not only the discourse relations in a small local context, but also the hierarchical tree structure for the full text: from the relations relating the smallest discourse units (called elementary discourse units, EDUs), to the ones connecting paragraphs. For example, Figure 1 shows a text fragment consisting of two sentences with four EDUs in total (e1-e4). Its discourse tree representation is Graeme Hirst Department of Computer Science University of Toronto Toronto, ON, Canada gh@cs.toronto.edu shown below the text, following the</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William Mann and Sandra Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The Theory and Practice of Discourse Parsing and Summarization.</title>
<date>2000</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="32041" citStr="Marcu (2000)" startWordPosition="5257" endWordPosition="5258"> 88.7 77.7 65.8 N/A *: significantly better than gSVMFH (p &lt; .01) †: significantly better than gCRF (p &lt; .01) Table 1: Performance of different models using gold-standard EDU segmentation, evaluated using the constituent accuracy (%) for span, nuclearity, and relation. For relation, we also report the macro-averaged F1-score (MAFS) for correctly retrieved constituents (before the slash) and for all constituents (after the slash). Statistical significance is verified using Wilcoxon’s signed-rank test. uation metrics, i.e., the unlabeled and labeled precision, recall, and F-score6 as defined by Marcu (2000). For evaluating relations, since there is a skewed distribution of different relation types in the corpus, we also include the macro-averaged F1-score (MAFS)7 as another metric, to emphasize the performance of infrequent relation types. We report the MAFS separately for the correctly retrieved constituents (i.e., the span boundary is correct) and all constituents in the reference tree. As demonstrated by Table 1, our greedy CRF models perform significantly better than the other two models. Since we do not have the actual output of Joty et al.’s model, we are unable to conduct significance tes</context>
</contexts>
<marker>Marcu, 2000</marker>
<rawString>Daniel Marcu. 2000. The Theory and Practice of Discourse Parsing and Summarization. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="29796" citStr="McCallum, 2002" startWordPosition="4901" endWordPosition="4902">e an 517 Cue phrase features: Whether a cue phrase occurs in the first or last EDU of each unit. The cue phrase list is based on the connectives collected by Knott and Dale (1994) Post-editing features: The depth of each unit in the initial tree. 8 Experiments For pre-processing, we use the Stanford CoreNLP (Klein and Manning, 2003; de Marneffe et al., 2006; Recasens et al., 2013) to syntactically parse the texts and extract coreference relations, and we use Penn2Malt5 to lexicalize syntactic trees to extract dominance features. For local models, our structure models are trained using MALLET (McCallum, 2002) to include constraints over transitions between adjacent labels, and our relation models are trained using CRFSuite (Okazaki, 2007), which is a fast implementation of linear-chain CRFs. The data that we use to develop and evaluate our discourse parser is the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), which is a large corpus annotated in the framework of RST. The RST-DT consists of 385 documents (347 for training and 38 for testing) from the Wall Street Journal. Following previous work on the RST-DT (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Muller</author>
<author>Stergos Afantenos</author>
<author>Pascal Denis</author>
<author>Nicholas Asher</author>
</authors>
<title>Constrained decoding for text-level discourse parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>1883--1900</pages>
<location>Mumbai, India,</location>
<marker>Muller, Afantenos, Denis, Asher, 2012</marker>
<rawString>Philippe Muller, Stergos Afantenos, Pascal Denis, and Nicholas Asher. 2012. Constrained decoding for text-level discourse parsing. In Proceedings of COLING 2012, pages 1883–1900, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>CRFsuite: a fast implementation of conditional random fields</title>
<date>2007</date>
<contexts>
<context position="29928" citStr="Okazaki, 2007" startWordPosition="4921" endWordPosition="4922">connectives collected by Knott and Dale (1994) Post-editing features: The depth of each unit in the initial tree. 8 Experiments For pre-processing, we use the Stanford CoreNLP (Klein and Manning, 2003; de Marneffe et al., 2006; Recasens et al., 2013) to syntactically parse the texts and extract coreference relations, and we use Penn2Malt5 to lexicalize syntactic trees to extract dominance features. For local models, our structure models are trained using MALLET (McCallum, 2002) to include constraints over transitions between adjacent labels, and our relation models are trained using CRFSuite (Okazaki, 2007), which is a fast implementation of linear-chain CRFs. The data that we use to develop and evaluate our discourse parser is the RST Discourse Treebank (RST-DT) (Carlson et al., 2001), which is a large corpus annotated in the framework of RST. The RST-DT consists of 385 documents (347 for training and 38 for testing) from the Wall Street Journal. Following previous work on the RST-DT (Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2012; Joty et al., 2013), we use 18 coarsegrained relation classes, and with nuclearity attached, we have a total set of 41 distinct relations. Non-binary </context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>Naoaki Okazaki. 2007. CRFsuite: a fast implementation of conditional random fields (CRFs). http://www.chokkan.org/software/crfsuite/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher Potts</author>
</authors>
<title>The life and death of discourse entities: Identifying singleton mentions.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>627--633</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<marker>Recasens, de Marneffe, Potts, 2013</marker>
<rawString>Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The life and death of discourse entities: Identifying singleton mentions. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 627–633, Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rajen Subba</author>
<author>Barbara Di Eugenio</author>
</authors>
<title>An effective discourse parser that uses rich linguistic information.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>566--574</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<marker>Subba, Di Eugenio, 2009</marker>
<rawString>Rajen Subba and Barbara Di Eugenio. 2009. An effective discourse parser that uses rich linguistic information. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 566–574, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
<author>Khashayar Rohanimanesh</author>
</authors>
<title>Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data.</title>
<date>2007</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>8</volume>
<contexts>
<context position="3406" citStr="Sutton et al., 2007" startWordPosition="531" endWordPosition="534">cond sub-task, and often uses manual EDU segmentation. The current state-of-the-art overall accuracy of the tree-building sub-task, evaluated on the RST Discourse Treebank (RST-DT, to be introduced in Section 8), is 55.73% by Joty et al. (2013). However, as an optimal discourse parser, Joty et al.’s model is highly inefficient in practice, with respect to both their DCRF-based local classifiers, and their CKY-like bottom-up parsing algorithm. DCRF (Dynamic Conditional Random Fields) is a generalization of linear-chain CRFs, in which each time slice contains a set of state variables and edges (Sutton et al., 2007). CKY parsing is a bottom-up parsing algorithm which searches all possible parsing paths by dynamic programming. Therefore, despite its superior performance, their model is infeasible in most realistic situations. The main objective of this work is to develop a more efficient discourse parser, with similar or even better performance with respect to Joty et al.’s optimal parser, but able to produce parsing results in real time. Our contribution is three-fold. First, with a 511 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 511–521, Baltimore, Mary</context>
<context position="13256" citStr="Sutton et al., 2007" startWordPosition="2108" endWordPosition="2111">2 U3 R3 S3 Uj Rj Sj Ut-1 Rt-1 St-1 and Mrel multi. 513 D S1 ... ... Sn Si Mintra Mintra Mintra ... ... TSn TpSn TSi TS, ... Pintra Pintra Pintra ... ... T p Si TP S1 Mmulti TD TP D Pmulti Figure 3: The work flow of our proposed discourse parser. In the figure, Mintra and Mmulti stand for the intra- and multi-sentential bottom-up tree-building models, and Pintra and Pmulti stand for the intra- and multi-sentential post-editing models. chain CRFs to model the structure and the relation separately. Although joint modeling has shown to be effective in various NLP and computer vision applications (Sutton et al., 2007; Yang et al., 2009; Wojek and Schiele, 2008), our choice of using two separate models is for the following reasons: First, it is not entirely appropriate to model the structure and the relation at the same time. For example, with respect to Figure 2, it is unclear how the relation node Rj is represented for a training instance whose structure node Sj = 0, i.e., the units Uj−1 and Uj are disjoint. Assume a special relation NO-REL is assigned for Rj. Then, in the tree-building process, we will have to deal with the situations where the joint model yields conflicting predictions: it is possible </context>
</contexts>
<marker>Sutton, McCallum, Rohanimanesh, 2007</marker>
<rawString>Charles Sutton, Andrew McCallum, and Khashayar Rohanimanesh. 2007. Dynamic conditional random fields: Factorized probabilistic models for labeling and segmenting sequence data. The Journal of Machine Learning Research, 8:693–723, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Wojek</author>
<author>Bernt Schiele</author>
</authors>
<title>A dynamic conditional random field model for joint labeling of object and scene classes.</title>
<date>2008</date>
<booktitle>In European Conference on Computer Vision (ECCV</booktitle>
<pages>733--747</pages>
<location>Marseille, France.</location>
<contexts>
<context position="13301" citStr="Wojek and Schiele, 2008" startWordPosition="2116" endWordPosition="2119">rel multi. 513 D S1 ... ... Sn Si Mintra Mintra Mintra ... ... TSn TpSn TSi TS, ... Pintra Pintra Pintra ... ... T p Si TP S1 Mmulti TD TP D Pmulti Figure 3: The work flow of our proposed discourse parser. In the figure, Mintra and Mmulti stand for the intra- and multi-sentential bottom-up tree-building models, and Pintra and Pmulti stand for the intra- and multi-sentential post-editing models. chain CRFs to model the structure and the relation separately. Although joint modeling has shown to be effective in various NLP and computer vision applications (Sutton et al., 2007; Yang et al., 2009; Wojek and Schiele, 2008), our choice of using two separate models is for the following reasons: First, it is not entirely appropriate to model the structure and the relation at the same time. For example, with respect to Figure 2, it is unclear how the relation node Rj is represented for a training instance whose structure node Sj = 0, i.e., the units Uj−1 and Uj are disjoint. Assume a special relation NO-REL is assigned for Rj. Then, in the tree-building process, we will have to deal with the situations where the joint model yields conflicting predictions: it is possible that the model predicts Sj = 1 and Rj = NO-RE</context>
</contexts>
<marker>Wojek, Schiele, 2008</marker>
<rawString>Christian Wojek and Bernt Schiele. 2008. A dynamic conditional random field model for joint labeling of object and scene classes. In European Conference on Computer Vision (ECCV 2008), pages 733–747, Marseille, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong Yang</author>
<author>Paul Dixon</author>
<author>Yi-Cheng Pan</author>
<author>Tasuku Oonishi</author>
<author>Masanobu Nakamura</author>
<author>Sadaoki Furui</author>
</authors>
<title>Combining a two-step conditional random field model and a joint source channel model for machine transliteration.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS</booktitle>
<pages>72--75</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="13275" citStr="Yang et al., 2009" startWordPosition="2112" endWordPosition="2115">t-1 Rt-1 St-1 and Mrel multi. 513 D S1 ... ... Sn Si Mintra Mintra Mintra ... ... TSn TpSn TSi TS, ... Pintra Pintra Pintra ... ... T p Si TP S1 Mmulti TD TP D Pmulti Figure 3: The work flow of our proposed discourse parser. In the figure, Mintra and Mmulti stand for the intra- and multi-sentential bottom-up tree-building models, and Pintra and Pmulti stand for the intra- and multi-sentential post-editing models. chain CRFs to model the structure and the relation separately. Although joint modeling has shown to be effective in various NLP and computer vision applications (Sutton et al., 2007; Yang et al., 2009; Wojek and Schiele, 2008), our choice of using two separate models is for the following reasons: First, it is not entirely appropriate to model the structure and the relation at the same time. For example, with respect to Figure 2, it is unclear how the relation node Rj is represented for a training instance whose structure node Sj = 0, i.e., the units Uj−1 and Uj are disjoint. Assume a special relation NO-REL is assigned for Rj. Then, in the tree-building process, we will have to deal with the situations where the joint model yields conflicting predictions: it is possible that the model pred</context>
</contexts>
<marker>Yang, Dixon, Pan, Oonishi, Nakamura, Furui, 2009</marker>
<rawString>Dong Yang, Paul Dixon, Yi-Cheng Pan, Tasuku Oonishi, Masanobu Nakamura, and Sadaoki Furui. 2009. Combining a two-step conditional random field model and a joint source channel model for machine transliteration. In Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration (NEWS 2009), pages 72–75, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>