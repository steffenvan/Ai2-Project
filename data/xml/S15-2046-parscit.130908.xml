<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010531">
<title confidence="0.994067">
ExB Themis: Extensive Feature Extraction from Word Alignments
for Semantic Textual Similarity
</title>
<author confidence="0.98795">
Christian H¨anig, Robert Remus, Xose De La Puente
</author>
<affiliation confidence="0.579922">
ExB Research &amp; Development GmbH
</affiliation>
<address confidence="0.814583">
Seeburgstr. 100
04103 Leipzig, Germany
</address>
<email confidence="0.98104">
{haenig, remus, puente}@exb.de
</email>
<sectionHeader confidence="0.99553" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9973614">
We present ExB Themis – a word alignment-
based semantic textual similarity system de-
veloped for SemEval-2015 Task 2: Semantic
Textual Similarity. It combines both string and
semantic similarity measures as well as align-
ment features using Support Vector Regres-
sion. It occupies the first three places on Span-
ish data and additionally places second on En-
glish data. ExB Themis proved to be the best
multilingual system among all participants.
</bodyText>
<sectionHeader confidence="0.99897" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999602853658537">
Semantic Textual Similarity (STS) is the task of
measuring the degree of semantic equivalence of a
sentence pair and is applicable to problems in Ma-
chine Translation and Summarization among others
(Agirre et al., 2012). STS has drawn a lot of atten-
tion in the last few years leading to the availabil-
ity of multilingual training and test data and to the
development of a variety of approaches. These ap-
proaches fall broadly into three categories (Han et
al., 2013):
Vector space approaches: Texts are represented as
bag-of-words vectors and a vector similarity –
e. g. cosine – is used to compute a similarity
score between two texts (Meadow et al., 1992).
Alignment approaches: Words and phrases in two
texts are aligned and the quality or coverage of
the resulting alignments are used as similarity
measure (Mihalcea et al., 2006; Sultan et al.,
2014).
Machine Learning approaches: Multiple similar-
ity measures and features are combined using
supervised Machine Learning (ML). This ap-
proach relies on the availability of training data
(B¨ar et al., 2012; ˇSari´c et al., 2012).
ExB Themis combines advantages of all three cat-
egories: we implemented a complex alignment al-
gorithm focusing on named entities, temporal ex-
pressions, measurement expressions and dedicated
negation handling. Unlike other alignment-based
approaches, we extract a variety of features to better
model the properties of alignments instead of pro-
viding only one alignment feature (see Section 4.1).
Moreover, we employ a variety of similarity mea-
sures based on strings and lexical items (see Sec-
tion 4.2). Our system integrates two well-known
language resources – WordNet1 and ConceptNet
(Speer and Havasi, 2012). Additionally, it uses word
embeddings to cope with data sparseness and the in-
sufficiency of overlaps between sentences.
Finally, we train a Support Vector Regression
(SVR) model using these features (see Section 5).
</bodyText>
<sectionHeader confidence="0.96605" genericHeader="introduction">
2 Preprocessing
</sectionHeader>
<bodyText confidence="0.992832333333333">
Our text preprocessing comprises tokenization, case
correction (e. g. US Flying Surveillance Missions
to Help Find Kidnapped Nigerian Girls is corrected
to US flying surveillance missions to help find kid-
napped Nigerian girls), unsupervised part-of-speech
(POS) tagging based on SVD2 (Lamar et al., 2010),
</bodyText>
<footnote confidence="0.8556365">
1English: we use the one described by Miller (1995); Span-
ish: we use the one presented in (Gonz´alez-Agirre et al., 2012).
</footnote>
<page confidence="0.959903">
264
</page>
<bodyText confidence="0.890831454545454">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 264–268,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
supervised POS tagging using the Stanford Maxi-
mum Entropy tagger2 as well as lemmatization using
Stanford CoreNLP3 for English and IXA Pipes4 for
Spanish. We also identify measurements (e. g. 55.8
g/mol) and temporal expressions (e. g. last week),
data set-specific stop words (e. g. A close-up of for
images dataset) using in-house algorithms as well as
named entities as described by H¨anig et al. (2014)
and their titles (e. g. President Barack Obama).
</bodyText>
<sectionHeader confidence="0.997799" genericHeader="method">
3 ExB Themis Alignment
</sectionHeader>
<bodyText confidence="0.999541566666667">
Our word alignment is direction-dependent and not
restricted to one-to-one alignments. Different map-
ping types are distinguished and handled differently
during feature extraction (see Section 4.1). We use
the same type labels as provided by the organizers
for the third subtask (interpretable STS) of this task
(Agirre et al., 2015): EQUI denotes semantically
equivalent chunks, oppositional meaning is labeled
with OPPO, SPE1/2 denote similar meaning of the
chunks, but the chunk in sentence 1/2 is more spe-
cific than the other one. SIM and REL denote sim-
ilar and related meanings, respectively. ALIC is not
used, because our algorithm is not restricted to one-
to-one alignments. Finally, all unaligned chunks are
labeled with NOALI.
Similar to Sultan et al. (2014), our alignment pro-
cess follows a strict chronological order:
Named entities are aligned to each other. Because
we did not observe text pairs with possibly am-
biguous name alignments (e. g. Michael in one
text and both Michael Jackson and Michael
Schumacher in the other) in the training data,
we simply aligned all name pairs that share at
least one identical token.
Normalized temporal expressions are aligned iff
they denote the same point in time or the same
time interval (e. g. 14:03 and 2.03 pm).
Measurement expressions are aligned iff they ex-
press the same absolute value (e. g. $100k and
100.000$).
</bodyText>
<footnote confidence="0.919385333333333">
2nlp.stanford.edu/software/tagger.shtml
3nlp.stanford.edu/software/corenlp.shtml
4ixa2.si.ehu.es/ixa-pipes/
</footnote>
<bodyText confidence="0.998601608695652">
Arbitrary token sequence alignment consists of
multiple steps and is very time consuming5.
We apply a high precision test for identical se-
quences based on Sultan et al. (2014): Our
test uses synonym-lookups and ignores case in-
formation, punctuation characters and symbols.
This enables us to match expressions like long
term and long-term6. If one of both sequences
consists of exactly one all-caps-token then we
test if it is the acronym of the other sequence
(e. g. US and United States).
We used WordNet and ConceptNet7 to ob-
tain information about synonymy, antonymy
and hypernymy and equip the resulting align-
ments with the corresponding type. We ad-
ditionally created a small database containing
high-frequency synonyms (e. g. does and do),
antonyms (e. g. doesn’t and does) and nega-
tions (e. g. don’t, never, no).
Negations can significantly effect the semantic sim-
ilarity of two sentences (e. g. You are a Chris-
tian. vs. Therefore you are not a Christian.).
Therefore, we explicitly model negations in our
alignment. Some negations are handled dur-
ing arbitrary token sequence alignment. We
resolve the scope of all remaining negations
using co-occurrence analysis: if exactly one
of both neighboring tokens w1/2
n−1 and w1/2
n+1 is
already aligned then the negation w1/2
n is at-
tached to it and we inverse the alignment type
(e. g. EQUI becomes OPPO and vice versa).
If both neighboring tokens are aligned then we
pick the one contained in the co-occurence out
1/2 of Cwn−1, wn�2&gt; and Cwn/2, wn+21&gt; yielding
the highest co-occurrence significance score.
Remaining content words are aligned using co-
sine similarity on word2vec vectors (Mikolov
et al., 2013). Analogously to Han et al. (2013),
we align each content word to the content word
of the other sentence with the same POS tag
that yields the highest similarity score. To
prevent weak alignments, we reject alignments
with a similarity less than 1/3.
</bodyText>
<footnote confidence="0.999393">
5Therefore, we restrict ourselves to a maximum of 5 tokens.
6A similar method was described by Han et al. (2013).
7From ConceptNet we only imported synonyms.
</footnote>
<page confidence="0.997872">
265
</page>
<sectionHeader confidence="0.992332" genericHeader="method">
4 Feature Extraction
</sectionHeader>
<bodyText confidence="0.999886875">
Some approaches to STS relying on word align-
ment are unsupervised and extract a defined score
based on the alignment process (e. g. proportions of
aligned content words (Sultan et al., 2014)), others
extract a single feature from the alignment and use it
along with other features to train a regression model
(e. g. align-and-penalize approach (Han et al., 2013;
Kashyap et al., 2014)).
Unlike these approaches, we extract 40 features
from our alignment (see Section 4.1) to (a) build a
complex model that is capable of modeling phenom-
ena like alignments of different types and negations,
and (b) not be forced to combine alignment proper-
ties arbitrarily.
We additionally extract 51 non-alignment features
(see Section 4.2) leading to a total of 91 features.
</bodyText>
<subsectionHeader confidence="0.996194">
4.1 Alignment Features
</subsectionHeader>
<bodyText confidence="0.999891666666667">
To encode the properties of a set of alignments A of
sentences s1 and s2 as comprehensive as possible,
we extract the following features8:
Proportion features describe the ratio of aligned
words of a specified group with respect to all
words of that group (Sultan et al., 2014)9:
</bodyText>
<equation confidence="0.982121">
2 prop1group prop2
group with
group
prop1 group + prop2
1/2— |{i:[∃j:(i,j)∈Agroup] and wi/ ∈C}|
propgroup — 1/2
{i:wi ∈C}|
</equation>
<bodyText confidence="0.914850545454545">
where C is the set of all content words. We
extract these features for alignments of type
EQUI, OPPO, SPE1/2, REL10 and NOALI
(5 features).
Frequency features are encoded in binary format.
We encode frequencies of alignments of type
OPPO (3 features), SPE1/2 (3), REL (3) and
NOALI (5). We also encode the frequency of
unaligned negations with 3 features.
UMBC align-and-penalize features: We also in-
clude two features11 based on Han et al.
</bodyText>
<footnote confidence="0.681816333333333">
8Type-filtered subsets of A are denoted by Atype.
9See Sultan et al. (2014) for details on the formulae.
10Each content word is weighted by the similarity score
achieved by word2vec for this type.
11Splitting STS = T − P0 into two features T and P0
achieves better results than keeping it in the original form.
</footnote>
<bodyText confidence="0.957407">
(2013): we use their T as it is and inte-
grated a simplified version of P0 with PiA =
</bodyText>
<equation confidence="0.87357875">
E
(t,g(t))EAi (1+wp(t)) B = I (t,9(t))∈Bi|
2 |si |and Pi 2 |si|
(2 features).
</equation>
<bodyText confidence="0.9730085">
All proportion features, binary frequency features of
REL-alignments, unaligned content words and un-
aligned negations were additionally computed and
extracted for nouns only (16 features).
</bodyText>
<subsectionHeader confidence="0.964949">
4.2 Non-Alignment Features
</subsectionHeader>
<bodyText confidence="0.9995109375">
We use a variety of non-alignment features:
UKP: We use several features described in B¨ar et al.
(2012): longest common substring (1 feature),
longest common subsequence (1), longest com-
mon subsequence with and without normaliza-
tion (2), greedy string tiling (1), character n-
grams for n = 2, 3, 4 with and without stop
words (6), word n-grams Jaccard coefficient for
n = 1, 2, 3, 4 (4), word n-grams Jaccard coeffi-
cient without stop words for n = 2, 4 (2), word
n-grams containment measure for n = 1, 2 (2)
as well as pairwise word similarity (1).
TakeLab: We use several features described in
ˇSari´c et al. (2012)12: PathLen similarity (1 fea-
ture), corpus-based word similarity (3), vec-
tor space sentence similarity (1), n-gram over-
lap of tokens and lemmas for n = 1, 2,3 (6),
weighted word overlap for lowercased tokens
and lemmas (2), normalized sentence length
difference (1), shallow named entity features
(4) and numbers overlap (3).
UMBC: We use several features described in Han
et al. (2013): word n-gram similarity for n =
1, 2, 3, 4 (4 features). Moreover, we used word
n-gram similarity for n = 1 where only nouns
or only verbs where taken into account (2).
Readability Indicators: We use several features
that are typically used as indicators for read-
ability (Oelke et al., 2012): relative difference
in sentence length, average word length in char-
acters, number of nouns per sentence, number
of verbs per sentence and noun-verb-ratio (5).
</bodyText>
<equation confidence="0.938288">
12takelab.fer.hr/sts/
propgroup =
</equation>
<page confidence="0.998523">
266
</page>
<sectionHeader confidence="0.997161" genericHeader="method">
5 STS Model
</sectionHeader>
<bodyText confidence="0.9999655">
We compute STS scores using ν-SVR (Sch¨olkopf
et al., 2000) as implemented by LibSVM13. We use
LibSVM’s default SVR parameter settings without
further optimization.
</bodyText>
<sectionHeader confidence="0.999371" genericHeader="method">
6 Interpretable STS Model
</sectionHeader>
<bodyText confidence="0.9999701">
We align chunks using our word alignment (see Sec-
tion 3). Because our word alignment itself does not
rely on chunks, we extend its alignments using given
chunk boundaries. If alignments overlap, we choose
the longest alignment and discard the others. We do
not differentiate between SIMI and REL – all REL
alignments are considered as SIMI alignments.
For chunking we use the OpenNLP14 chunker
with the default model trained on CoNLL-2000
shared task data (Sang and Buchholz, 2000).
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999793333333333">
For English we train on all available data sets from
STS challenges in 2012 (Agirre et al., 2012), 2013
(Agirre et al., 2013) and 2014 (Agirre et al., 2014).
For Spanish, each run trains on a different setting.
Mean Pearson correlation is employed as an evalua-
tion metric.
</bodyText>
<subsectionHeader confidence="0.99176">
7.1 Subtask 2a – STS English
</subsectionHeader>
<bodyText confidence="0.999945222222222">
Table 1 presents the official scores of our system.
Run default uses our system as it is. Run themis
only relies on alignment features in the belief model,
all other models are the same as for default. Our
third run – themisexp – is identical to run themis ex-
cept for one improvement: it penalizes scores of the
answers-students dataset exponentially to cope with
the high ratio of common content words that lead to
over-estimation of similarity scores.
</bodyText>
<subsectionHeader confidence="0.997917">
7.2 Subtask 2b – STS Spanish
</subsectionHeader>
<bodyText confidence="0.999943285714286">
Table 2 presents the official scores of our system.
Run trainEs was trained on both Spanish test sets of
2014. Run trainEn was trained on all available En-
glish data sets. Run trainMini uses different training
sets for each test set: Wikipedia model was trained
on the 2014 Wikipedia test set and the Newswire
model was trained on the News test set of 2014.
</bodyText>
<footnote confidence="0.998186">
13www.csie.ntu.edu.tw/˜cjlin/libsvm/
14opennlp.apache.org
</footnote>
<table confidence="0.999842857142857">
Dataset default themis themisexp
forum 0.6946 (10) 0.6946 (10) 0.6946 (10)
students 0.7505 (11) 0.7505 (11) 0.7784 (6)
belief 0.7521 (3) 0.7482 (6) 0.7482 (6)
headlines 0.8245 (7) 0.8245 (7) 0.8245 (7)
images 0.8527 (12) 0.8527 (12) 0.8527 (12)
Mean 0.7878 (8) 0.7873 (9) 0.7942 (2)
</table>
<tableCaption confidence="0.999447">
Table 1: Results (rank) of our three runs on English data.
</tableCaption>
<table confidence="0.99969825">
Dataset trainEs trainMini trainEn
Wikipedia 0.7055 (2) 0.7055 (1) 0.6763 (3)
Newswire 0.6830 (1) 0.6811 0.6705
Mean 0.6905 (1) 0.6893 (2) 0.6725 (3)
</table>
<tableCaption confidence="0.999749">
Table 2: Results (rank) of our three runs on Spanish data.
</tableCaption>
<subsectionHeader confidence="0.957845">
7.3 Subtask 2c – Interpretable STS
</subsectionHeader>
<bodyText confidence="0.999815928571429">
Our three runs only differ regarding the applied
alignment scorer method: we use the average simi-
larity score per alignment type as observed in STSint
training data, the most frequent similarity score per
alignment type as observed in STSint training data,
and an STS regression model per alignment type
trained on all available English STS data sets.
For subtrack gold chunks, our runs score 0.4885
to 0.4883 (F1 TYPE + SCORE) on headlines (ranks
10 - 12 out of 14) and 0.4296 to 0.4246 on images
(ranks 8 - 10). Using system chunks we achieve
scores of 0.4290 to 0.4284 on headlines (ranks 4–
6 out of 10) and 0.3870 to 0.3806 on images (ranks
4–6).
</bodyText>
<sectionHeader confidence="0.988966" genericHeader="conclusions">
8 Conclusions &amp; Future Work
</sectionHeader>
<bodyText confidence="0.9997876">
We presented our alignment-based STS system ExB
Themis. Our system outperformed all other partic-
ipants by a large margin on Spanish data. Further-
more, our system placed second on English data.
ExB Themis proved to be the best multilingual STS
system that easily can be adapted to further lan-
guages. We conclude that extensive feature extrac-
tion from word alignments is a very robust approach
– especially when being applied to languages that
lack high-quality resources.
In future work, we will investigate the influence
of particular features in more detail and we want to
enrich our model with structural information (Sev-
eryn et al., 2013; Sultan et al., 2014) and improved
phrase similarity computation.
</bodyText>
<page confidence="0.994398">
267
</page>
<sectionHeader confidence="0.994569" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999590504761904">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6 : A
Pilot on Semantic Textual Similarity. In First Joint
Conference on Lexical and Computational Semantics
(*SEM), pages 385–393.
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 shared
task: Semantic Textual Similarity. In Second Joint
Conference on Lexical and Computational Semantics
(*SEM), Volume 1: Proceedings of the Main Confer-
ence and the Shared Task, pages 32–43, Atlanta, USA.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer,
Mona Diab, Ann Arbor, Aitor Gonzalez-Agirre, Wei-
wei Guo, Rada Mihalcea, German Rigau, and Janyce
Wiebe. 2014. SemEval-2014 Task 10: Multilingual
Semantic Textual Similarity. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval2014), pages 81–91, Dublin, Ireland.
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihal-
cea, German Rigau, Larraitz Uria, and Janyce Wiebe.
2015. SemEval-2015 Task 2: Semantic Textual Sim-
ilarity, English, Spanish and Pilot on Interpretability.
In Proceedings of the 9th International Workshop on
Semantic Evaluation (SemEval 2015), Denver, CO.
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. UKP: Computing Semantic Textual
Similarity by Combining Multiple Content Similarity
Measures. In Proceedings of the First Joint Confer-
ence on Lexical and Computational Semantics Volume
2: Proceedings of the Sixth International Workshop on
Semantic Evaluation, pages 435–440.
Aitor Gonz´alez-Agirre, Egoitz Laparra, and German
Rigau. 2012. Multilingual Central Repository version
3.0: upgrading a very large lexical knowledge base. In
Proceedings of the Sixth International Global WordNet
Conference (GWC12), Matsue, Japan.
Lushan Han, Abhay Kashyap, Tim Finin, James May-
field, and Jonathan Weese. 2013. UMBC EBIQUITY-
CORE: Semantic Textual Similarity Systems. In Pro-
ceedings of the Second Joint Conference on Lexical
and Computational Semantics.
Christian H¨anig, Stefan Bordag, and Stefan Thomas.
2014. Modular Classifier Ensemble Architecture for
Named Entity Recognition on Low Resource Systems.
In Workshop Proceedings of the 12th Edition of the
KONVENS Conference, pages 113–116, Hildesheim,
Germany.
Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer
Sleeman, Taneeya Satyapanich, Sunil Gandhi, and
Tim Finin. 2014. Meerkat Mafia: Multilingual and
Cross-Level Semantic Textual Similarity Systems. In
Proceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval 2014), pages 416–423,
Dublin, Ireland.
Michael Lamar, Yariv Maron, Mark Johnson, and Elie
Bienenstock. 2010. SVD and Clustering for Unsu-
pervised POS Tagging. In Proceedings of ACL 2010,
pages 215–219, Uppsala, Sweden.
Charles T. Meadow, Bert R. Boyce, and Donald H. Kraft.
1992. Text Information Retrieval Systems, volume 2.
Academic Press San Diego.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the 21st
national conference on Artificial intelligence, pages
775–780.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient Estimation of Word Represen-
tations in Vector Space. In Proceedings of Workshop
at ICLR, pages 1–12, January.
George A. Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):39–41.
Daniela Oelke, David Spretke, Andreas Stoffel, and
Daniel A Keim. 2012. Visual readability analysis:
How to make your writings easier to read. IEEE
Transactions on Visualization and Computer Graph-
ics, 18(5):662–674.
Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. In-
troduction to the CoNLL-2000 Shared Task: Chunk-
ing. In Proceedings of CoNLL 2000, pages 127–132.
Bernhard Sch¨olkopf, Alex J Smola, Robert C
Williamson, and Peter L Bartlett. 2000. New
Support Vector Algorithms. Neural Computation,
12(5):1207–1245.
Aliaksei Severyn, Massimo Nicosia, and Alessandro
Moschitti. 2013. iKernels-Core: Tree Kernel Learn-
ing for Textual Similarity. In Second Joint Conference
on Lexical and Computational Semantics (*SEM), Vol-
ume 1: Proceedings of the Main Conference and the
Shared Task, pages 53–58, Atlanta, USA.
Robert Speer and Catherine Havasi. 2012. ConceptNet
5: A Large Semantic Network for Relational Knowl-
edge. In The Peoples Web Meets NLP, pages 161–176.
Md Arafat Sultan, Steven Bethard, and Tamara Sum-
ner. 2014. DLS@CU: Sentence Similarity from Word
Alignment. In Proceedings of the 8th International
Workshop on Semantic Evaluation (SemEval 2014),
pages 241–246, Dublin, Ireland.
Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder,
and Bojana Dalbelo Baˇsi´c. 2012. TakeLab: Systems
for Measuring Semantic Text Similarity. In First Joint
Conference on Lexical and Computational Semantics,
pages 441–448, Montreal, Canada.
</reference>
<page confidence="0.996834">
268
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.700576">
<title confidence="0.994759">ExB Themis: Extensive Feature Extraction from Word for Semantic Textual Similarity</title>
<author confidence="0.999702">Christian H¨anig</author>
<author confidence="0.999702">Robert Remus</author>
<author confidence="0.999702">Xose De_La</author>
<affiliation confidence="0.871838">ExB Research &amp; Development Seeburgstr.</affiliation>
<address confidence="0.993856">04103 Leipzig,</address>
<email confidence="0.959885">remus,</email>
<abstract confidence="0.999145818181818">present Themis a word alignmentbased semantic textual similarity system developed for SemEval-2015 Task 2: Semantic Textual Similarity. It combines both string and semantic similarity measures as well as alignment features using Support Vector Regression. It occupies the first three places on Spanish data and additionally places second on Endata. Themis to be the best multilingual system among all participants.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<date>2012</date>
<booktitle>SemEval-2012 Task 6 : A Pilot on Semantic Textual Similarity. In First Joint Conference on Lexical and Computational Semantics (*SEM),</booktitle>
<pages>385--393</pages>
<contexts>
<context position="929" citStr="Agirre et al., 2012" startWordPosition="138" endWordPosition="141"> textual similarity system developed for SemEval-2015 Task 2: Semantic Textual Similarity. It combines both string and semantic similarity measures as well as alignment features using Support Vector Regression. It occupies the first three places on Spanish data and additionally places second on English data. ExB Themis proved to be the best multilingual system among all participants. 1 Introduction Semantic Textual Similarity (STS) is the task of measuring the degree of semantic equivalence of a sentence pair and is applicable to problems in Machine Translation and Summarization among others (Agirre et al., 2012). STS has drawn a lot of attention in the last few years leading to the availability of multilingual training and test data and to the development of a variety of approaches. These approaches fall broadly into three categories (Han et al., 2013): Vector space approaches: Texts are represented as bag-of-words vectors and a vector similarity – e. g. cosine – is used to compute a similarity score between two texts (Meadow et al., 1992). Alignment approaches: Words and phrases in two texts are aligned and the quality or coverage of the resulting alignments are used as similarity measure (Mihalcea </context>
<context position="3058" citStr="Agirre et al., 2012" startWordPosition="472" endWordPosition="475">o cope with data sparseness and the insufficiency of overlaps between sentences. Finally, we train a Support Vector Regression (SVR) model using these features (see Section 5). 2 Preprocessing Our text preprocessing comprises tokenization, case correction (e. g. US Flying Surveillance Missions to Help Find Kidnapped Nigerian Girls is corrected to US flying surveillance missions to help find kidnapped Nigerian girls), unsupervised part-of-speech (POS) tagging based on SVD2 (Lamar et al., 2010), 1English: we use the one described by Miller (1995); Spanish: we use the one presented in (Gonz´alez-Agirre et al., 2012). 264 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 264–268, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics supervised POS tagging using the Stanford Maximum Entropy tagger2 as well as lemmatization using Stanford CoreNLP3 for English and IXA Pipes4 for Spanish. We also identify measurements (e. g. 55.8 g/mol) and temporal expressions (e. g. last week), data set-specific stop words (e. g. A close-up of for images dataset) using in-house algorithms as well as named entities as described by H¨anig et al. (2014) and </context>
<context position="11905" citStr="Agirre et al., 2012" startWordPosition="1913" endWordPosition="1916">on. 6 Interpretable STS Model We align chunks using our word alignment (see Section 3). Because our word alignment itself does not rely on chunks, we extend its alignments using given chunk boundaries. If alignments overlap, we choose the longest alignment and discard the others. We do not differentiate between SIMI and REL – all REL alignments are considered as SIMI alignments. For chunking we use the OpenNLP14 chunker with the default model trained on CoNLL-2000 shared task data (Sang and Buchholz, 2000). 7 Results For English we train on all available data sets from STS challenges in 2012 (Agirre et al., 2012), 2013 (Agirre et al., 2013) and 2014 (Agirre et al., 2014). For Spanish, each run trains on a different setting. Mean Pearson correlation is employed as an evaluation metric. 7.1 Subtask 2a – STS English Table 1 presents the official scores of our system. Run default uses our system as it is. Run themis only relies on alignment features in the belief model, all other models are the same as for default. Our third run – themisexp – is identical to run themis except for one improvement: it penalizes scores of the answers-students dataset exponentially to cope with the high ratio of common conten</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6 : A Pilot on Semantic Textual Similarity. In First Joint Conference on Lexical and Computational Semantics (*SEM), pages 385–393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>shared task: Semantic Textual Similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task,</booktitle>
<pages>32--43</pages>
<publisher>SEM</publisher>
<location>Atlanta, USA.</location>
<contexts>
<context position="11933" citStr="Agirre et al., 2013" startWordPosition="1918" endWordPosition="1921">l We align chunks using our word alignment (see Section 3). Because our word alignment itself does not rely on chunks, we extend its alignments using given chunk boundaries. If alignments overlap, we choose the longest alignment and discard the others. We do not differentiate between SIMI and REL – all REL alignments are considered as SIMI alignments. For chunking we use the OpenNLP14 chunker with the default model trained on CoNLL-2000 shared task data (Sang and Buchholz, 2000). 7 Results For English we train on all available data sets from STS challenges in 2012 (Agirre et al., 2012), 2013 (Agirre et al., 2013) and 2014 (Agirre et al., 2014). For Spanish, each run trains on a different setting. Mean Pearson correlation is employed as an evaluation metric. 7.1 Subtask 2a – STS English Table 1 presents the official scores of our system. Run default uses our system as it is. Run themis only relies on alignment features in the belief model, all other models are the same as for default. Our third run – themisexp – is identical to run themis except for one improvement: it penalizes scores of the answers-students dataset exponentially to cope with the high ratio of common content words that lead to over-es</context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *SEM 2013 shared task: Semantic Textual Similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 32–43, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Ann Arbor</author>
<author>Aitor Gonzalez-Agirre</author>
<author>Weiwei Guo</author>
<author>Rada Mihalcea</author>
<author>German Rigau</author>
<author>Janyce Wiebe</author>
</authors>
<date>2014</date>
<booktitle>SemEval-2014 Task 10: Multilingual Semantic Textual Similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval2014),</booktitle>
<pages>81--91</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="11964" citStr="Agirre et al., 2014" startWordPosition="1924" endWordPosition="1927">d alignment (see Section 3). Because our word alignment itself does not rely on chunks, we extend its alignments using given chunk boundaries. If alignments overlap, we choose the longest alignment and discard the others. We do not differentiate between SIMI and REL – all REL alignments are considered as SIMI alignments. For chunking we use the OpenNLP14 chunker with the default model trained on CoNLL-2000 shared task data (Sang and Buchholz, 2000). 7 Results For English we train on all available data sets from STS challenges in 2012 (Agirre et al., 2012), 2013 (Agirre et al., 2013) and 2014 (Agirre et al., 2014). For Spanish, each run trains on a different setting. Mean Pearson correlation is employed as an evaluation metric. 7.1 Subtask 2a – STS English Table 1 presents the official scores of our system. Run default uses our system as it is. Run themis only relies on alignment features in the belief model, all other models are the same as for default. Our third run – themisexp – is identical to run themis except for one improvement: it penalizes scores of the answers-students dataset exponentially to cope with the high ratio of common content words that lead to over-estimation of similarity scores. </context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, Arbor, Gonzalez-Agirre, Guo, Mihalcea, Rigau, Wiebe, 2014</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Ann Arbor, Aitor Gonzalez-Agirre, Weiwei Guo, Rada Mihalcea, German Rigau, and Janyce Wiebe. 2014. SemEval-2014 Task 10: Multilingual Semantic Textual Similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval2014), pages 81–91, Dublin, Ireland.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
</authors>
<title>Aitor Gonzalez-Agirre, Weiwei Guo, Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe.</title>
<date>2015</date>
<booktitle>SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, CO.</location>
<contexts>
<context position="4058" citStr="Agirre et al., 2015" startWordPosition="624" endWordPosition="627"> 55.8 g/mol) and temporal expressions (e. g. last week), data set-specific stop words (e. g. A close-up of for images dataset) using in-house algorithms as well as named entities as described by H¨anig et al. (2014) and their titles (e. g. President Barack Obama). 3 ExB Themis Alignment Our word alignment is direction-dependent and not restricted to one-to-one alignments. Different mapping types are distinguished and handled differently during feature extraction (see Section 4.1). We use the same type labels as provided by the organizers for the third subtask (interpretable STS) of this task (Agirre et al., 2015): EQUI denotes semantically equivalent chunks, oppositional meaning is labeled with OPPO, SPE1/2 denote similar meaning of the chunks, but the chunk in sentence 1/2 is more specific than the other one. SIM and REL denote similar and related meanings, respectively. ALIC is not used, because our algorithm is not restricted to oneto-one alignments. Finally, all unaligned chunks are labeled with NOALI. Similar to Sultan et al. (2014), our alignment process follows a strict chronological order: Named entities are aligned to each other. Because we did not observe text pairs with possibly ambiguous n</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, 2015</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, CO.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>435--440</pages>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. UKP: Computing Semantic Textual Similarity by Combining Multiple Content Similarity Measures. In Proceedings of the First Joint Conference on Lexical and Computational Semantics Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 435–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aitor Gonz´alez-Agirre</author>
<author>Egoitz Laparra</author>
<author>German Rigau</author>
</authors>
<title>Multilingual Central Repository version 3.0: upgrading a very large lexical knowledge base.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Global WordNet Conference (GWC12),</booktitle>
<location>Matsue, Japan.</location>
<marker>Gonz´alez-Agirre, Laparra, Rigau, 2012</marker>
<rawString>Aitor Gonz´alez-Agirre, Egoitz Laparra, and German Rigau. 2012. Multilingual Central Repository version 3.0: upgrading a very large lexical knowledge base. In Proceedings of the Sixth International Global WordNet Conference (GWC12), Matsue, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Jonathan Weese</author>
</authors>
<title>UMBC EBIQUITYCORE: Semantic Textual Similarity Systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the Second Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="1174" citStr="Han et al., 2013" startWordPosition="183" endWordPosition="186"> Spanish data and additionally places second on English data. ExB Themis proved to be the best multilingual system among all participants. 1 Introduction Semantic Textual Similarity (STS) is the task of measuring the degree of semantic equivalence of a sentence pair and is applicable to problems in Machine Translation and Summarization among others (Agirre et al., 2012). STS has drawn a lot of attention in the last few years leading to the availability of multilingual training and test data and to the development of a variety of approaches. These approaches fall broadly into three categories (Han et al., 2013): Vector space approaches: Texts are represented as bag-of-words vectors and a vector similarity – e. g. cosine – is used to compute a similarity score between two texts (Meadow et al., 1992). Alignment approaches: Words and phrases in two texts are aligned and the quality or coverage of the resulting alignments are used as similarity measure (Mihalcea et al., 2006; Sultan et al., 2014). Machine Learning approaches: Multiple similarity measures and features are combined using supervised Machine Learning (ML). This approach relies on the availability of training data (B¨ar et al., 2012; ˇSari´c</context>
<context position="6903" citStr="Han et al. (2013)" startWordPosition="1078" endWordPosition="1081">ce alignment. We resolve the scope of all remaining negations using co-occurrence analysis: if exactly one of both neighboring tokens w1/2 n−1 and w1/2 n+1 is already aligned then the negation w1/2 n is attached to it and we inverse the alignment type (e. g. EQUI becomes OPPO and vice versa). If both neighboring tokens are aligned then we pick the one contained in the co-occurence out 1/2 of Cwn−1, wn�2&gt; and Cwn/2, wn+21&gt; yielding the highest co-occurrence significance score. Remaining content words are aligned using cosine similarity on word2vec vectors (Mikolov et al., 2013). Analogously to Han et al. (2013), we align each content word to the content word of the other sentence with the same POS tag that yields the highest similarity score. To prevent weak alignments, we reject alignments with a similarity less than 1/3. 5Therefore, we restrict ourselves to a maximum of 5 tokens. 6A similar method was described by Han et al. (2013). 7From ConceptNet we only imported synonyms. 265 4 Feature Extraction Some approaches to STS relying on word alignment are unsupervised and extract a defined score based on the alignment process (e. g. proportions of aligned content words (Sultan et al., 2014)), others </context>
<context position="10621" citStr="Han et al. (2013)" startWordPosition="1703" endWordPosition="1706">), word n-grams Jaccard coefficient without stop words for n = 2, 4 (2), word n-grams containment measure for n = 1, 2 (2) as well as pairwise word similarity (1). TakeLab: We use several features described in ˇSari´c et al. (2012)12: PathLen similarity (1 feature), corpus-based word similarity (3), vector space sentence similarity (1), n-gram overlap of tokens and lemmas for n = 1, 2,3 (6), weighted word overlap for lowercased tokens and lemmas (2), normalized sentence length difference (1), shallow named entity features (4) and numbers overlap (3). UMBC: We use several features described in Han et al. (2013): word n-gram similarity for n = 1, 2, 3, 4 (4 features). Moreover, we used word n-gram similarity for n = 1 where only nouns or only verbs where taken into account (2). Readability Indicators: We use several features that are typically used as indicators for readability (Oelke et al., 2012): relative difference in sentence length, average word length in characters, number of nouns per sentence, number of verbs per sentence and noun-verb-ratio (5). 12takelab.fer.hr/sts/ propgroup = 266 5 STS Model We compute STS scores using ν-SVR (Sch¨olkopf et al., 2000) as implemented by LibSVM13. We use Li</context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay Kashyap, Tim Finin, James Mayfield, and Jonathan Weese. 2013. UMBC EBIQUITYCORE: Semantic Textual Similarity Systems. In Proceedings of the Second Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian H¨anig</author>
<author>Stefan Bordag</author>
<author>Stefan Thomas</author>
</authors>
<title>Modular Classifier Ensemble Architecture for Named Entity Recognition on Low Resource Systems.</title>
<date>2014</date>
<booktitle>In Workshop Proceedings of the 12th Edition of the KONVENS Conference,</booktitle>
<pages>113--116</pages>
<location>Hildesheim, Germany.</location>
<marker>H¨anig, Bordag, Thomas, 2014</marker>
<rawString>Christian H¨anig, Stefan Bordag, and Stefan Thomas. 2014. Modular Classifier Ensemble Architecture for Named Entity Recognition on Low Resource Systems. In Workshop Proceedings of the 12th Edition of the KONVENS Conference, pages 113–116, Hildesheim, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhay Kashyap</author>
<author>Lushan Han</author>
<author>Roberto Yus</author>
<author>Jennifer Sleeman</author>
<author>Taneeya Satyapanich</author>
<author>Sunil Gandhi</author>
<author>Tim Finin</author>
</authors>
<title>Meerkat Mafia: Multilingual and Cross-Level Semantic Textual Similarity Systems.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>416--423</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="7687" citStr="Kashyap et al., 2014" startWordPosition="1209" endWordPosition="1212">we reject alignments with a similarity less than 1/3. 5Therefore, we restrict ourselves to a maximum of 5 tokens. 6A similar method was described by Han et al. (2013). 7From ConceptNet we only imported synonyms. 265 4 Feature Extraction Some approaches to STS relying on word alignment are unsupervised and extract a defined score based on the alignment process (e. g. proportions of aligned content words (Sultan et al., 2014)), others extract a single feature from the alignment and use it along with other features to train a regression model (e. g. align-and-penalize approach (Han et al., 2013; Kashyap et al., 2014)). Unlike these approaches, we extract 40 features from our alignment (see Section 4.1) to (a) build a complex model that is capable of modeling phenomena like alignments of different types and negations, and (b) not be forced to combine alignment properties arbitrarily. We additionally extract 51 non-alignment features (see Section 4.2) leading to a total of 91 features. 4.1 Alignment Features To encode the properties of a set of alignments A of sentences s1 and s2 as comprehensive as possible, we extract the following features8: Proportion features describe the ratio of aligned words of a sp</context>
</contexts>
<marker>Kashyap, Han, Yus, Sleeman, Satyapanich, Gandhi, Finin, 2014</marker>
<rawString>Abhay Kashyap, Lushan Han, Roberto Yus, Jennifer Sleeman, Taneeya Satyapanich, Sunil Gandhi, and Tim Finin. 2014. Meerkat Mafia: Multilingual and Cross-Level Semantic Textual Similarity Systems. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 416–423, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lamar</author>
<author>Yariv Maron</author>
<author>Mark Johnson</author>
<author>Elie Bienenstock</author>
</authors>
<title>SVD and Clustering for Unsupervised POS Tagging.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010,</booktitle>
<pages>215--219</pages>
<location>Uppsala,</location>
<contexts>
<context position="2935" citStr="Lamar et al., 2010" startWordPosition="451" endWordPosition="454"> well-known language resources – WordNet1 and ConceptNet (Speer and Havasi, 2012). Additionally, it uses word embeddings to cope with data sparseness and the insufficiency of overlaps between sentences. Finally, we train a Support Vector Regression (SVR) model using these features (see Section 5). 2 Preprocessing Our text preprocessing comprises tokenization, case correction (e. g. US Flying Surveillance Missions to Help Find Kidnapped Nigerian Girls is corrected to US flying surveillance missions to help find kidnapped Nigerian girls), unsupervised part-of-speech (POS) tagging based on SVD2 (Lamar et al., 2010), 1English: we use the one described by Miller (1995); Spanish: we use the one presented in (Gonz´alez-Agirre et al., 2012). 264 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 264–268, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics supervised POS tagging using the Stanford Maximum Entropy tagger2 as well as lemmatization using Stanford CoreNLP3 for English and IXA Pipes4 for Spanish. We also identify measurements (e. g. 55.8 g/mol) and temporal expressions (e. g. last week), data set-specific stop words (e. g. A cl</context>
</contexts>
<marker>Lamar, Maron, Johnson, Bienenstock, 2010</marker>
<rawString>Michael Lamar, Yariv Maron, Mark Johnson, and Elie Bienenstock. 2010. SVD and Clustering for Unsupervised POS Tagging. In Proceedings of ACL 2010, pages 215–219, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles T Meadow</author>
<author>Bert R Boyce</author>
<author>Donald H Kraft</author>
</authors>
<date>1992</date>
<journal>Text Information Retrieval Systems,</journal>
<volume>2</volume>
<publisher>Academic Press</publisher>
<location>San Diego.</location>
<contexts>
<context position="1365" citStr="Meadow et al., 1992" startWordPosition="215" endWordPosition="218">is the task of measuring the degree of semantic equivalence of a sentence pair and is applicable to problems in Machine Translation and Summarization among others (Agirre et al., 2012). STS has drawn a lot of attention in the last few years leading to the availability of multilingual training and test data and to the development of a variety of approaches. These approaches fall broadly into three categories (Han et al., 2013): Vector space approaches: Texts are represented as bag-of-words vectors and a vector similarity – e. g. cosine – is used to compute a similarity score between two texts (Meadow et al., 1992). Alignment approaches: Words and phrases in two texts are aligned and the quality or coverage of the resulting alignments are used as similarity measure (Mihalcea et al., 2006; Sultan et al., 2014). Machine Learning approaches: Multiple similarity measures and features are combined using supervised Machine Learning (ML). This approach relies on the availability of training data (B¨ar et al., 2012; ˇSari´c et al., 2012). ExB Themis combines advantages of all three categories: we implemented a complex alignment algorithm focusing on named entities, temporal expressions, measurement expressions </context>
</contexts>
<marker>Meadow, Boyce, Kraft, 1992</marker>
<rawString>Charles T. Meadow, Bert R. Boyce, and Donald H. Kraft. 1992. Text Information Retrieval Systems, volume 2. Academic Press San Diego.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and Knowledge-based Measures of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st national conference on Artificial intelligence,</booktitle>
<pages>775--780</pages>
<contexts>
<context position="1541" citStr="Mihalcea et al., 2006" startWordPosition="243" endWordPosition="246">l., 2012). STS has drawn a lot of attention in the last few years leading to the availability of multilingual training and test data and to the development of a variety of approaches. These approaches fall broadly into three categories (Han et al., 2013): Vector space approaches: Texts are represented as bag-of-words vectors and a vector similarity – e. g. cosine – is used to compute a similarity score between two texts (Meadow et al., 1992). Alignment approaches: Words and phrases in two texts are aligned and the quality or coverage of the resulting alignments are used as similarity measure (Mihalcea et al., 2006; Sultan et al., 2014). Machine Learning approaches: Multiple similarity measures and features are combined using supervised Machine Learning (ML). This approach relies on the availability of training data (B¨ar et al., 2012; ˇSari´c et al., 2012). ExB Themis combines advantages of all three categories: we implemented a complex alignment algorithm focusing on named entities, temporal expressions, measurement expressions and dedicated negation handling. Unlike other alignment-based approaches, we extract a variety of features to better model the properties of alignments instead of providing onl</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and Knowledge-based Measures of Text Semantic Similarity. In Proceedings of the 21st national conference on Artificial intelligence, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient Estimation of Word Representations in Vector Space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at ICLR,</booktitle>
<pages>1--12</pages>
<contexts>
<context position="6869" citStr="Mikolov et al., 2013" startWordPosition="1072" endWordPosition="1075"> handled during arbitrary token sequence alignment. We resolve the scope of all remaining negations using co-occurrence analysis: if exactly one of both neighboring tokens w1/2 n−1 and w1/2 n+1 is already aligned then the negation w1/2 n is attached to it and we inverse the alignment type (e. g. EQUI becomes OPPO and vice versa). If both neighboring tokens are aligned then we pick the one contained in the co-occurence out 1/2 of Cwn−1, wn�2&gt; and Cwn/2, wn+21&gt; yielding the highest co-occurrence significance score. Remaining content words are aligned using cosine similarity on word2vec vectors (Mikolov et al., 2013). Analogously to Han et al. (2013), we align each content word to the content word of the other sentence with the same POS tag that yields the highest similarity score. To prevent weak alignments, we reject alignments with a similarity less than 1/3. 5Therefore, we restrict ourselves to a maximum of 5 tokens. 6A similar method was described by Han et al. (2013). 7From ConceptNet we only imported synonyms. 265 4 Feature Extraction Some approaches to STS relying on word alignment are unsupervised and extract a defined score based on the alignment process (e. g. proportions of aligned content wor</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient Estimation of Word Representations in Vector Space. In Proceedings of Workshop at ICLR, pages 1–12, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="2988" citStr="Miller (1995)" startWordPosition="462" endWordPosition="463">peer and Havasi, 2012). Additionally, it uses word embeddings to cope with data sparseness and the insufficiency of overlaps between sentences. Finally, we train a Support Vector Regression (SVR) model using these features (see Section 5). 2 Preprocessing Our text preprocessing comprises tokenization, case correction (e. g. US Flying Surveillance Missions to Help Find Kidnapped Nigerian Girls is corrected to US flying surveillance missions to help find kidnapped Nigerian girls), unsupervised part-of-speech (POS) tagging based on SVD2 (Lamar et al., 2010), 1English: we use the one described by Miller (1995); Spanish: we use the one presented in (Gonz´alez-Agirre et al., 2012). 264 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 264–268, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics supervised POS tagging using the Stanford Maximum Entropy tagger2 as well as lemmatization using Stanford CoreNLP3 for English and IXA Pipes4 for Spanish. We also identify measurements (e. g. 55.8 g/mol) and temporal expressions (e. g. last week), data set-specific stop words (e. g. A close-up of for images dataset) using in-house algorith</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniela Oelke</author>
<author>David Spretke</author>
<author>Andreas Stoffel</author>
<author>Daniel A Keim</author>
</authors>
<title>Visual readability analysis: How to make your writings easier to read.</title>
<date>2012</date>
<journal>IEEE Transactions on Visualization and Computer Graphics,</journal>
<volume>18</volume>
<issue>5</issue>
<contexts>
<context position="10913" citStr="Oelke et al., 2012" startWordPosition="1755" endWordPosition="1758">arity (3), vector space sentence similarity (1), n-gram overlap of tokens and lemmas for n = 1, 2,3 (6), weighted word overlap for lowercased tokens and lemmas (2), normalized sentence length difference (1), shallow named entity features (4) and numbers overlap (3). UMBC: We use several features described in Han et al. (2013): word n-gram similarity for n = 1, 2, 3, 4 (4 features). Moreover, we used word n-gram similarity for n = 1 where only nouns or only verbs where taken into account (2). Readability Indicators: We use several features that are typically used as indicators for readability (Oelke et al., 2012): relative difference in sentence length, average word length in characters, number of nouns per sentence, number of verbs per sentence and noun-verb-ratio (5). 12takelab.fer.hr/sts/ propgroup = 266 5 STS Model We compute STS scores using ν-SVR (Sch¨olkopf et al., 2000) as implemented by LibSVM13. We use LibSVM’s default SVR parameter settings without further optimization. 6 Interpretable STS Model We align chunks using our word alignment (see Section 3). Because our word alignment itself does not rely on chunks, we extend its alignments using given chunk boundaries. If alignments overlap, we </context>
</contexts>
<marker>Oelke, Spretke, Stoffel, Keim, 2012</marker>
<rawString>Daniela Oelke, David Spretke, Andreas Stoffel, and Daniel A Keim. 2012. Visual readability analysis: How to make your writings easier to read. IEEE Transactions on Visualization and Computer Graphics, 18(5):662–674.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Sabine Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 Shared Task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<pages>127--132</pages>
<contexts>
<context position="11796" citStr="Sang and Buchholz, 2000" startWordPosition="1893" endWordPosition="1896"> al., 2000) as implemented by LibSVM13. We use LibSVM’s default SVR parameter settings without further optimization. 6 Interpretable STS Model We align chunks using our word alignment (see Section 3). Because our word alignment itself does not rely on chunks, we extend its alignments using given chunk boundaries. If alignments overlap, we choose the longest alignment and discard the others. We do not differentiate between SIMI and REL – all REL alignments are considered as SIMI alignments. For chunking we use the OpenNLP14 chunker with the default model trained on CoNLL-2000 shared task data (Sang and Buchholz, 2000). 7 Results For English we train on all available data sets from STS challenges in 2012 (Agirre et al., 2012), 2013 (Agirre et al., 2013) and 2014 (Agirre et al., 2014). For Spanish, each run trains on a different setting. Mean Pearson correlation is employed as an evaluation metric. 7.1 Subtask 2a – STS English Table 1 presents the official scores of our system. Run default uses our system as it is. Run themis only relies on alignment features in the belief model, all other models are the same as for default. Our third run – themisexp – is identical to run themis except for one improvement: i</context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>Erik F. Tjong Kim Sang and Sabine Buchholz. 2000. Introduction to the CoNLL-2000 Shared Task: Chunking. In Proceedings of CoNLL 2000, pages 127–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Sch¨olkopf</author>
<author>Alex J Smola</author>
<author>Robert C Williamson</author>
<author>Peter L Bartlett</author>
</authors>
<title>New Support Vector Algorithms.</title>
<date>2000</date>
<journal>Neural Computation,</journal>
<volume>12</volume>
<issue>5</issue>
<marker>Sch¨olkopf, Smola, Williamson, Bartlett, 2000</marker>
<rawString>Bernhard Sch¨olkopf, Alex J Smola, Robert C Williamson, and Peter L Bartlett. 2000. New Support Vector Algorithms. Neural Computation, 12(5):1207–1245.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Massimo Nicosia</author>
<author>Alessandro Moschitti</author>
</authors>
<title>iKernels-Core: Tree Kernel Learning for Textual Similarity.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task,</booktitle>
<pages>53--58</pages>
<location>Atlanta, USA.</location>
<marker>Severyn, Nicosia, Moschitti, 2013</marker>
<rawString>Aliaksei Severyn, Massimo Nicosia, and Alessandro Moschitti. 2013. iKernels-Core: Tree Kernel Learning for Textual Similarity. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task, pages 53–58, Atlanta, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Speer</author>
<author>Catherine Havasi</author>
</authors>
<title>ConceptNet 5: A Large Semantic Network for Relational Knowledge. In The Peoples Web Meets NLP,</title>
<date>2012</date>
<pages>161--176</pages>
<contexts>
<context position="2397" citStr="Speer and Havasi, 2012" startWordPosition="372" endWordPosition="375"> et al., 2012). ExB Themis combines advantages of all three categories: we implemented a complex alignment algorithm focusing on named entities, temporal expressions, measurement expressions and dedicated negation handling. Unlike other alignment-based approaches, we extract a variety of features to better model the properties of alignments instead of providing only one alignment feature (see Section 4.1). Moreover, we employ a variety of similarity measures based on strings and lexical items (see Section 4.2). Our system integrates two well-known language resources – WordNet1 and ConceptNet (Speer and Havasi, 2012). Additionally, it uses word embeddings to cope with data sparseness and the insufficiency of overlaps between sentences. Finally, we train a Support Vector Regression (SVR) model using these features (see Section 5). 2 Preprocessing Our text preprocessing comprises tokenization, case correction (e. g. US Flying Surveillance Missions to Help Find Kidnapped Nigerian Girls is corrected to US flying surveillance missions to help find kidnapped Nigerian girls), unsupervised part-of-speech (POS) tagging based on SVD2 (Lamar et al., 2010), 1English: we use the one described by Miller (1995); Spanish</context>
</contexts>
<marker>Speer, Havasi, 2012</marker>
<rawString>Robert Speer and Catherine Havasi. 2012. ConceptNet 5: A Large Semantic Network for Relational Knowledge. In The Peoples Web Meets NLP, pages 161–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Md Arafat Sultan</author>
<author>Steven Bethard</author>
<author>Tamara Sumner</author>
</authors>
<title>DLS@CU: Sentence Similarity from Word Alignment.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>241--246</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1563" citStr="Sultan et al., 2014" startWordPosition="247" endWordPosition="250">n a lot of attention in the last few years leading to the availability of multilingual training and test data and to the development of a variety of approaches. These approaches fall broadly into three categories (Han et al., 2013): Vector space approaches: Texts are represented as bag-of-words vectors and a vector similarity – e. g. cosine – is used to compute a similarity score between two texts (Meadow et al., 1992). Alignment approaches: Words and phrases in two texts are aligned and the quality or coverage of the resulting alignments are used as similarity measure (Mihalcea et al., 2006; Sultan et al., 2014). Machine Learning approaches: Multiple similarity measures and features are combined using supervised Machine Learning (ML). This approach relies on the availability of training data (B¨ar et al., 2012; ˇSari´c et al., 2012). ExB Themis combines advantages of all three categories: we implemented a complex alignment algorithm focusing on named entities, temporal expressions, measurement expressions and dedicated negation handling. Unlike other alignment-based approaches, we extract a variety of features to better model the properties of alignments instead of providing only one alignment featur</context>
<context position="4491" citStr="Sultan et al. (2014)" startWordPosition="694" endWordPosition="697">erently during feature extraction (see Section 4.1). We use the same type labels as provided by the organizers for the third subtask (interpretable STS) of this task (Agirre et al., 2015): EQUI denotes semantically equivalent chunks, oppositional meaning is labeled with OPPO, SPE1/2 denote similar meaning of the chunks, but the chunk in sentence 1/2 is more specific than the other one. SIM and REL denote similar and related meanings, respectively. ALIC is not used, because our algorithm is not restricted to oneto-one alignments. Finally, all unaligned chunks are labeled with NOALI. Similar to Sultan et al. (2014), our alignment process follows a strict chronological order: Named entities are aligned to each other. Because we did not observe text pairs with possibly ambiguous name alignments (e. g. Michael in one text and both Michael Jackson and Michael Schumacher in the other) in the training data, we simply aligned all name pairs that share at least one identical token. Normalized temporal expressions are aligned iff they denote the same point in time or the same time interval (e. g. 14:03 and 2.03 pm). Measurement expressions are aligned iff they express the same absolute value (e. g. $100k and 100</context>
<context position="7493" citStr="Sultan et al., 2014" startWordPosition="1177" endWordPosition="1180">alogously to Han et al. (2013), we align each content word to the content word of the other sentence with the same POS tag that yields the highest similarity score. To prevent weak alignments, we reject alignments with a similarity less than 1/3. 5Therefore, we restrict ourselves to a maximum of 5 tokens. 6A similar method was described by Han et al. (2013). 7From ConceptNet we only imported synonyms. 265 4 Feature Extraction Some approaches to STS relying on word alignment are unsupervised and extract a defined score based on the alignment process (e. g. proportions of aligned content words (Sultan et al., 2014)), others extract a single feature from the alignment and use it along with other features to train a regression model (e. g. align-and-penalize approach (Han et al., 2013; Kashyap et al., 2014)). Unlike these approaches, we extract 40 features from our alignment (see Section 4.1) to (a) build a complex model that is capable of modeling phenomena like alignments of different types and negations, and (b) not be forced to combine alignment properties arbitrarily. We additionally extract 51 non-alignment features (see Section 4.2) leading to a total of 91 features. 4.1 Alignment Features To encod</context>
<context position="9004" citStr="Sultan et al. (2014)" startWordPosition="1427" endWordPosition="1430">group with group prop1 group + prop2 1/2— |{i:[∃j:(i,j)∈Agroup] and wi/ ∈C}| propgroup — 1/2 {i:wi ∈C}| where C is the set of all content words. We extract these features for alignments of type EQUI, OPPO, SPE1/2, REL10 and NOALI (5 features). Frequency features are encoded in binary format. We encode frequencies of alignments of type OPPO (3 features), SPE1/2 (3), REL (3) and NOALI (5). We also encode the frequency of unaligned negations with 3 features. UMBC align-and-penalize features: We also include two features11 based on Han et al. 8Type-filtered subsets of A are denoted by Atype. 9See Sultan et al. (2014) for details on the formulae. 10Each content word is weighted by the similarity score achieved by word2vec for this type. 11Splitting STS = T − P0 into two features T and P0 achieves better results than keeping it in the original form. (2013): we use their T as it is and integrated a simplified version of P0 with PiA = E (t,g(t))EAi (1+wp(t)) B = I (t,9(t))∈Bi| 2 |si |and Pi 2 |si| (2 features). All proportion features, binary frequency features of REL-alignments, unaligned content words and unaligned negations were additionally computed and extracted for nouns only (16 features). 4.2 Non-Alig</context>
</contexts>
<marker>Sultan, Bethard, Sumner, 2014</marker>
<rawString>Md Arafat Sultan, Steven Bethard, and Tamara Sumner. 2014. DLS@CU: Sentence Similarity from Word Alignment. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 241–246, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane ˇSari´c</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
<author>Jan ˇSnajder</author>
<author>Bojana Dalbelo Baˇsi´c</author>
</authors>
<title>TakeLab: Systems for Measuring Semantic Text Similarity.</title>
<date>2012</date>
<booktitle>In First Joint Conference on Lexical and Computational Semantics,</booktitle>
<pages>441--448</pages>
<location>Montreal, Canada.</location>
<marker>ˇSari´c, Glavaˇs, Karan, ˇSnajder, Baˇsi´c, 2012</marker>
<rawString>Frane ˇSari´c, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsi´c. 2012. TakeLab: Systems for Measuring Semantic Text Similarity. In First Joint Conference on Lexical and Computational Semantics, pages 441–448, Montreal, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>