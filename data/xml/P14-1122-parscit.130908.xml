<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.031515">
<title confidence="0.997999">
Validating and Extending Semantic Knowledge Bases
using Video Games with a Purpose
</title>
<author confidence="0.999579">
Daniele Vannella, David Jurgens, Daniele Scarfini, Domenico Toscani and Roberto Navigli
</author>
<affiliation confidence="0.9986515">
Department of Computer Science
Sapienza University of Rome
</affiliation>
<email confidence="0.993766">
surname@di.uniroma1.it
</email>
<sectionHeader confidence="0.993701" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999394611111111">
Large-scale knowledge bases are impor-
tant assets in NLP. Frequently, such re-
sources are constructed through automatic
mergers of complementary resources, such
as WordNet and Wikipedia. However,
manually validating these resources is pro-
hibitively expensive, even when using
methods such as crowdsourcing. We pro-
pose a cost-effective method of validat-
ing and extending knowledge bases using
video games with a purpose. Two video
games were created to validate concept-
concept and concept-image relations. In
experiments comparing with crowdsourc-
ing, we show that video game-based vali-
dation consistently leads to higher-quality
annotations, even when players are not
compensated.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999935806451613">
Large-scale knowledge bases are an essential
component of many approaches in Natural Lan-
guage Processing (NLP). Semantic knowledge
bases such as WordNet (Fellbaum, 1998), YAGO
(Suchanek et al., 2007), and BabelNet (Navigli
and Ponzetto, 2010) provide ontological struc-
ture that enables a wide range of tasks, such as
measuring semantic relatedness (Budanitsky and
Hirst, 2006) and similarity (Pilehvar et al., 2013),
paraphrasing (Kauchak and Barzilay, 2006), and
word sense disambiguation (Navigli and Ponzetto,
2012; Moro et al., 2014). Furthermore, such
knowledge bases are essential for building unsu-
pervised algorithms when training data is sparse
or unavailable. However, constructing and updat-
ing semantic knowledge bases is often limited by
the significant time and human resources required.
Recent approaches have attempted to build or
extend these knowledge bases automatically. For
example, Snow et al. (2006) and Navigli (2005)
extend WordNet using distributional or structural
features to identify novel semantic connections
between concepts. The recent advent of large
semi-structured resources has enabled the creation
of new semantic knowledge bases (Medelyan et
al., 2009; Hovy et al., 2013) through automati-
cally merging WordNet and Wikipedia (Suchanek
et al., 2007; Navigli and Ponzetto, 2010; Nie-
mann and Gurevych, 2011). While these auto-
matic approaches offer the scale needed for open-
domain applications, the automatic processes of-
ten introduce errors, which can prove detrimental
to downstream applications. To overcome issues
from fully-automatic construction methods, sev-
eral works have proposed validating or extending
knowledge bases using crowdsourcing (Biemann
and Nygaard, 2010; Eom et al., 2012; Sarasua et
al., 2012). However, these methods, too, are lim-
ited by the resources required for acquiring large
numbers of responses.
In this paper, we propose validating and extend-
ing semantic knowledge bases using video games
with a purpose. Here, the annotation tasks are
transformed into elements of a video game where
players accomplish their jobs by virtue of playing
the game, rather than by performing a more tradi-
tional annotation task. While prior efforts in NLP
have incorporated games for performing annota-
tion and validation (Siorpaes and Hepp, 2008b;
Herda˘gdelen and Baroni, 2012; Poesio et al.,
2013), these games have largely been text-based,
adding game-like features such as high-scores on
top of an existing annotation task. In contrast,
we introduce two video games with graphical 2D
gameplay that is similar to what game players are
familiar with. The fun nature of the games pro-
vides an intrinsic motivation for players to keep
playing, which can increase the quality of their
work and lower the cost per annotation.
Our work provides the following three contribu-
tions. First, we demonstrate effective video game-
based methods for both validating and extending
</bodyText>
<page confidence="0.942931">
1294
</page>
<note confidence="0.829976">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1294–1304,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999786583333333">
semantic networks, using two games that operate
on complementary sources of information: seman-
tic relations and sense-image mappings. In con-
trast to previous work, the annotation quality is
determined in a fully automatic way. Second, we
demonstrate that converting games with a purpose
into more traditional video games creates an in-
creased player incentive such that players annotate
for free, thereby significantly lowering annotation
costs below that of crowdsourcing. Third, for both
games, we show that games produce better quality
annotations than crowdsourcing.
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999938666666667">
Multiple works have proposed linguistic
annotation-based games with a purpose for
tasks such as anaphora resolution (Hladk´a et
al., 2009; Poesio et al., 2013), paraphrasing
(Chklovski and Gil, 2005), term associations
(Artignan et al., 2009; Lafourcade and Joubert,
2010), query expansion (Simko et al., 2011), and
word sense disambiguation (Chklovski and Mi-
halcea, 2002; Seemakurty et al., 2010; Venhuizen
et al., 2013). Notably, all of these linguistic games
focus on users interacting with text, in contrast
to other highly successful games with a purpose
in other domains, such as Foldit (Cooper et al.,
2010), in which players fold protein sequences,
and the ESP game (von Ahn and Dabbish, 2004),
where players label images with words.
Most similar to our work are games that create
or validate common sense knowledge. Two games
with a purpose have incorporated video game-
like mechanics for annotation. First, Herda˘gdelen
and Baroni (2012) validate automatically acquired
common sense relations using a slot machine
game where players must identify valid relations
and arguments from randomly aligned data within
a time limit. Although the validation is embedded
in a game-like setting, players are limited to one
action (pulling the lever) unlike our games, which
feature a variety of actions and rich gameplay ex-
perience to keep players interested longer. Sec-
ond, Kuo et al. (2009) describe a pet-raising game
where players must answer common sense ques-
tions in order to obtain pet food. While their game
is among the most video game-like, the annotation
task is a chore the player must perform in order to
return to the game, rather than an integrated, fun
part of the game’s objectives, which potentially
decreases motivation for answering correctly.
Several works have proposed adapting existing
word-based board game designs to create or val-
idate common sense knowledge. von Ahn et al.
(2006) generate common sense facts by using a
game similar to TabooTM, where one player must
list facts about a computer-selected lemma and a
second player must guess the original lemma hav-
ing seen only the facts. Similarly, Vickrey et al.
(2008) gather free associations to a target word
with the constraint, similar to TabooTM, where
players cannot enter a small set of banned words.
Vickrey et al. (2008) also present two games simi-
lar to the ScattergoriesTM, where players are given
a category and then must list things in that cate-
gory. The two variants differ in the constraints im-
posed on the players, such as beginning all items
with a specific letter. For all three games, two
players play the same game under time limits and
then are rewarded if their answers match.
Last, three two-player games have focused
on validating and extending knowledge bases.
Rzeniewicz and Szyma´nski (2013) extend Word-
Net with common-sense knowledge using a 20
Questions-like game. In a rapid-play style game,
OntoPronto attempts to classify Wikipedia pages
as either categories or individuals (Siorpaes and
Hepp, 2008a). SpotTheLink uses a similar rapid
question format to have players align the DBpedia
and PROTON ontologies by agreeing on the dis-
tinctions between classes (Thaler et al., 2011).
Unlike dynamic gaming elements common in
our video games, the above games are all focused
on interacting with textual items. Another major
limitation is their need for always having two play-
ers, which requires them to sustain enough inter-
est to always maintain an active pool of players.
While the computer can potentially act as a second
player, such a simulated player is often limited to
using preexisting knowledge or responses, which
makes it difficult to validate new types of entities
or create novel answers. In contrast, we drop this
requirement thanks to a new strategy for assign-
ing confidence scores to the annotations based on
negative associations.
</bodyText>
<sectionHeader confidence="0.978286" genericHeader="method">
3 Video Game with a Purpose Design
</sectionHeader>
<bodyText confidence="0.999369666666667">
To create video games, our development process
focused on a common design philosophy and a
common data set.
</bodyText>
<subsectionHeader confidence="0.995424">
3.1 Design Objectives
</subsectionHeader>
<bodyText confidence="0.9997715">
Three design objectives were used to develop the
video games. First, the annotation task should be
a central and natural action with familiar video
game mechanics. That is, the annotation should
</bodyText>
<page confidence="0.981375">
1295
</page>
<bodyText confidence="0.999986681818182">
be supplied by common actions such as collecting
items, puzzles, or destroying objects, rather than
through extrinsic tasks that players must complete
in order to return to the game. This design has
the benefits of (1) growing the annotator pool with
video games players, and (2) potentially increas-
ing annotator enjoyment.
Second, the game should be playable by a single
player, with reinforcement for correct game play
coming from gold standard examples.1 We note
that gold standard examples may come from both
true positive and true negative items.
Third, the game design should be sufficiently
general to annotate a variety of linguistic phenom-
ena, such that only the game data need be changed
to accomplish a different annotation task. While
some complex linguistic annotation tasks such as
preposition attachment may be difficult to inte-
grate directly into gameplay, many simpler but still
necessary annotation tasks such as word and im-
age associations can be easily modeled with tradi-
tional video game mechanics.
</bodyText>
<subsectionHeader confidence="0.999809">
3.2 Annotation Setup
</subsectionHeader>
<bodyText confidence="0.999905269230769">
Tasks We focused on two annotation tasks: (1)
validating associations between two concepts, and
(2) validating associations between a concept and
an image. For each task we developed a video
game with a purpose that integrates the task within
the game, as illustrated in Sections 4 and 5.
Knowledge base As the reference knowledge
base, we chose BabelNet2 (Navigli and Ponzetto,
2010), a large-scale multilingual semantic ontol-
ogy created by automatically merging WordNet
with other collaboratively-constructed resources
such as Wikipedia and OmegaWiki. BabelNet
data offers two necessary features for generat-
ing the games’ datasets. First, by connecting
WordNet synsets to Wikipedia pages, most synsets
are associated with a set of pictures; while often
noisy, these pictures sometimes illustrate the tar-
get concept and are an ideal case for validation.
Second, BabelNet contains the semantic relations
from both WordNet and hyperlinks in Wikipedia;
these relations are again an ideal case of valida-
tion, as not all hyperlinks connect semantically-
related pages in Wikipedia. Last, we stress that
while our games use BabelNet data, they could
easily validate or extend other knowledge bases
such as YAGO (Suchanek et al., 2007) as well.
</bodyText>
<footnote confidence="0.999960333333333">
1This design is in contrast to two-player games where mu-
tual agreement reinforces correct behavior.
2http://babelnet.org
</footnote>
<bodyText confidence="0.999845274509804">
Data We created a common set of concepts, C,
used in both games, containing sixty synsets se-
lected from all BabelNet synsets with at least fifty
associated images. Using the same set of synsets,
separate datasets were created for the two valida-
tion tasks. In each dataset, a concept c ∈ C is
associated with two sets: a set Vc containing items
to validate, and a set Nc with examples of true neg-
ative items (i.e., items where the relation to c does
not hold). We use the notation V and N when re-
ferring to the to-validate and true negative sets for
all concepts in a dataset, respectively.
For the concept-concept dataset, Vc is the union
of VcB, which contains the lemmas of all synsets
incident to c in BabelNet, and Vcn, which con-
tains novel lemmas derived from statistical asso-
ciations. Specifically, novel lemmas were selected
by computing the x2 statistic for co-occurrences
between the lemmas of c and all other part of
speech-tagged lemmas in Wikipedia. The 30 lem-
mas with the highest x2 are included in Vc. To
enable concept-to-concept annotations, we disam-
biguate novel lemmas using a simple heuristic
based on link co-occurrence count (Navigli and
Ponzetto, 2012). Each set Vc contains 77.6 lem-
mas on average.
For the concept-image data, Vc is the union of
Vc B, which contains all images associated with c in
BabelNet, and Vcn, which contains web-gathered
images using a lemma of c as the query. Web-
gathered images were retrieved using Yahoo! Boss
image search and the first result set (35 images)
was added to Vc. Each set Vc contains 77.0 images
on average.
For both datasets, each negative set Nc is con-
structed as ∪c/∈C\{c}Vc/B, i.e., from the items re-
lated in BabelNet to all other concepts in C. By
constructing Nc directly from the knowledge base,
play actions may be validated based on recogni-
tion of true negatives, removing the heavy burden
for ever manually creating a gold standard test set.
Annotation Aggregation In each game, an item
is annotated when players make a binary choice as
to whether the item’s relation is true (e.g., whether
an image is related to a concept). To produce a
final annotation, a rating of p − n is computed,
where p and n denote the number of times players
have marked the item’s relation as true or false, re-
spectively. Items with a positive rating after aggre-
gating are marked as true examples of the relation
and false otherwise.
</bodyText>
<page confidence="0.961711">
1296
</page>
<figure confidence="0.9896">
(a) The passphrase shown at the start (b) Main gameplay screen with a close-up of a player’s interaction with two humans
</figure>
<figureCaption confidence="0.999974">
Figure 1: Screenshots of the key elements of Infection
</figureCaption>
<sectionHeader confidence="0.828756" genericHeader="method">
4 Game 1: Infection
</sectionHeader>
<bodyText confidence="0.999894453333333">
The first game, Infection, validates the concept-
concept relation dataset.
Design Infection is designed as a top-down
shooter game in the style of Commando. Infection
features the classic game premise that a virus has
partially infected humanity, turning people into
zombies. The player’s responsibility is to stop
zombies from reaching the city and rescue humans
that are fleeing to the city. Both zombies and hu-
mans appear at the top of the screen, advance to
the bottom and, upon reaching it, enter the city.
In the game, some humans are infected, but
have not yet become zombies; these infected hu-
mans must be stopped before reaching the city.
Because infected and uninfected humans look
identical, the player uses a passphrase call-and-
response mechanism to distinguish between the
two. Each level features a randomly-chosen
passphrase that the player’s character shouts. Un-
infected humans are expected to respond with a
word or phrase related to the passphrase; in con-
trast, infected humans have become confused due
to the infection and will say something completely
unrelated in an attempt to sneak past. When an in-
fected human reaches the city, the city’s total in-
fection level increases; should the infection level
increase beyond a certain threshold, the player
fails the stage and must replay it to advance the
game. Furthermore, if any time after ten humans
have been seen, the player has killed more than
80% of the uninfected humans, the player’s gun is
taken by the survivors and she loses the stage.
Figure 1a shows instructions for the passphrase
“medicine.” In the corresponding gameplay,
shown in the close up of Figure 1b, a hu-
man shouts a valid response, “radiology” for the
level’s passphrase, while the nearby infected hu-
man shouts an incorrect response “longitude.”
Gameplay is divided into eight stages, each with
increasing difficulty. Each stage has a goal of
saving a specific number of uninfected humans.
Infection incorporates common game mechanics,
such as unlockable weapons, power-ups that re-
store health, and achievements. Scoring is based
on both the number of zombies killed and the per-
centage of uninfected humans saved, motivating
players to kill infected humans in order to increase
their score. Importantly, Infection also includes a
leaderboard where players compete for top posi-
tions based on their total scores.
Annotation Each human is assigned a response
selected uniformly from V or N. Humans with
responses from N are treated as infected. Players
annotate by selecting which humans are infected:
Allowing a human with a response from V to enter
the city is treated as a positive annotation; killing
that human is treated as a negative annotation.
The design of Infection enables annotating mul-
tiple types of conceptual relations such as syn-
onymy or antonymy by changing only the descrip-
tion of the passphrase and how uninfected humans
are expected to respond.
Quality Enforcement Mechanisms Infection in-
cludes two game mechanics to limit adversarial
players from creating many low quality annota-
tions. Specifically, the game prevents players
from both (1) allowing all humans to live, via the
city infection level and (2) killing all humans, via
survivors taking the player’s gun; these actions
would both generate many false positives and false
negatives, respectively. These mechanics ensure
the game naturally produces better quality anno-
tations; in contrast, common crowdsourcing plat-
forms do not support analogous mechanics for en-
forcing this type of correctness at annotation time.
</bodyText>
<sectionHeader confidence="0.913102" genericHeader="method">
5 Game 2: The Knowledge Towers
</sectionHeader>
<bodyText confidence="0.99954575">
The second game, The Knowledge Towers (TKT),
validates the concept-image dataset.
Design TKT is designed as a single-player role
playing game (RPG) where the player explores a
</bodyText>
<page confidence="0.962974">
1297
</page>
<figure confidence="0.989537">
(a) An example tower’s concept (b) Image selection screen (c) Gameplay
</figure>
<figureCaption confidence="0.99997">
Figure 2: Screenshots of the key elements of The Knowledge Towers.
</figureCaption>
<bodyText confidence="0.999853338983051">
series of towers to unlock long-forgotten knowl-
edge. At the start of each tower, a target con-
cept is shown, e.g., the tower of “tango,” along
with a description of the concept (Figure 2a). The
player must then recover the knowledge of the tar-
get concept by acquiring pictures of it. Pictures are
obtained through defeating monsters and opening
treasure chests, such as those shown in Figure 2c.
However, players must distinguish pictures of the
tower’s concept from unrelated pictures. When an
image is picked up, the player may keep or discard
it, as shown in Figure 2b. A player’s inventory is
limited to eight pictures to encourage them to se-
lect the most relevant pictures only.
Once the player has collected enough pictures,
the door to the boss room is unlocked and the
player may enter to defeat the boss and complete
the tower. Pictures may also be deposited in spe-
cial reward chests that grant experience bonuses if
the deposited pictures are from V . Gathering un-
related pictures has adverse effects on the player.
If the player finishes the level with a majority of
unrelated pictures, the player’s journey is unsuc-
cessful and she must replay the tower.
TKT includes RPG game elements commonly
found in game series such as Diablo and the Leg-
end of Zelda: players begin with a specific charac-
ter class that has class-specific skills, such as War-
rior or Thief, but will unlock the ability to play as
other classes by successfully completing the tow-
ers. Last, TKT includes a leaderboard where play-
ers can compete for positions; a player’s score is
based on increasing her character’s abilities and
her accuracy at discarding images from N.
Annotation Players annotate by deciding which
images to keep in their inventory. Images receive
positive rating annotations from: (1) depositing
the image in a reward chest, and (2) ending the
level with the image still in the inventory. Con-
versely, images receive a negative rating when a
player (1) views the image but intentionally avoids
picking it up or (2) drops the image from her in-
ventory.
TKT is designed to assist in the validation and
extension of automatically-created image libraries
that link to semantic concepts, such as ImageNet
(Deng et al., 2009) and that of Torralba et al.
(2008). However, its general design allows for
other types of annotations, such as image labeling,
by changing the tower’s instructions and pictures.
Quality Enforcement Mechanisms Similar to
Infection, TKT includes analogous mechanisms
for limiting adversarial player annotations. Play-
ers who collect no images are prevented from en-
tering the boss room, limiting their ability to gen-
erate false negative annotations. Similarly, players
who collect all images are likely to have half of
their images from N and therefore fail the tower’s
quality-check after defeating the boss.
</bodyText>
<sectionHeader confidence="0.999717" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999968">
Two experiments were performed with Infection
and TKT: (1) an evaluation of players’ ability to
play accurately and to validate semantic relations
and image associations and (2) a comprehensive
cost comparison. Each experiment compared (a)
free and financially-incentivized versions of each
game, (b) crowdsourcing, and (c) a non-video
game with a purpose.
</bodyText>
<subsectionHeader confidence="0.983221">
6.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.9998872">
Gold Standard Data To compare the quality of
annotation from games and crowdsourcing, a gold
standard annotation was produced for a 10% sam-
ple of each dataset (cf. Section 3.2). Two annota-
tors independently rated the items and, in cases of
disagreement, a third expert annotator adjudicated.
Unlike in the game setting, annotators were free to
consult additional resources such as Wikipedia.
To measure inter-annotator agreement (IAA) on
the gold standard annotations, we calculated Krip-
</bodyText>
<page confidence="0.963652">
1298
</page>
<bodyText confidence="0.998665777777778">
pendorff’s α (Krippendorff, 2004; Artstein and
Poesio, 2008); α ranges between [-1,1] where 1
indicates complete agreement, -1 indicates sys-
tematic disagreement, and values near 0 indicate
agreement at chance levels. Gold standard an-
notators had high agreement, 0.774, for concept-
concept relations. However, image-concept agree-
ment was only moderate, 0.549. A further analy-
sis revealed differences in the annotators’ thresh-
olds for determining association, with one anno-
tator permitting more abstract relations. However,
the adjudication process resolved these disputes,
resulting in substantial agreement by all annota-
tors on the final gold annotations.
Incentives At the start of each game, players were
shown brief descriptions of the game and a de-
scription of a contest where the top-ranked players
would win either (1) monetary prizes in the form
of gift cards, or (2) a mention and thanks in this
paper. We refer to these as the paid and free ver-
sions of the game, respectively. In the paid setting,
the five top-ranking players were offered gift cards
valued at 25, 15, 15, 10, and 10 USD, starting from
first place (a total of 75 USD per game). To in-
crease competition among players and to perform
a fairer time comparison with crowdsourcing, the
contest period was limited to two weeks.
</bodyText>
<subsectionHeader confidence="0.934474">
6.2 Comparison Methods
</subsectionHeader>
<bodyText confidence="0.99930937254902">
To compare with the video games, items were
annotated using two additional methods: crowd-
sourcing and a non-video game with a purpose.
Crowdsourcing Setup Crowdsourcing was per-
formed using the CrowdFlower platform. Anno-
tation tasks were designed to closely match each
game’s annotation process. A task begins with a
description of a target synset and its textual def-
inition; following, ten annotation questions are
shown. Separate tasks were used for validat-
ing concept-concept and concept-image relations.
Each tasks’ questions were shown as a binary
choice of whether the item is related to the task’s
concept. Workers were paid 0.05 USD per task.
Each question was answered by three workers.
Following common practices for guarding
against adversarial workers (Mason and Suri,
2012), the tasks for concept c include quality
check questions using items from N,. Workers
who rate too many relations from N, as valid are
removed by CrowdFlower and prevented from par-
ticipating further. One of the ten questions in a
task used an item from N,, resulting in a task mix-
ture of 90% annotation questions and 10% quality-
check questions. However, we note that both of
our video games use data that is 50% annotation,
50% quality-check. While the crowdsourcing task
could be adjusted to use an increased number of
quality-check options, such a design is uncommon
and artificially inflates the cost of the crowdsourc-
ing comparison beyond what would be expected.
Therefore, although the crowdsourcing and game-
based annotation tasks differ slightly, we chose to
use the common setup in order to create a fair cost-
comparison between the two.
Non-video Game with a Purpose To measure
the impact of the video game itself on the anno-
tation process, we developed a non-video game
with a purpose, referred to as SuchGame. Players
perform a single action in SuchGame: after be-
ing shown a concept c and its textual definition, a
player answers whether an item is related to the
concept. Items are drawn equally from V, and N,,
with players scoring a point each time they select
that an item from N is not related. A round of
gameplay contains ten questions. After the round
ends, players see their score for that round and the
current leaderboard. Two versions of SuchGame
were released, one for each dataset. SuchGame
was promoted with same free recognition incen-
tive as Infection and TKT.
</bodyText>
<subsectionHeader confidence="0.997878">
6.3 Game Release
</subsectionHeader>
<bodyText confidence="0.999947">
Both video games were released to multiple on-
line forums, social media sites, and Facebook
groups. SuchGame was released to separate Face-
book groups promoting free webgames and groups
for indie games. For each release, we estimated
an upper-bound of the audience sizes using avail-
able statistics such as Facebook group sites, web-
site analytics, and view counts. The free and paid
versions had sizes of 21,546 and 14,842 people,
respectively; SuchGame had an upper bound of
569,131 people. Notices promoting the game were
separated so that audiences saw promotions for
one of either the paid or free incentive version.
Games were also released in such a way as to pre-
serve the anonymity of the study, which limited
our ability to advertise to public venues where the
anonymity might be compromised.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="evaluation">
7 Results and Discussion
</sectionHeader>
<subsectionHeader confidence="0.990307">
7.1 Gameplay Analysis
</subsectionHeader>
<bodyText confidence="0.999854">
In this section we analyze the games in terms of
participation and player’s ability to correctly play.
Players completed over 1388 games during the
</bodyText>
<page confidence="0.991657">
1299
</page>
<figure confidence="0.9954421">
450
400
350
300
250
200
150
100
50
0
Correct
Incorrect
Correct
Incorrect
700
600
500
400
300
200
100
0
Correct
Incorrect
Correct
Incorrect
Number of Items
Number of Items
400
Number of Items
350
300
250
200
150
100
50
0
1600
Number of Items
1400
1200
1000
800
600
400
200
0
Player Player Player Player
(a) Infection (free) (b) Infection (paid) (c) TKT (free) (d) TKT (paid)
</figure>
<figureCaption confidence="0.99997">
Figure 3: Accuracy of the top-40 players in rejecting true negative items during gameplay.
</figureCaption>
<table confidence="0.999663875">
# Players # Anno. N-Acc. Krip.’s α G.S. Agreement Cost per Ann.
True Pos. True Neg. All
TKT free 100 3005 97.0 0.333 82.5 82.5 82.5 $0.000
TKT paid 97 3318 95.4 0.304 69.0 92.1 74.0 $0.023
Crowdflower 290 13854 - 0.478 59.5 93.7 66.2 $0.008
Infection free 89 3150 71.0 0.445 67.8 68.4 68.1 $0.000
Infection paid 163 3355 65.9 0.330 69.1 54.8 61.1 $0.022
Crowdflower 1097 13764 - 0.167 16.9 96.4 59.6 $0.008
</table>
<tableCaption confidence="0.999864">
Table 1: Annotation statistics from all sources. N-Accuracy denotes accuracy at rejecting items from N;
</tableCaption>
<bodyText confidence="0.989087697674419">
G.S. Agreement denotes percentage agreement of the aggregated annotations with the gold standard.
study period. The paid and free versions of TKT
had similar numbers of players, while the paid ver-
sion of Infection attracted nearly twice the play-
ers compared to the free version, shown in Ta-
ble 1, Column 1. However, both versions created
approximately the same number of annotations,
shown in Column 2. Surprisingly, SuchGame re-
ceived little attention, with only a few players
completing a full round of game play. We believe
this emphasizes the strength of video game-based
annotation; adding incentives and game-like fea-
tures to an annotation task will not necessarily in-
crease its appeal. Given SuchGame’s minimal in-
terest, we omit it from further analysis.
Second, the type of incentive did not change the
percentage of items from N that players correctly
reject, shown for all players as N-accuracy in Ta-
ble 1 Column 3 and per-player in Figure 3. How-
ever, players were much more accurate at reject-
ing items from N in TKT than in Infection. We
attribute this difference to the nature of the items
and the format of the games. The images used
by TKT provide concrete examples of a concept,
which can be easily compared with the game’s cur-
rent concept; in addition, TKT allows players to
inspect items as long as a player prefers. In con-
trast, concept-concept associations require more
background knowledge to determine if a relation
exists; furthermore, Infection gives players limited
time to decide (due to board length) and also con-
tains cognitive distractors (zombies). Neverthe-
less, player accuracy remains high for both games
(Table 1, Col. 3) indicating the games represent a
viable medium for making annotation decisions.
Last, the distribution of player annotation fre-
quencies (Figure 3) suggests that the leaderboard
and incentives motivated players. Especially in the
paid condition, a clear group appears in the top
five positions, which were advertised as receiving
prizes. The close proximity of players in the paid
positions is a result of continued competition as
players jostled for higher-paying prizes.
</bodyText>
<subsectionHeader confidence="0.999236">
7.2 Annotation Quality
</subsectionHeader>
<bodyText confidence="0.9999674375">
This section assesses the annotation quality of
both games and of CrowdFlower in terms of (1)
the IAA of the participants, measured using Krip-
pendorff’s α, and (2) the percentage agreement of
the resulting annotations with the gold standard.
Players in both free and paid games had similar
IAA, though the free version is consistently higher
(Table 1, Col. 4).3 For images, crowdsourcing
workers have a higher IAA than game players;
however, this increased agreement is due to ad-
versarial workers consistently selecting the same,
incorrect answer. In contrast, both video games
contain mechanisms for limiting such behavior.
The strength of both crowdsourcing and games
with a purpose comes from aggregating multiple
annotations of a single item; i.e., while IAA may
</bodyText>
<footnote confidence="0.998049">
3In conversations with players after the contest ended,
several mentioned that being aware their play was contribut-
ing to research motivated them to play more accurately.
</footnote>
<page confidence="0.992812">
1300
</page>
<tableCaption confidence="0.994391">
Table 2: Examples of the most-selected words and images from the free version of both games. Bolded
words and images with a dashed border denote items not in BabelNet. Only the items marked with a ‡
were rated as valid in the aggregated CrowdFlower annotations.
</tableCaption>
<table confidence="0.790721962962963">
Lemma Abbreviated Definition Most-selected Items
voicing, triad, tonality,‡ strum, note, harmony
chord A combination of three
or more notes
‡
color An attribute from re-
flected or emitted light
orange, brown,‡ video, sadness, RGB, pigment
‡ ‡ ‡ ‡
The state of combustion
in which inflammable
material burns
sprinkler, machine gun, chemical reduction, volcano, organic chemistry
‡ ‡ ‡
fire
The smallest possible
atom particle of a chemical
element
spectrum, nonparticulate radiation, molecule, hydrogen, electron
‡ ‡ ‡
The expression of
man’s belief in and
reverence for a super-
human power
polytheistic,‡ monotheistic, Jainism, Christianity,‡ Freedom of religion
‡ ‡ ‡
religion
</table>
<bodyText confidence="0.999921">
be low, the majority annotation of an item may be
correct. Therefore, in Table 1, we calculate the
percentage agreement of the aggregated annota-
tions with the gold standard annotations for ap-
proving valid relations (true positives; Col. 5), re-
jecting invalid relations (true negatives; Col. 6),
and for both combined (Col. 7). On average, both
video games in all settings produce more accurate
annotations than crowdsourcing. Indeed, despite
having lower IAA for images, the free version of
TKT provides an absolute 16.3% improvement in
gold standard agreement over crowdsourcing.
Examining the difference in annotation quality
for true positives and negatives, we see a strong
bias with crowdsourcing towards rejecting all
items. This bias leads to annotations with few false
positives, but as Column 5 shows, crowdflower
workers consistently performed much worse than
game players at identifying valid relations, pro-
ducing many false negative annotations. Indeed,
for concept-concept relations, workers identified
only 16.9% of the valid relations.
In contrast to crowdsourcing, both games were
effective at identifying valid relations. Table
2 shows examples of the most frequently cho-
sen items from V for the free versions of both
games. For both games, players were equally
likely to select novel items, suggesting the games
can serve a useful purpose of adding these miss-
ing relations in automatically constructed knowl-
edge bases. Highlighting one example, the five
most selected concept-concept relations for chord
were all novel; BabelNet included many relations
to highly-specific concepts (e.g., “Circle of fifths”)
but did not include relations to more commonly-
associated concepts, like note and harmony.
</bodyText>
<subsectionHeader confidence="0.999796">
7.3 Cost Analysis
</subsectionHeader>
<bodyText confidence="0.999987111111111">
This section provides a cost-comparison between
the video games and crowdsourcing. The free
versions of both games proved highly success-
ful, yielding high-quality annotations at no direct
cost. Both free and paid conditions produced sim-
ilar volumes of annotations, suggesting that play-
ers do not need financial incentives provided that
the games are fun to play. It could be argued that
the recognition incentive was motivating players
in the free condition and thus some incentive was
required. However, player behavior indicates oth-
erwise: After the contest period ended, no players
in the free setting registered for being acknowl-
edged by name, which strongly suggests the in-
centive was not contributing to their motivation for
playing. Furthermore, a minority of players con-
tinued to play even after the contest period ended,
suggesting that enjoyment was a driving factor.
</bodyText>
<page confidence="0.980128">
1301
</page>
<bodyText confidence="0.9999809">
Last, while crowdsourcing has seen different qual-
ity and volume from workers in paid and unpaid
settings (Rogstadius et al., 2011), in contrast, our
games produced approximately-equivalent results
from players in both settings.
Crowdsourcing was slightly more cost-effective
than both games in the paid condition, as shown
in Table 1, Column 8. However, three additional
factors need to be considered. First, both games
intentionally uniformly sample between V and N
to increase player engagement,4 which generates a
larger number of annotations for items in N than
are produced by crowdsourcing. When annota-
tions on items in N are included for both games
and crowdsourcing, the costs per annotation drop
to comparable levels: $0.007 for CrowdFlower
tasks, $0.008 for TKT, and $0.011 for Infection.
Second, for both annotation tasks, crowdsourc-
ing produced lower quality annotations, especially
for valid relations. Based on agreement with the
gold standard (Table 1, Col. 5), the estimated cost
for crowdsourcing a correct true positive annota-
tion increases to $0.014 for a concept-image and
a $0.048 for concepts-concept annotation. In con-
trast, the cost when using video games increases
only to $0.033 for concept-image and $0.031 for
concept-concept. These cost increases suggest
that crowdsourcing is not always cheaper with re-
spect to quality.
Third, we note that both video games in the paid
setting incur a fixed cost (for the prizes) and there-
fore additional games played can only further de-
crease the cost per annotation. Indeed, the present
study divided the audience pool into two separate
groups which effectively halved the potential num-
ber of annotations per game. Assuming combining
the audiences would produce the same number of
annotations, both our games’ costs per annotation
drop to $0.012.
Last, video games can potentially come with
indirect costs due to software development and
maintenance. Indeed, Poesio et al. (2013) report
spending 60,000£ in developing their Phrase De-
tectives game with a purpose over a two-year pe-
riod. In contrast, both games here were developed
as apart of student projects using open source soft-
ware and assets and thus incurred no cost; fur-
thermore, games were created in a few months,
rather than years. Given that few online games
attain significant sustained interest, we argue that
</bodyText>
<footnote confidence="0.821072666666667">
4Earlier versions that used mostly items from V proved
less engaging due to players frequently performing the same
action, e.g., saving most humans or collecting most pictures.
</footnote>
<bodyText confidence="0.9997914">
our lightweight model is preferable for producing
video games with a purpose. While using students
is not always possible, the development process
is fast enough to sufficiently reduce costs below
those reported for Phrase Detectives.
</bodyText>
<sectionHeader confidence="0.994294" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999896">
Two video games have been presented for vali-
dating and extending knowledge bases. The first
game, Infection, validates concept-concept rela-
tions, and the second, The Knowledge Towers,
validates image-concept relations. In experiments
involving online players, we demonstrate three
contributions. First, games were released in two
conditions whereby players either saw financial
incentives for playing or a personal satisfaction
incentive where they were thanked by us. We
demonstrated that both conditions produced nearly
identical numbers of annotations and, moreover,
that players were disinterested in the satisfaction
incentive, suggesting they played out of interest
in the game itself. Furthermore, we demonstrated
the effectiveness of a novel design for games with
a purpose which does not require two players for
validation and instead reinforces behavior only
using true negative items that required no man-
ual annotation. Second, in a comparison with
crowdsourcing, we demonstrate that video game-
based annotations consistently generated higher-
quality annotations. Last, we demonstrate that
video game-based annotation can be more cost-
effective than crowdsourcing or annotation tasks
with game-like features: The significant number
of annotations generated by the satisfaction incen-
tive condition shows that a fun game can generate
high-quality annotations at virtually no cost. All
annotated resources, demos of the games, and a
live version of the top-ranking items for each con-
cept are currently available online.5
In the future we will apply our video games
to the validation of more data, such as the new
Wikipedia bitaxonomy (Flati et al., 2014).
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998929285714286">
The authors gratefully acknowl-
edge the support of the ERC Start-
ing Grant MultiJEDI No. 259234.
We thank Francesco Cecconi for his support
with the websites and the many video game play-
ers without whose enjoyment this work would not
be possible.
</bodyText>
<footnote confidence="0.978751">
5http://lcl.uniroma1.it/games/
</footnote>
<page confidence="0.996524">
1302
</page>
<sectionHeader confidence="0.989997" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997395825688074">
Guillaume Artignan, Mountaz Hasco¨et, and Math-
ieu Lafourcade. 2009. Multiscale visual analysis
of lexical networks. In Proceedings of the Inter-
national Conference on Information Visualisation,
pages 685–690.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.
Chris Biemann and Valerie Nygaard. 2010. Crowd-
sourcing wordnet. In Proceedings of the 5th Global
WordNet conference.
Alexander Budanitsky and Graeme Hirst. 2006.
Evaluating WordNet-based measures of Lexical Se-
mantic Relatedness. Computational Linguistics,
32(1):13–47.
Timothy Chklovski and Yolanda Gil. 2005. Improv-
ing the design of intelligent acquisition interfaces for
collecting world knowledge from web contributors.
In Proceedings of the International Conference on
Knowledge Capture, pages 35–42. ACM.
Tim Chklovski and Rada Mihalcea. 2002. Building a
Sense Tagged Corpus with Open Mind Word Expert.
In Proceedings ofACL 2002 Workshop on WSD: Re-
cent Successes and Future Directions, Philadelphia,
PA, USA.
Seth Cooper, Firas Khatib, Adrien Treuille, Janos
Barbero, Jeehyung Lee, Michael Beenen, Andrew
Leaver-Fay, David Baker, Zoran Popovi´c, and Foldit
players. 2010. Predicting protein structures with a
multiplayer online game. Nature, 466(7307):756–
760.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai
Li, and Li Fei-Fei. 2009. ImageNet: A large-scale
hierarchical image database. In Proceedings of the
Conference on Computer Vision and Pattern Recog-
nition (CVPR), pages 248–255.
Soojeong Eom, Markus Dickinson, and Graham Katz.
2012. Using semi-experts to derive judgments on
word sense alignment: a pilot study. In Proceed-
ings of the Conference on Language Resources and
Evaluation (LREC), pages 605–611.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Tiziano Flati, Daniele Vannella, Tommaso Pasini, and
Roberto Navigli. 2014. Two is bigger (and better)
than one: the Wikipedia Bitaxonomy Project. In
Proceedings of the 52nd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), Bal-
timore, Maryland.
Amac¸ Herda˘gdelen and Marco Baroni. 2012. Boot-
strapping a game with a purpose for common sense
collection. ACM Transactions on Intelligent Sys-
tems and Technology, 3(4):1–24.
Barbora Hladk´a, Jiˇr´ı M´ırovsk`y, and Pavel Schlesinger.
2009. Play the language: Play coreference. In
Proceedings of the Joint Conference of the Asso-
ciation for Computational Linguistics and Inter-
national Joint Conference of the Asian Federation
of Natural Language Processing (ACL-IJCNLP),
pages 209–212. Association for Computational Lin-
guistics.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-
structured content and Artificial Intelligence: The
story so far. Artificial Intelligence, 194:2–27.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In Proceedings of the
Conference of the North American Chapter of the
Association of Computational Linguistics (NAACL),
pages 455–462.
Klaus Krippendorff. 2004. Content Analysis: An
Introduction to Its Methodology. Sage, Thousand
Oaks, CA, second edition.
Yen-ling Kuo, Jong-Chuan Lee, Kai-yang Chiang, Rex
Wang, Edward Shen, Cheng-wei Chan, and Jane
Yung-jen Hsu. 2009. Community-based game de-
sign: experiments on social games for common-
sense data collection. In Proceedings of the ACM
SIGKDD Workshop on Human Computation, pages
15–22.
Mathieu Lafourcade and Alain Joubert. 2010. Com-
puting trees of named word usages from a crowd-
sourced lexical network. In Proceedings of the In-
ternational Multiconference on Computer Science
and Information Technology (IMCSIT), pages 439–
446, Wisla, Poland.
Winter Mason and Siddharth Suri. 2012. Conducting
behavioral research on amazons mechanical turk.
Behavior Research Methods, 44(1):1–23.
Olena Medelyan, David Milne, Catherine Legg, and
Ian H. Witten. 2009. Mining meaning from
Wikipedia. International Journal of Human-
Computer Studies, 67(9):716–754.
Andrea Moro, Alessandro Raganato, and Roberto Nav-
igli. 2014. Entity Linking meets Word Sense Dis-
ambiguation: A Unified Approach. Transactions of
the Association for Computational Linguistics.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
BabelNet: Building a very large multilingual se-
mantic network. In Proceedings of the 48th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), Uppsala, Sweden, pages 216–225.
Roberto Navigli and Simone Paolo Ponzetto. 2012.
Joining forces pays off: Multilingual Joint Word
Sense Disambiguation. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 1399–
1410, Jeju, Korea.
</reference>
<page confidence="0.554671">
1303
</page>
<reference confidence="0.99983986">
Roberto Navigli. 2005. Semi-automatic extension of
large-scale linguistic knowledge bases. In Proceed-
ings of the 18th Internationa Florida AI Research
Symposium Conference, Clearwater Beach, Florida,
15–17 May 2005, pages 548–553.
Elisabeth Niemann and Iryna Gurevych. 2011. The
people’s web meets linguistic knowledge: Auto-
matic sense alignment of Wikipedia and WordNet.
In Proceedings of the International Conference on
Computational Semantics (IWCS), pages 205–214.
Mohammad Taher Pilehvar, David Jurgens, and
Roberto Navigli. 2013. Align, Disambiguate and
Walk: a Unified Approach for Measuring Semantic
Similarity. In Proceedings of the 51st Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 1341–1351, Sofia, Bulgaria.
Massimo Poesio, Jon Chamberlain, Udo Kruschwitz,
Livio Robaldo, and Luca Ducceschi. 2013. Phrase
detectives: Utilizing collective intelligence for
internet-scale language resource creation. ACM
Transactions on Interactive Intelligent Systems,
3(1):3:1–3:44, April.
Jakob Rogstadius, Vassilis Kostakos, Aniket Kittur,
Boris Smus, Jim Laredo, and Maja Vukovic. 2011.
An assessment of intrinsic and extrinsic motivation
on task performance in crowdsourcing markets. In
Proceedings of the International AAAI Conference
on Weblogs and Social Media (ICWSM).
Jacek Rzeniewicz and Julian Szyma´nski. 2013.
Bringing Common Sense to WordNet with a Word
Game. In Computational Collective Intelligence.
Technologies and Applications, volume 8083 of Lec-
ture Notes in Computer Science, pages 296–305.
Springer.
Cristina Sarasua, Elena Simperl, and Natalya F Noy.
2012. CrowdMap: Crowdsourcing ontology align-
ment with microtasks. In Proceedings of the Inter-
national Semantic Web Conference (ISWC), pages
525–541.
Nitin Seemakurty, Jonathan Chu, Luis Von Ahn, and
Anthony Tomasic. 2010. Word sense disambigua-
tion via human computation. In Proceedings of the
ACM SIGKDD Workshop on Human Computation,
pages 60–63. ACM.
Jakub Simko, Michal Tvarozek, and Maria Bielikova.
2011. Little search game: term network acquisition
via a human computation game. In Proceedings of
the ACM conference on Hypertext and Hypermedia,
pages 57–62.
Katharina Siorpaes and Martin Hepp. 2008a. Games
with a purpose for the semantic web. IEEE Intelli-
gent Systems, 23(3):50–60.
Katharina Siorpaes and Martin Hepp. 2008b. On-
togame: Weaving the semantic web by online
games. In Sean Bechhofer, Manfred Hauswirth, Jrg
Hoffmann, and Manolis Koubarakis, editors, The
Semantic Web: Research and Applications, volume
5021 of Lecture Notes in Computer Science, pages
751–766. Springer Berlin Heidelberg.
Rion Snow, Dan Jurafsky, and Andrew Ng. 2006.
Semantic taxonomy induction from heterogeneous
evidence. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Com-
putational Linguistics (COLING-ACL), Sydney, Aus-
tralia, pages 801–808.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. YAGO: A core of semantic knowl-
edge. unifying WordNet and Wikipedia. In Proceed-
ings of the 16th World Wide Web Conference, Banff,
Canada, 8–12 May 2007, pages 697–706.
Stefan Thaler, Elena Paslaru Bontas Simperl, and
Katharina Siorpaes. 2011. SpotTheLink: A Game
for Ontology Alignment. In Proceedings of the
6th Conference on Professional Knowledge Man-
agement: From Knowledge to Action, pages 246–
253.
Antonio Torralba, Robert Fergus, and William T Free-
man. 2008. 80 million tiny images: A large data
set for nonparametric object and scene recognition.
IEEE Transactions on Pattern Analysis and Machine
Intelligence, 30(11):1958–1970.
Noortje J. Venhuizen, Valerio Basile, Kilian Evang, and
Johan Bos. 2013. Gamification for word sense la-
beling. In Proceedings of the International Confer-
ence on Computational Semantics (IWCS).
David Vickrey, Aaron Bronzan, William Choi, Aman
Kumar, Jason Turner-Maier, Arthur Wang, and
Daphne Koller. 2008. Online word games for se-
mantic data collection. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 533–542.
Luis von Ahn and Laura Dabbish. 2004. Labeling im-
ages with a computer game. In Proceedings of the
Conference on Human Factors in Computing Sys-
tems (CHI), pages 319–326.
Luis von Ahn, Mihir Kedia, and Manuel Blum. 2006.
Verbosity: a game for collecting common-sense
facts. In Proceedings of the Conference on Human
Factors in Computing Systems (CHI), pages 75–78.
</reference>
<page confidence="0.996479">
1304
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.762963">
<title confidence="0.9585015">Validating and Extending Semantic Knowledge using Video Games with a Purpose</title>
<author confidence="0.998774">David Jurgens Vannella</author>
<author confidence="0.998774">Daniele Scarfini</author>
<author confidence="0.998774">Domenico Toscani</author>
<affiliation confidence="0.9944565">Department of Computer Sapienza University of</affiliation>
<email confidence="0.946428">surname@di.uniroma1.it</email>
<abstract confidence="0.994093473684211">Large-scale knowledge bases are important assets in NLP. Frequently, such resources are constructed through automatic mergers of complementary resources, such as WordNet and Wikipedia. However, manually validating these resources is prohibitively expensive, even when using methods such as crowdsourcing. We propose a cost-effective method of validating and extending knowledge bases using games a purpose. Two video games were created to validate conceptconcept and concept-image relations. In experiments comparing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Guillaume Artignan</author>
<author>Mountaz Hasco¨et</author>
<author>Mathieu Lafourcade</author>
</authors>
<title>Multiscale visual analysis of lexical networks.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Information Visualisation,</booktitle>
<pages>685--690</pages>
<marker>Artignan, Hasco¨et, Lafourcade, 2009</marker>
<rawString>Guillaume Artignan, Mountaz Hasco¨et, and Mathieu Lafourcade. 2009. Multiscale visual analysis of lexical networks. In Proceedings of the International Conference on Information Visualisation, pages 685–690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="21503" citStr="Artstein and Poesio, 2008" startWordPosition="3411" endWordPosition="3414">sourcing, and (c) a non-video game with a purpose. 6.1 Experimental Setup Gold Standard Data To compare the quality of annotation from games and crowdsourcing, a gold standard annotation was produced for a 10% sample of each dataset (cf. Section 3.2). Two annotators independently rated the items and, in cases of disagreement, a third expert annotator adjudicated. Unlike in the game setting, annotators were free to consult additional resources such as Wikipedia. To measure inter-annotator agreement (IAA) on the gold standard annotations, we calculated Krip1298 pendorff’s α (Krippendorff, 2004; Artstein and Poesio, 2008); α ranges between [-1,1] where 1 indicates complete agreement, -1 indicates systematic disagreement, and values near 0 indicate agreement at chance levels. Gold standard annotators had high agreement, 0.774, for conceptconcept relations. However, image-concept agreement was only moderate, 0.549. A further analysis revealed differences in the annotators’ thresholds for determining association, with one annotator permitting more abstract relations. However, the adjudication process resolved these disputes, resulting in substantial agreement by all annotators on the final gold annotations. Incen</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Valerie Nygaard</author>
</authors>
<title>Crowdsourcing wordnet.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th Global WordNet conference.</booktitle>
<contexts>
<context position="2662" citStr="Biemann and Nygaard, 2010" startWordPosition="372" endWordPosition="375"> semi-structured resources has enabled the creation of new semantic knowledge bases (Medelyan et al., 2009; Hovy et al., 2013) through automatically merging WordNet and Wikipedia (Suchanek et al., 2007; Navigli and Ponzetto, 2010; Niemann and Gurevych, 2011). While these automatic approaches offer the scale needed for opendomain applications, the automatic processes often introduce errors, which can prove detrimental to downstream applications. To overcome issues from fully-automatic construction methods, several works have proposed validating or extending knowledge bases using crowdsourcing (Biemann and Nygaard, 2010; Eom et al., 2012; Sarasua et al., 2012). However, these methods, too, are limited by the resources required for acquiring large numbers of responses. In this paper, we propose validating and extending semantic knowledge bases using video games with a purpose. Here, the annotation tasks are transformed into elements of a video game where players accomplish their jobs by virtue of playing the game, rather than by performing a more traditional annotation task. While prior efforts in NLP have incorporated games for performing annotation and validation (Siorpaes and Hepp, 2008b; Herda˘gdelen and </context>
</contexts>
<marker>Biemann, Nygaard, 2010</marker>
<rawString>Chris Biemann and Valerie Nygaard. 2010. Crowdsourcing wordnet. In Proceedings of the 5th Global WordNet conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of Lexical Semantic Relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="1331" citStr="Budanitsky and Hirst, 2006" startWordPosition="183" endWordPosition="186">ated to validate conceptconcept and concept-image relations. In experiments comparing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional </context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating WordNet-based measures of Lexical Semantic Relatedness. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Yolanda Gil</author>
</authors>
<title>Improving the design of intelligent acquisition interfaces for collecting world knowledge from web contributors.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Conference on Knowledge Capture,</booktitle>
<pages>35--42</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4850" citStr="Chklovski and Gil, 2005" startWordPosition="706" endWordPosition="709">rk, the annotation quality is determined in a fully automatic way. Second, we demonstrate that converting games with a purpose into more traditional video games creates an increased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing. Third, for both games, we show that games produce better quality annotations than crowdsourcing. 2 Related Work Multiple works have proposed linguistic annotation-based games with a purpose for tasks such as anaphora resolution (Hladk´a et al., 2009; Poesio et al., 2013), paraphrasing (Chklovski and Gil, 2005), term associations (Artignan et al., 2009; Lafourcade and Joubert, 2010), query expansion (Simko et al., 2011), and word sense disambiguation (Chklovski and Mihalcea, 2002; Seemakurty et al., 2010; Venhuizen et al., 2013). Notably, all of these linguistic games focus on users interacting with text, in contrast to other highly successful games with a purpose in other domains, such as Foldit (Cooper et al., 2010), in which players fold protein sequences, and the ESP game (von Ahn and Dabbish, 2004), where players label images with words. Most similar to our work are games that create or validat</context>
</contexts>
<marker>Chklovski, Gil, 2005</marker>
<rawString>Timothy Chklovski and Yolanda Gil. 2005. Improving the design of intelligent acquisition interfaces for collecting world knowledge from web contributors. In Proceedings of the International Conference on Knowledge Capture, pages 35–42. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Chklovski</author>
<author>Rada Mihalcea</author>
</authors>
<title>Building a Sense Tagged Corpus with Open Mind Word Expert.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL 2002 Workshop on WSD: Recent Successes and Future Directions,</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="5022" citStr="Chklovski and Mihalcea, 2002" startWordPosition="730" endWordPosition="734"> an increased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing. Third, for both games, we show that games produce better quality annotations than crowdsourcing. 2 Related Work Multiple works have proposed linguistic annotation-based games with a purpose for tasks such as anaphora resolution (Hladk´a et al., 2009; Poesio et al., 2013), paraphrasing (Chklovski and Gil, 2005), term associations (Artignan et al., 2009; Lafourcade and Joubert, 2010), query expansion (Simko et al., 2011), and word sense disambiguation (Chklovski and Mihalcea, 2002; Seemakurty et al., 2010; Venhuizen et al., 2013). Notably, all of these linguistic games focus on users interacting with text, in contrast to other highly successful games with a purpose in other domains, such as Foldit (Cooper et al., 2010), in which players fold protein sequences, and the ESP game (von Ahn and Dabbish, 2004), where players label images with words. Most similar to our work are games that create or validate common sense knowledge. Two games with a purpose have incorporated video gamelike mechanics for annotation. First, Herda˘gdelen and Baroni (2012) validate automatically a</context>
</contexts>
<marker>Chklovski, Mihalcea, 2002</marker>
<rawString>Tim Chklovski and Rada Mihalcea. 2002. Building a Sense Tagged Corpus with Open Mind Word Expert. In Proceedings ofACL 2002 Workshop on WSD: Recent Successes and Future Directions, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seth Cooper</author>
<author>Firas Khatib</author>
<author>Adrien Treuille</author>
<author>Janos Barbero</author>
<author>Jeehyung Lee</author>
<author>Michael Beenen</author>
<author>Andrew Leaver-Fay</author>
<author>David Baker</author>
<author>Zoran Popovi´c</author>
<author>Foldit players</author>
</authors>
<title>Predicting protein structures with a multiplayer online game.</title>
<date>2010</date>
<journal>Nature,</journal>
<volume>466</volume>
<issue>7307</issue>
<pages>760</pages>
<marker>Cooper, Khatib, Treuille, Barbero, Lee, Beenen, Leaver-Fay, Baker, Popovi´c, players, 2010</marker>
<rawString>Seth Cooper, Firas Khatib, Adrien Treuille, Janos Barbero, Jeehyung Lee, Michael Beenen, Andrew Leaver-Fay, David Baker, Zoran Popovi´c, and Foldit players. 2010. Predicting protein structures with a multiplayer online game. Nature, 466(7307):756– 760.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Deng</author>
<author>Wei Dong</author>
<author>Richard Socher</author>
<author>Li-Jia Li</author>
<author>Kai Li</author>
<author>Li Fei-Fei</author>
</authors>
<title>ImageNet: A large-scale hierarchical image database.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR),</booktitle>
<pages>248--255</pages>
<contexts>
<context position="19950" citStr="Deng et al., 2009" startWordPosition="3178" endWordPosition="3181">ities and her accuracy at discarding images from N. Annotation Players annotate by deciding which images to keep in their inventory. Images receive positive rating annotations from: (1) depositing the image in a reward chest, and (2) ending the level with the image still in the inventory. Conversely, images receive a negative rating when a player (1) views the image but intentionally avoids picking it up or (2) drops the image from her inventory. TKT is designed to assist in the validation and extension of automatically-created image libraries that link to semantic concepts, such as ImageNet (Deng et al., 2009) and that of Torralba et al. (2008). However, its general design allows for other types of annotations, such as image labeling, by changing the tower’s instructions and pictures. Quality Enforcement Mechanisms Similar to Infection, TKT includes analogous mechanisms for limiting adversarial player annotations. Players who collect no images are prevented from entering the boss room, limiting their ability to generate false negative annotations. Similarly, players who collect all images are likely to have half of their images from N and therefore fail the tower’s quality-check after defeating the</context>
</contexts>
<marker>Deng, Dong, Socher, Li, Li, Fei-Fei, 2009</marker>
<rawString>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A large-scale hierarchical image database. In Proceedings of the Conference on Computer Vision and Pattern Recognition (CVPR), pages 248–255.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soojeong Eom</author>
<author>Markus Dickinson</author>
<author>Graham Katz</author>
</authors>
<title>Using semi-experts to derive judgments on word sense alignment: a pilot study.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>605--611</pages>
<contexts>
<context position="2680" citStr="Eom et al., 2012" startWordPosition="376" endWordPosition="379">has enabled the creation of new semantic knowledge bases (Medelyan et al., 2009; Hovy et al., 2013) through automatically merging WordNet and Wikipedia (Suchanek et al., 2007; Navigli and Ponzetto, 2010; Niemann and Gurevych, 2011). While these automatic approaches offer the scale needed for opendomain applications, the automatic processes often introduce errors, which can prove detrimental to downstream applications. To overcome issues from fully-automatic construction methods, several works have proposed validating or extending knowledge bases using crowdsourcing (Biemann and Nygaard, 2010; Eom et al., 2012; Sarasua et al., 2012). However, these methods, too, are limited by the resources required for acquiring large numbers of responses. In this paper, we propose validating and extending semantic knowledge bases using video games with a purpose. Here, the annotation tasks are transformed into elements of a video game where players accomplish their jobs by virtue of playing the game, rather than by performing a more traditional annotation task. While prior efforts in NLP have incorporated games for performing annotation and validation (Siorpaes and Hepp, 2008b; Herda˘gdelen and Baroni, 2012; Poes</context>
</contexts>
<marker>Eom, Dickinson, Katz, 2012</marker>
<rawString>Soojeong Eom, Markus Dickinson, and Graham Katz. 2012. Using semi-experts to derive judgments on word sense alignment: a pilot study. In Proceedings of the Conference on Language Resources and Evaluation (LREC), pages 605–611.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiziano Flati</author>
<author>Daniele Vannella</author>
<author>Tommaso Pasini</author>
<author>Roberto Navigli</author>
</authors>
<title>Two is bigger (and better) than one: the Wikipedia Bitaxonomy Project.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<location>Baltimore, Maryland.</location>
<marker>Flati, Vannella, Pasini, Navigli, 2014</marker>
<rawString>Tiziano Flati, Daniele Vannella, Tommaso Pasini, and Roberto Navigli. 2014. Two is bigger (and better) than one: the Wikipedia Bitaxonomy Project. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), Baltimore, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amac¸ Herda˘gdelen</author>
<author>Marco Baroni</author>
</authors>
<title>Bootstrapping a game with a purpose for common sense collection.</title>
<date>2012</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>3</volume>
<issue>4</issue>
<marker>Herda˘gdelen, Baroni, 2012</marker>
<rawString>Amac¸ Herda˘gdelen and Marco Baroni. 2012. Bootstrapping a game with a purpose for common sense collection. ACM Transactions on Intelligent Systems and Technology, 3(4):1–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbora Hladk´a</author>
<author>Jiˇr´ı M´ırovsk`y</author>
<author>Pavel Schlesinger</author>
</authors>
<title>Play the language: Play coreference.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Association for Computational Linguistics and International Joint Conference of the Asian Federation of Natural Language Processing (ACL-IJCNLP),</booktitle>
<pages>209--212</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Hladk´a, M´ırovsk`y, Schlesinger, 2009</marker>
<rawString>Barbora Hladk´a, Jiˇr´ı M´ırovsk`y, and Pavel Schlesinger. 2009. Play the language: Play coreference. In Proceedings of the Joint Conference of the Association for Computational Linguistics and International Joint Conference of the Asian Federation of Natural Language Processing (ACL-IJCNLP), pages 209–212. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Collaboratively built semistructured content and Artificial Intelligence: The story so far.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--2</pages>
<contexts>
<context position="2163" citStr="Hovy et al., 2013" startWordPosition="302" endWordPosition="305"> building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify novel semantic connections between concepts. The recent advent of large semi-structured resources has enabled the creation of new semantic knowledge bases (Medelyan et al., 2009; Hovy et al., 2013) through automatically merging WordNet and Wikipedia (Suchanek et al., 2007; Navigli and Ponzetto, 2010; Niemann and Gurevych, 2011). While these automatic approaches offer the scale needed for opendomain applications, the automatic processes often introduce errors, which can prove detrimental to downstream applications. To overcome issues from fully-automatic construction methods, several works have proposed validating or extending knowledge bases using crowdsourcing (Biemann and Nygaard, 2010; Eom et al., 2012; Sarasua et al., 2012). However, these methods, too, are limited by the resources </context>
</contexts>
<marker>Hovy, Navigli, Ponzetto, 2013</marker>
<rawString>Eduard H. Hovy, Roberto Navigli, and Simone Paolo Ponzetto. 2013. Collaboratively built semistructured content and Artificial Intelligence: The story so far. Artificial Intelligence, 194:2–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association of Computational Linguistics (NAACL),</booktitle>
<pages>455--462</pages>
<contexts>
<context position="1413" citStr="Kauchak and Barzilay, 2006" startWordPosition="194" endWordPosition="197">ing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify novel semantic connections between concepts. Th</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of the Conference of the North American Chapter of the Association of Computational Linguistics (NAACL), pages 455–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology. Sage,</title>
<date>2004</date>
<location>Thousand Oaks, CA,</location>
<note>second edition.</note>
<contexts>
<context position="21475" citStr="Krippendorff, 2004" startWordPosition="3409" endWordPosition="3410">each game, (b) crowdsourcing, and (c) a non-video game with a purpose. 6.1 Experimental Setup Gold Standard Data To compare the quality of annotation from games and crowdsourcing, a gold standard annotation was produced for a 10% sample of each dataset (cf. Section 3.2). Two annotators independently rated the items and, in cases of disagreement, a third expert annotator adjudicated. Unlike in the game setting, annotators were free to consult additional resources such as Wikipedia. To measure inter-annotator agreement (IAA) on the gold standard annotations, we calculated Krip1298 pendorff’s α (Krippendorff, 2004; Artstein and Poesio, 2008); α ranges between [-1,1] where 1 indicates complete agreement, -1 indicates systematic disagreement, and values near 0 indicate agreement at chance levels. Gold standard annotators had high agreement, 0.774, for conceptconcept relations. However, image-concept agreement was only moderate, 0.549. A further analysis revealed differences in the annotators’ thresholds for determining association, with one annotator permitting more abstract relations. However, the adjudication process resolved these disputes, resulting in substantial agreement by all annotators on the f</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus Krippendorff. 2004. Content Analysis: An Introduction to Its Methodology. Sage, Thousand Oaks, CA, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yen-ling Kuo</author>
<author>Jong-Chuan Lee</author>
<author>Kai-yang Chiang</author>
<author>Rex Wang</author>
<author>Edward Shen</author>
<author>Cheng-wei Chan</author>
<author>Jane Yung-jen Hsu</author>
</authors>
<title>Community-based game design: experiments on social games for commonsense data collection.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACM SIGKDD Workshop on Human Computation,</booktitle>
<pages>15--22</pages>
<contexts>
<context position="6041" citStr="Kuo et al. (2009)" startWordPosition="894" endWordPosition="897">mes that create or validate common sense knowledge. Two games with a purpose have incorporated video gamelike mechanics for annotation. First, Herda˘gdelen and Baroni (2012) validate automatically acquired common sense relations using a slot machine game where players must identify valid relations and arguments from randomly aligned data within a time limit. Although the validation is embedded in a game-like setting, players are limited to one action (pulling the lever) unlike our games, which feature a variety of actions and rich gameplay experience to keep players interested longer. Second, Kuo et al. (2009) describe a pet-raising game where players must answer common sense questions in order to obtain pet food. While their game is among the most video game-like, the annotation task is a chore the player must perform in order to return to the game, rather than an integrated, fun part of the game’s objectives, which potentially decreases motivation for answering correctly. Several works have proposed adapting existing word-based board game designs to create or validate common sense knowledge. von Ahn et al. (2006) generate common sense facts by using a game similar to TabooTM, where one player mus</context>
</contexts>
<marker>Kuo, Lee, Chiang, Wang, Shen, Chan, Hsu, 2009</marker>
<rawString>Yen-ling Kuo, Jong-Chuan Lee, Kai-yang Chiang, Rex Wang, Edward Shen, Cheng-wei Chan, and Jane Yung-jen Hsu. 2009. Community-based game design: experiments on social games for commonsense data collection. In Proceedings of the ACM SIGKDD Workshop on Human Computation, pages 15–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mathieu Lafourcade</author>
<author>Alain Joubert</author>
</authors>
<title>Computing trees of named word usages from a crowdsourced lexical network.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Multiconference on Computer Science and Information Technology (IMCSIT),</booktitle>
<pages>439--446</pages>
<location>Wisla, Poland.</location>
<contexts>
<context position="4923" citStr="Lafourcade and Joubert, 2010" startWordPosition="716" endWordPosition="719">econd, we demonstrate that converting games with a purpose into more traditional video games creates an increased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing. Third, for both games, we show that games produce better quality annotations than crowdsourcing. 2 Related Work Multiple works have proposed linguistic annotation-based games with a purpose for tasks such as anaphora resolution (Hladk´a et al., 2009; Poesio et al., 2013), paraphrasing (Chklovski and Gil, 2005), term associations (Artignan et al., 2009; Lafourcade and Joubert, 2010), query expansion (Simko et al., 2011), and word sense disambiguation (Chklovski and Mihalcea, 2002; Seemakurty et al., 2010; Venhuizen et al., 2013). Notably, all of these linguistic games focus on users interacting with text, in contrast to other highly successful games with a purpose in other domains, such as Foldit (Cooper et al., 2010), in which players fold protein sequences, and the ESP game (von Ahn and Dabbish, 2004), where players label images with words. Most similar to our work are games that create or validate common sense knowledge. Two games with a purpose have incorporated vide</context>
</contexts>
<marker>Lafourcade, Joubert, 2010</marker>
<rawString>Mathieu Lafourcade and Alain Joubert. 2010. Computing trees of named word usages from a crowdsourced lexical network. In Proceedings of the International Multiconference on Computer Science and Information Technology (IMCSIT), pages 439– 446, Wisla, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Winter Mason</author>
<author>Siddharth Suri</author>
</authors>
<title>Conducting behavioral research on amazons mechanical turk.</title>
<date>2012</date>
<journal>Behavior Research Methods,</journal>
<volume>44</volume>
<issue>1</issue>
<contexts>
<context position="23547" citStr="Mason and Suri, 2012" startWordPosition="3737" endWordPosition="3740">cing was performed using the CrowdFlower platform. Annotation tasks were designed to closely match each game’s annotation process. A task begins with a description of a target synset and its textual definition; following, ten annotation questions are shown. Separate tasks were used for validating concept-concept and concept-image relations. Each tasks’ questions were shown as a binary choice of whether the item is related to the task’s concept. Workers were paid 0.05 USD per task. Each question was answered by three workers. Following common practices for guarding against adversarial workers (Mason and Suri, 2012), the tasks for concept c include quality check questions using items from N,. Workers who rate too many relations from N, as valid are removed by CrowdFlower and prevented from participating further. One of the ten questions in a task used an item from N,, resulting in a task mixture of 90% annotation questions and 10% qualitycheck questions. However, we note that both of our video games use data that is 50% annotation, 50% quality-check. While the crowdsourcing task could be adjusted to use an increased number of quality-check options, such a design is uncommon and artificially inflates the </context>
</contexts>
<marker>Mason, Suri, 2012</marker>
<rawString>Winter Mason and Siddharth Suri. 2012. Conducting behavioral research on amazons mechanical turk. Behavior Research Methods, 44(1):1–23.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Olena Medelyan</author>
</authors>
<location>David Milne, Catherine Legg, and</location>
<marker>Medelyan, </marker>
<rawString>Olena Medelyan, David Milne, Catherine Legg, and</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
</authors>
<title>Mining meaning from Wikipedia.</title>
<date>2009</date>
<journal>International Journal of HumanComputer Studies,</journal>
<volume>67</volume>
<issue>9</issue>
<marker>Witten, 2009</marker>
<rawString>Ian H. Witten. 2009. Mining meaning from Wikipedia. International Journal of HumanComputer Studies, 67(9):716–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Alessandro Raganato</author>
<author>Roberto Navigli</author>
</authors>
<title>Entity Linking meets Word Sense Disambiguation: A Unified Approach. Transactions of the Association for Computational Linguistics.</title>
<date>2014</date>
<contexts>
<context position="1492" citStr="Moro et al., 2014" startWordPosition="206" endWordPosition="209">igher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify novel semantic connections between concepts. The recent advent of large semi-structured resources has enabled the creation of </context>
</contexts>
<marker>Moro, Raganato, Navigli, 2014</marker>
<rawString>Andrea Moro, Alessandro Raganato, and Roberto Navigli. 2014. Entity Linking meets Word Sense Disambiguation: A Unified Approach. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: Building a very large multilingual semantic network.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>216--225</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1197" citStr="Navigli and Ponzetto, 2010" startWordPosition="163" endWordPosition="166">propose a cost-effective method of validating and extending knowledge bases using video games with a purpose. Two video games were created to validate conceptconcept and concept-image relations. In experiments comparing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build </context>
<context position="10291" citStr="Navigli and Ponzetto, 2010" startWordPosition="1582" endWordPosition="1585">eposition attachment may be difficult to integrate directly into gameplay, many simpler but still necessary annotation tasks such as word and image associations can be easily modeled with traditional video game mechanics. 3.2 Annotation Setup Tasks We focused on two annotation tasks: (1) validating associations between two concepts, and (2) validating associations between a concept and an image. For each task we developed a video game with a purpose that integrates the task within the game, as illustrated in Sections 4 and 5. Knowledge base As the reference knowledge base, we chose BabelNet2 (Navigli and Ponzetto, 2010), a large-scale multilingual semantic ontology created by automatically merging WordNet with other collaboratively-constructed resources such as Wikipedia and OmegaWiki. BabelNet data offers two necessary features for generating the games’ datasets. First, by connecting WordNet synsets to Wikipedia pages, most synsets are associated with a set of pictures; while often noisy, these pictures sometimes illustrate the target concept and are an ideal case for validation. Second, BabelNet contains the semantic relations from both WordNet and hyperlinks in Wikipedia; these relations are again an idea</context>
</contexts>
<marker>Navigli, Ponzetto, 2010</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2010. BabelNet: Building a very large multilingual semantic network. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), Uppsala, Sweden, pages 216–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Joining forces pays off: Multilingual Joint Word Sense Disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>1399--1410</pages>
<location>Jeju,</location>
<contexts>
<context position="1472" citStr="Navigli and Ponzetto, 2012" startWordPosition="202" endWordPosition="205">tion consistently leads to higher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify novel semantic connections between concepts. The recent advent of large semi-structured resources has enab</context>
<context position="12439" citStr="Navigli and Ponzetto, 2012" startWordPosition="1929" endWordPosition="1932">r all concepts in a dataset, respectively. For the concept-concept dataset, Vc is the union of VcB, which contains the lemmas of all synsets incident to c in BabelNet, and Vcn, which contains novel lemmas derived from statistical associations. Specifically, novel lemmas were selected by computing the x2 statistic for co-occurrences between the lemmas of c and all other part of speech-tagged lemmas in Wikipedia. The 30 lemmas with the highest x2 are included in Vc. To enable concept-to-concept annotations, we disambiguate novel lemmas using a simple heuristic based on link co-occurrence count (Navigli and Ponzetto, 2012). Each set Vc contains 77.6 lemmas on average. For the concept-image data, Vc is the union of Vc B, which contains all images associated with c in BabelNet, and Vcn, which contains web-gathered images using a lemma of c as the query. Webgathered images were retrieved using Yahoo! Boss image search and the first result set (35 images) was added to Vc. Each set Vc contains 77.0 images on average. For both datasets, each negative set Nc is constructed as ∪c/∈C\{c}Vc/B, i.e., from the items related in BabelNet to all other concepts in C. By constructing Nc directly from the knowledge base, play ac</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. Joining forces pays off: Multilingual Joint Word Sense Disambiguation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 1399– 1410, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Semi-automatic extension of large-scale linguistic knowledge bases.</title>
<date>2005</date>
<booktitle>In Proceedings of the 18th Internationa Florida AI Research Symposium Conference,</booktitle>
<pages>548--553</pages>
<location>Clearwater Beach, Florida,</location>
<contexts>
<context position="1894" citStr="Navigli (2005)" startWordPosition="266" endWordPosition="267">g semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify novel semantic connections between concepts. The recent advent of large semi-structured resources has enabled the creation of new semantic knowledge bases (Medelyan et al., 2009; Hovy et al., 2013) through automatically merging WordNet and Wikipedia (Suchanek et al., 2007; Navigli and Ponzetto, 2010; Niemann and Gurevych, 2011). While these automatic approaches offer the scale needed for opendomain applications, the automatic processes often introduce errors, which can prove detrimental to downstream applications. To overc</context>
</contexts>
<marker>Navigli, 2005</marker>
<rawString>Roberto Navigli. 2005. Semi-automatic extension of large-scale linguistic knowledge bases. In Proceedings of the 18th Internationa Florida AI Research Symposium Conference, Clearwater Beach, Florida, 15–17 May 2005, pages 548–553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elisabeth Niemann</author>
<author>Iryna Gurevych</author>
</authors>
<title>The people’s web meets linguistic knowledge: Automatic sense alignment of Wikipedia and WordNet.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Computational Semantics (IWCS),</booktitle>
<pages>205--214</pages>
<contexts>
<context position="2295" citStr="Niemann and Gurevych, 2011" startWordPosition="321" endWordPosition="325">knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify novel semantic connections between concepts. The recent advent of large semi-structured resources has enabled the creation of new semantic knowledge bases (Medelyan et al., 2009; Hovy et al., 2013) through automatically merging WordNet and Wikipedia (Suchanek et al., 2007; Navigli and Ponzetto, 2010; Niemann and Gurevych, 2011). While these automatic approaches offer the scale needed for opendomain applications, the automatic processes often introduce errors, which can prove detrimental to downstream applications. To overcome issues from fully-automatic construction methods, several works have proposed validating or extending knowledge bases using crowdsourcing (Biemann and Nygaard, 2010; Eom et al., 2012; Sarasua et al., 2012). However, these methods, too, are limited by the resources required for acquiring large numbers of responses. In this paper, we propose validating and extending semantic knowledge bases using</context>
</contexts>
<marker>Niemann, Gurevych, 2011</marker>
<rawString>Elisabeth Niemann and Iryna Gurevych. 2011. The people’s web meets linguistic knowledge: Automatic sense alignment of Wikipedia and WordNet. In Proceedings of the International Conference on Computational Semantics (IWCS), pages 205–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohammad Taher Pilehvar</author>
<author>David Jurgens</author>
<author>Roberto Navigli</author>
</authors>
<title>Align, Disambiguate and Walk: a Unified Approach for Measuring Semantic Similarity.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1341--1351</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1370" citStr="Pilehvar et al., 2013" startWordPosition="189" endWordPosition="192">image relations. In experiments comparing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify nove</context>
</contexts>
<marker>Pilehvar, Jurgens, Navigli, 2013</marker>
<rawString>Mohammad Taher Pilehvar, David Jurgens, and Roberto Navigli. 2013. Align, Disambiguate and Walk: a Unified Approach for Measuring Semantic Similarity. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL), pages 1341–1351, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Jon Chamberlain</author>
<author>Udo Kruschwitz</author>
<author>Livio Robaldo</author>
<author>Luca Ducceschi</author>
</authors>
<title>Phrase detectives: Utilizing collective intelligence for internet-scale language resource creation.</title>
<date>2013</date>
<journal>ACM Transactions on Interactive Intelligent Systems,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="3296" citStr="Poesio et al., 2013" startWordPosition="474" endWordPosition="477">2012; Sarasua et al., 2012). However, these methods, too, are limited by the resources required for acquiring large numbers of responses. In this paper, we propose validating and extending semantic knowledge bases using video games with a purpose. Here, the annotation tasks are transformed into elements of a video game where players accomplish their jobs by virtue of playing the game, rather than by performing a more traditional annotation task. While prior efforts in NLP have incorporated games for performing annotation and validation (Siorpaes and Hepp, 2008b; Herda˘gdelen and Baroni, 2012; Poesio et al., 2013), these games have largely been text-based, adding game-like features such as high-scores on top of an existing annotation task. In contrast, we introduce two video games with graphical 2D gameplay that is similar to what game players are familiar with. The fun nature of the games provides an intrinsic motivation for players to keep playing, which can increase the quality of their work and lower the cost per annotation. Our work provides the following three contributions. First, we demonstrate effective video gamebased methods for both validating and extending 1294 Proceedings of the 52nd Annu</context>
<context position="4810" citStr="Poesio et al., 2013" startWordPosition="701" endWordPosition="704">mappings. In contrast to previous work, the annotation quality is determined in a fully automatic way. Second, we demonstrate that converting games with a purpose into more traditional video games creates an increased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing. Third, for both games, we show that games produce better quality annotations than crowdsourcing. 2 Related Work Multiple works have proposed linguistic annotation-based games with a purpose for tasks such as anaphora resolution (Hladk´a et al., 2009; Poesio et al., 2013), paraphrasing (Chklovski and Gil, 2005), term associations (Artignan et al., 2009; Lafourcade and Joubert, 2010), query expansion (Simko et al., 2011), and word sense disambiguation (Chklovski and Mihalcea, 2002; Seemakurty et al., 2010; Venhuizen et al., 2013). Notably, all of these linguistic games focus on users interacting with text, in contrast to other highly successful games with a purpose in other domains, such as Foldit (Cooper et al., 2010), in which players fold protein sequences, and the ESP game (von Ahn and Dabbish, 2004), where players label images with words. Most similar to o</context>
<context position="35715" citStr="Poesio et al. (2013)" startWordPosition="5696" endWordPosition="5699"> with respect to quality. Third, we note that both video games in the paid setting incur a fixed cost (for the prizes) and therefore additional games played can only further decrease the cost per annotation. Indeed, the present study divided the audience pool into two separate groups which effectively halved the potential number of annotations per game. Assuming combining the audiences would produce the same number of annotations, both our games’ costs per annotation drop to $0.012. Last, video games can potentially come with indirect costs due to software development and maintenance. Indeed, Poesio et al. (2013) report spending 60,000£ in developing their Phrase Detectives game with a purpose over a two-year period. In contrast, both games here were developed as apart of student projects using open source software and assets and thus incurred no cost; furthermore, games were created in a few months, rather than years. Given that few online games attain significant sustained interest, we argue that 4Earlier versions that used mostly items from V proved less engaging due to players frequently performing the same action, e.g., saving most humans or collecting most pictures. our lightweight model is pref</context>
</contexts>
<marker>Poesio, Chamberlain, Kruschwitz, Robaldo, Ducceschi, 2013</marker>
<rawString>Massimo Poesio, Jon Chamberlain, Udo Kruschwitz, Livio Robaldo, and Luca Ducceschi. 2013. Phrase detectives: Utilizing collective intelligence for internet-scale language resource creation. ACM Transactions on Interactive Intelligent Systems, 3(1):3:1–3:44, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Rogstadius</author>
<author>Vassilis Kostakos</author>
<author>Aniket Kittur</author>
<author>Boris Smus</author>
<author>Jim Laredo</author>
<author>Maja Vukovic</author>
</authors>
<title>An assessment of intrinsic and extrinsic motivation on task performance in crowdsourcing markets.</title>
<date>2011</date>
<booktitle>In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM).</booktitle>
<contexts>
<context position="33901" citStr="Rogstadius et al., 2011" startWordPosition="5415" endWordPosition="5418">e recognition incentive was motivating players in the free condition and thus some incentive was required. However, player behavior indicates otherwise: After the contest period ended, no players in the free setting registered for being acknowledged by name, which strongly suggests the incentive was not contributing to their motivation for playing. Furthermore, a minority of players continued to play even after the contest period ended, suggesting that enjoyment was a driving factor. 1301 Last, while crowdsourcing has seen different quality and volume from workers in paid and unpaid settings (Rogstadius et al., 2011), in contrast, our games produced approximately-equivalent results from players in both settings. Crowdsourcing was slightly more cost-effective than both games in the paid condition, as shown in Table 1, Column 8. However, three additional factors need to be considered. First, both games intentionally uniformly sample between V and N to increase player engagement,4 which generates a larger number of annotations for items in N than are produced by crowdsourcing. When annotations on items in N are included for both games and crowdsourcing, the costs per annotation drop to comparable levels: $0.</context>
</contexts>
<marker>Rogstadius, Kostakos, Kittur, Smus, Laredo, Vukovic, 2011</marker>
<rawString>Jakob Rogstadius, Vassilis Kostakos, Aniket Kittur, Boris Smus, Jim Laredo, and Maja Vukovic. 2011. An assessment of intrinsic and extrinsic motivation on task performance in crowdsourcing markets. In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacek Rzeniewicz</author>
<author>Julian Szyma´nski</author>
</authors>
<title>Bringing Common Sense to WordNet with a Word Game.</title>
<date>2013</date>
<booktitle>In Computational Collective Intelligence. Technologies and Applications,</booktitle>
<volume>8083</volume>
<pages>296--305</pages>
<publisher>Springer.</publisher>
<marker>Rzeniewicz, Szyma´nski, 2013</marker>
<rawString>Jacek Rzeniewicz and Julian Szyma´nski. 2013. Bringing Common Sense to WordNet with a Word Game. In Computational Collective Intelligence. Technologies and Applications, volume 8083 of Lecture Notes in Computer Science, pages 296–305. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristina Sarasua</author>
<author>Elena Simperl</author>
<author>Natalya F Noy</author>
</authors>
<title>CrowdMap: Crowdsourcing ontology alignment with microtasks.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Semantic Web Conference (ISWC),</booktitle>
<pages>525--541</pages>
<contexts>
<context position="2703" citStr="Sarasua et al., 2012" startWordPosition="380" endWordPosition="383">eation of new semantic knowledge bases (Medelyan et al., 2009; Hovy et al., 2013) through automatically merging WordNet and Wikipedia (Suchanek et al., 2007; Navigli and Ponzetto, 2010; Niemann and Gurevych, 2011). While these automatic approaches offer the scale needed for opendomain applications, the automatic processes often introduce errors, which can prove detrimental to downstream applications. To overcome issues from fully-automatic construction methods, several works have proposed validating or extending knowledge bases using crowdsourcing (Biemann and Nygaard, 2010; Eom et al., 2012; Sarasua et al., 2012). However, these methods, too, are limited by the resources required for acquiring large numbers of responses. In this paper, we propose validating and extending semantic knowledge bases using video games with a purpose. Here, the annotation tasks are transformed into elements of a video game where players accomplish their jobs by virtue of playing the game, rather than by performing a more traditional annotation task. While prior efforts in NLP have incorporated games for performing annotation and validation (Siorpaes and Hepp, 2008b; Herda˘gdelen and Baroni, 2012; Poesio et al., 2013), these</context>
</contexts>
<marker>Sarasua, Simperl, Noy, 2012</marker>
<rawString>Cristina Sarasua, Elena Simperl, and Natalya F Noy. 2012. CrowdMap: Crowdsourcing ontology alignment with microtasks. In Proceedings of the International Semantic Web Conference (ISWC), pages 525–541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Seemakurty</author>
<author>Jonathan Chu</author>
<author>Luis Von Ahn</author>
<author>Anthony Tomasic</author>
</authors>
<title>Word sense disambiguation via human computation.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACM SIGKDD Workshop on Human Computation,</booktitle>
<pages>60--63</pages>
<publisher>ACM.</publisher>
<marker>Seemakurty, Chu, Von Ahn, Tomasic, 2010</marker>
<rawString>Nitin Seemakurty, Jonathan Chu, Luis Von Ahn, and Anthony Tomasic. 2010. Word sense disambiguation via human computation. In Proceedings of the ACM SIGKDD Workshop on Human Computation, pages 60–63. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Simko</author>
<author>Michal Tvarozek</author>
<author>Maria Bielikova</author>
</authors>
<title>Little search game: term network acquisition via a human computation game.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACM conference on Hypertext and Hypermedia,</booktitle>
<pages>57--62</pages>
<contexts>
<context position="4961" citStr="Simko et al., 2011" startWordPosition="722" endWordPosition="725"> a purpose into more traditional video games creates an increased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing. Third, for both games, we show that games produce better quality annotations than crowdsourcing. 2 Related Work Multiple works have proposed linguistic annotation-based games with a purpose for tasks such as anaphora resolution (Hladk´a et al., 2009; Poesio et al., 2013), paraphrasing (Chklovski and Gil, 2005), term associations (Artignan et al., 2009; Lafourcade and Joubert, 2010), query expansion (Simko et al., 2011), and word sense disambiguation (Chklovski and Mihalcea, 2002; Seemakurty et al., 2010; Venhuizen et al., 2013). Notably, all of these linguistic games focus on users interacting with text, in contrast to other highly successful games with a purpose in other domains, such as Foldit (Cooper et al., 2010), in which players fold protein sequences, and the ESP game (von Ahn and Dabbish, 2004), where players label images with words. Most similar to our work are games that create or validate common sense knowledge. Two games with a purpose have incorporated video gamelike mechanics for annotation. F</context>
</contexts>
<marker>Simko, Tvarozek, Bielikova, 2011</marker>
<rawString>Jakub Simko, Michal Tvarozek, and Maria Bielikova. 2011. Little search game: term network acquisition via a human computation game. In Proceedings of the ACM conference on Hypertext and Hypermedia, pages 57–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharina Siorpaes</author>
<author>Martin Hepp</author>
</authors>
<title>Games with a purpose for the semantic web.</title>
<date>2008</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="3242" citStr="Siorpaes and Hepp, 2008" startWordPosition="466" endWordPosition="469">ng crowdsourcing (Biemann and Nygaard, 2010; Eom et al., 2012; Sarasua et al., 2012). However, these methods, too, are limited by the resources required for acquiring large numbers of responses. In this paper, we propose validating and extending semantic knowledge bases using video games with a purpose. Here, the annotation tasks are transformed into elements of a video game where players accomplish their jobs by virtue of playing the game, rather than by performing a more traditional annotation task. While prior efforts in NLP have incorporated games for performing annotation and validation (Siorpaes and Hepp, 2008b; Herda˘gdelen and Baroni, 2012; Poesio et al., 2013), these games have largely been text-based, adding game-like features such as high-scores on top of an existing annotation task. In contrast, we introduce two video games with graphical 2D gameplay that is similar to what game players are familiar with. The fun nature of the games provides an intrinsic motivation for players to keep playing, which can increase the quality of their work and lower the cost per annotation. Our work provides the following three contributions. First, we demonstrate effective video gamebased methods for both vali</context>
<context position="7658" citStr="Siorpaes and Hepp, 2008" startWordPosition="1159" endWordPosition="1162">n a category and then must list things in that category. The two variants differ in the constraints imposed on the players, such as beginning all items with a specific letter. For all three games, two players play the same game under time limits and then are rewarded if their answers match. Last, three two-player games have focused on validating and extending knowledge bases. Rzeniewicz and Szyma´nski (2013) extend WordNet with common-sense knowledge using a 20 Questions-like game. In a rapid-play style game, OntoPronto attempts to classify Wikipedia pages as either categories or individuals (Siorpaes and Hepp, 2008a). SpotTheLink uses a similar rapid question format to have players align the DBpedia and PROTON ontologies by agreeing on the distinctions between classes (Thaler et al., 2011). Unlike dynamic gaming elements common in our video games, the above games are all focused on interacting with textual items. Another major limitation is their need for always having two players, which requires them to sustain enough interest to always maintain an active pool of players. While the computer can potentially act as a second player, such a simulated player is often limited to using preexisting knowledge o</context>
</contexts>
<marker>Siorpaes, Hepp, 2008</marker>
<rawString>Katharina Siorpaes and Martin Hepp. 2008a. Games with a purpose for the semantic web. IEEE Intelligent Systems, 23(3):50–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharina Siorpaes</author>
<author>Martin Hepp</author>
</authors>
<title>Ontogame: Weaving the semantic web by online games.</title>
<date>2008</date>
<booktitle>The Semantic Web: Research and Applications,</booktitle>
<volume>5021</volume>
<pages>751--766</pages>
<editor>In Sean Bechhofer, Manfred Hauswirth, Jrg Hoffmann, and Manolis Koubarakis, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="3242" citStr="Siorpaes and Hepp, 2008" startWordPosition="466" endWordPosition="469">ng crowdsourcing (Biemann and Nygaard, 2010; Eom et al., 2012; Sarasua et al., 2012). However, these methods, too, are limited by the resources required for acquiring large numbers of responses. In this paper, we propose validating and extending semantic knowledge bases using video games with a purpose. Here, the annotation tasks are transformed into elements of a video game where players accomplish their jobs by virtue of playing the game, rather than by performing a more traditional annotation task. While prior efforts in NLP have incorporated games for performing annotation and validation (Siorpaes and Hepp, 2008b; Herda˘gdelen and Baroni, 2012; Poesio et al., 2013), these games have largely been text-based, adding game-like features such as high-scores on top of an existing annotation task. In contrast, we introduce two video games with graphical 2D gameplay that is similar to what game players are familiar with. The fun nature of the games provides an intrinsic motivation for players to keep playing, which can increase the quality of their work and lower the cost per annotation. Our work provides the following three contributions. First, we demonstrate effective video gamebased methods for both vali</context>
<context position="7658" citStr="Siorpaes and Hepp, 2008" startWordPosition="1159" endWordPosition="1162">n a category and then must list things in that category. The two variants differ in the constraints imposed on the players, such as beginning all items with a specific letter. For all three games, two players play the same game under time limits and then are rewarded if their answers match. Last, three two-player games have focused on validating and extending knowledge bases. Rzeniewicz and Szyma´nski (2013) extend WordNet with common-sense knowledge using a 20 Questions-like game. In a rapid-play style game, OntoPronto attempts to classify Wikipedia pages as either categories or individuals (Siorpaes and Hepp, 2008a). SpotTheLink uses a similar rapid question format to have players align the DBpedia and PROTON ontologies by agreeing on the distinctions between classes (Thaler et al., 2011). Unlike dynamic gaming elements common in our video games, the above games are all focused on interacting with textual items. Another major limitation is their need for always having two players, which requires them to sustain enough interest to always maintain an active pool of players. While the computer can potentially act as a second player, such a simulated player is often limited to using preexisting knowledge o</context>
</contexts>
<marker>Siorpaes, Hepp, 2008</marker>
<rawString>Katharina Siorpaes and Martin Hepp. 2008b. Ontogame: Weaving the semantic web by online games. In Sean Bechhofer, Manfred Hauswirth, Jrg Hoffmann, and Manolis Koubarakis, editors, The Semantic Web: Research and Applications, volume 5021 of Lecture Notes in Computer Science, pages 751–766. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Dan Jurafsky</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic taxonomy induction from heterogeneous evidence.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL),</booktitle>
<pages>801--808</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="1875" citStr="Snow et al. (2006)" startWordPosition="261" endWordPosition="264">tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required. Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify novel semantic connections between concepts. The recent advent of large semi-structured resources has enabled the creation of new semantic knowledge bases (Medelyan et al., 2009; Hovy et al., 2013) through automatically merging WordNet and Wikipedia (Suchanek et al., 2007; Navigli and Ponzetto, 2010; Niemann and Gurevych, 2011). While these automatic approaches offer the scale needed for opendomain applications, the automatic processes often introduce errors, which can prove detrimental to downstream app</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2006</marker>
<rawString>Rion Snow, Dan Jurafsky, and Andrew Ng. 2006. Semantic taxonomy induction from heterogeneous evidence. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING-ACL), Sydney, Australia, pages 801–808.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian M Suchanek</author>
<author>Gjergji Kasneci</author>
<author>Gerhard Weikum</author>
</authors>
<title>YAGO: A core of semantic knowledge. unifying WordNet and Wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th World Wide Web Conference,</booktitle>
<pages>697--706</pages>
<location>Banff, Canada, 8–12</location>
<contexts>
<context position="1154" citStr="Suchanek et al., 2007" startWordPosition="157" endWordPosition="160">ing methods such as crowdsourcing. We propose a cost-effective method of validating and extending knowledge bases using video games with a purpose. Two video games were created to validate conceptconcept and concept-image relations. In experiments comparing with crowdsourcing, we show that video game-based validation consistently leads to higher-quality annotations, even when players are not compensated. 1 Introduction Large-scale knowledge bases are an essential component of many approaches in Natural Language Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological structure that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsupervised algorithms when training data is sparse or unavailable. However, constructing and updating semantic knowledge bases is often limited by the significant time and human resources required.</context>
<context position="11134" citStr="Suchanek et al., 2007" startWordPosition="1708" endWordPosition="1711">erating the games’ datasets. First, by connecting WordNet synsets to Wikipedia pages, most synsets are associated with a set of pictures; while often noisy, these pictures sometimes illustrate the target concept and are an ideal case for validation. Second, BabelNet contains the semantic relations from both WordNet and hyperlinks in Wikipedia; these relations are again an ideal case of validation, as not all hyperlinks connect semanticallyrelated pages in Wikipedia. Last, we stress that while our games use BabelNet data, they could easily validate or extend other knowledge bases such as YAGO (Suchanek et al., 2007) as well. 1This design is in contrast to two-player games where mutual agreement reinforces correct behavior. 2http://babelnet.org Data We created a common set of concepts, C, used in both games, containing sixty synsets selected from all BabelNet synsets with at least fifty associated images. Using the same set of synsets, separate datasets were created for the two validation tasks. In each dataset, a concept c ∈ C is associated with two sets: a set Vc containing items to validate, and a set Nc with examples of true negative items (i.e., items where the relation to c does not hold). We use th</context>
</contexts>
<marker>Suchanek, Kasneci, Weikum, 2007</marker>
<rawString>Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. YAGO: A core of semantic knowledge. unifying WordNet and Wikipedia. In Proceedings of the 16th World Wide Web Conference, Banff, Canada, 8–12 May 2007, pages 697–706.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thaler</author>
<author>Elena Paslaru Bontas Simperl</author>
<author>Katharina Siorpaes</author>
</authors>
<title>SpotTheLink: A Game for Ontology Alignment.</title>
<date>2011</date>
<booktitle>In Proceedings of the 6th Conference on Professional Knowledge Management: From Knowledge to Action,</booktitle>
<pages>246--253</pages>
<contexts>
<context position="7836" citStr="Thaler et al., 2011" startWordPosition="1187" endWordPosition="1190">ll three games, two players play the same game under time limits and then are rewarded if their answers match. Last, three two-player games have focused on validating and extending knowledge bases. Rzeniewicz and Szyma´nski (2013) extend WordNet with common-sense knowledge using a 20 Questions-like game. In a rapid-play style game, OntoPronto attempts to classify Wikipedia pages as either categories or individuals (Siorpaes and Hepp, 2008a). SpotTheLink uses a similar rapid question format to have players align the DBpedia and PROTON ontologies by agreeing on the distinctions between classes (Thaler et al., 2011). Unlike dynamic gaming elements common in our video games, the above games are all focused on interacting with textual items. Another major limitation is their need for always having two players, which requires them to sustain enough interest to always maintain an active pool of players. While the computer can potentially act as a second player, such a simulated player is often limited to using preexisting knowledge or responses, which makes it difficult to validate new types of entities or create novel answers. In contrast, we drop this requirement thanks to a new strategy for assigning conf</context>
</contexts>
<marker>Thaler, Simperl, Siorpaes, 2011</marker>
<rawString>Stefan Thaler, Elena Paslaru Bontas Simperl, and Katharina Siorpaes. 2011. SpotTheLink: A Game for Ontology Alignment. In Proceedings of the 6th Conference on Professional Knowledge Management: From Knowledge to Action, pages 246– 253.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Torralba</author>
<author>Robert Fergus</author>
<author>William T Freeman</author>
</authors>
<title>80 million tiny images: A large data set for nonparametric object and scene recognition.</title>
<date>2008</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>30</volume>
<issue>11</issue>
<contexts>
<context position="19985" citStr="Torralba et al. (2008)" startWordPosition="3185" endWordPosition="3188">rding images from N. Annotation Players annotate by deciding which images to keep in their inventory. Images receive positive rating annotations from: (1) depositing the image in a reward chest, and (2) ending the level with the image still in the inventory. Conversely, images receive a negative rating when a player (1) views the image but intentionally avoids picking it up or (2) drops the image from her inventory. TKT is designed to assist in the validation and extension of automatically-created image libraries that link to semantic concepts, such as ImageNet (Deng et al., 2009) and that of Torralba et al. (2008). However, its general design allows for other types of annotations, such as image labeling, by changing the tower’s instructions and pictures. Quality Enforcement Mechanisms Similar to Infection, TKT includes analogous mechanisms for limiting adversarial player annotations. Players who collect no images are prevented from entering the boss room, limiting their ability to generate false negative annotations. Similarly, players who collect all images are likely to have half of their images from N and therefore fail the tower’s quality-check after defeating the boss. 6 Experiments Two experiment</context>
</contexts>
<marker>Torralba, Fergus, Freeman, 2008</marker>
<rawString>Antonio Torralba, Robert Fergus, and William T Freeman. 2008. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(11):1958–1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noortje J Venhuizen</author>
<author>Valerio Basile</author>
<author>Kilian Evang</author>
<author>Johan Bos</author>
</authors>
<title>Gamification for word sense labeling.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Conference on Computational Semantics (IWCS).</booktitle>
<contexts>
<context position="5072" citStr="Venhuizen et al., 2013" startWordPosition="739" endWordPosition="742">te for free, thereby significantly lowering annotation costs below that of crowdsourcing. Third, for both games, we show that games produce better quality annotations than crowdsourcing. 2 Related Work Multiple works have proposed linguistic annotation-based games with a purpose for tasks such as anaphora resolution (Hladk´a et al., 2009; Poesio et al., 2013), paraphrasing (Chklovski and Gil, 2005), term associations (Artignan et al., 2009; Lafourcade and Joubert, 2010), query expansion (Simko et al., 2011), and word sense disambiguation (Chklovski and Mihalcea, 2002; Seemakurty et al., 2010; Venhuizen et al., 2013). Notably, all of these linguistic games focus on users interacting with text, in contrast to other highly successful games with a purpose in other domains, such as Foldit (Cooper et al., 2010), in which players fold protein sequences, and the ESP game (von Ahn and Dabbish, 2004), where players label images with words. Most similar to our work are games that create or validate common sense knowledge. Two games with a purpose have incorporated video gamelike mechanics for annotation. First, Herda˘gdelen and Baroni (2012) validate automatically acquired common sense relations using a slot machin</context>
</contexts>
<marker>Venhuizen, Basile, Evang, Bos, 2013</marker>
<rawString>Noortje J. Venhuizen, Valerio Basile, Kilian Evang, and Johan Bos. 2013. Gamification for word sense labeling. In Proceedings of the International Conference on Computational Semantics (IWCS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vickrey</author>
<author>Aaron Bronzan</author>
<author>William Choi</author>
<author>Aman Kumar</author>
<author>Jason Turner-Maier</author>
<author>Arthur Wang</author>
<author>Daphne Koller</author>
</authors>
<title>Online word games for semantic data collection.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>533--542</pages>
<contexts>
<context position="6796" citStr="Vickrey et al. (2008)" startWordPosition="1019" endWordPosition="1022">he most video game-like, the annotation task is a chore the player must perform in order to return to the game, rather than an integrated, fun part of the game’s objectives, which potentially decreases motivation for answering correctly. Several works have proposed adapting existing word-based board game designs to create or validate common sense knowledge. von Ahn et al. (2006) generate common sense facts by using a game similar to TabooTM, where one player must list facts about a computer-selected lemma and a second player must guess the original lemma having seen only the facts. Similarly, Vickrey et al. (2008) gather free associations to a target word with the constraint, similar to TabooTM, where players cannot enter a small set of banned words. Vickrey et al. (2008) also present two games similar to the ScattergoriesTM, where players are given a category and then must list things in that category. The two variants differ in the constraints imposed on the players, such as beginning all items with a specific letter. For all three games, two players play the same game under time limits and then are rewarded if their answers match. Last, three two-player games have focused on validating and extending</context>
</contexts>
<marker>Vickrey, Bronzan, Choi, Kumar, Turner-Maier, Wang, Koller, 2008</marker>
<rawString>David Vickrey, Aaron Bronzan, William Choi, Aman Kumar, Jason Turner-Maier, Arthur Wang, and Daphne Koller. 2008. Online word games for semantic data collection. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 533–542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labeling images with a computer game.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Human Factors in Computing Systems (CHI),</booktitle>
<pages>319--326</pages>
<marker>von Ahn, Dabbish, 2004</marker>
<rawString>Luis von Ahn and Laura Dabbish. 2004. Labeling images with a computer game. In Proceedings of the Conference on Human Factors in Computing Systems (CHI), pages 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis von Ahn</author>
<author>Mihir Kedia</author>
<author>Manuel Blum</author>
</authors>
<title>Verbosity: a game for collecting common-sense facts.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Human Factors in Computing Systems (CHI),</booktitle>
<pages>75--78</pages>
<marker>von Ahn, Kedia, Blum, 2006</marker>
<rawString>Luis von Ahn, Mihir Kedia, and Manuel Blum. 2006. Verbosity: a game for collecting common-sense facts. In Proceedings of the Conference on Human Factors in Computing Systems (CHI), pages 75–78.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>