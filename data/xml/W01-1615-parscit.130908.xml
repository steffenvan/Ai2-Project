<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.957125">
Integration of Referential Scope Limitations
into Japanese Pronoun Resolution
</title>
<author confidence="0.528378">
Michael Paul and Eiichiro Sumita
</author>
<address confidence="0.768394333333333">
ATR Spoken Language Translation Research Laboratories
2-2 Hikaridai, Seika-cho, Soraku-gun,
619-0288 Kyoto, Japan
</address>
<email confidence="0.402525">
paul@slt atr . co . jp, sumita@slt atr . co . jp
</email>
<sectionHeader confidence="0.979246" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999884647058824">
We propose a practical approach to
the anaphora resolution of Japanese
pronouns incorporating knowledge
about referential scope limitations
extracted from an annotated corpus.
A machine learning approach (deci-
sion tree) is utilized for the classi-
fication of the coreference relation
of a given anaphor and antecedent
candidates. The resolution scope of
each pronoun is limited according to
the relative distance distribution of
the training data, resulting in in-
creases in the classification accuracy
and analysis speed by causing only a
minor decrease in the recall perfor-
mance.
</bodyText>
<sectionHeader confidence="0.998737" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999742">
Various approaches have been proposed for
anaphora resolution, like rule-based (Mu-
rata and Nagao, 1997) and machine learning
(Aone and Bennett, 1995) approaches. These
approaches select the most salient candi-
date from among previously mentioned noun
phrases (history). A problem with these sys-
tems is that the resolution costs increase in
proportion to the history size. This problem
becomes especially serious in the analysis of
long conversations like in dialog understand-
ing or spoken language translation systems.
Most of the candidates in the history, how-
ever, are non-referential. This means that it
might not be necessary to analyze the com-
plete history. This paper focuses on the ques-
tion of how far do we have to look back in his-
tory to find the antecedent of a specific refer-
ential expression? We propose a classification
scheme that limits the scope of the resolution
analysis according to the distribution of the
relative distances between anaphora and an-
tecedents tagged in the training corpus.
Our anaphora resolution&apos; is carried out us-
ing a machine learning approach. A deci-
sion tree classifier is trained on the anno-
tated corpus described in Section 2 and de-
termines the coreferential relationship of the
given anaphor-candidate pairs (cf. Section 3).
In the general framework, the decision tree is
applied to all of the noun phrases preceding
the anaphoric expression in the history. Its
system performance is utilized for a baseline
comparison to the practical resolution scheme
proposed in this paper (cf. Section 5).
An investigation into the statistics of the
training corpus (cf. Section 4) reveals quite
different characteristics concerning the refer-
ential scope of specific anaphoric expressions
capable of being exploited not only to de-
crease the costs of the resolution process, but
also to increase the accuracy of the decision
tree classifier.
The proposed approach carries out the clas-
sification of coreferential relationships and
does not select a single candidate as the an-
tecedent of a given anaphor. However, the
decision tree classifier can be seen as a
fil-
ter that reduces noise, i.e., the elimination
of non-referential candidates, for a succes-
sive preference selection scheme, e.g., that in
(Kameyama, 1997).
</bodyText>
<footnote confidence="0.9986915">
1For the analysis of coreferential relationships we
utilize the framework introduced in (Paul et al., 1999).
</footnote>
<sectionHeader confidence="0.467152" genericHeader="method">
2 Tagged corpus
</sectionHeader>
<bodyText confidence="0.999784866666667">
For our experiments we use the ATR-ITL
Speech and Language Database (Takezawa et
al., 1998) consisting of 500 Japanese spoken-
language dialogs annotated with coreferential
tags. The anaphoric expressions used in our
experiments (described in Section 5) are lim-
ited to pronominal ones referring to nominal
antecedents (637 pronouns). We also include
morphosyntactic information like stem forms
as well as semantic codes (Olino and Haman-
id-a, 1981) for content words in this corpus.
In the example dialog between a clerk (r)
and a customer (c) listed in Figure 1, noun
phrases (candidates) are underlined and pro-
nouns (anaphora) are marked with a box.
</bodyText>
<figure confidence="0.881357666666667">
rl: VIV)-9--)7.7,-9--)7.7,Trn■t 1-,
[Circus Circus] [be]
&amp;quot;Thank you for calling Circus Circus.&amp;quot;
ci: 117-1J-:x)4ArIJODO)KffrL
[here] [Los Angeles] [stay] [Kazuo Suzuki][be called]
&amp;quot;Hello, my name is Kazuo Suzuki. I&apos;m staying in Los Angeles right now.&amp;quot;
c2:
[September,20th][family][LasVegas] [travel] [plan]
&amp;quot;We are planning to visit Las Vegas on the 20th of September.&amp;quot;
c3: CVS)7:2.1ottpr.*NOV_A05,5)fOLLKO)LA.,--el-c
[there] [casino] [others][family][enjoy] [place] [hear]
&amp;quot;I heard there are other family attractions besides the casinos.&amp;quot;
r2: 4)t) &lt; t — Of &apos;&apos;&apos;V)t L &amp;quot;
[yes][we] [free] [circus] [have]
&amp;quot;Yes, the circus is free of charge.&amp;quot;
4i0ALiflif*NtA,TV,4)/uT4)tzti-c
[guest] [family][all] [enjoy]
&amp;quot;You can enjoy it together with your family.&amp;quot;
c4: R1&apos;1.1 t I, &amp;quot;ett+ et0-ArYttc
[that] [interesting] [children][be glad]
&amp;quot;That sounds interesting. The kids will be delighted.&amp;quot;
</figure>
<figureCaption confidence="0.999989">
Figure 1: Example dialog
</figureCaption>
<bodyText confidence="0.9615435">
According to the tagging guidelines used
for our corpus, an anaphoric tag refers to the
most recent antecedent found in the dialog,
but this antecedent might also refer to a pre-
vious one. Therefore, the transitive closure
between the anaphora and the first mention of
the antecedent in the history defines the set of
positive examples, whereas the nominal can-
didates outside the transitive closure are con-
sidered negative examples for coreferential re-
lationships. In our example, the proper noun
(c2)&amp;quot; [Las Vegas]&amp;quot; is tagged as the
antecedent of the pronoun (c3)&amp;quot;-- 6 [there]&amp;quot; .
On the other hand, the anaphor-candidate
pair {(c3)&amp;quot;-- 6 [there]&amp;quot;, (r1)&amp;quot;-It —
[Circus Circus]&amp;quot; is not corefential, and there-
fore forms a negative example.
The difficulty of our task can be verified ac-
cording to the average number of antecedent
candidates, i.e., the sum of positive and neg-
ative examples, for a given pronoun. In our
corpus, the average number is 36.7.
</bodyText>
<sectionHeader confidence="0.8366" genericHeader="method">
3 Coreference analysis
</sectionHeader>
<bodyText confidence="0.999965133333333">
For the experiments described in Section 5,
we utilize a trainable resolution approach us-
ing shallow information, i.e., syntactic and se-
mantic word attributes as well as primitive
discourse information extracted from a mor-
phological analysis of the input.
To learn the coreferential relationships from
our corpus, we use the C5.0 machine learn-
ing algorithm (Quinlan, 2000). The set of at-
tributes employed for the decision tree learn-
ing consists of discrete and continuous val-
ues extracted from the training corpus. Two
decision tree classes are used to determine
whether there is a coreferential relationship
(class: coren or not (class: no-rel).
</bodyText>
<subsectionHeader confidence="0.999392">
3.1 Training attributes
</subsectionHeader>
<bodyText confidence="0.9999145">
For the learning of the decision tree we distin-
guish attributes by the stem forms of content
words, their semantic classifications, and their
parts-of-speech as illustrated in Table 1.
</bodyText>
<tableCaption confidence="0.999301">
Table 1: Training attributes
</tableCaption>
<table confidence="0.99721">
category sample
content word: ..... t 6 ,, , ..15.- &lt; ,,
semantic code {name}, {shop}
part-of-speech N r,IT [pronoun]
*it A [common noun]
*ON [verb]
functional word: particle conjunction
conjugation Lqt )) , LL )) ; LL L&amp;quot;, LLA,3 ))
&amp;quot; la: li \ &amp;quot; , &amp;quot; 41, 6 )&gt;,
discourse: distance (continuous values)
count (continuous values)
</table>
<bodyText confidence="0.994865291666667">
Moreover, we use information about syn-
tactical markers like particles or sentence con-
junctions as well as primitive discourse infor-
mation about distances and numbers of oc-
currences for the determination of coreferen-
tial relationships.
content word
For the resolution of pronouns, we check
not only which anaphoric expressions are
involved, but also the existence of other
content words, like sentence predicates, for
the respective input sentences.
semantic code
For the semantic classification of content
words, we use the Ruigo-Shin-Jiten, a
three-layered semantic hierarchy (Oltno and
Hamanislti, 1981). The top two layers are
utilized; they distinguish 100 classes.
part-of-speech
We distinguish 33 parts-of-speech for verbs
(e.g., *Wig, NAVA,trIT-ig), nominal ex-
pressions (e.g.,tr&apos;3.11-g, frl-g), adjectives
(e.g., TF%&apos;-`611g, RE), and functional words
(e.g., 4Wilg, NJ).
functional word
In Japanese, the grammatical role of spe-
cific content words is marked according to
particles succeeding the expression. We
distinguish case particles (e.g., tt ,
), conjunction particles (e.g., , A&apos;3), and
adverbial particles (e.g., h&amp;quot;, ). More-
over, the existence of specific conjunctions
(e.g., 7::r bc , ) and the conjugation form
of the sentence predicate, are verified for the
determination of coreferential relationships.
discourse
We use information concerned with the
number of occurrences of specific content
words and their distances in the discourse.
For the training of the decision tree, we pro-
vide the complete set of attributes described
above. No other coreference indicators are
used in our approach, such as the analysis
of discourse marker or topic and focus in-
formation. This is because these indicators,
which were proposed for previous resolution
systems, require a more sophisticated linguis-
tic analysis of the input data.
</bodyText>
<subsectionHeader confidence="0.999824">
3.2 Learning phase
</subsectionHeader>
<bodyText confidence="0.9999866">
During the iterative analysis of each dialog,
anaphoric expressions are identified according
to the assigned coreference tags. Previously
mentioned nouns are considered as possible
antecedent candidates.
Questions are applied to each anaphor-
candidate pair either by matching specified
expressions in the respective utterances (dis-
crete values) or by calculating attribute values
in the given context (continuous values).
The application of these questions yields a
single attribute vector classifying the charac-
teristics of the given reference. In the case
of antecedents, this vector is assigned to the
coreference class coref, whereas a separate
class no-rd is used for the vectors of non-
referential candidates.
The amount of attribute vectors for all of
the training samples forms the input of the
learning method. By optimizing the entropy
value for each subset, the automatic classi-
fier algorithm produces a decision tree that
ranks important attributes higher in the tree
in order to achieve an early decision about the
classification of the input (Quinlan, 1993).
</bodyText>
<subsectionHeader confidence="0.995862">
3.3 Application phase
</subsectionHeader>
<bodyText confidence="0.996901833333333">
For each anaphoric expression of the test
data, a candidate list, i.e., a list of the nom-
inal candidates preceding the anaphoric ele-
ment in the current discourse, is created. The
decision tree classifier is then successively ap-
plied to all of the anaphor-candidate pairs.
</bodyText>
<figure confidence="0.87607">
(anaphor—candidate attribute vector)
_L
</figure>
<figureCaption confidence="0.999259">
Figure 2: Decision tree classifier
</figureCaption>
<bodyText confidence="0.999976777777778">
Starting with the top node of the decision
tree, the question assigned to this node is
tested against the input, i.e., the respective
anaphor-candidate attribute vector. Depend-
ing on the truth value of the question, the pro-
cedure descends to the respective sub-branch.
The verification procedure is continued until a
leaf containing the classification result (comic
vs. no-rel) is reached (cf. Figure 2).
</bodyText>
<sectionHeader confidence="0.995759" genericHeader="method">
4 Referential scope
</sectionHeader>
<bodyText confidence="0.999676">
An investigation into the distribution of
the relative distances of annotated anaphor-
antecedent pairs in the training corpus shows
quite different characteristics concerning the
referential scope of the respective anaphoric
expressions. Each relative distance is mea-
</bodyText>
<figure confidence="0.9937618">
100
80
20
0
Relative distance (number of candidates)
60
40
100
98 96 94 92 90 88 86 84 82 80
Coverage (%)
</figure>
<bodyText confidence="0.994493333333333">
In order to prove the feasibility of our
approach we compare the following classifi-
cation systems:
</bodyText>
<listItem confidence="0.990577666666667">
• general: a single decision tree classifier
trained on the input samples of all of the
pronouns
• specific: decision tree classifiers (one for
each pronoun) trained on the input sam-
ples of their respective pronoun
</listItem>
<bodyText confidence="0.5271635">
Concerning the analysis scope of the above
systems we distinguish:
</bodyText>
<listItem confidence="0.949771666666667">
• history: all of the candidates preceding
the anaphoric expression
• scope: the candidates within a relative
distance defined as the coverage (in %) of
the distance distribution of the training
samples
</listItem>
<bodyText confidence="0.99979">
The performance of the baseline system
general+history and each specific classifica-
tion system (specific+history) are reported in
Section 5.2 and utilized for a comparison to
those systems with scope limitations, i.e., gen-
eral+scope and specific+scope, described in
Section 5.3.
</bodyText>
<subsectionHeader confidence="0.688296">
5.1 Criteria
</subsectionHeader>
<bodyText confidence="0.968191666666666">
For the evaluation of the system performance
we calculate the resolution costs (i.e., the
number of anaphor-candidate attribute vec-
tors (cases) to which the decision tree is ap-
plied), the accuracy of the decision tree clas-
sifier (i.e., the proportion of correct classified
objects), and the recall of the classification al-
gorithm (i.e., the proportion of annotated an-
tecedents (target cases) that the system iden-
tifies correctly).
Let a denote the number of target cases
classified correctly, b the number of non-
referential cases classified coreferentially, c
the number of target cases classified non-
referentially, and d the number of non-
referential cases classified correctly as illus-
trated below.
classzfication
coref no-rel classified as
a coref
no-rel
annotation
The costs, accuracy, and recall of the sys-
tem are defined as follows:
</bodyText>
<equation confidence="0.9921508">
costs=a+b+c+d
a-Fc
accuracy = a+b+c+d
recall = a
a+ c
</equation>
<bodyText confidence="0.929068722222222">
In the case of a scope limitation
tecedent candidates beyond the limit
classified by the decision tree. However, for
evaluation purposes, we assign the default
class no-rel to all out-of-scope candidates and
modify the evaluation criteria as given below.
out-of-scope classification
no-rel coref no-rel , classified as
e a c coref
f b d no-rel
annotation
Here, e denotes the number of correct an-
tecedents dropped due to the scope limitation
and f is the number of out-of-scope candi-
dates classified correctly by the default class
no-rel. In the case of a scope limitation, the
evaluation measures of the system are defined
as follows:
</bodyText>
<equation confidence="0.9314062">
costs„ope =d+b+c+d
a+ c- F f
accuracy„ope = a+b+c+d+e+ f
scope — ad-acd-e
recall
</equation>
<subsectionHeader confidence="0.926404">
5.2 General framework
</subsectionHeader>
<bodyText confidence="0.9996871">
In order to be able to judge the performance
of the proposed approach, we utilize the gen-
eral framework, i.e., the validation of all can-
didates in the history, for the baseline eval-
uation. In Table 2 we summarize the ac-
curacy and recall for the open test evalua-
tion of the baseline system (general+history)
and the anaphor-specific classification system
(specific+history) trained only on samples of
the respective anaphoric expressions.
</bodyText>
<tableCaption confidence="0.985195">
Table 2: Baseline performance
</tableCaption>
<bodyText confidence="0.825741">
classification accuracy recall
general+history 62.7% 82.1%
specific+history –7.5% –5.7%
The baseline accuracy is 62.7% and its recall
is 82.1%. However, the application of the spe-
cific classification schemes to all candidates in
</bodyText>
<figure confidence="0.996940913793104">
all an-
are not
100
90
80
Performance (%)
70
60
50
100
40
90
30
80
70
20
none
100 99 98 97 96 95 94 93 92 91 90 89 88 87 86 85
Coverage (%)
Percentage (%)
60
50
40
30
20
10
100
0
none
85
86
87
88
89
90
91
92
93
94
95
96
97
98
100 99
90
Coverage (%)
80
Performance (%)
70
60
50
40
30
20
none 100 99 98 97
90 89 88 87 86 85
96 95 94 93 92 91
Coverage (%)
</figure>
<bodyText confidence="0.997942538461538">
The misclassification of non-referential can-
didates is less harmful than the omission of
correct antecedents, because there is no recov-
ery from the latter case; non-referential candi-
dates can still be separated from coreferential
ones later on using saliency-based selection or
similar schemes.
Therefore, we focus on the regression of the
system recall for the selection of the opti-
mal system parameter. In Table 3, we use
a threshold (5%) for the maximal recall de-
crease of each classification system towards its
history results that we are willing to accept.
</bodyText>
<tableCaption confidence="0.999305">
Table 3: Effect of scope limitation
</tableCaption>
<table confidence="0.895114125">
classification (coverage) costs accuracy recall
general+ scope (95%) -7.7% +1.3% -4.9%
[here] (95%) -1.7% +0.2% -3.9%
[there] (94%) -3.4% +0.0% -4.2%
[this one] (97%) -78.5% +56.8% -2.5%
[that one] (96%) -71.4% +54.7% -4.6%
z- 0 [this] (91%) -23.5% +9.0% -2.0%
-.. 0 [that] (93%) -57.9% +29.5% -2.5%
</table>
<bodyText confidence="0.999312769230769">
A threshold larger than 5% causes an in-
crease in the cost reduction, but only a small
improvement in the system accuracy that
does not warrant a drop in the recall perfor-
mance anymore. If we do not apply any scope
limitations to the resolution of the anaphora
t 6 [here] and t 6 [there], since no gain in
accuracy can be achieved, there is no cost re-
duction, but we can reduce the recall regres-
sion of the overall system performance. Ta-
ble 4 shows the selected coverage rates for the
limitation of the analysis scope of the specific
decision tree classifiers and its performance.
</bodyText>
<tableCaption confidence="0.98042">
Table 4: Scope limitation
</tableCaption>
<bodyText confidence="0.779845111111111">
z- [here]: none t [there]: none
z- tt [this one]: 97% [that one]: 96%
z- 0 [this]: 91% 0 [that]: 93%
classification costs accuracy recall
specific+ scope -33.2% +17.4% -7.1%
The overall system performance of the clas-
sification scheme specific+ scope is then a cost
reduction of 33.2%, an increase of 17.4% in
accuracy, and a drop of 7.1% in recall.
</bodyText>
<sectionHeader confidence="0.994177" genericHeader="method">
6 Related Research
</sectionHeader>
<bodyText confidence="0.999980185185185">
Most of the resolution systems described in
literature, focus on the selection of a sin-
gle history candidate, whereby the recency of
candidates is frequently utilized as a saliency
measure. However, only a few systems try
to limit the scope of their resolution modules
according to the referential characteristics of
the respective anaphoric expressions.
(Kameyama, 1997) introduces a locality as-
sumption, which restricts the analysis scope
according to the anaphor type.3 However,
these limits are selected arbitrarily by the
author. Moreover, the pronominal anaphora
contained in the evaluation of thirty newspa-
per articles (MITC-6 coreference task) consist
mainly of 3rd person pronouns with infra-
sentential references.
(Ide and Cristea, 2000) analyzes the dis-
course structure of text taken from the MIX
corpus in order to determine domains of refer-
ential accessibility for each referential expres-
sion. The search space is reduced by skipping
subordinated discourse segments. However,
this approach requires an enhanced struc-
tural analysis and does not exploit any upper
boundary for the maximal referential scope of
the respective anaphoric expressions.
</bodyText>
<sectionHeader confidence="0.99564" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999918571428571">
This paper focuses on the incorporation of
referential scope characteristics of anaphora
into a corpus-based classification scheme for
the resolution of Japanese pronouns. The re-
sult of this incorporation is an increase in the
classification accuracy and a decrease in the
analysis costs as shown in Table 5.
</bodyText>
<tableCaption confidence="0.995106">
Table 5: System performance
</tableCaption>
<table confidence="0.9343214">
classification cost reduction accuracy recall
general+history 0.0% 62.7% 82.1%
specific+history 0.0% 55.2% 76.4%
general+scope 7.7% 64.0% 77.2%
specific+ scope 33.2% 80.1% 75.0%
</table>
<bodyText confidence="0.996831480519481">
The accuracy of the baseline system (g en-
eral+history) is 62.7% and its recall is 82.1%.
The usage of anaphor-specific classifiers (spe-
cific +history) results in a lower performance
of 55.2% and 76.4%, respectively, because
the learned referential characteristics of single
anaphora leads to a performance drop when
applied to all candidates in history.
3Unrestricted for proper nouns, 10 sentences for
definite noun phrase references, three sentences for
pronouns, and only the current sentence for reflexives.
With a scope limitation applied to the gen-
eral framework (general+scope), we achieve
an accuracy of 64.0%, a recall of 77.2%, and a
cost reduction of 7.7%. However, the largest
improvement in the overall system perfor-
mance resulting in an accuracy of 80.1%, a
recall of 75.0%, and a cost reduction of 33.2%,
is achieved by the specific+scope approach,
i.e., the utilization of anaphor-specific classi-
fication systems in combination with analysis
scope limitation according to the coverage of
the relative distance distribution of the train-
ing data.
Large differences in the feasibility of
this approach can be seen for the various
anaphoric expressions. An investigation into
the relative distance distribution of annotated
anaphor-antecedent pairs in the training cor-
pus revealed an even distribution with a large
referential scope for the pronouns := t [here]
and t [there]. Therefore, almost no ef-
fect could be achieved through the limitation
of the analysis scope, i.e., the validation of
the complete history is required in order to
achieve a high system performance for the res-
olution of these anaphoric expressions.
On the other hand, a drastic increase of
around 55% in accuracy in combination with
a high system recall of 90% (and more) could
be achieved for the demonstratives := 4&apos;1, [this
one] (accuracy: 86.5%, recall: 94.9%) and
2(1, [that one] (accuracy: 89.8%, recall: 88.5%)
due to a majority of short-ranged references.
The application of the scope limitation also
resulted in a high accuracy of over 75% and
a small decrease in the recall of 2% for the
determiners := [this] (accuracy: 74.5%, re-
call: 42.0%) and [that] (accuracy: 80.4%,
recall: 74%).
The system proposed in this paper does not
select a single candidate as the antecedent of
the anaphoric expression to be resolved, but
the high accuracy rates of the system enable
a large restriction of the search space, i.e., an
identification of around 80% of non-referential
candidates, for selection schemes using some
kinds of preference measures for the determi-
nation of the most salient candidate.
A problem with the current system is the
large number (around 20%) of correct an-
tecedents classified as non-referential. One
reason for this misclassification is an insuf-
ficient amount of training data. We used dif-
ferent numbers of training dialogs (50-400 di-
alogs) for the training of the decision tree.
The steadily increasing performance results
implied a lack of training data for the identifi-
cation of potential candidates. Currently, we
are extending our corpus and we expect that
a larger number of coreferential variants will
lead to an improvement of the system recall.
Moreover, investigations into the feasibil-
ity of our approach for languages other than
Japanese, e.g. the Englicit MIX corpus, will
enable us to compare this approach more pre-
cisely towards related research.
</bodyText>
<sectionHeader confidence="0.998523" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999799533333333">
C. Aone and S. Bennett. 1995. Evaluating Au-
tomated and Manual Acquisition of Anaphora
Resolution Strategies. In Proc. of the 33th
ACL, pages 122-129.
N. Ide and D. Cristea. 2000. A Hierarchical Ac-
count of Referential Accessibility. In Proc. of
the 38th ACL, pages 416-424, Hong Kong.
M. Kameyama. 1997. Recognizing Referential
Links: An Information Extraction Perspective.
In Proc. of the 36th ACL, Workshop Opera-
tional factors in practical, robust, anaphora res-
olution for unrestricted texts, pages 46-53.
M. Murata and N. Nagao. 1997. An Estimate
of Referents of Pronouns in Japanese Sentences
using Examples and Surface Expressions. Jour-
nal of NLP, 4(1):87-110.
S. Ohno and M. Hamanishi. 1981. Ruigo-Shin-
Jiten. Kadokawa.
M. Paul, K. Yamamoto, and E. Sumita. 1999.
Corpus-Based Anaphora Resolution Towards
Antecedent Preference. In Proc. of The 37th
ACL, Workshop Coreference and Its Applica-
tions, pages 47-52, Maryland, USA.
J. Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
J. Quinlan. 2000. C50. http://rulequest.com/.
T. Takezawa, T. Morimoto, and Y. Sagisaka.
1998. Speech and language database for speech
translation research in ATR. In Proc. of Ori-
ental COCOSDA Workshop&apos;98, pages 148-155.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.764806">
<title confidence="0.9968055">Integration of Referential Scope into Japanese Pronoun Resolution</title>
<author confidence="0.999541">Michael Paul</author>
<author confidence="0.999541">Eiichiro Sumita</author>
<affiliation confidence="0.995948">ATR Spoken Language Translation Research</affiliation>
<address confidence="0.947388">2-2 Hikaridai, Seika-cho, 619-0288 Kyoto,</address>
<email confidence="0.887511">paul@sltatr.co.jp,sumita@sltatr.co.jp</email>
<abstract confidence="0.997234833333333">a practical approach to the anaphora resolution of Japanese pronouns incorporating knowledge about referential scope limitations extracted from an annotated corpus. A machine learning approach (decision tree) is utilized for the classification of the coreference relation of a given anaphor and antecedent candidates. The resolution scope of each pronoun is limited according to the relative distance distribution of the training data, resulting in increases in the classification accuracy and analysis speed by causing only a minor decrease in the recall performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Aone</author>
<author>S Bennett</author>
</authors>
<title>Evaluating Automated and Manual Acquisition of Anaphora Resolution Strategies.</title>
<date>1995</date>
<booktitle>In Proc. of the 33th ACL,</booktitle>
<pages>122--129</pages>
<contexts>
<context position="1027" citStr="Aone and Bennett, 1995" startWordPosition="147" endWordPosition="150"> scope limitations extracted from an annotated corpus. A machine learning approach (decision tree) is utilized for the classification of the coreference relation of a given anaphor and antecedent candidates. The resolution scope of each pronoun is limited according to the relative distance distribution of the training data, resulting in increases in the classification accuracy and analysis speed by causing only a minor decrease in the recall performance. 1 Introduction Various approaches have been proposed for anaphora resolution, like rule-based (Murata and Nagao, 1997) and machine learning (Aone and Bennett, 1995) approaches. These approaches select the most salient candidate from among previously mentioned noun phrases (history). A problem with these systems is that the resolution costs increase in proportion to the history size. This problem becomes especially serious in the analysis of long conversations like in dialog understanding or spoken language translation systems. Most of the candidates in the history, however, are non-referential. This means that it might not be necessary to analyze the complete history. This paper focuses on the question of how far do we have to look back in history to fin</context>
</contexts>
<marker>Aone, Bennett, 1995</marker>
<rawString>C. Aone and S. Bennett. 1995. Evaluating Automated and Manual Acquisition of Anaphora Resolution Strategies. In Proc. of the 33th ACL, pages 122-129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>D Cristea</author>
</authors>
<title>A Hierarchical Account of Referential Accessibility.</title>
<date>2000</date>
<booktitle>In Proc. of the 38th ACL,</booktitle>
<pages>416--424</pages>
<location>Hong Kong.</location>
<contexts>
<context position="17245" citStr="Ide and Cristea, 2000" startWordPosition="2725" endWordPosition="2728">cency of candidates is frequently utilized as a saliency measure. However, only a few systems try to limit the scope of their resolution modules according to the referential characteristics of the respective anaphoric expressions. (Kameyama, 1997) introduces a locality assumption, which restricts the analysis scope according to the anaphor type.3 However, these limits are selected arbitrarily by the author. Moreover, the pronominal anaphora contained in the evaluation of thirty newspaper articles (MITC-6 coreference task) consist mainly of 3rd person pronouns with infrasentential references. (Ide and Cristea, 2000) analyzes the discourse structure of text taken from the MIX corpus in order to determine domains of referential accessibility for each referential expression. The search space is reduced by skipping subordinated discourse segments. However, this approach requires an enhanced structural analysis and does not exploit any upper boundary for the maximal referential scope of the respective anaphoric expressions. 7 Conclusion This paper focuses on the incorporation of referential scope characteristics of anaphora into a corpus-based classification scheme for the resolution of Japanese pronouns. The</context>
</contexts>
<marker>Ide, Cristea, 2000</marker>
<rawString>N. Ide and D. Cristea. 2000. A Hierarchical Account of Referential Accessibility. In Proc. of the 38th ACL, pages 416-424, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kameyama</author>
</authors>
<title>Recognizing Referential Links: An Information Extraction Perspective.</title>
<date>1997</date>
<booktitle>In Proc. of the 36th ACL, Workshop Operational</booktitle>
<pages>46--53</pages>
<contexts>
<context position="3122" citStr="Kameyama, 1997" startWordPosition="484" endWordPosition="485">fferent characteristics concerning the referential scope of specific anaphoric expressions capable of being exploited not only to decrease the costs of the resolution process, but also to increase the accuracy of the decision tree classifier. The proposed approach carries out the classification of coreferential relationships and does not select a single candidate as the antecedent of a given anaphor. However, the decision tree classifier can be seen as a filter that reduces noise, i.e., the elimination of non-referential candidates, for a successive preference selection scheme, e.g., that in (Kameyama, 1997). 1For the analysis of coreferential relationships we utilize the framework introduced in (Paul et al., 1999). 2 Tagged corpus For our experiments we use the ATR-ITL Speech and Language Database (Takezawa et al., 1998) consisting of 500 Japanese spokenlanguage dialogs annotated with coreferential tags. The anaphoric expressions used in our experiments (described in Section 5) are limited to pronominal ones referring to nominal antecedents (637 pronouns). We also include morphosyntactic information like stem forms as well as semantic codes (Olino and Hamanid-a, 1981) for content words in this c</context>
<context position="16870" citStr="Kameyama, 1997" startWordPosition="2673" endWordPosition="2674">curacy recall specific+ scope -33.2% +17.4% -7.1% The overall system performance of the classification scheme specific+ scope is then a cost reduction of 33.2%, an increase of 17.4% in accuracy, and a drop of 7.1% in recall. 6 Related Research Most of the resolution systems described in literature, focus on the selection of a single history candidate, whereby the recency of candidates is frequently utilized as a saliency measure. However, only a few systems try to limit the scope of their resolution modules according to the referential characteristics of the respective anaphoric expressions. (Kameyama, 1997) introduces a locality assumption, which restricts the analysis scope according to the anaphor type.3 However, these limits are selected arbitrarily by the author. Moreover, the pronominal anaphora contained in the evaluation of thirty newspaper articles (MITC-6 coreference task) consist mainly of 3rd person pronouns with infrasentential references. (Ide and Cristea, 2000) analyzes the discourse structure of text taken from the MIX corpus in order to determine domains of referential accessibility for each referential expression. The search space is reduced by skipping subordinated discourse se</context>
</contexts>
<marker>Kameyama, 1997</marker>
<rawString>M. Kameyama. 1997. Recognizing Referential Links: An Information Extraction Perspective. In Proc. of the 36th ACL, Workshop Operational factors in practical, robust, anaphora resolution for unrestricted texts, pages 46-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Murata</author>
<author>N Nagao</author>
</authors>
<title>An Estimate of Referents of Pronouns in Japanese Sentences using Examples and Surface Expressions.</title>
<date>1997</date>
<journal>Journal of NLP,</journal>
<pages>4--1</pages>
<contexts>
<context position="981" citStr="Murata and Nagao, 1997" startWordPosition="139" endWordPosition="143">ouns incorporating knowledge about referential scope limitations extracted from an annotated corpus. A machine learning approach (decision tree) is utilized for the classification of the coreference relation of a given anaphor and antecedent candidates. The resolution scope of each pronoun is limited according to the relative distance distribution of the training data, resulting in increases in the classification accuracy and analysis speed by causing only a minor decrease in the recall performance. 1 Introduction Various approaches have been proposed for anaphora resolution, like rule-based (Murata and Nagao, 1997) and machine learning (Aone and Bennett, 1995) approaches. These approaches select the most salient candidate from among previously mentioned noun phrases (history). A problem with these systems is that the resolution costs increase in proportion to the history size. This problem becomes especially serious in the analysis of long conversations like in dialog understanding or spoken language translation systems. Most of the candidates in the history, however, are non-referential. This means that it might not be necessary to analyze the complete history. This paper focuses on the question of how</context>
</contexts>
<marker>Murata, Nagao, 1997</marker>
<rawString>M. Murata and N. Nagao. 1997. An Estimate of Referents of Pronouns in Japanese Sentences using Examples and Surface Expressions. Journal of NLP, 4(1):87-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ohno</author>
<author>M Hamanishi</author>
</authors>
<date>1981</date>
<note>Ruigo-ShinJiten. Kadokawa.</note>
<marker>Ohno, Hamanishi, 1981</marker>
<rawString>S. Ohno and M. Hamanishi. 1981. Ruigo-ShinJiten. Kadokawa.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Paul</author>
<author>K Yamamoto</author>
<author>E Sumita</author>
</authors>
<title>Corpus-Based Anaphora Resolution Towards Antecedent Preference.</title>
<date>1999</date>
<booktitle>In Proc. of The 37th ACL, Workshop Coreference and Its Applications,</booktitle>
<pages>47--52</pages>
<location>Maryland, USA.</location>
<contexts>
<context position="3231" citStr="Paul et al., 1999" startWordPosition="498" endWordPosition="501">g exploited not only to decrease the costs of the resolution process, but also to increase the accuracy of the decision tree classifier. The proposed approach carries out the classification of coreferential relationships and does not select a single candidate as the antecedent of a given anaphor. However, the decision tree classifier can be seen as a filter that reduces noise, i.e., the elimination of non-referential candidates, for a successive preference selection scheme, e.g., that in (Kameyama, 1997). 1For the analysis of coreferential relationships we utilize the framework introduced in (Paul et al., 1999). 2 Tagged corpus For our experiments we use the ATR-ITL Speech and Language Database (Takezawa et al., 1998) consisting of 500 Japanese spokenlanguage dialogs annotated with coreferential tags. The anaphoric expressions used in our experiments (described in Section 5) are limited to pronominal ones referring to nominal antecedents (637 pronouns). We also include morphosyntactic information like stem forms as well as semantic codes (Olino and Hamanid-a, 1981) for content words in this corpus. In the example dialog between a clerk (r) and a customer (c) listed in Figure 1, noun phrases (candida</context>
</contexts>
<marker>Paul, Yamamoto, Sumita, 1999</marker>
<rawString>M. Paul, K. Yamamoto, and E. Sumita. 1999. Corpus-Based Anaphora Resolution Towards Antecedent Preference. In Proc. of The 37th ACL, Workshop Coreference and Its Applications, pages 47-52, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="9905" citStr="Quinlan, 1993" startWordPosition="1518" endWordPosition="1519"> yields a single attribute vector classifying the characteristics of the given reference. In the case of antecedents, this vector is assigned to the coreference class coref, whereas a separate class no-rd is used for the vectors of nonreferential candidates. The amount of attribute vectors for all of the training samples forms the input of the learning method. By optimizing the entropy value for each subset, the automatic classifier algorithm produces a decision tree that ranks important attributes higher in the tree in order to achieve an early decision about the classification of the input (Quinlan, 1993). 3.3 Application phase For each anaphoric expression of the test data, a candidate list, i.e., a list of the nominal candidates preceding the anaphoric element in the current discourse, is created. The decision tree classifier is then successively applied to all of the anaphor-candidate pairs. (anaphor—candidate attribute vector) _L Figure 2: Decision tree classifier Starting with the top node of the decision tree, the question assigned to this node is tested against the input, i.e., the respective anaphor-candidate attribute vector. Depending on the truth value of the question, the procedure</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Quinlan</author>
</authors>
<title>Speech and language database for speech translation research in ATR.</title>
<date>2000</date>
<booktitle>In Proc. of Oriental COCOSDA Workshop&apos;98,</booktitle>
<pages>148--155</pages>
<contexts>
<context position="6141" citStr="Quinlan, 2000" startWordPosition="944" endWordPosition="945">ple. The difficulty of our task can be verified according to the average number of antecedent candidates, i.e., the sum of positive and negative examples, for a given pronoun. In our corpus, the average number is 36.7. 3 Coreference analysis For the experiments described in Section 5, we utilize a trainable resolution approach using shallow information, i.e., syntactic and semantic word attributes as well as primitive discourse information extracted from a morphological analysis of the input. To learn the coreferential relationships from our corpus, we use the C5.0 machine learning algorithm (Quinlan, 2000). The set of attributes employed for the decision tree learning consists of discrete and continuous values extracted from the training corpus. Two decision tree classes are used to determine whether there is a coreferential relationship (class: coren or not (class: no-rel). 3.1 Training attributes For the learning of the decision tree we distinguish attributes by the stem forms of content words, their semantic classifications, and their parts-of-speech as illustrated in Table 1. Table 1: Training attributes category sample content word: ..... t 6 ,, , ..15.- &lt; ,, semantic code {name}, {shop} p</context>
</contexts>
<marker>Quinlan, 2000</marker>
<rawString>J. Quinlan. 2000. C50. http://rulequest.com/. T. Takezawa, T. Morimoto, and Y. Sagisaka. 1998. Speech and language database for speech translation research in ATR. In Proc. of Oriental COCOSDA Workshop&apos;98, pages 148-155.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>