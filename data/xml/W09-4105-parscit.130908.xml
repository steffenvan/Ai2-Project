<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000056">
<title confidence="0.998127">
Cross-lingual Adaptation as a Baseline:
Adapting Maximum Entropy Models to Bulgarian
</title>
<author confidence="0.823549">
Georgi Georgiev
</author>
<affiliation confidence="0.394627">
Ontotext AD
</affiliation>
<address confidence="0.892308333333333">
135 Tsarigradsko Chaussee
1784, Sofia
Bulgaria
</address>
<email confidence="0.96414">
georgi.georgiev@ontotext.com
</email>
<author confidence="0.67508">
Preslav Nakov
</author>
<affiliation confidence="0.999788">
Department of Computer Science
National University of Singapore
</affiliation>
<address confidence="0.975844">
13 Computing Drive
Singapore 117417
</address>
<email confidence="0.995023">
nakov@comp.nus.edu.sg
</email>
<author confidence="0.77367">
Petya Osenova and Kiril Simov
</author>
<affiliation confidence="0.902276">
Linguistic Modelling Laboratory
Institute for Parallel Processing
Bulgarian Academy of Sciences
</affiliation>
<address confidence="0.664921">
25A Acad. G. Bonchev St., 1113 Sofia, Bulgaria
</address>
<email confidence="0.976805">
{petya,kivs}@bultreebank.org
</email>
<sectionHeader confidence="0.998449" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899714285714">
We describe our efforts in adapting five basic nat-
ural language processing components to Bulgar-
ian: sentence splitter, tokenizer, part-of-speech
tagger, chunker, and syntactic parser. The com-
ponents were originally developed for English
within OpenNLP, an open source maximum en-
tropy based machine learning toolkit, and were
retrained based on manually annotated training
data from the BulTreeBank. The evaluation re-
sults show an F1 score of 92.54% for the sentence
splitter, 98.49% for the tokenizer, 94.43% for
the part-of-speech tagger, 84.60% for the chun-
ker, and 77.56% for the syntactic parser, which
should be interpreted as baseline for Bulgarian.
</bodyText>
<sectionHeader confidence="0.998408" genericHeader="keywords">
Keywords
</sectionHeader>
<keyword confidence="0.993683">
Part-of-speech tagging, syntactic parsing, shallow parsing,
chunking, tokenization, sentence splitting, maximum entropy.
</keyword>
<sectionHeader confidence="0.999815" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999841866666667">
Nowadays, the dominant approach in natural language
processing (NLP) is to acquire linguistic knowledge
using machine learning methods. Other approaches,
e.g., using manual rules, have proven to be both time-
consuming and error-prone. Still, using machine learn-
ing has one major limitation: it requires manually an-
notated corpora as training data, which can be quite
costly to create. Fortunately, for Bulgarian such a rich
resource already exists – the BulTreeBank1, an HPSG-
based Syntactic Treebank with rich annotations at var-
ious linguistic levels. The existence of such a resource
makes it possible to adapt to Bulgarian various NLP
tools that have been originally developed for other lan-
guages, e.g., English, and that have been trained on
similar kinds of resources, e.g., the Penn Treebank [4].
</bodyText>
<footnote confidence="0.911904333333333">
1 Created at the Linguistic Modelling Laboratory (LML), Insti-
tute for Parallel Processing, Bulgarian Academy of Sciences.
See http://www.bultreebank.orgfor details.
</footnote>
<bodyText confidence="0.99974525">
In this paper, we further stipulate that language
adaption should be no harder than domain adapta-
tion [2]. Similarly to Buyko &amp; al. [2], we experiment
with the OpenNLP tools2 since they are open source
and contain several platform-independent Java imple-
mentations of important NLP components. Moreover,
these tools are based on a single machine learning al-
gorithm, maximum entropy (ME) [1], as implemented
in the OpenNLP MaxEnt3 Java package. In our exper-
iments below, we focus on five basic components from
the OpenNLP tools: sentence detection, tokenization,
part-of-speech (Pos) tagging, chunking, and parsing.
Maximum entropy models search for a distribution
p(x∣y) that is consistent with the empirical observa-
tions about a particular feature f(x, y), computed
from a set of training examples T = {x, y}, e.g., a
sentence x and its labeling y; see [6] for details. From
all such distributions, the one with the highest entropy
is chosen [1]. It can be shown that the resulting dis-
tribution will have the following form:
</bodyText>
<equation confidence="0.903117">
p.,,,(y∣x) a exp(w ⋅ f(x, y)) (1)
</equation>
<bodyText confidence="0.9996812">
The features used in the OpenNLP framework com-
bine heterogeneous contextual information such as
words around the end of a sentence for the English
sentence splitter, or word, character n-grams and part-
of-speech tag alone and in various combinations for the
English chunker. These features are based on the pub-
lications of Sha and Pereira [7] for the chunker, and on
the dissertation of Ratnaparkhi [6] for the POS tagger
and the syntactic parser.
The remainder of this paper is organized as follows:
Section 2 describes the process of converting the Bul-
TreeBank XML data to Penn Treebank-style bracket-
ing, Section 3 describes the experiments and discusses
the results, and Section 4 concludes and suggests di-
rections for future work.
</bodyText>
<footnote confidence="0.9993855">
2 http://opennlp.sourceforge.net
3 http://maxent.sourceforge.net
</footnote>
<page confidence="0.994996">
35
</page>
<subsectionHeader confidence="0.348579">
Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 35–38
</subsectionHeader>
<bodyText confidence="0.868050214285714">
2 Converting the BulTreeBank
XML to Penn Treebank-style
bracketing
Converting the BulTreeBank XML [9, 5] to Penn
Treebank-style bracketing format is straightforward,
with some exceptions, for which we define custom
tools. Consider, for example, the following sentence:
Всички на някакъв етап от живота си сме
изправени пред проблеми и предизвикателства .
‘All at some point of life itself we face
to problems and challenges.’
We all at some point of our lives face
problems and challenges.
It has the following XML structure in BulTreeBank:
</bodyText>
<figure confidence="0.999868417910448">
&lt;S&gt;
&lt;VPA&gt;
&lt;VPS&gt;
&lt;Pron&gt;
&lt;w ana=&amp;quot;Pce-op&amp;quot;&gt;Всички&lt;/w&gt;
&lt;/Pron&gt;
&lt;PP&gt;
&lt;Prep&gt;
&lt;w ana=&amp;quot;R&amp;quot;&gt;на&lt;/w&gt;
&lt;/Prep&gt;
&lt;/PP&gt;
&lt;NPA&gt;
&lt;NPA&gt;
&lt;A&gt;
&lt;w ana=&amp;quot;Pfa--s-m&amp;quot;&gt;някакъв&lt;/w&gt;
&lt;/A&gt;
&lt;N&gt;
&lt;w ana=&amp;quot;Ncmsi&amp;quot;&gt;етап&lt;/w&gt;
&lt;/N&gt;
&lt;/NPA&gt;
&lt;PP&gt;
&lt;Prep&gt;
&lt;w ana=&amp;quot;R&amp;quot;&gt;от&lt;/w&gt;
&lt;/Prep&gt;
&lt;N&gt;
&lt;w ana=&amp;quot;Ncmsh&amp;quot;&gt;живота&lt;/w&gt;
&lt;/N&gt;
&lt;Pron idref=&amp;quot;id1&amp;quot;&gt;
&lt;w ana=&amp;quot;Psxto&amp;quot;&gt;си&lt;/w&gt;
&lt;/Pron&gt;
&lt;/PP&gt;
&lt;/NPA&gt;
&lt;/PP&gt;
&lt;VPC&gt;
&lt;V&gt;
&lt;w ana=&amp;quot;Vxitf-r1p&amp;quot;&gt;сме&lt;/w&gt;
&lt;/V&gt;
&lt;Participle&gt;
&lt;w ana=&amp;quot;Vpptcv--p-i&amp;quot;&gt;изправени&lt;/w&gt;
&lt;/Participle&gt;
&lt;PP&gt;
&lt;Prep&gt;
&lt;w ana=&amp;quot;R&amp;quot;&gt;пред&lt;/w&gt;
&lt;/Prep&gt;
&lt;CoordP&gt;
&lt;ConjArg&gt;
&lt;N&gt;
&lt;w ana=&amp;quot;Ncmpi&amp;quot;&gt;проблеми&lt;/w&gt;
&lt;/N&gt;
&lt;/ConjArg&gt;
&lt;Conj&gt;
&lt;C&gt;
&lt;w ana=&amp;quot;Cp&amp;quot;&gt;и&lt;/w&gt;
&lt;/C&gt;
&lt;/Conj&gt;
&lt;ConjArg&gt;
&lt;N&gt;
&lt;w ana=&amp;quot;Ncnpi&amp;quot;&gt;предизвикателства&lt;/w&gt;
&lt;/N&gt;
&lt;/ConjArg&gt;
&lt;/CoordP&gt;
&lt;/PP&gt;
&lt;/VPC&gt;
&lt;/VPS&gt;
&lt;/VPA&gt;
&lt;pt&gt;.&lt;/pt&gt;
&lt;/S&gt;
</figure>
<bodyText confidence="0.449588">
We transform the above XML into the following
Penn Treebank-style bracketing structure:
</bodyText>
<equation confidence="0.866440405405405">
((S
(NP
(PRP Всички)
)
(VP
(PP
(IN на)
(NP
(NP
(PRP някакъв)
(NN етап)
)
(PP
(IN от)
(NN живота)
(PP$ си)
)
)
)
(VP
(VB сме)
(VB изправени)
(PP
(IN пред)
(NP
(NP
(NNS проблеми)
)
(CC и)
(NP
(NNS предизвикателства)
)
)
)
)
)
(. .)
</equation>
<page confidence="0.957914">
36
</page>
<bodyText confidence="0.994195111111111">
In the process of transformation, we further apply
some simple rules for the coordinations in the BulTree-
Bank, e.g., “CoordP” and “ConjArg” typically become
“NP”, and “Conj” becomes a “CC” phrase; see [5] for
further details on the syntactic phrase naming conven-
tions of the BulTreeBank and [4] for those of the Penn
Treebank. We also remove the outer verb phrases in
the BulTreeBank, e.g., the phrases “VPA” and “VPS”
in the above example, as required by the Penn Tree-
bank bracketing structure. We further define specific
rules for pronouns. For example, the “ana” tag in
the BulTreeBank [10] is very important for pronoun
phrases of the following kind:
&lt;Pron&gt;&lt;w ana=&amp;quot;P...&amp;quot;&gt;....
First, in case the fourth position is filled by a “t”
and the tag starts with “Ps” as in “Ps*t*”4, this is a
possessive form and is part of the NP phrase in the
transformation structure. For example:
</bodyText>
<equation confidence="0.948156571428571">
хубавата ми кола
‘beautiful-the my-clitic car’
my beautiful car
Or:
ма$ика ми
‘mother my-clitic’
my mother
</equation>
<bodyText confidence="0.999982571428571">
Second, if the tag does not start with “Ps” then
the pronoun is part of the verb phrase because it is a
personal pronoun.
Finally, if there is no “t” on position four, but there
is “l” or “-” instead, we annotate this as an NP (see
the example above).
We further reduced the original BulTreeBank tagset
[10] to a much smaller one with just 95 tags. In most
cases, this meant losing some of the surface morpho-
logical forms. For example, in the example sentence
above, the word “си” (”our own-clitic”) was origi-
nally annotated with the tag “Psxt” (pronoun, pos-
sessive, reflexive, short form), which was transformed
to “PP$” (pronoun, possessive). Similarly, the last
word in the example sentence, “предизвикателства”
(challenges), had the tag “Ncnpi” (noun, common,
neuter, plural, indefinite), which was collapsed to
“NNS” (noun, plural). In other cases, the tags were
directly transformed, e.g., the word “от” (from) with
tag “R” (preposition) was transformed to the tag “IN”
(preposition or subordinating conjunction).
</bodyText>
<sectionHeader confidence="0.995083" genericHeader="method">
3 Experiments and evaluation
</sectionHeader>
<bodyText confidence="0.999317">
In our experiments, we used the training and the test-
ing sections of the BulTreeBank [5, 9] without further
</bodyText>
<sectionHeader confidence="0.931031" genericHeader="method">
4 Here * stands for a character that is not important for the
discussion.
</sectionHeader>
<bodyText confidence="0.995729666666667">
modifications in order to retrain for Bulgarian the sen-
tence splitter, the tokenizer, the POS tagger, the chun-
ker (shallow parser), and the syntactic parser from the
OpenNLP tools. The results are shown in Table 1.
The sentence splitter achieved an F1 score of 92.54%.
The false positives constituted most of the errors and
appeared in complicated sentences rather than at ab-
breviations of organization names as we expected. For
example, the following chunk was recognized as a sen-
tence:
Ко$и беше този човек?, би запитал то$и. Така че...
Who was that guy?, he would ask. So ...
However the actual sentence should have been:
Ко$и беше този човек?, би запитал то$и.
Who was that guy?, he would ask.
Some errors appeared in sentences annotated with
direct speech, e.g., the sentence tagger annotated the
following piece of text as a sentence:
</bodyText>
<listItem confidence="0.481586333333333">
Наведен напред, То$и впери поглед в мрака
и рече: - Да бъде светлина.
Inclined forward, he took a look in the dark
and said: - Let there be light.
Inclined forward, he stared at the gloom
and said: - Let there be light.
</listItem>
<bodyText confidence="0.99518124137931">
However, the actual sentence in the BulTreeBank
was the following one:
Наведен напред, То$и впери поглед в мрака
и рече:
Inclined forward, he took a look in the dark
and said:
Inclined forward, he stared at the gloom
and said:
The tokenizer achieved an F1 score of 98.49%.
The majority of the errors appeared in sparse abbre-
viations. For example, the abbreviation for kilograms,
“кг.”, was frequently tokenized as “кг”. The abbrevia-
tion for vehicle “horse power”, “к.с.”, was wrongly to-
kenized as “к.с”. Some errors involved words that con-
tained no space, e.g., “предсказание” (prediction),
which were wrongly tokenized as “казание” (an old
Bulgarian word that means “statement”). There were
also some rare errors in names of people and locations,
e.g., the name “Лазаръс” (Lazarus) was tokenized as
the Bulgarian name “Лазар” (Lazar), and the city
“Казанлък” (Kazanlak) was tokenized as “Казан”
(Kazan).
The POS tagger achieved an F1 score of 90.34%
on the full morpho-syntactic tagset of the BulTree-
Bank. We made use of the tagger’s ability to employ
a tag dictionary that has been automatically generated
from the training data and used internally to enumer-
ate over a fixed set of possible tags for each word as op-
posed to allowing all possible tags; this severely limited
</bodyText>
<page confidence="0.998689">
37
</page>
<note confidence="0.4858505">
Sentence splitter Tokenizer POS Tagger Chunker Parser
92.54 98.49 94.43 84.60 77.56
</note>
<tableCaption confidence="0.999685">
Table 1: The F1 scores (in %) of the OpenNLP components discussed in the study.
</tableCaption>
<bodyText confidence="0.999037142857143">
the number of decision options available per word. The
majority of the erroneous morpho-syntactic tags were
not entirely wrong; rather, only some of their compo-
nents were incorrect. For example, “чорбаджиите”
was wrongly annotated as “noun, common, feminine,
plural, definite” while the correct tag was “noun, com-
mon, masculine, plural, definite”.
</bodyText>
<figure confidence="0.6034685">
wrong annotation
‘‘чорбаджиите”(gaffer) with POS=Ncfpd
correct annotation
‘‘чорбаджиите”with POS=Ncmpd
</figure>
<bodyText confidence="0.9845045">
Another common error was the wrong annotation of
proper nouns as common nouns. Here is an example
for the person name “Странджата” (Strandzhata, a
literary character):
</bodyText>
<figure confidence="0.525770363636364">
wrong annotation
‘‘Странджата”with POS=Ncfsd
correct annotation
‘‘Странджата”with POS=Npfsd
Another location example was the word
“Балканът” (Balkanut, i.e., the Balkan moun-
tain):
wrong annotation
‘‘Балканът”with POS=Ncmsd
correct annotation
‘‘Балканът”with POS=Npmsd
</figure>
<bodyText confidence="0.999730461538462">
We further collapsed the original BulTreeBank
tagset, which contained 680 morpho-syntactic tags, to
a much smaller one, with 95 tags only; see Section 2
for details. The POS tagger with that reduced tagset
achieved an F1 score of 94.43% and was used in the ex-
periments on chunking and syntactic parsing described
below. For comparison, the best results for English on
the Penn Treebank are 97.33% [8], but using a different
learning algorithm: bidirectional perceptron.
The chunker achieved an F1 score of 84.60%. This
result is much lower than the best F1 score for English
reported at the CoNLL-2000 chunking competition:
94.13%. However, this comparison should be treated
with caution since we did no special adaptation of the
features to Bulgarian. We should also note that the
ChunkLink5 script, used at CoNLL-2000, was tailored
to the Penn Treebank tagset, and was thus not very
suitable to our collapsed BulTreeBank tagset.
The syntactic parser: Unfortunately, we were
unable to evaluate our parser with the full morpho-
syntactic tagset of the BulTreeBank; this would have
required coding efforts for some parts of speech, e.g.,
nouns, that go beyond simple adaptation. On our col-
lapsed tagset, we achieved an F1 score of 77.56%. For
comparison, the best parser for English that only uses
Penn Treebank data achieves F1=91.4% [3].
</bodyText>
<footnote confidence="0.706037">
5 http://ilk.kub.nl/˜sabine/chunklink
</footnote>
<sectionHeader confidence="0.965512" genericHeader="conclusions">
4 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999967764705882">
We have described our efforts in adapting five ba-
sic NLP components from English to Bulgarian using
manually annotated training data from the BulTree-
Bank. We have further presented the first systematic
evaluation of NLP components for Bulgarian on the
same dataset and within the same machine learning
framework: maximum entropy. The evaluation results
have shown an F1 score of 92.54% for the sentence
splitter, 98.49% for the tokenizer, 94.43% for the part-
of-speech tagger, 84.60% for the chunker, and 77.56%
for the syntactic parser, which should be interpreted
as baseline for Bulgarian.
In future work, we will improve on the above base-
line results, e.g., by adding language-specific features.
We further plan to adapt two other maximum entropy
based OpenNLP components to Bulgarian: for named
entity recognition and for co-reference resolution.
</bodyText>
<sectionHeader confidence="0.998201" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999707">
The work reported in the paper is partially supported
by the EU FP7-212578 project LTfLL – Language
Technologies for Life Long Learning.
</bodyText>
<sectionHeader confidence="0.999112" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999985371428571">
[1] A. Berger, S. D. Pietra, and V. D. Pietra. A maximum entropy
approach to natural language processing. Computational Lin-
guistics, 22:39–71, 1996.
[2] E. Buyko, J. Wermter, M. Poprat, and U. Hahn. Automatically
adapting an NLP core engine to the biology domain. In Joint
BioLINK-Bio-Ontologies Meeting, pages 65–68, 2006.
[3] E. Charniak and M. Johnson. Coarse-to-fine n-best parsing and
maxent discriminative reranking. In ACL’05: Proceedings of
the 43rd Annual Meeting of the Association for Computa-
tional Linguistics, pages 173–180, Ann Arbor, Michigan, June
2005.
[4] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Build-
ing a large annotated corpus of English: the Penn Treebank.
Computational Linguistics, 19, 1993.
[5] P. Osenova and K. Simov. BTB-TR05: BulTreeBank Style-
book. Technical report, BulTreeBank Project Technical Re-
port, 2004.
[6] A. Ratnaparkhi. Maximum Entropy Models for Natural Lan-
guage Ambiguity Resolution. PhD thesis, University of Penn-
sylvania, 1998.
[7] F. Sha and F. Pereira. Shallow parsing with conditional ran-
dom fields. In NAACL’03: Proceedings of the 2003 Confer-
ence of the North American Chapter of the Association for
Computational Linguistics on Human Language Technology,
pages 134–141, 2003.
[8] L. Shen, G. Satta, and A. Joshi. Guided learning for bidi-
rectional sequence classification. In ACL’07: Proceedings of
the 45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 760–767, Prague, Czech Republic,
June 2007.
[9] K. Simov. BTB-TR01: BulTreeBank Project Overview. Tech-
nical report, BulTreeBank Project Technical Report, 2004.
[10] K. Simov, P. Osenova, and M. Slavcheva. BTB-TR03: BulTree-
Bank Morphosyntactic Tagset. Technical report, BulTreeBank
Project Technical Report, 2004.
</reference>
<page confidence="0.999353">
38
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.280905">
<title confidence="0.979718">Cross-lingual Adaptation as a Adapting Maximum Entropy Models to Bulgarian</title>
<author confidence="0.994389">Georgi</author>
<affiliation confidence="0.947721">Ontotext</affiliation>
<address confidence="0.858453">135 Tsarigradsko 1784,</address>
<email confidence="0.867958">Preslav</email>
<affiliation confidence="0.900976666666667">Department of Computer National University of 13 Computing</affiliation>
<address confidence="0.733187">Singapore</address>
<author confidence="0.864557">Petya Osenova</author>
<author confidence="0.864557">Kiril Simov</author>
<affiliation confidence="0.985266333333333">Linguistic Modelling Laboratory Institute for Parallel Processing Bulgarian Academy of Sciences</affiliation>
<address confidence="0.985198">25A Acad. G. Bonchev St., 1113 Sofia, Bulgaria</address>
<abstract confidence="0.999637733333333">We describe our efforts in adapting five basic natural language processing components to Bulgarian: sentence splitter, tokenizer, part-of-speech tagger, chunker, and syntactic parser. The components were originally developed for English within OpenNLP, an open source maximum entropy based machine learning toolkit, and were retrained based on manually annotated training data from the BulTreeBank. The evaluation reshow an of 92.54% for the sentence splitter, 98.49% for the tokenizer, 94.43% for the part-of-speech tagger, 84.60% for the chunker, and 77.56% for the syntactic parser, which should be interpreted as baseline for Bulgarian.</abstract>
<keyword confidence="0.989852333333333">Keywords Part-of-speech tagging, syntactic parsing, shallow parsing, chunking, tokenization, sentence splitting, maximum entropy.</keyword>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S D Pietra</author>
<author>V D Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<contexts>
<context position="2692" citStr="[1]" startWordPosition="384" endWordPosition="384">kinds of resources, e.g., the Penn Treebank [4]. 1 Created at the Linguistic Modelling Laboratory (LML), Institute for Parallel Processing, Bulgarian Academy of Sciences. See http://www.bultreebank.orgfor details. In this paper, we further stipulate that language adaption should be no harder than domain adaptation [2]. Similarly to Buyko &amp; al. [2], we experiment with the OpenNLP tools2 since they are open source and contain several platform-independent Java implementations of important NLP components. Moreover, these tools are based on a single machine learning algorithm, maximum entropy (ME) [1], as implemented in the OpenNLP MaxEnt3 Java package. In our experiments below, we focus on five basic components from the OpenNLP tools: sentence detection, tokenization, part-of-speech (Pos) tagging, chunking, and parsing. Maximum entropy models search for a distribution p(x∣y) that is consistent with the empirical observations about a particular feature f(x, y), computed from a set of training examples T = {x, y}, e.g., a sentence x and its labeling y; see [6] for details. From all such distributions, the one with the highest entropy is chosen [1]. It can be shown that the resulting distrib</context>
</contexts>
<marker>[1]</marker>
<rawString>A. Berger, S. D. Pietra, and V. D. Pietra. A maximum entropy approach to natural language processing. Computational Linguistics, 22:39–71, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Buyko</author>
<author>J Wermter</author>
<author>M Poprat</author>
<author>U Hahn</author>
</authors>
<title>Automatically adapting an NLP core engine to the biology domain.</title>
<date>2006</date>
<booktitle>In Joint BioLINK-Bio-Ontologies Meeting,</booktitle>
<pages>65--68</pages>
<contexts>
<context position="2408" citStr="[2]" startWordPosition="340" endWordPosition="340">n HPSGbased Syntactic Treebank with rich annotations at various linguistic levels. The existence of such a resource makes it possible to adapt to Bulgarian various NLP tools that have been originally developed for other languages, e.g., English, and that have been trained on similar kinds of resources, e.g., the Penn Treebank [4]. 1 Created at the Linguistic Modelling Laboratory (LML), Institute for Parallel Processing, Bulgarian Academy of Sciences. See http://www.bultreebank.orgfor details. In this paper, we further stipulate that language adaption should be no harder than domain adaptation [2]. Similarly to Buyko &amp; al. [2], we experiment with the OpenNLP tools2 since they are open source and contain several platform-independent Java implementations of important NLP components. Moreover, these tools are based on a single machine learning algorithm, maximum entropy (ME) [1], as implemented in the OpenNLP MaxEnt3 Java package. In our experiments below, we focus on five basic components from the OpenNLP tools: sentence detection, tokenization, part-of-speech (Pos) tagging, chunking, and parsing. Maximum entropy models search for a distribution p(x∣y) that is consistent with the empiric</context>
</contexts>
<marker>[2]</marker>
<rawString>E. Buyko, J. Wermter, M. Poprat, and U. Hahn. Automatically adapting an NLP core engine to the biology domain. In Joint BioLINK-Bio-Ontologies Meeting, pages 65–68, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In ACL’05: Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="12788" citStr="[3]" startWordPosition="1986" endWordPosition="1986">atures to Bulgarian. We should also note that the ChunkLink5 script, used at CoNLL-2000, was tailored to the Penn Treebank tagset, and was thus not very suitable to our collapsed BulTreeBank tagset. The syntactic parser: Unfortunately, we were unable to evaluate our parser with the full morphosyntactic tagset of the BulTreeBank; this would have required coding efforts for some parts of speech, e.g., nouns, that go beyond simple adaptation. On our collapsed tagset, we achieved an F1 score of 77.56%. For comparison, the best parser for English that only uses Penn Treebank data achieves F1=91.4% [3]. 5 http://ilk.kub.nl/˜sabine/chunklink 4 Conclusions and future work We have described our efforts in adapting five basic NLP components from English to Bulgarian using manually annotated training data from the BulTreeBank. We have further presented the first systematic evaluation of NLP components for Bulgarian on the same dataset and within the same machine learning framework: maximum entropy. The evaluation results have shown an F1 score of 92.54% for the sentence splitter, 98.49% for the tokenizer, 94.43% for the partof-speech tagger, 84.60% for the chunker, and 77.56% for the syntactic p</context>
</contexts>
<marker>[3]</marker>
<rawString>E. Charniak and M. Johnson. Coarse-to-fine n-best parsing and maxent discriminative reranking. In ACL’05: Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 173–180, Ann Arbor, Michigan, June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="2136" citStr="[4]" startWordPosition="302" endWordPosition="302">e both timeconsuming and error-prone. Still, using machine learning has one major limitation: it requires manually annotated corpora as training data, which can be quite costly to create. Fortunately, for Bulgarian such a rich resource already exists – the BulTreeBank1, an HPSGbased Syntactic Treebank with rich annotations at various linguistic levels. The existence of such a resource makes it possible to adapt to Bulgarian various NLP tools that have been originally developed for other languages, e.g., English, and that have been trained on similar kinds of resources, e.g., the Penn Treebank [4]. 1 Created at the Linguistic Modelling Laboratory (LML), Institute for Parallel Processing, Bulgarian Academy of Sciences. See http://www.bultreebank.orgfor details. In this paper, we further stipulate that language adaption should be no harder than domain adaptation [2]. Similarly to Buyko &amp; al. [2], we experiment with the OpenNLP tools2 since they are open source and contain several platform-independent Java implementations of important NLP components. Moreover, these tools are based on a single machine learning algorithm, maximum entropy (ME) [1], as implemented in the OpenNLP MaxEnt3 Java</context>
<context position="6138" citStr="[4]" startWordPosition="913" endWordPosition="913"> the above XML into the following Penn Treebank-style bracketing structure: ((S (NP (PRP Всички) ) (VP (PP (IN на) (NP (NP (PRP някакъв) (NN етап) ) (PP (IN от) (NN живота) (PP$ си) ) ) ) (VP (VB сме) (VB изправени) (PP (IN пред) (NP (NP (NNS проблеми) ) (CC и) (NP (NNS предизвикателства) ) ) ) ) ) (. .) 36 In the process of transformation, we further apply some simple rules for the coordinations in the BulTreeBank, e.g., “CoordP” and “ConjArg” typically become “NP”, and “Conj” becomes a “CC” phrase; see [5] for further details on the syntactic phrase naming conventions of the BulTreeBank and [4] for those of the Penn Treebank. We also remove the outer verb phrases in the BulTreeBank, e.g., the phrases “VPA” and “VPS” in the above example, as required by the Penn Treebank bracketing structure. We further define specific rules for pronouns. For example, the “ana” tag in the BulTreeBank [10] is very important for pronoun phrases of the following kind: &lt;Pron&gt;&lt;w ana=&amp;quot;P...&amp;quot;&gt;.... First, in case the fourth position is filled by a “t” and the tag starts with “Ps” as in “Ps*t*”4, this is a possessive form and is part of the NP phrase in the transformation structure. For example: хубавата ми ко</context>
</contexts>
<marker>[4]</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Osenova</author>
<author>K Simov</author>
</authors>
<title>BTB-TR05: BulTreeBank Stylebook.</title>
<date>2004</date>
<tech>Technical report, BulTreeBank Project Technical Report,</tech>
<contexts>
<context position="4379" citStr="[9, 5]" startWordPosition="650" endWordPosition="651">OS tagger and the syntactic parser. The remainder of this paper is organized as follows: Section 2 describes the process of converting the BulTreeBank XML data to Penn Treebank-style bracketing, Section 3 describes the experiments and discusses the results, and Section 4 concludes and suggests directions for future work. 2 http://opennlp.sourceforge.net 3 http://maxent.sourceforge.net 35 Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 35–38 2 Converting the BulTreeBank XML to Penn Treebank-style bracketing Converting the BulTreeBank XML [9, 5] to Penn Treebank-style bracketing format is straightforward, with some exceptions, for which we define custom tools. Consider, for example, the following sentence: Всички на някакъв етап от живота си сме изправени пред проблеми и предизвикателства . ‘All at some point of life itself we face to problems and challenges.’ We all at some point of our lives face problems and challenges. It has the following XML structure in BulTreeBank: &lt;S&gt; &lt;VPA&gt; &lt;VPS&gt; &lt;Pron&gt; &lt;w ana=&amp;quot;Pce-op&amp;quot;&gt;Всички&lt;/w&gt; &lt;/Pron&gt; &lt;PP&gt; &lt;Prep&gt; &lt;w ana=&amp;quot;R&amp;quot;&gt;на&lt;/w&gt; &lt;/Prep&gt; &lt;/PP&gt; &lt;NPA&gt; &lt;NPA&gt; &lt;A&gt; &lt;w ana=&amp;quot;Pfa--s-m&amp;quot;&gt;някакъв&lt;/w&gt; &lt;/A&gt; &lt;N&gt; &lt;w ana</context>
<context position="6048" citStr="[5]" startWordPosition="898" endWordPosition="898">ства&lt;/w&gt; &lt;/N&gt; &lt;/ConjArg&gt; &lt;/CoordP&gt; &lt;/PP&gt; &lt;/VPC&gt; &lt;/VPS&gt; &lt;/VPA&gt; &lt;pt&gt;.&lt;/pt&gt; &lt;/S&gt; We transform the above XML into the following Penn Treebank-style bracketing structure: ((S (NP (PRP Всички) ) (VP (PP (IN на) (NP (NP (PRP някакъв) (NN етап) ) (PP (IN от) (NN живота) (PP$ си) ) ) ) (VP (VB сме) (VB изправени) (PP (IN пред) (NP (NP (NNS проблеми) ) (CC и) (NP (NNS предизвикателства) ) ) ) ) ) (. .) 36 In the process of transformation, we further apply some simple rules for the coordinations in the BulTreeBank, e.g., “CoordP” and “ConjArg” typically become “NP”, and “Conj” becomes a “CC” phrase; see [5] for further details on the syntactic phrase naming conventions of the BulTreeBank and [4] for those of the Penn Treebank. We also remove the outer verb phrases in the BulTreeBank, e.g., the phrases “VPA” and “VPS” in the above example, as required by the Penn Treebank bracketing structure. We further define specific rules for pronouns. For example, the “ana” tag in the BulTreeBank [10] is very important for pronoun phrases of the following kind: &lt;Pron&gt;&lt;w ana=&amp;quot;P...&amp;quot;&gt;.... First, in case the fourth position is filled by a “t” and the tag starts with “Ps” as in “Ps*t*”4, this is a possessive form</context>
<context position="7971" citStr="[5, 9]" startWordPosition="1216" endWordPosition="1217">tag “Psxt” (pronoun, possessive, reflexive, short form), which was transformed to “PP$” (pronoun, possessive). Similarly, the last word in the example sentence, “предизвикателства” (challenges), had the tag “Ncnpi” (noun, common, neuter, plural, indefinite), which was collapsed to “NNS” (noun, plural). In other cases, the tags were directly transformed, e.g., the word “от” (from) with tag “R” (preposition) was transformed to the tag “IN” (preposition or subordinating conjunction). 3 Experiments and evaluation In our experiments, we used the training and the testing sections of the BulTreeBank [5, 9] without further 4 Here * stands for a character that is not important for the discussion. modifications in order to retrain for Bulgarian the sentence splitter, the tokenizer, the POS tagger, the chunker (shallow parser), and the syntactic parser from the OpenNLP tools. The results are shown in Table 1. The sentence splitter achieved an F1 score of 92.54%. The false positives constituted most of the errors and appeared in complicated sentences rather than at abbreviations of organization names as we expected. For example, the following chunk was recognized as a sentence: Ко$и беше този човек?</context>
</contexts>
<marker>[5]</marker>
<rawString>P. Osenova and K. Simov. BTB-TR05: BulTreeBank Stylebook. Technical report, BulTreeBank Project Technical Report, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>Maximum Entropy Models for Natural Language Ambiguity Resolution.</title>
<date>1998</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="3159" citStr="[6]" startWordPosition="459" endWordPosition="459">mentations of important NLP components. Moreover, these tools are based on a single machine learning algorithm, maximum entropy (ME) [1], as implemented in the OpenNLP MaxEnt3 Java package. In our experiments below, we focus on five basic components from the OpenNLP tools: sentence detection, tokenization, part-of-speech (Pos) tagging, chunking, and parsing. Maximum entropy models search for a distribution p(x∣y) that is consistent with the empirical observations about a particular feature f(x, y), computed from a set of training examples T = {x, y}, e.g., a sentence x and its labeling y; see [6] for details. From all such distributions, the one with the highest entropy is chosen [1]. It can be shown that the resulting distribution will have the following form: p.,,,(y∣x) a exp(w ⋅ f(x, y)) (1) The features used in the OpenNLP framework combine heterogeneous contextual information such as words around the end of a sentence for the English sentence splitter, or word, character n-grams and partof-speech tag alone and in various combinations for the English chunker. These features are based on the publications of Sha and Pereira [7] for the chunker, and on the dissertation of Ratnaparkhi</context>
</contexts>
<marker>[6]</marker>
<rawString>A. Ratnaparkhi. Maximum Entropy Models for Natural Language Ambiguity Resolution. PhD thesis, University of Pennsylvania, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sha</author>
<author>F Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In NAACL’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>134--141</pages>
<contexts>
<context position="3703" citStr="[7]" startWordPosition="551" endWordPosition="551"> T = {x, y}, e.g., a sentence x and its labeling y; see [6] for details. From all such distributions, the one with the highest entropy is chosen [1]. It can be shown that the resulting distribution will have the following form: p.,,,(y∣x) a exp(w ⋅ f(x, y)) (1) The features used in the OpenNLP framework combine heterogeneous contextual information such as words around the end of a sentence for the English sentence splitter, or word, character n-grams and partof-speech tag alone and in various combinations for the English chunker. These features are based on the publications of Sha and Pereira [7] for the chunker, and on the dissertation of Ratnaparkhi [6] for the POS tagger and the syntactic parser. The remainder of this paper is organized as follows: Section 2 describes the process of converting the BulTreeBank XML data to Penn Treebank-style bracketing, Section 3 describes the experiments and discusses the results, and Section 4 concludes and suggests directions for future work. 2 http://opennlp.sourceforge.net 3 http://maxent.sourceforge.net 35 Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 35–38 2 Converting the BulTreeBank</context>
</contexts>
<marker>[7]</marker>
<rawString>F. Sha and F. Pereira. Shallow parsing with conditional random fields. In NAACL’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 134–141, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Shen</author>
<author>G Satta</author>
<author>A Joshi</author>
</authors>
<title>Guided learning for bidirectional sequence classification.</title>
<date>2007</date>
<booktitle>In ACL’07: Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>760--767</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="11853" citStr="[8]" startWordPosition="1837" endWordPosition="1837">‘Странджата”with POS=Npfsd Another location example was the word “Балканът” (Balkanut, i.e., the Balkan mountain): wrong annotation ‘‘Балканът”with POS=Ncmsd correct annotation ‘‘Балканът”with POS=Npmsd We further collapsed the original BulTreeBank tagset, which contained 680 morpho-syntactic tags, to a much smaller one, with 95 tags only; see Section 2 for details. The POS tagger with that reduced tagset achieved an F1 score of 94.43% and was used in the experiments on chunking and syntactic parsing described below. For comparison, the best results for English on the Penn Treebank are 97.33% [8], but using a different learning algorithm: bidirectional perceptron. The chunker achieved an F1 score of 84.60%. This result is much lower than the best F1 score for English reported at the CoNLL-2000 chunking competition: 94.13%. However, this comparison should be treated with caution since we did no special adaptation of the features to Bulgarian. We should also note that the ChunkLink5 script, used at CoNLL-2000, was tailored to the Penn Treebank tagset, and was thus not very suitable to our collapsed BulTreeBank tagset. The syntactic parser: Unfortunately, we were unable to evaluate our p</context>
</contexts>
<marker>[8]</marker>
<rawString>L. Shen, G. Satta, and A. Joshi. Guided learning for bidirectional sequence classification. In ACL’07: Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 760–767, Prague, Czech Republic, June 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Simov</author>
</authors>
<title>BTB-TR01: BulTreeBank Project Overview.</title>
<date>2004</date>
<tech>Technical report, BulTreeBank Project Technical Report,</tech>
<contexts>
<context position="4379" citStr="[9, 5]" startWordPosition="650" endWordPosition="651">OS tagger and the syntactic parser. The remainder of this paper is organized as follows: Section 2 describes the process of converting the BulTreeBank XML data to Penn Treebank-style bracketing, Section 3 describes the experiments and discusses the results, and Section 4 concludes and suggests directions for future work. 2 http://opennlp.sourceforge.net 3 http://maxent.sourceforge.net 35 Workshop Adaptation of Language Resources and Technology to New Domains 2009 - Borovets, Bulgaria, pages 35–38 2 Converting the BulTreeBank XML to Penn Treebank-style bracketing Converting the BulTreeBank XML [9, 5] to Penn Treebank-style bracketing format is straightforward, with some exceptions, for which we define custom tools. Consider, for example, the following sentence: Всички на някакъв етап от живота си сме изправени пред проблеми и предизвикателства . ‘All at some point of life itself we face to problems and challenges.’ We all at some point of our lives face problems and challenges. It has the following XML structure in BulTreeBank: &lt;S&gt; &lt;VPA&gt; &lt;VPS&gt; &lt;Pron&gt; &lt;w ana=&amp;quot;Pce-op&amp;quot;&gt;Всички&lt;/w&gt; &lt;/Pron&gt; &lt;PP&gt; &lt;Prep&gt; &lt;w ana=&amp;quot;R&amp;quot;&gt;на&lt;/w&gt; &lt;/Prep&gt; &lt;/PP&gt; &lt;NPA&gt; &lt;NPA&gt; &lt;A&gt; &lt;w ana=&amp;quot;Pfa--s-m&amp;quot;&gt;някакъв&lt;/w&gt; &lt;/A&gt; &lt;N&gt; &lt;w ana</context>
<context position="7971" citStr="[5, 9]" startWordPosition="1216" endWordPosition="1217">tag “Psxt” (pronoun, possessive, reflexive, short form), which was transformed to “PP$” (pronoun, possessive). Similarly, the last word in the example sentence, “предизвикателства” (challenges), had the tag “Ncnpi” (noun, common, neuter, plural, indefinite), which was collapsed to “NNS” (noun, plural). In other cases, the tags were directly transformed, e.g., the word “от” (from) with tag “R” (preposition) was transformed to the tag “IN” (preposition or subordinating conjunction). 3 Experiments and evaluation In our experiments, we used the training and the testing sections of the BulTreeBank [5, 9] without further 4 Here * stands for a character that is not important for the discussion. modifications in order to retrain for Bulgarian the sentence splitter, the tokenizer, the POS tagger, the chunker (shallow parser), and the syntactic parser from the OpenNLP tools. The results are shown in Table 1. The sentence splitter achieved an F1 score of 92.54%. The false positives constituted most of the errors and appeared in complicated sentences rather than at abbreviations of organization names as we expected. For example, the following chunk was recognized as a sentence: Ко$и беше този човек?</context>
</contexts>
<marker>[9]</marker>
<rawString>K. Simov. BTB-TR01: BulTreeBank Project Overview. Technical report, BulTreeBank Project Technical Report, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Simov</author>
<author>P Osenova</author>
<author>M Slavcheva</author>
</authors>
<title>BTB-TR03: BulTreeBank Morphosyntactic Tagset.</title>
<date>2004</date>
<tech>Technical report, BulTreeBank Project Technical Report,</tech>
<contexts>
<context position="6437" citStr="[10]" startWordPosition="964" endWordPosition="964">) (. .) 36 In the process of transformation, we further apply some simple rules for the coordinations in the BulTreeBank, e.g., “CoordP” and “ConjArg” typically become “NP”, and “Conj” becomes a “CC” phrase; see [5] for further details on the syntactic phrase naming conventions of the BulTreeBank and [4] for those of the Penn Treebank. We also remove the outer verb phrases in the BulTreeBank, e.g., the phrases “VPA” and “VPS” in the above example, as required by the Penn Treebank bracketing structure. We further define specific rules for pronouns. For example, the “ana” tag in the BulTreeBank [10] is very important for pronoun phrases of the following kind: &lt;Pron&gt;&lt;w ana=&amp;quot;P...&amp;quot;&gt;.... First, in case the fourth position is filled by a “t” and the tag starts with “Ps” as in “Ps*t*”4, this is a possessive form and is part of the NP phrase in the transformation structure. For example: хубавата ми кола ‘beautiful-the my-clitic car’ my beautiful car Or: ма$ика ми ‘mother my-clitic’ my mother Second, if the tag does not start with “Ps” then the pronoun is part of the verb phrase because it is a personal pronoun. Finally, if there is no “t” on position four, but there is “l” or “-” instead, we an</context>
</contexts>
<marker>[10]</marker>
<rawString>K. Simov, P. Osenova, and M. Slavcheva. BTB-TR03: BulTreeBank Morphosyntactic Tagset. Technical report, BulTreeBank Project Technical Report, 2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>