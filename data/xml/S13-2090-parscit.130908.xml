<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.102415">
<title confidence="0.97048">
bwbaugh : Hierarchical sentiment analysis with partial self-training
</title>
<author confidence="0.99867">
Wesley Baugh
</author>
<affiliation confidence="0.9993565">
Department of Computer Science
University of North Texas
</affiliation>
<email confidence="0.997793">
brianbaugh@my.unt.edu
</email>
<sectionHeader confidence="0.995631" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999193333333333">
Using labeled Twitter training data from
SemEval-2013, we train both a subjectivity
classifier and a polarity classifier separately,
and then combine the two into a single hier-
archical classifier. Using additional unlabeled
data that is believed to contain sentiment, we
allow the polarity classifier to continue learn-
ing using self-training. The resulting system is
capable of classifying a document as neutral,
positive, or negative with an overall accuracy
of 61.2% using our hierarchical Naive Bayes
classifier.1
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999831941176471">
Many people use social networks, such as Twitter,
to connect and communicate with others. Users of
social networks often share their experiences, such
as watching a recent movie or tv show, reading a
book, or a newly tried product or service. In addi-
tion, social networks provide an avenue for discus-
sion of current events, such as politics. Many people
and companies are often concerned with how others
perceive their product—which is sometimes them-
selves, as is the case for politicians—or their service.
By understanding and reacting to what the consumer
is thinking, they can attempt to maximize their good
press as well as to help minimize the bad. It would
therefore be useful to use the information users of
social networks share to perform sentiment analysis
in order to understand how people perceive targets
of interest.
</bodyText>
<footnote confidence="0.9880495">
1 A working demo of the system will be available for a short
time at:http://infertweet.bwbaugh.com
</footnote>
<bodyText confidence="0.99990056">
In general, sentiment analysis often involves the
use of machine learning, especially Naive Bayes,
SVM, and MaxEnt classifiers [Jose]. Features gen-
eral include n-grams and POS tags [Go et al., 2009;
Pak and Paroubek, 2010; Jose], as well as senti-
ment lexicons [Jose]. Go et al. [2009] achieved
around 82.5% accuracy for positive-negative polar-
ity detection, Jose achieved around 76% accuracy
for subjective-objective classification, and Pak and
Paroubek [2010] achieved around 70% accuracy for
a combined subjectivity-polarity classifier.
While determining whether a document known to
be subjective is positive or negative (polarity detec-
tion) is relatively easy, a currently more difficult task
in sentiment analysis is identifying whether a docu-
ment is subjective or objective (subjectivity analy-
sis). Many approaches simply ignore the objective
class [Go et al., 2009], which does not work for real
world problems as there are a substantial amount of
documents that are either partially or wholly objec-
tive [Koppel and Schler, 2006].
Many previous methods focus on either subjectiv-
ity analysis or polarity detection. Our method incor-
porates both subtasks into a single overall system in
order to perform sentiment analysis.
</bodyText>
<sectionHeader confidence="0.982272" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.991568285714286">
The sentiment analysis in Twitter task of SemEval-
2013 [Wilson et al., 2013] provides 9,864 la-
beled tweets from Twitter to be used as a train-
ing dataset. Each instance is labeled as either
positive, negative, or neutral, and was
annotated through Amazon’s Mechanical Turk. The
terms of service for Twitter puts restrictions on the
</bodyText>
<page confidence="0.971954">
539
</page>
<bodyText confidence="0.994206481481481">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 539–542, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
type of data that may be re-released, therefore par-
ticipants SemEval-2013 Task 2 participants were
required to download tweets directly from Twit-
ter. Due to deleted or otherwise unavailable tweets,
this system was only able to download approxi-
mately 8,750 training instances. Additionally, a
development dataset was provided with 1,654 la-
beled tweets, of which 340 are negative, 739 are
neutral, and 575 are positive. The provided
test set consisted of 3,813 instances, of which 601
are negative, 1640 are neutral, and 1572 are
positive.
In related work, Go et al. [2009] generated an au-
tomatically labeled noisy gold standard by search-
ing for tweets that contained one of several emoti-
cons2 (e.g. :) or :() that were mapped to ei-
ther the positive or negative class depend-
ing on the type of emoticon in the text. This sys-
tem also collected approximately one million tweets
using emoticons as a keyword search for match-
ing, however the data remained unlabeled. Though
these tweets are unlabeled, they are presumed to be
subjective—either positive or negative but
not neutral—because of the intuitive association
of emoticons with sentiment.
</bodyText>
<sectionHeader confidence="0.995435" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999632153846154">
The system uses a custom implementation of Multi-
nomial Naive Bayes as the classifier.3 We create
a hierarchical classifier, which in this case consists
of two binary classifiers. The first-level is the sub-
jectivity classifier, which can output objective
(neutral) or subjective. If the output of the first
level is subjective, then the second-level polar-
ity classifier decides if the instance is positive or
negative.
Both classifiers (subjective and polarity) are
trained on approximately 8,750 training instances,
which come from the released SemEval-2013 train-
ing dataset. The subjective classifier is not given any
</bodyText>
<footnote confidence="0.930352666666666">
2The term emoticon comes from a blending of the words
“emotion” and “icon”.
3 The machine learning components (Multinomial Naive
Bayes) were written for this system as a Python library,
and will be available on GitHub: https://github.com/
bwbaugh/infer. That toolkit was then used as a founda-
tion for writing the code for the system, which will also be
available on GitHub: https://github.com/bwbaugh/
infertweet.
</footnote>
<figureCaption confidence="0.981974714285714">
Figure 1: A single multinomial classifier, which can out-
put any class label.
Figure 2: A hierarchical classifier, which in this case con-
sists of two binary classifiers. The first level is a sub-
jectivity classifier, with an output of either subjective or
neutral. The second level is a sentiment polarity classi-
fier, with an output of either positive or negative.
</figureCaption>
<bodyText confidence="0.999617526315789">
additional training data. The system then uses its
current model to classify approximately one million
unlabeled tweets that are believed to be subjective.
The unlabeled tweets were classified one at a time.
If the system classified the tweet as subjective, it was
used to train the polarity classifier only if the confi-
dence in the predicted label was greater than 0.8.
We stopped the system after approximately 910k to-
tal training instances were used.
The core features extracted are unigrams and bi-
grams. Bigrams had an additional start and
end token at the beginning and end of the full
text of the training instance.
As part of a preprocessing step, we attempted to
find URLs in the text and replace them with a special
URL token. We shortened characters repeated
more than twice, such that “haaaaaaate” would be-
come “haate”. We attempted to find dates in the text
and replace them with a special DATE token.
</bodyText>
<page confidence="0.945642">
540
</page>
<figure confidence="0.9993001875">
performance
0.9
0.8
0.6
0.4
0.7
0.5
0.3
0.2
0.1
Single Classifier
1.0
SemEval Positive Negative Neutral Accuracy
training instances
103 104 105 106
training instances
</figure>
<figureCaption confidence="0.99351125">
Figure 3: Performance of the single (non-hierarchical) and hierarchical classifiers on the development set vs. the
number of training instances. The performance metric for positive, negative and neutral is F-measure, SemEval is the
simple average of the positive and negative performance, and accuracy is the overall number of correct instances. The
first 8,750 instances are labeled, while the rest are unlabeled instances that were added using self-training.
</figureCaption>
<figure confidence="0.999396307692308">
performance
0.9
0.8
0.6
0.4
0.7
0.5
0.3
0.2
0.1
1.0
SemEval Positive Negative Neutral Accuracy
Hierarchical Classifier
</figure>
<sectionHeader confidence="0.936003" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.868644">
4.1 Design
</subsectionHeader>
<bodyText confidence="0.999983269230769">
The system was incrementally trained one tweet at
a time, with the performance checked every so often
by using the current model to classify the develop-
ment set instances. Once all of the labeled training
data had been used, the subjectivity classifier was
given no additional training instances, and the re-
mainder of the subjectively charged unlabeled data
was used to train the polarity classifier.
Variables experimented on included: extracting
n-grams up to size 4 and trying all combinations;
mapping Twitter usernames to a special token; map-
ping substrings recognized as a date to a special to-
ken; combining a negation token such as “not” to the
following token; deleting characters repeated more
than twice; mapping numbers to a special token;
counting exclamation points; the confidence thresh-
old above which the predicted label for an unlabeled
instance would be used for training.
In addition to collecting unlabeled data using
emoticon keywords, we also experimented with us-
ing sentences from Wikipedia as neutrally labeled
text, as well as using a random subsample of all
English-language tweets from the Twitter public
stream as a source of unlabeled data for any class.
We also tried using a single non-hierarchical clas-
sifier using each source of unlabeled data.
</bodyText>
<sectionHeader confidence="0.627485" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<subsectionHeader confidence="0.911755">
4.2.1 SemEval-2013 development set
</subsectionHeader>
<bodyText confidence="0.9744705">
Using additional unlabeled data with the single
multinomial classifier always resulted in overall de-
</bodyText>
<page confidence="0.99161">
541
</page>
<table confidence="0.997088">
Neg graded performance.
Neu Hierarchical Classifier
Pos
208 71 61
143 420 176
87 135 353
Neg Neu Pos
</table>
<figureCaption confidence="0.9903215">
Figure 4: The confusion matrix on the development
set produced after training on a total of approximately
970k training instances. Rows are the true labels while
columns are the predicted labels.
</figureCaption>
<table confidence="0.9996964">
4.2.2 SemEval-2013 test set
gs \ pred negative neutral positive
negative 324 203 74
neutral 196 1168 276
positive 233 498 841
</table>
<tableCaption confidence="0.997421">
Table 1: Confusion matrix (hierarchical)
</tableCaption>
<table confidence="0.866837">
class prec recall fscore
negative 0.4303 0.5391 0.4786
neutral 0.6249 0.7122 0.6657
positive 0.7061 0.5350 0.6088
</table>
<tableCaption confidence="0.997989">
Table 2: Performance (hierarchical)
</tableCaption>
<bodyText confidence="0.99988225">
The average F-score of the positive and
negative classes is 0.5437, which is the main
evaluation metric used by SemEval-2013 Task 2.
The overall accuracy is 61.2%.
</bodyText>
<subsectionHeader confidence="0.566916">
4.2.3 Discussion
</subsectionHeader>
<bodyText confidence="0.99996492">
By using a hierarchical classifier, we are able to
prevent degradation of the performance of the clas-
sifier on neutrally labeled instances by only applying
additional training data to the polarity classifier.
The use of additional unlabeled data results in an
increase in performance for the hierarchical classi-
fier as seen in Figure 3. However, the increase in
performance comes with an exponential increase in
the number of unlabeled instances. Using appropri-
ate feature selection for online algorithms, such as
feature hashing, a system like this could train indefi-
nitely on additional data from a Twitter stream with-
out running out of memory.
The system’s lack of high-quality sources for ad-
ditional objective-OR-neutral data—either
labeled or unlabeled—appears to be our biggest ob-
stacle to increasing performance at this time. The
poor performance of the single multinomial classi-
fier when given additional unlabeled data can also
likely be attributed to this reason. Identifying ad-
ditional high-quality sources of neutral data would
likely go a long way towards improving the over-
all system performance. Active learning approaches
could also be applied with the goal of improving the
subjectivity classifier.
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9988778">
Using a hierarchical classifier comprised of two
Naive Bayes classifiers, we are able to improve the
performance of polarity detection with the addition
of unlabeled data in an online setting by isolating the
subjectivity classifier.
</bodyText>
<sectionHeader confidence="0.999159" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99827125">
Alec Go, Richa Bhayani, and Lei Huang. Twit-
ter sentiment classification using distant supervi-
sion. CS224N Project Report, Stanford, pages 1–
12, 2009.
Anthony K Jose. Twitter sentiment analysis.
Moshe Koppel and Jonathan Schler. The importance
of neutral examples for learning sentiment. Com-
putational Intelligence, 22(2):100–109, 2006.
Alexander Pak and Patrick Paroubek. Twitter as a
corpus for sentiment analysis and opinion mining.
In Proceedings of LREC, volume 2010, 2010.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Alan Ritter, Sara Rosenthal, and Veselin Stoy-
anov. Semeval-2013 task 2: Sentiment analysis
in twitter. In Proceedings of the 7th International
Workshop on Semantic Evaluation, 2013.
</reference>
<figure confidence="0.996380090909091">
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
</figure>
<page confidence="0.94329">
542
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.631666">
<title confidence="0.99726">bwbaugh : Hierarchical sentiment analysis with partial self-training</title>
<author confidence="0.995353">Wesley</author>
<affiliation confidence="0.9999365">Department of Computer University of North</affiliation>
<email confidence="0.999581">brianbaugh@my.unt.edu</email>
<abstract confidence="0.999582272727273">Using labeled Twitter training data from SemEval-2013, we train both a subjectivity classifier and a polarity classifier separately, and then combine the two into a single hierarchical classifier. Using additional unlabeled data that is believed to contain sentiment, we allow the polarity classifier to continue learning using self-training. The resulting system is of classifying a document as or an overall accuracy</abstract>
<intro confidence="0.639145">our hierarchical Naive Bayes</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter sentiment classification using distant supervision. CS224N Project Report,</title>
<date>2009</date>
<pages>1--12</pages>
<location>Stanford,</location>
<contexts>
<context position="1829" citStr="Go et al., 2009" startWordPosition="277" endWordPosition="280">derstanding and reacting to what the consumer is thinking, they can attempt to maximize their good press as well as to help minimize the bad. It would therefore be useful to use the information users of social networks share to perform sentiment analysis in order to understand how people perceive targets of interest. 1 A working demo of the system will be available for a short time at:http://infertweet.bwbaugh.com In general, sentiment analysis often involves the use of machine learning, especially Naive Bayes, SVM, and MaxEnt classifiers [Jose]. Features general include n-grams and POS tags [Go et al., 2009; Pak and Paroubek, 2010; Jose], as well as sentiment lexicons [Jose]. Go et al. [2009] achieved around 82.5% accuracy for positive-negative polarity detection, Jose achieved around 76% accuracy for subjective-objective classification, and Pak and Paroubek [2010] achieved around 70% accuracy for a combined subjectivity-polarity classifier. While determining whether a document known to be subjective is positive or negative (polarity detection) is relatively easy, a currently more difficult task in sentiment analysis is identifying whether a document is subjective or objective (subjectivity anal</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, pages 1– 12, 2009.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Anthony K Jose</author>
</authors>
<title>Twitter sentiment analysis.</title>
<marker>Jose, </marker>
<rawString>Anthony K Jose. Twitter sentiment analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
<author>Jonathan Schler</author>
</authors>
<title>The importance of neutral examples for learning sentiment.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="2666" citStr="Koppel and Schler, 2006" startWordPosition="403" endWordPosition="406">ctive classification, and Pak and Paroubek [2010] achieved around 70% accuracy for a combined subjectivity-polarity classifier. While determining whether a document known to be subjective is positive or negative (polarity detection) is relatively easy, a currently more difficult task in sentiment analysis is identifying whether a document is subjective or objective (subjectivity analysis). Many approaches simply ignore the objective class [Go et al., 2009], which does not work for real world problems as there are a substantial amount of documents that are either partially or wholly objective [Koppel and Schler, 2006]. Many previous methods focus on either subjectivity analysis or polarity detection. Our method incorporates both subtasks into a single overall system in order to perform sentiment analysis. 2 Background The sentiment analysis in Twitter task of SemEval2013 [Wilson et al., 2013] provides 9,864 labeled tweets from Twitter to be used as a training dataset. Each instance is labeled as either positive, negative, or neutral, and was annotated through Amazon’s Mechanical Turk. The terms of service for Twitter puts restrictions on the 539 Second Joint Conference on Lexical and Computational Semanti</context>
</contexts>
<marker>Koppel, Schler, 2006</marker>
<rawString>Moshe Koppel and Jonathan Schler. The importance of neutral examples for learning sentiment. Computational Intelligence, 22(2):100–109, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter as a corpus for sentiment analysis and opinion mining.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>volume</volume>
<contexts>
<context position="1853" citStr="Pak and Paroubek, 2010" startWordPosition="281" endWordPosition="284">eacting to what the consumer is thinking, they can attempt to maximize their good press as well as to help minimize the bad. It would therefore be useful to use the information users of social networks share to perform sentiment analysis in order to understand how people perceive targets of interest. 1 A working demo of the system will be available for a short time at:http://infertweet.bwbaugh.com In general, sentiment analysis often involves the use of machine learning, especially Naive Bayes, SVM, and MaxEnt classifiers [Jose]. Features general include n-grams and POS tags [Go et al., 2009; Pak and Paroubek, 2010; Jose], as well as sentiment lexicons [Jose]. Go et al. [2009] achieved around 82.5% accuracy for positive-negative polarity detection, Jose achieved around 76% accuracy for subjective-objective classification, and Pak and Paroubek [2010] achieved around 70% accuracy for a combined subjectivity-polarity classifier. While determining whether a document known to be subjective is positive or negative (polarity detection) is relatively easy, a currently more difficult task in sentiment analysis is identifying whether a document is subjective or objective (subjectivity analysis). Many approaches s</context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. Twitter as a corpus for sentiment analysis and opinion mining. In Proceedings of LREC, volume 2010, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation,</booktitle>
<contexts>
<context position="2946" citStr="Wilson et al., 2013" startWordPosition="447" endWordPosition="450"> in sentiment analysis is identifying whether a document is subjective or objective (subjectivity analysis). Many approaches simply ignore the objective class [Go et al., 2009], which does not work for real world problems as there are a substantial amount of documents that are either partially or wholly objective [Koppel and Schler, 2006]. Many previous methods focus on either subjectivity analysis or polarity detection. Our method incorporates both subtasks into a single overall system in order to perform sentiment analysis. 2 Background The sentiment analysis in Twitter task of SemEval2013 [Wilson et al., 2013] provides 9,864 labeled tweets from Twitter to be used as a training dataset. Each instance is labeled as either positive, negative, or neutral, and was annotated through Amazon’s Mechanical Turk. The terms of service for Twitter puts restrictions on the 539 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 539–542, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics type of data that may be re-released, therefore participants SemEval-2013 Task 2 particip</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Ritter, Rosenthal, Stoyanov, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Alan Ritter, Sara Rosenthal, and Veselin Stoyanov. Semeval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the 7th International Workshop on Semantic Evaluation, 2013.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>