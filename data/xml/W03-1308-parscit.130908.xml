<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010363">
<title confidence="0.994079">
Bio-Medical Entity Extraction using Support Vector Machines
</title>
<author confidence="0.985092">
Koichi Takeuchi and Nigel Collier
</author>
<affiliation confidence="0.992546">
National Institute of Informatics
</affiliation>
<address confidence="0.967108">
2-1-2 Hitotsubashi, Chiyoda-ku
Tokyo 101-8430, Japan
</address>
<email confidence="0.999477">
{koichi,collier}@nii.ac.jp
</email>
<sectionHeader confidence="0.998596" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997301">
Support Vector Machines have achieved
state of the art performance in several clas-
sification tasks. In this article we apply
them to the identification and semantic an-
notation of scientific and technical termi-
nology in the domain of molecular biol-
ogy. This illustrates the extensibility of
the traditional named entity task to spe-
cial domains with extensive terminologies
such as those in medicine and related dis-
ciplines. We illustrate SVM’s capabilities
using a sample of 100 journal abstracts
texts taken from the {human, blood cell,
transcription factor} domain of MED-
LINE. Approximately 3400 terms are an-
notated and the model performs at about
74% F-score on cross-validation tests. A
detailed analysis based on empirical ev-
idence shows the contribution of various
feature sets to performance.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999991675324675">
With the rapid growth in the number of published
papers in the scientific fields such as medicine there
has been growing interest in the application of In-
formation Extraction (IE), (Thomas et al., 1999)
(Craven and Kumlien, 1999), to help solve some
of the problems that are associated with informa-
tion overload. IE can benefit the medical sciences
by enabling the automatic extraction of facts related
to prototypical events such as those contained in pa-
tient records or research articles regarding molecular
processes and their affect on human health. These
facts can then be used to populate databases, aid in
searching or document summarization and a variety
of tasks which require the computer to have an in-
telligent understanding of the contents inside a doc-
ument.
Our aim here is to show a state of the art method
for identifying and classifying technical terminol-
ogy. This task is an extension of the named entity
task defined by the DARPA-sponsored Message Un-
derstanding Conferences (MUCs) (MUC, 1995) and
is aimed at acquiring the shallow semantic building
blocks that contribute to a high level understanding
of the text. Although our study here looks at shallow
semantics that can be captured using IE our basic
goal is to join this with deep semantic representa-
tions so that computers can obtain a full understand-
ing of the facts in a text using logical inference and
reasoning. The scenario is that human experts will
create taxonomies and axioms (ontologies) and by
providing a small set of annotated examples, ma-
chine learning can take over the role of instance cap-
turing though information extraction technology.
Recent studies into the use of supervised learning-
based models for the named entity task have
shown that models based on hidden Markov mod-
els (HMMs) (Bikel et al., 1997), and decision trees
(Sekine et al., 1998), and maximum entropy (Borth-
wick et al., 1998) are much more generalisable and
adaptable to new classes of words than systems
based on hand-built patterns (including wrappers)
and domain specific heuristic rules such as (Herzig
and Johns, 1997).
The method we use is based on support vec-
tor machines (SVMs)(Vapnik, 1995), a state of the
art model that has achieved new levels of perfor-
mance in many classification tasks. In previous
work we have shown SVMs to be superior to sev-
eral other commonly used machine learning meth-
ods for named entity in previous experiments such
as HMMs and C4.5 (citations omitted). This pa-
per explores the underlying SVM model and shows
through detailed empirical analysis the key features
and parameter settings.
To show the application of SVMs to term ex-
traction in unstructured texts related to the medi-
cal sciences we are using a collection of abstracts
from PubMed’s MEDLINE (MEDLINE, 1999). The
MEDLINE database is an online collection of ab-
stracts for published journal articles in biology and
medicine and contains more than nine million arti-
cles. The collection we use in our tests is a con-
trolled subset of MEDLINE obtained using three
search keywords in the domain of molecular biol-
ogy. From the retrieved abstracts 100 were ran-
domly chosen for annotation by a human expert ac-
cording to classes in a small top-level ontology.
In the remainder of this paper in Section (2) we
outline the background to the task and the data set
we are using; in Section (3) we described the basic
advantages of SVMs and the formal model we are
using as well as implementation specific issues such
as the choice of feature set and report experimental
results. In Section (4) we provide extensive results
and a discussion of four sets of experiments we con-
ducted that show the best feature sets and parameter
settings in our sample domain.
</bodyText>
<sectionHeader confidence="0.987269" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999705431034483">
The names that we are trying to extract fall into
a number of categories that are outside the defini-
tions used for the traditional named-entity task used
in MUC. For this reason we consider the task of
term identification and classification to be an ex-
tended named entity task (NE+) in which the goal
is to find types as well as individuals and where the
term classes belong to an explicitly defined ontol-
ogy. The use of an ontology allows us to associate
human-readable terms in the domain with a set of
computer-readable classes, relations, properties and
axioms (Gruber, 1993).
The particular difficulties with identifying and
classifying terms in scientific and technical domains
are the size of the vocabulary (Lindberg et al.,
1993), an open growing vocabulary (Lovis et al.,
1995), irregular naming conventions as well as ex-
tensive cross-over in vocabulary between named en-
tity classes. The irregular naming arises in part be-
cause of the number of researchers and practitioners
from different fields who are working on the same
knowledge discovery area as well as the large num-
ber of entities that need to be named. Despite the
best efforts of major journals to standardize the ter-
minology, there is also a significant problem with
synonymy so that often an entity has more than
one name that is widely used. In molecular bi-
ology for example class cross-over of terms may
arise because many DNA and RNA are named af-
ter the protein with which they transcribe. This se-
mantic ambiguity which is dependent on often com-
plex contextual conditions is one of the main rea-
sons why we need learnable models and why it is
difficult to re-use existing term lists and vocabular-
ies such as MeSH(NLM, 1997), UMLS (Lindberg et
al., 1993) or those found in databases such as Swis-
sProt (Bairoch and Apweiler, 1997). An additional
obstacle to re-use is that the classification scheme
used within an existing thesaurus or database may
not be the same as the one in the users’ ontology
which may change from time to time as the consen-
sus view of the structure of knowledge is refined.
Our work has focussed on identifying names be-
longing to the classes shown in Table 1 which are all
taken from the domain of molecular biology. Exam-
ple sentences from a marked up abstract are given in
Figure 1. The ontology (Tateishi et al., 2000) that
underlies this classification scheme describes a sim-
ple top-level model which is almost flat except for
the source class which shows places where genetic
activity occurs and has a number of sub-types. Fur-
ther discussion of our use of deep semantic struc-
tures in the ontology is given elsewhere1 and we
will now focus our attention on the machine learning
model used to capture low level semantics.
The training set we used in our experiments called
Bio1 consists of 100 MEDLINE abstracts, marked
up in XML by a doctoral-qualified domain expert
</bodyText>
<footnote confidence="0.781772">
1Now being submitted for publication
</footnote>
<construct confidence="0.8536978">
Class # Description
PROTEIN 2125 proteins, protein groups,
families, complexes and
substructures.
DNA 358 DNAs, DNA groups,
regions and genes
RNA 30 RNAs, RNA groups,
regions and genes
SOURCE.cl 93 cell line
SOURCE.ct 417 cell type
SOURCE.mo 21 mono-organism
SOURCE.mu 64 multiorganism
SOURCE.vi 90 virus
SOURCE.sl 77 sublocation
SOURCE.ti 37 tissue
</construct>
<tableCaption confidence="0.7714025">
Table 1: Markup classes used in Bio1 with the num-
ber of word tokens for each class.
</tableCaption>
<construct confidence="0.992773">
TI - Differential interactions of &lt;NAME cl=“PROTEIN”
&gt;Rel &lt;/NAME &gt;- &lt;NAME cl=“PROTEIN” &gt;NF-kappa B
&lt;/NAME &gt; complexes with &lt;NAME cl=“PROTEIN” &gt;I
kappa B alpha &lt;/NAME &gt; determine pools of constitutive and
inducible &lt;NAME cl=“PROTEIN” &gt;NF-kappa B &lt;/NAME &gt;
activity.
AB - The &lt;NAME cl=“PROTEIN” &gt;Rel &lt;/NAME &gt;-
&lt;NAME cl=“PROTEIN” &gt;NF-kappa B &lt;/NAME &gt; fam-
ily of transcription factors plays a crucial role in the regula-
tion of genes involved in inflammatory and immune responses.
We demonstrate that in vivo, in contrast to the other mem-
bers of the family, &lt;NAME cl=“PROTEIN” &gt;RelB &lt;/NAME
&gt;associates efficiently only with &lt;NAME cl=“PROTEIN”
&gt;NF-kappa B1 &lt;/NAME &gt; ( &lt;NAME cl=“PROTEIN”
&gt;p105-p50 &lt;/NAME &gt;) and &lt;NAME cl=“PROTEIN” &gt;NF-
kappa B2 &lt;/NAME &gt; ( &lt;NAME cl=“PROTEIN” &gt;p100-p52
&lt;/NAME &gt;), but not with &lt;NAME cl=“PROTEIN” &gt;cRel
&lt;/NAME &gt; or &lt;NAME cl=“PROTEIN” &gt;p65 &lt;/NAME &gt;.
The &lt;NAME cl=“PROTEIN” &gt;RelB &lt;/NAME &gt;- &lt;NAME
cl=“PROTEIN” &gt;p52 &lt;/NAME &gt;heterodimers display a
much lower affinity for &lt;NAME cl=“PROTEIN” &gt;I kappa
B alpha &lt;/NAME &gt; than &lt;NAME cl=“PROTEIN” &gt;RelB
&lt;/NAME &gt;- &lt;NAME cl=“PROTEIN” &gt;p50 &lt;/NAME &gt;
heterodimers or &lt;NAME cl=“PROTEIN” &gt;p65 &lt;/NAME &gt;
complexes.
</construct>
<figureCaption confidence="0.955479">
Figure 1: Example MEDLINE sentence marked up
</figureCaption>
<bodyText confidence="0.953537555555556">
in XML for molecular biology named-entities.
for the name classes given in Table 1. The number
of named entities that were marked up by class are
also given in Table 1 and the total number of words
in the corpus is 29940. The abstracts were chosen
from a sub-domain of molecular biology that we for-
mulated by searching under the terms human, blood
cell, transcription factor in the PubMed database.
An example can be seen in Figure 1
</bodyText>
<sectionHeader confidence="0.997391" genericHeader="method">
3 Method
</sectionHeader>
<subsectionHeader confidence="0.999718">
3.1 Basic model
</subsectionHeader>
<bodyText confidence="0.9999175">
The named entity task can be formulated as a type of
classification task. In the supervised machine learn-
ing approach which we adopt here we aim to esti-
mate a classification function f,
</bodyText>
<equation confidence="0.997009">
f : χN —* {+1} (1)
</equation>
<bodyText confidence="0.958751769230769">
so that error on unseen examples is minimized,
using training examples that are N dimensional vec-
tors xi with class labels yi. The sample set S with
m examples is
S = (x1, y1), (x2, y2), ... , (xm, ym) E χN X {+1}
(2)
The classification function returns either +1 if the
test data is a member of the class, or −1 if it is not.
SVMs use linear models to discriminate between
two classes. This raises the question of how can they
be used to capture non-linear classification func-
tions? The answer to this is by the use of a non-
linear mapping function called a kernel,
</bodyText>
<equation confidence="0.99299">
`b : χN —* Γ (3)
</equation>
<bodyText confidence="0.998895">
which maps the input space χN into a feature
space Γ. The kernel function k requires the evalu-
ation of a dot product
</bodyText>
<equation confidence="0.996652">
k(xi,xj) = (`b(xi) - `b(xj)) (4)
</equation>
<bodyText confidence="0.999534428571429">
Clearly the complexity of data being classified de-
termines which particular kernel should be used and
of course more complex kernels require longer train-
ing times.
By substituting `b(xi) for each training example
in S we derive the final form of the optimal decision
function f,
</bodyText>
<equation confidence="0.994326333333333">
m
f(x) = sgn( yiαik(x, xi) + b) (5)
i
</equation>
<bodyText confidence="0.994090666666667">
where b E R is the bias and the Lagrange pa-
rameters αi (αi &gt; 0) are estimated using quadratic
optimization to maximize the following function
</bodyText>
<equation confidence="0.974044428571428">
αiαjyiyjk(xi,xj) (6)
under the constraints that
m
αiyi = 0 (7)
i=1
and
0 &lt; αi &lt; C (8)
</equation>
<bodyText confidence="0.9959348">
for i = 1, ... , m. C is a constant that controls the
ratio between the complexity of the function and the
number of misclassified training examples.
The number of parameters to be estimated in α
therefore never exceeds the number of examples.
The influence of αi basically means that training
examples with αi &gt; 0 define the decision func-
tion (the support vectors) and those examples with
αi = 0 have no influence, making the final model
very compact and testing (but not training) very fast.
The point x is classified as positive (or negative) if
f(x) &gt; 0 (or f(x) &lt; 0).
The kernel function we explored in our exper-
iments was the polynomial function k(xi,xj) =
(xi · xj + 1)d for d = 2 which was found to be the
best by (Takeuchi and Collier, 2002). Once input
vectors have been mapped to the feature space the
linear discrimination function which is found is the
one which gives the maximum the geometric margin
between the two classes in the feature space.
Besides efficiency of representation, SVMs are
known to maximize their generalizability, making
them an ideal model for the NE+ task. Generaliz-
ability in SVMs is based on statistical learning the-
ory and the observation that it is useful sometimes
to misclassify some of the training data so that the
margin between other training points is maximized.
This is particularly useful for real world data sets
that often contain inseparable data points.
We implemented our method using the Tiny SVM
package from NAIST2 which is an implementation
of Vladimir Vapnik’s SVM combined with an op-
timization algorithm (Joachims, 1999). The multi-
class model is built up from combining binary clas-
sifiers and then applying majority voting.
</bodyText>
<subsectionHeader confidence="0.999878">
3.2 Generalising with features
</subsectionHeader>
<bodyText confidence="0.999961421052632">
In order for the model to be successful it must recog-
nize regularities in the training data that relate pre-
classified examples of terms with unseen terms that
will be encountered in testing.
Following on from previous studies in named en-
tity we chose a set of linguistically motivated word-
level features that include surface word forms, part
of speech tags using the Brill tagger (Brill, 1992)
and orthographic features. Additionally we used
head-noun features that were obtained from pre-
analysis of the training data set using the FDG shal-
low parser from Conexor (Tapanainen and J¨arvinen,
1997). A significant proportion of the terms in
our corpus undergo a local syntactic transforma-
tions such as coordination which introduces ambi-
guity that needs to be resolved by shallow parsing.
For example the c- and v-rel (proto) oncogenes and
NF-kappaB and I kappa B protein families. In these
cases the head noun features oncogene and fam-
ily would be added to each word in the constituent
phrase. Head information is also needed when de-
ciding the semantic category of a long term such as
tumor necrosis factor-alpha which should be a PRO-
TEIN, whereas tumor necrosis factor (TNF) gene
and tumor necrosis factor promoter region should
both be types of DNA.
Table 2 shows the orthographic features that we
used. We hypothesize that such features will help the
model to find similarities between known words that
were found in the training set and unknown words
(of zero frequency in the training set) and so over-
come the unknown word problem.
In the experiments we report below we use feature
vectors consisting of differing amounts of ‘context’
by varying the window around the focus word which
is to be classified into one of the semantic classes.
The full window of context considered in these ex-
periments is +3 about the focus word.
</bodyText>
<footnote confidence="0.961913">
2Tiny SVM is available from http:// http://cl.aist-nara.ac.jp/
taku-ku/software/ TinySVM/
</footnote>
<equation confidence="0.918159375">
m
i=1
1 m
E
i,j
w(α) =
αi −
2
</equation>
<table confidence="0.999876846153846">
Feature Example Feature Example
DigitNumber 15 CloseSquare ]
SingleCap M Colon :
GreekLetter alpha SemiColon ;
CapsAndDigits I2 Percent %
TwoCaps RalGDS OpenParen (
LettersAndDigits p52 CloseParen )
InitCap Interleukin Comma ,
LowCaps kappaB FullStop .
Lowercase kinases Determiner the
Hyphon - Conjunction and
Backslash / Other * + #
OpenSquare [
</table>
<tableCaption confidence="0.997697">
Table 2: Orthographic features with examples
</tableCaption>
<sectionHeader confidence="0.994407" genericHeader="evaluation">
4 Experiment and Discussion
</sectionHeader>
<bodyText confidence="0.999900666666667">
Results are given as F-scores (van Rijsbergen, 1979)
using the CoNLL evaluation script and are defined
as F = (2PR)/(P+R). where P denotes Precision
and R Recall. P is the ratio of the number of cor-
rectly found NE chunks to the number of found NE
chunks, and R is the ratio of the number of correctly
found NE chunks to the number of true NE chunks.
All results are calculated using 10-fold cross valida-
tion.
</bodyText>
<subsectionHeader confidence="0.991503">
4.1 Experiment 1: Effect of Training Set Size
</subsectionHeader>
<bodyText confidence="0.99994721875">
The effect of context window size is shown along
the top column of Tables 3 and 4. It can be seen
that without exception more training data results in
higher overall F-scores except at 10 per cent. where
the result seems to be biased by the small sample,
perhaps because one abstract is partly included in
the training and testing sets. As we would expect
larger training sets reduce the effects of data sparse-
ness and allow more accurate models to be induced.
The rate of increase in improvement however is
not uniform according to the feature sets that are
used. For surface word features and head noun
features the improvement in performance is consis-
tently increasing whereas the improvement for using
orthographic and part of speech features is quite er-
ratic. This may be an effect of the small sample of
training data that we used and we could not find any
consistent explanation why this occurred.
As we observed before, the best overall result
comes from using Or hd, i.e. surface words, or-
thographic and head features. However the to-
tal score hides the fact that three classes, i.e.
SOURCE.mo, SOURCE.mu and SOURCE.ti actu-
ally perform worse when using anything but sur-
face word forms (shown in Table 5). One possi-
ble explanation for this is that all of these classes
have very small numbers of samples and the effect
of adding features may be to blur the distinction be-
tween these and other more numerous classes in the
model. However it is interesting to note that this
does not happen with the RNA class which is also
very small.
</bodyText>
<subsectionHeader confidence="0.996224">
4.2 Experiment 2: Effect of Feature Sets
</subsectionHeader>
<bodyText confidence="0.999984855072464">
The effects of feature sets is of major importance in
modelling named entity. In general we would like
to identify only the necessary features that are re-
quired and to remove those that do not contribute to
an increase in performance. This also saves time in
training and testing.
The results from Tables 3 and 4 at 100 per cent.
training data are summarized in Table 5 and clearly
illustrate the value of surface word level features
combined with orthographic and head noun features.
Orthographic features allow us to capture many gen-
eralities that are not obvious at the surface word
level such as IkappaB alpha and IkappaB beta both
being PROTEINs and IL-10 and IL-2 both being
PROTEINs.
The orthographic-head noun feature combination
(Or hd) gives the best combined-class performance
of 74.23 at 100 per cent. training data on a -2+2 win-
dow. Overall orthographic features combined with
surface word features gave an improvement of be-
tween 4.9 and 22.0 per cent. at 100 per cent. data
depending on window size over surface words alone.
This was the biggest contribution by any feature ex-
cept the surface words. Head information for exam-
ple allowed us to correctly capture the fact that in
the phrase NF-kappaB consensus site the whole of
it is a DNA, whereas using orthographic informa-
tion alone the SVM could only say that NF-kappaB
was a PROTEIN and ignoring consensus site. We
see a similar case in the phrase primary NK cells
which is correctly classified as SOURCE.ct using
head noun and orthographic features but only NK
cells are found using orthographic features. This
mistake is a natural consequence of a limited con-
textual view which the head noun feature helped to
rectify.
Part of speech (POS) when combined with sur-
face word features gave an improvement of between
7.9 and 11.7 per cent. at 100 per cent. data. The
influence of POS though does not appear to be sus-
tained when combined with other features and we
found that it actually degraded performance slightly
in many cases. This may possibly be due to ei-
ther overlapping knowledge or more likely subtle
inconsistencies between POS features and say, or-
thographic features. This could have occurred dur-
ing training when the POS tagger was trained on an
out of domain (news) text collection. It is possible
that if the POS tagger was trained on in-domain texts
it would make a greater and more consistent con-
tribution. An example where orthographic features
allowed correct classification but adding POS fea-
tures resulted in failure is p50 in the phrase consist-
ing of 50 (p50) - and 65 (p65) -kDa proteins. Also
in the phrase c-Jun transactivation domain where
only c-Jun should be tagged as a protein, by using
orthographic features and POS the model tags the
whole phrase as a PROTEIN. This is probably be-
cause POS tagging gives a NN feature value (com-
mon noun) to each word. This is very general and
does not allow the model to discriminate between
them.
The fourth feature we investigated is related to
syntactic rather than lexical knowledge. We felt
though that there should exist a strong semantic re-
lation between a word in a term and the head noun
of that term. The results in Table 5 show that while
the overall contribution of the Head feature is quite
small, it is consistent for almost all classes.
</bodyText>
<sectionHeader confidence="0.995189" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999915">
The method we have shown for identifying and clas-
sifying technical terms has the advantage of be-
ing portable, not requiring large domain dependent
dictionaries and no hand-made patterns were used.
Additionally, since all the word level features are
found automatically there is no need for interven-
tion to create domain specific features. Indeed the
only thing that is required is a quite small corpus of
text containing entities tagged by a domain expert.
For future work we are now looking at how to bal-
ance the scores from SVM for each word-class over
the whole of a sentence using dynamic program-
ming. Theoretically the existing SVM model cannot
consider evidence from outside the context window,
in particular evidence related to named entity class
scores in the history and later in the sentence.
</bodyText>
<sectionHeader confidence="0.995963" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99822169047619">
A. Bairoch and R. Apweiler. 1997. The SWISS-PROT
protein sequence data bank and its new supplement
TrEMBL. Nucleic Acids Research, 25:31–36.
D. Bikel, S. Miller, R. Schwartz, and R. Wesichedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings of the Fifth Conference on Ap-
plied Natural Language Processing (ANLP’97), Wash-
ington D.C., USA., pages 194–201, 31 March – 3
April.
A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman.
1998. Exploiting diverse knowledge sources via max-
imum entropy in named entity recognition. In Pro-
ceedings of the Sixth Workshop on Very Large Corpora
(WVLC’98), Montreal, Canada, pages 152–160.
E. Brill. 1992. A simple rule-based part of speech tagger.
In Third Conference on Applied Natural Language
Processing – Association for Computational Linguis-
tics, Trento, Italy, pages 152–155, 31st March – 3rd
April.
M. Craven and J. Kumlien. 1999. Constructing biolog-
ical knowledge bases by extracting information from
text sources. In Proceedings of the 7th International
Conference on Intelligent Systemps for Molecular Bi-
ology (ISMB-99), pages 77–86, Heidelburg, Germany,
August 6–10.
T. R. Gruber. 1993. A translation approach to
portable ontology specifications. Knowledge Acqui-
sition, 6(2):199–221.
T. Herzig and M. Johns. 1997. Extraction of medical
information from textual sources: a statistical vari-
ant of the boundary word method. In Proceedings of
the American Medical Informatics Association (AMIA)
1997 Annual Fall Symposium, Nashville, USA, 25–29
October.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Scholkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. MIT Press.
Donald A.B. Lindberg, L. Humphreys, Betsy, and T. Mc-
Cray, Alexa. 1993. The unified medical language
system. Methods ofInformation in Medicine, 32:281–
291.
</reference>
<table confidence="0.999825608695652">
Feature Set &amp; Percentage of data used in experiment
Window Size
10 20 30 40 50 60 70 80 90 100
Wd -10 58.52 47.30 51.44 52.40 52.37 52.30 51.29 53.24 55.57 56.06
Wd -1+1 55.35 48.15 53.91 54.50 56.02 55.30 55.92 58.98 60.28 61.55
Wd -2+2 46.87 40.73 47.92 49.64 53.31 53.20 55.01 56.95 59.40 62.04
Wd -3+2 46.12 38.55 44.19 47.93 49.50 50.50 51.21 54.76 56.66 60.25
Wd -3+3 44.83 35.37 42.67 45.24 46.78 49.10 49.66 54.01 55.59 58.83
Or -10 60.33 55.08 63.49 63.41 64.09 63.04 62.97 62.64 64.59 65.63
Or -1+1 65.35 58.69 66.63 68.18 69.20 68.74 69.55 69.32 71.02 72.13
Or -2+2 60.84 58.90 66.44 67.17 69.88 68.81 69.68 69.62 71.41 72.12
Or -3+2 62.48 59.21 65.64 66.69 67.56 67.25 68.37 68.94 69.92 71.69
Or -3+3 59.61 58.65 64.95 65.68 67.11 66.65 67.85 68.84 69.54 71.78
Head-10 58.51 47.10 51.99 52.74 52.44 52.01 53.09 53.79 55.97 57.01
Head-1+1 57.50 50.00 55.81 57.88 58.03 57.84 58.81 61.08 62.64 63.93
Head -2+2 49.43 45.92 53.40 53.75 57.52 56.94 59.33 61.29 63.36 64.67
Head -3+2 46.51 39.42 49.39 49.75 54.54 54.81 56.95 58.13 59.25 61.96
Head -3+3 45.79 40.81 47.52 48.11 53.58 53.50 55.95 57.02 59.06 61.52
POS -10 61.62 52.89 61.14 62.04 62.62 61.51 61.05 60.78 62.71 62.63
POS -1+1 61.24 57.25 63.83 62.94 65.35 64.82 67.40 66.47 67.43 68.37
POS -2+2 57.52 53.11 59.39 59.98 62.86 62.16 63.72 64.17 64.56 66.92
POS -3+2 56.81 54.55 56.53 56.26 59.60 59.40 61.42 61.86 63.41 64.90
POS -3+3 54.76 53.28 56.79 55.02 57.46 57.66 59.60 59.89 62.39 63.50
</table>
<tableCaption confidence="0.918898">
Table 3: F-scores on Bio1 showing the effects of training set size, feature sets, and context window sizes.
Wd: surface word level features; Or: Orthographic features; Head: Head noun features; POS: part of speech
features.
</tableCaption>
<table confidence="0.999927608695652">
Feature Set &amp; Percentage of data used in experiment
Window Size
10 20 30 40 50 60 70 80 90 100
Or hd -10 62.16 57.80 64.31 65.70 65.20 63.84 64.90 64.73 66.46 67.31
Or hd -1+1 64.84 60.52 68.42 68.25 68.82 69.34 71.31 71.88 72.60 73.38
Or hd -2+2 61.16 61.10 68.06 67.42 69.32 69.62 70.91 71.31 72.31 74.23
Or hd -3+2 61.54 60.06 65.87 66.33 67.43 68.36 70.28 70.15 70.81 72.95
Or hd -3+3 59.68 57.03 64.58 65.76 66.84 67.16 69.07 69.22 70.73 72.12
Or POS -10 61.48 54.04 63.20 63.92 64.11 64.74 63.23 63.62 64.87 66.28
Or POS -1+1 64.57 58.89 66.52 66.77 67.83 67.90 69.32 69.07 70.84 71.70
Or POS -2+2 61.48 58.56 63.37 65.44 67.01 66.74 68.21 68.55 70.09 71.87
Or POS -3+2 61.08 57.14 64.23 63.39 65.53 65.11 67.31 67.78 68.64 71.54
Or POS -3+3 57.92 57.12 62.86 62.36 65.48 64.41 66.10 66.64 68.22 70.46
POS hd -10 64.90 55.39 61.14 61.65 61.91 61.29 61.88 60.51 63.27 63.82
POS hd -1+1 62.25 57.25 63.66 64.81 64.64 65.57 67.78 67.63 68.69 69.68
POS hd -2+2 58.08 53.23 58.91 60.28 62.55 62.06 64.19 64.51 66.18 67.66
POS hd -3+2 57.09 53.20 56.58 57.75 59.34 59.14 62.19 62.93 64.23 65.41
POS hd -3+3 54.69 51.09 55.67 55.46 58.31 58.28 60.88 61.17 62.94 64.31
Or POS hd -10 63.70 56.63 63.29 65.11 64.72 64.14 64.40 64.04 66.01 67.41
Or POS hd -1+1 66.20 59.65 66.49 67.91 68.44 68.14 70.01 70.61 71.80 72.95
Or POS hd -2+2 61.62 58.03 64.76 65.16 66.45 67.26 69.00 69.86 70.83 72.56
Or POS hd -3+2 62.06 57.28 63.74 64.50 66.10 66.25 68.01 69.05 69.44 71.59
Or POS hd -3+3 59.12 56.51 62.43 62.61 65.37 65.09 66.89 67.80 69.36 71.25
</table>
<tableCaption confidence="0.97121">
Table 4: F-scores on Bio1 showing the effects of training set size, feature sets, and context window sizes.
Wd: surface word level features; Or: Orthographic features; Head: Head noun features; POS: part of speech
features.
</tableCaption>
<table confidence="0.9945785">
NE+ Class Feature Set
Wd Or Head POS Or hd Or POS POS hd Or POS hd
DNA 44.53 56.49 50.88 47.33 62.78 58.12 47.30 59.19
PROTEIN 65.07 77.50 67.96 72.10 78.99 77.03 72.89 77.58
RNA 12.12 42.11 12.90 24.24 43.24 37.84 6.67 29.41
SOURCE.cl 52.63 57.14 51.52 54.79 59.21 55.90 56.94 59.87
SOURCE.ct 65.83 66.39 66.22 63.70 69.32 67.03 65.65 68.94
SOURCE.mo 32.00 16.67 9.09 17.39 17.39 16.67 17.39 17.39
SOURCE.mu 61.02 58.41 55.24 57.14 51.92 54.55 53.33 51.92
SOURCE.sl 55.22 62.86 62.69 51.20 68.53 62.41 54.84 63.38
SOURCE.ti 23.26 18.18 0.00 14.63 5.00 14.29 0.00 0.00
SOURCE.vi 76.54 75.16 79.50 73.68 80.25 74.84 75.00 73.33
</table>
<tableCaption confidence="0.966915">
Table 5: Class by class performance using a -2+2 window shown against feature sets. Wd: surface word
</tableCaption>
<reference confidence="0.99195215">
level features; Or: Orthographic features; Head: Head noun features; POS: part of speech features.
C. Lovis, P. Michel, R. Baud, and J. Scherrer. 1995.
Word segmentation processing: a way to exponentially
extend medical dictionaries. Medinfo, 8:28–32.
MEDLINE. 1999. The PubMed database can be found
at:. http://www.ncbi.nlm.nih.gov/PubMed/.
DARPA. 1995. Proceedings of the Sixth Message Under-
standing Conference(MUC-6), Columbia, MD, USA,
November. Morgan Kaufmann.
NLM. 1997. Medical subject headings, bethesda, MD.
National Library of Medicine.
Satoshi Sekine, Ralph Grishman, and Hiroyuki Shinnou.
1998. A Decision Tree Method for Finding and Clas-
sifying Names in Japanese Texts. In Proceedings of
the Sixth Workshop on Very Large Corpora, Montreal,
Canada, August.
K. Takeuchi and N. Collier. 2002. Use of support vec-
tor machines in extended named entity recognition. In
Proceedings of the 6th Conference on Natural Lan-
guage Learning 2002 (CoNLL-2002), Roth, D. and
van den Bosch, A. (eds), pages 119–125, August 31st.
P. Tapanainen and T. J¨arvinen. 1997. A non-projective
dependency parser. In Proceedings of the 5th Confer-
ence on Applied Natural Language Processing, Wash-
ington D.C., Association of Computational Linguis-
tics, pages 64–71.
Y. Tateishi, T. Ohta, N. Collier, C. Nobata, K. Ibushi, and
J. Tsujii. 2000. Building an annotated corpus in the
molecular-biology domain. In COLING’2000 Work-
shop on Semantic Annotation and Intelligent Content,
Luxemburg, 5th–6th August.
J. Thomas, D. Milward, C. Ouzounis, S. Pulman, and
M. Carroll. 1999. Automatic extraction of protein in-
teractions from scientific abstracts. In Proceedings of
the Pacific Symposium on Biocomputing’99 (PSB’99),
pages 1–12, Hawaii, USA, January 4–9.
C. J. van Rijsbergen. 1979. Information Retrieval. But-
terworths, London.
V. N. Vapnik. 1995. The Nature of Statistical Learning
Theory. Springer-Verlag, New York.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.652503">
<title confidence="0.999684">Bio-Medical Entity Extraction using Support Vector Machines</title>
<author confidence="0.995641">Takeuchi</author>
<affiliation confidence="0.999501">National Institute of</affiliation>
<address confidence="0.816882">2-1-2 Hitotsubashi, Tokyo 101-8430,</address>
<abstract confidence="0.999026142857143">Support Vector Machines have achieved state of the art performance in several classification tasks. In this article we apply them to the identification and semantic annotation of scientific and technical terminology in the domain of molecular biology. This illustrates the extensibility of the traditional named entity task to special domains with extensive terminologies such as those in medicine and related disciplines. We illustrate SVM’s capabilities using a sample of 100 journal abstracts taken from the blood cell, of MED- LINE. Approximately 3400 terms are annotated and the model performs at about 74% F-score on cross-validation tests. A detailed analysis based on empirical evidence shows the contribution of various feature sets to performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Bairoch</author>
<author>R Apweiler</author>
</authors>
<title>The SWISS-PROT protein sequence data bank and its new supplement TrEMBL.</title>
<date>1997</date>
<journal>Nucleic Acids Research,</journal>
<pages>25--31</pages>
<contexts>
<context position="6549" citStr="Bairoch and Apweiler, 1997" startWordPosition="1081" endWordPosition="1084">dize the terminology, there is also a significant problem with synonymy so that often an entity has more than one name that is widely used. In molecular biology for example class cross-over of terms may arise because many DNA and RNA are named after the protein with which they transcribe. This semantic ambiguity which is dependent on often complex contextual conditions is one of the main reasons why we need learnable models and why it is difficult to re-use existing term lists and vocabularies such as MeSH(NLM, 1997), UMLS (Lindberg et al., 1993) or those found in databases such as SwissProt (Bairoch and Apweiler, 1997). An additional obstacle to re-use is that the classification scheme used within an existing thesaurus or database may not be the same as the one in the users’ ontology which may change from time to time as the consensus view of the structure of knowledge is refined. Our work has focussed on identifying names belonging to the classes shown in Table 1 which are all taken from the domain of molecular biology. Example sentences from a marked up abstract are given in Figure 1. The ontology (Tateishi et al., 2000) that underlies this classification scheme describes a simple top-level model which is</context>
</contexts>
<marker>Bairoch, Apweiler, 1997</marker>
<rawString>A. Bairoch and R. Apweiler. 1997. The SWISS-PROT protein sequence data bank and its new supplement TrEMBL. Nucleic Acids Research, 25:31–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Wesichedel</author>
</authors>
<title>Nymble: a high-performance learning namefinder.</title>
<date>1997</date>
<journal></journal>
<booktitle>In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP’97),</booktitle>
<volume>3</volume>
<pages>194--201</pages>
<location>Washington D.C., USA.,</location>
<contexts>
<context position="2830" citStr="Bikel et al., 1997" startWordPosition="446" endWordPosition="449">ics that can be captured using IE our basic goal is to join this with deep semantic representations so that computers can obtain a full understanding of the facts in a text using logical inference and reasoning. The scenario is that human experts will create taxonomies and axioms (ontologies) and by providing a small set of annotated examples, machine learning can take over the role of instance capturing though information extraction technology. Recent studies into the use of supervised learningbased models for the named entity task have shown that models based on hidden Markov models (HMMs) (Bikel et al., 1997), and decision trees (Sekine et al., 1998), and maximum entropy (Borthwick et al., 1998) are much more generalisable and adaptable to new classes of words than systems based on hand-built patterns (including wrappers) and domain specific heuristic rules such as (Herzig and Johns, 1997). The method we use is based on support vector machines (SVMs)(Vapnik, 1995), a state of the art model that has achieved new levels of performance in many classification tasks. In previous work we have shown SVMs to be superior to several other commonly used machine learning methods for named entity in previous e</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Wesichedel, 1997</marker>
<rawString>D. Bikel, S. Miller, R. Schwartz, and R. Wesichedel. 1997. Nymble: a high-performance learning namefinder. In Proceedings of the Fifth Conference on Applied Natural Language Processing (ANLP’97), Washington D.C., USA., pages 194–201, 31 March – 3 April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Borthwick</author>
<author>J Sterling</author>
<author>E Agichtein</author>
<author>R Grishman</author>
</authors>
<title>Exploiting diverse knowledge sources via maximum entropy in named entity recognition.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora (WVLC’98),</booktitle>
<pages>152--160</pages>
<location>Montreal, Canada,</location>
<contexts>
<context position="2918" citStr="Borthwick et al., 1998" startWordPosition="460" endWordPosition="464">representations so that computers can obtain a full understanding of the facts in a text using logical inference and reasoning. The scenario is that human experts will create taxonomies and axioms (ontologies) and by providing a small set of annotated examples, machine learning can take over the role of instance capturing though information extraction technology. Recent studies into the use of supervised learningbased models for the named entity task have shown that models based on hidden Markov models (HMMs) (Bikel et al., 1997), and decision trees (Sekine et al., 1998), and maximum entropy (Borthwick et al., 1998) are much more generalisable and adaptable to new classes of words than systems based on hand-built patterns (including wrappers) and domain specific heuristic rules such as (Herzig and Johns, 1997). The method we use is based on support vector machines (SVMs)(Vapnik, 1995), a state of the art model that has achieved new levels of performance in many classification tasks. In previous work we have shown SVMs to be superior to several other commonly used machine learning methods for named entity in previous experiments such as HMMs and C4.5 (citations omitted). This paper explores the underlying</context>
</contexts>
<marker>Borthwick, Sterling, Agichtein, Grishman, 1998</marker>
<rawString>A. Borthwick, J. Sterling, E. Agichtein, and R. Grishman. 1998. Exploiting diverse knowledge sources via maximum entropy in named entity recognition. In Proceedings of the Sixth Workshop on Very Large Corpora (WVLC’98), Montreal, Canada, pages 152–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A simple rule-based part of speech tagger.</title>
<date>1992</date>
<booktitle>In Third Conference on Applied Natural Language Processing – Association for Computational Linguistics,</booktitle>
<pages>152--155</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="13346" citStr="Brill, 1992" startWordPosition="2258" endWordPosition="2259">on of Vladimir Vapnik’s SVM combined with an optimization algorithm (Joachims, 1999). The multiclass model is built up from combining binary classifiers and then applying majority voting. 3.2 Generalising with features In order for the model to be successful it must recognize regularities in the training data that relate preclassified examples of terms with unseen terms that will be encountered in testing. Following on from previous studies in named entity we chose a set of linguistically motivated wordlevel features that include surface word forms, part of speech tags using the Brill tagger (Brill, 1992) and orthographic features. Additionally we used head-noun features that were obtained from preanalysis of the training data set using the FDG shallow parser from Conexor (Tapanainen and J¨arvinen, 1997). A significant proportion of the terms in our corpus undergo a local syntactic transformations such as coordination which introduces ambiguity that needs to be resolved by shallow parsing. For example the c- and v-rel (proto) oncogenes and NF-kappaB and I kappa B protein families. In these cases the head noun features oncogene and family would be added to each word in the constituent phrase. H</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>E. Brill. 1992. A simple rule-based part of speech tagger. In Third Conference on Applied Natural Language Processing – Association for Computational Linguistics, Trento, Italy, pages 152–155, 31st March – 3rd April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Craven</author>
<author>J Kumlien</author>
</authors>
<title>Constructing biological knowledge bases by extracting information from text sources.</title>
<date>1999</date>
<booktitle>In Proceedings of the 7th International Conference on Intelligent Systemps for Molecular Biology (ISMB-99),</booktitle>
<pages>77--86</pages>
<location>Heidelburg, Germany,</location>
<contexts>
<context position="1260" citStr="Craven and Kumlien, 1999" startWordPosition="185" endWordPosition="188">nes. We illustrate SVM’s capabilities using a sample of 100 journal abstracts texts taken from the {human, blood cell, transcription factor} domain of MEDLINE. Approximately 3400 terms are annotated and the model performs at about 74% F-score on cross-validation tests. A detailed analysis based on empirical evidence shows the contribution of various feature sets to performance. 1 Introduction With the rapid growth in the number of published papers in the scientific fields such as medicine there has been growing interest in the application of Information Extraction (IE), (Thomas et al., 1999) (Craven and Kumlien, 1999), to help solve some of the problems that are associated with information overload. IE can benefit the medical sciences by enabling the automatic extraction of facts related to prototypical events such as those contained in patient records or research articles regarding molecular processes and their affect on human health. These facts can then be used to populate databases, aid in searching or document summarization and a variety of tasks which require the computer to have an intelligent understanding of the contents inside a document. Our aim here is to show a state of the art method for iden</context>
</contexts>
<marker>Craven, Kumlien, 1999</marker>
<rawString>M. Craven and J. Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. In Proceedings of the 7th International Conference on Intelligent Systemps for Molecular Biology (ISMB-99), pages 77–86, Heidelburg, Germany, August 6–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T R Gruber</author>
</authors>
<title>A translation approach to portable ontology specifications.</title>
<date>1993</date>
<journal>Knowledge Acquisition,</journal>
<volume>6</volume>
<issue>2</issue>
<contexts>
<context position="5325" citStr="Gruber, 1993" startWordPosition="874" endWordPosition="875">n our sample domain. 2 Background The names that we are trying to extract fall into a number of categories that are outside the definitions used for the traditional named-entity task used in MUC. For this reason we consider the task of term identification and classification to be an extended named entity task (NE+) in which the goal is to find types as well as individuals and where the term classes belong to an explicitly defined ontology. The use of an ontology allows us to associate human-readable terms in the domain with a set of computer-readable classes, relations, properties and axioms (Gruber, 1993). The particular difficulties with identifying and classifying terms in scientific and technical domains are the size of the vocabulary (Lindberg et al., 1993), an open growing vocabulary (Lovis et al., 1995), irregular naming conventions as well as extensive cross-over in vocabulary between named entity classes. The irregular naming arises in part because of the number of researchers and practitioners from different fields who are working on the same knowledge discovery area as well as the large number of entities that need to be named. Despite the best efforts of major journals to standardiz</context>
</contexts>
<marker>Gruber, 1993</marker>
<rawString>T. R. Gruber. 1993. A translation approach to portable ontology specifications. Knowledge Acquisition, 6(2):199–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Herzig</author>
<author>M Johns</author>
</authors>
<title>Extraction of medical information from textual sources: a statistical variant of the boundary word method.</title>
<date>1997</date>
<booktitle>In Proceedings of the American Medical Informatics Association (AMIA)</booktitle>
<pages>25--29</pages>
<location>Nashville, USA,</location>
<contexts>
<context position="3116" citStr="Herzig and Johns, 1997" startWordPosition="491" endWordPosition="494">ontologies) and by providing a small set of annotated examples, machine learning can take over the role of instance capturing though information extraction technology. Recent studies into the use of supervised learningbased models for the named entity task have shown that models based on hidden Markov models (HMMs) (Bikel et al., 1997), and decision trees (Sekine et al., 1998), and maximum entropy (Borthwick et al., 1998) are much more generalisable and adaptable to new classes of words than systems based on hand-built patterns (including wrappers) and domain specific heuristic rules such as (Herzig and Johns, 1997). The method we use is based on support vector machines (SVMs)(Vapnik, 1995), a state of the art model that has achieved new levels of performance in many classification tasks. In previous work we have shown SVMs to be superior to several other commonly used machine learning methods for named entity in previous experiments such as HMMs and C4.5 (citations omitted). This paper explores the underlying SVM model and shows through detailed empirical analysis the key features and parameter settings. To show the application of SVMs to term extraction in unstructured texts related to the medical scie</context>
</contexts>
<marker>Herzig, Johns, 1997</marker>
<rawString>T. Herzig and M. Johns. 1997. Extraction of medical information from textual sources: a statistical variant of the boundary word method. In Proceedings of the American Medical Informatics Association (AMIA) 1997 Annual Fall Symposium, Nashville, USA, 25–29 October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In B. Scholkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="12818" citStr="Joachims, 1999" startWordPosition="2170" endWordPosition="2171">s efficiency of representation, SVMs are known to maximize their generalizability, making them an ideal model for the NE+ task. Generalizability in SVMs is based on statistical learning theory and the observation that it is useful sometimes to misclassify some of the training data so that the margin between other training points is maximized. This is particularly useful for real world data sets that often contain inseparable data points. We implemented our method using the Tiny SVM package from NAIST2 which is an implementation of Vladimir Vapnik’s SVM combined with an optimization algorithm (Joachims, 1999). The multiclass model is built up from combining binary classifiers and then applying majority voting. 3.2 Generalising with features In order for the model to be successful it must recognize regularities in the training data that relate preclassified examples of terms with unseen terms that will be encountered in testing. Following on from previous studies in named entity we chose a set of linguistically motivated wordlevel features that include surface word forms, part of speech tags using the Brill tagger (Brill, 1992) and orthographic features. Additionally we used head-noun features that</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In B. Scholkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald A B Lindberg</author>
<author>L Humphreys</author>
<author>Betsy</author>
<author>T McCray</author>
<author>Alexa</author>
</authors>
<title>The unified medical language system. Methods ofInformation in Medicine, 32:281– 291. level features; Or: Orthographic features; Head: Head noun features; POS: part of speech features.</title>
<date>1993</date>
<contexts>
<context position="5484" citStr="Lindberg et al., 1993" startWordPosition="895" endWordPosition="898">he traditional named-entity task used in MUC. For this reason we consider the task of term identification and classification to be an extended named entity task (NE+) in which the goal is to find types as well as individuals and where the term classes belong to an explicitly defined ontology. The use of an ontology allows us to associate human-readable terms in the domain with a set of computer-readable classes, relations, properties and axioms (Gruber, 1993). The particular difficulties with identifying and classifying terms in scientific and technical domains are the size of the vocabulary (Lindberg et al., 1993), an open growing vocabulary (Lovis et al., 1995), irregular naming conventions as well as extensive cross-over in vocabulary between named entity classes. The irregular naming arises in part because of the number of researchers and practitioners from different fields who are working on the same knowledge discovery area as well as the large number of entities that need to be named. Despite the best efforts of major journals to standardize the terminology, there is also a significant problem with synonymy so that often an entity has more than one name that is widely used. In molecular biology f</context>
</contexts>
<marker>Lindberg, Humphreys, Betsy, McCray, Alexa, 1993</marker>
<rawString>Donald A.B. Lindberg, L. Humphreys, Betsy, and T. McCray, Alexa. 1993. The unified medical language system. Methods ofInformation in Medicine, 32:281– 291. level features; Or: Orthographic features; Head: Head noun features; POS: part of speech features.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Lovis</author>
<author>P Michel</author>
<author>R Baud</author>
<author>J Scherrer</author>
</authors>
<title>Word segmentation processing: a way to exponentially extend medical dictionaries.</title>
<date>1995</date>
<journal>Medinfo,</journal>
<pages>8--28</pages>
<contexts>
<context position="5533" citStr="Lovis et al., 1995" startWordPosition="903" endWordPosition="906">is reason we consider the task of term identification and classification to be an extended named entity task (NE+) in which the goal is to find types as well as individuals and where the term classes belong to an explicitly defined ontology. The use of an ontology allows us to associate human-readable terms in the domain with a set of computer-readable classes, relations, properties and axioms (Gruber, 1993). The particular difficulties with identifying and classifying terms in scientific and technical domains are the size of the vocabulary (Lindberg et al., 1993), an open growing vocabulary (Lovis et al., 1995), irregular naming conventions as well as extensive cross-over in vocabulary between named entity classes. The irregular naming arises in part because of the number of researchers and practitioners from different fields who are working on the same knowledge discovery area as well as the large number of entities that need to be named. Despite the best efforts of major journals to standardize the terminology, there is also a significant problem with synonymy so that often an entity has more than one name that is widely used. In molecular biology for example class cross-over of terms may arise be</context>
</contexts>
<marker>Lovis, Michel, Baud, Scherrer, 1995</marker>
<rawString>C. Lovis, P. Michel, R. Baud, and J. Scherrer. 1995. Word segmentation processing: a way to exponentially extend medical dictionaries. Medinfo, 8:28–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MEDLINE</author>
</authors>
<title>The PubMed database can be found at:.</title>
<date>1999</date>
<note>http://www.ncbi.nlm.nih.gov/PubMed/.</note>
<contexts>
<context position="3797" citStr="MEDLINE, 1999" startWordPosition="608" endWordPosition="609">ik, 1995), a state of the art model that has achieved new levels of performance in many classification tasks. In previous work we have shown SVMs to be superior to several other commonly used machine learning methods for named entity in previous experiments such as HMMs and C4.5 (citations omitted). This paper explores the underlying SVM model and shows through detailed empirical analysis the key features and parameter settings. To show the application of SVMs to term extraction in unstructured texts related to the medical sciences we are using a collection of abstracts from PubMed’s MEDLINE (MEDLINE, 1999). The MEDLINE database is an online collection of abstracts for published journal articles in biology and medicine and contains more than nine million articles. The collection we use in our tests is a controlled subset of MEDLINE obtained using three search keywords in the domain of molecular biology. From the retrieved abstracts 100 were randomly chosen for annotation by a human expert according to classes in a small top-level ontology. In the remainder of this paper in Section (2) we outline the background to the task and the data set we are using; in Section (3) we described the basic advan</context>
</contexts>
<marker>MEDLINE, 1999</marker>
<rawString>MEDLINE. 1999. The PubMed database can be found at:. http://www.ncbi.nlm.nih.gov/PubMed/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DARPA</author>
</authors>
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference(MUC-6),</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Columbia, MD, USA,</location>
<marker>DARPA, 1995</marker>
<rawString>DARPA. 1995. Proceedings of the Sixth Message Understanding Conference(MUC-6), Columbia, MD, USA, November. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>NLM</author>
</authors>
<title>Medical subject headings,</title>
<date>1997</date>
<institution>MD. National Library of Medicine.</institution>
<location>bethesda,</location>
<contexts>
<context position="6444" citStr="NLM, 1997" startWordPosition="1065" endWordPosition="1066">of entities that need to be named. Despite the best efforts of major journals to standardize the terminology, there is also a significant problem with synonymy so that often an entity has more than one name that is widely used. In molecular biology for example class cross-over of terms may arise because many DNA and RNA are named after the protein with which they transcribe. This semantic ambiguity which is dependent on often complex contextual conditions is one of the main reasons why we need learnable models and why it is difficult to re-use existing term lists and vocabularies such as MeSH(NLM, 1997), UMLS (Lindberg et al., 1993) or those found in databases such as SwissProt (Bairoch and Apweiler, 1997). An additional obstacle to re-use is that the classification scheme used within an existing thesaurus or database may not be the same as the one in the users’ ontology which may change from time to time as the consensus view of the structure of knowledge is refined. Our work has focussed on identifying names belonging to the classes shown in Table 1 which are all taken from the domain of molecular biology. Example sentences from a marked up abstract are given in Figure 1. The ontology (Tat</context>
</contexts>
<marker>NLM, 1997</marker>
<rawString>NLM. 1997. Medical subject headings, bethesda, MD. National Library of Medicine.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
<author>Ralph Grishman</author>
<author>Hiroyuki Shinnou</author>
</authors>
<title>A Decision Tree Method for Finding and Classifying Names in Japanese Texts.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora,</booktitle>
<location>Montreal, Canada,</location>
<contexts>
<context position="2872" citStr="Sekine et al., 1998" startWordPosition="453" endWordPosition="456">ic goal is to join this with deep semantic representations so that computers can obtain a full understanding of the facts in a text using logical inference and reasoning. The scenario is that human experts will create taxonomies and axioms (ontologies) and by providing a small set of annotated examples, machine learning can take over the role of instance capturing though information extraction technology. Recent studies into the use of supervised learningbased models for the named entity task have shown that models based on hidden Markov models (HMMs) (Bikel et al., 1997), and decision trees (Sekine et al., 1998), and maximum entropy (Borthwick et al., 1998) are much more generalisable and adaptable to new classes of words than systems based on hand-built patterns (including wrappers) and domain specific heuristic rules such as (Herzig and Johns, 1997). The method we use is based on support vector machines (SVMs)(Vapnik, 1995), a state of the art model that has achieved new levels of performance in many classification tasks. In previous work we have shown SVMs to be superior to several other commonly used machine learning methods for named entity in previous experiments such as HMMs and C4.5 (citation</context>
</contexts>
<marker>Sekine, Grishman, Shinnou, 1998</marker>
<rawString>Satoshi Sekine, Ralph Grishman, and Hiroyuki Shinnou. 1998. A Decision Tree Method for Finding and Classifying Names in Japanese Texts. In Proceedings of the Sixth Workshop on Very Large Corpora, Montreal, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Takeuchi</author>
<author>N Collier</author>
</authors>
<title>Use of support vector machines in extended named entity recognition.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Conference on Natural Language Learning</booktitle>
<pages>119--125</pages>
<contexts>
<context position="11986" citStr="Takeuchi and Collier, 2002" startWordPosition="2035" endWordPosition="2038">ssified training examples. The number of parameters to be estimated in α therefore never exceeds the number of examples. The influence of αi basically means that training examples with αi &gt; 0 define the decision function (the support vectors) and those examples with αi = 0 have no influence, making the final model very compact and testing (but not training) very fast. The point x is classified as positive (or negative) if f(x) &gt; 0 (or f(x) &lt; 0). The kernel function we explored in our experiments was the polynomial function k(xi,xj) = (xi · xj + 1)d for d = 2 which was found to be the best by (Takeuchi and Collier, 2002). Once input vectors have been mapped to the feature space the linear discrimination function which is found is the one which gives the maximum the geometric margin between the two classes in the feature space. Besides efficiency of representation, SVMs are known to maximize their generalizability, making them an ideal model for the NE+ task. Generalizability in SVMs is based on statistical learning theory and the observation that it is useful sometimes to misclassify some of the training data so that the margin between other training points is maximized. This is particularly useful for real w</context>
</contexts>
<marker>Takeuchi, Collier, 2002</marker>
<rawString>K. Takeuchi and N. Collier. 2002. Use of support vector machines in extended named entity recognition. In Proceedings of the 6th Conference on Natural Language Learning 2002 (CoNLL-2002), Roth, D. and van den Bosch, A. (eds), pages 119–125, August 31st.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Tapanainen</author>
<author>T J¨arvinen</author>
</authors>
<title>A non-projective dependency parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing, Washington D.C., Association of Computational Linguistics,</booktitle>
<pages>64--71</pages>
<marker>Tapanainen, J¨arvinen, 1997</marker>
<rawString>P. Tapanainen and T. J¨arvinen. 1997. A non-projective dependency parser. In Proceedings of the 5th Conference on Applied Natural Language Processing, Washington D.C., Association of Computational Linguistics, pages 64–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Tateishi</author>
<author>T Ohta</author>
<author>N Collier</author>
<author>C Nobata</author>
<author>K Ibushi</author>
<author>J Tsujii</author>
</authors>
<title>Building an annotated corpus in the molecular-biology domain.</title>
<date>2000</date>
<booktitle>In COLING’2000 Workshop on Semantic Annotation and Intelligent Content,</booktitle>
<location>Luxemburg, 5th–6th</location>
<contexts>
<context position="7063" citStr="Tateishi et al., 2000" startWordPosition="1174" endWordPosition="1177">97), UMLS (Lindberg et al., 1993) or those found in databases such as SwissProt (Bairoch and Apweiler, 1997). An additional obstacle to re-use is that the classification scheme used within an existing thesaurus or database may not be the same as the one in the users’ ontology which may change from time to time as the consensus view of the structure of knowledge is refined. Our work has focussed on identifying names belonging to the classes shown in Table 1 which are all taken from the domain of molecular biology. Example sentences from a marked up abstract are given in Figure 1. The ontology (Tateishi et al., 2000) that underlies this classification scheme describes a simple top-level model which is almost flat except for the source class which shows places where genetic activity occurs and has a number of sub-types. Further discussion of our use of deep semantic structures in the ontology is given elsewhere1 and we will now focus our attention on the machine learning model used to capture low level semantics. The training set we used in our experiments called Bio1 consists of 100 MEDLINE abstracts, marked up in XML by a doctoral-qualified domain expert 1Now being submitted for publication Class # Descr</context>
</contexts>
<marker>Tateishi, Ohta, Collier, Nobata, Ibushi, Tsujii, 2000</marker>
<rawString>Y. Tateishi, T. Ohta, N. Collier, C. Nobata, K. Ibushi, and J. Tsujii. 2000. Building an annotated corpus in the molecular-biology domain. In COLING’2000 Workshop on Semantic Annotation and Intelligent Content, Luxemburg, 5th–6th August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Thomas</author>
<author>D Milward</author>
<author>C Ouzounis</author>
<author>S Pulman</author>
<author>M Carroll</author>
</authors>
<title>Automatic extraction of protein interactions from scientific abstracts.</title>
<date>1999</date>
<booktitle>In Proceedings of the Pacific Symposium on Biocomputing’99 (PSB’99),</booktitle>
<pages>1--12</pages>
<location>Hawaii, USA,</location>
<contexts>
<context position="1233" citStr="Thomas et al., 1999" startWordPosition="181" endWordPosition="184">e and related disciplines. We illustrate SVM’s capabilities using a sample of 100 journal abstracts texts taken from the {human, blood cell, transcription factor} domain of MEDLINE. Approximately 3400 terms are annotated and the model performs at about 74% F-score on cross-validation tests. A detailed analysis based on empirical evidence shows the contribution of various feature sets to performance. 1 Introduction With the rapid growth in the number of published papers in the scientific fields such as medicine there has been growing interest in the application of Information Extraction (IE), (Thomas et al., 1999) (Craven and Kumlien, 1999), to help solve some of the problems that are associated with information overload. IE can benefit the medical sciences by enabling the automatic extraction of facts related to prototypical events such as those contained in patient records or research articles regarding molecular processes and their affect on human health. These facts can then be used to populate databases, aid in searching or document summarization and a variety of tasks which require the computer to have an intelligent understanding of the contents inside a document. Our aim here is to show a state</context>
</contexts>
<marker>Thomas, Milward, Ouzounis, Pulman, Carroll, 1999</marker>
<rawString>J. Thomas, D. Milward, C. Ouzounis, S. Pulman, and M. Carroll. 1999. Automatic extraction of protein interactions from scientific abstracts. In Proceedings of the Pacific Symposium on Biocomputing’99 (PSB’99), pages 1–12, Hawaii, USA, January 4–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Butterworths,</publisher>
<location>London.</location>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen. 1979. Information Retrieval. Butterworths, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V N Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag,</publisher>
<location>New York.</location>
<contexts>
<context position="3192" citStr="Vapnik, 1995" startWordPosition="506" endWordPosition="507">ake over the role of instance capturing though information extraction technology. Recent studies into the use of supervised learningbased models for the named entity task have shown that models based on hidden Markov models (HMMs) (Bikel et al., 1997), and decision trees (Sekine et al., 1998), and maximum entropy (Borthwick et al., 1998) are much more generalisable and adaptable to new classes of words than systems based on hand-built patterns (including wrappers) and domain specific heuristic rules such as (Herzig and Johns, 1997). The method we use is based on support vector machines (SVMs)(Vapnik, 1995), a state of the art model that has achieved new levels of performance in many classification tasks. In previous work we have shown SVMs to be superior to several other commonly used machine learning methods for named entity in previous experiments such as HMMs and C4.5 (citations omitted). This paper explores the underlying SVM model and shows through detailed empirical analysis the key features and parameter settings. To show the application of SVMs to term extraction in unstructured texts related to the medical sciences we are using a collection of abstracts from PubMed’s MEDLINE (MEDLINE, </context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. N. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag, New York.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>