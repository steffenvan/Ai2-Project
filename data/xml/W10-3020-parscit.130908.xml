<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.961497">
Features for Detecting Hedge Cues
</title>
<author confidence="0.853843">
Nobuyuki Shimizu Hiroshi Nakagawa
</author>
<affiliation confidence="0.812822">
Information Technology Center Information Technology Center
The University of Tokyo The University of Tokyo
</affiliation>
<email confidence="0.980461">
shimizu@r.dl.itc.u-tokyo.ac.jp n3@dl.itc.u-tokyo.ac.jp
</email>
<sectionHeader confidence="0.994378" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994325">
We present a sequential labeling approach
to hedge cue detection submitted to the bi-
ological portion of task 1 for the CoNLL-
2010 shared task. Our main approach is
as follows. We make use of partial syntac-
tic information together with features ob-
tained from the unlabeled corpus, and con-
vert the task into one of sequential BIO-
tagging. If a cue is found, a sentence is
classified as uncertain and certain other-
wise. To examine a large number of fea-
ture combinations, we employ a genetic al-
gorithm. While some features obtained by
this method are difficult to interpret, they
were shown to improve the performance of
the final system.
</bodyText>
<sectionHeader confidence="0.998776" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951349206349">
Research on automatically extracting factual in-
formation from biomedical texts has become pop-
ular in recent years. Since these texts are abundant
with hypotheses postulated by researchers, one
hurdle that an information extraction system must
overcome is to be able to determine whether or not
the information is part of a hypothesis or a factual
statement. Thus, detecting hedge cues that indi-
cate the uncertainty of the statement is an impor-
tant subtask of information extraction (IE). Hedge
cues include words such as “may”, “might”, “ap-
pear”, “suggest”, “putative” and “or”. They also
includes phrases such as “...raising an intriguing
question that...” As these expressions are sparsely
scattered throughout the texts, it is not easy to gen-
eralize results of machine learning from a training
set to a test set. Furthermore, simply finding the
expressions listed above does not guarantee that
a sentence contains a hedge. Their function as a
hedge cue depends on the surrounding context.
The primary objective of the CoNLL-2010
shared task (Farkas et al., 2010) is to detect hedge
cues and their scopes as are present in biomedi-
cal texts. In this paper, we focus on the biological
portion of task 1, and present a sequential labeling
approach to hedge cue detection. The following
summarizes the steps we took to achieve this goal.
Similarly to previous work in hedge cue detec-
tion (Morante and Daelemans, 2009), we first con-
vert the task into a sequential labeling task based
on the BIO scheme, where each word in a hedge
cue is labeled as B-CUE, I-CUE, or O, indicating
respectively the labeled word is at the beginning
of a cue, inside of a cue, or outside of a hedge
cue; this is similar to the tagging scheme from
the CoNLL-2001 shared task. We then prepared
features, and fed the training data to a sequential
labeling system, a discriminative Markov model
much like Conditional Random Fields (CRF), with
the difference being that the model parameters are
tuned using Bayes Point Machines (BPM), and
then compared our model against an equivalent
CRF model. To convert the result of sequential
labeling to sentence classification, we simply used
the presence of a hedge cue, i.e. if a cue is found, a
sentence is classified as uncertain and certain oth-
erwise.
To prepare features, we ran the GENIA tag-
ger to add partial syntactic parse and named en-
tity information. We also applied Porter’s stem-
mer (Jones and Willet, 1997) to each word in the
corpus. For each stem, we acquired the distribu-
tion of surrounding words from the unlabeled cor-
pus, and calculated the similarity between these
distributions and the distribution of hedge cues in
the training corpus. Given a stem and its similari-
ties to different hedge cues, we took the maximum
similarity and discretized it. All these features are
passed on to a sequential labeling system. Using
these base features, we then evaluated the effects
of feature combinations by repeatedly training the
system and selecting feature combinations that in-
creased the performance on a heldout set. To au-
</bodyText>
<page confidence="0.969445">
138
</page>
<bodyText confidence="0.948980125">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 138–143,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
tomate this process, we employed a genetic algo-
rithm.
The contribution of this paper is two-fold. First,
we describe our system, outlined above, that we
submitted to the CoNLL-2010 shared task in more
detail. Second, we analyze the effects of partic-
ular choices we made when building our system,
especially the feature combinations and learning
methods.
The rest of this paper is organized as follows.
In Section 2, we detail how the task of sequential
labeling is formalized in terms of linear classifi-
cation, and explain the Viterbi algorithm required
for prediction. We next present several algorithms
for optimizing the weight vector in a linear classi-
fier in Section 3. We then detail the complete list
of feature templates we used for the task of hedge
cue detection in Section 4. In order to evaluate the
effects of feature templates, in Section 5, we re-
move each feature template and find that several
feature templates overfit the training set. We fi-
nally conclude with Section 6.
</bodyText>
<sectionHeader confidence="0.757781" genericHeader="method">
2 Sequential Labeling
</sectionHeader>
<bodyText confidence="0.990726354166667">
We discriminatively train a Markov model us-
ing Bayes Point Machines (BPM). We will first
explain linear classification, and then apply a
Markov assumption to the classification formal-
ism. Then we will move on to BPM. Note that
we assume all features are binary in this and up-
coming sections as it is sufficient for the task at
hand.
In the setting of sequential labeling, given the
input sequence x = (x1, x2, x3, ...xn), a system
is asked to produce the output sequence y =
(y1, y2, y3, ...yn). Considering that y is a class,
sequential labeling is simply a classification with
a very large number of classes. Assuming that the
problem is one of linear classification, we may cre-
ate a binary feature vector O(x) for an input x and
have a weight vector wy of the same dimension
for each class y. We choose a class y that has the
highest dot product between the input vector and
the weight vector for the class y. For binary classi-
fication, this process is very simple: compare two
dot product values. Learning is therefore reduced
to specifying the weight vectors.
To follow the standard notations in sequential
labeling, let weight vectors wy be stacked into
one large vector w, and let O(x, y) be a binary
feature vector such that wTO(x, y) is equal to
wTy O(x). Classification is to choose y such that
y = argmaxy0(wTO(x, yl)).
Unfortunately, a large number of classes created
out of sequences makes the problem intractable,
so the Markov assumption factorizes y into a se-
quence of labels, such that a label yi is affected
only by the label before and after it (yi−1 and yi+1
respectively) in the sequence. Each structure, or
label y is now associated with a set of the parts
parts(y) such that y can be recomposed from the
parts. In the case of sequential labeling, parts con-
sist of states yi and transitions yi → yi+1 between
neighboring labels. We assume that the feature
vector for an entire structure y decomposes into
a sum over feature vectors for individual parts as
follows: O(x, y) = &amp;Eparts(y) O(x, r). Note that
we have overloaded the symbol O to apply to either
a structure y or its parts r.
The Markov assumption for factoring labels lets
us use the Viterbi algorithm (much like a Hidden
Markov Model) in order to find
</bodyText>
<equation confidence="0.99081575">
y = argmaxy, (w&gt;o(_, y0))
= argmaxy, (�nj=1 w&gt;o(_, //y0j)
+ �n−1
j=1 w&gt;ol_, y0j → y0j+1)).
</equation>
<sectionHeader confidence="0.981536" genericHeader="method">
3 Optimization
</sectionHeader>
<bodyText confidence="0.999979333333333">
We now turn to the optimization of the weight pa-
rameter w. We compare three approaches – Per-
ceptron, Bayes Point Machines and Conditional
Random Fields, using our c++ library for struc-
tured output prediction 1.
Perceptron is an online update scheme that
leaves the weights unchanged when the predicted
output matches the target, and changes them when
it does not. The update is:
</bodyText>
<equation confidence="0.910583">
wk := wk − O(xi, y) + O(xi, yi).
</equation>
<bodyText confidence="0.9873172">
Despite its seemingly simple update scheme, per-
ceptron is known for its effectiveness and perfor-
mance (Collins, 2002).
Conditional Random Fields (CRF) is a condi-
tional model
</bodyText>
<equation confidence="0.989548333333333">
1
P(y|x) = exp(wTO(x, y))
Zx
</equation>
<bodyText confidence="0.999913">
where w is the weight for each feature and Zx is a
normalization constant for each x.
</bodyText>
<equation confidence="0.444918">
EZx = exp(wTO(x, y))
y
</equation>
<footnote confidence="0.995754">
1Available at http://soplib.sourceforge.net/
</footnote>
<page confidence="0.99803">
139
</page>
<bodyText confidence="0.929665238095238">
for structured output prediction. To fit the weight
vector w using the training set {(xi, yi)}ni=1, we
use a standard gradient-descent method to find the
weight vector that maximizes the log likelihood
Eni logP(yi|xi) (Sha and Pereira, 2003). To
avoid overfitting, the log likelihood is often pe-
nalized with a spherical Gaussian weight prior:
Eni logP(yi|xi) − C||w||
2 . We also evaluated this
penalized version, varying the trade-off parameter
C.
Bayes Point Machines (BPM) for structured
prediction (Corston-Oliver et al., 2006) is an en-
semble learning algorithm that attempts to set the
weight w to be the Bayes Point which approxi-
mates to Bayesian inference for linear classifiers.
Assuming a uniform prior distribution over w, we
revise our belief of w after observing the training
data and produce a posterior distribution. We cre-
ate the final wbpm for classification using a poste-
rior distribution as follows:
</bodyText>
<equation confidence="0.961409">
wbpm = Ep(w|D)[w] =
</equation>
<bodyText confidence="0.999904818181818">
where p(w|D) is the posterior distribution of the
weights given the data D and Ep(w|D) is the ex-
pectation taken with respect to this distribution.
V (D) is the version space, which is the set of
weights wi that classify the training data correctly,
and |V (D) |is the size of the version space. In
practice, to explore the version space of weights
consistent with the training data, BPM trains a few
different perceptrons (Collins, 2002) by shuffling
the samples. The approximation of Bayes Point
wbpm is the average of these perceptron weights:
</bodyText>
<equation confidence="0.955241">
wbpm = Ep(w|D)[w] ≈
</equation>
<bodyText confidence="0.9991">
The pseudocode of the algorithm is shown in Al-
gorithm 3.1. We see that the inner loop is simply
a perceptron algorithm.
</bodyText>
<sectionHeader confidence="0.99994" genericHeader="method">
4 Features
</sectionHeader>
<subsectionHeader confidence="0.999757">
4.1 Base Features
</subsectionHeader>
<bodyText confidence="0.9989566">
For each sentence x, we have state features, rep-
resented by a binary vector O(x, y0j) and transition
features, again a binary vector O(x,y0j → y0j+1).
For transition features, we do not utilize lexical-
ized features. Thus, each dimension of O(x, y0j →
</bodyText>
<equation confidence="0.8995392">
Algorithm
3.1: BPM(K, T, {(xi, yi)}ni=1)
wbpm := 0;
fork:=1toK
Randomly shuffle the sequential order of
samples {(xi, yi)}ni=1
wk := 0;
for t := 1 to T # Perceptron iterations
for i := 1 to n # Iterate shuffled samples
y := argmaxy,(w&gt;k O(xi, y0))
if (y =�yi)
wk := wk − O(xi, y) + O(xi, yi);
1
wbpm := wbpm + K wk;
return (wbpm)
</equation>
<bodyText confidence="0.965301190476191">
y0j+1) is an indicator function that tests a com-
bination of labels, for example, O→B-CUE, B-
CUE→I-CUE or I-CUE→O.
For state features O(x, y0j), the indicator func-
tion for each dimension tests a combination of
y0j and lexical features obtained from x =
(x1, x2, x3, ...xn). We now list the base lexical
features that were considered for this experiment.
F0 a token, which is usually a word. As a part of
preprocessing, words in each input sentence
are tokenized using the GENIA tagger 2. This
tokenization coincides with Penn Treebank
style tokenization 3.
We add a subscript to indicate the position. Fj 0 is
exactly the input token xj. From xj, we also create
other lexical features such as Fj1 , Fj2 , Fj3, and so
on.
F1 the token in lower case, with digits replaced
by the symbol #.
F2 1 if the letters in the token are all capitalized,
0 otherwise.
</bodyText>
<footnote confidence="0.84716775">
F3 1 if the token contains a digit, 0 otherwise.
F4 1 if the token contains an uppercase letter, 0
otherwise.
F5 1 if the token contains a hyphen, 0 otherwise.
2Available at: http:// www-tsujii.is.s.u-tokyo.ac.jp/ GE-
NIA/ tagger/
3A tokenizer is available at: http:// www.cis.upenn.edu/
treebank/ tokenization.html
</footnote>
<equation confidence="0.625470875">
|V (D) |p(wi|D)wi
�
i=1
K
E
k=1
1
Kwk.
</equation>
<page confidence="0.946509">
140
</page>
<bodyText confidence="0.983568236842105">
F6 first letter in the token.
F7 first two letters in the token.
F8 first three letters in the token.
F9 last letter in the token.
F10 last two letters in the token.
F11 last three letters in the token.
The features F0 to F11 are known to be useful
for POS tagging. We postulated that since most
frequent hedge cues tend not to be nouns, these
features might help identify them.
The following three features are obtained by
running the GENIA tagger.
F12 a part of speech.
F13 a CoNLL-2000 style shallow parse. For ex-
ample, B-NP or I-NP indicates that the token
is a part of a base noun phrase, B-VP or I-VP
indicates that it is part of a verb phrase.
F14 named entity, especially a protein name.
F15 a word stem by Porter’s stemmer 4. Porter’s
stemmer removes common morphological
and inflectional endings from words in En-
glish. It is often used as part of an informa-
tion retrieval system.
Upon later inspection, it seems that Porter’s
stemmer may be too aggressive in stemming
words. The word putative, for example, after be-
ing processed by the stemmer, becomes simply put
(which is clearly erroneous).
The last nine types of features utilize the unla-
beled corpus for the biological portion of shared
task 1, provided by the shared task organizers.
For each stem, we acquire a histogram of sur-
rounding words, with a window size of 3, from
the unlabeled corpus. Each histogram is repre-
sented as a vector; the similarity between his-
tograms was then computed. The similarity met-
ric we used is called the Tanimoto coefficient, also
called extended/vector-based Jaccard coefficient.
</bodyText>
<equation confidence="0.9565115">
vi · vj
||vi ||+ ||vj ||− vi · vj
</equation>
<bodyText confidence="0.8753965">
It is based on the dot product of two vectors and
reduces to Jaccard coefficient for binary features.
</bodyText>
<footnote confidence="0.976181">
4Available at: http://tartarus.org/ martin/PorterStemmer/
</footnote>
<bodyText confidence="0.989982727272728">
This metric is known to perform quite well for
near-synonym discovery (Hagiwara et al., 2008).
Given a stem and its similarities to different hedge
cues, we took the maximum similarity and dis-
cretized it.
F16 1 if similarity is bigger than 0.9, 0 otherwise.
...
F19 1 if similarity is bigger than 0.6, 0 otherwise.
...
F24 1 if similarity is bigger than 0.1, 0 otherwise.
This concludes the base features we considered.
</bodyText>
<subsectionHeader confidence="0.99922">
4.2 Combinations of Base Features
</subsectionHeader>
<bodyText confidence="0.987179538461539">
In order to discover combinations of base features,
we implemented a genetic algorithm (Goldberg,
1989). It is an adaptive heuristic search algorithm
based on the evolutionary ideas of natural selec-
tion and genetics. After splitting the training set
into three partitions, given the first partition as the
training set, the fitness is measured by the score
of predicting the second partition. We removed
the feature sets that did not score high, and intro-
duced mutations – new feature sets – as replace-
ments. After several generations, surviving fea-
ture sets performed quite well. To avoid over fit-
ting, occasionally feature sets were evaluated on
the third partition, and we finally chose the feature
set according to this partition.
The features of the submitted system are listed
in Table 1. Note that Table 1 shows the dimensions
of the feature vector that evaluate to 1 given x and
y&apos;j. The actual feature vector is created by instan-
tiating all the combinations in the table using the
training set.
Surprisingly, our genetic algorithm removed
features F10 and F11, the last two/three let-
ters in a token. It also removed the POS in-
formation F12, but kept the sequence of POS
tags F12
</bodyText>
<equation confidence="0.99259125">
j−1, F12
j , F12
j+1, F12
j+2, F12
</equation>
<bodyText confidence="0.999654666666667">
j+3. The reason for
longer sequences is due to our heuristics for muta-
tions. Occasionally, we allowed the genetic algo-
rithm to insert a longer sequence of feature com-
binations at once. One other notable observation
is that shallow parses and NEs are removed. Be-
tween the various thresholds from F16 to F24,
it only kept F19, discovering 0.6 as a similarity
threshold.
</bodyText>
<page confidence="0.942205">
141
</page>
<equation confidence="0.99410069047619">
State φ(x, y0 )
j
0
yj
y0j, F0j−2
y0j, F0j−1
y0j, Fj0
y0 j, F j 0 , F 19
j
y0j,F 0 j−1,F j 0,F 0 j+1,F 0 j+2,F 0 j+3,F 0 j+4–(1)
y0j, F0j+1
y0 j, F0j+2
y0j, Fj 1
y0j, Fj2 –(2)
y0 j, F j 3
y0 j, F j 4
y0j, F4j−2, F4j−1, Fj4, F4 j+1, F4
j+2
y0j, Fj5
y0 j,F j 5,F7 j−1
y0j, Fj6
y0j, Fj7
y0j, Fj8
y0j, F9j−1, Fj9 , F9j+1, F9j+2, F9
j+3
y0 j, F 12
j−1, F 12
j , F 12
j+1, F 12
j+2, F 12
j+3
y0 j, F 15
j , F15
j+1, F15
j+2, F15
j+3
y0 j,F 19
j−2,F19
j−1, F 19
j , F 19
j+1, F 19
j+2
</equation>
<tableCaption confidence="0.891423">
Table 1: Features for Sequential Labeling
</tableCaption>
<sectionHeader confidence="0.992052" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9999608">
In order to examine the effects of learning parame-
ters, we conducted experiments on the test data af-
ter it was released to the participants of the shared
task.
While BPM has two parameters, K and T, we
fixed T = 5 and varied K, the number of percep-
trons. As increasing the number of perceptrons re-
sults in more thorough exploration of the version
space V (D), we expect that the performance of
the classifier would improve as K increases. Ta-
ble 2 shows how the number of perceptrons affects
the performance.
TP stands for True Positive, FP for False Pos-
itive, and FN for False Negative. The evaluation
metrics were precision P (the number of true pos-
</bodyText>
<table confidence="0.994735333333333">
K TP FP FN P (%) R (%) Fl (%)
10 641 80 149 88.90 81.14 84.84
20 644 79 146 89.07 81.52 85.13
30 644 80 146 88.95 81.52 85.07
40 645 81 145 88.84 81.65 85.09
50 645 80 145 88.97 81.65 85.15
</table>
<tableCaption confidence="0.999589">
Table 2: Effects of K in Bayes Point Machines
</tableCaption>
<bodyText confidence="0.999821047619048">
itives divided by the total number of elements la-
beled as belonging to the positive class) recall R
(the number of true positives divided by the to-
tal number of elements that actually belong to the
positive class) and their harmonic mean, the F1
score (F1 = 2PR/(P + R)). All figures in this
paper measure hedge cue detection performance at
the sentence classification level, not word/phrase
classification level. From the results, once the
number of perceptrons hits 20, the performance
stabilizes and does not seem to show any improve-
ment.
Next, in order to examine whether or not we
have overfitted to the training/heldout set, we re-
moved each row of Table 1 and reevaluated the
performance of the system. Reevaluation was
conducted on the labeled test set released by the
shared task organizers after our system’s output
had been initially evaluated. Thus, these figures
are comparable to the sentence classification re-
sults reported in Farkas et al. (2010).
</bodyText>
<table confidence="0.997981">
TP FP FN P (%) R (%) Fl (%)
1 647 79 143 89.12 81.90 85.36
2 647 80 143 89.00 81.90 85.30
1,2 647 81 143 88.87 81.90 85.24
</table>
<tableCaption confidence="0.827017">
Table 3: Effects of removing features (1) or (2), or
both
</tableCaption>
<bodyText confidence="0.979547260869565">
Table 3 shows the effect of removing (1), (2),
or both (1) and (2), showing that they overfit the
training data. Removing any other rows in Ta-
ble 1 resulted in decreased classification perfor-
mance. While there are other large combination
features such as ones involving F4, F9, F12, F15
and F19, we find that they do help improving the
performance of the classifier. Since these fea-
tures seem unintuitive to the authors, it is likely
that they would not have been found without the
genetic algorithm we employed. Error analysis
shows that inclusion of features involving F9 af-
fects prediction of “believe”, “possible”, “puta-
tive”, “assumed”, “seemed”, “if”, “presumably”,
“perhaps”, “suggestion”, “suppose” and “intrigu-
ing”. However, as this feature template is unfolded
into a large number of features, we were unable to
obtain further linguistic insights.
In the following experiments, we used the cur-
rently best performing features, that is, all fea-
tures except (1) in Table 1, and trained the classi-
fiers using the formalism of Perceptron and Con-
ditional Random Fields besides Bayes Point Ma-
</bodyText>
<page confidence="0.993835">
142
</page>
<bodyText confidence="0.999127066666667">
chines as we have been using. The results in Table
4 shows that BPM performs better than Percep-
tron or Conditional Random Fields. As the train-
ing time for BPM is better than CRF, our choice
of BPM helped us to run the genetic algorithm re-
peatedly as well. After several runs of empirical
tuning and tweaking, the hyper-parameters of the
algorithms were set as follows. Perceptron was
stopped at 40 iterations (T = 40). For BPM, we
fixed T = 5 and K = 20. For Conditional Ran-
dom Fields, we compared the penalized version
with C = 1 and the unpenalized version (C = 0).
The results in Table 4 is that of the unpenalized
version, as it performed better than the penalized
version.
</bodyText>
<figure confidence="0.975993777777778">
Perceptron
TP FP FN P (%) R (%) F1 (%)
671 128 119 83.98 84.94 84.46
Conditional Random Fields
TP FP FN P (%) R (%) F1 (%)
643 78 147 89.18 81.39 85.11
Bayes Point Machines
TP FP FN P (%) R (%) F1 (%)
647 79 143 89.12 81.90 85.36
</figure>
<tableCaption confidence="0.840238">
Table 4: Performance of different optimization
strategies
</tableCaption>
<sectionHeader confidence="0.998613" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999982083333333">
To tackle the hedge cue detection problem posed
by the CoNLL-2010 shared task, we utilized a
classifier for sequential labeling following previ-
ous work (Morante and Daelemans, 2009). An
essential part of this task is to discover the fea-
tures that allow us to predict unseen hedge expres-
sions. As hedge cue detection is semantic rather
than syntactic in nature, useful features such as
word stems tend to be specific to each word and
hard to generalize. However, by using a genetic al-
gorithm to examine a large number of feature com-
binations, we were able to find many features with
a wide context window of up to 5 words. While
some features are found to overfit, our analysis
shows that a number of these features are success-
fully applied to the test data yielding good general-
ized performance. Furthermore, we compared dif-
ferent optimization schemes for structured output
prediction using our c++ library, freely available
for download and use. We find that Bayes Point
Machines have a good trade-off between perfor-
mance and training speed, justifying our repeated
usage of BPM in the genetic algorithm for feature
selection.
</bodyText>
<sectionHeader confidence="0.997137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999982">
The authors would like to thank the reviewers for
their comments. This research was supported by
the Information Technology Center through their
grant to the first author. We would also like to
thank Mr. Ono, Mr. Yonetsuji and Mr. Yamada
for their contributions to the library.
</bodyText>
<sectionHeader confidence="0.999208" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999422666666667">
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of Empirical Methods in Natural Language Process-
ing (EMNLP).
Simon Corston-Oliver, Anthony Aue, Kevin Duh, and
Eric Ringger. 2006. Multilingual dependency pars-
ing using bayes point machines. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 160–167, June.
Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos
Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning (CoNLL-2010): Shared
Task, pages 1–12, Uppsala, Sweden. ACL.
David E. Goldberg. 1989. Genetic Algorithms
in Search, Optimization, and Machine Learning.
Addison-Wesley Professional.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2008. Context feature selection for distri-
butional similarity. In Proceedings of IJCNLP-08.
Karen Sp¨ark Jones and Peter Willet. 1997. Readings
in Information Retrieval. Morgan Kaufmann.
Roser Morante and Walter Daelemans. 2009. Learn-
ing the scope of hedge cues in biomedical texts.
In BioNLP ’09: Proceedings of the Workshop on
BioNLP, pages 28–36.
Fei Sha and Fernando Pereira. 2003. Shallow pars-
ing with conditional random fields. In Proceed-
ings of the Human Language Technology Confer-
ence (HLT).
</reference>
<page confidence="0.999112">
143
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.722500">
<title confidence="0.999734">Features for Detecting Hedge Cues</title>
<author confidence="0.991149">Nobuyuki Shimizu Hiroshi Nakagawa</author>
<affiliation confidence="0.9887275">Information Technology Center Information Technology Center The University of Tokyo The University of</affiliation>
<abstract confidence="0.985230444444445">shimizu@r.dl.itc.u-tokyo.ac.jp n3@dl.itc.u-tokyo.ac.jp Abstract We present a sequential labeling approach to hedge cue detection submitted to the biological portion of task 1 for the CoNLL- 2010 shared task. Our main approach is as follows. We make use of partial syntactic information together with features obtained from the unlabeled corpus, and convert the task into one of sequential BIOtagging. If a cue is found, a sentence is classified as uncertain and certain otherwise. To examine a large number of feature combinations, we employ a genetic algorithm. While some features obtained by this method are difficult to interpret, they were shown to improve the performance of the final system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</booktitle>
<contexts>
<context position="8021" citStr="Collins, 2002" startWordPosition="1350" endWordPosition="1351">y0)) = argmaxy, (�nj=1 w&gt;o(_, //y0j) + �n−1 j=1 w&gt;ol_, y0j → y0j+1)). 3 Optimization We now turn to the optimization of the weight parameter w. We compare three approaches – Perceptron, Bayes Point Machines and Conditional Random Fields, using our c++ library for structured output prediction 1. Perceptron is an online update scheme that leaves the weights unchanged when the predicted output matches the target, and changes them when it does not. The update is: wk := wk − O(xi, y) + O(xi, yi). Despite its seemingly simple update scheme, perceptron is known for its effectiveness and performance (Collins, 2002). Conditional Random Fields (CRF) is a conditional model 1 P(y|x) = exp(wTO(x, y)) Zx where w is the weight for each feature and Zx is a normalization constant for each x. EZx = exp(wTO(x, y)) y 1Available at http://soplib.sourceforge.net/ 139 for structured output prediction. To fit the weight vector w using the training set {(xi, yi)}ni=1, we use a standard gradient-descent method to find the weight vector that maximizes the log likelihood Eni logP(yi|xi) (Sha and Pereira, 2003). To avoid overfitting, the log likelihood is often penalized with a spherical Gaussian weight prior: Eni logP(yi|x</context>
<context position="9640" citStr="Collins, 2002" startWordPosition="1618" endWordPosition="1619">ter observing the training data and produce a posterior distribution. We create the final wbpm for classification using a posterior distribution as follows: wbpm = Ep(w|D)[w] = where p(w|D) is the posterior distribution of the weights given the data D and Ep(w|D) is the expectation taken with respect to this distribution. V (D) is the version space, which is the set of weights wi that classify the training data correctly, and |V (D) |is the size of the version space. In practice, to explore the version space of weights consistent with the training data, BPM trains a few different perceptrons (Collins, 2002) by shuffling the samples. The approximation of Bayes Point wbpm is the average of these perceptron weights: wbpm = Ep(w|D)[w] ≈ The pseudocode of the algorithm is shown in Algorithm 3.1. We see that the inner loop is simply a perceptron algorithm. 4 Features 4.1 Base Features For each sentence x, we have state features, represented by a binary vector O(x, y0j) and transition features, again a binary vector O(x,y0j → y0j+1). For transition features, we do not utilize lexicalized features. Thus, each dimension of O(x, y0j → Algorithm 3.1: BPM(K, T, {(xi, yi)}ni=1) wbpm := 0; fork:=1toK Randomly</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proceedings of Empirical Methods in Natural Language Processing (EMNLP).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Anthony Aue</author>
<author>Kevin Duh</author>
<author>Eric Ringger</author>
</authors>
<title>Multilingual dependency parsing using bayes point machines.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="8796" citStr="Corston-Oliver et al., 2006" startWordPosition="1471" endWordPosition="1474">ion constant for each x. EZx = exp(wTO(x, y)) y 1Available at http://soplib.sourceforge.net/ 139 for structured output prediction. To fit the weight vector w using the training set {(xi, yi)}ni=1, we use a standard gradient-descent method to find the weight vector that maximizes the log likelihood Eni logP(yi|xi) (Sha and Pereira, 2003). To avoid overfitting, the log likelihood is often penalized with a spherical Gaussian weight prior: Eni logP(yi|xi) − C||w|| 2 . We also evaluated this penalized version, varying the trade-off parameter C. Bayes Point Machines (BPM) for structured prediction (Corston-Oliver et al., 2006) is an ensemble learning algorithm that attempts to set the weight w to be the Bayes Point which approximates to Bayesian inference for linear classifiers. Assuming a uniform prior distribution over w, we revise our belief of w after observing the training data and produce a posterior distribution. We create the final wbpm for classification using a posterior distribution as follows: wbpm = Ep(w|D)[w] = where p(w|D) is the posterior distribution of the weights given the data D and Ep(w|D) is the expectation taken with respect to this distribution. V (D) is the version space, which is the set o</context>
</contexts>
<marker>Corston-Oliver, Aue, Duh, Ringger, 2006</marker>
<rawString>Simon Corston-Oliver, Anthony Aue, Kevin Duh, and Eric Ringger. 2006. Multilingual dependency parsing using bayes point machines. In Proceedings of the Human Language Technology Conference of the NAACL, Main Conference, pages 160–167, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Veronika Vincze</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>The CoNLL2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task,</booktitle>
<pages>1--12</pages>
<publisher>ACL.</publisher>
<location>Uppsala,</location>
<marker>Farkas, Vincze, M´ora, Csirik, Szarvas, 2010</marker>
<rawString>Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 1–12, Uppsala, Sweden. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David E Goldberg</author>
</authors>
<date>1989</date>
<booktitle>Genetic Algorithms in Search, Optimization, and Machine Learning.</booktitle>
<publisher>Addison-Wesley Professional.</publisher>
<contexts>
<context position="14038" citStr="Goldberg, 1989" startWordPosition="2389" endWordPosition="2390">4Available at: http://tartarus.org/ martin/PorterStemmer/ This metric is known to perform quite well for near-synonym discovery (Hagiwara et al., 2008). Given a stem and its similarities to different hedge cues, we took the maximum similarity and discretized it. F16 1 if similarity is bigger than 0.9, 0 otherwise. ... F19 1 if similarity is bigger than 0.6, 0 otherwise. ... F24 1 if similarity is bigger than 0.1, 0 otherwise. This concludes the base features we considered. 4.2 Combinations of Base Features In order to discover combinations of base features, we implemented a genetic algorithm (Goldberg, 1989). It is an adaptive heuristic search algorithm based on the evolutionary ideas of natural selection and genetics. After splitting the training set into three partitions, given the first partition as the training set, the fitness is measured by the score of predicting the second partition. We removed the feature sets that did not score high, and introduced mutations – new feature sets – as replacements. After several generations, surviving feature sets performed quite well. To avoid over fitting, occasionally feature sets were evaluated on the third partition, and we finally chose the feature s</context>
</contexts>
<marker>Goldberg, 1989</marker>
<rawString>David E. Goldberg. 1989. Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley Professional.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masato Hagiwara</author>
<author>Yasuhiro Ogawa</author>
<author>Katsuhiko Toyama</author>
</authors>
<title>Context feature selection for distributional similarity.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP-08.</booktitle>
<contexts>
<context position="13574" citStr="Hagiwara et al., 2008" startWordPosition="2309" endWordPosition="2312">zers. For each stem, we acquire a histogram of surrounding words, with a window size of 3, from the unlabeled corpus. Each histogram is represented as a vector; the similarity between histograms was then computed. The similarity metric we used is called the Tanimoto coefficient, also called extended/vector-based Jaccard coefficient. vi · vj ||vi ||+ ||vj ||− vi · vj It is based on the dot product of two vectors and reduces to Jaccard coefficient for binary features. 4Available at: http://tartarus.org/ martin/PorterStemmer/ This metric is known to perform quite well for near-synonym discovery (Hagiwara et al., 2008). Given a stem and its similarities to different hedge cues, we took the maximum similarity and discretized it. F16 1 if similarity is bigger than 0.9, 0 otherwise. ... F19 1 if similarity is bigger than 0.6, 0 otherwise. ... F24 1 if similarity is bigger than 0.1, 0 otherwise. This concludes the base features we considered. 4.2 Combinations of Base Features In order to discover combinations of base features, we implemented a genetic algorithm (Goldberg, 1989). It is an adaptive heuristic search algorithm based on the evolutionary ideas of natural selection and genetics. After splitting the tr</context>
</contexts>
<marker>Hagiwara, Ogawa, Toyama, 2008</marker>
<rawString>Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama. 2008. Context feature selection for distributional similarity. In Proceedings of IJCNLP-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Sp¨ark Jones</author>
<author>Peter Willet</author>
</authors>
<date>1997</date>
<booktitle>Readings in Information Retrieval.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="3320" citStr="Jones and Willet, 1997" startWordPosition="542" endWordPosition="545">sequential labeling system, a discriminative Markov model much like Conditional Random Fields (CRF), with the difference being that the model parameters are tuned using Bayes Point Machines (BPM), and then compared our model against an equivalent CRF model. To convert the result of sequential labeling to sentence classification, we simply used the presence of a hedge cue, i.e. if a cue is found, a sentence is classified as uncertain and certain otherwise. To prepare features, we ran the GENIA tagger to add partial syntactic parse and named entity information. We also applied Porter’s stemmer (Jones and Willet, 1997) to each word in the corpus. For each stem, we acquired the distribution of surrounding words from the unlabeled corpus, and calculated the similarity between these distributions and the distribution of hedge cues in the training corpus. Given a stem and its similarities to different hedge cues, we took the maximum similarity and discretized it. All these features are passed on to a sequential labeling system. Using these base features, we then evaluated the effects of feature combinations by repeatedly training the system and selecting feature combinations that increased the performance on a </context>
</contexts>
<marker>Jones, Willet, 1997</marker>
<rawString>Karen Sp¨ark Jones and Peter Willet. 1997. Readings in Information Retrieval. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>Learning the scope of hedge cues in biomedical texts.</title>
<date>2009</date>
<booktitle>In BioNLP ’09: Proceedings of the Workshop on BioNLP,</booktitle>
<pages>28--36</pages>
<contexts>
<context position="2303" citStr="Morante and Daelemans, 2009" startWordPosition="365" endWordPosition="368"> training set to a test set. Furthermore, simply finding the expressions listed above does not guarantee that a sentence contains a hedge. Their function as a hedge cue depends on the surrounding context. The primary objective of the CoNLL-2010 shared task (Farkas et al., 2010) is to detect hedge cues and their scopes as are present in biomedical texts. In this paper, we focus on the biological portion of task 1, and present a sequential labeling approach to hedge cue detection. The following summarizes the steps we took to achieve this goal. Similarly to previous work in hedge cue detection (Morante and Daelemans, 2009), we first convert the task into a sequential labeling task based on the BIO scheme, where each word in a hedge cue is labeled as B-CUE, I-CUE, or O, indicating respectively the labeled word is at the beginning of a cue, inside of a cue, or outside of a hedge cue; this is similar to the tagging scheme from the CoNLL-2001 shared task. We then prepared features, and fed the training data to a sequential labeling system, a discriminative Markov model much like Conditional Random Fields (CRF), with the difference being that the model parameters are tuned using Bayes Point Machines (BPM), and then </context>
<context position="20353" citStr="Morante and Daelemans, 2009" startWordPosition="3551" endWordPosition="3554">nd the unpenalized version (C = 0). The results in Table 4 is that of the unpenalized version, as it performed better than the penalized version. Perceptron TP FP FN P (%) R (%) F1 (%) 671 128 119 83.98 84.94 84.46 Conditional Random Fields TP FP FN P (%) R (%) F1 (%) 643 78 147 89.18 81.39 85.11 Bayes Point Machines TP FP FN P (%) R (%) F1 (%) 647 79 143 89.12 81.90 85.36 Table 4: Performance of different optimization strategies 6 Conclusion To tackle the hedge cue detection problem posed by the CoNLL-2010 shared task, we utilized a classifier for sequential labeling following previous work (Morante and Daelemans, 2009). An essential part of this task is to discover the features that allow us to predict unseen hedge expressions. As hedge cue detection is semantic rather than syntactic in nature, useful features such as word stems tend to be specific to each word and hard to generalize. However, by using a genetic algorithm to examine a large number of feature combinations, we were able to find many features with a wide context window of up to 5 words. While some features are found to overfit, our analysis shows that a number of these features are successfully applied to the test data yielding good generalize</context>
</contexts>
<marker>Morante, Daelemans, 2009</marker>
<rawString>Roser Morante and Walter Daelemans. 2009. Learning the scope of hedge cues in biomedical texts. In BioNLP ’09: Proceedings of the Workshop on BioNLP, pages 28–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Sha</author>
<author>Fernando Pereira</author>
</authors>
<title>Shallow parsing with conditional random fields.</title>
<date>2003</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT).</booktitle>
<contexts>
<context position="8506" citStr="Sha and Pereira, 2003" startWordPosition="1427" endWordPosition="1430">xi, y) + O(xi, yi). Despite its seemingly simple update scheme, perceptron is known for its effectiveness and performance (Collins, 2002). Conditional Random Fields (CRF) is a conditional model 1 P(y|x) = exp(wTO(x, y)) Zx where w is the weight for each feature and Zx is a normalization constant for each x. EZx = exp(wTO(x, y)) y 1Available at http://soplib.sourceforge.net/ 139 for structured output prediction. To fit the weight vector w using the training set {(xi, yi)}ni=1, we use a standard gradient-descent method to find the weight vector that maximizes the log likelihood Eni logP(yi|xi) (Sha and Pereira, 2003). To avoid overfitting, the log likelihood is often penalized with a spherical Gaussian weight prior: Eni logP(yi|xi) − C||w|| 2 . We also evaluated this penalized version, varying the trade-off parameter C. Bayes Point Machines (BPM) for structured prediction (Corston-Oliver et al., 2006) is an ensemble learning algorithm that attempts to set the weight w to be the Bayes Point which approximates to Bayesian inference for linear classifiers. Assuming a uniform prior distribution over w, we revise our belief of w after observing the training data and produce a posterior distribution. We create </context>
</contexts>
<marker>Sha, Pereira, 2003</marker>
<rawString>Fei Sha and Fernando Pereira. 2003. Shallow parsing with conditional random fields. In Proceedings of the Human Language Technology Conference (HLT).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>