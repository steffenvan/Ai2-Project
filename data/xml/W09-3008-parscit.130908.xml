<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.109460">
<title confidence="0.97337">
Designing a Language Game for Collecting Coreference Annotation
</title>
<author confidence="0.963886">
Barbora Hladk´a and JiˇriMirovsk´y and Pavel Schlesinger
</author>
<affiliation confidence="0.9614105">
Charles University in Prague
Institute of Formal and Applied Linguistics
</affiliation>
<email confidence="0.997513">
e-mail: {hladka, mirovsky, schlesinger@ufal.mff.cuni.cz}
</email>
<sectionHeader confidence="0.993902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99941">
PlayCoref is a concept of an on-line lan-
guage game designed to acquire a substan-
tial amount of text data with the corefer-
ence annotation. We describe in detail var-
ious aspects of the game design and dis-
cuss features that affect the quality of the
annotation.
</bodyText>
<sectionHeader confidence="0.999267" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999970148148148">
Creating a collection of high quality data is
resource-demanding regardless of the area of re-
search and type of the data. This fact has encour-
aged a formulation of an alternative way of data
collection, ”Games With a Purpose” methodology
(GWAP), (van Ahn and Dabbish, 2008). The
GWAP methodology exploits the capacity of In-
ternet users who like to play on-line games. The
games are designed to generate data for applica-
tions that either have not been implemented yet,
or have already been implemented with a perfor-
mance lower than human. The players work sim-
ply by playing the game - the data are generated as
a by-product of the game. The more enjoyable the
game is, the more users play it and the more data
is acquired.
The GWAP methodology was first used for on-
line games with images (van Ahn and Dabbish,
2004) and later with tunes (Law et al., 2007),1
in which the players try to agree on a caption of
the image/tune. The popularity of these games is
enormous and generates a huge amount of data.
Onto games (Siorpaes and Hepp, 2008) brought
another type of input data to GWAP – video and
text.2
The situation with text is slightly different. One
has to read a text in order to identify its topics.
</bodyText>
<footnote confidence="0.99994">
1www.gwap.org
2www.ontogame.org
</footnote>
<bodyText confidence="0.999834227272727">
Reading texts takes more time than observing im-
ages and the longer text, the worse. Since the
game must be of a dynamic character, it is unimag-
inable that the players would spend minutes read-
ing an input text. Therefore, it must be opened to
the players ’part’ by ’part’.
So far, besides the Onto games, two more games
with texts have appeared: What did Shannon
say?3, the goal of which is to help the speech
recognizer with difficult-to-recognize words, and
Phrase Detectives4 (Kruschwitz, Chamberlain,
Poesio, 2009), the goal of which is to identify re-
lationships between words and phrases in a text.
No information about their popularity has been
published yet.
Motivated by the GWAP portal, the LGame por-
tal5 dedicated to language games has been estab-
lished. The LGame portal has been opened with
the Shannon game, a game of intentionally hidden
words in the sentence, where players guess them,
and the Place the Space game, a game of word
segmentation.
</bodyText>
<sectionHeader confidence="0.985538" genericHeader="introduction">
2 Coreference
</sectionHeader>
<bodyText confidence="0.999212357142857">
Coreference occurs when several referring expres-
sions in a text refer to the same entity (e.g. per-
son, thing, fact). A coreferential pair is marked
between subsequent pairs of the referring expres-
sions. A sequence of coreferential pairs referring
to the same entity in a text forms a coreference
chain. The coreferential pairs and the coreference
chains cover only the identity relation.
Many projects for various languages on the
coreference annotation by linguists are running.
The annotated data serve as a basis for further
linguistic study of coreference, and most impor-
tantly also to train and test procedures for auto-
matic coreference resolution, which is a task that
</bodyText>
<footnote confidence="0.999061">
3lingo.clsp.jhu.edushannongame.html
4www.phrasedetectives.org
5www.lgame.cz
</footnote>
<page confidence="0.971458">
52
</page>
<note confidence="0.957815">
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 52–55,
Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999830444444444">
many other applications can benefit from, e.g. text
summarization, question answering, and informa-
tion retrieval.
Manual annotation is costly and time consum-
ing. We propose a design of the PlayCoref game
– to appear at the LGame portal – as an alternative
way of the coreference annotation collection, and
most importantly, of a substantially larger volume
than any expert annotation can ever achieve.
</bodyText>
<sectionHeader confidence="0.97296" genericHeader="method">
3 The PlayCoref Game
</sectionHeader>
<subsectionHeader confidence="0.996515">
3.1 Game Design
</subsectionHeader>
<bodyText confidence="0.999987414634146">
We prepare the game for Czech and English first.
However, PlayCoref can be played in any lan-
guage.
The game is designed for two players. The
game starts with several first sentences of the doc-
ument displayed in the players’ sentence window.
According to the restrictions put on the members
of the coreferential pairs, parts of the text are un-
locked (i.e. they are active) while the other parts
are locked (i.e. they are inactive); both of them
are graphically distinguished. In our case, only
nouns and selected pronouns are unlocked. The
players mark coreferential pairs between the in-
dividual unlocked words in the text (no phrases
are allowed). They mark the coreferential pairs as
undirected links.
During the session, the number of words the
opponent has linked into the coreferential pairs is
displayed to the player. The number of sentences
with at least one coreferential pair marked by the
opponent is displayed to the player as well. Re-
vealing more information about the opponent’s ac-
tions would affect the independency of the play-
ers’ decisions.
If the player finishes pairing all the related
words in the visible part of the document (visible
to him), he asks for the next sentence of the docu-
ment. It appears at the bottom of the player’s sen-
tence window. The player can remove pairs cre-
ated before at any time and can make new pairs in
the sentences read so far. The session goes on this
way until the end of the session time. More than
one document can be present in the session.
After the session, the players’ scores are calcu-
lated and displayed.
Instructions for the Players Instructions for the
players must be as comprehensible and concise as
possible. To mark a coreferential pair, no linguis-
tic knowledge is required, thus no extensive anno-
tation guidelines need to be formulated. It is all
about the text comprehension ability.
</bodyText>
<subsectionHeader confidence="0.999692">
3.2 Game Data
</subsectionHeader>
<bodyText confidence="0.999869428571429">
Any textual data can be used in the game, but the
following pre-processing steps are necessary.
Tagging Most importantly, the morphological
tagging (usually preceded by tokenization) is
required to recognize part-of-speech categories
(and sub-part-of-speech categories), in order to
lock/unlock individual words for the game. For
most languages, tagging is a well solved problem
(e.g. for Czech: the MORˇCE tagger6, for English:
TnT tagger7).
Text Parts Locking In the game, we work with
coreferential links between the individual words
only. The coreferential pairs that link larger text
parts consisting of clauses or even several sen-
tences are disregarded. Their marking requires lin-
guistic knowledge and extensive training.
Our research shows that pronouns that are usu-
ally members of such “undesirable” links can
be detected automatically in advance (at least in
Czech). They will get locked, so the players will
not consider them at all during the sessions.
</bodyText>
<sectionHeader confidence="0.417702" genericHeader="method">
Automatic Coreference Resolution According
</sectionHeader>
<bodyText confidence="0.9992616">
to the way we calculate the players scores (see be-
low), an automatic procedure for coreference res-
olution is required. If this procedure works on a
different layer than the surface layer, further auto-
matic processing of the data may be needed.
</bodyText>
<sectionHeader confidence="0.983333" genericHeader="method">
4 Data Quality
</sectionHeader>
<subsectionHeader confidence="0.981691">
4.1 Players’ Score
</subsectionHeader>
<bodyText confidence="0.99995025">
We want to obtain a large volume of data so we
must first attract the players and motivate them
to play the game more and more. As a reward
for their effort we present scoring. We hope that
the players’ appetite to win, to confront with their
opponents and to place well in the long-term top
scores tables correlates with our research aims and
objectives.
Our goal is to ensure the highest quality of the
annotation. The scoring function should reflect
the game data quality and thus motivate the play-
ers to produce the right data. An agreement with
</bodyText>
<footnote confidence="0.995570333333333">
6ufal.mff.cuni.cz/morce
7www.coli.uni-saarland.de/˜thorsten/
tnt/
</footnote>
<page confidence="0.998486">
53
</page>
<bodyText confidence="0.999888055555555">
the manual expert annotation would be a perfect
scoring function. But the manual annotation is not
available for all languages and above all, it is not
our goal to annotate already annotated data.
An automatic coreference resolution procedure
serves as a first approximation for the scoring
function. Since the procedure does not work for
“100%”, we need to add another component. We
suppose that most of the players will mark the
coreferential pairs reliably. Then an agreement
between the players’ pairs indicates correctness,
even if the pair differs from the output of auto-
matic coreference resolution procedure. There-
fore, the inter-player agreement will become the
second component of the scoring function. To mo-
tivate the players to ask for more parts of the text
(and not only “tune” links in the initially displayed
sentences), the third component of the scoring
function will award number of created coreferen-
tial links.
The players get points for their coreferential
pairs according to the equation ptsA = w1 *
ICA(A, acr) + w2 * ICA(A, B) + w3 * N(A)
where A and B are the players, acr is an automatic
coreference resolution procedure, ICA stands for
the inter-coder agreement that we can simultane-
ously express either by the F-measure or Krippen-
dorff’s α (Krippendorf, 2004), N is a contribu-
tion of the number of created links, and weights
0 G w1, w2 G 1, w1, w2, w3 E R (summing to 1)
are set empirically.
The score is calculated at the end of the ses-
sion and no running score is being presented dur-
ing the session. From the scientific point of view,
the scores serve for the long term quality control
of the players’ annotation.
</bodyText>
<subsectionHeader confidence="0.959239">
4.2 Interactivity Issues
</subsectionHeader>
<bodyText confidence="0.999996344827586">
The degree of a player-to-player interactivity con-
tributes to the attractiveness of the game. From the
player’s point of view, the more interactivity, the
better. For example, knowing both his and the op-
ponent’s running score would be very stimulating
for the mutual competitiveness. From the linguis-
tics’ point of view, once any kind of interaction is
allowed, statistically pure independency between
the players’ decisions is lost. A reasonable trade-
off between the interactivity and the independency
must be achieved. Interactivity that would lead to
cheating and decreasing the quality of the game
data must be avoided.
Allowing the players to see their own running
score would lead to cheating. The players might
adjust their decisions according to the changes in
the score. Another possible extension of interac-
tivity that would lead to cheating is highlighting
words that the opponent used in the coreferential
pairs. The players might then wait for the oppo-
nent’s choice and again, adjust their decisions ac-
cordingly. Such game data would be strongly bi-
ased. However, we still believe that a slight idea of
what the opponent is doing can boost inter-coder
agreement and yet avoid cheating. Revealing the
information about the opponent’s number of pairs
and number of sentences with at least one pair of-
fers not zero but low interactivity, yet it will not
harm the quality of the data.
</bodyText>
<subsectionHeader confidence="0.999932">
4.3 Post-Processing
</subsectionHeader>
<bodyText confidence="0.999989555555555">
The players mark the coreferential links undi-
rected. This strategy differs from the general con-
ception of coreference being understood as either
the anaphoric or cataphoric relation depending on
the “direction” of the link in the text. We believe
that the players will benefit from this simplifica-
tion and so will the data quality. After the ses-
sion, the coreference chains are automatically re-
constructed from the coreferential pairs.
</bodyText>
<subsectionHeader confidence="0.994423">
4.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.9999401">
Data with manually annotated coreference will be
used to measure the game data quality. We will
also study how much the scoring function suffers
from the difference between the output of the au-
tomatic coreference resolution procedure and the
manual annotation (gold standard). For Czech, we
will use the data from PDT 2.0, for English from
MUC-6.
PDT 2.0 8 contains the annotation of grammat-
ical and pronominal textual coreference. Nomi-
nal textual coreference is being annotated in PDT
2.0 in an ongoing project (Nedoluzhko, 2007).
Since the PDT 2.0 coreference annotation oper-
ates on the so-called tectogrammatical layer (layer
of meaning) and PlayCoref plays on the surface
layer, the coreferential pairs must be projected to
the surface first. The process consists of several
steps and only a part of the coreferential pairs is
actually projectable to the surface (links between
nodes that have no surface counterpart get lost).
</bodyText>
<page confidence="0.8278715">
8ufal.mff.cuni.cz/pdt2.0
54
</page>
<bodyText confidence="0.999295705882353">
MUC-6 9 operates on the surface layer. This
data can be used in a much more straightforward
way. The coreferential pairs are marked between
nouns, noun phrases, and pronouns and no projec-
tion is needed. The links with noun phrases are
disregarded.
Evaluation Methods For the game data evalu-
ation, well established methods for calculating an
inter-annotator agreement in the coreference anno-
tation will be employed. These methods consider
a coreference chain to be a set of words and they
measure the agreement on the membership of the
individual words in the sets (Passonneau, 2004).
Weighted agreement coefficients such as Krippen-
dorf’s α (Krippendorf, 2004) need to be used -
sets of words can differ only partially, which does
not mean a total disagreement.
</bodyText>
<sectionHeader confidence="0.999755" genericHeader="method">
5 Further Work
</sectionHeader>
<bodyText confidence="0.996528944444444">
Acquisition Evaluation Process The quality of
the game annotation undergoes standard evalua-
tion. Apart from collecting, assuming the game
reaches sufficient popularity, long-term monitor-
ing of the players’ outputs can bring into question
new issues concerning the game data quality: How
much can we benefit from presenting a document
into more sessions? Should we prefer the output of
more reliable and experienced players during the
evaluation? Should we omit the output of ’not-so-
reliable’ players?
Named Entity Recognition The step of the
named entity recognition will be applied in the
subsequent stages of the project. Multi-word ex-
pressions that form a named entity (e.g. “Czech
National Bank”) will be presented to the players
as a single unit of annotation. We also plan to im-
plement a GWAP for named entity recognition.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.977874176470588">
We have presented the concept of the PlayCoref
game, a proposed language game that brings a
novel approach to collecting coreference annota-
tion of texts using the enormous potential of In-
ternet users. We have described the design of the
game and discussed the issues of interactivity of
the players and measuring the player score – is-
sues that are crucial both for the attractiveness of
the game and for the quality of the game data. The
9cs.nyu.edu/faculty/grishman/muc6.html
game can be applied on any textual data in any lan-
guage, providing certain basic tools also discussed
in the paper exist. The GWAPs are open-ended
stories so until the game is released, it is hard to
say if the players will find it attractive enough. If
so, we hope to collect a large volume of data with
coreference annotation at extremely low costs.
</bodyText>
<sectionHeader confidence="0.998282" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.995807666666667">
We gratefully acknowledge the support of the
Czech Ministry of Education (grants MSM-
0021620838 and LC536), the Czech Grant
Agency (grant 405/09/0729), and the Grant
Agency of Charles University in Prague (project
GAUK 138309).
</bodyText>
<sectionHeader confidence="0.999108" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999792916666667">
Klaus Krippendorf. 2004. Content Analysis: An Introduc-
tion to Its Methodology, second edition, chapter 11, Sage,
Thousand Oaks, CA.
Udo Kruschwitz, Jon Chamberlain, Massimo Poesio. 2009.
(Linguistic) Science Through Web Collaboration in the
ANAWIKI project. In Proceedings of the WebSci’09: So-
ciety On-Line, Athens, Greece, in press.
Lucie Kuˇcov´a, Eva Hajiˇcov´a. 2005. Coreferential Relations
in the Prague Dependency Treebank. In Proceedings of
the 5th International Conference on Discourse Anaphora
and Anaphor Resolution, San Miguel, Azores, pp. 97–102.
Edith. L. M. Law et al. 2007. Tagatune: A game for music
and sound annotation. In Proceedings of the Music In-
formation Retrieval Conference, Austrian Computer Soc.,
pp. 361–364.
Anna Nedoluzhko. 2007. Zpr´ava k anotov´an´ı rozˇs´ıˇren´e
textov´e koreference a bridging vztah˚u v Praˇzsk´em
z´avoslostn´ım korpusu (Annotating extended coreference
and bridging relations in PDT). Technical Report, UFAL,
MFF UK, Prague, Czech Republic.
Rebecca J. Passonneau. 2004. Computing Reliability for
Coreference. Proceedings of LREC, vol. 4, pp. 1503–
1506, Lisbon.
Katharina Siorpaes and Martin Hepp. 2008. Games with a
purpose for the Semantic Web. IEEE Intelligent Systems
Vol. 23, number 3, pp. 50–60.
Luis van Ahn and Laura Dabbish. 2004. Labelling images
with a computer game. In Proceedings of the SIGHI Con-
ference on Human Factors in Computing Systems, ACM
Press, New York, pp. 319–326.
Luis van Ahn and Laura Dabbish. 2008. Designing Games
with a Purpose. Communications of the ACM, vol. 51, No.
8, pp. 58–67.
Marc Vilain et al. 1995. A Model-Theoretic Coreference
Scoring Scheme. Proceedings of the Sixth Message Un-
derstanding Conference, pp. 45–52, Columbia, MD.
</reference>
<page confidence="0.999061">
55
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.527105">
<title confidence="0.999582">Designing a Language Game for Collecting Coreference Annotation</title>
<author confidence="0.609469">Hladk´a</author>
<affiliation confidence="0.945093">Charles University in Institute of Formal and Applied</affiliation>
<email confidence="0.911982">mirovsky,</email>
<abstract confidence="0.993934">PlayCoref is a concept of an on-line language game designed to acquire a substantial amount of text data with the coreference annotation. We describe in detail various aspects of the game design and discuss features that affect the quality of the annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Klaus Krippendorf</author>
</authors>
<title>Content Analysis: An Introduction to Its Methodology,</title>
<date>2004</date>
<location>Thousand Oaks, CA.</location>
<note>second edition, chapter 11, Sage,</note>
<contexts>
<context position="9135" citStr="Krippendorf, 2004" startWordPosition="1496" endWordPosition="1497">e second component of the scoring function. To motivate the players to ask for more parts of the text (and not only “tune” links in the initially displayed sentences), the third component of the scoring function will award number of created coreferential links. The players get points for their coreferential pairs according to the equation ptsA = w1 * ICA(A, acr) + w2 * ICA(A, B) + w3 * N(A) where A and B are the players, acr is an automatic coreference resolution procedure, ICA stands for the inter-coder agreement that we can simultaneously express either by the F-measure or Krippendorff’s α (Krippendorf, 2004), N is a contribution of the number of created links, and weights 0 G w1, w2 G 1, w1, w2, w3 E R (summing to 1) are set empirically. The score is calculated at the end of the session and no running score is being presented during the session. From the scientific point of view, the scores serve for the long term quality control of the players’ annotation. 4.2 Interactivity Issues The degree of a player-to-player interactivity contributes to the attractiveness of the game. From the player’s point of view, the more interactivity, the better. For example, knowing both his and the opponent’s runnin</context>
<context position="13000" citStr="Krippendorf, 2004" startWordPosition="2126" endWordPosition="2127">is data can be used in a much more straightforward way. The coreferential pairs are marked between nouns, noun phrases, and pronouns and no projection is needed. The links with noun phrases are disregarded. Evaluation Methods For the game data evaluation, well established methods for calculating an inter-annotator agreement in the coreference annotation will be employed. These methods consider a coreference chain to be a set of words and they measure the agreement on the membership of the individual words in the sets (Passonneau, 2004). Weighted agreement coefficients such as Krippendorf’s α (Krippendorf, 2004) need to be used - sets of words can differ only partially, which does not mean a total disagreement. 5 Further Work Acquisition Evaluation Process The quality of the game annotation undergoes standard evaluation. Apart from collecting, assuming the game reaches sufficient popularity, long-term monitoring of the players’ outputs can bring into question new issues concerning the game data quality: How much can we benefit from presenting a document into more sessions? Should we prefer the output of more reliable and experienced players during the evaluation? Should we omit the output of ’not-sor</context>
</contexts>
<marker>Krippendorf, 2004</marker>
<rawString>Klaus Krippendorf. 2004. Content Analysis: An Introduction to Its Methodology, second edition, chapter 11, Sage, Thousand Oaks, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Udo Kruschwitz</author>
<author>Jon Chamberlain</author>
<author>Massimo Poesio</author>
</authors>
<title>(Linguistic) Science Through Web Collaboration in the ANAWIKI project.</title>
<date>2009</date>
<booktitle>In Proceedings of the WebSci’09: Society On-Line,</booktitle>
<location>Athens, Greece,</location>
<note>in press.</note>
<marker>Kruschwitz, Chamberlain, Poesio, 2009</marker>
<rawString>Udo Kruschwitz, Jon Chamberlain, Massimo Poesio. 2009. (Linguistic) Science Through Web Collaboration in the ANAWIKI project. In Proceedings of the WebSci’09: Society On-Line, Athens, Greece, in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucie Kuˇcov´a</author>
<author>Eva Hajiˇcov´a</author>
</authors>
<title>Coreferential Relations in the Prague Dependency Treebank.</title>
<date>2005</date>
<booktitle>In Proceedings of the 5th International Conference on Discourse Anaphora and Anaphor Resolution,</booktitle>
<pages>97--102</pages>
<location>San Miguel, Azores,</location>
<marker>Kuˇcov´a, Hajiˇcov´a, 2005</marker>
<rawString>Lucie Kuˇcov´a, Eva Hajiˇcov´a. 2005. Coreferential Relations in the Prague Dependency Treebank. In Proceedings of the 5th International Conference on Discourse Anaphora and Anaphor Resolution, San Miguel, Azores, pp. 97–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L M Law</author>
</authors>
<title>Tagatune: A game for music and sound annotation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Music Information Retrieval Conference, Austrian Computer Soc.,</booktitle>
<pages>361--364</pages>
<marker>Law, 2007</marker>
<rawString>Edith. L. M. Law et al. 2007. Tagatune: A game for music and sound annotation. In Proceedings of the Music Information Retrieval Conference, Austrian Computer Soc., pp. 361–364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Nedoluzhko</author>
</authors>
<title>Zpr´ava k anotov´an´ı rozˇs´ıˇren´e textov´e koreference a bridging vztah˚u v Praˇzsk´em z´avoslostn´ım korpusu (Annotating extended coreference and bridging relations in PDT).</title>
<date>2007</date>
<tech>Technical Report,</tech>
<location>UFAL, MFF UK, Prague, Czech Republic.</location>
<contexts>
<context position="11911" citStr="Nedoluzhko, 2007" startWordPosition="1956" endWordPosition="1957">ce chains are automatically reconstructed from the coreferential pairs. 4.4 Evaluation Data with manually annotated coreference will be used to measure the game data quality. We will also study how much the scoring function suffers from the difference between the output of the automatic coreference resolution procedure and the manual annotation (gold standard). For Czech, we will use the data from PDT 2.0, for English from MUC-6. PDT 2.0 8 contains the annotation of grammatical and pronominal textual coreference. Nominal textual coreference is being annotated in PDT 2.0 in an ongoing project (Nedoluzhko, 2007). Since the PDT 2.0 coreference annotation operates on the so-called tectogrammatical layer (layer of meaning) and PlayCoref plays on the surface layer, the coreferential pairs must be projected to the surface first. The process consists of several steps and only a part of the coreferential pairs is actually projectable to the surface (links between nodes that have no surface counterpart get lost). 8ufal.mff.cuni.cz/pdt2.0 54 MUC-6 9 operates on the surface layer. This data can be used in a much more straightforward way. The coreferential pairs are marked between nouns, noun phrases, and prono</context>
</contexts>
<marker>Nedoluzhko, 2007</marker>
<rawString>Anna Nedoluzhko. 2007. Zpr´ava k anotov´an´ı rozˇs´ıˇren´e textov´e koreference a bridging vztah˚u v Praˇzsk´em z´avoslostn´ım korpusu (Annotating extended coreference and bridging relations in PDT). Technical Report, UFAL, MFF UK, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
</authors>
<title>Computing Reliability for Coreference.</title>
<date>2004</date>
<booktitle>Proceedings of LREC,</booktitle>
<volume>4</volume>
<pages>1503--1506</pages>
<location>Lisbon.</location>
<contexts>
<context position="12923" citStr="Passonneau, 2004" startWordPosition="2116" endWordPosition="2117">lost). 8ufal.mff.cuni.cz/pdt2.0 54 MUC-6 9 operates on the surface layer. This data can be used in a much more straightforward way. The coreferential pairs are marked between nouns, noun phrases, and pronouns and no projection is needed. The links with noun phrases are disregarded. Evaluation Methods For the game data evaluation, well established methods for calculating an inter-annotator agreement in the coreference annotation will be employed. These methods consider a coreference chain to be a set of words and they measure the agreement on the membership of the individual words in the sets (Passonneau, 2004). Weighted agreement coefficients such as Krippendorf’s α (Krippendorf, 2004) need to be used - sets of words can differ only partially, which does not mean a total disagreement. 5 Further Work Acquisition Evaluation Process The quality of the game annotation undergoes standard evaluation. Apart from collecting, assuming the game reaches sufficient popularity, long-term monitoring of the players’ outputs can bring into question new issues concerning the game data quality: How much can we benefit from presenting a document into more sessions? Should we prefer the output of more reliable and exp</context>
</contexts>
<marker>Passonneau, 2004</marker>
<rawString>Rebecca J. Passonneau. 2004. Computing Reliability for Coreference. Proceedings of LREC, vol. 4, pp. 1503– 1506, Lisbon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katharina Siorpaes</author>
<author>Martin Hepp</author>
</authors>
<title>Games with a purpose for the Semantic Web.</title>
<date>2008</date>
<journal>IEEE Intelligent Systems</journal>
<volume>23</volume>
<pages>50--60</pages>
<contexts>
<context position="1572" citStr="Siorpaes and Hepp, 2008" startWordPosition="261" endWordPosition="264">plications that either have not been implemented yet, or have already been implemented with a performance lower than human. The players work simply by playing the game - the data are generated as a by-product of the game. The more enjoyable the game is, the more users play it and the more data is acquired. The GWAP methodology was first used for online games with images (van Ahn and Dabbish, 2004) and later with tunes (Law et al., 2007),1 in which the players try to agree on a caption of the image/tune. The popularity of these games is enormous and generates a huge amount of data. Onto games (Siorpaes and Hepp, 2008) brought another type of input data to GWAP – video and text.2 The situation with text is slightly different. One has to read a text in order to identify its topics. 1www.gwap.org 2www.ontogame.org Reading texts takes more time than observing images and the longer text, the worse. Since the game must be of a dynamic character, it is unimaginable that the players would spend minutes reading an input text. Therefore, it must be opened to the players ’part’ by ’part’. So far, besides the Onto games, two more games with texts have appeared: What did Shannon say?3, the goal of which is to help the </context>
</contexts>
<marker>Siorpaes, Hepp, 2008</marker>
<rawString>Katharina Siorpaes and Martin Hepp. 2008. Games with a purpose for the Semantic Web. IEEE Intelligent Systems Vol. 23, number 3, pp. 50–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis van Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Labelling images with a computer game.</title>
<date>2004</date>
<booktitle>In Proceedings of the SIGHI Conference on Human Factors in Computing Systems,</booktitle>
<pages>319--326</pages>
<publisher>ACM Press,</publisher>
<location>New York,</location>
<marker>van Ahn, Dabbish, 2004</marker>
<rawString>Luis van Ahn and Laura Dabbish. 2004. Labelling images with a computer game. In Proceedings of the SIGHI Conference on Human Factors in Computing Systems, ACM Press, New York, pp. 319–326.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luis van Ahn</author>
<author>Laura Dabbish</author>
</authors>
<title>Designing Games with a Purpose.</title>
<date>2008</date>
<journal>Communications of the ACM,</journal>
<volume>51</volume>
<pages>58--67</pages>
<marker>van Ahn, Dabbish, 2008</marker>
<rawString>Luis van Ahn and Laura Dabbish. 2008. Designing Games with a Purpose. Communications of the ACM, vol. 51, No. 8, pp. 58–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
</authors>
<title>A Model-Theoretic Coreference Scoring Scheme.</title>
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference,</booktitle>
<pages>45--52</pages>
<location>Columbia, MD.</location>
<marker>Vilain, 1995</marker>
<rawString>Marc Vilain et al. 1995. A Model-Theoretic Coreference Scoring Scheme. Proceedings of the Sixth Message Understanding Conference, pp. 45–52, Columbia, MD.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>