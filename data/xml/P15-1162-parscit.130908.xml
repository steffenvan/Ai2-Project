<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.9988935">
Deep Unordered Composition Rivals Syntactic Methods
for Text Classification
</title>
<author confidence="0.997278">
Mohit Iyyer,1 Varun Manjunatha,1 Jordan Boyd-Graber,2 Hal Daum´e III1
</author>
<affiliation confidence="0.9976985">
1University of Maryland, Department of Computer Science and UMIACS
2University of Colorado, Department of Computer Science
</affiliation>
<email confidence="0.996881">
{miyyer,varunm,hal}@umiacs.umd.edu, Jordan.Boyd.Graber@colorado.edu
</email>
<sectionHeader confidence="0.997369" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953363636363">
Many existing deep learning models for
natural language processing tasks focus on
learning the compositionality of their in-
puts, which requires many expensive com-
putations. We present a simple deep neural
network that competes with and, in some
cases, outperforms such models on sen-
timent analysis and factoid question an-
swering tasks while taking only a fraction
of the training time. While our model is
syntactically-ignorant, we show significant
improvements over previous bag-of-words
models by deepening our network and ap-
plying a novel variant of dropout. More-
over, our model performs better than syn-
tactic models on datasets with high syn-
tactic variance. We show that our model
makes similar errors to syntactically-aware
models, indicating that for the tasks we con-
sider, nonlinearly transforming the input is
more important than tailoring a network to
incorporate word order and syntax.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999853433333334">
Vector space models for natural language process-
ing (NLP) represent words using low dimensional
vectors called embeddings. To apply vector space
models to sentences or documents, one must first
select an appropriate composition function, which
is a mathematical process for combining multiple
words into a single vector.
Composition functions fall into two classes: un-
ordered and syntactic. Unordered functions treat in-
put texts as bags of word embeddings, while syntac-
tic functions take word order and sentence structure
into account. Previously published experimental
results have shown that syntactic functions outper-
form unordered functions on many tasks (Socher
et al., 2013b; Kalchbrenner and Blunsom, 2013).
However, there is a tradeoff: syntactic functions
require more training time than unordered compo-
sition functions and are prohibitively expensive in
the case of huge datasets or limited computing re-
sources. For example, the recursive neural network
(Section 2) computes costly matrix/tensor products
and nonlinearities at every node of a syntactic parse
tree, which limits it to smaller datasets that can be
reliably parsed.
We introduce a deep unordered model that ob-
tains near state-of-the-art accuracies on a variety of
sentence and document-level tasks with just min-
utes of training time on an average laptop computer.
This model, the deep averaging network (DAN),
works in three simple steps:
</bodyText>
<listItem confidence="0.898922333333333">
1. take the vector average of the embeddings
associated with an input sequence of tokens
2. pass that average through one or more feed-
forward layers
3. perform (linear) classification on the final
layer’s representation
</listItem>
<bodyText confidence="0.999888846153846">
The model can be improved by applying a novel
dropout-inspired regularizer: for each training in-
stance, randomly drop some of the tokens’ embed-
dings before computing the average.
We evaluate DANs on sentiment analysis and fac-
toid question answering tasks at both the sentence
and document level in Section 4. Our model’s suc-
cesses demonstrate that for these tasks, the choice
of composition function is not as important as ini-
tializing with pretrained embeddings and using a
deep network. Furthermore, DANs, unlike more
complex composition functions, can be effectively
trained on data that have high syntactic variance. A
</bodyText>
<page confidence="0.928738">
1681
</page>
<note confidence="0.976032">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1681–1691,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999342571428571">
qualitative analysis of the learned layers suggests
that the model works by magnifying tiny but mean-
ingful differences in the vector average through
multiple hidden layers, and a detailed error analy-
sis shows that syntactically-aware models actually
make very similar errors to those of the more naive
DAN.
</bodyText>
<sectionHeader confidence="0.949153" genericHeader="introduction">
2 Unordered vs. Syntactic Composition
</sectionHeader>
<bodyText confidence="0.999984363636364">
Our goal is to marry the speed of unordered func-
tions with the accuracy of syntactic functions.
In this section, we first describe a class of un-
ordered composition functions dubbed “neural bag-
of-words models” (NBOW). We then explore more
complex syntactic functions designed to avoid
many of the pitfalls associated with NBOW mod-
els. Finally, we present the deep averaging network
(DAN), which stacks nonlinear layers over the tradi-
tional NBOW model and achieves performance on
par with or better than that of syntactic functions.
</bodyText>
<subsectionHeader confidence="0.904839">
2.1 Neural Bag-of-Words Models
</subsectionHeader>
<bodyText confidence="0.998458375">
For simplicity, consider text classification: map an
input sequence of tokens X to one of k labels. We
first apply a composition function g to the sequence
of word embeddings vw for w E X. The output of
this composition function is a vector z that serves
as input to a logistic regression function.
In our instantiation of NBOW, g averages word
embeddings1
</bodyText>
<equation confidence="0.966921">
1 �
z = g(w E X) = �X�
w∈X
</equation>
<bodyText confidence="0.9970735">
Feeding z to a softmax layer induces estimated
probabilities for each output label
</bodyText>
<equation confidence="0.995451">
yˆ = softmax(W3 · z + b), (2)
</equation>
<bodyText confidence="0.99467">
where the softmax function is
</bodyText>
<equation confidence="0.995687">
softmax(q) = Ek exp q (3)
�,=1 exp qj
</equation>
<bodyText confidence="0.996269">
W3 is a k x d matrix for a dataset with k output
labels, and b is a bias term.
We train the NBOW model to minimize cross-
entropy error, which for a single training instance
with ground-truth label y is
</bodyText>
<equation confidence="0.949793">
k
Aˆy) = yp log(ˆyp). (4)
p=1
</equation>
<footnote confidence="0.812675333333333">
1Preliminary experiments indicate that averaging outper-
forms the vector sum used in NBOW from Kalchbrenner et al.
(2014).
</footnote>
<bodyText confidence="0.9999352">
Before we describe our deep extension of the
NBOW model, we take a quick detour to discuss
syntactic composition functions. Connections to
other representation frameworks are discussed fur-
ther in Section 4.
</bodyText>
<subsectionHeader confidence="0.999869">
2.2 Considering Syntax for Composition
</subsectionHeader>
<bodyText confidence="0.999836355555556">
Given a sentence like “You’ll be more entertained
getting hit by a bus”, an unordered model like
NBOW might be deceived by the word “entertained”
to return a positive prediction. In contrast, syn-
tactic composition functions rely on the order and
structure of the input to learn how one word or
phrase affects another, sacrificing computational
efficiency in the process. In subsequent sections,
we argue that this complexity is not matched by a
corresponding gain in performance.
Recursive neural networks (RecNNs) are syntac-
tic functions that rely on natural language’s inher-
ent structure to achieve state-of-the-art accuracies
on sentiment analysis tasks (Tai et al., 2015). As in
NBOW, each word type has an associated embed-
ding. However, the composition function g now
depends on a parse tree of the input sequence. The
representation for any internal node in a binary
parse tree is computed as a nonlinear function of
the representations of its children (Figure 1, left).
A more powerful RecNN variant is the recursive
neural tensor network (RecNTN), which modifies
g to include a costly tensor product (Socher et al.,
2013b).
While RecNNs can model complex linguistic
phenomena like negation (Hermann et al., 2013),
they require much more training time than NBOW
models. The nonlinearities and matrix/tensor prod-
ucts at each node of the parse tree are expen-
sive, especially as model dimensionality increases.
RecNNs also require an error signal at every node.
One root softmax is not strong enough for the
model to learn compositional relations and leads
to worse accuracies than standard bag-of-words
models (Li, 2014). Finally, RecNNs require rela-
tively consistent syntax between training and test
data due to their reliance on parse trees and thus
cannot effectively incorporate out-of-domain data,
as we show in our question-answering experiments.
Kim (2014) shows that some of these issues can
be avoided by using a convolutional network in-
stead of a RecNN, but the computational complex-
ity increases even further (see Section 4 for runtime
comparisons).
What contributes most to the power of syntactic
</bodyText>
<equation confidence="0.975097583333333">
vw. (1)
1682
DAN softmax
h2 = f(W2 · h1 + b2)
h1 = f(W1 · av + b1)
av =
�
%=1
4
C;
4
Predator is a masterpiece
c1 c2 c3 c4
RecNN
softmax
z3 = f(W [z2] + b)
softmax
[c2 ]
z2 = f(W + b)
z1
softmax
z1 = f(W [c3] + b)
Predator is a masterpiece
c1 c2 c3 c4
</equation>
<figureCaption confidence="0.916712">
Figure 1: On the left, a RecNN is given an input sentence for sentiment classification. Softmax layers
</figureCaption>
<bodyText confidence="0.916598764705883">
are placed above every internal node to avoid vanishing gradient issues. On the right is a two-layer DAN
taking the same input. While the RecNN has to compute a nonlinear representation (purple vectors) for
every node in the parse tree of its input, this DAN only computes two nonlinear layers for every possible
input.
functions: the compositionality or the nonlineari-
ties? Socher et al. (2013b) report that removing the
nonlinearities from their RecNN models drops per-
formance on the Stanford Sentiment Treebank by
over 5% absolute accuracy. Most unordered func-
tions are linear mappings between bag-of-words
features and output labels, so might they suffer
from the same issue? To isolate the effects of syn-
tactic composition from the nonlinear transforma-
tions that are crucial to RecNN performance, we
investigate how well a deep version of the NBOW
model performs on tasks that have recently been
dominated by syntactically-aware models.
</bodyText>
<sectionHeader confidence="0.991175" genericHeader="method">
3 Deep Averaging Networks
</sectionHeader>
<bodyText confidence="0.999951125">
The intuition behind deep feed-forward neural net-
works is that each layer learns a more abstract rep-
resentation of the input than the previous one (Ben-
gio et al., 2013). We can apply this concept to the
NBOW model discussed in Section 2.1 with the ex-
pectation that each layer will increasingly magnify
small but meaningful differences in the word em-
bedding average. To be more concrete, take s1 as
the sentence “I really loved Rosamund Pike’s per-
formance in the movie Gone Girl” and generate s2
and s3 by replacing “loved” with “liked” and then
again by “despised”. The vector averages of these
three sentences are almost identical, but the aver-
ages associated with the synonymous sentences s1
and s2 are slightly more similar to each other than
they are to s3’s average.
Could adding depth to NBOW make small such
distinctions as this one more apparent? In Equa-
tion 1, we compute z, the vector representation for
input text X, by averaging the word vectors vw∈X.
Instead of directly passing this representation to an
output layer, we can further transform z by adding
more layers before applying the softmax. Suppose
we have n layers, z1...n. We compute each layer
</bodyText>
<equation confidence="0.976392">
zi = g(zi−1) = f(Wi · zi−1 + bi) (5)
</equation>
<bodyText confidence="0.999876083333333">
and feed the final layer’s representation, zn, to a
softmax layer for prediction (Figure 1, right).
This model, which we call a deep averaging net-
work (DAN), is still unordered, but its depth allows
it to capture subtle variations in the input better
than the standard NBOW model. Furthermore, com-
puting each layer requires just a single matrix multi-
plication, so the complexity scales with the number
of layers rather than the number of nodes in a parse
tree. In practice, we find no significant difference
between the training time of a DAN and that of the
shallow NBOW model.
</bodyText>
<subsectionHeader confidence="0.999524">
3.1 Word Dropout Improves Robustness
</subsectionHeader>
<bodyText confidence="0.999987363636364">
Dropout regularizes neural networks by randomly
setting hidden and/or input units to zero with some
probability p (Hinton et al., 2012; Srivastava et
al., 2014). Given a neural network with n units,
dropout prevents overfitting by creating an ensem-
ble of 2n different networks that share parameters,
where each network consists of some combination
of dropped and undropped units. Instead of drop-
ping units, a natural extension for the DAN model is
to randomly drop word tokens’ entire word embed-
dings from the vector average. Using this method,
</bodyText>
<page confidence="0.920741">
1683
</page>
<bodyText confidence="0.9995479">
which we call word dropout, our network theoreti-
cally sees 2|X |different token sequences for each
input X.
We posit a vector r with |X |independent
Bernoulli trials, each of which equals 1 with prob-
ability p. The embedding vw for token w in X is
dropped from the average if rw is 0, which expo-
nentially increases the number of unique examples
the network sees during training. This allows us to
modify Equation 1:
</bodyText>
<equation confidence="0.9518055">
rw ti Bernoulli(p) (6)
Xˆ= {w|w E X and rw &gt; 01 (7)
z = g(w E X) = Ew∈ X vw
|ˆX|
</equation>
<bodyText confidence="0.999991277777778">
Depending on the choice of p, many of the
“dropped” versions of an original training instance
will be very similar to each other, but for shorter
inputs this is less likely. We might drop a very
important token, such as “horrible” in “the crab
rangoon was especially horrible”; however, since
the number of word types that are predictive of the
output labels is low compared to non-predictive
ones (e.g., neutral words in sentiment analysis), we
always see improvements using this technique.
Theoretically, word dropout can also be applied
to other neural network-based approaches. How-
ever, we observe no significant performance differ-
ences in preliminary experiments when applying
word dropout to leaf nodes in RecNNs for senti-
ment analysis (dropped leaf representations are set
to zero vectors), and it slightly hurts performance
on the question answering task.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999887">
We compare DANs to both the shallow NBOW
model as well as more complicated syntactic mod-
els on sentence and document-level sentiment anal-
ysis and factoid question answering tasks. The DAN
architecture we use for each task is almost identi-
cal, differing across tasks only in the type of output
layer and the choice of activation function. Our
results show that DANs outperform other bag-of-
words models and many syntactic models with very
little training time.2 On the question-answering
task, DANs effectively train on out-of-domain data,
while RecNNs struggle to reconcile the syntactic
differences between the training and test data.
</bodyText>
<footnote confidence="0.591067">
2Codeathttp://github.com/miyyer/dan.
</footnote>
<table confidence="0.999258470588235">
Model RT SST SST IMDB Time
fine bin (s)
DAN-ROOT — 46.9 85.7 — 31
DAN-RAND 77.3 45.4 83.2 88.8 136
DAN 80.3 47.7 86.3 89.4 136
NBOW-RAND 76.2 42.3 81.4 88.9 91
NBOW 79.0 43.6 83.6 89.0 91
BiNB — 41.9 83.1 — —
NBSVM-bi 79.4 — — 91.2 —
RecNN* 77.7 43.2 82.4 — —
RecNTN* — 45.7 85.4 — —
DRecNN — 49.8 86.6 — 431
TreeLSTM — 50.6 86.9 — —
DCNN* — 48.5 86.9 89.4 —
PVEC* — 48.7 87.8 92.6 —
CNN-MC 81.1 47.4 88.1 — 2,452
WRRBM* — — — 89.2 —
</table>
<tableCaption confidence="0.994027">
Table 1: DANs achieve comparable sentiment accu-
</tableCaption>
<bodyText confidence="0.9800872">
racies to syntactic functions (bottom third of table)
but require much less training time (measured as
time of a single epoch on the SST fine-grained task).
Asterisked models are initialized either with differ-
ent pretrained embeddings or randomly.
</bodyText>
<subsectionHeader confidence="0.999781">
4.1 Sentiment Analysis
</subsectionHeader>
<bodyText confidence="0.9999776">
Recently, syntactic composition functions have
revolutionized both fine-grained and binary (pos-
itive or negative) sentiment analysis. We conduct
sentence-level sentiment experiments on the Rot-
ten Tomatoes (RT) movie reviews dataset (Pang
and Lee, 2005) and its extension with phrase-level
labels, the Stanford Sentiment Treebank (SST) in-
troduced by Socher et al. (2013b). Our model is
also effective on the document-level IMDB movie
review dataset of Maas et al. (2011).
</bodyText>
<subsubsectionHeader confidence="0.599434">
4.1.1 Neural Baselines
</subsubsectionHeader>
<bodyText confidence="0.999871923076923">
Most neural approaches to sentiment analysis are
variants of either recursive or convolutional net-
works. Our recursive neural network baselines
include standard RecNNs (Socher et al., 2011b),
RecNTNs, the deep recursive network (DRecNN)
proposed by ˙Irsoy and Cardie (2014), and the
TREE-LSTM of (Tai et al., 2015). Convolu-
tional network baselines include the dynamic con-
volutional network (Kalchbrenner et al., 2014,
DCNN) and the convolutional neural network multi-
channel (Kim, 2014, CNN-MC). Our other neu-
ral baselines are the sliding-window based para-
graph vector (Le and Mikolov, 2014, PVEC)3 and
</bodyText>
<footnote confidence="0.838176">
3PVEC is computationally expensive at both training and
test time and requires enough memory to store a vector for
every paragraph in the training data.
</footnote>
<equation confidence="0.965176">
. (8)
</equation>
<page confidence="0.92535">
1684
</page>
<bodyText confidence="0.997122">
the word-representation restricted Boltzmann ma-
chine (Dahl et al., 2012, WRRBM), which only
works on the document-level IMDB task.4
</bodyText>
<subsubsectionHeader confidence="0.787901">
4.1.2 Non-Neural Baselines
</subsubsectionHeader>
<bodyText confidence="0.999965833333333">
We also compare to non-neural baselines, specif-
ically the bigram naive Bayes (BINB) and naive
Bayes support vector machine (NBSVM-BI) mod-
els introduced by Wang and Manning (2012), both
of which are memory-intensive due to huge feature
spaces of size JV12.
</bodyText>
<subsectionHeader confidence="0.649807">
4.1.3 DAN Configurations
</subsectionHeader>
<bodyText confidence="0.999984076923077">
In Table 1, we compare a variety of DAN and NBOW
configurations5 to the baselines described above. In
particular, we are interested in not only comparing
DAN accuracies to those of the baselines, but also
how initializing with pretrained embeddings and re-
stricting the model to only root-level labels affects
performance. With this in mind, the NBOW-RAND
and DAN-RAND models are initialized with ran-
dom 300-dimensional word embeddings, while the
other models are initialized with publicly-available
300-d GloVe vectors trained over the Common
Crawl (Pennington et al., 2014). The DAN-ROOT
model only has access to sentence-level labels for
SST experiments, while all other models are trained
on labeled phrases (if they exist) in addition to sen-
tences. We train all NBOW and DAN models using
AdaGrad (Duchi et al., 2011).
We apply DANs to documents by averaging the
embeddings for all of a document’s tokens and
then feeding that average through multiple layers
as before. Since the representations computed by
DANs are always d-dimensional vectors regardless
of the input size, they are efficient with respect to
both memory and computational cost. We find that
the hyperparameters selected on the SST also work
well for the IMDB task.
</bodyText>
<subsectionHeader confidence="0.489159">
4.1.4 Dataset Details
</subsectionHeader>
<bodyText confidence="0.999968833333333">
We evaluate over both fine-grained and binary
sentence-level classification tasks on the SST, and
just the binary task on RT and IMDB. In the fine-
grained SST setting, each sentence has a label from
zero to five where two is the neutral class. For the
binary task, we ignore all neutral sentences.6
</bodyText>
<footnote confidence="0.997424571428571">
4The WRRBM is trained using a slow Metropolis-Hastings
algorithm.
5Best hyperparameters chosen by cross-validation: three
300-d ReLu layers, word dropout probability P = 0.3, L2
regularization weight of 1e-5 applied to all parameters
6Our fine-grained SST split is {train: 8,544, dev: 1,101,
test: 2,2101, while our binary split is {train: 6,920, dev:872,
</footnote>
<sectionHeader confidence="0.805315" genericHeader="method">
4.1.5 Results
</sectionHeader>
<bodyText confidence="0.999955214285714">
The DAN achieves the second best reported result
on the RT dataset, behind only the significantly
slower CNN-MC model. It’s also competitive with
more complex models on the SST and outperforms
the DCNN and WRRBM on the document-level
IMDB task. Interestingly, the DAN achieves good
performance on the SST when trained with only
sentence-level labels, indicating that it does not
suffer from the vanishing error signal problem that
plagues RecNNs. Since acquiring labelled phrases
is often expensive (Sayeed et al., 2012; Iyyer et
al., 2014b), this result is promising for large or
messy datasets where fine-grained annotation is
infeasible.
</bodyText>
<subsectionHeader confidence="0.526941">
4.1.6 Timing Experiments
</subsectionHeader>
<bodyText confidence="0.999977642857143">
DANs require less time per epoch and—in general—
require fewer epochs than their syntactic coun-
terparts. We compare DAN runtime on the SST
to publicly-available implementations of syntactic
baselines in the last column of Table 1; the reported
times are for a single epoch to control for hyper-
parameter choices such as learning rate, and all
models use 300-d word vectors. Training a DAN
on just sentence-level labels on the SST takes under
five minutes on a single core of a laptop; when
labeled phrases are added as separate training in-
stances, training time jumps to twenty minutes.7
All timing experiments were performed on a single
core of an Intel I7 processor with 8GB of RAM.
</bodyText>
<subsectionHeader confidence="0.983671">
4.2 Factoid Question Answering
</subsectionHeader>
<bodyText confidence="0.999888153846154">
DANs work well for sentiment analysis, but how
do they do on other NLP tasks? We shift gears
to a paragraph-length factoid question answering
task and find that our model outperforms other
unordered functions as well as a more complex
syntactic RecNN model. More interestingly, we
find that unlike the RecNN, the DAN significantly
benefits from out-of-domain Wikipedia training
data.
Quiz bowl is a trivia competition in which play-
ers are asked four-to-six sentence questions about
entities (e.g., authors, battles, or events). It is an
ideal task to evaluate DANs because there is prior
</bodyText>
<footnote confidence="0.992583571428571">
test:1,8211. Split sizes increase by an order of magnitude
when labeled phrases are added to the training set. For RT,
we do 10-fold CV over a balanced binary dataset of 10,662
sentences. Similarly, for the IMDB experiments we use the
provided balanced binary training set of 25,000 documents.
7We also find that DANs take significantly fewer epochs to
reach convergence than syntactic models.
</footnote>
<page confidence="0.829476">
1685
</page>
<table confidence="0.9999875">
Model Pos 1 Pos 2 Full Time(s)
BoW-DT 35.4 57.7 60.2 —
IR 37.5 65.9 71.4 N/A
QANTA 47.1 72.1 73.7 314
DAN 46.4 70.8 71.8 18
IR-WIKI 53.7 76.6 77.5 N/A
QANTA-WIKI 46.5 72.8 73.9 1,648
DAN-WIKI 54.8 75.5 77.1 119
</table>
<tableCaption confidence="0.707161555555556">
Table 2: The DAN achieves slightly lower accu-
racies than the more complex QANTA in much
less training time, even at early sentence posi-
tions where compositionality plays a bigger role.
When Wikipedia is added to the training set (bot-
tom half of table), the DAN outperforms QANTA
and achieves comparable accuracy to a state-of-the-
art information retrieval baseline, which highlights
a benefit of ignoring word order for this task.
</tableCaption>
<table confidence="0.598532375">
Effect of Word Dropout
●
●
● ●
●
●
0.0 0.1 0.2 0.3 0.4 0.5
Dropout Probability
</table>
<figureCaption confidence="0.954931">
Figure 2: Randomly dropping out 30% of words
</figureCaption>
<bodyText confidence="0.949311">
from the vector average is optimal for the quiz bowl
task, yielding a gain in absolute accuracy of almost
3% on the quiz bowl question dataset compared to
the same model trained with no word dropout.
work using both syntactic and unordered models
for quiz bowl question answering. In Boyd-Graber
et al. (2012), naive Bayes bag-of-words models
(BOW-DT) and sequential language models work
well on easy questions but poorly on harder ones.
A dependency-tree RecNN called QANTA proposed
in Iyyer et al. (2014a) shows substantial improve-
ments, leading to the hypothesis that correctly mod-
eling compositionality is crucial for answering hard
questions.
</bodyText>
<subsubsectionHeader confidence="0.833237">
4.2.1 Dataset and Experimental Setup
</subsubsectionHeader>
<bodyText confidence="0.997543">
To test this, we train a DAN over the history ques-
tions from Iyyer et al. (2014a).8 This dataset is aug-
</bodyText>
<footnote confidence="0.6382065">
8The training set contains 14,219 sentences over 3,761
questions. For more detail about data and baseline systems,
</footnote>
<bodyText confidence="0.999511529411765">
mented with 49,581 sentence/page-title pairs from
the Wikipedia articles associated with the answers
in the dataset. For fair comparison with QANTA,
we use a normalized tanh activation function at the
last layer instead of ReLu, and we also change the
output layer from a softmax to the margin rank-
ing loss (Weston et al., 2011) used in QANTA. We
initialize the DAN with the same pretrained 100-
d word embeddings that were used to initialize
QANTA.
We also evaluate the effectiveness of word
dropout on this task in Figure 2. Cross-validation
indicates that p = 0.3 works best for question an-
swering, although the improvement in accuracy is
negligible for sentiment analysis. Finally, continu-
ing the trend observed in the sentiment experiments,
DAN converges much faster than QANTA.
</bodyText>
<subsubsectionHeader confidence="0.971753">
4.2.2 DANs Improve with Noisy Data
</subsubsectionHeader>
<bodyText confidence="0.999356034482759">
Table 2 shows that while DAN is slightly worse
than QANTA when trained only on question-answer
pairs, it improves when trained on additional out-
of-domain Wikipedia data (DAN-WIKI), reaching
performance comparable to that of a state-of-the-art
information retrieval system (IR-WIKI). QANTA,
in contrast, barely improves when Wikipedia data is
added (QANTA-WIKI) possibly due to the syntactic
differences between Wikipedia text and quiz bowl
question text.
The most common syntactic structures in quiz
bowl sentences are imperative constructions such
as “Identify this British author who wrote Wuther-
ing Heights”, which are almost never seen in
Wikipedia. Furthermore, the subject of most quiz
bowl sentences is a pronoun or pronomial mention
referring to the answer, a property that is not true
of Wikipedia sentences (e.g., “Little of Emily’s
work from this period survives, except for poems
spoken by characters.”). Finally, many Wikipedia
sentences do not uniquely identify the title of the
page they come from, such as the following sen-
tence from Emily Bront¨e’s page: “She does not
seem to have made any friends outside her family.”
While noisy data affect both DAN and QANTA, the
latter is further hampered by the syntactic diver-
gence between quiz bowl questions and Wikipedia,
which may explain the lack of improvement in ac-
curacy.
</bodyText>
<note confidence="0.841151">
see Iyyer et al. (2014a).
</note>
<figure confidence="0.951109636363636">
71
70
69
History QB Accuracy
1686
50
40
30
20
10
0
</figure>
<figureCaption confidence="0.935365">
Figure 3: Perturbation response (difference in 1-
norm) at each layer of a 5-layer DAN after replac-
</figureCaption>
<bodyText confidence="0.729588166666667">
ing awesome in the film’s performances were awe-
some with four words of varying sentiment polarity.
While the shallow NBOW model does not show any
meaningful distinctions, we see that as the network
gets deeper, negative sentences are increasingly
different from the original positive sentence.
</bodyText>
<figure confidence="0.958021583333333">
Effect of Depth on Sentiment Accurac
●
●
● ● ● ●
●
● ●
●
●
●DAN
● ● DAN−ROOT
0 2 4 6
Number of Layers
</figure>
<figureCaption confidence="0.989653">
Figure 4: Two to three layers is optimal for the
</figureCaption>
<bodyText confidence="0.682603333333333">
DAN on the SST binary sentiment analysis task, but
adding any depth at all is an improvement over the
shallow NBOW model.
</bodyText>
<sectionHeader confidence="0.983079" genericHeader="method">
5 How Do DANs Work?
</sectionHeader>
<bodyText confidence="0.999751">
In this section we first examine how the deep layers
of the DAN amplify tiny differences in the vector av-
erage that are predictive of the output labels. Next,
we compare DANs to DRecNNs on sentences that
contain negations and contrastive conjunctions and
find that both models make similar errors despite
the latter’s increased complexity. Finally, we an-
alyze the predictive ability of unsupervised word
embeddings on a simple sentiment task in an effort
to explain why initialization with these embeddings
improves the DAN.
</bodyText>
<subsectionHeader confidence="0.997219">
5.1 Perturbation Analysis
</subsectionHeader>
<bodyText confidence="0.999932764705882">
Following the work of ˙Irsoy and Cardie (2014), we
examine our network by measuring the response at
each hidden layer to perturbations in an input sen-
tence. In particular, we use the template the film’s
performances were awesome and replace the fi-
nal word with increasingly negative polarity words
(cool, okay, underwhelming, the worst). For each
perturbed sentence, we observe how much the hid-
den layers differ from those associated with the
original template in 1-norm.
Figure 3 shows that as a DAN gets deeper, the dif-
ferences between negative and positive sentences
become increasingly amplified. While nonexistent
in the shallow NBOW model, these differences are
visible even with just a single hidden layer, thus
explaining why deepening the NBOW improves sen-
timent analysis as shown in Figure 4.
</bodyText>
<subsectionHeader confidence="0.9970545">
5.2 Handling Negations and “but”: Where
Syntax is Still Needed
</subsectionHeader>
<bodyText confidence="0.999976888888889">
While DANs outperform other bag-of-words mod-
els, how can they model linguistic phenomena such
as negation without considering word order? To
evaluate DANs over tougher inputs, we collect 92
sentences, each of which contains at least one nega-
tion and one contrastive conjunction, from the dev
and test sets of the SST.9 Our fine-grained accuracy
is higher on this subset than on the full dataset,
improving almost five percent absolute accuracy
to 53.3%. The DRecNN model of ˙Irsoy and Cardie
(2014) obtains a similar accuracy of 51.1%, con-
trary to our intuition that syntactic functions should
outperform unordered functions on sentences that
clearly require syntax to understand.10
Are these sentences truly difficult to classify? A
close inspection reveals that both the DAN and the
DRecNN have an overwhelming tendency to pre-
dict negative sentiment (60.9% and 55.4% of the
time for the DAN and DRecNN respectively) when
they see a negation compared to positive sentiment
(35.9% for DANs, 34.8% for DRecNNs). If we fur-
ther restrict our subset of sentences to only those
with positive ground truth labels, we find that while
both models struggle, the DRecNN obtains 41.7%
accuracy, outperforming the DAN’s 37.5%.
To understand why a negation or contrastive con-
junction triggers a negative sentiment prediction,
</bodyText>
<footnote confidence="0.92983825">
9We search for non-neutral sentences containing not / n’t,
and but. 48 of the sentences are positive while 44 are negative.
10Both models are initialized with pretrained 300-d GloVe
embeddings for fair comparison.
</footnote>
<figure confidence="0.994836466666667">
Perturbation Response
0 1 2 3 4 5
Layer
cool
okay
the worst
underwhel
Perturbation Response vs. Layer
ming
87
86
85
84
83
Binary Classification Accuracy
</figure>
<page confidence="0.989591">
1687
</page>
<tableCaption confidence="0.941189">
Table 3: Predictions of DAN and DRecNN models on real (top) and synthetic (bottom) sentences that
</tableCaption>
<bodyText confidence="0.981228">
contain negations and contrastive conjunctions. In the first column, words colored red individually predict
the negative label when fed to a DAN, while blue words predict positive. The DAN learns that the negators
not and n’t are strong negative predictors, which means it is unable to capture double negation as in the
last real example and the last synthetic example. The DRecNN does slightly better on the synthetic double
negation, predicting a lower negative polarity.
</bodyText>
<figure confidence="0.961245962264151">
Sentence DAN DRecNN Ground Truth
unlistenable
if you’re not a prepubescent girl, you’ll be laughing at
britney spears’ movie-starring debut whenever it does n’t
have you impatiently squinting at your watch
blessed with immense physical prowess he may well be, but
ahola is simply
who knows what exactly godard is on about in this film, but
his words and images do n’t have to add up to mesmerize
you.
several fine performances, it’s not a total loss
negative
negative
negative
neutral
negative
positive
not an actor
positive
positive
positive
a lousy movie that’s not merely unwatchable , but also
it’s so good that its relentless, polished wit can
withstand
negative
not only inept school productions, but even oliver parker’s
movie adaptation
too bad, but thanks to some lovely comedic
moments and
negative
negative
positive
negative
negative
negative
positive
positive
this movie was bad
negative
negative
negative
the movie was not bad
positive
negative
negative
this movie was not good
negative
negative
negative
this movie was good
positive
positive
positive
</figure>
<bodyText confidence="0.997681875">
we show six sentences from the negation subset and
four synthetic sentences in Table 3, along with both
models’ predictions. The token-level predictions in
the table (shown as colored boxes) are computed by
passing each token through the DAN as separate test
instances. The tokens not and n’t are strongly pre-
dictive of negative sentiment. While this simplified
“negation” works for many sentences in the datasets
we consider, it prevents the DAN from reasoning
about double negatives, as in “this movie was not
bad”. The DRecNN does slightly better in this case
by predicting a lesser negative polarity than the
DAN; however, we theorize that still more powerful
syntactic composition functions (and more labelled
instances of negation and related phenomena) are
necessary to truly solve this problem.
</bodyText>
<subsectionHeader confidence="0.997579">
5.3 Unsupervised Embeddings Capture
Sentiment
</subsectionHeader>
<bodyText confidence="0.999893703703704">
Our model consistently converges slower to a worse
solution (dropping 3% in absolute accuracy on
coarse-grained SST) when we randomly initialize
the word embeddings. This does not apply to just
DANs; both convolutional and recursive networks
do the same (Kim, 2014; ˙Irsoy and Cardie, 2014).
Why are initializations with these embeddings so
crucial to obtaining good performance? Is it pos-
sible that unsupervised training algorithms are al-
ready capturing sentiment?
We investigate this theory by conducting a sim-
ple experiment: given a sentiment lexicon contain-
ing both positive and negative words, we train a
logistic regression to discriminate between the asso-
ciated word embeddings (without any fine-tuning).
We use the lexicon created by Hu and Liu (2004),
which consists of 2,006 positive words and 4,783
negative words. We balance and split the dataset
into 3,000 training words and 1,000 test words.
Using 300-dimensional GloVe embeddings pre-
trained over the Common Crawl, we obtain over
95% accuracy on the unseen test set, supporting the
hypothesis that unsupervised pretraining over large
corpora can capture properties such as sentiment.
Intuitively, after the embeddings are fine-tuned
during DAN training, we might expect a decrease
in the norms of stopwords and an increase in the
</bodyText>
<page confidence="0.974349">
1688
</page>
<bodyText confidence="0.9998328">
norms of sentiment-rich words like “awesome” or
“horrible”. However, we find no significant dif-
ferences between the L2 norms of stopwords and
words in the sentiment lexicon of Hu and Liu
(2004).
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999783111111111">
Our DAN model builds on the successes of both
simple vector operations and neural network-based
models for compositionality.
There are a variety of element-wise vector op-
erations that could replace the average used in the
DAN. Mitchell and Lapata (2008) experiment with
many of them to model the compositionality of
short phrases. Later, their work was extended to
take into account the syntactic relation between
words (Erk and Pad´o, 2008; Baroni and Zampar-
elli, 2010; Kartsaklis and Sadrzadeh, 2013) and
grammars (Coecke et al., 2010; Grefenstette and
Sadrzadeh, 2011). While the average works best for
the tasks that we consider, Banea et al. (2014) find
that simply summing word2vec embeddings out-
performs all other methods on the SemEval 2014
phrase-to-word and sentence-to-phrase similarity
tasks.
Once we compute the embedding average in a
DAN, we feed it to a deep neural network. In con-
trast, most previous work on neural network-based
methods for NLP tasks explicitly model word or-
der. Outside of sentiment analysis, RecNN-based
approaches have been successful for tasks such
as parsing (Socher et al., 2013a), machine trans-
lation (Liu et al., 2014), and paraphrase detec-
tion (Socher et al., 2011a). Convolutional net-
works also model word order in local windows and
have achieved performance comparable to or bet-
ter than that of RecNNs on many tasks (Collobert
and Weston, 2008; Kim, 2014). Meanwhile, feed-
forward architectures like that of the DAN have
been used for language modeling (Bengio et al.,
2003), selectional preference acquisition (Van de
Cruys, 2014), and dependency parsing (Chen and
Manning, 2014).
</bodyText>
<sectionHeader confidence="0.999781" genericHeader="method">
7 Future Work
</sectionHeader>
<bodyText confidence="0.999903625">
In Section 5, we showed that the performance of
our DAN model worsens on sentences that con-
tain lingustic phenomena such as double negation.
One promising future direction is to cascade clas-
sifiers such that syntactic models are used only
when a DAN is not confident in its prediction. We
can also extend the DAN’s success at incorporating
out-of-domain training data to sentiment analysis:
imagine training a DAN on labeled tweets for clas-
sification on newspaper reviews. Another poten-
tially interesting application is to add gated units
to a DAN,as has been done for recurrent and recur-
sive neural networks (Hochreiter and Schmidhuber,
1997; Cho et al., 2014; Sutskever et al., 2014; Tai
et al., 2015), to drop useless words rather than
randomly-selected ones.
</bodyText>
<sectionHeader confidence="0.998507" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999956277777778">
In this paper, we introduce the deep averaging net-
work, which feeds an unweighted average of word
vectors through multiple hidden layers before clas-
sification. The DAN performs competitively with
more complicated neural networks that explicitly
model semantic and syntactic compositionality. It
is further strengthened by word dropout, a regu-
larizer that reduces input redundancy. DANs ob-
tain close to state-of-the-art accuracy on both sen-
tence and document-level sentiment analysis and
factoid question-answering tasks with much less
training time than competing methods; in fact, all
experiments were performed in a matter of min-
utes on a single laptop core. We find that both
DANs and syntactic functions make similar errors
given syntactically-complex input, which motivates
research into more powerful models of composi-
tionality.
</bodyText>
<sectionHeader confidence="0.99863" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999971076923077">
We thank Ozan ˙Irsoy not only for many insight-
ful discussions but also for suggesting some of
the experiments that we included in the paper.
We also thank the anonymous reviewers, Richard
Socher, Arafat Sultan, and the members of the
UMD “Thinking on Your Feet” research group for
their helpful comments. This work was supported
by NSF Grant IIS-1320538. Boyd-Graber is also
supported by NSF Grants CCF-1409287 and NCSE-
1422492. Any opinions, findings, conclusions, or
recommendations expressed here are those of the
authors and do not necessarily reflect the view of
the sponsor.
</bodyText>
<page confidence="0.994256">
1689
</page>
<sectionHeader confidence="0.9963" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999582981981982">
Carmen Banea, Di Chen, Rada Mihalcea, Claire Cardie, and
Janyce Wiebe. 2014. Simcompass: Using deep learn-
ing word embeddings to assess cross-level similarity. In
SemEval.
Marco Baroni and Roberto Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings of
Empirical Methods in Natural Language Processing.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Chris-
tian Jauvin. 2003. A neural probabilistic language model.
Journal of Machine Learning Research.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013.
Representation learning: A review and new perspectives.
IEEE Transactions on Pattern Analysis and Machine Intel-
ligence, 35(8):1798–1828.
Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal Daum´e
III. 2012. Besting the quiz master: Crowdsourcing incre-
mental classification games. In Proceedings of Empirical
Methods in Natural Language Processing.
Danqi Chen and Christopher D Manning. 2014. A fast and
accurate dependency parser using neural networks. In
Proceedings of Empirical Methods in Natural Language
Processing.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,
Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
2014. Learning phrase representations using rnn encoder-
decoder for statistical machine translation. In Proceedings
of Empirical Methods in Natural Language Processing.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010.
Mathematical foundations for a compositional distribu-
tional model of meaning. Linguistic Analysis (Lambek
Festschirft).
Ronan Collobert and Jason Weston. 2008. A unified ar-
chitecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of the
International Conference of Machine Learning.
George E Dahl, Ryan P Adams, and Hugo Larochelle. 2012.
Training restricted boltzmann machines on word observa-
tions. In Proceedings of the International Conference of
Machine Learning.
John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive
subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research.
Katrin Erk and Sebastian Pad´o. 2008. A structured vector
space model for word meaning in context. In Proceedings
of Empirical Methods in Natural Language Processing.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Ex-
perimental support for a categorical compositional distri-
butional model of meaning. In Proceedings of Empirical
Methods in Natural Language Processing.
Karl Moritz Hermann, Edward Grefenstette, and Phil Blun-
som. 2013. ”not not bad” is not ”bad”: A distributional
account of negation. Proceedings of the ACL Workshop on
Continuous Vector Space Models and their Compositional-
ity.
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. 2012. Improving
neural networks by preventing co-adaptation of feature
detectors. CoRR, abs/1207.0580.
Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-
term memory. Neural computation.
Minqing Hu and Bing Liu. 2004. Mining and summariz-
ing customer reviews. In Knowledge Discovery and Data
Mining.
Ozan ˙Irsoy and Claire Cardie. 2014. Deep recursive neural
networks for compositionality in language. In Proceedings
of Advances in Neural Information Processing Systems.
Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,
Richard Socher, and Hal Daum´e III. 2014a. A neural
network for factoid question answering over paragraphs.
In Proceedings of Empirical Methods in Natural Language
Processing.
Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, and Philip
Resnik. 2014b. Political ideology detection using recursive
neural networks. In Proceedings of the Association for
Computational Linguistics.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent convo-
lutional neural networks for discourse compositionality. In
ACL Workshop on Continuous Vector Space Models and
their Compositionality.
Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom.
2014. A convolutional neural network for modelling sen-
tences. In Proceedings of the Association for Computa-
tional Linguistics.
Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013. Prior
disambiguation of word tensors for constructing sentence
vectors. In Proceedings of Empirical Methods in Natural
Language Processing.
Yoon Kim. 2014. Convolutional neural networks for sentence
classification. In Proceedings of Empirical Methods in
Natural Language Processing.
Quoc V Le and Tomas Mikolov. 2014. Distributed represen-
tations of sentences and documents. In Proceedings of the
International Conference of Machine Learning.
Jiwei Li. 2014. Feature weight tuning for recursive neural
networks. CoRR, abs/1412.3714.
Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014. A
recursive recurrent neural network for statistical machine
translation. In Proceedings of the Association for Compu-
tational Linguistics.
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan
Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learn-
ing word vectors for sentiment analysis. In Proceedings of
the Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based models
of semantic composition. In Proceedings of the Association
for Computational Linguistics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with respect
to rating scales. In Proceedings of the Association for
Computational Linguistics.
</reference>
<page confidence="0.744822">
1690
</page>
<reference confidence="0.9993280625">
Jeffrey Pennington, Richard Socher, and Christopher Manning.
2014. Glove: Global vectors for word representation. In
Proceedings of Empirical Methods in Natural Language
Processing.
Asad B. Sayeed, Jordan Boyd-Graber, Bryan Rusk, and Amy
Weinberg. 2012. Grammatical structures for word-level
sentiment detection. In North American Association of
Computational Linguistics.
Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y.
Ng, and Christopher D. Manning. 2011a. Dynamic Pool-
ing and Unfolding Recursive Autoencoders for Paraphrase
Detection. In Proceedings of Advances in Neural Informa-
tion Processing Systems.
Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y.
Ng, and Christopher D. Manning. 2011b. Semi-Supervised
Recursive Autoencoders for Predicting Sentiment Distri-
butions. In Proceedings of Empirical Methods in Natural
Language Processing.
Richard Socher, John Bauer, Christopher D. Manning, and
Andrew Y. Ng. 2013a. Parsing With Compositional Vector
Grammars. In Proceedings of the Association for Compu-
tational Linguistics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang,
Christopher D Manning, Andrew Y Ng, and Christopher
Potts. 2013b. Recursive deep models for semantic com-
positionality over a sentiment treebank. In Proceedings of
Empirical Methods in Natural Language Processing.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A
simple way to prevent neural networks from overfitting.
Journal of Machine Learning Research, 15(1).
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Se-
quence to sequence learning with neural networks. In
Proceedings of Advances in Neural Information Processing
Systems.
Kai Sheng Tai, Richard Socher, and Christopher D. Man-
ning. 2015. Improved semantic representations from tree-
structured long short-term memory networks.
Tim Van de Cruys. 2014. A neural network approach to selec-
tional preference acquisition. In Proceedings of Empirical
Methods in Natural Language Processing.
Sida I. Wang and Christopher D. Manning. 2012. Baselines
and bigrams: Simple, good sentiment and topic classifica-
tion. In Proceedings of the Association for Computational
Linguistics.
Jason Weston, Samy Bengio, and Nicolas Usunier. 2011.
Wsabie: Scaling up to large vocabulary image annotation.
In International Joint Conference on Artificial Intelligence.
</reference>
<page confidence="0.992503">
1691
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.416545">
<title confidence="0.998986">Deep Unordered Composition Rivals Syntactic for Text Classification</title>
<author confidence="0.995525">Varun Jordan Hal Daum´e</author>
<affiliation confidence="0.5721635">of Maryland, Department of Computer Science and of Colorado, Department of Computer</affiliation>
<abstract confidence="0.998882217391304">Many existing deep learning models for natural language processing tasks focus on the their inputs, which requires many expensive computations. We present a simple deep neural network that competes with and, in some cases, outperforms such models on sentiment analysis and factoid question answering tasks while taking only a fraction of the training time. While our model is syntactically-ignorant, we show significant improvements over previous bag-of-words models by deepening our network and applying a novel variant of dropout. Moreover, our model performs better than syntactic models on datasets with high syntactic variance. We show that our model makes similar errors to syntactically-aware models, indicating that for the tasks we consider, nonlinearly transforming the input is more important than tailoring a network to incorporate word order and syntax.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Rada Mihalcea Di Chen</author>
<author>Claire Cardie</author>
<author>Janyce Wiebe</author>
</authors>
<title>Simcompass: Using deep learning word embeddings to assess cross-level similarity.</title>
<date>2014</date>
<booktitle>In SemEval.</booktitle>
<marker>Banea, Di Chen, Cardie, Wiebe, 2014</marker>
<rawString>Carmen Banea, Di Chen, Rada Mihalcea, Claire Cardie, and Janyce Wiebe. 2014. Simcompass: Using deep learning word embeddings to assess cross-level similarity. In SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjectivenoun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="32605" citStr="Baroni and Zamparelli, 2010" startWordPosition="5278" endWordPosition="5282">rible”. However, we find no significant differences between the L2 norms of stopwords and words in the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN. Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN, we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successf</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjectivenoun constructions in semantic space. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="33654" citStr="Bengio et al., 2003" startWordPosition="5447" endWordPosition="5450">ast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work In Section 5, we showed that the performance of our DAN model worsens on sentences that contain lingustic phenomena such as double negation. One promising future direction is to cascade classifiers such that syntactic models are used only when a DAN is not confident in its prediction. We can also extend the DAN’s success at incorporating out-of-domain training data to sentiment analysis: imagine training a DAN on labeled tweets for classification on newspaper reviews. Anoth</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Aaron Courville</author>
<author>Pascal Vincent</author>
</authors>
<title>Representation learning: A review and new perspectives.</title>
<date>2013</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>35</volume>
<issue>8</issue>
<contexts>
<context position="9457" citStr="Bengio et al., 2013" startWordPosition="1508" endWordPosition="1512"> over 5% absolute accuracy. Most unordered functions are linear mappings between bag-of-words features and output labels, so might they suffer from the same issue? To isolate the effects of syntactic composition from the nonlinear transformations that are crucial to RecNN performance, we investigate how well a deep version of the NBOW model performs on tasks that have recently been dominated by syntactically-aware models. 3 Deep Averaging Networks The intuition behind deep feed-forward neural networks is that each layer learns a more abstract representation of the input than the previous one (Bengio et al., 2013). We can apply this concept to the NBOW model discussed in Section 2.1 with the expectation that each layer will increasingly magnify small but meaningful differences in the word embedding average. To be more concrete, take s1 as the sentence “I really loved Rosamund Pike’s performance in the movie Gone Girl” and generate s2 and s3 by replacing “loved” with “liked” and then again by “despised”. The vector averages of these three sentences are almost identical, but the averages associated with the synonymous sentences s1 and s2 are slightly more similar to each other than they are to s3’s avera</context>
</contexts>
<marker>Bengio, Courville, Vincent, 2013</marker>
<rawString>Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jordan Boyd-Graber</author>
<author>Brianna Satinoff</author>
<author>He He</author>
<author>Hal Daum´e</author>
</authors>
<title>Besting the quiz master: Crowdsourcing incremental classification games.</title>
<date>2012</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<marker>Boyd-Graber, Satinoff, He, Daum´e, 2012</marker>
<rawString>Jordan Boyd-Graber, Brianna Satinoff, He He, and Hal Daum´e III. 2012. Besting the quiz master: Crowdsourcing incremental classification games. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="33760" citStr="Chen and Manning, 2014" startWordPosition="5461" endWordPosition="5464">ide of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work In Section 5, we showed that the performance of our DAN model worsens on sentences that contain lingustic phenomena such as double negation. One promising future direction is to cascade classifiers such that syntactic models are used only when a DAN is not confident in its prediction. We can also extend the DAN’s success at incorporating out-of-domain training data to sentiment analysis: imagine training a DAN on labeled tweets for classification on newspaper reviews. Another potentially interesting application is to add gated units to a DAN,as has been done for recurrent and r</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
<author>Caglar Gulcehre</author>
<author>Fethi Bougares</author>
<author>Holger Schwenk</author>
<author>Yoshua Bengio</author>
</authors>
<title>Learning phrase representations using rnn encoderdecoder for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<marker>Cho, van Merrienboer, Gulcehre, Bougares, Schwenk, Bengio, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using rnn encoderdecoder for statistical machine translation. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis (Lambek Festschirft).</title>
<date>2010</date>
<contexts>
<context position="32672" citStr="Coecke et al., 2010" startWordPosition="5289" endWordPosition="5292"> stopwords and words in the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN. Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN, we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine transl</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis (Lambek Festschirft).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference of Machine Learning.</booktitle>
<contexts>
<context position="33524" citStr="Collobert and Weston, 2008" startWordPosition="5426" endWordPosition="5429">d and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN, we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work In Section 5, we showed that the performance of our DAN model worsens on sentences that contain lingustic phenomena such as double negation. One promising future direction is to cascade classifiers such that syntactic models are used only when a DAN is not confident in its prediction. We can also extend the DAN’s success at incorporating out-of-d</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Dahl</author>
<author>Ryan P Adams</author>
<author>Hugo Larochelle</author>
</authors>
<title>Training restricted boltzmann machines on word observations.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference of Machine Learning.</booktitle>
<contexts>
<context position="15787" citStr="Dahl et al., 2012" startWordPosition="2568" endWordPosition="2571">network (DRecNN) proposed by ˙Irsoy and Cardie (2014), and the TREE-LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN) and the convolutional neural network multichannel (Kim, 2014, CNN-MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. . (8) 1684 the word-representation restricted Boltzmann machine (Dahl et al., 2012, WRRBM), which only works on the document-level IMDB task.4 4.1.2 Non-Neural Baselines We also compare to non-neural baselines, specifically the bigram naive Bayes (BINB) and naive Bayes support vector machine (NBSVM-BI) models introduced by Wang and Manning (2012), both of which are memory-intensive due to huge feature spaces of size JV12. 4.1.3 DAN Configurations In Table 1, we compare a variety of DAN and NBOW configurations5 to the baselines described above. In particular, we are interested in not only comparing DAN accuracies to those of the baselines, but also how initializing with pret</context>
</contexts>
<marker>Dahl, Adams, Larochelle, 2012</marker>
<rawString>George E Dahl, Ryan P Adams, and Hugo Larochelle. 2012. Training restricted boltzmann machines on word observations. In Proceedings of the International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research.</journal>
<contexts>
<context position="16976" citStr="Duchi et al., 2011" startWordPosition="2754" endWordPosition="2757">lso how initializing with pretrained embeddings and restricting the model to only root-level labels affects performance. With this in mind, the NBOW-RAND and DAN-RAND models are initialized with random 300-dimensional word embeddings, while the other models are initialized with publicly-available 300-d GloVe vectors trained over the Common Crawl (Pennington et al., 2014). The DAN-ROOT model only has access to sentence-level labels for SST experiments, while all other models are trained on labeled phrases (if they exist) in addition to sentences. We train all NBOW and DAN models using AdaGrad (Duchi et al., 2011). We apply DANs to documents by averaging the embeddings for all of a document’s tokens and then feeding that average through multiple layers as before. Since the representations computed by DANs are always d-dimensional vectors regardless of the input size, they are efficient with respect to both memory and computational cost. We find that the hyperparameters selected on the SST also work well for the IMDB task. 4.1.4 Dataset Details We evaluate over both fine-grained and binary sentence-level classification tasks on the SST, and just the binary task on RT and IMDB. In the finegrained SST set</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="32707" citStr="Grefenstette and Sadrzadeh, 2011" startWordPosition="5293" endWordPosition="5296">in the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN. Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN, we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and parap</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>not not bad” is not ”bad”: A distributional account of negation.</title>
<date>2013</date>
<booktitle>Proceedings of the ACL Workshop on Continuous Vector Space Models and their Compositionality.</booktitle>
<contexts>
<context position="7072" citStr="Hermann et al., 2013" startWordPosition="1103" endWordPosition="1106">e state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW, each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. RecNNs also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their reliance on parse trees and thus cannot effectively incorporate out-of-domain data, as we show i</context>
</contexts>
<marker>Hermann, Grefenstette, Blunsom, 2013</marker>
<rawString>Karl Moritz Hermann, Edward Grefenstette, and Phil Blunsom. 2013. ”not not bad” is not ”bad”: A distributional account of negation. Proceedings of the ACL Workshop on Continuous Vector Space Models and their Compositionality.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
<author>Nitish Srivastava</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2012</date>
<location>CoRR, abs/1207.0580.</location>
<marker>Hinton, Srivastava, 2012</marker>
<rawString>Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. CoRR, abs/1207.0580.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sepp Hochreiter</author>
<author>J¨urgen Schmidhuber</author>
</authors>
<title>Long shortterm memory. Neural computation.</title>
<date>1997</date>
<contexts>
<context position="34418" citStr="Hochreiter and Schmidhuber, 1997" startWordPosition="5568" endWordPosition="5571"> 5, we showed that the performance of our DAN model worsens on sentences that contain lingustic phenomena such as double negation. One promising future direction is to cascade classifiers such that syntactic models are used only when a DAN is not confident in its prediction. We can also extend the DAN’s success at incorporating out-of-domain training data to sentiment analysis: imagine training a DAN on labeled tweets for classification on newspaper reviews. Another potentially interesting application is to add gated units to a DAN,as has been done for recurrent and recursive neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Sutskever et al., 2014; Tai et al., 2015), to drop useless words rather than randomly-selected ones. 8 Conclusion In this paper, we introduce the deep averaging network, which feeds an unweighted average of word vectors through multiple hidden layers before classification. The DAN performs competitively with more complicated neural networks that explicitly model semantic and syntactic compositionality. It is further strengthened by word dropout, a regularizer that reduces input redundancy. DANs obtain close to state-of-the-art accuracy on both sentence and document-level se</context>
</contexts>
<marker>Hochreiter, Schmidhuber, 1997</marker>
<rawString>Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long shortterm memory. Neural computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="31385" citStr="Hu and Liu (2004)" startWordPosition="5085" endWordPosition="5088">ze the word embeddings. This does not apply to just DANs; both convolutional and recursive networks do the same (Kim, 2014; ˙Irsoy and Cardie, 2014). Why are initializations with these embeddings so crucial to obtaining good performance? Is it possible that unsupervised training algorithms are already capturing sentiment? We investigate this theory by conducting a simple experiment: given a sentiment lexicon containing both positive and negative words, we train a logistic regression to discriminate between the associated word embeddings (without any fine-tuning). We use the lexicon created by Hu and Liu (2004), which consists of 2,006 positive words and 4,783 negative words. We balance and split the dataset into 3,000 training words and 1,000 test words. Using 300-dimensional GloVe embeddings pretrained over the Common Crawl, we obtain over 95% accuracy on the unseen test set, supporting the hypothesis that unsupervised pretraining over large corpora can capture properties such as sentiment. Intuitively, after the embeddings are fine-tuned during DAN training, we might expect a decrease in the norms of stopwords and an increase in the 1688 norms of sentiment-rich words like “awesome” or “horrible”.</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ozan ˙Irsoy</author>
<author>Claire Cardie</author>
</authors>
<title>Deep recursive neural networks for compositionality in language.</title>
<date>2014</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<marker>˙Irsoy, Cardie, 2014</marker>
<rawString>Ozan ˙Irsoy and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="false">
<authors>
<author>2014a</author>
</authors>
<title>A neural network for factoid question answering over paragraphs.</title>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5604" citStr="(2014)" startWordPosition="874" endWordPosition="874">antiation of NBOW, g averages word embeddings1 1 � z = g(w E X) = �X� w∈X Feeding z to a softmax layer induces estimated probabilities for each output label yˆ = softmax(W3 · z + b), (2) where the softmax function is softmax(q) = Ek exp q (3) �,=1 exp qj W3 is a k x d matrix for a dataset with k output labels, and b is a bias term. We train the NBOW model to minimize crossentropy error, which for a single training instance with ground-truth label y is k Aˆy) = yp log(ˆyp). (4) p=1 1Preliminary experiments indicate that averaging outperforms the vector sum used in NBOW from Kalchbrenner et al. (2014). Before we describe our deep extension of the NBOW model, we take a quick detour to discuss syntactic composition functions. Connections to other representation frameworks are discussed further in Section 4. 2.2 Considering Syntax for Composition Given a sentence like “You’ll be more entertained getting hit by a bus”, an unordered model like NBOW might be deceived by the word “entertained” to return a positive prediction. In contrast, syntactic composition functions rely on the order and structure of the input to learn how one word or phrase affects another, sacrificing computational efficien</context>
<context position="7720" citStr="(2014)" startWordPosition="1205" endWordPosition="1205">n NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. RecNNs also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their reliance on parse trees and thus cannot effectively incorporate out-of-domain data, as we show in our question-answering experiments. Kim (2014) shows that some of these issues can be avoided by using a convolutional network instead of a RecNN, but the computational complexity increases even further (see Section 4 for runtime comparisons). What contributes most to the power of syntactic vw. (1) 1682 DAN softmax h2 = f(W2 · h1 + b2) h1 = f(W1 · av + b1) av = � %=1 4 C; 4 Predator is a masterpiece c1 c2 c3 c4 RecNN softmax z3 = f(W [z2] + b) softmax [c2 ] z2 = f(W + b) z1 softmax z1 = f(W [c3] + b) Predator is a masterpiece c1 c2 c3 c4 Figure 1: On the left, a RecNN is given an input sentence for sentiment classification. Softmax layers</context>
<context position="15223" citStr="(2014)" startWordPosition="2482" endWordPosition="2482">entiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), RecNTNs, the deep recursive network (DRecNN) proposed by ˙Irsoy and Cardie (2014), and the TREE-LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN) and the convolutional neural network multichannel (Kim, 2014, CNN-MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. . (8) 1684 the word-representation restricted Boltzmann machine (Dahl et al., 2012, WRRBM), which only works on the do</context>
<context position="25636" citStr="(2014)" startWordPosition="4179" endWordPosition="4179">Ns Work? In this section we first examine how the deep layers of the DAN amplify tiny differences in the vector average that are predictive of the output labels. Next, we compare DANs to DRecNNs on sentences that contain negations and contrastive conjunctions and find that both models make similar errors despite the latter’s increased complexity. Finally, we analyze the predictive ability of unsupervised word embeddings on a simple sentiment task in an effort to explain why initialization with these embeddings improves the DAN. 5.1 Perturbation Analysis Following the work of ˙Irsoy and Cardie (2014), we examine our network by measuring the response at each hidden layer to perturbations in an input sentence. In particular, we use the template the film’s performances were awesome and replace the final word with increasingly negative polarity words (cool, okay, underwhelming, the worst). For each perturbed sentence, we observe how much the hidden layers differ from those associated with the original template in 1-norm. Figure 3 shows that as a DAN gets deeper, the differences between negative and positive sentences become increasingly amplified. While nonexistent in the shallow NBOW model, </context>
<context position="26954" citStr="(2014)" startWordPosition="4391" endWordPosition="4391">ves sentiment analysis as shown in Figure 4. 5.2 Handling Negations and “but”: Where Syntax is Still Needed While DANs outperform other bag-of-words models, how can they model linguistic phenomena such as negation without considering word order? To evaluate DANs over tougher inputs, we collect 92 sentences, each of which contains at least one negation and one contrastive conjunction, from the dev and test sets of the SST.9 Our fine-grained accuracy is higher on this subset than on the full dataset, improving almost five percent absolute accuracy to 53.3%. The DRecNN model of ˙Irsoy and Cardie (2014) obtains a similar accuracy of 51.1%, contrary to our intuition that syntactic functions should outperform unordered functions on sentences that clearly require syntax to understand.10 Are these sentences truly difficult to classify? A close inspection reveals that both the DAN and the DRecNN have an overwhelming tendency to predict negative sentiment (60.9% and 55.4% of the time for the DAN and DRecNN respectively) when they see a negation compared to positive sentiment (35.9% for DANs, 34.8% for DRecNNs). If we further restrict our subset of sentences to only those with positive ground truth</context>
<context position="32789" citStr="(2014)" startWordPosition="5311" endWordPosition="5311">imple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN. Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN, we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word ord</context>
</contexts>
<marker>2014a, </marker>
<rawString>Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal Daum´e III. 2014a. A neural network for factoid question answering over paragraphs. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohit Iyyer</author>
<author>Peter Enns</author>
<author>Jordan Boyd-Graber</author>
<author>Philip Resnik</author>
</authors>
<title>Political ideology detection using recursive neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="18622" citStr="Iyyer et al., 2014" startWordPosition="3015" endWordPosition="3018"> test: 2,2101, while our binary split is {train: 6,920, dev:872, 4.1.5 Results The DAN achieves the second best reported result on the RT dataset, behind only the significantly slower CNN-MC model. It’s also competitive with more complex models on the SST and outperforms the DCNN and WRRBM on the document-level IMDB task. Interestingly, the DAN achieves good performance on the SST when trained with only sentence-level labels, indicating that it does not suffer from the vanishing error signal problem that plagues RecNNs. Since acquiring labelled phrases is often expensive (Sayeed et al., 2012; Iyyer et al., 2014b), this result is promising for large or messy datasets where fine-grained annotation is infeasible. 4.1.6 Timing Experiments DANs require less time per epoch and—in general— require fewer epochs than their syntactic counterparts. We compare DAN runtime on the SST to publicly-available implementations of syntactic baselines in the last column of Table 1; the reported times are for a single epoch to control for hyperparameter choices such as learning rate, and all models use 300-d word vectors. Training a DAN on just sentence-level labels on the SST takes under five minutes on a single core of</context>
<context position="21721" citStr="Iyyer et al. (2014" startWordPosition="3531" endWordPosition="3534">ord Dropout ● ● ● ● ● ● 0.0 0.1 0.2 0.3 0.4 0.5 Dropout Probability Figure 2: Randomly dropping out 30% of words from the vector average is optimal for the quiz bowl task, yielding a gain in absolute accuracy of almost 3% on the quiz bowl question dataset compared to the same model trained with no word dropout. work using both syntactic and unordered models for quiz bowl question answering. In Boyd-Graber et al. (2012), naive Bayes bag-of-words models (BOW-DT) and sequential language models work well on easy questions but poorly on harder ones. A dependency-tree RecNN called QANTA proposed in Iyyer et al. (2014a) shows substantial improvements, leading to the hypothesis that correctly modeling compositionality is crucial for answering hard questions. 4.2.1 Dataset and Experimental Setup To test this, we train a DAN over the history questions from Iyyer et al. (2014a).8 This dataset is aug8The training set contains 14,219 sentences over 3,761 questions. For more detail about data and baseline systems, mented with 49,581 sentence/page-title pairs from the Wikipedia articles associated with the answers in the dataset. For fair comparison with QANTA, we use a normalized tanh activation function at the l</context>
<context position="24298" citStr="Iyyer et al. (2014" startWordPosition="3941" endWordPosition="3944">eferring to the answer, a property that is not true of Wikipedia sentences (e.g., “Little of Emily’s work from this period survives, except for poems spoken by characters.”). Finally, many Wikipedia sentences do not uniquely identify the title of the page they come from, such as the following sentence from Emily Bront¨e’s page: “She does not seem to have made any friends outside her family.” While noisy data affect both DAN and QANTA, the latter is further hampered by the syntactic divergence between quiz bowl questions and Wikipedia, which may explain the lack of improvement in accuracy. see Iyyer et al. (2014a). 71 70 69 History QB Accuracy 1686 50 40 30 20 10 0 Figure 3: Perturbation response (difference in 1- norm) at each layer of a 5-layer DAN after replacing awesome in the film’s performances were awesome with four words of varying sentiment polarity. While the shallow NBOW model does not show any meaningful distinctions, we see that as the network gets deeper, negative sentences are increasingly different from the original positive sentence. Effect of Depth on Sentiment Accurac ● ● ● ● ● ● ● ● ● ● ● ●DAN ● ● DAN−ROOT 0 2 4 6 Number of Layers Figure 4: Two to three layers is optimal for the D</context>
</contexts>
<marker>Iyyer, Enns, Boyd-Graber, Resnik, 2014</marker>
<rawString>Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, and Philip Resnik. 2014b. Political ideology detection using recursive neural networks. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent convolutional neural networks for discourse compositionality.</title>
<date>2013</date>
<booktitle>In ACL Workshop on Continuous Vector Space Models and their Compositionality.</booktitle>
<contexts>
<context position="1970" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="278" endWordPosition="281">w dimensional vectors called embeddings. To apply vector space models to sentences or documents, one must first select an appropriate composition function, which is a mathematical process for combining multiple words into a single vector. Composition functions fall into two classes: unordered and syntactic. Unordered functions treat input texts as bags of word embeddings, while syntactic functions take word order and sentence structure into account. Previously published experimental results have shown that syntactic functions outperform unordered functions on many tasks (Socher et al., 2013b; Kalchbrenner and Blunsom, 2013). However, there is a tradeoff: syntactic functions require more training time than unordered composition functions and are prohibitively expensive in the case of huge datasets or limited computing resources. For example, the recursive neural network (Section 2) computes costly matrix/tensor products and nonlinearities at every node of a syntactic parse tree, which limits it to smaller datasets that can be reliably parsed. We introduce a deep unordered model that obtains near state-of-the-art accuracies on a variety of sentence and document-level tasks with just minutes of training time on an </context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent convolutional neural networks for discourse compositionality. In ACL Workshop on Continuous Vector Space Models and their Compositionality.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5604" citStr="Kalchbrenner et al. (2014)" startWordPosition="871" endWordPosition="874">unction. In our instantiation of NBOW, g averages word embeddings1 1 � z = g(w E X) = �X� w∈X Feeding z to a softmax layer induces estimated probabilities for each output label yˆ = softmax(W3 · z + b), (2) where the softmax function is softmax(q) = Ek exp q (3) �,=1 exp qj W3 is a k x d matrix for a dataset with k output labels, and b is a bias term. We train the NBOW model to minimize crossentropy error, which for a single training instance with ground-truth label y is k Aˆy) = yp log(ˆyp). (4) p=1 1Preliminary experiments indicate that averaging outperforms the vector sum used in NBOW from Kalchbrenner et al. (2014). Before we describe our deep extension of the NBOW model, we take a quick detour to discuss syntactic composition functions. Connections to other representation frameworks are discussed further in Section 4. 2.2 Considering Syntax for Composition Given a sentence like “You’ll be more entertained getting hit by a bus”, an unordered model like NBOW might be deceived by the word “entertained” to return a positive prediction. In contrast, syntactic composition functions rely on the order and structure of the input to learn how one word or phrase affects another, sacrificing computational efficien</context>
<context position="15366" citStr="Kalchbrenner et al., 2014" startWordPosition="2501" endWordPosition="2504">vel labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), RecNTNs, the deep recursive network (DRecNN) proposed by ˙Irsoy and Cardie (2014), and the TREE-LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN) and the convolutional neural network multichannel (Kim, 2014, CNN-MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. . (8) 1684 the word-representation restricted Boltzmann machine (Dahl et al., 2012, WRRBM), which only works on the document-level IMDB task.4 4.1.2 Non-Neural Baselines We also compare to non-neural baselines, specifically the bigram naive Bayes (BINB) and nai</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dimitri Kartsaklis</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Prior disambiguation of word tensors for constructing sentence vectors.</title>
<date>2013</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="32638" citStr="Kartsaklis and Sadrzadeh, 2013" startWordPosition="5283" endWordPosition="5286">ignificant differences between the L2 norms of stopwords and words in the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN. Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN, we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Soc</context>
</contexts>
<marker>Kartsaklis, Sadrzadeh, 2013</marker>
<rawString>Dimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013. Prior disambiguation of word tensors for constructing sentence vectors. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="7720" citStr="Kim (2014)" startWordPosition="1204" endWordPosition="1205"> than NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. RecNNs also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their reliance on parse trees and thus cannot effectively incorporate out-of-domain data, as we show in our question-answering experiments. Kim (2014) shows that some of these issues can be avoided by using a convolutional network instead of a RecNN, but the computational complexity increases even further (see Section 4 for runtime comparisons). What contributes most to the power of syntactic vw. (1) 1682 DAN softmax h2 = f(W2 · h1 + b2) h1 = f(W1 · av + b1) av = � %=1 4 C; 4 Predator is a masterpiece c1 c2 c3 c4 RecNN softmax z3 = f(W [z2] + b) softmax [c2 ] z2 = f(W + b) z1 softmax z1 = f(W [c3] + b) Predator is a masterpiece c1 c2 c3 c4 Figure 1: On the left, a RecNN is given an input sentence for sentiment classification. Softmax layers</context>
<context position="15434" citStr="Kim, 2014" startWordPosition="2513" endWordPosition="2514">). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), RecNTNs, the deep recursive network (DRecNN) proposed by ˙Irsoy and Cardie (2014), and the TREE-LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN) and the convolutional neural network multichannel (Kim, 2014, CNN-MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. . (8) 1684 the word-representation restricted Boltzmann machine (Dahl et al., 2012, WRRBM), which only works on the document-level IMDB task.4 4.1.2 Non-Neural Baselines We also compare to non-neural baselines, specifically the bigram naive Bayes (BINB) and naive Bayes support vector machine (NBSVM-BI) models introduced by Wang</context>
<context position="30890" citStr="Kim, 2014" startWordPosition="5011" endWordPosition="5012">was not bad”. The DRecNN does slightly better in this case by predicting a lesser negative polarity than the DAN; however, we theorize that still more powerful syntactic composition functions (and more labelled instances of negation and related phenomena) are necessary to truly solve this problem. 5.3 Unsupervised Embeddings Capture Sentiment Our model consistently converges slower to a worse solution (dropping 3% in absolute accuracy on coarse-grained SST) when we randomly initialize the word embeddings. This does not apply to just DANs; both convolutional and recursive networks do the same (Kim, 2014; ˙Irsoy and Cardie, 2014). Why are initializations with these embeddings so crucial to obtaining good performance? Is it possible that unsupervised training algorithms are already capturing sentiment? We investigate this theory by conducting a simple experiment: given a sentiment lexicon containing both positive and negative words, we train a logistic regression to discriminate between the associated word embeddings (without any fine-tuning). We use the lexicon created by Hu and Liu (2004), which consists of 2,006 positive words and 4,783 negative words. We balance and split the dataset into </context>
<context position="33536" citStr="Kim, 2014" startWordPosition="5430" endWordPosition="5431">ilarity tasks. Once we compute the embedding average in a DAN, we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work In Section 5, we showed that the performance of our DAN model worsens on sentences that contain lingustic phenomena such as double negation. One promising future direction is to cascade classifiers such that syntactic models are used only when a DAN is not confident in its prediction. We can also extend the DAN’s success at incorporating out-of-domain traini</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In Proceedings of the International Conference of Machine Learning.</booktitle>
<contexts>
<context position="15539" citStr="Mikolov, 2014" startWordPosition="2530" endWordPosition="2531"> 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), RecNTNs, the deep recursive network (DRecNN) proposed by ˙Irsoy and Cardie (2014), and the TREE-LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN) and the convolutional neural network multichannel (Kim, 2014, CNN-MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. . (8) 1684 the word-representation restricted Boltzmann machine (Dahl et al., 2012, WRRBM), which only works on the document-level IMDB task.4 4.1.2 Non-Neural Baselines We also compare to non-neural baselines, specifically the bigram naive Bayes (BINB) and naive Bayes support vector machine (NBSVM-BI) models introduced by Wang and Manning (2012), both of which are memory-intensive due to huge feature spaces of size JV12. 4.1.3 DA</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of the International Conference of Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiwei Li</author>
</authors>
<title>Feature weight tuning for recursive neural networks.</title>
<date>2014</date>
<tech>CoRR, abs/1412.3714.</tech>
<contexts>
<context position="7477" citStr="Li, 2014" startWordPosition="1169" endWordPosition="1170">cursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. RecNNs also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their reliance on parse trees and thus cannot effectively incorporate out-of-domain data, as we show in our question-answering experiments. Kim (2014) shows that some of these issues can be avoided by using a convolutional network instead of a RecNN, but the computational complexity increases even further (see Section 4 for runtime comparisons). What contributes most to the power of syntactic vw. (1) 1682 DAN softmax h2 = f(W2 · h1 + b2) h1 = f(W1 · av + b1) av = � %=1 4 C; 4 Predator is a masterpiece </context>
</contexts>
<marker>Li, 2014</marker>
<rawString>Jiwei Li. 2014. Feature weight tuning for recursive neural networks. CoRR, abs/1412.3714.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujie Liu</author>
<author>Nan Yang</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
</authors>
<title>A recursive recurrent neural network for statistical machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="33296" citStr="Liu et al., 2014" startWordPosition="5388" endWordPosition="5391">nstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN, we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work In Section 5, we showed that the performance of our DAN model worsens on sentences that contain lingustic phenomena such</context>
</contexts>
<marker>Liu, Yang, Li, Zhou, 2014</marker>
<rawString>Shujie Liu, Nan Yang, Mu Li, and Ming Zhou. 2014. A recursive recurrent neural network for statistical machine translation. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew L Maas</author>
<author>Raymond E Daly</author>
<author>Peter T Pham</author>
<author>Dan Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Learning word vectors for sentiment analysis.</title>
<date>2011</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14925" citStr="Maas et al. (2011)" startWordPosition="2437" endWordPosition="2440">och on the SST fine-grained task). Asterisked models are initialized either with different pretrained embeddings or randomly. 4.1 Sentiment Analysis Recently, syntactic composition functions have revolutionized both fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), RecNTNs, the deep recursive network (DRecNN) proposed by ˙Irsoy and Cardie (2014), and the TREE-LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN) and the convolutional neural network multichannel (Kim, 2014, CNN-MC). Our other neural baselines are the sliding-window based paragraph vector (Le and</context>
</contexts>
<marker>Maas, Daly, Pham, Huang, Ng, Potts, 2011</marker>
<rawString>Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. Learning word vectors for sentiment analysis. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="32389" citStr="Mitchell and Lapata (2008)" startWordPosition="5244" endWordPosition="5247"> sentiment. Intuitively, after the embeddings are fine-tuned during DAN training, we might expect a decrease in the norms of stopwords and an increase in the 1688 norms of sentiment-rich words like “awesome” or “horrible”. However, we find no significant differences between the L2 norms of stopwords and words in the sentiment lexicon of Hu and Liu (2004). 6 Related Work Our DAN model builds on the successes of both simple vector operations and neural network-based models for compositionality. There are a variety of element-wise vector operations that could replace the average used in the DAN. Mitchell and Lapata (2008) experiment with many of them to model the compositionality of short phrases. Later, their work was extended to take into account the syntactic relation between words (Erk and Pad´o, 2008; Baroni and Zamparelli, 2010; Kartsaklis and Sadrzadeh, 2013) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN, </context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.</title>
<date>2005</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="14708" citStr="Pang and Lee, 2005" startWordPosition="2402" endWordPosition="2405">NN-MC 81.1 47.4 88.1 — 2,452 WRRBM* — — — 89.2 — Table 1: DANs achieve comparable sentiment accuracies to syntactic functions (bottom third of table) but require much less training time (measured as time of a single epoch on the SST fine-grained task). Asterisked models are initialized either with different pretrained embeddings or randomly. 4.1 Sentiment Analysis Recently, syntactic composition functions have revolutionized both fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), RecNTNs, the deep recursive network (DRecNN) proposed by ˙Irsoy and Cardie (2014), and the TREE-LSTM of (Tai et al., 2015). Convolutional network baselines include th</context>
</contexts>
<marker>Pang, Lee, 2005</marker>
<rawString>Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Pennington</author>
<author>Richard Socher</author>
<author>Christopher Manning</author>
</authors>
<title>Glove: Global vectors for word representation.</title>
<date>2014</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="16730" citStr="Pennington et al., 2014" startWordPosition="2712" endWordPosition="2715">ure spaces of size JV12. 4.1.3 DAN Configurations In Table 1, we compare a variety of DAN and NBOW configurations5 to the baselines described above. In particular, we are interested in not only comparing DAN accuracies to those of the baselines, but also how initializing with pretrained embeddings and restricting the model to only root-level labels affects performance. With this in mind, the NBOW-RAND and DAN-RAND models are initialized with random 300-dimensional word embeddings, while the other models are initialized with publicly-available 300-d GloVe vectors trained over the Common Crawl (Pennington et al., 2014). The DAN-ROOT model only has access to sentence-level labels for SST experiments, while all other models are trained on labeled phrases (if they exist) in addition to sentences. We train all NBOW and DAN models using AdaGrad (Duchi et al., 2011). We apply DANs to documents by averaging the embeddings for all of a document’s tokens and then feeding that average through multiple layers as before. Since the representations computed by DANs are always d-dimensional vectors regardless of the input size, they are efficient with respect to both memory and computational cost. We find that the hyperpa</context>
</contexts>
<marker>Pennington, Socher, Manning, 2014</marker>
<rawString>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asad B Sayeed</author>
<author>Jordan Boyd-Graber</author>
<author>Bryan Rusk</author>
<author>Amy Weinberg</author>
</authors>
<title>Grammatical structures for word-level sentiment detection.</title>
<date>2012</date>
<journal>In North American Association of Computational Linguistics.</journal>
<contexts>
<context position="18602" citStr="Sayeed et al., 2012" startWordPosition="3011" endWordPosition="3014">n: 8,544, dev: 1,101, test: 2,2101, while our binary split is {train: 6,920, dev:872, 4.1.5 Results The DAN achieves the second best reported result on the RT dataset, behind only the significantly slower CNN-MC model. It’s also competitive with more complex models on the SST and outperforms the DCNN and WRRBM on the document-level IMDB task. Interestingly, the DAN achieves good performance on the SST when trained with only sentence-level labels, indicating that it does not suffer from the vanishing error signal problem that plagues RecNNs. Since acquiring labelled phrases is often expensive (Sayeed et al., 2012; Iyyer et al., 2014b), this result is promising for large or messy datasets where fine-grained annotation is infeasible. 4.1.6 Timing Experiments DANs require less time per epoch and—in general— require fewer epochs than their syntactic counterparts. We compare DAN runtime on the SST to publicly-available implementations of syntactic baselines in the last column of Table 1; the reported times are for a single epoch to control for hyperparameter choices such as learning rate, and all models use 300-d word vectors. Training a DAN on just sentence-level labels on the SST takes under five minutes</context>
</contexts>
<marker>Sayeed, Boyd-Graber, Rusk, Weinberg, 2012</marker>
<rawString>Asad B. Sayeed, Jordan Boyd-Graber, Bryan Rusk, and Amy Weinberg. 2012. Grammatical structures for word-level sentiment detection. In North American Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric H Huang</author>
<author>Jeffrey Pennington</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection.</title>
<date>2011</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="15138" citStr="Socher et al., 2011" startWordPosition="2467" endWordPosition="2470">oth fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), RecNTNs, the deep recursive network (DRecNN) proposed by ˙Irsoy and Cardie (2014), and the TREE-LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN) and the convolutional neural network multichannel (Kim, 2014, CNN-MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. . (8) 1684 the word-representatio</context>
<context position="33343" citStr="Socher et al., 2011" startWordPosition="5396" endWordPosition="5399">ge works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN, we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work In Section 5, we showed that the performance of our DAN model worsens on sentences that contain lingustic phenomena such as double negation. One promising future direc</context>
</contexts>
<marker>Socher, Huang, Pennington, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011a. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="15138" citStr="Socher et al., 2011" startWordPosition="2467" endWordPosition="2470">oth fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), RecNTNs, the deep recursive network (DRecNN) proposed by ˙Irsoy and Cardie (2014), and the TREE-LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN) and the convolutional neural network multichannel (Kim, 2014, CNN-MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. . (8) 1684 the word-representatio</context>
<context position="33343" citStr="Socher et al., 2011" startWordPosition="5396" endWordPosition="5399">ge works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN, we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work In Section 5, we showed that the performance of our DAN model worsens on sentences that contain lingustic phenomena such as double negation. One promising future direc</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011b. Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing With Compositional Vector Grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1936" citStr="Socher et al., 2013" startWordPosition="274" endWordPosition="277">present words using low dimensional vectors called embeddings. To apply vector space models to sentences or documents, one must first select an appropriate composition function, which is a mathematical process for combining multiple words into a single vector. Composition functions fall into two classes: unordered and syntactic. Unordered functions treat input texts as bags of word embeddings, while syntactic functions take word order and sentence structure into account. Previously published experimental results have shown that syntactic functions outperform unordered functions on many tasks (Socher et al., 2013b; Kalchbrenner and Blunsom, 2013). However, there is a tradeoff: syntactic functions require more training time than unordered composition functions and are prohibitively expensive in the case of huge datasets or limited computing resources. For example, the recursive neural network (Section 2) computes costly matrix/tensor products and nonlinearities at every node of a syntactic parse tree, which limits it to smaller datasets that can be reliably parsed. We introduce a deep unordered model that obtains near state-of-the-art accuracies on a variety of sentence and document-level tasks with ju</context>
<context position="6980" citStr="Socher et al., 2013" startWordPosition="1090" endWordPosition="1093">cNNs) are syntactic functions that rely on natural language’s inherent structure to achieve state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW, each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. RecNNs also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their reli</context>
<context position="8715" citStr="Socher et al. (2013" startWordPosition="1391" endWordPosition="1394">oftmax z3 = f(W [z2] + b) softmax [c2 ] z2 = f(W + b) z1 softmax z1 = f(W [c3] + b) Predator is a masterpiece c1 c2 c3 c4 Figure 1: On the left, a RecNN is given an input sentence for sentiment classification. Softmax layers are placed above every internal node to avoid vanishing gradient issues. On the right is a two-layer DAN taking the same input. While the RecNN has to compute a nonlinear representation (purple vectors) for every node in the parse tree of its input, this DAN only computes two nonlinear layers for every possible input. functions: the compositionality or the nonlinearities? Socher et al. (2013b) report that removing the nonlinearities from their RecNN models drops performance on the Stanford Sentiment Treebank by over 5% absolute accuracy. Most unordered functions are linear mappings between bag-of-words features and output labels, so might they suffer from the same issue? To isolate the effects of syntactic composition from the nonlinear transformations that are crucial to RecNN performance, we investigate how well a deep version of the NBOW model performs on tasks that have recently been dominated by syntactically-aware models. 3 Deep Averaging Networks The intuition behind deep </context>
<context position="14824" citStr="Socher et al. (2013" startWordPosition="2420" endWordPosition="2423">functions (bottom third of table) but require much less training time (measured as time of a single epoch on the SST fine-grained task). Asterisked models are initialized either with different pretrained embeddings or randomly. 4.1 Sentiment Analysis Recently, syntactic composition functions have revolutionized both fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), RecNTNs, the deep recursive network (DRecNN) proposed by ˙Irsoy and Cardie (2014), and the TREE-LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN) and the convolutional neural network multichannel </context>
<context position="33254" citStr="Socher et al., 2013" startWordPosition="5381" endWordPosition="5384">13) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN, we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work In Section 5, we showed that the performance of our DAN model worsens on sente</context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D. Manning, and Andrew Y. Ng. 2013a. Parsing With Compositional Vector Grammars. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1936" citStr="Socher et al., 2013" startWordPosition="274" endWordPosition="277">present words using low dimensional vectors called embeddings. To apply vector space models to sentences or documents, one must first select an appropriate composition function, which is a mathematical process for combining multiple words into a single vector. Composition functions fall into two classes: unordered and syntactic. Unordered functions treat input texts as bags of word embeddings, while syntactic functions take word order and sentence structure into account. Previously published experimental results have shown that syntactic functions outperform unordered functions on many tasks (Socher et al., 2013b; Kalchbrenner and Blunsom, 2013). However, there is a tradeoff: syntactic functions require more training time than unordered composition functions and are prohibitively expensive in the case of huge datasets or limited computing resources. For example, the recursive neural network (Section 2) computes costly matrix/tensor products and nonlinearities at every node of a syntactic parse tree, which limits it to smaller datasets that can be reliably parsed. We introduce a deep unordered model that obtains near state-of-the-art accuracies on a variety of sentence and document-level tasks with ju</context>
<context position="6980" citStr="Socher et al., 2013" startWordPosition="1090" endWordPosition="1093">cNNs) are syntactic functions that rely on natural language’s inherent structure to achieve state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW, each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW models. The nonlinearities and matrix/tensor products at each node of the parse tree are expensive, especially as model dimensionality increases. RecNNs also require an error signal at every node. One root softmax is not strong enough for the model to learn compositional relations and leads to worse accuracies than standard bag-of-words models (Li, 2014). Finally, RecNNs require relatively consistent syntax between training and test data due to their reli</context>
<context position="8715" citStr="Socher et al. (2013" startWordPosition="1391" endWordPosition="1394">oftmax z3 = f(W [z2] + b) softmax [c2 ] z2 = f(W + b) z1 softmax z1 = f(W [c3] + b) Predator is a masterpiece c1 c2 c3 c4 Figure 1: On the left, a RecNN is given an input sentence for sentiment classification. Softmax layers are placed above every internal node to avoid vanishing gradient issues. On the right is a two-layer DAN taking the same input. While the RecNN has to compute a nonlinear representation (purple vectors) for every node in the parse tree of its input, this DAN only computes two nonlinear layers for every possible input. functions: the compositionality or the nonlinearities? Socher et al. (2013b) report that removing the nonlinearities from their RecNN models drops performance on the Stanford Sentiment Treebank by over 5% absolute accuracy. Most unordered functions are linear mappings between bag-of-words features and output labels, so might they suffer from the same issue? To isolate the effects of syntactic composition from the nonlinear transformations that are crucial to RecNN performance, we investigate how well a deep version of the NBOW model performs on tasks that have recently been dominated by syntactically-aware models. 3 Deep Averaging Networks The intuition behind deep </context>
<context position="14824" citStr="Socher et al. (2013" startWordPosition="2420" endWordPosition="2423">functions (bottom third of table) but require much less training time (measured as time of a single epoch on the SST fine-grained task). Asterisked models are initialized either with different pretrained embeddings or randomly. 4.1 Sentiment Analysis Recently, syntactic composition functions have revolutionized both fine-grained and binary (positive or negative) sentiment analysis. We conduct sentence-level sentiment experiments on the Rotten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), RecNTNs, the deep recursive network (DRecNN) proposed by ˙Irsoy and Cardie (2014), and the TREE-LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN) and the convolutional neural network multichannel </context>
<context position="33254" citStr="Socher et al., 2013" startWordPosition="5381" endWordPosition="5384">13) and grammars (Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011). While the average works best for the tasks that we consider, Banea et al. (2014) find that simply summing word2vec embeddings outperforms all other methods on the SemEval 2014 phrase-to-word and sentence-to-phrase similarity tasks. Once we compute the embedding average in a DAN, we feed it to a deep neural network. In contrast, most previous work on neural network-based methods for NLP tasks explicitly model word order. Outside of sentiment analysis, RecNN-based approaches have been successful for tasks such as parsing (Socher et al., 2013a), machine translation (Liu et al., 2014), and paraphrase detection (Socher et al., 2011a). Convolutional networks also model word order in local windows and have achieved performance comparable to or better than that of RecNNs on many tasks (Collobert and Weston, 2008; Kim, 2014). Meanwhile, feedforward architectures like that of the DAN have been used for language modeling (Bengio et al., 2003), selectional preference acquisition (Van de Cruys, 2014), and dependency parsing (Chen and Manning, 2014). 7 Future Work In Section 5, we showed that the performance of our DAN model worsens on sente</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitish Srivastava</author>
<author>Geoffrey Hinton</author>
</authors>
<title>Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.</title>
<date>2014</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>15</volume>
<issue>1</issue>
<marker>Srivastava, Hinton, 2014</marker>
<rawString>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In Proceedings of Advances in Neural Information Processing Systems.</booktitle>
<contexts>
<context position="34460" citStr="Sutskever et al., 2014" startWordPosition="5576" endWordPosition="5579">worsens on sentences that contain lingustic phenomena such as double negation. One promising future direction is to cascade classifiers such that syntactic models are used only when a DAN is not confident in its prediction. We can also extend the DAN’s success at incorporating out-of-domain training data to sentiment analysis: imagine training a DAN on labeled tweets for classification on newspaper reviews. Another potentially interesting application is to add gated units to a DAN,as has been done for recurrent and recursive neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Sutskever et al., 2014; Tai et al., 2015), to drop useless words rather than randomly-selected ones. 8 Conclusion In this paper, we introduce the deep averaging network, which feeds an unweighted average of word vectors through multiple hidden layers before classification. The DAN performs competitively with more complicated neural networks that explicitly model semantic and syntactic compositionality. It is further strengthened by word dropout, a regularizer that reduces input redundancy. DANs obtain close to state-of-the-art accuracy on both sentence and document-level sentiment analysis and factoid question-answ</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Proceedings of Advances in Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Sheng Tai</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
</authors>
<title>Improved semantic representations from treestructured long short-term memory networks.</title>
<date>2015</date>
<contexts>
<context position="6527" citStr="Tai et al., 2015" startWordPosition="1013" endWordPosition="1016">hit by a bus”, an unordered model like NBOW might be deceived by the word “entertained” to return a positive prediction. In contrast, syntactic composition functions rely on the order and structure of the input to learn how one word or phrase affects another, sacrificing computational efficiency in the process. In subsequent sections, we argue that this complexity is not matched by a corresponding gain in performance. Recursive neural networks (RecNNs) are syntactic functions that rely on natural language’s inherent structure to achieve state-of-the-art accuracies on sentiment analysis tasks (Tai et al., 2015). As in NBOW, each word type has an associated embedding. However, the composition function g now depends on a parse tree of the input sequence. The representation for any internal node in a binary parse tree is computed as a nonlinear function of the representations of its children (Figure 1, left). A more powerful RecNN variant is the recursive neural tensor network (RecNTN), which modifies g to include a costly tensor product (Socher et al., 2013b). While RecNNs can model complex linguistic phenomena like negation (Hermann et al., 2013), they require much more training time than NBOW models</context>
<context position="15264" citStr="Tai et al., 2015" startWordPosition="2487" endWordPosition="2490">tten Tomatoes (RT) movie reviews dataset (Pang and Lee, 2005) and its extension with phrase-level labels, the Stanford Sentiment Treebank (SST) introduced by Socher et al. (2013b). Our model is also effective on the document-level IMDB movie review dataset of Maas et al. (2011). 4.1.1 Neural Baselines Most neural approaches to sentiment analysis are variants of either recursive or convolutional networks. Our recursive neural network baselines include standard RecNNs (Socher et al., 2011b), RecNTNs, the deep recursive network (DRecNN) proposed by ˙Irsoy and Cardie (2014), and the TREE-LSTM of (Tai et al., 2015). Convolutional network baselines include the dynamic convolutional network (Kalchbrenner et al., 2014, DCNN) and the convolutional neural network multichannel (Kim, 2014, CNN-MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. . (8) 1684 the word-representation restricted Boltzmann machine (Dahl et al., 2012, WRRBM), which only works on the document-level IMDB task.4 4.1.2 Non-Neural</context>
<context position="34479" citStr="Tai et al., 2015" startWordPosition="5580" endWordPosition="5583">t contain lingustic phenomena such as double negation. One promising future direction is to cascade classifiers such that syntactic models are used only when a DAN is not confident in its prediction. We can also extend the DAN’s success at incorporating out-of-domain training data to sentiment analysis: imagine training a DAN on labeled tweets for classification on newspaper reviews. Another potentially interesting application is to add gated units to a DAN,as has been done for recurrent and recursive neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014; Sutskever et al., 2014; Tai et al., 2015), to drop useless words rather than randomly-selected ones. 8 Conclusion In this paper, we introduce the deep averaging network, which feeds an unweighted average of word vectors through multiple hidden layers before classification. The DAN performs competitively with more complicated neural networks that explicitly model semantic and syntactic compositionality. It is further strengthened by word dropout, a regularizer that reduces input redundancy. DANs obtain close to state-of-the-art accuracy on both sentence and document-level sentiment analysis and factoid question-answering tasks with mu</context>
</contexts>
<marker>Tai, Socher, Manning, 2015</marker>
<rawString>Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from treestructured long short-term memory networks.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tim Van de Cruys</author>
</authors>
<title>A neural network approach to selectional preference acquisition.</title>
<date>2014</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<marker>Van de Cruys, 2014</marker>
<rawString>Tim Van de Cruys. 2014. A neural network approach to selectional preference acquisition. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sida I Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Baselines and bigrams: Simple, good sentiment and topic classification.</title>
<date>2012</date>
<booktitle>In Proceedings of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="16053" citStr="Wang and Manning (2012)" startWordPosition="2608" endWordPosition="2611">2014, CNN-MC). Our other neural baselines are the sliding-window based paragraph vector (Le and Mikolov, 2014, PVEC)3 and 3PVEC is computationally expensive at both training and test time and requires enough memory to store a vector for every paragraph in the training data. . (8) 1684 the word-representation restricted Boltzmann machine (Dahl et al., 2012, WRRBM), which only works on the document-level IMDB task.4 4.1.2 Non-Neural Baselines We also compare to non-neural baselines, specifically the bigram naive Bayes (BINB) and naive Bayes support vector machine (NBSVM-BI) models introduced by Wang and Manning (2012), both of which are memory-intensive due to huge feature spaces of size JV12. 4.1.3 DAN Configurations In Table 1, we compare a variety of DAN and NBOW configurations5 to the baselines described above. In particular, we are interested in not only comparing DAN accuracies to those of the baselines, but also how initializing with pretrained embeddings and restricting the model to only root-level labels affects performance. With this in mind, the NBOW-RAND and DAN-RAND models are initialized with random 300-dimensional word embeddings, while the other models are initialized with publicly-availabl</context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Sida I. Wang and Christopher D. Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. In Proceedings of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Samy Bengio</author>
<author>Nicolas Usunier</author>
</authors>
<title>Wsabie: Scaling up to large vocabulary image annotation.</title>
<date>2011</date>
<booktitle>In International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="22447" citStr="Weston et al., 2011" startWordPosition="3648" endWordPosition="3651">rucial for answering hard questions. 4.2.1 Dataset and Experimental Setup To test this, we train a DAN over the history questions from Iyyer et al. (2014a).8 This dataset is aug8The training set contains 14,219 sentences over 3,761 questions. For more detail about data and baseline systems, mented with 49,581 sentence/page-title pairs from the Wikipedia articles associated with the answers in the dataset. For fair comparison with QANTA, we use a normalized tanh activation function at the last layer instead of ReLu, and we also change the output layer from a softmax to the margin ranking loss (Weston et al., 2011) used in QANTA. We initialize the DAN with the same pretrained 100- d word embeddings that were used to initialize QANTA. We also evaluate the effectiveness of word dropout on this task in Figure 2. Cross-validation indicates that p = 0.3 works best for question answering, although the improvement in accuracy is negligible for sentiment analysis. Finally, continuing the trend observed in the sentiment experiments, DAN converges much faster than QANTA. 4.2.2 DANs Improve with Noisy Data Table 2 shows that while DAN is slightly worse than QANTA when trained only on question-answer pairs, it impr</context>
</contexts>
<marker>Weston, Bengio, Usunier, 2011</marker>
<rawString>Jason Weston, Samy Bengio, and Nicolas Usunier. 2011. Wsabie: Scaling up to large vocabulary image annotation. In International Joint Conference on Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>