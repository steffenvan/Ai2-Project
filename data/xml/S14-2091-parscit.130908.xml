<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007823">
<title confidence="0.956315">
SAP-RI: Twitter Sentiment Analysis in Two Days
</title>
<author confidence="0.968932">
Akriti Vij1,2∗, Nishtha Malhotra1,2∗, Naveen Nandan1 and Daniel Dahlmeier1
</author>
<affiliation confidence="0.794768">
1SAP Research &amp; Innovation, Singapore
2Nanyang Technological University, Singapore
</affiliation>
<email confidence="0.994843">
{akriti.vij,nishtha.malhotra,naveen.nandan,d.dahlmeier}@sap.com
</email>
<sectionHeader confidence="0.997341" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999746">
We describe the submission of the SAP
Research &amp; Innovation team to the Se-
mEval 2014 Task 9: Sentiment Analy-
sis in Twitter. We challenged ourselves
to develop a competitive sentiment anal-
ysis system within a very limited time
frame. Our submission was developed
in less than two days and achieved an
F1 score of 77.26% for contextual polar-
ity disambiguation and 55.47% for mes-
sage polarity classification, which shows
that rapid prototyping of sentiment anal-
ysis systems with reasonable accuracy is
possible.
</bodyText>
<sectionHeader confidence="0.999393" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999206947368421">
Microblogging platforms and social networks
have become increasingly popular for expressing
opinions on a wide range of topics, hence mak-
ing them valuable and ever-growing logs of pub-
lic sentiment. This has motivated the development
of automatic natural language processing (NLP)
methods to analyse the sentiment expressed in
these short, informal messages (Liu, 2012; Pang
and Lee, 2008).
In particular, the study of sentiment and opin-
ions in messages from the Twitter microblogging
platform has attracted a lot of interest (Jansen et
al., 2009; Pak and Paroubek, 2010; Barbosa and
Feng, 2010; O’Connor et al., 2010; Bifet et al.,
2011). However, comparative evaluations of senti-
ment analysis of Twitter messages have previously
been hindered by the lack of a large benchmark
data set. The goal of the SemEval 2013 task 2:
Sentiment Analysis in Twitter (Nakov et al., 2013)
</bodyText>
<footnote confidence="0.8529902">
∗ The work was done during an internship at SAP.
This work is licenced under a Creative Commons Attribution
4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.999631516129032">
and this year’s continuation in the SemEval 2014
task 9: Sentiment Analysis in Twitter (Rosenthal
et al., 2014) is to close this gap by hosting a shared
task competition which provided a large corpus of
Twitter messages which are annotated with sen-
timent polarity labels. The task consists of two
subtasks: in subtask A contextual polarity disam-
biguation, participants need to predict the polarity
of a given word or phrase in the context of a tweet
message, in subtask B message polarity classifica-
tion, participants need to predict the dominating
sentiment of the complete message. Both tasks
consider sentiment analysis to be a three-way clas-
sification problem between positive, negative, and
neutral sentiment.
In this paper, we describe the submission of the
SAP-RI team to the SemEval 2014 task 9. We
challenged ourselves to develop a competitive sen-
timent analysis system within a very limited time
frame. The complete system was implemented
within only two days. Our system is based on
supervised classification with support vector ma-
chines with lexical and dictionary-based features.
Our system achieved an F1 score of 77.26% for
contextual polarity disambiguation and 55.47%
for message polarity classification. Although our
scores are about 10-20% behind the top-scoring
systems, we show that it is possible to develop
sentiment analysis systems via rapid prototyping
with reasonable accuracy in a very short amount
of time.
</bodyText>
<sectionHeader confidence="0.996601" genericHeader="introduction">
2 Methods
</sectionHeader>
<bodyText confidence="0.99989775">
Our system is based on supervised classification
with support vector machines and a variety of lex-
ical and dictionary-based features. From the be-
ginning, we decided to restrict ourselves to super-
vised classification and to focus on the constrained
system setting. Experiments with more data or
semi-supervised learning would have required ad-
ditional time and the results of last year’s task
</bodyText>
<page confidence="0.965651">
522
</page>
<note confidence="0.730086">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 522–526,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999979444444444">
did not show any convincing improvements using
from additional unconstrained data (Nakov et al.,
2013). We cast sentiment analysis as a multi-class
classification problem with three classes: positive,
negative, and neutral. For the features, we tried to
re-implement most of the features from the NRC-
Canada system (Mohammad et al., 2013) which
was the best performing system in last year’s task.
We describe the features in the following sections.
</bodyText>
<subsectionHeader confidence="0.996265">
2.1 Task A : Features
</subsectionHeader>
<bodyText confidence="0.997757777777778">
For the contextual polarity disambiguation task,
we extract features from the target phrase itself
and from a surrounding word window of four
words before and after the target phrase. To handle
negation, we append the suffix -neg to all words
in a negated context. A negated context includes
any word in the target phrase or context that is fol-
lowing a negation word 1 up to the next following
punctuation symbol.
</bodyText>
<listItem confidence="0.946476833333333">
• Word N-grams: all lowercased unigrams
and bigrams from the target phrase and the
context. We extract the lowercased full string
of the target phrase as an additional feature.
• Character N-grams: lowercased character
bigram and trigram prefixes and suffixes from
all words in the target phrase and the context.
• Elongations: binary feature that indicates the
presence of one or more words in the target
phrase or context that have a letter repeated
for 3 for more times e.g., coool.
• Emoticons: two binary features that indicate
the presence of positive or negative emoti-
cons in the target phrase or the context, re-
spectively. Two additional binary features
indicate the presence of positive or negative
emoticons at the end of the target phrase or
context2.
• Punctuation: three count features for the
number of tokens that consist only of excla-
mation marks, only of questions marks, or
a mix of exclamation marks and questions
marks, in the target phrase and context, re-
ceptively.
</listItem>
<footnote confidence="0.8322485">
1http://sentiment.christopherpotts.
net/lingstruc.html
</footnote>
<construct confidence="0.670013666666667">
2positive emoticons: :-), :), :B, :-B, :3, =), &lt;3, :D, :-D,
=D, :’), :d, ;), :}, :], :P, :-P, :-p, :p. negative emoticons: :-(,
:/, :{, :[, -.-, - -, :O, :o, :´(, :x, :X, v.v, ;(
</construct>
<listItem confidence="0.997740780487805">
• Casing: two binary features that indicate the
presence of at least one all upper-case word
and at least one title-cased word in the target
phrase or context, respectively.
• Stop words: a binary feature that indicates if
all the words in the target phrase or context
are stop words. If so, an additional feature
indicates the number of stop words: 1, 2, 3,
or more stop words.
• Length: the number of tokens in the target
phrase and the context, plus a binary feature
that indicates the presence of any word with
more than three characters.
• Position: three binary features that indicate
whether a target phrase is at the beginning, in
the middle, or at the end of the tweet.
• Hashtags: all hashtags in the target phrase
or the context. To handle hashtags which are
formed by concatenating words, e.g., #ihate-
mondays, we additionally split hashtags us-
ing a simple dictionary-based approach and
add each token of the segmented hashtag as
an additional features.
• Twitter user: binary feature that indicates
whether the context or the target phrase con-
tain a mention of a Twitter user.
• URL: binary feature that indicates whether
the context or the target phrase contains a
URL.
• Brown cluster: the word cluster index for
each word in the context. Cluster indexes are
obtained from the Brown word clusters of the
ARK Twitter tagger (Owoputi et al., 2013).
• Sentiment lexicons: we add the follow-
ing sentiment dictionary features for the tar-
get phrase and the context for four differ-
ent sentiment lexicons (NRC sentiment lex-
icon, NRC Hashtag lexicon (Mohammad et
al., 2013), MPQA sentiment lexicon (Wilson
et al., 2005), and Bing Liu lexicon (Hu and
Liu, 2004)):
</listItem>
<bodyText confidence="0.60795">
– the count of words with positive senti-
ment score.
– the sum of the sentiment scores for all
words.
</bodyText>
<page confidence="0.983251">
523
</page>
<bodyText confidence="0.9100315">
– the maximum non-negative sentiment
score for any word.
– the sentiment score of the last word with
positive sentiment score.
We extract these features for both the target
phrase and the context. For words that are
marked as negated, the sign of the sentiment
scores flipped. The MPQA lexicons requires
part of speech information. We use the ARK
Twitter part-of-speech tagger (Owoputi et al.,
2013) to tag the input with part of speech
tags.
</bodyText>
<subsectionHeader confidence="0.99689">
2.2 Task B : Features
</subsectionHeader>
<bodyText confidence="0.999620333333333">
For the message polarity task, we extract features
from the entire tweet message. The features are
similar to the features for phrase polarity disam-
biguation. As before we handle negation by ap-
pending the suffix -neg to all words that appear in
a negated context.
</bodyText>
<listItem confidence="0.993541740740741">
• Word N-grams: all lowercased N-grams for
N=1,...,4 from the message. We also in-
clude ”skipgrams” for each N-gram by re-
placing each token in the N-gram with a as-
terisk place holder, e.g., the cat → * cat,
the *.
• Character N-grams: lowercased charac-
ter level N-grams for N=3,..., 5 for all the
words in the message. Character N-grams do
not cross word boundaries.
• Elongations: count of words in the message
which have a letter repeated for 3 for more
times.
• Emoticons: similar to the contextual polarity
disambiguation task: two binary features for
presence of positive or negative emoticons in
the message and two binary features indicate
the presence of positive or negative emoti-
cons at the end of the message.
• Punctuation: similar to the contextual polar-
ity disambiguation task: three count features
for the number of tokens that consist only of
exclamation marks, only of questions marks,
or a mix of exclamation marks and questions
marks.
• Hashtags: all hashtags in the message. We
do not split concatonated hashtags.
</listItem>
<table confidence="0.999634428571429">
# Tokens # Tweets
Subtask A
Training (SemEval 2014 train) 160,992 7,884
Development (SemEval 2013 test) 76,409 3,710
Subtask B
Training (SemEval 2014 train) 139,128 7,112
Development (SemEval 2013 test) 47,052 2,405
</table>
<tableCaption confidence="0.999381">
Table 1: Overview of the data sets.
</tableCaption>
<listItem confidence="0.999604">
• Casing: the count of all upper-case words in
the message.
• Brown cluster: similar to the contextual po-
</listItem>
<bodyText confidence="0.637279">
larity disambiguation task: the cluster index
for each word in the message.
</bodyText>
<sectionHeader confidence="0.996905" genericHeader="related work">
3 Experiment and Results
</sectionHeader>
<bodyText confidence="0.999982848484848">
In this section, we report experimental result for
our method. We used the scikit-learn Python ma-
chine learning library (Pedregosa et al., 2011) to
implement the feature extraction pipeline and the
support vector machine classifier. We use a linear
kernel for the support vector machine and fixed the
SVM hyper-parameter C to 1.0. We found that
scikit-learn allowed us to implement the system
faster and resulted in much more compact code
than other machine learning tools we have worked
with in the past.
We used the official training set provided for the
SemEval 2014 task to train our system and evalu-
ated on the test set of the SemEval 2013 task which
served as development data for this year’s task 3.
Tweets in the training data that were not available
any more through the Twitter API were removed
from the training set. An overview of the data sets
is shown in Table 1. For the evaluation, we com-
pute precision, recall and Fi measure for the pos-
itive, negative, and neutral sentiment tweets. Fol-
lowing the official evaluation metric, the overall
precision, recall, and Fi measure of the system is
the average of the precision, recall, and Fi mea-
sures for positive and negative sentiment, respec-
tively.
Here, we report a feature ablation study: we
omitted each individual feature category from the
complete feature set to determine its influence on
the overall performance. Table 2 summarizes the
results for subtask A and B. Surprisingly many of
the features do not result in a reduction of the Fi
score when removed, or even increase the score,
</bodyText>
<footnote confidence="0.995183333333333">
3We also did some experiments with a 60:40 training/test
split of the SemEval 2014 training data which showed com-
parable results
</footnote>
<page confidence="0.988225">
524
</page>
<table confidence="0.999966962962963">
Features Positive Negative Neutral Overall
Subtask A P R F1 P R F1 P R F1 P R F1
All features 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.82 0.83 0.82
w/o Word N-grams 0.84 0.82 0.83 0.71 0.74 0.72 0.14 0.16 0.15 0.77 0.78 0.78
w/o character N-grams 0.85 0.89 0.87 0.80 0.78 0.79 0.27 0.12 0.17 0.82 0.83 0.83
w/o elongation 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.81 0.82 0.81
w/o emoticons 0.85 0.87 0.86 0.78 0.78 0.78 0.24 0.14 0.18 0.82 0.83 0.82
w/o punctuation 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.81 0.83 0.82
w/o casing 0.86 0.87 0.87 0.78 0.78 0.78 0.23 0.13 0.17 0.82 0.83 0.82
w/o stop words 0.86 0.87 0.86 0.78 0.79 0.78 0.24 0.15 0.18 0.82 0.83 0.82
w/o length 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.14 0.17 0.82 0.83 0.82
w/o position 0.86 0.87 0.86 0.77 0.78 0.78 0.24 0.13 0.17 0.81 0.83 0.82
w/o hashtags 0.86 0.87 0.87 0.78 0.78 0.78 0.24 0.14 0.18 0.82 0.83 0.82
w/o twitter user 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.82 0.83 0.82
w/o URL 0.86 0.87 0.86 0.78 0.78 0.78 0.23 0.13 0.17 0.81 0.82 0.81
w/o Brown cluster 0.86 0.88 0.87 0.78 0.80 0.79 0.25 0.13 0.17 0.82 0.84 0.83
w/o Sentiment lexicon 0.81 0.84 0.82 0.70 0.68 0.69 0.16 0.09 0.11 0.75 0.76 0.76
Subtask B
All features 0.81 0.54 0.65 0.66 0.34 0.44 0.59 0.89 0.71 0.74 0.44 0.54
w/o word N-grams 0.73 0.59 0.65 0.52 0.46 0.49 0.61 0.75 0.67 0.62 0.52 0.57
w/o character N-grams 0.80 0.49 0.61 0.65 0.23 0.34 0.56 0.90 0.69 0.72 0.36 0.48
w/o elongation 0.81 0.54 0.65 0.66 0.34 0.44 0.59 0.89 0.71 0.74 0.44 0.55
w/o emoticons 0.82 0.54 0.65 0.66 0.33 0.44 0.59 0.89 0.72 0.74 0.44 0.55
w/o punctuation 0.81 0.54 0.65 0.66 0.34 0.45 0.59 0.89 0.71 0.74 0.44 0.55
w/o casing 0.81 0.54 0.65 0.66 0.33 0.44 0.59 0.89 0.71 0.74 0.44 0.55
w/o hashtags 0.82 0.54 0.65 0.65 0.33 0.44 0.59 0.89 0.71 0.74 0.44 0.54
w/o Brown cluster 0.81 0.54 0.65 0.65 0.33 0.44 0.59 0.89 0.71 0.73 0.43 0.54
</table>
<tableCaption confidence="0.991906">
Table 2: Experimental Results for feature ablation study. Each row shows the precision, recall, and F1
</tableCaption>
<bodyText confidence="0.995986333333333">
score for the positive, negative, and neutral class and the overall precision, recall, and F1 score after
removing the particular feature from the features set.
although not significantly. The most effective fea-
tures are word N-grams and the sentiment lexi-
cons. It is interesting that the performance for the
neutral class is very low for subtask A and high
for subtask B. We can also see that for subtask B,
our system clearly has a problem with recall for
the positive and negative sentiment.
For the performance of our system in the Se-
mEval 2014 shared task, we report the official
overall F1 scores of our system as released by the
organizers on the official test set in Table 3. The
scores were reported separately for different test
sets: the SemEval 2013 Twitter test set, a new Se-
mEval 2014 Twitter test set, a new test set from
LiveJournal blogs, the SMS test set from the NUS
SMS corpus (Chen and Kan, 2012), and a new
test set of sarcastic tweets. We also include the F1
score of the best participating system for each test
set and the rank of our system among all partic-
ipating systems. The results of our system were
fairly robust across different domains, with the
exception of messages containing sarcasm which
shows understanding sarcasm requires a deeper
and more subtle understanding of the text that is
not captured well in a simple linear model.
</bodyText>
<table confidence="0.999027461538462">
Dataset Best score Our score Rank
Subtask A
LiveJournal 2014 85.61 77.68 18 / 27
SMS 2013 89.31 80.26 13 / 27
Twitter 2013 90.14 80.32 17 / 27
Twitter 2014 86.63 77.26 15 / 27
Twitter 2014 Sarcasm 82.75 70.64 14 / 27
Subtask B
LiveJournal 2014 74.84 57.86 33 / 42
SMS 2013 70.28 49.00 34 / 42
Twitter 2013 72.12 50.18 37 / 42
Twitter 2014 70.96 55.47 32 / 42
Twitter 2014 Sarcasm 58.16 48.64 15 / 42
</table>
<tableCaption confidence="0.993988">
Table 3: Official results for Semeval 2014 test set.
Reported scores are overall F1 scores.
</tableCaption>
<sectionHeader confidence="0.999007" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.9999326">
In this paper, we have described the submission of
the SAP-RI team to the SemEval 2014 task 9. We
showed that is possible to develop sentiment anal-
ysis systems via rapid prototyping with reasonable
accuracy within a couple of days.
</bodyText>
<sectionHeader confidence="0.975422" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.876268333333333">
The research is partially funded by the Economic
Development Board and the National Research
Foundation of Singapore.
</bodyText>
<page confidence="0.997743">
525
</page>
<sectionHeader confidence="0.996285" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99977125">
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on Twitter from biased and noisy
data. In Proceedings of the 23rd International Con-
ference on Computational Linguistics, pages 36–44.
Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer,
and Ricard Gavalda. 2011. Detecting sentiment
change in Twitter streaming data. Journal of Ma-
chine LearningResearch - Proceedings Track, 17:5–
11.
Tao Chen and Min-Yen Kan. 2012. Creating a live,
public short message service corpus: the NUS SMS
corpus. Language Resources and Evaluation, pages
1–37.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 168–177.
Bernhard J. Jansen, Mimi Zhang, Kate Sobel, and Ab-
dur Chowdury. 2009. Twitter power: Tweets as
electronic word of mouth. J. Am. Soc Inf. Sci. Tech-
nol., 60(11):2169–2188.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1–167.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the state-of-
the-art in sentiment analysis of Tweets. In Proceed-
ings of the 7th International Workshop on Semantic
Evaluation, pages 321–327.
Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara
Rosenthal, Veselin Stoyanov, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis
in Twitter. In Proceedings of the 7th International
Workshop on Semantic Evaluation, pages 312–320.
Brendan O’Connor, Routledge Bryan R. Balasubra-
manyan, Ramnath, and Noah A. Smith. 2010. From
Tweets to polls: Linking text sentiment to public
opinion time series. In Proceedings of the Fourth In-
ternational Conference on Weblogs and Social Me-
dia (ICWSM ’10), pages 122–129.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), pages 380–390.
Alexander Pak and Patrick Paroubek. 2010. Twitter
based system: Using Twitter to disambiguating sen-
timent ambiguous adjectives. In Proceedings of the
8th International Workshop on Semantic Evaluation,
pages 436–439.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1–135.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Oliver
Grisel, Mathieu Blondel, Peter. Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ´Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825–2830.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ’14, Dublin, Ireland.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of Human
Language Technology and Empirical Methods in
Natural Language Processing (HLT-EMNLP), pages
307–314.
</reference>
<page confidence="0.998496">
526
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.893124">
<title confidence="0.998212">SAP-RI: Twitter Sentiment Analysis in Two Days</title>
<author confidence="0.966983">Nishtha Naveen</author>
<affiliation confidence="0.9967295">Research &amp; Innovation, Technological University,</affiliation>
<abstract confidence="0.994894666666667">We describe the submission of the SAP Research &amp; Innovation team to the SemEval 2014 Task 9: Sentiment Analysis in Twitter. We challenged ourselves to develop a competitive sentiment analysis system within a very limited time frame. Our submission was developed in less than two days and achieved an of 77.26% for contextual polarity disambiguation and 55.47% for message polarity classification, which shows that rapid prototyping of sentiment analysis systems with reasonable accuracy is possible.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on Twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>36--44</pages>
<contexts>
<context position="1395" citStr="Barbosa and Feng, 2010" startWordPosition="200" endWordPosition="203"> is possible. 1 Introduction Microblogging platforms and social networks have become increasingly popular for expressing opinions on a wide range of topics, hence making them valuable and ever-growing logs of public sentiment. This has motivated the development of automatic natural language processing (NLP) methods to analyse the sentiment expressed in these short, informal messages (Liu, 2012; Pang and Lee, 2008). In particular, the study of sentiment and opinions in messages from the Twitter microblogging platform has attracted a lot of interest (Jansen et al., 2009; Pak and Paroubek, 2010; Barbosa and Feng, 2010; O’Connor et al., 2010; Bifet et al., 2011). However, comparative evaluations of sentiment analysis of Twitter messages have previously been hindered by the lack of a large benchmark data set. The goal of the SemEval 2013 task 2: Sentiment Analysis in Twitter (Nakov et al., 2013) ∗ The work was done during an internship at SAP. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ and this year’s continuation in the SemEval 2014 task 9:</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on Twitter from biased and noisy data. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 36–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Bifet</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Ricard Gavalda</author>
</authors>
<title>Detecting sentiment change in Twitter streaming data.</title>
<date>2011</date>
<journal>Journal of Machine LearningResearch - Proceedings Track,</journal>
<volume>17</volume>
<pages>11</pages>
<contexts>
<context position="1439" citStr="Bifet et al., 2011" startWordPosition="208" endWordPosition="211">forms and social networks have become increasingly popular for expressing opinions on a wide range of topics, hence making them valuable and ever-growing logs of public sentiment. This has motivated the development of automatic natural language processing (NLP) methods to analyse the sentiment expressed in these short, informal messages (Liu, 2012; Pang and Lee, 2008). In particular, the study of sentiment and opinions in messages from the Twitter microblogging platform has attracted a lot of interest (Jansen et al., 2009; Pak and Paroubek, 2010; Barbosa and Feng, 2010; O’Connor et al., 2010; Bifet et al., 2011). However, comparative evaluations of sentiment analysis of Twitter messages have previously been hindered by the lack of a large benchmark data set. The goal of the SemEval 2013 task 2: Sentiment Analysis in Twitter (Nakov et al., 2013) ∗ The work was done during an internship at SAP. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ and this year’s continuation in the SemEval 2014 task 9: Sentiment Analysis in Twitter (Rosenthal et</context>
</contexts>
<marker>Bifet, Holmes, Pfahringer, Gavalda, 2011</marker>
<rawString>Albert Bifet, Geoffrey Holmes, Bernhard Pfahringer, and Ricard Gavalda. 2011. Detecting sentiment change in Twitter streaming data. Journal of Machine LearningResearch - Proceedings Track, 17:5– 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Chen</author>
<author>Min-Yen Kan</author>
</authors>
<title>Creating a live, public short message service corpus: the NUS SMS corpus. Language Resources and Evaluation,</title>
<date>2012</date>
<pages>1--37</pages>
<contexts>
<context position="14616" citStr="Chen and Kan, 2012" startWordPosition="2452" endWordPosition="2455">ce for the neutral class is very low for subtask A and high for subtask B. We can also see that for subtask B, our system clearly has a problem with recall for the positive and negative sentiment. For the performance of our system in the SemEval 2014 shared task, we report the official overall F1 scores of our system as released by the organizers on the official test set in Table 3. The scores were reported separately for different test sets: the SemEval 2013 Twitter test set, a new SemEval 2014 Twitter test set, a new test set from LiveJournal blogs, the SMS test set from the NUS SMS corpus (Chen and Kan, 2012), and a new test set of sarcastic tweets. We also include the F1 score of the best participating system for each test set and the rank of our system among all participating systems. The results of our system were fairly robust across different domains, with the exception of messages containing sarcasm which shows understanding sarcasm requires a deeper and more subtle understanding of the text that is not captured well in a simple linear model. Dataset Best score Our score Rank Subtask A LiveJournal 2014 85.61 77.68 18 / 27 SMS 2013 89.31 80.26 13 / 27 Twitter 2013 90.14 80.32 17 / 27 Twitter </context>
</contexts>
<marker>Chen, Kan, 2012</marker>
<rawString>Tao Chen and Min-Yen Kan. 2012. Creating a live, public short message service corpus: the NUS SMS corpus. Language Resources and Evaluation, pages 1–37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>168--177</pages>
<contexts>
<context position="7686" citStr="Hu and Liu, 2004" startWordPosition="1225" endWordPosition="1228">rase contain a mention of a Twitter user. • URL: binary feature that indicates whether the context or the target phrase contains a URL. • Brown cluster: the word cluster index for each word in the context. Cluster indexes are obtained from the Brown word clusters of the ARK Twitter tagger (Owoputi et al., 2013). • Sentiment lexicons: we add the following sentiment dictionary features for the target phrase and the context for four different sentiment lexicons (NRC sentiment lexicon, NRC Hashtag lexicon (Mohammad et al., 2013), MPQA sentiment lexicon (Wilson et al., 2005), and Bing Liu lexicon (Hu and Liu, 2004)): – the count of words with positive sentiment score. – the sum of the sentiment scores for all words. 523 – the maximum non-negative sentiment score for any word. – the sentiment score of the last word with positive sentiment score. We extract these features for both the target phrase and the context. For words that are marked as negated, the sign of the sentiment scores flipped. The MPQA lexicons requires part of speech information. We use the ARK Twitter part-of-speech tagger (Owoputi et al., 2013) to tag the input with part of speech tags. 2.2 Task B : Features For the message polarity ta</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 168–177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard J Jansen</author>
<author>Mimi Zhang</author>
<author>Kate Sobel</author>
<author>Abdur Chowdury</author>
</authors>
<title>Twitter power: Tweets as electronic word of mouth.</title>
<date>2009</date>
<journal>J. Am. Soc Inf. Sci. Technol.,</journal>
<volume>60</volume>
<issue>11</issue>
<contexts>
<context position="1347" citStr="Jansen et al., 2009" startWordPosition="192" endWordPosition="195">ent analysis systems with reasonable accuracy is possible. 1 Introduction Microblogging platforms and social networks have become increasingly popular for expressing opinions on a wide range of topics, hence making them valuable and ever-growing logs of public sentiment. This has motivated the development of automatic natural language processing (NLP) methods to analyse the sentiment expressed in these short, informal messages (Liu, 2012; Pang and Lee, 2008). In particular, the study of sentiment and opinions in messages from the Twitter microblogging platform has attracted a lot of interest (Jansen et al., 2009; Pak and Paroubek, 2010; Barbosa and Feng, 2010; O’Connor et al., 2010; Bifet et al., 2011). However, comparative evaluations of sentiment analysis of Twitter messages have previously been hindered by the lack of a large benchmark data set. The goal of the SemEval 2013 task 2: Sentiment Analysis in Twitter (Nakov et al., 2013) ∗ The work was done during an internship at SAP. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ and this</context>
</contexts>
<marker>Jansen, Zhang, Sobel, Chowdury, 2009</marker>
<rawString>Bernhard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur Chowdury. 2009. Twitter power: Tweets as electronic word of mouth. J. Am. Soc Inf. Sci. Technol., 60(11):2169–2188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment analysis and opinion mining.</title>
<date>2012</date>
<journal>Synthesis Lectures on Human Language Technologies,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="1169" citStr="Liu, 2012" startWordPosition="164" endWordPosition="165">s and achieved an F1 score of 77.26% for contextual polarity disambiguation and 55.47% for message polarity classification, which shows that rapid prototyping of sentiment analysis systems with reasonable accuracy is possible. 1 Introduction Microblogging platforms and social networks have become increasingly popular for expressing opinions on a wide range of topics, hence making them valuable and ever-growing logs of public sentiment. This has motivated the development of automatic natural language processing (NLP) methods to analyse the sentiment expressed in these short, informal messages (Liu, 2012; Pang and Lee, 2008). In particular, the study of sentiment and opinions in messages from the Twitter microblogging platform has attracted a lot of interest (Jansen et al., 2009; Pak and Paroubek, 2010; Barbosa and Feng, 2010; O’Connor et al., 2010; Bifet et al., 2011). However, comparative evaluations of sentiment analysis of Twitter messages have previously been hindered by the lack of a large benchmark data set. The goal of the SemEval 2013 task 2: Sentiment Analysis in Twitter (Nakov et al., 2013) ∗ The work was done during an internship at SAP. This work is licenced under a Creative Comm</context>
</contexts>
<marker>Liu, 2012</marker>
<rawString>Bing Liu. 2012. Sentiment analysis and opinion mining. Synthesis Lectures on Human Language Technologies, 5(1):1–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the state-ofthe-art in sentiment analysis of Tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation,</booktitle>
<pages>321--327</pages>
<contexts>
<context position="4254" citStr="Mohammad et al., 2013" startWordPosition="641" endWordPosition="644"> system setting. Experiments with more data or semi-supervised learning would have required additional time and the results of last year’s task 522 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 522–526, Dublin, Ireland, August 23-24, 2014. did not show any convincing improvements using from additional unconstrained data (Nakov et al., 2013). We cast sentiment analysis as a multi-class classification problem with three classes: positive, negative, and neutral. For the features, we tried to re-implement most of the features from the NRCCanada system (Mohammad et al., 2013) which was the best performing system in last year’s task. We describe the features in the following sections. 2.1 Task A : Features For the contextual polarity disambiguation task, we extract features from the target phrase itself and from a surrounding word window of four words before and after the target phrase. To handle negation, we append the suffix -neg to all words in a negated context. A negated context includes any word in the target phrase or context that is following a negation word 1 up to the next following punctuation symbol. • Word N-grams: all lowercased unigrams and bigrams f</context>
<context position="7599" citStr="Mohammad et al., 2013" startWordPosition="1210" endWordPosition="1213">features. • Twitter user: binary feature that indicates whether the context or the target phrase contain a mention of a Twitter user. • URL: binary feature that indicates whether the context or the target phrase contains a URL. • Brown cluster: the word cluster index for each word in the context. Cluster indexes are obtained from the Brown word clusters of the ARK Twitter tagger (Owoputi et al., 2013). • Sentiment lexicons: we add the following sentiment dictionary features for the target phrase and the context for four different sentiment lexicons (NRC sentiment lexicon, NRC Hashtag lexicon (Mohammad et al., 2013), MPQA sentiment lexicon (Wilson et al., 2005), and Bing Liu lexicon (Hu and Liu, 2004)): – the count of words with positive sentiment score. – the sum of the sentiment scores for all words. 523 – the maximum non-negative sentiment score for any word. – the sentiment score of the last word with positive sentiment score. We extract these features for both the target phrase and the context. For words that are marked as negated, the sign of the sentiment scores flipped. The MPQA lexicons requires part of speech information. We use the ARK Twitter part-of-speech tagger (Owoputi et al., 2013) to ta</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-ofthe-art in sentiment analysis of Tweets. In Proceedings of the 7th International Workshop on Semantic Evaluation, pages 321–327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Zornitsa Kozareva</author>
<author>Alan Ritter</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in Twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation,</booktitle>
<pages>312--320</pages>
<contexts>
<context position="1676" citStr="Nakov et al., 2013" startWordPosition="247" endWordPosition="250">anguage processing (NLP) methods to analyse the sentiment expressed in these short, informal messages (Liu, 2012; Pang and Lee, 2008). In particular, the study of sentiment and opinions in messages from the Twitter microblogging platform has attracted a lot of interest (Jansen et al., 2009; Pak and Paroubek, 2010; Barbosa and Feng, 2010; O’Connor et al., 2010; Bifet et al., 2011). However, comparative evaluations of sentiment analysis of Twitter messages have previously been hindered by the lack of a large benchmark data set. The goal of the SemEval 2013 task 2: Sentiment Analysis in Twitter (Nakov et al., 2013) ∗ The work was done during an internship at SAP. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ and this year’s continuation in the SemEval 2014 task 9: Sentiment Analysis in Twitter (Rosenthal et al., 2014) is to close this gap by hosting a shared task competition which provided a large corpus of Twitter messages which are annotated with sentiment polarity labels. The task consists of two subtasks: in subtask A contextual polari</context>
<context position="4019" citStr="Nakov et al., 2013" startWordPosition="605" endWordPosition="608">ased on supervised classification with support vector machines and a variety of lexical and dictionary-based features. From the beginning, we decided to restrict ourselves to supervised classification and to focus on the constrained system setting. Experiments with more data or semi-supervised learning would have required additional time and the results of last year’s task 522 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 522–526, Dublin, Ireland, August 23-24, 2014. did not show any convincing improvements using from additional unconstrained data (Nakov et al., 2013). We cast sentiment analysis as a multi-class classification problem with three classes: positive, negative, and neutral. For the features, we tried to re-implement most of the features from the NRCCanada system (Mohammad et al., 2013) which was the best performing system in last year’s task. We describe the features in the following sections. 2.1 Task A : Features For the contextual polarity disambiguation task, we extract features from the target phrase itself and from a surrounding word window of four words before and after the target phrase. To handle negation, we append the suffix -neg to</context>
</contexts>
<marker>Nakov, Kozareva, Ritter, Rosenthal, Stoyanov, Wilson, 2013</marker>
<rawString>Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara Rosenthal, Veselin Stoyanov, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in Twitter. In Proceedings of the 7th International Workshop on Semantic Evaluation, pages 312–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Routledge Bryan R Balasubramanyan</author>
<author>Ramnath</author>
<author>Noah A Smith</author>
</authors>
<title>From Tweets to polls: Linking text sentiment to public opinion time series.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth International Conference on Weblogs and Social Media (ICWSM ’10),</booktitle>
<pages>122--129</pages>
<marker>O’Connor, Balasubramanyan, Ramnath, Smith, 2010</marker>
<rawString>Brendan O’Connor, Routledge Bryan R. Balasubramanyan, Ramnath, and Noah A. Smith. 2010. From Tweets to polls: Linking text sentiment to public opinion time series. In Proceedings of the Fourth International Conference on Weblogs and Social Media (ICWSM ’10), pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A. Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Pak</author>
<author>Patrick Paroubek</author>
</authors>
<title>Twitter based system: Using Twitter to disambiguating sentiment ambiguous adjectives.</title>
<date>2010</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation,</booktitle>
<pages>436--439</pages>
<contexts>
<context position="1371" citStr="Pak and Paroubek, 2010" startWordPosition="196" endWordPosition="199">with reasonable accuracy is possible. 1 Introduction Microblogging platforms and social networks have become increasingly popular for expressing opinions on a wide range of topics, hence making them valuable and ever-growing logs of public sentiment. This has motivated the development of automatic natural language processing (NLP) methods to analyse the sentiment expressed in these short, informal messages (Liu, 2012; Pang and Lee, 2008). In particular, the study of sentiment and opinions in messages from the Twitter microblogging platform has attracted a lot of interest (Jansen et al., 2009; Pak and Paroubek, 2010; Barbosa and Feng, 2010; O’Connor et al., 2010; Bifet et al., 2011). However, comparative evaluations of sentiment analysis of Twitter messages have previously been hindered by the lack of a large benchmark data set. The goal of the SemEval 2013 task 2: Sentiment Analysis in Twitter (Nakov et al., 2013) ∗ The work was done during an internship at SAP. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ and this year’s continuation in </context>
</contexts>
<marker>Pak, Paroubek, 2010</marker>
<rawString>Alexander Pak and Patrick Paroubek. 2010. Twitter based system: Using Twitter to disambiguating sentiment ambiguous adjectives. In Proceedings of the 8th International Workshop on Semantic Evaluation, pages 436–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis. Foundations and trends in information retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="1190" citStr="Pang and Lee, 2008" startWordPosition="166" endWordPosition="169">ved an F1 score of 77.26% for contextual polarity disambiguation and 55.47% for message polarity classification, which shows that rapid prototyping of sentiment analysis systems with reasonable accuracy is possible. 1 Introduction Microblogging platforms and social networks have become increasingly popular for expressing opinions on a wide range of topics, hence making them valuable and ever-growing logs of public sentiment. This has motivated the development of automatic natural language processing (NLP) methods to analyse the sentiment expressed in these short, informal messages (Liu, 2012; Pang and Lee, 2008). In particular, the study of sentiment and opinions in messages from the Twitter microblogging platform has attracted a lot of interest (Jansen et al., 2009; Pak and Paroubek, 2010; Barbosa and Feng, 2010; O’Connor et al., 2010; Bifet et al., 2011). However, comparative evaluations of sentiment analysis of Twitter messages have previously been hindered by the lack of a large benchmark data set. The goal of the SemEval 2013 task 2: Sentiment Analysis in Twitter (Nakov et al., 2013) ∗ The work was done during an internship at SAP. This work is licenced under a Creative Commons Attribution 4.0 I</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<marker>Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Oliver Grisel, Mathieu Blondel, Peter. Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<date>2014</date>
<booktitle>SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Preslav Nakov and Torsten Zesch, editors, Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval ’14,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2050" citStr="Rosenthal et al., 2014" startWordPosition="301" endWordPosition="304">t al., 2011). However, comparative evaluations of sentiment analysis of Twitter messages have previously been hindered by the lack of a large benchmark data set. The goal of the SemEval 2013 task 2: Sentiment Analysis in Twitter (Nakov et al., 2013) ∗ The work was done during an internship at SAP. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http: //creativecommons.org/licenses/by/4.0/ and this year’s continuation in the SemEval 2014 task 9: Sentiment Analysis in Twitter (Rosenthal et al., 2014) is to close this gap by hosting a shared task competition which provided a large corpus of Twitter messages which are annotated with sentiment polarity labels. The task consists of two subtasks: in subtask A contextual polarity disambiguation, participants need to predict the polarity of a given word or phrase in the context of a tweet message, in subtask B message polarity classification, participants need to predict the dominating sentiment of the complete message. Both tasks consider sentiment analysis to be a three-way classification problem between positive, negative, and neutral sentime</context>
</contexts>
<marker>Rosenthal, Nakov, Ritter, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Alan Ritter, and Veselin Stoyanov. 2014. SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Preslav Nakov and Torsten Zesch, editors, Proceedings of the 8th International Workshop on Semantic Evaluation, SemEval ’14, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP),</booktitle>
<pages>307--314</pages>
<contexts>
<context position="7645" citStr="Wilson et al., 2005" startWordPosition="1217" endWordPosition="1220">dicates whether the context or the target phrase contain a mention of a Twitter user. • URL: binary feature that indicates whether the context or the target phrase contains a URL. • Brown cluster: the word cluster index for each word in the context. Cluster indexes are obtained from the Brown word clusters of the ARK Twitter tagger (Owoputi et al., 2013). • Sentiment lexicons: we add the following sentiment dictionary features for the target phrase and the context for four different sentiment lexicons (NRC sentiment lexicon, NRC Hashtag lexicon (Mohammad et al., 2013), MPQA sentiment lexicon (Wilson et al., 2005), and Bing Liu lexicon (Hu and Liu, 2004)): – the count of words with positive sentiment score. – the sum of the sentiment scores for all words. 523 – the maximum non-negative sentiment score for any word. – the sentiment score of the last word with positive sentiment score. We extract these features for both the target phrase and the context. For words that are marked as negated, the sign of the sentiment scores flipped. The MPQA lexicons requires part of speech information. We use the ARK Twitter part-of-speech tagger (Owoputi et al., 2013) to tag the input with part of speech tags. 2.2 Task</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of Human Language Technology and Empirical Methods in Natural Language Processing (HLT-EMNLP), pages 307–314.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>