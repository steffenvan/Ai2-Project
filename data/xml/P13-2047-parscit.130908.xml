<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000170">
<title confidence="0.697158">
English→Russian MT evaluation campaign
</title>
<author confidence="0.231258">
Pavel Braslavslii Alexander Beloborodov Maxim Khalilov Serge Sharoff
</author>
<table confidence="0.37696">
Kontur Labs / Ural Federal University TAUS Labs University of Leeds
Ural Federal Russia The Netherlands UK
University,Russia xander-beloborodov maxim s.sharoff
pbras@yandex.ru @yandex.ru @tauslabs.com @leeds.ac.uk
</table>
<sectionHeader confidence="0.979934" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999866555555556">
This paper presents the settings and the re-
sults of the ROMIP 2013 MT shared task
for the English→Russian language direc-
tion. The quality of generated translations
was assessed using automatic metrics and
human evaluation. We also discuss ways
to reduce human evaluation efforts using
pairwise sentence comparisons by human
judges to simulate sort operations.
</bodyText>
<sectionHeader confidence="0.998734" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999747148148148">
Machine Translation (MT) between English and
Russian was one of the first translation directions
tested at the dawn of MT research in the 1950s
(Hutchins, 2000). Since then the MT paradigms
changed many times, many systems for this lan-
guage pair appeared (and disappeared), but as far
as we know there was no systematic quantitative
evaluation of a range of systems, analogous to
DARPA’94 (White et al., 1994) and later evalua-
tion campaigns. The Workshop on Statistical MT
(WMT) in 2013 has announced a Russian evalua-
tion track for the first time.1 However, this evalu-
ation is currently ongoing, it should include new
methods for building statistical MT (SMT) sys-
tems for Russian from the data provided in this
track, but it will not cover the performance of ex-
isting systems, especially rule-based (RBMT) or
hybrid ones.
Evaluation campaigns play an important role
in promotion of the progress for MT technolo-
gies. Recently, there have been a number of
MT shared tasks for combinations of several Eu-
ropean, Asian and Semitic languages (Callison-
Burch et al., 2011; Callison-Burch et al., 2012;
Federico et al., 2012), which we took into account
in designing the campaign for the English-Russian
direction. The evaluation has been held in the
</bodyText>
<footnote confidence="0.932336">
1http://www.statmt.org/wmt13/
</footnote>
<bodyText confidence="0.963380388888889">
context of ROMIP,2 which stands for Russian In-
formation Retrieval Evaluation Seminar and is a
TREC-like3 Russian initiative started in 2002.
One of the main challenges in developing MT
systems for Russian and for evaluating them is the
need to deal with its free word order and com-
plex morphology. Long-distance dependencies
are common, and this creates problems for both
RBMT and SMT systems (especially for phrase-
based ones). Complex morphology also leads
to considerable sparseness for word alignment in
SMT.
The language direction was chosen to be
English→Russian, first because of the availabil-
ity of native speakers for evaluation, second be-
cause the systems taking part in this evaluation are
mostly used in translation of English texts for the
Russian readers.
</bodyText>
<sectionHeader confidence="0.938466" genericHeader="method">
2 Corpus preparation
</sectionHeader>
<bodyText confidence="0.999689055555556">
In designing the set of texts for evaluation, we
had two issues in mind. First, it is known that
the domain and genre can influence MT perfor-
mance (Langlais, 2002; Babych et al., 2007), so
we wanted to control the set of genres. Second,
we were aiming at using sources allowing distri-
bution of texts under a Creative Commons licence.
In the end two genres were used coming from two
sources. The newswire texts were collected from
the English Wikinews website.4 The second genre
was represented by ‘regulations’ (laws, contracts,
rules, etc), which were collected from the Web
using a genre classification method described in
(Sharoff, 2010). The method provided a sufficient
accuracy (74%) for the initial selection of texts un-
der the category of ‘regulations,’ which was fol-
lowed by a manual check to reject texts clearly
outside of this genre category.
</bodyText>
<footnote confidence="0.999982">
2http://romip.ru/en/
3http://trec.nist.gov/
4http://en.wikinews.org/
</footnote>
<page confidence="0.91957">
262
</page>
<bodyText confidence="0.964898441860465">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 262–267,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
The initial corpus consists of 8,356 original
English texts that make up 148,864 sentences.
We chose to retain the entire texts in the cor-
pus rather than individual sentences, since some
MT systems may use information beyond isolated
sentences. 100,889 sentences originated from
Wikinews; 47,975 sentences came from the ‘reg-
ulations’ corpus. The first 1,002 sentences were
published in advance to allow potential partici-
pants time to adjust their systems to the corpus for-
mat. The remaining 147,862 sentences were the
corpus for testing translation into Russian. Two
examples of texts in the corpus:
90237 Ambassadors from the United States of
America, Australia and Britain have all met
with Fijian military officers to seek insur-
ances that there wasn’t going to be a coup.
102835 If you are given a discount for booking
more than one person onto the same date and
you later wish to transfer some of the dele-
gates to another event, the fees will be recal-
culated and you will be asked to pay addi-
tional fees due as well as any administrative
charge.
For automatic evaluation we randomly selected
947 ‘clean’ sentences, i.e. those with clear sen-
tence boundaries, no HTML markup remains, etc.
(such flaws sometimes occur in corpora collected
from the Web). 759 sentences originated from
the ‘news’ part of the corpus, the remaining 188
came from the ‘regulations’ part. The sentences
came from sources without published translations
into Russian, so that some of the participating sys-
tems do not get unfair advantage by using them for
training. These sentences were translated by pro-
fessional translators. For manual evaluation, we
randomly selected 330 sentences out of 947 used
for automatic evaluation, specifically, 190 from
the ‘news’ part and 140 from the ‘regulations’ part.
The organisers also provided participants with
access to the following additional resources:
</bodyText>
<listItem confidence="0.985733">
• 1 million sentences from the English-Russian
parallel corpus released by Yandex (the same
as used in WMT13)5;
• 119 thousand sentences from the English-
Russian parallel corpus from the TAUS Data
Repository.6
</listItem>
<bodyText confidence="0.8475015">
These resources are not related to the test corpus
of the evaluation campaign. Their purpose was
</bodyText>
<footnote confidence="0.997351">
5https://translate.yandex.ru/corpus?
lang=en
6https://www.tausdata.org
</footnote>
<bodyText confidence="0.999630666666667">
to make it easier to participate in the shared task
for teams without sufficient data for this language
pair.
</bodyText>
<sectionHeader confidence="0.991491" genericHeader="method">
3 Evaluation methodology
</sectionHeader>
<bodyText confidence="0.994772909090909">
The main idea of manual evaluation was (1) to
make the assessment as simple as possible for a
human judge and (2) to make the results of evalu-
ation unambiguous. We opted for pairwise com-
parison of MT outputs. This is different from
simultaneous ranking of several MT outputs, as
commonly used in WMT evaluation campaigns.
In case of a large number of participating sys-
tems each assessor ranks only a subset of MT out-
puts. However, a fair overall ranking cannot be al-
ways derived from such partial rankings (Callison-
Burch et al., 2012). The pairwise comparisons
we used can be directly converted into unambigu-
ous overall rankings. This task is also much sim-
pler for human judges to complete. On the other
hand, pairwise comparisons require a larger num-
ber of evaluation decisions, which is feasible only
for few participants (and we indeed had relatively
few submissions in this campaign). Below we also
discuss how to reduce the amount of human efforts
for evaluation.
In our case the assessors were asked to make a
pairwise comparison of two sentences translated
by two different MT systems against a gold stan-
dard translation. The question for them was to
judge translation adequacy, i.e., which MT output
conveys information from the reference translation
better. The source English sentence was not pre-
sented to the assessors, because we think that we
can have more trust in understanding of the source
text by a professional translator. The translator
also had access to the entire text, while the asses-
sors could only see a single sentence.
For human evaluation we employed the multi-
functional TAUS DQF tool7 in the ‘Quick Com-
parison’ mode.
Assessors’ judgements resulted in rankings for
each sentence in the test set. In case of ties the
ranks were averaged, e.g. when the ranks of the
systems in positions 2-4 and 7-8 were tied, their
ranks became: 1 3 3 3 5 6 7.5 7.5. To
produce the final ranking, the sentence-level ranks
were averaged over all sentences.
Pairwise comparisons are time-consuming: n
</bodyText>
<footnote confidence="0.95883">
7https://tauslabs.com/dynamic-quality/
dqf-tools-mt
</footnote>
<page confidence="0.992331">
263
</page>
<table confidence="0.995906538461538">
Metric OS1 OS2 OS3 OS4 P1 P2 P3 P4 P5 P6 P7
Automatic metrics ALL (947 sentences)
BLEU 0.150 0.141 0.133 0.124 0.157 0.112 0.105 0.073 0.094 0.071 0.073
NIST 5.12 4.94 4.80 4.67 5.00 4.46 4.11 2.38 4.16 3.362 3.38
Meteor 0.258 0.240 0.231 0.240 0.251 0.207 0.169 0.133 0.178 0.136 0.149
TER 0.755 0.766 0.764 0.758 0.758 0.796 0.901 0.931 0.826 0.934 0.830
GTM 0.351 0.338 0.332 0.336 0.349 0.303 0.246 0.207 0.275 0.208 0.230
Automatic metrics NEWS (759 sentences)
BLEU 0.137 0.131 0.123 0.114 0.153 0.103 0.096 0.070 0.083 0.066 0.067
NIST 4.86 4.72 4.55 4.35 4.79 4.26 3.83 2.47 3.90 3.20 3.19
Meteor 0.241 0.224 0.214 0.222 0.242 0.192 0.156 0.127 0.161 0.126 0.136
TER 0.772 0.776 0.784 0.777 0.768 0.809 0.908 0.936 0.844 0.938 0.839
GTM 0.335 0.324 0.317 0.320 0.339 0.290 0.233 0.201 0.257 0.199 0.217
</table>
<tableCaption confidence="0.81633">
Table 1: Automatic evaluation results
cases require n(n−1)
</tableCaption>
<bodyText confidence="0.985108948717949">
2 pairwise decisions. In this
study we also simulated a ‘human-assisted’ in-
sertion sort algorithm and its variant with binary
search. The idea is to run a standard sort algo-
rithm and ask a human judge each time a compar-
ison operation is required. This assumes that hu-
man perception of quality is transitive: if we know
that A &lt; B and B &lt; C, we can spare evaluation
of A and C. This approach also implies that sen-
tence pairs to judge are generated and presented to
assessors on the fly; each decision contributes to
selection of the pairs to be judged in the next step.
If the systems are pre-sorted in a reasonable way
(e.g. by an MT metric, under assumption that au-
tomatic pre-ranking is closer to the ‘ideal’ ranking
than a random one), then we can potentially save
even more pairwise comparison operations. Pre-
sorting makes ranking somewhat biased in favour
of the order established by an MT metric. For ex-
ample, if it favours one system against another,
while in human judgement they are equal, the final
ranking will preserve the initial order. Insertion
sort of n sentences requires n − 1 comparisons in
the best case of already sorted data and n(n−1)
2 in
the worst case (reversely ordered data). Insertion
sort with binary search requires ∼ n log n compar-
isons regardless of the initial order. For this study
we ran exhaustive pairwise evaluation and used its
results to simulate human-assisted sorting.
In addition to human evaluation, we also ran
system-level automatic evaluations using BLEU
(Papineni et al., 2001), NIST (Doddington,
2002), METEOR (Banerjee and Lavie, 2005),
TER (Snover et al., 2009), and GTM (Turian et
al., 2003). We also wanted to estimate the correla-
tions of these metrics with human judgements for
the English→Russian pair on the corpus level and
on the level of individual sentences.
</bodyText>
<sectionHeader confidence="0.999984" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.9999636">
We received results from five teams, two teams
submitted two runs each, which totals seven par-
ticipants’ runs (referred to as P1..P7 in the pa-
per). The participants represent SMT, RBMT,
and hybrid approaches. They included established
groups from academia and industry, as well as new
research teams. The evaluation runs also included
the translations of the 947 test sentences produced
by four free online systems in their default modes
(referred to as OS1..OS4). For 11 runs automatic
evaluation measures were calculated; eight runs
underwent manual evaluation (four online systems
plus four participants’ runs; no manual evaluation
was done by agreement with the participants for
the runs P3, P6, and P7 to reduce the workload).
</bodyText>
<sectionHeader confidence="0.568318" genericHeader="method">
ID Name and information
</sectionHeader>
<table confidence="0.576870333333333">
Phrase-based SMT
Phrase-based SMT
Hybrid (RBMT+statistical PE)
Dependency-based SMT
Compreno, Hybrid, ABBYY Corp
Pharaon, Moses, Yandex&amp;TAUS data
Balagur, Moses, Yandex&amp;news data
ETAP-3, RBMT, (Boguslavsky, 1995)
Pereved, Moses, Internet data
</table>
<bodyText confidence="0.8573165">
OS3 is a hybrid system based on RBMT with
SMT post-editing (PE). P1 is a hybrid system with
analysis and generation driven by statistical evalu-
ation of hypotheses.
</bodyText>
<figure confidence="0.542532222222222">
OS1
OS2
OS3
OS4
P1
P2
P3,4
P5
P6,7
</figure>
<page confidence="0.994899">
264
</page>
<table confidence="0.9998716">
All (330 sentences)
OS3 (highest) P1 OS1 OS2 OS4 P5 P2 P4 (lowest)
3.159 3.350 3.530 3.961 4.082 5.447 5.998 6.473
News (190 sentences)
OS3 (highest) P1 OS1 OS2 OS4 P5 P2 P4 (lowest)
2.947 3.450 3.482 4.084 4.242 5.474 5,968 6,353
Regulations (140 sentences)
P1 (highest) OS3 OS1 OS2 OS4 P5 P2 P4 (lowest)
3.214 3.446 3.596 3.793 3.864 5.411 6.039 6.636
Simulated dynamic ranking (insertion sort)
P1 (highest) OS1 OS3 OS2 OS4 P5 P4 P2 (lowest)
3.318 3.327 3.588 4.221 4.300 5.227 5.900 6.118
Simulated dynamic ranking (binary insertion sort)
OS1 (highest) P1 OS3 OS2 OS4 P5 P2 P4 (lowest)
2.924 3.045 3.303 3.812 4.267 5.833 5.903 6.882
</table>
<tableCaption confidence="0.998866">
Table 2: Human evaluation results
</tableCaption>
<bodyText confidence="0.998547181818182">
Table 1 gives the automatic scores for each
of participating runs and four online systems.
OS1 usually has the highest overall score (except
BLEU), it also has the highest scores for ‘regula-
tions’ (more formal texts), P1 scores are better for
the news documents.
14 assessors were recruited for evaluation (par-
ticipating team members and volunteers); the to-
tal volume of evaluation is 10,920 pairwise sen-
tence comparisons. Table 2 presents the rankings
of the participating systems using averaged ranks
from the human evaluation. There is no statisti-
cally significant difference (using Welch’s t-test at
p ≤ 0.05) in the overall ranks within the follow-
ing groups: (OS1, OS3, P1) &lt; (OS2, OS4) &lt; P5
&lt; (P2, P4). OS3 (mostly RBMT) belongs to the
troika of leaders in human evaluation contrary to
the results of its automatic scores (Table 1). Sim-
ilarly, P5 is consistently ranked higher than P2 by
the assessors, while the automatic scores suggest
the opposite. This observation confirms the well-
known fact that the automatic scores underesti-
mate RBMT systems, e.g., (Béchar et al., 2012).
To investigate applicability of the automatic
measures to the English-Russian language direc-
tion, we computed Spearman’s ρ correlation be-
tween the ranks given by the evaluators and by
the respective measures. Because of the amount
of variation for each measure on the sentence
level, robust estimates, such as the median and
the trimmed mean, are more informative than
the mean, since they discard the outliers (Huber,
1996). The results are listed in Table 3. All mea-
sures exhibit reasonable correlation on the corpus
level (330 sentences), but the sentence-level re-
sults are less impressive. While TER and GTM
are known to provide better correlation with post-
editing efforts for English (O’Brien, 2011), free
word order and greater data sparseness on the sen-
tence level makes TER much less reliable for Rus-
sian. METEOR (with its built-in Russian lemma-
tisation) and GTM offer the best correlation with
human judgements.
The lower part of Table 2 also reports the results
of simulated dynamic ranking (using the NIST
rankings as the initial order for the sort operation).
It resulted in a slightly different final ranking of
the systems since we did not account for ties and
‘averaged ranks’. However, the ranking is prac-
tically the same up to the statistically significant
rank differences in reference ranking (see above).
The advantage is that it requires a significantly
lower number of pairwise comparisons. Insertion
sort yielded 5,131 comparisons (15.5 per sentence;
56% of exhaustive comparisons for 330 sentences
and 8 systems); binary insertion sort yielded 4,327
comparisons (13.1 per sentence; 47% of exhaus-
tive comparisons).
Out of the original set of 330 sentences for
human evaluation, 60 sentences were evaluated
by two annotators (which resulted in 60*28=1680
pairwise comparisons), so we were able to calcu-
late the standard Kohen’s κ and Krippendorff’s α
scores (Artstein and Poesio, 2008). The results of
inter-annotator agreement are: percentage agree-
ment 0.56, κ = 0.34, α = 0.48, which is simi-
</bodyText>
<page confidence="0.996553">
265
</page>
<table confidence="0.999355857142857">
Metric Median Sentence level Corpus
Mean Trimmed level
BLEU 0.357 0.298 0.348 0.833
NIST 0.357 0.291 0.347 0.810
Meteor 0.429 0.348 0.393 0.714
TER 0.214 0.186 0.204 0.619
GTM 0.429 0.340 0.392 0.714
</table>
<tableCaption confidence="0.999957">
Table 3: Correlation to human judgements
</tableCaption>
<bodyText confidence="0.999827384615385">
lar to sentence ranking reported in other evaluation
campaigns (Callison-Burch et al., 2012; Callison-
Burch et al., 2011). It was interesting to see the
agreement results distinguishing the top three sys-
tems against the rest, i.e. by ignoring the assess-
ments for the pairs within each group, α = 0.53,
which indicates that the judges agree on the dif-
ference in quality between the top three systems
and the rest. On the other hand, the agreement re-
sults within the top three systems are low: κ =
0.23, α = 0.33, which is again in line with the re-
sults for similar evaluations between closely per-
forming systems (Callison-Burch et al., 2011).
</bodyText>
<sectionHeader confidence="0.993931" genericHeader="conclusions">
5 Conclusions and future plans
</sectionHeader>
<bodyText confidence="0.999967041666667">
This was the first attempt at making proper
quantitative and qualitative evaluation of the
English→Russian MT systems. In the future edi-
tions, we will be aiming at developing a new
test corpus with a wider genre palette. We
will probably complement the campaign with
Russian→English translation direction. We hope
to attract more participants, including interna-
tional ones and plan to prepare a ‘light version’
for students and young researchers. We will also
address the problem of tailoring automatic evalu-
ation measures to Russian — accounting for com-
plex morphology and free word order. To this
end we will re-use human evaluation data gath-
ered within the 2013 campaign. While the cam-
paign was based exclusively on data in one lan-
guage direction, the correlation results for auto-
matic MT quality measures should be applicable
to other languages with free word order and com-
plex morphology.
We have made the corpus comprising the source
sentences, their human translations, translations
by participating MT systems and the human eval-
uation data publicly available.8
</bodyText>
<footnote confidence="0.884844">
8http://romip.ru/mteval/
</footnote>
<sectionHeader confidence="0.987926" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999621">
We would like to thank the translators, asses-
sors, as well as Anna Tsygankova, Maxim Gubin,
and Marina Nekrestyanova for project coordina-
tion and organisational help. Research on corpus
preparation methods was supported by EU FP7
funding, contract No 251534 (HyghTra). Our spe-
cial gratitude goes to Yandex and ABBYY who
partially covered the expenses incurred on corpus
translation. We’re also grateful to the anonymous
reviewers for their useful comments.
</bodyText>
<sectionHeader confidence="0.998497" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999425025">
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.
Bogdan Babych, Anthony Hartley, Serge Sharoff, and
Olga Mudraya. 2007. Assisting translators in in-
direct lexical transfer. In Proc. of 45th ACL, pages
739–746, Prague.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 65–72, Ann Ar-
bor, Michigan, June.
Hanna Béchar, Raphaël Rubino, Yifan He, Yanjun Ma,
and Josef van Genabith. 2012. An evaluation
of statistical post-editing systems applied to RBMT
and SMT systems. In Proceedings of COLING’12,
Mumbai.
Igor Boguslavsky. 1995. A bi-directional Russian-to-
English machine translation system (ETAP-3). In
Proceedings of the Machine Translation Summit V,
Luxembourg.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar F Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In
Proceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22–64. Association for
Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montréal, Canada, June.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology, pages 138–145, San Diego, CA.
</reference>
<page confidence="0.977644">
266
</page>
<reference confidence="0.999295085106383">
Marcelo Federico, Mauro Cettolo, Luisa Bentivogli,
Michael Paul, and Sebastian Stuker. 2012.
Overview of the IWSLT 2012 evaluation campaign.
In Proceedings of the International Workshop on
Spoken Language Translation (IWSLT), pages 12–
34, Hong Kong, December.
Peter J. Huber. 1996. Robust Statistical Procedures.
Society for Industrial and Applied Mathematics.
John Hutchins, editor. 2000. Early years in ma-
chine translation: Memoirs and biographies of pi-
oneers. John Benjamins, Amsterdam, Philadel-
phia. http://www.hutchinsweb.me.uk/
EarlyYears-2000-TOC.htm.
Philippe Langlais. 2002. Improving a general-purpose
statistical translation engine by terminological lexi-
cons. In Proceedings of Second international work-
shop on computational terminology (COMPUTERM
2002), pages 1–7, Taipei, Taiwan. http://acl.
ldc.upenn.edu/W/W02/W02-1405.pdf.
Sharon O’Brien. 2011. Towards predicting
post-editing productivity. Machine translation,
25(3):197–215.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. Technical Re-
port RC22176 (W0109-022), IBM Thomas J. Wat-
son Research Center.
Serge Sharoff. 2010. In the garden and in the jun-
gle: Comparing genres in the BNC and Internet.
In Alexander Mehler, Serge Sharoff, and Marina
Santini, editors, Genres on the Web: Computa-
tional Models and Empirical Studies, pages 149–
166. Springer, Berlin/New York.
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. Fluency, adequacy, or
HTER? Exploring different human judgments with
a tunable MT metric. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
259–268, Athens, Greece, March.
Joseph Turian, Luke Shen, and I. Dan Melamed. 2003.
Evaluation of machine translation and its evaluation.
In Proceedings of Machine Translation Summit IX,
New Orleans, LA, USA, September.
John S. White, Theresa O’Connell, and Francis
O’Mara. 1994. The ARPA MT evaluation method-
ologies: Evolution, lessons, and further approaches.
In Proceedings of AMTA’94, pages 193–205.
</reference>
<page confidence="0.997292">
267
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.337971">
<title confidence="0.99805">MT evaluation campaign</title>
<author confidence="0.998104">Pavel Braslavslii Alexander Beloborodov Maxim Khalilov Serge Sharoff</author>
<affiliation confidence="0.9538895">Kontur Labs / Ural Federal University TAUS Labs University of Leeds Ural Federal Russia The Netherlands UK</affiliation>
<abstract confidence="0.899656833333333">University,Russia xander-beloborodov maxim s.sharoff pbras@yandex.ru @yandex.ru @tauslabs.com @leeds.ac.uk Abstract This paper presents the settings and the results of the ROMIP 2013 MT shared task the language direction. The quality of generated translations was assessed using automatic metrics and human evaluation. We also discuss ways to reduce human evaluation efforts using pairwise sentence comparisons by human judges to simulate sort operations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="15899" citStr="Artstein and Poesio, 2008" startWordPosition="2563" endWordPosition="2566">fferences in reference ranking (see above). The advantage is that it requires a significantly lower number of pairwise comparisons. Insertion sort yielded 5,131 comparisons (15.5 per sentence; 56% of exhaustive comparisons for 330 sentences and 8 systems); binary insertion sort yielded 4,327 comparisons (13.1 per sentence; 47% of exhaustive comparisons). Out of the original set of 330 sentences for human evaluation, 60 sentences were evaluated by two annotators (which resulted in 60*28=1680 pairwise comparisons), so we were able to calculate the standard Kohen’s κ and Krippendorff’s α scores (Artstein and Poesio, 2008). The results of inter-annotator agreement are: percentage agreement 0.56, κ = 0.34, α = 0.48, which is simi265 Metric Median Sentence level Corpus Mean Trimmed level BLEU 0.357 0.298 0.348 0.833 NIST 0.357 0.291 0.347 0.810 Meteor 0.429 0.348 0.393 0.714 TER 0.214 0.186 0.204 0.619 GTM 0.429 0.340 0.392 0.714 Table 3: Correlation to human judgements lar to sentence ranking reported in other evaluation campaigns (Callison-Burch et al., 2012; CallisonBurch et al., 2011). It was interesting to see the agreement results distinguishing the top three systems against the rest, i.e. by ignoring the a</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bogdan Babych</author>
<author>Anthony Hartley</author>
<author>Serge Sharoff</author>
<author>Olga Mudraya</author>
</authors>
<title>Assisting translators in indirect lexical transfer.</title>
<date>2007</date>
<booktitle>In Proc. of 45th ACL,</booktitle>
<pages>739--746</pages>
<location>Prague.</location>
<contexts>
<context position="2952" citStr="Babych et al., 2007" startWordPosition="456" endWordPosition="459">problems for both RBMT and SMT systems (especially for phrasebased ones). Complex morphology also leads to considerable sparseness for word alignment in SMT. The language direction was chosen to be English→Russian, first because of the availability of native speakers for evaluation, second because the systems taking part in this evaluation are mostly used in translation of English texts for the Russian readers. 2 Corpus preparation In designing the set of texts for evaluation, we had two issues in mind. First, it is known that the domain and genre can influence MT performance (Langlais, 2002; Babych et al., 2007), so we wanted to control the set of genres. Second, we were aiming at using sources allowing distribution of texts under a Creative Commons licence. In the end two genres were used coming from two sources. The newswire texts were collected from the English Wikinews website.4 The second genre was represented by ‘regulations’ (laws, contracts, rules, etc), which were collected from the Web using a genre classification method described in (Sharoff, 2010). The method provided a sufficient accuracy (74%) for the initial selection of texts under the category of ‘regulations,’ which was followed by </context>
</contexts>
<marker>Babych, Hartley, Sharoff, Mudraya, 2007</marker>
<rawString>Bogdan Babych, Anthony Hartley, Serge Sharoff, and Olga Mudraya. 2007. Assisting translators in indirect lexical transfer. In Proc. of 45th ACL, pages 739–746, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="10789" citStr="Banerjee and Lavie, 2005" startWordPosition="1738" endWordPosition="1741">le in human judgement they are equal, the final ranking will preserve the initial order. Insertion sort of n sentences requires n − 1 comparisons in the best case of already sorted data and n(n−1) 2 in the worst case (reversely ordered data). Insertion sort with binary search requires ∼ n log n comparisons regardless of the initial order. For this study we ran exhaustive pairwise evaluation and used its results to simulate human-assisted sorting. In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al., 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2009), and GTM (Turian et al., 2003). We also wanted to estimate the correlations of these metrics with human judgements for the English→Russian pair on the corpus level and on the level of individual sentences. 4 Results We received results from five teams, two teams submitted two runs each, which totals seven participants’ runs (referred to as P1..P7 in the paper). The participants represent SMT, RBMT, and hybrid approaches. They included established groups from academia and industry, as well as new research teams. The evaluation runs also included the translations of t</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72, Ann Arbor, Michigan, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hanna Béchar</author>
<author>Raphaël Rubino</author>
<author>Yifan He</author>
<author>Yanjun Ma</author>
<author>Josef van Genabith</author>
</authors>
<title>An evaluation of statistical post-editing systems applied to RBMT and SMT systems.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING’12,</booktitle>
<location>Mumbai.</location>
<marker>Béchar, Rubino, He, Ma, van Genabith, 2012</marker>
<rawString>Hanna Béchar, Raphaël Rubino, Yifan He, Yanjun Ma, and Josef van Genabith. 2012. An evaluation of statistical post-editing systems applied to RBMT and SMT systems. In Proceedings of COLING’12, Mumbai.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Boguslavsky</author>
</authors>
<title>A bi-directional Russian-toEnglish machine translation system (ETAP-3).</title>
<date>1995</date>
<booktitle>In Proceedings of the Machine Translation Summit V,</booktitle>
<contexts>
<context position="12001" citStr="Boguslavsky, 1995" startWordPosition="1926" endWordPosition="1927">s of the 947 test sentences produced by four free online systems in their default modes (referred to as OS1..OS4). For 11 runs automatic evaluation measures were calculated; eight runs underwent manual evaluation (four online systems plus four participants’ runs; no manual evaluation was done by agreement with the participants for the runs P3, P6, and P7 to reduce the workload). ID Name and information Phrase-based SMT Phrase-based SMT Hybrid (RBMT+statistical PE) Dependency-based SMT Compreno, Hybrid, ABBYY Corp Pharaon, Moses, Yandex&amp;TAUS data Balagur, Moses, Yandex&amp;news data ETAP-3, RBMT, (Boguslavsky, 1995) Pereved, Moses, Internet data OS3 is a hybrid system based on RBMT with SMT post-editing (PE). P1 is a hybrid system with analysis and generation driven by statistical evaluation of hypotheses. OS1 OS2 OS3 OS4 P1 P2 P3,4 P5 P6,7 264 All (330 sentences) OS3 (highest) P1 OS1 OS2 OS4 P5 P2 P4 (lowest) 3.159 3.350 3.530 3.961 4.082 5.447 5.998 6.473 News (190 sentences) OS3 (highest) P1 OS1 OS2 OS4 P5 P2 P4 (lowest) 2.947 3.450 3.482 4.084 4.242 5.474 5,968 6,353 Regulations (140 sentences) P1 (highest) OS3 OS1 OS2 OS4 P5 P2 P4 (lowest) 3.214 3.446 3.596 3.793 3.864 5.411 6.039 6.636 Simulated dy</context>
</contexts>
<marker>Boguslavsky, 1995</marker>
<rawString>Igor Boguslavsky. 1995. A bi-directional Russian-toEnglish machine translation system (ETAP-3). In Proceedings of the Machine Translation Summit V, Luxembourg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar F Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="16891" citStr="Callison-Burch et al., 2011" startWordPosition="2734" endWordPosition="2737">ence ranking reported in other evaluation campaigns (Callison-Burch et al., 2012; CallisonBurch et al., 2011). It was interesting to see the agreement results distinguishing the top three systems against the rest, i.e. by ignoring the assessments for the pairs within each group, α = 0.53, which indicates that the judges agree on the difference in quality between the top three systems and the rest. On the other hand, the agreement results within the top three systems are low: κ = 0.23, α = 0.33, which is again in line with the results for similar evaluations between closely performing systems (Callison-Burch et al., 2011). 5 Conclusions and future plans This was the first attempt at making proper quantitative and qualitative evaluation of the English→Russian MT systems. In the future editions, we will be aiming at developing a new test corpus with a wider genre palette. We will probably complement the campaign with Russian→English translation direction. We hope to attract more participants, including international ones and plan to prepare a ‘light version’ for students and young researchers. We will also address the problem of tailoring automatic evaluation measures to Russian — accounting for complex morpholo</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar F Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<location>Montréal, Canada,</location>
<contexts>
<context position="1798" citStr="Callison-Burch et al., 2012" startWordPosition="272" endWordPosition="275">ical MT (WMT) in 2013 has announced a Russian evaluation track for the first time.1 However, this evaluation is currently ongoing, it should include new methods for building statistical MT (SMT) systems for Russian from the data provided in this track, but it will not cover the performance of existing systems, especially rule-based (RBMT) or hybrid ones. Evaluation campaigns play an important role in promotion of the progress for MT technologies. Recently, there have been a number of MT shared tasks for combinations of several European, Asian and Semitic languages (CallisonBurch et al., 2011; Callison-Burch et al., 2012; Federico et al., 2012), which we took into account in designing the campaign for the English-Russian direction. The evaluation has been held in the 1http://www.statmt.org/wmt13/ context of ROMIP,2 which stands for Russian Information Retrieval Evaluation Seminar and is a TREC-like3 Russian initiative started in 2002. One of the main challenges in developing MT systems for Russian and for evaluating them is the need to deal with its free word order and complex morphology. Long-distance dependencies are common, and this creates problems for both RBMT and SMT systems (especially for phrasebased</context>
<context position="16343" citStr="Callison-Burch et al., 2012" startWordPosition="2635" endWordPosition="2638">ated by two annotators (which resulted in 60*28=1680 pairwise comparisons), so we were able to calculate the standard Kohen’s κ and Krippendorff’s α scores (Artstein and Poesio, 2008). The results of inter-annotator agreement are: percentage agreement 0.56, κ = 0.34, α = 0.48, which is simi265 Metric Median Sentence level Corpus Mean Trimmed level BLEU 0.357 0.298 0.348 0.833 NIST 0.357 0.291 0.347 0.810 Meteor 0.429 0.348 0.393 0.714 TER 0.214 0.186 0.204 0.619 GTM 0.429 0.340 0.392 0.714 Table 3: Correlation to human judgements lar to sentence ranking reported in other evaluation campaigns (Callison-Burch et al., 2012; CallisonBurch et al., 2011). It was interesting to see the agreement results distinguishing the top three systems against the rest, i.e. by ignoring the assessments for the pairs within each group, α = 0.53, which indicates that the judges agree on the difference in quality between the top three systems and the rest. On the other hand, the agreement results within the top three systems are low: κ = 0.23, α = 0.33, which is again in line with the results for similar evaluations between closely performing systems (Callison-Burch et al., 2011). 5 Conclusions and future plans This was the first </context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montréal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the second international conference on Human Language Technology,</booktitle>
<pages>138--145</pages>
<location>San Diego, CA.</location>
<contexts>
<context position="10754" citStr="Doddington, 2002" startWordPosition="1735" endWordPosition="1736">system against another, while in human judgement they are equal, the final ranking will preserve the initial order. Insertion sort of n sentences requires n − 1 comparisons in the best case of already sorted data and n(n−1) 2 in the worst case (reversely ordered data). Insertion sort with binary search requires ∼ n log n comparisons regardless of the initial order. For this study we ran exhaustive pairwise evaluation and used its results to simulate human-assisted sorting. In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al., 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2009), and GTM (Turian et al., 2003). We also wanted to estimate the correlations of these metrics with human judgements for the English→Russian pair on the corpus level and on the level of individual sentences. 4 Results We received results from five teams, two teams submitted two runs each, which totals seven participants’ runs (referred to as P1..P7 in the paper). The participants represent SMT, RBMT, and hybrid approaches. They included established groups from academia and industry, as well as new research teams. The evaluation runs </context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the second international conference on Human Language Technology, pages 138–145, San Diego, CA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marcelo Federico</author>
</authors>
<institution>Mauro Cettolo, Luisa Bentivogli,</institution>
<marker>Federico, </marker>
<rawString>Marcelo Federico, Mauro Cettolo, Luisa Bentivogli,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Paul</author>
<author>Sebastian Stuker</author>
</authors>
<title>Overview of the IWSLT 2012 evaluation campaign.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation (IWSLT),</booktitle>
<pages>12--34</pages>
<location>Hong Kong,</location>
<marker>Paul, Stuker, 2012</marker>
<rawString>Michael Paul, and Sebastian Stuker. 2012. Overview of the IWSLT 2012 evaluation campaign. In Proceedings of the International Workshop on Spoken Language Translation (IWSLT), pages 12– 34, Hong Kong, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter J Huber</author>
</authors>
<title>Robust Statistical Procedures.</title>
<date>1996</date>
<journal>Society for Industrial and Applied Mathematics.</journal>
<contexts>
<context position="14416" citStr="Huber, 1996" startWordPosition="2329" endWordPosition="2330">assessors, while the automatic scores suggest the opposite. This observation confirms the wellknown fact that the automatic scores underestimate RBMT systems, e.g., (Béchar et al., 2012). To investigate applicability of the automatic measures to the English-Russian language direction, we computed Spearman’s ρ correlation between the ranks given by the evaluators and by the respective measures. Because of the amount of variation for each measure on the sentence level, robust estimates, such as the median and the trimmed mean, are more informative than the mean, since they discard the outliers (Huber, 1996). The results are listed in Table 3. All measures exhibit reasonable correlation on the corpus level (330 sentences), but the sentence-level results are less impressive. While TER and GTM are known to provide better correlation with postediting efforts for English (O’Brien, 2011), free word order and greater data sparseness on the sentence level makes TER much less reliable for Russian. METEOR (with its built-in Russian lemmatisation) and GTM offer the best correlation with human judgements. The lower part of Table 2 also reports the results of simulated dynamic ranking (using the NIST ranking</context>
</contexts>
<marker>Huber, 1996</marker>
<rawString>Peter J. Huber. 1996. Robust Statistical Procedures. Society for Industrial and Applied Mathematics.</rawString>
</citation>
<citation valid="true">
<title>Early years in machine translation: Memoirs and biographies of pioneers. John Benjamins,</title>
<date>2000</date>
<editor>John Hutchins, editor.</editor>
<location>Amsterdam, Philadelphia.</location>
<note>http://www.hutchinsweb.me.uk/ EarlyYears-2000-TOC.htm.</note>
<marker>2000</marker>
<rawString>John Hutchins, editor. 2000. Early years in machine translation: Memoirs and biographies of pioneers. John Benjamins, Amsterdam, Philadelphia. http://www.hutchinsweb.me.uk/ EarlyYears-2000-TOC.htm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philippe Langlais</author>
</authors>
<title>Improving a general-purpose statistical translation engine by terminological lexicons.</title>
<date>2002</date>
<booktitle>In Proceedings of Second international workshop on computational terminology (COMPUTERM</booktitle>
<pages>1--7</pages>
<location>Taipei, Taiwan. http://acl.</location>
<contexts>
<context position="2930" citStr="Langlais, 2002" startWordPosition="454" endWordPosition="455">nd this creates problems for both RBMT and SMT systems (especially for phrasebased ones). Complex morphology also leads to considerable sparseness for word alignment in SMT. The language direction was chosen to be English→Russian, first because of the availability of native speakers for evaluation, second because the systems taking part in this evaluation are mostly used in translation of English texts for the Russian readers. 2 Corpus preparation In designing the set of texts for evaluation, we had two issues in mind. First, it is known that the domain and genre can influence MT performance (Langlais, 2002; Babych et al., 2007), so we wanted to control the set of genres. Second, we were aiming at using sources allowing distribution of texts under a Creative Commons licence. In the end two genres were used coming from two sources. The newswire texts were collected from the English Wikinews website.4 The second genre was represented by ‘regulations’ (laws, contracts, rules, etc), which were collected from the Web using a genre classification method described in (Sharoff, 2010). The method provided a sufficient accuracy (74%) for the initial selection of texts under the category of ‘regulations,’ </context>
</contexts>
<marker>Langlais, 2002</marker>
<rawString>Philippe Langlais. 2002. Improving a general-purpose statistical translation engine by terminological lexicons. In Proceedings of Second international workshop on computational terminology (COMPUTERM 2002), pages 1–7, Taipei, Taiwan. http://acl. ldc.upenn.edu/W/W02/W02-1405.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon O’Brien</author>
</authors>
<title>Towards predicting post-editing productivity.</title>
<date>2011</date>
<booktitle>Machine translation,</booktitle>
<pages>25--3</pages>
<marker>O’Brien, 2011</marker>
<rawString>Sharon O’Brien. 2011. Towards predicting post-editing productivity. Machine translation, 25(3):197–215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2001</date>
<journal>IBM Thomas J. Watson Research Center.</journal>
<tech>Technical Report RC22176 (W0109-022),</tech>
<contexts>
<context position="10729" citStr="Papineni et al., 2001" startWordPosition="1730" endWordPosition="1733">or example, if it favours one system against another, while in human judgement they are equal, the final ranking will preserve the initial order. Insertion sort of n sentences requires n − 1 comparisons in the best case of already sorted data and n(n−1) 2 in the worst case (reversely ordered data). Insertion sort with binary search requires ∼ n log n comparisons regardless of the initial order. For this study we ran exhaustive pairwise evaluation and used its results to simulate human-assisted sorting. In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al., 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2009), and GTM (Turian et al., 2003). We also wanted to estimate the correlations of these metrics with human judgements for the English→Russian pair on the corpus level and on the level of individual sentences. 4 Results We received results from five teams, two teams submitted two runs each, which totals seven participants’ runs (referred to as P1..P7 in the paper). The participants represent SMT, RBMT, and hybrid approaches. They included established groups from academia and industry, as well as new research te</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. BLEU: a method for automatic evaluation of machine translation. Technical Report RC22176 (W0109-022), IBM Thomas J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Serge Sharoff</author>
</authors>
<title>In the garden and in the jungle: Comparing genres in the BNC and Internet.</title>
<date>2010</date>
<booktitle>Genres on the Web: Computational Models and Empirical Studies,</booktitle>
<pages>149--166</pages>
<editor>In Alexander Mehler, Serge Sharoff, and Marina Santini, editors,</editor>
<publisher>Springer,</publisher>
<location>Berlin/New York.</location>
<contexts>
<context position="3408" citStr="Sharoff, 2010" startWordPosition="531" endWordPosition="532">xts for evaluation, we had two issues in mind. First, it is known that the domain and genre can influence MT performance (Langlais, 2002; Babych et al., 2007), so we wanted to control the set of genres. Second, we were aiming at using sources allowing distribution of texts under a Creative Commons licence. In the end two genres were used coming from two sources. The newswire texts were collected from the English Wikinews website.4 The second genre was represented by ‘regulations’ (laws, contracts, rules, etc), which were collected from the Web using a genre classification method described in (Sharoff, 2010). The method provided a sufficient accuracy (74%) for the initial selection of texts under the category of ‘regulations,’ which was followed by a manual check to reject texts clearly outside of this genre category. 2http://romip.ru/en/ 3http://trec.nist.gov/ 4http://en.wikinews.org/ 262 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 262–267, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics The initial corpus consists of 8,356 original English texts that make up 148,864 sentences. We chose to retain the entire tex</context>
</contexts>
<marker>Sharoff, 2010</marker>
<rawString>Serge Sharoff. 2010. In the garden and in the jungle: Comparing genres in the BNC and Internet. In Alexander Mehler, Serge Sharoff, and Marina Santini, editors, Genres on the Web: Computational Models and Empirical Studies, pages 149– 166. Springer, Berlin/New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>259--268</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="10816" citStr="Snover et al., 2009" startWordPosition="1743" endWordPosition="1746">qual, the final ranking will preserve the initial order. Insertion sort of n sentences requires n − 1 comparisons in the best case of already sorted data and n(n−1) 2 in the worst case (reversely ordered data). Insertion sort with binary search requires ∼ n log n comparisons regardless of the initial order. For this study we ran exhaustive pairwise evaluation and used its results to simulate human-assisted sorting. In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al., 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2009), and GTM (Turian et al., 2003). We also wanted to estimate the correlations of these metrics with human judgements for the English→Russian pair on the corpus level and on the level of individual sentences. 4 Results We received results from five teams, two teams submitted two runs each, which totals seven participants’ runs (referred to as P1..P7 in the paper). The participants represent SMT, RBMT, and hybrid approaches. They included established groups from academia and industry, as well as new research teams. The evaluation runs also included the translations of the 947 test sentences produ</context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or HTER? Exploring different human judgments with a tunable MT metric. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 259–268, Athens, Greece, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Luke Shen</author>
<author>I Dan Melamed</author>
</authors>
<title>Evaluation of machine translation and its evaluation.</title>
<date>2003</date>
<booktitle>In Proceedings of Machine Translation Summit IX,</booktitle>
<location>New Orleans, LA, USA,</location>
<contexts>
<context position="10847" citStr="Turian et al., 2003" startWordPosition="1749" endWordPosition="1752">eserve the initial order. Insertion sort of n sentences requires n − 1 comparisons in the best case of already sorted data and n(n−1) 2 in the worst case (reversely ordered data). Insertion sort with binary search requires ∼ n log n comparisons regardless of the initial order. For this study we ran exhaustive pairwise evaluation and used its results to simulate human-assisted sorting. In addition to human evaluation, we also ran system-level automatic evaluations using BLEU (Papineni et al., 2001), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2009), and GTM (Turian et al., 2003). We also wanted to estimate the correlations of these metrics with human judgements for the English→Russian pair on the corpus level and on the level of individual sentences. 4 Results We received results from five teams, two teams submitted two runs each, which totals seven participants’ runs (referred to as P1..P7 in the paper). The participants represent SMT, RBMT, and hybrid approaches. They included established groups from academia and industry, as well as new research teams. The evaluation runs also included the translations of the 947 test sentences produced by four free online systems</context>
</contexts>
<marker>Turian, Shen, Melamed, 2003</marker>
<rawString>Joseph Turian, Luke Shen, and I. Dan Melamed. 2003. Evaluation of machine translation and its evaluation. In Proceedings of Machine Translation Summit IX, New Orleans, LA, USA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John S White</author>
<author>Theresa O’Connell</author>
<author>Francis O’Mara</author>
</authors>
<title>The ARPA MT evaluation methodologies: Evolution, lessons, and further approaches.</title>
<date>1994</date>
<booktitle>In Proceedings of AMTA’94,</booktitle>
<pages>193--205</pages>
<marker>White, O’Connell, O’Mara, 1994</marker>
<rawString>John S. White, Theresa O’Connell, and Francis O’Mara. 1994. The ARPA MT evaluation methodologies: Evolution, lessons, and further approaches. In Proceedings of AMTA’94, pages 193–205.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>