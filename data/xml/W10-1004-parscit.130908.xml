<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023399">
<title confidence="0.963152">
Annotating ESL Errors: Challenges and Rewards
</title>
<author confidence="0.995244">
Alla Rozovskaya and Dan Roth
</author>
<affiliation confidence="0.997892">
University of Illinois at Urbana-Champaign
</affiliation>
<address confidence="0.813316">
Urbana, IL 61801
</address>
<email confidence="0.999082">
{rozovska,danr}@illinois.edu
</email>
<sectionHeader confidence="0.995644" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99963835">
In this paper, we present a corrected and error-
tagged corpus of essays written by non-native
speakers of English. The corpus contains
63000 words and includes data by learners of
English of nine first language backgrounds.
The annotation was performed at the sentence
level and involved correcting all errors in the
sentence. Error classification includes mis-
takes in preposition and article usage, errors
in grammar, word order, and word choice. We
show an analysis of errors in the annotated
corpus by error categories and first language
backgrounds, as well as inter-annotator agree-
ment on the task.
We also describe a computer program that was
developed to facilitate and standardize the an-
notation procedure for the task. The program
allows for the annotation of various types of
mistakes and was used in the annotation of the
corpus.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999588930232558">
Work on automated methods for detecting and cor-
recting context dependent mistakes (e.g., (Golding
and Roth, 1996; Golding and Roth, 1999; Carlson
et al., 2001)) has taken an interesting turn over the
last few years, and has focused on correcting mis-
takes made by non-native speakers of English. Non-
native writers make a variety of errors in grammar
and word usage. Recently, there has been a lot of
effort on building systems for detecting mistakes in
article and preposition usage (DeFelice, 2008; Eeg-
Olofsson, 2003; Gamon et al., 2008; Han et al.,
2006; Tetreault and Chodorow, 2008b). Izumi et al.
(2003) consider several error types, including article
and preposition mistakes, made by Japanese learn-
ers of English, and Nagata et al. (2006) focus on the
errors in mass/count noun distinctions with an ap-
plication to detecting article mistakes also made by
Japanese speakers. Article and preposition mistakes
have been shown to be very common mistakes for
learners of different first language (L1) backgrounds
(Dagneaux et al., 1998; Gamon et al., 2008; Izumi
et al., 2004; Tetreault and Chodorow, 2008a), but
there is no systematic study of a whole range of er-
rors non-native writers produce, nor is it clear what
the distribution of different types of mistakes is in
learner language.
In this paper, we describe a corpus of sentences
written by English as a Second Language (ESL)
speakers, annotated for the purposes of developing
an automated system for correcting mistakes in text.
Although the focus of the annotation were errors
in article and preposition usage, all mistakes in the
sentence have been corrected. The data for anno-
tation were taken from two sources: The Interna-
tional Corpus of Learner English (ICLE, (Granger
et al., 2002a)) and Chinese Learners of English Cor-
pus (CLEC, (Gui and Yang, 2003)). The annotated
corpus includes data from speakers of nine first lan-
guage backgrounds. To our knowledge, this is the
first corpus of non-native English text (learner cor-
pus) of fully-corrected sentences from such a diverse
group of learners1. The size of the annotated corpus
is 63000 words, or 2645 sentences. While a corpus
</bodyText>
<footnote confidence="0.8898295">
1Possibly, except for the Cambridge Learner Corpus
http://www.cambridge.org/elt
</footnote>
<page confidence="0.973667">
28
</page>
<note confidence="0.990563">
Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 28–36,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999953955555555">
of this size may not seem significant in many natu-
ral language applications, this is in fact a large cor-
pus for this field, especially considering the effort to
correct all mistakes, as opposed to focusing on one
language phenomenon. This corpus was used in the
experiments described in the companion paper (Ro-
zovskaya and Roth, 2010).
The annotation schema that we developed was
motivated by our special interest in errors in arti-
cle and preposition usage, but also includes errors
in verbs, morphology, and noun number. The cor-
pus contains 907 article corrections and 1309 prepo-
sition corrections, in addition to annotated mistakes
of other types.
While the focus of the present paper is on anno-
tating ESL mistakes, we have several goals in mind.
First, we present the annotation procedure for the
task, including an error classification schema, anno-
tation speed, and inter-annotator agreement. Sec-
ond, we describe a computer program that we de-
veloped to facilitate the annotation of mistakes in
text. Third, having such a diverse corpus allows
us to analyze the annotated data with respect to the
source language of the learner. We show the anal-
ysis of the annotated data through an overall break-
down of error types by the writer’s first language.
We also present a detailed analysis of errors in arti-
cle and preposition usage. Finally, it should be noted
that there are currently very few annotated learner
corpora available. Consequently, systems are eval-
uated on different data sets, which makes perfor-
mance comparison impossible. The annotation of
the data presented here is available2 and, thus, can
be used by researchers who obtain access to these
respective corpora3.
The rest of the paper is organized as follows.
First, we describe previous work on the annotation
of learner corpora and statistics on ESL mistakes.
Section 3 gives a description of the annotation pro-
cedure, Section 4 presents the annotation tool that
was developed for the purpose of this project and
used in the annotation. We then present error statis-
tics based on the annotated corpus across all error
types and separately for errors in article and preposi-
tion usage. Finally, in Section 6 we describe how we
</bodyText>
<footnote confidence="0.997585">
2Details about the annotation are accessible from
http://L2R.cs.uiuc.edu/˜cogcomp/
3The ICLE and CLEC corpora are commercially available.
</footnote>
<bodyText confidence="0.986162434782609">
evaluate inter-annotator agreement and show agree-
ment results for the task.
2 Learner Corpora and Error Tagging
In this section, we review research in the annota-
tion and error analysis of learner corpora. For a
review of learner corpus research see, for exam-
ple, (Diaz-Negrillo, 2006; Granger, 2002b; Pravec,
2002). Comparative error analysis is difficult, as
there are no standardized error-tagging schemas, but
we can get a general idea about the types of errors
prevalent with such speakers. Izumi et al. (2004a)
describe a speech corpus of Japanese learners of En-
glish (NICT JLE). The corpus is corrected and anno-
tated and consists of the transcripts (2 million words)
of the audio-recordings of the English oral profi-
ciency interview test. In the NICT corpus, whose
error tag set consists of 45 tags, about 26.6% of er-
rors are determiner related, and 10% are preposition
related, which makes these two error types the most
common in the corpus (Gamon et al., 2008). The
Chinese Learners of English corpus (CLEC, (Gui
and Yang, 2003)) is a collection of essays written
by Chinese learners of beginning, intermediate, and
advanced levels. This corpus is also corrected and
error-tagged, but the tagging schema does not allow
for an easy isolation of article and preposition errors.
The International Corpus of Learner English (ICLE,
(Granger et al., 2002a)) is a corpus of argumenta-
tive essays by advanced English learners. The cor-
pus contains 2 million words of writing by European
learners from 14 mother tongue backgrounds. While
the entire corpus is not error-tagged, the French sub-
part of the corpus along with other data by French
speakers of a lower level of proficiency has been an-
notated (Dagneaux et al., 1998). The most com-
mon errors for the advanced level of proficiency
were found to be lexical errors (words) (15%), regis-
ter (10%), articles (10%), pronouns (10%), spelling
(8%) , verbs (8%).
In a study of 53 post-intermediate ESOL (mi-
grant) learners in New Zealand (Bitchener et al.,
2005), the most common errors were found to be
prepositions (29%), articles (20%), and verb tense
(22%). Dalgish (1985) conducted a study of er-
rors produced by ESL students enrolled at CUNY.
It was found that across students of different first
</bodyText>
<page confidence="0.99759">
29
</page>
<bodyText confidence="0.99997825">
languages, the most common error types among
24 different error types were errors in article us-
age (28%), vocabulary error (20-25%) (word choice
and idioms), prepositions (18%), and verb-subject
agreement (15%). He also noted that the speakers of
languages without article system made considerably
more article errors, but the breakdown of other error
types across languages was surprisingly similar.
</bodyText>
<sectionHeader confidence="0.984857" genericHeader="introduction">
3 Annotation
</sectionHeader>
<subsectionHeader confidence="0.998822">
3.1 Data Selection
</subsectionHeader>
<bodyText confidence="0.999941791666667">
Data for annotation were extracted from the ICLE
corpus (Granger et al., 2002a) and CLEC (Gui and
Yang, 2003). As stated in Section 2, the ICLE con-
tains data by European speakers of advanced level
of proficiency, and the CLEC corpus contains es-
says by Chinese learners of different levels of pro-
ficiency. The annotated corpus includes sentences
written by speakers of nine languages: Bulgarian,
Chinese, Czech, French, German, Italian, Polish,
Russian, and Spanish. About half of the sentences
for annotation were selected based on their scores
with respect to a 4-gram language model built using
the English Gigaword corpus (LDC2005T12). This
was done in order to exclude sentences that would
require heavy editing and sentences with near-native
fluency, sentences with scores too high or too low.
Such sentences would be less likely to benefit from
a system on preposition/article correction. The sen-
tences for annotation were a random sample out of
the remaining 80% of the data.
To collect more data for errors in preposition us-
age, we also manually selected sentences that con-
tained such errors. This might explain why the pro-
portion of preposition errors is so high in our data.
</bodyText>
<subsectionHeader confidence="0.999758">
3.2 Annotation Procedure
</subsectionHeader>
<bodyText confidence="0.999981142857143">
The annotation was performed by three native
speakers of North American English, one under-
graduate and two graduate students, specializing in
foreign languages and Linguistics, with previous ex-
perience in natural language annotation. A sentence
was presented to the annotator in the context of the
essay from which it was extracted. Essay context
can become necessary, especially for the correction
of article errors, when an article is acceptable in the
context of a sentence, but is incorrect in the context
of the essay. The annotators were also encouraged
to propose more than one correction, as long as all
of their suggestions were consistent with the essay
context.
</bodyText>
<subsectionHeader confidence="0.999778">
3.3 Annotation Schema
</subsectionHeader>
<bodyText confidence="0.999849708333333">
While we were primarily interested in article and
preposition errors, the goal of the annotation was to
correct all mistakes in the sentence. Thus, our er-
ror classification schema4, though motivated by our
interest in errors in article and preposition usage,
was also intended to give us a general idea about
the types of mistakes ESL students make. A better
understanding of the nature of learners’ mistakes is
important for the development of a robust automated
system that detects errors and proposes corrections.
Even when the focus of a correction system is on
one language phenomenon, we would like to have
information about all mistakes in the context: Error
information around the target article or preposition
could help us understand how noisy data affect the
performance.
But more importantly, a learner corpus with er-
ror information could demonstrate how mistakes in-
teract in a sentence. A common approach to de-
tecting and correcting context-sensitive mistakes is
to deal with each phenomenon independently, but
sometimes errors cannot be corrected in isolation.
Consider, for example, the following sentences that
are a part of the corpus that we annotated.
</bodyText>
<listItem confidence="0.999714666666666">
1. ”I should know all important aspects of English.” —4 ”I should
know all of the important aspects of English.”
2. ”But some of the people thought about him as a parodist of a
rhythm-n-blues singer.” —4 ”But some people considered him to
be a parodist of a rhythm-n-blues singer.”
3. ”...to be a competent avionics engineer...” —4 ...”to become com-
petent avionics engineers...”
4. ”...which reflect a traditional female role and a traditional attitude
to a woman...” —4 ”...which reflect a traditional female role and
a traditional attitude towards women...”
5. ”Marx lived in the epoch when there were no entertainments.”
—4 ”Marx lived in an era when there was no entertainment.”
</listItem>
<bodyText confidence="0.999760333333333">
In the examples above, errors interact with one an-
other. In example 1, the context requires a definite
article, and the definite article, in turn, calls for the
</bodyText>
<footnote confidence="0.970950666666667">
4Our error classification was inspired by the classification
developed for the annotation of preposition errors (Tetreault and
Chodorow, 2008a).
</footnote>
<page confidence="0.997983">
30
</page>
<bodyText confidence="0.999991291666667">
preposition ”of”. In example 2, the definite article
after ”some of” is used extraneously, and deleting it
also requires deleting preposition ”of”. Another case
of interaction is caused by a word choice error: The
writer used the verb ”thought” instead of ”consid-
ered”; replacing the verb requires also changing the
syntactic construction of the verb complement. In
examples 3 and 4, the article choice before the words
”engineer” and ”woman” depends on the number
value of those nouns. To correctly determine which
article should be used, one needs to determine first
whether the context requires a singular noun ”engi-
neer” or plural ”engineers”. Finally, in example 5,
the form of the predicate in the relative clause de-
pends on the number value of the noun ”entertain-
ment”.
For the reasons mentioned above, the annotation
involved correcting all mistakes in a sentence. The
errors that we distinguish are noun number, spelling,
verb form, and word form, in addition to article and
preposition errors . All other corrections, the major-
ity of which are lexical errors, were marked as word
replacement, word deletion, and word insertion. Ta-
ble 1 gives a description of each error type.
</bodyText>
<sectionHeader confidence="0.986668" genericHeader="method">
4 Annotation Tool
</sectionHeader>
<bodyText confidence="0.999987346153846">
In this section, we describe a computer program that
was developed to facilitate the annotation process.
The main purpose of the program is to allow an an-
notator to easily mark the type of mistake, when cor-
recting it. In addition, the tool allows us to provide
the annotator with sufficient essay context. As de-
scribed in Section 3, sentences for annotation came
from different essays, so each new sentence was usu-
ally extracted from a new context. To ensure that
the annotators preserved the meaning of the sentence
being corrected, we needed to provide them with the
essay context. A wider context could affect the an-
notator’s decision, especially when determining the
correct article choice. The tool allowed us to effi-
ciently present to the annotator the essay context for
each target sentence.
Fig. 1 shows the program interface. The sentence
for annotation appears in the white text box and the
annotator can type corrections in the box, as if work-
ing in a word processor environment. Above and be-
low the text box we can see the context boxes, where
the rest of the essay is shown. Below the lower con-
text box, there is a list of buttons. The pink buttons
and the dark green buttons correspond to different
error types, the pink buttons are for correcting arti-
cle and preposition errors, and the dark green but-
tons – for correcting other errors. The annotator can
indicate the type of mistake being corrected by plac-
ing the cursor after the word that contains an error
and pressing the button that corresponds to this er-
ror type. Pressing on an error button inserts a pair of
delimiters after the word. The correction can then be
entered between the delimiters. The yellow buttons
and the three buttons next to the pink ones are the
shortcuts that can be used instead of typing in arti-
cles and common preposition corrections. The but-
ton None located next to the article buttons is used
for correcting cases of articles and prepositions used
superfluously. To correct other errors, the annotator
needs to determine the type of error, insert the corre-
sponding delimiters after the word by pressing one
of the error buttons and enter the correction between
the delimiters.
The annotation rate for the three annotators varied
between 30 and 40 sentences per hour.
Table 2 shows sample sentences annotated with
the tool. The proposed corrections are located inside
the delimiters and follow the word to which the cor-
rection refers. When replacing a sequence of words,
the sequence was surrounded with curly braces. This
is useful if a sequence is a multi-word expression,
such as at last.
</bodyText>
<sectionHeader confidence="0.967525" genericHeader="method">
5 Annotation Statistics
</sectionHeader>
<bodyText confidence="0.999907428571429">
In this section, we present the results of the anno-
tation by error type and the source language of the
writer.
Table 3 shows statistics for the annotated sen-
tences by language group and error type. Because
the sub-corpora differ in size, we show the number
of errors per hundred words. In total, the annotated
corpus contains 63000 words or 2645 sentences of
learner writing. Category punctuation was not spec-
ified in the annotation, but can be easily identified
and includes insertion, deletion, and replacement of
punctuation marks. The largest error category is
word replacement, which combines deleted, inserted
words and word substitutions. This is followed by
</bodyText>
<page confidence="0.99858">
31
</page>
<table confidence="0.999792307692308">
Error type Description Examples
Article error Any error involving an article ”Women were indignant at [None/the] inequality
from men.”
Preposition error Any error involving a preposition ”...to change their views [to/for] the better.”
Noun number Errors involving plural/singular ”Science is surviving by overcoming the mistakes not
confusion of a noun by uttering the [truths/truth] .”
Verb form Errors in verb tense and verb inflec- ”He [write/writes] poetry.”
tions
Word form Correct lexeme, but wrong suffix ”It is not [simply/simple] to make professional army
.”
Spelling Error in spelling ”...if a person [commited/committed] a crime...”
Word insertion, deletion, Other corrections that do not fall ”There is a [probability/possibility] that today’s fan-
or replacement into any of the above categories tasies will not be fantasies tomorrow.”
</table>
<tableCaption confidence="0.999725">
Table 1: Error classification used in annotation
</tableCaption>
<figureCaption confidence="0.99529225">
Figure 1: Example of a sentence for annotation as it appears in the annotation tool window. The target sentence is
shown in the white box. The surrounding essay context is shown in the brown boxes. The buttons appear below the
boxes with text: pink buttons (for marking article and preposition errors), dark green (for marking other errors), light
green (article buttons) and yellow (preposition buttons).
</figureCaption>
<listItem confidence="0.9315925">
Annotated sentence Corrected errors
1. Television becomes their life , and in many cases it replaces their real life /lives/ noun number (life → lives)
2. Here I ca n’t $help$ but mention that all these people were either bankers or the word insertion (help); word replacement (kind → kind,
Heads of companies or something of that kind @nature, kind@. nature)
3. We exterminated *have exterminated* different kinds of animals verb form (exterminated → have exterminated)
4.... nearly 30000 species of plants are under the &lt;a&gt; serious threat of disappear- article replacement (the → a); word form (disappear-
ance |disappearing |ance → disappearing)
5. There is &amp;a&amp; saying that laziness is the engine of the &lt;None&gt; progress article insertion (a); article deletion (the)
6. ...experience teaches people to strive to &lt;for&gt; the &lt;None&gt; possible things preposition replacement (to → for); article deletion
(the)
</listItem>
<tableCaption confidence="0.978327">
Table 2: Examples of sentences annotated using the annotation tool. Each type of mistake is marked using a different
set of delimiters. The corrected words are enclosed in the delimiters and follow the word to which the correction
refers. In example 2, the annotator preserved the author’s choice kind and added a better choice nature.
</tableCaption>
<page confidence="0.969323">
32
</page>
<table confidence="0.999925">
Source Total Total Errors per Corrections by Error Type
language sent. words 100 words
Articles Prepo- Verb Word Noun Word Spell. Word Punc.
sitions form form number order repl.
Bulgarian 244 6197 11.9 10.3% 12.1% 3.5% 3.1% 3.0% 2.0% 5.0% 46.7% 14.2%
Chinese 468 9327 15.1 12.7% 27.2% 7.9% 3.1% 4.6% 1.4% 5.4% 26.2% 11.3%
Czech 296 6570 12.9 16.3% 10.8% 5.2% 3.4% 2.7% 3.2% 8.3% 32.5% 17.5%
French 238 5656 5.8 6.7% 17.4% 2.1% 4.0% 4.6% 3.1% 9.8% 12.5% 39.8%
German 198 5086 11.4 4.0% 13.0% 4.3% 2.8% 1.9% 2.9% 4.7% 15.4% 51.0%
Italian 243 6843 10.6 5.9% 16.6% 6.4% 1.4% 3.0% 2.4% 4.6% 20.5% 39.3%
Polish 198 4642 10.1 15.1% 16.3% 4.0% 1.3% 1.3% 2.3% 2.1% 12.3% 45.2%
Russian 464 10844 13.0 19.2% 17.8% 3.7% 2.5% 2.5% 2.1% 5.0% 28.3% 18.8%
Spanish 296 7760 15.0 11.5% 14.2% 6.0% 3.8% 2.6% 1.6% 11.9% 37.7% 10.7%
All 2645 62925 12.2 12.5% 17.1% 5.2% 2.9% 3.0% 2.2% 6.5% 28.2% 22.5%
</table>
<tableCaption confidence="0.999957">
Table 3: Error statistics on the annotated data by source language and error type
</tableCaption>
<bodyText confidence="0.999760652173913">
the punctuation category, which comprises 22% of
all corrections. About 12% of all errors involve ar-
ticles, and prepositions comprise 17% of all errors.
We would expect the preposition category to be less
significant if we did not specifically look for such er-
rors, when selecting sentences for annotation. Two
other common categories are spelling and verb form.
Verb form combines errors in verb conjugation and
errors in verb tense. It can be observed from the
table that there is a significantly smaller proportion
of article errors for the speakers of languages that
have articles, such as French or German. Lexical
errors (word replacement) are more common in lan-
guage groups that have a higher rate of errors per
100 words. In contrast, the proportion of punctua-
tion mistakes is higher for those learners that make
fewer errors overall (cf. French, German, Italian,
and Polish). This suggests that punctuation errors
are difficult to master, maybe because rules of punc-
tuation are not generally taught in foreign language
classes. Besides, there is a high degree of variation
in the use of punctuation even among native speak-
ers.
</bodyText>
<subsectionHeader confidence="0.998311">
5.1 Statistics on Article Corrections
</subsectionHeader>
<bodyText confidence="0.999945925">
As stated in Section 2, article errors are one of the
most common mistakes made by non-native speak-
ers of English. This is especially true for the speak-
ers of languages that do not have articles, but for ad-
vanced French speakers this is also a very common
mistake (Dagneaux et al., 1998), suggesting that ar-
ticle usage in English is a very difficult language fea-
ture to master.
Han et al. (2006) show that about 13% of noun
phrases in TOEFL essays by Chinese, Japanese, and
Russian speakers have article mistakes. They also
show that learners do not confuse articles randomly
and the most common article mistakes are omissions
and superfluous article usage. Our findings are sum-
marized in Table 4 and are very similar. We also
distinguish between the superfluous use of a and
the, we allows us to observe that most of the cases
of extraneously used articles involve article the for
all language groups. In fact, extraneous the is the
most common article mistake for the majority of
our speakers. Superfluous the is usually followed
by the omission of the and the omission of a. An-
other statistic that our table demonstrates and that
was shown previously (e.g. (Dalgish, 1985)) is that
learners whose first language does not have articles
make more article mistakes: We can see from col-
umn 3 of the table that the speakers of German,
French and Italian are three to four times less likely
to make an article mistake than the speakers of Chi-
nese and all of the Slavic languages. The only ex-
ception are Spanish speakers. It is not clear whether
the higher error rate is only due to a difference in
overall language proficiency (as is apparent from the
average number of mistakes by these speakers in Ta-
ble 3) or to other factors. Finally, the last column in
the table indicates that confusing articles with pro-
nouns is a relatively common error and on average
accounts for 10% of all article mistakes5. Current
article correction systems do not address this error
type.
</bodyText>
<footnote confidence="0.988888">
5An example of such confusion is ” To pay for the crimes,
criminals are put in prison”, where the is used instead of their.
</footnote>
<page confidence="0.993182">
33
</page>
<table confidence="0.999231866666667">
Source Errors Errors Article mistakes by error type
language total per 100
words
Miss. Miss. Extr. Extr. Confu- Mult. Other
the a the a sion labels
Bulgarian 76 1.2 9% 25% 41% 3% 8% 1% 13%
Chinese 179 1.9 20% 12% 48% 4% 7% 2% 7%
Czech 138 2.1 29% 13% 29% 9% 7% 4% 9%
French 22 0.4 9% 14% 36% 14% 0% 23% 5%
German 23 0.5 22% 9% 22% 4% 8% 9% 26%
Italian 43 0.6 16% 40% 26% 2% 9% 0% 7%
Polish 71 1.5 37% 18% 17% 8% 11% 4% 4%
Russian 271 2.5 24% 18% 31% 6% 11% 1% 9%
Spanish 134 1.7 16% 10% 51% 7% 3% 1% 10%
All 957 1.5 22% 16% 36% 6% 8% 3% 9%
</table>
<tableCaption confidence="0.967627333333333">
Table 4: Distribution of article mistakes by error type and source language of the writer. Confusion error type refers to
confusing articles a and the. Multiple labels denotes cases where the annotator specified more than one article choice,
one of which was used by the learner. Other refers to confusing articles with possessive and demonstrative pronouns.
</tableCaption>
<subsectionHeader confidence="0.998986">
5.2 Statistics on Preposition Corrections
</subsectionHeader>
<bodyText confidence="0.999822722222222">
Table 5 shows statistics on errors in preposition us-
age. Preposition mistakes are classified into three
categories: replacements, insertions, and deletions.
Unlike with article errors, the most common type
of preposition errors is confusing two prepositions.
This category accounts for more than half of all er-
rors, and the breakdown is very similar for all lan-
guage groups. The fourth category in the table, with
original, refers to the preposition usages that were
found acceptable by the annotators, but with a bet-
ter suggestion provided. We distinguish this case
as a separate category because preposition usage is
highly variable, unlike, for example, article usage.
Tetreault and Chodorow (Tetreault and Chodorow,
2008a) show that agreement between two native
speakers on a cloze test targeting prepositions is
about 76%, which demonstrates that there are many
contexts that license multiple prepositions.
</bodyText>
<sectionHeader confidence="0.999375" genericHeader="method">
6 Inter-annotator Agreement
</sectionHeader>
<bodyText confidence="0.978988916666667">
Correcting non-native text for a variety of mistakes
is challenging and requires a number of decisions on
the part of the annotator. Human language allows for
many ways to express the same idea. Furthermore, it
is possible that the corrected sentence, even when it
does not contain clear mistakes, does not sound like
a sentence produced by a native speaker. The latter
is complicated by the fact that native speakers differ
widely with respect to what constitutes acceptable
usage (Tetreault and Chodorow, 2008a).
To date, a common approach to annotating non-
native text has been to use one rater (Gamon et al.,
</bodyText>
<table confidence="0.998522133333333">
Source Errors Errors Mistakes by error type
language total per 100
words
Repl. Ins. Del. With
orig.
Bulgarian 89 1.4 58% 22% 11% 8%
Chinese 384 4.1 52% 24% 22% 2%
Czech 91 1.4 51% 21% 24% 4%
French 57 1.0 61% 9% 12% 18%
German 75 1.5 61% 8% 16% 15%
Italian 120 1.8 57% 22% 12% 8%
Polish 77 1.7 49% 18% 16% 17%
Russian 251 2.3 53% 21% 17% 9%
Spanish 165 2.1 55% 20% 19% 6%
All 1309 2.1 54% 21% 18% 7%
</table>
<tableCaption confidence="0.7798725">
Table 5: Distribution of preposition mistakes by error
type and source language of the writer. With orig refers to
prepositions judged as acceptable by the annotators, but
with a better suggestion provided.
</tableCaption>
<bodyText confidence="0.999146777777778">
2008; Han et al., 2006; Izumi et al., 2004; Na-
gata et al., 2006). The output of human annota-
tion is viewed as the gold standard when evaluating
an error detection system. The question of reliabil-
ity of using one rater has been raised in (Tetreault
and Chodorow, 2008a), where an extensive reliabil-
ity study of human judgments in rating preposition
usage is described. In particular, it is shown that
inter-annotator agreement on preposition correction
is low (kappa value of 0.63) and that native speakers
do not always agree on whether a specific preposi-
tion constitutes acceptable usage.
We measure agreement by asking an annotator
whether a sentence corrected by another person is
correct. After all, our goal was to make the sentence
sound native-like, without enforcing that errors are
corrected in the same way. One hundred sentences
annotated by each person were selected and the cor-
</bodyText>
<page confidence="0.995394">
34
</page>
<table confidence="0.999646">
Agreement set Rater Judged Judged
correct incorrect
Agreement set 1 Rater #2 37 63
Rater #3 59 41
Agreement set 2 Rater #1 79 21
Rater #3 73 27
Agreement set 3 Rater #1 83 17
Rater #2 47 53
</table>
<tableCaption confidence="0.987656">
Table 6: Annotator agreement at the sentence level. The
</tableCaption>
<bodyText confidence="0.978337973684211">
number next to the agreement set denotes the annotator
who corrected the sentences on the first pass. Judged cor-
rect denotes the proportion of sentences in the agreement
set that the second rater did not change. Judged incorrect
denotes the proportion of sentences, in which the second
rater made corrections.
rections were applied. This corrected set was mixed
with new sentences and given to the other two anno-
tators. In this manner, each annotator received two
hundred sentences corrected by the other two anno-
tators. For each pair of the annotators, we compute
agreement based on the 100 sentences on which they
did a second pass after the initial corrections by the
third rater. To compute agreement at the sentence
level, we assign the annotated sentences to one of
the two categories: ”correct” and ”incorrect”: A sen-
tence is considered ”correct” if a rater did not make
any corrections in it on the second pass 6. Table 6
shows for each agreement set the number of sen-
tences that were corrected on the second pass. On
average, 40.8% of the agreement set sentences be-
long to the ”incorrect” category, but the proportion
of ”incorrect” sentences varies across annotators.
We also compute agreement on the two cate-
gories, ”correct” and ”incorrect”. The agreement
and the kappa values are shown in Table 7. Agree-
ment on the sentences corrected on the second pass
varies between 56% to 78% with kappa values rang-
ing from 0.16 to 0.40. The low numbers reflect the
difficulty of the task and the variability of the na-
tive speakers’ judgments about acceptable usage. In
fact, since the annotation requires looking at sev-
eral phenomena, we can expect a lower agreement,
when compared to agreement rate on one language
phenomenon. Suppose rater A disagrees with rater
B on a given phenomenon with probability 1/4,
then, when there are two phenomena, the probabil-
ity that he will disagree with at least on of them is
</bodyText>
<footnote confidence="0.875913">
6We ignore punctuation corrections.
</footnote>
<table confidence="0.99976225">
Agreement set Agreement kappa
Agreement set 1 56% 0.16
Agreement set 2 78% 0.40
Agreement set 3 60% 0.23
</table>
<tableCaption confidence="0.994876">
Table 7: Agreement at the sentence level. Agreement
</tableCaption>
<bodyText confidence="0.8491144">
shows how many sentences in each agreement set were
assigned to the same category (”correct”, ”incorrect”) for
each of the two raters.
1 − 9/16 = 7/16. And the probability goes down
with the number of phenomena.
</bodyText>
<sectionHeader confidence="0.998369" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9999794">
In this paper, we presented a corpus of essays by stu-
dents of English of nine first language backgrounds,
corrected and annotated for errors. To our knowl-
edge, this is the first fully-corrected corpus that con-
tains such diverse data. We have described an anno-
tation schema, have shown statistics on the error dis-
tribution for writers of different first language back-
grounds and inter-annotator agreement on the task.
We have also described a program that was devel-
oped to facilitate the annotation process.
While natural language annotation, especially in
the context of error correction, is a challenging and
time-consuming task, research in learner corpora
and annotation is important for the development of
robust systems for correcting and detecting errors.
</bodyText>
<sectionHeader confidence="0.998226" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999990333333333">
We thank the anonymous reviewers for their helpful
comments. This research is partly supported by a
grant from the U.S. Department of Education.
</bodyText>
<sectionHeader confidence="0.999273" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99976725">
J. Bitchener, S. Young and D. Cameron. 2005. The Ef-
fect of Different Types of Corrective Feedback on ESL
Student Writing. Journal of Second Language Writ-
ing.
A. J. Carlson and J. Rosen and D. Roth. 2001. Scaling
Up Context Sensitive Text Correction. IAAI, 45–50.
M. Chodorow, J. Tetreault and N-R. Han. 2007. De-
tection of Grammatical Errors Involving Prepositions.
Proceedings of the Fourth ACL-SIGSEM Workshop on
Prepositions.
E. Dagneaux, S. Denness and S. Granger. 1998.
Computer-aided Error Analysis. System, 26:163–174.
</reference>
<page confidence="0.982897">
35
</page>
<reference confidence="0.999926181818182">
G. Dalgish. 1985. Computer-assisted ESL Research.
CALICO Journal, 2(2).
G. Dalgish. 1991. Computer-Assisted Error Analysis
and Courseware Design: Applications for ESL in the
Swedish Context. CALICO Journal, 9.
R. De Felice and S. Pulman. 2008. A Classifier-Based
Approach to Preposition and Determiner Error Correc-
tion in L2 English. In Proceedings of COLING-08.
A. Diaz-Negrillo and J. Fern´andez-Dominguez. 2006.
Error Tagging Systems for Learner Corpora. RESLA,
19:83-102.
J. Eeg-Olofsson and O. Knuttson. 2003. Automatic
Grammar Checking for Second Language Learners -
the Use of Prepositions. In Nodalida.
M. Gamon, J. Gao, C. Brockett, A. Klementiev, W.
Dolan, D. Belenko and L. Vanderwende. 2008. Using
Contextual Speller Techniques and Language Model-
ing for ESL Error Correction. Proceedings ofIJCNLP.
A. R. Golding and D. Roth. 1996. Applying Winnow
to Context-Sensitive Spelling Correction. ICML, 182–
190.
A. R. Golding and D. Roth. 1999. A Winnow based ap-
proach to Context-Sensitive Spelling Correction. Ma-
chine Learning, 34(1-3):107–130.
S. Granger, E. Dagneaux and F. Meunier. 2002. Interna-
tional Corpus ofLearner English
S. Granger. 2002. A Bird’s-eye View of Learner Cor-
pus Research. Computer Learner Corpora, Second
Language Acquisition and Foreign Language Teach-
ing, Eds. S. Granger, J. Hung and S. Petch-Tyson,
Amsterdam: John Benjamins. 3–33.
S. Gui and H. Yang. 2003. Zhongguo Xuexizhe Yingyu
Yuliaohu. (Chinese Learner English Corpus). Shang-
hai Waiyu Jiaoyu Chubanshe. (In Chinese).
N. Han, M. Chodorow and C. Leacock. 2006. Detect-
ing Errors in English Article Usage by Non-native
Speakers. Journal of Natural Language Engineering,
12(2):115–129.
E. Izumi, K. Uchimoto, T. Saiga and H. Isahara. 2003.
Automatic Error Detection in the Japanese Leaners
English Spoken Data. ACL.
E. Izumi, K. Uchimoto and H. Isahara. 2004. The
Overview of the SST Speech Corpus of Japanese
Learner English and Evaluation through the Exper-
iment on Automatic Detection of Learners’ Errors.
LREC.
E. Izumi, K. Uchimoto and H. Isahara. 2004. The
NICT JLE Corpus: Exploiting the Language Learner’s
Speech Database for Research and Education. Inter-
national Journal of the Computer, the Internet and
Management, 12(2):119–125.
R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. A
Feedback-Augmented Method for Detecting Errors in
the Writing of Learners of English. ACL/COLING.
N. Pravec. 2002. Survey of learner corpora. ICAME
Journal, 26:81–114.
A. Rozovskaya and D. Roth 2010. Training Paradigms
for Correcting Errors in Grammar and Usage. In Pro-
ceedings of the NAACL-HLT, Los-Angeles, CA.
J. Tetreault and M. Chodorow. 2008. Native Judgments
of Non-Native Usage: Experiments in Preposition Er-
ror Detection. COLING Workshop on Human Judg-
ments in Computational Linguistics, Manchester, UK.
J. Tetreault and M. Chodorow. 2008. The Ups and
Downs of Preposition Error Detection in ESL Writing.
COLING, Manchester, UK.
</reference>
<page confidence="0.998943">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.889782">
<title confidence="0.999365">Annotating ESL Errors: Challenges and Rewards</title>
<author confidence="0.997807">Alla Rozovskaya</author>
<author confidence="0.997807">Dan</author>
<affiliation confidence="0.998534">University of Illinois at</affiliation>
<address confidence="0.92109">Urbana, IL</address>
<abstract confidence="0.998475095238095">In this paper, we present a corrected and errortagged corpus of essays written by non-native speakers of English. The corpus contains 63000 words and includes data by learners of English of nine first language backgrounds. The annotation was performed at the sentence level and involved correcting all errors in the sentence. Error classification includes mistakes in preposition and article usage, errors in grammar, word order, and word choice. We show an analysis of errors in the annotated corpus by error categories and first language backgrounds, as well as inter-annotator agreement on the task. We also describe a computer program that was developed to facilitate and standardize the annotation procedure for the task. The program allows for the annotation of various types of mistakes and was used in the annotation of the corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bitchener</author>
<author>S Young</author>
<author>D Cameron</author>
</authors>
<title>The Effect of Different Types</title>
<date>2005</date>
<journal>of Corrective Feedback on ESL Student Writing. Journal of Second Language Writing.</journal>
<contexts>
<context position="7803" citStr="Bitchener et al., 2005" startWordPosition="1256" endWordPosition="1259">ative essays by advanced English learners. The corpus contains 2 million words of writing by European learners from 14 mother tongue backgrounds. While the entire corpus is not error-tagged, the French subpart of the corpus along with other data by French speakers of a lower level of proficiency has been annotated (Dagneaux et al., 1998). The most common errors for the advanced level of proficiency were found to be lexical errors (words) (15%), register (10%), articles (10%), pronouns (10%), spelling (8%) , verbs (8%). In a study of 53 post-intermediate ESOL (migrant) learners in New Zealand (Bitchener et al., 2005), the most common errors were found to be prepositions (29%), articles (20%), and verb tense (22%). Dalgish (1985) conducted a study of errors produced by ESL students enrolled at CUNY. It was found that across students of different first 29 languages, the most common error types among 24 different error types were errors in article usage (28%), vocabulary error (20-25%) (word choice and idioms), prepositions (18%), and verb-subject agreement (15%). He also noted that the speakers of languages without article system made considerably more article errors, but the breakdown of other error types </context>
</contexts>
<marker>Bitchener, Young, Cameron, 2005</marker>
<rawString>J. Bitchener, S. Young and D. Cameron. 2005. The Effect of Different Types of Corrective Feedback on ESL Student Writing. Journal of Second Language Writing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Carlson</author>
<author>J Rosen</author>
<author>D Roth</author>
</authors>
<title>Scaling Up Context Sensitive Text Correction.</title>
<date>2001</date>
<booktitle>IAAI,</booktitle>
<pages>45--50</pages>
<contexts>
<context position="1187" citStr="Carlson et al., 2001" startWordPosition="180" endWordPosition="183">sage, errors in grammar, word order, and word choice. We show an analysis of errors in the annotated corpus by error categories and first language backgrounds, as well as inter-annotator agreement on the task. We also describe a computer program that was developed to facilitate and standardize the annotation procedure for the task. The program allows for the annotation of various types of mistakes and was used in the annotation of the corpus. 1 Introduction Work on automated methods for detecting and correcting context dependent mistakes (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the e</context>
</contexts>
<marker>Carlson, Rosen, Roth, 2001</marker>
<rawString>A. J. Carlson and J. Rosen and D. Roth. 2001. Scaling Up Context Sensitive Text Correction. IAAI, 45–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Chodorow</author>
<author>J Tetreault</author>
<author>N-R Han</author>
</authors>
<title>Detection of Grammatical Errors Involving Prepositions.</title>
<date>2007</date>
<booktitle>Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions.</booktitle>
<marker>Chodorow, Tetreault, Han, 2007</marker>
<rawString>M. Chodorow, J. Tetreault and N-R. Han. 2007. Detection of Grammatical Errors Involving Prepositions. Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Dagneaux</author>
<author>S Denness</author>
<author>S Granger</author>
</authors>
<title>Computer-aided Error Analysis. System,</title>
<date>1998</date>
<pages>26--163</pages>
<contexts>
<context position="2063" citStr="Dagneaux et al., 1998" startWordPosition="322" endWordPosition="325">ing systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic study of a whole range of errors non-native writers produce, nor is it clear what the distribution of different types of mistakes is in learner language. In this paper, we describe a corpus of sentences written by English as a Second Language (ESL) speakers, annotated for the purposes of developing an automated system for correcting mistakes in text. Although the focus of the annotation were errors in article and preposition usage, all mistakes in the sentence have been corrected. The data for </context>
<context position="7519" citStr="Dagneaux et al., 1998" startWordPosition="1208" endWordPosition="1211">ning, intermediate, and advanced levels. This corpus is also corrected and error-tagged, but the tagging schema does not allow for an easy isolation of article and preposition errors. The International Corpus of Learner English (ICLE, (Granger et al., 2002a)) is a corpus of argumentative essays by advanced English learners. The corpus contains 2 million words of writing by European learners from 14 mother tongue backgrounds. While the entire corpus is not error-tagged, the French subpart of the corpus along with other data by French speakers of a lower level of proficiency has been annotated (Dagneaux et al., 1998). The most common errors for the advanced level of proficiency were found to be lexical errors (words) (15%), register (10%), articles (10%), pronouns (10%), spelling (8%) , verbs (8%). In a study of 53 post-intermediate ESOL (migrant) learners in New Zealand (Bitchener et al., 2005), the most common errors were found to be prepositions (29%), articles (20%), and verb tense (22%). Dalgish (1985) conducted a study of errors produced by ESL students enrolled at CUNY. It was found that across students of different first 29 languages, the most common error types among 24 different error types were</context>
<context position="22002" citStr="Dagneaux et al., 1998" startWordPosition="3580" endWordPosition="3583">ll (cf. French, German, Italian, and Polish). This suggests that punctuation errors are difficult to master, maybe because rules of punctuation are not generally taught in foreign language classes. Besides, there is a high degree of variation in the use of punctuation even among native speakers. 5.1 Statistics on Article Corrections As stated in Section 2, article errors are one of the most common mistakes made by non-native speakers of English. This is especially true for the speakers of languages that do not have articles, but for advanced French speakers this is also a very common mistake (Dagneaux et al., 1998), suggesting that article usage in English is a very difficult language feature to master. Han et al. (2006) show that about 13% of noun phrases in TOEFL essays by Chinese, Japanese, and Russian speakers have article mistakes. They also show that learners do not confuse articles randomly and the most common article mistakes are omissions and superfluous article usage. Our findings are summarized in Table 4 and are very similar. We also distinguish between the superfluous use of a and the, we allows us to observe that most of the cases of extraneously used articles involve article the for all l</context>
</contexts>
<marker>Dagneaux, Denness, Granger, 1998</marker>
<rawString>E. Dagneaux, S. Denness and S. Granger. 1998. Computer-aided Error Analysis. System, 26:163–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Dalgish</author>
</authors>
<date>1985</date>
<journal>Computer-assisted ESL Research. CALICO Journal,</journal>
<volume>2</volume>
<issue>2</issue>
<contexts>
<context position="7917" citStr="Dalgish (1985)" startWordPosition="1276" endWordPosition="1277">er tongue backgrounds. While the entire corpus is not error-tagged, the French subpart of the corpus along with other data by French speakers of a lower level of proficiency has been annotated (Dagneaux et al., 1998). The most common errors for the advanced level of proficiency were found to be lexical errors (words) (15%), register (10%), articles (10%), pronouns (10%), spelling (8%) , verbs (8%). In a study of 53 post-intermediate ESOL (migrant) learners in New Zealand (Bitchener et al., 2005), the most common errors were found to be prepositions (29%), articles (20%), and verb tense (22%). Dalgish (1985) conducted a study of errors produced by ESL students enrolled at CUNY. It was found that across students of different first 29 languages, the most common error types among 24 different error types were errors in article usage (28%), vocabulary error (20-25%) (word choice and idioms), prepositions (18%), and verb-subject agreement (15%). He also noted that the speakers of languages without article system made considerably more article errors, but the breakdown of other error types across languages was surprisingly similar. 3 Annotation 3.1 Data Selection Data for annotation were extracted from</context>
<context position="22890" citStr="Dalgish, 1985" startWordPosition="3733" endWordPosition="3734">icles randomly and the most common article mistakes are omissions and superfluous article usage. Our findings are summarized in Table 4 and are very similar. We also distinguish between the superfluous use of a and the, we allows us to observe that most of the cases of extraneously used articles involve article the for all language groups. In fact, extraneous the is the most common article mistake for the majority of our speakers. Superfluous the is usually followed by the omission of the and the omission of a. Another statistic that our table demonstrates and that was shown previously (e.g. (Dalgish, 1985)) is that learners whose first language does not have articles make more article mistakes: We can see from column 3 of the table that the speakers of German, French and Italian are three to four times less likely to make an article mistake than the speakers of Chinese and all of the Slavic languages. The only exception are Spanish speakers. It is not clear whether the higher error rate is only due to a difference in overall language proficiency (as is apparent from the average number of mistakes by these speakers in Table 3) or to other factors. Finally, the last column in the table indicates </context>
</contexts>
<marker>Dalgish, 1985</marker>
<rawString>G. Dalgish. 1985. Computer-assisted ESL Research. CALICO Journal, 2(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Dalgish</author>
</authors>
<title>Computer-Assisted Error Analysis and Courseware Design: Applications for ESL in the Swedish Context.</title>
<date>1991</date>
<journal>CALICO Journal,</journal>
<volume>9</volume>
<marker>Dalgish, 1991</marker>
<rawString>G. Dalgish. 1991. Computer-Assisted Error Analysis and Courseware Design: Applications for ESL in the Swedish Context. CALICO Journal, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R De Felice</author>
<author>S Pulman</author>
</authors>
<title>A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING-08.</booktitle>
<marker>De Felice, Pulman, 2008</marker>
<rawString>R. De Felice and S. Pulman. 2008. A Classifier-Based Approach to Preposition and Determiner Error Correction in L2 English. In Proceedings of COLING-08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Diaz-Negrillo</author>
<author>J Fern´andez-Dominguez</author>
</authors>
<title>Error Tagging Systems for Learner Corpora.</title>
<date>2006</date>
<journal>RESLA,</journal>
<pages>19--83</pages>
<marker>Diaz-Negrillo, Fern´andez-Dominguez, 2006</marker>
<rawString>A. Diaz-Negrillo and J. Fern´andez-Dominguez. 2006. Error Tagging Systems for Learner Corpora. RESLA, 19:83-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eeg-Olofsson</author>
<author>O Knuttson</author>
</authors>
<title>Automatic Grammar Checking for Second Language Learners -the Use of Prepositions. In Nodalida.</title>
<date>2003</date>
<marker>Eeg-Olofsson, Knuttson, 2003</marker>
<rawString>J. Eeg-Olofsson and O. Knuttson. 2003. Automatic Grammar Checking for Second Language Learners -the Use of Prepositions. In Nodalida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gamon</author>
<author>J Gao</author>
<author>C Brockett</author>
<author>A Klementiev</author>
<author>W Dolan</author>
<author>D Belenko</author>
<author>L Vanderwende</author>
</authors>
<title>Using Contextual Speller Techniques and Language Modeling for ESL Error Correction.</title>
<date>2008</date>
<booktitle>Proceedings ofIJCNLP.</booktitle>
<contexts>
<context position="1564" citStr="Gamon et al., 2008" startWordPosition="244" endWordPosition="247">us types of mistakes and was used in the annotation of the corpus. 1 Introduction Work on automated methods for detecting and correcting context dependent mistakes (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic </context>
<context position="6765" citStr="Gamon et al., 2008" startWordPosition="1085" endWordPosition="1088">icult, as there are no standardized error-tagging schemas, but we can get a general idea about the types of errors prevalent with such speakers. Izumi et al. (2004a) describe a speech corpus of Japanese learners of English (NICT JLE). The corpus is corrected and annotated and consists of the transcripts (2 million words) of the audio-recordings of the English oral proficiency interview test. In the NICT corpus, whose error tag set consists of 45 tags, about 26.6% of errors are determiner related, and 10% are preposition related, which makes these two error types the most common in the corpus (Gamon et al., 2008). The Chinese Learners of English corpus (CLEC, (Gui and Yang, 2003)) is a collection of essays written by Chinese learners of beginning, intermediate, and advanced levels. This corpus is also corrected and error-tagged, but the tagging schema does not allow for an easy isolation of article and preposition errors. The International Corpus of Learner English (ICLE, (Granger et al., 2002a)) is a corpus of argumentative essays by advanced English learners. The corpus contains 2 million words of writing by European learners from 14 mother tongue backgrounds. While the entire corpus is not error-ta</context>
</contexts>
<marker>Gamon, Gao, Brockett, Klementiev, Dolan, Belenko, Vanderwende, 2008</marker>
<rawString>M. Gamon, J. Gao, C. Brockett, A. Klementiev, W. Dolan, D. Belenko and L. Vanderwende. 2008. Using Contextual Speller Techniques and Language Modeling for ESL Error Correction. Proceedings ofIJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>Applying Winnow to Context-Sensitive Spelling Correction.</title>
<date>1996</date>
<journal>ICML,</journal>
<volume>182</volume>
<pages>190</pages>
<contexts>
<context position="1140" citStr="Golding and Roth, 1996" startWordPosition="172" endWordPosition="175">n includes mistakes in preposition and article usage, errors in grammar, word order, and word choice. We show an analysis of errors in the annotated corpus by error categories and first language backgrounds, as well as inter-annotator agreement on the task. We also describe a computer program that was developed to facilitate and standardize the annotation procedure for the task. The program allows for the annotation of various types of mistakes and was used in the annotation of the corpus. 1 Introduction Work on automated methods for detecting and correcting context dependent mistakes (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of E</context>
</contexts>
<marker>Golding, Roth, 1996</marker>
<rawString>A. R. Golding and D. Roth. 1996. Applying Winnow to Context-Sensitive Spelling Correction. ICML, 182– 190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Golding</author>
<author>D Roth</author>
</authors>
<title>A Winnow based approach to Context-Sensitive Spelling Correction.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1164" citStr="Golding and Roth, 1999" startWordPosition="176" endWordPosition="179">reposition and article usage, errors in grammar, word order, and word choice. We show an analysis of errors in the annotated corpus by error categories and first language backgrounds, as well as inter-annotator agreement on the task. We also describe a computer program that was developed to facilitate and standardize the annotation procedure for the task. The program allows for the annotation of various types of mistakes and was used in the annotation of the corpus. 1 Introduction Work on automated methods for detecting and correcting context dependent mistakes (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al</context>
</contexts>
<marker>Golding, Roth, 1999</marker>
<rawString>A. R. Golding and D. Roth. 1999. A Winnow based approach to Context-Sensitive Spelling Correction. Machine Learning, 34(1-3):107–130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Granger</author>
<author>E Dagneaux</author>
<author>F Meunier</author>
</authors>
<date>2002</date>
<journal>International Corpus ofLearner English</journal>
<contexts>
<context position="2775" citStr="Granger et al., 2002" startWordPosition="441" endWordPosition="444">stematic study of a whole range of errors non-native writers produce, nor is it clear what the distribution of different types of mistakes is in learner language. In this paper, we describe a corpus of sentences written by English as a Second Language (ESL) speakers, annotated for the purposes of developing an automated system for correcting mistakes in text. Although the focus of the annotation were errors in article and preposition usage, all mistakes in the sentence have been corrected. The data for annotation were taken from two sources: The International Corpus of Learner English (ICLE, (Granger et al., 2002a)) and Chinese Learners of English Corpus (CLEC, (Gui and Yang, 2003)). The annotated corpus includes data from speakers of nine first language backgrounds. To our knowledge, this is the first corpus of non-native English text (learner corpus) of fully-corrected sentences from such a diverse group of learners1. The size of the annotated corpus is 63000 words, or 2645 sentences. While a corpus 1Possibly, except for the Cambridge Learner Corpus http://www.cambridge.org/elt 28 Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 2</context>
<context position="7153" citStr="Granger et al., 2002" startWordPosition="1145" endWordPosition="1148">w test. In the NICT corpus, whose error tag set consists of 45 tags, about 26.6% of errors are determiner related, and 10% are preposition related, which makes these two error types the most common in the corpus (Gamon et al., 2008). The Chinese Learners of English corpus (CLEC, (Gui and Yang, 2003)) is a collection of essays written by Chinese learners of beginning, intermediate, and advanced levels. This corpus is also corrected and error-tagged, but the tagging schema does not allow for an easy isolation of article and preposition errors. The International Corpus of Learner English (ICLE, (Granger et al., 2002a)) is a corpus of argumentative essays by advanced English learners. The corpus contains 2 million words of writing by European learners from 14 mother tongue backgrounds. While the entire corpus is not error-tagged, the French subpart of the corpus along with other data by French speakers of a lower level of proficiency has been annotated (Dagneaux et al., 1998). The most common errors for the advanced level of proficiency were found to be lexical errors (words) (15%), register (10%), articles (10%), pronouns (10%), spelling (8%) , verbs (8%). In a study of 53 post-intermediate ESOL (migrant</context>
<context position="8555" citStr="Granger et al., 2002" startWordPosition="1374" endWordPosition="1377"> of errors produced by ESL students enrolled at CUNY. It was found that across students of different first 29 languages, the most common error types among 24 different error types were errors in article usage (28%), vocabulary error (20-25%) (word choice and idioms), prepositions (18%), and verb-subject agreement (15%). He also noted that the speakers of languages without article system made considerably more article errors, but the breakdown of other error types across languages was surprisingly similar. 3 Annotation 3.1 Data Selection Data for annotation were extracted from the ICLE corpus (Granger et al., 2002a) and CLEC (Gui and Yang, 2003). As stated in Section 2, the ICLE contains data by European speakers of advanced level of proficiency, and the CLEC corpus contains essays by Chinese learners of different levels of proficiency. The annotated corpus includes sentences written by speakers of nine languages: Bulgarian, Chinese, Czech, French, German, Italian, Polish, Russian, and Spanish. About half of the sentences for annotation were selected based on their scores with respect to a 4-gram language model built using the English Gigaword corpus (LDC2005T12). This was done in order to exclude sent</context>
</contexts>
<marker>Granger, Dagneaux, Meunier, 2002</marker>
<rawString>S. Granger, E. Dagneaux and F. Meunier. 2002. International Corpus ofLearner English</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Granger</author>
</authors>
<title>A Bird’s-eye View of Learner Corpus Research. Computer Learner Corpora, Second Language Acquisition and Foreign Language Teaching, Eds.</title>
<date>2002</date>
<pages>3--33</pages>
<location>Amsterdam: John Benjamins.</location>
<contexts>
<context position="6094" citStr="Granger, 2002" startWordPosition="974" endWordPosition="975">esent error statistics based on the annotated corpus across all error types and separately for errors in article and preposition usage. Finally, in Section 6 we describe how we 2Details about the annotation are accessible from http://L2R.cs.uiuc.edu/˜cogcomp/ 3The ICLE and CLEC corpora are commercially available. evaluate inter-annotator agreement and show agreement results for the task. 2 Learner Corpora and Error Tagging In this section, we review research in the annotation and error analysis of learner corpora. For a review of learner corpus research see, for example, (Diaz-Negrillo, 2006; Granger, 2002b; Pravec, 2002). Comparative error analysis is difficult, as there are no standardized error-tagging schemas, but we can get a general idea about the types of errors prevalent with such speakers. Izumi et al. (2004a) describe a speech corpus of Japanese learners of English (NICT JLE). The corpus is corrected and annotated and consists of the transcripts (2 million words) of the audio-recordings of the English oral proficiency interview test. In the NICT corpus, whose error tag set consists of 45 tags, about 26.6% of errors are determiner related, and 10% are preposition related, which makes t</context>
</contexts>
<marker>Granger, 2002</marker>
<rawString>S. Granger. 2002. A Bird’s-eye View of Learner Corpus Research. Computer Learner Corpora, Second Language Acquisition and Foreign Language Teaching, Eds. S. Granger, J. Hung and S. Petch-Tyson, Amsterdam: John Benjamins. 3–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gui</author>
<author>H Yang</author>
</authors>
<title>Zhongguo Xuexizhe Yingyu Yuliaohu. (Chinese Learner English Corpus). Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).</title>
<date>2003</date>
<contexts>
<context position="2845" citStr="Gui and Yang, 2003" startWordPosition="453" endWordPosition="456">or is it clear what the distribution of different types of mistakes is in learner language. In this paper, we describe a corpus of sentences written by English as a Second Language (ESL) speakers, annotated for the purposes of developing an automated system for correcting mistakes in text. Although the focus of the annotation were errors in article and preposition usage, all mistakes in the sentence have been corrected. The data for annotation were taken from two sources: The International Corpus of Learner English (ICLE, (Granger et al., 2002a)) and Chinese Learners of English Corpus (CLEC, (Gui and Yang, 2003)). The annotated corpus includes data from speakers of nine first language backgrounds. To our knowledge, this is the first corpus of non-native English text (learner corpus) of fully-corrected sentences from such a diverse group of learners1. The size of the annotated corpus is 63000 words, or 2645 sentences. While a corpus 1Possibly, except for the Cambridge Learner Corpus http://www.cambridge.org/elt 28 Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 28–36, Los Angeles, California, June 2010. c�2010 Association for Compu</context>
<context position="6833" citStr="Gui and Yang, 2003" startWordPosition="1096" endWordPosition="1099">n get a general idea about the types of errors prevalent with such speakers. Izumi et al. (2004a) describe a speech corpus of Japanese learners of English (NICT JLE). The corpus is corrected and annotated and consists of the transcripts (2 million words) of the audio-recordings of the English oral proficiency interview test. In the NICT corpus, whose error tag set consists of 45 tags, about 26.6% of errors are determiner related, and 10% are preposition related, which makes these two error types the most common in the corpus (Gamon et al., 2008). The Chinese Learners of English corpus (CLEC, (Gui and Yang, 2003)) is a collection of essays written by Chinese learners of beginning, intermediate, and advanced levels. This corpus is also corrected and error-tagged, but the tagging schema does not allow for an easy isolation of article and preposition errors. The International Corpus of Learner English (ICLE, (Granger et al., 2002a)) is a corpus of argumentative essays by advanced English learners. The corpus contains 2 million words of writing by European learners from 14 mother tongue backgrounds. While the entire corpus is not error-tagged, the French subpart of the corpus along with other data by Fren</context>
<context position="8587" citStr="Gui and Yang, 2003" startWordPosition="1380" endWordPosition="1383">ts enrolled at CUNY. It was found that across students of different first 29 languages, the most common error types among 24 different error types were errors in article usage (28%), vocabulary error (20-25%) (word choice and idioms), prepositions (18%), and verb-subject agreement (15%). He also noted that the speakers of languages without article system made considerably more article errors, but the breakdown of other error types across languages was surprisingly similar. 3 Annotation 3.1 Data Selection Data for annotation were extracted from the ICLE corpus (Granger et al., 2002a) and CLEC (Gui and Yang, 2003). As stated in Section 2, the ICLE contains data by European speakers of advanced level of proficiency, and the CLEC corpus contains essays by Chinese learners of different levels of proficiency. The annotated corpus includes sentences written by speakers of nine languages: Bulgarian, Chinese, Czech, French, German, Italian, Polish, Russian, and Spanish. About half of the sentences for annotation were selected based on their scores with respect to a 4-gram language model built using the English Gigaword corpus (LDC2005T12). This was done in order to exclude sentences that would require heavy e</context>
</contexts>
<marker>Gui, Yang, 2003</marker>
<rawString>S. Gui and H. Yang. 2003. Zhongguo Xuexizhe Yingyu Yuliaohu. (Chinese Learner English Corpus). Shanghai Waiyu Jiaoyu Chubanshe. (In Chinese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Han</author>
<author>M Chodorow</author>
<author>C Leacock</author>
</authors>
<title>Detecting Errors in English Article Usage by Non-native Speakers.</title>
<date>2006</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="1582" citStr="Han et al., 2006" startWordPosition="248" endWordPosition="251"> and was used in the annotation of the corpus. 1 Introduction Work on automated methods for detecting and correcting context dependent mistakes (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic study of a whole r</context>
<context position="22110" citStr="Han et al. (2006)" startWordPosition="3600" endWordPosition="3603">e because rules of punctuation are not generally taught in foreign language classes. Besides, there is a high degree of variation in the use of punctuation even among native speakers. 5.1 Statistics on Article Corrections As stated in Section 2, article errors are one of the most common mistakes made by non-native speakers of English. This is especially true for the speakers of languages that do not have articles, but for advanced French speakers this is also a very common mistake (Dagneaux et al., 1998), suggesting that article usage in English is a very difficult language feature to master. Han et al. (2006) show that about 13% of noun phrases in TOEFL essays by Chinese, Japanese, and Russian speakers have article mistakes. They also show that learners do not confuse articles randomly and the most common article mistakes are omissions and superfluous article usage. Our findings are summarized in Table 4 and are very similar. We also distinguish between the superfluous use of a and the, we allows us to observe that most of the cases of extraneously used articles involve article the for all language groups. In fact, extraneous the is the most common article mistake for the majority of our speakers.</context>
<context position="26931" citStr="Han et al., 2006" startWordPosition="4451" endWordPosition="4454"> Errors Mistakes by error type language total per 100 words Repl. Ins. Del. With orig. Bulgarian 89 1.4 58% 22% 11% 8% Chinese 384 4.1 52% 24% 22% 2% Czech 91 1.4 51% 21% 24% 4% French 57 1.0 61% 9% 12% 18% German 75 1.5 61% 8% 16% 15% Italian 120 1.8 57% 22% 12% 8% Polish 77 1.7 49% 18% 16% 17% Russian 251 2.3 53% 21% 17% 9% Spanish 165 2.1 55% 20% 19% 6% All 1309 2.1 54% 21% 18% 7% Table 5: Distribution of preposition mistakes by error type and source language of the writer. With orig refers to prepositions judged as acceptable by the annotators, but with a better suggestion provided. 2008; Han et al., 2006; Izumi et al., 2004; Nagata et al., 2006). The output of human annotation is viewed as the gold standard when evaluating an error detection system. The question of reliability of using one rater has been raised in (Tetreault and Chodorow, 2008a), where an extensive reliability study of human judgments in rating preposition usage is described. In particular, it is shown that inter-annotator agreement on preposition correction is low (kappa value of 0.63) and that native speakers do not always agree on whether a specific preposition constitutes acceptable usage. We measure agreement by asking a</context>
</contexts>
<marker>Han, Chodorow, Leacock, 2006</marker>
<rawString>N. Han, M. Chodorow and C. Leacock. 2006. Detecting Errors in English Article Usage by Non-native Speakers. Journal of Natural Language Engineering, 12(2):115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>T Saiga</author>
<author>H Isahara</author>
</authors>
<title>Automatic Error Detection in the Japanese Leaners English Spoken Data.</title>
<date>2003</date>
<publisher>ACL.</publisher>
<contexts>
<context position="1635" citStr="Izumi et al. (2003)" startWordPosition="256" endWordPosition="259">ntroduction Work on automated methods for detecting and correcting context dependent mistakes (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic study of a whole range of errors non-native writers produce, nor is it </context>
</contexts>
<marker>Izumi, Uchimoto, Saiga, Isahara, 2003</marker>
<rawString>E. Izumi, K. Uchimoto, T. Saiga and H. Isahara. 2003. Automatic Error Detection in the Japanese Leaners English Spoken Data. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>H Isahara</author>
</authors>
<date>2004</date>
<booktitle>The Overview of the SST Speech Corpus of Japanese Learner English and Evaluation through the Experiment on Automatic Detection of Learners’ Errors. LREC.</booktitle>
<contexts>
<context position="2103" citStr="Izumi et al., 2004" startWordPosition="330" endWordPosition="333">le and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic study of a whole range of errors non-native writers produce, nor is it clear what the distribution of different types of mistakes is in learner language. In this paper, we describe a corpus of sentences written by English as a Second Language (ESL) speakers, annotated for the purposes of developing an automated system for correcting mistakes in text. Although the focus of the annotation were errors in article and preposition usage, all mistakes in the sentence have been corrected. The data for annotation were taken from two sources: </context>
<context position="6309" citStr="Izumi et al. (2004" startWordPosition="1006" endWordPosition="1009"> accessible from http://L2R.cs.uiuc.edu/˜cogcomp/ 3The ICLE and CLEC corpora are commercially available. evaluate inter-annotator agreement and show agreement results for the task. 2 Learner Corpora and Error Tagging In this section, we review research in the annotation and error analysis of learner corpora. For a review of learner corpus research see, for example, (Diaz-Negrillo, 2006; Granger, 2002b; Pravec, 2002). Comparative error analysis is difficult, as there are no standardized error-tagging schemas, but we can get a general idea about the types of errors prevalent with such speakers. Izumi et al. (2004a) describe a speech corpus of Japanese learners of English (NICT JLE). The corpus is corrected and annotated and consists of the transcripts (2 million words) of the audio-recordings of the English oral proficiency interview test. In the NICT corpus, whose error tag set consists of 45 tags, about 26.6% of errors are determiner related, and 10% are preposition related, which makes these two error types the most common in the corpus (Gamon et al., 2008). The Chinese Learners of English corpus (CLEC, (Gui and Yang, 2003)) is a collection of essays written by Chinese learners of beginning, interm</context>
<context position="26951" citStr="Izumi et al., 2004" startWordPosition="4455" endWordPosition="4458">y error type language total per 100 words Repl. Ins. Del. With orig. Bulgarian 89 1.4 58% 22% 11% 8% Chinese 384 4.1 52% 24% 22% 2% Czech 91 1.4 51% 21% 24% 4% French 57 1.0 61% 9% 12% 18% German 75 1.5 61% 8% 16% 15% Italian 120 1.8 57% 22% 12% 8% Polish 77 1.7 49% 18% 16% 17% Russian 251 2.3 53% 21% 17% 9% Spanish 165 2.1 55% 20% 19% 6% All 1309 2.1 54% 21% 18% 7% Table 5: Distribution of preposition mistakes by error type and source language of the writer. With orig refers to prepositions judged as acceptable by the annotators, but with a better suggestion provided. 2008; Han et al., 2006; Izumi et al., 2004; Nagata et al., 2006). The output of human annotation is viewed as the gold standard when evaluating an error detection system. The question of reliability of using one rater has been raised in (Tetreault and Chodorow, 2008a), where an extensive reliability study of human judgments in rating preposition usage is described. In particular, it is shown that inter-annotator agreement on preposition correction is low (kappa value of 0.63) and that native speakers do not always agree on whether a specific preposition constitutes acceptable usage. We measure agreement by asking an annotator whether </context>
</contexts>
<marker>Izumi, Uchimoto, Isahara, 2004</marker>
<rawString>E. Izumi, K. Uchimoto and H. Isahara. 2004. The Overview of the SST Speech Corpus of Japanese Learner English and Evaluation through the Experiment on Automatic Detection of Learners’ Errors. LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Izumi</author>
<author>K Uchimoto</author>
<author>H Isahara</author>
</authors>
<title>The NICT JLE Corpus: Exploiting the Language Learner’s Speech Database for Research and Education.</title>
<date>2004</date>
<journal>International Journal of the Computer, the Internet and Management,</journal>
<volume>12</volume>
<issue>2</issue>
<contexts>
<context position="2103" citStr="Izumi et al., 2004" startWordPosition="330" endWordPosition="333">le and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic study of a whole range of errors non-native writers produce, nor is it clear what the distribution of different types of mistakes is in learner language. In this paper, we describe a corpus of sentences written by English as a Second Language (ESL) speakers, annotated for the purposes of developing an automated system for correcting mistakes in text. Although the focus of the annotation were errors in article and preposition usage, all mistakes in the sentence have been corrected. The data for annotation were taken from two sources: </context>
<context position="6309" citStr="Izumi et al. (2004" startWordPosition="1006" endWordPosition="1009"> accessible from http://L2R.cs.uiuc.edu/˜cogcomp/ 3The ICLE and CLEC corpora are commercially available. evaluate inter-annotator agreement and show agreement results for the task. 2 Learner Corpora and Error Tagging In this section, we review research in the annotation and error analysis of learner corpora. For a review of learner corpus research see, for example, (Diaz-Negrillo, 2006; Granger, 2002b; Pravec, 2002). Comparative error analysis is difficult, as there are no standardized error-tagging schemas, but we can get a general idea about the types of errors prevalent with such speakers. Izumi et al. (2004a) describe a speech corpus of Japanese learners of English (NICT JLE). The corpus is corrected and annotated and consists of the transcripts (2 million words) of the audio-recordings of the English oral proficiency interview test. In the NICT corpus, whose error tag set consists of 45 tags, about 26.6% of errors are determiner related, and 10% are preposition related, which makes these two error types the most common in the corpus (Gamon et al., 2008). The Chinese Learners of English corpus (CLEC, (Gui and Yang, 2003)) is a collection of essays written by Chinese learners of beginning, interm</context>
<context position="26951" citStr="Izumi et al., 2004" startWordPosition="4455" endWordPosition="4458">y error type language total per 100 words Repl. Ins. Del. With orig. Bulgarian 89 1.4 58% 22% 11% 8% Chinese 384 4.1 52% 24% 22% 2% Czech 91 1.4 51% 21% 24% 4% French 57 1.0 61% 9% 12% 18% German 75 1.5 61% 8% 16% 15% Italian 120 1.8 57% 22% 12% 8% Polish 77 1.7 49% 18% 16% 17% Russian 251 2.3 53% 21% 17% 9% Spanish 165 2.1 55% 20% 19% 6% All 1309 2.1 54% 21% 18% 7% Table 5: Distribution of preposition mistakes by error type and source language of the writer. With orig refers to prepositions judged as acceptable by the annotators, but with a better suggestion provided. 2008; Han et al., 2006; Izumi et al., 2004; Nagata et al., 2006). The output of human annotation is viewed as the gold standard when evaluating an error detection system. The question of reliability of using one rater has been raised in (Tetreault and Chodorow, 2008a), where an extensive reliability study of human judgments in rating preposition usage is described. In particular, it is shown that inter-annotator agreement on preposition correction is low (kappa value of 0.63) and that native speakers do not always agree on whether a specific preposition constitutes acceptable usage. We measure agreement by asking an annotator whether </context>
</contexts>
<marker>Izumi, Uchimoto, Isahara, 2004</marker>
<rawString>E. Izumi, K. Uchimoto and H. Isahara. 2004. The NICT JLE Corpus: Exploiting the Language Learner’s Speech Database for Research and Education. International Journal of the Computer, the Internet and Management, 12(2):119–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nagata</author>
<author>A Kawai</author>
<author>K Morihiro</author>
<author>N Isu</author>
</authors>
<title>A Feedback-Augmented Method for Detecting Errors in the Writing of Learners of English.</title>
<date>2006</date>
<publisher>ACL/COLING.</publisher>
<contexts>
<context position="1772" citStr="Nagata et al. (2006)" startWordPosition="277" endWordPosition="280">d Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic study of a whole range of errors non-native writers produce, nor is it clear what the distribution of different types of mistakes is in learner language. In this paper, we describe a corpus of sentences writt</context>
<context position="26973" citStr="Nagata et al., 2006" startWordPosition="4459" endWordPosition="4463">e total per 100 words Repl. Ins. Del. With orig. Bulgarian 89 1.4 58% 22% 11% 8% Chinese 384 4.1 52% 24% 22% 2% Czech 91 1.4 51% 21% 24% 4% French 57 1.0 61% 9% 12% 18% German 75 1.5 61% 8% 16% 15% Italian 120 1.8 57% 22% 12% 8% Polish 77 1.7 49% 18% 16% 17% Russian 251 2.3 53% 21% 17% 9% Spanish 165 2.1 55% 20% 19% 6% All 1309 2.1 54% 21% 18% 7% Table 5: Distribution of preposition mistakes by error type and source language of the writer. With orig refers to prepositions judged as acceptable by the annotators, but with a better suggestion provided. 2008; Han et al., 2006; Izumi et al., 2004; Nagata et al., 2006). The output of human annotation is viewed as the gold standard when evaluating an error detection system. The question of reliability of using one rater has been raised in (Tetreault and Chodorow, 2008a), where an extensive reliability study of human judgments in rating preposition usage is described. In particular, it is shown that inter-annotator agreement on preposition correction is low (kappa value of 0.63) and that native speakers do not always agree on whether a specific preposition constitutes acceptable usage. We measure agreement by asking an annotator whether a sentence corrected b</context>
</contexts>
<marker>Nagata, Kawai, Morihiro, Isu, 2006</marker>
<rawString>R. Nagata, A. Kawai, K. Morihiro, and N. Isu. 2006. A Feedback-Augmented Method for Detecting Errors in the Writing of Learners of English. ACL/COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Pravec</author>
</authors>
<title>Survey of learner corpora.</title>
<date>2002</date>
<journal>ICAME Journal,</journal>
<pages>26--81</pages>
<contexts>
<context position="6110" citStr="Pravec, 2002" startWordPosition="976" endWordPosition="977">istics based on the annotated corpus across all error types and separately for errors in article and preposition usage. Finally, in Section 6 we describe how we 2Details about the annotation are accessible from http://L2R.cs.uiuc.edu/˜cogcomp/ 3The ICLE and CLEC corpora are commercially available. evaluate inter-annotator agreement and show agreement results for the task. 2 Learner Corpora and Error Tagging In this section, we review research in the annotation and error analysis of learner corpora. For a review of learner corpus research see, for example, (Diaz-Negrillo, 2006; Granger, 2002b; Pravec, 2002). Comparative error analysis is difficult, as there are no standardized error-tagging schemas, but we can get a general idea about the types of errors prevalent with such speakers. Izumi et al. (2004a) describe a speech corpus of Japanese learners of English (NICT JLE). The corpus is corrected and annotated and consists of the transcripts (2 million words) of the audio-recordings of the English oral proficiency interview test. In the NICT corpus, whose error tag set consists of 45 tags, about 26.6% of errors are determiner related, and 10% are preposition related, which makes these two error t</context>
</contexts>
<marker>Pravec, 2002</marker>
<rawString>N. Pravec. 2002. Survey of learner corpora. ICAME Journal, 26:81–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rozovskaya</author>
<author>D Roth</author>
</authors>
<title>Training Paradigms for Correcting Errors in Grammar and Usage.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT,</booktitle>
<location>Los-Angeles, CA.</location>
<contexts>
<context position="3800" citStr="Rozovskaya and Roth, 2010" startWordPosition="600" endWordPosition="604">ossibly, except for the Cambridge Learner Corpus http://www.cambridge.org/elt 28 Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 28–36, Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics of this size may not seem significant in many natural language applications, this is in fact a large corpus for this field, especially considering the effort to correct all mistakes, as opposed to focusing on one language phenomenon. This corpus was used in the experiments described in the companion paper (Rozovskaya and Roth, 2010). The annotation schema that we developed was motivated by our special interest in errors in article and preposition usage, but also includes errors in verbs, morphology, and noun number. The corpus contains 907 article corrections and 1309 preposition corrections, in addition to annotated mistakes of other types. While the focus of the present paper is on annotating ESL mistakes, we have several goals in mind. First, we present the annotation procedure for the task, including an error classification schema, annotation speed, and inter-annotator agreement. Second, we describe a computer progra</context>
</contexts>
<marker>Rozovskaya, Roth, 2010</marker>
<rawString>A. Rozovskaya and D. Roth 2010. Training Paradigms for Correcting Errors in Grammar and Usage. In Proceedings of the NAACL-HLT, Los-Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>M Chodorow</author>
</authors>
<title>Native Judgments of Non-Native Usage: Experiments in Preposition Error Detection.</title>
<date>2008</date>
<booktitle>COLING Workshop on Human Judgments in Computational Linguistics,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="1612" citStr="Tetreault and Chodorow, 2008" startWordPosition="252" endWordPosition="255">he annotation of the corpus. 1 Introduction Work on automated methods for detecting and correcting context dependent mistakes (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic study of a whole range of errors non-native writ</context>
<context position="12539" citStr="Tetreault and Chodorow, 2008" startWordPosition="2014" endWordPosition="2017">vionics engineers...” 4. ”...which reflect a traditional female role and a traditional attitude to a woman...” —4 ”...which reflect a traditional female role and a traditional attitude towards women...” 5. ”Marx lived in the epoch when there were no entertainments.” —4 ”Marx lived in an era when there was no entertainment.” In the examples above, errors interact with one another. In example 1, the context requires a definite article, and the definite article, in turn, calls for the 4Our error classification was inspired by the classification developed for the annotation of preposition errors (Tetreault and Chodorow, 2008a). 30 preposition ”of”. In example 2, the definite article after ”some of” is used extraneously, and deleting it also requires deleting preposition ”of”. Another case of interaction is caused by a word choice error: The writer used the verb ”thought” instead of ”considered”; replacing the verb requires also changing the syntactic construction of the verb complement. In examples 3 and 4, the article choice before the words ”engineer” and ”woman” depends on the number value of those nouns. To correctly determine which article should be used, one needs to determine first whether the context requ</context>
<context position="25474" citStr="Tetreault and Chodorow, 2008" startWordPosition="4195" endWordPosition="4198">o three categories: replacements, insertions, and deletions. Unlike with article errors, the most common type of preposition errors is confusing two prepositions. This category accounts for more than half of all errors, and the breakdown is very similar for all language groups. The fourth category in the table, with original, refers to the preposition usages that were found acceptable by the annotators, but with a better suggestion provided. We distinguish this case as a separate category because preposition usage is highly variable, unlike, for example, article usage. Tetreault and Chodorow (Tetreault and Chodorow, 2008a) show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions. 6 Inter-annotator Agreement Correcting non-native text for a variety of mistakes is challenging and requires a number of decisions on the part of the annotator. Human language allows for many ways to express the same idea. Furthermore, it is possible that the corrected sentence, even when it does not contain clear mistakes, does not sound like a sentence produced by a native speaker. The latter is complicated</context>
<context position="27175" citStr="Tetreault and Chodorow, 2008" startWordPosition="4495" endWordPosition="4498">15% Italian 120 1.8 57% 22% 12% 8% Polish 77 1.7 49% 18% 16% 17% Russian 251 2.3 53% 21% 17% 9% Spanish 165 2.1 55% 20% 19% 6% All 1309 2.1 54% 21% 18% 7% Table 5: Distribution of preposition mistakes by error type and source language of the writer. With orig refers to prepositions judged as acceptable by the annotators, but with a better suggestion provided. 2008; Han et al., 2006; Izumi et al., 2004; Nagata et al., 2006). The output of human annotation is viewed as the gold standard when evaluating an error detection system. The question of reliability of using one rater has been raised in (Tetreault and Chodorow, 2008a), where an extensive reliability study of human judgments in rating preposition usage is described. In particular, it is shown that inter-annotator agreement on preposition correction is low (kappa value of 0.63) and that native speakers do not always agree on whether a specific preposition constitutes acceptable usage. We measure agreement by asking an annotator whether a sentence corrected by another person is correct. After all, our goal was to make the sentence sound native-like, without enforcing that errors are corrected in the same way. One hundred sentences annotated by each person w</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>J. Tetreault and M. Chodorow. 2008. Native Judgments of Non-Native Usage: Experiments in Preposition Error Detection. COLING Workshop on Human Judgments in Computational Linguistics, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>M Chodorow</author>
</authors>
<date>2008</date>
<booktitle>The Ups and Downs of Preposition Error Detection in ESL Writing. COLING,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="1612" citStr="Tetreault and Chodorow, 2008" startWordPosition="252" endWordPosition="255">he annotation of the corpus. 1 Introduction Work on automated methods for detecting and correcting context dependent mistakes (e.g., (Golding and Roth, 1996; Golding and Roth, 1999; Carlson et al., 2001)) has taken an interesting turn over the last few years, and has focused on correcting mistakes made by non-native speakers of English. Nonnative writers make a variety of errors in grammar and word usage. Recently, there has been a lot of effort on building systems for detecting mistakes in article and preposition usage (DeFelice, 2008; EegOlofsson, 2003; Gamon et al., 2008; Han et al., 2006; Tetreault and Chodorow, 2008b). Izumi et al. (2003) consider several error types, including article and preposition mistakes, made by Japanese learners of English, and Nagata et al. (2006) focus on the errors in mass/count noun distinctions with an application to detecting article mistakes also made by Japanese speakers. Article and preposition mistakes have been shown to be very common mistakes for learners of different first language (L1) backgrounds (Dagneaux et al., 1998; Gamon et al., 2008; Izumi et al., 2004; Tetreault and Chodorow, 2008a), but there is no systematic study of a whole range of errors non-native writ</context>
<context position="12539" citStr="Tetreault and Chodorow, 2008" startWordPosition="2014" endWordPosition="2017">vionics engineers...” 4. ”...which reflect a traditional female role and a traditional attitude to a woman...” —4 ”...which reflect a traditional female role and a traditional attitude towards women...” 5. ”Marx lived in the epoch when there were no entertainments.” —4 ”Marx lived in an era when there was no entertainment.” In the examples above, errors interact with one another. In example 1, the context requires a definite article, and the definite article, in turn, calls for the 4Our error classification was inspired by the classification developed for the annotation of preposition errors (Tetreault and Chodorow, 2008a). 30 preposition ”of”. In example 2, the definite article after ”some of” is used extraneously, and deleting it also requires deleting preposition ”of”. Another case of interaction is caused by a word choice error: The writer used the verb ”thought” instead of ”considered”; replacing the verb requires also changing the syntactic construction of the verb complement. In examples 3 and 4, the article choice before the words ”engineer” and ”woman” depends on the number value of those nouns. To correctly determine which article should be used, one needs to determine first whether the context requ</context>
<context position="25474" citStr="Tetreault and Chodorow, 2008" startWordPosition="4195" endWordPosition="4198">o three categories: replacements, insertions, and deletions. Unlike with article errors, the most common type of preposition errors is confusing two prepositions. This category accounts for more than half of all errors, and the breakdown is very similar for all language groups. The fourth category in the table, with original, refers to the preposition usages that were found acceptable by the annotators, but with a better suggestion provided. We distinguish this case as a separate category because preposition usage is highly variable, unlike, for example, article usage. Tetreault and Chodorow (Tetreault and Chodorow, 2008a) show that agreement between two native speakers on a cloze test targeting prepositions is about 76%, which demonstrates that there are many contexts that license multiple prepositions. 6 Inter-annotator Agreement Correcting non-native text for a variety of mistakes is challenging and requires a number of decisions on the part of the annotator. Human language allows for many ways to express the same idea. Furthermore, it is possible that the corrected sentence, even when it does not contain clear mistakes, does not sound like a sentence produced by a native speaker. The latter is complicated</context>
<context position="27175" citStr="Tetreault and Chodorow, 2008" startWordPosition="4495" endWordPosition="4498">15% Italian 120 1.8 57% 22% 12% 8% Polish 77 1.7 49% 18% 16% 17% Russian 251 2.3 53% 21% 17% 9% Spanish 165 2.1 55% 20% 19% 6% All 1309 2.1 54% 21% 18% 7% Table 5: Distribution of preposition mistakes by error type and source language of the writer. With orig refers to prepositions judged as acceptable by the annotators, but with a better suggestion provided. 2008; Han et al., 2006; Izumi et al., 2004; Nagata et al., 2006). The output of human annotation is viewed as the gold standard when evaluating an error detection system. The question of reliability of using one rater has been raised in (Tetreault and Chodorow, 2008a), where an extensive reliability study of human judgments in rating preposition usage is described. In particular, it is shown that inter-annotator agreement on preposition correction is low (kappa value of 0.63) and that native speakers do not always agree on whether a specific preposition constitutes acceptable usage. We measure agreement by asking an annotator whether a sentence corrected by another person is correct. After all, our goal was to make the sentence sound native-like, without enforcing that errors are corrected in the same way. One hundred sentences annotated by each person w</context>
</contexts>
<marker>Tetreault, Chodorow, 2008</marker>
<rawString>J. Tetreault and M. Chodorow. 2008. The Ups and Downs of Preposition Error Detection in ESL Writing. COLING, Manchester, UK.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>