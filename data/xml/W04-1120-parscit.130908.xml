<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000510">
<title confidence="0.998711">
A New Chinese Natural Language Understanding Architecture
Based on Multilayer Search Mechanism
</title>
<author confidence="0.999544">
Wanxiang Che Ting Liu Sheng Li
</author>
<affiliation confidence="0.996986">
School of Computer Science and Technology
Harbin Institute of Technology
</affiliation>
<address confidence="0.9673625">
P.O. Box 321, HIT
Harbin China, 150001
</address>
<email confidence="0.99212">
{car, tliu, ls}@ir.hit.edu.cn
</email>
<sectionHeader confidence="0.979395" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999588294117647">
A classical Chinese Natural Language Under-
standing (NLU) architecture usually includes
several NLU components which are executed
with some mechanism. A new Multilayer Search
Mechanism (MSM) which integrates and quan-
tifies these components into a uniform multi-
layer treelike architecture is presented in this
paper. The mechanism gets the optimal re-
sult with search algorithms. The components
in MSM affect each other. At last, the per-
formance of each component is enhanced. We
built a practical system – CUP (Chinese Under-
standing Platform) based on MSM with three
layers. By the experiments on Word Segmen-
tation, a better performance was achieved. In
theory the normal cascade and feedback mech-
anism are just some special cases of MSM.
</bodyText>
<sectionHeader confidence="0.996286" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999944896551724">
At present a classical Chinese NLU architec-
ture usually includes several components, such
as Word Segmentation (Word-Seg), POS Tag-
ging, Phrase Analysis, Parsing, Word Sense Dis-
ambiguation (WSD) and so on. These compo-
nents are executed one by one from lower layers
(such as Word-Seg, POS Tagging) to higher lay-
ers (such as Parsing, WSD) to form a kind of
cascade mechanism. But when people build a
NLU system based on these complex language
analysis, it is a very serious problem since the
errors of each layer component are multiplied.
With more and more analysis components, the
final result becomes too bad to be applicable.
Another problem is that the components in
the system affect each other when people build a
practical but toy NLU system. Here the toy sys-
tem means that each component is ideal enough
with perfect input. But in fact, on the one hand
the lower layer components need the informa-
tion of higher layer components; on the other
hand the incorrect analysis of lower layers must
reduce the accuracy of higher layers. In Chinese
Word-Seg component, many segmentation am-
biguities which cannot be solved using only lexi-
cal information. In order to improve the perfor-
mance of Word-Seg, we have to use some syntax
and even semantic information. Without cor-
rect Word-Seg results, however the syntax and
semantic parser cannot obtain a correct analy-
sis. It is a chain debts problem.
People have tried to solve the error-multiplied
problem by integrating multi-layers into a uni-
form model (Gao et al., 2001; Nagata, 1994).
But with the increasing number of integrated
layers, the model becomes too complex to build
or solve.
The feedback mechanism (Wu and Jiang,
1998) helps to use the information of high lay-
ers to control the final result. If the analysis
at feedback point cannot be passed, the whole
analysis will be denied. This mechanism places
too much burden on the function of feedback
point. This leads to the problems that a correct
lower layer result may be rejected or an error
result may be accepted.
We propose a new Multilayer Search Mecha-
nism (MSM) to solve the problems mentioned
above. Based on the mechanism, we build a
practical Chinese NLU platform – CUP (Chi-
nese Understanding Platform). Section 2 intro-
duces the background and architecture of the
new mechanism and how to build it up. Exper-
imental results with CUP is given in Section 3.
In Section 4, we discuss why the new mechanism
gets better results than the old ones. Conclu-
sions and the some future work follow in Sec-
tion 5.
</bodyText>
<sectionHeader confidence="0.891702" genericHeader="method">
2 Multilayer Search Mechanism
</sectionHeader>
<bodyText confidence="0.999823166666667">
The novel Multilayer Search Mechanism (MSM)
integrates and quantifies NLU components into
a uniform multilayer treelike platform, such as
Word-Seg, POS Tagging, Parsing and so on.
These components affect each other by comput-
ing the final score and then get better results.
</bodyText>
<subsectionHeader confidence="0.979425">
2.1 Background
</subsectionHeader>
<bodyText confidence="0.999778785714286">
Considering a Chinese sentence, the sen-
tence analysis task can be formally defined
as finding a set of word segmentation se-
quence (W), a POS tagging sequence (POS),
a syntax dependency parsing tree (DP) and
so on which maximize their joint probability
P(W, POS, DP, · · ·). In this paper, we assume
that there are only three layers W, POS and
DP in MSM. It is relatively straightforward,
however, to extend the method to the case for
which there are more than three layers. There-
fore, the sentence analysis task can be described
as finding a triple &lt; W, POS, DP &gt; that max-
imize the joint probability P(W, POS, DP).
</bodyText>
<equation confidence="0.4262265">
&lt; W, POS, DP &gt;= arg max P(W, POS, DP)
W,POS,DP
</equation>
<bodyText confidence="0.963156">
The joint probability distribution
P(W, POS, DP) can be written in the fol-
lowing form using the chain rule of probability:
</bodyText>
<equation confidence="0.7386095">
P(W, POS, DP)
= P(W)P(POSJW)P(DPJW, POS)
</equation>
<bodyText confidence="0.996579222222222">
Where P(W) is considered as the probabil-
ity of the word segmentation layer, P(POSJW)
is the conditional probability of POS Tag-
ging with a given word segmentation result,
P(DPJW, POS) is the conditional probability
of a dependency parsing tree with a given word
segmentation and POS Tagging result similarly.
So the form of &lt; W, POS, DP &gt; can be trans-
formed into:
</bodyText>
<equation confidence="0.994404">
&lt; W, POS, DP &gt;
= arg max P(W, POS, DP)
W,POS,DP
= arg max P(W)P(POSJW)P(DPJW, POS)
W,POS,DP
= arg max log P(W) + log P(POSJW)
W,POS,DP
+ log P(DPJW, POS)
= arg min − log P(W) − log P(POSJW)
W,POS,DP
− log P(DPJW, POS)
</equation>
<bodyText confidence="0.999549285714286">
We consider that each inversion of probability’s
logarithm at the last step of the above equation
is a score given by a component (Such as Word-
Seg, POS Tagging and so on). So at last, we find
an n-tuple &lt; W, POS, DP, · · · &gt; that minimizes
the last score Sn of a sentence analysis result
with n layers. Sn is defined as:
</bodyText>
<equation confidence="0.989632">
Sn = s1 + s2 + ··· + sn (1)
</equation>
<bodyText confidence="0.9926475">
sz denotes the score of the ith layer compo-
nent.
</bodyText>
<subsectionHeader confidence="0.999358">
2.2 The Architecture of Multilayer
Search Mechanism
</subsectionHeader>
<bodyText confidence="0.999755173076923">
Because there are lots of analysis results at each
layer, it’s a combinatorial explosion problem to
find the optimal result. Assuming that each
component produces m results for an input on
average and there are n layers in a NLU system,
the final search space is mn. With the increas-
ing of n, it’s impossible for a system to find the
optimal result in the huge search space.
The classical cascade mechanism uses a
greedy algorithm to solve the problem. It only
keeps the optimal result at each layer. But if
it’s a fault analysis result for the optimal result
at a layer, it’s impossible for this mechanism to
find the final correct analysis result.
To overcome the difficulty, we build a new
Multilayer Search Mechanism (MSM). Different
from the cascade mechanism, MSM maintains a
number of results at each component, so that
the correct analysis should be included in these
results with high probability. Then MSM tries
to use the information of all layer components
to find out the correct analysis result. Different
from the feedback mechanism, the acceptance
of an analysis is not based on a higher layer
components alone. The lower layer components
provide some information to help to find the
correct analysis result as well.
According to the above idea, we design the
architecture of MSM with multilayer treelike
structure. The original input is root and the
several analysis results of the input become
branches. Iterating this progress, we get a big-
ger analysis tree. Figure 1 gives an analysis ex-
ample of a Chinese sentence “他喜爱c丽J鲜
花。” (He likes beautiful flowers). For the in-
put sentence, there are several Word-Seg results
with scores (the lower the better). Then for each
of Word-Seg results, there are several POS Tag-
ging results, too. And for each of POS Tagging
result, the same thing happens. So we get a big
tree structure and the correct analysis result is a
path in the tree from the root to the leaf except
for there is no correct analysis result in some
analysis components.
A search algorithm can be used to find out the
correct analysis result among the lowest score in
the tree. But because each layer cannot give the
exact score in Equation 1 as the standard score
and the ability of analysis are different with
different layers, we should weight every score.
Then the last score is the linear weighted sum
(Equation 2).
</bodyText>
<equation confidence="0.964837">
Sn = w1s1 + w2s2 + · · · + wnsn (2)
</equation>
<bodyText confidence="0.999129777777778">
sz denotes the score of the ith layer compo-
nent which we will introduce in Section 3; wz
denotes the weight of the ith layer components
which we will introduce in the next section.
In order to get the optimal result, all kinds
of tree search algorithms can be used. Here
the BEST-FIRST SEARCH Algorithm (Rus-
sell and Norvig, 1995) is used. Figure 2 shows
the main algorithm steps.
</bodyText>
<figure confidence="0.40626">
他喜爱美丽的鲜花。
</figure>
<figureCaption confidence="0.9120205">
Figure 1: An Example of Multilayer Search
Mechanism
</figureCaption>
<listItem confidence="0.998374166666667">
1. Add the initial node (starting point) to the
queue.
2. Compare the front node to the goal state. If
they match then the solution is found.
3. If they do not match then expand the front
node by adding all the nodes from its links.
4. If all nodes in the queue are expanded then the
goal state is not found (e.g.there is no solution).
Stop.
5. According to Equation 2 evaluate the score of
expanded nodes and reorder the nodes in the
queue.
</listItem>
<subsectionHeader confidence="0.992084">
2.3 Layer Weight
</subsectionHeader>
<bodyText confidence="0.9584044">
We should find out a group of appropriate
w1, w2, · · · , wn in Equation 2 to maximize the
number of the optimal paths in MSM which can
get the correct results. They are expressed by
W*.
</bodyText>
<equation confidence="0.923652">
ObjFun(minSn) (3)
</equation>
<bodyText confidence="0.994742">
Here W* is named as Whole Layer Weight.
ObjFun(*) denotes a function to value the re-
sult that a group of W can get. Here we can con-
sider that the performance of each layer is pro-
portional to the last performance of the whole
system in MSM. So it maybe the F-Score of
Word-Seg, precision of POS Tagging and so on.
min Sn returns the optimal analysis results with
the lowest score.
Here, the F-Score of Word-Seg can be defined
as the harmonic mean of recall and precision of
Word-Seg. That is to say:
</bodyText>
<equation confidence="0.851471142857143">
2 * Seg.Pre * Seg.Rec
Seg.F-Score =
Seg.Pre + Seg.Rec
#words correctly segmented
Seg.Pre =
#words segmented
Seg.Rec =
</equation>
<bodyText confidence="0.995014071428572">
#words correctly segmented
#words in input texts
Finding out the most suitable group of W
is an optimization problem. Genetic Algo-
rithms (GAs) (Mitchell, 1996) is just an adap-
tive heuristic search algorithm based on the evo-
lutionary ideas of natural selection and genetics
to solve optimization problems. It exploits his-
torical information to direct the search into the
region of better performance within the search
space.
To use GAs to solve optimization prob-
lems (Wall, 1996) the following three questions
should be answered:
</bodyText>
<listItem confidence="0.9954555">
1. How to describ genome?
2. What is the objective function?
3. Which controlling parameters to be se-
lected?
</listItem>
<figure confidence="0.9777312">
Word-Seg
POS Tag
108.3
他 喜爱 美丽 的 鲜花。
r v a u n
87.3
他 喜 爱美 丽 的 鲜花。
187.4
他 喜爱 美丽 的 鲜花。
b v a u n
56.2
他 喜爱 美丽 的 鲜花。
W* = arg max
W
6. Go to step 2. A solution to a problem is represented as a
</figure>
<figureCaption confidence="0.866655333333333">
genome. The genetic algorithm then creates a
Figure 2: BEST-FIRST SEARCH Algorithm population of solutions and applies genetic op-
erators such as mutation and crossover to evolve
</figureCaption>
<bodyText confidence="0.99986852631579">
the solutions in order to find the best one(s) af-
ter several generations. The numbers of popula-
tion and generation are given by controlling pa-
rameters. The objective function decides which
solution is better than others.
In MSM, the genome is just the group of W
which can be denoted by real numbers between
0 and 1. Because the result is a linear weighted
sum, we should normalize the weights to let w1+
w2 +• • •+wn = 1. The objective function is just
ObjFun(*) in Equation 3. Here the F-Score
of Word-Seg is used to describe it. We set the
genetic generations as 10 and the populations in
one generation as 30. The Whole Layer Weight
shows in the row of WLW in Table 4. The F-
Score of Word-Seg shows as Table 3.
We can see that the Word-Seg layer gets an
obviously large weight. So the final result is
inclined to the result of Word-Seg.
</bodyText>
<subsectionHeader confidence="0.985658">
2.4 Self Confidence
</subsectionHeader>
<bodyText confidence="0.999947178571429">
Our analysis indicates that the method of
weighting a whole layer uniformly cannot re-
flect the individual information of each sen-
tence to some component. So the F-Score of
Word-Seg drops somewhat comparing with us-
ing Only Word-Seg. For example, the most
sentences which have ambiguities in Word-Seg
component are still weighted high with Word-
Seg layer weight. Then the final result may still
be the same as the result of Word-Seg compo-
nent. It is ambiguous, too. So we must use a
parameter to decrease the weight of a compo-
nent with ambiguity. It is used to describe the
analysis ability of a component for an input. We
name it as Self Confident (SC) of a component.
It is described by the difference between the first
and the second score of a component. Then the
bigger SC of a component, the larger weight of
it.
There are lots of methods to value the differ-
ence between two numbers. So there are many
kinds of definitions of SC. We use A and B to
denote the first and the second score of a compo-
nent respectively. Then the SC can be defined
as B − A, BA and so on. We must select the bet-
ter one to represent SC. The better means that
a method which gets a lower Error Rate with a
threshold t* which gets the Minimal Error Rate.
</bodyText>
<equation confidence="0.98526">
t* = arg min
t
</equation>
<bodyText confidence="0.8823835">
ErrRate(t) denotes the Error Rate with the
threshold t. An error has two definitions:
</bodyText>
<listItem confidence="0.9917995">
• SC is higher than t but the first result is
fault
• SC is lower than t but the first result is
right
</listItem>
<bodyText confidence="0.969351733333333">
Then the Error Rate is the ratio between the
error number and the total number of sentences.
Table 2 is the comparison list between differ-
ent definitions of SC and their Minimal Error
Rate of Word-Seg. By this table we select B−A
as the last SC because it gets the minimal Min-
imal Error Rate within the different definitions
of SC.
SC is added into Equation 2 to describe the
individual information of each sentence inten-
sively. Equation 4 shows the new score method
of a path.
Sn = w1sc1s1 + w2sc2s2 + • • • + wnscnsn (4)
sci denotes the SC of a component in the ith
layer.
</bodyText>
<sectionHeader confidence="0.99697" genericHeader="method">
3 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999531">
3.1 Score of Components
</subsectionHeader>
<bodyText confidence="0.999893777777778">
We build a practical system CUP (Chinese
Understanding Platform) based on MSM with
three layers – Word-Seg, POS Tagging and
Parsing. Each component not only provides the
n-best analysis result, but also the score of each
result.
In the Word-Seg component, we use the uni-
gram model (Liu et al., 1998) to value different
results of Word-Seg. So the score of a result is:
</bodyText>
<equation confidence="0.7281825">
�
ScoreWord−Seg = −log P(W) = − log P(wi)
</equation>
<bodyText confidence="0.999818833333333">
wi denotes the ith word in the Word-Seg re-
sult of a sentence.
In the POS Tagging component the classical
Markov Model (Manning and Sch¨utze, 1999) is
used to select the n-best POS results of each
Word-Seg result. So the score of a result is:
</bodyText>
<equation confidence="0.993574375">
ScorePOS = − log P(POS|W)
P(W |POS)P(POS)
= − log
P(W)
� �
= − log P(wi|ti) − log P(ti|ti−1)
+ log P(W)
ErrRate(t)
</equation>
<bodyText confidence="0.9999328">
ti denotes the POS of the ith word in a Word-
Seg result of a sentence.
In the Parsing component, we use a Chinese
Dependency Parser System developed by HIT-
IRLab1. The score of a result is:
</bodyText>
<equation confidence="0.993585166666667">
ScoreParsing = − log P(DP|W, POS)
P(W, POS, DP)
= − log
P(W, POS)
= − log P(lij)
+ log P(W, POS)
</equation>
<bodyText confidence="0.9987046">
lij denotes a link between the ith and jth
word in a Word-Seg and POS Tagging result
of a sentence.
Table 1 gives the one and five-best results of
each component with a correct input. The test
data comes from Beijing Univ. and Fujitsu Chi-
nese corpus (Huiming et al., 2000). The F-Score
is used to value the performance of the Word-
Seg, Precision to POS Tagging and the correct
rate of links to Parsing.
</bodyText>
<tableCaption confidence="0.951341">
Table 1: The five-best results of each compo-
nent
</tableCaption>
<table confidence="0.99772375">
1-best 5-best
Word-Seg 87.83% 94.45%
POS Tag 85.34% 93.28%
Parsing 80.25% 82.13%
</table>
<bodyText confidence="0.998114090909091">
Chinese corpus (Huiming et al., 2000). In CUP
the five-best results of each component are se-
lected. Table 3 lists the F-Score of Word-Seg.
They use Only Word-Seg (OWS), Whole Layer
Weight (WLW), SC (SC) and FeedBack mecha-
nism (FB) separately. Using the feedback mech-
anism means that the last analysis result of a
sentence is decided by the Parsing. We select
the result which has the lowest score of Pars-
ing. Table 4 shows the weight distributions in
WLW and SC weighting methods.
</bodyText>
<subsectionHeader confidence="0.997596">
3.4 The Efficiency of CUP
</subsectionHeader>
<bodyText confidence="0.99993">
The efficiency test of CUP was done with 7112
sentences with 20 Chinese characters averagely.
It costs 58.97 seconds on a PC with PIV 2.0
CPU and 512M memory. The average cost of a
sentence is 0.0083 second.
</bodyText>
<sectionHeader confidence="0.99509" genericHeader="method">
4 Discussions
</sectionHeader>
<bodyText confidence="0.999711818181818">
According to Table 1, we can see that the per-
formance of each component improved with the
increasing of the number of results. But at the
same time, the processing time must increase.
So we should balance the efficiency and effec-
tiveness with an appropriate number of results.
Thus, it’s more possible for CUP to find out the
correct analysis than the original cascade mech-
anism if we can invent an appropriate method.
We define SC as B − A which gets the mini-
mal Minimal Error Rate with the analysis of the
</bodyText>
<subsectionHeader confidence="0.999801">
3.2 Self Confidence Selection
</subsectionHeader>
<bodyText confidence="0.999989888888889">
In order to select a better SC, we test all kinds of
definition form to calculate their Minimal Error
Rate. For example B−A, BA and so on. A and B
denote the first and the second score of a com-
ponent respectively. Table 2 shows the relation-
ship between definition forms of SC and their
Minimal Error Rate. Here, we experimented
with the first and the second Word-Seg results
of more than 7100 Chinese sentences.
</bodyText>
<subsectionHeader confidence="0.999205">
3.3 F-Score of Word-Seg
</subsectionHeader>
<bodyText confidence="0.999892428571429">
The result of Word-Seg is used to test our
system’s performance, which means that the
ObjFun(*) returns the F-Score of Word-Seg.
There are 1,500 sentences as training data
and 500 sentences as test data. Among these
data about 10% sentences have ambiguities and
the others come from Beijing Univ. and Fujitsu
</bodyText>
<footnote confidence="0.98344">
1The Parser has not been published still.
</footnote>
<tableCaption confidence="0.981806">
Table 2: SC and Minimal Error Rate
</tableCaption>
<table confidence="0.999721692307692">
Definition Form of SC Minimal
Error Rate
1 1 23.85%
A − B
B − A 21.07%
B 23.98%
A
B A 23.98%
A −B
B−A 24.12%
length of a sentence
B−A 23.71%
length of a sentence+100
</table>
<tableCaption confidence="0.992895">
Table 4: Layer Weight
</tableCaption>
<table confidence="0.997383333333333">
1-layer 2-layer 3-layer
In WLW 0.84 0.12 0.04
In SC 0.44 0.40 0.16
</table>
<tableCaption confidence="0.928951">
Table 3: F-Score of Word-Seg
</tableCaption>
<table confidence="0.9823565">
OWS WLW SC FB
F-Score 86.99% 85.80% 88.13% 80.72%
</table>
<tableCaption confidence="0.998533">
Table 2. Take the case of Word Segmentation:
</tableCaption>
<equation confidence="0.5528115">
B − A = � log P(W�Z ) − � log P(WB )
Z 9
</equation>
<bodyText confidence="0.999652136363636">
It’s just the difference between logarithms of
different word results’ probability of the first
and the second result of Word Segmentation.
Table 3 shows that MSM using SC gets a bet-
ter performance than other methods. For a Chi-
nese sentence “桌子下放着几坛酒。”. (There
are some drinks under the table). The CUP
gets the correct analysis – “桌子/n 下/nd 放/v
着/u 几/m 坛/q 酒/n 。/w”. But the cascade
and feedback mechanism’s result is “桌子/n 下
放/v 着/u 几/m 坛/q 酒/n 。/w”.
The cascade mechanism uses the Only Word-
Seg result. In this method P(下放) is more
than P(下) ∗ P(放). At the same time, the
wrong analysis is a grammatical sentence and
is accepted by Parsing. These create that these
two mechanisms cannot get the correct result.
But the MSM synthesizes all the information of
Word-Seg, POS Tagging and Parsing. Finally
it gets the correct analysis result.
Now, CUP integrates three layers and its effi-
ciency is high enough for practical applications.
</bodyText>
<sectionHeader confidence="0.993879" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999967425531915">
A new Chinese NLU architecture based on Mul-
tilayer Search Mechanism (MSM) integrates al-
most all of NLU components into a uniform
multilayer treelike platform and quantifies these
components to use the search algorithm to find
out the optimal result. Thus any component
can be added into MSM conveniently. They
only need to accept an input and give several
outputs with scores. By experiments we can see
that a practical system – CUP based on MSM
improves the performance of Word-Seg to a cer-
tain extent. And its efficiency is high enough for
most practical applications.
The cascade and the feedback mechanism are
JUST the special cases of MSM. If greedy algo-
rithm is used at each layer to expand the result
with the lowest score, MSM becomes the cas-
cade mechanism. If the weight of each layer
except the feedback point is set 0, the MSM be-
comes the feedback mechanism.
In the future we are going to add the Phrase
Analysis, WSD (Word Sense Disambiguation)
and Semantic Analysis components into CUP,
because it is impossible to analyze some sen-
tences correctly without semantic understand-
ing and the Phrase Analysis helps to en-
hance the performance of Parsing. At last,
CUP becomes a whole Chinese NLU platform
with Word-Seg, POS Tagging, Phrase Analy-
sis, Parsing, WSD and Semantic Analysis, six
components from lower layers to higher layers.
Under the framework of MSM, it becomes very
easy to add these components.
With the increasing of layers the handle speed
must decrease. So some heuristic search algo-
rithms will be used to improve the speed of
searching while enhancing the speed of each
component. Under the MSM framework, we can
do these easily.
The performance of each component should
be improved in the future. At least, it is impos-
sible for MSM to find out the correct analysis
result if there is a component which cannot give
a correct result within n-best results with a cor-
rect input. In addition, we are going to evalu-
ate the performance of each component not just
Word-Seg only.
</bodyText>
<sectionHeader confidence="0.998427" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999730714285714">
We thank Liqi Gao and Zhuoran Wang provide
the Word Segmentation tool, Wei He provide
the POS Tagging tool and Jinshan Ma provide
the Parser tool for us. We acknowledge Dekang
Lin for his valuable comments on the earlier ver-
sions of this paper. This work was supported by
NSFC 60203020.
</bodyText>
<sectionHeader confidence="0.998011" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993037925">
Shan Gao, Yan Zhang, Bo Xu, ChengQing
Zong, ZhaoBing Han, and RangShen Zhang.
2001. The research on integrated chinese
words segmentation and labeling based on tri-
gram statistic model. In Proceedings of IJCL-
2001, Tai Yuan, Shan Xi, China.
Duan Huiming, Song Jing, Xu Guowei,
Hu Guoxin, and Yu Shiwen. 2000. The de-
velopment of a large-scale tagged chinese cor-
pus and its applications. Applied Linguistics,
(2):72–77.
Ting Liu, Yan Wu, and Kaizhu Wang. 1998.
The problem and algorithm of maximal prob-
ability word segmentation. Journal of Harbin
Institute of Technology, 30(6):37–41.
Christopher D. Manning and Hinrich Sch¨utze.
1999. Foundations of Statistical Natural Lan-
guage Processing. The MIT Press, Cam-
bridge, Massachusetts.
Melanie Mitchell. 1996. An Introduction to
Genetic Algorithms. The MIT Press, Cam-
bridge, Massachusetts.
Masaaki Nagata. 1994. A stochastic japanese
morphological analyzer using a forward-dp
backward-A* n-best search algorithm. In
Proceedings of the 15th International Con-
ference on Computational Linguistics, pages
201–207.
Stuart Russell and Peter Norvig. 1995. Artifi-
cial Intelligence: A Modern Approach. Pren-
tice Hall Series in Artificial Intelligence, En-
glewood Cliffs, NJ, USA.
Matthew Wall. 1996. GAlib: A C++ Li-
brary of Genetic Algorithms components.
http://lancet.mit.edu/ga/.
Andi Wu and Zixin Jiang. 1998. Word segmen-
tation in sentence analysis. In Proceedings
of the 1998 International Conference on Chi-
nese Information Processing, pages 169–180,
Beijing, China.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.444250">
<title confidence="0.982996">A New Chinese Natural Language Understanding Based on Multilayer Search Mechanism</title>
<author confidence="0.996767">Wanxiang Che Ting Liu Sheng Li</author>
<affiliation confidence="0.9853555">School of Computer Science and Harbin Institute of</affiliation>
<address confidence="0.712748">P.O. Box 321, Harbin China,</address>
<email confidence="0.995286">tliu,</email>
<abstract confidence="0.997638888888889">A classical Chinese Natural Language Understanding (NLU) architecture usually includes several NLU components which are executed with some mechanism. A new Multilayer Search Mechanism (MSM) which integrates and quantifies these components into a uniform multilayer treelike architecture is presented in this paper. The mechanism gets the optimal result with search algorithms. The components in MSM affect each other. At last, the performance of each component is enhanced. We built a practical system – CUP (Chinese Understanding Platform) based on MSM with three layers. By the experiments on Word Segmentation, a better performance was achieved. In theory the normal cascade and feedback mechanism are just some special cases of MSM.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shan Gao</author>
<author>Yan Zhang</author>
<author>Bo Xu</author>
<author>ChengQing Zong</author>
<author>ZhaoBing Han</author>
<author>RangShen Zhang</author>
</authors>
<title>The research on integrated chinese words segmentation and labeling based on trigram statistic model.</title>
<date>2001</date>
<booktitle>In Proceedings of IJCL2001,</booktitle>
<location>Tai Yuan, Shan Xi, China.</location>
<contexts>
<context position="2545" citStr="Gao et al., 2001" startWordPosition="410" endWordPosition="413">mation of higher layer components; on the other hand the incorrect analysis of lower layers must reduce the accuracy of higher layers. In Chinese Word-Seg component, many segmentation ambiguities which cannot be solved using only lexical information. In order to improve the performance of Word-Seg, we have to use some syntax and even semantic information. Without correct Word-Seg results, however the syntax and semantic parser cannot obtain a correct analysis. It is a chain debts problem. People have tried to solve the error-multiplied problem by integrating multi-layers into a uniform model (Gao et al., 2001; Nagata, 1994). But with the increasing number of integrated layers, the model becomes too complex to build or solve. The feedback mechanism (Wu and Jiang, 1998) helps to use the information of high layers to control the final result. If the analysis at feedback point cannot be passed, the whole analysis will be denied. This mechanism places too much burden on the function of feedback point. This leads to the problems that a correct lower layer result may be rejected or an error result may be accepted. We propose a new Multilayer Search Mechanism (MSM) to solve the problems mentioned above. B</context>
</contexts>
<marker>Gao, Zhang, Xu, Zong, Han, Zhang, 2001</marker>
<rawString>Shan Gao, Yan Zhang, Bo Xu, ChengQing Zong, ZhaoBing Han, and RangShen Zhang. 2001. The research on integrated chinese words segmentation and labeling based on trigram statistic model. In Proceedings of IJCL2001, Tai Yuan, Shan Xi, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duan Huiming</author>
<author>Song Jing</author>
<author>Xu Guowei</author>
<author>Hu Guoxin</author>
<author>Yu Shiwen</author>
</authors>
<title>The development of a large-scale tagged chinese corpus and its applications. Applied Linguistics,</title>
<date>2000</date>
<contexts>
<context position="15079" citStr="Huiming et al., 2000" startWordPosition="2714" endWordPosition="2717">(W) � � = − log P(wi|ti) − log P(ti|ti−1) + log P(W) ErrRate(t) ti denotes the POS of the ith word in a WordSeg result of a sentence. In the Parsing component, we use a Chinese Dependency Parser System developed by HITIRLab1. The score of a result is: ScoreParsing = − log P(DP|W, POS) P(W, POS, DP) = − log P(W, POS) = − log P(lij) + log P(W, POS) lij denotes a link between the ith and jth word in a Word-Seg and POS Tagging result of a sentence. Table 1 gives the one and five-best results of each component with a correct input. The test data comes from Beijing Univ. and Fujitsu Chinese corpus (Huiming et al., 2000). The F-Score is used to value the performance of the WordSeg, Precision to POS Tagging and the correct rate of links to Parsing. Table 1: The five-best results of each component 1-best 5-best Word-Seg 87.83% 94.45% POS Tag 85.34% 93.28% Parsing 80.25% 82.13% Chinese corpus (Huiming et al., 2000). In CUP the five-best results of each component are selected. Table 3 lists the F-Score of Word-Seg. They use Only Word-Seg (OWS), Whole Layer Weight (WLW), SC (SC) and FeedBack mechanism (FB) separately. Using the feedback mechanism means that the last analysis result of a sentence is decided by the </context>
</contexts>
<marker>Huiming, Jing, Guowei, Guoxin, Shiwen, 2000</marker>
<rawString>Duan Huiming, Song Jing, Xu Guowei, Hu Guoxin, and Yu Shiwen. 2000. The development of a large-scale tagged chinese corpus and its applications. Applied Linguistics, (2):72–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ting Liu</author>
<author>Yan Wu</author>
<author>Kaizhu Wang</author>
</authors>
<title>The problem and algorithm of maximal probability word segmentation.</title>
<date>1998</date>
<journal>Journal of Harbin Institute of Technology,</journal>
<volume>30</volume>
<issue>6</issue>
<contexts>
<context position="14053" citStr="Liu et al., 1998" startWordPosition="2513" endWordPosition="2516">ifferent definitions of SC. SC is added into Equation 2 to describe the individual information of each sentence intensively. Equation 4 shows the new score method of a path. Sn = w1sc1s1 + w2sc2s2 + • • • + wnscnsn (4) sci denotes the SC of a component in the ith layer. 3 Experimental Results 3.1 Score of Components We build a practical system CUP (Chinese Understanding Platform) based on MSM with three layers – Word-Seg, POS Tagging and Parsing. Each component not only provides the n-best analysis result, but also the score of each result. In the Word-Seg component, we use the unigram model (Liu et al., 1998) to value different results of Word-Seg. So the score of a result is: � ScoreWord−Seg = −log P(W) = − log P(wi) wi denotes the ith word in the Word-Seg result of a sentence. In the POS Tagging component the classical Markov Model (Manning and Sch¨utze, 1999) is used to select the n-best POS results of each Word-Seg result. So the score of a result is: ScorePOS = − log P(POS|W) P(W |POS)P(POS) = − log P(W) � � = − log P(wi|ti) − log P(ti|ti−1) + log P(W) ErrRate(t) ti denotes the POS of the ith word in a WordSeg result of a sentence. In the Parsing component, we use a Chinese Dependency Parser </context>
</contexts>
<marker>Liu, Wu, Wang, 1998</marker>
<rawString>Ting Liu, Yan Wu, and Kaizhu Wang. 1998. The problem and algorithm of maximal probability word segmentation. Journal of Harbin Institute of Technology, 30(6):37–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie Mitchell</author>
</authors>
<title>An Introduction to Genetic Algorithms.</title>
<date>1996</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="9987" citStr="Mitchell, 1996" startWordPosition="1749" endWordPosition="1750">each layer is proportional to the last performance of the whole system in MSM. So it maybe the F-Score of Word-Seg, precision of POS Tagging and so on. min Sn returns the optimal analysis results with the lowest score. Here, the F-Score of Word-Seg can be defined as the harmonic mean of recall and precision of Word-Seg. That is to say: 2 * Seg.Pre * Seg.Rec Seg.F-Score = Seg.Pre + Seg.Rec #words correctly segmented Seg.Pre = #words segmented Seg.Rec = #words correctly segmented #words in input texts Finding out the most suitable group of W is an optimization problem. Genetic Algorithms (GAs) (Mitchell, 1996) is just an adaptive heuristic search algorithm based on the evolutionary ideas of natural selection and genetics to solve optimization problems. It exploits historical information to direct the search into the region of better performance within the search space. To use GAs to solve optimization problems (Wall, 1996) the following three questions should be answered: 1. How to describ genome? 2. What is the objective function? 3. Which controlling parameters to be selected? Word-Seg POS Tag 108.3 他 喜爱 美丽 的 鲜花。 r v a u n 87.3 他 喜 爱美 丽 的 鲜花。 187.4 他 喜爱 美丽 的 鲜花。 b v a u n 56.2 他 喜爱 美丽 的 鲜花。 W* = </context>
</contexts>
<marker>Mitchell, 1996</marker>
<rawString>Melanie Mitchell. 1996. An Introduction to Genetic Algorithms. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A stochastic japanese morphological analyzer using a forward-dp backward-A* n-best search algorithm.</title>
<date>1994</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>201--207</pages>
<contexts>
<context position="2560" citStr="Nagata, 1994" startWordPosition="414" endWordPosition="415">ayer components; on the other hand the incorrect analysis of lower layers must reduce the accuracy of higher layers. In Chinese Word-Seg component, many segmentation ambiguities which cannot be solved using only lexical information. In order to improve the performance of Word-Seg, we have to use some syntax and even semantic information. Without correct Word-Seg results, however the syntax and semantic parser cannot obtain a correct analysis. It is a chain debts problem. People have tried to solve the error-multiplied problem by integrating multi-layers into a uniform model (Gao et al., 2001; Nagata, 1994). But with the increasing number of integrated layers, the model becomes too complex to build or solve. The feedback mechanism (Wu and Jiang, 1998) helps to use the information of high layers to control the final result. If the analysis at feedback point cannot be passed, the whole analysis will be denied. This mechanism places too much burden on the function of feedback point. This leads to the problems that a correct lower layer result may be rejected or an error result may be accepted. We propose a new Multilayer Search Mechanism (MSM) to solve the problems mentioned above. Based on the mec</context>
</contexts>
<marker>Nagata, 1994</marker>
<rawString>Masaaki Nagata. 1994. A stochastic japanese morphological analyzer using a forward-dp backward-A* n-best search algorithm. In Proceedings of the 15th International Conference on Computational Linguistics, pages 201–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Russell</author>
<author>Peter Norvig</author>
</authors>
<title>Artificial Intelligence: A Modern Approach.</title>
<date>1995</date>
<booktitle>Series in Artificial Intelligence,</booktitle>
<publisher>Prentice Hall</publisher>
<location>Englewood Cliffs, NJ, USA.</location>
<contexts>
<context position="8434" citStr="Russell and Norvig, 1995" startWordPosition="1460" endWordPosition="1464">ore in the tree. But because each layer cannot give the exact score in Equation 1 as the standard score and the ability of analysis are different with different layers, we should weight every score. Then the last score is the linear weighted sum (Equation 2). Sn = w1s1 + w2s2 + · · · + wnsn (2) sz denotes the score of the ith layer component which we will introduce in Section 3; wz denotes the weight of the ith layer components which we will introduce in the next section. In order to get the optimal result, all kinds of tree search algorithms can be used. Here the BEST-FIRST SEARCH Algorithm (Russell and Norvig, 1995) is used. Figure 2 shows the main algorithm steps. 他喜爱美丽的鲜花。 Figure 1: An Example of Multilayer Search Mechanism 1. Add the initial node (starting point) to the queue. 2. Compare the front node to the goal state. If they match then the solution is found. 3. If they do not match then expand the front node by adding all the nodes from its links. 4. If all nodes in the queue are expanded then the goal state is not found (e.g.there is no solution). Stop. 5. According to Equation 2 evaluate the score of expanded nodes and reorder the nodes in the queue. 2.3 Layer Weight We should find out a group o</context>
</contexts>
<marker>Russell, Norvig, 1995</marker>
<rawString>Stuart Russell and Peter Norvig. 1995. Artificial Intelligence: A Modern Approach. Prentice Hall Series in Artificial Intelligence, Englewood Cliffs, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Wall</author>
</authors>
<title>GAlib: A C++ Library of Genetic Algorithms components.</title>
<date>1996</date>
<note>http://lancet.mit.edu/ga/.</note>
<contexts>
<context position="10306" citStr="Wall, 1996" startWordPosition="1801" endWordPosition="1802">hat is to say: 2 * Seg.Pre * Seg.Rec Seg.F-Score = Seg.Pre + Seg.Rec #words correctly segmented Seg.Pre = #words segmented Seg.Rec = #words correctly segmented #words in input texts Finding out the most suitable group of W is an optimization problem. Genetic Algorithms (GAs) (Mitchell, 1996) is just an adaptive heuristic search algorithm based on the evolutionary ideas of natural selection and genetics to solve optimization problems. It exploits historical information to direct the search into the region of better performance within the search space. To use GAs to solve optimization problems (Wall, 1996) the following three questions should be answered: 1. How to describ genome? 2. What is the objective function? 3. Which controlling parameters to be selected? Word-Seg POS Tag 108.3 他 喜爱 美丽 的 鲜花。 r v a u n 87.3 他 喜 爱美 丽 的 鲜花。 187.4 他 喜爱 美丽 的 鲜花。 b v a u n 56.2 他 喜爱 美丽 的 鲜花。 W* = arg max W 6. Go to step 2. A solution to a problem is represented as a genome. The genetic algorithm then creates a Figure 2: BEST-FIRST SEARCH Algorithm population of solutions and applies genetic operators such as mutation and crossover to evolve the solutions in order to find the best one(s) after several generatio</context>
</contexts>
<marker>Wall, 1996</marker>
<rawString>Matthew Wall. 1996. GAlib: A C++ Library of Genetic Algorithms components. http://lancet.mit.edu/ga/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andi Wu</author>
<author>Zixin Jiang</author>
</authors>
<title>Word segmentation in sentence analysis.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1998 International Conference on Chinese Information Processing,</booktitle>
<pages>169--180</pages>
<location>Beijing, China.</location>
<contexts>
<context position="2707" citStr="Wu and Jiang, 1998" startWordPosition="436" endWordPosition="439">onent, many segmentation ambiguities which cannot be solved using only lexical information. In order to improve the performance of Word-Seg, we have to use some syntax and even semantic information. Without correct Word-Seg results, however the syntax and semantic parser cannot obtain a correct analysis. It is a chain debts problem. People have tried to solve the error-multiplied problem by integrating multi-layers into a uniform model (Gao et al., 2001; Nagata, 1994). But with the increasing number of integrated layers, the model becomes too complex to build or solve. The feedback mechanism (Wu and Jiang, 1998) helps to use the information of high layers to control the final result. If the analysis at feedback point cannot be passed, the whole analysis will be denied. This mechanism places too much burden on the function of feedback point. This leads to the problems that a correct lower layer result may be rejected or an error result may be accepted. We propose a new Multilayer Search Mechanism (MSM) to solve the problems mentioned above. Based on the mechanism, we build a practical Chinese NLU platform – CUP (Chinese Understanding Platform). Section 2 introduces the background and architecture of t</context>
</contexts>
<marker>Wu, Jiang, 1998</marker>
<rawString>Andi Wu and Zixin Jiang. 1998. Word segmentation in sentence analysis. In Proceedings of the 1998 International Conference on Chinese Information Processing, pages 169–180, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>