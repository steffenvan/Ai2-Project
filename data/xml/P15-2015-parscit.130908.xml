<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007059">
<title confidence="0.984416">
Spectral Semi-Supervised Discourse Relation Classification
</title>
<author confidence="0.993631">
Robert Fisher
</author>
<affiliation confidence="0.984279">
Carnegie Mellon University
</affiliation>
<address confidence="0.785252">
5000 Forbes Ave
Pittsburgh, PA 15213
</address>
<email confidence="0.998561">
rwfisher@cs.cmu.edu
</email>
<author confidence="0.939551">
Reid Simmons
</author>
<affiliation confidence="0.934861">
Carnegie Mellon University
</affiliation>
<address confidence="0.7547185">
5000 Forbes Ave
Pittsburgh, PA 15213
</address>
<email confidence="0.998842">
reids@cs.cmu.edu
</email>
<sectionHeader confidence="0.993895" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998917954545455">
Discourse parsing is the process of dis-
covering the latent relational structure of
a long form piece of text and remains a
significant open challenge. One of the
most difficult tasks in discourse parsing is
the classification of implicit discourse re-
lations. Most state-of-the-art systems do
not leverage the great volume of unlabeled
text available on the web–they rely instead
on human annotated training data. By in-
corporating a mixture of labeled and unla-
beled data, we are able to improve relation
classification accuracy, reduce the need for
annotated data, while still retaining the ca-
pacity to use labeled data to ensure that
specific desired relations are learned. We
achieve this using a latent variable model
that is trained in a reduced dimensionality
subspace using spectral methods. Our ap-
proach achieves an F1 score of 0.485 on
the implicit relation labeling task for the
Penn Discourse Treebank.
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965160714286">
Discourse parsing is a fundamental task in natural
language processing that entails the discovery of
the latent relational structure in a multi-sentence
piece of text. Unlike semantic and syntactic pars-
ing, which are used for single sentence pars-
ing, discourse parsing is used to discover inter-
sentential relations in longer pieces of text. With-
out discourse, parsing methods can only be used to
understand documents as sequences of unrelated
sentences.
Unfortunately, manual annotation of discourse
structure in text is costly and time consuming.
Multiple annotators are required for each relation
to estimate inter-annotator agreement. The Penn
Discourse Treebank (PDTB) (Prasad et al., 2008).
is one of the largest annotated discourse parsing
datasets, with 16,224 implicit relations. However,
this pales in comparison to unlabeled datasets that
can include millions of sentences of text. By aug-
menting a labeled dataset with unlabeled data, we
can use a bootstrapping framework to improve
predictive accuracy, and reduce the need for la-
beled data–which could make it much easier to
port discourse parsing algorithms to new domains.
On the other hand, a fully unsupervised parser may
not be desirable because in many applications spe-
cific discourse relations must be identified, which
would be difficult to achieve without the use of la-
beled examples.
There has recently been growing interest in a
breed of algorithms based on spectral decomposi-
tion, which are well suited to training with unla-
beled data. Spectral algorithms utilize matrix fac-
torization algorithms such as Singular Value De-
composition (SVD) and rank factorization to dis-
cover low-rank decompositions of matrices or ten-
sors of empirical moments. In many models, these
decompositions allow us to identify the subspace
spanned by a group of parameter vectors or the
actual parameter vectors themselves. For tasks
where they can be applied, spectral methods pro-
vide statistically consistent results that avoid lo-
cal maxima. Also, spectral algorithms tend to
be much faster—sometimes orders of magnitude
faster—than competing approaches, which makes
them ideal for tackling large datasets. These meth-
ods can be viewed as inferring something about
the latent structure of a domain—for example, in a
hidden Markov model, the number of latent states
and the sparsity pattern of the transition matrix are
forms of latent structure, and spectral methods can
recover both in the limit.
This paper presents a semi-supervised spectral
model for a sequential relation labeling task for
discourse parsing. Besides the theoretically desir-
able properties mentioned above, we also demon-
</bodyText>
<page confidence="0.992637">
89
</page>
<bodyText confidence="0.8775184">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 89–93,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
strate the practical advantages of the model with
an empirical evaluation on the Penn Discourse
Treebank (PDTB) (Prasad et al., 2008) dataset,
which yields an F1 score of 0.485. This accuracy
shows a 7-9 percentage point improvement over
approaches that do not utilize unlabeled training
data.
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999987">
There has been quite a bit of work concerning
fully supervised relation classification with the
PDTB (Lin et al., 2014; Feng and Hirst, 2012;
Webber et al., 2012). Semi-supervised relation
classification is much less common however. One
recent example of an attempt to leverage unla-
beled data appears in (Hernault et al., 2011),
which showed that moderate classification accu-
racy can be achieved with very small labeled
datasets. However, this approach is not compet-
itive with fully supervised classifiers when more
training data is available. Recently there has
also been some work to use Conditional Random
Fields (CRFs) to represent the global properties of
a parse sequence (Joty et al., 2013; Feng and Hirst,
2014), though this work has focused on the RST-
DT corpus, rather than the PDTB.
In addition to requiring a fully supervised train-
ing set, most existing discourse parsers use non-
spectral optimization that is often slow and inex-
act. However, there has been some work in other
parsing tasks to employ spectral methods in both
supervised and semi-supervised settings (Parikh et
al., 2014; Cohen et al., 2014). Spectral methods
have also been applied very successfully in many
non-linguistic domains (Hsu et al., 2012; Boots
and Gordon, 2010; Fisher et al., 2014).
</bodyText>
<sectionHeader confidence="0.883223" genericHeader="method">
3 Problem Definition and Dataset
</sectionHeader>
<bodyText confidence="0.999969734375001">
This section defines the discourse parsing prob-
lem and discusses the characteristics of the PDTB.
The PDTB consists of annotated articles from the
Wall Street Journal and is used in our empiri-
cal evaluations. This is combined with the New
York Times Annotated Corpus (Sandhaus, 2008),
which includes 1.8 million New York Times arti-
cles printed between 1987 and 2007.
Discourse parsing can be reduced to three sepa-
rate tasks. First, the text must be decomposed into
elementary discourse units (EDUs), which may or
may not coincide with sentence boundaries. The
EDUs are often independent clauses that may be
connected with conjunctions. After the text has
been partitioned into EDUs, the discourse struc-
ture must be identified. This requires us to iden-
tify all pairs of EDUs that will be connected with
some discourse relation. These relational links in-
duce the skeletal structure of the discourse parse
tree. Finally, each connection identified in the pre-
vious step must be labeled using a known set of
relations. Examples of these discourse relations
include concession, causal, and instantiation rela-
tions. In the PDTB, only adjacent discourse units
are connected with a discourse relation, so with
this dataset we are considering parse sequences
rather than parse trees.
In this work, we focus on the relation labeling
task, as fairly simple methods perform quite well
at the other two tasks (Webber et al., 2012). We
use the ground truth parse structures provided by
the PDTB dataset, so as to isolate the error intro-
duced by relation labeling in our results, but in
practice a greedy structure learning algorithm can
be used if the parse structures are not known a pri-
ori.
Some of the relations in the dataset are induced
by specific connective words in the text. For exam-
ple, a contrast relation may be explicitly revealed
by the conjunction but. Simple classifiers using
only the text of the discourse connective with POS
tags can find explicit relations with high accu-
racy (Lin et al., 2014). The following sentence
shows an example of a more difficult implicit re-
lation. In this sentence, two EDUs are connected
with an explanatory relation, shown in bold, al-
though the connective word does not occur in the
text.
“But a few funds have taken other defen-
sive steps. Some have raised their cash
positions to record levels. [BECAUSE]
High cash positions help buffer a fund
when the market falls.”
We focus on the more difficult implicit relations
that are not induced by coordinating connectives
in the text. The implicit relations have been shown
to require more sophisticated feature sets includ-
ing syntactic and linguistic information (Lin et al.,
2009). The PDTB dataset includes 16,053 exam-
ples of implicit relations.
A full list of the PDTB relations is available
in (Prasad et al., 2008). The relations are orga-
nized hierarchically into top level, types, and sub-
types. Our experiments focus on learning only up
</bodyText>
<page confidence="0.99312">
90
</page>
<figureCaption confidence="0.90382475">
Figure 1: An example of the latent variable dis-
course parsing model taken from the Penn Dis-
course Treebank Dataset. The relation here is an
example of a cause attribution relation.
</figureCaption>
<bodyText confidence="0.999639833333333">
to level 2, as the level 3 (sub-type) relations are
too specific and show only 80% inter-annotator
agreement. There are 16 level 2 relations in the
PDTB, but the 5 least common relations only ap-
pear a handful of times in the dataset and are omit-
ted from our tests, yielding 11 possible classes.
</bodyText>
<sectionHeader confidence="0.998217" genericHeader="method">
4 Approach
</sectionHeader>
<bodyText confidence="0.999644714285714">
We incorporate unlabeled data into our spectral
discourse parsing model using a bootstrapping
framework. The model is trained over several iter-
ations, and the most useful unlabeled sequences
are added as labeled training data after each it-
eration. Our method also utilizes Markovian la-
tent states to compactly capture global informa-
tion about a parse sequence, with one latent vari-
able for each relation in the discourse parsing se-
quence. Most discourse parsing frameworks will
label relations independently of the rest of the ac-
companying parse sequence, but this model allows
for information about the global structure of the
discourse parse to be used when labeling a rela-
tion. A graphical representation of one link in the
parsing model is shown in Figure 1.
Specifically, each potential relation rij between
elementary discourse units ei and ej is accompa-
nied by a corresponding latent variable as hij. Ac-
cording to the model assumptions, the following
equality holds:
</bodyText>
<equation confidence="0.986272">
P(rij = r|r1,2, r2,3...rn+1,n) = P(rij = r|hij)
</equation>
<bodyText confidence="0.999965866666667">
To maintain notational consistency with other
latent variable models, we will denote these re-
lation variables as x1...xn, keeping in mind that
there is one possible relation for each adjacent pair
of elementary discourse units.
For the Penn Discourse Treebank Dataset, the
discourse parses behave like sequence of random
variables representing the relations, which allows
us to use an HMM-like latent variable model based
on the framework presented in (Hsu et al., 2012).
If the discourse parses were instead trees, such as
those seen in Rhetorical Structure Theory (RST)
datasets, we can modify the standard model to in-
clude separate parameters for left and right chil-
dren, as demonstrated in (Dhillon et al., 2012).
</bodyText>
<subsectionHeader confidence="0.995582">
4.1 Spectral Learning
</subsectionHeader>
<bodyText confidence="0.999969125">
This section briefly describes the process of learn-
ing a spectral HMM. Much more detail about the
process is available in (Hsu et al., 2012). Learn-
ing in this model will occur in a subspace of di-
mensionality m, but system dynamics will be the
same if m is not less than the rank of the obser-
vation matrix. If our original feature space has
dimensionality n, we define a transformation ma-
trix U E Rn×m, which can be computed using
Singular Value Decomposition. Given the matrix
U, coupled with the empirical unigram (P1), bi-
gram (P2,1), and trigram matrices (P3,x,1), we are
able to estimate the subspace initial state distribu-
tion (ˆπU) and observable operator ( ˆAU) using the
following equalities (wherein the Moore-Penrose
pseudo-inverse of matrix X is denoted by X+):
</bodyText>
<equation confidence="0.99978">
ˆπU = UTP1
ˆAU = UTP3,x,1(UTP2,1)+Vx
</equation>
<bodyText confidence="0.999992071428571">
For our original feature space, we use the
rich linguistic discourse parsing features defined
in (Feng and Hirst, 2014), which includes syn-
tactic and linguistic features taken from depen-
dency parsing, POS tagging, and semantic simi-
larity measures. We augment this feature space
with a vector space representation of semantics. A
term-document co-occurrence matrix is computed
using all of Wikipedia and Latent Dirichlet Anal-
ysis was performed using this matrix. The top 200
concepts from the vector space representation for
each pair of EDUs in the dataset are included in
the feature space, with a concept regularization pa-
rameter of 0.01.
</bodyText>
<subsectionHeader confidence="0.948846">
4.2 Semi-Supervised Training
</subsectionHeader>
<bodyText confidence="0.986813">
To begin semi-supervised training, we perform
a syntactic parse of the unlabeled data and ex-
</bodyText>
<figure confidence="0.517109777777778">
This hasn&apos;t been Kellogg Co.&apos;s year
edu1
edu2
The oat-bran craze has cost the world&apos;s largest
cereal maker market share, and
the company&apos;s president quit suddenly.
r12
(Contingency.Cause.Reason)
h12
</figure>
<page confidence="0.95077">
91
</page>
<note confidence="0.747121">
F1 Prediction Score
</note>
<bodyText confidence="0.999525636363636">
tract EDU segments using the method described in
(Feng and Hirst, 2014). The model is then trained
using the labeled dataset, and the unlabeled re-
lations are predicted using the Viterbi algorithm.
The most informative sequences in the unlabeled
training set are added to the labeled training set as
labeled examples. To measure how informative a
sequence of relations is, we use density-weighted
certainty sampling (DCS). Specifically for a se-
quence of relations r1...rn taken from a document,
d, we use the following formula:
</bodyText>
<figure confidence="0.975475764705882">
0 10
20 30 40 50 60 70 80 90
100
10
Spectral HMM
Lin 14
Baseline
50
45
40
35
30
25
20
15
Percentage of Labeled Training Data Used
1
</figure>
<equation confidence="0.792831833333333">
DCS(d) =
n
n
i=1
ˆp(ri)
H(ri)
</equation>
<bodyText confidence="0.999949357142857">
In this equation, H(ri) represented the entropy of
the distribution of label predictions for the rela-
tion ri generated by the current spectral model,
which is a measure of the model’s uncertainty for
the label of the given relation. Density is de-
noted ˆp(ri), and this quantity measures the extent
to which the text corresponding to this relation
is representative of the labeled corpus. To com-
pute this measure, we create a Kernel Density Es-
timate (KDE) over a 100 dimensional LDA vector
space representation of all EDU’s in the labeled
corpus. We then compute the density of the KDE
for the text associated with relation ri, which gives
us ˆp(ri). All sequences of relations in the unla-
beled dataset are ranked according to their aver-
age density-weighted certainty score, and all se-
quences scoring above a parameter 0 are added
to the training set. The model is then retrained,
the unlabeled data re-scored, and the process is
repeated for several iterations. In iteration i, the
labeled data in the training set is weighted wli,
and the unlabeled data is weighted wui , with the
unlabeled data receiving higher weight in subse-
quent iterations. The KDE kernel bandwidth and
the parameters 0, wli, wui , and the number of hid-
den states are chosen in experiments using 10-fold
cross validation on the labeled training set, cou-
pled with a subset of the unlabeled data.
</bodyText>
<sectionHeader confidence="0.999728" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.953178176470588">
Figure 2 shows the F1 scores of the model using
various sizes of labeled training sets. In all cases,
the entirety of the unlabeled data is made avail-
able, and 7 rounds of bootstrapping is conducted.
Sections 2-22 of the PDTB are used for training,
with section 23 being withheld for testing, as rec-
ommended by the dataset guidelines (Prasad et al.,
Figure 2: Empirical results for labeling of implicit
relations.
2008). The results are compared against those re-
ported in (Lin et al., 2014), as well as a simple
baseline classifier that labels all relations with the
most common class, EntRel. Compared to the
semi-supervised method described in (Hernault et
al., 2011), we show significant gains in accuracy
at various sizes of dataset, although the unlabeled
dataset used in our experiments is much larger.
When the spectral HMM is trained using only
the labeled dataset, with no unlabeled data, it pro-
duces an F1 score of 41.1%, which is comparable
to the results reported in (Lin et al., 2014). By
comparison, the semi-supervised classifier is able
to obtain similar accuracy when using approxi-
mately 50% of the labeled training data. When
given access to the full labeled dataset, we see
an improvement in the F1 score of 7-9 percent-
age points. Recent work has shown promising re-
sults using CRFs for discourse parsing (Joty et al.,
2013; Feng and Hirst, 2014), but the results re-
ported in this work were taken from the RST-DT
corpus and are not directly comparable. However,
supervised CRFs and HMMs show similar accu-
racy in other language tasks (Ponomareva et al.,
2007; Awasthi et al., 2006).
</bodyText>
<sectionHeader confidence="0.999667" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.99993325">
In this work, we have shown that we are able
to outperform fully-supervised relation classifiers
by augmenting the training data with unlabeled
text. The spectral optimization used in this ap-
proach makes computation tractable even when
using over one million documents. In future work,
we would like to further improve the performance
of this method when very small labeled training
</bodyText>
<page confidence="0.985823">
92
</page>
<bodyText confidence="0.98227">
sets are available, which would allow discourse
analysis to be applied in many new and interest-
ing domains.
</bodyText>
<sectionHeader confidence="0.990286" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999368444444444">
We give thanks to Carolyn Penstein Ros´e and
Geoff Gordon for their helpful discussions and
suggestions. We also gratefully acknowledge
the National Science Foundation for their support
through EAGER grant number IIS1450543. This
material is also based upon work supported by
the Quality of Life Technology Center and the
National Science Foundation under Cooperative
Agreement EEC-0540865.
</bodyText>
<sectionHeader confidence="0.998931" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997019683544304">
Pranjal Awasthi, Delip Rao, and Balaraman Ravindran.
2006. Part of speech tagging and chunking with
hmm and crf. Proceedings of NLP Association of
India (NLPAI) Machine Learning Contest 2006.
Byron Boots and Geoffrey J Gordon. 2010. Predictive
state temporal difference learning. arXiv preprint
arXiv:1011.0041.
Shay B Cohen, Karl Stratos, Michael Collins, Dean P
Foster, and Lyle Ungar. 2014. Spectral learning of
latent-variable pcfgs: Algorithms and sample com-
plexity. The Journal of Machine Learning Research,
15(1):2399–2449.
Paramveer S Dhillon, Jordan Rodu, Michael Collins,
Dean P Foster, and Lyle H Ungar. 2012. Spectral
dependency parsing with latent variables. In Pro-
ceedings of the 2012 joint conference on empirical
methods in natural language processing and compu-
tational natural language learning, pages 205–213.
Association for Computational Linguistics.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-
level discourse parsing with rich linguistic fea-
tures. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 60–68. Association
for Computational Linguistics.
Vanessa Wei Feng and Graeme Hirst. 2014. A linear-
time bottom-up discourse parser with constraints
and post-editing. In Proceedings of The 52nd An-
nual Meeting of the Association for Computational
Linguistics (ACL 2014), Baltimore, USA, June.
Robert Fisher, Reid Simmons, Cheng-Shiu Chung,
Rory Cooper, Garrett Grindle, Annmarie Kelleher,
Hsinyi Liu, and Yu Kuang Wu. 2014. Spectral ma-
chine learning for predicting power wheelchair exer-
cise compliance. In Foundations of Intelligent Sys-
tems, pages 174–183. Springer.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2011. Semi-supervised discourse relation
classification with structural learning. In Compu-
tational Linguistics and Intelligent Text Processing,
pages 340–352. Springer.
Daniel Hsu, Sham M Kakade, and Tong Zhang. 2012.
A spectral algorithm for learning hidden markov
models. Journal of Computer and System Sciences,
78(5):1460–1480.
Shafiq R Joty, Giuseppe Carenini, Raymond T Ng, and
Yashar Mehdad. 2013. Combining intra-and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In ACL (1), pages 486–496.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1, pages 343–351.
Association for Computational Linguistics.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014.
A pdtb-styled end-to-end discourse parser. Natural
Language Engineering, pages 1–34.
Ankur Parikh, Shay B Cohen, and Eric Xing. 2014.
Spectral unsupervised parsing with additive tree
metrics. In Proceedings of the 52nd Annual Meet-
ing of the Association for Computational Linguis-
tics: Long Papers. Association for Computational
Linguistics.
Natalia Ponomareva, Paolo Rosso, Ferr´an Pla, and An-
tonio Molina. 2007. Conditional random fields vs.
hidden markov models in a biomedical named en-
tity recognition task. In Proc. of Int. Conf. Recent
Advances in Natural Language Processing, RANLP,
pages 479–483.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The penn discourse treebank
2.0. In LREC. Citeseer.
Evan Sandhaus. 2008. The new york times annotated
corpus ldc2008t19. Linguistic Data Consortium.
Bonnie Webber, Markus Egg, and Valia Kordoni.
2012. Discourse structure and language technology.
Natural Language Engineering, 18(4):437–490.
</reference>
<page confidence="0.999173">
93
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.538201">
<title confidence="0.999885">Spectral Semi-Supervised Discourse Relation Classification</title>
<author confidence="0.998779">Robert</author>
<affiliation confidence="0.999086">Carnegie Mellon</affiliation>
<address confidence="0.984661">5000 Forbes Pittsburgh, PA</address>
<email confidence="0.999498">rwfisher@cs.cmu.edu</email>
<author confidence="0.956272">Reid</author>
<affiliation confidence="0.998193">Carnegie Mellon</affiliation>
<address confidence="0.984745">5000 Forbes Pittsburgh, PA</address>
<email confidence="0.999669">reids@cs.cmu.edu</email>
<abstract confidence="0.980375043478261">Discourse parsing is the process of discovering the latent relational structure of a long form piece of text and remains a significant open challenge. One of the most difficult tasks in discourse parsing is the classification of implicit discourse relations. Most state-of-the-art systems do not leverage the great volume of unlabeled text available on the web–they rely instead on human annotated training data. By incorporating a mixture of labeled and unlabeled data, we are able to improve relation classification accuracy, reduce the need for annotated data, while still retaining the capacity to use labeled data to ensure that specific desired relations are learned. We achieve this using a latent variable model that is trained in a reduced dimensionality subspace using spectral methods. Our apachieves an of 0.485 on the implicit relation labeling task for the Penn Discourse Treebank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Pranjal Awasthi</author>
<author>Delip Rao</author>
<author>Balaraman Ravindran</author>
</authors>
<title>Part of speech tagging and chunking with hmm and crf.</title>
<date>2006</date>
<booktitle>Proceedings of NLP Association of India (NLPAI) Machine Learning Contest</booktitle>
<contexts>
<context position="16352" citStr="Awasthi et al., 2006" startWordPosition="2653" endWordPosition="2656">n (Lin et al., 2014). By comparison, the semi-supervised classifier is able to obtain similar accuracy when using approximately 50% of the labeled training data. When given access to the full labeled dataset, we see an improvement in the F1 score of 7-9 percentage points. Recent work has shown promising results using CRFs for discourse parsing (Joty et al., 2013; Feng and Hirst, 2014), but the results reported in this work were taken from the RST-DT corpus and are not directly comparable. However, supervised CRFs and HMMs show similar accuracy in other language tasks (Ponomareva et al., 2007; Awasthi et al., 2006). 6 Conclusions In this work, we have shown that we are able to outperform fully-supervised relation classifiers by augmenting the training data with unlabeled text. The spectral optimization used in this approach makes computation tractable even when using over one million documents. In future work, we would like to further improve the performance of this method when very small labeled training 92 sets are available, which would allow discourse analysis to be applied in many new and interesting domains. Acknowledgements We give thanks to Carolyn Penstein Ros´e and Geoff Gordon for their helpf</context>
</contexts>
<marker>Awasthi, Rao, Ravindran, 2006</marker>
<rawString>Pranjal Awasthi, Delip Rao, and Balaraman Ravindran. 2006. Part of speech tagging and chunking with hmm and crf. Proceedings of NLP Association of India (NLPAI) Machine Learning Contest 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Byron Boots</author>
<author>Geoffrey J Gordon</author>
</authors>
<title>Predictive state temporal difference learning. arXiv preprint arXiv:1011.0041.</title>
<date>2010</date>
<contexts>
<context position="5661" citStr="Boots and Gordon, 2010" startWordPosition="868" endWordPosition="871">epresent the global properties of a parse sequence (Joty et al., 2013; Feng and Hirst, 2014), though this work has focused on the RSTDT corpus, rather than the PDTB. In addition to requiring a fully supervised training set, most existing discourse parsers use nonspectral optimization that is often slow and inexact. However, there has been some work in other parsing tasks to employ spectral methods in both supervised and semi-supervised settings (Parikh et al., 2014; Cohen et al., 2014). Spectral methods have also been applied very successfully in many non-linguistic domains (Hsu et al., 2012; Boots and Gordon, 2010; Fisher et al., 2014). 3 Problem Definition and Dataset This section defines the discourse parsing problem and discusses the characteristics of the PDTB. The PDTB consists of annotated articles from the Wall Street Journal and is used in our empirical evaluations. This is combined with the New York Times Annotated Corpus (Sandhaus, 2008), which includes 1.8 million New York Times articles printed between 1987 and 2007. Discourse parsing can be reduced to three separate tasks. First, the text must be decomposed into elementary discourse units (EDUs), which may or may not coincide with sentence</context>
</contexts>
<marker>Boots, Gordon, 2010</marker>
<rawString>Byron Boots and Geoffrey J Gordon. 2010. Predictive state temporal difference learning. arXiv preprint arXiv:1011.0041.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shay B Cohen</author>
<author>Karl Stratos</author>
<author>Michael Collins</author>
<author>Dean P Foster</author>
<author>Lyle Ungar</author>
</authors>
<title>Spectral learning of latent-variable pcfgs: Algorithms and sample complexity.</title>
<date>2014</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="5529" citStr="Cohen et al., 2014" startWordPosition="848" endWordPosition="851">sifiers when more training data is available. Recently there has also been some work to use Conditional Random Fields (CRFs) to represent the global properties of a parse sequence (Joty et al., 2013; Feng and Hirst, 2014), though this work has focused on the RSTDT corpus, rather than the PDTB. In addition to requiring a fully supervised training set, most existing discourse parsers use nonspectral optimization that is often slow and inexact. However, there has been some work in other parsing tasks to employ spectral methods in both supervised and semi-supervised settings (Parikh et al., 2014; Cohen et al., 2014). Spectral methods have also been applied very successfully in many non-linguistic domains (Hsu et al., 2012; Boots and Gordon, 2010; Fisher et al., 2014). 3 Problem Definition and Dataset This section defines the discourse parsing problem and discusses the characteristics of the PDTB. The PDTB consists of annotated articles from the Wall Street Journal and is used in our empirical evaluations. This is combined with the New York Times Annotated Corpus (Sandhaus, 2008), which includes 1.8 million New York Times articles printed between 1987 and 2007. Discourse parsing can be reduced to three se</context>
</contexts>
<marker>Cohen, Stratos, Collins, Foster, Ungar, 2014</marker>
<rawString>Shay B Cohen, Karl Stratos, Michael Collins, Dean P Foster, and Lyle Ungar. 2014. Spectral learning of latent-variable pcfgs: Algorithms and sample complexity. The Journal of Machine Learning Research, 15(1):2399–2449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paramveer S Dhillon</author>
<author>Jordan Rodu</author>
<author>Michael Collins</author>
<author>Dean P Foster</author>
<author>Lyle H Ungar</author>
</authors>
<title>Spectral dependency parsing with latent variables.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning,</booktitle>
<pages>205--213</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10870" citStr="Dhillon et al., 2012" startWordPosition="1728" endWordPosition="1731">tion variables as x1...xn, keeping in mind that there is one possible relation for each adjacent pair of elementary discourse units. For the Penn Discourse Treebank Dataset, the discourse parses behave like sequence of random variables representing the relations, which allows us to use an HMM-like latent variable model based on the framework presented in (Hsu et al., 2012). If the discourse parses were instead trees, such as those seen in Rhetorical Structure Theory (RST) datasets, we can modify the standard model to include separate parameters for left and right children, as demonstrated in (Dhillon et al., 2012). 4.1 Spectral Learning This section briefly describes the process of learning a spectral HMM. Much more detail about the process is available in (Hsu et al., 2012). Learning in this model will occur in a subspace of dimensionality m, but system dynamics will be the same if m is not less than the rank of the observation matrix. If our original feature space has dimensionality n, we define a transformation matrix U E Rn×m, which can be computed using Singular Value Decomposition. Given the matrix U, coupled with the empirical unigram (P1), bigram (P2,1), and trigram matrices (P3,x,1), we are ab</context>
</contexts>
<marker>Dhillon, Rodu, Collins, Foster, Ungar, 2012</marker>
<rawString>Paramveer S Dhillon, Jordan Rodu, Michael Collins, Dean P Foster, and Lyle H Ungar. 2012. Spectral dependency parsing with latent variables. In Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning, pages 205–213. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>Textlevel discourse parsing with rich linguistic features.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1,</booktitle>
<pages>60--68</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4552" citStr="Feng and Hirst, 2012" startWordPosition="690" endWordPosition="693">ernational Joint Conference on Natural Language Processing (Short Papers), pages 89–93, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics strate the practical advantages of the model with an empirical evaluation on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) dataset, which yields an F1 score of 0.485. This accuracy shows a 7-9 percentage point improvement over approaches that do not utilize unlabeled training data. 2 Related Work There has been quite a bit of work concerning fully supervised relation classification with the PDTB (Lin et al., 2014; Feng and Hirst, 2012; Webber et al., 2012). Semi-supervised relation classification is much less common however. One recent example of an attempt to leverage unlabeled data appears in (Hernault et al., 2011), which showed that moderate classification accuracy can be achieved with very small labeled datasets. However, this approach is not competitive with fully supervised classifiers when more training data is available. Recently there has also been some work to use Conditional Random Fields (CRFs) to represent the global properties of a parse sequence (Joty et al., 2013; Feng and Hirst, 2014), though this work ha</context>
</contexts>
<marker>Feng, Hirst, 2012</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2012. Textlevel discourse parsing with rich linguistic features. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 60–68. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Wei Feng</author>
<author>Graeme Hirst</author>
</authors>
<title>A lineartime bottom-up discourse parser with constraints and post-editing.</title>
<date>2014</date>
<booktitle>In Proceedings of The 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014),</booktitle>
<location>Baltimore, USA,</location>
<contexts>
<context position="5131" citStr="Feng and Hirst, 2014" startWordPosition="781" endWordPosition="784">TB (Lin et al., 2014; Feng and Hirst, 2012; Webber et al., 2012). Semi-supervised relation classification is much less common however. One recent example of an attempt to leverage unlabeled data appears in (Hernault et al., 2011), which showed that moderate classification accuracy can be achieved with very small labeled datasets. However, this approach is not competitive with fully supervised classifiers when more training data is available. Recently there has also been some work to use Conditional Random Fields (CRFs) to represent the global properties of a parse sequence (Joty et al., 2013; Feng and Hirst, 2014), though this work has focused on the RSTDT corpus, rather than the PDTB. In addition to requiring a fully supervised training set, most existing discourse parsers use nonspectral optimization that is often slow and inexact. However, there has been some work in other parsing tasks to employ spectral methods in both supervised and semi-supervised settings (Parikh et al., 2014; Cohen et al., 2014). Spectral methods have also been applied very successfully in many non-linguistic domains (Hsu et al., 2012; Boots and Gordon, 2010; Fisher et al., 2014). 3 Problem Definition and Dataset This section </context>
<context position="11822" citStr="Feng and Hirst, 2014" startWordPosition="1889" endWordPosition="1892">f our original feature space has dimensionality n, we define a transformation matrix U E Rn×m, which can be computed using Singular Value Decomposition. Given the matrix U, coupled with the empirical unigram (P1), bigram (P2,1), and trigram matrices (P3,x,1), we are able to estimate the subspace initial state distribution (ˆπU) and observable operator ( ˆAU) using the following equalities (wherein the Moore-Penrose pseudo-inverse of matrix X is denoted by X+): ˆπU = UTP1 ˆAU = UTP3,x,1(UTP2,1)+Vx For our original feature space, we use the rich linguistic discourse parsing features defined in (Feng and Hirst, 2014), which includes syntactic and linguistic features taken from dependency parsing, POS tagging, and semantic similarity measures. We augment this feature space with a vector space representation of semantics. A term-document co-occurrence matrix is computed using all of Wikipedia and Latent Dirichlet Analysis was performed using this matrix. The top 200 concepts from the vector space representation for each pair of EDUs in the dataset are included in the feature space, with a concept regularization parameter of 0.01. 4.2 Semi-Supervised Training To begin semi-supervised training, we perform a s</context>
<context position="16118" citStr="Feng and Hirst, 2014" startWordPosition="2613" endWordPosition="2616">ugh the unlabeled dataset used in our experiments is much larger. When the spectral HMM is trained using only the labeled dataset, with no unlabeled data, it produces an F1 score of 41.1%, which is comparable to the results reported in (Lin et al., 2014). By comparison, the semi-supervised classifier is able to obtain similar accuracy when using approximately 50% of the labeled training data. When given access to the full labeled dataset, we see an improvement in the F1 score of 7-9 percentage points. Recent work has shown promising results using CRFs for discourse parsing (Joty et al., 2013; Feng and Hirst, 2014), but the results reported in this work were taken from the RST-DT corpus and are not directly comparable. However, supervised CRFs and HMMs show similar accuracy in other language tasks (Ponomareva et al., 2007; Awasthi et al., 2006). 6 Conclusions In this work, we have shown that we are able to outperform fully-supervised relation classifiers by augmenting the training data with unlabeled text. The spectral optimization used in this approach makes computation tractable even when using over one million documents. In future work, we would like to further improve the performance of this method </context>
</contexts>
<marker>Feng, Hirst, 2014</marker>
<rawString>Vanessa Wei Feng and Graeme Hirst. 2014. A lineartime bottom-up discourse parser with constraints and post-editing. In Proceedings of The 52nd Annual Meeting of the Association for Computational Linguistics (ACL 2014), Baltimore, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Fisher</author>
<author>Reid Simmons</author>
<author>Cheng-Shiu Chung</author>
<author>Rory Cooper</author>
<author>Garrett Grindle</author>
<author>Annmarie Kelleher</author>
<author>Hsinyi Liu</author>
<author>Yu Kuang Wu</author>
</authors>
<title>Spectral machine learning for predicting power wheelchair exercise compliance.</title>
<date>2014</date>
<booktitle>In Foundations of Intelligent Systems,</booktitle>
<pages>174--183</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="5683" citStr="Fisher et al., 2014" startWordPosition="872" endWordPosition="875">erties of a parse sequence (Joty et al., 2013; Feng and Hirst, 2014), though this work has focused on the RSTDT corpus, rather than the PDTB. In addition to requiring a fully supervised training set, most existing discourse parsers use nonspectral optimization that is often slow and inexact. However, there has been some work in other parsing tasks to employ spectral methods in both supervised and semi-supervised settings (Parikh et al., 2014; Cohen et al., 2014). Spectral methods have also been applied very successfully in many non-linguistic domains (Hsu et al., 2012; Boots and Gordon, 2010; Fisher et al., 2014). 3 Problem Definition and Dataset This section defines the discourse parsing problem and discusses the characteristics of the PDTB. The PDTB consists of annotated articles from the Wall Street Journal and is used in our empirical evaluations. This is combined with the New York Times Annotated Corpus (Sandhaus, 2008), which includes 1.8 million New York Times articles printed between 1987 and 2007. Discourse parsing can be reduced to three separate tasks. First, the text must be decomposed into elementary discourse units (EDUs), which may or may not coincide with sentence boundaries. The EDUs </context>
</contexts>
<marker>Fisher, Simmons, Chung, Cooper, Grindle, Kelleher, Liu, Wu, 2014</marker>
<rawString>Robert Fisher, Reid Simmons, Cheng-Shiu Chung, Rory Cooper, Garrett Grindle, Annmarie Kelleher, Hsinyi Liu, and Yu Kuang Wu. 2014. Spectral machine learning for predicting power wheelchair exercise compliance. In Foundations of Intelligent Systems, pages 174–183. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
<author>Danushka Bollegala</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Semi-supervised discourse relation classification with structural learning.</title>
<date>2011</date>
<booktitle>In Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>340--352</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="4739" citStr="Hernault et al., 2011" startWordPosition="719" endWordPosition="722">ctical advantages of the model with an empirical evaluation on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) dataset, which yields an F1 score of 0.485. This accuracy shows a 7-9 percentage point improvement over approaches that do not utilize unlabeled training data. 2 Related Work There has been quite a bit of work concerning fully supervised relation classification with the PDTB (Lin et al., 2014; Feng and Hirst, 2012; Webber et al., 2012). Semi-supervised relation classification is much less common however. One recent example of an attempt to leverage unlabeled data appears in (Hernault et al., 2011), which showed that moderate classification accuracy can be achieved with very small labeled datasets. However, this approach is not competitive with fully supervised classifiers when more training data is available. Recently there has also been some work to use Conditional Random Fields (CRFs) to represent the global properties of a parse sequence (Joty et al., 2013; Feng and Hirst, 2014), though this work has focused on the RSTDT corpus, rather than the PDTB. In addition to requiring a fully supervised training set, most existing discourse parsers use nonspectral optimization that is often s</context>
<context position="15423" citStr="Hernault et al., 2011" startWordPosition="2494" endWordPosition="2497"> various sizes of labeled training sets. In all cases, the entirety of the unlabeled data is made available, and 7 rounds of bootstrapping is conducted. Sections 2-22 of the PDTB are used for training, with section 23 being withheld for testing, as recommended by the dataset guidelines (Prasad et al., Figure 2: Empirical results for labeling of implicit relations. 2008). The results are compared against those reported in (Lin et al., 2014), as well as a simple baseline classifier that labels all relations with the most common class, EntRel. Compared to the semi-supervised method described in (Hernault et al., 2011), we show significant gains in accuracy at various sizes of dataset, although the unlabeled dataset used in our experiments is much larger. When the spectral HMM is trained using only the labeled dataset, with no unlabeled data, it produces an F1 score of 41.1%, which is comparable to the results reported in (Lin et al., 2014). By comparison, the semi-supervised classifier is able to obtain similar accuracy when using approximately 50% of the labeled training data. When given access to the full labeled dataset, we see an improvement in the F1 score of 7-9 percentage points. Recent work has sho</context>
</contexts>
<marker>Hernault, Bollegala, Ishizuka, 2011</marker>
<rawString>Hugo Hernault, Danushka Bollegala, and Mitsuru Ishizuka. 2011. Semi-supervised discourse relation classification with structural learning. In Computational Linguistics and Intelligent Text Processing, pages 340–352. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Hsu</author>
<author>Sham M Kakade</author>
<author>Tong Zhang</author>
</authors>
<title>A spectral algorithm for learning hidden markov models.</title>
<date>2012</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>78</volume>
<issue>5</issue>
<contexts>
<context position="5637" citStr="Hsu et al., 2012" startWordPosition="864" endWordPosition="867">Fields (CRFs) to represent the global properties of a parse sequence (Joty et al., 2013; Feng and Hirst, 2014), though this work has focused on the RSTDT corpus, rather than the PDTB. In addition to requiring a fully supervised training set, most existing discourse parsers use nonspectral optimization that is often slow and inexact. However, there has been some work in other parsing tasks to employ spectral methods in both supervised and semi-supervised settings (Parikh et al., 2014; Cohen et al., 2014). Spectral methods have also been applied very successfully in many non-linguistic domains (Hsu et al., 2012; Boots and Gordon, 2010; Fisher et al., 2014). 3 Problem Definition and Dataset This section defines the discourse parsing problem and discusses the characteristics of the PDTB. The PDTB consists of annotated articles from the Wall Street Journal and is used in our empirical evaluations. This is combined with the New York Times Annotated Corpus (Sandhaus, 2008), which includes 1.8 million New York Times articles printed between 1987 and 2007. Discourse parsing can be reduced to three separate tasks. First, the text must be decomposed into elementary discourse units (EDUs), which may or may no</context>
<context position="10624" citStr="Hsu et al., 2012" startWordPosition="1687" endWordPosition="1690">sponding latent variable as hij. According to the model assumptions, the following equality holds: P(rij = r|r1,2, r2,3...rn+1,n) = P(rij = r|hij) To maintain notational consistency with other latent variable models, we will denote these relation variables as x1...xn, keeping in mind that there is one possible relation for each adjacent pair of elementary discourse units. For the Penn Discourse Treebank Dataset, the discourse parses behave like sequence of random variables representing the relations, which allows us to use an HMM-like latent variable model based on the framework presented in (Hsu et al., 2012). If the discourse parses were instead trees, such as those seen in Rhetorical Structure Theory (RST) datasets, we can modify the standard model to include separate parameters for left and right children, as demonstrated in (Dhillon et al., 2012). 4.1 Spectral Learning This section briefly describes the process of learning a spectral HMM. Much more detail about the process is available in (Hsu et al., 2012). Learning in this model will occur in a subspace of dimensionality m, but system dynamics will be the same if m is not less than the rank of the observation matrix. If our original feature </context>
</contexts>
<marker>Hsu, Kakade, Zhang, 2012</marker>
<rawString>Daniel Hsu, Sham M Kakade, and Tong Zhang. 2012. A spectral algorithm for learning hidden markov models. Journal of Computer and System Sciences, 78(5):1460–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq R Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
<author>Yashar Mehdad</author>
</authors>
<title>Combining intra-and multisentential rhetorical parsing for document-level discourse analysis.</title>
<date>2013</date>
<booktitle>In ACL (1),</booktitle>
<pages>486--496</pages>
<contexts>
<context position="5108" citStr="Joty et al., 2013" startWordPosition="777" endWordPosition="780">ication with the PDTB (Lin et al., 2014; Feng and Hirst, 2012; Webber et al., 2012). Semi-supervised relation classification is much less common however. One recent example of an attempt to leverage unlabeled data appears in (Hernault et al., 2011), which showed that moderate classification accuracy can be achieved with very small labeled datasets. However, this approach is not competitive with fully supervised classifiers when more training data is available. Recently there has also been some work to use Conditional Random Fields (CRFs) to represent the global properties of a parse sequence (Joty et al., 2013; Feng and Hirst, 2014), though this work has focused on the RSTDT corpus, rather than the PDTB. In addition to requiring a fully supervised training set, most existing discourse parsers use nonspectral optimization that is often slow and inexact. However, there has been some work in other parsing tasks to employ spectral methods in both supervised and semi-supervised settings (Parikh et al., 2014; Cohen et al., 2014). Spectral methods have also been applied very successfully in many non-linguistic domains (Hsu et al., 2012; Boots and Gordon, 2010; Fisher et al., 2014). 3 Problem Definition an</context>
<context position="16095" citStr="Joty et al., 2013" startWordPosition="2609" endWordPosition="2612">s of dataset, although the unlabeled dataset used in our experiments is much larger. When the spectral HMM is trained using only the labeled dataset, with no unlabeled data, it produces an F1 score of 41.1%, which is comparable to the results reported in (Lin et al., 2014). By comparison, the semi-supervised classifier is able to obtain similar accuracy when using approximately 50% of the labeled training data. When given access to the full labeled dataset, we see an improvement in the F1 score of 7-9 percentage points. Recent work has shown promising results using CRFs for discourse parsing (Joty et al., 2013; Feng and Hirst, 2014), but the results reported in this work were taken from the RST-DT corpus and are not directly comparable. However, supervised CRFs and HMMs show similar accuracy in other language tasks (Ponomareva et al., 2007; Awasthi et al., 2006). 6 Conclusions In this work, we have shown that we are able to outperform fully-supervised relation classifiers by augmenting the training data with unlabeled text. The spectral optimization used in this approach makes computation tractable even when using over one million documents. In future work, we would like to further improve the perf</context>
</contexts>
<marker>Joty, Carenini, Ng, Mehdad, 2013</marker>
<rawString>Shafiq R Joty, Giuseppe Carenini, Raymond T Ng, and Yashar Mehdad. 2013. Combining intra-and multisentential rhetorical parsing for document-level discourse analysis. In ACL (1), pages 486–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Min-Yen Kan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Recognizing implicit discourse relations in the penn discourse treebank.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>1</volume>
<pages>343--351</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8380" citStr="Lin et al., 2009" startWordPosition="1318" endWordPosition="1321"> more difficult implicit relation. In this sentence, two EDUs are connected with an explanatory relation, shown in bold, although the connective word does not occur in the text. “But a few funds have taken other defensive steps. Some have raised their cash positions to record levels. [BECAUSE] High cash positions help buffer a fund when the market falls.” We focus on the more difficult implicit relations that are not induced by coordinating connectives in the text. The implicit relations have been shown to require more sophisticated feature sets including syntactic and linguistic information (Lin et al., 2009). The PDTB dataset includes 16,053 examples of implicit relations. A full list of the PDTB relations is available in (Prasad et al., 2008). The relations are organized hierarchically into top level, types, and subtypes. Our experiments focus on learning only up 90 Figure 1: An example of the latent variable discourse parsing model taken from the Penn Discourse Treebank Dataset. The relation here is an example of a cause attribution relation. to level 2, as the level 3 (sub-type) relations are too specific and show only 80% inter-annotator agreement. There are 16 level 2 relations in the PDTB, </context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the penn discourse treebank. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 343–351. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A pdtb-styled end-to-end discourse parser. Natural Language Engineering,</title>
<date>2014</date>
<pages>1--34</pages>
<contexts>
<context position="4530" citStr="Lin et al., 2014" startWordPosition="686" endWordPosition="689">cs and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 89–93, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics strate the practical advantages of the model with an empirical evaluation on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) dataset, which yields an F1 score of 0.485. This accuracy shows a 7-9 percentage point improvement over approaches that do not utilize unlabeled training data. 2 Related Work There has been quite a bit of work concerning fully supervised relation classification with the PDTB (Lin et al., 2014; Feng and Hirst, 2012; Webber et al., 2012). Semi-supervised relation classification is much less common however. One recent example of an attempt to leverage unlabeled data appears in (Hernault et al., 2011), which showed that moderate classification accuracy can be achieved with very small labeled datasets. However, this approach is not competitive with fully supervised classifiers when more training data is available. Recently there has also been some work to use Conditional Random Fields (CRFs) to represent the global properties of a parse sequence (Joty et al., 2013; Feng and Hirst, 2014</context>
<context position="7717" citStr="Lin et al., 2014" startWordPosition="1209" endWordPosition="1212">er two tasks (Webber et al., 2012). We use the ground truth parse structures provided by the PDTB dataset, so as to isolate the error introduced by relation labeling in our results, but in practice a greedy structure learning algorithm can be used if the parse structures are not known a priori. Some of the relations in the dataset are induced by specific connective words in the text. For example, a contrast relation may be explicitly revealed by the conjunction but. Simple classifiers using only the text of the discourse connective with POS tags can find explicit relations with high accuracy (Lin et al., 2014). The following sentence shows an example of a more difficult implicit relation. In this sentence, two EDUs are connected with an explanatory relation, shown in bold, although the connective word does not occur in the text. “But a few funds have taken other defensive steps. Some have raised their cash positions to record levels. [BECAUSE] High cash positions help buffer a fund when the market falls.” We focus on the more difficult implicit relations that are not induced by coordinating connectives in the text. The implicit relations have been shown to require more sophisticated feature sets in</context>
<context position="15244" citStr="Lin et al., 2014" startWordPosition="2466" endWordPosition="2469">experiments using 10-fold cross validation on the labeled training set, coupled with a subset of the unlabeled data. 5 Results Figure 2 shows the F1 scores of the model using various sizes of labeled training sets. In all cases, the entirety of the unlabeled data is made available, and 7 rounds of bootstrapping is conducted. Sections 2-22 of the PDTB are used for training, with section 23 being withheld for testing, as recommended by the dataset guidelines (Prasad et al., Figure 2: Empirical results for labeling of implicit relations. 2008). The results are compared against those reported in (Lin et al., 2014), as well as a simple baseline classifier that labels all relations with the most common class, EntRel. Compared to the semi-supervised method described in (Hernault et al., 2011), we show significant gains in accuracy at various sizes of dataset, although the unlabeled dataset used in our experiments is much larger. When the spectral HMM is trained using only the labeled dataset, with no unlabeled data, it produces an F1 score of 41.1%, which is comparable to the results reported in (Lin et al., 2014). By comparison, the semi-supervised classifier is able to obtain similar accuracy when using</context>
</contexts>
<marker>Lin, Ng, Kan, 2014</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2014. A pdtb-styled end-to-end discourse parser. Natural Language Engineering, pages 1–34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ankur Parikh</author>
<author>Shay B Cohen</author>
<author>Eric Xing</author>
</authors>
<title>Spectral unsupervised parsing with additive tree metrics.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association</booktitle>
<contexts>
<context position="5508" citStr="Parikh et al., 2014" startWordPosition="844" endWordPosition="847">fully supervised classifiers when more training data is available. Recently there has also been some work to use Conditional Random Fields (CRFs) to represent the global properties of a parse sequence (Joty et al., 2013; Feng and Hirst, 2014), though this work has focused on the RSTDT corpus, rather than the PDTB. In addition to requiring a fully supervised training set, most existing discourse parsers use nonspectral optimization that is often slow and inexact. However, there has been some work in other parsing tasks to employ spectral methods in both supervised and semi-supervised settings (Parikh et al., 2014; Cohen et al., 2014). Spectral methods have also been applied very successfully in many non-linguistic domains (Hsu et al., 2012; Boots and Gordon, 2010; Fisher et al., 2014). 3 Problem Definition and Dataset This section defines the discourse parsing problem and discusses the characteristics of the PDTB. The PDTB consists of annotated articles from the Wall Street Journal and is used in our empirical evaluations. This is combined with the New York Times Annotated Corpus (Sandhaus, 2008), which includes 1.8 million New York Times articles printed between 1987 and 2007. Discourse parsing can b</context>
</contexts>
<marker>Parikh, Cohen, Xing, 2014</marker>
<rawString>Ankur Parikh, Shay B Cohen, and Eric Xing. 2014. Spectral unsupervised parsing with additive tree metrics. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Long Papers. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Natalia Ponomareva</author>
<author>Paolo Rosso</author>
<author>Ferr´an Pla</author>
<author>Antonio Molina</author>
</authors>
<title>Conditional random fields vs. hidden markov models in a biomedical named entity recognition task.</title>
<date>2007</date>
<booktitle>In Proc. of Int. Conf. Recent Advances in Natural Language Processing, RANLP,</booktitle>
<pages>479--483</pages>
<contexts>
<context position="16329" citStr="Ponomareva et al., 2007" startWordPosition="2649" endWordPosition="2652">to the results reported in (Lin et al., 2014). By comparison, the semi-supervised classifier is able to obtain similar accuracy when using approximately 50% of the labeled training data. When given access to the full labeled dataset, we see an improvement in the F1 score of 7-9 percentage points. Recent work has shown promising results using CRFs for discourse parsing (Joty et al., 2013; Feng and Hirst, 2014), but the results reported in this work were taken from the RST-DT corpus and are not directly comparable. However, supervised CRFs and HMMs show similar accuracy in other language tasks (Ponomareva et al., 2007; Awasthi et al., 2006). 6 Conclusions In this work, we have shown that we are able to outperform fully-supervised relation classifiers by augmenting the training data with unlabeled text. The spectral optimization used in this approach makes computation tractable even when using over one million documents. In future work, we would like to further improve the performance of this method when very small labeled training 92 sets are available, which would allow discourse analysis to be applied in many new and interesting domains. Acknowledgements We give thanks to Carolyn Penstein Ros´e and Geoff</context>
</contexts>
<marker>Ponomareva, Rosso, Pla, Molina, 2007</marker>
<rawString>Natalia Ponomareva, Paolo Rosso, Ferr´an Pla, and Antonio Molina. 2007. Conditional random fields vs. hidden markov models in a biomedical named entity recognition task. In Proc. of Int. Conf. Recent Advances in Natural Language Processing, RANLP, pages 479–483.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rashmi Prasad</author>
<author>Nikhil Dinesh</author>
<author>Alan Lee</author>
<author>Eleni Miltsakaki</author>
<author>Livio Robaldo</author>
<author>Aravind K Joshi</author>
<author>Bonnie L Webber</author>
</authors>
<title>The penn discourse treebank 2.0. In LREC.</title>
<date>2008</date>
<publisher>Citeseer.</publisher>
<contexts>
<context position="1881" citStr="Prasad et al., 2008" startWordPosition="278" endWordPosition="281">t entails the discovery of the latent relational structure in a multi-sentence piece of text. Unlike semantic and syntactic parsing, which are used for single sentence parsing, discourse parsing is used to discover intersentential relations in longer pieces of text. Without discourse, parsing methods can only be used to understand documents as sequences of unrelated sentences. Unfortunately, manual annotation of discourse structure in text is costly and time consuming. Multiple annotators are required for each relation to estimate inter-annotator agreement. The Penn Discourse Treebank (PDTB) (Prasad et al., 2008). is one of the largest annotated discourse parsing datasets, with 16,224 implicit relations. However, this pales in comparison to unlabeled datasets that can include millions of sentences of text. By augmenting a labeled dataset with unlabeled data, we can use a bootstrapping framework to improve predictive accuracy, and reduce the need for labeled data–which could make it much easier to port discourse parsing algorithms to new domains. On the other hand, a fully unsupervised parser may not be desirable because in many applications specific discourse relations must be identified, which would </context>
<context position="4236" citStr="Prasad et al., 2008" startWordPosition="638" endWordPosition="641">th in the limit. This paper presents a semi-supervised spectral model for a sequential relation labeling task for discourse parsing. Besides the theoretically desirable properties mentioned above, we also demon89 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 89–93, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics strate the practical advantages of the model with an empirical evaluation on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) dataset, which yields an F1 score of 0.485. This accuracy shows a 7-9 percentage point improvement over approaches that do not utilize unlabeled training data. 2 Related Work There has been quite a bit of work concerning fully supervised relation classification with the PDTB (Lin et al., 2014; Feng and Hirst, 2012; Webber et al., 2012). Semi-supervised relation classification is much less common however. One recent example of an attempt to leverage unlabeled data appears in (Hernault et al., 2011), which showed that moderate classification accuracy can be achieved with very small labeled data</context>
<context position="8518" citStr="Prasad et al., 2008" startWordPosition="1342" endWordPosition="1345">onnective word does not occur in the text. “But a few funds have taken other defensive steps. Some have raised their cash positions to record levels. [BECAUSE] High cash positions help buffer a fund when the market falls.” We focus on the more difficult implicit relations that are not induced by coordinating connectives in the text. The implicit relations have been shown to require more sophisticated feature sets including syntactic and linguistic information (Lin et al., 2009). The PDTB dataset includes 16,053 examples of implicit relations. A full list of the PDTB relations is available in (Prasad et al., 2008). The relations are organized hierarchically into top level, types, and subtypes. Our experiments focus on learning only up 90 Figure 1: An example of the latent variable discourse parsing model taken from the Penn Discourse Treebank Dataset. The relation here is an example of a cause attribution relation. to level 2, as the level 3 (sub-type) relations are too specific and show only 80% inter-annotator agreement. There are 16 level 2 relations in the PDTB, but the 5 least common relations only appear a handful of times in the dataset and are omitted from our tests, yielding 11 possible classe</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind K Joshi, and Bonnie L Webber. 2008. The penn discourse treebank 2.0. In LREC. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The new york times annotated corpus ldc2008t19. Linguistic Data Consortium.</title>
<date>2008</date>
<contexts>
<context position="6001" citStr="Sandhaus, 2008" startWordPosition="925" endWordPosition="926">ork in other parsing tasks to employ spectral methods in both supervised and semi-supervised settings (Parikh et al., 2014; Cohen et al., 2014). Spectral methods have also been applied very successfully in many non-linguistic domains (Hsu et al., 2012; Boots and Gordon, 2010; Fisher et al., 2014). 3 Problem Definition and Dataset This section defines the discourse parsing problem and discusses the characteristics of the PDTB. The PDTB consists of annotated articles from the Wall Street Journal and is used in our empirical evaluations. This is combined with the New York Times Annotated Corpus (Sandhaus, 2008), which includes 1.8 million New York Times articles printed between 1987 and 2007. Discourse parsing can be reduced to three separate tasks. First, the text must be decomposed into elementary discourse units (EDUs), which may or may not coincide with sentence boundaries. The EDUs are often independent clauses that may be connected with conjunctions. After the text has been partitioned into EDUs, the discourse structure must be identified. This requires us to identify all pairs of EDUs that will be connected with some discourse relation. These relational links induce the skeletal structure of </context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The new york times annotated corpus ldc2008t19. Linguistic Data Consortium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
<author>Markus Egg</author>
<author>Valia Kordoni</author>
</authors>
<title>Discourse structure and language technology.</title>
<date>2012</date>
<journal>Natural Language Engineering,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="4574" citStr="Webber et al., 2012" startWordPosition="694" endWordPosition="697">rence on Natural Language Processing (Short Papers), pages 89–93, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics strate the practical advantages of the model with an empirical evaluation on the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) dataset, which yields an F1 score of 0.485. This accuracy shows a 7-9 percentage point improvement over approaches that do not utilize unlabeled training data. 2 Related Work There has been quite a bit of work concerning fully supervised relation classification with the PDTB (Lin et al., 2014; Feng and Hirst, 2012; Webber et al., 2012). Semi-supervised relation classification is much less common however. One recent example of an attempt to leverage unlabeled data appears in (Hernault et al., 2011), which showed that moderate classification accuracy can be achieved with very small labeled datasets. However, this approach is not competitive with fully supervised classifiers when more training data is available. Recently there has also been some work to use Conditional Random Fields (CRFs) to represent the global properties of a parse sequence (Joty et al., 2013; Feng and Hirst, 2014), though this work has focused on the RSTDT</context>
<context position="7134" citStr="Webber et al., 2012" startWordPosition="1107" endWordPosition="1110"> with some discourse relation. These relational links induce the skeletal structure of the discourse parse tree. Finally, each connection identified in the previous step must be labeled using a known set of relations. Examples of these discourse relations include concession, causal, and instantiation relations. In the PDTB, only adjacent discourse units are connected with a discourse relation, so with this dataset we are considering parse sequences rather than parse trees. In this work, we focus on the relation labeling task, as fairly simple methods perform quite well at the other two tasks (Webber et al., 2012). We use the ground truth parse structures provided by the PDTB dataset, so as to isolate the error introduced by relation labeling in our results, but in practice a greedy structure learning algorithm can be used if the parse structures are not known a priori. Some of the relations in the dataset are induced by specific connective words in the text. For example, a contrast relation may be explicitly revealed by the conjunction but. Simple classifiers using only the text of the discourse connective with POS tags can find explicit relations with high accuracy (Lin et al., 2014). The following s</context>
</contexts>
<marker>Webber, Egg, Kordoni, 2012</marker>
<rawString>Bonnie Webber, Markus Egg, and Valia Kordoni. 2012. Discourse structure and language technology. Natural Language Engineering, 18(4):437–490.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>