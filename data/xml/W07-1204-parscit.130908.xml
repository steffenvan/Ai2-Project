<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000098">
<title confidence="0.993023">
Exploiting Semantic Information for HPSG Parse Selection
</title>
<author confidence="0.66623">
Sanae Fujita,n Francis Bond,♠ Stephan Oepen,46 Takaaki Tanakan
n {sanae,takaaki}@cslab.kecl.ntt.co.jp, ♠ bond@ieee.org,46oe@ifi.uio.no
</author>
<affiliation confidence="0.973503666666667">
n NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation
♠ National Institute of Information and Communications Technology (Japan)
46 University of Oslo, Department of Informatics (Norway)
</affiliation>
<sectionHeader confidence="0.978761" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9989245">
In this paper we present a framework for
experimentation on parse selection using
syntactic and semantic features. Results
are given for syntactic features, depen-
dency relations and the use of semantic
classes.
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99926224">
In this paper we investigate the use of semantic in-
formation in parse selection.
Recently, significant improvements have been
made in combining symbolic and statistical ap-
proaches to various natural language processing
tasks. In parsing, for example, symbolic grammars
are combined with stochastic models (Oepen et al.,
2004; Malouf and van Noord, 2004). Much of the
gain in statistical parsing using lexicalized models
comes from the use of a small set of function words
(Klein and Manning, 2003). Features based on gen-
eral relations provide little improvement, presum-
ably because the data is too sparse: in the Penn
treebank standardly used to train and test statisti-
cal parsers stocks and skyrocket never appear to-
gether. However, the superordinate concepts capi-
tal (D stocks) and move upward (D sky rocket) fre-
quently appear together, which suggests that using
word senses and their hypernyms as features may be
useful
However, to date, there have been few combina-
tions of sense information together with symbolic
grammars and statistical models. We hypothesize
that one of the reasons for the lack of success is
that there has been no resource annotated with both
</bodyText>
<page confidence="0.969028">
25
</page>
<bodyText confidence="0.999561">
syntactic and semantic information. In this paper,
we use a treebank with both syntactic information
(HPSG parses) and semantic information (sense tags
from a lexicon) (Bond et al., 2007). We use this to
train parse selection models using both syntactic and
semantic features. A model trained using syntactic
features combined with semantic information out-
performs a model using purely syntactic information
by a wide margin (69.4% sentence parse accuracy
vs. 63.8% on definition sentences).
</bodyText>
<sectionHeader confidence="0.959756" genericHeader="method">
2 The Hinoki Corpus
</sectionHeader>
<bodyText confidence="0.9999811">
There are now some corpora being built with the
syntactic and semantic information necessary to in-
vestigate the use of semantic information in parse
selection. In English, the OntoNotes project (Hovy
et al., 2006) is combining sense tags with the Penn
treebank. We are using Japanese data from the Hi-
noki Corpus consisting of around 95,000 dictionary
definition and example sentences (Bond et al., 2007)
annotated with both syntactic parses and senses from
the same dictionary.
</bodyText>
<subsectionHeader confidence="0.962354">
2.1 Syntactic Annotation
</subsectionHeader>
<bodyText confidence="0.9999285">
Syntactic annotation in Hinoki is grammar based
corpus annotation done by selecting the best parse
(or parses) from the full analyses derived by a broad-
coverage precision grammar. The grammar is an
HPSG implementation (JACY: Siegel and Bender,
2002), which provides a high level of detail, mark-
ing not only dependency and constituent structure
but also detailed semantic relations. As the gram-
mar is based on a monostratal theory of grammar
(HPSG: Pollard and Sag, 1994), annotation by man-
ual disambiguation determines syntactic and seman-
tic structure at the same time. Using a grammar
</bodyText>
<note confidence="0.9245145">
Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 25–32,
Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999671">
helps treebank consistency — all sentences anno-
tated are guaranteed to have well-formed parses.
The flip side to this is that any sentences which the
parser cannot parse remain unannotated, at least un-
less we were to fall back on full manual mark-up of
their analyses. The actual annotation process uses
the same tools as the Redwoods treebank of English
(Oepen et al., 2004).
A (simplified) example of an entry is given in Fig-
ure 1. Each entry contains the word itself, its part
of speech, and its lexical type(s) in the grammar.
Each sense then contains definition and example
sentences, links to other senses in the lexicon (such
as hypernym), and links to other resources, such
as the Goi-Taikei Japanese Lexicon (Ikehara et al.,
1997) and WordNet (Fellbaum, 1998). Each content
word of the definition and example sentences is an-
notated with sense tags from the same lexicon.
There were 4 parses for the definition sentence.
The correct parse, shown as a phrase structure tree,
is shown in Figure 2. The two sources of ambigu-
ity are the conjunction and the relative clause. The
parser also allows the conjunction to combine2M
densha andJ�,hito. In Japanese, relative clauses
can have gapped and non-gapped readings. In the
gapped reading (selected here),J�,hito is the subject
ofA+Kunten “drive”. In the non-gapped reading
there is some underspecified relation between the
modifee and the verb phrase. This is similar to the
difference in the two readings of the day he knew
in English: “the day that he knew about” (gapped)
vs “the day on which he knew (something)” (non-
gapped). Such semantic ambiguity is resolved by
selecting the correct derivation tree that includes the
applied rules in building the tree (Fig 3).
The semantic representation is Minimal Recur-
sion Semantics (Copestake et al., 2005). We sim-
plify this into a dependency representation, further
abstracting away from quantification, as shown in
Figure 4. One of the advantages of the HPSG sign
is that it contains all this information, making it pos-
sible to extract the particular view needed. In or-
der to make linking to other resources, such as the
sense annotation, easier predicates are labeled with
pointers back to their position in the original sur-
face string. For example, the predicate densha n 1
links to the surface characters between positions 0
and 3:2M.
</bodyText>
<figure confidence="0.4189015">
1*℄nm* AU 1_6 �
densha ya jidousha o unten suru hito
train or car ACC drive do person
ifUT1 “chauffeur”: “a person who drives a train or car”
</figure>
<figureCaption confidence="0.856189">
Figure 2: Syntactic View of the Definition of M+K
-f-1 untenshu “chauffeur”
</figureCaption>
<equation confidence="0.996914285714286">
e2:unknown&lt;0:13&gt;[ARG x5:_hito_n]
x7:densha_n_1&lt;0:3&gt;[]
x12:_jidousha_n&lt;4:7&gt;[]
x13:_ya_p_conj&lt;0:4&gt;[LIDX x7:_densha_n_1,
RIDX x12:_jidousha_n]
e23:_unten_s_2&lt;8:10&gt;[ARG1 x5:_hito_n]
e23:_unten_s_2&lt;8:10&gt;[ARG2 x13:_ya_p_conj]
</equation>
<figureCaption confidence="0.992989">
Figure 4: Simplified Dependency View of the Defi-
nition ofA+K-f-1 untenshu “chauffeur”
</figureCaption>
<subsectionHeader confidence="0.998934">
2.2 Semantic Annotation
</subsectionHeader>
<bodyText confidence="0.999829823529412">
The lexical semantic annotation uses the sense in-
ventory from Lexeed (Kasahara et al., 2004). All
words in the fundamental vocabulary are tagged
with their sense. For example, the wordJ�,-�L )ookii
“big” (of example sentence in Figure 1) is tagged as
sense 5 in the example sentence, with the meaning
“elder, older”.
The word senses are further linked to semantic
classes in a Japanese ontology. The ontology, Goi-
Taikei, consists of a hierarchy of 2,710 semantic
classes, defined for over 264,312 nouns, with a max-
imum depth of 12 (Ikehara et al., 1997). We show
the top 3 levels of the Goi-Taikei common noun on-
tology in Figure 5. The semantic classes are prin-
cipally defined for nouns (including verbal nouns),
although there is some information for verbs and ad-
jectives.
</bodyText>
<sectionHeader confidence="0.986377" genericHeader="method">
3 Parse Selection
</sectionHeader>
<bodyText confidence="0.981977">
Combining the broad-coverage JACY grammar and
the Hinoki corpus, we build a parse selection model
on top of the symbolic grammar. Given a set of can-
</bodyText>
<figure confidence="0.718794285714286">
UTTERANCE
NP
VP N
PP V
NP
PP
N CONJ N CASE-P V V
</figure>
<page confidence="0.763083">
26
</page>
<equation confidence="0.9900456875">
�
� �
1 �
�INDEXAU3untenshu
� �POS noun � �r 1
DEFINITION [1*1 ℄ n ¥*1 k AU1 2d J.4 a person who drives trains and cars]
� �
� � �
� �EXAMPLE )�&amp; (5 C &lt; 8b \*1 G AU31 D Xd6 G %P3 22 o
SENSE 1 �I dream of growing up and becoming a train driver
� �
� �HYPERNYMJ.4 hito “person”
� �
� �
� �SEM. CLASS (292:driver) (C (4:person))
WORDNET motorman1
</equation>
<figureCaption confidence="0.999579">
Figure 1: Dictionary Entry forA+K-f-1 untenshu “chauffeur”
</figureCaption>
<figure confidence="0.984556363636363">
frag-np
rel-cl-sbj-gap
hd-complement noun-le
hd-complement v-light
hd-complement
hd-complement case-p-acc-le
noun-le conj-le noun-le vn-trans-le v-light-le
\* ℄ n¥* k AU 2d J.
densha ya jidousha o unten suru hito
train or car ACC drive do person
AU31 “chauffeur”: “a person who drives a train or car”
</figure>
<figureCaption confidence="0.999887">
Figure 3: Derivation Tree of the Definition ofA+K-f-1 untenshu “chauffeur”
</figureCaption>
<bodyText confidence="0.944404090909091">
Phrasal nodes are labeled with identifiers of grammar rules, and (pre-terminal) lexical nodes with class names for types of lexical
entries.
Lvl 0 Lvl 1 Lvl 2 Lvl 3
didate analyses (for some Japanese string) according
to JACY, the goal is to rank parse trees by their prob-
ability: training a stochastic parse selection model
on the available treebank, we estimate statistics of
various features of candidate analyses from the tree-
bank. The definition and selection of features, thus,
is a central parameter in the design of an effective
parse selection model.
</bodyText>
<subsectionHeader confidence="0.999272">
3.1 Syntactic Features
</subsectionHeader>
<bodyText confidence="0.999989416666667">
The first model that we trained uses syntactic fea-
tures defined over HPSG derivation trees as summa-
rized in Table 1. For the closely related purpose of
parse selection over the English Redwoods treebank,
Toutanova et al. (2005) train a discriminative log-
linear model, using features defined over derivation
trees with non-terminals representing the construc-
tion types and lexical types of the HPSG grammar.
The basic feature set of our parse selection model
for Japanese is defined in the same way (correspond-
ing to the PCFG-S model of Toutanova et al. (2005)):
each feature capturing a sub-tree from the deriva-
</bodyText>
<figure confidence="0.977108394736842">
human
agent organization
facility
place region
natural place
object animate
inanimate
abstract mental state
noun thing action
human activity
event phenomenon
natural phen.
existence
system
relationship
property
relation state
shape
amount
location
time
Figure 5: Top 3 levels of the GoiTaikei Ontology
c
o
n
c
r
e
t
e
a
b
s
t
r
a
c
t
</figure>
<page confidence="0.985168">
27
</page>
<table confidence="0.9996365">
# sample features
1 (0 rel-cl-sbj-gap hd-complement noun-le)
1 (1 frag-np rel-cl-sbj-gap hd-complement noun-le)
1 (2 0 frag-np rel-cl-sbj-gap hd-complement noun-le)
2 (0 rel-cl-sbj-gap hd-complement)
2 (0 rel-cl-sbj-gap noun-le)
2 (1 frag-np rel-cl-sbj-gap hd-complement)
2 (1 frag-np rel-cl-sbj-gap noun-le)
3 (1 conj-le ya)
3 (2 noun-le conj-le ya)
3 (3 &lt; noun-le conj-le ya)
4 (1 conj-le)
4 (2 noun-le conj-le)
4 (3 &lt; noun-le conj-le)
</table>
<tableCaption confidence="0.999162">
Table 1: Example structural features extracted from
</tableCaption>
<bodyText confidence="0.998545785714286">
the derivation tree in Figure 3. The first column
numbers the feature template corresponding to each
example; in the examples, the first integer value
is a parameter to feature templates, i.e. the depth
of grandparenting (types #1 and#2) or n-gram size
(types #3 and #4). The special symbols 0 and &lt;
denote the root of the tree and left periphery of the
yield, respectively.
tion limited to depth one. Table 1 shows example
features extracted from our running example (Fig-
ure 3 above) in our MaxEnt models, where the fea-
ture template #1 corresponds to local derivation sub-
trees. We will refer to the parse selection model us-
ing only local structural features as SYN-1.
</bodyText>
<subsectionHeader confidence="0.52917">
3.1.1 Dominance Features
</subsectionHeader>
<bodyText confidence="0.999958538461538">
To reduce the effects of data sparseness, feature
type #2 in Table 1 provides a back-off to deriva-
tion sub-trees, where the sequence of daughters is
reduced to just the head daughter. Conversely, to
facilitate sampling of larger contexts than just sub-
trees of depth one, feature template #1 allows op-
tional grandparenting, including the upwards chain
of dominating nodes in some features. In our ex-
periments, we found that grandparenting of up to
three dominating nodes gave the best balance of en-
larged context vs. data sparseness. Enriching our ba-
sic model SYN-1 with these features we will hence-
forth call SYN-GP.
</bodyText>
<subsubsectionHeader confidence="0.531342">
3.1.2 N-Gram Features
</subsubsectionHeader>
<bodyText confidence="0.9999349375">
In addition to these dominance-oriented features
taken from the derivation trees of each parse tree,
our models also include more surface-oriented fea-
tures, viz. n-grams of lexical types with or without
lexicalization. Feature type #3 in Table 1 defines
n-grams of variable size, where (in a loose anal-
ogy to part-of-speech tagging) sequences of lexical
types capture syntactic category assignments. Fea-
ture templates #3 and #4 only differ with regard to
lexicalization, as the former includes the surface to-
ken associated with the rightmost element of each
n-gram (loosely corresponding to the emission prob-
abilities in an HMM tagger). We used a maximum
n-gram size of two in the experiments reported here,
again due to its empirically determined best overall
performance.
</bodyText>
<subsectionHeader confidence="0.997681">
3.2 Semantic Features
</subsectionHeader>
<bodyText confidence="0.999995047619048">
In order to define semantic parse selection features,
we use a reduction of the full semantic representa-
tion (MRS) into ‘variable-free’ elementary depen-
dencies. The conversion centrally rests on a notion
of one distinguished variable in each semantic rela-
tion. For most types of relations, the distinguished
variable corresponds to the main index (ARG0 in the
examples above), e.g. an event variable for verbal re-
lations and a referential index for nominals. Assum-
ing further that, by and large, there is a unique re-
lation for each semantic variable for which it serves
as the main index (thus assuming, for example, that
adjectives and adverbs have event variables of their
own, which can be motivated in predicative usages
at least), an MRS can be broken down into a set of
basic dependency tuples of the form shown in Fig-
ure 4 (Oepen and Lønning, 2006).
All predicates are indexed to the position of the
word or words that introduced them in the input sen-
tence (&lt;start:end&gt;). This allows us to link them
to the sense annotations in the corpus.
</bodyText>
<subsectionHeader confidence="0.930772">
3.2.1 Basic Semantic Dependencies
</subsectionHeader>
<bodyText confidence="0.999957272727273">
The basic semantic model, SEM-Dep, consists of
features based on a predicate and its arguments taken
from the elementary dependencies. For example,
consider the dependencies for densha ya jidousha-
wo unten suru hito “a person who drives a train or
car” given in Figure 4. The predicate unten “drive”
has two arguments: ARG1 hito “person” and ARG2
jidousha “car”.
From these, we produce several features (See Ta-
ble 2). One has all arguments and their labels (#20).
We also produce various back offs: #21 introduces
</bodyText>
<page confidence="0.989282">
28
</page>
<table confidence="0.999971636363636">
# sample features
20 (0 unten s ARG1 hito n 1 ARG2 ya p conj)
20 (0 ya p conj LIDX densha n 1 RIDX jidousha n 1)
21 (1 unten s ARG1 hito n 1)
21 (1 unten s ARG2 jidousha n 1)
21 (1 ya p conj LIDX densha n 1)
21 (1 ya p conj RIDX jidousha n 1)
22 (2 unten s hito n 1 jidousha n 1)
23 (3 unten s hito n 1)
23 (3 unten s jidousha n 1)
� � �
</table>
<tableCaption confidence="0.941217">
Table 2: Example semantic features (SEM-Dep) ex-
tracted from the dependency tree in Figure 4.
</tableCaption>
<bodyText confidence="0.993827333333333">
only one argument at a time, #22 provides unlabeled
relations, #23 provides one unlabeled relation at a
time and so on.
Each combination of a predicate and its related
argument(s) becomes a feature. These resemble the
basic semantic features used by Toutanova et al.
(2005). We further simplify these by collapsing
some non-informative predicates, e.g. the unknown
predicate used in fragments.
</bodyText>
<subsectionHeader confidence="0.963532">
3.2.2 Word Sense and Semantic Class
Dependencies
</subsectionHeader>
<bodyText confidence="0.999966473684211">
We created two sets of features based only on the
word senses. For SEM-WS we used the sense anno-
tation to replace each underspecified MRS predicate
by a predicate indicating the word sense. This used
the gold standard sense tags. For SEM-Class, we used
the sense annotation to replace each predicate by its
Goi-Taikei semantic class.
In addition, to capture more useful relationships,
conjunctions were followed down into the left and
right daughters, and added as separate features. The
semantic classes for 2ft*1densha “train” and H A
*1jidousha “car” are both (988:land vehicle),
while A+R1 unten “drive” is (2003:motion) and
04 hito “person”is (4: human). The sample features
of SEM-Class are shown in Table 3.
These features provide more specific information,
in the case of the word sense, and semantic smooth-
ing in the case of the semantic classes, as words are
binned into only 2,700 classes.
</bodyText>
<subsectionHeader confidence="0.969898">
3.2.3 Superordinate Semantic Classes
</subsectionHeader>
<bodyText confidence="0.999974666666667">
We further smooth these features by replacing the
semantic classes with their hypernyms at a given
level (SEM-L). We investigated levels 2 to 5. Pred-
</bodyText>
<table confidence="0.99991075">
# sample features
40 (0 unten s ARG1 C4 ARG2 C988)
40 (1 C2003 ARG1 C4 ARG2 C988)
40 (1 C2003 ARG1 C4 ARG2 C988)
40 (0 ya p conj LIDX C988 RIDX C988)
41 (2 unten s ARG1 C4)
41 (2 unten s ARG2 C988)
� � �
</table>
<tableCaption confidence="0.922516">
Table 3: Example semantic class features (SEM-
Class).
</tableCaption>
<figureCaption confidence="0.920779666666667">
icates are binned into only 9 classes at level 2, 30
classes at level 3, 136 classes at level 4, and 392
classes at level 5.
</figureCaption>
<bodyText confidence="0.8061647">
For example, at level 3, the hypernym class
for (988:land vehicle) is (706:inanimate),
(2003:motion) is (1236:human activity)
and (4:human) is unchanged. So we used
(706:inanimate) and (1236:human activity)
to make features in the same way as Table 3.
An advantage of these underspecified semantic
classes is that they are more robust to errors in word
sense disambiguation — fine grained sense distinc-
tions can be ignored.
</bodyText>
<subsectionHeader confidence="0.688289">
3.2.4 Valency Dictionary Compatability
</subsectionHeader>
<bodyText confidence="0.999956173913044">
The last kind of semantic information we use is
valency information, taken from the Japanese side
of the Goi-Taikei Japanese-English valency dictio-
nary as extended by Fujita and Bond (2004).This va-
lency dictionary has detailed information about the
argument properties of verbs and adjectives, includ-
ing subcategorization and selectional restrictions. A
simplified entry of the Japanese side for A +R t
7D unten-suru “drive” is shown in Figure 6.
Each entry has a predicate and several case-slots.
Each case-slot has information such as grammatical
function, case-marker, case-role (N1, N2, ...) and
semantic restrictions. The semantic restrictions are
defined by the Goi-Taikei’s semantic classes.
On the Japanese side of Goi-Taikei’s valency
dictionary, there are 10,146 types of verbs giving
18,512 entries and 1,723 types of adjectives giving
2,618 entries.
The valency based features were constructed by
first finding the most appropriate pattern, and then
recording how well it matched.
To find the most appropriate pattern, we extracted
candidate dictionary entries whose lemma is the
</bodyText>
<page confidence="0.967041">
29
</page>
<equation confidence="0.7453495">
PID:300513
LN1 &lt;4:people&gt; &amp;quot;%&amp;quot; ga
�N2 &lt;986:vehicles&gt; &amp;quot;k&amp;quot; o
LAfj,&apos;2dunten-suru
</equation>
<figureCaption confidence="0.8734875">
Figure 6:Afj,&apos;2dunten-suru “N1 drive N2”.
PID is the verb’s Pattern ID
</figureCaption>
<table confidence="0.999945181818182">
# sample features
31 (0 High)
31 (1 300513 High)
31 (2 2)
31 (3 R:High)
31 (4 300513 R:High)
32 (1 unten s High)
32 (4 unten s R:High)
33 (5 N1 C High)
33 (7 C)
� � �
</table>
<tableCaption confidence="0.999875">
Table 4: Example semantic features (SP)
</tableCaption>
<bodyText confidence="0.9999346">
same as the predicate in the sentence: for exam-
ple we look up all entries forAfj,&apos;2dunten-
suru “drive”. Then, for each candidate pattern, we
mapped its arguments to the target predicate’s ar-
guments via case-markers. If the target predicate
has no suitable argument, we mapped to comitative
phrase. Finally, for each candidate patterns, we cal-
culate a matching score1 and select the pattern which
has the best score.
Once we have the most appropriate pattern,
we then construct features that record how good
the match is (Table 4). These include: the to-
tal score, with or without the verb’s Pattern ID
(High/Med/Low/Zero: #31 0,1), the number of filled
arguments (#31 2), the fraction of filled arguments
vs all arguments (High/Med/Low/Zero: #31 3,4),
the score for each argument of the pattern (#32 5)
and the types of matches (#32 5,7).
These scores allow us to use information about
word usage in an exisiting dictionary.
</bodyText>
<sectionHeader confidence="0.987276" genericHeader="method">
4 Evaluation and Results
</sectionHeader>
<bodyText confidence="0.99923125">
We trained and tested on a subset of the dictionary
definition and example sentences in the Hinoki cor-
pus. This consists of those sentences with ambigu-
ous parses which have been annotated so that the
</bodyText>
<footnote confidence="0.9203905">
1The scoring method follows Bond and Shirai (1997), and
depends on the goodness of the matches of the arguments.
</footnote>
<bodyText confidence="0.999852428571429">
number of parses has been reduced (Table 5). That
is, we excluded unambiguous sentences (with a sin-
gle parse), and those where the annotators judged
that no parse gave the correct semantics. This does
not necessarily mean that there is a single correct
parse, we allow the annotator to claim that two or
more parses are equally appropriate.
</bodyText>
<table confidence="0.999185">
Corpus # Sents Length Parses/Sent
(Ave) (Ave)
Definitions Train 30,345 9.3 190.1
Test 2,790 10.1 177.0
Examples Train 27,081 10.9 74.1
Test 2,587 10.4 47.3
</table>
<tableCaption confidence="0.999504">
Table 5: Data of Sets for Evaluation
</tableCaption>
<bodyText confidence="0.999831636363636">
Dictionary definition sentences are a different
genre to other commonly used test sets (e.g news-
paper text in the Penn Treebank or travel dialogues
in Redwoods). However, they are valid examples
of naturally occurring texts and a native speaker can
read and understand them without special training.
The main differences with newspaper text is that
the definition sentences are shorter, contain more
fragments (especially NPs as single utterances) and
fewer quoting and proper names. The main differ-
ences with travel dialogues is the lack of questions.
</bodyText>
<subsectionHeader confidence="0.995614">
4.1 A Maximum Entropy Ranker
</subsectionHeader>
<bodyText confidence="0.99956">
Log-linear models provide a very flexible frame-
work that has been widely used for a range of tasks
in NLP, including parse selection and reranking for
machine translation. We use a maximum entropy
/ minimum divergence (MEMD) modeler to train
the parse selection model. Specifically, we use the
open-source Toolkit for Advanced Discriminative
Modeling (TADM:2 Malouf, 2002) for training, us-
ing its limited-memory variable metric as the opti-
mization method and determining best-performing
convergence thresholds and prior sizes experimen-
tally. A comparison of this learner with the use
of support vector machines over similar data found
that the SVMs gave comparable results but were far
slower (Baldridge and Osborne, 2007). Because we
are investigating the effects of various different fea-
tures, we chose the faster learner.
</bodyText>
<footnote confidence="0.967072">
2http://tadm.sourceforge.net
</footnote>
<page confidence="0.991685">
30
</page>
<table confidence="0.995217888888889">
Method Definitions Examples
Accuracy Features Accuracy Features
(%) (×1000) (%) (×1000)
SYN-1 52.8 7 67.6 8
SYN-GP 62.7 266 76.0 196
SYN-ALL 63.8 316 76.2 245
SYN baseline 16.4 random 22.3 random
SEM-Dep 57.3 1,189 58.7 675
+SEM-WS 56.2 1,904 59.0 1,486
+SEM-Class 57.5 2,018 59.7 1,669
+SEM-L2 60.3 808 62.9 823
+SEM-L3 59.8 876 62.8 879
+SEM-L4 59.9 1,000 62.3 973
+SEM-L5 60.4 1,240 61.3 1,202
+SP 59.1 1,218 68.2 819
+SEM-ALL 62.7 3,384 69.1 2,693
SYN-SEM 69.5 2,476 79.2 2,126
SEM baseline 20.3 random 22.8 random
</table>
<tableCaption confidence="0.979785">
Table 6: Parse Selection Results
</tableCaption>
<sectionHeader confidence="0.569802" genericHeader="evaluation">
4.2 Results
</sectionHeader>
<bodyText confidence="0.999904933333333">
The results for most of the models discussed in the
previous section are shown in Table 6. The accuracy
is exact match for the entire sentence: a model gets
a point only if its top ranked analysis is the same as
an analysis selected as correct in Hinoki. This is a
stricter metric than component based measures (e.g.,
labelled precision) which award partial credit for in-
correct parses. For the syntactic models, the base-
line (random choice) is 16.4% for the definitions and
22.3% for the examples. Definition sentences are
harder to parse than the example sentences. This
is mainly because they have fewer relative clauses
and coordinate NPs, both large sources of ambigu-
ity. For the semantic and combined models, multiple
sentences can have different parses but the same se-
mantics. In this case all sentences with the correct
semantics are scored as good. This raises the base-
lines to 20.3 and 22.8% respectively.
Even the simplest models (SYN-1 and SEM-Dep)
give a large improvement over the baseline. Adding
grandparenting to the syntactic model has a large
improvement (SYN-GP), but adding lexical n-grams
gave only a slight improvement over this (SYN-ALL).
The effect of smoothing by superordinate seman-
tic classes (SEM-Class), shows a modest improve-
ment. The syntactic model already contains a back-
off to lexical-types, we hypothesize that the seman-
tic classes behave in the same way. Surprisingly, as
we add more data, the very top level of the seman-
tic class hierarchy performs almost as well as the
</bodyText>
<figure confidence="0.9911405">
0 20 40 60 80 100
% of training data (30,345 sentences)
</figure>
<figureCaption confidence="0.999933">
Figure 7: Learning Curves (Definitions)
</figureCaption>
<bodyText confidence="0.99981555">
more detailed levels. The features using the valency
dictionary (SP) also provide a considerable improve-
ment over the basic dependencies.
Combining all the semantic features (SEM-ALL)
provides a clear improvement, suggesting that the
information is heterogeneous. Finally, combing the
syntactic and semantic features gives the best results
by far (SYN-SEM: SYN-ALL + SEM-Dep + SEM-Class +
SEM-L2 + SP). The definitions sentences are harder
syntactically, and thus get more of a boost from the
semantics. The semantics still improve performance
for the example sentences.
The semantic class based sense features used here
are based on manual annotation, and thus show an
upper bound on the effects of these features. This
is not an absolute upper bound on the use of sense
information — it may be possible to improve further
through feature engineering. The learning curves
(Fig 7) have not yet flattened out. We can still im-
prove by increasing the size of the training data.
</bodyText>
<sectionHeader confidence="0.999649" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9999194">
Bikel (2000) combined sense information and parse
information using a subset of SemCor (with Word-
Net senses and Penn-II treebanks) to produce a com-
bined model. This model did not use semantic de-
pendency relations, but only syntactic dependen-
cies augmented with heads, which suggests that the
deeper structural semantics provided by the HPSG
parser is important. Xiong et al. (2005) achieved
only a very minor improvement over a plain syntac-
tic model, using features based on both the corre-
lation between predicates and their arguments, and
between predicates and the hypernyms of their argu-
ments (using HowNet). However, they do not inves-
tigate generalizing to different levels than a word’s
immediate hypernym.
</bodyText>
<figure confidence="0.9977188125">
+bcld
20
ld ldld ld ld ld ld ld ld
+ + + + + + + + + +
bc bc bc bc bc bc bc bc bc
ld
bc
70
60
50
40
30
SYN-SEM
SEM-ALL
SYN-ALL
Sent. Accuracy
</figure>
<page confidence="0.999874">
31
</page>
<bodyText confidence="0.999986894736842">
Pioneering work by Toutanova et al. (2005) and
Baldridge and Osborne (2007) on parse selection for
an English HPSG treebank used simpler semantic
features without sense information, and got a far less
dramatic improvement when they combined syntac-
tic and semantic information.
The use of hand-crafted lexical resources such as
the Goi-Taikei ontology is sometimes criticized on
the grounds that such resources are hard to produce
and scarce. While it is true that valency lexicons
and sense hierarchies are hard to produce, they are
of such value that they have already been created for
all of the languages we know of which have large
treebanks. In fact, there are more languages with
WordNets than large treebanks.
In future work we intend to confirm that we can
get improved results with raw sense disambiguation
results not just the gold standard annotations and test
the results on other sections of the Hinoki corpus.
</bodyText>
<sectionHeader confidence="0.999731" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999995">
We have shown that sense-based semantic features
combined with ontological information are effec-
tive for parse selection. Training and testing on
the definition subset of the Hinoki corpus, a com-
bined model gave a 5.6% improvement in parse se-
lection accuracy over a model using only syntactic
features (63.8% → 69.4%). Similar results (76.2%
→ 79.2%) were found with example sentences.
</bodyText>
<sectionHeader confidence="0.999252" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999834580246914">
Jason Baldridge and Miles Osborne. 2007. Active learning and
logarithmic opinion pools for HPSG parse selection. Natural
Language Engineering, 13(1):1–32.
Daniel M. Bikel. 2000. A statistical model for parsing and
word-sense disambiguation. In Proceedings of the Joint SIG-
DAT Conference on Empirical Methods in Natural Language
Processing and Very Large Corpora, pages 155–163. Hong
Kong.
Francis Bond, Sanae Fujita, and Takaaki Tanaka. 2007. The Hi-
noki syntactic and semantic treebank of Japanese. Language
Resources and Evaluation. (Special issue on Asian language
technology).
Francis Bond and Satoshi Shirai. 1997. Practical and efficient
organization of a large valency dictionary. In Workshop on
Multilingual Information Processing — Natural Language
Processing Pacific Rim Symposium ’97: NLPRS-97. Phuket.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag.
2005. Minimal Recursion Semantics. An introduction. Re-
search on Language and Computation, 3(4):281–332.
Christine Fellbaum, editor. 1998. WordNet: An Electronic Lex-
ical Database. MIT Press.
Sanae Fujita and Francis Bond. 2004. A method of creating
new bilingual valency entries using alternations. In Gilles
S´erasset, editor, COLING 2004 Multilingual Linguistic Re-
sources, pages 41–48. Geneva.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The
90% solution. In Proceedings of the Human Language
Technology Conference of the NAACL, Companion Volume:
Short Papers, pages 57–60. Association for Computational
Linguistics, New York City, USA. URL http://www.
aclweb.org/anthology/N/N06/N06-2015.
Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio
Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi
Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei —
A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 vol-
umes/CDROM.
Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki
Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki
Amano. 2004. Construction of a Japanese semantic lexicon:
Lexeed. In IPSG SIG: 2004-NLC-159, pages 75–82. Tokyo.
(in Japanese).
Dan Klein and Christopher D. Manning. 2003. Accurate un-
lexicalized parsing. In Erhard Hinrichs and Dan Roth, edi-
tors, Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics, pages 423–430. URL
http://www.aclweb.org/anthology/P03-1054.pdf.
Robert Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In CONLL-2002, pages
49–55. Taipei, Taiwan.
Robert Malouf and Gertjan van Noord. 2004. Wide cover-
age parsing with stochastic attribute value grammars. In
IJCNLP-04 Workshop: Beyond shallow analyses - For-
malisms and statistical modeling for deep analyses. JST
CREST. URL http://www-tsujii.is.s.u-tokyo.ac.
jp/bsa/papers/malouf.pdf.
Stephan Oepen, Dan Flickinger, Kristina Toutanova, and
Christoper D. Manning. 2004. LinGO redwoods: A rich and
dynamic treebank for HPSG. Research on Language and
Computation, 2(4):575–596.
Stephan Oepen and Jan Tore Lønning. 2006. Discriminant-
based MRS banking. In Proceedings of the 5th International
Conference on Language Resources and Evaluation (LREC
2006). Genoa, Italy.
Carl Pollard and Ivan A. Sag. 1994. Head Driven Phrase Struc-
ture Grammar. University of Chicago Press, Chicago.
Melanie Siegel and Emily M. Bender. 2002. Efficient deep pro-
cessing of Japanese. In Proceedings of the 3rd Workshop on
Asian Language Resources and International Standardiza-
tion at the 19th International Conference on Computational
Linguistics, pages 1–8. Taipei.
Kristina Toutanova, Christopher D. Manning, Dan Flickinger,
and Stephan Oepen. 2005. Stochastic HPSG parse disam-
biguation using the redwoods corpus. Research on Language
and Computation, 3(1):83–105.
Deyi Xiong, Qun Liu Shuanglong Li and, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the Penn Chinese treebank
with semantic knowledge. In Robert Dale, Jian Su Kam-Fai
Wong and, and Oi Yee Kwong, editors, Natural Language
Processing — IJCNLP 005: Second International Joint Con-
ference Proceedings, pages 70–81. Springer-Verlag.
</reference>
<page confidence="0.999297">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.578762">
<title confidence="0.999996">Exploiting Semantic Information for HPSG Parse Selection</title>
<author confidence="0.995651">Francis Takaaki</author>
<affiliation confidence="0.995275">Communication Science Laboratories, Nippon Telegraph and Telephone Institute of Information and Communications Technology</affiliation>
<address confidence="0.627931">of Oslo, Department of Informatics (Norway)</address>
<abstract confidence="0.990070714285714">In this paper we present a framework for experimentation on parse selection using syntactic and semantic features. Results are given for syntactic features, dependency relations and the use of semantic classes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jason Baldridge</author>
<author>Miles Osborne</author>
</authors>
<title>Active learning and logarithmic opinion pools for HPSG parse selection.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>1</issue>
<contexts>
<context position="21344" citStr="Baldridge and Osborne, 2007" startWordPosition="3490" endWordPosition="3493">asks in NLP, including parse selection and reranking for machine translation. We use a maximum entropy / minimum divergence (MEMD) modeler to train the parse selection model. Specifically, we use the open-source Toolkit for Advanced Discriminative Modeling (TADM:2 Malouf, 2002) for training, using its limited-memory variable metric as the optimization method and determining best-performing convergence thresholds and prior sizes experimentally. A comparison of this learner with the use of support vector machines over similar data found that the SVMs gave comparable results but were far slower (Baldridge and Osborne, 2007). Because we are investigating the effects of various different features, we chose the faster learner. 2http://tadm.sourceforge.net 30 Method Definitions Examples Accuracy Features Accuracy Features (%) (×1000) (%) (×1000) SYN-1 52.8 7 67.6 8 SYN-GP 62.7 266 76.0 196 SYN-ALL 63.8 316 76.2 245 SYN baseline 16.4 random 22.3 random SEM-Dep 57.3 1,189 58.7 675 +SEM-WS 56.2 1,904 59.0 1,486 +SEM-Class 57.5 2,018 59.7 1,669 +SEM-L2 60.3 808 62.9 823 +SEM-L3 59.8 876 62.8 879 +SEM-L4 59.9 1,000 62.3 973 +SEM-L5 60.4 1,240 61.3 1,202 +SP 59.1 1,218 68.2 819 +SEM-ALL 62.7 3,384 69.1 2,693 SYN-SEM 69.5 </context>
<context position="25568" citStr="Baldridge and Osborne (2007)" startWordPosition="4196" endWordPosition="4199">tural semantics provided by the HPSG parser is important. Xiong et al. (2005) achieved only a very minor improvement over a plain syntactic model, using features based on both the correlation between predicates and their arguments, and between predicates and the hypernyms of their arguments (using HowNet). However, they do not investigate generalizing to different levels than a word’s immediate hypernym. +bcld 20 ld ldld ld ld ld ld ld ld + + + + + + + + + + bc bc bc bc bc bc bc bc bc ld bc 70 60 50 40 30 SYN-SEM SEM-ALL SYN-ALL Sent. Accuracy 31 Pioneering work by Toutanova et al. (2005) and Baldridge and Osborne (2007) on parse selection for an English HPSG treebank used simpler semantic features without sense information, and got a far less dramatic improvement when they combined syntactic and semantic information. The use of hand-crafted lexical resources such as the Goi-Taikei ontology is sometimes criticized on the grounds that such resources are hard to produce and scarce. While it is true that valency lexicons and sense hierarchies are hard to produce, they are of such value that they have already been created for all of the languages we know of which have large treebanks. In fact, there are more lang</context>
</contexts>
<marker>Baldridge, Osborne, 2007</marker>
<rawString>Jason Baldridge and Miles Osborne. 2007. Active learning and logarithmic opinion pools for HPSG parse selection. Natural Language Engineering, 13(1):1–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>A statistical model for parsing and word-sense disambiguation.</title>
<date>2000</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>155--163</pages>
<publisher>Hong Kong.</publisher>
<contexts>
<context position="24648" citStr="Bikel (2000)" startWordPosition="4033" endWordPosition="4034"> SEM-L2 + SP). The definitions sentences are harder syntactically, and thus get more of a boost from the semantics. The semantics still improve performance for the example sentences. The semantic class based sense features used here are based on manual annotation, and thus show an upper bound on the effects of these features. This is not an absolute upper bound on the use of sense information — it may be possible to improve further through feature engineering. The learning curves (Fig 7) have not yet flattened out. We can still improve by increasing the size of the training data. 5 Discussion Bikel (2000) combined sense information and parse information using a subset of SemCor (with WordNet senses and Penn-II treebanks) to produce a combined model. This model did not use semantic dependency relations, but only syntactic dependencies augmented with heads, which suggests that the deeper structural semantics provided by the HPSG parser is important. Xiong et al. (2005) achieved only a very minor improvement over a plain syntactic model, using features based on both the correlation between predicates and their arguments, and between predicates and the hypernyms of their arguments (using HowNet). </context>
</contexts>
<marker>Bikel, 2000</marker>
<rawString>Daniel M. Bikel. 2000. A statistical model for parsing and word-sense disambiguation. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 155–163. Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Sanae Fujita</author>
<author>Takaaki Tanaka</author>
</authors>
<title>The Hinoki syntactic and semantic treebank of Japanese. Language Resources and Evaluation. (Special issue on Asian language technology).</title>
<date>2007</date>
<contexts>
<context position="2003" citStr="Bond et al., 2007" startWordPosition="295" endWordPosition="298">erordinate concepts capital (D stocks) and move upward (D sky rocket) frequently appear together, which suggests that using word senses and their hypernyms as features may be useful However, to date, there have been few combinations of sense information together with symbolic grammars and statistical models. We hypothesize that one of the reasons for the lack of success is that there has been no resource annotated with both 25 syntactic and semantic information. In this paper, we use a treebank with both syntactic information (HPSG parses) and semantic information (sense tags from a lexicon) (Bond et al., 2007). We use this to train parse selection models using both syntactic and semantic features. A model trained using syntactic features combined with semantic information outperforms a model using purely syntactic information by a wide margin (69.4% sentence parse accuracy vs. 63.8% on definition sentences). 2 The Hinoki Corpus There are now some corpora being built with the syntactic and semantic information necessary to investigate the use of semantic information in parse selection. In English, the OntoNotes project (Hovy et al., 2006) is combining sense tags with the Penn treebank. We are using </context>
</contexts>
<marker>Bond, Fujita, Tanaka, 2007</marker>
<rawString>Francis Bond, Sanae Fujita, and Takaaki Tanaka. 2007. The Hinoki syntactic and semantic treebank of Japanese. Language Resources and Evaluation. (Special issue on Asian language technology).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francis Bond</author>
<author>Satoshi Shirai</author>
</authors>
<title>Practical and efficient organization of a large valency dictionary.</title>
<date>1997</date>
<booktitle>In Workshop on Multilingual Information Processing — Natural Language Processing Pacific Rim Symposium ’97: NLPRS-97.</booktitle>
<location>Phuket.</location>
<contexts>
<context position="19443" citStr="Bond and Shirai (1997)" startWordPosition="3188" endWordPosition="3191">t the verb’s Pattern ID (High/Med/Low/Zero: #31 0,1), the number of filled arguments (#31 2), the fraction of filled arguments vs all arguments (High/Med/Low/Zero: #31 3,4), the score for each argument of the pattern (#32 5) and the types of matches (#32 5,7). These scores allow us to use information about word usage in an exisiting dictionary. 4 Evaluation and Results We trained and tested on a subset of the dictionary definition and example sentences in the Hinoki corpus. This consists of those sentences with ambiguous parses which have been annotated so that the 1The scoring method follows Bond and Shirai (1997), and depends on the goodness of the matches of the arguments. number of parses has been reduced (Table 5). That is, we excluded unambiguous sentences (with a single parse), and those where the annotators judged that no parse gave the correct semantics. This does not necessarily mean that there is a single correct parse, we allow the annotator to claim that two or more parses are equally appropriate. Corpus # Sents Length Parses/Sent (Ave) (Ave) Definitions Train 30,345 9.3 190.1 Test 2,790 10.1 177.0 Examples Train 27,081 10.9 74.1 Test 2,587 10.4 47.3 Table 5: Data of Sets for Evaluation Dic</context>
</contexts>
<marker>Bond, Shirai, 1997</marker>
<rawString>Francis Bond and Satoshi Shirai. 1997. Practical and efficient organization of a large valency dictionary. In Workshop on Multilingual Information Processing — Natural Language Processing Pacific Rim Symposium ’97: NLPRS-97. Phuket.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Minimal Recursion Semantics. An introduction.</title>
<date>2005</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="5389" citStr="Copestake et al., 2005" startWordPosition="838" endWordPosition="841">have gapped and non-gapped readings. In the gapped reading (selected here),J�,hito is the subject ofA+Kunten “drive”. In the non-gapped reading there is some underspecified relation between the modifee and the verb phrase. This is similar to the difference in the two readings of the day he knew in English: “the day that he knew about” (gapped) vs “the day on which he knew (something)” (nongapped). Such semantic ambiguity is resolved by selecting the correct derivation tree that includes the applied rules in building the tree (Fig 3). The semantic representation is Minimal Recursion Semantics (Copestake et al., 2005). We simplify this into a dependency representation, further abstracting away from quantification, as shown in Figure 4. One of the advantages of the HPSG sign is that it contains all this information, making it possible to extract the particular view needed. In order to make linking to other resources, such as the sense annotation, easier predicates are labeled with pointers back to their position in the original surface string. For example, the predicate densha n 1 links to the surface characters between positions 0 and 3:2M. 1*℄nm* AU 1_6 � densha ya jidousha o unten suru hito train or car </context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag. 2005. Minimal Recursion Semantics. An introduction. Research on Language and Computation, 3(4):281–332.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christine Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christine Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanae Fujita</author>
<author>Francis Bond</author>
</authors>
<title>A method of creating new bilingual valency entries using alternations.</title>
<date>2004</date>
<booktitle>COLING 2004 Multilingual Linguistic Resources,</booktitle>
<pages>41--48</pages>
<editor>In Gilles S´erasset, editor,</editor>
<location>Geneva.</location>
<contexts>
<context position="16984" citStr="Fujita and Bond (2004)" startWordPosition="2786" endWordPosition="2789">ypernym class for (988:land vehicle) is (706:inanimate), (2003:motion) is (1236:human activity) and (4:human) is unchanged. So we used (706:inanimate) and (1236:human activity) to make features in the same way as Table 3. An advantage of these underspecified semantic classes is that they are more robust to errors in word sense disambiguation — fine grained sense distinctions can be ignored. 3.2.4 Valency Dictionary Compatability The last kind of semantic information we use is valency information, taken from the Japanese side of the Goi-Taikei Japanese-English valency dictionary as extended by Fujita and Bond (2004).This valency dictionary has detailed information about the argument properties of verbs and adjectives, including subcategorization and selectional restrictions. A simplified entry of the Japanese side for A +R t 7D unten-suru “drive” is shown in Figure 6. Each entry has a predicate and several case-slots. Each case-slot has information such as grammatical function, case-marker, case-role (N1, N2, ...) and semantic restrictions. The semantic restrictions are defined by the Goi-Taikei’s semantic classes. On the Japanese side of Goi-Taikei’s valency dictionary, there are 10,146 types of verbs g</context>
</contexts>
<marker>Fujita, Bond, 2004</marker>
<rawString>Sanae Fujita and Francis Bond. 2004. A method of creating new bilingual valency entries using alternations. In Gilles S´erasset, editor, COLING 2004 Multilingual Linguistic Resources, pages 41–48. Geneva.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>57--60</pages>
<location>New York City, USA.</location>
<note>URL http://www.</note>
<contexts>
<context position="2541" citStr="Hovy et al., 2006" startWordPosition="378" endWordPosition="381">parses) and semantic information (sense tags from a lexicon) (Bond et al., 2007). We use this to train parse selection models using both syntactic and semantic features. A model trained using syntactic features combined with semantic information outperforms a model using purely syntactic information by a wide margin (69.4% sentence parse accuracy vs. 63.8% on definition sentences). 2 The Hinoki Corpus There are now some corpora being built with the syntactic and semantic information necessary to investigate the use of semantic information in parse selection. In English, the OntoNotes project (Hovy et al., 2006) is combining sense tags with the Penn treebank. We are using Japanese data from the Hinoki Corpus consisting of around 95,000 dictionary definition and example sentences (Bond et al., 2007) annotated with both syntactic parses and senses from the same dictionary. 2.1 Syntactic Annotation Syntactic annotation in Hinoki is grammar based corpus annotation done by selecting the best parse (or parses) from the full analyses derived by a broadcoverage precision grammar. The grammar is an HPSG implementation (JACY: Siegel and Bender, 2002), which provides a high level of detail, marking not only dep</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 57–60. Association for Computational Linguistics, New York City, USA. URL http://www. aclweb.org/anthology/N/N06/N06-2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoru Ikehara</author>
<author>Masahiro Miyazaki</author>
<author>Satoshi Shirai</author>
</authors>
<title>Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi.</title>
<date>1997</date>
<booktitle>Goi-Taikei — A Japanese Lexicon. Iwanami Shoten,</booktitle>
<volume>5</volume>
<pages>volumes/CDROM.</pages>
<location>Tokyo.</location>
<contexts>
<context position="4322" citStr="Ikehara et al., 1997" startWordPosition="665" endWordPosition="668">is that any sentences which the parser cannot parse remain unannotated, at least unless we were to fall back on full manual mark-up of their analyses. The actual annotation process uses the same tools as the Redwoods treebank of English (Oepen et al., 2004). A (simplified) example of an entry is given in Figure 1. Each entry contains the word itself, its part of speech, and its lexical type(s) in the grammar. Each sense then contains definition and example sentences, links to other senses in the lexicon (such as hypernym), and links to other resources, such as the Goi-Taikei Japanese Lexicon (Ikehara et al., 1997) and WordNet (Fellbaum, 1998). Each content word of the definition and example sentences is annotated with sense tags from the same lexicon. There were 4 parses for the definition sentence. The correct parse, shown as a phrase structure tree, is shown in Figure 2. The two sources of ambiguity are the conjunction and the relative clause. The parser also allows the conjunction to combine2M densha andJ�,hito. In Japanese, relative clauses can have gapped and non-gapped readings. In the gapped reading (selected here),J�,hito is the subject ofA+Kunten “drive”. In the non-gapped reading there is som</context>
<context position="7024" citStr="Ikehara et al., 1997" startWordPosition="1088" endWordPosition="1091">e Definition ofA+K-f-1 untenshu “chauffeur” 2.2 Semantic Annotation The lexical semantic annotation uses the sense inventory from Lexeed (Kasahara et al., 2004). All words in the fundamental vocabulary are tagged with their sense. For example, the wordJ�,-�L )ookii “big” (of example sentence in Figure 1) is tagged as sense 5 in the example sentence, with the meaning “elder, older”. The word senses are further linked to semantic classes in a Japanese ontology. The ontology, GoiTaikei, consists of a hierarchy of 2,710 semantic classes, defined for over 264,312 nouns, with a maximum depth of 12 (Ikehara et al., 1997). We show the top 3 levels of the Goi-Taikei common noun ontology in Figure 5. The semantic classes are principally defined for nouns (including verbal nouns), although there is some information for verbs and adjectives. 3 Parse Selection Combining the broad-coverage JACY grammar and the Hinoki corpus, we build a parse selection model on top of the symbolic grammar. Given a set of canUTTERANCE NP VP N PP V NP PP N CONJ N CASE-P V V 26 � � � 1 � �INDEXAU3untenshu � �POS noun � �r 1 DEFINITION [1*1 ℄ n ¥*1 k AU1 2d J.4 a person who drives trains and cars] � � � � � � �EXAMPLE )�&amp; (5 C &lt; 8b \*1 G</context>
</contexts>
<marker>Ikehara, Miyazaki, Shirai, 1997</marker>
<rawString>Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura, Yoshifumi Ooyama, and Yoshihiko Hayashi. 1997. Goi-Taikei — A Japanese Lexicon. Iwanami Shoten, Tokyo. 5 volumes/CDROM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaname Kasahara</author>
<author>Hiroshi Sato</author>
<author>Francis Bond</author>
<author>Takaaki Tanaka</author>
<author>Sanae Fujita</author>
<author>Tomoko Kanasugi</author>
<author>Shigeaki Amano</author>
</authors>
<title>Construction of a Japanese semantic lexicon: Lexeed.</title>
<date>2004</date>
<booktitle>In IPSG SIG: 2004-NLC-159,</booktitle>
<pages>75--82</pages>
<location>Tokyo.</location>
<note>(in Japanese).</note>
<contexts>
<context position="6563" citStr="Kasahara et al., 2004" startWordPosition="1010" endWordPosition="1013">nsha ya jidousha o unten suru hito train or car ACC drive do person ifUT1 “chauffeur”: “a person who drives a train or car” Figure 2: Syntactic View of the Definition of M+K -f-1 untenshu “chauffeur” e2:unknown&lt;0:13&gt;[ARG x5:_hito_n] x7:densha_n_1&lt;0:3&gt;[] x12:_jidousha_n&lt;4:7&gt;[] x13:_ya_p_conj&lt;0:4&gt;[LIDX x7:_densha_n_1, RIDX x12:_jidousha_n] e23:_unten_s_2&lt;8:10&gt;[ARG1 x5:_hito_n] e23:_unten_s_2&lt;8:10&gt;[ARG2 x13:_ya_p_conj] Figure 4: Simplified Dependency View of the Definition ofA+K-f-1 untenshu “chauffeur” 2.2 Semantic Annotation The lexical semantic annotation uses the sense inventory from Lexeed (Kasahara et al., 2004). All words in the fundamental vocabulary are tagged with their sense. For example, the wordJ�,-�L )ookii “big” (of example sentence in Figure 1) is tagged as sense 5 in the example sentence, with the meaning “elder, older”. The word senses are further linked to semantic classes in a Japanese ontology. The ontology, GoiTaikei, consists of a hierarchy of 2,710 semantic classes, defined for over 264,312 nouns, with a maximum depth of 12 (Ikehara et al., 1997). We show the top 3 levels of the Goi-Taikei common noun ontology in Figure 5. The semantic classes are principally defined for nouns (incl</context>
</contexts>
<marker>Kasahara, Sato, Bond, Tanaka, Fujita, Kanasugi, Amano, 2004</marker>
<rawString>Kaname Kasahara, Hiroshi Sato, Francis Bond, Takaaki Tanaka, Sanae Fujita, Tomoko Kanasugi, and Shigeaki Amano. 2004. Construction of a Japanese semantic lexicon: Lexeed. In IPSG SIG: 2004-NLC-159, pages 75–82. Tokyo. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<editor>In Erhard Hinrichs and Dan Roth, editors,</editor>
<contexts>
<context position="1141" citStr="Klein and Manning, 2003" startWordPosition="155" endWordPosition="158">antic features. Results are given for syntactic features, dependency relations and the use of semantic classes. 1 Introduction In this paper we investigate the use of semantic information in parse selection. Recently, significant improvements have been made in combining symbolic and statistical approaches to various natural language processing tasks. In parsing, for example, symbolic grammars are combined with stochastic models (Oepen et al., 2004; Malouf and van Noord, 2004). Much of the gain in statistical parsing using lexicalized models comes from the use of a small set of function words (Klein and Manning, 2003). Features based on general relations provide little improvement, presumably because the data is too sparse: in the Penn treebank standardly used to train and test statistical parsers stocks and skyrocket never appear together. However, the superordinate concepts capital (D stocks) and move upward (D sky rocket) frequently appear together, which suggests that using word senses and their hypernyms as features may be useful However, to date, there have been few combinations of sense information together with symbolic grammars and statistical models. We hypothesize that one of the reasons for the</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Erhard Hinrichs and Dan Roth, editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 423–430. URL http://www.aclweb.org/anthology/P03-1054.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
</authors>
<title>A comparison of algorithms for maximum entropy parameter estimation.</title>
<date>2002</date>
<booktitle>In CONLL-2002,</booktitle>
<pages>49--55</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="20994" citStr="Malouf, 2002" startWordPosition="3439" endWordPosition="3440">is that the definition sentences are shorter, contain more fragments (especially NPs as single utterances) and fewer quoting and proper names. The main differences with travel dialogues is the lack of questions. 4.1 A Maximum Entropy Ranker Log-linear models provide a very flexible framework that has been widely used for a range of tasks in NLP, including parse selection and reranking for machine translation. We use a maximum entropy / minimum divergence (MEMD) modeler to train the parse selection model. Specifically, we use the open-source Toolkit for Advanced Discriminative Modeling (TADM:2 Malouf, 2002) for training, using its limited-memory variable metric as the optimization method and determining best-performing convergence thresholds and prior sizes experimentally. A comparison of this learner with the use of support vector machines over similar data found that the SVMs gave comparable results but were far slower (Baldridge and Osborne, 2007). Because we are investigating the effects of various different features, we chose the faster learner. 2http://tadm.sourceforge.net 30 Method Definitions Examples Accuracy Features Accuracy Features (%) (×1000) (%) (×1000) SYN-1 52.8 7 67.6 8 SYN-GP </context>
</contexts>
<marker>Malouf, 2002</marker>
<rawString>Robert Malouf. 2002. A comparison of algorithms for maximum entropy parameter estimation. In CONLL-2002, pages 49–55. Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars. In IJCNLP-04 Workshop: Beyond shallow analyses - Formalisms and statistical modeling for deep analyses. JST CREST.</title>
<date>2004</date>
<note>URL http://www-tsujii.is.s.u-tokyo.ac. jp/bsa/papers/malouf.pdf.</note>
<marker>Malouf, van Noord, 2004</marker>
<rawString>Robert Malouf and Gertjan van Noord. 2004. Wide coverage parsing with stochastic attribute value grammars. In IJCNLP-04 Workshop: Beyond shallow analyses - Formalisms and statistical modeling for deep analyses. JST CREST. URL http://www-tsujii.is.s.u-tokyo.ac. jp/bsa/papers/malouf.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Dan Flickinger</author>
<author>Kristina Toutanova</author>
<author>Christoper D Manning</author>
</authors>
<title>LinGO redwoods: A rich and dynamic treebank for HPSG.</title>
<date>2004</date>
<journal>Research on Language and Computation,</journal>
<volume>2</volume>
<issue>4</issue>
<contexts>
<context position="968" citStr="Oepen et al., 2004" startWordPosition="125" endWordPosition="128"> 46 University of Oslo, Department of Informatics (Norway) Abstract In this paper we present a framework for experimentation on parse selection using syntactic and semantic features. Results are given for syntactic features, dependency relations and the use of semantic classes. 1 Introduction In this paper we investigate the use of semantic information in parse selection. Recently, significant improvements have been made in combining symbolic and statistical approaches to various natural language processing tasks. In parsing, for example, symbolic grammars are combined with stochastic models (Oepen et al., 2004; Malouf and van Noord, 2004). Much of the gain in statistical parsing using lexicalized models comes from the use of a small set of function words (Klein and Manning, 2003). Features based on general relations provide little improvement, presumably because the data is too sparse: in the Penn treebank standardly used to train and test statistical parsers stocks and skyrocket never appear together. However, the superordinate concepts capital (D stocks) and move upward (D sky rocket) frequently appear together, which suggests that using word senses and their hypernyms as features may be useful H</context>
<context position="3958" citStr="Oepen et al., 2004" startWordPosition="603" endWordPosition="606">determines syntactic and semantic structure at the same time. Using a grammar Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 25–32, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics helps treebank consistency — all sentences annotated are guaranteed to have well-formed parses. The flip side to this is that any sentences which the parser cannot parse remain unannotated, at least unless we were to fall back on full manual mark-up of their analyses. The actual annotation process uses the same tools as the Redwoods treebank of English (Oepen et al., 2004). A (simplified) example of an entry is given in Figure 1. Each entry contains the word itself, its part of speech, and its lexical type(s) in the grammar. Each sense then contains definition and example sentences, links to other senses in the lexicon (such as hypernym), and links to other resources, such as the Goi-Taikei Japanese Lexicon (Ikehara et al., 1997) and WordNet (Fellbaum, 1998). Each content word of the definition and example sentences is annotated with sense tags from the same lexicon. There were 4 parses for the definition sentence. The correct parse, shown as a phrase structure</context>
</contexts>
<marker>Oepen, Flickinger, Toutanova, Manning, 2004</marker>
<rawString>Stephan Oepen, Dan Flickinger, Kristina Toutanova, and Christoper D. Manning. 2004. LinGO redwoods: A rich and dynamic treebank for HPSG. Research on Language and Computation, 2(4):575–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Jan Tore Lønning</author>
</authors>
<title>Discriminantbased MRS banking.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="13247" citStr="Oepen and Lønning, 2006" startWordPosition="2131" endWordPosition="2134">le in each semantic relation. For most types of relations, the distinguished variable corresponds to the main index (ARG0 in the examples above), e.g. an event variable for verbal relations and a referential index for nominals. Assuming further that, by and large, there is a unique relation for each semantic variable for which it serves as the main index (thus assuming, for example, that adjectives and adverbs have event variables of their own, which can be motivated in predicative usages at least), an MRS can be broken down into a set of basic dependency tuples of the form shown in Figure 4 (Oepen and Lønning, 2006). All predicates are indexed to the position of the word or words that introduced them in the input sentence (&lt;start:end&gt;). This allows us to link them to the sense annotations in the corpus. 3.2.1 Basic Semantic Dependencies The basic semantic model, SEM-Dep, consists of features based on a predicate and its arguments taken from the elementary dependencies. For example, consider the dependencies for densha ya jidoushawo unten suru hito “a person who drives a train or car” given in Figure 4. The predicate unten “drive” has two arguments: ARG1 hito “person” and ARG2 jidousha “car”. From these, </context>
</contexts>
<marker>Oepen, Lønning, 2006</marker>
<rawString>Stephan Oepen and Jan Tore Lønning. 2006. Discriminantbased MRS banking. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC 2006). Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>University of Chicago Press,</publisher>
<location>Chicago.</location>
<contexts>
<context position="3301" citStr="Pollard and Sag, 1994" startWordPosition="499" endWordPosition="502">definition and example sentences (Bond et al., 2007) annotated with both syntactic parses and senses from the same dictionary. 2.1 Syntactic Annotation Syntactic annotation in Hinoki is grammar based corpus annotation done by selecting the best parse (or parses) from the full analyses derived by a broadcoverage precision grammar. The grammar is an HPSG implementation (JACY: Siegel and Bender, 2002), which provides a high level of detail, marking not only dependency and constituent structure but also detailed semantic relations. As the grammar is based on a monostratal theory of grammar (HPSG: Pollard and Sag, 1994), annotation by manual disambiguation determines syntactic and semantic structure at the same time. Using a grammar Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 25–32, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics helps treebank consistency — all sentences annotated are guaranteed to have well-formed parses. The flip side to this is that any sentences which the parser cannot parse remain unannotated, at least unless we were to fall back on full manual mark-up of their analyses. The actual annotation process uses the same tools</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head Driven Phrase Structure Grammar. University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Melanie Siegel</author>
<author>Emily M Bender</author>
</authors>
<title>Efficient deep processing of Japanese.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd Workshop on Asian Language Resources and International Standardization at the 19th International Conference on Computational Linguistics,</booktitle>
<pages>1--8</pages>
<location>Taipei.</location>
<contexts>
<context position="3080" citStr="Siegel and Bender, 2002" startWordPosition="462" endWordPosition="465">information in parse selection. In English, the OntoNotes project (Hovy et al., 2006) is combining sense tags with the Penn treebank. We are using Japanese data from the Hinoki Corpus consisting of around 95,000 dictionary definition and example sentences (Bond et al., 2007) annotated with both syntactic parses and senses from the same dictionary. 2.1 Syntactic Annotation Syntactic annotation in Hinoki is grammar based corpus annotation done by selecting the best parse (or parses) from the full analyses derived by a broadcoverage precision grammar. The grammar is an HPSG implementation (JACY: Siegel and Bender, 2002), which provides a high level of detail, marking not only dependency and constituent structure but also detailed semantic relations. As the grammar is based on a monostratal theory of grammar (HPSG: Pollard and Sag, 1994), annotation by manual disambiguation determines syntactic and semantic structure at the same time. Using a grammar Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 25–32, Prague, Czech Republic, June, 2007. c�2007 Association for Computational Linguistics helps treebank consistency — all sentences annotated are guaranteed to have well-formed parses. T</context>
</contexts>
<marker>Siegel, Bender, 2002</marker>
<rawString>Melanie Siegel and Emily M. Bender. 2002. Efficient deep processing of Japanese. In Proceedings of the 3rd Workshop on Asian Language Resources and International Standardization at the 19th International Conference on Computational Linguistics, pages 1–8. Taipei.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>Stochastic HPSG parse disambiguation using the redwoods corpus.</title>
<date>2005</date>
<journal>Research on Language and Computation,</journal>
<volume>3</volume>
<issue>1</issue>
<contexts>
<context position="9052" citStr="Toutanova et al. (2005)" startWordPosition="1444" endWordPosition="1447">ome Japanese string) according to JACY, the goal is to rank parse trees by their probability: training a stochastic parse selection model on the available treebank, we estimate statistics of various features of candidate analyses from the treebank. The definition and selection of features, thus, is a central parameter in the design of an effective parse selection model. 3.1 Syntactic Features The first model that we trained uses syntactic features defined over HPSG derivation trees as summarized in Table 1. For the closely related purpose of parse selection over the English Redwoods treebank, Toutanova et al. (2005) train a discriminative loglinear model, using features defined over derivation trees with non-terminals representing the construction types and lexical types of the HPSG grammar. The basic feature set of our parse selection model for Japanese is defined in the same way (corresponding to the PCFG-S model of Toutanova et al. (2005)): each feature capturing a sub-tree from the derivahuman agent organization facility place region natural place object animate inanimate abstract mental state noun thing action human activity event phenomenon natural phen. existence system relationship property relat</context>
<context position="14693" citStr="Toutanova et al. (2005)" startWordPosition="2404" endWordPosition="2407">X densha n 1 RIDX jidousha n 1) 21 (1 unten s ARG1 hito n 1) 21 (1 unten s ARG2 jidousha n 1) 21 (1 ya p conj LIDX densha n 1) 21 (1 ya p conj RIDX jidousha n 1) 22 (2 unten s hito n 1 jidousha n 1) 23 (3 unten s hito n 1) 23 (3 unten s jidousha n 1) � � � Table 2: Example semantic features (SEM-Dep) extracted from the dependency tree in Figure 4. only one argument at a time, #22 provides unlabeled relations, #23 provides one unlabeled relation at a time and so on. Each combination of a predicate and its related argument(s) becomes a feature. These resemble the basic semantic features used by Toutanova et al. (2005). We further simplify these by collapsing some non-informative predicates, e.g. the unknown predicate used in fragments. 3.2.2 Word Sense and Semantic Class Dependencies We created two sets of features based only on the word senses. For SEM-WS we used the sense annotation to replace each underspecified MRS predicate by a predicate indicating the word sense. This used the gold standard sense tags. For SEM-Class, we used the sense annotation to replace each predicate by its Goi-Taikei semantic class. In addition, to capture more useful relationships, conjunctions were followed down into the left</context>
<context position="25535" citStr="Toutanova et al. (2005)" startWordPosition="4191" endWordPosition="4194">ggests that the deeper structural semantics provided by the HPSG parser is important. Xiong et al. (2005) achieved only a very minor improvement over a plain syntactic model, using features based on both the correlation between predicates and their arguments, and between predicates and the hypernyms of their arguments (using HowNet). However, they do not investigate generalizing to different levels than a word’s immediate hypernym. +bcld 20 ld ldld ld ld ld ld ld ld + + + + + + + + + + bc bc bc bc bc bc bc bc bc ld bc 70 60 50 40 30 SYN-SEM SEM-ALL SYN-ALL Sent. Accuracy 31 Pioneering work by Toutanova et al. (2005) and Baldridge and Osborne (2007) on parse selection for an English HPSG treebank used simpler semantic features without sense information, and got a far less dramatic improvement when they combined syntactic and semantic information. The use of hand-crafted lexical resources such as the Goi-Taikei ontology is sometimes criticized on the grounds that such resources are hard to produce and scarce. While it is true that valency lexicons and sense hierarchies are hard to produce, they are of such value that they have already been created for all of the languages we know of which have large treeba</context>
</contexts>
<marker>Toutanova, Manning, Flickinger, Oepen, 2005</marker>
<rawString>Kristina Toutanova, Christopher D. Manning, Dan Flickinger, and Stephan Oepen. 2005. Stochastic HPSG parse disambiguation using the redwoods corpus. Research on Language and Computation, 3(1):83–105.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
</authors>
<title>Qun Liu Shuanglong Li and, Shouxun Lin, and Yueliang Qian.</title>
<date>2005</date>
<booktitle>Natural Language Processing — IJCNLP 005: Second International Joint Conference Proceedings,</booktitle>
<pages>70--81</pages>
<editor>In Robert Dale, Jian Su Kam-Fai Wong and, and Oi Yee Kwong, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<marker>Xiong, 2005</marker>
<rawString>Deyi Xiong, Qun Liu Shuanglong Li and, Shouxun Lin, and Yueliang Qian. 2005. Parsing the Penn Chinese treebank with semantic knowledge. In Robert Dale, Jian Su Kam-Fai Wong and, and Oi Yee Kwong, editors, Natural Language Processing — IJCNLP 005: Second International Joint Conference Proceedings, pages 70–81. Springer-Verlag.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>