<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000160">
<title confidence="0.732186">
DISSECT - DIStributional SEmantics Composition Toolkit
</title>
<author confidence="0.901436">
Georgiana Dinu and Nghia The Pham and Marco Baroni
</author>
<affiliation confidence="0.886931">
Center for Mind/Brain Sciences (University of Trento, Italy)
</affiliation>
<email confidence="0.955641">
(georgiana.dinu|thenghia.pham|marco.baroni)@unitn.it
</email>
<sectionHeader confidence="0.996736" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999807352941177">
We introduce DISSECT, a toolkit to
build and explore computational models
of word, phrase and sentence meaning
based on the principles of distributional
semantics. The toolkit focuses in partic-
ular on compositional meaning, and im-
plements a number of composition meth-
ods that have been proposed in the litera-
ture. Furthermore, DISSECT can be use-
ful to researchers and practitioners who
need models of word meaning (without
composition) as well, as it supports var-
ious methods to construct distributional
semantic spaces, assessing similarity and
even evaluating against benchmarks, that
are independent of the composition infras-
tructure.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971595744681">
Distributional methods for meaning similarity are
based on the observation that similar words oc-
cur in similar contexts and measure similarity
based on patterns of word occurrence in large cor-
pora (Clark, 2012; Erk, 2012; Turney and Pan-
tel, 2010). More precisely, they represent words,
or any other target linguistic elements, as high-
dimensional vectors, where the dimensions repre-
sent context features. Semantic relatedness is as-
sessed by comparing vectors, leading, for exam-
ple, to determine that car and vehicle are very sim-
ilar in meaning, since they have similar contextual
distributions. Despite the appeal of these meth-
ods, modeling words in isolation has limited ap-
plications and ideally we want to model semantics
beyond word level by representing the meaning of
phrases or sentences. These combinations are in-
finite and compositional methods are called for to
derive the meaning of a larger construction from
the meaning of its parts. For this reason, the ques-
tion of compositionality within the distributional
paradigm has received a lot of attention in recent
years and a number of compositional frameworks
have been proposed in the distributional seman-
tic literature, see, e.g., Coecke et al. (2010) and
Mitchell and Lapata (2010). For example, in such
frameworks, the distributional representations of
red and car may be combined, through various op-
erations, in order to obtain a vector for red car.
The DISSECT toolkit (http://clic.
cimec.unitn.it/composes/toolkit)
is, to the best of our knowledge, the first to
provide an easy-to-use implementation of many
compositional methods proposed in the literature.
As such, we hope that it will foster further work
on compositional distributional semantics, as well
as making the relevant techniques easily available
to those interested in their many potential applica-
tions, e.g., to context-based polysemy resolution,
recognizing textual entailment or paraphrase
detection. Moreover, the DISSECT tools to
construct distributional semantic spaces from
raw co-occurrence counts, to measure similarity
and to evaluate these spaces might also be of
use to researchers who are not interested in the
compositional framework. DISSECT is freely
available under the GNU General Public License.
</bodyText>
<subsectionHeader confidence="0.5614315">
2 Building and composing distributional
semantic representations
</subsectionHeader>
<bodyText confidence="0.997536333333333">
The pipeline from corpora to compositional mod-
els of meaning can be roughly summarized as con-
sisting of three stages:1
</bodyText>
<listItem confidence="0.996462">
1. Extraction of co-occurrence counts from cor-
pora In this stage, an input corpus is used to ex-
tract counts of target elements co-occurring with
some contextual features. The target elements
can vary from words (for lexical similarity), to
pairs of words (e.g., for relation categorization),
</listItem>
<footnote confidence="0.997669">
1See Turney and Pantel (2010) for a technical overview of
distributional methods for semantics.
</footnote>
<page confidence="0.998857">
31
</page>
<bodyText confidence="0.752779166666667">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 31–36,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
to paths in syntactic trees (for unsupervised para-
phrasing). Context features can also vary from
shallow window-based collocates to syntactic de-
pendencies.
</bodyText>
<listItem confidence="0.679287692307692">
2. Transformation of the raw counts This
stage may involve the application of weighting
schemes such as Pointwise Mutual Information,
feature selection, dimensionality reduction meth-
ods such as Singular Value Decomposition, etc.
The goal is to eliminate the biases that typically
affect raw counts and to produce vectors which
better approximate similarity in meaning.
3. Application of composition functions
Once meaningful representations have been
constructed for the atomic target elements of
interest (typically, words), various methods, such
as vector addition or multiplication, can be used
</listItem>
<bodyText confidence="0.89302688">
for combining them to derive context-sensitive
representations or for constructing representations
for larger phrases or even entire sentences.
DISSECT can be used for the second and
third stages of this pipeline, as well as to measure
similarity among the resulting word or phrase vec-
tors. The first step is highly language-, task- and
corpus-annotation-dependent. We do not attempt
to implement all the corpus pre-processing and
co-occurrence extraction routines that it would
require to be of general use, and expect instead as
input a matrix of raw target-context co-occurrence
counts.2 DISSECT provides various methods to
re-weight the counts with association measures,
dimensionality reduction methods as well as the
composition functions proposed by Mitchell and
Lapata (2010) (Additive, Multiplicative and Dila-
tion), Baroni and Zamparelli (2010)/Coecke et al.
(2010) (Lexfunc) and Guevara (2010)/Zanzotto et
al. (2010) (Fulladd). In DISSECT we define and
implement these in a unified framework and in a
computationally efficient manner. The focus of
DISSECT is to provide an intuitive interface for
researchers and to allow easy extension by adding
other composition methods.
</bodyText>
<sectionHeader confidence="0.995769" genericHeader="method">
3 DISSECT overview
</sectionHeader>
<bodyText confidence="0.8952355">
DISSECT is written in Python. We provide many
standard functionalities through a set of power-
</bodyText>
<footnote confidence="0.814272666666667">
2These counts can be read from a text file containing two
strings (the target and context items) and a number (the corre-
sponding count) on each line (e.g., maggot food 15) or
from a matrix in format word freq1 freq2 ...
#create a semantic space from counts in
#dense format(&amp;quot;dm&amp;quot;): word freq1 freq2 ..
</footnote>
<equation confidence="0.826884">
ss = Space.build(data=&amp;quot;counts.txt&amp;quot;,
format=&amp;quot;dm&amp;quot;)
#apply transformations
ss = ss.apply(PpmiWeighting())
ss = ss.apply(Svd(300))
</equation>
<bodyText confidence="0.6365945">
#retrieve the vector of a target element
print ss.get_row(&amp;quot;car&amp;quot;)
</bodyText>
<figureCaption confidence="0.990057">
Figure 1: Creating a semantic space.
</figureCaption>
<bodyText confidence="0.999818285714286">
ful command-line tools, however users with ba-
sic Python familiarity are encouraged to use the
Python interface that DISSECT provides. This
section focuses on this interface (see the online
documentation on how to perform the same oper-
ations with the command-line tools), that consists
of the following top-level packages:
</bodyText>
<construct confidence="0.759596714285714">
#DISSECT packages
composes.matrix
composes.semantic_space
composes.transformation
composes.similarity
composes.composition
composes.utils
</construct>
<bodyText confidence="0.999585142857143">
Semantic spaces and transforma-
tions The concept of a semantic space
(composes.semantic space) is at the
core of the DISSECT toolkit. A semantic
space consists of co-occurrence values, stored
as a matrix, together with strings associated to
the rows of this matrix (by design, the target
linguistic elements) and a (potentially empty)
list of strings associated to the columns (the
context features). A number of transforma-
tions (composes.transformation) can
be applied to semantic spaces. We implement
weighting schemes such as positive Pointwise
Mutual Information (ppmi) and Local Mu-
tual Information, feature selection methods,
dimensionality reduction (Singular Value De-
composition (SVD) and Nonnegative Matrix
Factorization (NMF)), and new methods can
be easily added.3 Going from raw counts to a
transformed space is accomplished in just a few
lines of code (Figure 1).
</bodyText>
<footnote confidence="0.98883475">
3The complete list of transformations currently sup-
ported can be found at http://clic.cimec.unitn.
it/composes/toolkit/spacetrans.html#
spacetrans.
</footnote>
<page confidence="0.995716">
32
</page>
<figure confidence="0.998289">
Model Composition function Parameters
#load a previously saved space
ss = io—utils.load(&amp;quot;ss.pkl&amp;quot;)
#compute cosine similarity
print ss.get—sim(&amp;quot;car&amp;quot;, &amp;quot;book&amp;quot;,
CosSimilarity())
#the two nearest neighbours of &amp;quot;car&amp;quot;
print ss.get—neighbours(&amp;quot;car&amp;quot;, 2,
CosSimilarity())
</figure>
<figureCaption confidence="0.999765">
Figure 2: Similarity queries in a semantic space.
</figureCaption>
<bodyText confidence="0.999587615384615">
Furthermore DISSECT allows the pos-
sibility of adding new data to a seman-
tic space in an online manner (using the
semantic space.peripheral space
functionality). This can be used as a way to effi-
ciently expand a co-occurrence matrix with new
rows, without re-applying the transformations to
the entire space. In some other cases, the user may
want to represent phrases that are specialization
of words already existing in the space (e.g.,
slimy maggot and maggot), without distorting the
computation of association measures by counting
the same context twice. In this case, adding slimy
maggot as a “peripheral” row to a semantic space
that already contains maggot implements the
desired behaviour.
Similarity queries Semantic spaces are used for
the computation of similarity scores. DISSECT
provides a series of similarity measures such as co-
sine, inverse Euclidean distance and Lin similarity,
implemented in the composes.similarity
package. Similarity of two elements can be com-
puted within one semantic space or across two
spaces that have the same dimensionality. Figure
2 exemplifies (word) similarity computations with
DISSECT.
Composition functions Composition functions
in DISSECT (composes.composition) take
as arguments a list of element pairs to be com-
posed, and one or two spaces where the elements
to be composed are represented. They return a se-
mantic space containing the distributional repre-
sentations of the composed items, which can be
further transformed, used for similarity queries, or
used as inputs to another round of composition,
thus scaling up beyond binary composition. Fig-
ure 3 shows a Multiplicative composition exam-
ple. See Table 1 for the currently available com-
position models, their definitions and parameters.
</bodyText>
<table confidence="0.956409">
Add. w1~u + w2~v w1(= 1), w2(= 1)
Mult. u~ O v~ -
Dilation ��~u��22~v + (λ − 1)(~u,~v)~u λ(= 2)
Fulladd W1~u + W2~v W1, W2 E Rm×m
Lexfunc Au~v Au E Rm×m
</table>
<tableCaption confidence="0.867343833333333">
Table 1: Currently implemented composition
functions of inputs (u, v) together with parame-
ters and their default values in parenthesis, where
defined. Note that in Lexfunc each functor word
corresponds to a separate matrix or tensor Au (Ba-
roni and Zamparelli, 2010).
</tableCaption>
<bodyText confidence="0.996340785714286">
Parameter estimation All composition models
except Multiplicative have parameters to be esti-
mated. For simple models with few parameters,
such as as Additive, the parameters can be passed
by hand. However, DISSECT supports automated
parameter estimation from training examples. In
particular, we extend to all composition methods
the idea originally proposed by Baroni and Zam-
parelli (2010) for Lexfunc and Guevara (2010) for
Fulladd, namely to use corpus-extracted example
vectors of both the input (typically, words) and
output elements (typically, phrases) in order to op-
timize the composition operation parameters. The
problem can be generally stated as:
</bodyText>
<equation confidence="0.991999">
0∗ = arg min
θ ||P − fcompθ(U,V )||F
</equation>
<bodyText confidence="0.99996885">
where U, V and P are matrices containing input
and output vectors respectively. For example U
may contain adjective vectors such as red, blue,
V noun vectors such as car, sky and P corpus-
extracted vectors for the corresponding phrases
red car, blue sky. fcompθ is a composition func-
tion and 0 stands for a list of parameters that this
composition function is associated with.4 We im-
plement standard least-squares estimation meth-
ods as well as Ridge regression with the option
for generalized cross-validation, but other meth-
ods such as partial least-squares regression can be
easily added. Figure 4 exemplifies the Fulladd
model.
Composition output examples DISSECT pro-
vides functions to evaluate (compositional) distri-
butional semantic spaces against benchmarks in
the composes.utils package. However, as a
more qualitatively interesting example of what can
be done with DISSECT, Table 2 shows the nearest
</bodyText>
<footnote confidence="0.9404845">
4Details on the extended corpus-extracted vector estima-
tion method in DISSECT can be found in Dinu et al. (2013).
</footnote>
<page confidence="0.998056">
33
</page>
<figure confidence="0.935841777777778">
#instantiate a multiplicative model
mult_model = Multiplicative()
#use the model to compose words from input space input_space
comp_space = mult_model.compose([(&amp;quot;red&amp;quot;, &amp;quot;book&amp;quot;, &amp;quot;my_red_book&amp;quot;),
(&amp;quot;red&amp;quot;, &amp;quot;car&amp;quot;, &amp;quot;my_red_car&amp;quot;)],
input_space)
#compute similarity of: 1) two composed phrases and 2) a composed phrase and a word
print comp_space.get_sim(&amp;quot;my_red_book&amp;quot;, &amp;quot;my_red_car&amp;quot;, CosSimilarity())
print comp_space.get_sim(&amp;quot;my_red_book&amp;quot;, &amp;quot;book&amp;quot;, CosSimilarity(), input_space)
</figure>
<figureCaption confidence="0.999259">
Figure 3: Creating and using Multiplicative phrase vectors.
</figureCaption>
<figure confidence="0.357355">
#training data for learning an adjective-noun phrase model
train_data = [(&amp;quot;red&amp;quot;,&amp;quot;book&amp;quot;,&amp;quot;red_book&amp;quot;), (&amp;quot;blue&amp;quot;,&amp;quot;car&amp;quot;,&amp;quot;blue_car&amp;quot;)]
#train a fulladd model
fa_model = FullAdditive()
fa_model.train(train_data, input_space, phrase_space)
#use the model to compose a phrase from new words and retrieve its nearest neighb.
comp_space = fa_model.compose([(&amp;quot;yellow&amp;quot;, &amp;quot;table&amp;quot;, &amp;quot;my_yellow_table&amp;quot;)], input_space)
print comp_space.get_neighbours(&amp;quot;my_yellow_table&amp;quot;, 10, CosSimilarity())
</figure>
<figureCaption confidence="0.990759">
Figure 4: Estimating a Fulladd model and using it to create phrase vectors.
</figureCaption>
<table confidence="0.9982366">
Target Method Neighbours
florist Corpus Harrod, wholesaler, stockist
flora + -ist Fulladd flora, fauna, ecologist
Lexfunc ornithologist, naturalist, botanist
Additive flora, fauna, ecosystem
</table>
<tableCaption confidence="0.996106">
Table 3: Compositional models for morphol-
</tableCaption>
<bodyText confidence="0.981807272727273">
ogy. Top 3 neighbours of florist using its (low-
frequency) corpus-extracted vector, and when the
vector is obtained through composition of flora
and -ist with Fulladd, Lexfunc and Additive.
neighbours of false belief obtained through com-
position with the Fulladd, Lexfunc and Additive
models. In Table 3, we exemplify a less typical ap-
plication of compositional models to derivational
morphology, namely obtaining a representation of
florist compositionally from distributional repre-
sentations offlora and -ist (Lazaridou et al., 2013).
</bodyText>
<sectionHeader confidence="0.995938" genericHeader="method">
4 Main features
</sectionHeader>
<bodyText confidence="0.9976951">
Support for dense and sparse representations
Co-occurrence matrices, as extracted from text,
tend to be very sparse structures, especially when
using detailed context features which include syn-
tactic information, for example. On the other
hand, dimensionality reduction operations, which
are often used in distributional models, lead to
smaller, dense structures, for which sparse rep-
resentations are not optimal. This is our motiva-
tion for supporting both dense and sparse repre-
sentations. The choice of dense vs. sparse is ini-
tially determined by the input format, if a space
is created from co-occurrence counts. By default,
DISSECT switches to dense representations af-
ter dimensionality reduction, however the user can
freely switch from one representation to the other,
in order to optimize computations. For this pur-
pose DISSECT provides wrappers around matrix
operations, as well as around common linear alge-
bra operations, in the composes.matrix pack-
age. The underlying Python functionality is pro-
vided by numpy.array and scipy.sparse.
Efficient computations DISSECT is optimized
for speed since most operations are cast as matrix
operations, that are very efficiently implemented
in Python’s numpy and scipy modules5. Ta-
bles 4 and 5 show running times for typical DIS-
SECT operations: application of the ppmi weight-
ing scheme, nearest neighbour queries and estima-
tion of composition function parameters (on a 2.1
</bodyText>
<footnote confidence="0.99914475">
5For SVD on sparse structures, we use sparsesvd
(https://pypi.python.org/pypi/sparsesvd/).
For NMF, we adapted http://www.csie.ntu.edu.
tw/˜cjlin/nmf/ (Lin, 2007).
</footnote>
<page confidence="0.994279">
34
</page>
<table confidence="0.9985544">
Target Method Neighbours
belief Corpus moral, dogma, worldview, religion, world-view, morality, theism, tenet, agnosticism, dogmatic
false belief Fulladd pantheist, belief, agnosticism, religiosity, dogmatism, pantheism, theist, fatalism, deism, mind-set
Lexfunc self-deception, untruth, credulity, obfuscation, misapprehension, deceiver, disservice, falsehood
Additive belief, assertion, falsity, falsehood, truth, credence, dogma, supposition, hearsay, denial
</table>
<tableCaption confidence="0.9412605">
Table 2: Top nearest neighbours of belief and of false belief obtained through composition with the
Fulladd, Lexfunc and Additive models.
</tableCaption>
<table confidence="0.999372">
300 Method Fulladd Lexfunc Add. Dilation composes/toolkit/composing.htmlfor
an example of using DISSECT for estimating
such tensors.
in
Extensive documentation The DISSECT
documentation can be found at http://clic.
Time (s.) 2864 787 46 68
Table
times
4: Composition model parameter estimation
(in seconds) for 1 million training points
-dimensional space.
Matrix size (nnz) Ppmi Query
100Kx300 (30M) 5.8 0.5
100Kx100K (250M) 52.6 9.5
</table>
<tableCaption confidence="0.995301">
Table 5: Running times (in seconds) for 1) appli-
</tableCaption>
<bodyText confidence="0.987000924528302">
cation of ppmi weighting and 2) querying for the
top neighbours of a word (cosine similarity) for
different matrix sizes (nnz: number of non-zero
entries, in millions).
GHz machine). The price to pay for fast computa-
tions is that data must be stored in main memory.
We do not think that this is a major inconvenience.
For example, a typical symmetric co-occurrence
matrix extracted from a corpus of several billion
words, defining context in terms of 5-word win-
dows and considering the top 100K×100K most
frequent words, contains ≈ 250 million entries and
requires only 2GB of memory for (double preci-
sion) storage.
Simple design We have opted for a very simple
and intuitive design as the classes interact in
very natural ways: A semantic space stores
the actual data matrix and structures to index
its rows and columns, and supports similarity
queries and transformations. Transformations
take one semantic space as input to return
another, transformed, space. Composition func-
tions take one or more input spaces and yield
a composed-elements space, which can further
undergo transformations and be used for similarity
queries. In fact, DISSECT semantic spaces also
support higher-order tensor representations, not
just vectors. Higher-order representations are
used, for example, to represent transitive verbs
and other multi-argument functors by Coecke
et al. (2010) and Grefenstette et al. (2013).
See http://clic.cimec.unitn.it/
cimec.unitn.it/composes/toolkit.
We provide a tutorial which guides the user
through the creation of some toy semantic spaces,
estimation of the parameters of composition
models and similarity computations in semantic
spaces. We also provide a full-scale example
of intransitive verb-subject composition. We
show how to go from co-occurrence counts to
composed representations and make the data used
in the examples available for download.
Comparison to existing software In terms of
design choices, DISSECT most resembles the
Gensim toolkit (ˇReh˚uˇrek and Sojka, 2010). How-
ever Gensim is intended for topic modeling, and
therefore diverges considerably from DISSECT in
its functionality. The SSpace package of Jurgens
and Stevens (2010) also overlaps to some degree
with DISSECT in terms of its intended use, how-
ever, like Gensim, it does not support composi-
tional operations that, as far as we know, are an
unique feature of DISSECT.
</bodyText>
<sectionHeader confidence="0.997453" genericHeader="method">
5 Future extensions
</sectionHeader>
<bodyText confidence="0.999952857142857">
We implemented and are currently testing DIS-
SECT functions supporting other composition
methods, including the one proposed by Socher
et al. (2012). Adding further methods is our top-
priority goal. In particular, several distributional
models of word meaning in context share impor-
tant similarities with composition models, and we
plan to add them to DISSECT. Dinu et al. (2012)
show, for example, that well-performing, simpli-
fied variants of the method in Thater et al. (2010),
Thater et al. (2011) and Erk and Pad´o (2008) can
be reduced to relatively simple matrix operations,
making them particularly suitable for a DISSECT
implementation.
</bodyText>
<page confidence="0.997454">
35
</page>
<bodyText confidence="0.999839538461539">
DISSECT is currently optimized for the compo-
sition of many phrases of the same type. This is in
line with most of the current evaluations of com-
positional models, which focus on specific phe-
nomena, such as adjectival modification, noun-
noun compounds or intransitive verbs, to name a
few. In the future we plan to provide a module for
composing entire sentences, taking syntactic trees
as input and returning composed representations
for each node in the input trees.
Finally, we intend to make use of the exist-
ing Python plotting libraries to add a visualization
module to DISSECT.
</bodyText>
<sectionHeader confidence="0.999698" genericHeader="conclusions">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.9959765">
We thank Angeliki Lazaridou for helpful dis-
cussions. This research was supported by the
ERC 2011 Starting Independent Research Grant
n. 283554 (COMPOSES).
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99987575308642">
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183–1193, Boston,
MA.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345–384.
Georgiana Dinu, Stefan Thater, and S¨oren Laue. 2012.
A comparison of models of word meaning in con-
text. In Proceedings of NAACL HLT, pages 611–
615, Montreal, Canada.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. A general framework for the estimation of
distributional composition functions. In Proceed-
ings of ACL Workshop on Continuous Vector Space
Models and their Compositionality, Sofia, Bulgaria.
In press.
Katrin Erk and Sebastian Pad´o. 2008. A structured
vector space model for word meaning in context. In
Proceedings of EMNLP, pages 897–906, Honolulu,
HI.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635–653.
Edward Grefenstette, Georgiana Dinu, Yao-Zhong
Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni.
2013. Multi-step regression learning for composi-
tional distributional semantics. In Proceedings of
IWCS, pages 131–142, Potsdam, Germany.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33–37,
Uppsala, Sweden.
David Jurgens and Keith Stevens. 2010. The S-Space
package: an open source package for word space
models. In Proceedings of the ACL 2010 System
Demonstrations, pages 30–35, Uppsala, Sweden.
Angeliki Lazaridou, Marco Marelli, Roberto Zampar-
elli, and Marco Baroni. 2013. Compositional-ly
derived representations of morphologically complex
words in distributional semantics. In Proceedings of
ACL, Sofia, Bulgaria. In press.
Chih-Jen Lin. 2007. Projected gradient methods for
Nonnegative Matrix Factorization. Neural Compu-
tation, 19(10):2756–2779.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of the LREC 2010 Workshop on New Chal-
lenges for NLP Frameworks, pages 45–50, Valletta,
Malta.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201–1211, Jeju Island, Ko-
rea.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of ACL, pages 948–957, Uppsala, Sweden.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and ef-
fective vector model. In Proceedings of IJCNLP,
pages 1134–1143, Chiang Mai, Thailand.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263–
1271, Beijing, China.
</reference>
<page confidence="0.998943">
36
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.264815">
<title confidence="0.68143"></title>
<author confidence="0.369947">Dinu The Pham</author>
<affiliation confidence="0.505426">Center for Mind/Brain Sciences (University of Trento, Italy)</affiliation>
<email confidence="0.99233">(georgiana.dinu|thenghia.pham|marco.baroni)@unitn.it</email>
<abstract confidence="0.9886415">We introduce DISSECT, a toolkit to build and explore computational models of word, phrase and sentence meaning based on the principles of distributional semantics. The toolkit focuses in particular on compositional meaning, and implements a number of composition methods that have been proposed in the literature. Furthermore, DISSECT can be useful to researchers and practitioners who need models of word meaning (without composition) as well, as it supports various methods to construct distributional semantic spaces, assessing similarity and even evaluating against benchmarks, that are independent of the composition infrastructure.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1183--1193</pages>
<location>Boston, MA.</location>
<contexts>
<context position="5494" citStr="Baroni and Zamparelli (2010)" startWordPosition="803" endWordPosition="806">to measure similarity among the resulting word or phrase vectors. The first step is highly language-, task- and corpus-annotation-dependent. We do not attempt to implement all the corpus pre-processing and co-occurrence extraction routines that it would require to be of general use, and expect instead as input a matrix of raw target-context co-occurrence counts.2 DISSECT provides various methods to re-weight the counts with association measures, dimensionality reduction methods as well as the composition functions proposed by Mitchell and Lapata (2010) (Additive, Multiplicative and Dilation), Baroni and Zamparelli (2010)/Coecke et al. (2010) (Lexfunc) and Guevara (2010)/Zanzotto et al. (2010) (Fulladd). In DISSECT we define and implement these in a unified framework and in a computationally efficient manner. The focus of DISSECT is to provide an intuitive interface for researchers and to allow easy extension by adding other composition methods. 3 DISSECT overview DISSECT is written in Python. We provide many standard functionalities through a set of power2These counts can be read from a text file containing two strings (the target and context items) and a number (the corresponding count) on each line (e.g., m</context>
<context position="10429" citStr="Baroni and Zamparelli, 2010" startWordPosition="1536" endWordPosition="1540">und of composition, thus scaling up beyond binary composition. Figure 3 shows a Multiplicative composition example. See Table 1 for the currently available composition models, their definitions and parameters. Add. w1~u + w2~v w1(= 1), w2(= 1) Mult. u~ O v~ - Dilation ��~u��22~v + (λ − 1)(~u,~v)~u λ(= 2) Fulladd W1~u + W2~v W1, W2 E Rm×m Lexfunc Au~v Au E Rm×m Table 1: Currently implemented composition functions of inputs (u, v) together with parameters and their default values in parenthesis, where defined. Note that in Lexfunc each functor word corresponds to a separate matrix or tensor Au (Baroni and Zamparelli, 2010). Parameter estimation All composition models except Multiplicative have parameters to be estimated. For simple models with few parameters, such as as Additive, the parameters can be passed by hand. However, DISSECT supports automated parameter estimation from training examples. In particular, we extend to all composition methods the idea originally proposed by Baroni and Zamparelli (2010) for Lexfunc and Guevara (2010) for Fulladd, namely to use corpus-extracted example vectors of both the input (typically, words) and output elements (typically, phrases) in order to optimize the composition o</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, pages 1183–1193, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
</authors>
<title>Vector space models of lexical meaning.</title>
<date>2012</date>
<booktitle>Handbook of Contemporary Semantics, 2nd edition.</booktitle>
<editor>In Shalom Lappin and Chris Fox, editors,</editor>
<publisher>In press.</publisher>
<location>Blackwell, Malden, MA.</location>
<contexts>
<context position="1091" citStr="Clark, 2012" startWordPosition="154" endWordPosition="155">composition methods that have been proposed in the literature. Furthermore, DISSECT can be useful to researchers and practitioners who need models of word meaning (without composition) as well, as it supports various methods to construct distributional semantic spaces, assessing similarity and even evaluating against benchmarks, that are independent of the composition infrastructure. 1 Introduction Distributional methods for meaning similarity are based on the observation that similar words occur in similar contexts and measure similarity based on patterns of word occurrence in large corpora (Clark, 2012; Erk, 2012; Turney and Pantel, 2010). More precisely, they represent words, or any other target linguistic elements, as highdimensional vectors, where the dimensions represent context features. Semantic relatedness is assessed by comparing vectors, leading, for example, to determine that car and vehicle are very similar in meaning, since they have similar contextual distributions. Despite the appeal of these methods, modeling words in isolation has limited applications and ideally we want to model semantics beyond word level by representing the meaning of phrases or sentences. These combinati</context>
</contexts>
<marker>Clark, 2012</marker>
<rawString>Stephen Clark. 2012. Vector space models of lexical meaning. In Shalom Lappin and Chris Fox, editors, Handbook of Contemporary Semantics, 2nd edition. Blackwell, Malden, MA. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis,</title>
<date>2010</date>
<pages>36--345</pages>
<contexts>
<context position="2094" citStr="Coecke et al. (2010)" startWordPosition="311" endWordPosition="314">. Despite the appeal of these methods, modeling words in isolation has limited applications and ideally we want to model semantics beyond word level by representing the meaning of phrases or sentences. These combinations are infinite and compositional methods are called for to derive the meaning of a larger construction from the meaning of its parts. For this reason, the question of compositionality within the distributional paradigm has received a lot of attention in recent years and a number of compositional frameworks have been proposed in the distributional semantic literature, see, e.g., Coecke et al. (2010) and Mitchell and Lapata (2010). For example, in such frameworks, the distributional representations of red and car may be combined, through various operations, in order to obtain a vector for red car. The DISSECT toolkit (http://clic. cimec.unitn.it/composes/toolkit) is, to the best of our knowledge, the first to provide an easy-to-use implementation of many compositional methods proposed in the literature. As such, we hope that it will foster further work on compositional distributional semantics, as well as making the relevant techniques easily available to those interested in their many po</context>
<context position="5515" citStr="Coecke et al. (2010)" startWordPosition="806" endWordPosition="809">he resulting word or phrase vectors. The first step is highly language-, task- and corpus-annotation-dependent. We do not attempt to implement all the corpus pre-processing and co-occurrence extraction routines that it would require to be of general use, and expect instead as input a matrix of raw target-context co-occurrence counts.2 DISSECT provides various methods to re-weight the counts with association measures, dimensionality reduction methods as well as the composition functions proposed by Mitchell and Lapata (2010) (Additive, Multiplicative and Dilation), Baroni and Zamparelli (2010)/Coecke et al. (2010) (Lexfunc) and Guevara (2010)/Zanzotto et al. (2010) (Fulladd). In DISSECT we define and implement these in a unified framework and in a computationally efficient manner. The focus of DISSECT is to provide an intuitive interface for researchers and to allow easy extension by adding other composition methods. 3 DISSECT overview DISSECT is written in Python. We provide many standard functionalities through a set of power2These counts can be read from a text file containing two strings (the target and context items) and a number (the corresponding count) on each line (e.g., maggot food 15) or fro</context>
<context position="18051" citStr="Coecke et al. (2010)" startWordPosition="2620" endWordPosition="2623">es the actual data matrix and structures to index its rows and columns, and supports similarity queries and transformations. Transformations take one semantic space as input to return another, transformed, space. Composition functions take one or more input spaces and yield a composed-elements space, which can further undergo transformations and be used for similarity queries. In fact, DISSECT semantic spaces also support higher-order tensor representations, not just vectors. Higher-order representations are used, for example, to represent transitive verbs and other multi-argument functors by Coecke et al. (2010) and Grefenstette et al. (2013). See http://clic.cimec.unitn.it/ cimec.unitn.it/composes/toolkit. We provide a tutorial which guides the user through the creation of some toy semantic spaces, estimation of the parameters of composition models and similarity computations in semantic spaces. We also provide a full-scale example of intransitive verb-subject composition. We show how to go from co-occurrence counts to composed representations and make the data used in the examples available for download. Comparison to existing software In terms of design choices, DISSECT most resembles the Gensim t</context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. Linguistic Analysis, 36:345–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Stefan Thater</author>
<author>S¨oren Laue</author>
</authors>
<title>A comparison of models of word meaning in context.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL HLT,</booktitle>
<pages>611--615</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="19450" citStr="Dinu et al. (2012)" startWordPosition="2831" endWordPosition="2834">ens and Stevens (2010) also overlaps to some degree with DISSECT in terms of its intended use, however, like Gensim, it does not support compositional operations that, as far as we know, are an unique feature of DISSECT. 5 Future extensions We implemented and are currently testing DISSECT functions supporting other composition methods, including the one proposed by Socher et al. (2012). Adding further methods is our toppriority goal. In particular, several distributional models of word meaning in context share important similarities with composition models, and we plan to add them to DISSECT. Dinu et al. (2012) show, for example, that well-performing, simplified variants of the method in Thater et al. (2010), Thater et al. (2011) and Erk and Pad´o (2008) can be reduced to relatively simple matrix operations, making them particularly suitable for a DISSECT implementation. 35 DISSECT is currently optimized for the composition of many phrases of the same type. This is in line with most of the current evaluations of compositional models, which focus on specific phenomena, such as adjectival modification, nounnoun compounds or intransitive verbs, to name a few. In the future we plan to provide a module f</context>
</contexts>
<marker>Dinu, Thater, Laue, 2012</marker>
<rawString>Georgiana Dinu, Stefan Thater, and S¨oren Laue. 2012. A comparison of models of word meaning in context. In Proceedings of NAACL HLT, pages 611– 615, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>A general framework for the estimation of distributional composition functions.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<location>Sofia, Bulgaria.</location>
<note>In press.</note>
<contexts>
<context position="12146" citStr="Dinu et al. (2013)" startWordPosition="1804" endWordPosition="1807">ast-squares estimation methods as well as Ridge regression with the option for generalized cross-validation, but other methods such as partial least-squares regression can be easily added. Figure 4 exemplifies the Fulladd model. Composition output examples DISSECT provides functions to evaluate (compositional) distributional semantic spaces against benchmarks in the composes.utils package. However, as a more qualitatively interesting example of what can be done with DISSECT, Table 2 shows the nearest 4Details on the extended corpus-extracted vector estimation method in DISSECT can be found in Dinu et al. (2013). 33 #instantiate a multiplicative model mult_model = Multiplicative() #use the model to compose words from input space input_space comp_space = mult_model.compose([(&amp;quot;red&amp;quot;, &amp;quot;book&amp;quot;, &amp;quot;my_red_book&amp;quot;), (&amp;quot;red&amp;quot;, &amp;quot;car&amp;quot;, &amp;quot;my_red_car&amp;quot;)], input_space) #compute similarity of: 1) two composed phrases and 2) a composed phrase and a word print comp_space.get_sim(&amp;quot;my_red_book&amp;quot;, &amp;quot;my_red_car&amp;quot;, CosSimilarity()) print comp_space.get_sim(&amp;quot;my_red_book&amp;quot;, &amp;quot;book&amp;quot;, CosSimilarity(), input_space) Figure 3: Creating and using Multiplicative phrase vectors. #training data for learning an adjective-noun phrase model train_d</context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013. A general framework for the estimation of distributional composition functions. In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality, Sofia, Bulgaria. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>897--906</pages>
<location>Honolulu, HI.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of EMNLP, pages 897–906, Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector space models of word meaning and phrase meaning: A survey.</title>
<date>2012</date>
<journal>Language and Linguistics Compass,</journal>
<volume>6</volume>
<issue>10</issue>
<contexts>
<context position="1102" citStr="Erk, 2012" startWordPosition="156" endWordPosition="157">ethods that have been proposed in the literature. Furthermore, DISSECT can be useful to researchers and practitioners who need models of word meaning (without composition) as well, as it supports various methods to construct distributional semantic spaces, assessing similarity and even evaluating against benchmarks, that are independent of the composition infrastructure. 1 Introduction Distributional methods for meaning similarity are based on the observation that similar words occur in similar contexts and measure similarity based on patterns of word occurrence in large corpora (Clark, 2012; Erk, 2012; Turney and Pantel, 2010). More precisely, they represent words, or any other target linguistic elements, as highdimensional vectors, where the dimensions represent context features. Semantic relatedness is assessed by comparing vectors, leading, for example, to determine that car and vehicle are very similar in meaning, since they have similar contextual distributions. Despite the appeal of these methods, modeling words in isolation has limited applications and ideally we want to model semantics beyond word level by representing the meaning of phrases or sentences. These combinations are inf</context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Katrin Erk. 2012. Vector space models of word meaning and phrase meaning: A survey. Language and Linguistics Compass, 6(10):635–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Georgiana Dinu</author>
<author>Yao-Zhong Zhang</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Marco Baroni</author>
</authors>
<title>Multi-step regression learning for compositional distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of IWCS,</booktitle>
<pages>131--142</pages>
<location>Potsdam, Germany.</location>
<contexts>
<context position="18082" citStr="Grefenstette et al. (2013)" startWordPosition="2625" endWordPosition="2628"> and structures to index its rows and columns, and supports similarity queries and transformations. Transformations take one semantic space as input to return another, transformed, space. Composition functions take one or more input spaces and yield a composed-elements space, which can further undergo transformations and be used for similarity queries. In fact, DISSECT semantic spaces also support higher-order tensor representations, not just vectors. Higher-order representations are used, for example, to represent transitive verbs and other multi-argument functors by Coecke et al. (2010) and Grefenstette et al. (2013). See http://clic.cimec.unitn.it/ cimec.unitn.it/composes/toolkit. We provide a tutorial which guides the user through the creation of some toy semantic spaces, estimation of the parameters of composition models and similarity computations in semantic spaces. We also provide a full-scale example of intransitive verb-subject composition. We show how to go from co-occurrence counts to composed representations and make the data used in the examples available for download. Comparison to existing software In terms of design choices, DISSECT most resembles the Gensim toolkit (ˇReh˚uˇrek and Sojka, 2</context>
</contexts>
<marker>Grefenstette, Dinu, Zhang, Sadrzadeh, Baroni, 2013</marker>
<rawString>Edward Grefenstette, Georgiana Dinu, Yao-Zhong Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni. 2013. Multi-step regression learning for compositional distributional semantics. In Proceedings of IWCS, pages 131–142, Potsdam, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of GEMS,</booktitle>
<pages>33--37</pages>
<location>Uppsala,</location>
<contexts>
<context position="5544" citStr="Guevara (2010)" startWordPosition="812" endWordPosition="813">. The first step is highly language-, task- and corpus-annotation-dependent. We do not attempt to implement all the corpus pre-processing and co-occurrence extraction routines that it would require to be of general use, and expect instead as input a matrix of raw target-context co-occurrence counts.2 DISSECT provides various methods to re-weight the counts with association measures, dimensionality reduction methods as well as the composition functions proposed by Mitchell and Lapata (2010) (Additive, Multiplicative and Dilation), Baroni and Zamparelli (2010)/Coecke et al. (2010) (Lexfunc) and Guevara (2010)/Zanzotto et al. (2010) (Fulladd). In DISSECT we define and implement these in a unified framework and in a computationally efficient manner. The focus of DISSECT is to provide an intuitive interface for researchers and to allow easy extension by adding other composition methods. 3 DISSECT overview DISSECT is written in Python. We provide many standard functionalities through a set of power2These counts can be read from a text file containing two strings (the target and context items) and a number (the corresponding count) on each line (e.g., maggot food 15) or from a matrix in format word fre</context>
<context position="10852" citStr="Guevara (2010)" startWordPosition="1601" endWordPosition="1602">ther with parameters and their default values in parenthesis, where defined. Note that in Lexfunc each functor word corresponds to a separate matrix or tensor Au (Baroni and Zamparelli, 2010). Parameter estimation All composition models except Multiplicative have parameters to be estimated. For simple models with few parameters, such as as Additive, the parameters can be passed by hand. However, DISSECT supports automated parameter estimation from training examples. In particular, we extend to all composition methods the idea originally proposed by Baroni and Zamparelli (2010) for Lexfunc and Guevara (2010) for Fulladd, namely to use corpus-extracted example vectors of both the input (typically, words) and output elements (typically, phrases) in order to optimize the composition operation parameters. The problem can be generally stated as: 0∗ = arg min θ ||P − fcompθ(U,V )||F where U, V and P are matrices containing input and output vectors respectively. For example U may contain adjective vectors such as red, blue, V noun vectors such as car, sky and P corpusextracted vectors for the corresponding phrases red car, blue sky. fcompθ is a composition function and 0 stands for a list of parameters </context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of GEMS, pages 33–37, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Keith Stevens</author>
</authors>
<title>The S-Space package: an open source package for word space models.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>30--35</pages>
<location>Uppsala,</location>
<contexts>
<context position="18854" citStr="Jurgens and Stevens (2010)" startWordPosition="2733" endWordPosition="2736">mantic spaces, estimation of the parameters of composition models and similarity computations in semantic spaces. We also provide a full-scale example of intransitive verb-subject composition. We show how to go from co-occurrence counts to composed representations and make the data used in the examples available for download. Comparison to existing software In terms of design choices, DISSECT most resembles the Gensim toolkit (ˇReh˚uˇrek and Sojka, 2010). However Gensim is intended for topic modeling, and therefore diverges considerably from DISSECT in its functionality. The SSpace package of Jurgens and Stevens (2010) also overlaps to some degree with DISSECT in terms of its intended use, however, like Gensim, it does not support compositional operations that, as far as we know, are an unique feature of DISSECT. 5 Future extensions We implemented and are currently testing DISSECT functions supporting other composition methods, including the one proposed by Socher et al. (2012). Adding further methods is our toppriority goal. In particular, several distributional models of word meaning in context share important similarities with composition models, and we plan to add them to DISSECT. Dinu et al. (2012) sho</context>
</contexts>
<marker>Jurgens, Stevens, 2010</marker>
<rawString>David Jurgens and Keith Stevens. 2010. The S-Space package: an open source package for word space models. In Proceedings of the ACL 2010 System Demonstrations, pages 30–35, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angeliki Lazaridou</author>
<author>Marco Marelli</author>
<author>Roberto Zamparelli</author>
<author>Marco Baroni</author>
</authors>
<title>Compositional-ly derived representations of morphologically complex words in distributional semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Sofia, Bulgaria.</location>
<note>In press.</note>
<contexts>
<context position="13992" citStr="Lazaridou et al., 2013" startWordPosition="2030" endWordPosition="2033">list, botanist Additive flora, fauna, ecosystem Table 3: Compositional models for morphology. Top 3 neighbours of florist using its (lowfrequency) corpus-extracted vector, and when the vector is obtained through composition of flora and -ist with Fulladd, Lexfunc and Additive. neighbours of false belief obtained through composition with the Fulladd, Lexfunc and Additive models. In Table 3, we exemplify a less typical application of compositional models to derivational morphology, namely obtaining a representation of florist compositionally from distributional representations offlora and -ist (Lazaridou et al., 2013). 4 Main features Support for dense and sparse representations Co-occurrence matrices, as extracted from text, tend to be very sparse structures, especially when using detailed context features which include syntactic information, for example. On the other hand, dimensionality reduction operations, which are often used in distributional models, lead to smaller, dense structures, for which sparse representations are not optimal. This is our motivation for supporting both dense and sparse representations. The choice of dense vs. sparse is initially determined by the input format, if a space is c</context>
</contexts>
<marker>Lazaridou, Marelli, Zamparelli, Baroni, 2013</marker>
<rawString>Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli, and Marco Baroni. 2013. Compositional-ly derived representations of morphologically complex words in distributional semantics. In Proceedings of ACL, Sofia, Bulgaria. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Jen Lin</author>
</authors>
<title>Projected gradient methods for Nonnegative Matrix Factorization.</title>
<date>2007</date>
<journal>Neural Computation,</journal>
<volume>19</volume>
<issue>10</issue>
<contexts>
<context position="15593" citStr="Lin, 2007" startWordPosition="2267" endWordPosition="2268">ying Python functionality is provided by numpy.array and scipy.sparse. Efficient computations DISSECT is optimized for speed since most operations are cast as matrix operations, that are very efficiently implemented in Python’s numpy and scipy modules5. Tables 4 and 5 show running times for typical DISSECT operations: application of the ppmi weighting scheme, nearest neighbour queries and estimation of composition function parameters (on a 2.1 5For SVD on sparse structures, we use sparsesvd (https://pypi.python.org/pypi/sparsesvd/). For NMF, we adapted http://www.csie.ntu.edu. tw/˜cjlin/nmf/ (Lin, 2007). 34 Target Method Neighbours belief Corpus moral, dogma, worldview, religion, world-view, morality, theism, tenet, agnosticism, dogmatic false belief Fulladd pantheist, belief, agnosticism, religiosity, dogmatism, pantheism, theist, fatalism, deism, mind-set Lexfunc self-deception, untruth, credulity, obfuscation, misapprehension, deceiver, disservice, falsehood Additive belief, assertion, falsity, falsehood, truth, credence, dogma, supposition, hearsay, denial Table 2: Top nearest neighbours of belief and of false belief obtained through composition with the Fulladd, Lexfunc and Additive mod</context>
</contexts>
<marker>Lin, 2007</marker>
<rawString>Chih-Jen Lin. 2007. Projected gradient methods for Nonnegative Matrix Factorization. Neural Computation, 19(10):2756–2779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="2125" citStr="Mitchell and Lapata (2010)" startWordPosition="316" endWordPosition="319">hese methods, modeling words in isolation has limited applications and ideally we want to model semantics beyond word level by representing the meaning of phrases or sentences. These combinations are infinite and compositional methods are called for to derive the meaning of a larger construction from the meaning of its parts. For this reason, the question of compositionality within the distributional paradigm has received a lot of attention in recent years and a number of compositional frameworks have been proposed in the distributional semantic literature, see, e.g., Coecke et al. (2010) and Mitchell and Lapata (2010). For example, in such frameworks, the distributional representations of red and car may be combined, through various operations, in order to obtain a vector for red car. The DISSECT toolkit (http://clic. cimec.unitn.it/composes/toolkit) is, to the best of our knowledge, the first to provide an easy-to-use implementation of many compositional methods proposed in the literature. As such, we hope that it will foster further work on compositional distributional semantics, as well as making the relevant techniques easily available to those interested in their many potential applications, e.g., to </context>
<context position="5424" citStr="Mitchell and Lapata (2010)" startWordPosition="794" endWordPosition="797">e used for the second and third stages of this pipeline, as well as to measure similarity among the resulting word or phrase vectors. The first step is highly language-, task- and corpus-annotation-dependent. We do not attempt to implement all the corpus pre-processing and co-occurrence extraction routines that it would require to be of general use, and expect instead as input a matrix of raw target-context co-occurrence counts.2 DISSECT provides various methods to re-weight the counts with association measures, dimensionality reduction methods as well as the composition functions proposed by Mitchell and Lapata (2010) (Additive, Multiplicative and Dilation), Baroni and Zamparelli (2010)/Coecke et al. (2010) (Lexfunc) and Guevara (2010)/Zanzotto et al. (2010) (Fulladd). In DISSECT we define and implement these in a unified framework and in a computationally efficient manner. The focus of DISSECT is to provide an intuitive interface for researchers and to allow easy extension by adding other composition methods. 3 DISSECT overview DISSECT is written in Python. We provide many standard functionalities through a set of power2These counts can be read from a text file containing two strings (the target and conte</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim ˇReh˚uˇrek</author>
<author>Petr Sojka</author>
</authors>
<title>Software framework for topic modelling with large corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,</booktitle>
<pages>45--50</pages>
<location>Valletta,</location>
<marker>ˇReh˚uˇrek, Sojka, 2010</marker>
<rawString>Radim ˇReh˚uˇrek and Petr Sojka. 2010. Software framework for topic modelling with large corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45–50, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Brody Huval</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Semantic compositionality through recursive matrix-vector spaces.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1201--1211</pages>
<location>Jeju Island,</location>
<contexts>
<context position="19220" citStr="Socher et al. (2012)" startWordPosition="2794" endWordPosition="2797">design choices, DISSECT most resembles the Gensim toolkit (ˇReh˚uˇrek and Sojka, 2010). However Gensim is intended for topic modeling, and therefore diverges considerably from DISSECT in its functionality. The SSpace package of Jurgens and Stevens (2010) also overlaps to some degree with DISSECT in terms of its intended use, however, like Gensim, it does not support compositional operations that, as far as we know, are an unique feature of DISSECT. 5 Future extensions We implemented and are currently testing DISSECT functions supporting other composition methods, including the one proposed by Socher et al. (2012). Adding further methods is our toppriority goal. In particular, several distributional models of word meaning in context share important similarities with composition models, and we plan to add them to DISSECT. Dinu et al. (2012) show, for example, that well-performing, simplified variants of the method in Thater et al. (2010), Thater et al. (2011) and Erk and Pad´o (2008) can be reduced to relatively simple matrix operations, making them particularly suitable for a DISSECT implementation. 35 DISSECT is currently optimized for the composition of many phrases of the same type. This is in line </context>
</contexts>
<marker>Socher, Huval, Manning, Ng, 2012</marker>
<rawString>Richard Socher, Brody Huval, Christopher Manning, and Andrew Ng. 2012. Semantic compositionality through recursive matrix-vector spaces. In Proceedings of EMNLP, pages 1201–1211, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>948--957</pages>
<location>Uppsala,</location>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of ACL, pages 948–957, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Word meaning in context: A simple and effective vector model.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>1134--1143</pages>
<location>Chiang Mai, Thailand.</location>
<marker>Thater, F¨urstenau, Pinkal, 2011</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2011. Word meaning in context: A simple and effective vector model. In Proceedings of IJCNLP, pages 1134–1143, Chiang Mai, Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1128" citStr="Turney and Pantel, 2010" startWordPosition="158" endWordPosition="162"> have been proposed in the literature. Furthermore, DISSECT can be useful to researchers and practitioners who need models of word meaning (without composition) as well, as it supports various methods to construct distributional semantic spaces, assessing similarity and even evaluating against benchmarks, that are independent of the composition infrastructure. 1 Introduction Distributional methods for meaning similarity are based on the observation that similar words occur in similar contexts and measure similarity based on patterns of word occurrence in large corpora (Clark, 2012; Erk, 2012; Turney and Pantel, 2010). More precisely, they represent words, or any other target linguistic elements, as highdimensional vectors, where the dimensions represent context features. Semantic relatedness is assessed by comparing vectors, leading, for example, to determine that car and vehicle are very similar in meaning, since they have similar contextual distributions. Despite the appeal of these methods, modeling words in isolation has limited applications and ideally we want to model semantics beyond word level by representing the meaning of phrases or sentences. These combinations are infinite and compositional me</context>
<context position="3632" citStr="Turney and Pantel (2010)" startWordPosition="538" endWordPosition="541">o are not interested in the compositional framework. DISSECT is freely available under the GNU General Public License. 2 Building and composing distributional semantic representations The pipeline from corpora to compositional models of meaning can be roughly summarized as consisting of three stages:1 1. Extraction of co-occurrence counts from corpora In this stage, an input corpus is used to extract counts of target elements co-occurring with some contextual features. The target elements can vary from words (for lexical similarity), to pairs of words (e.g., for relation categorization), 1See Turney and Pantel (2010) for a technical overview of distributional methods for semantics. 31 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 31–36, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics to paths in syntactic trees (for unsupervised paraphrasing). Context features can also vary from shallow window-based collocates to syntactic dependencies. 2. Transformation of the raw counts This stage may involve the application of weighting schemes such as Pointwise Mutual Information, feature selection, dimensionality reduction methods suc</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Zanzotto</author>
<author>Ioannis Korkontzelos</author>
<author>Francesca Falucchi</author>
<author>Suresh Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1263--1271</pages>
<location>Beijing, China.</location>
<contexts>
<context position="5567" citStr="Zanzotto et al. (2010)" startWordPosition="813" endWordPosition="816">p is highly language-, task- and corpus-annotation-dependent. We do not attempt to implement all the corpus pre-processing and co-occurrence extraction routines that it would require to be of general use, and expect instead as input a matrix of raw target-context co-occurrence counts.2 DISSECT provides various methods to re-weight the counts with association measures, dimensionality reduction methods as well as the composition functions proposed by Mitchell and Lapata (2010) (Additive, Multiplicative and Dilation), Baroni and Zamparelli (2010)/Coecke et al. (2010) (Lexfunc) and Guevara (2010)/Zanzotto et al. (2010) (Fulladd). In DISSECT we define and implement these in a unified framework and in a computationally efficient manner. The focus of DISSECT is to provide an intuitive interface for researchers and to allow easy extension by adding other composition methods. 3 DISSECT overview DISSECT is written in Python. We provide many standard functionalities through a set of power2These counts can be read from a text file containing two strings (the target and context items) and a number (the corresponding count) on each line (e.g., maggot food 15) or from a matrix in format word freq1 freq2 ... #create a </context>
</contexts>
<marker>Zanzotto, Korkontzelos, Falucchi, Manandhar, 2010</marker>
<rawString>Fabio Zanzotto, Ioannis Korkontzelos, Francesca Falucchi, and Suresh Manandhar. 2010. Estimating linear models for compositional distributional semantics. In Proceedings of COLING, pages 1263– 1271, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>