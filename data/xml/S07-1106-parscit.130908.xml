<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.038048">
<title confidence="0.979611">
UofL: Word Sense Disambiguation Using Lexical Cohesion
</title>
<author confidence="0.991955">
Yllias Chali
</author>
<affiliation confidence="0.9917965">
Department of Computer Science
University of Lethbridge
</affiliation>
<address confidence="0.927785">
Lethbridge, Alberta, Canada, T1K 3M4
</address>
<email confidence="0.998361">
chali@cs.uleth.ca
</email>
<author confidence="0.823975">
Shafiq R. Joty
</author>
<affiliation confidence="0.985795">
Department of Computer Science
University of Lethbridge
</affiliation>
<address confidence="0.744639">
Lethbridge, Alberta, Canada, T1K 3M4
</address>
<email confidence="0.999025">
jotys@cs.uleth.ca
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998801461538461">
One of the main challenges in the applica-
tions (i.e.: text summarization, question an-
swering, information retrieval, etc.) of
Natural Language Processing is to deter-
mine which of the several senses of a word
is used in a given context. The problem is
phrased as “Word Sense Disambiguation
(WSD)” in the NLP community. This paper
presents the dictionary based disambigua-
tion technique that adopts the assumption
of one sense per discourse in the context of
SemEval-2007 Task 7: “Coarse-grained
English all-words”.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999347705882353">
Cohesion can be defined as the way certain words
or grammatical features of a sentence can connect
it to its predecessors (and successors) in a text.
(Halliday and Hasan, 1976) defined cohesion as
“the set of possibilities that exist in the language
for making text hang together”. Cohesion occurs
where the interpretation of some element in the
discourse is dependent on that of another. For ex-
ample, an understanding of the reference of a pro-
noun (i.e.: he, she, it, etc.) requires to look back to
something that has been said before. Through this
cohesion relation, two text clauses are linked to-
gether.
Cohesion is achieved through the use in the text
of semantically related terms, reference, ellipse and
conjunctions (Barzilay and Elhadad, 1997). Among
the different cohesion-building devices, the most
easily identifiable and the most frequent type is
lexical cohesion. Lexical cohesion is created by
using semantically related words (repetitions,
synonyms, hypernyms, hyponyms, meronyms and
holonyms, glosses, etc.)
Our technique used WordNet (Miller, 1990) as
the knowledge source to find the semantic relations
among the words in a text. We assign weights to
the semantic relations. The technique can be de-
composed into two steps: (1) building a representa-
tion of all possible senses of the words and (2) dis-
ambiguating the words based on the highest score.
The remainder of this paper is organized as fol-
lows. In the next section, we review previous work.
In Section 3, we define the semantic relations and
their weights. Section 4 presents our two step pro-
cedure for WSD. We conclude with the evaluation.
</bodyText>
<sectionHeader confidence="0.996685" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999590833333333">
Lexical Chaining is the process of connecting se-
mantically related words, creating a set of chains
that represent different threads of cohesion through
the text (Galley and McKeown, 2003). This inter-
mediate representation of text has been used in
many natural language processing applications,
including automatic summarization (Barzilay and
Elhadad, 1997; Silber and McCoy, 2003), informa-
tion retrieval (Al-Halimi and Kazman, 1998), and
intelligent spell checking (Hirst and St-Onge,
1998).
Morris and Hirst (1991) at first proposed a man-
ual method for computing lexical chains and first
computational model of lexical chains was intro-
duced by Hirst and St-Onge (1997). This linear-
time algorithm, however, suffers from inaccurate
WSD, since their greedy strategy immediately dis-
ambiguates a word as it is first encountered. Later
</bodyText>
<page confidence="0.963106">
476
</page>
<bodyText confidence="0.98719695">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 476–479,
Prague, June 2007. c�2007 Association for Computational Linguistics
research (Barzilay and Elhadad, 1997) significantly
alleviated this problem at the cost of a worse run-
ning time (quadratic); computational inefficiency is
due to their processing of many possible combina-
tions of word senses in the text in order to decide
which assignment is the most likely. Silber and
McCoy (2003) presented an efficient linear-time
algorithm to compute lexical chains, which models
Barzilay’s approach, but nonetheless has inaccura-
cies in WSD.
More recently, Galley and McKeown (2003)
suggested an efficient chaining method that sepa-
rated WSD from the actual chaining. It performs
the WSD before the construction of the chains.
They showed that it could achieve more accuracy
than the earlier ones. Our method follows the simi-
lar technique with some new semantic relations
(i.e.: gloss, holonym, meronym).
</bodyText>
<sectionHeader confidence="0.99143" genericHeader="method">
3 Semantic Relations
</sectionHeader>
<bodyText confidence="0.999971166666667">
We used WordNet2.11 (Miller, 1990) and eXtended
WordNet (Moldovan and Mihalcea, 2001) as our
knowledge source to find the semantic relations
among the words in a context. We assigned a
weight to each semantic relation. The relations and
their scores are summarized in the table 1.
</bodyText>
<sectionHeader confidence="0.929571" genericHeader="method">
4 System Overview
</sectionHeader>
<figureCaption confidence="0.951920666666667">
The global architecture of our system is shown in
Figure 1. Each of the modules of the system is de-
scribed below.
</figureCaption>
<subsectionHeader confidence="0.995054">
4.1 Context Processing
</subsectionHeader>
<bodyText confidence="0.999689583333333">
Context-processing involves preprocessing the con-
texts using several tools. We have used the follow-
ing tools:
Extracting the main text: This module extracts
the context of the target word from the source xml
document removing the unnecessary tags and
makes the context ready for further processing.
Sentence Splitting, Text Stemming and
Chunking: This module splits the context into sen-
tences, then stems out the words and chunks those.
We used OAK systems2 (Sekine, 2002) for this
purpose.
</bodyText>
<footnote confidence="0.9956105">
1 http://wordnet.princeton.edu/
2 http://nlp.cs.nyu.edu/oak/
</footnote>
<bodyText confidence="0.910599">
Candidate Words Extraction: This module ex-
tracts the candidate words (for task 7: noun, verb,
adjective and adverb) from the chunked text.
</bodyText>
<subsectionHeader confidence="0.995234">
4.2 All Sense Representation
</subsectionHeader>
<bodyText confidence="0.999852571428571">
Each candidate word is expanded to all of its
senses. We created a hash representation to identify
all possible word representations, motivated from
Galley and McKeown (2003). Each word sense is
inserted into the hash entry having the index value
equal to its synsetID. For example, athlete and jock
are inserted into the same hash entry (Figure 2).
</bodyText>
<figureCaption confidence="0.978651">
Figure 2. Hash indexed by synsetID
</figureCaption>
<bodyText confidence="0.894968764705882">
On insertion of the candidate sense into the hash
we check to see if there exists an entry into the in-
dex value, with which the current word sense has
one of the above mentioned relations. No disam-
biguation is done at this point; the only purpose is
to build a representation used in the next stage of
the algorithm. This representation can be shown as
a disambiguation graph (Galley and McKeown,
2003) where the nodes represent word instances
with their WordNet senses and weighted edges
connecting the senses of two different words repre-
sent semantic relations (Figure: 3).
Figure 3. Partial Disambiguation graph, Bass has
two senses, 1. Food related 2. Music instrument
related sense. The instrument sense dominates over
the fish sense as it has more relations (score) with
the other words in the context.
</bodyText>
<figure confidence="0.9995068125">
Jock
Gymnast
Athlete
Hypernym/
Hyponym
09675378
10002518
Fish
sound
property
ground
bass
Pitch
Food sense
Bass
Instrument sense
</figure>
<page confidence="0.982202">
477
</page>
<subsectionHeader confidence="0.996699">
4.3 Sense Disambiguation
</subsectionHeader>
<bodyText confidence="0.969846733333333">
We use the intermediate representation (disam-
biguation graph) to perform the WSD. We sum the
weight of all edges leaving the nodes under their
different senses. The one sense with the highest
score is considered the most probable sense. For
example in fig: 3 Bass is connected with three
words: Pitch, ground bass and sound property by
its instrument sense and with one word: Fish by its
Food sense. For this specific example all the se-
mantic relations are of Hyponym/Hypernym type
(score 0.33). So we get the score as in table 2.
In case of tie between two or more senses, we
select the one sense that comes first in WordNet,
since WordNet orders the senses of a word by de-
creasing order of frequency.
</bodyText>
<table confidence="0.999132571428571">
Sense Mne- Score Disambigu-
monic ated Sense
4928349 Musical 3*0.33 Musical In-
Instru- =0.99 strument
ment (4928349)
7672239 Fish or 0.33
Food
</table>
<tableCaption confidence="0.997862">
Table 2. Score of the senses of word “Bass”
</tableCaption>
<table confidence="0.9996581875">
Relation Definition Example Weight
Repetition Same occurrences of the word Weather is great in Atlanta. Florida is 1
having a really bad weather.
Synonym Words belonging to the same syn- Not all criminals are outlaws. 1
set in WordNet
Hypernym Y is a hypernym of X if X is a Peter bought a computer. It was a Dell 0.33
and Hypo- (kind of) Y And machine.
nym X is a hyponym of Y if X is a (kind
of) Y.
Holonym Y is a holonym of X if X is a part The keyboard of this computer is not 0.33
And of Y And working.
Meronym X is a meronym of Y if X is a part
of Y
Gloss Definition and/or example sen- Gloss of word “dormitory” is 0.33
tences for a synset. {a college or university building con-
taining living quarters for students}
</table>
<tableCaption confidence="0.999845">
Table 1: The relations and their associated weights
</tableCaption>
<figureCaption confidence="0.997101">
Figure 1: Overview of WSD System
</figureCaption>
<figure confidence="0.931801666666667">
Source Con-
text
Disambiguated
Sense
Context
Processing
Sense Disam-
biguation
Chunked Text
Candidate words
Extraction
All Sense Represen-
tation
478
5 Evaluation
</figure>
<figureCaption confidence="0.4397415">
Chali, Y. and Kolla, M. 2004. Summarization
techniques at DUC 2004. In Proceedings of the
</figureCaption>
<table confidence="0.995491222222222">
Document Understanding Conference, pages
105 -111, Boston. NIST.
In SemEval-2007, we participated in Task 7:
“Coarse-grained English all-words”. The evalua-
tion of our system is given below:
Cases Precision Recall F1-measure
Average 0.52592 0.48744 0.50595
Best 0.61408 0.59239 0.60304
Worst 0.44375 0.41159 0.42707
</table>
<sectionHeader confidence="0.997957" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999898541666667">
In this paper, we presented briefly our WSD sys-
tem in the context of SemEval 2007 Task 7. Along
with normal WordNet relations, our method also
included additional relations such as repetition and
gloss using semantically enhanced tool, eXtended
WordNet. After disambiguation, the intermediate
representation (disambiguation graph) can be used
to build the lexical chains which in tern can be used
as an intermediate representation for other NLP
applications such as text summarization, question
answering, text clustering. This method (summing
edge weights in selecting the right sense) of WSD
before constructing the chain (Gallery and McKe-
own, 2003) outperforms the earlier methods of
Barzilay and Elhadad (1997) and Silber and
McCoy (2003) but this method is highly dependent
on the lexical cohesion among words in a context.
So the length of context is an important factor for
our system to achieve good performance. For the
task the context given for a tagged word was not so
large to capture the semantic relations among
words. This may be the one of the reasons for
which our system could not achieve one of the best
results.
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999865852941176">
Barzilay, R. and Elhadad, M. 1997. Using Lexical
Chains for Text Summarization. In Proceedings
of the 35th Annual Meeting of the Association
for Computational Linguistics and the 8th Euro-
pean Chapter Meeting of the Association for
Computational Linguistics, Workshop on Intel-
ligent Scalable Test Summarization, pages 10-
17, Madrid.
Galley, M. and McKeown, K. 2003. Improving
Word Sense Disambiguation in Lexical Chain-
ing. In Proceedings of the 18th International
Joint Conference on Artificial Intelligence
(IJCAI’03), pages 1486-1488, Acapulco, Mex-
ico.
Halliday M. and Hasan R. 1976. Cohesion in Eng-
lish. Longman, London.
Harabagiu S. and Moldovan D. 1998. WordNet:
An Electronic Lexical Database, chapter Knowl-
edge Processing on an Extended WordNet. MIT
press.
Hirst G. and St-Onge D. 1997. Lexical Chains as
representation of context for the detection and
correction of malapropisms. In Christiane Fell-
baum, editor, WordNet: An Electronic Lexical
Database and Some of its Applications. MIT
Press, pages 305-332.
Morris J. and Hirst. G. 1991, Lexical Cohesion
Computed by Thesaural Relations as an Indica-
tor of the Structure of Text .Computational Lin-
guistics, 17(1):21-48.
Silber H.G. and McCoy K.F. 2002. Efficiently Com-
puted Lexical Chains As an Intermediate Representa-
tion for Automatic Text Summarization. Computa-
tional Linguistics, 28(4):487-496.
</reference>
<page confidence="0.999163">
479
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.322697">
<title confidence="0.999764">UofL: Word Sense Disambiguation Using Lexical Cohesion</title>
<author confidence="0.997473">Yllias Chali</author>
<affiliation confidence="0.9999805">Department of Computer Science University of Lethbridge</affiliation>
<address confidence="0.99906">Lethbridge, Alberta, Canada, T1K 3M4</address>
<email confidence="0.993607">chali@cs.uleth.ca</email>
<author confidence="0.999766">Shafiq R Joty</author>
<affiliation confidence="0.999963">Department of Computer Science University of Lethbridge</affiliation>
<address confidence="0.999332">Lethbridge, Alberta, Canada, T1K 3M4</address>
<email confidence="0.996784">jotys@cs.uleth.ca</email>
<abstract confidence="0.983561">One of the main challenges in the applications (i.e.: text summarization, question answering, information retrieval, etc.) of Natural Language Processing is to determine which of the several senses of a word is used in a given context. The problem is phrased as “Word Sense Disambiguation (WSD)” in the NLP community. This paper presents the dictionary based disambiguation technique that adopts the assumption of one sense per discourse in the context of</abstract>
<note confidence="0.574518">SemEval-2007 Task 7: “Coarse-grained English all-words”.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using Lexical Chains for Text Summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th European Chapter Meeting of the Association for Computational Linguistics, Workshop on Intelligent Scalable Test Summarization,</booktitle>
<pages>10--17</pages>
<location>Madrid.</location>
<contexts>
<context position="1593" citStr="Barzilay and Elhadad, 1997" startWordPosition="242" endWordPosition="245">nd successors) in a text. (Halliday and Hasan, 1976) defined cohesion as “the set of possibilities that exist in the language for making text hang together”. Cohesion occurs where the interpretation of some element in the discourse is dependent on that of another. For example, an understanding of the reference of a pronoun (i.e.: he, she, it, etc.) requires to look back to something that has been said before. Through this cohesion relation, two text clauses are linked together. Cohesion is achieved through the use in the text of semantically related terms, reference, ellipse and conjunctions (Barzilay and Elhadad, 1997). Among the different cohesion-building devices, the most easily identifiable and the most frequent type is lexical cohesion. Lexical cohesion is created by using semantically related words (repetitions, synonyms, hypernyms, hyponyms, meronyms and holonyms, glosses, etc.) Our technique used WordNet (Miller, 1990) as the knowledge source to find the semantic relations among the words in a text. We assign weights to the semantic relations. The technique can be decomposed into two steps: (1) building a representation of all possible senses of the words and (2) disambiguating the words based on th</context>
<context position="2830" citStr="Barzilay and Elhadad, 1997" startWordPosition="435" endWordPosition="438">core. The remainder of this paper is organized as follows. In the next section, we review previous work. In Section 3, we define the semantic relations and their weights. Section 4 presents our two step procedure for WSD. We conclude with the evaluation. 2 Previous Work Lexical Chaining is the process of connecting semantically related words, creating a set of chains that represent different threads of cohesion through the text (Galley and McKeown, 2003). This intermediate representation of text has been used in many natural language processing applications, including automatic summarization (Barzilay and Elhadad, 1997; Silber and McCoy, 2003), information retrieval (Al-Halimi and Kazman, 1998), and intelligent spell checking (Hirst and St-Onge, 1998). Morris and Hirst (1991) at first proposed a manual method for computing lexical chains and first computational model of lexical chains was introduced by Hirst and St-Onge (1997). This lineartime algorithm, however, suffers from inaccurate WSD, since their greedy strategy immediately disambiguates a word as it is first encountered. Later 476 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 476–479, Prague, June 2007. </context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Barzilay, R. and Elhadad, M. 1997. Using Lexical Chains for Text Summarization. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and the 8th European Chapter Meeting of the Association for Computational Linguistics, Workshop on Intelligent Scalable Test Summarization, pages 10-17, Madrid.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>K McKeown</author>
</authors>
<title>Improving Word Sense Disambiguation in Lexical Chaining.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI’03),</booktitle>
<pages>1486--1488</pages>
<location>Acapulco, Mexico.</location>
<contexts>
<context position="2662" citStr="Galley and McKeown, 2003" startWordPosition="413" endWordPosition="416">e technique can be decomposed into two steps: (1) building a representation of all possible senses of the words and (2) disambiguating the words based on the highest score. The remainder of this paper is organized as follows. In the next section, we review previous work. In Section 3, we define the semantic relations and their weights. Section 4 presents our two step procedure for WSD. We conclude with the evaluation. 2 Previous Work Lexical Chaining is the process of connecting semantically related words, creating a set of chains that represent different threads of cohesion through the text (Galley and McKeown, 2003). This intermediate representation of text has been used in many natural language processing applications, including automatic summarization (Barzilay and Elhadad, 1997; Silber and McCoy, 2003), information retrieval (Al-Halimi and Kazman, 1998), and intelligent spell checking (Hirst and St-Onge, 1998). Morris and Hirst (1991) at first proposed a manual method for computing lexical chains and first computational model of lexical chains was introduced by Hirst and St-Onge (1997). This lineartime algorithm, however, suffers from inaccurate WSD, since their greedy strategy immediately disambiguat</context>
<context position="3982" citStr="Galley and McKeown (2003)" startWordPosition="606" endWordPosition="609">n Semantic Evaluations (SemEval-2007), pages 476–479, Prague, June 2007. c�2007 Association for Computational Linguistics research (Barzilay and Elhadad, 1997) significantly alleviated this problem at the cost of a worse running time (quadratic); computational inefficiency is due to their processing of many possible combinations of word senses in the text in order to decide which assignment is the most likely. Silber and McCoy (2003) presented an efficient linear-time algorithm to compute lexical chains, which models Barzilay’s approach, but nonetheless has inaccuracies in WSD. More recently, Galley and McKeown (2003) suggested an efficient chaining method that separated WSD from the actual chaining. It performs the WSD before the construction of the chains. They showed that it could achieve more accuracy than the earlier ones. Our method follows the similar technique with some new semantic relations (i.e.: gloss, holonym, meronym). 3 Semantic Relations We used WordNet2.11 (Miller, 1990) and eXtended WordNet (Moldovan and Mihalcea, 2001) as our knowledge source to find the semantic relations among the words in a context. We assigned a weight to each semantic relation. The relations and their scores are sum</context>
<context position="5655" citStr="Galley and McKeown (2003)" startWordPosition="868" endWordPosition="871">ready for further processing. Sentence Splitting, Text Stemming and Chunking: This module splits the context into sentences, then stems out the words and chunks those. We used OAK systems2 (Sekine, 2002) for this purpose. 1 http://wordnet.princeton.edu/ 2 http://nlp.cs.nyu.edu/oak/ Candidate Words Extraction: This module extracts the candidate words (for task 7: noun, verb, adjective and adverb) from the chunked text. 4.2 All Sense Representation Each candidate word is expanded to all of its senses. We created a hash representation to identify all possible word representations, motivated from Galley and McKeown (2003). Each word sense is inserted into the hash entry having the index value equal to its synsetID. For example, athlete and jock are inserted into the same hash entry (Figure 2). Figure 2. Hash indexed by synsetID On insertion of the candidate sense into the hash we check to see if there exists an entry into the index value, with which the current word sense has one of the above mentioned relations. No disambiguation is done at this point; the only purpose is to build a representation used in the next stage of the algorithm. This representation can be shown as a disambiguation graph (Galley and M</context>
</contexts>
<marker>Galley, McKeown, 2003</marker>
<rawString>Galley, M. and McKeown, K. 2003. Improving Word Sense Disambiguation in Lexical Chaining. In Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI’03), pages 1486-1488, Acapulco, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Halliday</author>
<author>R Hasan</author>
</authors>
<title>Cohesion in English.</title>
<date>1976</date>
<publisher>Longman,</publisher>
<location>London.</location>
<contexts>
<context position="1018" citStr="Halliday and Hasan, 1976" startWordPosition="148" endWordPosition="151">ion answering, information retrieval, etc.) of Natural Language Processing is to determine which of the several senses of a word is used in a given context. The problem is phrased as “Word Sense Disambiguation (WSD)” in the NLP community. This paper presents the dictionary based disambiguation technique that adopts the assumption of one sense per discourse in the context of SemEval-2007 Task 7: “Coarse-grained English all-words”. 1 Introduction Cohesion can be defined as the way certain words or grammatical features of a sentence can connect it to its predecessors (and successors) in a text. (Halliday and Hasan, 1976) defined cohesion as “the set of possibilities that exist in the language for making text hang together”. Cohesion occurs where the interpretation of some element in the discourse is dependent on that of another. For example, an understanding of the reference of a pronoun (i.e.: he, she, it, etc.) requires to look back to something that has been said before. Through this cohesion relation, two text clauses are linked together. Cohesion is achieved through the use in the text of semantically related terms, reference, ellipse and conjunctions (Barzilay and Elhadad, 1997). Among the different coh</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Halliday M. and Hasan R. 1976. Cohesion in English. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Harabagiu</author>
<author>D Moldovan</author>
</authors>
<title>WordNet: An Electronic Lexical Database, chapter Knowledge Processing on an Extended WordNet.</title>
<date>1998</date>
<publisher>MIT press.</publisher>
<marker>Harabagiu, Moldovan, 1998</marker>
<rawString>Harabagiu S. and Moldovan D. 1998. WordNet: An Electronic Lexical Database, chapter Knowledge Processing on an Extended WordNet. MIT press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Hirst</author>
<author>D St-Onge</author>
</authors>
<title>Lexical Chains as representation of context for the detection and correction of malapropisms.</title>
<date>1997</date>
<booktitle>In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database and Some of its Applications.</booktitle>
<pages>305--332</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="3144" citStr="Hirst and St-Onge (1997)" startWordPosition="484" endWordPosition="487"> semantically related words, creating a set of chains that represent different threads of cohesion through the text (Galley and McKeown, 2003). This intermediate representation of text has been used in many natural language processing applications, including automatic summarization (Barzilay and Elhadad, 1997; Silber and McCoy, 2003), information retrieval (Al-Halimi and Kazman, 1998), and intelligent spell checking (Hirst and St-Onge, 1998). Morris and Hirst (1991) at first proposed a manual method for computing lexical chains and first computational model of lexical chains was introduced by Hirst and St-Onge (1997). This lineartime algorithm, however, suffers from inaccurate WSD, since their greedy strategy immediately disambiguates a word as it is first encountered. Later 476 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 476–479, Prague, June 2007. c�2007 Association for Computational Linguistics research (Barzilay and Elhadad, 1997) significantly alleviated this problem at the cost of a worse running time (quadratic); computational inefficiency is due to their processing of many possible combinations of word senses in the text in order to decide which assi</context>
</contexts>
<marker>Hirst, St-Onge, 1997</marker>
<rawString>Hirst G. and St-Onge D. 1997. Lexical Chains as representation of context for the detection and correction of malapropisms. In Christiane Fellbaum, editor, WordNet: An Electronic Lexical Database and Some of its Applications. MIT Press, pages 305-332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G</author>
</authors>
<date>1991</date>
<booktitle>Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text .Computational Linguistics,</booktitle>
<pages>17--1</pages>
<marker>G, 1991</marker>
<rawString>Morris J. and Hirst. G. 1991, Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text .Computational Linguistics, 17(1):21-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H G Silber</author>
<author>K F McCoy</author>
</authors>
<title>Efficiently Computed Lexical Chains As an Intermediate Representation for Automatic Text Summarization.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--4</pages>
<marker>Silber, McCoy, 2002</marker>
<rawString>Silber H.G. and McCoy K.F. 2002. Efficiently Computed Lexical Chains As an Intermediate Representation for Automatic Text Summarization. Computational Linguistics, 28(4):487-496.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>