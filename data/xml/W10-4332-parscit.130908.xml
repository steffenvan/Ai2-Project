<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013724">
<title confidence="0.84063">
Enhanced Monitoring Tools and Online Dialogue Optimisation Merged
into a New Spoken Dialogue System Design Experience
Ghislain Putois Romain Laroche Philippe Bretier
</title>
<author confidence="0.767861">
Orange Labs Orange Labs Orange Labs
</author>
<affiliation confidence="0.919693">
Lannion, France Issy-les-Moulineaux, France Lannion, France
</affiliation>
<email confidence="0.818963">
firstname.surname@orange-ftgroup.com
</email>
<sectionHeader confidence="0.989146" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.98106937037037">
Building an industrial spoken dialogue
system (SDS) requires several iterations
of design, deployment, test, and evalua-
tion phases. Most industrial SDS develop-
ers use a graphical tool to design dialogue
strategies. They are critical to get good
system performances, but their evaluation
is not part of the design phase.
We propose integrating dialogue logs into
the design tool so that developers can
jointly monitor call flows and their asso-
ciated Key Performance Indicators (KPI).
It drastically shortens the complete devel-
opment cycle, and offers a new design ex-
perience.
Orange Dialogue Design Studio (ODDS),
our design tool, allows developers to de-
sign several alternatives and compare their
relative performances. It helps the SDS
developers to understand and analyse the
user behaviour, with the assistance of a re-
inforcement learning algorithm. The SDS
developers can thus confront the different
KPI and control the further SDS choices
by updating the call flow alternatives.
Index Terms: Dialogue Design, Online Learning,
Spoken Dialogue Systems, Monitoring Tools
</bodyText>
<sectionHeader confidence="0.998971" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999767888888889">
Recent research in spoken dialogue systems
(SDS) has called for a “synergistic convergence”
between research and industry (Pieraccini and
Huerta, 2005). This call for convergence concerns
architectures, abstractions and methods from both
communities. Under this motivation, several re-
search orientations have been proposed. This pa-
per discusses three of them : dialogue design, di-
alogue management, and dialogue evaluation. Di-
alogue design and dialogue management reflect in
this paper the respective paths that industry and
research have followed for building their SDS. Di-
alogue evaluation is a concern for both communi-
ties, but remains hard to put into operational per-
spectives.
The second Section presents the context and
related research. The third Section is devoted to
the presentation of the tools : the historical design
tool, its adaptation to provide monitoring function-
alities and the insertion of design alternatives. It is
eventually concluded with an attempt to reassess-
ing the dialogue evaluation. The fourth Section de-
scribes the learning integration to the tool, the con-
straints we impose to the learning technique and
the synergy between the tools and the embedded
learning capabilities. Finally, the last Section con-
cludes the paper.
</bodyText>
<sectionHeader confidence="0.992699" genericHeader="introduction">
2 Context
</sectionHeader>
<bodyText confidence="0.999935777777778">
The spoken dialogue industry is structured
around the architecture of the well known in-
dustrial standard VoiceXML 1. The underlying di-
alogue model of VoiceXML is a mapping of
the simplistic turn-based linguistic model on the
browser-server based Web architecture (McTear,
2004). The browser controls the speech engines
(recognition and text-to-speech) integrated into
the voice platform according to the VoiceXML
document served by an application server. A
VoiceXML document contains a set of prompts to
play and the list of the possible interactions the
user is supposed to have at each point of the di-
alogue. The SDS developers 2, reusing Web stan-
dards and technologies (e.g. J2EE, JSP, XML...),
are used to designing directed dialogues modelled
by finite state automata. Such controlled and de-
terministic development process allows the spoken
</bodyText>
<footnote confidence="0.97625525">
1. http ://www.w3c.org/TR/voicexml20/
2. In this paper, the term “SDS developers” denotes with-
out any distinction VUI designers, application developers,
and any industry engineers acting in SDS building.
</footnote>
<note confidence="0.790377">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 185–192,
</note>
<affiliation confidence="0.623997">
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.999581">
185
</page>
<bodyText confidence="0.999457814814815">
dialogue industry to reach a balance between us-
ability and cost (Paek, 2007). This paper argues
that tools are facilitators that improve both the us-
ability vs. cost trade-off and the reliability of new
technologies.
Spoken dialogue research has developed vari-
ous models and abstractions for dialogue manage-
ment : rational agency (Sadek et al., 1997), Infor-
mation State Update (Bos et al., 2003), functional
models (Pieraccini et al., 2001), planning problem
solving (Ferguson and Allen, 1998). Only a very
small number of these concepts have been trans-
ferred to industry. Since the late 90’s, the research
has tackled the ambitious problem of automating
the dialogue design (Lemon and Pietquin, 2007),
aiming at both reducing the development cost
and optimising the dialogue efficiency and robust-
ness. Recently, criticisms (Paek and Pieraccini,
2008) have been formulated and novel approaches
(Williams, 2008) have been proposed, both aiming
at bridging the gap between research –focused on
Markov-Decision-Process (Bellman, 1957) based
dialogue management– and industry –focused on
dialogue design process, model, and tools. This
paper contributes to extend this effort. It addresses
all these convergence questions together as a way
for research and industry to reach a technological
breakthrough.
Regarding the dialogue evaluation topic, Paek
(Paek, 2007) has pointed out that while research
has exerted attention about “how best to evaluate
a dialogue system?”, the industry has focused on
“how best to design dialogue systems ?”. This pa-
per unifies those two approaches by merging sys-
tem and design evaluation in a single graphical
tool. To our knowledge, ODDS is the only indus-
trial tool which handles the complete system life-
cycle, from design to evaluation.
The tools and methods presented below have
been tested and validated during the design and
implementation of a large real-world commercial
system: the 1013+ service is the Spoken Dialogue
System for landline troubleshooting for France.
It receives millions of calls a year and schedules
around 8, 000 appointments a week. When the
user calls the system, she is presented with an open
question asking her for the reason of her call. If
her landline is out of service, the Spoken Dialogue
System then performs some automated tests on
the line, and if the problem is confirmed, try and
schedule an appointment with the user for a man-
ual intervention. If the system and the user cannot
agree on an appointment slot, the call is transferred
to a human operator.
</bodyText>
<sectionHeader confidence="0.981332" genericHeader="method">
3 The tools
</sectionHeader>
<bodyText confidence="0.9998312">
Industry follows the VUI-completeness princi-
ple (Pieraccini and Huerta, 2005) : “the behaviour
of an application needs to be completely speci-
fied with respect to every possible situation that
may arise during the interaction. No unpredictable
user input should ever lead to unforeseeable be-
haviour”. The SDS developers consider reliable
the technologies, tools, and methodologies that
help them to reach the VUI-completeness and to
control it.
</bodyText>
<subsectionHeader confidence="0.997127">
3.1 The Dialogue Design Tool
</subsectionHeader>
<bodyText confidence="0.999532588235294">
The graphical abstraction proposed by our dia-
logue design tool conforms to the general graph
representation of finite state automata, with the
difference that global and local variables enable to
factorise several dialogue states in a single node.
Transitions relate to user inputs or to internal ap-
plication events such as conditions based on in-
ternal information from the current dialogue state,
from the back-end, or from the dialogue history. In
that sense, dialogue design in the industry gener-
ally covers more than strict dialogue management,
since its specification may indicate the type of spo-
ken utterance expected from the user at each stage
of the dialogue, up to the precise speech recogni-
tion model and parameter values to use, and the
generation of the system utterance, from natural
language generation to speech synthesis or audio
recordings.
Our dialogue design tool offers to the SDS de-
velopers a graphical abstraction of the dialogue
logic, sometimes also named the call flow. Thanks
to a dynamic VoiceXML generation functional-
ity, our dialogue design tool brings the SDS de-
velopers the guarantee that VUI-completeness at
the design level automatically implies a similar
completeness at the implementation level. During
maintenance, If the SDS developers modify a spe-
cific part of the dialogue design, the tool guar-
antees that solely the corresponding code is im-
pacted. This guarantee impacts positively VUI-
completeness, reliability, and development cost.
Figure 1 presents the design of a typical
VoiceXML page. This page is used when the sys-
tem asks the user to accept an appointment time
</bodyText>
<page confidence="0.994916">
186
</page>
<figureCaption confidence="0.518028">
FIGURE 1 – 1013+ design excerpt: the system asks the user to confirm an appointment slot
</figureCaption>
<bodyText confidence="0.999959733333333">
slot. It first begins with a prompt box mixing
static and dynamic prompts (the dynamic parts are
underlined and realised by service-specific java
code). A log box is then used some contextual ses-
sion variables. Then, an interaction box is used to
model the system reaction to the user behaviour :
on the lower part of the Figure, we program the
reaction to user inactivity or recognizer misunder-
standing. In the upper part, we use a recognition
box followed by a Natural Language Understand-
ing (NLU), and we program the different output
classes : repeat, yes, no and not understood. Each
output is linked to a transition box, which indi-
cates which VoiceXML page the service should
call next.
</bodyText>
<subsectionHeader confidence="0.9990205">
3.2 Monitoring Functionalities inside the
Design Tool
</subsectionHeader>
<bodyText confidence="0.999988511627907">
While researchers are focused on measuring the
progress they incrementally reach, industry engi-
neers have to deal with SDS tuning and upgrade.
Their first dialogue evaluation KPI is task com-
pletion also called the automation rate because a
SDS is deployed to automate specifically selected
tasks. Most of the time, task completion is esti-
mated thanks to the KPI. The KPI are difficult to
exhaustively list and classify. Some are related to
system measures, others are obtained thanks to di-
alogue annotations and the last ones are collected
from users through questionnaires.
Some studies (Abella et al., 2004) investigated
graphical monitoring tools. The corpus to visualise
is a set of dialogue logs. The tool aims at reveal-
ing how the system transits between its possible
states. As a dialogue system is too complex to enu-
merate all its possible states, the dialogue logs are
regarded as a set of variables that evolve during
time and the tool proposes to make a projection on
a subset of these variables. This way, the generated
graphs can either display the call flow, how the dif-
ferent steps are reached and where they lead, or
display how different variables, as the number of
errors evolve. This is mainly a tool for understand-
ing how the users behave, because it has no direct
connection with the way how the system was built.
As consequence to this, it does not help to diag-
nose how to make it better. In other words, it does
evaluate the system but does not meet one of our
goal : the convergence between design and evalu-
ation.
On the opposite, our graphical design tool pro-
vides an innovative functionality : local KPI pro-
jection into the original dialogue design thanks to
an extensive logging. A large part of the KPI are
automatically computed and displayed. As a con-
sequence, it is possible to display percentage of
which responses the system recognised, the users
actually gave, and see how these numbers match
the various KPI. It is one example among the nu-
merous analysis views this graphical tool can pro-
vide.
</bodyText>
<subsectionHeader confidence="0.99984">
3.3 Insertion of Alternatives
</subsectionHeader>
<bodyText confidence="0.999688210526316">
The 1013+ service has been used to test three
kinds of design alternatives. The first kind is a
strategy alternative : the service can choose be-
tween offering an appointment time slot to the
client, or asking her for a time slot. This deci-
sion defines whether the next dialogue step will
be system-initiative or user-initiative. The second
kind is a speaking style alternative : the service
can either be personified by using the “I” pronoun,
adopt a corporate style by using the “We” pro-
noun, or speak in an impersonal style by using the
passive mode. The third kind is a Text-To-Speech
alternative: the service can use a different wording
or prosody for a given sentence.
Figure 2 displays a monitoring view of an in-
teraction implementation with alternatives. The
recognition rate is the projected KPI on the graph
at each branch. Other performance indicators are
displayed at the bottom of the window : here, it
</bodyText>
<page confidence="0.989774">
187
</page>
<figureCaption confidence="0.844439">
FIGURE 2 – Some user experience feedbacks related to a selected prompt alternative.
</figureCaption>
<bodyText confidence="0.974380097222223">
is the actual rate of correct semantic decoding, the
semantic substitution rate, and the semantic rejec-
tion rate. The selection of the highlighted box con-
ditions the displayed logs.
Our design tool also provides a multivariate
testing functionality. This method consists in test-
ing multiple alternatives and selecting the best one
on a fixed set of predetermined criteria. Regarding
the VUI-completeness, presenting the complete
automaton to the SDS developers is acceptable, as
long as they can inspect and control every branch
of the design. In general, they even come up with
several competing designs or points of choice,
which can only be properly selected from in a sta-
tistical manner. The ability to compare all the di-
alogue design alternatives in the same test-field is
a major factor to boost up SDS enhancement by
drastically reducing the time needed. When we
were developing the current 1013+ version, we
have been able to develop the 5 main alternatives
in less than a month, where it had taken a month
and a half for a unique alternative in previous ver-
sions. It brings a statistical relevance in the causal
link between the tested alternatives and the differ-
ences in performance measures, because it ensures
a good random input space coverage.
The KPI graphical projection into the dialogue
design covers the dialogue alternatives : KPI com-
putation just needs to be conditioned by the alter-
natives. Figure 2 illustrates the merge of several
system prompt alternatives inside a single design.
It represents the prompt alternatives the system
can choose when proposing an appointment time
slot. An action block informs the Learning Man-
ager about the current dialogue state and avail-
able dialogue alternatives. An “If” block then ac-
tivates the prompt alternative corresponding to a
local variable “choixPDC” filled by the Learning
Manager. The rest of the design is identical to the
design presented in Figure 1.
The displayed KPI are conditioned by the se-
lected alternative (here, the second wording cir-
cled in bold grey). ODDS then indicates how the
dialogue call flow is breakdown into the different
alternatives. As we have here conditioned the dis-
played information by the second alternative, this
alternative receives 100% of the calls displayed,
when the other alternatives are not used. We can
then see the different outcomes for the selected
alternative : the customer answer have lead to a
timeout of the recognition in 11.78% of the cases,
and amongst the recognised sentences, 80% were
an agreement, 13.33% were a reject, and 6.67%
were not understood.
On the bottom-left part, one can display more
specific KPI, such as good interpretation rate, sub-
stitution rate, and reject rate. These KPI are com-
puted after the collected logs have been manually
annotated, which remains an indispensable pro-
cess to monitor and improve the recognition and
NLU quality, and thus the overall service quality.
Conditioning on another alternative would have
immediately led to different results, and someway,
embedding the user experience feedback inside the
dialogue design forms a new material to touch and
feel : the SDS developers can now sculpt a unique
reactive material which contains the design and the
KPI measures distribution. By looking at the influ-
ence of each alternative on the KPI when graphi-
cally selecting the alternatives, the SDS develop-
ers are given a reliable means to understand how
to improve the system.
</bodyText>
<page confidence="0.996445">
188
</page>
<subsectionHeader confidence="0.959099">
3.4 Reassessing Dialogue Evaluation
</subsectionHeader>
<bodyText confidence="0.99999175">
The traditional approaches to dialogue evalu-
ation attempt to measure how best the SDS is
adapted to the users. We remind that each inter-
action between the user and the SDS appears to
be a unique performance. First, each new dialogue
is co-built in a unique way according to both the
person-specific abilities of the user and the possi-
bilities of the SDS. Second, the user adapts very
quickly to new situations and accordingly changes
her practices. The traditional approaches to dia-
logue evaluation are eventually based on the frag-
ile reference frame of the user, not reliable enough
for a scientific and an industrial approach of the
spoken dialogue field, mostly because of the in-
ability to get statistical call volumes for all the di-
alogue alternatives.
This suggests for a shift in the reference frame
used for dialogue evaluation : instead of trying to
measure the adequacy between the SDS and the
user in the user’s reference frame, one can measure
the adequacy between the user and the SDS in the
design reference frame composed by the dialogue
logic, the KPI and their expected values. Taking
the design as the reference allows reassessing the
dialogue evaluation. The proposed basis for dia-
logue evaluation is reliable for the SDS developers
because it is both stable and entirely under con-
trol. Deviations from the predicted situations are
directly translated into anomalous values of mea-
surable KPI that raise alerts. These automatically
computable alerts warn the SDS developers about
the presence of issues in their dialogue design.
</bodyText>
<sectionHeader confidence="0.932797" genericHeader="method">
4 Dialogue design learning
</sectionHeader>
<bodyText confidence="0.999908571428571">
As presented in previous Section, the alterna-
tive insertion is an enabler for the dialogue system
analysis tools. It provides the SDS developers with
a novel call flow visualisation experience. The fur-
ther step to this approach is to automate at least
a part of those analyses and improvements with
learning capabilities.
</bodyText>
<subsectionHeader confidence="0.963234">
4.1 Constraints
</subsectionHeader>
<bodyText confidence="0.99996284375">
The objective is to automatically choose online
the best alternative among those proposed in the
design tool, and to report this choice to the SDS
developers via the monitoring functionalities that
are integrated to the design tool. This approach
differs from the classical reinforcement learning
methods used in the dialogue literature, which
make their decisions at the dialogue turn level.
We use a technique from a previous work
(Laroche et al., 2009). It does not need to de-
clare the reachable states : they are automatically
created when reached. This is also a parameter-
free algorithm, which is very important when we
consider that most dialogue application developers
are not familiar with reinforcement learning the-
ory. We keep the developer focussed on its main
task. The two additional tasks required for the re-
inforcement learning are to define the variable set
on which the alternative choice should depend,
and to implement a reward function based on the
expected evaluation of the task completion, in or-
der to get a fully automated optimisation with an
online evaluation. The dialogue system automatic
evaluation is a large problem that goes beyond
the scope of this paper. However, sometimes, the
dialogue application enables to have an explicit
validation from the user. For instance, in an ap-
pointment scheduling application, the user is re-
quired to explicitly confirm the schedule he was
proposed. This user performative act completes
the task and provides a reliable automatic evalu-
ation.
</bodyText>
<subsectionHeader confidence="0.996531">
4.2 Learning and Monitoring Synergy in the
Design Optimisation
</subsectionHeader>
<bodyText confidence="0.999963565217391">
The learning algorithm and the SDS developers
are two actors on the same object : the dialogue
system. But, they work at a different time space.
The learning algorithm updates its policy after
each dialogue while the SDS developers moni-
tor the system behaviour more occasionally. The
same kind of opposition can be made on the action
space of those actors. The learning algorithm can
only change its policy among a limited amount of
alternatives, while the SDS developers can make
deeper changes, such as implementing a new dia-
logue branch, adding new alternatives, new alter-
native points, removing alternatives, etc...
Last but not least, their sight ranges vary a lot
too. The learning algorithm is concentrated on the
alternative sets and automatic evaluation and ig-
nores the rest, while the SDS developers can ap-
prehend the dialogue application as a whole, as a
system or as a service. They can also have access
to additional evaluations through annotations, or
user subjective evaluations.
These functionality differences make their re-
spective roles complementary. The SDS develop-
</bodyText>
<page confidence="0.989936">
189
</page>
<figure confidence="0.98955375">
Score
0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
day
FIGURE 3 – Evolution of the system score
24
23
22
21
20
19
3-day means
daily score
</figure>
<bodyText confidence="0.999663666666667">
ers have the responsibility for the whole appli-
cation and the macro-strategic changes while the
learning manager holds the real-time optimisation.
</bodyText>
<subsectionHeader confidence="0.984257">
4.3 Control vs. Automation : the Trusting
Threshold
</subsectionHeader>
<bodyText confidence="0.999984982758621">
As argued by Pieraccini and Huerta (Pieraccini
and Huerta, 2005), finite state machine applied to
dialogue management does not restrict the dia-
logue model to strictly directed dialogues. Finite
state machines are easily extensible to powerful
and flexible dialogue models. Our dialogue design
tool offers various extensions : dialogue modules,
hierarchical design, arbitrary function invocation
at any point of the design, conditional statements
to split the flow in different paths. All those ex-
tensions allow designing any topology of the fi-
nite state machine required to handle complex dia-
logue models like mixed-initiative interaction. Di-
alogue model is not the point where research and
industry fail to converge.
The divergence point concerns the control as-
pect of VUI-completeness versus the automation
of the dialogue design. As pointed out by recent
works (Paek and Pieraccini, 2008), MDP-based
dialogue management aiming at automating the
whole dialogue design is rejected by the SDS de-
velopers. Even more adaptive, it is seen as an un-
controllable black box sensitive to the tuning pro-
cess. The SDS developers do not rely on systems
that dynamically build their dialogue logic without
a sufficient degree of monitoring and control.
Williams (Williams, 2008) has made a substan-
tial effort to meet this industrial requirement. His
system is a hybridisation of a conventional dia-
logue system following an industrial process, with
a POMDP decision module, which is a MDP-
based approach to dialogue management enhanced
with dialogue state abstractions to model uncer-
tainties. The responsibilities of each part of the
system are shared as follows : the conventional
system elects several candidate dialogue moves
and the POMDP decision module selects the most
competitive one. This is a great step towards in-
dustry because the dialogue move chosen by the
POMDP module has been first controlled by the
conventional system design. Nevertheless, the so-
built hybrid system is still not fully compliant with
the industrial constraints for the following reasons.
First, contrary to our approach, the SDS devel-
oper is called upon specific skills that cannot be
demanded to a developer (modeling and tuning a
(PO)MDP). This is a no-go for further integration
in an industrial process.
Second, such a predictive module is not self-
explanatory. Although the SDS developers have
the control on the possible behaviour presented to
the POMDP decision module, they are given no
clue to understand how the choices are made. In
fact, a learnt feature can never be exported to an-
other context. At the opposite, our approach al-
lows us to learn at the design level and conse-
quently to report in the automaton the optimisa-
tion. The learning results are therefore understand-
</bodyText>
<page confidence="0.990544">
190
</page>
<bodyText confidence="0.999938">
able, analysable and replicable on a larger scale, in
a way similar to classical ergonomics guidelines
(but statistically proved).
</bodyText>
<subsectionHeader confidence="0.995704">
4.4 Learning results on the 1013+ service
</subsectionHeader>
<bodyText confidence="0.999983826086957">
In the 1013+ service, our experiments have fo-
cused on the appointment scheduling domain. We
have chosen to integrate the following rewards in
the service : each time a user successfully man-
ages to get an appointment, the system is given a
+30 reward. If the system is unable to provide an
appointment, but manages to transfer the user to a
human operator, the system is given a +10 (a “re-
sit”). Last, if the user hangs up, the system is not
given any positive reward. Every time the system
does not hear nor understand the user, it is given a
penalty of 1.
In the beginning of the experiment, when the
system is still using a random policy, the comple-
tion rate is as low as 51%, and the transfer rate is
around 36%. When the system has learned its op-
timal policy, the completion rate raises up to 70%,
with a transfer rate around 20%. In our experi-
ment, the system has learned to favour an imper-
sonal speaking style (passive mode) and it prefers
proposing appointment time slots rather than ask-
ing the user to make a proposition (the later case
leading to lot of “in private” user talks and hesita-
tions, and worse recognition performance).
Figure 3 shows the evolution of the mean di-
alogue score during the first month. Each server
have its own Learning Manager database, and op-
timises separately. This is a welcome feature, as
each server can address a different part of the
user population, which is a frequent operational
requirement.
The dialogue score drawn on Figure 3 is com-
puted by averaging the mean dialogue score per
server. The crossed line represents the daily mean
dialogue score. The normal line represents the 3-
day smoothed dialogue mean score. The grayed
area represents the 95% confidence interval. Dur-
ing this first month of commercial exploitation,
one can notice two major trends : at first, the di-
alogue score is gradually increasing until day 20,
then the performances noticeably drops, before
rising up again. It turns out that new servers were
introduced on day 20, which had to learn the op-
timal dialogue policy. Ultimately (on the second
month), they converge to the same solution as the
first servers.
</bodyText>
<sectionHeader confidence="0.999295" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<subsectionHeader confidence="0.987358">
5.1 A New Basis for Trusting Automatic
Learning
</subsectionHeader>
<bodyText confidence="0.999935259259259">
This paper presents an original dialogue design
tool that mixes dialogue design and dialogue eval-
uation in the same graphical interface. The de-
sign paradigm supported by the tool leads the SDS
developers to predict value ranges of local KPI
while designing the dialogue logic. It results a new
evaluation paradigm using the system design as
the reference and trying to measure deviations be-
tween the predicted and the measured values of
the designed local KPI. The SDS developers rely
on the tool to fulfil the VUI-completeness princi-
ple. Classically applied to dialogue design, the tool
enables its application to the dialogue evaluation,
leading to the comparison of dialogue design al-
ternatives.
This places the SDS developers in a dialogue
design improvement cycle close to the reinforce-
ment learning decision process. Moreover, the in-
spector offered by the user experience feedback
functionality allows the SDS developers to un-
derstand, analyse and generalize all the decisions
among the dialogue design alternatives. Combin-
ing the learning framework and the design tool
guarantees the SDS developers keep control of the
system. It preserves VUI-completeness and opens
the way to a reliable learning based dialogue man-
agement.
</bodyText>
<subsectionHeader confidence="0.966194">
5.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999947909090909">
This approach to learning led us to deploy in
October 2009 the first commercial spoken dia-
logue system with online learning. The system’s
task is to schedule an appointment between the
customer and a technician. This service receives
approximately 8, 000 calls every month. At the
time those lines are written, we are already in a vir-
tuous circle of removing low-rated alternatives and
replacing them with new ones, based on what the
system learnt and what the designer understands
from the data.
</bodyText>
<subsectionHeader confidence="0.751571">
5.3 Future Work
</subsectionHeader>
<bodyText confidence="0.9997366">
On a social studies side, we are interested in
collaborations to test advanced dialogue strategies
and/or information presentation via generation. In-
deed, we consider our system as a good opportu-
nity for large scope experiments.
</bodyText>
<page confidence="0.99813">
191
</page>
<sectionHeader confidence="0.999076" genericHeader="acknowledgments">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998411">
This research has received funding from the
European Community’s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agreement
number 216594 (CLASSIC project: www.classic-
project.org).
</bodyText>
<sectionHeader confidence="0.999084" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999782517857143">
A. Abella, J.H. Wright, and A.L. Gorin. 2004. Dia-
log trajectory analysis. In IEEE International Con-
ference on Acoustics, Speech and Signal Processing
(ICASSP), volume 1, pages 441–444, May.
R.E. Bellman. 1957. A markovian decision process.
Journal ofMathematics and Mechanics, 6 :679–684.
J. Bos, E. Klein, O. Lemon, and T. Oka. 2003. Dipper:
Description and formalisation of an information-
state update dialogue system architecture.
George Ferguson and James F. Allen. 1998. Trips: An
integrated intelligent problem-solving assistant. In
In Proc. 15th Nat. Conf. AI, pages 567–572. AAAI
Press.
R. Laroche, G. Putois, P. Bretier, and B. Bouchon-
Meunier. 2009. Hybridisation of expertise and
reinforcement learning in dialogue systems. In
Proceedings of Interspeech. Special Session : Ma-
chine Learning for Adaptivity in Spoken Dialogue,
Brighton (United Knigdom), September.
O. Lemon and O. Pietquin. 2007. Machine learn-
ing for spoken dialogue systems. In Proceedings
of the European Conference on Speech Commu-
nication and Technologies (Interspeech’07), pages
2685–2688, August.
M. F. McTear. 2004. Spoken Dialogue Technol-
ogy : Toward the Conversational User Interface.
Springer, August.
T. Paek and R. Pieraccini. 2008. Automating spoken
dialogue management design using machine learn-
ing : An industry perspective. Speech Communica-
tion, 50 :716–729.
T. Paek. 2007. Toward evaluation that leads to
best practices : Reconciling dialog evaluation in re-
search and industry. In Proceedings of the Work-
shop on Bridging the Gap : Academic and Indus-
trial Research in Dialog Technologies, pages 40–
47, Rochester, NY, April. Association for Compu-
tational Linguistics.
R. Pieraccini and J. Huerta. 2005. Where do we go
from here? research and commercial spoken dialog
systems. In Laila Dybkjaer and Wolfgang Minker,
editors, Proceedings of the 6th SIGdial Workshop on
Discourse and Dialogue, pages 1–10.
R. Pieraccini, S. Caskey, K. Dayanidhi, B. Carpenter,
and M. Phillips. 2001. Etude, a recursive dialog
manager with embedded user interface patterns. In
Automatic Speech Recognition and Understanding,
2001 IEEE Workshop on, pages 244–247.
M. D. Sadek, P. Bretier, and F. Panaget. 1997. Ar-
timis : Natural dialogue meets rational agency. In
in Proceedings of IJCAI-97, pages 1030–1035. Mor-
gan Kaufmann.
J. D. Williams. 2008. The best of both worlds : Uni-
fying conventional dialog systems and POMDPs. In
International Conference on Speech and Language
Processing.
</reference>
<page confidence="0.998119">
192
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.216062">
<title confidence="0.993237">Enhanced Monitoring Tools and Online Dialogue Optimisation into a New Spoken Dialogue System Design Experience</title>
<author confidence="0.959848">Ghislain Putois Romain Laroche Philippe Bretier</author>
<affiliation confidence="0.9683685">Orange Labs Orange Labs Orange Labs Lannion, France Issy-les-Moulineaux, France Lannion,</affiliation>
<email confidence="0.999915">firstname.surname@orange-ftgroup.com</email>
<abstract confidence="0.999450076923077">Building an industrial spoken dialogue system (SDS) requires several iterations of design, deployment, test, and evaluation phases. Most industrial SDS developers use a graphical tool to design dialogue strategies. They are critical to get good system performances, but their evaluation is not part of the design phase. We propose integrating dialogue logs into the design tool so that developers can jointly monitor call flows and their associated Key Performance Indicators (KPI). It drastically shortens the complete development cycle, and offers a new design experience. Orange Dialogue Design Studio (ODDS), our design tool, allows developers to design several alternatives and compare their relative performances. It helps the SDS developers to understand and analyse the user behaviour, with the assistance of a reinforcement learning algorithm. The SDS developers can thus confront the different KPI and control the further SDS choices by updating the call flow alternatives.</abstract>
<affiliation confidence="0.589056">Dialogue Design, Online Learning,</affiliation>
<intro confidence="0.415668">Spoken Dialogue Systems, Monitoring Tools</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abella</author>
<author>J H Wright</author>
<author>A L Gorin</author>
</authors>
<title>Dialog trajectory analysis.</title>
<date>2004</date>
<booktitle>In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),</booktitle>
<volume>1</volume>
<pages>441--444</pages>
<contexts>
<context position="10002" citStr="Abella et al., 2004" startWordPosition="1553" endWordPosition="1556">esign Tool While researchers are focused on measuring the progress they incrementally reach, industry engineers have to deal with SDS tuning and upgrade. Their first dialogue evaluation KPI is task completion also called the automation rate because a SDS is deployed to automate specifically selected tasks. Most of the time, task completion is estimated thanks to the KPI. The KPI are difficult to exhaustively list and classify. Some are related to system measures, others are obtained thanks to dialogue annotations and the last ones are collected from users through questionnaires. Some studies (Abella et al., 2004) investigated graphical monitoring tools. The corpus to visualise is a set of dialogue logs. The tool aims at revealing how the system transits between its possible states. As a dialogue system is too complex to enumerate all its possible states, the dialogue logs are regarded as a set of variables that evolve during time and the tool proposes to make a projection on a subset of these variables. This way, the generated graphs can either display the call flow, how the different steps are reached and where they lead, or display how different variables, as the number of errors evolve. This is mai</context>
</contexts>
<marker>Abella, Wright, Gorin, 2004</marker>
<rawString>A. Abella, J.H. Wright, and A.L. Gorin. 2004. Dialog trajectory analysis. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), volume 1, pages 441–444, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Bellman</author>
</authors>
<title>A markovian decision process.</title>
<date>1957</date>
<journal>Journal ofMathematics and Mechanics,</journal>
<volume>6</volume>
<pages>679--684</pages>
<contexts>
<context position="4966" citStr="Bellman, 1957" startWordPosition="738" endWordPosition="739">ieraccini et al., 2001), planning problem solving (Ferguson and Allen, 1998). Only a very small number of these concepts have been transferred to industry. Since the late 90’s, the research has tackled the ambitious problem of automating the dialogue design (Lemon and Pietquin, 2007), aiming at both reducing the development cost and optimising the dialogue efficiency and robustness. Recently, criticisms (Paek and Pieraccini, 2008) have been formulated and novel approaches (Williams, 2008) have been proposed, both aiming at bridging the gap between research –focused on Markov-Decision-Process (Bellman, 1957) based dialogue management– and industry –focused on dialogue design process, model, and tools. This paper contributes to extend this effort. It addresses all these convergence questions together as a way for research and industry to reach a technological breakthrough. Regarding the dialogue evaluation topic, Paek (Paek, 2007) has pointed out that while research has exerted attention about “how best to evaluate a dialogue system?”, the industry has focused on “how best to design dialogue systems ?”. This paper unifies those two approaches by merging system and design evaluation in a single gra</context>
</contexts>
<marker>Bellman, 1957</marker>
<rawString>R.E. Bellman. 1957. A markovian decision process. Journal ofMathematics and Mechanics, 6 :679–684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Bos</author>
<author>E Klein</author>
<author>O Lemon</author>
<author>T Oka</author>
</authors>
<title>Dipper: Description and formalisation of an informationstate update dialogue system architecture.</title>
<date>2003</date>
<contexts>
<context position="4330" citStr="Bos et al., 2003" startWordPosition="644" endWordPosition="647">ceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 185–192, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 185 dialogue industry to reach a balance between usability and cost (Paek, 2007). This paper argues that tools are facilitators that improve both the usability vs. cost trade-off and the reliability of new technologies. Spoken dialogue research has developed various models and abstractions for dialogue management : rational agency (Sadek et al., 1997), Information State Update (Bos et al., 2003), functional models (Pieraccini et al., 2001), planning problem solving (Ferguson and Allen, 1998). Only a very small number of these concepts have been transferred to industry. Since the late 90’s, the research has tackled the ambitious problem of automating the dialogue design (Lemon and Pietquin, 2007), aiming at both reducing the development cost and optimising the dialogue efficiency and robustness. Recently, criticisms (Paek and Pieraccini, 2008) have been formulated and novel approaches (Williams, 2008) have been proposed, both aiming at bridging the gap between research –focused on Mar</context>
</contexts>
<marker>Bos, Klein, Lemon, Oka, 2003</marker>
<rawString>J. Bos, E. Klein, O. Lemon, and T. Oka. 2003. Dipper: Description and formalisation of an informationstate update dialogue system architecture.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Ferguson</author>
<author>James F Allen</author>
</authors>
<title>Trips: An integrated intelligent problem-solving assistant. In</title>
<date>1998</date>
<booktitle>In Proc. 15th Nat. Conf. AI,</booktitle>
<pages>567--572</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="4428" citStr="Ferguson and Allen, 1998" startWordPosition="657" endWordPosition="660">se and Dialogue, pages 185–192, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 185 dialogue industry to reach a balance between usability and cost (Paek, 2007). This paper argues that tools are facilitators that improve both the usability vs. cost trade-off and the reliability of new technologies. Spoken dialogue research has developed various models and abstractions for dialogue management : rational agency (Sadek et al., 1997), Information State Update (Bos et al., 2003), functional models (Pieraccini et al., 2001), planning problem solving (Ferguson and Allen, 1998). Only a very small number of these concepts have been transferred to industry. Since the late 90’s, the research has tackled the ambitious problem of automating the dialogue design (Lemon and Pietquin, 2007), aiming at both reducing the development cost and optimising the dialogue efficiency and robustness. Recently, criticisms (Paek and Pieraccini, 2008) have been formulated and novel approaches (Williams, 2008) have been proposed, both aiming at bridging the gap between research –focused on Markov-Decision-Process (Bellman, 1957) based dialogue management– and industry –focused on dialogue </context>
</contexts>
<marker>Ferguson, Allen, 1998</marker>
<rawString>George Ferguson and James F. Allen. 1998. Trips: An integrated intelligent problem-solving assistant. In In Proc. 15th Nat. Conf. AI, pages 567–572. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Laroche</author>
<author>G Putois</author>
<author>P Bretier</author>
<author>B BouchonMeunier</author>
</authors>
<title>Hybridisation of expertise and reinforcement learning in dialogue systems.</title>
<date>2009</date>
<booktitle>In Proceedings of Interspeech. Special Session : Machine Learning for Adaptivity in Spoken Dialogue,</booktitle>
<location>Brighton (United Knigdom),</location>
<contexts>
<context position="18296" citStr="Laroche et al., 2009" startWordPosition="2931" endWordPosition="2934">alisation experience. The further step to this approach is to automate at least a part of those analyses and improvements with learning capabilities. 4.1 Constraints The objective is to automatically choose online the best alternative among those proposed in the design tool, and to report this choice to the SDS developers via the monitoring functionalities that are integrated to the design tool. This approach differs from the classical reinforcement learning methods used in the dialogue literature, which make their decisions at the dialogue turn level. We use a technique from a previous work (Laroche et al., 2009). It does not need to declare the reachable states : they are automatically created when reached. This is also a parameterfree algorithm, which is very important when we consider that most dialogue application developers are not familiar with reinforcement learning theory. We keep the developer focussed on its main task. The two additional tasks required for the reinforcement learning are to define the variable set on which the alternative choice should depend, and to implement a reward function based on the expected evaluation of the task completion, in order to get a fully automated optimisa</context>
</contexts>
<marker>Laroche, Putois, Bretier, BouchonMeunier, 2009</marker>
<rawString>R. Laroche, G. Putois, P. Bretier, and B. BouchonMeunier. 2009. Hybridisation of expertise and reinforcement learning in dialogue systems. In Proceedings of Interspeech. Special Session : Machine Learning for Adaptivity in Spoken Dialogue, Brighton (United Knigdom), September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Lemon</author>
<author>O Pietquin</author>
</authors>
<title>Machine learning for spoken dialogue systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the European Conference on Speech Communication and Technologies (Interspeech’07),</booktitle>
<pages>2685--2688</pages>
<contexts>
<context position="4636" citStr="Lemon and Pietquin, 2007" startWordPosition="691" endWordPosition="694">). This paper argues that tools are facilitators that improve both the usability vs. cost trade-off and the reliability of new technologies. Spoken dialogue research has developed various models and abstractions for dialogue management : rational agency (Sadek et al., 1997), Information State Update (Bos et al., 2003), functional models (Pieraccini et al., 2001), planning problem solving (Ferguson and Allen, 1998). Only a very small number of these concepts have been transferred to industry. Since the late 90’s, the research has tackled the ambitious problem of automating the dialogue design (Lemon and Pietquin, 2007), aiming at both reducing the development cost and optimising the dialogue efficiency and robustness. Recently, criticisms (Paek and Pieraccini, 2008) have been formulated and novel approaches (Williams, 2008) have been proposed, both aiming at bridging the gap between research –focused on Markov-Decision-Process (Bellman, 1957) based dialogue management– and industry –focused on dialogue design process, model, and tools. This paper contributes to extend this effort. It addresses all these convergence questions together as a way for research and industry to reach a technological breakthrough. </context>
</contexts>
<marker>Lemon, Pietquin, 2007</marker>
<rawString>O. Lemon and O. Pietquin. 2007. Machine learning for spoken dialogue systems. In Proceedings of the European Conference on Speech Communication and Technologies (Interspeech’07), pages 2685–2688, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M F McTear</author>
</authors>
<title>Spoken Dialogue Technology : Toward the Conversational User Interface.</title>
<date>2004</date>
<publisher>Springer,</publisher>
<contexts>
<context position="2937" citStr="McTear, 2004" startWordPosition="433" endWordPosition="434">s. It is eventually concluded with an attempt to reassessing the dialogue evaluation. The fourth Section describes the learning integration to the tool, the constraints we impose to the learning technique and the synergy between the tools and the embedded learning capabilities. Finally, the last Section concludes the paper. 2 Context The spoken dialogue industry is structured around the architecture of the well known industrial standard VoiceXML 1. The underlying dialogue model of VoiceXML is a mapping of the simplistic turn-based linguistic model on the browser-server based Web architecture (McTear, 2004). The browser controls the speech engines (recognition and text-to-speech) integrated into the voice platform according to the VoiceXML document served by an application server. A VoiceXML document contains a set of prompts to play and the list of the possible interactions the user is supposed to have at each point of the dialogue. The SDS developers 2, reusing Web standards and technologies (e.g. J2EE, JSP, XML...), are used to designing directed dialogues modelled by finite state automata. Such controlled and deterministic development process allows the spoken 1. http ://www.w3c.org/TR/voice</context>
</contexts>
<marker>McTear, 2004</marker>
<rawString>M. F. McTear. 2004. Spoken Dialogue Technology : Toward the Conversational User Interface. Springer, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
<author>R Pieraccini</author>
</authors>
<title>Automating spoken dialogue management design using machine learning : An industry perspective.</title>
<date>2008</date>
<journal>Speech Communication,</journal>
<volume>50</volume>
<pages>716--729</pages>
<contexts>
<context position="4786" citStr="Paek and Pieraccini, 2008" startWordPosition="712" endWordPosition="715">ialogue research has developed various models and abstractions for dialogue management : rational agency (Sadek et al., 1997), Information State Update (Bos et al., 2003), functional models (Pieraccini et al., 2001), planning problem solving (Ferguson and Allen, 1998). Only a very small number of these concepts have been transferred to industry. Since the late 90’s, the research has tackled the ambitious problem of automating the dialogue design (Lemon and Pietquin, 2007), aiming at both reducing the development cost and optimising the dialogue efficiency and robustness. Recently, criticisms (Paek and Pieraccini, 2008) have been formulated and novel approaches (Williams, 2008) have been proposed, both aiming at bridging the gap between research –focused on Markov-Decision-Process (Bellman, 1957) based dialogue management– and industry –focused on dialogue design process, model, and tools. This paper contributes to extend this effort. It addresses all these convergence questions together as a way for research and industry to reach a technological breakthrough. Regarding the dialogue evaluation topic, Paek (Paek, 2007) has pointed out that while research has exerted attention about “how best to evaluate a dia</context>
<context position="21728" citStr="Paek and Pieraccini, 2008" startWordPosition="3486" endWordPosition="3489">models. Our dialogue design tool offers various extensions : dialogue modules, hierarchical design, arbitrary function invocation at any point of the design, conditional statements to split the flow in different paths. All those extensions allow designing any topology of the finite state machine required to handle complex dialogue models like mixed-initiative interaction. Dialogue model is not the point where research and industry fail to converge. The divergence point concerns the control aspect of VUI-completeness versus the automation of the dialogue design. As pointed out by recent works (Paek and Pieraccini, 2008), MDP-based dialogue management aiming at automating the whole dialogue design is rejected by the SDS developers. Even more adaptive, it is seen as an uncontrollable black box sensitive to the tuning process. The SDS developers do not rely on systems that dynamically build their dialogue logic without a sufficient degree of monitoring and control. Williams (Williams, 2008) has made a substantial effort to meet this industrial requirement. His system is a hybridisation of a conventional dialogue system following an industrial process, with a POMDP decision module, which is a MDPbased approach t</context>
</contexts>
<marker>Paek, Pieraccini, 2008</marker>
<rawString>T. Paek and R. Pieraccini. 2008. Automating spoken dialogue management design using machine learning : An industry perspective. Speech Communication, 50 :716–729.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
</authors>
<title>Toward evaluation that leads to best practices : Reconciling dialog evaluation in research and industry.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Bridging the Gap : Academic and Industrial Research in Dialog Technologies,</booktitle>
<pages>40--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, NY,</location>
<contexts>
<context position="4012" citStr="Paek, 2007" startWordPosition="595" endWordPosition="596">delled by finite state automata. Such controlled and deterministic development process allows the spoken 1. http ://www.w3c.org/TR/voicexml20/ 2. In this paper, the term “SDS developers” denotes without any distinction VUI designers, application developers, and any industry engineers acting in SDS building. Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 185–192, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 185 dialogue industry to reach a balance between usability and cost (Paek, 2007). This paper argues that tools are facilitators that improve both the usability vs. cost trade-off and the reliability of new technologies. Spoken dialogue research has developed various models and abstractions for dialogue management : rational agency (Sadek et al., 1997), Information State Update (Bos et al., 2003), functional models (Pieraccini et al., 2001), planning problem solving (Ferguson and Allen, 1998). Only a very small number of these concepts have been transferred to industry. Since the late 90’s, the research has tackled the ambitious problem of automating the dialogue design (L</context>
<context position="5294" citStr="Paek, 2007" startWordPosition="785" endWordPosition="786">and optimising the dialogue efficiency and robustness. Recently, criticisms (Paek and Pieraccini, 2008) have been formulated and novel approaches (Williams, 2008) have been proposed, both aiming at bridging the gap between research –focused on Markov-Decision-Process (Bellman, 1957) based dialogue management– and industry –focused on dialogue design process, model, and tools. This paper contributes to extend this effort. It addresses all these convergence questions together as a way for research and industry to reach a technological breakthrough. Regarding the dialogue evaluation topic, Paek (Paek, 2007) has pointed out that while research has exerted attention about “how best to evaluate a dialogue system?”, the industry has focused on “how best to design dialogue systems ?”. This paper unifies those two approaches by merging system and design evaluation in a single graphical tool. To our knowledge, ODDS is the only industrial tool which handles the complete system lifecycle, from design to evaluation. The tools and methods presented below have been tested and validated during the design and implementation of a large real-world commercial system: the 1013+ service is the Spoken Dialogue Syst</context>
</contexts>
<marker>Paek, 2007</marker>
<rawString>T. Paek. 2007. Toward evaluation that leads to best practices : Reconciling dialog evaluation in research and industry. In Proceedings of the Workshop on Bridging the Gap : Academic and Industrial Research in Dialog Technologies, pages 40– 47, Rochester, NY, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Pieraccini</author>
<author>J Huerta</author>
</authors>
<title>Where do we go from here? research and commercial spoken dialog systems.</title>
<date>2005</date>
<booktitle>In Laila Dybkjaer and Wolfgang Minker, editors, Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="1547" citStr="Pieraccini and Huerta, 2005" startWordPosition="217" endWordPosition="220">, our design tool, allows developers to design several alternatives and compare their relative performances. It helps the SDS developers to understand and analyse the user behaviour, with the assistance of a reinforcement learning algorithm. The SDS developers can thus confront the different KPI and control the further SDS choices by updating the call flow alternatives. Index Terms: Dialogue Design, Online Learning, Spoken Dialogue Systems, Monitoring Tools 1 Introduction Recent research in spoken dialogue systems (SDS) has called for a “synergistic convergence” between research and industry (Pieraccini and Huerta, 2005). This call for convergence concerns architectures, abstractions and methods from both communities. Under this motivation, several research orientations have been proposed. This paper discusses three of them : dialogue design, dialogue management, and dialogue evaluation. Dialogue design and dialogue management reflect in this paper the respective paths that industry and research have followed for building their SDS. Dialogue evaluation is a concern for both communities, but remains hard to put into operational perspectives. The second Section presents the context and related research. The thi</context>
<context position="6548" citStr="Pieraccini and Huerta, 2005" startWordPosition="994" endWordPosition="997">oting for France. It receives millions of calls a year and schedules around 8, 000 appointments a week. When the user calls the system, she is presented with an open question asking her for the reason of her call. If her landline is out of service, the Spoken Dialogue System then performs some automated tests on the line, and if the problem is confirmed, try and schedule an appointment with the user for a manual intervention. If the system and the user cannot agree on an appointment slot, the call is transferred to a human operator. 3 The tools Industry follows the VUI-completeness principle (Pieraccini and Huerta, 2005) : “the behaviour of an application needs to be completely specified with respect to every possible situation that may arise during the interaction. No unpredictable user input should ever lead to unforeseeable behaviour”. The SDS developers consider reliable the technologies, tools, and methodologies that help them to reach the VUI-completeness and to control it. 3.1 The Dialogue Design Tool The graphical abstraction proposed by our dialogue design tool conforms to the general graph representation of finite state automata, with the difference that global and local variables enable to factoris</context>
<context position="20901" citStr="Pieraccini and Huerta, 2005" startWordPosition="3361" endWordPosition="3364"> as a system or as a service. They can also have access to additional evaluations through annotations, or user subjective evaluations. These functionality differences make their respective roles complementary. The SDS develop189 Score 0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 day FIGURE 3 – Evolution of the system score 24 23 22 21 20 19 3-day means daily score ers have the responsibility for the whole application and the macro-strategic changes while the learning manager holds the real-time optimisation. 4.3 Control vs. Automation : the Trusting Threshold As argued by Pieraccini and Huerta (Pieraccini and Huerta, 2005), finite state machine applied to dialogue management does not restrict the dialogue model to strictly directed dialogues. Finite state machines are easily extensible to powerful and flexible dialogue models. Our dialogue design tool offers various extensions : dialogue modules, hierarchical design, arbitrary function invocation at any point of the design, conditional statements to split the flow in different paths. All those extensions allow designing any topology of the finite state machine required to handle complex dialogue models like mixed-initiative interaction. Dialogue model is not th</context>
</contexts>
<marker>Pieraccini, Huerta, 2005</marker>
<rawString>R. Pieraccini and J. Huerta. 2005. Where do we go from here? research and commercial spoken dialog systems. In Laila Dybkjaer and Wolfgang Minker, editors, Proceedings of the 6th SIGdial Workshop on Discourse and Dialogue, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Pieraccini</author>
<author>S Caskey</author>
<author>K Dayanidhi</author>
<author>B Carpenter</author>
<author>M Phillips</author>
</authors>
<title>Etude, a recursive dialog manager with embedded user interface patterns. In Automatic Speech Recognition and Understanding,</title>
<date>2001</date>
<booktitle>IEEE Workshop on,</booktitle>
<pages>244--247</pages>
<contexts>
<context position="4375" citStr="Pieraccini et al., 2001" startWordPosition="650" endWordPosition="653">ual Meeting of the Special Interest Group on Discourse and Dialogue, pages 185–192, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 185 dialogue industry to reach a balance between usability and cost (Paek, 2007). This paper argues that tools are facilitators that improve both the usability vs. cost trade-off and the reliability of new technologies. Spoken dialogue research has developed various models and abstractions for dialogue management : rational agency (Sadek et al., 1997), Information State Update (Bos et al., 2003), functional models (Pieraccini et al., 2001), planning problem solving (Ferguson and Allen, 1998). Only a very small number of these concepts have been transferred to industry. Since the late 90’s, the research has tackled the ambitious problem of automating the dialogue design (Lemon and Pietquin, 2007), aiming at both reducing the development cost and optimising the dialogue efficiency and robustness. Recently, criticisms (Paek and Pieraccini, 2008) have been formulated and novel approaches (Williams, 2008) have been proposed, both aiming at bridging the gap between research –focused on Markov-Decision-Process (Bellman, 1957) based di</context>
</contexts>
<marker>Pieraccini, Caskey, Dayanidhi, Carpenter, Phillips, 2001</marker>
<rawString>R. Pieraccini, S. Caskey, K. Dayanidhi, B. Carpenter, and M. Phillips. 2001. Etude, a recursive dialog manager with embedded user interface patterns. In Automatic Speech Recognition and Understanding, 2001 IEEE Workshop on, pages 244–247.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D Sadek</author>
<author>P Bretier</author>
<author>F Panaget</author>
</authors>
<title>Artimis : Natural dialogue meets rational agency.</title>
<date>1997</date>
<booktitle>In in Proceedings of IJCAI-97,</booktitle>
<pages>1030--1035</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="4285" citStr="Sadek et al., 1997" startWordPosition="636" endWordPosition="639"> industry engineers acting in SDS building. Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 185–192, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 185 dialogue industry to reach a balance between usability and cost (Paek, 2007). This paper argues that tools are facilitators that improve both the usability vs. cost trade-off and the reliability of new technologies. Spoken dialogue research has developed various models and abstractions for dialogue management : rational agency (Sadek et al., 1997), Information State Update (Bos et al., 2003), functional models (Pieraccini et al., 2001), planning problem solving (Ferguson and Allen, 1998). Only a very small number of these concepts have been transferred to industry. Since the late 90’s, the research has tackled the ambitious problem of automating the dialogue design (Lemon and Pietquin, 2007), aiming at both reducing the development cost and optimising the dialogue efficiency and robustness. Recently, criticisms (Paek and Pieraccini, 2008) have been formulated and novel approaches (Williams, 2008) have been proposed, both aiming at brid</context>
</contexts>
<marker>Sadek, Bretier, Panaget, 1997</marker>
<rawString>M. D. Sadek, P. Bretier, and F. Panaget. 1997. Artimis : Natural dialogue meets rational agency. In in Proceedings of IJCAI-97, pages 1030–1035. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Williams</author>
</authors>
<title>The best of both worlds : Unifying conventional dialog systems and POMDPs.</title>
<date>2008</date>
<booktitle>In International Conference on Speech and Language Processing.</booktitle>
<contexts>
<context position="4845" citStr="Williams, 2008" startWordPosition="722" endWordPosition="723">logue management : rational agency (Sadek et al., 1997), Information State Update (Bos et al., 2003), functional models (Pieraccini et al., 2001), planning problem solving (Ferguson and Allen, 1998). Only a very small number of these concepts have been transferred to industry. Since the late 90’s, the research has tackled the ambitious problem of automating the dialogue design (Lemon and Pietquin, 2007), aiming at both reducing the development cost and optimising the dialogue efficiency and robustness. Recently, criticisms (Paek and Pieraccini, 2008) have been formulated and novel approaches (Williams, 2008) have been proposed, both aiming at bridging the gap between research –focused on Markov-Decision-Process (Bellman, 1957) based dialogue management– and industry –focused on dialogue design process, model, and tools. This paper contributes to extend this effort. It addresses all these convergence questions together as a way for research and industry to reach a technological breakthrough. Regarding the dialogue evaluation topic, Paek (Paek, 2007) has pointed out that while research has exerted attention about “how best to evaluate a dialogue system?”, the industry has focused on “how best to de</context>
<context position="22103" citStr="Williams, 2008" startWordPosition="3548" endWordPosition="3549">del is not the point where research and industry fail to converge. The divergence point concerns the control aspect of VUI-completeness versus the automation of the dialogue design. As pointed out by recent works (Paek and Pieraccini, 2008), MDP-based dialogue management aiming at automating the whole dialogue design is rejected by the SDS developers. Even more adaptive, it is seen as an uncontrollable black box sensitive to the tuning process. The SDS developers do not rely on systems that dynamically build their dialogue logic without a sufficient degree of monitoring and control. Williams (Williams, 2008) has made a substantial effort to meet this industrial requirement. His system is a hybridisation of a conventional dialogue system following an industrial process, with a POMDP decision module, which is a MDPbased approach to dialogue management enhanced with dialogue state abstractions to model uncertainties. The responsibilities of each part of the system are shared as follows : the conventional system elects several candidate dialogue moves and the POMDP decision module selects the most competitive one. This is a great step towards industry because the dialogue move chosen by the POMDP mod</context>
</contexts>
<marker>Williams, 2008</marker>
<rawString>J. D. Williams. 2008. The best of both worlds : Unifying conventional dialog systems and POMDPs. In International Conference on Speech and Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>