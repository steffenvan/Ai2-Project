<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006157">
<title confidence="0.995202">
Loss Minimization in Parse Reranking
</title>
<author confidence="0.997723">
Ivan Titov
</author>
<affiliation confidence="0.9990795">
Department of Computer Science
University of Geneva
</affiliation>
<address confidence="0.93662">
24, rue G´en´eral Dufour
CH-1211 Gen`eve 4, Switzerland
</address>
<email confidence="0.995738">
ivan.titov@cui.unige.ch
</email>
<author confidence="0.981918">
James Henderson
</author>
<affiliation confidence="0.9987765">
School of Informatics
University of Edinburgh
</affiliation>
<address confidence="0.7575515">
2 Buccleuch Place
Edinburgh EH8 9LW, United Kingdom
</address>
<email confidence="0.998154">
james.henderson@ed.ac.uk
</email>
<sectionHeader confidence="0.997378" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999967045454545">
We propose a general method for reranker
construction which targets choosing the
candidate with the least expected loss,
rather than the most probable candidate.
Different approaches to expected loss ap-
proximation are considered, including es-
timating from the probabilistic model used
to generate the candidates, estimating
from a discriminative model trained to
rerank the candidates, and learning to ap-
proximate the expected loss. The pro-
posed methods are applied to the parse
reranking task, with various baseline mod-
els, achieving significant improvement
both over the probabilistic models and the
discriminative rerankers. When a neural
network parser is used as the probabilistic
model and the Voted Perceptron algorithm
with data-defined kernels as the learning
algorithm, the loss minimization model
achieves 90.0% labeled constituents F1
score on the standard WSJ parsing task.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965509803922">
The reranking approach is widely used in pars-
ing (Collins and Koo, 2005; Koo and Collins,
2005; Henderson and Titov, 2005; Shen and Joshi,
2003) as well as in other structured classifica-
tion problems. For structured classification tasks,
where labels are complex and have an internal
structure of interdependency, the 0-1 loss consid-
ered in classical formulation of classification al-
gorithms is not a natural choice and different loss
functions are normally employed. To tackle this
problem, several approaches have been proposed
to accommodate loss functions in learning algo-
rithms (Tsochantaridis et al., 2004; Taskar et al.,
2004; Henderson and Titov, 2005). A very differ-
ent use of loss functions was considered in the ar-
eas of signal processing and machine translation,
where direct minimization of expected loss (Min-
imum Bayes Risk decoding) on word sequences
was considered (Kumar and Byrne, 2004; Stol-
cke et al., 1997). The only attempt to use Mini-
mum Bayes Risk (MBR) decoding in parsing was
made in (Goodman, 1996), where a parsing al-
gorithm for constituent recall minimization was
constructed. However, their approach is limited
to binarized PCFG models and, consequently, is
not applicable to state-of-the-art parsing meth-
ods (Charniak and Johnson, 2005; Henderson,
2004; Collins, 2000). In this paper we consider
several approaches to loss approximation on the
basis of a candidate list provided by a baseline
probabilistic model.
The intuitive motivation for expected loss mini-
mization can be seen from the following example.
Consider the situation where there are a group of
several very similar candidates and one very dif-
ferent candidate whose probability is just slightly
larger than the probability of any individual candi-
date in the group, but much smaller than their total
probability. A method which chooses the maxi-
mum probability candidate will choose this outlier
candidate, which is correct if you are only inter-
ested in getting the label exactly correct (i.e. 0-1
loss), and you think the estimates are accurate. But
if you are interested in a loss function where the
loss is small when you choose a candidate which
is similar to the correct candidate, then it is better
to choose one of the candidates in the group. With
this choice the loss will only be large if the outlier
turns out to be correct, while if the outlier is cho-
sen then the loss will be large if any of the group
are correct. In other words, the expected loss of
</bodyText>
<page confidence="0.94765">
560
</page>
<note confidence="0.852211">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 560–567,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.87619025">
choosing a member of the group will be smaller
than that for the outlier.
More formally, the Bayes risk of a model y =
h(x) is defined as
</bodyText>
<equation confidence="0.998602">
R(h) = Ex,yA(y, h(x)), (1)
</equation>
<bodyText confidence="0.9982775">
where the expectation is taken over all the possi-
ble inputs x and labels y and A(y, y0) denotes a
loss incurred by assigning x to y0 when the correct
label is y. We assume that the loss function pos-
sesses values within the range from 0 to 1, which
is equivalent to the requirement that the loss func-
tion is bounded in (Tsochantaridis et al., 2004). It
follows that an optimal reranker h* is one which
chooses the label y that minimizes the expected
loss:
</bodyText>
<equation confidence="0.919115">
�
h*(x) = arg min P(y|x)A(y, y0), (2)
y&apos;∈G(x) y
</equation>
<bodyText confidence="0.9999742">
where G(x) denotes a candidate list provided by
a baseline probabilistic model for the input x.
In this paper we propose different approaches to
loss approximation. We apply them to the parse
reranking problem where the baseline probabilis-
tic model is a neural network parser (Henderson,
2003), and to parse reranking of candidates pro-
vided by the (Collins, 1999) model. The result-
ing reranking method achieves very significant im-
provement in the considered loss function and im-
provement in most other standard measures of ac-
curacy.
In the following three sections we will discuss
three approaches to learning such a classifier. The
first two derive a classification criteria for use with
a predefined probability model (the first genera-
tive, the second discriminative). The third de-
fines a kernel for use with a classification method
for minimizing loss. All use previously proposed
learning algorithms and optimization criteria.
</bodyText>
<sectionHeader confidence="0.908324" genericHeader="method">
2 Loss Approximation with a
</sectionHeader>
<subsectionHeader confidence="0.892472">
Probabilistic Model
</subsectionHeader>
<bodyText confidence="0.999087705882353">
In this section we discuss approximating the ex-
pected loss using probability estimates given by
a baseline probabilistic model. Use of probabil-
ity estimates is not a serious limitation of this
approach because in practice candidates are nor-
mally provided by some probabilistic model and
its probability estimates are used as additional fea-
tures in the reranker (Collins and Koo, 2005; Shen
and Joshi, 2003; Henderson and Titov, 2005).
In order to estimate the expected loss on the ba-
sis of a candidate list, we make the assumption that
the total probability of the labels not in the can-
didate list is sufficiently small that the difference
δ(x, y0) of expected loss between the labels in the
candidate list and the labels not in the candidate
list does not have an impact on the loss defined
in (1):
</bodyText>
<equation confidence="0.999864">
Ey�∈G(x) P(y|x)A(y, y0)
δ(x, y0) = − (3)
Ey�G(x) P(y|x)
Ey∈G(x) P(y|x)A(y, y0)
Ey∈G(x) P(y|x)
</equation>
<bodyText confidence="0.842471">
This gives us the following approximation to the
expected loss for the label:
</bodyText>
<equation confidence="0.999480285714286">
E y∈G(x) P (y|x)A(y, y0)
l(x, y0) =
E
y∈G(x) P(y|x) .(4)
�
�h(x) = arg min P(x, y|B)A(y, y0), (5)
y&apos;∈G(x) y∈G(x)
</equation>
<bodyText confidence="0.999964833333333">
where θ� denotes the parameters of the probabilis-
tic model learned from the training data. This ap-
proach for expected loss approximation was con-
sidered in the context of word error rate minimiza-
tion in speech recognition, see for example (Stol-
cke et al., 1997).
</bodyText>
<sectionHeader confidence="0.931129" genericHeader="method">
3 Estimating Expected Loss with
Discriminative Classifiers
</sectionHeader>
<bodyText confidence="0.998935409090909">
In this section we propose a method to improve on
the loss approximation used in (5) by constructing
the probability estimates using a trained discrimi-
native classifier. Special emphasis is placed on lin-
ear classifiers with data-defined kernels for rerank-
ing (Henderson and Titov, 2005), because they do
not require any additional domain knowledge not
already encoded in the probabilistic model, and
they have demonstrated significant improvement
over the baseline probabilistic model for the parse
reranking task. This kernel construction can be
motivated by the existence of a function which
maps a linear function in the feature space of the
kernel to probability estimates which are superior
to the estimates of the ori
ginal probabilistic model.
For the reranking case, often the probabilistic
model only estimates the joint probability P(x, y).
However, neither this difference nor the denomi-
nator in (4) affects the classification. Thus, replac-
ing the true probabilities with their estimates, we
can define the classifier
</bodyText>
<page confidence="0.995243">
561
</page>
<subsectionHeader confidence="0.999527">
3.1 Estimation with Fisher Kernels
</subsectionHeader>
<bodyText confidence="0.999344875">
The Fisher kernel for structured classification
is a trivial generalization of one of the best
known data-defined kernels for binary classifica-
tion (Jaakkola and Haussler, 1998). The Fisher
score of an example input-label pair (x, y) is a
vector of partial derivatives of the log-likelihood
of the example with respect to the model parame-
tersl:
</bodyText>
<equation confidence="0.9986915">
φF K
θ� (x, y) = (6)
</equation>
<subsectionHeader confidence="0.9819445">
3.2 Estimation with TOP Kernels for
Reranking
</subsectionHeader>
<bodyText confidence="0.9999566">
The TOP Reranking kernel was defined in (Hen-
derson and Titov, 2005), as a generalization of the
TOP kernel (Tsuda et al., 2002) proposed for bi-
nary classification tasks. The feature extractor for
the TOP reranking kernel is given by:
</bodyText>
<equation confidence="0.996127909090909">
φT K
θ� (x, y) = (9)
e), ∂v(x, y, 0) ,...,
∂θ1
(v(x, y,
∂v(x, y, �θ)
),
∂θl
(logP(x, y |�θ), ∂logP (x,y|�θ) ∂logP(x,y|�θ) where P(x, y&apos;|�θ).
,..., ). X
∂θ1 ∂θl v(x, y, B) = log P(x, y|�θ) − log
</equation>
<bodyText confidence="0.9992885">
This kernel defines a feature space which is appro-
priate for estimating the discriminative probability
in the candidate list in the form of a normalized
exponential
</bodyText>
<equation confidence="0.9990325">
Py0EG(x) exp(w?TφF K
θ� (x, y&apos;))
</equation>
<bodyText confidence="0.999884333333333">
for some choice of the decision vector w = w?
with the first component equal to one.
It follows that it is natural to use an estimator
of the discriminative probability P(y|x) in expo-
nential form and, therefore, the appropriate form
of the loss minimizing classifier is the following:
</bodyText>
<equation confidence="0.9985368">
�hFK(x) = (8)
X
arg min exp(A wT φF K
θ� (x, y&apos;))A(y, y&apos;),
y0EG(x) yEG(x)
</equation>
<bodyText confidence="0.9800218">
where w� is learned during classifier training and
the scalar parameter A can be tuned on the devel-
opment set. From the construction of the Fisher
kernel, it follows that the optimal value A is ex-
pected to be close to inverse of the first component
of w, 1/ 1u1.
If an SVM is used to learn the classifier, then
the form (7) is the same as that proposed by (Platt,
1999), where it is proposed to use the logistic sig-
moid of the SVM output as the probability estima-
tor for binary classification problems.
&apos;The first component logP(x, y0) is not in the strict
sense part of the Fisher score, but usually added to kernel
features in practice (Henderson and Titov, 2005).
y0EG(x)−{y}
The TOP reranking kernel has been demon-
strated to perform better than the Fisher kernel
for the parse reranking task (Henderson and Titov,
2005). The construction of this kernel is moti-
vated by the minimization of the classification er-
ror of a linear classifier wT φB(x, y). This linear
classifier has been shown to converge, assuming
estimation of the discriminative probability in the
candidate list can be in the form of the logistic sig-
moid (Titov and Henderson, 2005):
</bodyText>
<equation confidence="0.99945125">
Py0EG(x) P x, y&apos;) ≈ (10)
1
1 + exp(−w?TφT K
θ� (x, y))
</equation>
<bodyText confidence="0.99908425">
for some choice of the decision vector w = w?
with the first component equal to one. From this
fact, the form of the loss minimizing classifier fol-
lows:
</bodyText>
<equation confidence="0.999081">
hTK(x) = (11)
X
arg min g(A wT φT K
θ� (x, y&apos;))A(y, y&apos;),
y0EG(x) yEG(x)
</equation>
<bodyText confidence="0.99998575">
where g is the logistic sigmoid and the scalar pa-
rameter A should be selected on the development
set. As for the Fisher kernel, the optimal value of
A should be close to 1/ w1.
</bodyText>
<subsectionHeader confidence="0.997048">
3.3 Estimates from Arbitrary Classifiers
</subsectionHeader>
<bodyText confidence="0.99974">
Although in this paper we focus on approaches
which do not require additional domain knowl-
edge, the output of most classifiers can be used
to estimate the discriminative probability in equa-
tion (7). As mentioned above, the form of (7)
</bodyText>
<equation confidence="0.999815714285714">
P(x, y)
Py0EG(x) P (x, y&apos;)
exp(w?TφF K
θ� (x, y))
≈
(7)
P(x, y)
</equation>
<page confidence="0.980283">
562
</page>
<bodyText confidence="0.999972545454546">
is appropriate for the SVM learning task with
arbitrary kernels, as follows from (Platt, 1999).
Also, for models which combine classifiers using
votes (e.g. the Voted Perceptron), the number of
votes cast for each candidate can be used to de-
fine this discriminative probability. The discrim-
inative probability of a candidate is simply the
number of votes cast for that candidate normalized
across candidates. Intuitively, we can think of this
method as treating the votes as a sample from the
discriminative distribution.
</bodyText>
<sectionHeader confidence="0.999326" genericHeader="method">
4 Expected Loss Learning
</sectionHeader>
<bodyText confidence="0.999973333333333">
In this section, another approach to loss approx-
imation is proposed. We consider learning a lin-
ear classifier to choose the least loss candidate,
and propose two constructions of data-defined loss
kernels which define different feature spaces for
the classification. In addition to the kernel, this
approach differs from the previous one in that the
classifier is assumed to be linear, rather than the
nonlinear functions in equations (8) and (11).
</bodyText>
<subsectionHeader confidence="0.994832">
4.1 Loss Kernel
</subsectionHeader>
<bodyText confidence="0.970189">
The Loss Kernel feature extractor is composed of
the logarithm of the loss estimated by the proba-
bilistic model and its first derivatives with respect
to each model parameter:
</bodyText>
<equation confidence="0.98889175">
φLK
� (x, y) = (12)
e), ∂v(x, y, 0) ,...,
∂θ1
</equation>
<bodyText confidence="0.821209">
where
</bodyText>
<equation confidence="0.998741">
v(x, y, �θ) = log( � P(y0,x|0)A(y0,y)).
y&apos;∈G(x)
</equation>
<bodyText confidence="0.999762375">
The motivation for this kernel is very similar to
that for the Fisher kernel for structured classifica-
tion. The feature space of the kernel guarantees
convergence of an estimator for the expected loss
if the estimator is in normalized exponential form.
The standard Fisher kernel for structured classifi-
cation is a special case of this Loss Kernel when
A(y, y0) is 0-1 loss.
</bodyText>
<subsectionHeader confidence="0.986432">
4.2 Loss Logit Kernel
</subsectionHeader>
<bodyText confidence="0.999817428571428">
As the Loss kernel was a generalization of the
Fisher kernel to arbitrary loss function, so the Loss
Logit Kernel is a generalization of the TOP kernel
for reranking. The construction of the Loss Logit
Kernel, like the TOP kernel for reranking, can be
motivated by the minimization of the classification
error of a linear classifier wT φLLK
</bodyText>
<equation confidence="0.9948125">
� (x, y), where
φLLK
� (x, y) is the feature extractor of the kernel
given by:
φLLK � (x,y) = (13)
(v(x, y,
</equation>
<bodyText confidence="0.995645">
where
</bodyText>
<sectionHeader confidence="0.999226" genericHeader="method">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.999994">
To perform empirical evaluations of the proposed
methods, we considered the task of parsing the
Penn Treebank Wall Street Journal corpus (Mar-
cus et al., 1993). First, we perform experiments
with SVM Struct (Tsochantaridis et al., 2004) as
the learner. Since SVM Struct already uses the
loss function during training to rescale the margin
or slack variables, this learner allows us to test the
hypothesis that loss functions are useful in pars-
ing not only to define the optimization criteria but
also to define the classifier and to define the feature
space. However, SVM Struct training for large
scale parsing experiments is computationally ex-
pensive2, so here we use only a small portion of
the available training data to perform evaluations
of the different approaches. In the other two sets
of experiments, described below, we test our best
model on the standard Wall Street Journal parsing
benchmark (Collins, 1999) with the Voted Percep-
tron algorithm as the learner.
</bodyText>
<subsectionHeader confidence="0.979396">
5.1 The Probabilistic Models of Parsing
</subsectionHeader>
<bodyText confidence="0.999850571428571">
To perform the experiments with data-defined ker-
nels, we need to select a probabilistic model of
parsing. Data-defined kernels can be applied to
any kind of parameterized probabilistic model.
For our first set of experiments, we choose
to use a publicly available neural network based
probabilistic model of parsing (Henderson, 2003).
</bodyText>
<footnote confidence="0.807481">
2In (Shen and Joshi, 2003) it was proposed to use an
ensemble of SVMs trained the Wall Street Journal corpus,
but the generalization performance of the resulting classifier
might be compromised in this approach.
</footnote>
<equation confidence="0.996330857142857">
(v(x, y,
∂v(x, y, �θ)
),
∂θl
eav(
) x, y, 0) ,...,
∂θ1
∂v(x, y, �θ)
),
∂θl
v(x, y, �θ) = log( � P(y0|x,�θ)(1−A(y0,y)))−
y&apos;∈G(x)
log( � P(y0|x, �θ)A(y0, y)).
y&apos;∈G(x)
</equation>
<page confidence="0.988191">
563
</page>
<bodyText confidence="0.999976212121212">
This parsing model is a good candidate for our ex-
periments because it achieves state-of-the-art re-
sults on the standard Wall Street Journal (WSJ)
parsing problem (Henderson, 2003), and data-
defined kernels derived from this parsing model
have recently been used with the Voted Percep-
tron algorithm on the WSJ parsing task, achiev-
ing a significant improvement in accuracy over the
neural network parser alone (Henderson and Titov,
2005). This gives us a baseline which is hard to
beat, and allows us to compare results of our new
approaches with the results of the original data-
defined kernels for reranking.
The probabilistic model of parsing in (Hender-
son, 2003) has two levels of parameterization. The
first level of parameterization is in terms of a
history-based generative probability model. These
parameters are estimated using a neural network,
the weights of which form the second level of pa-
rameterization. This approach allows the prob-
ability model to have an infinite number of pa-
rameters; the neural network only estimates the
bounded number of parameters which are relevant
to a given partial parse. We define data-defined
kernels in terms of the second level of parameteri-
zation (the network weights).
For the last set of experiments, we used the
probabilistic model described in (Collins, 1999)
(model 2), and the Tree Kernel (Collins and Duffy,
2002). However, in these experiments we only
used the estimates from the discriminative classi-
fier, so the details of the probabilistic model are
not relevant.
</bodyText>
<subsectionHeader confidence="0.998044">
5.2 Experiments with SVM Struct
</subsectionHeader>
<bodyText confidence="0.9999508125">
Both the neural network probabilistic model and
the kernel based classifiers were trained on sec-
tion 0 (1,921 sentences, 40,930 words). Section 24
(1,346 sentences, 29,125 words) was used as the
validation set during the neural network learning
and for choosing parameters of the models. Sec-
tion 23 (2,416 sentences, 54,268 words) was used
for the final testing of the models.
We used a publicly available tagger (Ratna-
parkhi, 1996) to provide the part-of-speech tags
for each word in the sentence. For each tag, there
is an unknown-word vocabulary item which is
used for all those words which are not sufficiently
frequent with that tag to be included individually
in the vocabulary. For these experiments, we only
included a specific tag-word pair in the vocabu-
</bodyText>
<table confidence="0.9996845">
R P F1 CM
SSN 80.9 81.7 81.3 18.3
TRK 81.1 82.4 81.7 18.2
SSN-Estim 81.4 82.3 81.8 18.3
LLK-Learn 81.2 82.4 81.8 17.6
LK-Learn 81.5 82.2 81.8 17.8
FK-Estim 81.4 82.6 82.0 18.3
TRK-Estim 81.5 82.8 82.1 18.6
</table>
<tableCaption confidence="0.8909955">
Table 1: Percentage labeled constituent recall (R),
precision (P), combination of both (F1) and per-
</tableCaption>
<bodyText confidence="0.990184891891892">
centage complete match (CM) on the testing set.
lary if it occurred at least 20 time in the training
set, which (with tag-unknown-word pairs) led to
the very small vocabulary of 271 tag-word pairs.
The same model was used both for choosing the
list of candidate parses and for the probabilistic
model used for loss estimation and kernel feature
extraction. For training and testing of the kernel
models, we provided a candidate list consisting of
the top 20 parses found by the probabilistic model.
For the testing set, selecting the candidate with an
oracle results in an F1 score of 89.1%.
We used the SVM Struct software pack-
age (Tsochantaridis et al., 2004) to train the SVM
for all the approaches based on discriminative
classifier learning, with slack rescaling and lin-
ear slack penalty. The loss function is defined as
A(y, y&apos;) = 1 − F1(y, y&apos;), where F1 denotes F1
measure on bracketed constituents. This loss was
used both for rescaling the slacks in the SVM and
for defining our classification models and kernels.
We performed initial testing of the models on
the validation set and preselected the best model
for each of the approaches before testing it on
the final testing set. Standard measures of pars-
ing accuracy, plus complete match accuracy, are
shown in table 1.3 As the baselines, the table in-
cludes the results of the standard TOP reranking
kernel (TRK) (Henderson and Titov, 2005) and
the baseline probabilistic model (SSN) (Hender-
son, 2003). SSN-Estim is the model using loss
estimation on the basic probabilistic model, as ex-
plained in section 2. LLK-Learn and LK-Learn are
the models which define the kernel based on loss,
using the Loss Logit Kernel (equation (13)) and
the Loss Kernel (equation (12)), respectively. FK-
Estim and TRK-Estim are the models which esti-
</bodyText>
<footnote confidence="0.9701915">
3All our results are computed with the evalb pro-
gram (Collins, 1999).
</footnote>
<page confidence="0.997774">
564
</page>
<bodyText confidence="0.999883">
mate the loss with data-defined kernels, using the
Fisher Kernel (equation (8)) and the TOP Rerank-
ing kernel (equation (11)), respectively.
All our proposed models show better F1 accu-
racy than the baseline probabilistic model SSN,
and all these differences are statistically signifi-
cant.4 The difference in F1 between TRK-Estim
and FK-Estim is not statistically significant, but
otherwise TRK-Estim demonstrates a statistically
significant improvement over all other models. It
should also be noted that exact match measures for
TRK-Estim and SSN-Estim are not negatively af-
fected, even though the F1 loss function was opti-
mized. It is important to point out that SSN-Estim,
which improves significantly over SSN, does not
require the learning of a discriminative classifier,
and differs from the SSN only by use of the dif-
ferent classification model (equation (5)), which
means that it is extremely easy to apply in prac-
tice.
One surprising aspect of these results is the fail-
ure of LLK-Learn and LK-Learn to achieve im-
provement over SSN-Estim. This might be ex-
plained by the difficulty of learning a linear ap-
proximation to (4). Under this explanation, the
performance of LLK-Learn and LK-Learn could
be explained by the fact that the first component of
their kernels is a monotonic function of the SSN-
Estim estimation. To test this hypothesis, we did
an additional experiment where we removed the
first component of Loss Logit Kernel (13) from
the feature vector and performed learning. Sur-
prisingly, the model achieved virtually the same
results, rather than the predicted worse perfor-
mance. This result might indicate that the LLK-
Learn model still can be useful for different prob-
lems where discriminative learning gives more ad-
vantage over generative approaches.
These experimental results demonstrate that
the loss approximation reranking approaches pro-
posed in this paper demonstrate significant im-
provement over the baseline models, achieving
about the same relative error reduction as previ-
ously achieved with data-defined kernels (Hender-
son and Titov, 2005). This improvement is despite
the fact that the loss function is already used in the
definition of the training criteria for all the mod-
els except SSN. It is also interesting to note that
the best result on the validation set for estimation
</bodyText>
<footnote confidence="0.974077">
4We measured significance of all the experiments in this
paper with the randomized significance test (Yeh, 2000).
</footnote>
<bodyText confidence="0.9997572">
of the loss with data-defined kernels (12) and (13)
was achieved when the parameter A is close to the
inverse of the first component of the learned de-
cision vector, which confirms the motivation for
these kernels.
</bodyText>
<subsectionHeader confidence="0.9984775">
5.3 Experiments with Voted Perceptron and
Data-Defined Kernels
</subsectionHeader>
<bodyText confidence="0.999953853658537">
The above experiments with the SVM Struct
demonstrate empirically the viability of our ap-
proaches. The aim of experiments on the entire
WSJ is to test whether our approaches still achieve
significant improvement when more accurate gen-
erative models are used, and also to show that
they generalize well to learning methods different
from SVMs. We perform experiments on the stan-
dard WSJ parsing data using the standard split into
training, validation and testing sets. We replicate
completely the setup of experiments in (Hender-
son and Titov, 2005). For a detailed description of
the experiment setup, we refer the reader to (Hen-
derson and Titov, 2005). We only note here that
the candidate list has 20 candidates, and, for the
testing set, selecting the candidate with an oracle
results in an F1 score of 95.4%.
We selected the TRK-Estim approach for these
experiments because it demonstrated the best re-
sults in the previous set of experiments (5.2). We
trained the Voted Perceptron (VP) modification
described in (Henderson and Titov, 2005) with the
TOP Reranking kernel. VP is not a linear classi-
fier, so we were not able to use a classifier in the
form (11). Instead the normalized counts of votes
given to the candidate parses were used as proba-
bility estimates, as discussed in section 3.3.
The resulting accuracies of this model are pre-
sented in table 2, together with results of the
TOP Reranking kernel VP (Henderson and Titov,
2005) and the SSN probabilistic model (Hender-
son, 2003). Model TRK-Estim achieves signifi-
cantly better results than the previously proposed
models, which were evaluated in the same exper-
imental setup. Again, the relative error reduction
is about the same as that of TRK. The resulting
system, consisting of the generative model and
the reranker, achieves results at the state-of-the-art
level. We believe that this method can be applied
to most parsing models to achieve a significant im-
provement.
</bodyText>
<page confidence="0.994974">
565
</page>
<table confidence="0.99985875">
R P F1
Henderson, 2003 88.8 89.5 89.1
Henderson&amp;Titov, 2005 89.1 90.1 89.6
TRK-Estim 89.5 90.5 90.0
</table>
<tableCaption confidence="0.981021333333333">
Table 2: Percentage labeled constituent recall (R),
precision (P), combination of both (F1) on the test-
ing set.
</tableCaption>
<subsectionHeader confidence="0.9689205">
5.4 Experiments with Voted Perceptron and
Tree Kernel
</subsectionHeader>
<bodyText confidence="0.99991875">
In this series of experiments we validate the state-
ment in section 3.3, where we suggested that loss
approximation from a discriminative classifier is
not limited only to models with data-defined ker-
nels. We apply the same method as used in
the TRK-Estim model above to the Tree Ker-
nel (Collins and Duffy, 2002), which we call the
TK-Estim model.
We replicated the parse reranking experimen-
tal setup used for the evaluation of the Tree Ker-
nel in (Collins and Duffy, 2002), where the can-
didate list was provided by the generative proba-
bilistic model (Collins, 1999) (model 2). A list of
on average 29 candidates was used, with an oracle
F1 score on the testing set of 95.0%. We trained
VP using the same parameters for the Tree Ker-
nel and probability feature weighting as described
in (Collins and Duffy, 2002). A publicly avail-
able efficient implementation of the Tree Kernel
was utilized to speed up computations (Moschitti,
2004). As in the previous section, votes of the per-
ceptron were used to define the probability esti-
mate used in the classifier.
The results for the MBR decoding method (TK-
Estim), defined in section 3.3, along with the stan-
dard Tree Kernel VP results (Collins and Duffy,
2002) (TK) and the probabilistic baseline (Collins,
1999) (CO99) are presented in table 3. The pro-
posed model improves in F1 score over the stan-
dard VP results. Differences between all the mod-
els are statistically significant. The error reduction
of TK-Estim is again about the same as the error
reduction of TK. This improvement is achieved
without adding any additional linguistic features.
It is important to note that the model improves
in other accuracy measures as well. We would
expect even better results with MBR-decoding if
larger n-best lists are used. The n-best parsing al-
gorithm (Huang and Chiang, 2005) can be used to
efficiently produce candidate lists as large as 106
</bodyText>
<table confidence="0.9615792">
R P F1. CB 0C 2C
CO99 88.1 88.3 88.2 1.06 64.0 85.1
TK 88.6 88.9 88.7 0.99 66.5 86.3
TK-Estim 89.0 89.5 89.2 0.91 66.6 87.4
* Fl for previous models may have rounding errors.
</table>
<tableCaption confidence="0.996663">
Table 3: Result on the testing set. Percentage la-
</tableCaption>
<bodyText confidence="0.940165166666667">
beled constituent recall (R), precision (P), combi-
nation of both (F1), an average number of cross-
ing brackets per sentence (CB), percentage of sen-
tences with 0 and &lt; 2 crossing brackets (0C and
2C, respectively).
parse trees with the model of (Collins, 1999).
</bodyText>
<sectionHeader confidence="0.999891" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999969666666667">
This paper considers methods for the estimation of
expected loss for parse reranking tasks. The pro-
posed methods include estimation of the loss from
a probabilistic model, estimation from a discrim-
inative classifier, and learning of the loss using a
specialized kernel. An empirical comparison of
these approaches on parse reranking tasks is pre-
sented. Special emphasis is given to data-defined
kernels for reranking, as they do not require the
introduction of any additional domain knowledge
not already encoded in the probabilistic model.
The best approach, estimation of the loss on the
basis of a discriminative classifier, achieves very
significant improvements over the baseline gener-
ative probabilistic models and the discriminative
classifier itself. Though the largest improvement is
demonstrated in the measure which corresponds to
the considered loss functional, other measures of
accuracy are also improved. The proposed method
achieves 90.0% F1 score on the standard Wall
Street Journal parsing task when the SSN neural
network is used as the probabilistic model and VP
with a TOP Reranking kernel as the discriminative
classifier.
</bodyText>
<sectionHeader confidence="0.998811" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999851142857143">
We would like to thank Michael Collins and
Terry Koo for providing us their data and use-
ful comments on experimental setup, and Alessan-
dro Moschitti for providing us the source code for
his Tree Kernel implementation. We also thank
anonymous reviewers for their constructive com-
ments.
</bodyText>
<page confidence="0.997359">
566
</page>
<sectionHeader confidence="0.996258" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999830923076923">
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proc. 43rd Meeting ofAssociation for
Computational Linguistics, pages 173–180, Ann Ar-
bor, MI.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels
over discrete structures and the voted perceptron.
In Proc. 40th Meeting of Association for Computa-
tional Linguistics, pages 263–270, Philadelphia, PA.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25–69.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia, PA.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proc. 17th Int. Conf. on
Machine Learning, pages 175–182, Stanford, CA.
Joshua Goodman. 1996. Parsing algorithms and meth-
ods. In Proc. 34th Meeting of the Association for
Computational Linguistics, pages 177–183, Santa
Cruz, CA.
James Henderson and Ivan Titov. 2005. Data-defined
kernels for parse reranking derived from probabilis-
tic models. In Proc. 43rd Meeting ofAssociation for
Computational Linguistics, Ann Arbor, MI.
James Henderson. 2003. Inducing history representa-
tions for broad coverage statistical parsing. In Proc.
joint meeting of North American Chapter of the As-
sociation for Computational Linguistics and the Hu-
man Language Technology Conf., pages 103–110,
Edmonton, Canada.
James Henderson. 2004. Discriminative training of
a neural network statistical parser. In Proc. 42nd
Meeting of Association for Computational Linguis-
tics, Barcelona, Spain.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proc. 9th Int. Workshop on Parsing Tech-
nologies, Vancouver, Canada.
Tommi S. Jaakkola and David Haussler. 1998. Ex-
ploiting generative models in discriminative classi-
fiers. Advances in Neural Information Processes
Systems 11.
Terry Koo and Michael Collins. 2005. Hidden-
variable models for discriminative reranking. In
Proc. Conf. on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference and Meeting of the North Amer-
ican Chapter of the Association for Computational
Linguistics, Boston, MA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Alessandro Moschitti. 2004. A study on convolutional
kernels for shallow semantic parsing. In Proc. 42nd
Meeting of the Association for Computational Lin-
guistics, Barcelona, Spain.
John C. Platt. 1999. Probabilistic outputs for sup-
port vector machines and comparision to regular-
ized likelihood methods. In A. Smola, P. Bartlett,
B. Scholkopf, and D. Schuurmans, editors, Ad-
vances in Large Margin Classifiers, pages 61–74.
MIT Press.
Adwait Ratnaparkhi. 1996. A maximum entropy
model for part-of-speech tagging. In Proc. Conf. on
Empirical Methods in Natural Language Process-
ing, pages 133–142, Univ. of Pennsylvania, PA.
Libin Shen and Aravind K. Joshi. 2003. An SVM
based voting algorithm with application to parse
reranking. In Proc. of the 7th Conf. on Computa-
tional Natural Language Learning, pages 9–16, Ed-
monton, Canada.
Andreas Stolcke, Yochai Konig, and Mitchel Wein-
traub. 1997. Explicit word error minimization in
n-best list rescoring. In Proc. of 5th European Con-
ference on Speech Communication and Technology,
pages 163–165, Rhodes, Greece.
Ben Taskar, Dan Klein, Michael Collins, Daphne
Koller, and Christopher Manning. 2004. Max-
margin parsing. In Proc. Conf. on Empirical Meth-
ods in Natural Language Processing, Barcelona,
Spain.
Ivan Titov and James Henderson. 2005. Deriving ker-
nels from MLP probability estimators for large cate-
gorization problems. In International Joint Confer-
ence on Neural Networks, Montreal, Canada.
Ioannis Tsochantaridis, Thomas Hofmann, Thorsten
Joachims, and Yasemin Altun. 2004. Support vector
machine learning for interdependent and structured
output spaces. In Proc. 21st Int. Conf. on Machine
Learning, pages 823–830, Banff, Alberta, Canada.
K. Tsuda, M. Kawanabe, G. Ratsch, S. Sonnenburg,
and K. Muller. 2002. A new discriminative ker-
nel from probabilistic models. Neural Computation,
14(10):2397–2414.
Alexander Yeh. 2000. More accurate tests for the
statistical significance of the result differences. In
Proc. 17th International Conf. on Computational
Linguistics, pages 947–953, Saarbruken, Germany.
</reference>
<page confidence="0.997311">
567
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.358833">
<title confidence="0.999927">Loss Minimization in Parse Reranking</title>
<author confidence="0.997097">Ivan</author>
<affiliation confidence="0.9992175">Department of Computer University of</affiliation>
<address confidence="0.568945">24, rue G´en´eral CH-1211 Gen`eve 4,</address>
<email confidence="0.736362">ivan.titov@cui.unige.ch</email>
<author confidence="0.943488">James</author>
<affiliation confidence="0.990916">School of University of 2 Buccleuch</affiliation>
<address confidence="0.957174">Edinburgh EH8 9LW, United</address>
<email confidence="0.995141">james.henderson@ed.ac.uk</email>
<abstract confidence="0.999458739130435">We propose a general method for reranker construction which targets choosing the candidate with the least expected loss, rather than the most probable candidate. Different approaches to expected loss approximation are considered, including estimating from the probabilistic model used to generate the candidates, estimating from a discriminative model trained to rerank the candidates, and learning to approximate the expected loss. The proposed methods are applied to the parse reranking task, with various baseline models, achieving significant improvement both over the probabilistic models and the discriminative rerankers. When a neural network parser is used as the probabilistic model and the Voted Perceptron algorithm with data-defined kernels as the learning algorithm, the loss minimization model 90.0% labeled constituents score on the standard WSJ parsing task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarseto-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. 43rd Meeting ofAssociation for Computational Linguistics,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="2495" citStr="Charniak and Johnson, 2005" startWordPosition="368" endWordPosition="371">rson and Titov, 2005). A very different use of loss functions was considered in the areas of signal processing and machine translation, where direct minimization of expected loss (Minimum Bayes Risk decoding) on word sequences was considered (Kumar and Byrne, 2004; Stolcke et al., 1997). The only attempt to use Minimum Bayes Risk (MBR) decoding in parsing was made in (Goodman, 1996), where a parsing algorithm for constituent recall minimization was constructed. However, their approach is limited to binarized PCFG models and, consequently, is not applicable to state-of-the-art parsing methods (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000). In this paper we consider several approaches to loss approximation on the basis of a candidate list provided by a baseline probabilistic model. The intuitive motivation for expected loss minimization can be seen from the following example. Consider the situation where there are a group of several very similar candidates and one very different candidate whose probability is just slightly larger than the probability of any individual candidate in the group, but much smaller than their total probability. A method which chooses the maximum probability candidate w</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarseto-fine n-best parsing and MaxEnt discriminative reranking. In Proc. 43rd Meeting ofAssociation for Computational Linguistics, pages 173–180, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proc. 40th Meeting of Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="16608" citStr="Collins and Duffy, 2002" startWordPosition="2761" endWordPosition="2764">terms of a history-based generative probability model. These parameters are estimated using a neural network, the weights of which form the second level of parameterization. This approach allows the probability model to have an infinite number of parameters; the neural network only estimates the bounded number of parameters which are relevant to a given partial parse. We define data-defined kernels in terms of the second level of parameterization (the network weights). For the last set of experiments, we used the probabilistic model described in (Collins, 1999) (model 2), and the Tree Kernel (Collins and Duffy, 2002). However, in these experiments we only used the estimates from the discriminative classifier, so the details of the probabilistic model are not relevant. 5.2 Experiments with SVM Struct Both the neural network probabilistic model and the kernel based classifiers were trained on section 0 (1,921 sentences, 40,930 words). Section 24 (1,346 sentences, 29,125 words) was used as the validation set during the neural network learning and for choosing parameters of the models. Section 23 (2,416 sentences, 54,268 words) was used for the final testing of the models. We used a publicly available tagger </context>
<context position="24942" citStr="Collins and Duffy, 2002" startWordPosition="4130" endWordPosition="4133">dels to achieve a significant improvement. 565 R P F1 Henderson, 2003 88.8 89.5 89.1 Henderson&amp;Titov, 2005 89.1 90.1 89.6 TRK-Estim 89.5 90.5 90.0 Table 2: Percentage labeled constituent recall (R), precision (P), combination of both (F1) on the testing set. 5.4 Experiments with Voted Perceptron and Tree Kernel In this series of experiments we validate the statement in section 3.3, where we suggested that loss approximation from a discriminative classifier is not limited only to models with data-defined kernels. We apply the same method as used in the TRK-Estim model above to the Tree Kernel (Collins and Duffy, 2002), which we call the TK-Estim model. We replicated the parse reranking experimental setup used for the evaluation of the Tree Kernel in (Collins and Duffy, 2002), where the candidate list was provided by the generative probabilistic model (Collins, 1999) (model 2). A list of on average 29 candidates was used, with an oracle F1 score on the testing set of 95.0%. We trained VP using the same parameters for the Tree Kernel and probability feature weighting as described in (Collins and Duffy, 2002). A publicly available efficient implementation of the Tree Kernel was utilized to speed up computatio</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures and the voted perceptron. In Proc. 40th Meeting of Association for Computational Linguistics, pages 263–270, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1301" citStr="Collins and Koo, 2005" startWordPosition="182" endWordPosition="185">trained to rerank the candidates, and learning to approximate the expected loss. The proposed methods are applied to the parse reranking task, with various baseline models, achieving significant improvement both over the probabilistic models and the discriminative rerankers. When a neural network parser is used as the probabilistic model and the Voted Perceptron algorithm with data-defined kernels as the learning algorithm, the loss minimization model achieves 90.0% labeled constituents F1 score on the standard WSJ parsing task. 1 Introduction The reranking approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems. For structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 2004; Henderson and Titov, 2005). A very di</context>
<context position="5906" citStr="Collins and Koo, 2005" startWordPosition="943" endWordPosition="946">, the second discriminative). The third defines a kernel for use with a classification method for minimizing loss. All use previously proposed learning algorithms and optimization criteria. 2 Loss Approximation with a Probabilistic Model In this section we discuss approximating the expected loss using probability estimates given by a baseline probabilistic model. Use of probability estimates is not a serious limitation of this approach because in practice candidates are normally provided by some probabilistic model and its probability estimates are used as additional features in the reranker (Collins and Koo, 2005; Shen and Joshi, 2003; Henderson and Titov, 2005). In order to estimate the expected loss on the basis of a candidate list, we make the assumption that the total probability of the labels not in the candidate list is sufficiently small that the difference δ(x, y0) of expected loss between the labels in the candidate list and the labels not in the candidate list does not have an impact on the loss defined in (1): Ey�∈G(x) P(y|x)A(y, y0) δ(x, y0) = − (3) Ey�G(x) P(y|x) Ey∈G(x) P(y|x)A(y, y0) Ey∈G(x) P(y|x) This gives us the following approximation to the expected loss for the label: E y∈G(x) P </context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>Michael Collins and Terry Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4907" citStr="Collins, 1999" startWordPosition="790" endWordPosition="791">quivalent to the requirement that the loss function is bounded in (Tsochantaridis et al., 2004). It follows that an optimal reranker h* is one which chooses the label y that minimizes the expected loss: � h*(x) = arg min P(y|x)A(y, y0), (2) y&apos;∈G(x) y where G(x) denotes a candidate list provided by a baseline probabilistic model for the input x. In this paper we propose different approaches to loss approximation. We apply them to the parse reranking problem where the baseline probabilistic model is a neural network parser (Henderson, 2003), and to parse reranking of candidates provided by the (Collins, 1999) model. The resulting reranking method achieves very significant improvement in the considered loss function and improvement in most other standard measures of accuracy. In the following three sections we will discuss three approaches to learning such a classifier. The first two derive a classification criteria for use with a predefined probability model (the first generative, the second discriminative). The third defines a kernel for use with a classification method for minimizing loss. All use previously proposed learning algorithms and optimization criteria. 2 Loss Approximation with a Prob</context>
<context position="14434" citStr="Collins, 1999" startWordPosition="2408" endWordPosition="2409"> training to rescale the margin or slack variables, this learner allows us to test the hypothesis that loss functions are useful in parsing not only to define the optimization criteria but also to define the classifier and to define the feature space. However, SVM Struct training for large scale parsing experiments is computationally expensive2, so here we use only a small portion of the available training data to perform evaluations of the different approaches. In the other two sets of experiments, described below, we test our best model on the standard Wall Street Journal parsing benchmark (Collins, 1999) with the Voted Perceptron algorithm as the learner. 5.1 The Probabilistic Models of Parsing To perform the experiments with data-defined kernels, we need to select a probabilistic model of parsing. Data-defined kernels can be applied to any kind of parameterized probabilistic model. For our first set of experiments, we choose to use a publicly available neural network based probabilistic model of parsing (Henderson, 2003). 2In (Shen and Joshi, 2003) it was proposed to use an ensemble of SVMs trained the Wall Street Journal corpus, but the generalization performance of the resulting classifier</context>
<context position="16551" citStr="Collins, 1999" startWordPosition="2753" endWordPosition="2754">ion. The first level of parameterization is in terms of a history-based generative probability model. These parameters are estimated using a neural network, the weights of which form the second level of parameterization. This approach allows the probability model to have an infinite number of parameters; the neural network only estimates the bounded number of parameters which are relevant to a given partial parse. We define data-defined kernels in terms of the second level of parameterization (the network weights). For the last set of experiments, we used the probabilistic model described in (Collins, 1999) (model 2), and the Tree Kernel (Collins and Duffy, 2002). However, in these experiments we only used the estimates from the discriminative classifier, so the details of the probabilistic model are not relevant. 5.2 Experiments with SVM Struct Both the neural network probabilistic model and the kernel based classifiers were trained on section 0 (1,921 sentences, 40,930 words). Section 24 (1,346 sentences, 29,125 words) was used as the validation set during the neural network learning and for choosing parameters of the models. Section 23 (2,416 sentences, 54,268 words) was used for the final te</context>
<context position="19720" citStr="Collins, 1999" startWordPosition="3285" endWordPosition="3286">match accuracy, are shown in table 1.3 As the baselines, the table includes the results of the standard TOP reranking kernel (TRK) (Henderson and Titov, 2005) and the baseline probabilistic model (SSN) (Henderson, 2003). SSN-Estim is the model using loss estimation on the basic probabilistic model, as explained in section 2. LLK-Learn and LK-Learn are the models which define the kernel based on loss, using the Loss Logit Kernel (equation (13)) and the Loss Kernel (equation (12)), respectively. FKEstim and TRK-Estim are the models which esti3All our results are computed with the evalb program (Collins, 1999). 564 mate the loss with data-defined kernels, using the Fisher Kernel (equation (8)) and the TOP Reranking kernel (equation (11)), respectively. All our proposed models show better F1 accuracy than the baseline probabilistic model SSN, and all these differences are statistically significant.4 The difference in F1 between TRK-Estim and FK-Estim is not statistically significant, but otherwise TRK-Estim demonstrates a statistically significant improvement over all other models. It should also be noted that exact match measures for TRK-Estim and SSN-Estim are not negatively affected, even though </context>
<context position="25195" citStr="Collins, 1999" startWordPosition="4175" endWordPosition="4176">Experiments with Voted Perceptron and Tree Kernel In this series of experiments we validate the statement in section 3.3, where we suggested that loss approximation from a discriminative classifier is not limited only to models with data-defined kernels. We apply the same method as used in the TRK-Estim model above to the Tree Kernel (Collins and Duffy, 2002), which we call the TK-Estim model. We replicated the parse reranking experimental setup used for the evaluation of the Tree Kernel in (Collins and Duffy, 2002), where the candidate list was provided by the generative probabilistic model (Collins, 1999) (model 2). A list of on average 29 candidates was used, with an oracle F1 score on the testing set of 95.0%. We trained VP using the same parameters for the Tree Kernel and probability feature weighting as described in (Collins and Duffy, 2002). A publicly available efficient implementation of the Tree Kernel was utilized to speed up computations (Moschitti, 2004). As in the previous section, votes of the perceptron were used to define the probability estimate used in the classifier. The results for the MBR decoding method (TKEstim), defined in section 3.3, along with the standard Tree Kernel</context>
<context position="26994" citStr="Collins, 1999" startWordPosition="4487" endWordPosition="4488"> are used. The n-best parsing algorithm (Huang and Chiang, 2005) can be used to efficiently produce candidate lists as large as 106 R P F1. CB 0C 2C CO99 88.1 88.3 88.2 1.06 64.0 85.1 TK 88.6 88.9 88.7 0.99 66.5 86.3 TK-Estim 89.0 89.5 89.2 0.91 66.6 87.4 * Fl for previous models may have rounding errors. Table 3: Result on the testing set. Percentage labeled constituent recall (R), precision (P), combination of both (F1), an average number of crossing brackets per sentence (CB), percentage of sentences with 0 and &lt; 2 crossing brackets (0C and 2C, respectively). parse trees with the model of (Collins, 1999). 6 Conclusions This paper considers methods for the estimation of expected loss for parse reranking tasks. The proposed methods include estimation of the loss from a probabilistic model, estimation from a discriminative classifier, and learning of the loss using a specialized kernel. An empirical comparison of these approaches on parse reranking tasks is presented. Special emphasis is given to data-defined kernels for reranking, as they do not require the introduction of any additional domain knowledge not already encoded in the probabilistic model. The best approach, estimation of the loss o</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In Proc. 17th Int. Conf. on Machine Learning,</booktitle>
<pages>175--182</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="2528" citStr="Collins, 2000" startWordPosition="374" endWordPosition="375">f loss functions was considered in the areas of signal processing and machine translation, where direct minimization of expected loss (Minimum Bayes Risk decoding) on word sequences was considered (Kumar and Byrne, 2004; Stolcke et al., 1997). The only attempt to use Minimum Bayes Risk (MBR) decoding in parsing was made in (Goodman, 1996), where a parsing algorithm for constituent recall minimization was constructed. However, their approach is limited to binarized PCFG models and, consequently, is not applicable to state-of-the-art parsing methods (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000). In this paper we consider several approaches to loss approximation on the basis of a candidate list provided by a baseline probabilistic model. The intuitive motivation for expected loss minimization can be seen from the following example. Consider the situation where there are a group of several very similar candidates and one very different candidate whose probability is just slightly larger than the probability of any individual candidate in the group, but much smaller than their total probability. A method which chooses the maximum probability candidate will choose this outlier candidate</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In Proc. 17th Int. Conf. on Machine Learning, pages 175–182, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Parsing algorithms and methods.</title>
<date>1996</date>
<booktitle>In Proc. 34th Meeting of the Association for Computational Linguistics,</booktitle>
<pages>177--183</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="2254" citStr="Goodman, 1996" startWordPosition="336" endWordPosition="337">hoice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 2004; Henderson and Titov, 2005). A very different use of loss functions was considered in the areas of signal processing and machine translation, where direct minimization of expected loss (Minimum Bayes Risk decoding) on word sequences was considered (Kumar and Byrne, 2004; Stolcke et al., 1997). The only attempt to use Minimum Bayes Risk (MBR) decoding in parsing was made in (Goodman, 1996), where a parsing algorithm for constituent recall minimization was constructed. However, their approach is limited to binarized PCFG models and, consequently, is not applicable to state-of-the-art parsing methods (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000). In this paper we consider several approaches to loss approximation on the basis of a candidate list provided by a baseline probabilistic model. The intuitive motivation for expected loss minimization can be seen from the following example. Consider the situation where there are a group of several very similar candidates an</context>
</contexts>
<marker>Goodman, 1996</marker>
<rawString>Joshua Goodman. 1996. Parsing algorithms and methods. In Proc. 34th Meeting of the Association for Computational Linguistics, pages 177–183, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
<author>Ivan Titov</author>
</authors>
<title>Data-defined kernels for parse reranking derived from probabilistic models.</title>
<date>2005</date>
<booktitle>In Proc. 43rd Meeting ofAssociation for Computational Linguistics,</booktitle>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="1351" citStr="Henderson and Titov, 2005" startWordPosition="190" endWordPosition="193"> to approximate the expected loss. The proposed methods are applied to the parse reranking task, with various baseline models, achieving significant improvement both over the probabilistic models and the discriminative rerankers. When a neural network parser is used as the probabilistic model and the Voted Perceptron algorithm with data-defined kernels as the learning algorithm, the loss minimization model achieves 90.0% labeled constituents F1 score on the standard WSJ parsing task. 1 Introduction The reranking approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems. For structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 2004; Henderson and Titov, 2005). A very different use of loss functions was considered in th</context>
<context position="5956" citStr="Henderson and Titov, 2005" startWordPosition="951" endWordPosition="954">es a kernel for use with a classification method for minimizing loss. All use previously proposed learning algorithms and optimization criteria. 2 Loss Approximation with a Probabilistic Model In this section we discuss approximating the expected loss using probability estimates given by a baseline probabilistic model. Use of probability estimates is not a serious limitation of this approach because in practice candidates are normally provided by some probabilistic model and its probability estimates are used as additional features in the reranker (Collins and Koo, 2005; Shen and Joshi, 2003; Henderson and Titov, 2005). In order to estimate the expected loss on the basis of a candidate list, we make the assumption that the total probability of the labels not in the candidate list is sufficiently small that the difference δ(x, y0) of expected loss between the labels in the candidate list and the labels not in the candidate list does not have an impact on the loss defined in (1): Ey�∈G(x) P(y|x)A(y, y0) δ(x, y0) = − (3) Ey�G(x) P(y|x) Ey∈G(x) P(y|x)A(y, y0) Ey∈G(x) P(y|x) This gives us the following approximation to the expected loss for the label: E y∈G(x) P (y|x)A(y, y0) l(x, y0) = E y∈G(x) P(y|x) .(4) � �h</context>
<context position="7215" citStr="Henderson and Titov, 2005" startWordPosition="1168" endWordPosition="1171">(5) y&apos;∈G(x) y∈G(x) where θ� denotes the parameters of the probabilistic model learned from the training data. This approach for expected loss approximation was considered in the context of word error rate minimization in speech recognition, see for example (Stolcke et al., 1997). 3 Estimating Expected Loss with Discriminative Classifiers In this section we propose a method to improve on the loss approximation used in (5) by constructing the probability estimates using a trained discriminative classifier. Special emphasis is placed on linear classifiers with data-defined kernels for reranking (Henderson and Titov, 2005), because they do not require any additional domain knowledge not already encoded in the probabilistic model, and they have demonstrated significant improvement over the baseline probabilistic model for the parse reranking task. This kernel construction can be motivated by the existence of a function which maps a linear function in the feature space of the kernel to probability estimates which are superior to the estimates of the ori ginal probabilistic model. For the reranking case, often the probabilistic model only estimates the joint probability P(x, y). However, neither this difference no</context>
<context position="8476" citStr="Henderson and Titov, 2005" startWordPosition="1367" endWordPosition="1371">e classification. Thus, replacing the true probabilities with their estimates, we can define the classifier 561 3.1 Estimation with Fisher Kernels The Fisher kernel for structured classification is a trivial generalization of one of the best known data-defined kernels for binary classification (Jaakkola and Haussler, 1998). The Fisher score of an example input-label pair (x, y) is a vector of partial derivatives of the log-likelihood of the example with respect to the model parametersl: φF K θ� (x, y) = (6) 3.2 Estimation with TOP Kernels for Reranking The TOP Reranking kernel was defined in (Henderson and Titov, 2005), as a generalization of the TOP kernel (Tsuda et al., 2002) proposed for binary classification tasks. The feature extractor for the TOP reranking kernel is given by: φT K θ� (x, y) = (9) e), ∂v(x, y, 0) ,..., ∂θ1 (v(x, y, ∂v(x, y, �θ) ), ∂θl (logP(x, y |�θ), ∂logP (x,y|�θ) ∂logP(x,y|�θ) where P(x, y&apos;|�θ). ,..., ). X ∂θ1 ∂θl v(x, y, B) = log P(x, y|�θ) − log This kernel defines a feature space which is appropriate for estimating the discriminative probability in the candidate list in the form of a normalized exponential Py0EG(x) exp(w?TφF K θ� (x, y&apos;)) for some choice of the decision vector w </context>
<context position="10059" citStr="Henderson and Titov, 2005" startWordPosition="1659" endWordPosition="1662">raining and the scalar parameter A can be tuned on the development set. From the construction of the Fisher kernel, it follows that the optimal value A is expected to be close to inverse of the first component of w, 1/ 1u1. If an SVM is used to learn the classifier, then the form (7) is the same as that proposed by (Platt, 1999), where it is proposed to use the logistic sigmoid of the SVM output as the probability estimator for binary classification problems. &apos;The first component logP(x, y0) is not in the strict sense part of the Fisher score, but usually added to kernel features in practice (Henderson and Titov, 2005). y0EG(x)−{y} The TOP reranking kernel has been demonstrated to perform better than the Fisher kernel for the parse reranking task (Henderson and Titov, 2005). The construction of this kernel is motivated by the minimization of the classification error of a linear classifier wT φB(x, y). This linear classifier has been shown to converge, assuming estimation of the discriminative probability in the candidate list can be in the form of the logistic sigmoid (Titov and Henderson, 2005): Py0EG(x) P x, y&apos;) ≈ (10) 1 1 + exp(−w?TφT K θ� (x, y)) for some choice of the decision vector w = w? with the fi</context>
<context position="15677" citStr="Henderson and Titov, 2005" startWordPosition="2609" endWordPosition="2612">ised in this approach. (v(x, y, ∂v(x, y, �θ) ), ∂θl eav( ) x, y, 0) ,..., ∂θ1 ∂v(x, y, �θ) ), ∂θl v(x, y, �θ) = log( � P(y0|x,�θ)(1−A(y0,y)))− y&apos;∈G(x) log( � P(y0|x, �θ)A(y0, y)). y&apos;∈G(x) 563 This parsing model is a good candidate for our experiments because it achieves state-of-the-art results on the standard Wall Street Journal (WSJ) parsing problem (Henderson, 2003), and datadefined kernels derived from this parsing model have recently been used with the Voted Perceptron algorithm on the WSJ parsing task, achieving a significant improvement in accuracy over the neural network parser alone (Henderson and Titov, 2005). This gives us a baseline which is hard to beat, and allows us to compare results of our new approaches with the results of the original datadefined kernels for reranking. The probabilistic model of parsing in (Henderson, 2003) has two levels of parameterization. The first level of parameterization is in terms of a history-based generative probability model. These parameters are estimated using a neural network, the weights of which form the second level of parameterization. This approach allows the probability model to have an infinite number of parameters; the neural network only estimates </context>
<context position="19264" citStr="Henderson and Titov, 2005" startWordPosition="3208" endWordPosition="3211">ear slack penalty. The loss function is defined as A(y, y&apos;) = 1 − F1(y, y&apos;), where F1 denotes F1 measure on bracketed constituents. This loss was used both for rescaling the slacks in the SVM and for defining our classification models and kernels. We performed initial testing of the models on the validation set and preselected the best model for each of the approaches before testing it on the final testing set. Standard measures of parsing accuracy, plus complete match accuracy, are shown in table 1.3 As the baselines, the table includes the results of the standard TOP reranking kernel (TRK) (Henderson and Titov, 2005) and the baseline probabilistic model (SSN) (Henderson, 2003). SSN-Estim is the model using loss estimation on the basic probabilistic model, as explained in section 2. LLK-Learn and LK-Learn are the models which define the kernel based on loss, using the Loss Logit Kernel (equation (13)) and the Loss Kernel (equation (12)), respectively. FKEstim and TRK-Estim are the models which esti3All our results are computed with the evalb program (Collins, 1999). 564 mate the loss with data-defined kernels, using the Fisher Kernel (equation (8)) and the TOP Reranking kernel (equation (11)), respectively</context>
<context position="21794" citStr="Henderson and Titov, 2005" startWordPosition="3607" endWordPosition="3611">from the feature vector and performed learning. Surprisingly, the model achieved virtually the same results, rather than the predicted worse performance. This result might indicate that the LLKLearn model still can be useful for different problems where discriminative learning gives more advantage over generative approaches. These experimental results demonstrate that the loss approximation reranking approaches proposed in this paper demonstrate significant improvement over the baseline models, achieving about the same relative error reduction as previously achieved with data-defined kernels (Henderson and Titov, 2005). This improvement is despite the fact that the loss function is already used in the definition of the training criteria for all the models except SSN. It is also interesting to note that the best result on the validation set for estimation 4We measured significance of all the experiments in this paper with the randomized significance test (Yeh, 2000). of the loss with data-defined kernels (12) and (13) was achieved when the parameter A is close to the inverse of the first component of the learned decision vector, which confirms the motivation for these kernels. 5.3 Experiments with Voted Perc</context>
<context position="23077" citStr="Henderson and Titov, 2005" startWordPosition="3818" endWordPosition="3822">h the SVM Struct demonstrate empirically the viability of our approaches. The aim of experiments on the entire WSJ is to test whether our approaches still achieve significant improvement when more accurate generative models are used, and also to show that they generalize well to learning methods different from SVMs. We perform experiments on the standard WSJ parsing data using the standard split into training, validation and testing sets. We replicate completely the setup of experiments in (Henderson and Titov, 2005). For a detailed description of the experiment setup, we refer the reader to (Henderson and Titov, 2005). We only note here that the candidate list has 20 candidates, and, for the testing set, selecting the candidate with an oracle results in an F1 score of 95.4%. We selected the TRK-Estim approach for these experiments because it demonstrated the best results in the previous set of experiments (5.2). We trained the Voted Perceptron (VP) modification described in (Henderson and Titov, 2005) with the TOP Reranking kernel. VP is not a linear classifier, so we were not able to use a classifier in the form (11). Instead the normalized counts of votes given to the candidate parses were used as probab</context>
</contexts>
<marker>Henderson, Titov, 2005</marker>
<rawString>James Henderson and Ivan Titov. 2005. Data-defined kernels for parse reranking derived from probabilistic models. In Proc. 43rd Meeting ofAssociation for Computational Linguistics, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Inducing history representations for broad coverage statistical parsing.</title>
<date>2003</date>
<booktitle>In Proc. joint meeting of North American Chapter of the Association for Computational Linguistics and the Human Language Technology Conf.,</booktitle>
<pages>103--110</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="4837" citStr="Henderson, 2003" startWordPosition="778" endWordPosition="779"> loss function possesses values within the range from 0 to 1, which is equivalent to the requirement that the loss function is bounded in (Tsochantaridis et al., 2004). It follows that an optimal reranker h* is one which chooses the label y that minimizes the expected loss: � h*(x) = arg min P(y|x)A(y, y0), (2) y&apos;∈G(x) y where G(x) denotes a candidate list provided by a baseline probabilistic model for the input x. In this paper we propose different approaches to loss approximation. We apply them to the parse reranking problem where the baseline probabilistic model is a neural network parser (Henderson, 2003), and to parse reranking of candidates provided by the (Collins, 1999) model. The resulting reranking method achieves very significant improvement in the considered loss function and improvement in most other standard measures of accuracy. In the following three sections we will discuss three approaches to learning such a classifier. The first two derive a classification criteria for use with a predefined probability model (the first generative, the second discriminative). The third defines a kernel for use with a classification method for minimizing loss. All use previously proposed learning </context>
<context position="14860" citStr="Henderson, 2003" startWordPosition="2474" endWordPosition="2475">m evaluations of the different approaches. In the other two sets of experiments, described below, we test our best model on the standard Wall Street Journal parsing benchmark (Collins, 1999) with the Voted Perceptron algorithm as the learner. 5.1 The Probabilistic Models of Parsing To perform the experiments with data-defined kernels, we need to select a probabilistic model of parsing. Data-defined kernels can be applied to any kind of parameterized probabilistic model. For our first set of experiments, we choose to use a publicly available neural network based probabilistic model of parsing (Henderson, 2003). 2In (Shen and Joshi, 2003) it was proposed to use an ensemble of SVMs trained the Wall Street Journal corpus, but the generalization performance of the resulting classifier might be compromised in this approach. (v(x, y, ∂v(x, y, �θ) ), ∂θl eav( ) x, y, 0) ,..., ∂θ1 ∂v(x, y, �θ) ), ∂θl v(x, y, �θ) = log( � P(y0|x,�θ)(1−A(y0,y)))− y&apos;∈G(x) log( � P(y0|x, �θ)A(y0, y)). y&apos;∈G(x) 563 This parsing model is a good candidate for our experiments because it achieves state-of-the-art results on the standard Wall Street Journal (WSJ) parsing problem (Henderson, 2003), and datadefined kernels derived from</context>
<context position="19325" citStr="Henderson, 2003" startWordPosition="3218" endWordPosition="3220"> y&apos;), where F1 denotes F1 measure on bracketed constituents. This loss was used both for rescaling the slacks in the SVM and for defining our classification models and kernels. We performed initial testing of the models on the validation set and preselected the best model for each of the approaches before testing it on the final testing set. Standard measures of parsing accuracy, plus complete match accuracy, are shown in table 1.3 As the baselines, the table includes the results of the standard TOP reranking kernel (TRK) (Henderson and Titov, 2005) and the baseline probabilistic model (SSN) (Henderson, 2003). SSN-Estim is the model using loss estimation on the basic probabilistic model, as explained in section 2. LLK-Learn and LK-Learn are the models which define the kernel based on loss, using the Loss Logit Kernel (equation (13)) and the Loss Kernel (equation (12)), respectively. FKEstim and TRK-Estim are the models which esti3All our results are computed with the evalb program (Collins, 1999). 564 mate the loss with data-defined kernels, using the Fisher Kernel (equation (8)) and the TOP Reranking kernel (equation (11)), respectively. All our proposed models show better F1 accuracy than the ba</context>
<context position="23918" citStr="Henderson, 2003" startWordPosition="3964" endWordPosition="3966"> it demonstrated the best results in the previous set of experiments (5.2). We trained the Voted Perceptron (VP) modification described in (Henderson and Titov, 2005) with the TOP Reranking kernel. VP is not a linear classifier, so we were not able to use a classifier in the form (11). Instead the normalized counts of votes given to the candidate parses were used as probability estimates, as discussed in section 3.3. The resulting accuracies of this model are presented in table 2, together with results of the TOP Reranking kernel VP (Henderson and Titov, 2005) and the SSN probabilistic model (Henderson, 2003). Model TRK-Estim achieves significantly better results than the previously proposed models, which were evaluated in the same experimental setup. Again, the relative error reduction is about the same as that of TRK. The resulting system, consisting of the generative model and the reranker, achieves results at the state-of-the-art level. We believe that this method can be applied to most parsing models to achieve a significant improvement. 565 R P F1 Henderson, 2003 88.8 89.5 89.1 Henderson&amp;Titov, 2005 89.1 90.1 89.6 TRK-Estim 89.5 90.5 90.0 Table 2: Percentage labeled constituent recall (R), p</context>
</contexts>
<marker>Henderson, 2003</marker>
<rawString>James Henderson. 2003. Inducing history representations for broad coverage statistical parsing. In Proc. joint meeting of North American Chapter of the Association for Computational Linguistics and the Human Language Technology Conf., pages 103–110, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Discriminative training of a neural network statistical parser.</title>
<date>2004</date>
<booktitle>In Proc. 42nd Meeting of Association for Computational Linguistics,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2512" citStr="Henderson, 2004" startWordPosition="372" endWordPosition="373">y different use of loss functions was considered in the areas of signal processing and machine translation, where direct minimization of expected loss (Minimum Bayes Risk decoding) on word sequences was considered (Kumar and Byrne, 2004; Stolcke et al., 1997). The only attempt to use Minimum Bayes Risk (MBR) decoding in parsing was made in (Goodman, 1996), where a parsing algorithm for constituent recall minimization was constructed. However, their approach is limited to binarized PCFG models and, consequently, is not applicable to state-of-the-art parsing methods (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000). In this paper we consider several approaches to loss approximation on the basis of a candidate list provided by a baseline probabilistic model. The intuitive motivation for expected loss minimization can be seen from the following example. Consider the situation where there are a group of several very similar candidates and one very different candidate whose probability is just slightly larger than the probability of any individual candidate in the group, but much smaller than their total probability. A method which chooses the maximum probability candidate will choose this o</context>
</contexts>
<marker>Henderson, 2004</marker>
<rawString>James Henderson. 2004. Discriminative training of a neural network statistical parser. In Proc. 42nd Meeting of Association for Computational Linguistics, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proc. 9th Int. Workshop on Parsing Technologies,</booktitle>
<location>Vancouver, Canada.</location>
<contexts>
<context position="26444" citStr="Huang and Chiang, 2005" startWordPosition="4384" endWordPosition="4387">uffy, 2002) (TK) and the probabilistic baseline (Collins, 1999) (CO99) are presented in table 3. The proposed model improves in F1 score over the standard VP results. Differences between all the models are statistically significant. The error reduction of TK-Estim is again about the same as the error reduction of TK. This improvement is achieved without adding any additional linguistic features. It is important to note that the model improves in other accuracy measures as well. We would expect even better results with MBR-decoding if larger n-best lists are used. The n-best parsing algorithm (Huang and Chiang, 2005) can be used to efficiently produce candidate lists as large as 106 R P F1. CB 0C 2C CO99 88.1 88.3 88.2 1.06 64.0 85.1 TK 88.6 88.9 88.7 0.99 66.5 86.3 TK-Estim 89.0 89.5 89.2 0.91 66.6 87.4 * Fl for previous models may have rounding errors. Table 3: Result on the testing set. Percentage labeled constituent recall (R), precision (P), combination of both (F1), an average number of crossing brackets per sentence (CB), percentage of sentences with 0 and &lt; 2 crossing brackets (0C and 2C, respectively). parse trees with the model of (Collins, 1999). 6 Conclusions This paper considers methods for t</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proc. 9th Int. Workshop on Parsing Technologies, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tommi S Jaakkola</author>
<author>David Haussler</author>
</authors>
<title>Exploiting generative models in discriminative classifiers.</title>
<date>1998</date>
<booktitle>Advances in Neural Information Processes Systems 11.</booktitle>
<contexts>
<context position="8174" citStr="Jaakkola and Haussler, 1998" startWordPosition="1313" endWordPosition="1316">in the feature space of the kernel to probability estimates which are superior to the estimates of the ori ginal probabilistic model. For the reranking case, often the probabilistic model only estimates the joint probability P(x, y). However, neither this difference nor the denominator in (4) affects the classification. Thus, replacing the true probabilities with their estimates, we can define the classifier 561 3.1 Estimation with Fisher Kernels The Fisher kernel for structured classification is a trivial generalization of one of the best known data-defined kernels for binary classification (Jaakkola and Haussler, 1998). The Fisher score of an example input-label pair (x, y) is a vector of partial derivatives of the log-likelihood of the example with respect to the model parametersl: φF K θ� (x, y) = (6) 3.2 Estimation with TOP Kernels for Reranking The TOP Reranking kernel was defined in (Henderson and Titov, 2005), as a generalization of the TOP kernel (Tsuda et al., 2002) proposed for binary classification tasks. The feature extractor for the TOP reranking kernel is given by: φT K θ� (x, y) = (9) e), ∂v(x, y, 0) ,..., ∂θ1 (v(x, y, ∂v(x, y, �θ) ), ∂θl (logP(x, y |�θ), ∂logP (x,y|�θ) ∂logP(x,y|�θ) where P(x</context>
</contexts>
<marker>Jaakkola, Haussler, 1998</marker>
<rawString>Tommi S. Jaakkola and David Haussler. 1998. Exploiting generative models in discriminative classifiers. Advances in Neural Information Processes Systems 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Hiddenvariable models for discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. Conf. on Empirical Methods in Natural Language Processing,</booktitle>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="1324" citStr="Koo and Collins, 2005" startWordPosition="186" endWordPosition="189">andidates, and learning to approximate the expected loss. The proposed methods are applied to the parse reranking task, with various baseline models, achieving significant improvement both over the probabilistic models and the discriminative rerankers. When a neural network parser is used as the probabilistic model and the Voted Perceptron algorithm with data-defined kernels as the learning algorithm, the loss minimization model achieves 90.0% labeled constituents F1 score on the standard WSJ parsing task. 1 Introduction The reranking approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems. For structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 2004; Henderson and Titov, 2005). A very different use of loss fun</context>
</contexts>
<marker>Koo, Collins, 2005</marker>
<rawString>Terry Koo and Michael Collins. 2005. Hiddenvariable models for discriminative reranking. In Proc. Conf. on Empirical Methods in Natural Language Processing, Vancouver, B.C., Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>Minimum bayes-risk decoding for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference and Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Boston, MA.</location>
<contexts>
<context position="2133" citStr="Kumar and Byrne, 2004" startWordPosition="311" endWordPosition="314">l structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 2004; Henderson and Titov, 2005). A very different use of loss functions was considered in the areas of signal processing and machine translation, where direct minimization of expected loss (Minimum Bayes Risk decoding) on word sequences was considered (Kumar and Byrne, 2004; Stolcke et al., 1997). The only attempt to use Minimum Bayes Risk (MBR) decoding in parsing was made in (Goodman, 1996), where a parsing algorithm for constituent recall minimization was constructed. However, their approach is limited to binarized PCFG models and, consequently, is not applicable to state-of-the-art parsing methods (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000). In this paper we consider several approaches to loss approximation on the basis of a candidate list provided by a baseline probabilistic model. The intuitive motivation for expected loss minimization can</context>
</contexts>
<marker>Kumar, Byrne, 2004</marker>
<rawString>Shankar Kumar and William Byrne. 2004. Minimum bayes-risk decoding for statistical machine translation. In Proceedings of the Human Language Technology Conference and Meeting of the North American Chapter of the Association for Computational Linguistics, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="13672" citStr="Marcus et al., 1993" startWordPosition="2282" endWordPosition="2286">was a generalization of the Fisher kernel to arbitrary loss function, so the Loss Logit Kernel is a generalization of the TOP kernel for reranking. The construction of the Loss Logit Kernel, like the TOP kernel for reranking, can be motivated by the minimization of the classification error of a linear classifier wT φLLK � (x, y), where φLLK � (x, y) is the feature extractor of the kernel given by: φLLK � (x,y) = (13) (v(x, y, where 5 Experimental Evaluation To perform empirical evaluations of the proposed methods, we considered the task of parsing the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). First, we perform experiments with SVM Struct (Tsochantaridis et al., 2004) as the learner. Since SVM Struct already uses the loss function during training to rescale the margin or slack variables, this learner allows us to test the hypothesis that loss functions are useful in parsing not only to define the optimization criteria but also to define the classifier and to define the feature space. However, SVM Struct training for large scale parsing experiments is computationally expensive2, so here we use only a small portion of the available training data to perform evaluations of the differe</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolutional kernels for shallow semantic parsing.</title>
<date>2004</date>
<booktitle>In Proc. 42nd Meeting of the Association for Computational Linguistics,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="25562" citStr="Moschitti, 2004" startWordPosition="4238" endWordPosition="4239">ich we call the TK-Estim model. We replicated the parse reranking experimental setup used for the evaluation of the Tree Kernel in (Collins and Duffy, 2002), where the candidate list was provided by the generative probabilistic model (Collins, 1999) (model 2). A list of on average 29 candidates was used, with an oracle F1 score on the testing set of 95.0%. We trained VP using the same parameters for the Tree Kernel and probability feature weighting as described in (Collins and Duffy, 2002). A publicly available efficient implementation of the Tree Kernel was utilized to speed up computations (Moschitti, 2004). As in the previous section, votes of the perceptron were used to define the probability estimate used in the classifier. The results for the MBR decoding method (TKEstim), defined in section 3.3, along with the standard Tree Kernel VP results (Collins and Duffy, 2002) (TK) and the probabilistic baseline (Collins, 1999) (CO99) are presented in table 3. The proposed model improves in F1 score over the standard VP results. Differences between all the models are statistically significant. The error reduction of TK-Estim is again about the same as the error reduction of TK. This improvement is ac</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolutional kernels for shallow semantic parsing. In Proc. 42nd Meeting of the Association for Computational Linguistics, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John C Platt</author>
</authors>
<title>Probabilistic outputs for support vector machines and comparision to regularized likelihood methods.</title>
<date>1999</date>
<booktitle>Advances in Large Margin Classifiers,</booktitle>
<pages>61--74</pages>
<editor>In A. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9763" citStr="Platt, 1999" startWordPosition="1610" endWordPosition="1611">o use an estimator of the discriminative probability P(y|x) in exponential form and, therefore, the appropriate form of the loss minimizing classifier is the following: �hFK(x) = (8) X arg min exp(A wT φF K θ� (x, y&apos;))A(y, y&apos;), y0EG(x) yEG(x) where w� is learned during classifier training and the scalar parameter A can be tuned on the development set. From the construction of the Fisher kernel, it follows that the optimal value A is expected to be close to inverse of the first component of w, 1/ 1u1. If an SVM is used to learn the classifier, then the form (7) is the same as that proposed by (Platt, 1999), where it is proposed to use the logistic sigmoid of the SVM output as the probability estimator for binary classification problems. &apos;The first component logP(x, y0) is not in the strict sense part of the Fisher score, but usually added to kernel features in practice (Henderson and Titov, 2005). y0EG(x)−{y} The TOP reranking kernel has been demonstrated to perform better than the Fisher kernel for the parse reranking task (Henderson and Titov, 2005). The construction of this kernel is motivated by the minimization of the classification error of a linear classifier wT φB(x, y). This linear cla</context>
<context position="11442" citStr="Platt, 1999" startWordPosition="1911" endWordPosition="1912">re g is the logistic sigmoid and the scalar parameter A should be selected on the development set. As for the Fisher kernel, the optimal value of A should be close to 1/ w1. 3.3 Estimates from Arbitrary Classifiers Although in this paper we focus on approaches which do not require additional domain knowledge, the output of most classifiers can be used to estimate the discriminative probability in equation (7). As mentioned above, the form of (7) P(x, y) Py0EG(x) P (x, y&apos;) exp(w?TφF K θ� (x, y)) ≈ (7) P(x, y) 562 is appropriate for the SVM learning task with arbitrary kernels, as follows from (Platt, 1999). Also, for models which combine classifiers using votes (e.g. the Voted Perceptron), the number of votes cast for each candidate can be used to define this discriminative probability. The discriminative probability of a candidate is simply the number of votes cast for that candidate normalized across candidates. Intuitively, we can think of this method as treating the votes as a sample from the discriminative distribution. 4 Expected Loss Learning In this section, another approach to loss approximation is proposed. We consider learning a linear classifier to choose the least loss candidate, a</context>
</contexts>
<marker>Platt, 1999</marker>
<rawString>John C. Platt. 1999. Probabilistic outputs for support vector machines and comparision to regularized likelihood methods. In A. Smola, P. Bartlett, B. Scholkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 61–74. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proc. Conf. on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<location>Univ. of Pennsylvania, PA.</location>
<contexts>
<context position="17227" citStr="Ratnaparkhi, 1996" startWordPosition="2861" endWordPosition="2863"> However, in these experiments we only used the estimates from the discriminative classifier, so the details of the probabilistic model are not relevant. 5.2 Experiments with SVM Struct Both the neural network probabilistic model and the kernel based classifiers were trained on section 0 (1,921 sentences, 40,930 words). Section 24 (1,346 sentences, 29,125 words) was used as the validation set during the neural network learning and for choosing parameters of the models. Section 23 (2,416 sentences, 54,268 words) was used for the final testing of the models. We used a publicly available tagger (Ratnaparkhi, 1996) to provide the part-of-speech tags for each word in the sentence. For each tag, there is an unknown-word vocabulary item which is used for all those words which are not sufficiently frequent with that tag to be included individually in the vocabulary. For these experiments, we only included a specific tag-word pair in the vocabuR P F1 CM SSN 80.9 81.7 81.3 18.3 TRK 81.1 82.4 81.7 18.2 SSN-Estim 81.4 82.3 81.8 18.3 LLK-Learn 81.2 82.4 81.8 17.6 LK-Learn 81.5 82.2 81.8 17.8 FK-Estim 81.4 82.6 82.0 18.3 TRK-Estim 81.5 82.8 82.1 18.6 Table 1: Percentage labeled constituent recall (R), precision (</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proc. Conf. on Empirical Methods in Natural Language Processing, pages 133–142, Univ. of Pennsylvania, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>An SVM based voting algorithm with application to parse reranking.</title>
<date>2003</date>
<booktitle>In Proc. of the 7th Conf. on Computational Natural Language Learning,</booktitle>
<pages>9--16</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1374" citStr="Shen and Joshi, 2003" startWordPosition="194" endWordPosition="197">d loss. The proposed methods are applied to the parse reranking task, with various baseline models, achieving significant improvement both over the probabilistic models and the discriminative rerankers. When a neural network parser is used as the probabilistic model and the Voted Perceptron algorithm with data-defined kernels as the learning algorithm, the loss minimization model achieves 90.0% labeled constituents F1 score on the standard WSJ parsing task. 1 Introduction The reranking approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems. For structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 2004; Henderson and Titov, 2005). A very different use of loss functions was considered in the areas of signal proce</context>
<context position="5928" citStr="Shen and Joshi, 2003" startWordPosition="947" endWordPosition="950">tive). The third defines a kernel for use with a classification method for minimizing loss. All use previously proposed learning algorithms and optimization criteria. 2 Loss Approximation with a Probabilistic Model In this section we discuss approximating the expected loss using probability estimates given by a baseline probabilistic model. Use of probability estimates is not a serious limitation of this approach because in practice candidates are normally provided by some probabilistic model and its probability estimates are used as additional features in the reranker (Collins and Koo, 2005; Shen and Joshi, 2003; Henderson and Titov, 2005). In order to estimate the expected loss on the basis of a candidate list, we make the assumption that the total probability of the labels not in the candidate list is sufficiently small that the difference δ(x, y0) of expected loss between the labels in the candidate list and the labels not in the candidate list does not have an impact on the loss defined in (1): Ey�∈G(x) P(y|x)A(y, y0) δ(x, y0) = − (3) Ey�G(x) P(y|x) Ey∈G(x) P(y|x)A(y, y0) Ey∈G(x) P(y|x) This gives us the following approximation to the expected loss for the label: E y∈G(x) P (y|x)A(y, y0) l(x, y0)</context>
<context position="14888" citStr="Shen and Joshi, 2003" startWordPosition="2477" endWordPosition="2480">fferent approaches. In the other two sets of experiments, described below, we test our best model on the standard Wall Street Journal parsing benchmark (Collins, 1999) with the Voted Perceptron algorithm as the learner. 5.1 The Probabilistic Models of Parsing To perform the experiments with data-defined kernels, we need to select a probabilistic model of parsing. Data-defined kernels can be applied to any kind of parameterized probabilistic model. For our first set of experiments, we choose to use a publicly available neural network based probabilistic model of parsing (Henderson, 2003). 2In (Shen and Joshi, 2003) it was proposed to use an ensemble of SVMs trained the Wall Street Journal corpus, but the generalization performance of the resulting classifier might be compromised in this approach. (v(x, y, ∂v(x, y, �θ) ), ∂θl eav( ) x, y, 0) ,..., ∂θ1 ∂v(x, y, �θ) ), ∂θl v(x, y, �θ) = log( � P(y0|x,�θ)(1−A(y0,y)))− y&apos;∈G(x) log( � P(y0|x, �θ)A(y0, y)). y&apos;∈G(x) 563 This parsing model is a good candidate for our experiments because it achieves state-of-the-art results on the standard Wall Street Journal (WSJ) parsing problem (Henderson, 2003), and datadefined kernels derived from this parsing model have rec</context>
</contexts>
<marker>Shen, Joshi, 2003</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2003. An SVM based voting algorithm with application to parse reranking. In Proc. of the 7th Conf. on Computational Natural Language Learning, pages 9–16, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Yochai Konig</author>
<author>Mitchel Weintraub</author>
</authors>
<title>Explicit word error minimization in n-best list rescoring.</title>
<date>1997</date>
<booktitle>In Proc. of 5th European Conference on Speech Communication and Technology,</booktitle>
<pages>163--165</pages>
<location>Rhodes, Greece.</location>
<contexts>
<context position="2156" citStr="Stolcke et al., 1997" startWordPosition="315" endWordPosition="319">endency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 2004; Henderson and Titov, 2005). A very different use of loss functions was considered in the areas of signal processing and machine translation, where direct minimization of expected loss (Minimum Bayes Risk decoding) on word sequences was considered (Kumar and Byrne, 2004; Stolcke et al., 1997). The only attempt to use Minimum Bayes Risk (MBR) decoding in parsing was made in (Goodman, 1996), where a parsing algorithm for constituent recall minimization was constructed. However, their approach is limited to binarized PCFG models and, consequently, is not applicable to state-of-the-art parsing methods (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000). In this paper we consider several approaches to loss approximation on the basis of a candidate list provided by a baseline probabilistic model. The intuitive motivation for expected loss minimization can be seen from the follo</context>
<context position="6868" citStr="Stolcke et al., 1997" startWordPosition="1115" endWordPosition="1119">t in the candidate list does not have an impact on the loss defined in (1): Ey�∈G(x) P(y|x)A(y, y0) δ(x, y0) = − (3) Ey�G(x) P(y|x) Ey∈G(x) P(y|x)A(y, y0) Ey∈G(x) P(y|x) This gives us the following approximation to the expected loss for the label: E y∈G(x) P (y|x)A(y, y0) l(x, y0) = E y∈G(x) P(y|x) .(4) � �h(x) = arg min P(x, y|B)A(y, y0), (5) y&apos;∈G(x) y∈G(x) where θ� denotes the parameters of the probabilistic model learned from the training data. This approach for expected loss approximation was considered in the context of word error rate minimization in speech recognition, see for example (Stolcke et al., 1997). 3 Estimating Expected Loss with Discriminative Classifiers In this section we propose a method to improve on the loss approximation used in (5) by constructing the probability estimates using a trained discriminative classifier. Special emphasis is placed on linear classifiers with data-defined kernels for reranking (Henderson and Titov, 2005), because they do not require any additional domain knowledge not already encoded in the probabilistic model, and they have demonstrated significant improvement over the baseline probabilistic model for the parse reranking task. This kernel construction</context>
</contexts>
<marker>Stolcke, Konig, Weintraub, 1997</marker>
<rawString>Andreas Stolcke, Yochai Konig, and Mitchel Weintraub. 1997. Explicit word error minimization in n-best list rescoring. In Proc. of 5th European Conference on Speech Communication and Technology, pages 163–165, Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Dan Klein</author>
<author>Michael Collins</author>
<author>Daphne Koller</author>
<author>Christopher Manning</author>
</authors>
<title>Maxmargin parsing.</title>
<date>2004</date>
<booktitle>In Proc. Conf. on Empirical Methods in Natural Language Processing,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="1862" citStr="Taskar et al., 2004" startWordPosition="267" endWordPosition="270"> approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems. For structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 2004; Henderson and Titov, 2005). A very different use of loss functions was considered in the areas of signal processing and machine translation, where direct minimization of expected loss (Minimum Bayes Risk decoding) on word sequences was considered (Kumar and Byrne, 2004; Stolcke et al., 1997). The only attempt to use Minimum Bayes Risk (MBR) decoding in parsing was made in (Goodman, 1996), where a parsing algorithm for constituent recall minimization was constructed. However, their approach is limited to binarized PCFG models and, consequently, is not applicable to state-of-the-art parsing me</context>
</contexts>
<marker>Taskar, Klein, Collins, Koller, Manning, 2004</marker>
<rawString>Ben Taskar, Dan Klein, Michael Collins, Daphne Koller, and Christopher Manning. 2004. Maxmargin parsing. In Proc. Conf. on Empirical Methods in Natural Language Processing, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>James Henderson</author>
</authors>
<title>Deriving kernels from MLP probability estimators for large categorization problems.</title>
<date>2005</date>
<booktitle>In International Joint Conference on Neural Networks,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="10545" citStr="Titov and Henderson, 2005" startWordPosition="1739" endWordPosition="1742">ogP(x, y0) is not in the strict sense part of the Fisher score, but usually added to kernel features in practice (Henderson and Titov, 2005). y0EG(x)−{y} The TOP reranking kernel has been demonstrated to perform better than the Fisher kernel for the parse reranking task (Henderson and Titov, 2005). The construction of this kernel is motivated by the minimization of the classification error of a linear classifier wT φB(x, y). This linear classifier has been shown to converge, assuming estimation of the discriminative probability in the candidate list can be in the form of the logistic sigmoid (Titov and Henderson, 2005): Py0EG(x) P x, y&apos;) ≈ (10) 1 1 + exp(−w?TφT K θ� (x, y)) for some choice of the decision vector w = w? with the first component equal to one. From this fact, the form of the loss minimizing classifier follows: hTK(x) = (11) X arg min g(A wT φT K θ� (x, y&apos;))A(y, y&apos;), y0EG(x) yEG(x) where g is the logistic sigmoid and the scalar parameter A should be selected on the development set. As for the Fisher kernel, the optimal value of A should be close to 1/ w1. 3.3 Estimates from Arbitrary Classifiers Although in this paper we focus on approaches which do not require additional domain knowledge, the </context>
</contexts>
<marker>Titov, Henderson, 2005</marker>
<rawString>Ivan Titov and James Henderson. 2005. Deriving kernels from MLP probability estimators for large categorization problems. In International Joint Conference on Neural Networks, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thomas Hofmann</author>
<author>Thorsten Joachims</author>
<author>Yasemin Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proc. 21st Int. Conf. on Machine Learning,</booktitle>
<pages>823--830</pages>
<location>Banff, Alberta, Canada.</location>
<contexts>
<context position="1841" citStr="Tsochantaridis et al., 2004" startWordPosition="263" endWordPosition="266"> 1 Introduction The reranking approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems. For structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 2004; Henderson and Titov, 2005). A very different use of loss functions was considered in the areas of signal processing and machine translation, where direct minimization of expected loss (Minimum Bayes Risk decoding) on word sequences was considered (Kumar and Byrne, 2004; Stolcke et al., 1997). The only attempt to use Minimum Bayes Risk (MBR) decoding in parsing was made in (Goodman, 1996), where a parsing algorithm for constituent recall minimization was constructed. However, their approach is limited to binarized PCFG models and, consequently, is not applicable to state-</context>
<context position="4388" citStr="Tsochantaridis et al., 2004" startWordPosition="700" endWordPosition="703">anguage Processing (EMNLP 2006), pages 560–567, Sydney, July 2006. c�2006 Association for Computational Linguistics choosing a member of the group will be smaller than that for the outlier. More formally, the Bayes risk of a model y = h(x) is defined as R(h) = Ex,yA(y, h(x)), (1) where the expectation is taken over all the possible inputs x and labels y and A(y, y0) denotes a loss incurred by assigning x to y0 when the correct label is y. We assume that the loss function possesses values within the range from 0 to 1, which is equivalent to the requirement that the loss function is bounded in (Tsochantaridis et al., 2004). It follows that an optimal reranker h* is one which chooses the label y that minimizes the expected loss: � h*(x) = arg min P(y|x)A(y, y0), (2) y&apos;∈G(x) y where G(x) denotes a candidate list provided by a baseline probabilistic model for the input x. In this paper we propose different approaches to loss approximation. We apply them to the parse reranking problem where the baseline probabilistic model is a neural network parser (Henderson, 2003), and to parse reranking of candidates provided by the (Collins, 1999) model. The resulting reranking method achieves very significant improvement in t</context>
<context position="13749" citStr="Tsochantaridis et al., 2004" startWordPosition="2294" endWordPosition="2297"> so the Loss Logit Kernel is a generalization of the TOP kernel for reranking. The construction of the Loss Logit Kernel, like the TOP kernel for reranking, can be motivated by the minimization of the classification error of a linear classifier wT φLLK � (x, y), where φLLK � (x, y) is the feature extractor of the kernel given by: φLLK � (x,y) = (13) (v(x, y, where 5 Experimental Evaluation To perform empirical evaluations of the proposed methods, we considered the task of parsing the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). First, we perform experiments with SVM Struct (Tsochantaridis et al., 2004) as the learner. Since SVM Struct already uses the loss function during training to rescale the margin or slack variables, this learner allows us to test the hypothesis that loss functions are useful in parsing not only to define the optimization criteria but also to define the classifier and to define the feature space. However, SVM Struct training for large scale parsing experiments is computationally expensive2, so here we use only a small portion of the available training data to perform evaluations of the different approaches. In the other two sets of experiments, described below, we test</context>
<context position="18524" citStr="Tsochantaridis et al., 2004" startWordPosition="3082" endWordPosition="3085">e testing set. lary if it occurred at least 20 time in the training set, which (with tag-unknown-word pairs) led to the very small vocabulary of 271 tag-word pairs. The same model was used both for choosing the list of candidate parses and for the probabilistic model used for loss estimation and kernel feature extraction. For training and testing of the kernel models, we provided a candidate list consisting of the top 20 parses found by the probabilistic model. For the testing set, selecting the candidate with an oracle results in an F1 score of 89.1%. We used the SVM Struct software package (Tsochantaridis et al., 2004) to train the SVM for all the approaches based on discriminative classifier learning, with slack rescaling and linear slack penalty. The loss function is defined as A(y, y&apos;) = 1 − F1(y, y&apos;), where F1 denotes F1 measure on bracketed constituents. This loss was used both for rescaling the slacks in the SVM and for defining our classification models and kernels. We performed initial testing of the models on the validation set and preselected the best model for each of the approaches before testing it on the final testing set. Standard measures of parsing accuracy, plus complete match accuracy, ar</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Ioannis Tsochantaridis, Thomas Hofmann, Thorsten Joachims, and Yasemin Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proc. 21st Int. Conf. on Machine Learning, pages 823–830, Banff, Alberta, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Tsuda</author>
<author>M Kawanabe</author>
<author>G Ratsch</author>
<author>S Sonnenburg</author>
<author>K Muller</author>
</authors>
<title>A new discriminative kernel from probabilistic models.</title>
<date>2002</date>
<journal>Neural Computation,</journal>
<volume>14</volume>
<issue>10</issue>
<contexts>
<context position="8536" citStr="Tsuda et al., 2002" startWordPosition="1379" endWordPosition="1382"> estimates, we can define the classifier 561 3.1 Estimation with Fisher Kernels The Fisher kernel for structured classification is a trivial generalization of one of the best known data-defined kernels for binary classification (Jaakkola and Haussler, 1998). The Fisher score of an example input-label pair (x, y) is a vector of partial derivatives of the log-likelihood of the example with respect to the model parametersl: φF K θ� (x, y) = (6) 3.2 Estimation with TOP Kernels for Reranking The TOP Reranking kernel was defined in (Henderson and Titov, 2005), as a generalization of the TOP kernel (Tsuda et al., 2002) proposed for binary classification tasks. The feature extractor for the TOP reranking kernel is given by: φT K θ� (x, y) = (9) e), ∂v(x, y, 0) ,..., ∂θ1 (v(x, y, ∂v(x, y, �θ) ), ∂θl (logP(x, y |�θ), ∂logP (x,y|�θ) ∂logP(x,y|�θ) where P(x, y&apos;|�θ). ,..., ). X ∂θ1 ∂θl v(x, y, B) = log P(x, y|�θ) − log This kernel defines a feature space which is appropriate for estimating the discriminative probability in the candidate list in the form of a normalized exponential Py0EG(x) exp(w?TφF K θ� (x, y&apos;)) for some choice of the decision vector w = w? with the first component equal to one. It follows that </context>
</contexts>
<marker>Tsuda, Kawanabe, Ratsch, Sonnenburg, Muller, 2002</marker>
<rawString>K. Tsuda, M. Kawanabe, G. Ratsch, S. Sonnenburg, and K. Muller. 2002. A new discriminative kernel from probabilistic models. Neural Computation, 14(10):2397–2414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Yeh</author>
</authors>
<title>More accurate tests for the statistical significance of the result differences.</title>
<date>2000</date>
<booktitle>In Proc. 17th International Conf. on Computational Linguistics,</booktitle>
<pages>947--953</pages>
<location>Saarbruken, Germany.</location>
<contexts>
<context position="22147" citStr="Yeh, 2000" startWordPosition="3670" endWordPosition="3671">t the loss approximation reranking approaches proposed in this paper demonstrate significant improvement over the baseline models, achieving about the same relative error reduction as previously achieved with data-defined kernels (Henderson and Titov, 2005). This improvement is despite the fact that the loss function is already used in the definition of the training criteria for all the models except SSN. It is also interesting to note that the best result on the validation set for estimation 4We measured significance of all the experiments in this paper with the randomized significance test (Yeh, 2000). of the loss with data-defined kernels (12) and (13) was achieved when the parameter A is close to the inverse of the first component of the learned decision vector, which confirms the motivation for these kernels. 5.3 Experiments with Voted Perceptron and Data-Defined Kernels The above experiments with the SVM Struct demonstrate empirically the viability of our approaches. The aim of experiments on the entire WSJ is to test whether our approaches still achieve significant improvement when more accurate generative models are used, and also to show that they generalize well to learning methods</context>
</contexts>
<marker>Yeh, 2000</marker>
<rawString>Alexander Yeh. 2000. More accurate tests for the statistical significance of the result differences. In Proc. 17th International Conf. on Computational Linguistics, pages 947–953, Saarbruken, Germany.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>