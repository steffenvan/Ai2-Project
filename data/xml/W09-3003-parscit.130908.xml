<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011315">
<title confidence="0.994221">
Assessing the benefits of partial automatic pre-labeling for frame-semantic
annotation
</title>
<author confidence="0.971266">
Ines Rehbein and Josef Ruppenhofer and Caroline Sporleder
</author>
<affiliation confidence="0.974821">
Computational Linguistics
Saarland University
</affiliation>
<email confidence="0.997918">
{rehbein,josefr,csporled}@coli.uni-sb.de
</email>
<sectionHeader confidence="0.994775" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999869285714286">
In this paper, we present the results of an
experiment in which we assess the useful-
ness of partial semi-automatic annotation
for frame labeling. While we found no con-
clusive evidence that it can speed up human
annotation, automatic pre-annotation does
increase its overall quality.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960923076923">
Linguistically annotated resources play a crucial
role in natural language processing. Many recent
advances in areas such as part-of-speech tagging,
parsing, co-reference resolution, and semantic role
labeling have only been possible because of the cre-
ation of manually annotated corpora, which then
serve as training data for machine-learning based
NLP tools. However, human annotation of linguis-
tic categories is time-consuming and expensive.
While this is already a problem for major languages
like English, it is an even bigger problem for less-
used languages.
This data acquisition bottleneck is a well-known
problem and there have been numerous efforts to
address it on the algorithmic side. Examples in-
clude the development of weakly supervised learn-
ing methods such as co-training and active learning.
However, addressing only the algorithmic side is
not always possible and not always desirable in all
scenarios. First, some machine learning solutions
are not as generally applicable or widely re-usable
as one might think. It has been shown, for example,
that co-training does not work well for problems
which cannot easily be factorized into two indepen-
dent views (Mueller et al., 2002; Ng and Cardie,
2003). Some active learning studies suggest both
that the utility of the selected examples strongly
depends on the model used for classification and
that the example pool selected for one model can
turn out to be sub-optimal when another model is
trained on it at a later stage (Baldridge and Os-
borne, 2004). Furthermore, there are a number of
scenarios for which there is simply no alternative
to high-quality, manually annotated data; for exam-
ple, if the annotated corpus is used for empirical
research in linguistics (Meurers and M¨uller, 2007;
Meurers, 2005).
In this paper, we look at this problem from the
data creation side. Specifically we explore whether
a semi-automatic annotation set-up in which a hu-
man expert corrects the output of an automatic sys-
tem can help to speed up the annotation process
without sacrificing annotation quality.
For our study, we explore the task of frame-
semantic argument structure annotation (Baker et
al., 1998). We chose this particular task because it
is a rather complex – and therefore time-consuming
– undertaking, and it involves making a number of
different but interdependent annotation decisions
for each instance to be labeled (e.g. frame as-
signment and labeling of frame elements, see Sec-
tion 3.1). Semi-automatic support would thus be of
real benefit.
More specifically, we explore the usefulness of
automatic pre-annotation for the first step in the an-
notation process, namely frame assignment (word
sense disambiguation). Since the available inven-
tory of frame elements is dependent on the cho-
sen frame, this step is crucial for the whole anno-
tation process. Furthermore, semi-automatic an-
notation is more feasible for the frame labeling
sub-task. Most automatic semantic role labeling
systems (ASRL), including ours, tend to perform
much better on frame assignment than on frame
role labeling and correcting an erroneously chosen
</bodyText>
<page confidence="0.992388">
19
</page>
<note confidence="0.996805">
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 19–26,
Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999938888888889">
frame typically also requires fewer physical opera-
tions from the annotator than correcting a number
of wrongly assigned frame elements.
We aim to answer three research questions in our
study: First, we explore whether pre-annotation of
frame labels can indeed speed up the annotation
process. This question is important because frame
assignment, in terms of physical operations of the
annotator, is a relatively minor effort compared to
frame role assignment and because checking a pre-
annotated frame still involves all the usual men-
tal operations that annotation from scratch does.
Our second major question is whether annotation
quality would remain acceptably high. Here the
concern is that annotators might tend to simply go
along with the pre-annotation, which would lead to
an overall lower annotation quality than they could
produce by annotating from scratch.1 Depending
on the purpose for which the annotations are to be
used, trading off accuracy for speed may or may
not be acceptable. Our third research question con-
cerns the required quality of pre-annotation for it
to have any positive effect. If the quality is too low,
the annotation process might actually be slowed
down because annotations by the automatic system
would have to be deleted before the new correct
one could be made. In fact, annotators might ig-
nore the pre-annotations completely. To determine
the effect of the pre-annotation quality, we not only
compared a null condition of providing no prior
annotation to one where we did, but we in fact com-
pared the null condition to two different quality
levels of pre-annotation, one that reflects the per-
formance of a state-of-the-art ASRL system and
an enhanced one that we artificially produced from
the gold standard.
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9995863">
While semi-automatic annotation is frequently em-
ployed to create labeled data more quickly (see,
e.g., Brants and Plaehn (2000)), there are compar-
atively few studies which systematically look at
the benefits or limitations of this approach. One
of the earliest studies that investigated the advan-
tages of manually correcting automatic annotations
for linguistic data was carried out by Marcus et
al. (1993) in the context of the construction of the
Penn Treebank. Marcus et al. (1993) employed
</bodyText>
<footnote confidence="0.988311333333333">
1This problem is also known in the context of resources
that are collaboratively constructed via the web (Kruschwitz
et al., 2009)
</footnote>
<bodyText confidence="0.999878138888889">
a post-correction set-up for both part-of-speech
and syntactic structure annotation. For pos-tagging
they compared the semi-automatic approach to a
fully manual annotation. They found that the semi-
automatic method resulted both in a significant
reduction of annotation time, effectively doubling
the word annotation rate, and in increased inter-
annotator agreement and accuracy.
Chiou et al. (2001) explored the effect of au-
tomatic pre-annotation for treebank construction.
For the automatic step, they experimented with two
different parsers and found that both reduce over-
all annotation time significantly while preserving
accuracy. Later experiments by Xue et al. (2002)
confirmed these findings.
Ganchev et al. (2007) looked at semi-automatic
gene identification in the biomedical domain. They,
too, experimented with correcting the output of an
automatic annotation system. However, rather than
employing an off-the-shelf named entity tagger,
they trained a tagger maximized for recall. The
human annotators were then instructed to filter the
annotation, rejecting falsely labeled expressions.
Ganchev et al. (2007) report a noticeable increase
in speed compared to a fully manual set-up.
The approach that is closest to ours is that of
Chou et al. (2006) who investigate the effect of au-
tomatic pre-annotation for Propbank-style semantic
argument structure labeling. However that study
only looks into the properties of the semi-automatic
set-up; the authors did not carry out a control study
with a fully manual approach. Nevertheless Chou
et al. (2006) provide an upper bound of the savings
obtained by the semi-automatic process in terms
of annotator operations. They report a reduction in
annotation effort of up to 46%.
</bodyText>
<sectionHeader confidence="0.999145" genericHeader="method">
3 Experimental setup
</sectionHeader>
<subsectionHeader confidence="0.999633">
3.1 Frame-Semantic Annotation
</subsectionHeader>
<bodyText confidence="0.999957083333333">
The annotation scheme we use is that of FrameNet
(FN), a lexicographic project that produces a
database of frame-semantic descriptions of English
vocabulary. Frames are representations of proto-
typical events or states and their participants in the
sense of Fillmore (1982). In the FN database, both
frames and their participant roles are arranged in
various hierarchical relations (most prominently,
the is-a relation).
FrameNet links these descriptions of frames with
the words and multi-words (lexical units, LUs) that
evoke these conceptual structures. It also docu-
</bodyText>
<page confidence="0.967656">
20
</page>
<bodyText confidence="0.986936444444444">
ments all the ways in which the semantic roles
(frame elements, FEs) can be realized as syntactic
arguments of each frame-evoking word by labeling
corpus attestations. As a small example, consider
the Collaboration frame, evoked in English by lexi-
cal units such as collaborate.v, conspire.v, collabo-
rator.n and others. The core set of frame-specific
roles that apply include Partneri, Partnere, Partners
and Undertaking. A labeled example sentence is
</bodyText>
<listItem confidence="0.626592">
(1) [The two researchers Partners] COLLAB-
</listItem>
<bodyText confidence="0.971709571428572">
ORATED [on many papers Undertaking].
FrameNet uses two modes of annotation: full-
text, where the goal is to exhaustively annotate
the running text of a document with all the differ-
ent frames and roles that occur, and lexicographic,
where only instances of particular target words used
in particular frames are labeled.
</bodyText>
<subsectionHeader confidence="0.999878">
3.2 Pilot Study
</subsectionHeader>
<bodyText confidence="0.99999409375">
Prior to the present study we carried out a pilot
experiment comparing manual and semi-automatic
annotation of different segments of running text.
In this experiment we saw no significant effect
from pre-annotation. Instead we found that the
annotation speed and accuracy depended largely
on the order in which the texts were annotated and
on the difficulty of the segments. The influence
of order is due to the fact that FrameNet has more
than 825 frames and each frame has around two to
five core frame elements plus a number of non-core
elements. Therefore even experienced annotators
can benefit from the re-occuring of frames during
the ongoing annotation process.
Drawing on our experiences with the first exper-
iment, we chose a different experimental set-up for
the present study. To reduce the training effect, we
opted for annotation in lexicographic mode, restrict-
ing the number of lemmas (and thereby frames)
to annotate, and we started the experiment with
a training phase (see Section 3.5). Annotating in
lexicographic mode also gave us better control over
the difficulty of the different batches of data. Since
these now consist of unrelated sentences, we can
control the distribution of lemmas across the seg-
ments (see Section 3.4).
Furthermore, since the annotators in our pi-
lot study had often ignored the error-prone pre-
annotation, in particular for frame elements, we de-
cided not to pre-annotate frame elements and to ex-
periment with an enhanced level of pre-annotation
to explore the effect of pre-annotation quality.
</bodyText>
<subsectionHeader confidence="0.999582">
3.3 Annotation Set-Up
</subsectionHeader>
<bodyText confidence="0.999995823529412">
The annotators included the authors and three com-
putational linguistics undergraduates who have
been performing frame-semantic annotation for at
least one year. While we use FrameNet data, our
annotation set-up is different. The annotation con-
sists of decorating automatically derived syntactic
constituency trees with semantic role labels using
the Salto tool (Burchardt et al., 2006) (see Figure 1).
By contrast, in FrameNet annotation a chunk parser
is used to provide phrase type and grammatical rela-
tions for the arguments of the target words. Further,
FrameNet annotators need to correct mistakes of
the automatic grammatical analysis, unlike in our
experiment. The first annotation step, frame as-
signment, involves choosing the correct frame for
the target lemma from a pull down menu; the sec-
ond step, role assignment, requires the annotators
to draw the available frame element links to the
appropriate syntactic constituent(s).
The annotators performed their annotation on
computers where access to the FrameNet website,
where gold annotations could have been found, was
blocked. They did, however, have access to local
copies of the frame descriptions needed for the
lexical units in our experiment. As the overall time
needed for the annotation was too long to do in
one sitting, the annotators did it over several days.
They were instructed to record the time (in minutes)
that they took for the annotation of each annotation
session.
Our ASRL system for state-of-the-art pre-
annotation was Shalmaneser (Erk and Pado, 2006).
The enhanced pre-annotation was created by man-
ually inserting errors into the gold standard.
</bodyText>
<subsectionHeader confidence="0.949062">
3.4 Data
</subsectionHeader>
<bodyText confidence="0.999992714285714">
We annotated 360 sentences exemplifying all the
senses that were defined for six different lemmas in
FrameNet release 1.3. The lemmas were the verbs
rush, look, follow, throw, feel and scream. These
verbs were chosen for three reasons. First, they
have enough annotated instances in the FN release
that we could use some instances for testing and
still be left with a set of instances sufficiently large
to train our ASRL system. Second,we knew from
prior work with our automatic role labeler that it
had a reasonably good performance on these lem-
mas. Third, these LUs exhibit a range of difficulty
in terms of the number of senses they have in FN
(see Table 1) and the subtlety of the sense distinc-
</bodyText>
<page confidence="0.998716">
21
</page>
<figureCaption confidence="0.999747">
Figure 1: The Salto Annotation Tool
</figureCaption>
<table confidence="0.999029428571429">
Instances Senses
feel 134 6
follow 113 3
look 185 4
rush 168 2
scream 148 2
throw 155 2
</table>
<tableCaption confidence="0.999851">
Table 1: Lemmas used
</tableCaption>
<bodyText confidence="0.99995">
tions – e.g. the FrameNet senses of look are harder
to distinguish than those of rush. We randomly
grouped our sentences into three batches of equal
size and for each batch we produced three versions
corresponding to our three levels of annotation.
</bodyText>
<subsectionHeader confidence="0.947326">
3.5 Study design
</subsectionHeader>
<bodyText confidence="0.999672789473684">
In line with the research questions that we want
to address and the annotators that we have avail-
able, we choose an experimental design that is
amenable to an analysis of variance. Specifically,
we randomly assign our 6 annotators (1-6) to three
groups of two (Groups I-III). Each annotator expe-
riences all three annotation conditions, namely no
pre-annotation (N), state-of-the-art pre-annotation
(S), and enhanced pre-annotation (E). This is the
within-subjects factor in our design, all other fac-
tors are between subjects. Namely, each group was
randomly matched to one of three different orders
in which the conditions can be experienced (see
Table 2). The orderings are designed to control
for the effects that increasing experience may have
on speed and quality. While all annotators end up
labeling all the same data, the groups also differ
as to which batch of data is presented in which
condition. This is intended as a check on any inher-
</bodyText>
<table confidence="0.9780075">
1st 2nd 3rd Annotators
Group I E S N 5, 6
Group II S N E 2, 4
Group III N E S 1, 3
</table>
<tableCaption confidence="0.998867">
Table 2: Annotation condition by order and group
</tableCaption>
<bodyText confidence="0.999026818181818">
ent differences in annotation difficulty that might
exist between the data sets. Finally, to rule out
difficulties with unfamiliar frames and frame el-
ements needed for the lexical units used in this
study, we provided some training to the annota-
tors. In the week prior to the experiment, they were
given 240 sentences exemplifying all 6 verbs in all
their senses to annotate and then met to discuss any
questions they might have about frame or FE dis-
tinctions etc. These 240 sentences were also used
to train the ASRL system.
</bodyText>
<sectionHeader confidence="0.999954" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.9999703">
In addition to time, we measured precision, recall
and f-score for frame assignment and semantic role
assignment for each annotator. We then performed
an analysis of variance (ANOVA) on the outcomes
of our experiment. Our basic results are presented
in Table 3. As can be seen and as we expected,
our annotators differed in their performance both
with regard to annotation quality and speed. Below
we discuss our results with respect to the research
questions named above.
</bodyText>
<subsectionHeader confidence="0.872808">
4.1 Can pre-annotation of frame assignment
</subsectionHeader>
<bodyText confidence="0.852820333333333">
speed up the annotation process?
Not surprisingly, there are considerable differences
in speed between the six annotators (Table 3),
</bodyText>
<page confidence="0.997683">
22
</page>
<table confidence="0.99998436">
Precision Recall F t p
Annotator 1
94/103 91.3 94/109 86.2 88.68 75 N
99/107 92.5 99/112 88.4 90.40 61 E
105/111 94.6 105/109 96.3 95.44 65 S
Annotator 2
93/105 88.6 93/112 83.0 85.71 135 S
86/98 87.8 86/112 76.8 81.93 103 N
98/106 92.5 98/113 86.7 89.51 69 E
Annotator 3
95/107 88.8 95/112 84.8 86.75 168 N
103/110 93.6 103/112 92.0 92.79 94 E
99/113 87.6 99/113 87.6 87.60 117 S
Annotator 4
106/111 95.5 106/112 94.6 95.05 80 S
99/108 91.7 99/113 87.6 89.60 59 N
105/112 93.8 105/113 92.9 93.35 52 E
Annotator 5
104/110 94.5 (104/112) 92.9 93.69 170 E
91/103 88.3 (91/113) 80.5 84.22 105 S
96/100 96.0 (96/113) 85.0 90.17 105 N
Annotator 6
102/106 96.2 102/112 91.1 93.58 124 E
94/105 89.5 94/112 83.9 86.61 125 S
93/100 93.0 93/113 82.3 87.32 135 N
</table>
<tableCaption confidence="0.992014">
Table 3: Results for frame assignment: precision,
</tableCaption>
<bodyText confidence="0.96992941025641">
recall, f-score (F), time (t) (frame and role as-
signment), pre-annotation (p): Non, Enhanced,
Shalmaneser
which are statistically significant with p &lt; 0.05.
Focussing on the order in which the text segments
were given to the annotators, we observe a sig-
nificant difference (p &lt; 0.05) in annotation time
needed for each of the segments. With one ex-
ception, all annotators took the most time on the
text segment given to them first, which hints at an
ongoing training effect.
The different conditions of pre-annotation (none,
state-of-the-art, enhanced) did not have a signifi-
cant effect on annotation time. However, all anno-
tators except one were in fact faster under the en-
hanced condition than under the unannotated con-
dition. The one annotator who was not faster anno-
tated the segment with the enhanced pre-annotation
before the other two segments; hence there might
have been an interaction between time savings from
pre-annotation and time savings due to a training
effect. This interaction between training effect and
degree of pre-annotation might be one reason why
we do not find a significant effect between anno-
tation time and pre-annotation condition. Another
reason might be that the pre-annotation only re-
duces the physical effort needed to annotate the
correct frame which is relatively minor compared
to the cognitive effort of determining (or verifying)
the right frame, which is required for all degrees of
pre-annotation.
4.2 Is annotation quality influenced by
automatic pre-annotation?
To answer the second question, we looked at the
relation between pre-annotation condition and f-
score. Even though the results in f-score for the
different annotators vary in extent (Table 4), there is
no significant difference between annotation qual-
ity for the six annotators.
</bodyText>
<table confidence="0.911065">
Anot1 Anot2 Anot3 Anot4 Anot5 Anot6
91.5 85.7 89.0 92.7 89.4 89.2
</table>
<tableCaption confidence="0.999189">
Table 4: Average f-score for the 6 annotators
</tableCaption>
<bodyText confidence="0.999360583333333">
Next we performed a two-way ANOVA (Within-
Subjects design), and crossed the dependent vari-
able (f-score) with the two independent vari-
ables (order of text segments, condition of pre-
annotation). Here we found a significant effect
(p &lt; 0.05) for the impact of pre-annotation on an-
notation quality. All annotators achieved higher
f-scores for frame assignment on the enhanced pre-
annotated text segments than on the ones with no
pre-annotation. With one exception, all annotators
also improved on the already high baseline for the
enhanced pre-annotation (Table 5).
</bodyText>
<table confidence="0.999685111111111">
Seg. Precision Recall f-score
Shalmaneser
A (70/112) 62.5 (70/96) 72.9 67.30
B (75/113) 66.4 (75/101) 74.3 70.13
C (66/113) 58.4 (66/98) 67.3 62.53
Enhanced Pre-Annotation
A (104/112) 92.9 (104/111) 93.7 93.30
B (103/112) 92.0 (103/112) 92.0 92.00
C (99/113) 87.6 (99/113) 87.6 87.60
</table>
<tableCaption confidence="0.866906">
Table 5: Baselines for automatic pre-annotation
(Shalmaneser) and enhanced pre-annotation
</tableCaption>
<bodyText confidence="0.999619076923077">
The next issue concerns the question of whether
annotators make different types of errors when pro-
vided with the different styles of pre-annotation.
We would like to know if erroneous frame assign-
ment, as done by a state-of-the-art ASRL will tempt
annotators to accept errors they would not make in
the first place. To investigate this issue, we com-
pared f-scores for each of the frames for all three
pre-annotation conditions with f-scores for frame
assignment achieved by Shalmaneser. The boxplot
in Figure 2 shows the distribution of f-scores for
each frame for the different pre-annotation styles
and for Shalmaneser. We can see that the same
</bodyText>
<page confidence="0.998501">
23
</page>
<figureCaption confidence="0.942341">
Figure 2: F-Scores per frame for human annotators on different levels of pre-annotation and for Shal-
maneser
</figureCaption>
<bodyText confidence="0.999442538461539">
error types are made by human annotators through-
out all three annotation trials, and that these errors
are different from the ones made by the ASRL.
Indicated by f-score, the most difficult frames
in our data set are Scrutiny, Fluidic motion, Seek-
ing, Make noise and Communication noise. This
shows that automatic pre-annotation, even if noisy
and of low quality, does not corrupt human anno-
tators on a grand scale. Furthermore, if the pre-
annotation is good it can even improve the overall
annotation quality. This is in line with previous
studies for other annotation tasks (Marcus et al.,
1993).
</bodyText>
<subsectionHeader confidence="0.797399">
4.3 How good does pre-annotation need to be
to have a positive effect?
</subsectionHeader>
<bodyText confidence="0.999585527777778">
Comparing annotation quality on the automatically
pre-annotated texts using Shalmaneser, four out of
six annotators achieved a higher f-score than on the
non-annotated sentences. The effect, however, is
not statistically significant. This means that pre-
annotation produced by a state-of-the-art ASRL
system is not yet good enough a) to significantly
speed up the annotation process, and b) to improve
the quality of the annotation itself. On the positive
side, we also found no evidence that the error-prone
pre-annotation decreases annotation quality.
Most interestingly, the two annotators who
showed a decrease in f-score on the text segments
pre-annotated by Shalmaneser (compared to the
text segments with no pre-annotation provided)
had been assigned to the same group (Group I).
Both had first annotated the enhanced, high-quality
pre-annotation, in the second trial the sentences
pre-annotated by Shalmaneser, and finally the texts
with no pre-annotation. It might be possible that
they benefitted from the ongoing training, resulting
in a higher f-score for the third text segment (no
pre-annotation). For this reason, we excluded their
annotation results from the data set and performed
another ANOVA, considering the remaining four
annotators only.
Figure 3 illustrates a noticeable trend for the in-
teraction between pre-annotation and annotation
quality: all four annotators show a decrease in
annotation quality on the text segments without
pre-annotation, while both types of pre-annotation
(Shalmaneser, Enhanced) increase f-scores for hu-
man annotation. There are, however, differences
between the impact of the two pre-annotation types
on human annotation quality: two annotators show
better results on the enhanced, high-quality pre-
</bodyText>
<page confidence="0.998474">
24
</page>
<figureCaption confidence="0.95961925">
Figure 3: Interaction between pre-annotation and
f-score
Figure 4: Interaction between pre-annotation and
time
</figureCaption>
<bodyText confidence="0.999454428571429">
annotation, the other two perform better on the
texts pre-annotated by the state-of-the-art ASRL.
The interaction between pre-annotation and f-score
computed for the four annotators is weakly signifi-
cant with p &lt; 0.1.
Next we investigated the influence of pre-
annotation style on annotation time for the four
annotators. Again we can see an interesting pat-
tern: The two annotators (A1, A3) who annotated
in the order N-E-S, both take most time for the
texts without pre-annotation, getting faster on the
text pre-processed by Shalmaneser, while the least
amount of time was needed for the enhanced pre-
annotated texts (Figure 4). The two annotators (A2,
A4) who processed the texts in the order S-N-E,
showed a continuous reduction in annotation time,
probably caused by the interaction of training and
data quality. These observations, however, should
be taken with a grain of salt, as they outline trends,
but due to the low number of annotators, could not
be substantiated by statistical tests.
</bodyText>
<subsectionHeader confidence="0.987053">
4.4 Semantic Role Assignment
</subsectionHeader>
<bodyText confidence="0.999499066666667">
As described in Section 3.5, we provided pre-
annotation for frame assignment only, therefore
we did not expect any significant effects of the dif-
ferent conditions of pre-annotation on the task of
semantic role labeling. To allow for a meaningful
comparison, the evaluation of semantic role assign-
ment was done on the subset of frames annotated
correctly by all annotators.
As with frame assignment, there are consid-
erable differences in annotation quality between
the annotators. In contrast to frame assignment,
here the differences are statistically significant
(p &lt; 0.05). Table 6 shows the average f-score
for each annotator on the semantic role assignment
task.
</bodyText>
<table confidence="0.8900395">
Anot1 Anot2 Anot3 Anot4 Anot5 Anot6
85.2 80.1 87.7 89.2 82.5 84.3
</table>
<tableCaption confidence="0.996173">
Table 6: Average f-scores for the 6 annotators
</tableCaption>
<bodyText confidence="0.99933675">
As expected, neither the condition of pre-
annotation nor the order of text segments had any
significant effect on the quality of semantic role
assignment.2
</bodyText>
<sectionHeader confidence="0.983749" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.847158142857143">
In the paper we presented experiments to assess
the benefits of partial automatic pre-annotation on
a frame assignment (word sense disambiguation)
task. We compared the impact of a) pre-annotations
2The annotation of frame and role assignment was done as
a combined task, therefore we do not report separate results
for annotation time for semantic role assignment.
</bodyText>
<page confidence="0.994776">
25
</page>
<bodyText confidence="0.999980510638298">
provided by a state-of-the-art ASRL, and b) en-
hanced, high-quality pre-annotation on the annota-
tion process. We showed that pre-annotation has
a positive effect on the quality of human annota-
tion: the enhanced pre-annotation clearly increased
f-scores for all annotators, and even the noisy, error-
prone pre-annotations provided by the ASRL sys-
tem did not lower the quality of human annotation.
We suspect that there is a strong interaction
between the order in which the text segments
are given to the annotators and the three annota-
tion conditions, resulting in lower f-scores for the
group of annotators who processed the ASRL pre-
annotations in the first trial, where they could not
yet profit from the same amount of training as the
other two groups.
The same problem occurs with annotation time.
We have not been able to show that automatic
pre-annotation speeds up the annotation process.
However, we suspect that here, too, the interaction
between training effect and annotation condition
made it difficult to reach a significant improve-
ment. One way to avoid the problem would be a
further split of the test data, so that the different
types of pre-annotation could be presented to the
annotators at different stages of the annotation pro-
cess. This would allow us to control for the strong
bias through incremental training, which we can-
not avoid if one group of annotators is assigned
data of a given pre-annotation type in the first trial,
while another group encounters the same type of
data in the last trial. Due to the limited number
of annotators we had at our disposal as well as
the amount of time needed for the experiments we
could not sort out the interaction between order
and annotation conditions. We will take this issue
up in future work, which also needs to address the
question of how good the automatic pre-annotation
should be to support human annotation. F-scores
for the enhanced pre-annotation provided in our
experiments were quite high, but it is possible that
a similar effect could be reached with automatic
pre-annotations of somewhat lower quality.
The outcome of our experiments provides strong
motivation to improve ASRL systems, as automatic
pre-annotation of acceptable quality does increase
the quality of human annotation.
</bodyText>
<sectionHeader confidence="0.99893" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999897266666667">
C. F. Baker, C. J. Fillmore, J. B. Lowe. 1998. The
berkeley framenet project. In Proceedings of the
17th international conference on Computational lin-
guistics, 86–90, Morristown, NJ, USA. Association
for Computational Linguistics.
J. Baldridge, M. Osborne. 2004. Active learning
and the total cost of annotation. In Proceedings of
EMNLP.
T. Brants, O. Plaehn. 2000. Interactive corpus annota-
tion. In Proceedings of LREC-2000.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pad´o.
2006. SALTO – a versatile multi-level annotation
tool. In Proceedings of LREC.
F.-D. Chiou, D. Chiang, M. Palmer. 2001. Facilitat-
ing treebank annotation using a statistical parser. In
Proceedings of HLT-2001.
W.-C. Chou, R. T.-H. Tsai, Y.-S. Su, W. Ku, T.-Y. Sung,
W.-L. Hsu. 2006. A semi-automatic method for an-
notation a biomedical proposition bank. In Proceed-
ings of FLAC-2006.
K. Erk, S. Pado. 2006. Shalmaneser - a flexible tool-
box for semantic role assignment. In Proceedings of
LREC, Genoa, Italy.
C. J. F. C. J. Fillmore, 1982. Linguistics in the Morning
Calm, chapter Frame Semantics, 111–137. Hanshin
Publishing, Seoul, 1982.
K. Ganchev, F. Pereira, M. Mandel, S. Carroll, P. White.
2007. Semi-automated named entity annotation.
In Proceedings of the Linguistic Annotation Work-
shop, 53–56, Prague, Czech Republic. Association
for Computational Linguistics.
U. Kruschwitz, J. Chamberlain, M. Poesio. 2009. (lin-
guistic) science through web collaboration in the
ANAWIKI project. In Proceedings of WebSci’09.
M. P. Marcus, B. Santorini, M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313–330.
W. D. Meurers, S. M¨uller. 2007. Corpora and syntax
(article 44). In A. L¨udeling, M. Kyt¨o, eds., Corpus
linguistics. Mouton de Gruyter, Berlin.
W. D. Meurers. 2005. On the use of electronic cor-
pora for theoretical linguistics. case studies from
the syntax of german. Lingua, 115(11):1619–
1639. http://purl.org/net/dm/papers/
meurers-03.html.
C. Mueller, S. Rapp, M. Strube. 2002. Applying co-
training to reference resolution. In Proceedings of
40th Annual Meeting of the Association for Com-
putational Linguistics, 352–359, Philadelphia, Penn-
sylvania, USA. Association for Computational Lin-
guistics.
V. Ng, C. Cardie. 2003. Bootstrapping corefer-
ence classifiers with multiple machine learning algo-
rithms. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Processing
(EMNLP-2003).
N. Xue, F.-D. Chiou, M. Palmer. 2002. Building a
large-scale annotated chinese corpus. In Proceed-
ings of Coling-2002.
</reference>
<page confidence="0.998052">
26
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.422225">
<title confidence="0.996351">Assessing the benefits of partial automatic pre-labeling for frame-semantic annotation</title>
<author confidence="0.826741">Rehbein Ruppenhofer</author>
<affiliation confidence="0.759623">Computational</affiliation>
<address confidence="0.483612">Saarland</address>
<abstract confidence="0.998389625">In this paper, we present the results of an experiment in which we assess the usefulness of partial semi-automatic annotation for frame labeling. While we found no conclusive evidence that it can speed up human annotation, automatic pre-annotation does increase its overall quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C F Baker</author>
<author>C J Fillmore</author>
<author>J B Lowe</author>
</authors>
<title>The berkeley framenet project.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics, 86–90,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2703" citStr="Baker et al., 1998" startWordPosition="409" endWordPosition="412"> number of scenarios for which there is simply no alternative to high-quality, manually annotated data; for example, if the annotated corpus is used for empirical research in linguistics (Meurers and M¨uller, 2007; Meurers, 2005). In this paper, we look at this problem from the data creation side. Specifically we explore whether a semi-automatic annotation set-up in which a human expert corrects the output of an automatic system can help to speed up the annotation process without sacrificing annotation quality. For our study, we explore the task of framesemantic argument structure annotation (Baker et al., 1998). We chose this particular task because it is a rather complex – and therefore time-consuming – undertaking, and it involves making a number of different but interdependent annotation decisions for each instance to be labeled (e.g. frame assignment and labeling of frame elements, see Section 3.1). Semi-automatic support would thus be of real benefit. More specifically, we explore the usefulness of automatic pre-annotation for the first step in the annotation process, namely frame assignment (word sense disambiguation). Since the available inventory of frame elements is dependent on the chosen </context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>C. F. Baker, C. J. Fillmore, J. B. Lowe. 1998. The berkeley framenet project. In Proceedings of the 17th international conference on Computational linguistics, 86–90, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Baldridge</author>
<author>M Osborne</author>
</authors>
<title>Active learning and the total cost of annotation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2058" citStr="Baldridge and Osborne, 2004" startWordPosition="305" endWordPosition="309">s desirable in all scenarios. First, some machine learning solutions are not as generally applicable or widely re-usable as one might think. It has been shown, for example, that co-training does not work well for problems which cannot easily be factorized into two independent views (Mueller et al., 2002; Ng and Cardie, 2003). Some active learning studies suggest both that the utility of the selected examples strongly depends on the model used for classification and that the example pool selected for one model can turn out to be sub-optimal when another model is trained on it at a later stage (Baldridge and Osborne, 2004). Furthermore, there are a number of scenarios for which there is simply no alternative to high-quality, manually annotated data; for example, if the annotated corpus is used for empirical research in linguistics (Meurers and M¨uller, 2007; Meurers, 2005). In this paper, we look at this problem from the data creation side. Specifically we explore whether a semi-automatic annotation set-up in which a human expert corrects the output of an automatic system can help to speed up the annotation process without sacrificing annotation quality. For our study, we explore the task of framesemantic argum</context>
</contexts>
<marker>Baldridge, Osborne, 2004</marker>
<rawString>J. Baldridge, M. Osborne. 2004. Active learning and the total cost of annotation. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Brants</author>
<author>O Plaehn</author>
</authors>
<title>Interactive corpus annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC-2000.</booktitle>
<contexts>
<context position="5674" citStr="Brants and Plaehn (2000)" startWordPosition="877" endWordPosition="880">he new correct one could be made. In fact, annotators might ignore the pre-annotations completely. To determine the effect of the pre-annotation quality, we not only compared a null condition of providing no prior annotation to one where we did, but we in fact compared the null condition to two different quality levels of pre-annotation, one that reflects the performance of a state-of-the-art ASRL system and an enhanced one that we artificially produced from the gold standard. 2 Related Work While semi-automatic annotation is frequently employed to create labeled data more quickly (see, e.g., Brants and Plaehn (2000)), there are comparatively few studies which systematically look at the benefits or limitations of this approach. One of the earliest studies that investigated the advantages of manually correcting automatic annotations for linguistic data was carried out by Marcus et al. (1993) in the context of the construction of the Penn Treebank. Marcus et al. (1993) employed 1This problem is also known in the context of resources that are collaboratively constructed via the web (Kruschwitz et al., 2009) a post-correction set-up for both part-of-speech and syntactic structure annotation. For pos-tagging t</context>
</contexts>
<marker>Brants, Plaehn, 2000</marker>
<rawString>T. Brants, O. Plaehn. 2000. Interactive corpus annotation. In Proceedings of LREC-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Burchardt</author>
<author>K Erk</author>
<author>A Frank</author>
<author>A Kowalski</author>
<author>S Pad´o</author>
</authors>
<title>SALTO – a versatile multi-level annotation tool.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC.</booktitle>
<marker>Burchardt, Erk, Frank, Kowalski, Pad´o, 2006</marker>
<rawString>A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pad´o. 2006. SALTO – a versatile multi-level annotation tool. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F-D Chiou</author>
<author>D Chiang</author>
<author>M Palmer</author>
</authors>
<title>Facilitating treebank annotation using a statistical parser.</title>
<date>2001</date>
<booktitle>In Proceedings of HLT-2001.</booktitle>
<contexts>
<context position="6569" citStr="Chiou et al. (2001)" startWordPosition="1012" endWordPosition="1015">93) in the context of the construction of the Penn Treebank. Marcus et al. (1993) employed 1This problem is also known in the context of resources that are collaboratively constructed via the web (Kruschwitz et al., 2009) a post-correction set-up for both part-of-speech and syntactic structure annotation. For pos-tagging they compared the semi-automatic approach to a fully manual annotation. They found that the semiautomatic method resulted both in a significant reduction of annotation time, effectively doubling the word annotation rate, and in increased interannotator agreement and accuracy. Chiou et al. (2001) explored the effect of automatic pre-annotation for treebank construction. For the automatic step, they experimented with two different parsers and found that both reduce overall annotation time significantly while preserving accuracy. Later experiments by Xue et al. (2002) confirmed these findings. Ganchev et al. (2007) looked at semi-automatic gene identification in the biomedical domain. They, too, experimented with correcting the output of an automatic annotation system. However, rather than employing an off-the-shelf named entity tagger, they trained a tagger maximized for recall. The hu</context>
</contexts>
<marker>Chiou, Chiang, Palmer, 2001</marker>
<rawString>F.-D. Chiou, D. Chiang, M. Palmer. 2001. Facilitating treebank annotation using a statistical parser. In Proceedings of HLT-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W-C Chou</author>
<author>R T-H Tsai</author>
<author>Y-S Su</author>
<author>W Ku</author>
<author>T-Y Sung</author>
<author>W-L Hsu</author>
</authors>
<title>A semi-automatic method for annotation a biomedical proposition bank.</title>
<date>2006</date>
<booktitle>In Proceedings of FLAC-2006.</booktitle>
<contexts>
<context position="7431" citStr="Chou et al. (2006)" startWordPosition="1140" endWordPosition="1143">xperiments by Xue et al. (2002) confirmed these findings. Ganchev et al. (2007) looked at semi-automatic gene identification in the biomedical domain. They, too, experimented with correcting the output of an automatic annotation system. However, rather than employing an off-the-shelf named entity tagger, they trained a tagger maximized for recall. The human annotators were then instructed to filter the annotation, rejecting falsely labeled expressions. Ganchev et al. (2007) report a noticeable increase in speed compared to a fully manual set-up. The approach that is closest to ours is that of Chou et al. (2006) who investigate the effect of automatic pre-annotation for Propbank-style semantic argument structure labeling. However that study only looks into the properties of the semi-automatic set-up; the authors did not carry out a control study with a fully manual approach. Nevertheless Chou et al. (2006) provide an upper bound of the savings obtained by the semi-automatic process in terms of annotator operations. They report a reduction in annotation effort of up to 46%. 3 Experimental setup 3.1 Frame-Semantic Annotation The annotation scheme we use is that of FrameNet (FN), a lexicographic project</context>
</contexts>
<marker>Chou, Tsai, Su, Ku, Sung, Hsu, 2006</marker>
<rawString>W.-C. Chou, R. T.-H. Tsai, Y.-S. Su, W. Ku, T.-Y. Sung, W.-L. Hsu. 2006. A semi-automatic method for annotation a biomedical proposition bank. In Proceedings of FLAC-2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Erk</author>
<author>S Pado</author>
</authors>
<title>Shalmaneser - a flexible toolbox for semantic role assignment.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="12445" citStr="Erk and Pado, 2006" startWordPosition="1920" endWordPosition="1923">actic constituent(s). The annotators performed their annotation on computers where access to the FrameNet website, where gold annotations could have been found, was blocked. They did, however, have access to local copies of the frame descriptions needed for the lexical units in our experiment. As the overall time needed for the annotation was too long to do in one sitting, the annotators did it over several days. They were instructed to record the time (in minutes) that they took for the annotation of each annotation session. Our ASRL system for state-of-the-art preannotation was Shalmaneser (Erk and Pado, 2006). The enhanced pre-annotation was created by manually inserting errors into the gold standard. 3.4 Data We annotated 360 sentences exemplifying all the senses that were defined for six different lemmas in FrameNet release 1.3. The lemmas were the verbs rush, look, follow, throw, feel and scream. These verbs were chosen for three reasons. First, they have enough annotated instances in the FN release that we could use some instances for testing and still be left with a set of instances sufficiently large to train our ASRL system. Second,we knew from prior work with our automatic role labeler tha</context>
</contexts>
<marker>Erk, Pado, 2006</marker>
<rawString>K. Erk, S. Pado. 2006. Shalmaneser - a flexible toolbox for semantic role assignment. In Proceedings of LREC, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J F C J Fillmore</author>
</authors>
<date>1982</date>
<booktitle>Linguistics in the Morning Calm, chapter Frame Semantics, 111–137. Hanshin Publishing, Seoul,</booktitle>
<contexts>
<context position="8225" citStr="Fillmore (1982)" startWordPosition="1262" endWordPosition="1263">matic set-up; the authors did not carry out a control study with a fully manual approach. Nevertheless Chou et al. (2006) provide an upper bound of the savings obtained by the semi-automatic process in terms of annotator operations. They report a reduction in annotation effort of up to 46%. 3 Experimental setup 3.1 Frame-Semantic Annotation The annotation scheme we use is that of FrameNet (FN), a lexicographic project that produces a database of frame-semantic descriptions of English vocabulary. Frames are representations of prototypical events or states and their participants in the sense of Fillmore (1982). In the FN database, both frames and their participant roles are arranged in various hierarchical relations (most prominently, the is-a relation). FrameNet links these descriptions of frames with the words and multi-words (lexical units, LUs) that evoke these conceptual structures. It also docu20 ments all the ways in which the semantic roles (frame elements, FEs) can be realized as syntactic arguments of each frame-evoking word by labeling corpus attestations. As a small example, consider the Collaboration frame, evoked in English by lexical units such as collaborate.v, conspire.v, collabora</context>
</contexts>
<marker>Fillmore, 1982</marker>
<rawString>C. J. F. C. J. Fillmore, 1982. Linguistics in the Morning Calm, chapter Frame Semantics, 111–137. Hanshin Publishing, Seoul, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ganchev</author>
<author>F Pereira</author>
<author>M Mandel</author>
<author>S Carroll</author>
<author>P White</author>
</authors>
<title>Semi-automated named entity annotation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Linguistic Annotation Workshop,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>53–56, Prague, Czech Republic.</location>
<contexts>
<context position="6892" citStr="Ganchev et al. (2007)" startWordPosition="1059" endWordPosition="1062">g they compared the semi-automatic approach to a fully manual annotation. They found that the semiautomatic method resulted both in a significant reduction of annotation time, effectively doubling the word annotation rate, and in increased interannotator agreement and accuracy. Chiou et al. (2001) explored the effect of automatic pre-annotation for treebank construction. For the automatic step, they experimented with two different parsers and found that both reduce overall annotation time significantly while preserving accuracy. Later experiments by Xue et al. (2002) confirmed these findings. Ganchev et al. (2007) looked at semi-automatic gene identification in the biomedical domain. They, too, experimented with correcting the output of an automatic annotation system. However, rather than employing an off-the-shelf named entity tagger, they trained a tagger maximized for recall. The human annotators were then instructed to filter the annotation, rejecting falsely labeled expressions. Ganchev et al. (2007) report a noticeable increase in speed compared to a fully manual set-up. The approach that is closest to ours is that of Chou et al. (2006) who investigate the effect of automatic pre-annotation for P</context>
</contexts>
<marker>Ganchev, Pereira, Mandel, Carroll, White, 2007</marker>
<rawString>K. Ganchev, F. Pereira, M. Mandel, S. Carroll, P. White. 2007. Semi-automated named entity annotation. In Proceedings of the Linguistic Annotation Workshop, 53–56, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Kruschwitz</author>
<author>J Chamberlain</author>
<author>M Poesio</author>
</authors>
<title>(linguistic) science through web collaboration in the ANAWIKI project.</title>
<date>2009</date>
<booktitle>In Proceedings of WebSci’09.</booktitle>
<contexts>
<context position="6171" citStr="Kruschwitz et al., 2009" startWordPosition="956" endWordPosition="959">While semi-automatic annotation is frequently employed to create labeled data more quickly (see, e.g., Brants and Plaehn (2000)), there are comparatively few studies which systematically look at the benefits or limitations of this approach. One of the earliest studies that investigated the advantages of manually correcting automatic annotations for linguistic data was carried out by Marcus et al. (1993) in the context of the construction of the Penn Treebank. Marcus et al. (1993) employed 1This problem is also known in the context of resources that are collaboratively constructed via the web (Kruschwitz et al., 2009) a post-correction set-up for both part-of-speech and syntactic structure annotation. For pos-tagging they compared the semi-automatic approach to a fully manual annotation. They found that the semiautomatic method resulted both in a significant reduction of annotation time, effectively doubling the word annotation rate, and in increased interannotator agreement and accuracy. Chiou et al. (2001) explored the effect of automatic pre-annotation for treebank construction. For the automatic step, they experimented with two different parsers and found that both reduce overall annotation time signif</context>
</contexts>
<marker>Kruschwitz, Chamberlain, Poesio, 2009</marker>
<rawString>U. Kruschwitz, J. Chamberlain, M. Poesio. 2009. (linguistic) science through web collaboration in the ANAWIKI project. In Proceedings of WebSci’09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5953" citStr="Marcus et al. (1993)" startWordPosition="920" endWordPosition="923">ition to two different quality levels of pre-annotation, one that reflects the performance of a state-of-the-art ASRL system and an enhanced one that we artificially produced from the gold standard. 2 Related Work While semi-automatic annotation is frequently employed to create labeled data more quickly (see, e.g., Brants and Plaehn (2000)), there are comparatively few studies which systematically look at the benefits or limitations of this approach. One of the earliest studies that investigated the advantages of manually correcting automatic annotations for linguistic data was carried out by Marcus et al. (1993) in the context of the construction of the Penn Treebank. Marcus et al. (1993) employed 1This problem is also known in the context of resources that are collaboratively constructed via the web (Kruschwitz et al., 2009) a post-correction set-up for both part-of-speech and syntactic structure annotation. For pos-tagging they compared the semi-automatic approach to a fully manual annotation. They found that the semiautomatic method resulted both in a significant reduction of annotation time, effectively doubling the word annotation rate, and in increased interannotator agreement and accuracy. Chi</context>
<context position="20915" citStr="Marcus et al., 1993" startWordPosition="3335" endWordPosition="3338">ion and for Shalmaneser error types are made by human annotators throughout all three annotation trials, and that these errors are different from the ones made by the ASRL. Indicated by f-score, the most difficult frames in our data set are Scrutiny, Fluidic motion, Seeking, Make noise and Communication noise. This shows that automatic pre-annotation, even if noisy and of low quality, does not corrupt human annotators on a grand scale. Furthermore, if the preannotation is good it can even improve the overall annotation quality. This is in line with previous studies for other annotation tasks (Marcus et al., 1993). 4.3 How good does pre-annotation need to be to have a positive effect? Comparing annotation quality on the automatically pre-annotated texts using Shalmaneser, four out of six annotators achieved a higher f-score than on the non-annotated sentences. The effect, however, is not statistically significant. This means that preannotation produced by a state-of-the-art ASRL system is not yet good enough a) to significantly speed up the annotation process, and b) to improve the quality of the annotation itself. On the positive side, we also found no evidence that the error-prone pre-annotation decr</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W D Meurers</author>
<author>S M¨uller</author>
</authors>
<title>Corpora and syntax (article 44). In</title>
<date>2007</date>
<booktitle>Corpus linguistics. Mouton de Gruyter,</booktitle>
<editor>A. L¨udeling, M. Kyt¨o, eds.,</editor>
<location>Berlin.</location>
<marker>Meurers, M¨uller, 2007</marker>
<rawString>W. D. Meurers, S. M¨uller. 2007. Corpora and syntax (article 44). In A. L¨udeling, M. Kyt¨o, eds., Corpus linguistics. Mouton de Gruyter, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W D Meurers</author>
</authors>
<title>On the use of electronic corpora for theoretical linguistics. case studies from the syntax of german.</title>
<date>2005</date>
<journal>Lingua,</journal>
<volume>115</volume>
<issue>11</issue>
<pages>1639</pages>
<note>http://purl.org/net/dm/papers/ meurers-03.html.</note>
<contexts>
<context position="2313" citStr="Meurers, 2005" startWordPosition="347" endWordPosition="348">pendent views (Mueller et al., 2002; Ng and Cardie, 2003). Some active learning studies suggest both that the utility of the selected examples strongly depends on the model used for classification and that the example pool selected for one model can turn out to be sub-optimal when another model is trained on it at a later stage (Baldridge and Osborne, 2004). Furthermore, there are a number of scenarios for which there is simply no alternative to high-quality, manually annotated data; for example, if the annotated corpus is used for empirical research in linguistics (Meurers and M¨uller, 2007; Meurers, 2005). In this paper, we look at this problem from the data creation side. Specifically we explore whether a semi-automatic annotation set-up in which a human expert corrects the output of an automatic system can help to speed up the annotation process without sacrificing annotation quality. For our study, we explore the task of framesemantic argument structure annotation (Baker et al., 1998). We chose this particular task because it is a rather complex – and therefore time-consuming – undertaking, and it involves making a number of different but interdependent annotation decisions for each instanc</context>
</contexts>
<marker>Meurers, 2005</marker>
<rawString>W. D. Meurers. 2005. On the use of electronic corpora for theoretical linguistics. case studies from the syntax of german. Lingua, 115(11):1619– 1639. http://purl.org/net/dm/papers/ meurers-03.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Mueller</author>
<author>S Rapp</author>
<author>M Strube</author>
</authors>
<title>Applying cotraining to reference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>352–359, Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="1734" citStr="Mueller et al., 2002" startWordPosition="250" endWordPosition="253">cquisition bottleneck is a well-known problem and there have been numerous efforts to address it on the algorithmic side. Examples include the development of weakly supervised learning methods such as co-training and active learning. However, addressing only the algorithmic side is not always possible and not always desirable in all scenarios. First, some machine learning solutions are not as generally applicable or widely re-usable as one might think. It has been shown, for example, that co-training does not work well for problems which cannot easily be factorized into two independent views (Mueller et al., 2002; Ng and Cardie, 2003). Some active learning studies suggest both that the utility of the selected examples strongly depends on the model used for classification and that the example pool selected for one model can turn out to be sub-optimal when another model is trained on it at a later stage (Baldridge and Osborne, 2004). Furthermore, there are a number of scenarios for which there is simply no alternative to high-quality, manually annotated data; for example, if the annotated corpus is used for empirical research in linguistics (Meurers and M¨uller, 2007; Meurers, 2005). In this paper, we l</context>
</contexts>
<marker>Mueller, Rapp, Strube, 2002</marker>
<rawString>C. Mueller, S. Rapp, M. Strube. 2002. Applying cotraining to reference resolution. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, 352–359, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
<author>C Cardie</author>
</authors>
<title>Bootstrapping coreference classifiers with multiple machine learning algorithms.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP-2003).</booktitle>
<contexts>
<context position="1756" citStr="Ng and Cardie, 2003" startWordPosition="254" endWordPosition="257">is a well-known problem and there have been numerous efforts to address it on the algorithmic side. Examples include the development of weakly supervised learning methods such as co-training and active learning. However, addressing only the algorithmic side is not always possible and not always desirable in all scenarios. First, some machine learning solutions are not as generally applicable or widely re-usable as one might think. It has been shown, for example, that co-training does not work well for problems which cannot easily be factorized into two independent views (Mueller et al., 2002; Ng and Cardie, 2003). Some active learning studies suggest both that the utility of the selected examples strongly depends on the model used for classification and that the example pool selected for one model can turn out to be sub-optimal when another model is trained on it at a later stage (Baldridge and Osborne, 2004). Furthermore, there are a number of scenarios for which there is simply no alternative to high-quality, manually annotated data; for example, if the annotated corpus is used for empirical research in linguistics (Meurers and M¨uller, 2007; Meurers, 2005). In this paper, we look at this problem fr</context>
</contexts>
<marker>Ng, Cardie, 2003</marker>
<rawString>V. Ng, C. Cardie. 2003. Bootstrapping coreference classifiers with multiple machine learning algorithms. In Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing (EMNLP-2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F-D Chiou</author>
<author>M Palmer</author>
</authors>
<title>Building a large-scale annotated chinese corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of Coling-2002.</booktitle>
<contexts>
<context position="6844" citStr="Xue et al. (2002)" startWordPosition="1052" endWordPosition="1055">ntactic structure annotation. For pos-tagging they compared the semi-automatic approach to a fully manual annotation. They found that the semiautomatic method resulted both in a significant reduction of annotation time, effectively doubling the word annotation rate, and in increased interannotator agreement and accuracy. Chiou et al. (2001) explored the effect of automatic pre-annotation for treebank construction. For the automatic step, they experimented with two different parsers and found that both reduce overall annotation time significantly while preserving accuracy. Later experiments by Xue et al. (2002) confirmed these findings. Ganchev et al. (2007) looked at semi-automatic gene identification in the biomedical domain. They, too, experimented with correcting the output of an automatic annotation system. However, rather than employing an off-the-shelf named entity tagger, they trained a tagger maximized for recall. The human annotators were then instructed to filter the annotation, rejecting falsely labeled expressions. Ganchev et al. (2007) report a noticeable increase in speed compared to a fully manual set-up. The approach that is closest to ours is that of Chou et al. (2006) who investig</context>
</contexts>
<marker>Xue, Chiou, Palmer, 2002</marker>
<rawString>N. Xue, F.-D. Chiou, M. Palmer. 2002. Building a large-scale annotated chinese corpus. In Proceedings of Coling-2002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>