<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000318">
<title confidence="0.9958785">
Enhanced and Portable Dependency Projection Algorithms
Using Interlinear Glossed Text
</title>
<author confidence="0.998496">
Ryan Georgi
</author>
<affiliation confidence="0.999415">
University of Washington
</affiliation>
<address confidence="0.730582">
Seattle, WA 98195, USA
</address>
<email confidence="0.9992">
rgeorgi@uw.edu
</email>
<author confidence="0.993262">
Fei Xia
</author>
<affiliation confidence="0.99841">
University of Washington
</affiliation>
<address confidence="0.730595">
Seattle, WA 98195, USA
</address>
<email confidence="0.999052">
fxia@uw.edu
</email>
<author confidence="0.975475">
William D. Lewis
</author>
<affiliation confidence="0.953302">
Microsoft Research
</affiliation>
<address confidence="0.919833">
Redmond, WA 98052, USA
</address>
<email confidence="0.999104">
wilewis@microsoft.com
</email>
<sectionHeader confidence="0.993902" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999994782608696">
As most of the world’s languages are
under-resourced, projection algorithms
offer an enticing way to bootstrap the
resources available for one resource-
poor language from a resource-rich lan-
guage by means of parallel text and
word alignment. These algorithms,
however, make the strong assumption
that the language pairs share common
structures and that the parse trees will
resemble one another. This assump-
tion is useful but often leads to errors
in projection. In this paper, we will
address this weakness by using trees
created from instances of Interlinear
Glossed Text (IGT) to discover pat-
terns of divergence between the lan-
guages. We will show that this method
improves the performance of projection
algorithms significantly in some lan-
guages by accounting for divergence be-
tween languages using only the partial
supervision of a few corrected trees.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999968181818182">
While thousands of languages are spoken
in the world, most of them are considered
resource-poor in the sense that they do not
have a large number of electronic resources
that can be used to build NLP systems. For
instance, some languages may lack treebanks,
thus making it difficult to build a high-quality
statistical parser.
One common approach to address this prob-
lem is to take advantage of bitext between a
resource-rich language (e.g., English) and a
resource-poor language by projecting informa-
tion from the former to the latter (Yarowsky
and Ngai, 2001; Hwa et al., 2004). While pro-
jection methods can provide a great deal of in-
formation at minimal cost to the researchers,
they do suffer from structural divergence be-
tween the language-poor language (aka target
language) and the resource-rich language (aka
source language).
In this paper, we propose a middle ground
between manually creating a large-scale tree-
bank (which is expensive and time-consuming)
and relying on the syntactic structures pro-
duced by a projection algorithm alone (which
are error-prone).
Our approach has several steps. First, we
utilize instances of Interlinear Glossed Text
(IGT) following Xia and Lewis (2007) as seen
in Figure 1(a) to create a small set of parallel
dependency trees through projection and then
manually correct the dependency trees. Sec-
ond, we automatically analyze this small set
of parallel trees to find patterns where the cor-
rected data differs from the projection. Third,
those patterns are incorporated to the projec-
tion algorithm to improve the quality of pro-
jection. Finally, the features extracted from
the projected trees are added to a statisti-
cal parser to improve parsing quality. The
outcome of this work are both an enhanced
projection algorithm and a better parser for
resource-poor languages that require a mini-
mal amount of manual effort.
</bodyText>
<sectionHeader confidence="0.994013" genericHeader="introduction">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999312">
For this paper, we will be building upon
the standard projection algorithm for depen-
dency structures as outlined in Quirk et al.
(2005) and illustrated in Figure 1. First, a
sentence pair between resource-rich (source)
and resource-poor (target) languages is word
aligned [Fig 1(a)]. Second, the source sen-
tence is parsed by a dependency parser for
the source language [Fig 1(b)]. Third, sponta-
</bodyText>
<page confidence="0.990216">
306
</page>
<bodyText confidence="0.3920828">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 306–311,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
siwA ne pAnI se GadZe ko BarA
Sita erg water with clay-pot acc filled
Sita filled the clay-pot with water
</bodyText>
<figure confidence="0.997815888888889">
(a) An Interlinear Glossed Text (IGT) instance in Hindi
and word alignment between the gloss line and the
English translation.
filled
Sita clay-pot with
water
(b) Dependency parse of English translation.
BarA
siwA
</figure>
<figureCaption confidence="0.964153428571429">
(d) Siblings in the tree are reordered based on the word
order of the Hindi sentence and spontaneous Hindi
words are attached as indicated by dotted lines. The
words pAnI and se are incorrectly inverted, as indi-
cated by the curved arrow.
Figure 1: An example of projecting a depen-
dency tree from English to Hindi.
</figureCaption>
<bodyText confidence="0.99392768">
neous (unaligned) source words are removed,
and the remaining words are replaced with
corresponding words in the target side [Fig
1(c)]. Finally, spontaneous target words are
re-attached heuristically and the children of a
head are ordered based on the word order in
the target sentence [Fig 1(d)]. The resulting
tree may have errors (e.g., pAni should depend
on se in Figure 1(d)), and the goal of this study
is to reduce common types of projection errors.
In Georgi et al. (2012a), we proposed a
method for analyzing parallel dependency cor-
pora in which word alignment between trees
was used to determine three types of edge con-
figurations: merged, swapped, and spon-
taneous. Merged alignments were those in
which multiple words in the target tree aligned
to a single word in the source tree, as in Figure
2. Swapped alignments were those in which
a parent node in the source tree aligned to a
child in the target tree and vice-versa. Finally,
spontaneous alignments were those for which
a word did not align to any word on the other
side. These edge configurations could be de-
tected from simple parent–child edges and the
alignment (or lack of) between words in the
language pairs. Using these simple, language-
agnostic measures allows one to look for diver-
gence types such as those described by Dorr
(1994).
Georgi et al. (2012b) described a method
in which new features were extracted from
the projected trees and added to the feature
vectors for a statistical dependency parser.
The rationale was that, although the projected
trees were error-prone, the parsing model
should be able to set appropriate weights of
these features based on how reliable these fea-
tures were in indicating the dependency struc-
ture. We started with the MSTParser (Mc-
Donald et al., 2005) and modified it so that the
edges from the projected trees could be used
as features at parse time. Experiments showed
that adding new features improved parsing
performance.
In this paper, we use the small training cor-
pus built in Georgi et al. (2012b) to improve
the projection algorithm itself. The improved
projected trees are in turn fed to the statistical
parser to further improve parsing results.
</bodyText>
<sectionHeader confidence="0.716659" genericHeader="method">
3 Enhancements to the projection
algorithm
</sectionHeader>
<bodyText confidence="0.896338333333333">
We propose to enhance the projection algo-
rithm by addressing the three alignment types
discussed earlier:
</bodyText>
<listItem confidence="0.996659">
1. Merge: better informed choice for head
for multiply-aligned words.
2. Swap: post-projection correction of fre-
quently swapped word pairs.
3. Spontaneous: better informed attach-
ment of target spontaneous words.
</listItem>
<bodyText confidence="0.989313">
The detail of the enhancements are ex-
plained below.
</bodyText>
<subsectionHeader confidence="0.99922">
3.1 Merge Correction
</subsectionHeader>
<bodyText confidence="0.9998576">
“Merged” words, or multiple words on the tar-
get side that align to a single source word, are
problematic for the projection algorithm be-
cause it is not clear which target word should
be the head and which word should be the
</bodyText>
<figure confidence="0.98811225">
the pAnI
(c) English words are replaced with Hindi words and
spontaneous word “the” are removed from the tree.
BarA
siwA
se
GadZe
ne pAnI ko
the
GadZe se
307
rAma buxXimAna lagawA hE
Ram intelligent seem be-Pres
“Ram seems intelligent”
Ram intelligent ram buxXimAna hE
NNP JJ Ram intelligent be-Pres
</figure>
<figureCaption confidence="0.994876">
Figure 2: An example of merged alignment,
</figureCaption>
<bodyText confidence="0.999903513513513">
where the English word seems align to two
Hindi words hE and lagawA. Below the IGT
are the dependency trees for English and
Hindi. Dotted arrows indicate word align-
ment, and the solid arrow indicates that hE
should depend on lagawA.
dependent. An example is given in Figure 2,
where the English word seems align to two
Hindi words hE and lagawA.
On the other hand, from the small amount
of labeled training data (i.e., a set of hand-
corrected tree pairs), we can learn what kind
of source words are likely to align to multiple
target words, and which target word is likely to
the head. The process is illustrated in Figure
3. In this example, the target words tm and
tn are both aligned with the source word si
whose POS tag is POSi, and tm appears before
tn in the target sentence. Going through the
examples of merged alignments in the training
data, we keep a count for the POS tag of the
source word and the position of the head on
the target side.1 Based on these counts, our
system will generate rules such as the ones in
Figure 3(c) which says if a source word whose
POS is POSi aligns to two target words, the
probability of the right target word depending
on the left one is 75%, and the probability of
the left target word depending on the right one
is 25%. We use maximum likelihood estimate
(MLE) to calculate the probability.
The projection algorithm will use those rules
to handle merged alignment; that is, when a
source word aligns to multiple target words,
the algorithm determines the direction of de-
pendency edge based on the direction prefer-
ence stored in the rules. In addition to rules for
</bodyText>
<footnote confidence="0.667926">
1We use the position of the head, not the POS tag of
the head, because the POS tags of the target words are
not available when running the projection algorithm on
the test data.
</footnote>
<figure confidence="0.980542">
tm
s.
POS.
(a) Alignment between a source word and two target
words, and one target word t. is the parent of the
other word tn.
tm tn to ... tp
(b) Target sentence showing the “left” dependency be-
tween t. and tn.
POSi left 0.75
POSi right 0.25
(c) Rules for handling merged alignment
</figure>
<figureCaption confidence="0.9841855">
Figure 3: Example of merged alignment and
rules derived from such an example
</figureCaption>
<bodyText confidence="0.998496">
an individual source POS tag, our method also
keeps track of the overall direction preference
for all the merged examples in that language.
For merges in which the source POS tag is un-
seen or there are no rules for that tag, this
language-wide preference is used as a backoff.
</bodyText>
<subsectionHeader confidence="0.998904">
3.2 Swap Correction
</subsectionHeader>
<bodyText confidence="0.999969090909091">
An example of swapped alignment is in Figure
4(a), where (sj, si) is an edge in the source
tree, (tm, tn) is an edge in the target tree, and
sj aligns to tn and si aligns to tm. Figure
1(d) shows an error made by the projection
algorithm due to swapped alignment. In order
to correct such errors, we count the number
of (POSchild, POSparent) dependency edges in
the source trees, and the number of times that
the directions of the edges are reversed on the
target side. Figure 4(b) shows a possible set of
counts resulting from this approach. Based on
the counts, we keep only the POS pairs that
appear in at least 10% of training sentences
and the percentage of swap for the pairs are
no less than 70%.2 We say that those pairs
trigger a swap operation.
At the test time, swap rules are applied as a
post-processing step to the projected tree. Af-
ter the projected tree is completed, our swap
handling step checks each edge in the source
tree. If the POS tag pair for the edge triggers
</bodyText>
<figure confidence="0.902951333333334">
2These thresholds are set empirically.
seems
VBZ
lagawA
seems
to
</figure>
<page confidence="0.664505">
308
</page>
<listItem confidence="0.420679">
(a) A swapped alignment between source words sj and
si and target words tm and tn.
</listItem>
<table confidence="0.94759625">
POS Pair Swaps Total %
(POSi, POSj) 16 21 76
(POSk, POSl) 1 1 100
(POSn,POSo) 1 10 10
</table>
<listItem confidence="0.926816">
(b) Example set of learned swap rules. Swaps counts the
number of times the given (child, parent) pair is seen
in a swap configuration in the source side, and total
is the number of times said pair occurs overall.
</listItem>
<figureCaption confidence="0.997172">
Figure 4: Example swap configuration and col-
lected statistics.
</figureCaption>
<figure confidence="0.811245">
h h
</figure>
<figureCaption confidence="0.948746333333333">
Figure 5: Swap operation: on the left is the
original tree; on the right is the tree after
swapping node l with its parent j.
</figureCaption>
<bodyText confidence="0.990711666666667">
a swap operation, the corresponding nodes in
the projected tree will be swapped, as illus-
trated in Figure 5.
</bodyText>
<subsectionHeader confidence="0.999213">
3.3 Spontaneous Reattachment
</subsectionHeader>
<bodyText confidence="0.9999408">
Target spontaneous words are difficult to han-
dle because they do not align to any source
word and thus there is nothing to project to
them. To address this problem, we collect two
types of information from the training data.
First, we keep track of all the lexical items
that appear in the training trees, and the rel-
ative position of their head. This lexical ap-
proach may be useful in handling closed-class
words which account for a large percentage of
spontaneous words. Second, we use the train-
ing trees to determine the favored attachment
direction for the language as a whole.
At the test time, for each spontaneous word
in the target sentence, if it is one of the words
for which we have gathered statistics from the
training data, we attach it to the next word
in the preferred direction for that word. If the
word is unseen, we attach it using the overall
language preference as a backoff.
</bodyText>
<subsectionHeader confidence="0.988478">
3.4 Parser Enhancements
</subsectionHeader>
<bodyText confidence="0.999991333333333">
In addition to above enhancements to the pro-
jection algorithm itself, we train a dependency
parser on the training data, with new features
from the projected trees following Georgi et al.
(2012b). Furthermore, we add features that
indicate whether the current word appears in
a merge or swap configuration. The results
of this combination of additional features and
improved projection is shown in Table 1(b).
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999933388888889">
For evaluation, we use the same data sets as
in Georgi et al. (2012b), where there is a small
number (ranging from 46 to 147) of tree pairs
for each of the eight languages. The IGT
instances for those tree pairs come from the
Hindi Treebank (Bhatt et al., 2009) and the
Online Database of Interlinear Text (ODIN)
(Lewis and Xia, 2010).
We ran 10-fold cross validation and reported
the average of 10 runs in Table 1. The top ta-
ble shows the accuracy of the projection algo-
rithm, and the bottom table shows parsing ac-
curacy of MSTParser with or without adding
features from the projected trees. In both ta-
bles, the Best row uses the enhanced projec-
tion algorithm. The Baseline rows use the
original projection algorithm in Quirk et al.
(2005) where the word in the parentheses in-
dicates the direction of merge. The Error Re-
duction row shows the error reduction of the
Best system over the best performing baseline
for each language. The No Projection row in
the second table shows parsing results when
no features from the projected trees are added
to the parser, and the last row in that table
shows the error reduction of the Best row over
the No Projection row.
Table 1 shows that using features from the
projected trees provides a big boost to the
quality of the statistical parser. Furthermore,
the enhancements laid out in Section 3 im-
prove the performance of both the projection
algorithm and the parser that uses features
from projected trees. The degree of improve-
ment may depend on the properties of a par-
ticular language pair and the labeled data we
</bodyText>
<figure confidence="0.994409538461538">
sE
POSE
si
POSi
tm
to
o p
m n
i j k i l
l m n
o p
j
k
</figure>
<page confidence="0.920144">
309
</page>
<bodyText confidence="0.726407">
(a) The accuracies of the original projection algorithm (the Baselin rows) and the enhanced algorithm (the Best
row) on eight language pairs. For each language, the best performing baseline is in italic. The last row shows
the error reduction of the Best row over the best performing baseline, which is calculated by the formula
</bodyText>
<table confidence="0.952560823529412">
ErrorRate = Best−BestBaseline X 100
100−BestBaseline
YAQ WLS HIN KKN GLI HUA GER MEX
Best 88.03 94.90 77.44 91.75 87.70 90.11 88.71 93.05
Baseline (Right) 87.28 89.80 57.48 90.34 86.90 79.31 88.03 89.57
Baseline (Left) 84.29 89.80 68.11 88.93 76.98 79.54 88.03 89.57
Error Reduction 5.90 50.00 29.26 14.60 6.11 51.66 5.68 33.37
(b) The parsing accuracies of the MSTParser with or without new features extracted from projected trees. There
are two error reduction rows: one is with respect to the best performing baseline for each language, the other
is with respect to No Projection where the parser does not use features from projected trees.
YAQ WLS HIN KKN GLI HUA GER MEX
Best 89.28 94.90 81.35 92.96 81.35 88.74 92.93 93.05
Baseline (Right) 88.28 94.22 78.03 92.35 80.95 87.59 90.48 92.43
Baseline (Left) 87.88 94.22 79.64 90.95 80.95 89.20 90.48 92.43
No Projection 66.08 91.32 65.16 80.75 55.16 72.22 62.72 73.03
Error Reduction (BestBaseline) 8.53 11.76 8.40 7.97 2.10 -4.26 25.74 8.19
Error Reduction (No Projection) 68.39 41.24 46.47 63.43 58.41 59.47 81.04 74.23
</table>
<tableCaption confidence="0.9636445">
Table 1: System performance on eight languages: Yaqui (YAQ), Welsh (WLS), Hindi (HIN),
Korean (KKN), Gaelic (GLI), Hausa (HUA), German (GER), and Malagasy (MEX).
</tableCaption>
<bodyText confidence="0.996222692307692">
have for that language pair. For instance,
swap is quite common for the Hindi-English
pair because postpositions depend on nouns
in Hindi whereas nouns depend on preposi-
tions in English. As a result, the enhancement
for the swapped alignment alone results in a
large error reduction, as in Table 2. This ta-
ble shows the projection accuracy on the Hindi
data when each of the three enhancements is
turned on or off. The rows are sorted by de-
scending overall accuracy, and the row that
corresponds to the system labeled “Best” in
Table 1 is in bold.
</bodyText>
<sectionHeader confidence="0.998825" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999895125">
Existing projection algorithms suffer from the
effects of structural divergence between lan-
guage pairs. We propose to learn common di-
vergence types from a small number of tree
pairs and use the learned rules to improve pro-
jection accuracy. Our experiments show no-
table gains for both projection and parsing
when tested on eight language pairs. As IGT
data is available for hundreds of languages
through the ODIN database and other sources,
one could produce a small parallel treebank
for a language pair after spending a few hours
manually correcting the output of a projec-
tion algorithm. From the treebank, a bet-
ter projection algorithm and a better parser
can be built automatically using our approach.
</bodyText>
<table confidence="0.999592230769231">
Spont Swap Merge Direction Accuracy
✓ ✓ Left 78.07
✓ ✓ Informed 77.44
✓ Left 76.69
✓ Informed 76.06
✓ Left 69.49
✓ Informed 68.96
Left 68.11
Informed 67.58
✓ ✓ Right 66.32
✓ Right 64.97
✓ Right 58.84
Right 57.48
</table>
<tableCaption confidence="0.99441">
Table 2: Projection accuracy on the Hindi
</tableCaption>
<bodyText confidence="0.999718666666667">
data, with the three enhancements turning
on or off. The “spont” and “swap” columns
show a checkmark when the enhancements
are turned on. The merge direction indicates
whether a left or right choice was made as a
baseline, or whether the choice was informed
by the rules learned from the training data.
While the improvements for some languages
are incremental, the scope of coverage for this
method is potentially enormous, enabling the
rapid creation of tools for under-resourced lan-
guages of all kinds at a minimal cost.
</bodyText>
<sectionHeader confidence="0.961084" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.80264825">
This work is supported by the National Sci-
ence Foundation Grant BCS-0748919. We
would also like to thank the reviewers for help-
ful comments.
</bodyText>
<page confidence="0.997302">
310
</page>
<sectionHeader confidence="0.995876" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999801066666667">
Rajesh Bhatt, Bhuvana Narasimhan, Martha
Palmer, Owen Rambow, Dipti Misra
Sharma, and Fei Xia. A multi-
representational and multi-layered treebank
for Hindi/Urdu. In ACL-IJCNLP &apos;09: Pro-
ceedings of the Third Linguistic Annotation
Workshop. Association for Computational
Linguistics, August 2009.
Bonnie Jean Dorr. Machine translation di-
vergences: a formal description and pro-
posed solution. Computational Linguistics,
20:597–633, December 1994.
R Georgi, F Xia, and W D Lewis. Measur-
ing the Divergence of Dependency Struc-
tures Cross-Linguistically to Improve Syn-
tactic Projection Algorithms. In Proceedings
of the Sixth International Conference on
Language Resources and Evaluation (LREC
2012), Istanbul, Turkey, May 2012a.
Ryan Georgi, Fei Xia, and William D Lewis.
Improving Dependency Parsing with Inter-
linear Glossed Text and Syntactic Projec-
tion. In Proceedings of the 24th Interna-
tional Conference on Computational Lin-
guistics (COLING 2012), Mumbai, India,
December 2012b.
Rebecca Hwa, Philip Resnik, Amy Weinberg,
Clara Cabezas, and Okan Kolak. Bootstrap-
ping parsers via syntactic projection across
parallel texts. Natural Language Engineer-
ing, 1(1):1–15, 2004.
William D Lewis and Fei Xia. Developing
ODIN: A Multilingual Repository of Anno-
tated Language Data for Hundreds of the
World’s Languages. 2010.
R. McDonald, F. Pereira, K. Ribarov, and
J. Hajic. Non-projective dependency parsing
using spanning tree algorithms. Proceedings
of the conference on Human Language Tech-
nology and Empirical Methods in Natural
Language Processing, pages 523–530, 2005.
Chris Quirk, Arul Menezes, and Colin Cherry.
Dependency treelet translation: Syntacti-
cally informed phrasal SMT. In Proceed-
ings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics. Mi-
crosoft Research, 2005.
Fei Xia and William D Lewis. Multilin-
gual Structural Projection across Interlin-
ear Text. In Human Language Technologies:
The Annual Conference of the North Amer-
ican Chapter of the Association for Compu-
tational Linguistics (NAACL), 2007.
David Yarowsky and Grace Ngai. Inducing
multilingual POS taggers and NP bracketers
via robust projection across aligned corpora.
In Second meeting of the North American
Association for Computational Linguistics
(NAACL), Stroudsburg, PA, 2001. Johns
Hopkins University.
</reference>
<page confidence="0.999014">
311
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.902948">
<title confidence="0.9987095">Enhanced and Portable Dependency Projection Using Interlinear Glossed Text</title>
<author confidence="0.9975">Ryan</author>
<affiliation confidence="0.999815">University of</affiliation>
<address confidence="0.998655">Seattle, WA 98195,</address>
<email confidence="0.999657">rgeorgi@uw.edu</email>
<author confidence="0.960986">Fei</author>
<affiliation confidence="0.999695">University of</affiliation>
<address confidence="0.997634">Seattle, WA 98195,</address>
<email confidence="0.999868">fxia@uw.edu</email>
<author confidence="0.990186">D William</author>
<affiliation confidence="0.961939">Microsoft</affiliation>
<address confidence="0.998374">Redmond, WA 98052,</address>
<email confidence="0.999824">wilewis@microsoft.com</email>
<abstract confidence="0.999864416666666">As most of the world’s languages are under-resourced, projection algorithms offer an enticing way to bootstrap the resources available for one resourcepoor language from a resource-rich language by means of parallel text and word alignment. These algorithms, however, make the strong assumption that the language pairs share common structures and that the parse trees will resemble one another. This assumption is useful but often leads to errors in projection. In this paper, we will address this weakness by using trees created from instances of Interlinear Glossed Text (IGT) to discover patterns of divergence between the languages. We will show that this method improves the performance of projection algorithms significantly in some languages by accounting for divergence between languages using only the partial supervision of a few corrected trees.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rajesh Bhatt</author>
<author>Bhuvana Narasimhan</author>
<author>Martha Palmer</author>
<author>Owen Rambow</author>
<author>Dipti Misra Sharma</author>
<author>Fei Xia</author>
</authors>
<title>A multirepresentational and multi-layered treebank for Hindi/Urdu.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP &apos;09: Proceedings of the Third Linguistic Annotation Workshop. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="13279" citStr="Bhatt et al., 2009" startWordPosition="2259" endWordPosition="2262">itself, we train a dependency parser on the training data, with new features from the projected trees following Georgi et al. (2012b). Furthermore, we add features that indicate whether the current word appears in a merge or swap configuration. The results of this combination of additional features and improved projection is shown in Table 1(b). 4 Results For evaluation, we use the same data sets as in Georgi et al. (2012b), where there is a small number (ranging from 46 to 147) of tree pairs for each of the eight languages. The IGT instances for those tree pairs come from the Hindi Treebank (Bhatt et al., 2009) and the Online Database of Interlinear Text (ODIN) (Lewis and Xia, 2010). We ran 10-fold cross validation and reported the average of 10 runs in Table 1. The top table shows the accuracy of the projection algorithm, and the bottom table shows parsing accuracy of MSTParser with or without adding features from the projected trees. In both tables, the Best row uses the enhanced projection algorithm. The Baseline rows use the original projection algorithm in Quirk et al. (2005) where the word in the parentheses indicates the direction of merge. The Error Reduction row shows the error reduction of</context>
</contexts>
<marker>Bhatt, Narasimhan, Palmer, Rambow, Sharma, Xia, 2009</marker>
<rawString>Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer, Owen Rambow, Dipti Misra Sharma, and Fei Xia. A multirepresentational and multi-layered treebank for Hindi/Urdu. In ACL-IJCNLP &apos;09: Proceedings of the Third Linguistic Annotation Workshop. Association for Computational Linguistics, August 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Jean Dorr</author>
</authors>
<title>Machine translation divergences: a formal description and proposed solution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<pages>20--597</pages>
<contexts>
<context position="5596" citStr="Dorr (1994)" startWordPosition="898" endWordPosition="899">re those in which multiple words in the target tree aligned to a single word in the source tree, as in Figure 2. Swapped alignments were those in which a parent node in the source tree aligned to a child in the target tree and vice-versa. Finally, spontaneous alignments were those for which a word did not align to any word on the other side. These edge configurations could be detected from simple parent–child edges and the alignment (or lack of) between words in the language pairs. Using these simple, languageagnostic measures allows one to look for divergence types such as those described by Dorr (1994). Georgi et al. (2012b) described a method in which new features were extracted from the projected trees and added to the feature vectors for a statistical dependency parser. The rationale was that, although the projected trees were error-prone, the parsing model should be able to set appropriate weights of these features based on how reliable these features were in indicating the dependency structure. We started with the MSTParser (McDonald et al., 2005) and modified it so that the edges from the projected trees could be used as features at parse time. Experiments showed that adding new featu</context>
</contexts>
<marker>Dorr, 1994</marker>
<rawString>Bonnie Jean Dorr. Machine translation divergences: a formal description and proposed solution. Computational Linguistics, 20:597–633, December 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Georgi</author>
<author>F Xia</author>
<author>W D Lewis</author>
</authors>
<title>Measuring the Divergence of Dependency Structures Cross-Linguistically to Improve Syntactic Projection Algorithms.</title>
<date>2012</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC 2012),</booktitle>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="4768" citStr="Georgi et al. (2012" startWordPosition="754" endWordPosition="757">se are incorrectly inverted, as indicated by the curved arrow. Figure 1: An example of projecting a dependency tree from English to Hindi. neous (unaligned) source words are removed, and the remaining words are replaced with corresponding words in the target side [Fig 1(c)]. Finally, spontaneous target words are re-attached heuristically and the children of a head are ordered based on the word order in the target sentence [Fig 1(d)]. The resulting tree may have errors (e.g., pAni should depend on se in Figure 1(d)), and the goal of this study is to reduce common types of projection errors. In Georgi et al. (2012a), we proposed a method for analyzing parallel dependency corpora in which word alignment between trees was used to determine three types of edge configurations: merged, swapped, and spontaneous. Merged alignments were those in which multiple words in the target tree aligned to a single word in the source tree, as in Figure 2. Swapped alignments were those in which a parent node in the source tree aligned to a child in the target tree and vice-versa. Finally, spontaneous alignments were those for which a word did not align to any word on the other side. These edge configurations could be dete</context>
<context position="6306" citStr="Georgi et al. (2012" startWordPosition="1015" endWordPosition="1018">ojected trees and added to the feature vectors for a statistical dependency parser. The rationale was that, although the projected trees were error-prone, the parsing model should be able to set appropriate weights of these features based on how reliable these features were in indicating the dependency structure. We started with the MSTParser (McDonald et al., 2005) and modified it so that the edges from the projected trees could be used as features at parse time. Experiments showed that adding new features improved parsing performance. In this paper, we use the small training corpus built in Georgi et al. (2012b) to improve the projection algorithm itself. The improved projected trees are in turn fed to the statistical parser to further improve parsing results. 3 Enhancements to the projection algorithm We propose to enhance the projection algorithm by addressing the three alignment types discussed earlier: 1. Merge: better informed choice for head for multiply-aligned words. 2. Swap: post-projection correction of frequently swapped word pairs. 3. Spontaneous: better informed attachment of target spontaneous words. The detail of the enhancements are explained below. 3.1 Merge Correction “Merged” wor</context>
<context position="12791" citStr="Georgi et al. (2012" startWordPosition="2174" endWordPosition="2177">ning trees to determine the favored attachment direction for the language as a whole. At the test time, for each spontaneous word in the target sentence, if it is one of the words for which we have gathered statistics from the training data, we attach it to the next word in the preferred direction for that word. If the word is unseen, we attach it using the overall language preference as a backoff. 3.4 Parser Enhancements In addition to above enhancements to the projection algorithm itself, we train a dependency parser on the training data, with new features from the projected trees following Georgi et al. (2012b). Furthermore, we add features that indicate whether the current word appears in a merge or swap configuration. The results of this combination of additional features and improved projection is shown in Table 1(b). 4 Results For evaluation, we use the same data sets as in Georgi et al. (2012b), where there is a small number (ranging from 46 to 147) of tree pairs for each of the eight languages. The IGT instances for those tree pairs come from the Hindi Treebank (Bhatt et al., 2009) and the Online Database of Interlinear Text (ODIN) (Lewis and Xia, 2010). We ran 10-fold cross validation and r</context>
</contexts>
<marker>Georgi, Xia, Lewis, 2012</marker>
<rawString>R Georgi, F Xia, and W D Lewis. Measuring the Divergence of Dependency Structures Cross-Linguistically to Improve Syntactic Projection Algorithms. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC 2012), Istanbul, Turkey, May 2012a.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Georgi</author>
<author>Fei Xia</author>
<author>William D Lewis</author>
</authors>
<title>Improving Dependency Parsing with Interlinear Glossed Text and Syntactic Projection.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<location>Mumbai, India,</location>
<contexts>
<context position="4768" citStr="Georgi et al. (2012" startWordPosition="754" endWordPosition="757">se are incorrectly inverted, as indicated by the curved arrow. Figure 1: An example of projecting a dependency tree from English to Hindi. neous (unaligned) source words are removed, and the remaining words are replaced with corresponding words in the target side [Fig 1(c)]. Finally, spontaneous target words are re-attached heuristically and the children of a head are ordered based on the word order in the target sentence [Fig 1(d)]. The resulting tree may have errors (e.g., pAni should depend on se in Figure 1(d)), and the goal of this study is to reduce common types of projection errors. In Georgi et al. (2012a), we proposed a method for analyzing parallel dependency corpora in which word alignment between trees was used to determine three types of edge configurations: merged, swapped, and spontaneous. Merged alignments were those in which multiple words in the target tree aligned to a single word in the source tree, as in Figure 2. Swapped alignments were those in which a parent node in the source tree aligned to a child in the target tree and vice-versa. Finally, spontaneous alignments were those for which a word did not align to any word on the other side. These edge configurations could be dete</context>
<context position="6306" citStr="Georgi et al. (2012" startWordPosition="1015" endWordPosition="1018">ojected trees and added to the feature vectors for a statistical dependency parser. The rationale was that, although the projected trees were error-prone, the parsing model should be able to set appropriate weights of these features based on how reliable these features were in indicating the dependency structure. We started with the MSTParser (McDonald et al., 2005) and modified it so that the edges from the projected trees could be used as features at parse time. Experiments showed that adding new features improved parsing performance. In this paper, we use the small training corpus built in Georgi et al. (2012b) to improve the projection algorithm itself. The improved projected trees are in turn fed to the statistical parser to further improve parsing results. 3 Enhancements to the projection algorithm We propose to enhance the projection algorithm by addressing the three alignment types discussed earlier: 1. Merge: better informed choice for head for multiply-aligned words. 2. Swap: post-projection correction of frequently swapped word pairs. 3. Spontaneous: better informed attachment of target spontaneous words. The detail of the enhancements are explained below. 3.1 Merge Correction “Merged” wor</context>
<context position="12791" citStr="Georgi et al. (2012" startWordPosition="2174" endWordPosition="2177">ning trees to determine the favored attachment direction for the language as a whole. At the test time, for each spontaneous word in the target sentence, if it is one of the words for which we have gathered statistics from the training data, we attach it to the next word in the preferred direction for that word. If the word is unseen, we attach it using the overall language preference as a backoff. 3.4 Parser Enhancements In addition to above enhancements to the projection algorithm itself, we train a dependency parser on the training data, with new features from the projected trees following Georgi et al. (2012b). Furthermore, we add features that indicate whether the current word appears in a merge or swap configuration. The results of this combination of additional features and improved projection is shown in Table 1(b). 4 Results For evaluation, we use the same data sets as in Georgi et al. (2012b), where there is a small number (ranging from 46 to 147) of tree pairs for each of the eight languages. The IGT instances for those tree pairs come from the Hindi Treebank (Bhatt et al., 2009) and the Online Database of Interlinear Text (ODIN) (Lewis and Xia, 2010). We ran 10-fold cross validation and r</context>
</contexts>
<marker>Georgi, Xia, Lewis, 2012</marker>
<rawString>Ryan Georgi, Fei Xia, and William D Lewis. Improving Dependency Parsing with Interlinear Glossed Text and Syntactic Projection. In Proceedings of the 24th International Conference on Computational Linguistics (COLING 2012), Mumbai, India, December 2012b.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
</authors>
<title>Clara Cabezas, and Okan Kolak. Bootstrapping parsers via syntactic projection across parallel texts.</title>
<date>2004</date>
<journal>Natural Language Engineering,</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="1770" citStr="Hwa et al., 2004" startWordPosition="270" endWordPosition="273">a few corrected trees. 1 Introduction While thousands of languages are spoken in the world, most of them are considered resource-poor in the sense that they do not have a large number of electronic resources that can be used to build NLP systems. For instance, some languages may lack treebanks, thus making it difficult to build a high-quality statistical parser. One common approach to address this problem is to take advantage of bitext between a resource-rich language (e.g., English) and a resource-poor language by projecting information from the former to the latter (Yarowsky and Ngai, 2001; Hwa et al., 2004). While projection methods can provide a great deal of information at minimal cost to the researchers, they do suffer from structural divergence between the language-poor language (aka target language) and the resource-rich language (aka source language). In this paper, we propose a middle ground between manually creating a large-scale treebank (which is expensive and time-consuming) and relying on the syntactic structures produced by a projection algorithm alone (which are error-prone). Our approach has several steps. First, we utilize instances of Interlinear Glossed Text (IGT) following Xia</context>
</contexts>
<marker>Hwa, Resnik, Weinberg, 2004</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. Bootstrapping parsers via syntactic projection across parallel texts. Natural Language Engineering, 1(1):1–15, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William D Lewis</author>
<author>Fei Xia</author>
</authors>
<title>Developing ODIN: A Multilingual Repository of Annotated Language Data for Hundreds of the World’s Languages.</title>
<date>2010</date>
<contexts>
<context position="13352" citStr="Lewis and Xia, 2010" startWordPosition="2271" endWordPosition="2274">ures from the projected trees following Georgi et al. (2012b). Furthermore, we add features that indicate whether the current word appears in a merge or swap configuration. The results of this combination of additional features and improved projection is shown in Table 1(b). 4 Results For evaluation, we use the same data sets as in Georgi et al. (2012b), where there is a small number (ranging from 46 to 147) of tree pairs for each of the eight languages. The IGT instances for those tree pairs come from the Hindi Treebank (Bhatt et al., 2009) and the Online Database of Interlinear Text (ODIN) (Lewis and Xia, 2010). We ran 10-fold cross validation and reported the average of 10 runs in Table 1. The top table shows the accuracy of the projection algorithm, and the bottom table shows parsing accuracy of MSTParser with or without adding features from the projected trees. In both tables, the Best row uses the enhanced projection algorithm. The Baseline rows use the original projection algorithm in Quirk et al. (2005) where the word in the parentheses indicates the direction of merge. The Error Reduction row shows the error reduction of the Best system over the best performing baseline for each language. The</context>
</contexts>
<marker>Lewis, Xia, 2010</marker>
<rawString>William D Lewis and Fei Xia. Developing ODIN: A Multilingual Repository of Annotated Language Data for Hundreds of the World’s Languages. 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>523--530</pages>
<contexts>
<context position="6055" citStr="McDonald et al., 2005" startWordPosition="970" endWordPosition="974">) between words in the language pairs. Using these simple, languageagnostic measures allows one to look for divergence types such as those described by Dorr (1994). Georgi et al. (2012b) described a method in which new features were extracted from the projected trees and added to the feature vectors for a statistical dependency parser. The rationale was that, although the projected trees were error-prone, the parsing model should be able to set appropriate weights of these features based on how reliable these features were in indicating the dependency structure. We started with the MSTParser (McDonald et al., 2005) and modified it so that the edges from the projected trees could be used as features at parse time. Experiments showed that adding new features improved parsing performance. In this paper, we use the small training corpus built in Georgi et al. (2012b) to improve the projection algorithm itself. The improved projected trees are in turn fed to the statistical parser to further improve parsing results. 3 Enhancements to the projection algorithm We propose to enhance the projection algorithm by addressing the three alignment types discussed earlier: 1. Merge: better informed choice for head for </context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajic. Non-projective dependency parsing using spanning tree algorithms. Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 523–530, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics. Microsoft Research,</booktitle>
<contexts>
<context position="3204" citStr="Quirk et al. (2005)" startWordPosition="499" endWordPosition="502">lel trees to find patterns where the corrected data differs from the projection. Third, those patterns are incorporated to the projection algorithm to improve the quality of projection. Finally, the features extracted from the projected trees are added to a statistical parser to improve parsing quality. The outcome of this work are both an enhanced projection algorithm and a better parser for resource-poor languages that require a minimal amount of manual effort. 2 Previous Work For this paper, we will be building upon the standard projection algorithm for dependency structures as outlined in Quirk et al. (2005) and illustrated in Figure 1. First, a sentence pair between resource-rich (source) and resource-poor (target) languages is word aligned [Fig 1(a)]. Second, the source sentence is parsed by a dependency parser for the source language [Fig 1(b)]. Third, sponta306 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 306–311, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics siwA ne pAnI se GadZe ko BarA Sita erg water with clay-pot acc filled Sita filled the clay-pot with water (a) An Interlinear Glossed Text (IGT) instan</context>
<context position="13758" citStr="Quirk et al. (2005)" startWordPosition="2343" endWordPosition="2346">147) of tree pairs for each of the eight languages. The IGT instances for those tree pairs come from the Hindi Treebank (Bhatt et al., 2009) and the Online Database of Interlinear Text (ODIN) (Lewis and Xia, 2010). We ran 10-fold cross validation and reported the average of 10 runs in Table 1. The top table shows the accuracy of the projection algorithm, and the bottom table shows parsing accuracy of MSTParser with or without adding features from the projected trees. In both tables, the Best row uses the enhanced projection algorithm. The Baseline rows use the original projection algorithm in Quirk et al. (2005) where the word in the parentheses indicates the direction of merge. The Error Reduction row shows the error reduction of the Best system over the best performing baseline for each language. The No Projection row in the second table shows parsing results when no features from the projected trees are added to the parser, and the last row in that table shows the error reduction of the Best row over the No Projection row. Table 1 shows that using features from the projected trees provides a big boost to the quality of the statistical parser. Furthermore, the enhancements laid out in Section 3 imp</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. Dependency treelet translation: Syntactically informed phrasal SMT. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics. Microsoft Research, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>William D Lewis</author>
</authors>
<title>Multilingual Structural Projection across Interlinear Text. In Human Language Technologies:</title>
<date>2007</date>
<booktitle>The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL),</booktitle>
<contexts>
<context position="2387" citStr="Xia and Lewis (2007)" startWordPosition="364" endWordPosition="367">04). While projection methods can provide a great deal of information at minimal cost to the researchers, they do suffer from structural divergence between the language-poor language (aka target language) and the resource-rich language (aka source language). In this paper, we propose a middle ground between manually creating a large-scale treebank (which is expensive and time-consuming) and relying on the syntactic structures produced by a projection algorithm alone (which are error-prone). Our approach has several steps. First, we utilize instances of Interlinear Glossed Text (IGT) following Xia and Lewis (2007) as seen in Figure 1(a) to create a small set of parallel dependency trees through projection and then manually correct the dependency trees. Second, we automatically analyze this small set of parallel trees to find patterns where the corrected data differs from the projection. Third, those patterns are incorporated to the projection algorithm to improve the quality of projection. Finally, the features extracted from the projected trees are added to a statistical parser to improve parsing quality. The outcome of this work are both an enhanced projection algorithm and a better parser for resour</context>
</contexts>
<marker>Xia, Lewis, 2007</marker>
<rawString>Fei Xia and William D Lewis. Multilingual Structural Projection across Interlinear Text. In Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
</authors>
<title>Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In</title>
<date>2001</date>
<booktitle>Second meeting of the North American Association for Computational Linguistics (NAACL),</booktitle>
<institution>Johns Hopkins University.</institution>
<location>Stroudsburg, PA,</location>
<contexts>
<context position="1751" citStr="Yarowsky and Ngai, 2001" startWordPosition="266" endWordPosition="269">e partial supervision of a few corrected trees. 1 Introduction While thousands of languages are spoken in the world, most of them are considered resource-poor in the sense that they do not have a large number of electronic resources that can be used to build NLP systems. For instance, some languages may lack treebanks, thus making it difficult to build a high-quality statistical parser. One common approach to address this problem is to take advantage of bitext between a resource-rich language (e.g., English) and a resource-poor language by projecting information from the former to the latter (Yarowsky and Ngai, 2001; Hwa et al., 2004). While projection methods can provide a great deal of information at minimal cost to the researchers, they do suffer from structural divergence between the language-poor language (aka target language) and the resource-rich language (aka source language). In this paper, we propose a middle ground between manually creating a large-scale treebank (which is expensive and time-consuming) and relying on the syntactic structures produced by a projection algorithm alone (which are error-prone). Our approach has several steps. First, we utilize instances of Interlinear Glossed Text </context>
</contexts>
<marker>Yarowsky, Ngai, 2001</marker>
<rawString>David Yarowsky and Grace Ngai. Inducing multilingual POS taggers and NP bracketers via robust projection across aligned corpora. In Second meeting of the North American Association for Computational Linguistics (NAACL), Stroudsburg, PA, 2001. Johns Hopkins University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>