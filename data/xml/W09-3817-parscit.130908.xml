<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000048">
<title confidence="0.984145">
Capturing Consistency between Intra-clause and Inter-clause Relations
in Knowledge-rich Dependency and Case Structure Analysis
</title>
<author confidence="0.996867">
Daisuke Kawahara
</author>
<affiliation confidence="0.851703">
National Institute of Information and
Communications Technology,
</affiliation>
<address confidence="0.969847">
3-5 Hikaridai Seika-cho, Soraku-gun,
Kyoto, 619-0289, Japan
</address>
<email confidence="0.999343">
dk@nict.go.jp
</email>
<sectionHeader confidence="0.997395" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999917222222222">
We present a method for dependency and
case structure analysis that captures the
consistency between intra-clause relations
(i.e., case structures or predicate-argument
structures) and inter-clause relations. We
assess intra-clause relations on the basis
of case frames and inter-clause relations
on the basis of transition knowledge be-
tween case frames. Both knowledge bases
are automatically acquired from a mas-
sive amount of parses of a Web corpus.
The significance of this study is that the
proposed method selects the best depen-
dency and case structure that are con-
sistent within each clause and between
clauses. We confirm that this method con-
tributes to the improvement of dependency
parsing of Japanese.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990793947368421">
The approaches of dependency parsing basically
assess the likelihood of a dependency relation be-
tween two words or phrases and subsequently
collect all the assessments for these pairs as the
dependency parse of the sentence. To improve
dependency parsing, it is important to consider
as broad a context as possible, rather than a
word/phrase pair.
In the recent evaluation workshops (shared
tasks) of multilingual dependency parsing (Buch-
holz and Marsi, 2006; Nivre et al., 2007),
transition-based and graph-based methods
achieved good performance by incorporating rich
context. Transition-based dependency parsers
consider the words following the word under
consideration as features of machine learning
(Kudo and Matsumoto, 2002; Nivre and Scholz,
2004; Sassano, 2004). Graph-based dependency
parsers consider sibling and grandparent nodes,
</bodyText>
<author confidence="0.36512">
Sadao Kurohashi
</author>
<affiliation confidence="0.539604">
Graduate School of Informatics,
Kyoto University,
</affiliation>
<address confidence="0.44989">
Yoshida-Honmachi, Sakyo-ku,
Kyoto, 606-8501, Japan
</address>
<email confidence="0.988503">
kuro@i.kyoto-u.ac.jp
</email>
<bodyText confidence="0.999723975609756">
i.e., second-order and higher-order features
(McDonald and Pereira, 2006; Carreras, 2007;
Nakagawa, 2007).
It is desirable to consider a wider-range phrase,
clause, or a whole sentence, but it is difficult to
judge whether the structure of such a wide-range
expression is linguistically correct. One of the rea-
sons for this is the scarcity of the knowledge re-
quired to make such a judgment. When we use
the Penn Treebank (Marcus et al., 1993), which is
one of the largest corpora among the available ana-
lyzed corpora, as training data, even bi-lexical de-
pendencies cannot be learned sufficiently (Bikel,
2004). To circumvent such scarcity, for instance,
Koo et al. (2008) proposed the use of word classes
induced by clustering words in a large raw cor-
pus. They succeeded in improving the accuracy of
a higher-order dependency parser.
On the other hand, some researchers have pro-
posed other approaches where linguistic units such
as predicate-argument structures (also known as
case structures and logical forms) are considered
instead of arbitrary nodes such as sibling nodes.
To solve the problem of knowledge scarcity, they
learned knowledge of such predicate-argument
structures from a very large number of automat-
ically analyzed corpora (Abekawa and Okumura,
2006; Kawahara and Kurohashi, 2006b). While
Abekawa and Okumura (2006) used only co-
occurrence statistics of verbal arguments, Kawa-
hara and Kurohashi (2006b) assessed predicate-
argument structures by checking case frames,
which are semantic frames that are automatically
compiled for each predicate sense from a large raw
corpus. These methods outperformed the accuracy
of supervised dependency parsers.
In such linguistically-motivated approaches,
well-formedness within a clause was considered,
but coherence between clauses was not con-
sidered. Even if intra-clause relations (i.e., a
predicate-argument structure within a clause) are
</bodyText>
<page confidence="0.981134">
108
</page>
<note confidence="0.9877685">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 108–116,
Paris, October 2009. c�2009 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999777">
Figure 1: Possible dependency and case structures of sentence (1).
</figureCaption>
<bodyText confidence="0.999874178571429">
optimized, they might not be optimum when look-
ing at clause pairs or sequences. To improve the
accuracy of dependency parsing, we propose a
method for dependency and case structure analy-
sis that considers the consistency between intra-
clause and inter-clause relations. This method an-
alyzes intra-clause relations on the basis of case
frames and inter-clause relations on the basis of
transition knowledge between case frames. These
two knowledge sources are automatically acquired
from a massive amount of parses of a Web corpus.
The contributions of this paper are two-fold.
First, we acquire transition knowledge not be-
tween verbs or verb phrases but between case
frames, which are semantically disambiguated
representations. Second, we incorporate the tran-
sition knowledge into dependency and case struc-
ture analysis to capture the consistency between
intra-clause and inter-clause relations.
The remainder of this paper is organized as
follows. Section 2 illustrates our idea. Section
3 describes a method for acquiring the transi-
tion knowledge. Section 4 explains the proposed
method of incorporating the acquired transition
knowledge into a probabilistic model of depen-
dency and case structure analysis. Section 5 re-
ports experimental results. Section 6 gives the
conclusions.
</bodyText>
<page confidence="0.999454">
109
</page>
<sectionHeader confidence="0.968018" genericHeader="introduction">
2 Idea of Capturing Consistency between
Intra-clause and Inter-clause Relations
</sectionHeader>
<bodyText confidence="0.9973385">
We propose a method for generative dependency
parsing that captures the consistency between
intra-clause and inter-clause relations.
Figure 1 shows the ambiguities of dependency
and case structure of pointo-wa (point-TOP) in the
following sentence:
</bodyText>
<equation confidence="0.985253833333333">
(1) pointo-wa, hitotsu-ni matomete
point-TOP one-DAT pack
takuhaibin-de okuru koto-desu
courier-CMI send be that
(The point is that (we) pack (one’s bag-
gage) and send (it) using courier service.)
</equation>
<bodyText confidence="0.999898806451613">
The correct structure is (c1), which is surrounded
by the dotted rectangle. Structures (c2), (c3)
and so on have the same dependency structure as
(c1), but have incorrect case structures, in which
incorrect case frames are selected. Note that
matomeru:5, okuru:6 and so on in the figure rep-
resent the IDs of the case frames.
The parser of Kawahara and Kurohashi (2006b)
(and also conventional Japanese parsers) erro-
neously analyzes the head of pointo-wa (point-
TOP)&apos; as matomete (organize), whereas the cor-
rect head is koto-desu (be that), as shown in struc-
ture (a1) in Figure 1.
This error is caused by the incorrect selection
of the case frame matomeru:6 (organize), which is
shown in Table 1. This case frame locally matches
the input predicate-argument structure “pointo-wa
hitotsu-ni matomeru” (organize points). There-
fore, this method considers only intra-clause re-
lations, and falls into local optimum.
If we consider the wide range of two clauses,
this error can be corrected. In structure (a1)
in Figure 1, the generative probability of case
frame transition, P(matomeru:6|okuru:6), is con-
sidered. This probability value is very low, be-
cause there are few relations between the case
frame matomeru:6 (organize) and the case frame
okuru:6 (send baggage) in corpora.
Consequently, structure (c1) is chosen as the
correct one, where both intra-clause and inter-
clause relations can be interpreted by the case
</bodyText>
<footnote confidence="0.988611">
&apos;In this paper, we use the following abbreviations:
NOM (nominative), ACC (accusative), ABL (ablative),
CMI (comitative) and TOP (topic marker).
</footnote>
<tableCaption confidence="0.99302275">
Table 1: Case frame examples for matomeru and
okuru. “CS” represents case slot. Argument words
are written only in English. “&lt;num&gt;” represents
the class of numerals.
</tableCaption>
<table confidence="0.995222777777778">
case frame ID CS example words
. ...
. ...
.
matomeru:5 ga I, person, ...
(pack) wo baggage, luggage, variables, ...
ni &lt;num&gt;, pieces, compact,...
matomeru:6 ga doctor, ...
(organize) wo point, singularity, ...
ni &lt;num&gt;, pieces, below,...
. ...
. ...
.
okuru:1 ga person, I, ...
(send) wo mail, message, information, ...
ni friend, address, direction, ...
de mail, post, postage,...
. ...
. ...
.
okuru:6 ga woman,...
(send) wo baggage, supply, goods,...
ni person, Japan, parental house, ...
de mail, post, courier,...
. ...
. ...
.
</table>
<bodyText confidence="0.9737735">
frames and the transition knowledge between case
frames.
</bodyText>
<sectionHeader confidence="0.925088" genericHeader="method">
3 Acquiring Transition Knowledge
between Case Frames
</sectionHeader>
<bodyText confidence="0.9985683">
We automatically acquire large-scale transition
knowledge of inter-clause relations from a raw
corpus. The following two points are different
from previous studies on the acquisition of inter-
clause knowledge such as entailment/synonym
knowledge (Lin and Pantel, 2001; Torisawa, 2006;
Pekar, 2006; Zanzotto et al., 2006), verb relation
knowledge (Chklovski and Pantel, 2004), causal
knowledge (Inui et al., 2005) and event relation
knowledge (Abe et al., 2008):
</bodyText>
<listItem confidence="0.740489857142857">
• the unit of knowledge is disambiguated and
generalized
The unit in previous studies was a verb or a
verb phrase, in which verb sense ambiguities
still remain. Our unit is case frames that are
semantically disambiguated.
• the variation of relations is not limited
</listItem>
<bodyText confidence="0.99858775">
Although previous studies focused on lim-
ited kinds of semantic relations, we compre-
hensively collect generic relations between
clauses.
</bodyText>
<page confidence="0.986643">
110
</page>
<bodyText confidence="0.999975833333333">
In this section, we first describe our unit of
transition knowledge, case frames, briefly. We
then detail the acquisition method of the transition
knowledge, and report experimental results. Fi-
nally, we refer to related work to the acquisition of
such knowledge.
</bodyText>
<subsectionHeader confidence="0.9963215">
3.1 The Unit of Transition Knowledge: Case
Frames
</subsectionHeader>
<bodyText confidence="0.999982555555556">
In this paper, we regard case frames as the unit of
transition knowledge. Case frames are constructed
from unambiguous structures and are semantically
clustered according to their meanings and usages.
Therefore, case frames can be a less ambiguous
and more generalized unit than a verb and a verb
phrase. Due to these characteristics, case frames
are a suitable unit for acquiring transition knowl-
edge and weaken the influence of data sparseness.
</bodyText>
<subsectionHeader confidence="0.6231745">
3.1.1 Automatic Construction of Case
Frames
</subsectionHeader>
<bodyText confidence="0.999989361111111">
We employ the method of Kawahara and Kuro-
hashi (2006a) to automatically construct case
frames. In this section, we outline the method for
constructing the case frames.
In this method, a large raw corpus is auto-
matically parsed, and the case frames are con-
structed from argument-head examples in the re-
sulting parses. The problems in automatic case
frame construction are syntactic and semantic am-
biguities. In other words, the parsing results in-
evitably contain errors, and verb senses are intrin-
sically ambiguous. To cope with these problems,
case frames are gradually constructed from reli-
able argument-head examples.
First, argument-head examples that have no
syntactic ambiguity are extracted, and they are dis-
ambiguated by a pair comprising a verb and its
closest case component. Such pairs are explic-
itly expressed on the surface of the text and can be
considered to play an important role in conveying
the meaning of a sentence. For instance, exam-
ples are distinguished not by verbs (e.g., “tsumu”
(load/accumulate)), but by pairs (e.g., “nimotsu-
wo tsumu” (load baggage) and “keiken-wo tsumu”
(accumulate experience)). argument-head exam-
ples are aggregated in this manner, and they yield
basic case frames.
Thereafter, the basic case frames are clustered
in order to merge similar case frames, including
similar case frames that are made from scram-
bled sentences. For example, since “nimotsu-
wo tsumu” (load baggage) and “busshi-wo tsumu”
(load supply) are similar, they are clustered to-
gether. The similarity is measured by using a dis-
tributional thesaurus based on the study described
in Lin (1998).
</bodyText>
<subsectionHeader confidence="0.9887945">
3.2 Acquisition of Transition Knowledge
from Large Corpus
</subsectionHeader>
<bodyText confidence="0.999207333333333">
To acquire the transition knowledge, we collect the
clause pairs in a large raw corpus that have a de-
pendency relation and represent them as pairs of
case frames. For example, from the following sen-
tence, a case frame pair, (matomeru:5, okuru:6), is
extracted.
</bodyText>
<listItem confidence="0.529562">
(2) nimotsu-wo matomete, takuhaibin-de
baggage-ACC pack courier-CMI
</listItem>
<bodyText confidence="0.926593">
okutta
sent
(packed one’s baggage and sent (it) using
courier service)
These case frames are determined by applying
a conventional case structure analyzer (Kawa-
hara and Kurohashi, 2006b), which selects the
case frames most similar to the input expres-
sions “nimotu-wo matomeru” (pack baggage) and
“takuhaibin-de okuru” (send with courier service)
from among the case frames of matomeru (or-
ganize/settle/pack/...) and okuru (send/remit/see
off/...); some of the case frames of matomeru and
okuru are listed in Table 1.
We adopt the following steps to acquire the tran-
sition knowledge between case frames:
</bodyText>
<listItem confidence="0.991339888888889">
1. Apply dependency and case structure analy-
sis to assign case frame IDs to each clause in
a large raw corpus.
2. Collect clause pairs that have a dependency
relation, and represent them as pairs of case
frame IDs.
3. Count the frequency of each pair of case
frame IDs; these statistics are used in the
analysis described in Section 4.
</listItem>
<bodyText confidence="0.6406965">
At step 2, we collect both syntactically ambigu-
ous and unambiguous relations in order to allevi-
ate data sparseness. The influence of a small num-
ber of dependency parsing errors would be hidden
by a large number of correct (unambiguous) rela-
tions.
</bodyText>
<page confidence="0.998752">
111
</page>
<tableCaption confidence="0.997833">
Table 2: Examples of automatically acquired transition knowledge between case frames.
</tableCaption>
<bodyText confidence="0.772899210526316">
pairs of case frame IDs
(okuru:1, okuru:6)
(aru:1, okuru:6)
(suru:1, okuru:6)
(issyoda:10, okuru:6)
(kaku:1, okuru:6)
...
(matomeru:5, okuru:6)
(dasu:3, okuru:6)
...
meaning
(send mails, send baggage)
(have, send baggage)
(do, send baggage)
(get together, send baggage)
(write, send baggage)
...
(pack, send baggage)
(indicate, send baggage)
</bodyText>
<figure confidence="0.982682909090909">
...
freq.
186
150
134
118
115
...
12
12
...
</figure>
<subsectionHeader confidence="0.9330645">
3.3 Experiments of Acquiring Transition
Knowledge between Case Frames
</subsectionHeader>
<bodyText confidence="0.999935838709677">
To obtain the case frames and the transition knowl-
edge between case frames, we first built a Japanese
Web corpus by using the method of Kawahara and
Kurohashi (2006a). We first crawled 100 million
Japanese Web pages, and then, we extracted and
unduplicated Japanese sentences from the Web
pages. Consequently, we developed a Web corpus
consisting of 1.6 billion Japanese sentences.
Using the procedure of case frame construction
presented in Section 3.1.1, we constructed case
frames from the whole Web corpus. They con-
sisted of 43,000 predicates, and the average num-
ber of case frames for a predicate was 22.2.
Then, we acquired the transition knowledge be-
tween case frames using 500 million sentences of
the Web corpus. The resulting knowledge con-
sisted of 108 million unique case frame pairs. Ta-
ble 2 lists some examples of the acquired transition
knowledge. In the acquired transition knowledge,
we can find various kinds of relation such as en-
tailment, cause-effect and temporal relations.
Let us compare this result with the results of
previous studies. For example, Chklovski and
Pantel (2004) obtained 29,165 verb pairs for sev-
eral semantic relations in VerbOcean. The tran-
sition knowledge acquired in this study is several
thousand times larger than that in VerbOcean. It
is very difficult to make a meaningful compari-
son, but it can be seen that we have succeeded in
acquiring generic transition knowledge on a large
scale.
</bodyText>
<sectionHeader confidence="0.971993" genericHeader="method">
3.4 Related Work
</sectionHeader>
<bodyText confidence="0.99998317948718">
In order to realize practical natural language pro-
cessing (NLP) systems such as intelligent dialog
systems, a lot of effort has been made to develop
world knowledge or inference knowledge. For ex-
ample, in the CYC (Lenat, 1995) and Open Mind
(Stork, 1999) projects, such knowledge has been
obtained manually, but it is difficult to manually
develop broad-coverage knowledge that is suffi-
cient for practical use in NLP applications.
On the other hand, the automatic acquisition of
such inference knowledge from corpora has at-
tracted much attention in recent years. First, se-
mantic knowledge between entities has been au-
tomatically obtained (Girju and Moldovan, 2002;
Ravichandran and Hovy, 2002; Pantel and Pennac-
chiotti, 2006). For example, Pantel and Pennac-
chiotti (2006) proposed the Espresso algorithm,
which iteratively acquires entity pairs and extrac-
tion patterns using reciprocal relationship between
entities and patterns.
As for the acquisition of the knowledge be-
tween events or clauses, which is most relevant
to this study, many approaches have been adopted
to acquire entailment knowledge. Lin and Pan-
tel (2001) and Szpektor and Dagan (2008) learned
entailment rules based on distributional similar-
ity between instances that have a relation to a
rule. Torisawa (2006) extracted entailment knowl-
edge using coordinated verb pairs and noun-verb
co-occurrences. Pekar (2006) also collected en-
tailment knowledge with discourse structure con-
straints. Zanzotto et al. (2006) obtained entailment
knowledge using nominalized verbs.
There have been some studies on relations other
than entailment relations. Chklovski and Pan-
tel (2004) obtained verb pairs that have one of
five semantic relations by using a search engine.
Inui et al. (2005) classified the occurrences of
the Japanese connective marker tame. Abe et al.
</bodyText>
<page confidence="0.99665">
112
</page>
<bodyText confidence="0.999916">
(2008) learned event relation knowledge for two
semantic relations. They first gave seed pairs of
verbs or verb phrases and extracted the patterns
that matched these seed pairs. Subsequently, by
using the Espresso algorithm (Pantel and Pennac-
chiotti, 2006), this process was iterated to augment
both instances and patterns. The acquisition unit
in these studies was a verb or a verb phrase.
In contrast to these studies, we obtained generic
transition knowledge between case frames without
limiting target semantic relations.
</bodyText>
<sectionHeader confidence="0.964007333333333" genericHeader="method">
4 Incorporating Transition Knowledge
into Dependency and Case Structure
Analysis
</sectionHeader>
<bodyText confidence="0.999918923076923">
We employ the probabilistic generative model of
dependency and case structure analysis (Kawahara
and Kurohashi, 2006b) as a base model. We incor-
porate the obtained transition knowledge into this
base parser.
Our model assigns a probability to each possi-
ble dependency structure, T, and case structure,
L, of the input sentence, S, and outputs the de-
pendency and case structure that have the highest
probability. In other words, the model selects the
dependency structure Tbest and the case structure
Lbest that maximize the probability P(T, L|S) or
its equivalent, P(T, L, S), as follows:
</bodyText>
<equation confidence="0.9953915">
(Tbest, Lbest) = argmax (T L)P(T, L|S)
P(T, L, S)
P(S)
= argmax (T L)P(T, L, S). (1)
</equation>
<bodyText confidence="0.99987925">
The last equation follows from the fact that P(S)
is constant.
In the model, a clause (or predicate-argument
structure) is considered as a generation unit and
the input sentence is generated from the end of the
sentence. The probability P(T, L, S) is defined
as the product of the probabilities of generating
clauses Ci as follows:
</bodyText>
<equation confidence="0.942575">
∏
P(T,L,S) = Ci∈SP(Ci|Ch), (2)
</equation>
<bodyText confidence="0.999958176470588">
where Ch is the modifying clause of Ci. Since the
Japanese language is head final, the main clause at
the end of a sentence does not have a modifying
head; we account for this by assuming Ch = EOS
(End Of Sentence).
The probability P(Ci|Ch) is defined in a man-
ner similar to that in Kawahara and Kurohashi
(2006b). However, the difference between the
probability in the above-mentioned study and that
in our study is the generative probability of the
case frames, i.e., the probability of generating a
case frame CFi from its modifying case frame
CFh. The base model approximated this proba-
bility as the product of the probability of gener-
ating a predicate vi from its modifying predicate
vh and the probability of generating a case frame
CFi from the predicate vi as follows:
</bodyText>
<equation confidence="0.9999335">
P(CFi|CFh) �
P(vi|vh) x P(CFi|vi). (3)
</equation>
<bodyText confidence="0.999410333333333">
Our proposed model directly estimates the proba-
bility P(CFi|CFh) and considers the transition
likelihood between case frames. This probabil-
ity is calculated from the transition knowledge be-
tween case frames using maximum likelihood.
In practice, to avoid the data sparseness prob-
lem, we interpolate the probability P(CFi|CFh)
with the probability of generating predicates,
P(vi|vh), as follows:
</bodyText>
<equation confidence="0.9996675">
P′(CFi|CFh) �
AP(CFi|CFh) + (1 − A)P(vi|vh), (4)
</equation>
<bodyText confidence="0.999698666666667">
where A is determined using the frequencies of the
case frame pairs, (CFi, CFh), in the same man-
ner as in Collins (1999).
</bodyText>
<sectionHeader confidence="0.999164" genericHeader="conclusions">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99786152631579">
We evaluated the dependency structures that were
output by our new dependency parser. The case
frames used in these experiments are the same as
those described in Section 3.3, which were au-
tomatically constructed from 1.6 billion Japanese
sentences obtained from the Web.
In this study, the parameters related to unlexi-
cal types were calculated from the Kyoto Univer-
sity Text Corpus, which is a small tagged corpus
of newspaper articles, and lexical parameters were
obtained from a large Web corpus. To evaluate the
effectiveness of our model, our experiments were
conducted using sentences obtained from the Web.
As a test corpus, we used 759 Web sentences2,
which were manually annotated using the same
criteria as those in the case of the Kyoto Univer-
sity Text Corpus. We also used the Kyoto Univer-
sity Text Corpus as a development corpus to op-
timize some smoothing parameters. The system
</bodyText>
<footnote confidence="0.884414">
2The test set was not used to construct case frames and
estimate probabilities.
</footnote>
<equation confidence="0.372834">
= argmax (T L)
</equation>
<page confidence="0.99918">
113
</page>
<tableCaption confidence="0.999907">
Table 3: The dependency accuracies in our experiments.
</tableCaption>
<table confidence="0.9940735">
syn syn+case syn+case+cons
all 4,555/5,122 (88.9%) 4,581/5,122 (89.4%) 4,599/5,122 (89.8%)
NP—*VP 2,115/2,383 (88.8%) 2,142/2,383 (89.9%) 2,151/2,383 (90.3%)
NP—*NP 1,068/1,168 (91.4%) 1,068/1,168 (91.4%) 1,068/1,168 (91.4%)
VP—*VP 779/928 (83.9%) 777/928 (83.7%) 783/928 (84.4%)
VP—*NP 579/623 (92.9%) 579/623 (92.9%) 582/623 (93.4%)
</table>
<bodyText confidence="0.9983369">
input was automatically tagged using the JUMAN
morphological analyzer 3.
We used two baseline systems for the purposes
of comparison: a rule-based dependency parser
(Kurohashi and Nagao, 1994) and the probabilistic
generative model of dependency and case struc-
ture analysis (Kawahara and Kurohashi, 2006b)4.
We use the above-mentioned case frames also in
the latter baseline parser, which also requires au-
tomatically constructed case frames.
</bodyText>
<subsectionHeader confidence="0.998925">
5.1 Evaluation of Dependency Structures
</subsectionHeader>
<bodyText confidence="0.999993789473684">
We evaluated the obtained dependency structures
in terms of phrase-based dependency accuracy —
the proportion of correct dependencies out of all
dependencies5.
Table 3 lists the dependency accuracies. In this
table, “syn” represents the rule-based dependency
parser, “syn+case” represents the probabilistic
parser of syntactic and case structure (Kawahara
and Kurohashi, 2006b)6, and “syn+case+cons”
represents our proposed model. In the table, the
dependency accuracies are classified into four cat-
egories on the basis of the phrase classes (VP:
verb phrase7 and NP: noun phrase) of a dependent
and its head. The parser “syn+case+cons” signif-
icantly outperformed the two baselines for “all”
(McNemar’s test; p &lt; 0.05). In particular, the ac-
curacy of the intra-clause (predicate-argument) re-
lations (“NP—*VP”) was improved by 1.5% from
“syn” and by 0.4% from “syn+case.” These im-
</bodyText>
<footnote confidence="0.997191857142857">
3http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/juman-e.html
4http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/knp-e.html
5Since Japanese is head-final, the second to last phrase
unambiguously depends on the last phrase. However, we in-
clude such dependencies into our evaluation as in most of
previous studies.
6The accuracy described in Kawahara and Kurohashi
(2006b) is different from that of this paper due to the different
evaluation measure excluding the unambiguous dependencies
of the second last phrases.
7VP includes not only verbs but also adjectives and nouns
with copula.
</footnote>
<bodyText confidence="0.999847222222222">
provements are due to the incorporation of the
transition knowledge into syntactic/case structure
analysis.
In order to compare our results with a state-of-
the-art discriminative dependency parser, we in-
put the test corpus into an SVM-based Japanese
dependency parser, CaboCha8(Kudo and Mat-
sumoto, 2002), which was trained using the Kyoto
University Text Corpus. Its dependency accuracy
was 88.6% (4,540/5,122), which is close to that of
“syn.” This low accuracy is attributed to the lack
of knowledge of both intra-clause and inter-clause
relations. Another cause of the low accuracy is the
out-of-domain training corpus. In other words, the
parser was trained on a newspaper corpus, while
the test corpus was obtained from the Web because
a tagged Web corpus that is large enough to train
a supervised parser is not available.
</bodyText>
<subsectionHeader confidence="0.989477">
5.2 Discussions
</subsectionHeader>
<bodyText confidence="0.99680795">
Figure 2 shows some improved analyses; here, the
dotted lines represent the results of the analysis
performed using the baseline “syn + case,” and
the solid lines represent the analysis performed
using the proposed method, “syn+case+cons.”
These sentences are incorrectly analyzed by the
baseline but correctly analyzed by the proposed
method. For example, in sentence (a), the head of
gunegunemichi-wo (winding road-ACC) was cor-
rectly analyzed as yurareru (be jolted). This is
because the case frame of “basu-ni yurareru” (be
jolted by bus) is likely to generate tatsu (stand)
that does not take the wo (ACC) slot. In this man-
ner, by considering the transition knowledge be-
tween case frames, the selection of case frames
became accurate, and thus, the accuracy of the
dependencies within clauses (predicate-argument
structures) was improved.
In the case of the dependencies between pred-
icates (VP—*VP), however, only small improve-
</bodyText>
<footnote confidence="0.996935">
8http://chasen.org/∼taku/software/
cabocha/
</footnote>
<page confidence="0.99584">
114
</page>
<figure confidence="0.9839795">
❄ ❄
ama basu-ni yu
(a) gunegunemichi-wo tattamrareru toko-wo kakugoshimashita.
winding road-ACC stand bus-DAT be jolted (that)-ACC be resolved
(be resolved to be jolted standing on the bus by the winding road.)
❄
(b) nanika-wo eru tame-ni suteta mono-nimo miren-wo nokoshiteiru.
something-ACC get for discarded thing-also lingering desire-ACC retain
(retain a lingering desire also for the thing that was discarded to get something.)
(c) senbei-no hako-wa, kankaku-wo akete chinretsusareteiruno-ga mata yoi.
rice cracker-GEN box-TOP interval-ACC place be displayed-NOM also good
(It is also good that boxes of rice cracker are displayed placing an interval.)
</figure>
<figureCaption confidence="0.99415">
Figure 2: Improved examples.
</figureCaption>
<figure confidence="0.98533175">
❄ ❄
(d) ketsuron-kara itteshimaeba, kaitearukoto-wa machigattenaishi, juyouna kotodato-wa wakaru.
conclusion-ABL say content-TOP not wrong important (that)-TOP understand
(Saying from conclusions, the content is not wrong and (I) understand that (it) is important)
</figure>
<figureCaption confidence="0.802953">
Figure 3: An erroneous example.
</figureCaption>
<figure confidence="0.75928">
❄
❄
❄
</figure>
<bodyText confidence="0.999883357142857">
ments were achieved by using the transition
knowledge between case frames. This is mainly
because the heads of the predicates are intrinsi-
cally ambiguous in many cases.
For example, in sentence (d) in Figure 3, the
correct head of itteshimaeba (say) is wakaru (un-
derstand) as designated by the solid line, but our
model incorrectly judged the head to be machigat-
teinaishi, (not wrong) as designated by the dotted
line. However, in this case, both the phrases that
are being modified are semantically related to the
modifier. To solve this problem, it is necessary to
re-consider the evaluation metrics of dependency
parsing.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="acknowledgments">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999962583333333">
In this paper, we have described a method for ac-
quiring the transition knowledge of inter-clause re-
lations and a method for incorporating this knowl-
edge into dependency and case structure analy-
sis. The significance of this study is that the pro-
posed parsing method selects the best dependency
and case structures that are consistent within each
clause and between clauses. We confirmed that
this method contributed to the improvement of the
dependency parsing of Japanese.
The case frames that are acquired from 1.6 bil-
lion Japanese sentences have been made freely
available to the public9. In addition, we are prepar-
ing to make the acquired transition knowledge ac-
cessible on the Web.
In future, we will investigate the iteration of
knowledge acquisition and parsing based on the
acquired knowledge. Since our parser is a gener-
ative model, we are expecting a performance gain
by the iteration. Furthermore, we would like to ex-
plore the use of the transition knowledge between
case frames to improve NLP applications such as
recognizing textual entailment (RTE) and sentence
generation.
</bodyText>
<sectionHeader confidence="0.994962" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.852964818181818">
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Acquiring event relation knowledge by learning
cooccurrence patterns and fertilizing cooccurrence
samples with verbal nouns. In Proceedings of IJC-
NLP2008, pages 497–504.
Takeshi Abekawa and Manabu Okumura. 2006.
Japanese dependency parsing using co-occurrence
information and a combination of case elements. In
Proceedings of COLING-ACL2006, pages 833–840.
Daniel M. Bikel. 2004. Intricacies of Collins’ parsing
model. Computational Linguistics, 30(4):479–511.
</reference>
<footnote confidence="0.8942065">
9http://nlp.kuee.kyoto-u.ac.jp/
nl-resource/caseframe-e.html
</footnote>
<page confidence="0.99648">
115
</page>
<reference confidence="0.999921835051546">
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL-X, pages 149–164.
Xavier Carreras. 2007. Experiments with a higher-
order projective dependency parser. In Proceedings
of EMNLP-CoNLL2007 Shared Task, pages 957–
961.
Timothy Chklovski and Patrick Pantel. 2004. VerbO-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP2004, pages
33–40.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Roxana Girju and Dan Moldovan. 2002. Mining an-
swers for causation questions. In Proceedings of
AAAI Spring Symposium.
Takashi Inui, Kentaro Inui, and Yuji Matsumoto.
2005. Acquiring causal knowledge from text us-
ing the connective marker tame. ACM Transactions
on Asian Language Information Processing (ACM-
TALIP), 4(4):435–474.
Daisuke Kawahara and Sadao Kurohashi. 2006a.
Case frame compilation from the web using
high-performance computing. In Proceedings of
LREC2006.
Daisuke Kawahara and Sadao Kurohashi. 2006b. A
fully-lexicalized probabilistic model for Japanese
syntactic and case structure analysis. In Proceed-
ings of HLT-NAACL2006, pages 176–183.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08:HLT, pages 595–603.
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of CoNLL2002, pages 29–35.
Sadao Kurohashi and Makoto Nagao. 1994. A syn-
tactic analysis method of long Japanese sentences
based on the detection of conjunctive structures.
Computational Linguistics, 20(4):507–534.
Douglas B. Lenat. 1995. CYC: A large-scale invest-
ment in knowledge infrastructure. Communications
of the ACM, 38(11):32–38.
Dekang Lin and Patrick Pantel. 2001. DIRT - discov-
ery of inference rules from text. In Proceedings of
ACM SIGKDD Conference on Knowledge Discovery
and Data Mining, pages 323–328.
Dekang Lin. 1998. Automatic retrieval and cluster-
ing of similar words. In Proceedings of COLING-
ACL98, pages 768–774.
Mitchell Marcus, Beatrice Santorini, and Mary
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn Treebank. Computa-
tional Linguistics, 19(2):313–330.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL2006, pages 81–88.
Tetsuji Nakagawa. 2007. Multilingual dependency
parsing using global features. In Proceedings of
EMNLP-CoNLL2007 Shared Task, pages 952–956.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of COLING2004, pages 64–70.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 shared task on
dependency parsing. In Proceedings of EMNLP-
CoNLL2007, pages 915–932.
Patrick Pantel and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automati-
cally harvesting semantic relations. In Proceedings
of COLING-ACL2006, pages 113–120.
Viktor Pekar. 2006. Acquisition of verb entailment
from text. In Proceedings of HLT-NAACL2006,
pages 49–56.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In Proceedings of ACL2002, pages 41–47.
Manabu Sassano. 2004. Linear-time dependency anal-
ysis for Japanese. In Proceedings of COLING2004,
pages 8–14.
David G. Stork. 1999. Character and document re-
search in the open mind initiative. In Proceedings
of International Conference on Document Analysis
and Recognition, pages 1–12.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In Proceedings of
COLING2008, pages 849–856.
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using Japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of HLT-NAACL2006, pages 57–64.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In Proceedings of COLING-
ACL2006, pages 849–856.
</reference>
<page confidence="0.999026">
116
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.558348">
<title confidence="0.9988835">Capturing Consistency between Intra-clause and Inter-clause in Knowledge-rich Dependency and Case Structure Analysis</title>
<author confidence="0.988008">Daisuke Kawahara</author>
<affiliation confidence="0.886465">National Institute of Information Communications</affiliation>
<address confidence="0.780369">3-5 Hikaridai Seika-cho, Kyoto, 619-0289, Japan</address>
<email confidence="0.96631">dk@nict.go.jp</email>
<abstract confidence="0.996071736842105">We present a method for dependency and case structure analysis that captures the consistency between intra-clause relations (i.e., case structures or predicate-argument structures) and inter-clause relations. We assess intra-clause relations on the basis of case frames and inter-clause relations on the basis of transition knowledge between case frames. Both knowledge bases are automatically acquired from a massive amount of parses of a Web corpus. The significance of this study is that the proposed method selects the best dependency and case structure that are consistent within each clause and between clauses. We confirm that this method contributes to the improvement of dependency parsing of Japanese.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shuya Abe</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Acquiring event relation knowledge by learning cooccurrence patterns and fertilizing cooccurrence samples with verbal nouns.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP2008,</booktitle>
<pages>497--504</pages>
<contexts>
<context position="8774" citStr="Abe et al., 2008" startWordPosition="1301" endWordPosition="1304">e, ... de mail, post, courier,... . ... . ... . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 110 In this section, we first describe our unit of transition knowledge, case frames, briefly. We then detail the acquisition method of the transition knowledge, and report experimental results. F</context>
</contexts>
<marker>Abe, Inui, Matsumoto, 2008</marker>
<rawString>Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008. Acquiring event relation knowledge by learning cooccurrence patterns and fertilizing cooccurrence samples with verbal nouns. In Proceedings of IJCNLP2008, pages 497–504.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Abekawa</author>
<author>Manabu Okumura</author>
</authors>
<title>Japanese dependency parsing using co-occurrence information and a combination of case elements.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL2006,</booktitle>
<pages>833--840</pages>
<contexts>
<context position="3267" citStr="Abekawa and Okumura, 2006" startWordPosition="473" endWordPosition="476">r instance, Koo et al. (2008) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbitrary nodes such as sibling nodes. To solve the problem of knowledge scarcity, they learned knowledge of such predicate-argument structures from a very large number of automatically analyzed corpora (Abekawa and Okumura, 2006; Kawahara and Kurohashi, 2006b). While Abekawa and Okumura (2006) used only cooccurrence statistics of verbal arguments, Kawahara and Kurohashi (2006b) assessed predicateargument structures by checking case frames, which are semantic frames that are automatically compiled for each predicate sense from a large raw corpus. These methods outperformed the accuracy of supervised dependency parsers. In such linguistically-motivated approaches, well-formedness within a clause was considered, but coherence between clauses was not considered. Even if intra-clause relations (i.e., a predicate-argument </context>
</contexts>
<marker>Abekawa, Okumura, 2006</marker>
<rawString>Takeshi Abekawa and Manabu Okumura. 2006. Japanese dependency parsing using co-occurrence information and a combination of case elements. In Proceedings of COLING-ACL2006, pages 833–840.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<date>2004</date>
<journal>Intricacies of Collins’ parsing model. Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="2609" citStr="Bikel, 2004" startWordPosition="374" endWordPosition="375">jp i.e., second-order and higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Nakagawa, 2007). It is desirable to consider a wider-range phrase, clause, or a whole sentence, but it is difficult to judge whether the structure of such a wide-range expression is linguistically correct. One of the reasons for this is the scarcity of the knowledge required to make such a judgment. When we use the Penn Treebank (Marcus et al., 1993), which is one of the largest corpora among the available analyzed corpora, as training data, even bi-lexical dependencies cannot be learned sufficiently (Bikel, 2004). To circumvent such scarcity, for instance, Koo et al. (2008) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbitrary nodes such as sibling nodes. To solve the problem of knowledge scarcity, they learned knowledge of such predicate-argument structures from a very large number of</context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics, 30(4):479–511.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-X,</booktitle>
<pages>149--164</pages>
<contexts>
<context position="1477" citStr="Buchholz and Marsi, 2006" startWordPosition="209" endWordPosition="213">t are consistent within each clause and between clauses. We confirm that this method contributes to the improvement of dependency parsing of Japanese. 1 Introduction The approaches of dependency parsing basically assess the likelihood of a dependency relation between two words or phrases and subsequently collect all the assessments for these pairs as the dependency parse of the sentence. To improve dependency parsing, it is important to consider as broad a context as possible, rather than a word/phrase pair. In the recent evaluation workshops (shared tasks) of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, Sadao Kurohashi Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan kuro@i.kyoto-u.ac.jp i.e., second-order and higher-order features (McDonald and Pereira, 2006; Car</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CoNLL-X, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higherorder projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL2007 Shared Task,</booktitle>
<pages>957--961</pages>
<contexts>
<context position="2088" citStr="Carreras, 2007" startWordPosition="286" endWordPosition="287">006; Nivre et al., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, Sadao Kurohashi Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan kuro@i.kyoto-u.ac.jp i.e., second-order and higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Nakagawa, 2007). It is desirable to consider a wider-range phrase, clause, or a whole sentence, but it is difficult to judge whether the structure of such a wide-range expression is linguistically correct. One of the reasons for this is the scarcity of the knowledge required to make such a judgment. When we use the Penn Treebank (Marcus et al., 1993), which is one of the largest corpora among the available analyzed corpora, as training data, even bi-lexical dependencies cannot be learned sufficiently (Bikel, 2004). To circumvent such scarcity, for instance, Koo et al. (2008) proposed the use</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higherorder projective dependency parser. In Proceedings of EMNLP-CoNLL2007 Shared Task, pages 957– 961.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Chklovski</author>
<author>Patrick Pantel</author>
</authors>
<title>VerbOcean: Mining the web for fine-grained semantic verb relations.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP2004,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="8688" citStr="Chklovski and Pantel, 2004" startWordPosition="1287" endWordPosition="1290"> ... . okuru:6 ga woman,... (send) wo baggage, supply, goods,... ni person, Japan, parental house, ... de mail, post, courier,... . ... . ... . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 110 In this section, we first describe our unit of transition knowledge, case frames, briefly. We then detail </context>
<context position="14813" citStr="Chklovski and Pantel (2004)" startWordPosition="2257" endWordPosition="2260">ase frames from the whole Web corpus. They consisted of 43,000 predicates, and the average number of case frames for a predicate was 22.2. Then, we acquired the transition knowledge between case frames using 500 million sentences of the Web corpus. The resulting knowledge consisted of 108 million unique case frame pairs. Table 2 lists some examples of the acquired transition knowledge. In the acquired transition knowledge, we can find various kinds of relation such as entailment, cause-effect and temporal relations. Let us compare this result with the results of previous studies. For example, Chklovski and Pantel (2004) obtained 29,165 verb pairs for several semantic relations in VerbOcean. The transition knowledge acquired in this study is several thousand times larger than that in VerbOcean. It is very difficult to make a meaningful comparison, but it can be seen that we have succeeded in acquiring generic transition knowledge on a large scale. 3.4 Related Work In order to realize practical natural language processing (NLP) systems such as intelligent dialog systems, a lot of effort has been made to develop world knowledge or inference knowledge. For example, in the CYC (Lenat, 1995) and Open Mind (Stork, </context>
<context position="16801" citStr="Chklovski and Pantel (2004)" startWordPosition="2562" endWordPosition="2566"> relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted entailment knowledge using coordinated verb pairs and noun-verb co-occurrences. Pekar (2006) also collected entailment knowledge with discourse structure constraints. Zanzotto et al. (2006) obtained entailment knowledge using nominalized verbs. There have been some studies on relations other than entailment relations. Chklovski and Pantel (2004) obtained verb pairs that have one of five semantic relations by using a search engine. Inui et al. (2005) classified the occurrences of the Japanese connective marker tame. Abe et al. 112 (2008) learned event relation knowledge for two semantic relations. They first gave seed pairs of verbs or verb phrases and extracted the patterns that matched these seed pairs. Subsequently, by using the Espresso algorithm (Pantel and Pennacchiotti, 2006), this process was iterated to augment both instances and patterns. The acquisition unit in these studies was a verb or a verb phrase. In contrast to these</context>
</contexts>
<marker>Chklovski, Pantel, 2004</marker>
<rawString>Timothy Chklovski and Patrick Pantel. 2004. VerbOcean: Mining the web for fine-grained semantic verb relations. In Proceedings of EMNLP2004, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="20014" citStr="Collins (1999)" startWordPosition="3089" endWordPosition="3090">s follows: P(CFi|CFh) � P(vi|vh) x P(CFi|vi). (3) Our proposed model directly estimates the probability P(CFi|CFh) and considers the transition likelihood between case frames. This probability is calculated from the transition knowledge between case frames using maximum likelihood. In practice, to avoid the data sparseness problem, we interpolate the probability P(CFi|CFh) with the probability of generating predicates, P(vi|vh), as follows: P′(CFi|CFh) � AP(CFi|CFh) + (1 − A)P(vi|vh), (4) where A is determined using the frequencies of the case frame pairs, (CFi, CFh), in the same manner as in Collins (1999). 5 Experiments We evaluated the dependency structures that were output by our new dependency parser. The case frames used in these experiments are the same as those described in Section 3.3, which were automatically constructed from 1.6 billion Japanese sentences obtained from the Web. In this study, the parameters related to unlexical types were calculated from the Kyoto University Text Corpus, which is a small tagged corpus of newspaper articles, and lexical parameters were obtained from a large Web corpus. To evaluate the effectiveness of our model, our experiments were conducted using sen</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Dan Moldovan</author>
</authors>
<title>Mining answers for causation questions.</title>
<date>2002</date>
<booktitle>In Proceedings of</booktitle>
<publisher>AAAI Spring Symposium.</publisher>
<contexts>
<context position="15827" citStr="Girju and Moldovan, 2002" startWordPosition="2421" endWordPosition="2424">anguage processing (NLP) systems such as intelligent dialog systems, a lot of effort has been made to develop world knowledge or inference knowledge. For example, in the CYC (Lenat, 1995) and Open Mind (Stork, 1999) projects, such knowledge has been obtained manually, but it is difficult to manually develop broad-coverage knowledge that is sufficient for practical use in NLP applications. On the other hand, the automatic acquisition of such inference knowledge from corpora has attracted much attention in recent years. First, semantic knowledge between entities has been automatically obtained (Girju and Moldovan, 2002; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). For example, Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which iteratively acquires entity pairs and extraction patterns using reciprocal relationship between entities and patterns. As for the acquisition of the knowledge between events or clauses, which is most relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule.</context>
</contexts>
<marker>Girju, Moldovan, 2002</marker>
<rawString>Roxana Girju and Dan Moldovan. 2002. Mining answers for causation questions. In Proceedings of AAAI Spring Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takashi Inui</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Acquiring causal knowledge from text using the connective marker tame.</title>
<date>2005</date>
<journal>ACM Transactions on Asian Language Information Processing (ACMTALIP),</journal>
<volume>4</volume>
<issue>4</issue>
<contexts>
<context position="8726" citStr="Inui et al., 2005" startWordPosition="1293" endWordPosition="1296">supply, goods,... ni person, Japan, parental house, ... de mail, post, courier,... . ... . ... . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 110 In this section, we first describe our unit of transition knowledge, case frames, briefly. We then detail the acquisition method of the transiti</context>
<context position="16907" citStr="Inui et al. (2005)" startWordPosition="2582" endWordPosition="2585">nd Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted entailment knowledge using coordinated verb pairs and noun-verb co-occurrences. Pekar (2006) also collected entailment knowledge with discourse structure constraints. Zanzotto et al. (2006) obtained entailment knowledge using nominalized verbs. There have been some studies on relations other than entailment relations. Chklovski and Pantel (2004) obtained verb pairs that have one of five semantic relations by using a search engine. Inui et al. (2005) classified the occurrences of the Japanese connective marker tame. Abe et al. 112 (2008) learned event relation knowledge for two semantic relations. They first gave seed pairs of verbs or verb phrases and extracted the patterns that matched these seed pairs. Subsequently, by using the Espresso algorithm (Pantel and Pennacchiotti, 2006), this process was iterated to augment both instances and patterns. The acquisition unit in these studies was a verb or a verb phrase. In contrast to these studies, we obtained generic transition knowledge between case frames without limiting target semantic re</context>
</contexts>
<marker>Inui, Inui, Matsumoto, 2005</marker>
<rawString>Takashi Inui, Kentaro Inui, and Yuji Matsumoto. 2005. Acquiring causal knowledge from text using the connective marker tame. ACM Transactions on Asian Language Information Processing (ACMTALIP), 4(4):435–474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Case frame compilation from the web using high-performance computing.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC2006.</booktitle>
<contexts>
<context position="3297" citStr="Kawahara and Kurohashi, 2006" startWordPosition="477" endWordPosition="480">8) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbitrary nodes such as sibling nodes. To solve the problem of knowledge scarcity, they learned knowledge of such predicate-argument structures from a very large number of automatically analyzed corpora (Abekawa and Okumura, 2006; Kawahara and Kurohashi, 2006b). While Abekawa and Okumura (2006) used only cooccurrence statistics of verbal arguments, Kawahara and Kurohashi (2006b) assessed predicateargument structures by checking case frames, which are semantic frames that are automatically compiled for each predicate sense from a large raw corpus. These methods outperformed the accuracy of supervised dependency parsers. In such linguistically-motivated approaches, well-formedness within a clause was considered, but coherence between clauses was not considered. Even if intra-clause relations (i.e., a predicate-argument structure within a clause) are</context>
<context position="6314" citStr="Kawahara and Kurohashi (2006" startWordPosition="919" endWordPosition="922">se structure of pointo-wa (point-TOP) in the following sentence: (1) pointo-wa, hitotsu-ni matomete point-TOP one-DAT pack takuhaibin-de okuru koto-desu courier-CMI send be that (The point is that (we) pack (one’s baggage) and send (it) using courier service.) The correct structure is (c1), which is surrounded by the dotted rectangle. Structures (c2), (c3) and so on have the same dependency structure as (c1), but have incorrect case structures, in which incorrect case frames are selected. Note that matomeru:5, okuru:6 and so on in the figure represent the IDs of the case frames. The parser of Kawahara and Kurohashi (2006b) (and also conventional Japanese parsers) erroneously analyzes the head of pointo-wa (pointTOP)&apos; as matomete (organize), whereas the correct head is koto-desu (be that), as shown in structure (a1) in Figure 1. This error is caused by the incorrect selection of the case frame matomeru:6 (organize), which is shown in Table 1. This case frame locally matches the input predicate-argument structure “pointo-wa hitotsu-ni matomeru” (organize points). Therefore, this method considers only intra-clause relations, and falls into local optimum. If we consider the wide range of two clauses, this error c</context>
<context position="10038" citStr="Kawahara and Kurohashi (2006" startWordPosition="1501" endWordPosition="1505">the acquisition of such knowledge. 3.1 The Unit of Transition Knowledge: Case Frames In this paper, we regard case frames as the unit of transition knowledge. Case frames are constructed from unambiguous structures and are semantically clustered according to their meanings and usages. Therefore, case frames can be a less ambiguous and more generalized unit than a verb and a verb phrase. Due to these characteristics, case frames are a suitable unit for acquiring transition knowledge and weaken the influence of data sparseness. 3.1.1 Automatic Construction of Case Frames We employ the method of Kawahara and Kurohashi (2006a) to automatically construct case frames. In this section, we outline the method for constructing the case frames. In this method, a large raw corpus is automatically parsed, and the case frames are constructed from argument-head examples in the resulting parses. The problems in automatic case frame construction are syntactic and semantic ambiguities. In other words, the parsing results inevitably contain errors, and verb senses are intrinsically ambiguous. To cope with these problems, case frames are gradually constructed from reliable argument-head examples. First, argument-head examples th</context>
<context position="12165" citStr="Kawahara and Kurohashi, 2006" startWordPosition="1833" endWordPosition="1837">al thesaurus based on the study described in Lin (1998). 3.2 Acquisition of Transition Knowledge from Large Corpus To acquire the transition knowledge, we collect the clause pairs in a large raw corpus that have a dependency relation and represent them as pairs of case frames. For example, from the following sentence, a case frame pair, (matomeru:5, okuru:6), is extracted. (2) nimotsu-wo matomete, takuhaibin-de baggage-ACC pack courier-CMI okutta sent (packed one’s baggage and sent (it) using courier service) These case frames are determined by applying a conventional case structure analyzer (Kawahara and Kurohashi, 2006b), which selects the case frames most similar to the input expressions “nimotu-wo matomeru” (pack baggage) and “takuhaibin-de okuru” (send with courier service) from among the case frames of matomeru (organize/settle/pack/...) and okuru (send/remit/see off/...); some of the case frames of matomeru and okuru are listed in Table 1. We adopt the following steps to acquire the transition knowledge between case frames: 1. Apply dependency and case structure analysis to assign case frame IDs to each clause in a large raw corpus. 2. Collect clause pairs that have a dependency relation, and represent</context>
<context position="13877" citStr="Kawahara and Kurohashi (2006" startWordPosition="2108" endWordPosition="2111">frames. pairs of case frame IDs (okuru:1, okuru:6) (aru:1, okuru:6) (suru:1, okuru:6) (issyoda:10, okuru:6) (kaku:1, okuru:6) ... (matomeru:5, okuru:6) (dasu:3, okuru:6) ... meaning (send mails, send baggage) (have, send baggage) (do, send baggage) (get together, send baggage) (write, send baggage) ... (pack, send baggage) (indicate, send baggage) ... freq. 186 150 134 118 115 ... 12 12 ... 3.3 Experiments of Acquiring Transition Knowledge between Case Frames To obtain the case frames and the transition knowledge between case frames, we first built a Japanese Web corpus by using the method of Kawahara and Kurohashi (2006a). We first crawled 100 million Japanese Web pages, and then, we extracted and unduplicated Japanese sentences from the Web pages. Consequently, we developed a Web corpus consisting of 1.6 billion Japanese sentences. Using the procedure of case frame construction presented in Section 3.1.1, we constructed case frames from the whole Web corpus. They consisted of 43,000 predicates, and the average number of case frames for a predicate was 22.2. Then, we acquired the transition knowledge between case frames using 500 million sentences of the Web corpus. The resulting knowledge consisted of 108 m</context>
<context position="17713" citStr="Kawahara and Kurohashi, 2006" startWordPosition="2701" endWordPosition="2704">of verbs or verb phrases and extracted the patterns that matched these seed pairs. Subsequently, by using the Espresso algorithm (Pantel and Pennacchiotti, 2006), this process was iterated to augment both instances and patterns. The acquisition unit in these studies was a verb or a verb phrase. In contrast to these studies, we obtained generic transition knowledge between case frames without limiting target semantic relations. 4 Incorporating Transition Knowledge into Dependency and Case Structure Analysis We employ the probabilistic generative model of dependency and case structure analysis (Kawahara and Kurohashi, 2006b) as a base model. We incorporate the obtained transition knowledge into this base parser. Our model assigns a probability to each possible dependency structure, T, and case structure, L, of the input sentence, S, and outputs the dependency and case structure that have the highest probability. In other words, the model selects the dependency structure Tbest and the case structure Lbest that maximize the probability P(T, L|S) or its equivalent, P(T, L, S), as follows: (Tbest, Lbest) = argmax (T L)P(T, L|S) P(T, L, S) P(S) = argmax (T L)P(T, L, S). (1) The last equation follows from the fact th</context>
<context position="18945" citStr="Kawahara and Kurohashi (2006" startWordPosition="2917" endWordPosition="2920">(S) is constant. In the model, a clause (or predicate-argument structure) is considered as a generation unit and the input sentence is generated from the end of the sentence. The probability P(T, L, S) is defined as the product of the probabilities of generating clauses Ci as follows: ∏ P(T,L,S) = Ci∈SP(Ci|Ch), (2) where Ch is the modifying clause of Ci. Since the Japanese language is head final, the main clause at the end of a sentence does not have a modifying head; we account for this by assuming Ch = EOS (End Of Sentence). The probability P(Ci|Ch) is defined in a manner similar to that in Kawahara and Kurohashi (2006b). However, the difference between the probability in the above-mentioned study and that in our study is the generative probability of the case frames, i.e., the probability of generating a case frame CFi from its modifying case frame CFh. The base model approximated this probability as the product of the probability of generating a predicate vi from its modifying predicate vh and the probability of generating a case frame CFi from the predicate vi as follows: P(CFi|CFh) � P(vi|vh) x P(CFi|vi). (3) Our proposed model directly estimates the probability P(CFi|CFh) and considers the transition l</context>
<context position="21714" citStr="Kawahara and Kurohashi, 2006" startWordPosition="3345" endWordPosition="3348">se syn+case+cons all 4,555/5,122 (88.9%) 4,581/5,122 (89.4%) 4,599/5,122 (89.8%) NP—*VP 2,115/2,383 (88.8%) 2,142/2,383 (89.9%) 2,151/2,383 (90.3%) NP—*NP 1,068/1,168 (91.4%) 1,068/1,168 (91.4%) 1,068/1,168 (91.4%) VP—*VP 779/928 (83.9%) 777/928 (83.7%) 783/928 (84.4%) VP—*NP 579/623 (92.9%) 579/623 (92.9%) 582/623 (93.4%) input was automatically tagged using the JUMAN morphological analyzer 3. We used two baseline systems for the purposes of comparison: a rule-based dependency parser (Kurohashi and Nagao, 1994) and the probabilistic generative model of dependency and case structure analysis (Kawahara and Kurohashi, 2006b)4. We use the above-mentioned case frames also in the latter baseline parser, which also requires automatically constructed case frames. 5.1 Evaluation of Dependency Structures We evaluated the obtained dependency structures in terms of phrase-based dependency accuracy — the proportion of correct dependencies out of all dependencies5. Table 3 lists the dependency accuracies. In this table, “syn” represents the rule-based dependency parser, “syn+case” represents the probabilistic parser of syntactic and case structure (Kawahara and Kurohashi, 2006b)6, and “syn+case+cons” represents our propos</context>
<context position="23128" citStr="Kawahara and Kurohashi (2006" startWordPosition="3542" endWordPosition="3545">d. The parser “syn+case+cons” significantly outperformed the two baselines for “all” (McNemar’s test; p &lt; 0.05). In particular, the accuracy of the intra-clause (predicate-argument) relations (“NP—*VP”) was improved by 1.5% from “syn” and by 0.4% from “syn+case.” These im3http://nlp.kuee.kyoto-u.ac.jp/ nl-resource/juman-e.html 4http://nlp.kuee.kyoto-u.ac.jp/ nl-resource/knp-e.html 5Since Japanese is head-final, the second to last phrase unambiguously depends on the last phrase. However, we include such dependencies into our evaluation as in most of previous studies. 6The accuracy described in Kawahara and Kurohashi (2006b) is different from that of this paper due to the different evaluation measure excluding the unambiguous dependencies of the second last phrases. 7VP includes not only verbs but also adjectives and nouns with copula. provements are due to the incorporation of the transition knowledge into syntactic/case structure analysis. In order to compare our results with a state-ofthe-art discriminative dependency parser, we input the test corpus into an SVM-based Japanese dependency parser, CaboCha8(Kudo and Matsumoto, 2002), which was trained using the Kyoto University Text Corpus. Its dependency accur</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006a. Case frame compilation from the web using high-performance computing. In Proceedings of LREC2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
</authors>
<title>A fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL2006,</booktitle>
<pages>176--183</pages>
<contexts>
<context position="3297" citStr="Kawahara and Kurohashi, 2006" startWordPosition="477" endWordPosition="480">8) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbitrary nodes such as sibling nodes. To solve the problem of knowledge scarcity, they learned knowledge of such predicate-argument structures from a very large number of automatically analyzed corpora (Abekawa and Okumura, 2006; Kawahara and Kurohashi, 2006b). While Abekawa and Okumura (2006) used only cooccurrence statistics of verbal arguments, Kawahara and Kurohashi (2006b) assessed predicateargument structures by checking case frames, which are semantic frames that are automatically compiled for each predicate sense from a large raw corpus. These methods outperformed the accuracy of supervised dependency parsers. In such linguistically-motivated approaches, well-formedness within a clause was considered, but coherence between clauses was not considered. Even if intra-clause relations (i.e., a predicate-argument structure within a clause) are</context>
<context position="6314" citStr="Kawahara and Kurohashi (2006" startWordPosition="919" endWordPosition="922">se structure of pointo-wa (point-TOP) in the following sentence: (1) pointo-wa, hitotsu-ni matomete point-TOP one-DAT pack takuhaibin-de okuru koto-desu courier-CMI send be that (The point is that (we) pack (one’s baggage) and send (it) using courier service.) The correct structure is (c1), which is surrounded by the dotted rectangle. Structures (c2), (c3) and so on have the same dependency structure as (c1), but have incorrect case structures, in which incorrect case frames are selected. Note that matomeru:5, okuru:6 and so on in the figure represent the IDs of the case frames. The parser of Kawahara and Kurohashi (2006b) (and also conventional Japanese parsers) erroneously analyzes the head of pointo-wa (pointTOP)&apos; as matomete (organize), whereas the correct head is koto-desu (be that), as shown in structure (a1) in Figure 1. This error is caused by the incorrect selection of the case frame matomeru:6 (organize), which is shown in Table 1. This case frame locally matches the input predicate-argument structure “pointo-wa hitotsu-ni matomeru” (organize points). Therefore, this method considers only intra-clause relations, and falls into local optimum. If we consider the wide range of two clauses, this error c</context>
<context position="10038" citStr="Kawahara and Kurohashi (2006" startWordPosition="1501" endWordPosition="1505">the acquisition of such knowledge. 3.1 The Unit of Transition Knowledge: Case Frames In this paper, we regard case frames as the unit of transition knowledge. Case frames are constructed from unambiguous structures and are semantically clustered according to their meanings and usages. Therefore, case frames can be a less ambiguous and more generalized unit than a verb and a verb phrase. Due to these characteristics, case frames are a suitable unit for acquiring transition knowledge and weaken the influence of data sparseness. 3.1.1 Automatic Construction of Case Frames We employ the method of Kawahara and Kurohashi (2006a) to automatically construct case frames. In this section, we outline the method for constructing the case frames. In this method, a large raw corpus is automatically parsed, and the case frames are constructed from argument-head examples in the resulting parses. The problems in automatic case frame construction are syntactic and semantic ambiguities. In other words, the parsing results inevitably contain errors, and verb senses are intrinsically ambiguous. To cope with these problems, case frames are gradually constructed from reliable argument-head examples. First, argument-head examples th</context>
<context position="12165" citStr="Kawahara and Kurohashi, 2006" startWordPosition="1833" endWordPosition="1837">al thesaurus based on the study described in Lin (1998). 3.2 Acquisition of Transition Knowledge from Large Corpus To acquire the transition knowledge, we collect the clause pairs in a large raw corpus that have a dependency relation and represent them as pairs of case frames. For example, from the following sentence, a case frame pair, (matomeru:5, okuru:6), is extracted. (2) nimotsu-wo matomete, takuhaibin-de baggage-ACC pack courier-CMI okutta sent (packed one’s baggage and sent (it) using courier service) These case frames are determined by applying a conventional case structure analyzer (Kawahara and Kurohashi, 2006b), which selects the case frames most similar to the input expressions “nimotu-wo matomeru” (pack baggage) and “takuhaibin-de okuru” (send with courier service) from among the case frames of matomeru (organize/settle/pack/...) and okuru (send/remit/see off/...); some of the case frames of matomeru and okuru are listed in Table 1. We adopt the following steps to acquire the transition knowledge between case frames: 1. Apply dependency and case structure analysis to assign case frame IDs to each clause in a large raw corpus. 2. Collect clause pairs that have a dependency relation, and represent</context>
<context position="13877" citStr="Kawahara and Kurohashi (2006" startWordPosition="2108" endWordPosition="2111">frames. pairs of case frame IDs (okuru:1, okuru:6) (aru:1, okuru:6) (suru:1, okuru:6) (issyoda:10, okuru:6) (kaku:1, okuru:6) ... (matomeru:5, okuru:6) (dasu:3, okuru:6) ... meaning (send mails, send baggage) (have, send baggage) (do, send baggage) (get together, send baggage) (write, send baggage) ... (pack, send baggage) (indicate, send baggage) ... freq. 186 150 134 118 115 ... 12 12 ... 3.3 Experiments of Acquiring Transition Knowledge between Case Frames To obtain the case frames and the transition knowledge between case frames, we first built a Japanese Web corpus by using the method of Kawahara and Kurohashi (2006a). We first crawled 100 million Japanese Web pages, and then, we extracted and unduplicated Japanese sentences from the Web pages. Consequently, we developed a Web corpus consisting of 1.6 billion Japanese sentences. Using the procedure of case frame construction presented in Section 3.1.1, we constructed case frames from the whole Web corpus. They consisted of 43,000 predicates, and the average number of case frames for a predicate was 22.2. Then, we acquired the transition knowledge between case frames using 500 million sentences of the Web corpus. The resulting knowledge consisted of 108 m</context>
<context position="17713" citStr="Kawahara and Kurohashi, 2006" startWordPosition="2701" endWordPosition="2704">of verbs or verb phrases and extracted the patterns that matched these seed pairs. Subsequently, by using the Espresso algorithm (Pantel and Pennacchiotti, 2006), this process was iterated to augment both instances and patterns. The acquisition unit in these studies was a verb or a verb phrase. In contrast to these studies, we obtained generic transition knowledge between case frames without limiting target semantic relations. 4 Incorporating Transition Knowledge into Dependency and Case Structure Analysis We employ the probabilistic generative model of dependency and case structure analysis (Kawahara and Kurohashi, 2006b) as a base model. We incorporate the obtained transition knowledge into this base parser. Our model assigns a probability to each possible dependency structure, T, and case structure, L, of the input sentence, S, and outputs the dependency and case structure that have the highest probability. In other words, the model selects the dependency structure Tbest and the case structure Lbest that maximize the probability P(T, L|S) or its equivalent, P(T, L, S), as follows: (Tbest, Lbest) = argmax (T L)P(T, L|S) P(T, L, S) P(S) = argmax (T L)P(T, L, S). (1) The last equation follows from the fact th</context>
<context position="18945" citStr="Kawahara and Kurohashi (2006" startWordPosition="2917" endWordPosition="2920">(S) is constant. In the model, a clause (or predicate-argument structure) is considered as a generation unit and the input sentence is generated from the end of the sentence. The probability P(T, L, S) is defined as the product of the probabilities of generating clauses Ci as follows: ∏ P(T,L,S) = Ci∈SP(Ci|Ch), (2) where Ch is the modifying clause of Ci. Since the Japanese language is head final, the main clause at the end of a sentence does not have a modifying head; we account for this by assuming Ch = EOS (End Of Sentence). The probability P(Ci|Ch) is defined in a manner similar to that in Kawahara and Kurohashi (2006b). However, the difference between the probability in the above-mentioned study and that in our study is the generative probability of the case frames, i.e., the probability of generating a case frame CFi from its modifying case frame CFh. The base model approximated this probability as the product of the probability of generating a predicate vi from its modifying predicate vh and the probability of generating a case frame CFi from the predicate vi as follows: P(CFi|CFh) � P(vi|vh) x P(CFi|vi). (3) Our proposed model directly estimates the probability P(CFi|CFh) and considers the transition l</context>
<context position="21714" citStr="Kawahara and Kurohashi, 2006" startWordPosition="3345" endWordPosition="3348">se syn+case+cons all 4,555/5,122 (88.9%) 4,581/5,122 (89.4%) 4,599/5,122 (89.8%) NP—*VP 2,115/2,383 (88.8%) 2,142/2,383 (89.9%) 2,151/2,383 (90.3%) NP—*NP 1,068/1,168 (91.4%) 1,068/1,168 (91.4%) 1,068/1,168 (91.4%) VP—*VP 779/928 (83.9%) 777/928 (83.7%) 783/928 (84.4%) VP—*NP 579/623 (92.9%) 579/623 (92.9%) 582/623 (93.4%) input was automatically tagged using the JUMAN morphological analyzer 3. We used two baseline systems for the purposes of comparison: a rule-based dependency parser (Kurohashi and Nagao, 1994) and the probabilistic generative model of dependency and case structure analysis (Kawahara and Kurohashi, 2006b)4. We use the above-mentioned case frames also in the latter baseline parser, which also requires automatically constructed case frames. 5.1 Evaluation of Dependency Structures We evaluated the obtained dependency structures in terms of phrase-based dependency accuracy — the proportion of correct dependencies out of all dependencies5. Table 3 lists the dependency accuracies. In this table, “syn” represents the rule-based dependency parser, “syn+case” represents the probabilistic parser of syntactic and case structure (Kawahara and Kurohashi, 2006b)6, and “syn+case+cons” represents our propos</context>
<context position="23128" citStr="Kawahara and Kurohashi (2006" startWordPosition="3542" endWordPosition="3545">d. The parser “syn+case+cons” significantly outperformed the two baselines for “all” (McNemar’s test; p &lt; 0.05). In particular, the accuracy of the intra-clause (predicate-argument) relations (“NP—*VP”) was improved by 1.5% from “syn” and by 0.4% from “syn+case.” These im3http://nlp.kuee.kyoto-u.ac.jp/ nl-resource/juman-e.html 4http://nlp.kuee.kyoto-u.ac.jp/ nl-resource/knp-e.html 5Since Japanese is head-final, the second to last phrase unambiguously depends on the last phrase. However, we include such dependencies into our evaluation as in most of previous studies. 6The accuracy described in Kawahara and Kurohashi (2006b) is different from that of this paper due to the different evaluation measure excluding the unambiguous dependencies of the second last phrases. 7VP includes not only verbs but also adjectives and nouns with copula. provements are due to the incorporation of the transition knowledge into syntactic/case structure analysis. In order to compare our results with a state-ofthe-art discriminative dependency parser, we input the test corpus into an SVM-based Japanese dependency parser, CaboCha8(Kudo and Matsumoto, 2002), which was trained using the Kyoto University Text Corpus. Its dependency accur</context>
</contexts>
<marker>Kawahara, Kurohashi, 2006</marker>
<rawString>Daisuke Kawahara and Sadao Kurohashi. 2006b. A fully-lexicalized probabilistic model for Japanese syntactic and case structure analysis. In Proceedings of HLT-NAACL2006, pages 176–183.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08:HLT,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="2671" citStr="Koo et al. (2008)" startWordPosition="382" endWordPosition="385"> and Pereira, 2006; Carreras, 2007; Nakagawa, 2007). It is desirable to consider a wider-range phrase, clause, or a whole sentence, but it is difficult to judge whether the structure of such a wide-range expression is linguistically correct. One of the reasons for this is the scarcity of the knowledge required to make such a judgment. When we use the Penn Treebank (Marcus et al., 1993), which is one of the largest corpora among the available analyzed corpora, as training data, even bi-lexical dependencies cannot be learned sufficiently (Bikel, 2004). To circumvent such scarcity, for instance, Koo et al. (2008) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbitrary nodes such as sibling nodes. To solve the problem of knowledge scarcity, they learned knowledge of such predicate-argument structures from a very large number of automatically analyzed corpora (Abekawa and Okumura, 2006; Ka</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of ACL-08:HLT, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Japanese dependency analysis using cascaded chunking.</title>
<date>2002</date>
<booktitle>In Proceedings of CoNLL2002,</booktitle>
<pages>29--35</pages>
<contexts>
<context position="1749" citStr="Kudo and Matsumoto, 2002" startWordPosition="245" endWordPosition="248">words or phrases and subsequently collect all the assessments for these pairs as the dependency parse of the sentence. To improve dependency parsing, it is important to consider as broad a context as possible, rather than a word/phrase pair. In the recent evaluation workshops (shared tasks) of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, Sadao Kurohashi Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan kuro@i.kyoto-u.ac.jp i.e., second-order and higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Nakagawa, 2007). It is desirable to consider a wider-range phrase, clause, or a whole sentence, but it is difficult to judge whether the structure of such a wide-range expression is linguistically correct. One of the reasons for this is the scarcity of the kn</context>
<context position="23648" citStr="Kudo and Matsumoto, 2002" startWordPosition="3619" endWordPosition="3623">to our evaluation as in most of previous studies. 6The accuracy described in Kawahara and Kurohashi (2006b) is different from that of this paper due to the different evaluation measure excluding the unambiguous dependencies of the second last phrases. 7VP includes not only verbs but also adjectives and nouns with copula. provements are due to the incorporation of the transition knowledge into syntactic/case structure analysis. In order to compare our results with a state-ofthe-art discriminative dependency parser, we input the test corpus into an SVM-based Japanese dependency parser, CaboCha8(Kudo and Matsumoto, 2002), which was trained using the Kyoto University Text Corpus. Its dependency accuracy was 88.6% (4,540/5,122), which is close to that of “syn.” This low accuracy is attributed to the lack of knowledge of both intra-clause and inter-clause relations. Another cause of the low accuracy is the out-of-domain training corpus. In other words, the parser was trained on a newspaper corpus, while the test corpus was obtained from the Web because a tagged Web corpus that is large enough to train a supervised parser is not available. 5.2 Discussions Figure 2 shows some improved analyses; here, the dotted li</context>
</contexts>
<marker>Kudo, Matsumoto, 2002</marker>
<rawString>Taku Kudo and Yuji Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. In Proceedings of CoNLL2002, pages 29–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="21603" citStr="Kurohashi and Nagao, 1994" startWordPosition="3329" endWordPosition="3332"> estimate probabilities. = argmax (T L) 113 Table 3: The dependency accuracies in our experiments. syn syn+case syn+case+cons all 4,555/5,122 (88.9%) 4,581/5,122 (89.4%) 4,599/5,122 (89.8%) NP—*VP 2,115/2,383 (88.8%) 2,142/2,383 (89.9%) 2,151/2,383 (90.3%) NP—*NP 1,068/1,168 (91.4%) 1,068/1,168 (91.4%) 1,068/1,168 (91.4%) VP—*VP 779/928 (83.9%) 777/928 (83.7%) 783/928 (84.4%) VP—*NP 579/623 (92.9%) 579/623 (92.9%) 582/623 (93.4%) input was automatically tagged using the JUMAN morphological analyzer 3. We used two baseline systems for the purposes of comparison: a rule-based dependency parser (Kurohashi and Nagao, 1994) and the probabilistic generative model of dependency and case structure analysis (Kawahara and Kurohashi, 2006b)4. We use the above-mentioned case frames also in the latter baseline parser, which also requires automatically constructed case frames. 5.1 Evaluation of Dependency Structures We evaluated the obtained dependency structures in terms of phrase-based dependency accuracy — the proportion of correct dependencies out of all dependencies5. Table 3 lists the dependency accuracies. In this table, “syn” represents the rule-based dependency parser, “syn+case” represents the probabilistic par</context>
</contexts>
<marker>Kurohashi, Nagao, 1994</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1994. A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures. Computational Linguistics, 20(4):507–534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas B Lenat</author>
</authors>
<title>CYC: A large-scale investment in knowledge infrastructure.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="15390" citStr="Lenat, 1995" startWordPosition="2356" endWordPosition="2357">xample, Chklovski and Pantel (2004) obtained 29,165 verb pairs for several semantic relations in VerbOcean. The transition knowledge acquired in this study is several thousand times larger than that in VerbOcean. It is very difficult to make a meaningful comparison, but it can be seen that we have succeeded in acquiring generic transition knowledge on a large scale. 3.4 Related Work In order to realize practical natural language processing (NLP) systems such as intelligent dialog systems, a lot of effort has been made to develop world knowledge or inference knowledge. For example, in the CYC (Lenat, 1995) and Open Mind (Stork, 1999) projects, such knowledge has been obtained manually, but it is difficult to manually develop broad-coverage knowledge that is sufficient for practical use in NLP applications. On the other hand, the automatic acquisition of such inference knowledge from corpora has attracted much attention in recent years. First, semantic knowledge between entities has been automatically obtained (Girju and Moldovan, 2002; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). For example, Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which iteratively acq</context>
</contexts>
<marker>Lenat, 1995</marker>
<rawString>Douglas B. Lenat. 1995. CYC: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11):32–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT - discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining,</booktitle>
<pages>323--328</pages>
<contexts>
<context position="8581" citStr="Lin and Pantel, 2001" startWordPosition="1272" endWordPosition="1275">ail, message, information, ... ni friend, address, direction, ... de mail, post, postage,... . ... . ... . okuru:6 ga woman,... (send) wo baggage, supply, goods,... ni person, Japan, parental house, ... de mail, post, courier,... . ... . ... . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 110</context>
<context position="16287" citStr="Lin and Pantel (2001)" startWordPosition="2489" endWordPosition="2493">e from corpora has attracted much attention in recent years. First, semantic knowledge between entities has been automatically obtained (Girju and Moldovan, 2002; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). For example, Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which iteratively acquires entity pairs and extraction patterns using reciprocal relationship between entities and patterns. As for the acquisition of the knowledge between events or clauses, which is most relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted entailment knowledge using coordinated verb pairs and noun-verb co-occurrences. Pekar (2006) also collected entailment knowledge with discourse structure constraints. Zanzotto et al. (2006) obtained entailment knowledge using nominalized verbs. There have been some studies on relations other than entailment relations. Chklovski and Pantel (2004) obtained verb pairs that have one of five semantic relations by using a search engine</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT - discovery of inference rules from text. In Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL98,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="11592" citStr="Lin (1998)" startWordPosition="1749" endWordPosition="1750"> “tsumu” (load/accumulate)), but by pairs (e.g., “nimotsuwo tsumu” (load baggage) and “keiken-wo tsumu” (accumulate experience)). argument-head examples are aggregated in this manner, and they yield basic case frames. Thereafter, the basic case frames are clustered in order to merge similar case frames, including similar case frames that are made from scrambled sentences. For example, since “nimotsuwo tsumu” (load baggage) and “busshi-wo tsumu” (load supply) are similar, they are clustered together. The similarity is measured by using a distributional thesaurus based on the study described in Lin (1998). 3.2 Acquisition of Transition Knowledge from Large Corpus To acquire the transition knowledge, we collect the clause pairs in a large raw corpus that have a dependency relation and represent them as pairs of case frames. For example, from the following sentence, a case frame pair, (matomeru:5, okuru:6), is extracted. (2) nimotsu-wo matomete, takuhaibin-de baggage-ACC pack courier-CMI okutta sent (packed one’s baggage and sent (it) using courier service) These case frames are determined by applying a conventional case structure analyzer (Kawahara and Kurohashi, 2006b), which selects the case </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of COLINGACL98, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2442" citStr="Marcus et al., 1993" startWordPosition="346" endWordPosition="349">consider sibling and grandparent nodes, Sadao Kurohashi Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan kuro@i.kyoto-u.ac.jp i.e., second-order and higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Nakagawa, 2007). It is desirable to consider a wider-range phrase, clause, or a whole sentence, but it is difficult to judge whether the structure of such a wide-range expression is linguistically correct. One of the reasons for this is the scarcity of the knowledge required to make such a judgment. When we use the Penn Treebank (Marcus et al., 1993), which is one of the largest corpora among the available analyzed corpora, as training data, even bi-lexical dependencies cannot be learned sufficiently (Bikel, 2004). To circumvent such scarcity, for instance, Koo et al. (2008) proposed the use of word classes induced by clustering words in a large raw corpus. They succeeded in improving the accuracy of a higher-order dependency parser. On the other hand, some researchers have proposed other approaches where linguistic units such as predicate-argument structures (also known as case structures and logical forms) are considered instead of arbi</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell Marcus, Beatrice Santorini, and Mary Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL2006,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="2072" citStr="McDonald and Pereira, 2006" startWordPosition="282" endWordPosition="285">rsing (Buchholz and Marsi, 2006; Nivre et al., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, Sadao Kurohashi Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan kuro@i.kyoto-u.ac.jp i.e., second-order and higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Nakagawa, 2007). It is desirable to consider a wider-range phrase, clause, or a whole sentence, but it is difficult to judge whether the structure of such a wide-range expression is linguistically correct. One of the reasons for this is the scarcity of the knowledge required to make such a judgment. When we use the Penn Treebank (Marcus et al., 1993), which is one of the largest corpora among the available analyzed corpora, as training data, even bi-lexical dependencies cannot be learned sufficiently (Bikel, 2004). To circumvent such scarcity, for instance, Koo et al. (2008) </context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL2006, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Nakagawa</author>
</authors>
<title>Multilingual dependency parsing using global features.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL2007 Shared Task,</booktitle>
<pages>952--956</pages>
<contexts>
<context position="2105" citStr="Nakagawa, 2007" startWordPosition="288" endWordPosition="289">., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, Sadao Kurohashi Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan kuro@i.kyoto-u.ac.jp i.e., second-order and higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Nakagawa, 2007). It is desirable to consider a wider-range phrase, clause, or a whole sentence, but it is difficult to judge whether the structure of such a wide-range expression is linguistically correct. One of the reasons for this is the scarcity of the knowledge required to make such a judgment. When we use the Penn Treebank (Marcus et al., 1993), which is one of the largest corpora among the available analyzed corpora, as training data, even bi-lexical dependencies cannot be learned sufficiently (Bikel, 2004). To circumvent such scarcity, for instance, Koo et al. (2008) proposed the use of word classes </context>
</contexts>
<marker>Nakagawa, 2007</marker>
<rawString>Tetsuji Nakagawa. 2007. Multilingual dependency parsing using global features. In Proceedings of EMNLP-CoNLL2007 Shared Task, pages 952–956.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Mario Scholz</author>
</authors>
<title>Deterministic dependency parsing of English text.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING2004,</booktitle>
<pages>64--70</pages>
<contexts>
<context position="1773" citStr="Nivre and Scholz, 2004" startWordPosition="249" endWordPosition="252">quently collect all the assessments for these pairs as the dependency parse of the sentence. To improve dependency parsing, it is important to consider as broad a context as possible, rather than a word/phrase pair. In the recent evaluation workshops (shared tasks) of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, Sadao Kurohashi Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan kuro@i.kyoto-u.ac.jp i.e., second-order and higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Nakagawa, 2007). It is desirable to consider a wider-range phrase, clause, or a whole sentence, but it is difficult to judge whether the structure of such a wide-range expression is linguistically correct. One of the reasons for this is the scarcity of the knowledge required to make</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Joakim Nivre and Mario Scholz. 2004. Deterministic dependency parsing of English text. In Proceedings of COLING2004, pages 64–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>shared task on dependency parsing.</title>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>In Proceedings of EMNLPCoNLL2007,</booktitle>
<pages>915--932</pages>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of EMNLPCoNLL2007, pages 915–932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging generic patterns for automatically harvesting semantic relations.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL2006,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="15889" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="2429" endWordPosition="2433">alog systems, a lot of effort has been made to develop world knowledge or inference knowledge. For example, in the CYC (Lenat, 1995) and Open Mind (Stork, 1999) projects, such knowledge has been obtained manually, but it is difficult to manually develop broad-coverage knowledge that is sufficient for practical use in NLP applications. On the other hand, the automatic acquisition of such inference knowledge from corpora has attracted much attention in recent years. First, semantic knowledge between entities has been automatically obtained (Girju and Moldovan, 2002; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). For example, Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which iteratively acquires entity pairs and extraction patterns using reciprocal relationship between entities and patterns. As for the acquisition of the knowledge between events or clauses, which is most relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted entailment knowledge using coordina</context>
<context position="17246" citStr="Pantel and Pennacchiotti, 2006" startWordPosition="2633" endWordPosition="2637">ints. Zanzotto et al. (2006) obtained entailment knowledge using nominalized verbs. There have been some studies on relations other than entailment relations. Chklovski and Pantel (2004) obtained verb pairs that have one of five semantic relations by using a search engine. Inui et al. (2005) classified the occurrences of the Japanese connective marker tame. Abe et al. 112 (2008) learned event relation knowledge for two semantic relations. They first gave seed pairs of verbs or verb phrases and extracted the patterns that matched these seed pairs. Subsequently, by using the Espresso algorithm (Pantel and Pennacchiotti, 2006), this process was iterated to augment both instances and patterns. The acquisition unit in these studies was a verb or a verb phrase. In contrast to these studies, we obtained generic transition knowledge between case frames without limiting target semantic relations. 4 Incorporating Transition Knowledge into Dependency and Case Structure Analysis We employ the probabilistic generative model of dependency and case structure analysis (Kawahara and Kurohashi, 2006b) as a base model. We incorporate the obtained transition knowledge into this base parser. Our model assigns a probability to each p</context>
</contexts>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>Patrick Pantel and Marco Pennacchiotti. 2006. Espresso: Leveraging generic patterns for automatically harvesting semantic relations. In Proceedings of COLING-ACL2006, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viktor Pekar</author>
</authors>
<title>Acquisition of verb entailment from text.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL2006,</booktitle>
<pages>49--56</pages>
<contexts>
<context position="8610" citStr="Pekar, 2006" startWordPosition="1278" endWordPosition="1279">nd, address, direction, ... de mail, post, postage,... . ... . ... . okuru:6 ga woman,... (send) wo baggage, supply, goods,... ni person, Japan, parental house, ... de mail, post, courier,... . ... . ... . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 110 In this section, we first de</context>
<context position="16546" citStr="Pekar (2006)" startWordPosition="2529" endWordPosition="2530">) proposed the Espresso algorithm, which iteratively acquires entity pairs and extraction patterns using reciprocal relationship between entities and patterns. As for the acquisition of the knowledge between events or clauses, which is most relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted entailment knowledge using coordinated verb pairs and noun-verb co-occurrences. Pekar (2006) also collected entailment knowledge with discourse structure constraints. Zanzotto et al. (2006) obtained entailment knowledge using nominalized verbs. There have been some studies on relations other than entailment relations. Chklovski and Pantel (2004) obtained verb pairs that have one of five semantic relations by using a search engine. Inui et al. (2005) classified the occurrences of the Japanese connective marker tame. Abe et al. 112 (2008) learned event relation knowledge for two semantic relations. They first gave seed pairs of verbs or verb phrases and extracted the patterns that matc</context>
</contexts>
<marker>Pekar, 2006</marker>
<rawString>Viktor Pekar. 2006. Acquisition of verb entailment from text. In Proceedings of HLT-NAACL2006, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL2002,</booktitle>
<pages>41--47</pages>
<contexts>
<context position="15856" citStr="Ravichandran and Hovy, 2002" startWordPosition="2425" endWordPosition="2428">ystems such as intelligent dialog systems, a lot of effort has been made to develop world knowledge or inference knowledge. For example, in the CYC (Lenat, 1995) and Open Mind (Stork, 1999) projects, such knowledge has been obtained manually, but it is difficult to manually develop broad-coverage knowledge that is sufficient for practical use in NLP applications. On the other hand, the automatic acquisition of such inference knowledge from corpora has attracted much attention in recent years. First, semantic knowledge between entities has been automatically obtained (Girju and Moldovan, 2002; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). For example, Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which iteratively acquires entity pairs and extraction patterns using reciprocal relationship between entities and patterns. As for the acquisition of the knowledge between events or clauses, which is most relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted en</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In Proceedings of ACL2002, pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Sassano</author>
</authors>
<title>Linear-time dependency analysis for Japanese.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING2004,</booktitle>
<pages>8--14</pages>
<contexts>
<context position="1789" citStr="Sassano, 2004" startWordPosition="253" endWordPosition="254">assessments for these pairs as the dependency parse of the sentence. To improve dependency parsing, it is important to consider as broad a context as possible, rather than a word/phrase pair. In the recent evaluation workshops (shared tasks) of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007), transition-based and graph-based methods achieved good performance by incorporating rich context. Transition-based dependency parsers consider the words following the word under consideration as features of machine learning (Kudo and Matsumoto, 2002; Nivre and Scholz, 2004; Sassano, 2004). Graph-based dependency parsers consider sibling and grandparent nodes, Sadao Kurohashi Graduate School of Informatics, Kyoto University, Yoshida-Honmachi, Sakyo-ku, Kyoto, 606-8501, Japan kuro@i.kyoto-u.ac.jp i.e., second-order and higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Nakagawa, 2007). It is desirable to consider a wider-range phrase, clause, or a whole sentence, but it is difficult to judge whether the structure of such a wide-range expression is linguistically correct. One of the reasons for this is the scarcity of the knowledge required to make such a judgment</context>
</contexts>
<marker>Sassano, 2004</marker>
<rawString>Manabu Sassano. 2004. Linear-time dependency analysis for Japanese. In Proceedings of COLING2004, pages 8–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David G Stork</author>
</authors>
<title>Character and document research in the open mind initiative.</title>
<date>1999</date>
<booktitle>In Proceedings of International Conference on Document Analysis and Recognition,</booktitle>
<pages>1--12</pages>
<contexts>
<context position="15418" citStr="Stork, 1999" startWordPosition="2361" endWordPosition="2362"> (2004) obtained 29,165 verb pairs for several semantic relations in VerbOcean. The transition knowledge acquired in this study is several thousand times larger than that in VerbOcean. It is very difficult to make a meaningful comparison, but it can be seen that we have succeeded in acquiring generic transition knowledge on a large scale. 3.4 Related Work In order to realize practical natural language processing (NLP) systems such as intelligent dialog systems, a lot of effort has been made to develop world knowledge or inference knowledge. For example, in the CYC (Lenat, 1995) and Open Mind (Stork, 1999) projects, such knowledge has been obtained manually, but it is difficult to manually develop broad-coverage knowledge that is sufficient for practical use in NLP applications. On the other hand, the automatic acquisition of such inference knowledge from corpora has attracted much attention in recent years. First, semantic knowledge between entities has been automatically obtained (Girju and Moldovan, 2002; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). For example, Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which iteratively acquires entity pairs and extra</context>
</contexts>
<marker>Stork, 1999</marker>
<rawString>David G. Stork. 1999. Character and document research in the open mind initiative. In Proceedings of International Conference on Document Analysis and Recognition, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning entailment rules for unary templates.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING2008,</booktitle>
<pages>849--856</pages>
<contexts>
<context position="16317" citStr="Szpektor and Dagan (2008)" startWordPosition="2495" endWordPosition="2498">ed much attention in recent years. First, semantic knowledge between entities has been automatically obtained (Girju and Moldovan, 2002; Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). For example, Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which iteratively acquires entity pairs and extraction patterns using reciprocal relationship between entities and patterns. As for the acquisition of the knowledge between events or clauses, which is most relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted entailment knowledge using coordinated verb pairs and noun-verb co-occurrences. Pekar (2006) also collected entailment knowledge with discourse structure constraints. Zanzotto et al. (2006) obtained entailment knowledge using nominalized verbs. There have been some studies on relations other than entailment relations. Chklovski and Pantel (2004) obtained verb pairs that have one of five semantic relations by using a search engine. Inui et al. (2005) classifie</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In Proceedings of COLING2008, pages 849–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Torisawa</author>
</authors>
<title>Acquiring inference rules with temporal constraints by using Japanese coordinated sentences and noun-verb co-occurrences.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL2006,</booktitle>
<pages>57--64</pages>
<contexts>
<context position="8597" citStr="Torisawa, 2006" startWordPosition="1276" endWordPosition="1277">ion, ... ni friend, address, direction, ... de mail, post, postage,... . ... . ... . okuru:6 ga woman,... (send) wo baggage, supply, goods,... ni person, Japan, parental house, ... de mail, post, courier,... . ... . ... . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 110 In this section</context>
<context position="16443" citStr="Torisawa (2006)" startWordPosition="2516" endWordPosition="2517"> Ravichandran and Hovy, 2002; Pantel and Pennacchiotti, 2006). For example, Pantel and Pennacchiotti (2006) proposed the Espresso algorithm, which iteratively acquires entity pairs and extraction patterns using reciprocal relationship between entities and patterns. As for the acquisition of the knowledge between events or clauses, which is most relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted entailment knowledge using coordinated verb pairs and noun-verb co-occurrences. Pekar (2006) also collected entailment knowledge with discourse structure constraints. Zanzotto et al. (2006) obtained entailment knowledge using nominalized verbs. There have been some studies on relations other than entailment relations. Chklovski and Pantel (2004) obtained verb pairs that have one of five semantic relations by using a search engine. Inui et al. (2005) classified the occurrences of the Japanese connective marker tame. Abe et al. 112 (2008) learned event relation knowledge for two seman</context>
</contexts>
<marker>Torisawa, 2006</marker>
<rawString>Kentaro Torisawa. 2006. Acquiring inference rules with temporal constraints by using Japanese coordinated sentences and noun-verb co-occurrences. In Proceedings of HLT-NAACL2006, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Maria Teresa Pazienza</author>
</authors>
<title>Discovering asymmetric entailment relations between verbs using selectional preferences.</title>
<date>2006</date>
<booktitle>In Proceedings of COLINGACL2006,</booktitle>
<pages>849--856</pages>
<contexts>
<context position="8634" citStr="Zanzotto et al., 2006" startWordPosition="1280" endWordPosition="1283">direction, ... de mail, post, postage,... . ... . ... . okuru:6 ga woman,... (send) wo baggage, supply, goods,... ni person, Japan, parental house, ... de mail, post, courier,... . ... . ... . frames and the transition knowledge between case frames. 3 Acquiring Transition Knowledge between Case Frames We automatically acquire large-scale transition knowledge of inter-clause relations from a raw corpus. The following two points are different from previous studies on the acquisition of interclause knowledge such as entailment/synonym knowledge (Lin and Pantel, 2001; Torisawa, 2006; Pekar, 2006; Zanzotto et al., 2006), verb relation knowledge (Chklovski and Pantel, 2004), causal knowledge (Inui et al., 2005) and event relation knowledge (Abe et al., 2008): • the unit of knowledge is disambiguated and generalized The unit in previous studies was a verb or a verb phrase, in which verb sense ambiguities still remain. Our unit is case frames that are semantically disambiguated. • the variation of relations is not limited Although previous studies focused on limited kinds of semantic relations, we comprehensively collect generic relations between clauses. 110 In this section, we first describe our unit of trans</context>
<context position="16643" citStr="Zanzotto et al. (2006)" startWordPosition="2541" endWordPosition="2544">on patterns using reciprocal relationship between entities and patterns. As for the acquisition of the knowledge between events or clauses, which is most relevant to this study, many approaches have been adopted to acquire entailment knowledge. Lin and Pantel (2001) and Szpektor and Dagan (2008) learned entailment rules based on distributional similarity between instances that have a relation to a rule. Torisawa (2006) extracted entailment knowledge using coordinated verb pairs and noun-verb co-occurrences. Pekar (2006) also collected entailment knowledge with discourse structure constraints. Zanzotto et al. (2006) obtained entailment knowledge using nominalized verbs. There have been some studies on relations other than entailment relations. Chklovski and Pantel (2004) obtained verb pairs that have one of five semantic relations by using a search engine. Inui et al. (2005) classified the occurrences of the Japanese connective marker tame. Abe et al. 112 (2008) learned event relation knowledge for two semantic relations. They first gave seed pairs of verbs or verb phrases and extracted the patterns that matched these seed pairs. Subsequently, by using the Espresso algorithm (Pantel and Pennacchiotti, 20</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Pazienza, 2006</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Maria Teresa Pazienza. 2006. Discovering asymmetric entailment relations between verbs using selectional preferences. In Proceedings of COLINGACL2006, pages 849–856.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>