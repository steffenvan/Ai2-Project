<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9918925">
Polynomial Time and Space Shift-Reduce Parsing
of Arbitrary Context-free Grammars.*
</title>
<author confidence="0.989826">
Yves Schabes
</author>
<affiliation confidence="0.894184333333333">
Dept. of Computer &amp; Information Science
University of Pennsylvania
Philadelphia, PA 19104-6389, USA
</affiliation>
<email confidence="0.992576">
e-mail: schabes©linc.cis.upenn.edu
</email>
<sectionHeader confidence="0.998561" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999872347826087">
We introduce an algorithm for designing a predictive
left to right shift-reduce non-deterministic push-down
machine corresponding to an arbitrary unrestricted
context-free grammar and an algorithm for efficiently
driving this machine in pseudo-parallel. The perfor-
mance of the resulting parser is formally proven to be
superior to Earley&apos;s parser (1970).
The technique employed consists in constructing
before run-time a parsing table that encodes a non-
deterministic machine in the which the predictive be-
havior has been compiled out. At run time, the ma-
chine is driven in pseudo-parallel with the help of a
chart.
The recognizer behaves in the worst case in
0(1G12n3)-time and O(lGIn2)-space. However in
practice it is always superior to Earley&apos;s parser since
the prediction steps have been compiled before run-
time.
Finally, we explain how other more efficient vari-
ants of the basic parser can be obtained by deter-
minizing portions of the basic non-deterministic push-
down machine while still using the same pseudo-
parallel driver.
</bodyText>
<sectionHeader confidence="0.999426" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.969138666666667">
Predictive bottom-up parsers (Earley, 1968; Earley,
1970; Graham et al., 1980) are often used for natural
language processing because of their superior average
performance compared to purely bottom-up parsers
*We are extremely indebted to Fernando Pereira and Stuart
Shieber for providing valuable technical comments during dis-
cussions about earlier versions of this algorithm. We are also
grateful to Aravind Joshi for his support of this research. We
also thank Robert Frank. All remaining errors are the author&apos;s
responsibility alone. This research was partially funded by
ARO grant DAAL03-89-00031PRI and DARPA grant N00014-
90-J-1863.
such as CKY-style parsers (Kasami, 1965; Younger,
1967). Their practical superiority is mainly obtained
because of the top-down filtering accomplished by the
predictive component of the parser. Compiling out
as much as possible this predictive component before
run-time will result in a more efficient parser so long
as the worst case behavior is not deteriorated.
Approaches in this direction have been investigated
(Earley, 1968; Lang, 1974; Tomita, 1985; Tomita,
1987), however none of them is satisfying, either be-
cause the worst case complexity is deteriorated (worse
than Earley&apos;s parser) or because the technique is not
general. Furthermore, none of these approaches have
been formally proven to have a behavior superior to
well known parsers such as Earley&apos;s parser.
Earley himself ([1968] pages 69-89) proposed to pre-
compile the state sets generated by his algorithm to
make it as efficient as LR(k) parsers (Knuth, 1965)
when used on LR(k) grammars by precomputing all
possible states sets that the parser could create. How-
ever, some context-free grammars, including most
likely most natural language grammars, cannot be
compiled using his technique and the problem of
knowing if a grammar can be compiled with this tech-
nique is undecidable (Earley [1968], page 99).
Lang (1974) proposed a technique for evaluating
in pseudo-parallel non-deterministic push down au-
tomata. Although this technique achieves a worst
case complexity of 0(713)-time with respect to the
length of input, it requires that at most two symbols
are popped from the stack in a single move. When the
technique is used for shift-reduce parsing, this con-
straint requires that the context-free grammar is in
Chomsky normal form (CNF). As far as the grammar
size is concerned, an exponential worst case behavior
is reached when used with the characteristic LR(0)
</bodyText>
<page confidence="0.995734">
106
</page>
<bodyText confidence="0.995754833333333">
machine.1
Tomita (1985; 1987) proposed to extend LR(0)
parsers to non-deterministic context-free grammars
by explicitly using a graph structured stack which
represents the pseudo-parallel evaluation of the moves
of a non-deterministic LR(0) push-down automaton.
Tomita&apos;s encoding of the non-deterministic push-
down automaton suffers from an exponential time
and space worst case complexity with respect to the
input length and also with respect to the grammar
size (Johnson [1989] and also page 72 in Tomita
[1985]). Although Tomita reports experimental data
that seem to show that the parser behaves in practice
better than Earley&apos;s parser (which is proven to take
in the worst case 0(1G120)-time), the duplication of
the same experiments shows no conclusive outcome.
Modifications to Tomita&apos;s algorithm have been pro-
posed in order to alleviate the exponential complex-
ity with respect to the input length (Kipps, 1989) but,
according to Kipps, the modified algorithm does not
lead to a practical parser. Furthermore, the algorithm
is doomed to behave in the worst case in exponential
time with respect to the grammar size for some am-
biguous grammars and inputs (Johnson, 1989).2 So
far, there is no formal proof showing that the Tomita&apos;s
parser can be superior for some grammars and in-
puts to Earley&apos;s parser, and its worst case complexity
seems to contradict the experimental data.
As explained, the previous attempts to compile
the predictive component are not general and achieve
a worst case complexity (with respect to the gram-
mar size and the input length) worse than standard
parsers.
The methodology we follow in order to compile the
predictive component of Earley&apos;s parser is to define
a predictive bottom-up pushdown machine equiva-
lent to the given grammar which we drive in pseudo-
parallel. Following Johnson&apos;s (1989) argument, any
parsing algorithm based on the LR(0) characteris-
tic machine is doomed to behave in exponential time
with respect to the grammar size for some ambigu-
ous grammars and inputs. This is a result of the fact
that the number of states of an LR(0) characteristic
machine can be exponential and that there are some
grammars and inputs for which an exponential num-
ber of states must be reached (See Johnson [1989] for
examples of such grammars and inputs). One must
therefore design a different pushdown machine which
</bodyText>
<footnote confidence="0.979097428571429">
1The same argument for the exponential grammar size com-
plexity of Tomita&apos;s parser (Johnson, 1989) holds for Lang&apos;s
technique.
2This problem is particularly acute for natural language pro-
cessing since in this context the input length is typically small
(10-20 words) and the grammar size very large (hundreds or
thousands of rules and symbols).
</footnote>
<bodyText confidence="0.99836009375">
can be driven efficiently in pseudo-parallel.
We construct a non-deterministic predictive push-
down machine given an arbitrary context-free gram-
mar whose number of states is proportional to the size
of the grammar. Then at run time, we efficiently drive
this machine in pseudo-parallel. Even if all the states
of the machine are reached for some grammars and
inputs, a polynomial complexity will still be obtained
since the number of states is bounded by the gram-
mar size. We therefore introduce a shift-reduce driver
for this machine in which all of the predictive compo-
nent has been compiled in the finite state control of
the machine. The technique makes no requirement on
the form of the context-free grammar and it behaves
in the worst case as well as Earley&apos;s parser (Earley,
1970). The push-down machine is built before run-
time and it is encoded as parsing tables in the which
the predictive behavior has been compiled out.
In the worst case, the recognizer behaves in the
same 0(IGI2n3)-time and 0(IGIn2)-space as Earley&apos;s
parser. However in practice it is always superior
to Earley&apos;s parser since the prediction steps have
been eliminated before run-time. We show that the
items produced in the chart correspond to equiva-
lence classes on the items produced for the same input
by Earley&apos;s parser. This mapping formally shows its
practical superior behavior.3
Finally, we explain how other more efficient vari-
ants of the basic parser can be obtained by deter-
minizing portions of the basic non-deterministic push-
down machine while still using the same pseudo-
parallel driver.
</bodyText>
<sectionHeader confidence="0.955604" genericHeader="introduction">
2 The Parser
</sectionHeader>
<bodyText confidence="0.991353066666667">
The parser we propose handles any context-free gram-
mar; the grammar can be ambiguous and need not be
in any normal form. The parser is a predictive shift-
reduce bottom-up parser that uses compiled top down
prediction information in the form of tables. Before
run-time, a non-deterministic push down automa-
ton (NPDA) is constructed from a given context-free
grammar. The parsing tables encode the finite state
control and the moves of the NPDA. At run-time,
the NPDA is then driven in pseudo-parallel with the
help of a chart. We show the construction of a basic
machine which will be driven non-deterministically.
In the following, the input string is w = al • an
and the context-free grammar being considered is
G = (E, NT, /3, S), where E is the set of terminal
</bodyText>
<footnote confidence="0.99658025">
3The characteristic LR(0) machine is the result of deter-
minizing the machine we introduce. Since this procedure in-
troduce exponentially more states, the LR(0) machine can be
exponentially large.
</footnote>
<page confidence="0.998592">
107
</page>
<bodyText confidence="0.998742619047619">
symbols, NT the set of non-terminal symbols, P a
set of production rules, S the start symbol. We will
need to refer to the subsequence of the input string
w = al • • • aN from position i to j, wm, which we
define as follows:
,ifi&lt;j
,ifi&gt;j
We explain the data-structures used by the parser,
the moves of the parser, and how the parsing tables
are constructed for the basic NPDA. Then, we study
the formal characteristics of the parser.
The parser uses two moves: shift and reduce. As in
standard shift-reduce parsers, shift moves recognize
new terminal symbols and reduce moves perform the
recognition of an entire context-free rule. However in
the parser we propose, shift and reduce moves behave
differently on rules whose recognition has just started
(i.e. rules that have been predicted) than on rules
of which some portion has been recognized. This be-
havior enables the parser to efficiently perform reduce
moves when ambiguity arises.
</bodyText>
<subsectionHeader confidence="0.9722305">
2.1 Data-Structures and the Moves of
the Parser
</subsectionHeader>
<bodyText confidence="0.999677866666666">
The parser collects items into a set called the chart,
C. Each item encodes a well formed substring of the
input. The parser proceeds until no more items can
be added to the chart C.
An item is defined as a triple (s, i,j), where s is a
state in the control of the NPDA, i and j are indices
referring to positions in the input string (i, j E [0, n]).
In an item (s, i, j), j corresponds to the current
position in the input string and i is a position in the
input which will facilitate the reduce move.
A dotted rule of a context-free grammar G is defined
as a production of G associated with a dot at some
position of the right hand side: A a • /3 with
A —+ a# EP.
We distinguish two kinds of dotted rules. Kernel
dotted rules, which are of the form A a • # with a
non empty, and non-kernel dotted rules, which have
the dot at the left most position in the right hand
side (A —4 .0). As we will see, non-kernel dotted
rules correspond to the predictive component of the
parser.
We will later see each state s of the NPDA corre-
sponds to a set of dotted rules for the grammar G.
The set of all possible states in the control of the
NPDA is written S. Section 2.2 explains how the
states are constructed.
The algorithm maintains the following property
(which guarantees its soundness)4: if an item (s,i, j)
is in the chart C then for all dotted rules A a0/3 E s
the following is satisfied:
</bodyText>
<listItem confidence="0.8246575">
(i) if a E (E U NT)+, then 37 E (NT U E)* such
that ‘51.1vp:),ijA7 and a -itvici];
(ii) if a is the empty string, then 3-1 E (NT U
such that S wpmA-y.
</listItem>
<bodyText confidence="0.958113133333333">
The parser uses three tables to determine which
move(s) to perform: an action table, ACTION, and
two goto tables, the kernel goto table, GOTOk, and
the non-kernel goto table, GOTOnk•
The goto tables are accessed by a state and a non-
terminal symbol. They each contain a set of states:
GOTOk(s,X) = {r}, GOTOnk(s, X) = {r&apos;} with
r, r&apos;, S E S, X E NT. The use of these tables is ex-
plained below.
The action table is accessed by a state and a ter-
minal symbol. It contains a set of actions. Given
an item, (s,i, j), the possible actions are determined
by the content of ACTION(s,ai+i) where ai+1 is the
j input token. The possible actions contained
in ACTION(s, ai+i) are the following:
</bodyText>
<listItem confidence="0.997872818181818">
• KERNEL SHIFT s&apos;, (ksh(s&apos;) for short), for 8/ E
S. A new token is recognized in a kernel dotted
rule A —4 a • a# and a push move is performed.
The item (8&apos;, i, j + 1) is added to the chart, since
aa spans in this case wyd+1].
• NON-KERNEL SHIFT s&apos;, (nksh(e) for short),
for s&apos; E S. A new token is recognized in a non-
kernel dotted rule of the form A --+ oaf 3. The
item (s&apos;, j, j + 1) is is added to the chart, since a
spans in this case
• REDUCE X —* f3, (red(X )3) for short), for
</listItem>
<bodyText confidence="0.974770142857143">
X E P. The context-free rule X —+ # has
been totally recognized. The rule spans the sub-
string ai+i • • • ai. For all items in the chart of the
form (s&apos;,k,i), perform the following two steps:
— for all r1 E GOTOk(si, X), it adds the item
(ri, k, j) to the chart. In this case, a dotted
rule of the form A —+ a • X13 is combined
with X fie to form A aX • f3; since a
spans wikm and X spans wjJ, aX spans
- for all r2 E GOTO„k(s&apos;, X), it adds the item
(r2,i, j) to the chart. In this case, a dot-
ted rule of the form A • X/3 is combined
with X fie to form A X • )3; in this
case X spans
</bodyText>
<footnote confidence="0.5860225">
4 This property holds for all machines derived from the basic
NPDA.
</footnote>
<equation confidence="0.983927">
&apos;Mid] =
aii-i. • • • a •
</equation>
<page confidence="0.98859">
108
</page>
<table confidence="0.891878333333333">
The recognizer follows:
begin (* recognizer *)
Input:
al • • on (* input string *)
ACTION (* action table *)
GO TO,. (* kernel goto table *)
GOTOnk (* non-kernel goto table *)
start E S (* start state *)
C S (* set of final states *)
Output:acceptance or rejection of the input
string.
Initialization: C := {(start, 0,0))
</table>
<bodyText confidence="0.859866">
Perform the following three operations until no
more items can be added to the chart C:
</bodyText>
<listItem confidence="0.979589142857143">
(1) KERNEL SHIFT: if (s, j) E C and
if ksh(s&apos;) E ACTION(s, ai+i), then
(s&apos;, j 1) is added to C.
(2) NON-KERNEL SHIFT: if (s,i,j) E C
and if nksh(s&apos;) E ACTION(s, ai+i), then
+ 1) is added to C.
(3) REDUCE: if (s, i, j) E C, then for all
X --+ /3 s.t. red(X E ACTION(s, a54 i)
and for all (s&apos;, k, i) E C, perform the follow-
ing:
• for all r1 E GOTOk(s&apos;,X), (ri,k,j) is
added to C;
• for all r2 E GOTOnk(st, X), (r2, i,j) is
added to C.
</listItem>
<bodyText confidence="0.569532333333333">
If f(s, 0, n) I (s, 0,n) E C and s E 0 0
then return acceptance
otherwise return rejection.
</bodyText>
<equation confidence="0.503752">
end (* recognizer *)
</equation>
<bodyText confidence="0.997804">
In the above algorithm, non-determinism arises
from multiple entries in ACTION(s, a) and also from
the fact that GOTOk(s, X) and GOTOnk(s, X) con-
tain a set of states.
</bodyText>
<subsectionHeader confidence="0.999788">
2.2 Construction of the Parsing Tables
</subsectionHeader>
<bodyText confidence="0.987363">
We shall give an LR(0)-like method for constructing
the parsing tables corresponding to the basic NPDA.
Several other methods (such as LR(k)-like, SLR(k)-
like) can also be used for constructing the parsing
tables and are described in (Schabes, 1991).
To construct the LR(0)-like finite state control
for the basic non-deterministic push-down automaton
that the parser simulates, we define three functions,
closure, gotok and Monk •
If s is a state, then closure(s) is the state con-
structed from s by the two rules:
</bodyText>
<listItem confidence="0.811222666666667">
(i) Initially, every dotted rule in s is added to
closure(s);
(ii) If A—. a • B13 is in closure(s) and B 7 is a
</listItem>
<bodyText confidence="0.965748142857143">
production, then add the dotted rule B .7 to
closure(s) (if it is not already there). This rule
is applied until no more new dotted rules can be
added to closure(s).
If s is a state and if X is a non-terminal or terminal
symbol, gotok(s, X) and gotonk(s, X) are the set of
states defined as follows:
</bodyText>
<equation confidence="0.9698252">
gotok(s, X) =
{closure({A aX • 13})I A --+ a • X/3 E $
and a E (E U NT)+1
gotonk(s, X) =
{closure({A X • fl})1 A -4- .X13 Es)
</equation>
<bodyText confidence="0.967195461538462">
The goto functions we define differ from the one de-
fined for the LR(0) construction in two ways: first we
have distinguished transitions on symbols from ker-
nel items and non-kernel items; second, each state
in gotok(s, X) and gotonk(s, X) contains exactly one
kernel item whereas for the LR(0) construction they
may contain more than one.
We are now ready to compute the set of states $
defining the finite state control of the parser.
The SET OF STATES CONSTRUCTION is con-
structed as follows:
procedure states(G)
begin
</bodyText>
<equation confidence="0.9851602">
S := {closure({S •aIS— a E Pi)}
repeat
for each state s in S
for each X E EU NT terminal
for each r E gotok(s, X) U gotonk(s, X)
</equation>
<bodyText confidence="0.514509">
add r to S
until no more states can be added to S
</bodyText>
<note confidence="0.168559">
end
</note>
<tableCaption confidence="0.739128142857143">
PARSING TABLES. Now we construct the LR(0)
parsing tables ACTION, GOTOk and GOTOnk from
the finite state control constructed above. Given a
context-free grammar G, we construct 5, the set of
states for G with the procedure given above. We con-
struct the action table ACTION and the goto tables
using the following algorithm.
</tableCaption>
<figure confidence="0.653466">
begin (CONSTRUCTION OF THE PARSING TABLES)
Input: A context-free grammar
G = (E, NT, P, S).
</figure>
<footnote confidence="0.743236">
Output: The parsing tables ACTION, GOTOk
and GOTOnk for G, the start state start and
the set of final states F.
</footnote>
<page confidence="0.997797">
109
</page>
<bodyText confidence="0.767144">
Step 1. Construct S = {so, • • • , sm}, the set of states
for G.
Step 2. The parsing actions for state si are deter-
mined for all terminal symbols a E E as follows:
</bodyText>
<listItem confidence="0.971098714285714">
(i) for all r E gotok(si, a), add ksh(r) to
ACTION(si, a);
(ii) for all r E gotonk(si, a), add nksh(r) to to
ACTION(si, a);
(iii) if A -+ a. is in si, then add red(A -+ a)
to ACTION(si, a) for all terminal symbol a
and for the end marker S.
</listItem>
<bodyText confidence="0.657766">
Step 4. The kernel and non-kernel goto tables for
state si are determined for all non-terminal sym-
bols X as follows:
</bodyText>
<listItem confidence="0.4999635">
(i) VX E NT, GOTOk(si, X) := gotok(si, X)
(ii) VX E NT,
</listItem>
<equation confidence="0.724232333333333">
GOTOnk(si, X) :=-- gotonk (si, X)
Step 3. The start state of the parser is
start := closure({S *al S a E P))
</equation>
<bodyText confidence="0.55610625">
Step 4. The set of final states of the parser is
T:={sESI3S-*a EP s.t.S-4a. s}
end (CONSTRUCTION OF THE PARSING TABLES)
Appendix A gives an example of a parsing table.
</bodyText>
<sectionHeader confidence="0.995035" genericHeader="method">
3 Complexity
</sectionHeader>
<bodyText confidence="0.9995054">
The recognizer requires in the worst case 0(1GIn2)-
space and 0(IGI2n5)-time; n is the length of the input
string, IGI is the size of the grammar computed as
the sum of the lengths of the right hand side of each
productions:
</bodyText>
<equation confidence="0.736827">
IGI = E Ial where lal is the length of a.
A—*cr EP
</equation>
<bodyText confidence="0.999988580645161">
One of the objectives for the design of the non-
deterministic machine was to make sure that it was
not possible to reach an exponential number of states,
a property without which the machine is doomed to
have exponential complexity (Johnson, 1989). First
we observe that the number of states of the finite
state control of the non-deterministic machine that
we constructed in Section 2.2 is proportional to the
size of the grammar, IG. By construction, each state
(except for the start state) contains exactly one ker-
nel dotted rule. Therefore, the number of states is
bounded by the maximum number of kernel rules of
the form A -4 a4o)3 (with a non empty), and is 0(IGI).
We conclude that the algorithm requires in the worst
case 0(IGIn2)-space since the maximum number of
items (s, i, j) in the chart is proportional to IGIn2.
A close look at the moves of the parser reveals that
the reduce move is the most complex one since it in-
volves a pair of states (s,i, j) and (s&apos;, k , j). This move
can be instantiated at most 0(IGI2n3)-time since
j, k C [0,n] and there are in the worst case 0(1012)
pairs of states involved in this move.5 The parser
therefore behaves in the worst case in 0(1G1270)-time.
One should however note that in order to bound the
worst case complexity as stated above, arrays similar
to the one needed for Earley&apos;s parser must be used to
implement efficiently the shift and reduce moves.6
As for Earley&apos;s parser, it can also be shown that the
algorithm requires in the worst case 0(l02n2)-time
for unambiguous context-free grammars and behaves
in linear time on a large class of grammars.
</bodyText>
<sectionHeader confidence="0.959735" genericHeader="method">
4 Retrieving a Parse
</sectionHeader>
<bodyText confidence="0.999938666666667">
The algorithm that we described in Section 2 is a rec-
ognizer. However, if we include pointers from an item
to the other items (to a pair of items for the reduce
moves or to an item for the shift moves) which caused
it to be placed in the chart, the recognizer can be
modified to record all parse trees of the input string.
The representation is similar to a shared forest.
The worst case time complexity of the parser is the
same as for the recognizer (0(IG12n3)-time) but, as
for Earley&apos;s parser, the worst case space complexity
increases to 0(102n5) because of the additional book-
keeping.
</bodyText>
<sectionHeader confidence="0.9597125" genericHeader="method">
5 Correctness and Comparison
with Earley&apos;s Parser
</sectionHeader>
<bodyText confidence="0.999998428571429">
We derive the correctness of the parser by showing
how it can be mapped to Earley&apos;s parser. In the pro-
cess, we will also be able to show why this parser can
be more efficient than Earley&apos;s parser. The detailed
proofs are given in (Schabes, 1991).
We are also interested in formally characterizing
the differences in performance between the parser
we propose and Earley&apos;s parser. We show that the
parser behaves in the worst scenario as well as Ear-
ley&apos;s parser by mapping it into Earley&apos;s parser. The
parser behaves better than Earley&apos;s parser because it
has eliminated the prediction step which takes in the
worst case 0(IGIn)-time for Earley&apos;s parser. There-
fore, in the most favorable scenario, the parser we
</bodyText>
<footnote confidence="0.9973598">
6Kernel shift and non-kernel shift moves require both at
most 0( I GI n2 )- time.
6Due to the lack of space, the details of the implementation
are not given in this paper but they are given in (Schabes,
1991).
</footnote>
<page confidence="0.997396">
110
</page>
<bodyText confidence="0.998895384615385">
propose will require 0(JGIn) less time than Earley&apos;s
parser.
For a given context-free grammar G and an input
string al • • • an, let C be the set of items produced by
the parser and Cearley be the set of items produced
by Earley&apos;s parser. Earley&apos;s parser (Earley, 1970)
produces items of the form (A --+ a • ,3, i, j) where
A -+ a • /3 is a single dotted rule and not a set of
dotted rules.
The following lemma shows how one can map the
items that the parser produces to the items that Ear-
ley&apos;s parser produces for the same grammar and in-
put:
</bodyText>
<equation confidence="0.79927525">
Lemma 1 If (s, j) E C then we have:
(i) for all kernel dotted rules A a • /3 E s, we
have (A a • /3, i, j)c
_ _ earley
</equation>
<bodyText confidence="0.9800275">
(ii) and for all non-kernel dotted rules A E
s, we have (A j, j) E Ceariey
The proof of the above lemma is by induction on
the number of items added to the chart C.
This shows that an item is mapped into a set of
items produced by Earley&apos;s parser.
By construction, in a given state s E 8, non-kernel
dotted rules have been introduced before run-time by
the closure of kernel dotted rules. It follows that Ear-
ley&apos;s parser can require °(1Gin) more space since all
Earley&apos;s items of the form (A -&gt; oa, i, i) (i E [0, n])
are not stored separately from the kernel dotted rule
which introduced them.
Conversely, each kernel item in the chart created by
Earley&apos;s parser can be put into correspondence with
an item created by the parser we propose.
Lemma 2 If (A a • /3, i, j) E Cearley and if a e,
then (s, j) E C where s = closure({A a •
The proof of the above lemma is by induction on
the number of kernel items added to the chart created
by Earley&apos;s parser.
The correctness of the parser follows from Lemma 1
and its completeness from Lemma 2 since it is well
known that the items created by Earley&apos;s parser are
characterized as follows (see, for example, page 323 in
Aho and Ullman [1973] for a proof of this invariant):
Lemma 3 The item (A a • # , i, j) E Ceariey
if and only if, BT E (VNT U VT)* such that
Stvio,ipCT and X..FA wymA.
The parser we propose is therefore more efficient
than Earley&apos;s parser since it has compiled out predic-
tion before run time. How much more efficient it is,
depends on how prolific the prediction is and therefore
on the nature of the grammar and the input string.
</bodyText>
<sectionHeader confidence="0.991365" genericHeader="method">
6 Optimizations
</sectionHeader>
<bodyText confidence="0.997758333333333">
The parser can be easily extended to incorporate stan-
dard optimization techniques proposed for predictive
parsers.
The closure operation which defines how a state
is constructed already optimizes the parser on chain
derivations in a manner very similar to the tech-
niques originally proposed by Graham et al. (1980)
and later also used by Leiss (1990).
In addition, the closure operation can be designed
to optimize the processing of non-terminal symbols
that derive the empty string in manner very simi-
lar to the one proposed by Graham et al. (1980) and
Leiss (1990). The idea is to perform the reduction
of symbols that derive the empty string at compila-
tion time, i.e. include this type of reduction in the
definition of closure by adding
If s is a state, then closure(s) is now the state con-
structed from s by the three rules:
</bodyText>
<listItem confidence="0.989251166666667">
(i) Initially, every dotted rule in s is added to
closure(s);
(ii) if A -4 a • B /3 is in closure(s) and B 7 is
a production, then add the dotted rule B -+ .7
to closure(s) (if it is not already there);
(iii) if A -4 a•B # is in closure(s) and if B e, then
</listItem>
<bodyText confidence="0.982558208333333">
add the dotted rule A aB • # to closure(s)
(if it is not already there).
Rules (ii) and (iii) are applied until no more new
dotted rules can be added to closure(s).
The rest of the parser remains as before.
7 Variants on the basic ma-
chine
In the previous section we have constructed a ma-
chine whose number of states is in the worst case
proportional to the size of the grammar. This re-
quirement is essential to guarantee that the complex-
ity of the resulting parser with respect to the gram-
mar size is not exponential or worse than O(1G12)-
time as other well known parsers. However, we may
use some non-determinism in the machine to guaran-
tee this property. The non-determinism of the ma-
chine is not a problem since we have shown how the
non-deterministic machine can be efficiently driven in
pseudo-parallel (in O( IGI2n3)-time).
We can now ask the question of whether it is pos-
sible to determinize the finite state control of the ma-
chine while still being able to bound the complexity
of the parser to 0(IGI2n3)-time. Johnson (1989) ex-
hibits grammars for which the full determinization
</bodyText>
<page confidence="0.996206">
111
</page>
<bodyText confidence="0.999989954545455">
of the finite state control (the LR(0) construction)
leads to a parser with exponential complexity, because
the finite state control has an exponential number of
states and also because there are some input string
for which an exponential number of states will be
reached. However, there are also cases where the full
determinization either will not increase the number
of states or will not lead to a parser with exponential
complexity because there are no input that require to
reach an exponential number of states. We are cur-
rently studying the classes of grammars for which this
is the case.
One can also try to determinize portions of the fi-
nite state automaton from which the control is derived
while making sure that the number of states does not
become larger than 0(IG1).
All these variants of the basic parser obtained by
determinizing portions of the basic non-deterministic
push-down machine can be driven in pseudo-parallel
by the same pseudo-parallel driver that we previously
defined. These variants lead to a set of more efficient
machines since the non-determinism is decreased.
</bodyText>
<sectionHeader confidence="0.997292" genericHeader="method">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.99997517948718">
We have introduced a shift-reduce parser for unre-
stricted context-free grammars based on the construc-
tion of a non-deterministic machine and we have for-
mally proven its superior performance compared to
Earley&apos;s parser.
The technique which we employed consists of con-
structing before run-time a parsing table that encodes
a non-deterministic machine in the which the predic-
tive behavior has been compiled out. At run time, the
machine is driven in pseudo-parallel with the help a
chart.
By defining two kinds of shift moves (on kernel dot-
ted rules and on non-kernel dotted rules) and two
kinds of reduce moves (on kernel and non-kernel dot-
ted rules), we have been able to efficiently evaluate in
pseudo-parallel the non-deterministic push down ma-
chine constructed for the given context-free grammar.
The same worst case complexity as Earley&apos;s rec-
ognizer is achieved: 0(IGI2n3)-time and 0(IGIn2)-
space. However, in practice, it is superior to Earley&apos;s
parser since all the prediction steps and some of the
completion steps have been compiled before run-time.
The parser can be modified to simulate other types
of machines (such LR(k)-like or SLR-like automata).
It can also be extended to handle unification based
grammars using a similar method as that employed
by Shieber (1985) for extending Earley&apos;s algorithm.
Furthermore, the algorithm can be tuned to a par-
ticular grammar and therefore be made more effi-
cient by carefully determinizing portions of the non-
deterministic machine while making sure that the
number of states in not increased. These variants
lead to more efficient parsers than the one based on
the basic non-deterministic push-down machine. Fur-
thermore, the same pseudo-parallel driver can be used
for all these machines.
We have adapted the technique presented in this
paper to other grammatical formalism such as tree-
adjoining grammars (Schabes, 1991).
</bodyText>
<sectionHeader confidence="0.941229" genericHeader="method">
Bibliography
</sectionHeader>
<reference confidence="0.99899393939394">
A. V. Aho and J. D. Ullman. 1973. Theory of Pars-
ing, Translation and Compiling. Vol 1: Parsing.
Prentice-Hall, Englewood Cliffs, NJ.
Jay C. Earley. 1968. An Efficient Context-Free Pars-
ing Algorithm. Ph.D. thesis, Carnegie-Mellon Uni-
versity, Pittsburgh, PA.
Jay C. Earley. 1970. An efficient context-free parsing
algorithm. Commun. ACM, 13(2):94-102.
S.L. Graham, M.A. Harrison, and W.L. Ruzzo. 1980.
An improved context-free recognizer. ACM Trans-
actions on Programming Languages and Systems,
2(3):415-462, July.
Mark Johnson. 1989. The computational complex-
ity of Tomita&apos;s algorithm. In Proceedings of the
International Workshop on Parsing Technologies,
Pittsburgh, August.
T. Kasami. 1965. An efficient recognition and syn-
tax algorithm for context-free languages. Technical
Report AF-CRL-65-758, Air Force Cambridge Re-
search Laboratory, Bedford, MA.
James R. Kipps. 1989. Analysis of Tomita&apos;s al-
gorithm for general context-free parsing. In Pro-
ceedings of the International Workshop on Parsing
Technologies, Pittsburgh, August.
D. E. Knuth. 1965. On the translation of languages
from left to right. Information and Control, 8:607-
639.
Bernard Lang. 1974. Deterministic tech-
niques for efficient non-deterministic parsers. In
Jacques Loeckx, editor, Automata, Languages
and Programming, 2nd Colloquium, University of
Saarbriicken. Lecture Notes in Computer Science,
Springer Verlag.
</reference>
<page confidence="0.982197">
112
</page>
<reference confidence="0.998438761904762">
Hans Leiss. 1990. On Kilbury&apos;s modification of Ear-
ley&apos;s algorithm. ACM Transactions on Program-
ming Languages and Systems, 12(4):610-640, Oc-
tober.
Yves Schabes. 1991. Polynomial time and space
shift-reduce parsing of context-free grammars and
of tree-adjoining grammars. In preparation.
Stuart M. Shieber. 1985. Using restriction to ex-
tend parsing algorithms for complex-feature-based
formalisms. In 23&amp;quot;1 Meeting of the Association
for Computational Linguistics (ACL &apos;85), Chicago,
July.
Masaru Tomita. 1985. Efficient Parsing for Natural
Language, A Fast Algorithm for Practical Systems.
Kluwer Academic Publishers.
Masaru Tomita. 1987. An efficient augmented-
context-free parsing algorithm. Computational
Linguistics, 13:31-46.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189-208.
</reference>
<sectionHeader confidence="0.475798" genericHeader="method">
A An Example
</sectionHeader>
<bodyText confidence="0.9986555">
We give an example that illustrates how the recog-
nizer works. The grammar used for the example gen-
erates the language L = fa(ba)&apos;1In &gt; 0) and is in-
finitely ambiguous:
</bodyText>
<equation confidence="0.385633">
S —+ S
S a
</equation>
<bodyText confidence="0.974958777777778">
The set of states and the goto function are shown
in Figure 1. In Figure 1, the set of states is
{0,1,2,3,4,5). We have marked with a sharp sign (#)
transitions on a non-kernel dotted rule. If an arc from
s1 to 82 is labeled by a non-sharped symbol X, then
S2 is in gotok(si, X). If an arc from si to 82 is labeled
by a sharped symbol XL then s2 is in gotonk(si, X).
The parsing table corresponding to this grammar
is given in Figure 2.
</bodyText>
<figure confidence="0.998266666666667">
S ACTION G G
t 0 0
a T T
t 0 0
e k nk
a b [ $ S S
0 nksh(3) ksh(4) red(S -4 S) {5} {1,2}
1 red(S —). S) red(S —t. S) red(S -4 a) {1,2}
2 red(S -4 a) red(S -4 a) reci(S.--•_,S6S)
3 nksh(3) red(S -4 SbS)
4 red(S -4 SbS)
5
</figure>
<figureCaption confidence="0.6278825">
Figure 2: An LR(0) parsing table for L =
{a(ba)Th I n &gt; 0}. The start state is 0, the set of
final states is {2,3,5). $ stands for the end marker of
the input string.
</figureCaption>
<bodyText confidence="0.999939857142857">
The input string given to the recognizer is: ababaS
($ is the end marker). The chart is shown in Fig-
ure 3. In Figure 3, an arc labeled by s from position
i to position j denotes the item (8, i, j). The input is
accepted since the final states 2 and 5 span the en-
tire string ((2,0,5) E C and (5,0,5) E C). Notice that
there are multiple arcs subsuming the same substring.
</bodyText>
<figureCaption confidence="0.716912153846154">
input read items in the chart
(0,0,0)
a (3,0,1) (2,0,1) (1,0,1)
ab (4,0,2)
aba (3,2,3) (2,0,3) (2,2,3)
(1,0,3) (1,2,3) (5,0,3)
abab (4,0,4) (4,2,4)
ababa (3,4,5) (2,0,5) (2,2,5)
(2,4,5) (1,0,5) (1,2,5)
(1,4,5) (5,0,5) (5,2,5)
Figure 3: Chart created for the input
o a 62a364a5$.
Figure 1: Example of set of states and goto function.
</figureCaption>
<page confidence="0.997877">
113
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948007">
<title confidence="0.9993725">Polynomial Time and Space Shift-Reduce Parsing of Arbitrary Context-free Grammars.*</title>
<author confidence="0.998845">Yves Schabes</author>
<affiliation confidence="0.99928">of Computer Science University of Pennsylvania</affiliation>
<address confidence="0.999983">Philadelphia, PA 19104-6389, USA</address>
<email confidence="0.999969">e-mail:schabes©linc.cis.upenn.edu</email>
<abstract confidence="0.997942583333333">We introduce an algorithm for designing a predictive left to right shift-reduce non-deterministic push-down machine corresponding to an arbitrary unrestricted context-free grammar and an algorithm for efficiently driving this machine in pseudo-parallel. The performance of the resulting parser is formally proven to be superior to Earley&apos;s parser (1970). The technique employed consists in constructing before run-time a parsing table that encodes a nondeterministic machine in the which the predictive behavior has been compiled out. At run time, the machine is driven in pseudo-parallel with the help of a chart. The recognizer behaves in the worst case in and However in practice it is always superior to Earley&apos;s parser since the prediction steps have been compiled before runtime. Finally, we explain how other more efficient variants of the basic parser can be obtained by determinizing portions of the basic non-deterministic pushdown machine while still using the same pseudoparallel driver.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J D Ullman</author>
</authors>
<date>1973</date>
<booktitle>Theory of Parsing, Translation and Compiling. Vol 1: Parsing. Prentice-Hall,</booktitle>
<location>Englewood Cliffs, NJ.</location>
<marker>Aho, Ullman, 1973</marker>
<rawString>A. V. Aho and J. D. Ullman. 1973. Theory of Parsing, Translation and Compiling. Vol 1: Parsing. Prentice-Hall, Englewood Cliffs, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay C Earley</author>
</authors>
<title>An Efficient Context-Free Parsing Algorithm.</title>
<date>1968</date>
<tech>Ph.D. thesis,</tech>
<institution>Carnegie-Mellon University,</institution>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="1329" citStr="Earley, 1968" startWordPosition="192" endWordPosition="193">n the which the predictive behavior has been compiled out. At run time, the machine is driven in pseudo-parallel with the help of a chart. The recognizer behaves in the worst case in 0(1G12n3)-time and O(lGIn2)-space. However in practice it is always superior to Earley&apos;s parser since the prediction steps have been compiled before runtime. Finally, we explain how other more efficient variants of the basic parser can be obtained by determinizing portions of the basic non-deterministic pushdown machine while still using the same pseudoparallel driver. 1 Introduction Predictive bottom-up parsers (Earley, 1968; Earley, 1970; Graham et al., 1980) are often used for natural language processing because of their superior average performance compared to purely bottom-up parsers *We are extremely indebted to Fernando Pereira and Stuart Shieber for providing valuable technical comments during discussions about earlier versions of this algorithm. We are also grateful to Aravind Joshi for his support of this research. We also thank Robert Frank. All remaining errors are the author&apos;s responsibility alone. This research was partially funded by ARO grant DAAL03-89-00031PRI and DARPA grant N00014- 90-J-1863. su</context>
</contexts>
<marker>Earley, 1968</marker>
<rawString>Jay C. Earley. 1968. An Efficient Context-Free Parsing Algorithm. Ph.D. thesis, Carnegie-Mellon University, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay C Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Commun. ACM,</journal>
<pages>13--2</pages>
<contexts>
<context position="1343" citStr="Earley, 1970" startWordPosition="194" endWordPosition="195">e predictive behavior has been compiled out. At run time, the machine is driven in pseudo-parallel with the help of a chart. The recognizer behaves in the worst case in 0(1G12n3)-time and O(lGIn2)-space. However in practice it is always superior to Earley&apos;s parser since the prediction steps have been compiled before runtime. Finally, we explain how other more efficient variants of the basic parser can be obtained by determinizing portions of the basic non-deterministic pushdown machine while still using the same pseudoparallel driver. 1 Introduction Predictive bottom-up parsers (Earley, 1968; Earley, 1970; Graham et al., 1980) are often used for natural language processing because of their superior average performance compared to purely bottom-up parsers *We are extremely indebted to Fernando Pereira and Stuart Shieber for providing valuable technical comments during discussions about earlier versions of this algorithm. We are also grateful to Aravind Joshi for his support of this research. We also thank Robert Frank. All remaining errors are the author&apos;s responsibility alone. This research was partially funded by ARO grant DAAL03-89-00031PRI and DARPA grant N00014- 90-J-1863. such as CKY-styl</context>
<context position="7251" citStr="Earley, 1970" startWordPosition="1131" endWordPosition="1132">proportional to the size of the grammar. Then at run time, we efficiently drive this machine in pseudo-parallel. Even if all the states of the machine are reached for some grammars and inputs, a polynomial complexity will still be obtained since the number of states is bounded by the grammar size. We therefore introduce a shift-reduce driver for this machine in which all of the predictive component has been compiled in the finite state control of the machine. The technique makes no requirement on the form of the context-free grammar and it behaves in the worst case as well as Earley&apos;s parser (Earley, 1970). The push-down machine is built before runtime and it is encoded as parsing tables in the which the predictive behavior has been compiled out. In the worst case, the recognizer behaves in the same 0(IGI2n3)-time and 0(IGIn2)-space as Earley&apos;s parser. However in practice it is always superior to Earley&apos;s parser since the prediction steps have been eliminated before run-time. We show that the items produced in the chart correspond to equivalence classes on the items produced for the same input by Earley&apos;s parser. This mapping formally shows its practical superior behavior.3 Finally, we explain </context>
<context position="21452" citStr="Earley, 1970" startWordPosition="3799" endWordPosition="3800">ion step which takes in the worst case 0(IGIn)-time for Earley&apos;s parser. Therefore, in the most favorable scenario, the parser we 6Kernel shift and non-kernel shift moves require both at most 0( I GI n2 )- time. 6Due to the lack of space, the details of the implementation are not given in this paper but they are given in (Schabes, 1991). 110 propose will require 0(JGIn) less time than Earley&apos;s parser. For a given context-free grammar G and an input string al • • • an, let C be the set of items produced by the parser and Cearley be the set of items produced by Earley&apos;s parser. Earley&apos;s parser (Earley, 1970) produces items of the form (A --+ a • ,3, i, j) where A -+ a • /3 is a single dotted rule and not a set of dotted rules. The following lemma shows how one can map the items that the parser produces to the items that Earley&apos;s parser produces for the same grammar and input: Lemma 1 If (s, j) E C then we have: (i) for all kernel dotted rules A a • /3 E s, we have (A a • /3, i, j)c _ _ earley (ii) and for all non-kernel dotted rules A E s, we have (A j, j) E Ceariey The proof of the above lemma is by induction on the number of items added to the chart C. This shows that an item is mapped into a s</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>Jay C. Earley. 1970. An efficient context-free parsing algorithm. Commun. ACM, 13(2):94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S L Graham</author>
<author>M A Harrison</author>
<author>W L Ruzzo</author>
</authors>
<title>An improved context-free recognizer.</title>
<date>1980</date>
<journal>ACM Transactions on Programming Languages and Systems,</journal>
<pages>2--3</pages>
<contexts>
<context position="1365" citStr="Graham et al., 1980" startWordPosition="196" endWordPosition="199">ehavior has been compiled out. At run time, the machine is driven in pseudo-parallel with the help of a chart. The recognizer behaves in the worst case in 0(1G12n3)-time and O(lGIn2)-space. However in practice it is always superior to Earley&apos;s parser since the prediction steps have been compiled before runtime. Finally, we explain how other more efficient variants of the basic parser can be obtained by determinizing portions of the basic non-deterministic pushdown machine while still using the same pseudoparallel driver. 1 Introduction Predictive bottom-up parsers (Earley, 1968; Earley, 1970; Graham et al., 1980) are often used for natural language processing because of their superior average performance compared to purely bottom-up parsers *We are extremely indebted to Fernando Pereira and Stuart Shieber for providing valuable technical comments during discussions about earlier versions of this algorithm. We are also grateful to Aravind Joshi for his support of this research. We also thank Robert Frank. All remaining errors are the author&apos;s responsibility alone. This research was partially funded by ARO grant DAAL03-89-00031PRI and DARPA grant N00014- 90-J-1863. such as CKY-style parsers (Kasami, 196</context>
<context position="23771" citStr="Graham et al. (1980)" startWordPosition="4250" endWordPosition="4253">Stvio,ipCT and X..FA wymA. The parser we propose is therefore more efficient than Earley&apos;s parser since it has compiled out prediction before run time. How much more efficient it is, depends on how prolific the prediction is and therefore on the nature of the grammar and the input string. 6 Optimizations The parser can be easily extended to incorporate standard optimization techniques proposed for predictive parsers. The closure operation which defines how a state is constructed already optimizes the parser on chain derivations in a manner very similar to the techniques originally proposed by Graham et al. (1980) and later also used by Leiss (1990). In addition, the closure operation can be designed to optimize the processing of non-terminal symbols that derive the empty string in manner very similar to the one proposed by Graham et al. (1980) and Leiss (1990). The idea is to perform the reduction of symbols that derive the empty string at compilation time, i.e. include this type of reduction in the definition of closure by adding If s is a state, then closure(s) is now the state constructed from s by the three rules: (i) Initially, every dotted rule in s is added to closure(s); (ii) if A -4 a • B /3 </context>
</contexts>
<marker>Graham, Harrison, Ruzzo, 1980</marker>
<rawString>S.L. Graham, M.A. Harrison, and W.L. Ruzzo. 1980. An improved context-free recognizer. ACM Transactions on Programming Languages and Systems, 2(3):415-462, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>The computational complexity of Tomita&apos;s algorithm.</title>
<date>1989</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies,</booktitle>
<location>Pittsburgh,</location>
<contexts>
<context position="4947" citStr="Johnson, 1989" startWordPosition="751" endWordPosition="752"> seem to show that the parser behaves in practice better than Earley&apos;s parser (which is proven to take in the worst case 0(1G120)-time), the duplication of the same experiments shows no conclusive outcome. Modifications to Tomita&apos;s algorithm have been proposed in order to alleviate the exponential complexity with respect to the input length (Kipps, 1989) but, according to Kipps, the modified algorithm does not lead to a practical parser. Furthermore, the algorithm is doomed to behave in the worst case in exponential time with respect to the grammar size for some ambiguous grammars and inputs (Johnson, 1989).2 So far, there is no formal proof showing that the Tomita&apos;s parser can be superior for some grammars and inputs to Earley&apos;s parser, and its worst case complexity seems to contradict the experimental data. As explained, the previous attempts to compile the predictive component are not general and achieve a worst case complexity (with respect to the grammar size and the input length) worse than standard parsers. The methodology we follow in order to compile the predictive component of Earley&apos;s parser is to define a predictive bottom-up pushdown machine equivalent to the given grammar which we </context>
<context position="6218" citStr="Johnson, 1989" startWordPosition="961" endWordPosition="962">ent, any parsing algorithm based on the LR(0) characteristic machine is doomed to behave in exponential time with respect to the grammar size for some ambiguous grammars and inputs. This is a result of the fact that the number of states of an LR(0) characteristic machine can be exponential and that there are some grammars and inputs for which an exponential number of states must be reached (See Johnson [1989] for examples of such grammars and inputs). One must therefore design a different pushdown machine which 1The same argument for the exponential grammar size complexity of Tomita&apos;s parser (Johnson, 1989) holds for Lang&apos;s technique. 2This problem is particularly acute for natural language processing since in this context the input length is typically small (10-20 words) and the grammar size very large (hundreds or thousands of rules and symbols). can be driven efficiently in pseudo-parallel. We construct a non-deterministic predictive pushdown machine given an arbitrary context-free grammar whose number of states is proportional to the size of the grammar. Then at run time, we efficiently drive this machine in pseudo-parallel. Even if all the states of the machine are reached for some grammars</context>
<context position="18233" citStr="Johnson, 1989" startWordPosition="3226" endWordPosition="3227">RUCTION OF THE PARSING TABLES) Appendix A gives an example of a parsing table. 3 Complexity The recognizer requires in the worst case 0(1GIn2)- space and 0(IGI2n5)-time; n is the length of the input string, IGI is the size of the grammar computed as the sum of the lengths of the right hand side of each productions: IGI = E Ial where lal is the length of a. A—*cr EP One of the objectives for the design of the nondeterministic machine was to make sure that it was not possible to reach an exponential number of states, a property without which the machine is doomed to have exponential complexity (Johnson, 1989). First we observe that the number of states of the finite state control of the non-deterministic machine that we constructed in Section 2.2 is proportional to the size of the grammar, IG. By construction, each state (except for the start state) contains exactly one kernel dotted rule. Therefore, the number of states is bounded by the maximum number of kernel rules of the form A -4 a4o)3 (with a non empty), and is 0(IGI). We conclude that the algorithm requires in the worst case 0(IGIn2)-space since the maximum number of items (s, i, j) in the chart is proportional to IGIn2. A close look at th</context>
<context position="25578" citStr="Johnson (1989)" startWordPosition="4591" endWordPosition="4592"> that the complexity of the resulting parser with respect to the grammar size is not exponential or worse than O(1G12)- time as other well known parsers. However, we may use some non-determinism in the machine to guarantee this property. The non-determinism of the machine is not a problem since we have shown how the non-deterministic machine can be efficiently driven in pseudo-parallel (in O( IGI2n3)-time). We can now ask the question of whether it is possible to determinize the finite state control of the machine while still being able to bound the complexity of the parser to 0(IGI2n3)-time. Johnson (1989) exhibits grammars for which the full determinization 111 of the finite state control (the LR(0) construction) leads to a parser with exponential complexity, because the finite state control has an exponential number of states and also because there are some input string for which an exponential number of states will be reached. However, there are also cases where the full determinization either will not increase the number of states or will not lead to a parser with exponential complexity because there are no input that require to reach an exponential number of states. We are currently studyi</context>
</contexts>
<marker>Johnson, 1989</marker>
<rawString>Mark Johnson. 1989. The computational complexity of Tomita&apos;s algorithm. In Proceedings of the International Workshop on Parsing Technologies, Pittsburgh, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kasami</author>
</authors>
<title>An efficient recognition and syntax algorithm for context-free languages.</title>
<date>1965</date>
<tech>Technical Report AF-CRL-65-758,</tech>
<institution>Air Force Cambridge Research Laboratory,</institution>
<location>Bedford, MA.</location>
<contexts>
<context position="1966" citStr="Kasami, 1965" startWordPosition="286" endWordPosition="287"> al., 1980) are often used for natural language processing because of their superior average performance compared to purely bottom-up parsers *We are extremely indebted to Fernando Pereira and Stuart Shieber for providing valuable technical comments during discussions about earlier versions of this algorithm. We are also grateful to Aravind Joshi for his support of this research. We also thank Robert Frank. All remaining errors are the author&apos;s responsibility alone. This research was partially funded by ARO grant DAAL03-89-00031PRI and DARPA grant N00014- 90-J-1863. such as CKY-style parsers (Kasami, 1965; Younger, 1967). Their practical superiority is mainly obtained because of the top-down filtering accomplished by the predictive component of the parser. Compiling out as much as possible this predictive component before run-time will result in a more efficient parser so long as the worst case behavior is not deteriorated. Approaches in this direction have been investigated (Earley, 1968; Lang, 1974; Tomita, 1985; Tomita, 1987), however none of them is satisfying, either because the worst case complexity is deteriorated (worse than Earley&apos;s parser) or because the technique is not general. Fur</context>
</contexts>
<marker>Kasami, 1965</marker>
<rawString>T. Kasami. 1965. An efficient recognition and syntax algorithm for context-free languages. Technical Report AF-CRL-65-758, Air Force Cambridge Research Laboratory, Bedford, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Kipps</author>
</authors>
<title>Analysis of Tomita&apos;s algorithm for general context-free parsing.</title>
<date>1989</date>
<booktitle>In Proceedings of the International Workshop on Parsing Technologies,</booktitle>
<location>Pittsburgh,</location>
<contexts>
<context position="4689" citStr="Kipps, 1989" startWordPosition="708" endWordPosition="709">ushdown automaton suffers from an exponential time and space worst case complexity with respect to the input length and also with respect to the grammar size (Johnson [1989] and also page 72 in Tomita [1985]). Although Tomita reports experimental data that seem to show that the parser behaves in practice better than Earley&apos;s parser (which is proven to take in the worst case 0(1G120)-time), the duplication of the same experiments shows no conclusive outcome. Modifications to Tomita&apos;s algorithm have been proposed in order to alleviate the exponential complexity with respect to the input length (Kipps, 1989) but, according to Kipps, the modified algorithm does not lead to a practical parser. Furthermore, the algorithm is doomed to behave in the worst case in exponential time with respect to the grammar size for some ambiguous grammars and inputs (Johnson, 1989).2 So far, there is no formal proof showing that the Tomita&apos;s parser can be superior for some grammars and inputs to Earley&apos;s parser, and its worst case complexity seems to contradict the experimental data. As explained, the previous attempts to compile the predictive component are not general and achieve a worst case complexity (with respe</context>
</contexts>
<marker>Kipps, 1989</marker>
<rawString>James R. Kipps. 1989. Analysis of Tomita&apos;s algorithm for general context-free parsing. In Proceedings of the International Workshop on Parsing Technologies, Pittsburgh, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Knuth</author>
</authors>
<title>On the translation of languages from left to right.</title>
<date>1965</date>
<journal>Information and Control,</journal>
<pages>8--607</pages>
<contexts>
<context position="2857" citStr="Knuth, 1965" startWordPosition="424" endWordPosition="425">s the worst case behavior is not deteriorated. Approaches in this direction have been investigated (Earley, 1968; Lang, 1974; Tomita, 1985; Tomita, 1987), however none of them is satisfying, either because the worst case complexity is deteriorated (worse than Earley&apos;s parser) or because the technique is not general. Furthermore, none of these approaches have been formally proven to have a behavior superior to well known parsers such as Earley&apos;s parser. Earley himself ([1968] pages 69-89) proposed to precompile the state sets generated by his algorithm to make it as efficient as LR(k) parsers (Knuth, 1965) when used on LR(k) grammars by precomputing all possible states sets that the parser could create. However, some context-free grammars, including most likely most natural language grammars, cannot be compiled using his technique and the problem of knowing if a grammar can be compiled with this technique is undecidable (Earley [1968], page 99). Lang (1974) proposed a technique for evaluating in pseudo-parallel non-deterministic push down automata. Although this technique achieves a worst case complexity of 0(713)-time with respect to the length of input, it requires that at most two symbols ar</context>
</contexts>
<marker>Knuth, 1965</marker>
<rawString>D. E. Knuth. 1965. On the translation of languages from left to right. Information and Control, 8:607-639.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Lang</author>
</authors>
<title>Deterministic techniques for efficient non-deterministic parsers.</title>
<date>1974</date>
<booktitle>Automata, Languages and Programming, 2nd Colloquium, University of Saarbriicken. Lecture Notes in Computer Science,</booktitle>
<editor>In Jacques Loeckx, editor,</editor>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="2369" citStr="Lang, 1974" startWordPosition="346" endWordPosition="347">rank. All remaining errors are the author&apos;s responsibility alone. This research was partially funded by ARO grant DAAL03-89-00031PRI and DARPA grant N00014- 90-J-1863. such as CKY-style parsers (Kasami, 1965; Younger, 1967). Their practical superiority is mainly obtained because of the top-down filtering accomplished by the predictive component of the parser. Compiling out as much as possible this predictive component before run-time will result in a more efficient parser so long as the worst case behavior is not deteriorated. Approaches in this direction have been investigated (Earley, 1968; Lang, 1974; Tomita, 1985; Tomita, 1987), however none of them is satisfying, either because the worst case complexity is deteriorated (worse than Earley&apos;s parser) or because the technique is not general. Furthermore, none of these approaches have been formally proven to have a behavior superior to well known parsers such as Earley&apos;s parser. Earley himself ([1968] pages 69-89) proposed to precompile the state sets generated by his algorithm to make it as efficient as LR(k) parsers (Knuth, 1965) when used on LR(k) grammars by precomputing all possible states sets that the parser could create. However, som</context>
</contexts>
<marker>Lang, 1974</marker>
<rawString>Bernard Lang. 1974. Deterministic techniques for efficient non-deterministic parsers. In Jacques Loeckx, editor, Automata, Languages and Programming, 2nd Colloquium, University of Saarbriicken. Lecture Notes in Computer Science, Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Leiss</author>
</authors>
<title>On Kilbury&apos;s modification of Earley&apos;s algorithm.</title>
<date>1990</date>
<journal>ACM Transactions on Programming Languages and Systems,</journal>
<pages>12--4</pages>
<contexts>
<context position="23807" citStr="Leiss (1990)" startWordPosition="4259" endWordPosition="4260">pose is therefore more efficient than Earley&apos;s parser since it has compiled out prediction before run time. How much more efficient it is, depends on how prolific the prediction is and therefore on the nature of the grammar and the input string. 6 Optimizations The parser can be easily extended to incorporate standard optimization techniques proposed for predictive parsers. The closure operation which defines how a state is constructed already optimizes the parser on chain derivations in a manner very similar to the techniques originally proposed by Graham et al. (1980) and later also used by Leiss (1990). In addition, the closure operation can be designed to optimize the processing of non-terminal symbols that derive the empty string in manner very similar to the one proposed by Graham et al. (1980) and Leiss (1990). The idea is to perform the reduction of symbols that derive the empty string at compilation time, i.e. include this type of reduction in the definition of closure by adding If s is a state, then closure(s) is now the state constructed from s by the three rules: (i) Initially, every dotted rule in s is added to closure(s); (ii) if A -4 a • B /3 is in closure(s) and B 7 is a produc</context>
</contexts>
<marker>Leiss, 1990</marker>
<rawString>Hans Leiss. 1990. On Kilbury&apos;s modification of Earley&apos;s algorithm. ACM Transactions on Programming Languages and Systems, 12(4):610-640, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Polynomial time and space shift-reduce parsing of context-free grammars and of tree-adjoining grammars.</title>
<date>1991</date>
<booktitle>In preparation.</booktitle>
<contexts>
<context position="14804" citStr="Schabes, 1991" startWordPosition="2569" endWordPosition="2570">r2 E GOTOnk(st, X), (r2, i,j) is added to C. If f(s, 0, n) I (s, 0,n) E C and s E 0 0 then return acceptance otherwise return rejection. end (* recognizer *) In the above algorithm, non-determinism arises from multiple entries in ACTION(s, a) and also from the fact that GOTOk(s, X) and GOTOnk(s, X) contain a set of states. 2.2 Construction of the Parsing Tables We shall give an LR(0)-like method for constructing the parsing tables corresponding to the basic NPDA. Several other methods (such as LR(k)-like, SLR(k)- like) can also be used for constructing the parsing tables and are described in (Schabes, 1991). To construct the LR(0)-like finite state control for the basic non-deterministic push-down automaton that the parser simulates, we define three functions, closure, gotok and Monk • If s is a state, then closure(s) is the state constructed from s by the two rules: (i) Initially, every dotted rule in s is added to closure(s); (ii) If A—. a • B13 is in closure(s) and B 7 is a production, then add the dotted rule B .7 to closure(s) (if it is not already there). This rule is applied until no more new dotted rules can be added to closure(s). If s is a state and if X is a non-terminal or terminal s</context>
<context position="20504" citStr="Schabes, 1991" startWordPosition="3633" endWordPosition="3634">ied to record all parse trees of the input string. The representation is similar to a shared forest. The worst case time complexity of the parser is the same as for the recognizer (0(IG12n3)-time) but, as for Earley&apos;s parser, the worst case space complexity increases to 0(102n5) because of the additional bookkeeping. 5 Correctness and Comparison with Earley&apos;s Parser We derive the correctness of the parser by showing how it can be mapped to Earley&apos;s parser. In the process, we will also be able to show why this parser can be more efficient than Earley&apos;s parser. The detailed proofs are given in (Schabes, 1991). We are also interested in formally characterizing the differences in performance between the parser we propose and Earley&apos;s parser. We show that the parser behaves in the worst scenario as well as Earley&apos;s parser by mapping it into Earley&apos;s parser. The parser behaves better than Earley&apos;s parser because it has eliminated the prediction step which takes in the worst case 0(IGIn)-time for Earley&apos;s parser. Therefore, in the most favorable scenario, the parser we 6Kernel shift and non-kernel shift moves require both at most 0( I GI n2 )- time. 6Due to the lack of space, the details of the impleme</context>
</contexts>
<marker>Schabes, 1991</marker>
<rawString>Yves Schabes. 1991. Polynomial time and space shift-reduce parsing of context-free grammars and of tree-adjoining grammars. In preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Using restriction to extend parsing algorithms for complex-feature-based formalisms.</title>
<date>1985</date>
<booktitle>In 23&amp;quot;1 Meeting of the Association for Computational Linguistics (ACL &apos;85),</booktitle>
<location>Chicago,</location>
<contexts>
<context position="28025" citStr="Shieber (1985)" startWordPosition="4985" endWordPosition="4986">le to efficiently evaluate in pseudo-parallel the non-deterministic push down machine constructed for the given context-free grammar. The same worst case complexity as Earley&apos;s recognizer is achieved: 0(IGI2n3)-time and 0(IGIn2)- space. However, in practice, it is superior to Earley&apos;s parser since all the prediction steps and some of the completion steps have been compiled before run-time. The parser can be modified to simulate other types of machines (such LR(k)-like or SLR-like automata). It can also be extended to handle unification based grammars using a similar method as that employed by Shieber (1985) for extending Earley&apos;s algorithm. Furthermore, the algorithm can be tuned to a particular grammar and therefore be made more efficient by carefully determinizing portions of the nondeterministic machine while making sure that the number of states in not increased. These variants lead to more efficient parsers than the one based on the basic non-deterministic push-down machine. Furthermore, the same pseudo-parallel driver can be used for all these machines. We have adapted the technique presented in this paper to other grammatical formalism such as treeadjoining grammars (Schabes, 1991). Bibli</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Stuart M. Shieber. 1985. Using restriction to extend parsing algorithms for complex-feature-based formalisms. In 23&amp;quot;1 Meeting of the Association for Computational Linguistics (ACL &apos;85), Chicago, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language, A Fast Algorithm for Practical Systems.</title>
<date>1985</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="2383" citStr="Tomita, 1985" startWordPosition="348" endWordPosition="349">maining errors are the author&apos;s responsibility alone. This research was partially funded by ARO grant DAAL03-89-00031PRI and DARPA grant N00014- 90-J-1863. such as CKY-style parsers (Kasami, 1965; Younger, 1967). Their practical superiority is mainly obtained because of the top-down filtering accomplished by the predictive component of the parser. Compiling out as much as possible this predictive component before run-time will result in a more efficient parser so long as the worst case behavior is not deteriorated. Approaches in this direction have been investigated (Earley, 1968; Lang, 1974; Tomita, 1985; Tomita, 1987), however none of them is satisfying, either because the worst case complexity is deteriorated (worse than Earley&apos;s parser) or because the technique is not general. Furthermore, none of these approaches have been formally proven to have a behavior superior to well known parsers such as Earley&apos;s parser. Earley himself ([1968] pages 69-89) proposed to precompile the state sets generated by his algorithm to make it as efficient as LR(k) parsers (Knuth, 1965) when used on LR(k) grammars by precomputing all possible states sets that the parser could create. However, some context-free</context>
<context position="3793" citStr="Tomita (1985" startWordPosition="573" endWordPosition="574">rley [1968], page 99). Lang (1974) proposed a technique for evaluating in pseudo-parallel non-deterministic push down automata. Although this technique achieves a worst case complexity of 0(713)-time with respect to the length of input, it requires that at most two symbols are popped from the stack in a single move. When the technique is used for shift-reduce parsing, this constraint requires that the context-free grammar is in Chomsky normal form (CNF). As far as the grammar size is concerned, an exponential worst case behavior is reached when used with the characteristic LR(0) 106 machine.1 Tomita (1985; 1987) proposed to extend LR(0) parsers to non-deterministic context-free grammars by explicitly using a graph structured stack which represents the pseudo-parallel evaluation of the moves of a non-deterministic LR(0) push-down automaton. Tomita&apos;s encoding of the non-deterministic pushdown automaton suffers from an exponential time and space worst case complexity with respect to the input length and also with respect to the grammar size (Johnson [1989] and also page 72 in Tomita [1985]). Although Tomita reports experimental data that seem to show that the parser behaves in practice better tha</context>
</contexts>
<marker>Tomita, 1985</marker>
<rawString>Masaru Tomita. 1985. Efficient Parsing for Natural Language, A Fast Algorithm for Practical Systems. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaru Tomita</author>
</authors>
<title>An efficient augmentedcontext-free parsing algorithm.</title>
<date>1987</date>
<journal>Computational Linguistics,</journal>
<pages>13--31</pages>
<contexts>
<context position="2398" citStr="Tomita, 1987" startWordPosition="350" endWordPosition="351"> are the author&apos;s responsibility alone. This research was partially funded by ARO grant DAAL03-89-00031PRI and DARPA grant N00014- 90-J-1863. such as CKY-style parsers (Kasami, 1965; Younger, 1967). Their practical superiority is mainly obtained because of the top-down filtering accomplished by the predictive component of the parser. Compiling out as much as possible this predictive component before run-time will result in a more efficient parser so long as the worst case behavior is not deteriorated. Approaches in this direction have been investigated (Earley, 1968; Lang, 1974; Tomita, 1985; Tomita, 1987), however none of them is satisfying, either because the worst case complexity is deteriorated (worse than Earley&apos;s parser) or because the technique is not general. Furthermore, none of these approaches have been formally proven to have a behavior superior to well known parsers such as Earley&apos;s parser. Earley himself ([1968] pages 69-89) proposed to precompile the state sets generated by his algorithm to make it as efficient as LR(k) parsers (Knuth, 1965) when used on LR(k) grammars by precomputing all possible states sets that the parser could create. However, some context-free grammars, incl</context>
</contexts>
<marker>Tomita, 1987</marker>
<rawString>Masaru Tomita. 1987. An efficient augmentedcontext-free parsing algorithm. Computational Linguistics, 13:31-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages</title>
<date>1967</date>
<booktitle>in time n3. Information and Control,</booktitle>
<pages>10--2</pages>
<contexts>
<context position="1982" citStr="Younger, 1967" startWordPosition="288" endWordPosition="289">e often used for natural language processing because of their superior average performance compared to purely bottom-up parsers *We are extremely indebted to Fernando Pereira and Stuart Shieber for providing valuable technical comments during discussions about earlier versions of this algorithm. We are also grateful to Aravind Joshi for his support of this research. We also thank Robert Frank. All remaining errors are the author&apos;s responsibility alone. This research was partially funded by ARO grant DAAL03-89-00031PRI and DARPA grant N00014- 90-J-1863. such as CKY-style parsers (Kasami, 1965; Younger, 1967). Their practical superiority is mainly obtained because of the top-down filtering accomplished by the predictive component of the parser. Compiling out as much as possible this predictive component before run-time will result in a more efficient parser so long as the worst case behavior is not deteriorated. Approaches in this direction have been investigated (Earley, 1968; Lang, 1974; Tomita, 1985; Tomita, 1987), however none of them is satisfying, either because the worst case complexity is deteriorated (worse than Earley&apos;s parser) or because the technique is not general. Furthermore, none o</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>D. H. Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10(2):189-208.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>