<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000053">
<title confidence="0.9883255">
Two Approaches to Aspect Assignment in an English-Polish Machine
Translation System
</title>
<author confidence="0.933356">
Anna Kupge
</author>
<affiliation confidence="0.9781985">
Polish Academy of Sciences, Institute of Computer Science and
Carnegie Mellon University, Language Technologies Institute
</affiliation>
<email confidence="0.996787">
aniak@cs.cmu.edu
</email>
<sectionHeader confidence="0.998592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999847666666667">
The paper presents two approaches
to aspect assignment in a knowledge-
based English-Polish machine transla-
tion (MT) system. The first method uses
a set of heuristic rules based on interlin-
gua (IR) representation provided by the
system, whereas the other employs ma-
chine learning techniques. Both meth-
ods have similar performance and ob-
tain high accuracy of over 88% on test
data. The crucial difference, however,
is the development effort: the machine
learning technique is fully automatic,
whereas heuristic rules are derived man-
ually.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999915461538462">
The paper presents two methods to deal with as-
pect assignment in a prototype of a knowledge-
based English-Polish machine translation (MT)
system. Although there is no agreement
among linguists as to its precise definition, e.g.,
Vendler (1967), Comrie (1976), Dowty (1986),
aspect is a result of complex interplay of seman-
tics, tense, mood and pragmatics and it strongly
affects overall text understanding. In English, as-
pect is usually not explicitly indicated on a verb.
On the other hand, in Polish it is overtly man-
ifested and incorporated into verb morphology.
This difference between the two languages makes
English-Polish translation particularly difficult as
it requires contextual and semantic analysis of the
English input in order to derive aspect value for
the Polish output.
The MT system presented in this paper takes
advantage of a knowledge-based interlingua (IR)
representation in order to assign aspect in Polish
translation. We propose two approaches based
on this representation. First, we provide a set
of human-defined heuristic rules (similar to &apos;cues
strategy&apos; presented in Gawroriska (1993)), and
second, we use machine learning techniques to
learn aspect assignment rules. The former ap-
proach has been incorporated into the system,
whereas the latter has been, so far, run separately
as an experiment. The results obtained by both
methods are quite similar. The crucial difference,
however, is the effort put into their development:
the machine learning approach is fully automatic
and rules are derived from examples rather than
hand-coded.
The organization of the paper is as follows:
section 2 briefly presents the system architecture,
sections 3 and 4 describe heuristic rules and the
machine learning approach, respectively. Finally,
section 5 contains conclusions.
</bodyText>
<sectionHeader confidence="0.988639" genericHeader="method">
2 System description
</sectionHeader>
<bodyText confidence="0.998641875">
The English-Polish MT project presented in this
paper is an extension of the existing multilin-
gual KANTO() system (a reimplementation of
the KANT system, cf. Mitamura et al. (1991),
Mitamura and Nyberg (1992)) developed at
Carnegie Mellon University. KANTO() is a
knowledge-based, high-quality, domain-specific
MT system (in the English-Polish MT project,
</bodyText>
<page confidence="0.998309">
17
</page>
<bodyText confidence="0.931412090909091">
the domain is restricted to printer manuals) and it
uses Interlingua (IR) as a semantic representation,
see Leavitt et al. (1994). The system takes as
an input a text written in constrained English
(controlled language), which limits vocabulary
and grammar of sentences accepted by the system,
cf. Kamprath et al. (1998). Example (1) presents
a sample English input along with the IR repre-
sentation and its Polish translation provided by
the system.
(0 The printer prints pages.
</bodyText>
<equation confidence="0.6060684375">
(*A-PRINT
(agent
(*O-PRINTER
(number singular)
(reference
definite)))
(argument-class
agent+theme)
(mood declarative)
(punctuation period)
(tense present)
(theme
(*O-PAGE
(number plural)
(reference
no-reference))))
</equation>
<bodyText confidence="0.985658710526316">
Drukarka drukuje strony.
IR illustrated in (1) is the input for the Polish
generation module. The module consists of four
components: a mapper, a unification grammar (a
type of context-free grammar), a morphological
generator and a post-processing module. Mapping
rules transform the IR semantic representation into
a syntactic structure corresponding to the Polish
output. The structure is a functional structure or
FS in the LFG (Lexical Functional Grammar) for-
malism, cf. Bresnan (1982). Generation grammar
rules convert this FS into a list of lexical tokens
(FS frames), which are then fed to the morphol-
ogy module responsible for generating appropriate
inflected forms. Finally, a set of post-processing
rules is applied to produce the resulting surface
form of translation by cleaning up spacing, adding
capitalization, inserting punctuation, etc. In order
to develop the current system, a small corpus of
about 280 English sentences from a printer man-
ual has been examined. This corpus served as a
baseline to develop the two approaches to aspect
assignment presented in the paper.
As mentioned above, aspect is incorporated
into verb morphology in Polish. Polish verbs
may have two aspect forms: imperfective, e.g.,
drukuje &apos;prints&apos;, or perfective, e.g., wydrukuje
&apos;will printlsg (out)&apos;. Aspect is independent of
tense or mood as it is also present on infini-
tives: drukowa. &apos;to printimper f and wydrukolvae
&apos;to printp„f (out)&apos;, or on gerunds: drukowanie
&apos;printingimp„f &apos; and wydrukowanie `printingp„f
(out)&apos;.
Since English verbs do not have morphologi-
cal aspect, we consider lexical concepts, e.g., *A_
PRINT in (1), ambiguous: they can be trans-
lated by either a perfective or an imperfective verb,
see (2).
</bodyText>
<equation confidence="0.947408833333333">
(2) *A-PRINT = ([?verb] drukowac)
drukowac =
(*OR* ((morph verb-imperf)
(root drukuje))
((morph verb-perf)
(root wydrukuje))),
</equation>
<bodyText confidence="0.99958675">
The role of aspect assignment rules is to specify
which form to use in translation. The next two
sections describe two methods which provide such
rules based on IR specification.
</bodyText>
<sectionHeader confidence="0.98791" genericHeader="method">
3 Heuristic rules
</sectionHeader>
<bodyText confidence="0.999820166666667">
Heuristic rules are specified in the mapper and
they assign aspect according to attributes found
in IR. The rules are ordered so that more general
cases are considered first and if they do not hold,
more specific rules are applied. Aspect assignment
rules proposed in the system are presented below.
</bodyText>
<subsectionHeader confidence="0.984441">
3.1 Declarative Mood
</subsectionHeader>
<bodyText confidence="0.999949666666667">
For finite verbs in declarative mood, aspect pri-
marily depends on tense. First, all continuous
forms, marked as (progressive +) in IR,
are translated as imperfective. Next, forms of
perfective tenses, i.e., (perfective +) , are
translated as perfective. Then, verbs in simple
</bodyText>
<page confidence="0.998474">
18
</page>
<bodyText confidence="0.998738666666667">
past, (tense past ) , or future simple tenses,
(tense future) , are translated as perfective.
Similar assignment rules have been proposed in
Gawrofiska (1993).
Additionally, we assume that certain types of
subordinate conjunctions, e.g., &apos;while&apos;, &apos;once&apos;,
&apos;before&apos;, etc., impose aspect requirements on a
verb in the subordinate clause. The following as-
signments have been proposed:
</bodyText>
<listItem confidence="0.9997025">
• &apos;while&apos;: imperfective
(3) You can send an electronic fax while the
</listItem>
<bodyText confidence="0.896633">
printer makes copies.
Mona wyslae elektroniczny faks,
can senclinf electronic fax
podczas gdy drukarka robi
while printer makesimp„f
kopie.
copies
</bodyText>
<listItem confidence="0.9992025">
• &apos;once&apos;, &apos;after&apos;, &apos;before&apos;, &apos;until&apos;: perfective; addi-
tionally clauses introduced by the conjunction &apos;un-
til&apos; have to be negated in Polish
(4) Jobs also queue and wait until another job
</listItem>
<bodyText confidence="0.937344571428572">
finishes.
Zadania take ustawiaj4 si w kolejce
jobs also stand REFL in queue
i czekaj4, dopoki inne zadanie nie
and wait until another job not
skoficzy sic.
finishesp„f REEL
</bodyText>
<listItem confidence="0.9964722">
• ty&apos;±gerund: imperfective; such clauses are
translated into Polish by a contemporary adverbial
participle derived only from imperfective verbs,
see Saloni and widzitiski (1985)
(5) Close the document by selecting Close
</listItem>
<bodyText confidence="0.8517154">
from the File menu.
Zamknij dokument wybieraj4c
close document selectingimp„f
Zamknij z menu Plik.
Close from menu File
If none of the above cases hold, we assume that
aspect of present tense verbs is imperfective. This
assignment is valid also for gerunds as they are
represented in IR as present tense verbs with an
additional attribute ( nominal +) .
</bodyText>
<subsectionHeader confidence="0.998845">
3.2 Imperative Mood
</subsectionHeader>
<bodyText confidence="0.9999876">
After a brief analysis of Polish technical doc-
umentation, we decided to condition aspect in
imperative mood on negation. Negated impera-
tives more often appear with imperfective forms
(86.2%), whereas perfective aspect prevails with
non-negated imperatives (83.5%).
Heuristic rules used in the system conform with
the above statistics: we translate non-negated im-
peratives as perfective, (6), and negated impera-
tives as imperfective verbs, (7).
</bodyText>
<listItem confidence="0.9383356">
(6) Print a test page.
Wydrukuj stronc prang,.
printp„f page test
(7) Do not move the lever after the scanner
has begun sending the page.
</listItem>
<bodyText confidence="0.99508575">
Nie przesuwaj diwigni, gdy skaner
not moveimp„f lever when scanner
zacz41 wysylanie strony.
started sending page
</bodyText>
<subsectionHeader confidence="0.984522">
3.3 Infinitives
</subsectionHeader>
<bodyText confidence="0.980765363636364">
Infinitives have no mood or tense specified and
we need separate rules to resolve aspect of these
forms. In general, English infinitives appear as ei-
ther complements of other verbs, e.g., modals, or
they head infinitive clauses introduced by a con-
junction such as &apos;in order to&apos;. We assume that in
the former case, aspect of the infinitive depends
on the governing verb while in the latter — on the
subordinate conjunction.
For the conjunction &apos;in order to&apos;, we assume
that it requires a perfective infinitive argument, (8).
</bodyText>
<listItem confidence="0.866565">
(8) You must unhook the other device in or-
der to connect the printer.
</listItem>
<bodyText confidence="0.998198666666667">
Trzeba wy14czye inne urz4dzenie,
need unhookr„f another device
aby podkczye drukar4.
in order to connectp„f printer
Modal verbs are represented in IR by a set of
semantic attributes such as ability, possi-
bility, tentativity, necessity, obli-
gation, see Leavitt et al. (1994). The following
aspect assignment has been adopted in the system:
</bodyText>
<page confidence="0.989467">
19
</page>
<listItem confidence="0.999952153846154">
• &apos;can&apos;, (ability +) or (possibi-
lity +): perfective;
• &apos;cannot&apos;, (ability +) (nega-
tion +): imperfective;
• &apos;cannot&apos;, (possibility +) (nega-
tion +): perfective;
• &apos;could&apos;, (possibility +) (tenta-
tivity low): perfective;
• &apos;may&apos;, (possibility +) (tentati-
vity medium): perfective;
• &apos;must&apos;, (obligation medium): perfec-
tive;
• &apos;should&apos;, (expectation +): perfective.
</listItem>
<sectionHeader confidence="0.724146" genericHeader="method">
3.4 Results
</sectionHeader>
<bodyText confidence="0.998089333333333">
As mentioned above, aspect strongly depends on
semantic and pragmatic context. Since such in-
formation is impoverished in KANTOO, the pro-
posed rules cannot be perfect. In order to evalu-
ate their performance, results obtained by the sys-
tem have been compared with human translations
of the initial (training) English corpus (280 sen-
tences). The heuristic rules have been developed
in order to accommodate data in the training cor-
pus. Therefore, in order to obtain a more objective
verification of the proposed rules, we additionally
tested the system performance on a separate set of
24 (test) sentences taken from the same manual.
The results obtained on training and test sets are
summarized in Fig. 1.
</bodyText>
<table confidence="0.99265925">
result train test
#verbs % #verbs %
correct 430 88.1% 53 88.3%
incorrect 58 11.9% 7 11.7%
</table>
<figureCaption confidence="0.993325">
Figure 1: Performance of heuristic rules
</figureCaption>
<sectionHeader confidence="0.998321" genericHeader="method">
4 Machine Learning
</sectionHeader>
<bodyText confidence="0.999912666666667">
The machine learning approach described in this
section is also based on the IR representation pro-
vided by the MT system. In this experiment, we
used the C4.5 software to build a decision tree.
Training and test data have been derived from the
same sentences the heuristic rules have been pro-
posed for and evaluated on. We have run the
experiment twice, using two different measures
to build the decision tree: information gain and
gain ratio. Performance of both algorithms has
been evaluated on unpruned and pruned trees. Ad-
ditionally, the optimal (pruned) trees have been
transformed into rules and their accuracy has been
measured as well. Details of the experiment and
its results are presented below.
</bodyText>
<subsectionHeader confidence="0.968752">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999217411764706">
Data used for training and testing were taken
from the same set of sentences the heuristic rules
have been applied to. All sentences have been
analysed by the KANTO() analyser and the re-
sulting IR served as an input for preparing the
data. In particular, we have selected 12 at-
tributes which had been crucial for development
of the heuristic rules: ability, expecta-
tion, marker, mood, necessity, nega-
tion, obligation, perfective, possi-
bility, progressive, tense, tentativ-
ity.
Most of these attributes are taken directly from
IR, with an exception to marker, which has been
introduced to indicate the type of subordinate con-
junction, e.g., &apos;while&apos;, &apos;unless&apos;, &apos;once&apos;, etc. Note
that not all attributes are specified in IR for ev-
ery verb, e g , infinitives do not have the mood at-
tribute. We have slightly modified the mapper to
make sure that all 12 attributes required for learn-
ing are present for every verb and have their values
specified. Values of attributes missing in IR are ei-
ther set to `—&apos; or none, depending on whether the
attribute is binary or has more values. In addition,
every verb in the data set has been labelled with
the correct aspect value based on the human trans-
lation. The resulting 13-tuples served as training
data for the decision tree. The target concept (as-
pect) has been represented by a binary attribute:
0 corresponds to imperfective, 1 to perfective as-
pect. The test data has the same format.
Due to changes in the mapper, the final number
of examples used in the experiment was smaller
than in the original system. The decision tree was
</bodyText>
<page confidence="0.973295">
20
</page>
<bodyText confidence="0.857651">
trained on 417 and tested on 55 examples.
</bodyText>
<subsectionHeader confidence="0.950911">
4.2 Decision Trees
</subsectionHeader>
<bodyText confidence="0.990289909090909">
As mentioned above, we employed two mea-
sures to build a decision tree: information gain,
Quinlan (1986), and gain ratio, Quinlan (1986;
Quinlan (1993). The main difference between the
two techniques is in the size of the resulting tree:
the former favours attributes with multiple values,
which results in a wider (and usually bigger) tree.
Indeed, the tree built according to gain ratio is
smaller (31 nodes), Fig. 2, whereas the one based
on information gain is slightly bigger (33 nodes),
Fig. 3.
</bodyText>
<equation confidence="0.99934540625">
mood = none: 1
mood = declarative:
tense = past: 1
tense = future: 1
tense = none: 0
tense = present:
progressive = +: 0
progressive =
possibility = +: 1
possibility =
marker = to-inf: 1
marker = while: 0
marker = because: 0
marker = if: 0
marker = until: 1
marker = by_ing: 0
marker = in-order-to: 0
marker = after: 1
marker = when: 1
marker = unless: 1
marker = once: 1
marker = so that: 1
marker = none:
I ability = +: 1
I ability = -;
I I obligation =
none: 0
I obligation =
medium: 1
mood = imperative:
I negation = +: 0
I negation = 1
</equation>
<figureCaption confidence="0.995939">
Figure 2: Decision tree based on gain ratio
</figureCaption>
<bodyText confidence="0.999834142857143">
The produced trees turned out to be optimal
with respect to the learning algorithm (every node
in the tree produced an improvement over the
training data) and no nodes were pruned. Evalu-
ation of the decision trees on the training and test
data is summarized in Fig. 4.
The error estimate presented in Fig. 4 indicates
</bodyText>
<equation confidence="0.999569470588235">
mood = none: 1
mood = declarative:
marker = to-inf: 1
marker = while: 0
marker = because: 0
marker = if: 0
marker = until: 1
marker = by_ing: 0
marker = in-order-to: 0
marker = after: 1
marker = unless: 1
marker = once: 1
marker = so that: 1
marker = none:
I tense = past: 1
I tense = future: 1
I tense = none: 0
I tense = present:
possibility = +:
I progressive = +: 0
I progressive = 1
possibility =
I ability = +: 1
I ability
I I obligation =
none: 0
I I obligation =
medium: 1
marker = when:
progressive = +: 0
progressive = -: 1
mood = imperative:
negation = +: 0
negation = 1
</equation>
<figureCaption confidence="0.998363">
Figure 3: Decision tree based on information gain
</figureCaption>
<table confidence="0.866835">
measure used in error
decision tree train estimate test
gain ratio 9.8% 11.1% 10.9%
information gain 9.6% 10.8% 10.9%
</table>
<figureCaption confidence="0.993833">
Figure 4: Performance of decision trees
</figureCaption>
<page confidence="0.997457">
21
</page>
<bodyText confidence="0.999928647058824">
the predicted error rate on unseen examples (the
so-called pessimistic estimate): the upper bound
of the error based on the observed error on the
training data for a given confidence level (set to
95% in the experiment). As shown in Fig. 4,
the decision tree built according to gain ratio per-
formed 0.2% worse on training data and it had
0.3% higher error estimate than the information
gain tree. The gain ratio estimate overestimates
the actual error on test (unseen) data by 0.2%,
whereas the information gain estimate underesti-
mates it by 0.1%. Hence, the results obtained by
both classifiers are very similar and difference may
be attributed to random noise. In order to elimi-
nate this effect, they should be tested on a bigger
sample, which was unavailable in the present ex-
periment.
</bodyText>
<subsectionHeader confidence="0.998178">
4.3 Automatically Learned Rules
</subsectionHeader>
<bodyText confidence="0.999889482758621">
The final part of the experiment consisted in con-
verting the decision trees into rules and verify their
performance. Initially, both trees were represented
by the same number of rules (21) but after evalu-
ation on the training data, one rule (Rule 18) has
been removed from the gain ratio tree. The rules
obtained from both decision trees are very simi-
lar but they appear in a different order and may
have different accuracy, see Fig. 5 and Fig. 6. The
rules are grouped according to their output class
(i.e., aspect value), ordered with respect to accu-
racy within this class and applied in the obtained
order. Examples to which none of the rules ap-
ply fall into the default class, computed indi-
vidually for each tree. Performance of both sets
of rules is identical: 9.4% errors on training and
10.9% errors on test data. Therefore, the learned
rules score higher than the heuristic rules which
have 11.9% errors on training and 11.7% errors on
test data.
Note that the learned rules comprise the heuris-
tic rules discussed in sec. 3. The only exception is
Rule 5, which does not take into account nega-
tion and misclassifies complements of &apos;cannot&apos;
as perfective. Some of the heuristic rules do not
have explicit counterparts among the learned rules.
Heuristic rules referring to perfective, ten-
tativity, expectation or the marker &apos;be-
fore&apos; are not overtly present in the decision trees.
</bodyText>
<table confidence="0.652275845070422">
Rule 4:
marker = to-inf
-&gt; class 1 [99.7%]
Rule 13:
marker = after
-&gt; class 1 [99.0%]
Rule 7:
obligation = medium
-&gt; class 1 [98.3%]
Rule 11:
marker = until
-&gt; class 1 [97.5%]
Rule 1:
mood = none
-&gt; class 1 [96.3%]
Rule 15:
marker = unless
-&gt; class 1 [95.0%]
Rule 16:
marker = once
-&gt; class 1 [95.0%]
Rule 21:
mood = imperative
negation = -
-&gt; class 1 [93.2%]
Rule 19:
tense - future
-&gt; class 1 [78.2%]
Rule 18:
tense - past
-&gt; class 1 [74.6%]
Rule 3:
possibility = +
progressive = -
-&gt; class 1 [73.6%]
Rule 5:
ability = +
-&gt; class 1 [70.6%]
Rule 14:
marker = when
progressive = -
-&gt; class 1 [62.0%]
Rule 2:
progressive - +
-&gt; class 0 [99.8%]
Rule 12:
marker = by_ing
-&gt; class 0 [99.4%]
Rule 8:
marker = while
-&gt; class 0 [99.0%]
Rule 20:
mood = imperative
negation = +
-&gt; class 0 [98.3%]
Rule 9:
marker = because
-&gt; class 0 [97.5%]
Rule 6:
ability = -
marker = none
mood - declarative
obligation - none
possibility = -
tense = present
-&gt; class 0 [88.196]
Rule 10:
marker = if
-&gt; class 0 [78.9%]
Default class: 1
Figure 5: Automatic rules for the gain ratio tree
</table>
<page confidence="0.982797">
22
</page>
<bodyText confidence="0.998834">
Recall, however, that all these rules resolved as-
pect to perfective. In the machine learning ap-
proach, they are covered by the default rule.
Finally, note that in the machine learning approach
several new rules have been discovered: Rules 9,
10, 14 and 15 in Fig. 5 (11, 12, 16 and 17 in Fig. 6)
do not correspond to any of the heuristic rules.
</bodyText>
<table confidence="0.887037424242424">
Rule 2:
marker = to-inf
-&gt; class 1 [99.7%]
Rule 8:
marker = none
tense = past
-&gt; class 1 [99.0%]
Rule 15:
marker = after
-&gt; class 1 [99.0%]
Rule 7:
obligation - medium
-&gt; class 1 [98.3%]
Rule 13:
marker = until
-&gt; class 1 [97.5%]
Rule 1:
mood - none
-&gt; class 1 [96.3%]
Rule 17:
marker = unless
-&gt; class 1 [95.0%]
Rule 18:
marker = once
-&gt; class 1 [95.0%]
Rule 21:
mood - imperative
negation = -
-&gt; class 1 [93.2%]
Rule 9:
tense = future
-&gt; class 1 [78.2%]
Rule 4:
</table>
<figure confidence="0.899574631578948">
possibility = +
progressive = -
-&gt; class 1 [73.6%]
Rule 5:
ability = +
-&gt; class 1 [70.6%]
Rule 16:
marker - when
progressive = -
-&gt; class 1 [62.0%]
Rule 3:
progressive = +
-&gt; class 0 [99.8%]
Rule 14:
marker - by_ing
-&gt; class 0 [99.4%]
Rule 10:
marker = while
-&gt; class 0 [99.0%]
Rule 20:
mood = imperative
negation = +
-&gt; class 0 [98.3%]
Rule 11:
marker = because
-&gt; class 0 [97.5%]
Rule 6:
ability = -
marker = none
mood = declarative
obligation = none
possibility = -
tense = present
-&gt; class 0 [88.1%]
Rule 12:
marker - if
-&gt; class 0 [78.9%]
Default class: 1
</figure>
<figureCaption confidence="0.979823">
Figure 6: Automatic rules for the information gain
tree
</figureCaption>
<sectionHeader confidence="0.998444" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999892472222222">
In this paper, we presented two approaches to as-
pect assignment in a knowledge-based English-
Polish MT system: heuristic rules and machine
learning. As for approaches which do not rely on
semantics or pragmatics, accuracy of both meth-
ods is very high: heuristic rules achieve 88.3%
and automatically learned rules 89.1% accuracy
on test data. Although the final results turn out
to be very similar, the crucial difference between
the two methods is the development effort: the ma-
chine learning technique acquires rules automati-
cally, while heuristic rules are hand-coded. An-
other advantage of the machine learning approach
is that it allows for more concise encoding of the
heuristic rules and discovering new rules.
It has to be noted that the success of the machine
learning approach strongly relies on the choice of
attributes used for learning. The heuristic rules
and the decision trees employ the same attributes.
Therefore, human knowledge is necessary to limit
the search space in the automatic approach. An-
other factor which contributed to the high system
performance is the restricted domain of transla-
tion and use of controlled language. Although
some heuristics are quite general (e.g., the rules
compatible with those independently proposed in
Gawrofiska (1993)), the system probably will not
be fully scalable to an open-domain unrestricted
natural language text. Providing reliable heuris-
tics in a general purpose MT system will be much
more difficult than for a domain-specific MT sys-
tem. On the other hand, having set the learning
attributes (or corresponding surface / syntactic pat-
terns), machine learning methods can be success-
fully applied to automatically acquire rules from
annotated data.
</bodyText>
<page confidence="0.9972">
23
</page>
<sectionHeader confidence="0.999114" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99993025">
I wish to thank Krzysztof Czuba, Kathryn L.
Baker and two anonymous reviewers of the EACL
EAMT Workshop for their comments and sugges-
tions to improve this paper.
</bodyText>
<sectionHeader confidence="0.99924" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997743414634147">
Joan Bresnan, editor. 1982. The Mental Representa-
tion of Grammatical Relations. MIT Press Series on
Cognitive Theory and Mental Representation. The
MIT Press, Cambridge, MA.
Bernard Comrie. 1976. Aspect. An introduction to the
study of verbal aspect and related problems. Cam-
bridge University Press, Cambridge.
David Dowty. 1986. The effects of aspectual class
on the temporal structure of discourse: Semantics or
pragmatics? Linguistics and Philosophy, 9:37-61.
Barbara Gawrofiska. 1993. An MT Oriented Model
of Aspect and Article Semantics. Lund University
Press.
Christine Kamprath, Eric Adolphson, Teruko Mita-
mura, and Eric Nyberg. 1998. Controlled language
for multilingual document production: Experience
with Caterpillar technical English. In Proceedings
of the Second International Workshop on Controlled
Language Applications (CLAW &apos;98). Available from
http://www.lti.cs.cmu.edu/Research/Kant/claw98ck.pdf.
John R. Leavitt, Deryle W. Lonsdale, and Alexan-
der M. Franz. 1994. A reasoned inter-
lingua for knowledge-based machine translation.
In Proceedings of CSCSI-94. Available from:
http://www.lti.cs.cmu.edu/Research/Kant/.
Teruko Mitamura and Eric Nyberg. 1992. The KANT
system: Fast, accurate, high-quality translation in
practical domains. In Proceedings of COLING-92.
Teruko Mitamura, Eric Nyberg, and Jaimie Carbonell.
1991. An efficient interlingua translation system for
multi-lingual document production. In Proceedings
of the Third Machine Translation Summit.
J. Ross Quinlan. 1986. Induction of decision trees.
Machine Learning, 1(1):81-106.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann, San Mateo, CA.
Zygmunt Saloni and Marek Swidziriski. 1985. Sktad-
nia VVspolczesnego Jczyka Polskiego. Pafistwowe
Wydawnictwo Naukowe, Warszawa, 2nd edition.
Zeno Vendler. 1967. Linguistics in Philosophy. Cor-
nell University Press, Ithaca, NY.
</reference>
<page confidence="0.999214">
24
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.302550">
<title confidence="0.996108">Two Approaches to Aspect Assignment in an English-Polish Machine Translation System</title>
<author confidence="0.913013">Anna</author>
<affiliation confidence="0.7284315">Polish Academy of Sciences, Institute of Computer Science Carnegie Mellon University, Language Technologies</affiliation>
<email confidence="0.998523">aniak@cs.cmu.edu</email>
<abstract confidence="0.977812">The paper presents two approaches to aspect assignment in a knowledgebased English-Polish machine translation (MT) system. The first method uses a set of heuristic rules based on interlingua (IR) representation provided by the system, whereas the other employs machine learning techniques. Both methods have similar performance and obtain high accuracy of over 88% on test data. The crucial difference, however, is the development effort: the machine learning technique is fully automatic, whereas heuristic rules are derived manually.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>The Mental Representation of Grammatical Relations.</title>
<date>1982</date>
<booktitle>Series on Cognitive Theory and Mental Representation. The</booktitle>
<editor>Joan Bresnan, editor.</editor>
<publisher>MIT Press</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4153" citStr="(1982)" startWordPosition="615" endWordPosition="615"> (mood declarative) (punctuation period) (tense present) (theme (*O-PAGE (number plural) (reference no-reference)))) Drukarka drukuje strony. IR illustrated in (1) is the input for the Polish generation module. The module consists of four components: a mapper, a unification grammar (a type of context-free grammar), a morphological generator and a post-processing module. Mapping rules transform the IR semantic representation into a syntactic structure corresponding to the Polish output. The structure is a functional structure or FS in the LFG (Lexical Functional Grammar) formalism, cf. Bresnan (1982). Generation grammar rules convert this FS into a list of lexical tokens (FS frames), which are then fed to the morphology module responsible for generating appropriate inflected forms. Finally, a set of post-processing rules is applied to produce the resulting surface form of translation by cleaning up spacing, adding capitalization, inserting punctuation, etc. In order to develop the current system, a small corpus of about 280 English sentences from a printer manual has been examined. This corpus served as a baseline to develop the two approaches to aspect assignment presented in the paper. </context>
</contexts>
<marker>1982</marker>
<rawString>Joan Bresnan, editor. 1982. The Mental Representation of Grammatical Relations. MIT Press Series on Cognitive Theory and Mental Representation. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernard Comrie</author>
</authors>
<title>Aspect. An introduction to the study of verbal aspect and related problems.</title>
<date>1976</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="1051" citStr="Comrie (1976)" startWordPosition="154" endWordPosition="155"> representation provided by the system, whereas the other employs machine learning techniques. Both methods have similar performance and obtain high accuracy of over 88% on test data. The crucial difference, however, is the development effort: the machine learning technique is fully automatic, whereas heuristic rules are derived manually. 1 Introduction The paper presents two methods to deal with aspect assignment in a prototype of a knowledgebased English-Polish machine translation (MT) system. Although there is no agreement among linguists as to its precise definition, e.g., Vendler (1967), Comrie (1976), Dowty (1986), aspect is a result of complex interplay of semantics, tense, mood and pragmatics and it strongly affects overall text understanding. In English, aspect is usually not explicitly indicated on a verb. On the other hand, in Polish it is overtly manifested and incorporated into verb morphology. This difference between the two languages makes English-Polish translation particularly difficult as it requires contextual and semantic analysis of the English input in order to derive aspect value for the Polish output. The MT system presented in this paper takes advantage of a knowledge-b</context>
</contexts>
<marker>Comrie, 1976</marker>
<rawString>Bernard Comrie. 1976. Aspect. An introduction to the study of verbal aspect and related problems. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
</authors>
<title>The effects of aspectual class on the temporal structure of discourse: Semantics or pragmatics? Linguistics and Philosophy,</title>
<date>1986</date>
<pages>9--37</pages>
<contexts>
<context position="1065" citStr="Dowty (1986)" startWordPosition="156" endWordPosition="157"> provided by the system, whereas the other employs machine learning techniques. Both methods have similar performance and obtain high accuracy of over 88% on test data. The crucial difference, however, is the development effort: the machine learning technique is fully automatic, whereas heuristic rules are derived manually. 1 Introduction The paper presents two methods to deal with aspect assignment in a prototype of a knowledgebased English-Polish machine translation (MT) system. Although there is no agreement among linguists as to its precise definition, e.g., Vendler (1967), Comrie (1976), Dowty (1986), aspect is a result of complex interplay of semantics, tense, mood and pragmatics and it strongly affects overall text understanding. In English, aspect is usually not explicitly indicated on a verb. On the other hand, in Polish it is overtly manifested and incorporated into verb morphology. This difference between the two languages makes English-Polish translation particularly difficult as it requires contextual and semantic analysis of the English input in order to derive aspect value for the Polish output. The MT system presented in this paper takes advantage of a knowledge-based interling</context>
</contexts>
<marker>Dowty, 1986</marker>
<rawString>David Dowty. 1986. The effects of aspectual class on the temporal structure of discourse: Semantics or pragmatics? Linguistics and Philosophy, 9:37-61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Gawrofiska</author>
</authors>
<title>An MT Oriented Model of Aspect and Article Semantics.</title>
<date>1993</date>
<publisher>Lund University Press.</publisher>
<contexts>
<context position="6462" citStr="Gawrofiska (1993)" startWordPosition="977" endWordPosition="978">neral cases are considered first and if they do not hold, more specific rules are applied. Aspect assignment rules proposed in the system are presented below. 3.1 Declarative Mood For finite verbs in declarative mood, aspect primarily depends on tense. First, all continuous forms, marked as (progressive +) in IR, are translated as imperfective. Next, forms of perfective tenses, i.e., (perfective +) , are translated as perfective. Then, verbs in simple 18 past, (tense past ) , or future simple tenses, (tense future) , are translated as perfective. Similar assignment rules have been proposed in Gawrofiska (1993). Additionally, we assume that certain types of subordinate conjunctions, e.g., &apos;while&apos;, &apos;once&apos;, &apos;before&apos;, etc., impose aspect requirements on a verb in the subordinate clause. The following assignments have been proposed: • &apos;while&apos;: imperfective (3) You can send an electronic fax while the printer makes copies. Mona wyslae elektroniczny faks, can senclinf electronic fax podczas gdy drukarka robi while printer makesimp„f kopie. copies • &apos;once&apos;, &apos;after&apos;, &apos;before&apos;, &apos;until&apos;: perfective; additionally clauses introduced by the conjunction &apos;until&apos; have to be negated in Polish (4) Jobs also queue and</context>
<context position="21255" citStr="Gawrofiska (1993)" startWordPosition="3563" endWordPosition="3564"> encoding of the heuristic rules and discovering new rules. It has to be noted that the success of the machine learning approach strongly relies on the choice of attributes used for learning. The heuristic rules and the decision trees employ the same attributes. Therefore, human knowledge is necessary to limit the search space in the automatic approach. Another factor which contributed to the high system performance is the restricted domain of translation and use of controlled language. Although some heuristics are quite general (e.g., the rules compatible with those independently proposed in Gawrofiska (1993)), the system probably will not be fully scalable to an open-domain unrestricted natural language text. Providing reliable heuristics in a general purpose MT system will be much more difficult than for a domain-specific MT system. On the other hand, having set the learning attributes (or corresponding surface / syntactic patterns), machine learning methods can be successfully applied to automatically acquire rules from annotated data. 23 Acknowledgments I wish to thank Krzysztof Czuba, Kathryn L. Baker and two anonymous reviewers of the EACL EAMT Workshop for their comments and suggestions to </context>
</contexts>
<marker>Gawrofiska, 1993</marker>
<rawString>Barbara Gawrofiska. 1993. An MT Oriented Model of Aspect and Article Semantics. Lund University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Kamprath</author>
<author>Eric Adolphson</author>
<author>Teruko Mitamura</author>
<author>Eric Nyberg</author>
</authors>
<title>Controlled language for multilingual document production: Experience with Caterpillar technical English.</title>
<date>1998</date>
<booktitle>In Proceedings of the Second International Workshop on Controlled Language Applications (CLAW &apos;98). Available from http://www.lti.cs.cmu.edu/Research/Kant/claw98ck.pdf.</booktitle>
<contexts>
<context position="3290" citStr="Kamprath et al. (1998)" startWordPosition="492" endWordPosition="495">an extension of the existing multilingual KANTO() system (a reimplementation of the KANT system, cf. Mitamura et al. (1991), Mitamura and Nyberg (1992)) developed at Carnegie Mellon University. KANTO() is a knowledge-based, high-quality, domain-specific MT system (in the English-Polish MT project, 17 the domain is restricted to printer manuals) and it uses Interlingua (IR) as a semantic representation, see Leavitt et al. (1994). The system takes as an input a text written in constrained English (controlled language), which limits vocabulary and grammar of sentences accepted by the system, cf. Kamprath et al. (1998). Example (1) presents a sample English input along with the IR representation and its Polish translation provided by the system. (0 The printer prints pages. (*A-PRINT (agent (*O-PRINTER (number singular) (reference definite))) (argument-class agent+theme) (mood declarative) (punctuation period) (tense present) (theme (*O-PAGE (number plural) (reference no-reference)))) Drukarka drukuje strony. IR illustrated in (1) is the input for the Polish generation module. The module consists of four components: a mapper, a unification grammar (a type of context-free grammar), a morphological generator </context>
</contexts>
<marker>Kamprath, Adolphson, Mitamura, Nyberg, 1998</marker>
<rawString>Christine Kamprath, Eric Adolphson, Teruko Mitamura, and Eric Nyberg. 1998. Controlled language for multilingual document production: Experience with Caterpillar technical English. In Proceedings of the Second International Workshop on Controlled Language Applications (CLAW &apos;98). Available from http://www.lti.cs.cmu.edu/Research/Kant/claw98ck.pdf.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Leavitt</author>
<author>Deryle W Lonsdale</author>
<author>Alexander M Franz</author>
</authors>
<title>A reasoned interlingua for knowledge-based machine translation.</title>
<date>1994</date>
<contexts>
<context position="3099" citStr="Leavitt et al. (1994)" startWordPosition="462" endWordPosition="465">cribe heuristic rules and the machine learning approach, respectively. Finally, section 5 contains conclusions. 2 System description The English-Polish MT project presented in this paper is an extension of the existing multilingual KANTO() system (a reimplementation of the KANT system, cf. Mitamura et al. (1991), Mitamura and Nyberg (1992)) developed at Carnegie Mellon University. KANTO() is a knowledge-based, high-quality, domain-specific MT system (in the English-Polish MT project, 17 the domain is restricted to printer manuals) and it uses Interlingua (IR) as a semantic representation, see Leavitt et al. (1994). The system takes as an input a text written in constrained English (controlled language), which limits vocabulary and grammar of sentences accepted by the system, cf. Kamprath et al. (1998). Example (1) presents a sample English input along with the IR representation and its Polish translation provided by the system. (0 The printer prints pages. (*A-PRINT (agent (*O-PRINTER (number singular) (reference definite))) (argument-class agent+theme) (mood declarative) (punctuation period) (tense present) (theme (*O-PAGE (number plural) (reference no-reference)))) Drukarka drukuje strony. IR illustr</context>
<context position="9431" citStr="Leavitt et al. (1994)" startWordPosition="1442" endWordPosition="1445">njunction such as &apos;in order to&apos;. We assume that in the former case, aspect of the infinitive depends on the governing verb while in the latter — on the subordinate conjunction. For the conjunction &apos;in order to&apos;, we assume that it requires a perfective infinitive argument, (8). (8) You must unhook the other device in order to connect the printer. Trzeba wy14czye inne urz4dzenie, need unhookr„f another device aby podkczye drukar4. in order to connectp„f printer Modal verbs are represented in IR by a set of semantic attributes such as ability, possibility, tentativity, necessity, obligation, see Leavitt et al. (1994). The following aspect assignment has been adopted in the system: 19 • &apos;can&apos;, (ability +) or (possibility +): perfective; • &apos;cannot&apos;, (ability +) (negation +): imperfective; • &apos;cannot&apos;, (possibility +) (negation +): perfective; • &apos;could&apos;, (possibility +) (tentativity low): perfective; • &apos;may&apos;, (possibility +) (tentativity medium): perfective; • &apos;must&apos;, (obligation medium): perfective; • &apos;should&apos;, (expectation +): perfective. 3.4 Results As mentioned above, aspect strongly depends on semantic and pragmatic context. Since such information is impoverished in KANTOO, the proposed rules cannot be p</context>
</contexts>
<marker>Leavitt, Lonsdale, Franz, 1994</marker>
<rawString>John R. Leavitt, Deryle W. Lonsdale, and Alexander M. Franz. 1994. A reasoned interlingua for knowledge-based machine translation.</rawString>
</citation>
<citation valid="false">
<booktitle>In Proceedings of CSCSI-94. Available from: http://www.lti.cs.cmu.edu/Research/Kant/.</booktitle>
<marker></marker>
<rawString>In Proceedings of CSCSI-94. Available from: http://www.lti.cs.cmu.edu/Research/Kant/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruko Mitamura</author>
<author>Eric Nyberg</author>
</authors>
<title>The KANT system: Fast, accurate, high-quality translation in practical domains.</title>
<date>1992</date>
<booktitle>In Proceedings of COLING-92.</booktitle>
<contexts>
<context position="2819" citStr="Mitamura and Nyberg (1992)" startWordPosition="422" endWordPosition="425">erence, however, is the effort put into their development: the machine learning approach is fully automatic and rules are derived from examples rather than hand-coded. The organization of the paper is as follows: section 2 briefly presents the system architecture, sections 3 and 4 describe heuristic rules and the machine learning approach, respectively. Finally, section 5 contains conclusions. 2 System description The English-Polish MT project presented in this paper is an extension of the existing multilingual KANTO() system (a reimplementation of the KANT system, cf. Mitamura et al. (1991), Mitamura and Nyberg (1992)) developed at Carnegie Mellon University. KANTO() is a knowledge-based, high-quality, domain-specific MT system (in the English-Polish MT project, 17 the domain is restricted to printer manuals) and it uses Interlingua (IR) as a semantic representation, see Leavitt et al. (1994). The system takes as an input a text written in constrained English (controlled language), which limits vocabulary and grammar of sentences accepted by the system, cf. Kamprath et al. (1998). Example (1) presents a sample English input along with the IR representation and its Polish translation provided by the system.</context>
</contexts>
<marker>Mitamura, Nyberg, 1992</marker>
<rawString>Teruko Mitamura and Eric Nyberg. 1992. The KANT system: Fast, accurate, high-quality translation in practical domains. In Proceedings of COLING-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruko Mitamura</author>
<author>Eric Nyberg</author>
<author>Jaimie Carbonell</author>
</authors>
<title>An efficient interlingua translation system for multi-lingual document production.</title>
<date>1991</date>
<booktitle>In Proceedings of the Third Machine Translation Summit.</booktitle>
<contexts>
<context position="2791" citStr="Mitamura et al. (1991)" startWordPosition="418" endWordPosition="421">imilar. The crucial difference, however, is the effort put into their development: the machine learning approach is fully automatic and rules are derived from examples rather than hand-coded. The organization of the paper is as follows: section 2 briefly presents the system architecture, sections 3 and 4 describe heuristic rules and the machine learning approach, respectively. Finally, section 5 contains conclusions. 2 System description The English-Polish MT project presented in this paper is an extension of the existing multilingual KANTO() system (a reimplementation of the KANT system, cf. Mitamura et al. (1991), Mitamura and Nyberg (1992)) developed at Carnegie Mellon University. KANTO() is a knowledge-based, high-quality, domain-specific MT system (in the English-Polish MT project, 17 the domain is restricted to printer manuals) and it uses Interlingua (IR) as a semantic representation, see Leavitt et al. (1994). The system takes as an input a text written in constrained English (controlled language), which limits vocabulary and grammar of sentences accepted by the system, cf. Kamprath et al. (1998). Example (1) presents a sample English input along with the IR representation and its Polish transla</context>
</contexts>
<marker>Mitamura, Nyberg, Carbonell, 1991</marker>
<rawString>Teruko Mitamura, Eric Nyberg, and Jaimie Carbonell. 1991. An efficient interlingua translation system for multi-lingual document production. In Proceedings of the Third Machine Translation Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>Induction of decision trees.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<pages>1--1</pages>
<contexts>
<context position="13178" citStr="Quinlan (1986)" startWordPosition="2069" endWordPosition="2070">belled with the correct aspect value based on the human translation. The resulting 13-tuples served as training data for the decision tree. The target concept (aspect) has been represented by a binary attribute: 0 corresponds to imperfective, 1 to perfective aspect. The test data has the same format. Due to changes in the mapper, the final number of examples used in the experiment was smaller than in the original system. The decision tree was 20 trained on 417 and tested on 55 examples. 4.2 Decision Trees As mentioned above, we employed two measures to build a decision tree: information gain, Quinlan (1986), and gain ratio, Quinlan (1986; Quinlan (1993). The main difference between the two techniques is in the size of the resulting tree: the former favours attributes with multiple values, which results in a wider (and usually bigger) tree. Indeed, the tree built according to gain ratio is smaller (31 nodes), Fig. 2, whereas the one based on information gain is slightly bigger (33 nodes), Fig. 3. mood = none: 1 mood = declarative: tense = past: 1 tense = future: 1 tense = none: 0 tense = present: progressive = +: 0 progressive = possibility = +: 1 possibility = marker = to-inf: 1 marker = while: </context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>J. Ross Quinlan. 1986. Induction of decision trees. Machine Learning, 1(1):81-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA.</location>
<contexts>
<context position="13225" citStr="Quinlan (1993)" startWordPosition="2076" endWordPosition="2077">he human translation. The resulting 13-tuples served as training data for the decision tree. The target concept (aspect) has been represented by a binary attribute: 0 corresponds to imperfective, 1 to perfective aspect. The test data has the same format. Due to changes in the mapper, the final number of examples used in the experiment was smaller than in the original system. The decision tree was 20 trained on 417 and tested on 55 examples. 4.2 Decision Trees As mentioned above, we employed two measures to build a decision tree: information gain, Quinlan (1986), and gain ratio, Quinlan (1986; Quinlan (1993). The main difference between the two techniques is in the size of the resulting tree: the former favours attributes with multiple values, which results in a wider (and usually bigger) tree. Indeed, the tree built according to gain ratio is smaller (31 nodes), Fig. 2, whereas the one based on information gain is slightly bigger (33 nodes), Fig. 3. mood = none: 1 mood = declarative: tense = past: 1 tense = future: 1 tense = none: 0 tense = present: progressive = +: 0 progressive = possibility = +: 1 possibility = marker = to-inf: 1 marker = while: 0 marker = because: 0 marker = if: 0 marker = u</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Ross Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann, San Mateo, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zygmunt Saloni</author>
<author>Marek Swidziriski</author>
</authors>
<title>Sktadnia VVspolczesnego Jczyka Polskiego. Pafistwowe Wydawnictwo Naukowe,</title>
<date>1985</date>
<location>Warszawa,</location>
<marker>Saloni, Swidziriski, 1985</marker>
<rawString>Zygmunt Saloni and Marek Swidziriski. 1985. Sktadnia VVspolczesnego Jczyka Polskiego. Pafistwowe Wydawnictwo Naukowe, Warszawa, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeno Vendler</author>
</authors>
<title>Linguistics in Philosophy.</title>
<date>1967</date>
<publisher>Cornell University Press,</publisher>
<location>Ithaca, NY.</location>
<contexts>
<context position="1036" citStr="Vendler (1967)" startWordPosition="152" endWordPosition="153">interlingua (IR) representation provided by the system, whereas the other employs machine learning techniques. Both methods have similar performance and obtain high accuracy of over 88% on test data. The crucial difference, however, is the development effort: the machine learning technique is fully automatic, whereas heuristic rules are derived manually. 1 Introduction The paper presents two methods to deal with aspect assignment in a prototype of a knowledgebased English-Polish machine translation (MT) system. Although there is no agreement among linguists as to its precise definition, e.g., Vendler (1967), Comrie (1976), Dowty (1986), aspect is a result of complex interplay of semantics, tense, mood and pragmatics and it strongly affects overall text understanding. In English, aspect is usually not explicitly indicated on a verb. On the other hand, in Polish it is overtly manifested and incorporated into verb morphology. This difference between the two languages makes English-Polish translation particularly difficult as it requires contextual and semantic analysis of the English input in order to derive aspect value for the Polish output. The MT system presented in this paper takes advantage o</context>
</contexts>
<marker>Vendler, 1967</marker>
<rawString>Zeno Vendler. 1967. Linguistics in Philosophy. Cornell University Press, Ithaca, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>