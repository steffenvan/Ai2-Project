<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000061">
<title confidence="0.9969115">
Cross language Text Categorization by acquiring
Multilingual Domain Models from Comparable Corpora
</title>
<note confidence="0.646235666666667">
Alfio Gliozzo and Carlo Strapparava
ITC-Irst
via Sommarive, I-38050, Trento, ITALY
</note>
<email confidence="0.994804">
{gliozzo,strappa}@itc.it
</email>
<sectionHeader confidence="0.996585" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999841961538462">
In a multilingual scenario, the classical
monolingual text categorization problem
can be reformulated as a cross language
TC task, in which we have to cope with
two or more languages (e.g. English and
Italian). In this setting, the system is
trained using labeled examples in a source
language (e.g. English), and it classifies
documents in a different target language
(e.g. Italian).
In this paper we propose a novel ap-
proach to solve the cross language text
categorization problem based on acquir-
ing Multilingual Domain Models from
comparable corpora in a totally unsuper-
vised way and without using any external
knowledge source (e.g. bilingual dictio-
naries). These Multilingual Domain Mod-
els are exploited to define a generalized
similarity function (i.e. a kernel function)
among documents in different languages,
which is used inside a Support Vector Ma-
chines classification framework. The re-
sults show that our approach is a feasi-
ble and cheap solution that largely outper-
forms a baseline.
</bodyText>
<sectionHeader confidence="0.999157" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998868666666667">
Text categorization (TC) is the task of assigning cat-
egory labels to documents. Categories are usually
defined according to a variety of topics (e.g. SPORT,
</bodyText>
<page confidence="0.954895">
9
</page>
<bodyText confidence="0.99972455882353">
POLITICS, etc.) and, even if a large amount of
hand tagged texts is required, the state-of-the-art su-
pervised learning techniques represent a viable and
well-performing solution for monolingual catego-
rization problems.
On the other hand in the worldwide scenario of
the web age, multilinguality is a crucial issue to deal
with and to investigate, leading us to reformulate
most of the classical NLP problems. In particular,
monolingual Text Categorization can be reformu-
lated as a cross language TC task, in which we have
to cope with two or more languages (e.g. English
and Italian). In this setting, the system is trained
using labeled examples in a source language (e.g.
English), and it classifies documents in a different
target language (e.g. Italian).
In this paper we propose a novel approach to solve
the cross language text categorization problem based
on acquiring Multilingual Domain Models (MDM)
from comparable corpora in an unsupervised way.
A MDM is a set of clusters formed by terms in dif-
ferent languages. While in the monolingual settings
semantic domains are clusters of related terms that
co-occur in texts regarding similar topics (Gliozzo et
al., 2004), in the multilingual settings such clusters
are composed by terms in different languages ex-
pressing concepts in the same semantic field. Thus,
the basic relation modeled by a MDM is the domain
similarity among terms in different languages. Our
claim is that such a relation is sufficient to capture
relevant aspects of topic similarity that can be prof-
itably used for TC purposes.
The paper is organized as follows. After a brief
discussion about comparable corpora, we introduce
</bodyText>
<note confidence="0.9905005">
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 9–16,
Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<bodyText confidence="0.9976012">
a multilingual Vector Space Model, in which docu-
ments in different languages can be represented and
then compared. In Section 4 we define the MDMs
and we present a totally unsupervised technique
to acquire them from comparable corpora. This
methodology does not require any external knowl-
edge source (e.g. bilingual dictionaries) and it is
based on Latent Semantic Analysis (LSA) (Deer-
wester et al., 1990). MDMs are then exploited to
define a Multilingual Domain Kernel, a generalized
similarity function among documents in different
languages that exploits a MDM (see Section 5). The
Multilingual Domain Kernel is used inside a Sup-
port Vector Machines (SVM) classification frame-
work for TC (Joachims, 2002). In Section 6 we will
evaluate our technique in a Cross Language catego-
rization task. The results show that our approach is
a feasible and cheap solution, largely outperforming
a baseline. Conclusions and future works are finally
reported in Section 7.
</bodyText>
<sectionHeader confidence="0.953906" genericHeader="method">
2 Comparable Corpora
</sectionHeader>
<bodyText confidence="0.999942948275862">
Comparable corpora are collections of texts in dif-
ferent languages regarding similar topics (e.g. a col-
lection of news published by agencies in the same
period). More restrictive requirements are expected
for parallel corpora (i.e. corpora composed by texts
which are mutual translations), while the class of
the multilingual corpora (i.e. collection of texts ex-
pressed in different languages without any addi-
tional requirement) is the more general. Obviously
parallel corpora are also comparable, while compa-
rable corpora are also multilingual.
In a more precise way, let L = {L1, L2, ... , Ll}
be a set of languages, let Ti = {ti1, ti2, ... , tin} be a
collection of texts expressed in the language Li E L,
and let ψ(tjh, tiz) be a function that returns 1 if tiz is
the translation of tjh and 0 otherwise. A multilingual
corpus is the collection of texts defined by T * =
Ui Ti. If the function ψ exists for every text tiz E T*
and for every language Lj, and is known, then the
corpus is parallel and aligned at document level.
For the purpose of this paper it is enough to as-
sume that two corpora are comparable, i.e. they are
composed by documents about the same topics and
produced in the same period (e.g. possibly from dif-
ferent news agencies), and it is not known if a func-
tion ψ exists, even if in principle it could exist and
return 1 for a strict subset of document pairs.
There exist many interesting works about us-
ing parallel corpora for multilingual applications
(Melamed, 2001), such as Machine Translation,
Cross language Information Retrieval (Littman et
al., 1998), lexical acquisition, and so on.
However it is not always easy to find or build par-
allel corpora. This is the main reason because the
weaker notion of comparable corpora is a matter re-
cent interest in the field of Computational Linguis-
tics (Gaussier et al., 2004).
The texts inside comparable corpora, being about
the same topics (i.e. about the same semantic do-
mains), should refer to the same concepts by using
various expressions in different languages. On the
other hand, most of the proper nouns, relevant enti-
ties and words that are not yet lexicalized in the lan-
guage, are expressed by using their original terms.
As a consequence the same entities will be denoted
with the same words in different languages, allow-
ing to automatically detect couples of translation
pairs just by looking at the word shape (Koehn and
Knight, 2002). Our hypothesis is that comparable
corpora contain a large amount of such words, just
because texts, referring to the same topics in differ-
ent languages, will often adopt the same terms to
denote the same entitiesl.
However, the simple presence of these shared
words is not enough to get significant results in TC
tasks. As we will see, we need to exploit these com-
mon words to induce a second-order similarity for
the other words in the lexicons.
</bodyText>
<sectionHeader confidence="0.96238" genericHeader="method">
3 The Multilingual Vector Space Model
</sectionHeader>
<bodyText confidence="0.983503">
Let T = {t1, t2, ... , tn} be a corpus, and V =
{w1, w2, ... , wk} be its vocabulary. In the mono-
lingual settings, the Vector Space Model (VSM) is a
k-dimensional space Rk, in which the text tj E T
is represented by means of the vector ~tj such that
the zth component of ~tj is the frequency of wz in tj.
The similarity among two texts in the VSM is then
estimated by computing the cosine of their vectors
in the VSM.
&apos;According to our assumption, a possible additional crite-
rion to decide whether two corpora are comparable is to esti-
mate the percentage of terms in the intersection of their vocab-
ularies.
</bodyText>
<page confidence="0.997056">
10
</page>
<bodyText confidence="0.999982263157895">
Unfortunately, such a model cannot be adopted in
the multilingual settings, because the VSMs of dif-
ferent languages are mainly disjoint, and the similar-
ity between two texts in different languages would
always turn out zero. This situation is represented
in Figure 1, in which both the left-bottom and the
rigth-upper regions of the matrix are totally filled by
zeros.
A first attempt to solve this problem is to ex-
ploit the information provided by external knowl-
edge sources, such as bilingual dictionaries, to col-
lapse all the rows representing translation pairs. In
this setting, the similarity among texts in different
languages could be estimated by exploiting the clas-
sical VSM just described. However, the main dis-
advantage of this approach to estimate inter-lingual
text similarity is that it strongly relies on the avail-
ability of a multilingual lexical resource containing
a list of translation pairs. For languages with scarce
resources a bilingual dictionary could be not eas-
ily available. Secondly, an important requirement
of such a resource is its coverage (i.e. the amount
of possible translation pairs that are actually con-
tained in it). Finally, another problem is that am-
biguos terms could be translated in different ways,
leading to collapse together rows describing terms
with very different meanings.
On the other hand, the assumption of corpora
comparability seen in Section 2, implies the pres-
ence of a number of common words, represented by
the central rows of the matrix in Figure 1.
As we will show in Section 6, this model is rather
poor because of its sparseness. In the next section,
we will show how to use such words as seeds to in-
duce a Multilingual Domain VSM, in which second
order relations among terms and documents in dif-
ferent languages are considered to improve the sim-
ilarity estimation.
</bodyText>
<sectionHeader confidence="0.994682" genericHeader="method">
4 Multilingual Domain Models
</sectionHeader>
<bodyText confidence="0.999890294117647">
A MDM is a multilingual extension of the concept
of Domain Model. In the literature, Domain Mod-
els have been introduced to represent ambiguity and
variability (Gliozzo et al., 2004) and successfully
exploited in many NLP applications, such us Word
Sense Disambiguation (Strapparava et al., 2004),
Text Categorization and Term Categorization.
A Domain Model is composed by soft clusters of
terms. Each cluster represents a semantic domain,
i.e. a set of terms that often co-occur in texts hav-
ing similar topics. Such clusters identifies groups of
words belonging to the same semantic field, and thus
highly paradigmatically related. MDMs are Domain
Models containing terms in more than one language.
A MDM is represented by a matrix D, contain-
ing the degree of association among terms in all the
languages and domains, as illustrated in Table 1.
</bodyText>
<table confidence="0.776142625">
MEDICINE COMPUTER SCIENCE
HIVe/i 1 0
AIDSe/i 1 0
viruse/i 0.5 0.5
hospitale 1 0
laptope 0 1
Microsofte/i 0 1
clinicai 1 0
</table>
<tableCaption confidence="0.983979">
Table 1: Example of Domain Matrix. we denotes
</tableCaption>
<bodyText confidence="0.999466925925926">
English terms, wi Italian terms and we/i the com-
mon terms to both languages.
MDMs can be used to describe lexical ambiguity,
variability and inter-lingual domain relations. Lexi-
cal ambiguity is represented by associating one term
to more than one domain, while variability is rep-
resented by associating different terms to the same
domain. For example the term virus is associated
to both the domain COMPUTER SCIENCE and the
domain MEDICINE while the domain MEDICINE is
associated to both the terms AIDS and HIV. Inter-
lingual domain relations are captured by placing dif-
ferent terms of different languages in the same se-
mantic field (as for example HIV e/i, AIDSe/i,
hospitale, and clinicai). Most of the named enti-
ties, such as Microsoft and HIV are expressed using
the same string in both languages.
When similarity among texts in different lan-
guages has to be estimated, the information con-
tained in the MDM is crucial. For example the two
sentences “I went to the hospital to make an HIV
check” and “Ieri ho fatto il test dell’AIDS in clin-
ica” (lit. yesterday I did the AIDS test in a clinic)
are very highly related, even if they share no to-
kens. Having an “a priori” knowledge about the
inter-lingual domain similarity among AIDS, HIV,
hospital and clinica is then a useful information to
</bodyText>
<page confidence="0.996546">
11
</page>
<table confidence="0.998117208333333">
� English documents Italian documents
� de1 de2 ··· den−1 den di1 di2 ··· di,−1 di, �
�
� we1 0 1 ··· 0 1 0 0 ··· �
� � � we2 1 1 ··· 1 0 0 ... � � �
English
� � Lexicon � �
� � � ... ... 0 � � �
...
� � � we 0 1 ··· 0 0 ... � � �
p−1 0
� � wep 0 1 ··· 0 0 ··· 0 0 � �
� � common wi e/i 0 1 ··· 0 0 0 0 ··· 1 0 � �
w 1
� � � ... � � �
� � wi1 0 0 ··· 0 1 ··· 1 1 � �
� � wi2 0 ... 1 1 ··· 0 � �
Italian 1
� � Lexicon � �
� � � ... ... 0 ... � � �
� � wiq−1 ... 0 0 1 ··· 0 � �
� 1
0 �
wiq ··· 0 0 0 1 ··· 1
</table>
<figureCaption confidence="0.997374">
Figure 1: Multilingual term-by-document matrix
</figureCaption>
<bodyText confidence="0.986701652173913">
recognize inter-lingual topic similarity. Obviously
this relation is less restrictive than a stronger associ-
ation among translation pair. In this paper we will
show that such a representation is sufficient for TC
puposes, and easier to acquire.
In the rest of this section we will provide a formal
definition of the concept of MDM, and we define
some similarity metrics that exploit it.
Formally, let V � = {wi, w2, ... , wki} be the vo-
cabulary of the corpus T� composed by document
expressed in the language L�, let V ∗ = Uj V&apos; be
the set of all the terms in all the languages, and
let k∗ = |V ∗ |be the cardinality of this set. Let
D = {D1, D2,..., Dd} be a set of domains. A DM
is fully defined by a k∗ × d domain matrix D rep-
resenting in each cell di�z the domain relevance of
the ith term of V ∗ with respect to the domain Dz.
The domain matrix D is used to define a function
D : Rk*  Rd, that maps the document vectors ~tj
expressed into the multilingual classical VSM, into
~
the vectors t0j in the multilingual domain VSM. The
function D is defined bye
</bodyText>
<footnote confidence="0.356085">
2In (Wong et al., 1985) the formula 1 is used to define a
Generalized Vector Space Model, of which the Domain VSM is
a particular instance.
</footnote>
<equation confidence="0.9056105">
D(~tj) = ~tj(IIDFD) = ~t0 (1)
j
where IIDF is a diagonal matrix such that i�DF
��� =
</equation>
<bodyText confidence="0.998614">
IDF(wl�), ~tj is represented as a row vector, and
IDF(wl�) is the Inverse Document Frequency of wl�
evaluated in the corpus Tl.
The matrix D can be determined for example us-
ing hand-made lexical resources, such as WORD-
NET DOMAINS (Magnini and Cavagli`a, 2000). In
the present work we followed the way to acquire
D automatically from corpora, exploiting the tech-
nique described below.
</bodyText>
<subsectionHeader confidence="0.9816365">
4.1 Automatic Acquisition of Multilingual
Domain Models
</subsectionHeader>
<bodyText confidence="0.999329">
In this work we propose the use of Latent Seman-
tic Analysis (LSA) (Deerwester et al., 1990) to in-
duce a MDM from comparable corpora. LSA is an
unsupervised technique for estimating the similar-
ity among texts and terms in a large corpus. In the
monolingual settings LSA is performed by means
of a Singular Value Decomposition (SVD) of the
term-by-document matrix T describing the corpus.
SVD decomposes the term-by-document matrix T
into three matrixes T  VEk&apos;UT where Ek&apos; is the
diagonal k × k matrix containing the highest k0  k
</bodyText>
<page confidence="0.994007">
12
</page>
<bodyText confidence="0.99967478125">
eigenvalues of T, and all the remaining elements are
set to 0. The parameter k&apos; is the dimensionality of
the Domain VSM and can be fixed in advance (i.e.
k&apos; = d).
In the literature (Littman et al., 1998) LSA has
been used in multilingual settings to define a mul-
tilingual space in which texts in different languages
can be represented and compared. In that work LSA
strongly relied on the availability of aligned parallel
corpora: documents in all the languages are repre-
sented in a term-by-document matrix (see Figure 1)
and then the columns corresponding to sets of trans-
lated documents are collapsed (i.e. they are substi-
tuted by their sum) before starting the LSA process.
The effect of this step is to merge the subspaces (i.e.
the right and the left sectors of the matrix in Figure
1) in which the documents have been originally rep-
resented.
In this paper we propose a variation of this strat-
egy, performing a multilingual LSA in the case in
which an aligned parallel corpus is not available.
It exploits the presence of common words among
different languages in the term-by-document matrix.
The SVD process has the effect of creating a LSA
space in which documents in both languages are rep-
resented. Of course, the higher the number of com-
mon words, the more information will be provided
to the SVD algorithm to find common LSA dimen-
sion for the two languages. The resulting LSA di-
mensions can be perceived as multilingual clusters
of terms and document. LSA can then be used to
define a Multilingual Domain Matrix DLSA.
</bodyText>
<equation confidence="0.964376">
�
DLSA = INV Ek&apos; (2)
</equation>
<bodyText confidence="0.670205166666667">
where IN is a diagonal matrix such that iN�,� =
�w&apos;i is the ith row of the matrix V&apos;\/Ek&apos;.
Thus DLSA3 can be exploited to estimate simi-
larity among texts expressed in different languages
(see Section 5).
3When DLSA is substituted in Equation 1 the Domain VSM
is equivalent to a Latent Semantic Space (Deerwester et al.,
1990). The only difference in our formulation is that the vectors
representing the terms in the Domain VSM are normalized by
the matrix IN, and then rescaled, according to their IDF value,
by matrix IIDF. Note the analogy with the tf idf term weighting
schema, widely adopted in Information Retrieval.
</bodyText>
<subsectionHeader confidence="0.67297">
4.2 Similarity in the multilingual domain space
</subsectionHeader>
<bodyText confidence="0.999671666666667">
As an example of the second-order similarity pro-
vided by this approach, we can see in Table 2 the five
most similar terms to the lemma bank. The similar-
ity among terms is calculated by cosine among the
rows in the matrix DLSA, acquired from the data
set used in our experiments (see Section 6.2). It is
worth noting that the Italian lemma banca (i.e. bank
in English) has a high similarity score to the English
lemma bank. While this is not enough to have a pre-
cise term translation, it is sufficient to capture rele-
vant aspects of topic similarity in a cross-language
text categorization task.
</bodyText>
<subsubsectionHeader confidence="0.42822">
Lemma#Pos Similarity Score Language
</subsubsectionHeader>
<table confidence="0.8916368">
banking#n 0.96 Eng
credit#n 0.90 Eng
amro#n 0.89 Eng
unicredito#n 0.85 Ita
banca#n 0.83 Ita
</table>
<tableCaption confidence="0.9431415">
Table 2: Terms with high similarity to the English
lemma bank#n, in the Multilingual Domain Model
</tableCaption>
<sectionHeader confidence="0.980349" genericHeader="method">
5 The Multilingual Domain Kernel
</sectionHeader>
<bodyText confidence="0.9999425">
Kernel Methods are the state-of-the-art supervised
framework for learning, and they have been success-
fully adopted to approach the TC task (Joachims,
2002).
The basic idea behind kernel methods is to em-
bed the data into a suitable feature space F via a
mapping function 0 : X — F, and then to use a
linear algorithm for discovering nonlinear patterns.
Kernel methods allow us to build a modular system,
as the kernel function acts as an interface between
the data and the learning algorithm. Thus the ker-
nel function becomes the only domain specific mod-
ule of the system, while the learning algorithm is a
general purpose component. Potentially any kernel
function can work with any kernel-based algorithm,
as for example Support Vector Machines (SVMs).
During the learning phase SVMs assign a weight
Ai &gt; 0 to any example xi E X. All the labeled
instances xi such that Ai &gt; 0 are called Support
Vectors. Support Vectors lie close to the best sepa-
rating hyper-plane between positive and negative ex-
amples. New examples are then assigned to the class
</bodyText>
<equation confidence="0.850818666666667">
1
� ,
(~wi,~wz)
</equation>
<page confidence="0.981043">
13
</page>
<bodyText confidence="0.7875165">
of the closest support vectors, according to equation
3.
</bodyText>
<equation confidence="0.993057">
n
f(x) = AiK(xi, x) + A0 (3)
i=1
</equation>
<bodyText confidence="0.999757">
The kernel function K(xi, x) returns the simi-
larity between two instances in the input space X,
and can be designed just by taking care that some
formal requirements are satisfied, as described in
(Sch¨olkopf and Smola, 2001).
In this section we define the Multilingual Domain
Kernel, and we apply it to a cross language TC task.
This kernel can be exploited to estimate the topic
similarity among two texts expressed in different
languages by taking into account the external knowl-
edge provided by a MDM. It defines an explicit map-
ping D : Rk —* Rk&apos; from the Multilingual VSM
into the Multilingual Domain VSM. The Multilin-
gual Domain Kernel is specified by
</bodyText>
<equation confidence="0.997725333333333">
�D(ti),D(tj)�
KD(ti,tj) = (4)
�(D(tj ), D(tj))(D(ti), D(ti))
</equation>
<bodyText confidence="0.99989825">
where D is the Domain Mapping defined in equa-
tion 1. Thus the Multilingual Domain Kernel re-
quires Multilingual Domain Matrix D, in particular
DLSA that can be acquired from comparable cor-
pora, as explained in Section 4.1.
To evaluate the Multilingual Domain Kernel we
compared it to a baseline kernel function, namely the
bag of words kernel, that simply estimates the topic
similarity in the Multilingual VSM, as described in
Section 3. The BoW kernel is a particular case of
the Domain Kernel, in which D = I, and I is the
identity matrix.
</bodyText>
<sectionHeader confidence="0.994889" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.9997607">
In this section we present the data set (two compara-
ble English and Italian corpora) used in the evalua-
tion, and we show the results of the Cross Language
TC tasks. In particular we tried both to train the
system on the English data set and classify Italian
documents and to train using Italian and classify the
English test set. We compare the learning curves of
the Multilingual Domain Kernel with the standard
BoW kernel, which is considered as a baseline for
this task.
</bodyText>
<subsectionHeader confidence="0.999321">
6.1 Implementation details
</subsectionHeader>
<bodyText confidence="0.999936384615385">
As a supervised learning device, we used the SVM
implementation described in (Joachims, 1999). The
Multilingual Domain Kernel is implemented by
defining an explicit feature mapping as explained
above, and by normalizing each vector. All the ex-
periments have been performed with the standard
SVM parameter settings.
We acquired a Multilingual Domain Model by
performing the Singular Value Decomposition pro-
cess on the term-by-document matrices representing
the merged training partitions (i.e. English and Ital-
ian), and we considered only the first 400 dimen-
sions4.
</bodyText>
<subsectionHeader confidence="0.999495">
6.2 Data set description
</subsectionHeader>
<bodyText confidence="0.9999137">
We used a news corpus kindly put at our dis-
posal by ADNKRONOS, an important Italian news
provider. The corpus consists of 32,354 Ital-
ian and 27,821 English news partitioned by
ADNKRONOS in a number of four fixed cate-
gories: Quality of Life, Made in Italy,
Tourism, Culture and School. The corpus
is comparable, in the sense stated in Section 2, i.e.
they covered the same topics and the same period of
time. Some news are translated in the other language
(but no alignment indication is given), some others
are present only in the English set, and some others
only in the Italian. The average length of the news
is about 300 words. We randomly split both the En-
glish and Italian part into 75% training and 25% test
(see Table 3). In both the data sets we postagged the
texts and we considered only the noun, verb, adjec-
tive, and adverb parts of speech, representing them
by vectors containing the frequencies of each lemma
with its part of speech.
</bodyText>
<subsectionHeader confidence="0.999536">
6.3 Monolingual Results
</subsectionHeader>
<bodyText confidence="0.999084">
Before going to a cross-language TC task, we con-
ducted two tests of classical monolingual TC by
training and testing the system on Italian and En-
glish documents separately. For these tests we used
the SVM with the BoW kernel. Figures 2 and 3 re-
port the results.
</bodyText>
<footnote confidence="0.9398385">
4To perform the SVD operation we used LIBSVDC
http://tedlab.mit.edu/∼dr/SVDLIBC/.
</footnote>
<page confidence="0.9926895">
14
15
</page>
<table confidence="0.998514571428571">
Categories English Total Italian Total
Training Test Training Test
Quality of Life 5759 1989 7748 5781 1901 7682
Made in Italy 5711 1864 7575 6111 2068 8179
Tourism 5731 1857 7588 6090 2015 8105
Culture and School 3665 1245 4910 6284 2104 8388
Total 20866 6955 27821 24266 8088 32354
</table>
<tableCaption confidence="0.956723">
Table 3: Number of documents in the data set partitions
</tableCaption>
<figure confidence="0.997523607142857">
F1 measure
0.95
0.85
0.75
0.65
0.55
0.9
0.8
0.7
0.6
0.5
1
BoW Kernel
F1 measure
0.55
0.45
0.35
0.25
0.15
0.6
0.5
0.4
0.3
0.2
Multilingual Domain Kernel
Bow Kernel
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fraction of training data
</figure>
<figureCaption confidence="0.706057">
Figure 2: Learning curves for the English part of the
corpus
</figureCaption>
<figure confidence="0.9996096">
F1 measure
0.95
0.85
0.75
0.65
0.55
0.9
0.8
0.7
0.6
0.5
1
BoW Kernel
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fraction of training data
</figure>
<figureCaption confidence="0.8715545">
Figure 3: Learning curves for the Italian part of the
corpus
</figureCaption>
<figure confidence="0.995104">
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fraction of training data
</figure>
<figureCaption confidence="0.967822">
Figure 4: Cross-language (training on Italian, test on
</figureCaption>
<figure confidence="0.976485875">
English) learning curves
F1 measure
0.65
0.55
0.45
0.35
0.25
0.7
0.6
0.5
0.4
0.3
Multilingual Domain Kernel
Bow Kernel
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Fraction of training data
</figure>
<figureCaption confidence="0.990777">
Figure 5: Cross-language (training on English, test
on Italian) learning curves
</figureCaption>
<bodyText confidence="0.8944439">
6.4 A Cross Language Text Categorization task
As far as the cross language TC task is concerned,
we tried the two possible options: we trained on the
English part and we classified the Italian part, and
we trained on the Italian and classified on the En-
glish part. The Multilingual Domain Model was ac-
quired running the SVD only on the joint (English
and Italian) training parts.
Table 4 reports the vocabulary dimensions of the
English and Italian training partitions, the vocabu-
</bodyText>
<table confidence="0.9939004">
# lemmata
English training 22,704
Italian training 26,404
English + Italian 43,384
common lemmata 5,724
</table>
<tableCaption confidence="0.8168835">
Table 4: Number of lemmata in the training parts of
the corpus
</tableCaption>
<bodyText confidence="0.9998655">
lary of the merged training, and how many com-
mon lemmata are present (about 14% of the total).
Among the common lemmata, 97% are nouns and
most of them are proper nouns. Thus the initial term-
by-document matrix is a 43,384 × 45,132 matrix,
while the DLSA matrix is 43,384 × 400. For this
task we consider as a baseline the BoW kernel.
The results are reported in Figures 4 and 5. An-
alyzing the learning curves, it is worth noting that
when the quantity of training increases, the per-
formance becomes better and better for the Multi-
lingual Domain Kernel, suggesting that with more
available training it could be possible to go closer to
typical monolingual TC results.
</bodyText>
<sectionHeader confidence="0.998892" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998429571428571">
In this paper we proposed a solution to cross lan-
guage Text Categorization based on acquiring Mul-
tilingual Domain Models from comparable corpora
in a totally unsupervised way and without using any
external knowledge source (e.g. bilingual dictionar-
ies). These Multilingual Domain Models are ex-
ploited to define a generalized similarity function
(i.e. a kernel function) among documents in differ-
ent languages, which is used inside a Support Vec-
tor Machines classification framework. The basis of
the similarity function exploits the presence of com-
mon words to induce a second-order similarity for
the other words in the lexicons. The results have
shown that this technique is sufficient to capture rel-
evant aspects of topic similarity in cross-language
TC tasks, obtaining substantial improvements over
a simple baseline. As future work we will investi-
gate the performance of this approach to more than
two languages TC task, and a possible generaliza-
tion of the assumption about equality of the common
words.
</bodyText>
<sectionHeader confidence="0.997131" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996298333333333">
This work has been partially supported by the
ONTOTEXT project, funded by the Autonomous
Province of Trento under the FUP-2004 program.
</bodyText>
<sectionHeader confidence="0.993651" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999415952380953">
S. Deerwester, S. T. Dumais, G. W. Furnas, T.K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent se-
mantic analysis. Journal of the American Society for
Information Science, 41(6):391–407.
E. Gaussier, J. M. Renders, I. Matveeva, C. Goutte, and
H. Dejean. 2004. A geometric view on bilingual lexi-
con extraction from comparable corpora. In Proceed-
ings ofACL-04, Barcelona, Spain, July.
A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsu-
pervised and supervised exploitation of semantic do-
mains in lexical disambiguation. Computer Speech
and Language, 18:275–299.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Sch¨olkopf, C. Burges, and A. Smola,
editors, Advances in kernel methods: support vector
learning, chapter 11, pages 169 – 184. The MIT Press.
T. Joachims. 2002. Learning to Classify Text using Sup-
port Vector Machines. Kluwer Academic Publishers.
P. Koehn and K. Knight. 2002. Learning a translation
lexicon from monolingual corpora. In Proceedings of
ACL Workshop on Unsupervised Lexical Acquisition,
Philadelphia, July.
M. Littman, S. Dumais, and T. Landauer. 1998. Auto-
matic cross-language information retrieval using latent
semantic indexing. In G. Grefenstette, editor, Cross
Language Information Retrieval, pages 51–62. Kluwer
Academic Publishers.
B. Magnini and G. Cavagli`a. 2000. Integrating subject
field codes into WordNet. In Proceedings of LREC-
2000, Athens, Greece, June.
D. Melamed. 2001. Empirical Methods for Exploiting
Parallel Texts. The MIT Press.
B. Sch¨olkopf and A. J. Smola. 2001. Learning with Ker-
nels. Support Vector Machines, Regularization, Opti-
mization, and Beyond. The MIT Press.
C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pat-
tern abstraction and term similarity for word sense
disambiguation. In Proceedings of SENSEVAL-3,
Barcelona, Spain, July.
S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Gen-
eralized vector space model in information retrieval.
In Proceedings of the 81h ACM SIGIR Conference.
</reference>
<page confidence="0.998666">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.353179">
<title confidence="0.9205385">Cross language Text Categorization by Multilingual Domain Models from Comparable Corpora</title>
<author confidence="0.897533">Alfio Gliozzo</author>
<author confidence="0.897533">Carlo</author>
<affiliation confidence="0.423136">ITC-Irst</affiliation>
<address confidence="0.617841">via Sommarive, I-38050, Trento, ITALY</address>
<abstract confidence="0.999291653846154">In a multilingual scenario, the classical monolingual text categorization problem be reformulated as a language in which we have to cope with or more languages (e.g. In this setting, the system is trained using labeled examples in a source (e.g. and it classifies documents in a different target language In this paper we propose a novel approach to solve the cross language text categorization problem based on acquiring Multilingual Domain Models from comparable corpora in a totally unsupervised way and without using any external knowledge source (e.g. bilingual dictionaries). These Multilingual Domain Models are exploited to define a generalized similarity function (i.e. a kernel function) among documents in different languages, which is used inside a Support Vector Machines classification framework. The results show that our approach is a feasible and cheap solution that largely outperforms a baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<issue>6</issue>
<contexts>
<context position="3601" citStr="Deerwester et al., 1990" startWordPosition="559" endWordPosition="563"> After a brief discussion about comparable corpora, we introduce Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 9–16, Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005 a multilingual Vector Space Model, in which documents in different languages can be represented and then compared. In Section 4 we define the MDMs and we present a totally unsupervised technique to acquire them from comparable corpora. This methodology does not require any external knowledge source (e.g. bilingual dictionaries) and it is based on Latent Semantic Analysis (LSA) (Deerwester et al., 1990). MDMs are then exploited to define a Multilingual Domain Kernel, a generalized similarity function among documents in different languages that exploits a MDM (see Section 5). The Multilingual Domain Kernel is used inside a Support Vector Machines (SVM) classification framework for TC (Joachims, 2002). In Section 6 we will evaluate our technique in a Cross Language categorization task. The results show that our approach is a feasible and cheap solution, largely outperforming a baseline. Conclusions and future works are finally reported in Section 7. 2 Comparable Corpora Comparable corpora are </context>
<context position="14235" citStr="Deerwester et al., 1990" startWordPosition="2492" endWordPosition="2495">stance. D(~tj) = ~tj(IIDFD) = ~t0 (1) j where IIDF is a diagonal matrix such that i�DF ��� = IDF(wl�), ~tj is represented as a row vector, and IDF(wl�) is the Inverse Document Frequency of wl� evaluated in the corpus Tl. The matrix D can be determined for example using hand-made lexical resources, such as WORDNET DOMAINS (Magnini and Cavagli`a, 2000). In the present work we followed the way to acquire D automatically from corpora, exploiting the technique described below. 4.1 Automatic Acquisition of Multilingual Domain Models In this work we propose the use of Latent Semantic Analysis (LSA) (Deerwester et al., 1990) to induce a MDM from comparable corpora. LSA is an unsupervised technique for estimating the similarity among texts and terms in a large corpus. In the monolingual settings LSA is performed by means of a Singular Value Decomposition (SVD) of the term-by-document matrix T describing the corpus. SVD decomposes the term-by-document matrix T into three matrixes T  VEk&apos;UT where Ek&apos; is the diagonal k × k matrix containing the highest k0  k 12 eigenvalues of T, and all the remaining elements are set to 0. The parameter k&apos; is the dimensionality of the Domain VSM and can be fixed in advance (i.e. k&apos;</context>
<context position="16553" citStr="Deerwester et al., 1990" startWordPosition="2900" endWordPosition="2903">words, the more information will be provided to the SVD algorithm to find common LSA dimension for the two languages. The resulting LSA dimensions can be perceived as multilingual clusters of terms and document. LSA can then be used to define a Multilingual Domain Matrix DLSA. � DLSA = INV Ek&apos; (2) where IN is a diagonal matrix such that iN�,� = �w&apos;i is the ith row of the matrix V&apos;\/Ek&apos;. Thus DLSA3 can be exploited to estimate similarity among texts expressed in different languages (see Section 5). 3When DLSA is substituted in Equation 1 the Domain VSM is equivalent to a Latent Semantic Space (Deerwester et al., 1990). The only difference in our formulation is that the vectors representing the terms in the Domain VSM are normalized by the matrix IN, and then rescaled, according to their IDF value, by matrix IIDF. Note the analogy with the tf idf term weighting schema, widely adopted in Information Retrieval. 4.2 Similarity in the multilingual domain space As an example of the second-order similarity provided by this approach, we can see in Table 2 the five most similar terms to the lemma bank. The similarity among terms is calculated by cosine among the rows in the matrix DLSA, acquired from the data set u</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. T. Dumais, G. W. Furnas, T.K. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41(6):391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gaussier</author>
<author>J M Renders</author>
<author>I Matveeva</author>
<author>C Goutte</author>
<author>H Dejean</author>
</authors>
<title>A geometric view on bilingual lexicon extraction from comparable corpora.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL-04,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="6025" citStr="Gaussier et al., 2004" startWordPosition="975" endWordPosition="978"> different news agencies), and it is not known if a function ψ exists, even if in principle it could exist and return 1 for a strict subset of document pairs. There exist many interesting works about using parallel corpora for multilingual applications (Melamed, 2001), such as Machine Translation, Cross language Information Retrieval (Littman et al., 1998), lexical acquisition, and so on. However it is not always easy to find or build parallel corpora. This is the main reason because the weaker notion of comparable corpora is a matter recent interest in the field of Computational Linguistics (Gaussier et al., 2004). The texts inside comparable corpora, being about the same topics (i.e. about the same semantic domains), should refer to the same concepts by using various expressions in different languages. On the other hand, most of the proper nouns, relevant entities and words that are not yet lexicalized in the language, are expressed by using their original terms. As a consequence the same entities will be denoted with the same words in different languages, allowing to automatically detect couples of translation pairs just by looking at the word shape (Koehn and Knight, 2002). Our hypothesis is that co</context>
</contexts>
<marker>Gaussier, Renders, Matveeva, Goutte, Dejean, 2004</marker>
<rawString>E. Gaussier, J. M. Renders, I. Matveeva, C. Goutte, and H. Dejean. 2004. A geometric view on bilingual lexicon extraction from comparable corpora. In Proceedings ofACL-04, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gliozzo</author>
<author>C Strapparava</author>
<author>I Dagan</author>
</authors>
<title>Unsupervised and supervised exploitation of semantic domains in lexical disambiguation. Computer Speech and Language,</title>
<date>2004</date>
<pages>18--275</pages>
<contexts>
<context position="2561" citStr="Gliozzo et al., 2004" startWordPosition="395" endWordPosition="398">es (e.g. English and Italian). In this setting, the system is trained using labeled examples in a source language (e.g. English), and it classifies documents in a different target language (e.g. Italian). In this paper we propose a novel approach to solve the cross language text categorization problem based on acquiring Multilingual Domain Models (MDM) from comparable corpora in an unsupervised way. A MDM is a set of clusters formed by terms in different languages. While in the monolingual settings semantic domains are clusters of related terms that co-occur in texts regarding similar topics (Gliozzo et al., 2004), in the multilingual settings such clusters are composed by terms in different languages expressing concepts in the same semantic field. Thus, the basic relation modeled by a MDM is the domain similarity among terms in different languages. Our claim is that such a relation is sufficient to capture relevant aspects of topic similarity that can be profitably used for TC purposes. The paper is organized as follows. After a brief discussion about comparable corpora, we introduce Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 9–16, Ann Arbor, June 2005. c�Association f</context>
<context position="9734" citStr="Gliozzo et al., 2004" startWordPosition="1610" endWordPosition="1613">nce of a number of common words, represented by the central rows of the matrix in Figure 1. As we will show in Section 6, this model is rather poor because of its sparseness. In the next section, we will show how to use such words as seeds to induce a Multilingual Domain VSM, in which second order relations among terms and documents in different languages are considered to improve the similarity estimation. 4 Multilingual Domain Models A MDM is a multilingual extension of the concept of Domain Model. In the literature, Domain Models have been introduced to represent ambiguity and variability (Gliozzo et al., 2004) and successfully exploited in many NLP applications, such us Word Sense Disambiguation (Strapparava et al., 2004), Text Categorization and Term Categorization. A Domain Model is composed by soft clusters of terms. Each cluster represents a semantic domain, i.e. a set of terms that often co-occur in texts having similar topics. Such clusters identifies groups of words belonging to the same semantic field, and thus highly paradigmatically related. MDMs are Domain Models containing terms in more than one language. A MDM is represented by a matrix D, containing the degree of association among ter</context>
</contexts>
<marker>Gliozzo, Strapparava, Dagan, 2004</marker>
<rawString>A. Gliozzo, C. Strapparava, and I. Dagan. 2004. Unsupervised and supervised exploitation of semantic domains in lexical disambiguation. Computer Speech and Language, 18:275–299.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in kernel methods: support vector learning, chapter 11,</booktitle>
<pages>169--184</pages>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="20779" citStr="Joachims, 1999" startWordPosition="3629" endWordPosition="3630">matrix. 6 Evaluation In this section we present the data set (two comparable English and Italian corpora) used in the evaluation, and we show the results of the Cross Language TC tasks. In particular we tried both to train the system on the English data set and classify Italian documents and to train using Italian and classify the English test set. We compare the learning curves of the Multilingual Domain Kernel with the standard BoW kernel, which is considered as a baseline for this task. 6.1 Implementation details As a supervised learning device, we used the SVM implementation described in (Joachims, 1999). The Multilingual Domain Kernel is implemented by defining an explicit feature mapping as explained above, and by normalizing each vector. All the experiments have been performed with the standard SVM parameter settings. We acquired a Multilingual Domain Model by performing the Singular Value Decomposition process on the term-by-document matrices representing the merged training partitions (i.e. English and Italian), and we considered only the first 400 dimensions4. 6.2 Data set description We used a news corpus kindly put at our disposal by ADNKRONOS, an important Italian news provider. The </context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-scale SVM learning practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in kernel methods: support vector learning, chapter 11, pages 169 – 184. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Learning to Classify Text using Support Vector Machines.</title>
<date>2002</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="3903" citStr="Joachims, 2002" startWordPosition="608" endWordPosition="609">ted and then compared. In Section 4 we define the MDMs and we present a totally unsupervised technique to acquire them from comparable corpora. This methodology does not require any external knowledge source (e.g. bilingual dictionaries) and it is based on Latent Semantic Analysis (LSA) (Deerwester et al., 1990). MDMs are then exploited to define a Multilingual Domain Kernel, a generalized similarity function among documents in different languages that exploits a MDM (see Section 5). The Multilingual Domain Kernel is used inside a Support Vector Machines (SVM) classification framework for TC (Joachims, 2002). In Section 6 we will evaluate our technique in a Cross Language categorization task. The results show that our approach is a feasible and cheap solution, largely outperforming a baseline. Conclusions and future works are finally reported in Section 7. 2 Comparable Corpora Comparable corpora are collections of texts in different languages regarding similar topics (e.g. a collection of news published by agencies in the same period). More restrictive requirements are expected for parallel corpora (i.e. corpora composed by texts which are mutual translations), while the class of the multilingual</context>
<context position="17907" citStr="Joachims, 2002" startWordPosition="3128" endWordPosition="3129">e to the English lemma bank. While this is not enough to have a precise term translation, it is sufficient to capture relevant aspects of topic similarity in a cross-language text categorization task. Lemma#Pos Similarity Score Language banking#n 0.96 Eng credit#n 0.90 Eng amro#n 0.89 Eng unicredito#n 0.85 Ita banca#n 0.83 Ita Table 2: Terms with high similarity to the English lemma bank#n, in the Multilingual Domain Model 5 The Multilingual Domain Kernel Kernel Methods are the state-of-the-art supervised framework for learning, and they have been successfully adopted to approach the TC task (Joachims, 2002). The basic idea behind kernel methods is to embed the data into a suitable feature space F via a mapping function 0 : X — F, and then to use a linear algorithm for discovering nonlinear patterns. Kernel methods allow us to build a modular system, as the kernel function acts as an interface between the data and the learning algorithm. Thus the kernel function becomes the only domain specific module of the system, while the learning algorithm is a general purpose component. Potentially any kernel function can work with any kernel-based algorithm, as for example Support Vector Machines (SVMs). D</context>
</contexts>
<marker>Joachims, 2002</marker>
<rawString>T. Joachims. 2002. Learning to Classify Text using Support Vector Machines. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>K Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL Workshop on Unsupervised Lexical Acquisition,</booktitle>
<location>Philadelphia,</location>
<contexts>
<context position="6598" citStr="Koehn and Knight, 2002" startWordPosition="1071" endWordPosition="1074"> Computational Linguistics (Gaussier et al., 2004). The texts inside comparable corpora, being about the same topics (i.e. about the same semantic domains), should refer to the same concepts by using various expressions in different languages. On the other hand, most of the proper nouns, relevant entities and words that are not yet lexicalized in the language, are expressed by using their original terms. As a consequence the same entities will be denoted with the same words in different languages, allowing to automatically detect couples of translation pairs just by looking at the word shape (Koehn and Knight, 2002). Our hypothesis is that comparable corpora contain a large amount of such words, just because texts, referring to the same topics in different languages, will often adopt the same terms to denote the same entitiesl. However, the simple presence of these shared words is not enough to get significant results in TC tasks. As we will see, we need to exploit these common words to induce a second-order similarity for the other words in the lexicons. 3 The Multilingual Vector Space Model Let T = {t1, t2, ... , tn} be a corpus, and V = {w1, w2, ... , wk} be its vocabulary. In the monolingual settings</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>P. Koehn and K. Knight. 2002. Learning a translation lexicon from monolingual corpora. In Proceedings of ACL Workshop on Unsupervised Lexical Acquisition, Philadelphia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Littman</author>
<author>S Dumais</author>
<author>T Landauer</author>
</authors>
<title>Automatic cross-language information retrieval using latent semantic indexing.</title>
<date>1998</date>
<booktitle>Cross Language Information Retrieval,</booktitle>
<pages>51--62</pages>
<editor>In G. Grefenstette, editor,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="5761" citStr="Littman et al., 1998" startWordPosition="928" endWordPosition="931">s known, then the corpus is parallel and aligned at document level. For the purpose of this paper it is enough to assume that two corpora are comparable, i.e. they are composed by documents about the same topics and produced in the same period (e.g. possibly from different news agencies), and it is not known if a function ψ exists, even if in principle it could exist and return 1 for a strict subset of document pairs. There exist many interesting works about using parallel corpora for multilingual applications (Melamed, 2001), such as Machine Translation, Cross language Information Retrieval (Littman et al., 1998), lexical acquisition, and so on. However it is not always easy to find or build parallel corpora. This is the main reason because the weaker notion of comparable corpora is a matter recent interest in the field of Computational Linguistics (Gaussier et al., 2004). The texts inside comparable corpora, being about the same topics (i.e. about the same semantic domains), should refer to the same concepts by using various expressions in different languages. On the other hand, most of the proper nouns, relevant entities and words that are not yet lexicalized in the language, are expressed by using </context>
<context position="14882" citStr="Littman et al., 1998" startWordPosition="2608" endWordPosition="2611">arable corpora. LSA is an unsupervised technique for estimating the similarity among texts and terms in a large corpus. In the monolingual settings LSA is performed by means of a Singular Value Decomposition (SVD) of the term-by-document matrix T describing the corpus. SVD decomposes the term-by-document matrix T into three matrixes T  VEk&apos;UT where Ek&apos; is the diagonal k × k matrix containing the highest k0  k 12 eigenvalues of T, and all the remaining elements are set to 0. The parameter k&apos; is the dimensionality of the Domain VSM and can be fixed in advance (i.e. k&apos; = d). In the literature (Littman et al., 1998) LSA has been used in multilingual settings to define a multilingual space in which texts in different languages can be represented and compared. In that work LSA strongly relied on the availability of aligned parallel corpora: documents in all the languages are represented in a term-by-document matrix (see Figure 1) and then the columns corresponding to sets of translated documents are collapsed (i.e. they are substituted by their sum) before starting the LSA process. The effect of this step is to merge the subspaces (i.e. the right and the left sectors of the matrix in Figure 1) in which the</context>
</contexts>
<marker>Littman, Dumais, Landauer, 1998</marker>
<rawString>M. Littman, S. Dumais, and T. Landauer. 1998. Automatic cross-language information retrieval using latent semantic indexing. In G. Grefenstette, editor, Cross Language Information Retrieval, pages 51–62. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>G Cavagli`a</author>
</authors>
<title>Integrating subject field codes into WordNet.</title>
<date>2000</date>
<booktitle>In Proceedings of LREC2000,</booktitle>
<location>Athens, Greece,</location>
<marker>Magnini, Cavagli`a, 2000</marker>
<rawString>B. Magnini and G. Cavagli`a. 2000. Integrating subject field codes into WordNet. In Proceedings of LREC2000, Athens, Greece, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Melamed</author>
</authors>
<title>Empirical Methods for Exploiting Parallel Texts.</title>
<date>2001</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5671" citStr="Melamed, 2001" startWordPosition="918" endWordPosition="919">. If the function ψ exists for every text tiz E T* and for every language Lj, and is known, then the corpus is parallel and aligned at document level. For the purpose of this paper it is enough to assume that two corpora are comparable, i.e. they are composed by documents about the same topics and produced in the same period (e.g. possibly from different news agencies), and it is not known if a function ψ exists, even if in principle it could exist and return 1 for a strict subset of document pairs. There exist many interesting works about using parallel corpora for multilingual applications (Melamed, 2001), such as Machine Translation, Cross language Information Retrieval (Littman et al., 1998), lexical acquisition, and so on. However it is not always easy to find or build parallel corpora. This is the main reason because the weaker notion of comparable corpora is a matter recent interest in the field of Computational Linguistics (Gaussier et al., 2004). The texts inside comparable corpora, being about the same topics (i.e. about the same semantic domains), should refer to the same concepts by using various expressions in different languages. On the other hand, most of the proper nouns, relevan</context>
</contexts>
<marker>Melamed, 2001</marker>
<rawString>D. Melamed. 2001. Empirical Methods for Exploiting Parallel Texts. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Sch¨olkopf</author>
<author>A J Smola</author>
</authors>
<title>Learning with Kernels. Support Vector Machines, Regularization, Optimization, and Beyond.</title>
<date>2001</date>
<publisher>The MIT Press.</publisher>
<marker>Sch¨olkopf, Smola, 2001</marker>
<rawString>B. Sch¨olkopf and A. J. Smola. 2001. Learning with Kernels. Support Vector Machines, Regularization, Optimization, and Beyond. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>A Gliozzo</author>
<author>C Giuliano</author>
</authors>
<title>Pattern abstraction and term similarity for word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of SENSEVAL-3,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="9848" citStr="Strapparava et al., 2004" startWordPosition="1626" endWordPosition="1629"> Section 6, this model is rather poor because of its sparseness. In the next section, we will show how to use such words as seeds to induce a Multilingual Domain VSM, in which second order relations among terms and documents in different languages are considered to improve the similarity estimation. 4 Multilingual Domain Models A MDM is a multilingual extension of the concept of Domain Model. In the literature, Domain Models have been introduced to represent ambiguity and variability (Gliozzo et al., 2004) and successfully exploited in many NLP applications, such us Word Sense Disambiguation (Strapparava et al., 2004), Text Categorization and Term Categorization. A Domain Model is composed by soft clusters of terms. Each cluster represents a semantic domain, i.e. a set of terms that often co-occur in texts having similar topics. Such clusters identifies groups of words belonging to the same semantic field, and thus highly paradigmatically related. MDMs are Domain Models containing terms in more than one language. A MDM is represented by a matrix D, containing the degree of association among terms in all the languages and domains, as illustrated in Table 1. MEDICINE COMPUTER SCIENCE HIVe/i 1 0 AIDSe/i 1 0 v</context>
</contexts>
<marker>Strapparava, Gliozzo, Giuliano, 2004</marker>
<rawString>C. Strapparava, A. Gliozzo, and C. Giuliano. 2004. Pattern abstraction and term similarity for word sense disambiguation. In Proceedings of SENSEVAL-3, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K M Wong</author>
<author>W Ziarko</author>
<author>P C N Wong</author>
</authors>
<title>Generalized vector space model in information retrieval.</title>
<date>1985</date>
<booktitle>In Proceedings of the 81h ACM SIGIR Conference.</booktitle>
<contexts>
<context position="13502" citStr="Wong et al., 1985" startWordPosition="2365" endWordPosition="2368">sed by document expressed in the language L�, let V ∗ = Uj V&apos; be the set of all the terms in all the languages, and let k∗ = |V ∗ |be the cardinality of this set. Let D = {D1, D2,..., Dd} be a set of domains. A DM is fully defined by a k∗ × d domain matrix D representing in each cell di�z the domain relevance of the ith term of V ∗ with respect to the domain Dz. The domain matrix D is used to define a function D : Rk*  Rd, that maps the document vectors ~tj expressed into the multilingual classical VSM, into ~ the vectors t0j in the multilingual domain VSM. The function D is defined bye 2In (Wong et al., 1985) the formula 1 is used to define a Generalized Vector Space Model, of which the Domain VSM is a particular instance. D(~tj) = ~tj(IIDFD) = ~t0 (1) j where IIDF is a diagonal matrix such that i�DF ��� = IDF(wl�), ~tj is represented as a row vector, and IDF(wl�) is the Inverse Document Frequency of wl� evaluated in the corpus Tl. The matrix D can be determined for example using hand-made lexical resources, such as WORDNET DOMAINS (Magnini and Cavagli`a, 2000). In the present work we followed the way to acquire D automatically from corpora, exploiting the technique described below. 4.1 Automatic </context>
</contexts>
<marker>Wong, Ziarko, Wong, 1985</marker>
<rawString>S.K.M. Wong, W. Ziarko, and P.C.N. Wong. 1985. Generalized vector space model in information retrieval. In Proceedings of the 81h ACM SIGIR Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>