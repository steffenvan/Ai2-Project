<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.991028">
Unsupervised Semantic Role Labelling
</title>
<author confidence="0.950876">
Robert S. Swier and Suzanne Stevenson
</author>
<affiliation confidence="0.889377333333333">
Department of Computer Science
University of Toronto
Toronto, Ontario, Canada M5S 3G4
</affiliation>
<email confidence="0.997139">
swier,suzanne @cs.toronto.edu
</email>
<sectionHeader confidence="0.993835" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999919076923077">
We present an unsupervised method for labelling
the arguments of verbs with their semantic roles.
Our bootstrapping algorithm makes initial unam-
biguous role assignments, and then iteratively up-
dates the probability model on which future assign-
ments are based. A novel aspect of our approach
is the use of verb, slot, and noun class informa-
tion as the basis for backing off in our probability
model. We achieve 50–65% reduction in the error
rate over an informed baseline, indicating the po-
tential of our approach for a task that has heretofore
relied on large amounts of manually generated train-
ing data.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894015151515">
Semantic annotation of text corpora is needed to
support tasks such as information extraction and
question-answering (e.g., Riloff and Schmelzen-
bach, 1998; Niu and Hirst, 2004). In particular, la-
belling the semantic roles of the arguments of a verb
(or any predicate), as in (1) and (2), provides crucial
information about the relations among event partic-
ipants.
Because of the importance of this task, a number of
recent methods have been proposed for automatic
semantic role labelling (e.g., Gildea and Jurafsky,
2002; Gildea and Palmer, 2002; Chen and Rambow,
2003; Fleischman et al., 2003; Hacioglu et al., 2003;
Thompson et al., 2003). These supervised methods
are limited by their reliance on the manually role-
tagged corpora of FrameNet (Baker et al., 1998)
or PropBank (Palmer et al., 2003) as training data,
which are expensive to produce, are limited in size,
and may not be representative.
We have developed a novel method of unsuper-
vised semantic role labelling that avoids the need
for expensive manual labelling of text, and enables
the use of a large, representative corpus. To achieve
this, we take a “bootstrapping” approach (e.g., Hin-
dle and Rooth, 1993; Yarowsky, 1995; Jones et al.,
1999), which initially makes only the role assign-
ments that are unambiguous according to a verb
lexicon. We then iteratively: create a probability
model based on the currently annotated semantic
roles, use this probability model to assign roles that
are deemed to have sufficient evidence, and add the
newly labelled arguments to our annotated set. As
we iterate, we gradually both grow the size of the
annotated set, and relax the evidence thresholds for
the probability model, until all arguments have been
assigned roles.
To our knowledge, this is the first unsupervised
semantic role labelling system applied to general
semantic roles in a domain-general corpus. In a
similar vein of work, Riloff and colleagues (Riloff
and Schmelzenbach, 1998; Jones et al., 1999) used
bootstrapping to learn “case frames” for verbs, but
their approach has been applied in very narrow topic
domains with topic-specific roles. In other work,
Gildea (2002) has explored unsupervised methods
to discover role-slot mappings for verbs, but not to
apply this knowledge to label text with roles.
Our approach also differs from earlier work in
its novel use of classes of information in back-
ing off to less specific role probabilities (in con-
trast to using simple subsets of information, as
in Gildea and Jurafsky, 2002). If warranted, we
base our decisions on the probability of a role
given the verb, the syntactic slot (syntactic ar-
gument position), and the noun occurring in that
slot. For example, the assignment to the first
argument of sentence (1) above may be based
on Experiencer subject . When
backing off from this probability, we use statistics
over more general classes of information, such as
conditioning over the semantic class of the verb in-
stead of the verb itself—for this example, psycho-
logical state verbs. Our approach yields a very
simple probability model which emphasizes class-
based generalizations.
The first step in our algorithm is to use the verb
</bodyText>
<listItem confidence="0.8856995">
1. Kiva admires Mats
2. Jo returned to London
</listItem>
<bodyText confidence="0.999917294117647">
lexicon to determine the argument slots and the
roles available for them. In Section 2, we discuss
the lexicon we use, and our initial steps of syntac-
tic frame matching and “unambiguous” role assign-
ment. This unambiguous data is leveraged by us-
ing those role assignments as the basis for the ini-
tial estimates for the probability model described in
Section 3. Section 4 presents the algorithm which
brings these two components together, iteratively
updating the probability estimates as more and more
data is labelled. In Section 5, we describe details of
the materials and methods used for the experiments
presented in Section 6. Our results show a large
improvement over an informed baseline. This kind
of unsupervised approach to role labelling is quite
new, and we conclude with a discussion of limita-
tions and on-going work in Section 7.
</bodyText>
<sectionHeader confidence="0.619854" genericHeader="method">
2 Determining Slots and Role Sets
</sectionHeader>
<bodyText confidence="0.999960733333333">
Previous work has divided the semantic role la-
belling task into the identification of the arguments
to be labelled, and the tagging of each argument
with a role (Gildea and Jurafsky, 2002; Fleischman
et al., 2003). Our algorithm addresses both these
steps. Also, the unsupervised nature of the approach
highlights an intermediate step of determining the
set of possible roles for each argument. Because we
need to constrain the role set as much as possible,
and cannot draw on extensive training data, this lat-
ter step takes on greater significance in our work.
We first describe the lexicon that specifies the
syntactic arguments and possible roles for the verbs,
and then discuss our process of argument and role
set identification.
</bodyText>
<subsectionHeader confidence="0.995878">
2.1 The Verb Lexicon
</subsectionHeader>
<bodyText confidence="0.983261666666667">
In semantic role labelling, a lexicon is used which
lists the possible roles for each syntactic argument
of each predicate. Supervised approaches to this
task have thus far used the predicate lexicon of
FrameNet, or the verb lexicon of PropBank, since
each has an associated labelled corpus for train-
ing. We instead make use of VerbNet (Kipper et al.,
2000), a manually developed hierarchical verb lexi-
con based on the verb classification of Levin (1993).
For each of 191 verb classes, including around 3000
verbs in total, VerbNet specifies the syntactic frames
along with the semantic role assigned to each slot
of a frame. Throughout the paper we use the term
“frame” to refer to a syntactic frame—the set of syn-
tactic arguments of a verb—possibly labelled with
roles, as exemplified in the VerbNet entry in Table 1.
While FrameNet uses semantic roles specific to
a particular situation (such as Speaker, Message,
</bodyText>
<figure confidence="0.5679486">
admire
Frames:
Experiencer V Cause
Experiencer V Cause Prep(in) Oblique
Experiencer V Oblique Prep(for) Cause
</figure>
<subsectionHeader confidence="0.168256">
Verbs in same (sub)class:
</subsectionHeader>
<bodyText confidence="0.235903">
[admire, adore, appreciate, cherish, enjoy, ...]
</bodyText>
<tableCaption confidence="0.996843">
Table 1: A portion of a VerbNet entry.
</tableCaption>
<bodyText confidence="0.997552">
Addressee), and PropBank uses roles specific to
a verb (such as Arg0, Arg1, Arg2), VerbNet uses
an intermediate level of thematic roles (such as
Agent, Theme, Recipient). These general thematic
roles are commonly assumed in linguistic theory,
and have some advantages in terms of capturing
commonalities of argument relations across a wide
range of predicates. It is worth noting that although
there are fewer of these thematic roles than the more
situation-specific roles of FrameNet, the role la-
belling task is not necessarily easier: there may be
more data per role, but possibly less discriminating
data, since each role applies to more general rela-
tions. (Indeed, in comparing the use of FrameNet
roles to general thematic roles, Gildea and Jurafsky
(2002) found very little difference in performance.)
</bodyText>
<subsectionHeader confidence="0.999475">
2.2 Frame Matching
</subsectionHeader>
<bodyText confidence="0.999934761904762">
We devise a frame matching procedure that uses
the verb lexicon to determine, for each instance of
a verb, the argument slots and their possible the-
matic roles. The potential argument slots are sub-
ject, object, indirect object, and PP-object, where
the latter is specialized by the individual preposi-
tion.1 Given chunked sentences with our verbs, the
frame matcher uses VerbNet both to restrict the list
of candidate roles for each slot, and to eliminate
some of the PP slots that are likely not arguments.
To initialize the candidate roles precisely, we only
choose roles from frames in the verb’s lexical en-
try (cf. Table 1) that are the best syntactic matches
with the chunker output. We align the slots of each
frame with the chunked slots, and compute the por-
tion %Frame of frame slots that can be mapped to a
chunked slot, and the portion %Chunks of chunked
slots that can be mapped to the frame. The score
for each frame is computed by %Frame %Chunks,
and only frames having the highest score contribute
candidate roles to the chunked slots. An example
</bodyText>
<footnote confidence="0.9919788">
&apos;As in VerbNet, we assume that when a verb takes a PP
argument, the slot receiving the thematic role from the verb
is the NP object of the preposition. Also, VerbNet has few
verbs that take sentence complements, and for now we do not
consider them.
</footnote>
<table confidence="0.98504">
Possible Frames for V Extracted Slots %Frame %Chunks Score
SUBJ OBJ POBJ
Agent V Agent 100 33 133
Agent V Theme Agent Theme 100 67 167
Instrument V Theme Instrument Theme 100 67 167
Agent V Theme P Instrument Agent Theme Instrument 100 100 200
Agent V Recipient Theme Agent Recipient 67 67 133
</table>
<tableCaption confidence="0.999771">
Table 2: An example of frame matching.
</tableCaption>
<bodyText confidence="0.999140533333333">
scoring is shown in Table 2.
This frame matching step is very restrictive and
greatly reduces potential role ambiguity. Many syn-
tactic slots receive only a single candidate role, pro-
viding the initial unambiguous data for our boot-
strapping algorithm. Some slots receive no can-
didate roles, which is an error for argument slots
but which is correct for adjuncts. The reduction of
candidate roles in general is very helpful in light-
ening the load on the probability model, but note
that it may also cause the correct role to be omit-
ted. In future work, we plan to explore other possi-
ble methods of selecting roles from the frames, such
as choosing candidates from all frames, or setting a
threshold value on the matching score.
</bodyText>
<sectionHeader confidence="0.997428" genericHeader="method">
3 The Probability Model
</sectionHeader>
<bodyText confidence="0.999984333333333">
Once slots are initialized as above, our algorithm
uses an iteratively updated probability model for
role labelling. The probability model predicts the
role for a slot given certain conditioning informa-
tion. We use a backoff approach with three levels
of specificity of probabilities. If a candidate role
fails to meet the threshold of evidence (counts to-
wards that probability) for a given level, we backoff
to the next level. For any given slot, we use the most
specific level that reaches the evidence threshold for
any of the candidates. We only use information at a
single level to compare candidates for a single slot.
We assume the probability of a role for a slot is
independent of other slots; we do not ensure a con-
sistent role assignment across an instance of a verb.
</bodyText>
<subsectionHeader confidence="0.999533">
3.1 The Backoff Levels
</subsectionHeader>
<bodyText confidence="0.983996102564102">
Our most specific probability uses the exact combi-
nation of verb, slot, and noun filling that slot, yield-
ing .2
2We use only the head noun of potential arguments, not the
full NP, in our probability model. Our combination of slot plus
head word provides similar information (head of argument and
its syntactic relation to the verb) to that captured by the features
of Gildea and Jurafsky (2002) or Thompson et al. (2003).
For our first backoff level, we introduce a novel
way to generalize over the verb, slot, and noun in-
formation of . Here we use a linear in-
terpolation of three probabilities, each of which: (1)
drops one source of conditioning information from
the most specific probability, and (2) generalizes
a second source of conditioning information to a
class-based conditioning event. Specifically, we use
the following probability formula:
where is slot class, is noun class, is verb
class, and the individual probabilities are (currently)
equally weighted (i.e., all ’s have a value of ).
Note that all three component probabilities make
use of the verb or its class information. In , the
noun component is dropped, and the slot is gener-
alized to the appropriate slot class. In , the slot
component is dropped, and the noun is generalized
to the appropriate noun class. Although it may seem
counterintuitive to drop the slot, this helps us cap-
ture generalizations over “alternations,” in which
the same semantic argument may appear in differ-
ent syntactic slots (as in The ice melted and The sun
melted the ice). In , again the noun component
is dropped, but in this case the verb is generalized
to the appropriate verb class. Each type of class is
described in the following subsection.
The last backoff level simply uses the probabil-
ity of the role given the slot class, . The
backoff model is summarized in Figure 1. We use
maximum likelihood estimates (MLE) for each of
the probability formulas.
</bodyText>
<subsectionHeader confidence="0.99994">
3.2 Classes of Information
</subsectionHeader>
<bodyText confidence="0.997846">
For slots, true generalization to a class only oc-
curs for the prepositional slots, all of which are
mapped to a single PP slot class. All other slots—
subject, object, and indirect object—each form their
own singleton slot class. Thus, differs from
by dropping the noun, and by treating
all prepositional slots as the same slot. This formula
allows us to generalize over a slot regardless of the
</bodyText>
<figureCaption confidence="0.999098">
Figure 1: The backoff model.
</figureCaption>
<bodyText confidence="0.999767470588235">
particular noun, and preposition if there is one, used
in the instance.
Classes of nouns in the model are given by the
WordNet hierarchy. Determining the appropriate
level of generalization for a noun is an open problem
(e.g., Clark and Weir, 2002). Currently, we use a cut
through WordNet including all the top categories,
except for the category “entity”; the latter, because
of its generality, is replaced in the cut by its imme-
diate children (Schulte im Walde, 2003). Given a
noun argument, all of its ancestors that appear in this
cut are used as the class(es) for the noun. (Credit
for a noun is apportioned equally across multiple
classes.) Unknown words placed in a separate cat-
egory. This yields a noun classification system that
is very coarse and that does not distinguish between
senses, but which is simple and computationally
feasible. thus captures consistent relations be-
tween a verb and a class of nouns, regardless of the
slot in which the noun occurs.
Verb classes have been shown to be very im-
portant in capturing generalizations across verb be-
haviour in computational systems (e.g., Palmer,
2000; Merlo and Stevenson, 2001). In semantic role
labelling using VerbNet, they are particularly rel-
evant since the classes are based on a commonal-
ity of role-labelled syntactic frames (Kipper et al.,
2000). The class of a verb in our model is its Verb-
Net class that is compatible with the current frame.
When multiple classes are compatible, we apportion
the counts uniformly among them. For probability
, then, we generalize over all verbs in a class of
the target verb, giving us much more extensive data
over relevant role assignments to a particular slot.
</bodyText>
<sectionHeader confidence="0.987986" genericHeader="method">
4 The Bootstrapping Algorithm
</sectionHeader>
<bodyText confidence="0.979700189189189">
We have described the frame matcher that produces
a set of slots with candidate role lists (some unam-
biguous), and our backoff probability model. All
that remains is to specify the parameters that guide
the iterative use of the probability model to assign
roles.
The evidence count for each of the conditional
probabilities refers to the number of times we have
observed the conjunction of its conditioning events.
For example, for , this is the number of
times the particular combination of verb, slot, and
noun have been observed. For a probability to be
used, its evidence count must reach a given thresh-
old, .
The “goodness” of a role assignment is deter-
mined by taking the log of the ratio between the
probabilities of the top two candidates for a slot
(when the evidence of both meet )
(e.g., Hindle and Rooth, 1993). A role is only
assigned if the log likelihood ratio is defined and
meets a threshold; in this case, the candidate role
with highest probability is assigned to the slot.
(Note that in the current implementation, we do not
allow re-labelling: an assigned label is fixed.) In
the algorithm, the log ratio threshold is initially set
high and gradually reduced until it reaches 0. In the
case of remaining ties, we assign the role for which
is highest.
Because our evidence count and log ratio restric-
tions may not be met even when we have a very
good candidate for a slot, we reduce the evidence
count threshold to the minimum value of 1 when
the log ratio threshold reaches 1.3 By this point, we
assume competitor candidates have been given suf-
ficient opportunity to amass the relevant counts.
Algorithm 1 shows the bootstrapping algorithm.
Algorithm 1 Bootstrapping Algorithm
</bodyText>
<subsectionHeader confidence="0.297759">
Frame Matching, Slot Initialization:
</subsectionHeader>
<bodyText confidence="0.728740222222222">
1: Perform Frame Matching to determine the slots to be la-
belled, along with their candidate lists of roles.
2: Let be the set of annotated slots; .
Let be the set of unannotated slots, initially all slots.
Let be the set of newly annotated slots; .
3: Add to each slot whose role assignment is
unambiguous—whose candidate list has one element.
Set to and set to (where and
remove/add elements of the second set from/to the first).
</bodyText>
<subsectionHeader confidence="0.366426">
Probability Model Application:
repeat
repeat
</subsectionHeader>
<bodyText confidence="0.5644733">
(Re)compute the probability model, using counts over
the items in .
Add to all slots in for which:
–at least two candidates meet the evidence count
threshold for a given probability level (see Figure 1);
and
–the log ratio between the two highest probability can-
didates meets the log ratio threshold.
For each slot in , assign the highest probability role.
Set to and set to .
</bodyText>
<subsectionHeader confidence="0.236626">
until
</subsectionHeader>
<bodyText confidence="0.394759">
Decrement the log ratio threshold.
</bodyText>
<footnote confidence="0.846266">
Adjust evidence count threshold if log ratio threshold is 1.
until log ratio threshold = 0
Resolve ties and terminate.
3We also allow cases in which the log ratio is undefined to
be assigned at this point—this occurs when only one of multi-
ple candidates has evidence.
</footnote>
<sectionHeader confidence="0.965032" genericHeader="method">
5 Materials and Methods
</sectionHeader>
<subsectionHeader confidence="0.992399">
5.1 Verbs, Verb Classes and Roles
</subsectionHeader>
<bodyText confidence="0.999956083333333">
For the initial set of experiments, we chose 54
target verbs from three top-level VerbNet classes:
preparing-26.3, transfer mesg-37.1, and contribute-
13.2. We looked for classes that contained a large
number of medium to high frequency verbs dis-
playing a variety of interesting properties, such as
having ambiguous (or unambiguous) semantic roles
given certain syntactic constructions, or having am-
biguous semantic role assignments that could (or al-
ternatively, could not) be distinguished by knowl-
edge of verb class.
From the set of target verbs, we derived an ex-
tended verb set that comprises all of the original
target verbs as well as any verb that shares a class
with one of those target verbs. This gives us a set
of 1159 verbs to observe in total, and increases the
likelihood that some verb class information is avail-
able for each of the possible classes of the target
verbs. Observing the entire extended set also pro-
vides more data for our probability estimators that
do not use verb class information.
We have made several changes to the semantic
roles as given by VerbNet. First, selectional re-
strictions such as [+Animate] are removed since our
coarse model of noun class does not allow us to re-
liably determine whether such restrictions are met.
Second, a few semantic distinctions that are made
in VerbNet appeared to be too fine-grained to cap-
ture, so we map these to a more coarse-grained sub-
set of the VerbNet roles. For instance, the role Ac-
tor is merged with Agent, and Patient with Theme.
We are left with a set of 16 roles: Agent, Amount,
Attribute, Beneficiary, Cause, Destination, Expe-
riencer, Instrument, Location, Material, Predicate,
Recipient, Source, Stimulus, Theme, Time. Of
these, 13 actually occur in our target verb classes.
</bodyText>
<subsectionHeader confidence="0.999321">
5.2 The Corpus and Preprocessing
</subsectionHeader>
<bodyText confidence="0.999989454545455">
Our corpus consists of a random selection of 20% of
the sentences in the British National Corpus (BNC
Reference Guide, 2000). This corpus is processed
by the chunker of Abney (1991), from whose out-
put we can identify the probable head words of verb
arguments with some degree of error. For instance,
distant subjects are often not found, and PPs identi-
fied as arguments are often adjuncts. To reduce the
number of adjuncts, we ignore dates and any PPs
that are not known to (possibly) introduce an argu-
ment to one of the verbs in our extended set.
</bodyText>
<subsectionHeader confidence="0.991018">
5.3 Validation and Test Data
</subsectionHeader>
<bodyText confidence="0.999901266666667">
We extracted two sets of sentences: a validation
set consisting of 5 random examples of each tar-
get verb, and a test set, consisting of 10 random
examples of each target verb. The data sets were
chunked as above, and the role for each potential
argument slot was labelled by two human annota-
tors, choosing from the simplified role set allowed
by each verb according to VerbNet. A slot could
also be labelled as an adjunct, or as “bad” (incor-
rectly chunked). Agreement between the two anno-
tators was high, yielding a kappa statistic of 0.83.
After performing the labelling task individually, the
annotators reconciled their responses (in consulta-
tion with a third annotator) to yield a set of human
judgements used for evaluation.
</bodyText>
<subsectionHeader confidence="0.997608">
5.4 Setting the Bootstrapping Parameters
</subsectionHeader>
<bodyText confidence="0.9999563125">
In our development experiments, we tried an evi-
dence count threshold of either the mean or me-
dian over all counts of a particular conjunction of
conditioning events. (For example, for ,
this is the mean or median count across all combi-
nations of verb, slot, and noun.) The more lenient
median setting worked slightly better on the valida-
tion set, and was retained for our test experiments.
We also experimented with initial starting values of
2, 3, and 8 for the log likelihood ratio threshold. An
initial setting of 8 showed an improvement in per-
formance, as lower values enabled too many early
role assignments, so we used the value of 8 in our
test experiments. In all experiments, a decrement of
.5 was used to gradually reduce the log likelihood
ratio threshold.
</bodyText>
<sectionHeader confidence="0.996628" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<bodyText confidence="0.998938823529412">
Of over 960K slots we extracted from the corpus,
120K occurred with one of 54 target verbs. Of these,
our validation data consisted of 278 slots, and our
test data of 554 slots. We focus on the analysis
of test data; the pattern on the validation data was
nearly identical in all respects.
The target slots fall into several categories, de-
pending on the human judgements: argument slots,
adjunct slots, and “bad” slots (chunking errors). We
report detailed analysis over the slots identified as
arguments. We also report overall accuracy if ad-
junct and “bad” slots are included in the slots to be
labelled. This comparison is similar to that made
by Gildea and Jurafsky (2002) and others, either us-
ing arguments as delimited in the FrameNet corpus,
or having to automatically locate argument bound-
aries.4 Furthermore, we report results over individ-
</bodyText>
<footnote confidence="0.952817">
4The comparison is not identical: in the case of manually
</footnote>
<bodyText confidence="0.90852">
ual slot classes (subject, object, indirect object, and
PP object), as well as over all slots.
</bodyText>
<subsectionHeader confidence="0.998875">
6.1 Evaluation Measures and Comparisons
</subsectionHeader>
<bodyText confidence="0.999993684210527">
We report results after the “unambiguous” data is
assigned, and at the end of the algorithm, when no
more slots can be labelled. At either of these steps it
is possible for some slots to have been assigned and
some to remain unassigned. Rather than performing
a simple precision/recall analysis, we report a finer
grained elaboration that gives a more precise pic-
ture of the results. For the assigned slots, we report
percent correct (of total, not of assigned) and per-
cent incorrect. For the unassigned slots, we report
percent “possible” (i.e., slots whose candidate list
contains the correct role) and percent “impossible”
(i.e., slots whose candidate list does not contain the
correct role—and which may in fact be empty). All
these percent figures are out of all argument slots
(for the first set of results), and out of all slots (for
the second set); see Table 3. Correctness is deter-
mined by the human judgements on the chunked
slots, as reported above.
Using our notion of slot class, we compare our re-
sults to a baseline that assigns all slots the role with
the highest probability for that slot class, .
When using general thematic roles, this is a more in-
formed baseline than , as used in other work.
We are using a very different verb lexicon, cor-
pus, and human standard than in previous research.
The closest work is that of Gildea and Jurafsky
(2002) which maps FrameNet roles to a set of 18
thematic roles very similar to our roles, and also
operates on a subset of the BNC (albeit manually
rather than randomly selected). We mention the per-
formance of their method where appropriate below.
However, our results are compared to human anno-
tation of chunked data, while theirs (and other su-
pervised results) are compared to manually anno-
tated full sentences. Our percentage correct values
therefore do not take into account argument con-
stituents that are simply missed by the chunker.
</bodyText>
<subsectionHeader confidence="0.983059">
6.2 Results on Argument Slots
</subsectionHeader>
<bodyText confidence="0.999919946428572">
Table 3 summarizes our results. In this section, we
focus on argument slots as identified by our human
judges (the first panel of results in the table). There
are a number of things to note. First, our perfor-
mance on these slots is very high, 90.1% correct at
the end of the algorithm, with 7.0% incorrect, and
delimited arguments, others train, as well as test, only on such
arguments. In our approach, all previously annotated slots are
used in the iterative training of the probability model. Thus,
even when we report results on argument slots only, adjunct
and “bad” slots may have induced errors in their labelling.
only 2.9% left unassigned. (The latter have null
candidate lists.) This is a 56% reduction in error
rate over the baseline. Second, we see that even
after the initial unambiguous role assignment step,
the algorithm achieves close to the baseline percent
correct. Furthermore, over 96% of the initially as-
signed roles are correct. This means that much of
the work in narrowing down the candidate lists is
actually being preformed during frame matching. It
is noteworthy that such a simple method of choosing
the initial candidates can be so useful, and it would
seem that even supervised methods might benefit
from employing such an explicit use of the lexicon
to narrow down role candidates for a slot.
After unambiguous role assignment, about 21%
of the test data remains unassigned (116 slots). Of
these 116 slots, 100 have a non-null candidate list.
These 100 are assigned by our iterative probabil-
ity model, so we are especially interested in the re-
sults on them. We find that 76 of these 100 are as-
signed correctly (accounting for the 13.7% increase
to 90.1%), and 24 are assigned incorrectly, yielding
a 76% accuracy for the probability model portion of
our algorithm on identified argument slots.
Moreover, we also find that all specificity lev-
els of the probability model (see Figure 1) are em-
ployed in making these decisions—about a third
of the decisions are made by each level. This in-
dicates that while there is sufficient data in many
cases to warrant using the exact probability for-
mula , the class-based generalizations
we propose prove to be very useful to the algorithm.
As a point of comparison, the supervised method
of Gildea and Jurafsky (2002) achieved 82.1% ac-
curacy on identified arguments using general the-
matic roles. However, they had a larger and more
varied target set, consisting of 1462 predicates from
67 FrameNet frames (classes), which makes their
task harder than ours. We are aware that our test
set is small compared to supervised approaches,
which have a large amount of labelled data avail-
able. However, our almost identical results across
the validation and test sets indicates consistent be-
haviour that may generalize to a larger test set, at
least on similar classes of verbs.
</bodyText>
<subsectionHeader confidence="0.998568">
6.3 Differences Among Slot Classes
</subsectionHeader>
<bodyText confidence="0.999814285714286">
When using general thematic roles with a small set
of verb classes, the probability used for the base-
line, , works very well for subjects and ob-
jects (which are primarily Agents and Themes, re-
spectively, for our verbs). Indeed, when we exam-
ine each of the slot classes individually, we find
that, for subjects and objects, the percent correct
</bodyText>
<table confidence="0.998950142857143">
Role Assignments Identified Arguments All Target Slots
Algorithm Algorithm
Baseline “Unambig” Final Baseline “Unambig” Final
Assigned Correct 77.3 76.4 90.1 63.7 75.9 87.2
Incorrect 22.7 2.7 7.0 36.3 6.8 10.4
Unassigned Possible 0 17.1 0 0 14.1 0
Impossible 0 3.8 2.9 0 3.1 2.4
</table>
<tableCaption confidence="0.939715">
Table 3: Evaluation of test data on 554 identified arguments (see Section 6.2) and on all 672 target slots (see
Section 6.4).
</tableCaption>
<bodyText confidence="0.9984829">
achieved by the algorithm is indistinguishable from
the baseline (both are around 93%, for both sub-
jects and objects). For PP objects, on the other
hand, the baseline is only around 11% correct, while
we achieve 78.5% correct, a 76% reduction in error
rate. Clearly, when more roles are available, even
becomes a weak predictor.5
We could just assign the default role for sub-
jects and objects when using general thematic roles,
but we think this is too simplistic. First, when we
broaden our range of verb classes, subjects and ob-
jects will have more possible roles. As we have
seen with PPs, when more roles are available, the
performance of a default role degrades. Second, al-
though we achieve the same correctness as the base-
line, our algorithm does not simply assign the domi-
nant role in these cases. Some subjects are assigned
Theme, while some objects are assigned Recipient
or Source. These roles would never be possible in
these slots if a default assignment were followed.
</bodyText>
<subsectionHeader confidence="0.758831">
6.4 Results Including All Target Slots
</subsectionHeader>
<bodyText confidence="0.999963470588235">
We also consider our performance given frame
matching and chunking errors, which can lead to
adjuncts or even “bad” constituents being labelled.
Only arguments should be labelled, while non-
arguments should remain unlabelled. Of 98 slots
judged to be adjuncts, 19 erroneously are given la-
bels. Including the adjunct slots, our percent cor-
rect goes from 90.1% to 88.7%. Of the 20 “bad”
slots, 12 were labelled. Including these, correctness
is reduced slightly further, to 87.2%, as shown in
the second panel of results in Table 3. The error
rate reduction here of 65% is higher than on argu-
ments only, because the baseline always labels (in
error) adjuncts and “bad” slots. (Gildea and Ju-
rafsky (2002) achieved 63.6% accuracy when hav-
ing to identify arguments for thematic roles, though
note again that this is on a much larger and more
</bodyText>
<footnote confidence="0.786498666666667">
5Due to the rarity of indirect object slots in the chunker out-
put, the test data included no such slots. The validation set
included one, which the algorithm correctly labelled.
</footnote>
<bodyText confidence="0.9996586">
general test set. Also, although we take into account
errors on identified chunks that are not arguments,
we are are not counting chunker errors of missing
arguments.)
As others have shown (Gildea and Palmer, 2002),
semantic role labelling is more accurate with better
preprocessing of the data. However, we also think
our algorithm may be extendable to deal with many
of the adjunct cases we observed. Often, adjuncts
express time or location; while not argument roles,
these do express generalizable semantic relations.
In future work, we plan to explore the notion of
expanding our frame matching step to go beyond
VerbNet by initializing potential adjuncts with ap-
propriate roles.
</bodyText>
<sectionHeader confidence="0.997272" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999974882352941">
Using an unsupervised algorithm for semantic role
labelling, we have achieved 90% correct on identi-
fied arguments, well over an informed baseline of
77%, and have achieved 87% correct on all slots
(64% baseline). On PP objects, our conservative
role assignment shows promise at leaving adjuncts
unlabelled. However, PP objects also have the low-
est performance (of 78% correct on identified argu-
ments, compared to 93% for subjects or objects).
More work is required on our frame matching ap-
proach to determine appropriate roles for PP objects
given the specification in the lexicon, which (in the
case of VerbNet) often over-constrains the allowable
prepositions for a slot.
Although these results are promising, they are
only a first step in demonstrating the potential of the
approach. We need to test more verbs, from a wider
variety of verb classes (or even a different kind of
predicate classification, such as FrameNet), to de-
termine the generalizability of our findings. Using
FrameNet would also have the advantage of provid-
ing large amounts of labelled test data for our eval-
uation. We also hope to integrate some processing
of adjunct roles, rather than limiting ourselves to the
specified arguments.
A unique aspect of our method is the probabil-
ity model, which is novel in its generalizations over
verb, slot, and noun classes for role labelling. How-
ever, these have room for improvement—our noun
classes are coarse, and prepositions clearly have the
potential to be divided into more informative sub-
classes, such as spatial or time relations. Our on-
going work is investigating better class models to
make the backoff process even more effective.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999838">
We gratefully acknowledge the support of NSERC
of Canada. We also thank Martha Palmer for pro-
viding us with the VerbNet data, Eric Joanis for help
with the chunker, Vivian Tsang and Ryan North for
helpful discussion, and two anonymous reviewers.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999914530120482">
S. Abney. 1991. Parsing by chunks. In R. Berwick,
S. Abney, and C. Tenny, editors, Principle-Based
Parsing. Kluwer Academic Publishers.
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings
of COLING-ACL.
BNC Reference Guide. 2000. Reference Guide
for the British National Corpus (World Edition).
http://www.hcu.ox.ac.uk/BNC, second edition.
J. Chen and O. Rambow. 2003. Use of deep linguis-
tic features for the recognition and labeling of se-
mantic arguments. In Proc. of the Conf. on Em-
pirical Methods in Natural Language Processing.
S. Clark and D. Weir. 2002. Probability estimation
using a semantic hierarchy. Computational Lin-
guistics, 28(2):187–206.
M. Fleischman, N. Kwon, and E. Hovy. 2003. Max-
imum entropy models for FrameNet classifica-
tion. In Proc. of the Conf. on Empirical Methods
in Natural Language Processing.
D. Gildea. 2002. Probabilistic models of verb-
argument structure. In Proc. of the 19th Interna-
tional Conference on Computational Linguistics
(COLING-02), p. 308–314.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
23(3):245–288.
D. Gildea and M. Palmer. 2002. The necessity of
syntactic parsing for predicate argument recogni-
tion. In Proc. of the 40th Annual Conf. of the As-
soc. for Computational Linguistics, p. 239–246.
K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and
D. Jurafsky. 2003. Semantic role labeling by tag-
ging syntactic chunks. In Proc. of the 8th Conf.
on Computational Natural Language Learning.
D. Hindle and M. Rooth. 1993. Structural ambiguity
and lexical relations. Computational Linguistics,
19(1):103–120.
R. Jones, A. McCallum, K. Nigam, and E. Riloff.
1999. Bootstrapping for text learning tasks. In
IJCAI-99 Workshop on Text Mining: Founda-
tions, Techniques and Applications.
K. Kipper, H. T. Dang, and M. Palmer. 2000. Class
based construction of a verb lexicon. In Proc. of
the 17th National Conference on Artificial Intel-
ligence (AAAI-2000).
B. Levin. 1993. English Verb Classes and Alterna-
tions: A Preliminary Investigation. University of
Chicago Press.
P. Merlo and S. Stevenson. 2001. Automatic verb
classification based on statistical distributions of
argument structure. Computational Linguistics,
27(3):373–408.
Y. Niu and G. Hirst. 2004. Analysis of semantic
classes in medical text for question answering. In
Workshop on Question Answering in Restricted
Domains, 42nd Annual Meeting of the Assoc. for
Computational Linguistics.
M. Palmer. 2000. Consistent criteria for sense dis-
tinctions. Special Issue of Computers and the Hu-
manities, SENSEVAL98: Evaluating Word Sense
Disambiguation Systems, 34(1–2).
M. Palmer, D. Gildea, and P. Kingsbury. 2003. The
Proposition Bank: An annotated corpus of se-
mantic roles. Submitted to Computational Lin-
guistics.
E. Riloff and M. Schmelzenbach. 1998. An empir-
ical approach to conceptual case frame acquisi-
tion. In Proc. of the 6th Workshop on Very Large
Corpora.
S. Schulte im Walde. 2003. Experiments on the
choice of features for learning verb classes. In
Proc. of the 10th Conf. of the European Chapter
of the Assoc. for Computational Linguistics.
C. Thompson, R. Levy, and C. Manning. 2003. A
generative model for FrameNet semantic role la-
beling. In Proc. of the Fourteenth European Conf.
on Machine Learning (ECML-03).
D. Yarowsky. 1995. Unsupervised word sense dis-
ambiguation methods rivaling supervised meth-
ods”. In Proceedings of the 33rd Annual Meeting
of the Association for Computational Linguistics,
p. 189–196.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945700">
<title confidence="0.999826">Unsupervised Semantic Role Labelling</title>
<author confidence="0.999869">S Swier</author>
<affiliation confidence="0.999799">Department of Computer University of</affiliation>
<address confidence="0.977506">Toronto, Ontario, Canada M5S</address>
<email confidence="0.994568">swier,suzanne@cs.toronto.edu</email>
<abstract confidence="0.997987357142857">We present an unsupervised method for labelling the arguments of verbs with their semantic roles. Our bootstrapping algorithm makes initial unambiguous role assignments, and then iteratively updates the probability model on which future assignments are based. A novel aspect of our approach is the use of verb, slot, and noun class information as the basis for backing off in our probability model. We achieve 50–65% reduction in the error rate over an informed baseline, indicating the potential of our approach for a task that has heretofore relied on large amounts of manually generated training data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by chunks. In</title>
<date>1991</date>
<editor>R. Berwick, S. Abney, and C. Tenny, editors, Principle-Based Parsing.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="19816" citStr="Abney (1991)" startWordPosition="3319" endWordPosition="3320">re, so we map these to a more coarse-grained subset of the VerbNet roles. For instance, the role Actor is merged with Agent, and Patient with Theme. We are left with a set of 16 roles: Agent, Amount, Attribute, Beneficiary, Cause, Destination, Experiencer, Instrument, Location, Material, Predicate, Recipient, Source, Stimulus, Theme, Time. Of these, 13 actually occur in our target verb classes. 5.2 The Corpus and Preprocessing Our corpus consists of a random selection of 20% of the sentences in the British National Corpus (BNC Reference Guide, 2000). This corpus is processed by the chunker of Abney (1991), from whose output we can identify the probable head words of verb arguments with some degree of error. For instance, distant subjects are often not found, and PPs identified as arguments are often adjuncts. To reduce the number of adjuncts, we ignore dates and any PPs that are not known to (possibly) introduce an argument to one of the verbs in our extended set. 5.3 Validation and Test Data We extracted two sets of sentences: a validation set consisting of 5 random examples of each target verb, and a test set, consisting of 10 random examples of each target verb. The data sets were chunked a</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>S. Abney. 1991. Parsing by chunks. In R. Berwick, S. Abney, and C. Tenny, editors, Principle-Based Parsing. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C F Baker</author>
<author>C J Fillmore</author>
<author>J B Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL.</booktitle>
<contexts>
<context position="1583" citStr="Baker et al., 1998" startWordPosition="246" endWordPosition="249">bach, 1998; Niu and Hirst, 2004). In particular, labelling the semantic roles of the arguments of a verb (or any predicate), as in (1) and (2), provides crucial information about the relations among event participants. Because of the importance of this task, a number of recent methods have been proposed for automatic semantic role labelling (e.g., Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Fleischman et al., 2003; Hacioglu et al., 2003; Thompson et al., 2003). These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2003) as training data, which are expensive to produce, are limited in size, and may not be representative. We have developed a novel method of unsupervised semantic role labelling that avoids the need for expensive manual labelling of text, and enables the use of a large, representative corpus. To achieve this, we take a “bootstrapping” approach (e.g., Hindle and Rooth, 1993; Yarowsky, 1995; Jones et al., 1999), which initially makes only the role assignments that are unambiguous according to a verb lexicon. We then iteratively: create a probability model based on</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of COLING-ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>BNC Reference Guide</author>
</authors>
<title>Reference Guide for the British National Corpus (World Edition).</title>
<date>2000</date>
<note>http://www.hcu.ox.ac.uk/BNC, second edition.</note>
<contexts>
<context position="19759" citStr="Guide, 2000" startWordPosition="3309" endWordPosition="3310"> made in VerbNet appeared to be too fine-grained to capture, so we map these to a more coarse-grained subset of the VerbNet roles. For instance, the role Actor is merged with Agent, and Patient with Theme. We are left with a set of 16 roles: Agent, Amount, Attribute, Beneficiary, Cause, Destination, Experiencer, Instrument, Location, Material, Predicate, Recipient, Source, Stimulus, Theme, Time. Of these, 13 actually occur in our target verb classes. 5.2 The Corpus and Preprocessing Our corpus consists of a random selection of 20% of the sentences in the British National Corpus (BNC Reference Guide, 2000). This corpus is processed by the chunker of Abney (1991), from whose output we can identify the probable head words of verb arguments with some degree of error. For instance, distant subjects are often not found, and PPs identified as arguments are often adjuncts. To reduce the number of adjuncts, we ignore dates and any PPs that are not known to (possibly) introduce an argument to one of the verbs in our extended set. 5.3 Validation and Test Data We extracted two sets of sentences: a validation set consisting of 5 random examples of each target verb, and a test set, consisting of 10 random e</context>
</contexts>
<marker>Guide, 2000</marker>
<rawString>BNC Reference Guide. 2000. Reference Guide for the British National Corpus (World Edition). http://www.hcu.ox.ac.uk/BNC, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chen</author>
<author>O Rambow</author>
</authors>
<title>Use of deep linguistic features for the recognition and labeling of semantic arguments.</title>
<date>2003</date>
<booktitle>In Proc. of the Conf. on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1387" citStr="Chen and Rambow, 2003" startWordPosition="214" endWordPosition="217">s of manually generated training data. 1 Introduction Semantic annotation of text corpora is needed to support tasks such as information extraction and question-answering (e.g., Riloff and Schmelzenbach, 1998; Niu and Hirst, 2004). In particular, labelling the semantic roles of the arguments of a verb (or any predicate), as in (1) and (2), provides crucial information about the relations among event participants. Because of the importance of this task, a number of recent methods have been proposed for automatic semantic role labelling (e.g., Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Fleischman et al., 2003; Hacioglu et al., 2003; Thompson et al., 2003). These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2003) as training data, which are expensive to produce, are limited in size, and may not be representative. We have developed a novel method of unsupervised semantic role labelling that avoids the need for expensive manual labelling of text, and enables the use of a large, representative corpus. To achieve this, we take a “bootstrapping” approach (e.g., Hindle and Rooth, 1</context>
</contexts>
<marker>Chen, Rambow, 2003</marker>
<rawString>J. Chen and O. Rambow. 2003. Use of deep linguistic features for the recognition and labeling of semantic arguments. In Proc. of the Conf. on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>D Weir</author>
</authors>
<title>Probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="13433" citStr="Clark and Weir, 2002" startWordPosition="2235" endWordPosition="2238">s for the prepositional slots, all of which are mapped to a single PP slot class. All other slots— subject, object, and indirect object—each form their own singleton slot class. Thus, differs from by dropping the noun, and by treating all prepositional slots as the same slot. This formula allows us to generalize over a slot regardless of the Figure 1: The backoff model. particular noun, and preposition if there is one, used in the instance. Classes of nouns in the model are given by the WordNet hierarchy. Determining the appropriate level of generalization for a noun is an open problem (e.g., Clark and Weir, 2002). Currently, we use a cut through WordNet including all the top categories, except for the category “entity”; the latter, because of its generality, is replaced in the cut by its immediate children (Schulte im Walde, 2003). Given a noun argument, all of its ancestors that appear in this cut are used as the class(es) for the noun. (Credit for a noun is apportioned equally across multiple classes.) Unknown words placed in a separate category. This yields a noun classification system that is very coarse and that does not distinguish between senses, but which is simple and computationally feasible</context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>S. Clark and D. Weir. 2002. Probability estimation using a semantic hierarchy. Computational Linguistics, 28(2):187–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Fleischman</author>
<author>N Kwon</author>
<author>E Hovy</author>
</authors>
<title>Maximum entropy models for FrameNet classification.</title>
<date>2003</date>
<booktitle>In Proc. of the Conf. on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1412" citStr="Fleischman et al., 2003" startWordPosition="218" endWordPosition="221"> training data. 1 Introduction Semantic annotation of text corpora is needed to support tasks such as information extraction and question-answering (e.g., Riloff and Schmelzenbach, 1998; Niu and Hirst, 2004). In particular, labelling the semantic roles of the arguments of a verb (or any predicate), as in (1) and (2), provides crucial information about the relations among event participants. Because of the importance of this task, a number of recent methods have been proposed for automatic semantic role labelling (e.g., Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Fleischman et al., 2003; Hacioglu et al., 2003; Thompson et al., 2003). These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2003) as training data, which are expensive to produce, are limited in size, and may not be representative. We have developed a novel method of unsupervised semantic role labelling that avoids the need for expensive manual labelling of text, and enables the use of a large, representative corpus. To achieve this, we take a “bootstrapping” approach (e.g., Hindle and Rooth, 1993; Yarowsky, 1995; Jone</context>
<context position="5115" citStr="Fleischman et al., 2003" startWordPosition="828" endWordPosition="831">ability estimates as more and more data is labelled. In Section 5, we describe details of the materials and methods used for the experiments presented in Section 6. Our results show a large improvement over an informed baseline. This kind of unsupervised approach to role labelling is quite new, and we conclude with a discussion of limitations and on-going work in Section 7. 2 Determining Slots and Role Sets Previous work has divided the semantic role labelling task into the identification of the arguments to be labelled, and the tagging of each argument with a role (Gildea and Jurafsky, 2002; Fleischman et al., 2003). Our algorithm addresses both these steps. Also, the unsupervised nature of the approach highlights an intermediate step of determining the set of possible roles for each argument. Because we need to constrain the role set as much as possible, and cannot draw on extensive training data, this latter step takes on greater significance in our work. We first describe the lexicon that specifies the syntactic arguments and possible roles for the verbs, and then discuss our process of argument and role set identification. 2.1 The Verb Lexicon In semantic role labelling, a lexicon is used which lists</context>
</contexts>
<marker>Fleischman, Kwon, Hovy, 2003</marker>
<rawString>M. Fleischman, N. Kwon, and E. Hovy. 2003. Maximum entropy models for FrameNet classification. In Proc. of the Conf. on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
</authors>
<title>Probabilistic models of verbargument structure.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th International Conference on Computational Linguistics (COLING-02),</booktitle>
<pages>308--314</pages>
<contexts>
<context position="2964" citStr="Gildea (2002)" startWordPosition="470" endWordPosition="471"> our annotated set. As we iterate, we gradually both grow the size of the annotated set, and relax the evidence thresholds for the probability model, until all arguments have been assigned roles. To our knowledge, this is the first unsupervised semantic role labelling system applied to general semantic roles in a domain-general corpus. In a similar vein of work, Riloff and colleagues (Riloff and Schmelzenbach, 1998; Jones et al., 1999) used bootstrapping to learn “case frames” for verbs, but their approach has been applied in very narrow topic domains with topic-specific roles. In other work, Gildea (2002) has explored unsupervised methods to discover role-slot mappings for verbs, but not to apply this knowledge to label text with roles. Our approach also differs from earlier work in its novel use of classes of information in backing off to less specific role probabilities (in contrast to using simple subsets of information, as in Gildea and Jurafsky, 2002). If warranted, we base our decisions on the probability of a role given the verb, the syntactic slot (syntactic argument position), and the noun occurring in that slot. For example, the assignment to the first argument of sentence (1) above </context>
</contexts>
<marker>Gildea, 2002</marker>
<rawString>D. Gildea. 2002. Probabilistic models of verbargument structure. In Proc. of the 19th International Conference on Computational Linguistics (COLING-02), p. 308–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1339" citStr="Gildea and Jurafsky, 2002" startWordPosition="206" endWordPosition="209">or a task that has heretofore relied on large amounts of manually generated training data. 1 Introduction Semantic annotation of text corpora is needed to support tasks such as information extraction and question-answering (e.g., Riloff and Schmelzenbach, 1998; Niu and Hirst, 2004). In particular, labelling the semantic roles of the arguments of a verb (or any predicate), as in (1) and (2), provides crucial information about the relations among event participants. Because of the importance of this task, a number of recent methods have been proposed for automatic semantic role labelling (e.g., Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Fleischman et al., 2003; Hacioglu et al., 2003; Thompson et al., 2003). These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2003) as training data, which are expensive to produce, are limited in size, and may not be representative. We have developed a novel method of unsupervised semantic role labelling that avoids the need for expensive manual labelling of text, and enables the use of a large, representative corpus. To achieve this, we take a “bo</context>
<context position="3322" citStr="Gildea and Jurafsky, 2002" startWordPosition="528" endWordPosition="531">milar vein of work, Riloff and colleagues (Riloff and Schmelzenbach, 1998; Jones et al., 1999) used bootstrapping to learn “case frames” for verbs, but their approach has been applied in very narrow topic domains with topic-specific roles. In other work, Gildea (2002) has explored unsupervised methods to discover role-slot mappings for verbs, but not to apply this knowledge to label text with roles. Our approach also differs from earlier work in its novel use of classes of information in backing off to less specific role probabilities (in contrast to using simple subsets of information, as in Gildea and Jurafsky, 2002). If warranted, we base our decisions on the probability of a role given the verb, the syntactic slot (syntactic argument position), and the noun occurring in that slot. For example, the assignment to the first argument of sentence (1) above may be based on Experiencer subject . When backing off from this probability, we use statistics over more general classes of information, such as conditioning over the semantic class of the verb instead of the verb itself—for this example, psychological state verbs. Our approach yields a very simple probability model which emphasizes classbased generalizat</context>
<context position="5089" citStr="Gildea and Jurafsky, 2002" startWordPosition="824" endWordPosition="827">eratively updating the probability estimates as more and more data is labelled. In Section 5, we describe details of the materials and methods used for the experiments presented in Section 6. Our results show a large improvement over an informed baseline. This kind of unsupervised approach to role labelling is quite new, and we conclude with a discussion of limitations and on-going work in Section 7. 2 Determining Slots and Role Sets Previous work has divided the semantic role labelling task into the identification of the arguments to be labelled, and the tagging of each argument with a role (Gildea and Jurafsky, 2002; Fleischman et al., 2003). Our algorithm addresses both these steps. Also, the unsupervised nature of the approach highlights an intermediate step of determining the set of possible roles for each argument. Because we need to constrain the role set as much as possible, and cannot draw on extensive training data, this latter step takes on greater significance in our work. We first describe the lexicon that specifies the syntactic arguments and possible roles for the verbs, and then discuss our process of argument and role set identification. 2.1 The Verb Lexicon In semantic role labelling, a l</context>
<context position="7560" citStr="Gildea and Jurafsky (2002)" startWordPosition="1222" endWordPosition="1225"> (such as Agent, Theme, Recipient). These general thematic roles are commonly assumed in linguistic theory, and have some advantages in terms of capturing commonalities of argument relations across a wide range of predicates. It is worth noting that although there are fewer of these thematic roles than the more situation-specific roles of FrameNet, the role labelling task is not necessarily easier: there may be more data per role, but possibly less discriminating data, since each role applies to more general relations. (Indeed, in comparing the use of FrameNet roles to general thematic roles, Gildea and Jurafsky (2002) found very little difference in performance.) 2.2 Frame Matching We devise a frame matching procedure that uses the verb lexicon to determine, for each instance of a verb, the argument slots and their possible thematic roles. The potential argument slots are subject, object, indirect object, and PP-object, where the latter is specialized by the individual preposition.1 Given chunked sentences with our verbs, the frame matcher uses VerbNet both to restrict the list of candidate roles for each slot, and to eliminate some of the PP slots that are likely not arguments. To initialize the candidate</context>
<context position="11214" citStr="Gildea and Jurafsky (2002)" startWordPosition="1858" endWordPosition="1861"> single level to compare candidates for a single slot. We assume the probability of a role for a slot is independent of other slots; we do not ensure a consistent role assignment across an instance of a verb. 3.1 The Backoff Levels Our most specific probability uses the exact combination of verb, slot, and noun filling that slot, yielding .2 2We use only the head noun of potential arguments, not the full NP, in our probability model. Our combination of slot plus head word provides similar information (head of argument and its syntactic relation to the verb) to that captured by the features of Gildea and Jurafsky (2002) or Thompson et al. (2003). For our first backoff level, we introduce a novel way to generalize over the verb, slot, and noun information of . Here we use a linear interpolation of three probabilities, each of which: (1) drops one source of conditioning information from the most specific probability, and (2) generalizes a second source of conditioning information to a class-based conditioning event. Specifically, we use the following probability formula: where is slot class, is noun class, is verb class, and the individual probabilities are (currently) equally weighted (i.e., all ’s have a val</context>
<context position="22450" citStr="Gildea and Jurafsky (2002)" startWordPosition="3771" endWordPosition="3774">20K occurred with one of 54 target verbs. Of these, our validation data consisted of 278 slots, and our test data of 554 slots. We focus on the analysis of test data; the pattern on the validation data was nearly identical in all respects. The target slots fall into several categories, depending on the human judgements: argument slots, adjunct slots, and “bad” slots (chunking errors). We report detailed analysis over the slots identified as arguments. We also report overall accuracy if adjunct and “bad” slots are included in the slots to be labelled. This comparison is similar to that made by Gildea and Jurafsky (2002) and others, either using arguments as delimited in the FrameNet corpus, or having to automatically locate argument boundaries.4 Furthermore, we report results over individ4The comparison is not identical: in the case of manually ual slot classes (subject, object, indirect object, and PP object), as well as over all slots. 6.1 Evaluation Measures and Comparisons We report results after the “unambiguous” data is assigned, and at the end of the algorithm, when no more slots can be labelled. At either of these steps it is possible for some slots to have been assigned and some to remain unassigned</context>
<context position="24180" citStr="Gildea and Jurafsky (2002)" startWordPosition="4066" endWordPosition="4069">rcent figures are out of all argument slots (for the first set of results), and out of all slots (for the second set); see Table 3. Correctness is determined by the human judgements on the chunked slots, as reported above. Using our notion of slot class, we compare our results to a baseline that assigns all slots the role with the highest probability for that slot class, . When using general thematic roles, this is a more informed baseline than , as used in other work. We are using a very different verb lexicon, corpus, and human standard than in previous research. The closest work is that of Gildea and Jurafsky (2002) which maps FrameNet roles to a set of 18 thematic roles very similar to our roles, and also operates on a subset of the BNC (albeit manually rather than randomly selected). We mention the performance of their method where appropriate below. However, our results are compared to human annotation of chunked data, while theirs (and other supervised results) are compared to manually annotated full sentences. Our percentage correct values therefore do not take into account argument constituents that are simply missed by the chunker. 6.2 Results on Argument Slots Table 3 summarizes our results. In t</context>
<context position="27031" citStr="Gildea and Jurafsky (2002)" startWordPosition="4548" endWordPosition="4551"> 13.7% increase to 90.1%), and 24 are assigned incorrectly, yielding a 76% accuracy for the probability model portion of our algorithm on identified argument slots. Moreover, we also find that all specificity levels of the probability model (see Figure 1) are employed in making these decisions—about a third of the decisions are made by each level. This indicates that while there is sufficient data in many cases to warrant using the exact probability formula , the class-based generalizations we propose prove to be very useful to the algorithm. As a point of comparison, the supervised method of Gildea and Jurafsky (2002) achieved 82.1% accuracy on identified arguments using general thematic roles. However, they had a larger and more varied target set, consisting of 1462 predicates from 67 FrameNet frames (classes), which makes their task harder than ours. We are aware that our test set is small compared to supervised approaches, which have a large amount of labelled data available. However, our almost identical results across the validation and test sets indicates consistent behaviour that may generalize to a larger test set, at least on similar classes of verbs. 6.3 Differences Among Slot Classes When using </context>
<context position="30087" citStr="Gildea and Jurafsky (2002)" startWordPosition="5061" endWordPosition="5065">, which can lead to adjuncts or even “bad” constituents being labelled. Only arguments should be labelled, while nonarguments should remain unlabelled. Of 98 slots judged to be adjuncts, 19 erroneously are given labels. Including the adjunct slots, our percent correct goes from 90.1% to 88.7%. Of the 20 “bad” slots, 12 were labelled. Including these, correctness is reduced slightly further, to 87.2%, as shown in the second panel of results in Table 3. The error rate reduction here of 65% is higher than on arguments only, because the baseline always labels (in error) adjuncts and “bad” slots. (Gildea and Jurafsky (2002) achieved 63.6% accuracy when having to identify arguments for thematic roles, though note again that this is on a much larger and more 5Due to the rarity of indirect object slots in the chunker output, the test data included no such slots. The validation set included one, which the algorithm correctly labelled. general test set. Also, although we take into account errors on identified chunks that are not arguments, we are are not counting chunker errors of missing arguments.) As others have shown (Gildea and Palmer, 2002), semantic role labelling is more accurate with better preprocessing of </context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>D. Gildea and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 23(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>M Palmer</author>
</authors>
<title>The necessity of syntactic parsing for predicate argument recognition.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Conf. of the Assoc. for Computational Linguistics,</booktitle>
<pages>239--246</pages>
<contexts>
<context position="1364" citStr="Gildea and Palmer, 2002" startWordPosition="210" endWordPosition="213">re relied on large amounts of manually generated training data. 1 Introduction Semantic annotation of text corpora is needed to support tasks such as information extraction and question-answering (e.g., Riloff and Schmelzenbach, 1998; Niu and Hirst, 2004). In particular, labelling the semantic roles of the arguments of a verb (or any predicate), as in (1) and (2), provides crucial information about the relations among event participants. Because of the importance of this task, a number of recent methods have been proposed for automatic semantic role labelling (e.g., Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Fleischman et al., 2003; Hacioglu et al., 2003; Thompson et al., 2003). These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2003) as training data, which are expensive to produce, are limited in size, and may not be representative. We have developed a novel method of unsupervised semantic role labelling that avoids the need for expensive manual labelling of text, and enables the use of a large, representative corpus. To achieve this, we take a “bootstrapping” approach (e.</context>
<context position="30615" citStr="Gildea and Palmer, 2002" startWordPosition="5151" endWordPosition="5154">ause the baseline always labels (in error) adjuncts and “bad” slots. (Gildea and Jurafsky (2002) achieved 63.6% accuracy when having to identify arguments for thematic roles, though note again that this is on a much larger and more 5Due to the rarity of indirect object slots in the chunker output, the test data included no such slots. The validation set included one, which the algorithm correctly labelled. general test set. Also, although we take into account errors on identified chunks that are not arguments, we are are not counting chunker errors of missing arguments.) As others have shown (Gildea and Palmer, 2002), semantic role labelling is more accurate with better preprocessing of the data. However, we also think our algorithm may be extendable to deal with many of the adjunct cases we observed. Often, adjuncts express time or location; while not argument roles, these do express generalizable semantic relations. In future work, we plan to explore the notion of expanding our frame matching step to go beyond VerbNet by initializing potential adjuncts with appropriate roles. 7 Conclusions and Future Work Using an unsupervised algorithm for semantic role labelling, we have achieved 90% correct on identi</context>
</contexts>
<marker>Gildea, Palmer, 2002</marker>
<rawString>D. Gildea and M. Palmer. 2002. The necessity of syntactic parsing for predicate argument recognition. In Proc. of the 40th Annual Conf. of the Assoc. for Computational Linguistics, p. 239–246.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hacioglu</author>
<author>S Pradhan</author>
<author>W Ward</author>
<author>J H Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Semantic role labeling by tagging syntactic chunks.</title>
<date>2003</date>
<booktitle>In Proc. of the 8th Conf. on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="1435" citStr="Hacioglu et al., 2003" startWordPosition="222" endWordPosition="225">ction Semantic annotation of text corpora is needed to support tasks such as information extraction and question-answering (e.g., Riloff and Schmelzenbach, 1998; Niu and Hirst, 2004). In particular, labelling the semantic roles of the arguments of a verb (or any predicate), as in (1) and (2), provides crucial information about the relations among event participants. Because of the importance of this task, a number of recent methods have been proposed for automatic semantic role labelling (e.g., Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Fleischman et al., 2003; Hacioglu et al., 2003; Thompson et al., 2003). These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2003) as training data, which are expensive to produce, are limited in size, and may not be representative. We have developed a novel method of unsupervised semantic role labelling that avoids the need for expensive manual labelling of text, and enables the use of a large, representative corpus. To achieve this, we take a “bootstrapping” approach (e.g., Hindle and Rooth, 1993; Yarowsky, 1995; Jones et al., 1999), which </context>
</contexts>
<marker>Hacioglu, Pradhan, Ward, Martin, Jurafsky, 2003</marker>
<rawString>K. Hacioglu, S. Pradhan, W. Ward, J. H. Martin, and D. Jurafsky. 2003. Semantic role labeling by tagging syntactic chunks. In Proc. of the 8th Conf. on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Hindle</author>
<author>M Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="1990" citStr="Hindle and Rooth, 1993" startWordPosition="313" endWordPosition="317">en and Rambow, 2003; Fleischman et al., 2003; Hacioglu et al., 2003; Thompson et al., 2003). These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2003) as training data, which are expensive to produce, are limited in size, and may not be representative. We have developed a novel method of unsupervised semantic role labelling that avoids the need for expensive manual labelling of text, and enables the use of a large, representative corpus. To achieve this, we take a “bootstrapping” approach (e.g., Hindle and Rooth, 1993; Yarowsky, 1995; Jones et al., 1999), which initially makes only the role assignments that are unambiguous according to a verb lexicon. We then iteratively: create a probability model based on the currently annotated semantic roles, use this probability model to assign roles that are deemed to have sufficient evidence, and add the newly labelled arguments to our annotated set. As we iterate, we gradually both grow the size of the annotated set, and relax the evidence thresholds for the probability model, until all arguments have been assigned roles. To our knowledge, this is the first unsuper</context>
<context position="15711" citStr="Hindle and Rooth, 1993" startWordPosition="2622" endWordPosition="2625">ide the iterative use of the probability model to assign roles. The evidence count for each of the conditional probabilities refers to the number of times we have observed the conjunction of its conditioning events. For example, for , this is the number of times the particular combination of verb, slot, and noun have been observed. For a probability to be used, its evidence count must reach a given threshold, . The “goodness” of a role assignment is determined by taking the log of the ratio between the probabilities of the top two candidates for a slot (when the evidence of both meet ) (e.g., Hindle and Rooth, 1993). A role is only assigned if the log likelihood ratio is defined and meets a threshold; in this case, the candidate role with highest probability is assigned to the slot. (Note that in the current implementation, we do not allow re-labelling: an assigned label is fixed.) In the algorithm, the log ratio threshold is initially set high and gradually reduced until it reaches 0. In the case of remaining ties, we assign the role for which is highest. Because our evidence count and log ratio restrictions may not be met even when we have a very good candidate for a slot, we reduce the evidence count </context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>D. Hindle and M. Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19(1):103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jones</author>
<author>A McCallum</author>
<author>K Nigam</author>
<author>E Riloff</author>
</authors>
<title>Bootstrapping for text learning tasks.</title>
<date>1999</date>
<booktitle>In IJCAI-99 Workshop on Text Mining: Foundations, Techniques and Applications.</booktitle>
<contexts>
<context position="2027" citStr="Jones et al., 1999" startWordPosition="320" endWordPosition="323">2003; Hacioglu et al., 2003; Thompson et al., 2003). These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2003) as training data, which are expensive to produce, are limited in size, and may not be representative. We have developed a novel method of unsupervised semantic role labelling that avoids the need for expensive manual labelling of text, and enables the use of a large, representative corpus. To achieve this, we take a “bootstrapping” approach (e.g., Hindle and Rooth, 1993; Yarowsky, 1995; Jones et al., 1999), which initially makes only the role assignments that are unambiguous according to a verb lexicon. We then iteratively: create a probability model based on the currently annotated semantic roles, use this probability model to assign roles that are deemed to have sufficient evidence, and add the newly labelled arguments to our annotated set. As we iterate, we gradually both grow the size of the annotated set, and relax the evidence thresholds for the probability model, until all arguments have been assigned roles. To our knowledge, this is the first unsupervised semantic role labelling system </context>
</contexts>
<marker>Jones, McCallum, Nigam, Riloff, 1999</marker>
<rawString>R. Jones, A. McCallum, K. Nigam, and E. Riloff. 1999. Bootstrapping for text learning tasks. In IJCAI-99 Workshop on Text Mining: Foundations, Techniques and Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>H T Dang</author>
<author>M Palmer</author>
</authors>
<title>Class based construction of a verb lexicon.</title>
<date>2000</date>
<booktitle>In Proc. of the 17th National Conference on Artificial Intelligence (AAAI-2000).</booktitle>
<contexts>
<context position="6015" citStr="Kipper et al., 2000" startWordPosition="976" endWordPosition="979">ng data, this latter step takes on greater significance in our work. We first describe the lexicon that specifies the syntactic arguments and possible roles for the verbs, and then discuss our process of argument and role set identification. 2.1 The Verb Lexicon In semantic role labelling, a lexicon is used which lists the possible roles for each syntactic argument of each predicate. Supervised approaches to this task have thus far used the predicate lexicon of FrameNet, or the verb lexicon of PropBank, since each has an associated labelled corpus for training. We instead make use of VerbNet (Kipper et al., 2000), a manually developed hierarchical verb lexicon based on the verb classification of Levin (1993). For each of 191 verb classes, including around 3000 verbs in total, VerbNet specifies the syntactic frames along with the semantic role assigned to each slot of a frame. Throughout the paper we use the term “frame” to refer to a syntactic frame—the set of syntactic arguments of a verb—possibly labelled with roles, as exemplified in the VerbNet entry in Table 1. While FrameNet uses semantic roles specific to a particular situation (such as Speaker, Message, admire Frames: Experiencer V Cause Exper</context>
<context position="14504" citStr="Kipper et al., 2000" startWordPosition="2412" endWordPosition="2415">ields a noun classification system that is very coarse and that does not distinguish between senses, but which is simple and computationally feasible. thus captures consistent relations between a verb and a class of nouns, regardless of the slot in which the noun occurs. Verb classes have been shown to be very important in capturing generalizations across verb behaviour in computational systems (e.g., Palmer, 2000; Merlo and Stevenson, 2001). In semantic role labelling using VerbNet, they are particularly relevant since the classes are based on a commonality of role-labelled syntactic frames (Kipper et al., 2000). The class of a verb in our model is its VerbNet class that is compatible with the current frame. When multiple classes are compatible, we apportion the counts uniformly among them. For probability , then, we generalize over all verbs in a class of the target verb, giving us much more extensive data over relevant role assignments to a particular slot. 4 The Bootstrapping Algorithm We have described the frame matcher that produces a set of slots with candidate role lists (some unambiguous), and our backoff probability model. All that remains is to specify the parameters that guide the iterativ</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>K. Kipper, H. T. Dang, and M. Palmer. 2000. Class based construction of a verb lexicon. In Proc. of the 17th National Conference on Artificial Intelligence (AAAI-2000).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Levin</author>
</authors>
<title>English Verb Classes and Alternations: A Preliminary Investigation.</title>
<date>1993</date>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="6112" citStr="Levin (1993)" startWordPosition="993" endWordPosition="994">pecifies the syntactic arguments and possible roles for the verbs, and then discuss our process of argument and role set identification. 2.1 The Verb Lexicon In semantic role labelling, a lexicon is used which lists the possible roles for each syntactic argument of each predicate. Supervised approaches to this task have thus far used the predicate lexicon of FrameNet, or the verb lexicon of PropBank, since each has an associated labelled corpus for training. We instead make use of VerbNet (Kipper et al., 2000), a manually developed hierarchical verb lexicon based on the verb classification of Levin (1993). For each of 191 verb classes, including around 3000 verbs in total, VerbNet specifies the syntactic frames along with the semantic role assigned to each slot of a frame. Throughout the paper we use the term “frame” to refer to a syntactic frame—the set of syntactic arguments of a verb—possibly labelled with roles, as exemplified in the VerbNet entry in Table 1. While FrameNet uses semantic roles specific to a particular situation (such as Speaker, Message, admire Frames: Experiencer V Cause Experiencer V Cause Prep(in) Oblique Experiencer V Oblique Prep(for) Cause Verbs in same (sub)class: [</context>
</contexts>
<marker>Levin, 1993</marker>
<rawString>B. Levin. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Merlo</author>
<author>S Stevenson</author>
</authors>
<title>Automatic verb classification based on statistical distributions of argument structure.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>3</issue>
<contexts>
<context position="14329" citStr="Merlo and Stevenson, 2001" startWordPosition="2384" endWordPosition="2387">at appear in this cut are used as the class(es) for the noun. (Credit for a noun is apportioned equally across multiple classes.) Unknown words placed in a separate category. This yields a noun classification system that is very coarse and that does not distinguish between senses, but which is simple and computationally feasible. thus captures consistent relations between a verb and a class of nouns, regardless of the slot in which the noun occurs. Verb classes have been shown to be very important in capturing generalizations across verb behaviour in computational systems (e.g., Palmer, 2000; Merlo and Stevenson, 2001). In semantic role labelling using VerbNet, they are particularly relevant since the classes are based on a commonality of role-labelled syntactic frames (Kipper et al., 2000). The class of a verb in our model is its VerbNet class that is compatible with the current frame. When multiple classes are compatible, we apportion the counts uniformly among them. For probability , then, we generalize over all verbs in a class of the target verb, giving us much more extensive data over relevant role assignments to a particular slot. 4 The Bootstrapping Algorithm We have described the frame matcher that</context>
</contexts>
<marker>Merlo, Stevenson, 2001</marker>
<rawString>P. Merlo and S. Stevenson. 2001. Automatic verb classification based on statistical distributions of argument structure. Computational Linguistics, 27(3):373–408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Niu</author>
<author>G Hirst</author>
</authors>
<title>Analysis of semantic classes in medical text for question answering.</title>
<date>2004</date>
<booktitle>In Workshop on Question Answering in Restricted Domains, 42nd Annual Meeting of the Assoc. for Computational Linguistics.</booktitle>
<contexts>
<context position="996" citStr="Niu and Hirst, 2004" startWordPosition="150" endWordPosition="153">en iteratively updates the probability model on which future assignments are based. A novel aspect of our approach is the use of verb, slot, and noun class information as the basis for backing off in our probability model. We achieve 50–65% reduction in the error rate over an informed baseline, indicating the potential of our approach for a task that has heretofore relied on large amounts of manually generated training data. 1 Introduction Semantic annotation of text corpora is needed to support tasks such as information extraction and question-answering (e.g., Riloff and Schmelzenbach, 1998; Niu and Hirst, 2004). In particular, labelling the semantic roles of the arguments of a verb (or any predicate), as in (1) and (2), provides crucial information about the relations among event participants. Because of the importance of this task, a number of recent methods have been proposed for automatic semantic role labelling (e.g., Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Fleischman et al., 2003; Hacioglu et al., 2003; Thompson et al., 2003). These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et al., 1998) or PropBank </context>
</contexts>
<marker>Niu, Hirst, 2004</marker>
<rawString>Y. Niu and G. Hirst. 2004. Analysis of semantic classes in medical text for question answering. In Workshop on Question Answering in Restricted Domains, 42nd Annual Meeting of the Assoc. for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
</authors>
<title>Consistent criteria for sense distinctions.</title>
<date>2000</date>
<booktitle>Special Issue of Computers and the Humanities, SENSEVAL98: Evaluating Word Sense Disambiguation Systems,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="14301" citStr="Palmer, 2000" startWordPosition="2382" endWordPosition="2383">s ancestors that appear in this cut are used as the class(es) for the noun. (Credit for a noun is apportioned equally across multiple classes.) Unknown words placed in a separate category. This yields a noun classification system that is very coarse and that does not distinguish between senses, but which is simple and computationally feasible. thus captures consistent relations between a verb and a class of nouns, regardless of the slot in which the noun occurs. Verb classes have been shown to be very important in capturing generalizations across verb behaviour in computational systems (e.g., Palmer, 2000; Merlo and Stevenson, 2001). In semantic role labelling using VerbNet, they are particularly relevant since the classes are based on a commonality of role-labelled syntactic frames (Kipper et al., 2000). The class of a verb in our model is its VerbNet class that is compatible with the current frame. When multiple classes are compatible, we apportion the counts uniformly among them. For probability , then, we generalize over all verbs in a class of the target verb, giving us much more extensive data over relevant role assignments to a particular slot. 4 The Bootstrapping Algorithm We have desc</context>
</contexts>
<marker>Palmer, 2000</marker>
<rawString>M. Palmer. 2000. Consistent criteria for sense distinctions. Special Issue of Computers and the Humanities, SENSEVAL98: Evaluating Word Sense Disambiguation Systems, 34(1–2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Palmer</author>
<author>D Gildea</author>
<author>P Kingsbury</author>
</authors>
<title>The Proposition Bank: An annotated corpus of semantic roles.</title>
<date>2003</date>
<note>Submitted to Computational Linguistics.</note>
<contexts>
<context position="1617" citStr="Palmer et al., 2003" startWordPosition="252" endWordPosition="255"> In particular, labelling the semantic roles of the arguments of a verb (or any predicate), as in (1) and (2), provides crucial information about the relations among event participants. Because of the importance of this task, a number of recent methods have been proposed for automatic semantic role labelling (e.g., Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Fleischman et al., 2003; Hacioglu et al., 2003; Thompson et al., 2003). These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2003) as training data, which are expensive to produce, are limited in size, and may not be representative. We have developed a novel method of unsupervised semantic role labelling that avoids the need for expensive manual labelling of text, and enables the use of a large, representative corpus. To achieve this, we take a “bootstrapping” approach (e.g., Hindle and Rooth, 1993; Yarowsky, 1995; Jones et al., 1999), which initially makes only the role assignments that are unambiguous according to a verb lexicon. We then iteratively: create a probability model based on the currently annotated semantic </context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2003</marker>
<rawString>M. Palmer, D. Gildea, and P. Kingsbury. 2003. The Proposition Bank: An annotated corpus of semantic roles. Submitted to Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>M Schmelzenbach</author>
</authors>
<title>An empirical approach to conceptual case frame acquisition.</title>
<date>1998</date>
<booktitle>In Proc. of the 6th Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="974" citStr="Riloff and Schmelzenbach, 1998" startWordPosition="145" endWordPosition="149">biguous role assignments, and then iteratively updates the probability model on which future assignments are based. A novel aspect of our approach is the use of verb, slot, and noun class information as the basis for backing off in our probability model. We achieve 50–65% reduction in the error rate over an informed baseline, indicating the potential of our approach for a task that has heretofore relied on large amounts of manually generated training data. 1 Introduction Semantic annotation of text corpora is needed to support tasks such as information extraction and question-answering (e.g., Riloff and Schmelzenbach, 1998; Niu and Hirst, 2004). In particular, labelling the semantic roles of the arguments of a verb (or any predicate), as in (1) and (2), provides crucial information about the relations among event participants. Because of the importance of this task, a number of recent methods have been proposed for automatic semantic role labelling (e.g., Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Fleischman et al., 2003; Hacioglu et al., 2003; Thompson et al., 2003). These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et a</context>
<context position="2769" citStr="Riloff and Schmelzenbach, 1998" startWordPosition="437" endWordPosition="440">atively: create a probability model based on the currently annotated semantic roles, use this probability model to assign roles that are deemed to have sufficient evidence, and add the newly labelled arguments to our annotated set. As we iterate, we gradually both grow the size of the annotated set, and relax the evidence thresholds for the probability model, until all arguments have been assigned roles. To our knowledge, this is the first unsupervised semantic role labelling system applied to general semantic roles in a domain-general corpus. In a similar vein of work, Riloff and colleagues (Riloff and Schmelzenbach, 1998; Jones et al., 1999) used bootstrapping to learn “case frames” for verbs, but their approach has been applied in very narrow topic domains with topic-specific roles. In other work, Gildea (2002) has explored unsupervised methods to discover role-slot mappings for verbs, but not to apply this knowledge to label text with roles. Our approach also differs from earlier work in its novel use of classes of information in backing off to less specific role probabilities (in contrast to using simple subsets of information, as in Gildea and Jurafsky, 2002). If warranted, we base our decisions on the pr</context>
</contexts>
<marker>Riloff, Schmelzenbach, 1998</marker>
<rawString>E. Riloff and M. Schmelzenbach. 1998. An empirical approach to conceptual case frame acquisition. In Proc. of the 6th Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Schulte im Walde</author>
</authors>
<title>Experiments on the choice of features for learning verb classes.</title>
<date>2003</date>
<booktitle>In Proc. of the 10th Conf. of the European Chapter of the Assoc. for Computational Linguistics.</booktitle>
<contexts>
<context position="13655" citStr="Walde, 2003" startWordPosition="2274" endWordPosition="2275">ing all prepositional slots as the same slot. This formula allows us to generalize over a slot regardless of the Figure 1: The backoff model. particular noun, and preposition if there is one, used in the instance. Classes of nouns in the model are given by the WordNet hierarchy. Determining the appropriate level of generalization for a noun is an open problem (e.g., Clark and Weir, 2002). Currently, we use a cut through WordNet including all the top categories, except for the category “entity”; the latter, because of its generality, is replaced in the cut by its immediate children (Schulte im Walde, 2003). Given a noun argument, all of its ancestors that appear in this cut are used as the class(es) for the noun. (Credit for a noun is apportioned equally across multiple classes.) Unknown words placed in a separate category. This yields a noun classification system that is very coarse and that does not distinguish between senses, but which is simple and computationally feasible. thus captures consistent relations between a verb and a class of nouns, regardless of the slot in which the noun occurs. Verb classes have been shown to be very important in capturing generalizations across verb behaviou</context>
</contexts>
<marker>Walde, 2003</marker>
<rawString>S. Schulte im Walde. 2003. Experiments on the choice of features for learning verb classes. In Proc. of the 10th Conf. of the European Chapter of the Assoc. for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Thompson</author>
<author>R Levy</author>
<author>C Manning</author>
</authors>
<title>A generative model for FrameNet semantic role labeling.</title>
<date>2003</date>
<booktitle>In Proc. of the Fourteenth European Conf. on Machine Learning (ECML-03).</booktitle>
<contexts>
<context position="1459" citStr="Thompson et al., 2003" startWordPosition="226" endWordPosition="229">on of text corpora is needed to support tasks such as information extraction and question-answering (e.g., Riloff and Schmelzenbach, 1998; Niu and Hirst, 2004). In particular, labelling the semantic roles of the arguments of a verb (or any predicate), as in (1) and (2), provides crucial information about the relations among event participants. Because of the importance of this task, a number of recent methods have been proposed for automatic semantic role labelling (e.g., Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Fleischman et al., 2003; Hacioglu et al., 2003; Thompson et al., 2003). These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2003) as training data, which are expensive to produce, are limited in size, and may not be representative. We have developed a novel method of unsupervised semantic role labelling that avoids the need for expensive manual labelling of text, and enables the use of a large, representative corpus. To achieve this, we take a “bootstrapping” approach (e.g., Hindle and Rooth, 1993; Yarowsky, 1995; Jones et al., 1999), which initially makes only the</context>
<context position="11240" citStr="Thompson et al. (2003)" startWordPosition="1863" endWordPosition="1866">dates for a single slot. We assume the probability of a role for a slot is independent of other slots; we do not ensure a consistent role assignment across an instance of a verb. 3.1 The Backoff Levels Our most specific probability uses the exact combination of verb, slot, and noun filling that slot, yielding .2 2We use only the head noun of potential arguments, not the full NP, in our probability model. Our combination of slot plus head word provides similar information (head of argument and its syntactic relation to the verb) to that captured by the features of Gildea and Jurafsky (2002) or Thompson et al. (2003). For our first backoff level, we introduce a novel way to generalize over the verb, slot, and noun information of . Here we use a linear interpolation of three probabilities, each of which: (1) drops one source of conditioning information from the most specific probability, and (2) generalizes a second source of conditioning information to a class-based conditioning event. Specifically, we use the following probability formula: where is slot class, is noun class, is verb class, and the individual probabilities are (currently) equally weighted (i.e., all ’s have a value of ). Note that all thr</context>
</contexts>
<marker>Thompson, Levy, Manning, 2003</marker>
<rawString>C. Thompson, R. Levy, and C. Manning. 2003. A generative model for FrameNet semantic role labeling. In Proc. of the Fourteenth European Conf. on Machine Learning (ECML-03).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation methods rivaling supervised methods”.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="2006" citStr="Yarowsky, 1995" startWordPosition="318" endWordPosition="319">ischman et al., 2003; Hacioglu et al., 2003; Thompson et al., 2003). These supervised methods are limited by their reliance on the manually roletagged corpora of FrameNet (Baker et al., 1998) or PropBank (Palmer et al., 2003) as training data, which are expensive to produce, are limited in size, and may not be representative. We have developed a novel method of unsupervised semantic role labelling that avoids the need for expensive manual labelling of text, and enables the use of a large, representative corpus. To achieve this, we take a “bootstrapping” approach (e.g., Hindle and Rooth, 1993; Yarowsky, 1995; Jones et al., 1999), which initially makes only the role assignments that are unambiguous according to a verb lexicon. We then iteratively: create a probability model based on the currently annotated semantic roles, use this probability model to assign roles that are deemed to have sufficient evidence, and add the newly labelled arguments to our annotated set. As we iterate, we gradually both grow the size of the annotated set, and relax the evidence thresholds for the probability model, until all arguments have been assigned roles. To our knowledge, this is the first unsupervised semantic r</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation methods rivaling supervised methods”. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, p. 189–196.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>