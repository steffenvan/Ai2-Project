<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013691">
<title confidence="0.987034">
Automatic Detection of Sentence Fragments
</title>
<author confidence="0.812165">
Chak Yan Yeung and John Lee
</author>
<affiliation confidence="0.932328">
Halliday Centre for Intelligent Applications of Language Studies
Department of Linguistics and Translation
City University of Hong Kong
</affiliation>
<email confidence="0.947026">
chak.yeung@my.cityu.edu.hk
jsylee@cityu.edu.hk
</email>
<sectionHeader confidence="0.993768" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999887923076923">
We present and evaluate a method for au-
tomatically detecting sentence fragments
in English texts written by non-native
speakers. Our method combines syntactic
parse tree patterns and parts-of-speech in-
formation produced by a tagger to detect
this phenomenon. When evaluated on a
corpus of authentic learner texts, our best
model achieved a precision of 0.84 and a
recall of 0.62, a statistically significant im-
provement over baselines using non-parse
features, as well as a popular grammar
checker.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99991436">
It is challenging to detect and correct sentence-
level grammatical errors because it involves au-
tomatic syntactic analysis on noisy, learner sen-
tences. Indeed, none of the teams achieved any re-
call for comma splices in the most recent CoNLL
shared task (Ng et al., 2014). Sentence fragments
fared hardly better: of the thirteen teams, two
scored a recall of 0.25 for correction and another
scored 0.2; the rest did not achieve any recall.
Although parser performance degrades on
learner text (Foster, 2007), parsers can still be use-
ful for identifying grammatical errors if they pro-
duce consistent patterns that indicate these errors.
We show that parse tree patterns, automatically de-
rived from training data, significantly improve sys-
tem performance on detecting sentence fragments.
The rest of the paper is organized as follows.
The next section defines the types of sentence frag-
ments treated in this paper. Section 3 reviews re-
lated work. Section 4 describes the features used
in our model. Section 5 discusses the datasets and
section 6 analyzes the experiment results. Our best
model significantly outperforms baselines that do
not consider syntactic information and a widely
used grammar checker.
</bodyText>
<sectionHeader confidence="0.97715" genericHeader="introduction">
2 Sentence Fragment
</sectionHeader>
<bodyText confidence="0.999867238095238">
Every English sentence must have a main or in-
dependent clause. Most linguists require a clause
to contain a subject and a finite verb (Hunt, 1965;
Polio, 1997); otherwise, it is considered a sentence
fragment. Following Bram (1995), we classify
sentence fragments into the following four cate-
gories:
No Subject. Fragments that lack a subject,1
such as “According to the board, is $100.”
No finite verb. Fragments that lack a finite
verb. These may be a nonfinite verb phrase, or
a noun phrase, such as “Mrs. Kern in a show.”
No subject and finite verb. Fragments lacking
both a subject and a finite verb; a typical example
is a prepositional phrase, such as “Up through the
ranks.”
Subordinate clause. These fragments consist
of a stand-alone subordinate clause; the clause
typically begins with a relative pronoun or a sub-
ordinating conjunction, such as “While they take
pains to hide their assets.”
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="related work">
3 Related Work
</sectionHeader>
<bodyText confidence="0.942454615384615">
Using parse tree patterns to judge the grammati-
cality of a sentence is not new. Wong and Dras
(2011) exploited probabilistic context-free gram-
mar (PCFG) rules as features for native language
identification. In addition to production rules, Post
(2011) incorporated parse fragment features com-
puted from derivations of tree substitution gram-
mars. Heilman et al. (2014) used the parse scores
and syntactic features to classify the comprehensi-
bility of learner text, though they made no attempt
to correct the errors.
In current grammatical error correction sys-
tems, parser output is used mainly to locate
</bodyText>
<footnote confidence="0.9959425">
1Our evaluation data distinguishes between imperatives
and fragments. Our automatic classifier, however, makes no
such attempt because it would require analysis of the context
and significant real-world knowledge.
</footnote>
<page confidence="0.740096">
599
</page>
<bodyText confidence="0.952332228571429">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 599–603,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
relevant information involved in long-distance
grammatical constructions (Tetreault et al., 2010;
Yoshimoto et al., 2013; Zhang and Wang, 2014).
To the best of our knowledge, the only previous
work that used distinctive parse patterns to detect
specific grammatical errors was concerned with
comma splices. Lee et al. (2014) manually identi-
fied distinctive production rules which, when used
as features in a CRF, significantly improved the
precision and recall in locating comma splices in
learner text. Our method will similarly leverage
parse tree patterns, but with the goal of detecting
sentence fragment errors. More importantly, our
approach is fully automatic, and can thus poten-
tially be broadly applied on other syntax-related
learner errors.
Many commercial systems, such as the Cri-
terion Online Writing Service (Burstein et al.,
2004), Grammarly2, and WhiteSmoke3, give feed-
back about sentence fragments. To the best of our
knowledge, these systems do not explicitly con-
sider parse tree patterns. The grammar checker
embedded in Microsoft Word also gives feedback
about sentence fragments, and will serve as one of
our baselines.
Aside from the CoNLL-2014 shared task (see
Section 1), the only other reported evaluation on
detecting or correcting sentence fragments has
been performed on Microsoft ESL Assistant and
the NTNU Grammar Checker (Chen, 2009). Nei-
ther tool detected any of the sentence fragments in
the test set.
</bodyText>
<sectionHeader confidence="0.985994" genericHeader="method">
4 Fragment Detection
</sectionHeader>
<bodyText confidence="0.9999057">
We cast the problem of sentence fragment detec-
tion as a multiclass classification task. Given a
sentence, the system would mark it either as false,
if it is not a fragment, or as one of the four frag-
ment categories described in Section 2. Rather
than a binary decision on whether a sentence is a
fragment, this categorisation provides more useful
feedback to the learner, since each of the four frag-
ment categories requires its own correction strat-
egy.
</bodyText>
<subsectionHeader confidence="0.993128">
4.1 Models
</subsectionHeader>
<bodyText confidence="0.996120666666667">
Baseline Models. We trained three baseline mod-
els with features that incorporate an increasing
amount of information about sentence structure.
</bodyText>
<footnote confidence="0.9998785">
2www.grammarly.com
3www.whitesmoke.com
</footnote>
<bodyText confidence="0.999835137254902">
The first baseline model was trained on the word
trigrams of the sentences, the second model on
part-of-speech unigrams, and the third on part-of-
speech trigrams. All of these features can be ob-
tained without syntactic parsing. To reduce the
number of features, we filtered out the word tri-
grams that occur less than twenty times and the
POS trigrams that occur less than a hundred times
in the training data.
Parse Models. Our approach uses parse tree
patterns as features. Although any arbitrary sub-
tree structure can potentially serve as a feature, the
children of the root of the tree tend to be most
salient. These nodes usually denote the syntac-
tic constituents of the sentence, and so often re-
veal differences between well-formed sentences
and fragments. Consider the sentence “While Pe-
ter was a good boy.”, shown in the parse tree in
Figure 1. The child of the root of the tree is SBAR.
When the subordinating conjunction “while” is re-
moved to yield a well-formed sentence, the chil-
dren nodes change accordingly into the expected
NP and VP. In contrast, the POS tags, used in the
baseline models, tend to remain the same.
We use the label of the root and the trigrams of
its children nodes as features, similar to Sj¨obergh
(2005) and Lin et al. (2011). We also extend our
patterns to grandchildren in some cases. When
analyzing an ill-formed sentence, the parser can
sometimes group words into constituents to which
they do not belong, such as forming a VP that does
not contain a verb. For example, the phrase “up the
hill” was analyzed as a VP in the fragment “A new
challenger up the hill” when in fact the sentence is
missing a verb. To take into account such misanal-
yses, we also include the POS tag of the first child
of all NP, VP, PP, ADVP, and ADJP as features.
The first child is chosen because it often exposes
the parsing error, as is the case with the preposi-
tion “up” in the purported VP “up the hill” in the
above example.
We trained two models for experiments: the
“Parse” model used the parser’s POS tags and the
“Parse + Tag” model used the tags produced by
the POS tagger, which was trained on local fea-
tures and tends to be less affected by ill-formed
sentence structures. For example, in the sentence
“Certainly was not true.”, the word “certainly” was
tagged as a plural noun by the parser while the tag-
ger correctly identified it as an adverb. The NP
construction in the fragment was encoded as “NP-
</bodyText>
<page confidence="0.984985">
600
</page>
<bodyText confidence="0.9998485">
NNP” in the “Parse” model and “NP-RB” in the
“Parse + Tag” model. To reduce the number of
features, we filtered out the node trigrams that oc-
cur less than ten times in the training data.
</bodyText>
<figureCaption confidence="0.867414">
Figure 1: The POS-tagged words and parse trees
</figureCaption>
<bodyText confidence="0.6076675">
of the fragment “While Peter was a good boy.” and
the well-formed sentence “Peter was a good boy.”.
</bodyText>
<sectionHeader confidence="0.997795" genericHeader="method">
5 Data
</sectionHeader>
<subsectionHeader confidence="0.999224">
5.1 Training Data
</subsectionHeader>
<bodyText confidence="0.999945956521739">
We automatically produced training data from the
New York Times portion of the AQUAINT Cor-
pus of English News Text (Graff, 2002). Similar
to Foster and Andersen (2009), we artificially gen-
erate fragments that correspond to the four cate-
gories (Section 2) by removing different compo-
nents from well-formed English sentences. For
the “no subject” category, the NP immediately un-
der the topmost S was removed. For the “no finite
verb” category, we removed the finite verb in the
VP immediately under the topmost S. For the “no
subject and finite verb” category, we removed both
the NP and the finite verb in the VP immediately
under the topmost S. For the “subordinate clause”
category, we looked for any SBAR in the sentence
that is preceded by a comma and consists of an
IN child followed by an S. The words under the
SBAR are extracted as the fragment. Using this
method, we created a total of 60,000 fragments,
with 15,000 sentences in each category. Together
with the original sentences, our training data con-
sists of 120,000 sentences, half of which are frag-
ments.
</bodyText>
<subsectionHeader confidence="0.997424">
5.2 Evaluation Data
</subsectionHeader>
<bodyText confidence="0.999991846153846">
Fragment was among the 28 error types introduced
in the CoNLL-2014 shared task (Ng et al., 2014),
but the test set used in the task only contained 16
such errors and is too small for our purpose. In-
stead, we evaluated our system on the NUCLE
corpus (Dahlmeier et al., 2013), which was used
as the training data in the shared task. The error
label “SFrag” in the NUCLE corpus was used for
sentence fragments in a wider sense than the four
categories defined by Bram (1995) (see Section
2). For example, “SFrag” also labels sentences
with stylistic issues, such as those beginning with
“therefore” or “hence”, and sentences that, though
well-formed, should be merged with its neighbor,
such as “In Singapore, we can see that this prob-
lem is occurring. This is so as there is a huge dis-
crepancy in the education levels.”.
We asked two human annotators to classify the
fragments into the different categories described
in Section 2. The kappa was 0.84. Most of
the disagreements involved sentences that con-
tain a semi-colon which, when replaced with a
comma, would become well-formed. One anno-
tator flagged these cases as fragments while the
other did not, considering them to be punctua-
tion errors. Another source of disagreements was
whether a sentence should be considered an im-
perative.
Among the 249 sentences marked as fragments,
86 were classified as one of the Bram (1995) cat-
egories by at least one of the annotators. Most of
the fragments belong to categories “no finite verb”
and “subordinate clause”, accounting for 43.0%
and 31.4% of the cases respectively. The cate-
gories “no subject and finite verb” and “no sub-
ject” both account for 12.8% of the cases. We
left all errors in the sentences in place so as to re-
flect our models’ performance on authentic learner
data.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999835777777778">
We obtained the POS tags and parse trees of the
sentences in our datasets with the Stanford POS
tagger (Toutanova et al., 2003) and the Stanford
parser (Manning et al., 2014). We used the logis-
tic regression implementation in scikit-learn (Pe-
dregosa et al., 2011) for the maximum entropy
models in our experiments. In addition to the
three baseline models described in Section 4.1,
we computed a fourth baseline using the grammar
</bodyText>
<figure confidence="0.998517909090909">
VP
NP
Peter
was a good boy
While/IN Peter/NNP was/VBD a/DT good/JJ
boy/NN
FRAG
SBAR
While Peter was a good boy
Peter/NNP was/VBD a/DT good/JJ boy/NN
S
</figure>
<page confidence="0.992636">
601
</page>
<bodyText confidence="0.978311666666667">
checker in Microsoft Word 2013 by configuring
the checker to capture “Fragments and Run-ons”
and “Fragment - stylistic suggestions”.
</bodyText>
<subsectionHeader confidence="0.998245">
6.1 Fragment detection
</subsectionHeader>
<bodyText confidence="0.99999696969697">
We first evaluated the systems’ ability to detect
fragments. The fragment categories are disre-
garded in this evaluation and the system’s result
is considered correct even if its output category
does not match the one marked by the annota-
tors. We adopted the metric used in the CoNLL-
2014 shared task, F0.5, which emphasizes preci-
sion twice as much as recall because it is important
to minimize false alarms for language learners4.
The results are shown in Table 1. The “Parse”
model achieved a precision of 0.82, a recall of
0.57 and an F0.5 of 0.75. Using the POS tags
produced by the POS tagger instead of the ones
produced by the parser, the “Parse + Tag” model
achieved a precision of 0.84, a recall of 0.62 and
an F0.5 of 0.78, improving upon the results of the
“Parse” model and significantly outperforming all
four baselines5.
Most of the false negatives are in the “no fi-
nite verb” category and many of them involve
fragments with subordinate clauses, such as “The
increased of longevity as the elderly are leading
longer lives.”. In order to create parse trees that fit
those of complete sentences, the parser tended to
interpret the verbs in the subordinate clauses (e.g.,
“are” in the above example) as the fragments’
main verbs, causing the errors. For false positives,
the errors were caused mostly by the presence of
introductory phrases. The parse trees of these sen-
tences usually contain a PP or an ADVP immedi-
ately under the root, which is a pattern shared by
fragments. The system also flagged some impera-
tive sentences as fragments.
</bodyText>
<subsectionHeader confidence="0.999852">
6.2 Fragment classification
</subsectionHeader>
<bodyText confidence="0.999826">
For the fragments that the system has correctly
identified, we evaluated their classification accu-
racy6. Table 2 shows the confusion matrix of the
system’s results.
The largest source of error is the system
wrongly classifying ‘no finite verb” and “subor-
</bodyText>
<footnote confidence="0.993450666666667">
4F0.5 is calculated by F0.5 = (1 + 0.52) x R x P / (R + 0.52
x P) for recall R and precision P.
5At p ≤ 0.002 by McNemar’s test.
6The grammar checker in Microsoft Word is excluded
from this evaluation because it does not provide any correc-
tion suggestions for fragments.
</footnote>
<table confidence="0.998982714285714">
System P/R/F0.5
Word Trigrams 0.20/0.03/0.09
POS Tags 0.56/0.33/0.47
POS Trigrams 0.55/0.42/0.52
MS Word 0.80/0.15/0.43
Parse 0.82/0.57/0.75
Parse + Tag 0.84/0.62/0.78
</table>
<tableCaption confidence="0.999138">
Table 1: System precision, recall and F-measure
</tableCaption>
<bodyText confidence="0.9696494">
for fragment detection.
dinate clause” fragments as “no subject and finite
verb”. Most of these involve fragments that begin
with a prepositional phrase, such as “for example”,
followed by a comma. The annotators treated the
prepositional phrase as introductory phrase and fo-
cused on the segment after the comma. In con-
trast, based on the parser output, the system often
treated the entire fragment as a PP, which should
then belong to “no subject and finite verb”. It can
be argued that both interpretations are valid. For
instance, the fragment “For example, apples and
oranges” can be corrected as “For example, apples
and oranges are fruits” or, alternatively, “I love
fruits, for example, apples and oranges”.
</bodyText>
<table confidence="0.933012333333333">
→ Expected S V SV C
↓ System
S [6] 4 1 1
V 0 [12] 2 0
SV 0 5 [2] 11
C 0 0 0 [9]
</table>
<tableCaption confidence="0.939009">
Table 2: The confusion matrix of the system for
</tableCaption>
<bodyText confidence="0.97937075">
classifying the detected sentence fragments into
the categories no subject (S), no finite verb (V), no
subject and finite verb (SV) and subordinate clause
(C).
</bodyText>
<sectionHeader confidence="0.998477" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9997945">
We have presented a data-driven method for auto-
matically detecting sentence fragments. We have
shown that our method, which uses syntactic parse
tree patterns and POS tagger output, significantly
improves accuracy in detecting fragments in En-
glish learner texts.
</bodyText>
<sectionHeader confidence="0.998401" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.921951">
This work was supported in part by a Strategic Re-
search Grant (#7008166) from City University of
</bodyText>
<page confidence="0.991603">
602
</page>
<bodyText confidence="0.332051">
Hong Kong.
</bodyText>
<sectionHeader confidence="0.896814" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99975287254902">
Barli Bram. 1995. Write Well, Improving Writing
Skills. Kanisius.
Jill Burstein, Martin Chodorow, and Claudia Leacock.
2004. Automated essay evaluation: The Criterion
online writing service. AI Magazine, 25(3):27.
Hao-Jan Howard Chen. 2009. Evaluating two web-
based grammar checkers-Microsoft ESL Assistant
and NTNU statistical grammar checker. Computa-
tional Linguistics and Chinese Language Process-
ing, 14(2):161–180.
Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.
2013. Building a large annotated corpus of learner
english: The NUS Corpus of Learner English. In
Proceedings of the Eighth Workshop on Innovative
Use of NLP for Building Educational Applications,
pages 22–31.
Jennifer Foster and Øistein E Andersen. 2009. Gen-
ERRate: generating errors for use in grammatical er-
ror detection. In Proceedings of the fourth workshop
on innovative use of NLP for building educational
applications, pages 82–90. Association for Compu-
tational Linguistics.
Jennifer Foster. 2007. Treebanks gone bad. Interna-
tional Journal of Document Analysis and Recogni-
tion (IJDAR), 10(3-4):129–145.
David Graff. 2002. The AQUAINT corpus of English
news text. Linguistic Data Consortium, Philadel-
phia.
Michael Heilman, Joel Tetreault, Aoife Cahill, Nitin
Madnani, Melissa Lopez, and Matthew Mulholland.
2014. Predicting grammaticality on an ordinal scale.
In Proceedings of ACL-2014.
Kellogg W Hunt. 1965. Grammatical structures writ-
ten at three grade levels. NCTE research report no.
3.
John Lee, Chak Yan Yeung, and Martin Chodorow.
2014. Automatic detection of comma splices. In
Proceedings of PACLIC-2014.
Nay Yee Lin, Khin Mar Soe, and Ni Lar Thein.
2011. Developing a chunk-based grammar checker
for translated English sentences. In Proceedings of
PACLIC-2011, pages 245–254.
Christopher D Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J Bethard, and David Mc-
Closky. 2014. The Stanford CoreNLP natural lan-
guage processing toolkit. In Proceedings of 52nd
Annual Meeting of the Association for Computa-
tional Linguistics: System Demonstrations, pages
55–60.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 shared task
on grammatical error correction. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, et al. 2011. Scikit-learn:
Machine learning in python. The Journal of Ma-
chine Learning Research, 12:2825–2830.
Charlene G Polio. 1997. Measures of linguistic accu-
racy in second language writing research. Language
learning, 47(1):101–143.
Matt Post. 2011. Judging grammaticality with tree
substitution grammar derivations. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies: short papers-Volume 2, pages 217–222. Asso-
ciation for Computational Linguistics.
Jonas Sj¨obergh. 2005. Chunking: an unsupervised
method to find errors in text. In Proceedings of the
15th NODALIDA conference, pages 180–185.
Joel Tetreault, Jennifer Foster, and Martin Chodorow.
2010. Using parse features for preposition selection
and error detection. In Proceedings of ACL-2010,
pages 353–358. Association for Computational Lin-
guistics.
Kristina Toutanova, Dan Klein, Christopher D Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 173–180. Association for Compu-
tational Linguistics.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploit-
ing parse structures for native language identifica-
tion. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1600–1610. Association for Computational Linguis-
tics.
Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa,
Keisuke Sakaguchi, Tomoya Mizumoto, Yuta
Hayashibe, Mamoru Komachi, and Yuji Matsumoto.
2013. NAIST at 2013 CoNLL grammatical error
correction shared task. In Proceedings of the Seven-
teenth Conference on Computational Natural Lan-
guage Learning: Shared Task, volume 26.
Longkai Zhang and Houfeng Wang. 2014. Go climb a
dependency tree and correct the grammatical errors.
In Proceedings of EMNLP-2014.
</reference>
<page confidence="0.999122">
603
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.604347">
<title confidence="0.994644">Automatic Detection of Sentence Fragments</title>
<author confidence="0.896732">Chak Yan Yeung</author>
<author confidence="0.896732">John</author>
<affiliation confidence="0.944084">Halliday Centre for Intelligent Applications of Language Department of Linguistics and City University of Hong</affiliation>
<email confidence="0.975023">jsylee@cityu.edu.hk</email>
<abstract confidence="0.984997642857143">We present and evaluate a method for automatically detecting sentence fragments in English texts written by non-native speakers. Our method combines syntactic parse tree patterns and parts-of-speech information produced by a tagger to detect this phenomenon. When evaluated on a corpus of authentic learner texts, our best model achieved a precision of 0.84 and a recall of 0.62, a statistically significant improvement over baselines using non-parse features, as well as a popular grammar checker.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Barli Bram</author>
</authors>
<title>Write Well, Improving Writing Skills.</title>
<date>1995</date>
<publisher>Kanisius.</publisher>
<contexts>
<context position="2231" citStr="Bram (1995)" startWordPosition="342" endWordPosition="343">ion defines the types of sentence fragments treated in this paper. Section 3 reviews related work. Section 4 describes the features used in our model. Section 5 discusses the datasets and section 6 analyzes the experiment results. Our best model significantly outperforms baselines that do not consider syntactic information and a widely used grammar checker. 2 Sentence Fragment Every English sentence must have a main or independent clause. Most linguists require a clause to contain a subject and a finite verb (Hunt, 1965; Polio, 1997); otherwise, it is considered a sentence fragment. Following Bram (1995), we classify sentence fragments into the following four categories: No Subject. Fragments that lack a subject,1 such as “According to the board, is $100.” No finite verb. Fragments that lack a finite verb. These may be a nonfinite verb phrase, or a noun phrase, such as “Mrs. Kern in a show.” No subject and finite verb. Fragments lacking both a subject and a finite verb; a typical example is a prepositional phrase, such as “Up through the ranks.” Subordinate clause. These fragments consist of a stand-alone subordinate clause; the clause typically begins with a relative pronoun or a subordinati</context>
<context position="10466" citStr="Bram (1995)" startWordPosition="1720" endWordPosition="1721">ry. Together with the original sentences, our training data consists of 120,000 sentences, half of which are fragments. 5.2 Evaluation Data Fragment was among the 28 error types introduced in the CoNLL-2014 shared task (Ng et al., 2014), but the test set used in the task only contained 16 such errors and is too small for our purpose. Instead, we evaluated our system on the NUCLE corpus (Dahlmeier et al., 2013), which was used as the training data in the shared task. The error label “SFrag” in the NUCLE corpus was used for sentence fragments in a wider sense than the four categories defined by Bram (1995) (see Section 2). For example, “SFrag” also labels sentences with stylistic issues, such as those beginning with “therefore” or “hence”, and sentences that, though well-formed, should be merged with its neighbor, such as “In Singapore, we can see that this problem is occurring. This is so as there is a huge discrepancy in the education levels.”. We asked two human annotators to classify the fragments into the different categories described in Section 2. The kappa was 0.84. Most of the disagreements involved sentences that contain a semi-colon which, when replaced with a comma, would become wel</context>
</contexts>
<marker>Bram, 1995</marker>
<rawString>Barli Bram. 1995. Write Well, Improving Writing Skills. Kanisius.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill Burstein</author>
<author>Martin Chodorow</author>
<author>Claudia Leacock</author>
</authors>
<title>Automated essay evaluation: The Criterion online writing service.</title>
<date>2004</date>
<journal>AI Magazine,</journal>
<volume>25</volume>
<issue>3</issue>
<contexts>
<context position="4858" citStr="Burstein et al., 2004" startWordPosition="741" endWordPosition="744">se patterns to detect specific grammatical errors was concerned with comma splices. Lee et al. (2014) manually identified distinctive production rules which, when used as features in a CRF, significantly improved the precision and recall in locating comma splices in learner text. Our method will similarly leverage parse tree patterns, but with the goal of detecting sentence fragment errors. More importantly, our approach is fully automatic, and can thus potentially be broadly applied on other syntax-related learner errors. Many commercial systems, such as the Criterion Online Writing Service (Burstein et al., 2004), Grammarly2, and WhiteSmoke3, give feedback about sentence fragments. To the best of our knowledge, these systems do not explicitly consider parse tree patterns. The grammar checker embedded in Microsoft Word also gives feedback about sentence fragments, and will serve as one of our baselines. Aside from the CoNLL-2014 shared task (see Section 1), the only other reported evaluation on detecting or correcting sentence fragments has been performed on Microsoft ESL Assistant and the NTNU Grammar Checker (Chen, 2009). Neither tool detected any of the sentence fragments in the test set. 4 Fragment</context>
</contexts>
<marker>Burstein, Chodorow, Leacock, 2004</marker>
<rawString>Jill Burstein, Martin Chodorow, and Claudia Leacock. 2004. Automated essay evaluation: The Criterion online writing service. AI Magazine, 25(3):27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao-Jan Howard Chen</author>
</authors>
<title>Evaluating two webbased grammar checkers-Microsoft ESL Assistant and NTNU statistical grammar checker.</title>
<date>2009</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="5377" citStr="Chen, 2009" startWordPosition="823" endWordPosition="824">any commercial systems, such as the Criterion Online Writing Service (Burstein et al., 2004), Grammarly2, and WhiteSmoke3, give feedback about sentence fragments. To the best of our knowledge, these systems do not explicitly consider parse tree patterns. The grammar checker embedded in Microsoft Word also gives feedback about sentence fragments, and will serve as one of our baselines. Aside from the CoNLL-2014 shared task (see Section 1), the only other reported evaluation on detecting or correcting sentence fragments has been performed on Microsoft ESL Assistant and the NTNU Grammar Checker (Chen, 2009). Neither tool detected any of the sentence fragments in the test set. 4 Fragment Detection We cast the problem of sentence fragment detection as a multiclass classification task. Given a sentence, the system would mark it either as false, if it is not a fragment, or as one of the four fragment categories described in Section 2. Rather than a binary decision on whether a sentence is a fragment, this categorisation provides more useful feedback to the learner, since each of the four fragment categories requires its own correction strategy. 4.1 Models Baseline Models. We trained three baseline m</context>
</contexts>
<marker>Chen, 2009</marker>
<rawString>Hao-Jan Howard Chen. 2009. Evaluating two webbased grammar checkers-Microsoft ESL Assistant and NTNU statistical grammar checker. Computational Linguistics and Chinese Language Processing, 14(2):161–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
</authors>
<title>Building a large annotated corpus of learner english: The NUS Corpus of Learner English.</title>
<date>2013</date>
<booktitle>In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,</booktitle>
<pages>22--31</pages>
<contexts>
<context position="10268" citStr="Dahlmeier et al., 2013" startWordPosition="1682" endWordPosition="1685">d by a comma and consists of an IN child followed by an S. The words under the SBAR are extracted as the fragment. Using this method, we created a total of 60,000 fragments, with 15,000 sentences in each category. Together with the original sentences, our training data consists of 120,000 sentences, half of which are fragments. 5.2 Evaluation Data Fragment was among the 28 error types introduced in the CoNLL-2014 shared task (Ng et al., 2014), but the test set used in the task only contained 16 such errors and is too small for our purpose. Instead, we evaluated our system on the NUCLE corpus (Dahlmeier et al., 2013), which was used as the training data in the shared task. The error label “SFrag” in the NUCLE corpus was used for sentence fragments in a wider sense than the four categories defined by Bram (1995) (see Section 2). For example, “SFrag” also labels sentences with stylistic issues, such as those beginning with “therefore” or “hence”, and sentences that, though well-formed, should be merged with its neighbor, such as “In Singapore, we can see that this problem is occurring. This is so as there is a huge discrepancy in the education levels.”. We asked two human annotators to classify the fragment</context>
</contexts>
<marker>Dahlmeier, Ng, Wu, 2013</marker>
<rawString>Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu. 2013. Building a large annotated corpus of learner english: The NUS Corpus of Learner English. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 22–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Øistein E Andersen</author>
</authors>
<title>GenERRate: generating errors for use in grammatical error detection.</title>
<date>2009</date>
<booktitle>In Proceedings of the fourth workshop on innovative use of NLP for building educational applications,</booktitle>
<pages>82--90</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9074" citStr="Foster and Andersen (2009)" startWordPosition="1472" endWordPosition="1475">ger correctly identified it as an adverb. The NP construction in the fragment was encoded as “NP600 NNP” in the “Parse” model and “NP-RB” in the “Parse + Tag” model. To reduce the number of features, we filtered out the node trigrams that occur less than ten times in the training data. Figure 1: The POS-tagged words and parse trees of the fragment “While Peter was a good boy.” and the well-formed sentence “Peter was a good boy.”. 5 Data 5.1 Training Data We automatically produced training data from the New York Times portion of the AQUAINT Corpus of English News Text (Graff, 2002). Similar to Foster and Andersen (2009), we artificially generate fragments that correspond to the four categories (Section 2) by removing different components from well-formed English sentences. For the “no subject” category, the NP immediately under the topmost S was removed. For the “no finite verb” category, we removed the finite verb in the VP immediately under the topmost S. For the “no subject and finite verb” category, we removed both the NP and the finite verb in the VP immediately under the topmost S. For the “subordinate clause” category, we looked for any SBAR in the sentence that is preceded by a comma and consists of </context>
</contexts>
<marker>Foster, Andersen, 2009</marker>
<rawString>Jennifer Foster and Øistein E Andersen. 2009. GenERRate: generating errors for use in grammatical error detection. In Proceedings of the fourth workshop on innovative use of NLP for building educational applications, pages 82–90. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
</authors>
<title>Treebanks gone bad.</title>
<date>2007</date>
<journal>International Journal of Document Analysis and Recognition (IJDAR),</journal>
<pages>10--3</pages>
<contexts>
<context position="1281" citStr="Foster, 2007" startWordPosition="192" endWordPosition="193">t improvement over baselines using non-parse features, as well as a popular grammar checker. 1 Introduction It is challenging to detect and correct sentencelevel grammatical errors because it involves automatic syntactic analysis on noisy, learner sentences. Indeed, none of the teams achieved any recall for comma splices in the most recent CoNLL shared task (Ng et al., 2014). Sentence fragments fared hardly better: of the thirteen teams, two scored a recall of 0.25 for correction and another scored 0.2; the rest did not achieve any recall. Although parser performance degrades on learner text (Foster, 2007), parsers can still be useful for identifying grammatical errors if they produce consistent patterns that indicate these errors. We show that parse tree patterns, automatically derived from training data, significantly improve system performance on detecting sentence fragments. The rest of the paper is organized as follows. The next section defines the types of sentence fragments treated in this paper. Section 3 reviews related work. Section 4 describes the features used in our model. Section 5 discusses the datasets and section 6 analyzes the experiment results. Our best model significantly o</context>
</contexts>
<marker>Foster, 2007</marker>
<rawString>Jennifer Foster. 2007. Treebanks gone bad. International Journal of Document Analysis and Recognition (IJDAR), 10(3-4):129–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<date>2002</date>
<booktitle>The AQUAINT corpus of English news text. Linguistic Data Consortium,</booktitle>
<location>Philadelphia.</location>
<contexts>
<context position="9035" citStr="Graff, 2002" startWordPosition="1468" endWordPosition="1469"> the parser while the tagger correctly identified it as an adverb. The NP construction in the fragment was encoded as “NP600 NNP” in the “Parse” model and “NP-RB” in the “Parse + Tag” model. To reduce the number of features, we filtered out the node trigrams that occur less than ten times in the training data. Figure 1: The POS-tagged words and parse trees of the fragment “While Peter was a good boy.” and the well-formed sentence “Peter was a good boy.”. 5 Data 5.1 Training Data We automatically produced training data from the New York Times portion of the AQUAINT Corpus of English News Text (Graff, 2002). Similar to Foster and Andersen (2009), we artificially generate fragments that correspond to the four categories (Section 2) by removing different components from well-formed English sentences. For the “no subject” category, the NP immediately under the topmost S was removed. For the “no finite verb” category, we removed the finite verb in the VP immediately under the topmost S. For the “no subject and finite verb” category, we removed both the NP and the finite verb in the VP immediately under the topmost S. For the “subordinate clause” category, we looked for any SBAR in the sentence that </context>
</contexts>
<marker>Graff, 2002</marker>
<rawString>David Graff. 2002. The AQUAINT corpus of English news text. Linguistic Data Consortium, Philadelphia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Heilman</author>
<author>Joel Tetreault</author>
<author>Aoife Cahill</author>
<author>Nitin Madnani</author>
<author>Melissa Lopez</author>
<author>Matthew Mulholland</author>
</authors>
<title>Predicting grammaticality on an ordinal scale.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL-2014.</booktitle>
<contexts>
<context position="3283" citStr="Heilman et al. (2014)" startWordPosition="511" endWordPosition="514">through the ranks.” Subordinate clause. These fragments consist of a stand-alone subordinate clause; the clause typically begins with a relative pronoun or a subordinating conjunction, such as “While they take pains to hide their assets.” 3 Related Work Using parse tree patterns to judge the grammaticality of a sentence is not new. Wong and Dras (2011) exploited probabilistic context-free grammar (PCFG) rules as features for native language identification. In addition to production rules, Post (2011) incorporated parse fragment features computed from derivations of tree substitution grammars. Heilman et al. (2014) used the parse scores and syntactic features to classify the comprehensibility of learner text, though they made no attempt to correct the errors. In current grammatical error correction systems, parser output is used mainly to locate 1Our evaluation data distinguishes between imperatives and fragments. Our automatic classifier, however, makes no such attempt because it would require analysis of the context and significant real-world knowledge. 599 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Lang</context>
</contexts>
<marker>Heilman, Tetreault, Cahill, Madnani, Lopez, Mulholland, 2014</marker>
<rawString>Michael Heilman, Joel Tetreault, Aoife Cahill, Nitin Madnani, Melissa Lopez, and Matthew Mulholland. 2014. Predicting grammaticality on an ordinal scale. In Proceedings of ACL-2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kellogg W Hunt</author>
</authors>
<title>Grammatical structures written at three grade levels.</title>
<date>1965</date>
<note>NCTE research report no. 3.</note>
<contexts>
<context position="2145" citStr="Hunt, 1965" startWordPosition="330" endWordPosition="331">ting sentence fragments. The rest of the paper is organized as follows. The next section defines the types of sentence fragments treated in this paper. Section 3 reviews related work. Section 4 describes the features used in our model. Section 5 discusses the datasets and section 6 analyzes the experiment results. Our best model significantly outperforms baselines that do not consider syntactic information and a widely used grammar checker. 2 Sentence Fragment Every English sentence must have a main or independent clause. Most linguists require a clause to contain a subject and a finite verb (Hunt, 1965; Polio, 1997); otherwise, it is considered a sentence fragment. Following Bram (1995), we classify sentence fragments into the following four categories: No Subject. Fragments that lack a subject,1 such as “According to the board, is $100.” No finite verb. Fragments that lack a finite verb. These may be a nonfinite verb phrase, or a noun phrase, such as “Mrs. Kern in a show.” No subject and finite verb. Fragments lacking both a subject and a finite verb; a typical example is a prepositional phrase, such as “Up through the ranks.” Subordinate clause. These fragments consist of a stand-alone su</context>
</contexts>
<marker>Hunt, 1965</marker>
<rawString>Kellogg W Hunt. 1965. Grammatical structures written at three grade levels. NCTE research report no. 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lee</author>
<author>Chak Yan Yeung</author>
<author>Martin Chodorow</author>
</authors>
<title>Automatic detection of comma splices.</title>
<date>2014</date>
<booktitle>In Proceedings of PACLIC-2014.</booktitle>
<contexts>
<context position="4337" citStr="Lee et al. (2014)" startWordPosition="662" endWordPosition="665">d knowledge. 599 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 599–603, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics relevant information involved in long-distance grammatical constructions (Tetreault et al., 2010; Yoshimoto et al., 2013; Zhang and Wang, 2014). To the best of our knowledge, the only previous work that used distinctive parse patterns to detect specific grammatical errors was concerned with comma splices. Lee et al. (2014) manually identified distinctive production rules which, when used as features in a CRF, significantly improved the precision and recall in locating comma splices in learner text. Our method will similarly leverage parse tree patterns, but with the goal of detecting sentence fragment errors. More importantly, our approach is fully automatic, and can thus potentially be broadly applied on other syntax-related learner errors. Many commercial systems, such as the Criterion Online Writing Service (Burstein et al., 2004), Grammarly2, and WhiteSmoke3, give feedback about sentence fragments. To the b</context>
</contexts>
<marker>Lee, Yeung, Chodorow, 2014</marker>
<rawString>John Lee, Chak Yan Yeung, and Martin Chodorow. 2014. Automatic detection of comma splices. In Proceedings of PACLIC-2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nay Yee Lin</author>
<author>Khin Mar Soe</author>
<author>Ni Lar Thein</author>
</authors>
<title>Developing a chunk-based grammar checker for translated English sentences.</title>
<date>2011</date>
<booktitle>In Proceedings of PACLIC-2011,</booktitle>
<pages>245--254</pages>
<contexts>
<context position="7374" citStr="Lin et al. (2011)" startWordPosition="1161" endWordPosition="1164">e syntactic constituents of the sentence, and so often reveal differences between well-formed sentences and fragments. Consider the sentence “While Peter was a good boy.”, shown in the parse tree in Figure 1. The child of the root of the tree is SBAR. When the subordinating conjunction “while” is removed to yield a well-formed sentence, the children nodes change accordingly into the expected NP and VP. In contrast, the POS tags, used in the baseline models, tend to remain the same. We use the label of the root and the trigrams of its children nodes as features, similar to Sj¨obergh (2005) and Lin et al. (2011). We also extend our patterns to grandchildren in some cases. When analyzing an ill-formed sentence, the parser can sometimes group words into constituents to which they do not belong, such as forming a VP that does not contain a verb. For example, the phrase “up the hill” was analyzed as a VP in the fragment “A new challenger up the hill” when in fact the sentence is missing a verb. To take into account such misanalyses, we also include the POS tag of the first child of all NP, VP, PP, ADVP, and ADJP as features. The first child is chosen because it often exposes the parsing error, as is the </context>
</contexts>
<marker>Lin, Soe, Thein, 2011</marker>
<rawString>Nay Yee Lin, Khin Mar Soe, and Ni Lar Thein. 2011. Developing a chunk-based grammar checker for translated English sentences. In Proceedings of PACLIC-2011, pages 245–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="11957" citStr="Manning et al., 2014" startWordPosition="1971" endWordPosition="1974">lassified as one of the Bram (1995) categories by at least one of the annotators. Most of the fragments belong to categories “no finite verb” and “subordinate clause”, accounting for 43.0% and 31.4% of the cases respectively. The categories “no subject and finite verb” and “no subject” both account for 12.8% of the cases. We left all errors in the sentences in place so as to reflect our models’ performance on authentic learner data. 6 Results We obtained the POS tags and parse trees of the sentences in our datasets with the Stanford POS tagger (Toutanova et al., 2003) and the Stanford parser (Manning et al., 2014). We used the logistic regression implementation in scikit-learn (Pedregosa et al., 2011) for the maximum entropy models in our experiments. In addition to the three baseline models described in Section 4.1, we computed a fourth baseline using the grammar VP NP Peter was a good boy While/IN Peter/NNP was/VBD a/DT good/JJ boy/NN FRAG SBAR While Peter was a good boy Peter/NNP was/VBD a/DT good/JJ boy/NN S 601 checker in Microsoft Word 2013 by configuring the checker to capture “Fragments and Run-ons” and “Fragment - stylistic suggestions”. 6.1 Fragment detection We first evaluated the systems’ a</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Ted Briscoe</author>
<author>Christian Hadiwinoto</author>
<author>Raymond Hendy Susanto</author>
<author>Christopher Bryant</author>
</authors>
<title>The CoNLL-2014 shared task on grammatical error correction.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task.</booktitle>
<contexts>
<context position="1045" citStr="Ng et al., 2014" startWordPosition="153" endWordPosition="156">ee patterns and parts-of-speech information produced by a tagger to detect this phenomenon. When evaluated on a corpus of authentic learner texts, our best model achieved a precision of 0.84 and a recall of 0.62, a statistically significant improvement over baselines using non-parse features, as well as a popular grammar checker. 1 Introduction It is challenging to detect and correct sentencelevel grammatical errors because it involves automatic syntactic analysis on noisy, learner sentences. Indeed, none of the teams achieved any recall for comma splices in the most recent CoNLL shared task (Ng et al., 2014). Sentence fragments fared hardly better: of the thirteen teams, two scored a recall of 0.25 for correction and another scored 0.2; the rest did not achieve any recall. Although parser performance degrades on learner text (Foster, 2007), parsers can still be useful for identifying grammatical errors if they produce consistent patterns that indicate these errors. We show that parse tree patterns, automatically derived from training data, significantly improve system performance on detecting sentence fragments. The rest of the paper is organized as follows. The next section defines the types of </context>
<context position="10091" citStr="Ng et al., 2014" startWordPosition="1648" endWordPosition="1651">d both the NP and the finite verb in the VP immediately under the topmost S. For the “subordinate clause” category, we looked for any SBAR in the sentence that is preceded by a comma and consists of an IN child followed by an S. The words under the SBAR are extracted as the fragment. Using this method, we created a total of 60,000 fragments, with 15,000 sentences in each category. Together with the original sentences, our training data consists of 120,000 sentences, half of which are fragments. 5.2 Evaluation Data Fragment was among the 28 error types introduced in the CoNLL-2014 shared task (Ng et al., 2014), but the test set used in the task only contained 16 such errors and is too small for our purpose. Instead, we evaluated our system on the NUCLE corpus (Dahlmeier et al., 2013), which was used as the training data in the shared task. The error label “SFrag” in the NUCLE corpus was used for sentence fragments in a wider sense than the four categories defined by Bram (1995) (see Section 2). For example, “SFrag” also labels sentences with stylistic issues, such as those beginning with “therefore” or “hence”, and sentences that, though well-formed, should be merged with its neighbor, such as “In </context>
</contexts>
<marker>Ng, Wu, Briscoe, Hadiwinoto, Susanto, Bryant, 2014</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher Bryant. 2014. The CoNLL-2014 shared task on grammatical error correction. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand</author>
</authors>
<title>Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer,</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, et</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Bertrand, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in python. The Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charlene G Polio</author>
</authors>
<title>Measures of linguistic accuracy in second language writing research. Language learning,</title>
<date>1997</date>
<pages>47--1</pages>
<contexts>
<context position="2159" citStr="Polio, 1997" startWordPosition="332" endWordPosition="333">e fragments. The rest of the paper is organized as follows. The next section defines the types of sentence fragments treated in this paper. Section 3 reviews related work. Section 4 describes the features used in our model. Section 5 discusses the datasets and section 6 analyzes the experiment results. Our best model significantly outperforms baselines that do not consider syntactic information and a widely used grammar checker. 2 Sentence Fragment Every English sentence must have a main or independent clause. Most linguists require a clause to contain a subject and a finite verb (Hunt, 1965; Polio, 1997); otherwise, it is considered a sentence fragment. Following Bram (1995), we classify sentence fragments into the following four categories: No Subject. Fragments that lack a subject,1 such as “According to the board, is $100.” No finite verb. Fragments that lack a finite verb. These may be a nonfinite verb phrase, or a noun phrase, such as “Mrs. Kern in a show.” No subject and finite verb. Fragments lacking both a subject and a finite verb; a typical example is a prepositional phrase, such as “Up through the ranks.” Subordinate clause. These fragments consist of a stand-alone subordinate clau</context>
</contexts>
<marker>Polio, 1997</marker>
<rawString>Charlene G Polio. 1997. Measures of linguistic accuracy in second language writing research. Language learning, 47(1):101–143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
</authors>
<title>Judging grammaticality with tree substitution grammar derivations.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2,</booktitle>
<pages>217--222</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3167" citStr="Post (2011)" startWordPosition="496" endWordPosition="497">gments lacking both a subject and a finite verb; a typical example is a prepositional phrase, such as “Up through the ranks.” Subordinate clause. These fragments consist of a stand-alone subordinate clause; the clause typically begins with a relative pronoun or a subordinating conjunction, such as “While they take pains to hide their assets.” 3 Related Work Using parse tree patterns to judge the grammaticality of a sentence is not new. Wong and Dras (2011) exploited probabilistic context-free grammar (PCFG) rules as features for native language identification. In addition to production rules, Post (2011) incorporated parse fragment features computed from derivations of tree substitution grammars. Heilman et al. (2014) used the parse scores and syntactic features to classify the comprehensibility of learner text, though they made no attempt to correct the errors. In current grammatical error correction systems, parser output is used mainly to locate 1Our evaluation data distinguishes between imperatives and fragments. Our automatic classifier, however, makes no such attempt because it would require analysis of the context and significant real-world knowledge. 599 Proceedings of the 53rd Annual</context>
</contexts>
<marker>Post, 2011</marker>
<rawString>Matt Post. 2011. Judging grammaticality with tree substitution grammar derivations. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 217–222. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonas Sj¨obergh</author>
</authors>
<title>Chunking: an unsupervised method to find errors in text.</title>
<date>2005</date>
<booktitle>In Proceedings of the 15th NODALIDA conference,</booktitle>
<pages>180--185</pages>
<marker>Sj¨obergh, 2005</marker>
<rawString>Jonas Sj¨obergh. 2005. Chunking: an unsupervised method to find errors in text. In Proceedings of the 15th NODALIDA conference, pages 180–185.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Jennifer Foster</author>
<author>Martin Chodorow</author>
</authors>
<title>Using parse features for preposition selection and error detection.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL-2010,</booktitle>
<pages>353--358</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4109" citStr="Tetreault et al., 2010" startWordPosition="625" endWordPosition="628">output is used mainly to locate 1Our evaluation data distinguishes between imperatives and fragments. Our automatic classifier, however, makes no such attempt because it would require analysis of the context and significant real-world knowledge. 599 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 599–603, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics relevant information involved in long-distance grammatical constructions (Tetreault et al., 2010; Yoshimoto et al., 2013; Zhang and Wang, 2014). To the best of our knowledge, the only previous work that used distinctive parse patterns to detect specific grammatical errors was concerned with comma splices. Lee et al. (2014) manually identified distinctive production rules which, when used as features in a CRF, significantly improved the precision and recall in locating comma splices in learner text. Our method will similarly leverage parse tree patterns, but with the goal of detecting sentence fragment errors. More importantly, our approach is fully automatic, and can thus potentially be </context>
</contexts>
<marker>Tetreault, Foster, Chodorow, 2010</marker>
<rawString>Joel Tetreault, Jennifer Foster, and Martin Chodorow. 2010. Using parse features for preposition selection and error detection. In Proceedings of ACL-2010, pages 353–358. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-ofspeech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11910" citStr="Toutanova et al., 2003" startWordPosition="1963" endWordPosition="1966"> the 249 sentences marked as fragments, 86 were classified as one of the Bram (1995) categories by at least one of the annotators. Most of the fragments belong to categories “no finite verb” and “subordinate clause”, accounting for 43.0% and 31.4% of the cases respectively. The categories “no subject and finite verb” and “no subject” both account for 12.8% of the cases. We left all errors in the sentences in place so as to reflect our models’ performance on authentic learner data. 6 Results We obtained the POS tags and parse trees of the sentences in our datasets with the Stanford POS tagger (Toutanova et al., 2003) and the Stanford parser (Manning et al., 2014). We used the logistic regression implementation in scikit-learn (Pedregosa et al., 2011) for the maximum entropy models in our experiments. In addition to the three baseline models described in Section 4.1, we computed a fourth baseline using the grammar VP NP Peter was a good boy While/IN Peter/NNP was/VBD a/DT good/JJ boy/NN FRAG SBAR While Peter was a good boy Peter/NNP was/VBD a/DT good/JJ boy/NN S 601 checker in Microsoft Word 2013 by configuring the checker to capture “Fragments and Run-ons” and “Fragment - stylistic suggestions”. 6.1 Fragm</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer. 2003. Feature-rich part-ofspeech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language TechnologyVolume 1, pages 173–180. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Exploiting parse structures for native language identification.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1600--1610</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3016" citStr="Wong and Dras (2011)" startWordPosition="474" endWordPosition="477"> verb. Fragments that lack a finite verb. These may be a nonfinite verb phrase, or a noun phrase, such as “Mrs. Kern in a show.” No subject and finite verb. Fragments lacking both a subject and a finite verb; a typical example is a prepositional phrase, such as “Up through the ranks.” Subordinate clause. These fragments consist of a stand-alone subordinate clause; the clause typically begins with a relative pronoun or a subordinating conjunction, such as “While they take pains to hide their assets.” 3 Related Work Using parse tree patterns to judge the grammaticality of a sentence is not new. Wong and Dras (2011) exploited probabilistic context-free grammar (PCFG) rules as features for native language identification. In addition to production rules, Post (2011) incorporated parse fragment features computed from derivations of tree substitution grammars. Heilman et al. (2014) used the parse scores and syntactic features to classify the comprehensibility of learner text, though they made no attempt to correct the errors. In current grammatical error correction systems, parser output is used mainly to locate 1Our evaluation data distinguishes between imperatives and fragments. Our automatic classifier, h</context>
</contexts>
<marker>Wong, Dras, 2011</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting parse structures for native language identification. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1600–1610. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ippei Yoshimoto</author>
</authors>
<title>Tomoya Kose, Kensuke Mitsuzawa, Keisuke Sakaguchi, Tomoya Mizumoto, Yuta Hayashibe, Mamoru Komachi, and Yuji Matsumoto.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<volume>26</volume>
<marker>Yoshimoto, 2013</marker>
<rawString>Ippei Yoshimoto, Tomoya Kose, Kensuke Mitsuzawa, Keisuke Sakaguchi, Tomoya Mizumoto, Yuta Hayashibe, Mamoru Komachi, and Yuji Matsumoto. 2013. NAIST at 2013 CoNLL grammatical error correction shared task. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, volume 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longkai Zhang</author>
<author>Houfeng Wang</author>
</authors>
<title>Go climb a dependency tree and correct the grammatical errors.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP-2014.</booktitle>
<contexts>
<context position="4156" citStr="Zhang and Wang, 2014" startWordPosition="633" endWordPosition="636">data distinguishes between imperatives and fragments. Our automatic classifier, however, makes no such attempt because it would require analysis of the context and significant real-world knowledge. 599 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 599–603, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics relevant information involved in long-distance grammatical constructions (Tetreault et al., 2010; Yoshimoto et al., 2013; Zhang and Wang, 2014). To the best of our knowledge, the only previous work that used distinctive parse patterns to detect specific grammatical errors was concerned with comma splices. Lee et al. (2014) manually identified distinctive production rules which, when used as features in a CRF, significantly improved the precision and recall in locating comma splices in learner text. Our method will similarly leverage parse tree patterns, but with the goal of detecting sentence fragment errors. More importantly, our approach is fully automatic, and can thus potentially be broadly applied on other syntax-related learner</context>
</contexts>
<marker>Zhang, Wang, 2014</marker>
<rawString>Longkai Zhang and Houfeng Wang. 2014. Go climb a dependency tree and correct the grammatical errors. In Proceedings of EMNLP-2014.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>