<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030544">
<title confidence="0.973244">
The Clarity-Brevity Trade-off in Generating Referring Expressions *
</title>
<author confidence="0.94758">
Imtiaz Hussain Khan and Graeme Ritchie and Kees van Deemter
</author>
<affiliation confidence="0.9974145">
Department of Computing Science
University of Aberdeen
</affiliation>
<address confidence="0.91426">
Aberdeen AB24 3UE, U.K.
</address>
<email confidence="0.999588">
{ikhan,gritchie,kvdeemte}@csd.abdn.ac.uk
</email>
<sectionHeader confidence="0.994817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902666666666">
Existing algorithms for the Generation of
Referring Expressions (GRE) aim at gen-
erating descriptions that allow a hearer to
identify its intended referent uniquely; the
length of the expression is also considered,
usually as a secondary issue. We explore
the possibility of making the trade-off be-
tween these two factors more explicit, via
a general cost function which scores these
two aspects separately. We sketch some
more complex phenomena which might be
amenable to this treatment.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999393">
Until recently, GRE algorithms have focussed on
the generation of distinguishing descriptions that
are either as short as possible (e.g. (Dale, 1992;
Gardent, 2002)) or almost as short as possible (e.g.
(Dale and Reiter, 1995)). Since reductions in am-
biguity are achieved by increases in length, there
is a tension between these factors, and algorithms
usually resolve this in some fixed way. However,
the need for a distinguishing description is usually
assumed, and typically built in to GRE algorithms.
We will suggest a way to make explicit this bal-
ance between clarity (i.e. lack of ambiguity) and
brevity, and we indicate some phenomena which
we believe may be illuminated by this approach.
The ideas in this paper can be seen as a loosen-
ing of some of the many simplifying assumptions
often made in GRE work.
</bodyText>
<footnote confidence="0.9983265">
*This work is supported by a University of Aberdeen
Sixth Century Studentship, and the TUNA project (EPSRC,
UK) under grant number GR/S13330/01. We thank Ielka van
der Sluis and Albert Gatt for valuable comments.
</footnote>
<subsectionHeader confidence="0.361961">
2 Clarity, Brevity and Cost
</subsectionHeader>
<bodyText confidence="0.984556210526316">
We consider only simple GRE, where the aim is to
construct a conjunction of unary properties which
distinguish a single target object from a set of po-
tential distractors. Our notation is as follows. A
domain consists of a set D of objects, and a set P
of properties applicable to objects in D. A descrip-
tion is a subset of P. The denotation of S, written
QS],is{xED  |bpES:p(x)}.
(Krahmer et al., 2003) describe an approach to
GRE in which a cost function guides search for a
suitable description, and show that some existing
GRE algorithms fit into this framework. However,
they follow the practice of concentrating solely on
distinguishing descriptions, treating cost as a mat-
ter of brevity. We suggest that decomposing cost
into two components, for the clarity and brevity
of descriptions, permits the examination of trade-
offs. For now, we will take the cost of a description
S to be the sum of two terms:
</bodyText>
<equation confidence="0.998861">
cost(S) = fC(S) + fB(S).
</equation>
<bodyText confidence="0.9991296">
where fC counts ambiguity (lack of clarity) and
fB counts size (lack of brevity). Even with this
decomposition of cost, some existing algorithms
can still be seen as cost-minimisation. For exam-
ple, the cost functions:
</bodyText>
<equation confidence="0.9999425">
fC(S) = |P  |X  |Q S ] |
fB(S) =  |S |
</equation>
<bodyText confidence="0.999542">
allow the Full Brevity algorithm (Dale, 1992) to
be viewed as minimising cost(S), and the in-
cremental algorithm (Dale and Reiter, 1995) as
hill-climbing (strictly, hill-descending), guided by
the property-ordering which that algorithm re-
quires. Whereas Krahmer et al.’s cost functions
are (brevity-based) heuristic guidance functions,
our alternative here is a global quantity for opti-
misation. Hence their simulation of Full Brevity
</bodyText>
<page confidence="0.990344">
89
</page>
<bodyText confidence="0.9338438">
Proceedings of the Fourth International Natural Language Generation Conference, pages 89–91,
Sydney, July 2006. c�2006 Association for Computational Linguistics
relies on the details of their algorithm (rather than
cost) to ensure clarity, while our own cost function
ensures both brevity and clarity.
</bodyText>
<sectionHeader confidence="0.994466" genericHeader="method">
3 Exploring the Trade-off
</sectionHeader>
<subsectionHeader confidence="0.999896">
3.1 Varying penalties for distractors
</subsectionHeader>
<bodyText confidence="0.9999895625">
Imagine the following situation. You are prepar-
ing a meal in a friend’s house, and you wish to
obtain, from your own kitchen, a bottle of Italian
extra virgin olive oil which you know is there. The
only way open to you is to phone home and ask
your young child to bring it round for you. You
know that also in your kitchen cupboard are some
distractors: one bottle each of Spanish extra virgin
olive oil, Italian non-virgin olive oil, cheap veg-
etable oil, linseed oil (for varnishing) and cam-
phorated oil (medicinal). It is imperative that you
do not get the linseed or camphorated oil, and
preferable that you receive olive oil. A full ex-
pression, Italian extra virgin olive oil, guarantees
clarity, but may overload your helper’s abilities. A
very short expression, oil, is risky. You might well
settle for the intermediate olive oil.
To model this situation, fC could take a much
higher value if Q S ] contains a distractor which
must not be selected (e.g. varnish rather than cook-
ing oil). That is, instead of a simple linear function
of the size of Q S ], there is a curve where the cost
drops more steeply as the more undesirable dis-
tractors are excluded. For example, each object
could be assigned a numerical rating of how unde-
sirable it is, with the target having a score of zero,
and the fC value for a set A could be the maxi-
mum rating of any element of A. (This would, of
course, require a suitably rich domain model.)
The brevity cost function fB could still be a rel-
atively simple linear function, providing fB values
do not mask the effect of the shape of the fC curve.
</bodyText>
<subsectionHeader confidence="0.99991">
3.2 Fuzziness of target
</subsectionHeader>
<bodyText confidence="0.965871068965517">
Suppose Mrs X has dropped a piece of raw
chicken meat on the kitchen table, and immedi-
ately removed the meat. She would now like Mr
X to wipe the area clean. The meat leaves no visi-
ble stain, so she has to explain where it was. In this
case, it appears that there is no such thing as a dis-
tinguishing description (i.e. a description that pins
down the area precisely), although Mrs X can ar-
bitrarily increase precision, by adding properties:
– the edge of the table,
– the edge of the table, on the left (etc.)
The ideal description would describe the dirty area
and nothing more, but a larger area will also do,
if not too large. Here, the domain D is implic-
itly defined as all conceivable subareas of the ta-
ble, the target is again one element of D, but – un-
like the traditional set-up with discrete elements –
a description (fuzzily) defines one such area, not
a disjoint collection of individual items. Our fC
operates on the description S, not just on the num-
ber of distractors, so it can assess the aptness of
the denotation of any potential S. However, it has
to ensure that this denotation (subarea of the sur-
face) contains the target (contaminated area), and
does not contain too much beyond that. Hence,
we may need to augment our clarity cost function
with another argument: the target itself. In gen-
eral, more complex domains may need more com-
plicated functions.
</bodyText>
<subsectionHeader confidence="0.996829">
3.3 Underspecification in dialogue
</subsectionHeader>
<bodyText confidence="0.999773125">
Standard GRE algorithms assume that the speaker
knows what the hearer knows (Dale and Reiter,
1995). In practice, speakers can often only guess.
It has been observed that speakers sometimes pro-
duce referring expressions that are only disam-
biguated through negotiation with the hearer, as
exemplified in the following excerpt (quoted in
(Hirst, 2002)).
</bodyText>
<listItem confidence="0.999858666666667">
1. A: What’s that weird creature over there?
2. B: In the corner?
3. A: [affirmative noise]
4. B: It’s just a fern plant.
5. A: No, the one to the left of it.
6. B: That’s the television aerial. It pulls out.
</listItem>
<bodyText confidence="0.975766117647059">
A and B are in the same room, in an informal set-
ting, so A can be relatively interactive in convey-
ing information. Also, the situation does not ap-
pear to be highly critical, in comparison to a mil-
itary officer directing gunfire, or a surgeon guid-
ing an incision. Initially, A produces an expres-
sion which is not very detailed. It may be that he
thinks this is adequate (the object is sufficiently
salient that B will uniquely determine the refer-
ent), or he doesn’t really know, but is willing to
make an opening bid in a negotiation to reach the
goal of reference. In the former case, a GRE algo-
rithm which took account of salience (e.g. (Krah-
mer and Theune, 1999)), operating with A’s model
of B’s knowledge, should produce this sort of ef-
fect. (A dialogue model might also be needed.) In
the latter case, we need an algorithm which can
</bodyText>
<page confidence="0.995153">
90
</page>
<bodyText confidence="0.999874125">
this approach will ultimately shed light not only
on the effect of the discourse situation, but also
some aspects of generating indefinite descriptions.
relax the need for complete clarity. This could be
arranged by having fC give similar scores to deno-
tations where there are no distractors and to deno-
tations where there are just a few distractors, with
fB making a large contribution to the cost.
</bodyText>
<sectionHeader confidence="0.765377" genericHeader="method">
References
</sectionHeader>
<subsectionHeader confidence="0.998076">
3.4 Over-specification
</subsectionHeader>
<bodyText confidence="0.999985851851852">
Recently, interest has been growing in ‘overspec-
ified’ referring expressions, which contain more
information than is required to identify their in-
tended referent. Some of this work is mainly or ex-
clusively experimental (Jordan and Walker, 2000;
Arts, 2004), but algorithmic consequences are also
being explored (Horacek, 2005; Paraboni and van
Deemter, 2002; van der Sluis and Krahmer, 2005).
Over-specification could also arise in a dialogue
situation (comparable to that in Section 3.3) if a
speaker is unclear about the hearer’s knowledge,
and so over-specifies (relative to his own knowl-
edge) to increase the chances of success.
This goes beyond the classical algorithms,
where the main goal is total clarity, with no rea-
son for the algorithm to add further properties to
an already unambiguous expression. That is, such
algorithms assume that every description S for
which I Q S ] J= 1 has the same level of clarity
(fC value). This assumption could be relaxed. For
example, the approach of (Horacek, 2005) to GRE
allows degrees of uncertainty about the effective-
ness of properties to affect their selection. Within
such a framework, one could separately compute
costs for clarity (e.g. likelihood of being under-
stood) and brevity (which might include the com-
plexity of expressing the properties).
</bodyText>
<sectionHeader confidence="0.996502" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9996314">
We have argued that the GRE task becomes very
different when some commonly-made assump-
tions are abandoned: some distractors might be
worse than others (section 3.1); the target may be
impossible to distinguish precisely (section 3.2);
the speaker may be unsure what the hearer knows
(section 3.3); or there may be a need for over-
specification (section 3.4)). As a result, it may be
necessary to consider other aspects of the descrip-
tions and their denotations, not simply counting
distractors or numbers of properties. Some effects
could perhaps be modelled using costs which are
not simple linear functions, but which give varying
importance to particular aspects of the denotation
of a description, or of its content. We hope that
</bodyText>
<reference confidence="0.996349744680851">
Anja Arts. 2004. Overspecification in Instructive Text.
Ph.D. thesis, Tilburg University, The Netherlands.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18:233–263.
Robert Dale. 1992. Generating Referring Expres-
sions: Building Descriptions in a Domain of Objects
and Processes. MIT Press.
Claire Gardent. 2002. Generating minimal distin-
guishing descriptions. In Proceedings of the 40th
Annual Meeting of the ACL (ACL’02), Philadelphia,
USA.
Graeme Hirst. 2002. Negotiation, compromise, and
collaboration in interpersonal and human–computer
conversations. In Proceedings of Workshop on
Meaning Negotiation, 18th National Conference
on Artificial Intelligence, pages 1–4, Edmonton,
Canada.
Helmut Horacek. 2005. Generating referential de-
scriptions under conditions of uncertainty. In Gra-
ham Wilcock, Kristiina Jokinen, Chris Mellish, and
Ehud Reiter, editors, Proceedings of the 10th Eu-
ropean Workshop on Natural Language Generation
(ENLG-05), pages 58–67.
Pamela Jordan and Marilyn Walker. 2000. Learning
attribute selections for non-pronominal expressions.
In Proceedings of the 38th Annual Meeting of the
ACL (ACL-00), pages 181–190.
Emiel Krahmer and Mari¨et Theune. 1999. Efficient
generation of descriptions in context. In Proceed-
ings of the ESSLLI workshop on the generation of
nominals, Utrecht, The Netherlands.
Emiel Krahmer, Sebastiaan van Erk, and Andr´e Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53–72.
Ivandr´e Paraboni and Kees van Deemter. 2002. Gener-
ating easy references: the case of document deixis.
In Proceedings of the Second International Confer-
ence on Natural Language Generation, New York,
USA.
Ielka van der Sluis and Emiel Krahmer. 2005. Towards
the generation of overspecified multimodal referring
expressions. In Proceedings of the Symposium on
Dialogue Modelling and Generation at the 15th An-
nual Meeting of the ST &amp; D (STD-05), Amsterdam,
The Netherlands.
</reference>
<page confidence="0.999148">
91
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.353045">
<title confidence="0.999867">Clarity-Brevity Trade-off in Generating Referring Expressions</title>
<author confidence="0.999953">Hussain Khan Ritchie van</author>
<affiliation confidence="0.985622">Department of Computing University of</affiliation>
<note confidence="0.386664">Aberdeen AB24 3UE,</note>
<abstract confidence="0.998713615384616">Existing algorithms for the Generation of Expressions aim at generating descriptions that allow a hearer to identify its intended referent uniquely; the length of the expression is also considered, usually as a secondary issue. We explore the possibility of making the trade-off between these two factors more explicit, via a general cost function which scores these two aspects separately. We sketch some more complex phenomena which might be amenable to this treatment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anja Arts</author>
</authors>
<title>Overspecification in Instructive Text.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Tilburg University, The Netherlands.</institution>
<contexts>
<context position="8918" citStr="Arts, 2004" startWordPosition="1520" endWordPosition="1521">e situation, but also some aspects of generating indefinite descriptions. relax the need for complete clarity. This could be arranged by having fC give similar scores to denotations where there are no distractors and to denotations where there are just a few distractors, with fB making a large contribution to the cost. References 3.4 Over-specification Recently, interest has been growing in ‘overspecified’ referring expressions, which contain more information than is required to identify their intended referent. Some of this work is mainly or exclusively experimental (Jordan and Walker, 2000; Arts, 2004), but algorithmic consequences are also being explored (Horacek, 2005; Paraboni and van Deemter, 2002; van der Sluis and Krahmer, 2005). Over-specification could also arise in a dialogue situation (comparable to that in Section 3.3) if a speaker is unclear about the hearer’s knowledge, and so over-specifies (relative to his own knowledge) to increase the chances of success. This goes beyond the classical algorithms, where the main goal is total clarity, with no reason for the algorithm to add further properties to an already unambiguous expression. That is, such algorithms assume that every de</context>
</contexts>
<marker>Arts, 2004</marker>
<rawString>Anja Arts. 2004. Overspecification in Instructive Text. Ph.D. thesis, Tilburg University, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<pages>18--233</pages>
<contexts>
<context position="986" citStr="Dale and Reiter, 1995" startWordPosition="140" endWordPosition="143">llow a hearer to identify its intended referent uniquely; the length of the expression is also considered, usually as a secondary issue. We explore the possibility of making the trade-off between these two factors more explicit, via a general cost function which scores these two aspects separately. We sketch some more complex phenomena which might be amenable to this treatment. 1 Introduction Until recently, GRE algorithms have focussed on the generation of distinguishing descriptions that are either as short as possible (e.g. (Dale, 1992; Gardent, 2002)) or almost as short as possible (e.g. (Dale and Reiter, 1995)). Since reductions in ambiguity are achieved by increases in length, there is a tension between these factors, and algorithms usually resolve this in some fixed way. However, the need for a distinguishing description is usually assumed, and typically built in to GRE algorithms. We will suggest a way to make explicit this balance between clarity (i.e. lack of ambiguity) and brevity, and we indicate some phenomena which we believe may be illuminated by this approach. The ideas in this paper can be seen as a loosening of some of the many simplifying assumptions often made in GRE work. *This work</context>
<context position="3139" citStr="Dale and Reiter, 1995" startWordPosition="512" endWordPosition="515">est that decomposing cost into two components, for the clarity and brevity of descriptions, permits the examination of tradeoffs. For now, we will take the cost of a description S to be the sum of two terms: cost(S) = fC(S) + fB(S). where fC counts ambiguity (lack of clarity) and fB counts size (lack of brevity). Even with this decomposition of cost, some existing algorithms can still be seen as cost-minimisation. For example, the cost functions: fC(S) = |P |X |Q S ] | fB(S) = |S | allow the Full Brevity algorithm (Dale, 1992) to be viewed as minimising cost(S), and the incremental algorithm (Dale and Reiter, 1995) as hill-climbing (strictly, hill-descending), guided by the property-ordering which that algorithm requires. Whereas Krahmer et al.’s cost functions are (brevity-based) heuristic guidance functions, our alternative here is a global quantity for optimisation. Hence their simulation of Full Brevity 89 Proceedings of the Fourth International Natural Language Generation Conference, pages 89–91, Sydney, July 2006. c�2006 Association for Computational Linguistics relies on the details of their algorithm (rather than cost) to ensure clarity, while our own cost function ensures both brevity and clari</context>
<context position="6925" citStr="Dale and Reiter, 1995" startWordPosition="1172" endWordPosition="1175">f individual items. Our fC operates on the description S, not just on the number of distractors, so it can assess the aptness of the denotation of any potential S. However, it has to ensure that this denotation (subarea of the surface) contains the target (contaminated area), and does not contain too much beyond that. Hence, we may need to augment our clarity cost function with another argument: the target itself. In general, more complex domains may need more complicated functions. 3.3 Underspecification in dialogue Standard GRE algorithms assume that the speaker knows what the hearer knows (Dale and Reiter, 1995). In practice, speakers can often only guess. It has been observed that speakers sometimes produce referring expressions that are only disambiguated through negotiation with the hearer, as exemplified in the following excerpt (quoted in (Hirst, 2002)). 1. A: What’s that weird creature over there? 2. B: In the corner? 3. A: [affirmative noise] 4. B: It’s just a fern plant. 5. A: No, the one to the left of it. 6. B: That’s the television aerial. It pulls out. A and B are in the same room, in an informal setting, so A can be relatively interactive in conveying information. Also, the situation doe</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 18:233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Generating Referring Expressions: Building Descriptions in a Domain of Objects and Processes.</title>
<date>1992</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="908" citStr="Dale, 1992" startWordPosition="129" endWordPosition="130"> Referring Expressions (GRE) aim at generating descriptions that allow a hearer to identify its intended referent uniquely; the length of the expression is also considered, usually as a secondary issue. We explore the possibility of making the trade-off between these two factors more explicit, via a general cost function which scores these two aspects separately. We sketch some more complex phenomena which might be amenable to this treatment. 1 Introduction Until recently, GRE algorithms have focussed on the generation of distinguishing descriptions that are either as short as possible (e.g. (Dale, 1992; Gardent, 2002)) or almost as short as possible (e.g. (Dale and Reiter, 1995)). Since reductions in ambiguity are achieved by increases in length, there is a tension between these factors, and algorithms usually resolve this in some fixed way. However, the need for a distinguishing description is usually assumed, and typically built in to GRE algorithms. We will suggest a way to make explicit this balance between clarity (i.e. lack of ambiguity) and brevity, and we indicate some phenomena which we believe may be illuminated by this approach. The ideas in this paper can be seen as a loosening </context>
<context position="3049" citStr="Dale, 1992" startWordPosition="499" endWordPosition="500">y on distinguishing descriptions, treating cost as a matter of brevity. We suggest that decomposing cost into two components, for the clarity and brevity of descriptions, permits the examination of tradeoffs. For now, we will take the cost of a description S to be the sum of two terms: cost(S) = fC(S) + fB(S). where fC counts ambiguity (lack of clarity) and fB counts size (lack of brevity). Even with this decomposition of cost, some existing algorithms can still be seen as cost-minimisation. For example, the cost functions: fC(S) = |P |X |Q S ] | fB(S) = |S | allow the Full Brevity algorithm (Dale, 1992) to be viewed as minimising cost(S), and the incremental algorithm (Dale and Reiter, 1995) as hill-climbing (strictly, hill-descending), guided by the property-ordering which that algorithm requires. Whereas Krahmer et al.’s cost functions are (brevity-based) heuristic guidance functions, our alternative here is a global quantity for optimisation. Hence their simulation of Full Brevity 89 Proceedings of the Fourth International Natural Language Generation Conference, pages 89–91, Sydney, July 2006. c�2006 Association for Computational Linguistics relies on the details of their algorithm (rathe</context>
</contexts>
<marker>Dale, 1992</marker>
<rawString>Robert Dale. 1992. Generating Referring Expressions: Building Descriptions in a Domain of Objects and Processes. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Gardent</author>
</authors>
<title>Generating minimal distinguishing descriptions.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the ACL (ACL’02),</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="924" citStr="Gardent, 2002" startWordPosition="131" endWordPosition="132">xpressions (GRE) aim at generating descriptions that allow a hearer to identify its intended referent uniquely; the length of the expression is also considered, usually as a secondary issue. We explore the possibility of making the trade-off between these two factors more explicit, via a general cost function which scores these two aspects separately. We sketch some more complex phenomena which might be amenable to this treatment. 1 Introduction Until recently, GRE algorithms have focussed on the generation of distinguishing descriptions that are either as short as possible (e.g. (Dale, 1992; Gardent, 2002)) or almost as short as possible (e.g. (Dale and Reiter, 1995)). Since reductions in ambiguity are achieved by increases in length, there is a tension between these factors, and algorithms usually resolve this in some fixed way. However, the need for a distinguishing description is usually assumed, and typically built in to GRE algorithms. We will suggest a way to make explicit this balance between clarity (i.e. lack of ambiguity) and brevity, and we indicate some phenomena which we believe may be illuminated by this approach. The ideas in this paper can be seen as a loosening of some of the m</context>
</contexts>
<marker>Gardent, 2002</marker>
<rawString>Claire Gardent. 2002. Generating minimal distinguishing descriptions. In Proceedings of the 40th Annual Meeting of the ACL (ACL’02), Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graeme Hirst</author>
</authors>
<title>Negotiation, compromise, and collaboration in interpersonal and human–computer conversations.</title>
<date>2002</date>
<booktitle>In Proceedings of Workshop on Meaning Negotiation, 18th National Conference on Artificial Intelligence,</booktitle>
<pages>1--4</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="7175" citStr="Hirst, 2002" startWordPosition="1212" endWordPosition="1213">taminated area), and does not contain too much beyond that. Hence, we may need to augment our clarity cost function with another argument: the target itself. In general, more complex domains may need more complicated functions. 3.3 Underspecification in dialogue Standard GRE algorithms assume that the speaker knows what the hearer knows (Dale and Reiter, 1995). In practice, speakers can often only guess. It has been observed that speakers sometimes produce referring expressions that are only disambiguated through negotiation with the hearer, as exemplified in the following excerpt (quoted in (Hirst, 2002)). 1. A: What’s that weird creature over there? 2. B: In the corner? 3. A: [affirmative noise] 4. B: It’s just a fern plant. 5. A: No, the one to the left of it. 6. B: That’s the television aerial. It pulls out. A and B are in the same room, in an informal setting, so A can be relatively interactive in conveying information. Also, the situation does not appear to be highly critical, in comparison to a military officer directing gunfire, or a surgeon guiding an incision. Initially, A produces an expression which is not very detailed. It may be that he thinks this is adequate (the object is suff</context>
</contexts>
<marker>Hirst, 2002</marker>
<rawString>Graeme Hirst. 2002. Negotiation, compromise, and collaboration in interpersonal and human–computer conversations. In Proceedings of Workshop on Meaning Negotiation, 18th National Conference on Artificial Intelligence, pages 1–4, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Horacek</author>
</authors>
<title>Generating referential descriptions under conditions of uncertainty. In</title>
<date>2005</date>
<booktitle>Proceedings of the 10th European Workshop on Natural Language Generation (ENLG-05),</booktitle>
<pages>58--67</pages>
<editor>Graham Wilcock, Kristiina Jokinen, Chris Mellish, and Ehud Reiter, editors,</editor>
<contexts>
<context position="8987" citStr="Horacek, 2005" startWordPosition="1529" endWordPosition="1530">tions. relax the need for complete clarity. This could be arranged by having fC give similar scores to denotations where there are no distractors and to denotations where there are just a few distractors, with fB making a large contribution to the cost. References 3.4 Over-specification Recently, interest has been growing in ‘overspecified’ referring expressions, which contain more information than is required to identify their intended referent. Some of this work is mainly or exclusively experimental (Jordan and Walker, 2000; Arts, 2004), but algorithmic consequences are also being explored (Horacek, 2005; Paraboni and van Deemter, 2002; van der Sluis and Krahmer, 2005). Over-specification could also arise in a dialogue situation (comparable to that in Section 3.3) if a speaker is unclear about the hearer’s knowledge, and so over-specifies (relative to his own knowledge) to increase the chances of success. This goes beyond the classical algorithms, where the main goal is total clarity, with no reason for the algorithm to add further properties to an already unambiguous expression. That is, such algorithms assume that every description S for which I Q S ] J= 1 has the same level of clarity (fC </context>
</contexts>
<marker>Horacek, 2005</marker>
<rawString>Helmut Horacek. 2005. Generating referential descriptions under conditions of uncertainty. In Graham Wilcock, Kristiina Jokinen, Chris Mellish, and Ehud Reiter, editors, Proceedings of the 10th European Workshop on Natural Language Generation (ENLG-05), pages 58–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela Jordan</author>
<author>Marilyn Walker</author>
</authors>
<title>Learning attribute selections for non-pronominal expressions.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the ACL (ACL-00),</booktitle>
<pages>181--190</pages>
<contexts>
<context position="8905" citStr="Jordan and Walker, 2000" startWordPosition="1516" endWordPosition="1519">he effect of the discourse situation, but also some aspects of generating indefinite descriptions. relax the need for complete clarity. This could be arranged by having fC give similar scores to denotations where there are no distractors and to denotations where there are just a few distractors, with fB making a large contribution to the cost. References 3.4 Over-specification Recently, interest has been growing in ‘overspecified’ referring expressions, which contain more information than is required to identify their intended referent. Some of this work is mainly or exclusively experimental (Jordan and Walker, 2000; Arts, 2004), but algorithmic consequences are also being explored (Horacek, 2005; Paraboni and van Deemter, 2002; van der Sluis and Krahmer, 2005). Over-specification could also arise in a dialogue situation (comparable to that in Section 3.3) if a speaker is unclear about the hearer’s knowledge, and so over-specifies (relative to his own knowledge) to increase the chances of success. This goes beyond the classical algorithms, where the main goal is total clarity, with no reason for the algorithm to add further properties to an already unambiguous expression. That is, such algorithms assume </context>
</contexts>
<marker>Jordan, Walker, 2000</marker>
<rawString>Pamela Jordan and Marilyn Walker. 2000. Learning attribute selections for non-pronominal expressions. In Proceedings of the 38th Annual Meeting of the ACL (ACL-00), pages 181–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Mari¨et Theune</author>
</authors>
<title>Efficient generation of descriptions in context.</title>
<date>1999</date>
<booktitle>In Proceedings of the ESSLLI workshop on the generation of nominals,</booktitle>
<location>Utrecht, The Netherlands.</location>
<contexts>
<context position="8051" citStr="Krahmer and Theune, 1999" startWordPosition="1376" endWordPosition="1380">ormal setting, so A can be relatively interactive in conveying information. Also, the situation does not appear to be highly critical, in comparison to a military officer directing gunfire, or a surgeon guiding an incision. Initially, A produces an expression which is not very detailed. It may be that he thinks this is adequate (the object is sufficiently salient that B will uniquely determine the referent), or he doesn’t really know, but is willing to make an opening bid in a negotiation to reach the goal of reference. In the former case, a GRE algorithm which took account of salience (e.g. (Krahmer and Theune, 1999)), operating with A’s model of B’s knowledge, should produce this sort of effect. (A dialogue model might also be needed.) In the latter case, we need an algorithm which can 90 this approach will ultimately shed light not only on the effect of the discourse situation, but also some aspects of generating indefinite descriptions. relax the need for complete clarity. This could be arranged by having fC give similar scores to denotations where there are no distractors and to denotations where there are just a few distractors, with fB making a large contribution to the cost. References 3.4 Over-spe</context>
</contexts>
<marker>Krahmer, Theune, 1999</marker>
<rawString>Emiel Krahmer and Mari¨et Theune. 1999. Efficient generation of descriptions in context. In Proceedings of the ESSLLI workshop on the generation of nominals, Utrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Sebastiaan van Erk</author>
<author>Andr´e Verleg</author>
</authors>
<title>Graph-based generation of referring expressions.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Krahmer, van Erk, Verleg, 2003</marker>
<rawString>Emiel Krahmer, Sebastiaan van Erk, and Andr´e Verleg. 2003. Graph-based generation of referring expressions. Computational Linguistics, 29(1):53–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivandr´e Paraboni</author>
<author>Kees van Deemter</author>
</authors>
<title>Generating easy references: the case of document deixis.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Natural Language Generation,</booktitle>
<location>New York, USA.</location>
<marker>Paraboni, van Deemter, 2002</marker>
<rawString>Ivandr´e Paraboni and Kees van Deemter. 2002. Generating easy references: the case of document deixis. In Proceedings of the Second International Conference on Natural Language Generation, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ielka van der Sluis</author>
<author>Emiel Krahmer</author>
</authors>
<title>Towards the generation of overspecified multimodal referring expressions.</title>
<date>2005</date>
<booktitle>In Proceedings of the Symposium on Dialogue Modelling and Generation at the 15th Annual Meeting of the ST &amp; D (STD-05),</booktitle>
<location>Amsterdam, The Netherlands.</location>
<marker>van der Sluis, Krahmer, 2005</marker>
<rawString>Ielka van der Sluis and Emiel Krahmer. 2005. Towards the generation of overspecified multimodal referring expressions. In Proceedings of the Symposium on Dialogue Modelling and Generation at the 15th Annual Meeting of the ST &amp; D (STD-05), Amsterdam, The Netherlands.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>