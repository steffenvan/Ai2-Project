<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004071">
<title confidence="0.984528">
Plug Latent Structures and Play Coreference Resolution
</title>
<author confidence="0.996759">
Sebastian Martschat, Patrick Claus and Michael Strube
</author>
<affiliation confidence="0.967471">
Heidelberg Institute for Theoretical Studies gGmbH
</affiliation>
<address confidence="0.838653">
Schloss-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
</address>
<email confidence="0.857144">
(sebastian.martschat|patrick.claus|michael.strube)@h-its.org
</email>
<sectionHeader confidence="0.995719" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999762888888889">
We present cort, a modular toolkit for de-
vising, implementing, comparing and an-
alyzing approaches to coreference resolu-
tion. The toolkit allows for a unified rep-
resentation of popular coreference reso-
lution approaches by making explicit the
structures they operate on. Several of the
implemented approaches achieve state-of-
the-art performance.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999863423076923">
Coreference resolution is the task of determining
which mentions in a text refer to the same en-
tity. Machine learning approaches to coreference
resolution range from simple binary classification
models on mention pairs (Soon et al., 2001) to
complex structured prediction approaches (Durrett
and Klein, 2013; Fernandes et al., 2014).
In this paper, we present a toolkit that imple-
ments a framework that unifies these approaches:
in the framework, we obtain a unified representa-
tion of many coreference approaches by making
explicit the latent structures they operate on.
Our toolkit provides an interface for defining
structures for coreference resolution, which we
use to implement several popular approaches. An
evaluation of the approaches on CoNLL shared
task data (Pradhan et al., 2012) shows that they
obtain state-of-the-art results. The toolkit also can
perform end-to-end coreference resolution.
We implemented this functionality on top of the
coreference resolution error analysis toolkit cort
(Martschat et al., 2015). Hence, this toolkit now
provides functionality for devising, implementing,
comparing and analyzing approaches to corefer-
ence resolution. cort is released as open source1
and is available from the Python Package Index2.
</bodyText>
<footnote confidence="0.995390666666667">
1http://smartschat.de/software
2http://pypi.python.org/pypi. Install it via
pip install cort.
</footnote>
<sectionHeader confidence="0.998215" genericHeader="method">
2 A Framework for Coreference
Resolution
</sectionHeader>
<bodyText confidence="0.998094">
In this section we briefly describe a structured pre-
diction framework for coreference resolution.
</bodyText>
<subsectionHeader confidence="0.944434">
2.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999776533333333">
The popular mention pair approach (Soon et al.,
2001; Ng and Cardie, 2002) operates on a list of
mention pairs. Each mention pair is considered in-
dividually for learning and prediction. In contrast,
antecedent tree models (Yu and Joachims, 2009;
Fernandes et al., 2014; Bj¨orkelund and Kuhn,
2014) operate on a tree which encodes all anaphor-
antecedent decisions in a document.
Conceptually, both approaches have in common
that the structures they employ are not annotated
in the data (in coreference resolution, the annota-
tion consists of a mapping of mentions to entity
identifiers). Hence, we can view both approaches
as instantiations of a generic structured prediction
approach with latent variables.
</bodyText>
<subsectionHeader confidence="0.999708">
2.2 Setting
</subsectionHeader>
<bodyText confidence="0.9999308">
Our aim is to learn a prediction function f that,
given an input document x E X, predicts a pair
(h, z) E HxZ. h is the (unobserved) latent struc-
ture encoding the coreference relations between
mentions in x. z is the mapping of mentions to
entity identifiers (which is observed in the training
data). Usually, z is obtained from h by taking the
transitive closure over coreference decisions en-
coded in h. H and Z are the spaces containing
all such structures and mappings.
</bodyText>
<subsectionHeader confidence="0.997396">
2.3 Representation
</subsectionHeader>
<bodyText confidence="0.9999488">
For a document x E X, we write Mx =
{m1, . . . , mn}for the mentions in x. Follow-
ing previous work (Chang et al., 2012; Fernandes
et al., 2014), we make use of a dummy mention
which we denote as mo. If mo is predicted as the
</bodyText>
<page confidence="0.993153">
61
</page>
<note confidence="0.6936395">
Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 61–66,
Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP
</note>
<bodyText confidence="0.99827675">
antecedent of a mention mi, we consider mi non-
anaphoric. We define M0 x = {m0} U Mx.
Inspired by previous work (Bengtson and Roth,
2008; Fernandes et al., 2014; Martschat and
Strube, 2014), we adopt a graph-based represen-
tation of the latent structures h E &apos;H. In particular,
we express structures by labeled directed graphs
with vertex set M0x.
</bodyText>
<figureCaption confidence="0.641756">
Figure 1: Latent structure underlying the mention
</figureCaption>
<bodyText confidence="0.9900449">
ranking and the antecedent tree approach. The
black nodes and arcs represent one substructure
for the mention ranking approach.
Figure 1 shows a structure underlying the men-
tion ranking and the antecedent tree approach.
An arc between two mentions signals coreference.
For antecedent trees (Fernandes et al., 2014), the
whole structure is considered, while for mention
ranking (Denis and Baldridge, 2008; Chang et
al., 2012) only the antecedent decision for one
anaphor is examined. This can be expressed via
an appropriate segmentation into subgraphs which
we refer to as substructures. One such substruc-
ture encoding the antecedent decision for m3 is
colored black in the figure.
Via arc labels we can express additional infor-
mation. For example, mention pair models (Soon
et al., 2001) distinguish between positive and neg-
ative instances. This can be modeled by labeling
arcs with appropriate labels, such as + and −.
</bodyText>
<subsectionHeader confidence="0.990095">
2.4 Inference and Learning
</subsectionHeader>
<bodyText confidence="0.995699333333333">
As is common in natural language processing, we
model the prediction of (h, z) via a linear model.
That is,
</bodyText>
<equation confidence="0.9951805">
f(x) = fe(x) = arg max (θ, φ(x, h, z)),
(h,z)EWXS
</equation>
<bodyText confidence="0.999902363636364">
where θ E Rd is a parameter vector and φ: X x
x x i —* Rd is a joint feature representation
for inputs and outputs. When employing substruc-
tures, one maximization problem has to be solved
for each substructure (instead of one maximization
problem for the whole structure).
To learn the parameter vector θ E Rd from
training data, we employ a latent structured per-
ceptron (Sun et al., 2009) with cost-augmented
inference (Crammer et al., 2006) and averaging
(Collins, 2002).
</bodyText>
<sectionHeader confidence="0.996591" genericHeader="method">
3 Implementation
</sectionHeader>
<bodyText confidence="0.999945">
We now describe our implementation of the frame-
work presented in the previous section.
</bodyText>
<subsectionHeader confidence="0.998788">
3.1 Aims
</subsectionHeader>
<bodyText confidence="0.999964333333333">
By expressing approaches in the framework, re-
searchers can quickly devise, implement, com-
pare and analyze approaches for coreference res-
olution. To facilitate development, it should be as
easy as possible to define a coreference resolution
approach. We first describe the general architec-
ture of our toolkit before giving a detailed descrip-
tion of how to implement specific coreference res-
olution approaches.
</bodyText>
<subsectionHeader confidence="0.999813">
3.2 Architecture
</subsectionHeader>
<bodyText confidence="0.999996818181818">
The toolkit is implemented in Python. It can pro-
cess raw text and data conforming to the format of
the CoNLL-2012 shared task on coreference res-
olution (Pradhan et al., 2012). The toolkit is or-
ganized in four modules: the preprocessing
module contains functionality for processing raw
text, the core module provides mention extrac-
tion and computation of mention properties, the
analysis module contains error analysis meth-
ods, and the coreference module implements
the framework described in the previous section.
</bodyText>
<subsectionHeader confidence="0.960763">
3.2.1 preprocessing
</subsectionHeader>
<bodyText confidence="0.9999896">
By making use of NLTK3, this module provides
classes and functions for performing the prepro-
cessing tasks necessary for mention extraction
and coreference resolution: tokenization, sentence
splitting, parsing and named entity recognition.
</bodyText>
<subsectionHeader confidence="0.878642">
3.2.2 core
</subsectionHeader>
<bodyText confidence="0.999900714285714">
We employ a rule-based mention extractor, which
also computes a rich set of mention attributes, in-
cluding tokens, head, part-of-speech tags, named
entity tags, gender, number, semantic class, gram-
matical function and mention type. These at-
tributes, from which features are computed, can
be extended easily.
</bodyText>
<footnote confidence="0.4256">
3http://www.nltk.org/
</footnote>
<figure confidence="0.806111347826087">
m0
m1
m2
m3
m4
m5
62
Listing 3 Cost function for the mention ranking
model with latent antecedents.
def cost_based_on_consistency(arc):
ana, ante = arc
consistent = \
ana.decision_is_consistent(ante)
# false new
if not consistent and \
ante.is_dummy():
return 2
# wrong link
elif not consistent:
return 1
# correct
else:
return 0
</figure>
<bodyText confidence="0.998317">
tent structure for one document into substructures,
and the candidate arcs for each substructure.
Listing 1 shows source code of the instance ex-
tractor for the mention ranking model with latent
antecedents. In this model, each antecedent de-
cision for a mention corresponds to one substruc-
ture. Therefore, the extractor iterates over all men-
tions. For each mention, arcs to all preceding men-
tions are extracted and stored as candidate arcs for
one substructure.
</bodyText>
<subsectionHeader confidence="0.81613">
3.3.2 Decoders
</subsectionHeader>
<bodyText confidence="0.998803090909091">
The decoder solves the maximization problems
for obtaining the highest-scoring latent substruc-
tures consistent with the gold annotation, and the
highest-scoring cost-augmented latent substruc-
tures.
Listing 2 shows source code of a decoder for the
mention ranking model with latent antecedents.
The input to the decoder is a substructure, which
is a set of arcs, and a mapping from arcs to infor-
mation about arcs, such as features or costs. The
output is a tuple containing
</bodyText>
<listItem confidence="0.95463625">
• a list of arcs that constitute the highest-
scoring substructure, together with their la-
bels (if any) and scores,
• the same for the highest-scoring substructure
consistent with the gold annotation,
• the information whether the highest-scoring
substructure is consistent with the gold anno-
tation.
</listItem>
<bodyText confidence="0.999956235294118">
To obtain this prediction, we invoke the aux-
iliary function self.find best arcs. This
function searches through a set of arcs to find the
overall highest-scoring arc and the overall highest-
scoring arc consistent with the gold annotation.
Furthermore, it also outputs the scores of these
arcs according to the model, and whether the pre-
diction of the best arc is consistent with the gold
annotation.
For the mention ranking model, we let the func-
tion search through all candidate arcs for a sub-
structure, since these represent the antecedent de-
cision for one anaphor. Note that the mention
ranking model does not use any labels.
The update of the parameter vector is handled
by our implementation of the structured percep-
tron.
</bodyText>
<subsectionHeader confidence="0.958629">
3.3.3 Cost Functions
</subsectionHeader>
<bodyText confidence="0.999967555555556">
Cost functions allow to bias the learner towards
specific substructures, which leads to a large mar-
gin approach. For the mention ranking model, we
employ a cost function that assigns a higher cost to
erroneously determining anaphoricity than to se-
lecting a wrong link, similar to the cost functions
employed by Durrett and Klein (2013) and Fer-
nandes et al. (2014). The source code is displayed
in Listing 3.
</bodyText>
<subsectionHeader confidence="0.954857">
3.3.4 Clustering Algorithms
</subsectionHeader>
<bodyText confidence="0.999984">
The mention ranking model selects one antecedent
for each anaphor, therefore there is no need to
cluster antecedent decisions. Our toolkit provides
clustering algorithms commonly used for men-
tion pair models, such as closestfirst (Soon et al.,
2001) or bestfirst (Ng and Cardie, 2002).
</bodyText>
<subsectionHeader confidence="0.987095">
3.4 Running cort
</subsectionHeader>
<bodyText confidence="0.999131">
cort can be used as a Python library, but also pro-
vides two command line tools cort-train and
cort-predict.
</bodyText>
<sectionHeader confidence="0.999174" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999983285714286">
We implemented a mention pair model with best-
first clustering (Ng and Cardie, 2002), the mention
ranking model with closest (Denis and Baldridge,
2008) and latent (Chang et al., 2012) antecedents,
and antecedent trees (Fernandes et al., 2014).
Only slight modifications of the source code dis-
played in Listings 1 and 2 were necessary to im-
plement these approaches. For the ranking models
and antecedent trees we use the cost function de-
scribed in Listing 3.
We evaluate the models on the English test data
of the CoNLL-2012 shared task on multilingual
coreference resolution (Pradhan et al., 2012). We
use the reference implementation of the CoNLL
</bodyText>
<page confidence="0.998998">
64
</page>
<table confidence="0.988146777777778">
MUC B3 CEAF,
Model R P F1 R P F1 R P F1 Average F1
CoNLL-2012 English test data
Fernandes et al. (2014) 65.83 75.91 70.51 51.55 65.19 57.58 50.82 57.28 53.86 60.65
Bj¨orkelund and Kuhn (2014) 67.46 74.30 70.72 54.96 62.71 58.58 52.27 59.40 55.61 61.63
Mention Pair 67.16 71.48 69.25 51.97 60.55 55.93 51.02 51.89 51.45 58.88
Ranking: Closest 67.96 76.61 72.03 54.07 64.98 59.03 51.45 59.02 54.97 62.01
Ranking: Latent 68.13 76.72 72.17 54.22 66.12 59.58 52.33 59.47 55.67 62.47
Antecedent Trees 65.34 78.12 71.16 50.23 67.36 57.54 49.76 58.43 53.75 60.82
</table>
<tableCaption confidence="0.867594">
Table 1: Results of different systems and models on CoNLL-2012 English test data. Models below the
dashed lines are implemented in our toolkit.
</tableCaption>
<bodyText confidence="0.997121529411765">
scorer (Pradhan et al., 2014), which computes
the average of the evaluation metrics MUC (Vi-
lain et al., 1995), B3, (Bagga and Baldwin, 1998)
and CEAFe (Luo, 2005). The models are trained
on the concatenation of training and development
data.
The evaluation of the models is shown in Table
1. To put the numbers into context, we compare
with Fernandes et al. (2014), the winning system
of the CoNLL-2012 shared task, and the state-of-
the-art system of Bj¨orkelund and Kuhn (2014).
The mention pair model performs decently,
while the antecedent tree model exhibits perfor-
mance comparable to Fernandes et al. (2014), who
use a very similar model. The ranking models out-
perform Bj¨orkelund and Kuhn (2014), obtaining
state-of-the-art performance.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999889235294118">
Many researchers on coreference resolution re-
lease an implementation of the coreference model
described in their paper (Lee et al., 2013; Durrett
and Klein, 2013; Bj¨orkelund and Kuhn, 2014, in-
ter alia). However, these implementations imple-
ment only one approach following one paradigm
(such as mention ranking or antecedent trees).
Similarly to cort, research toolkits such as
BART (Versley et al., 2008) or Reconcile (Stoy-
anov et al., 2009) provide a framework to im-
plement and compare coreference resolution ap-
proaches. In contrast to these toolkits, we make
the latent structure underlying coreference ap-
proaches explicit, which facilitates development
of new approaches and renders the development
more transparent. Furthermore, we provide a
generic and customizable learning algorithm.
</bodyText>
<sectionHeader confidence="0.998582" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999979">
We presented an implementation of a frame-
work for coreference resolution that represents ap-
proaches to coreference resolution by the struc-
tures they operate on. In the implementation we
placed emphasis on facilitating the definition of
new models in the framework.
The presented toolkit cort can process raw text
and CoNLL shared task data. It achieves state-of-
the-art performance on the shared task data.
The framework and toolkit presented in this pa-
per help researchers to devise, analyze and com-
pare representations for coreference resolution.
</bodyText>
<sectionHeader confidence="0.982685" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9993182">
We thank Benjamin Heinzerling for helpful com-
ments on drafts of this paper. This work has been
funded by the Klaus Tschira Foundation, Ger-
many. The first author has been supported by a
HITS Ph.D. scholarship.
</bodyText>
<sectionHeader confidence="0.825752" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.957134642857143">
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings
of the 1st International Conference on Language
Resources and Evaluation, Granada, Spain, 28–30
May 1998, pages 563–566.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Waikiki,
Honolulu, Hawaii, 25–27 October 2008, pages 294–
303.
Anders Bj¨orkelund and Jonas Kuhn. 2014. Learn-
ing structured perceptrons for coreference resolu-
tion with latent antecedents and non-local features.
</bodyText>
<page confidence="0.999252">
65
</page>
<reference confidence="0.998159189189189">
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume
1: Long Papers), Baltimore, Md., 22–27 June 2014,
pages 47–57.
Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,
Mark Sammons, and Dan Roth. 2012. Illinois-
Coref: The UI system in the CoNLL-2012 shared
task. In Proceedings of the Shared Task of the
16th Conference on Computational Natural Lan-
guage Learning, Jeju Island, Korea, 12–14 July
2012, pages 113–117.
Michael Collins. 2002. Discriminative training meth-
ods for Hidden Markov Models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, Philadelphia, Penn.,
6–7 July 2002, pages 1–8.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551–585.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Waikiki,
Honolulu, Hawaii, 25–27 October 2008, pages 660–
669.
Greg Durrett and Dan Klein. 2013. Easy victories
and uphill battles in coreference resolution. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, Seattle, Wash.,
18–21 October 2013, pages 1971–1982.
Eraldo Fernandes, C´ıcero dos Santos, and Ruy Milidi´u.
2014. Latent trees for coreference resolution. Com-
putational Linguistics, 40(4):801–835.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4):885–916.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of the Hu-
man Language Technology Conference and the 2005
Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada, 6–8
October 2005, pages 25–32.
Sebastian Martschat and Michael Strube. 2014. Recall
error analysis for coreference resolution. In Pro-
ceedings of the 2014 Conference on Empirical Meth-
ods in Natural Language Processing, Doha, Qatar,
25–29 October 2014, pages 2070–2081.
Sebastian Martschat, Thierry G¨ockel, and Michael
Strube. 2015. Analyzing and visualizing corefer-
ence resolution errors. In Proceedings of the 2015
Conference of the North American Chapter of the
Association for Computational Linguistics: Demon-
strations, Denver, Col., 31 May – 5 June 2015, pages
6–10.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, Philadel-
phia, Penn., 7–12 July 2002, pages 104–111.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of the Shared Task of the 16th Conference on Com-
putational Natural Language Learning, Jeju Island,
Korea, 12–14 July 2012, pages 1–40.
Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring coreference partitions of predicted men-
tions: A reference implementation. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), Baltimore, Md., 22–27 June 2014, pages 30–
35.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521–544.
Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen
Riloff, David Buttler, and David Hysom. 2009.
Reconcile: A coreference resolution research plat-
form. Technical report, Cornell University.
Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, and
Jun’ichi Tsujii. 2009. Latent variable perceptron al-
gorithm for structured classification. In Proceedings
of the 21th International Joint Conference on Artifi-
cial Intelligence, Pasadena, Cal., 14–17 July 2009,
pages 1236–1242.
Yannick Versley, Simone Paolo Ponzetto, Massimo
Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolu-
tion. In Companion Volume to the Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics, Columbus, Ohio, 15–20 June
2008, pages 9–12.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th Message Understanding Conference
(MUC-6), pages 45–52, San Mateo, Cal. Morgan
Kaufmann.
Chun-Nam John Yu and Thorsten Joachims. 2009.
Learning structural SVMs with latent variables. In
Proceedings of the 26th International Conference on
Machine Learning, Montr´eal, Qu´ebec, Canada, 14–
18 June 2009, pages 1169–1176.
</reference>
<page confidence="0.988534">
66
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.542034">
<title confidence="0.999005">Plug Latent Structures and Play Coreference Resolution</title>
<author confidence="0.976997">Patrick Claus Martschat</author>
<affiliation confidence="0.7852485">Heidelberg Institute for Theoretical Studies Schloss-Wolfsbrunnenweg</affiliation>
<address confidence="0.999819">69118 Heidelberg, Germany</address>
<email confidence="0.999049">(sebastian.martschat|patrick.claus|michael.strube)@h-its.org</email>
<abstract confidence="0.994871">present a modular toolkit for devising, implementing, comparing and analyzing approaches to coreference resolution. The toolkit allows for a unified representation of popular coreference resolution approaches by making explicit the structures they operate on. Several of the implemented approaches achieve state-ofthe-art performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</booktitle>
<volume>1</volume>
<pages>47--57</pages>
<location>Long Papers), Baltimore, Md.,</location>
<contexts>
<context position="10013" citStr="(2014)" startWordPosition="1571" endWordPosition="1571">since these represent the antecedent decision for one anaphor. Note that the mention ranking model does not use any labels. The update of the parameter vector is handled by our implementation of the structured perceptron. 3.3.3 Cost Functions Cost functions allow to bias the learner towards specific substructures, which leads to a large margin approach. For the mention ranking model, we employ a cost function that assigns a higher cost to erroneously determining anaphoricity than to selecting a wrong link, similar to the cost functions employed by Durrett and Klein (2013) and Fernandes et al. (2014). The source code is displayed in Listing 3. 3.3.4 Clustering Algorithms The mention ranking model selects one antecedent for each anaphor, therefore there is no need to cluster antecedent decisions. Our toolkit provides clustering algorithms commonly used for mention pair models, such as closestfirst (Soon et al., 2001) or bestfirst (Ng and Cardie, 2002). 3.4 Running cort cort can be used as a Python library, but also provides two command line tools cort-train and cort-predict. 4 Evaluation We implemented a mention pair model with bestfirst clustering (Ng and Cardie, 2002), the mention rankin</context>
<context position="11264" citStr="(2014)" startWordPosition="1780" endWordPosition="1780">08) and latent (Chang et al., 2012) antecedents, and antecedent trees (Fernandes et al., 2014). Only slight modifications of the source code displayed in Listings 1 and 2 were necessary to implement these approaches. For the ranking models and antecedent trees we use the cost function described in Listing 3. We evaluate the models on the English test data of the CoNLL-2012 shared task on multilingual coreference resolution (Pradhan et al., 2012). We use the reference implementation of the CoNLL 64 MUC B3 CEAF, Model R P F1 R P F1 R P F1 Average F1 CoNLL-2012 English test data Fernandes et al. (2014) 65.83 75.91 70.51 51.55 65.19 57.58 50.82 57.28 53.86 60.65 Bj¨orkelund and Kuhn (2014) 67.46 74.30 70.72 54.96 62.71 58.58 52.27 59.40 55.61 61.63 Mention Pair 67.16 71.48 69.25 51.97 60.55 55.93 51.02 51.89 51.45 58.88 Ranking: Closest 67.96 76.61 72.03 54.07 64.98 59.03 51.45 59.02 54.97 62.01 Ranking: Latent 68.13 76.72 72.17 54.22 66.12 59.58 52.33 59.47 55.67 62.47 Antecedent Trees 65.34 78.12 71.16 50.23 67.36 57.54 49.76 58.43 53.75 60.82 Table 1: Results of different systems and models on CoNLL-2012 English test data. Models below the dashed lines are implemented in our toolkit. scor</context>
<context position="12471" citStr="(2014)" startWordPosition="1976" endWordPosition="1976">r (Pradhan et al., 2014), which computes the average of the evaluation metrics MUC (Vilain et al., 1995), B3, (Bagga and Baldwin, 1998) and CEAFe (Luo, 2005). The models are trained on the concatenation of training and development data. The evaluation of the models is shown in Table 1. To put the numbers into context, we compare with Fernandes et al. (2014), the winning system of the CoNLL-2012 shared task, and the state-ofthe-art system of Bj¨orkelund and Kuhn (2014). The mention pair model performs decently, while the antecedent tree model exhibits performance comparable to Fernandes et al. (2014), who use a very similar model. The ranking models outperform Bj¨orkelund and Kuhn (2014), obtaining state-of-the-art performance. 5 Related Work Many researchers on coreference resolution release an implementation of the coreference model described in their paper (Lee et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014, inter alia). However, these implementations implement only one approach following one paradigm (such as mention ranking or antecedent trees). Similarly to cort, research toolkits such as BART (Versley et al., 2008) or Reconcile (Stoyanov et al., 2009) provide a f</context>
</contexts>
<marker>2014</marker>
<rawString>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Baltimore, Md., 22–27 June 2014, pages 47–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai-Wei Chang</author>
<author>Rajhans Samdani</author>
<author>Alla Rozovskaya</author>
<author>Mark Sammons</author>
<author>Dan Roth</author>
</authors>
<title>IllinoisCoref: The UI system in the CoNLL-2012 shared task.</title>
<date>2012</date>
<booktitle>In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning,</booktitle>
<pages>113--117</pages>
<location>Jeju Island,</location>
<contexts>
<context position="3466" citStr="Chang et al., 2012" startWordPosition="516" endWordPosition="519">Setting Our aim is to learn a prediction function f that, given an input document x E X, predicts a pair (h, z) E HxZ. h is the (unobserved) latent structure encoding the coreference relations between mentions in x. z is the mapping of mentions to entity identifiers (which is observed in the training data). Usually, z is obtained from h by taking the transitive closure over coreference decisions encoded in h. H and Z are the spaces containing all such structures and mappings. 2.3 Representation For a document x E X, we write Mx = {m1, . . . , mn}for the mentions in x. Following previous work (Chang et al., 2012; Fernandes et al., 2014), we make use of a dummy mention which we denote as mo. If mo is predicted as the 61 Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 61–66, Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP antecedent of a mention mi, we consider mi nonanaphoric. We define M0 x = {m0} U Mx. Inspired by previous work (Bengtson and Roth, 2008; Fernandes et al., 2014; Martschat and Strube, 2014), we adopt a graph-based representation of the latent structures h E &apos;H. In particular, we express structures by labeled directed graphs with vertex set M0x. Figure 1: Latent struc</context>
<context position="10693" citStr="Chang et al., 2012" startWordPosition="1677" endWordPosition="1680"> Algorithms The mention ranking model selects one antecedent for each anaphor, therefore there is no need to cluster antecedent decisions. Our toolkit provides clustering algorithms commonly used for mention pair models, such as closestfirst (Soon et al., 2001) or bestfirst (Ng and Cardie, 2002). 3.4 Running cort cort can be used as a Python library, but also provides two command line tools cort-train and cort-predict. 4 Evaluation We implemented a mention pair model with bestfirst clustering (Ng and Cardie, 2002), the mention ranking model with closest (Denis and Baldridge, 2008) and latent (Chang et al., 2012) antecedents, and antecedent trees (Fernandes et al., 2014). Only slight modifications of the source code displayed in Listings 1 and 2 were necessary to implement these approaches. For the ranking models and antecedent trees we use the cost function described in Listing 3. We evaluate the models on the English test data of the CoNLL-2012 shared task on multilingual coreference resolution (Pradhan et al., 2012). We use the reference implementation of the CoNLL 64 MUC B3 CEAF, Model R P F1 R P F1 R P F1 Average F1 CoNLL-2012 English test data Fernandes et al. (2014) 65.83 75.91 70.51 51.55 65.1</context>
</contexts>
<marker>Chang, Samdani, Rozovskaya, Sammons, Roth, 2012</marker>
<rawString>Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya, Mark Sammons, and Dan Roth. 2012. IllinoisCoref: The UI system in the CoNLL-2012 shared task. In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning, Jeju Island, Korea, 12–14 July 2012, pages 113–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, Penn.,</location>
<contexts>
<context position="5672" citStr="Collins, 2002" startWordPosition="888" endWordPosition="889">natural language processing, we model the prediction of (h, z) via a linear model. That is, f(x) = fe(x) = arg max (θ, φ(x, h, z)), (h,z)EWXS where θ E Rd is a parameter vector and φ: X x x x i —* Rd is a joint feature representation for inputs and outputs. When employing substructures, one maximization problem has to be solved for each substructure (instead of one maximization problem for the whole structure). To learn the parameter vector θ E Rd from training data, we employ a latent structured perceptron (Sun et al., 2009) with cost-augmented inference (Crammer et al., 2006) and averaging (Collins, 2002). 3 Implementation We now describe our implementation of the framework presented in the previous section. 3.1 Aims By expressing approaches in the framework, researchers can quickly devise, implement, compare and analyze approaches for coreference resolution. To facilitate development, it should be as easy as possible to define a coreference resolution approach. We first describe the general architecture of our toolkit before giving a detailed description of how to implement specific coreference resolution approaches. 3.2 Architecture The toolkit is implemented in Python. It can process raw te</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for Hidden Markov Models: Theory and experiments with perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, Philadelphia, Penn., 6–7 July 2002, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="5642" citStr="Crammer et al., 2006" startWordPosition="882" endWordPosition="885">ference and Learning As is common in natural language processing, we model the prediction of (h, z) via a linear model. That is, f(x) = fe(x) = arg max (θ, φ(x, h, z)), (h,z)EWXS where θ E Rd is a parameter vector and φ: X x x x i —* Rd is a joint feature representation for inputs and outputs. When employing substructures, one maximization problem has to be solved for each substructure (instead of one maximization problem for the whole structure). To learn the parameter vector θ E Rd from training data, we employ a latent structured perceptron (Sun et al., 2009) with cost-augmented inference (Crammer et al., 2006) and averaging (Collins, 2002). 3 Implementation We now describe our implementation of the framework presented in the previous section. 3.1 Aims By expressing approaches in the framework, researchers can quickly devise, implement, compare and analyze approaches for coreference resolution. To facilitate development, it should be as easy as possible to define a coreference resolution approach. We first describe the general architecture of our toolkit before giving a detailed description of how to implement specific coreference resolution approaches. 3.2 Architecture The toolkit is implemented in</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Jason Baldridge</author>
</authors>
<title>Specialized models and ranking for coreference resolution.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>660--669</pages>
<location>Waikiki, Honolulu, Hawaii,</location>
<contexts>
<context position="4497" citStr="Denis and Baldridge, 2008" startWordPosition="684" endWordPosition="687"> Strube, 2014), we adopt a graph-based representation of the latent structures h E &apos;H. In particular, we express structures by labeled directed graphs with vertex set M0x. Figure 1: Latent structure underlying the mention ranking and the antecedent tree approach. The black nodes and arcs represent one substructure for the mention ranking approach. Figure 1 shows a structure underlying the mention ranking and the antecedent tree approach. An arc between two mentions signals coreference. For antecedent trees (Fernandes et al., 2014), the whole structure is considered, while for mention ranking (Denis and Baldridge, 2008; Chang et al., 2012) only the antecedent decision for one anaphor is examined. This can be expressed via an appropriate segmentation into subgraphs which we refer to as substructures. One such substructure encoding the antecedent decision for m3 is colored black in the figure. Via arc labels we can express additional information. For example, mention pair models (Soon et al., 2001) distinguish between positive and negative instances. This can be modeled by labeling arcs with appropriate labels, such as + and −. 2.4 Inference and Learning As is common in natural language processing, we model t</context>
<context position="10661" citStr="Denis and Baldridge, 2008" startWordPosition="1671" endWordPosition="1674">isplayed in Listing 3. 3.3.4 Clustering Algorithms The mention ranking model selects one antecedent for each anaphor, therefore there is no need to cluster antecedent decisions. Our toolkit provides clustering algorithms commonly used for mention pair models, such as closestfirst (Soon et al., 2001) or bestfirst (Ng and Cardie, 2002). 3.4 Running cort cort can be used as a Python library, but also provides two command line tools cort-train and cort-predict. 4 Evaluation We implemented a mention pair model with bestfirst clustering (Ng and Cardie, 2002), the mention ranking model with closest (Denis and Baldridge, 2008) and latent (Chang et al., 2012) antecedents, and antecedent trees (Fernandes et al., 2014). Only slight modifications of the source code displayed in Listings 1 and 2 were necessary to implement these approaches. For the ranking models and antecedent trees we use the cost function described in Listing 3. We evaluate the models on the English test data of the CoNLL-2012 shared task on multilingual coreference resolution (Pradhan et al., 2012). We use the reference implementation of the CoNLL 64 MUC B3 CEAF, Model R P F1 R P F1 R P F1 Average F1 CoNLL-2012 English test data Fernandes et al. (20</context>
</contexts>
<marker>Denis, Baldridge, 2008</marker>
<rawString>Pascal Denis and Jason Baldridge. 2008. Specialized models and ranking for coreference resolution. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Waikiki, Honolulu, Hawaii, 25–27 October 2008, pages 660– 669.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Dan Klein</author>
</authors>
<title>Easy victories and uphill battles in coreference resolution.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1971--1982</pages>
<location>Seattle, Wash.,</location>
<contexts>
<context position="949" citStr="Durrett and Klein, 2013" startWordPosition="121" endWordPosition="124">sing, implementing, comparing and analyzing approaches to coreference resolution. The toolkit allows for a unified representation of popular coreference resolution approaches by making explicit the structures they operate on. Several of the implemented approaches achieve state-ofthe-art performance. 1 Introduction Coreference resolution is the task of determining which mentions in a text refer to the same entity. Machine learning approaches to coreference resolution range from simple binary classification models on mention pairs (Soon et al., 2001) to complex structured prediction approaches (Durrett and Klein, 2013; Fernandes et al., 2014). In this paper, we present a toolkit that implements a framework that unifies these approaches: in the framework, we obtain a unified representation of many coreference approaches by making explicit the latent structures they operate on. Our toolkit provides an interface for defining structures for coreference resolution, which we use to implement several popular approaches. An evaluation of the approaches on CoNLL shared task data (Pradhan et al., 2012) shows that they obtain state-of-the-art results. The toolkit also can perform end-to-end coreference resolution. We</context>
<context position="9985" citStr="Durrett and Klein (2013)" startWordPosition="1562" endWordPosition="1565">hrough all candidate arcs for a substructure, since these represent the antecedent decision for one anaphor. Note that the mention ranking model does not use any labels. The update of the parameter vector is handled by our implementation of the structured perceptron. 3.3.3 Cost Functions Cost functions allow to bias the learner towards specific substructures, which leads to a large margin approach. For the mention ranking model, we employ a cost function that assigns a higher cost to erroneously determining anaphoricity than to selecting a wrong link, similar to the cost functions employed by Durrett and Klein (2013) and Fernandes et al. (2014). The source code is displayed in Listing 3. 3.3.4 Clustering Algorithms The mention ranking model selects one antecedent for each anaphor, therefore there is no need to cluster antecedent decisions. Our toolkit provides clustering algorithms commonly used for mention pair models, such as closestfirst (Soon et al., 2001) or bestfirst (Ng and Cardie, 2002). 3.4 Running cort cort can be used as a Python library, but also provides two command line tools cort-train and cort-predict. 4 Evaluation We implemented a mention pair model with bestfirst clustering (Ng and Cardi</context>
<context position="12778" citStr="Durrett and Klein, 2013" startWordPosition="2019" endWordPosition="2022">put the numbers into context, we compare with Fernandes et al. (2014), the winning system of the CoNLL-2012 shared task, and the state-ofthe-art system of Bj¨orkelund and Kuhn (2014). The mention pair model performs decently, while the antecedent tree model exhibits performance comparable to Fernandes et al. (2014), who use a very similar model. The ranking models outperform Bj¨orkelund and Kuhn (2014), obtaining state-of-the-art performance. 5 Related Work Many researchers on coreference resolution release an implementation of the coreference model described in their paper (Lee et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014, inter alia). However, these implementations implement only one approach following one paradigm (such as mention ranking or antecedent trees). Similarly to cort, research toolkits such as BART (Versley et al., 2008) or Reconcile (Stoyanov et al., 2009) provide a framework to implement and compare coreference resolution approaches. In contrast to these toolkits, we make the latent structure underlying coreference approaches explicit, which facilitates development of new approaches and renders the development more transparent. Furthermore, we provide a generic and cu</context>
</contexts>
<marker>Durrett, Klein, 2013</marker>
<rawString>Greg Durrett and Dan Klein. 2013. Easy victories and uphill battles in coreference resolution. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, Wash., 18–21 October 2013, pages 1971–1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eraldo Fernandes</author>
</authors>
<title>C´ıcero dos Santos, and Ruy Milidi´u.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>4</issue>
<marker>Fernandes, 2014</marker>
<rawString>Eraldo Fernandes, C´ıcero dos Santos, and Ruy Milidi´u. 2014. Latent trees for coreference resolution. Computational Linguistics, 40(4):801–835.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="12753" citStr="Lee et al., 2013" startWordPosition="2015" endWordPosition="2018">wn in Table 1. To put the numbers into context, we compare with Fernandes et al. (2014), the winning system of the CoNLL-2012 shared task, and the state-ofthe-art system of Bj¨orkelund and Kuhn (2014). The mention pair model performs decently, while the antecedent tree model exhibits performance comparable to Fernandes et al. (2014), who use a very similar model. The ranking models outperform Bj¨orkelund and Kuhn (2014), obtaining state-of-the-art performance. 5 Related Work Many researchers on coreference resolution release an implementation of the coreference model described in their paper (Lee et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014, inter alia). However, these implementations implement only one approach following one paradigm (such as mention ranking or antecedent trees). Similarly to cort, research toolkits such as BART (Versley et al., 2008) or Reconcile (Stoyanov et al., 2009) provide a framework to implement and compare coreference resolution approaches. In contrast to these toolkits, we make the latent structure underlying coreference approaches explicit, which facilitates development of new approaches and renders the development more transparent. Furthermore, we</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics, 39(4):885–916.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaoqiang Luo</author>
</authors>
<title>On coreference resolution performance metrics.</title>
<date>2005</date>
<booktitle>In Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>25--32</pages>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="12022" citStr="Luo, 2005" startWordPosition="1902" endWordPosition="1903"> Mention Pair 67.16 71.48 69.25 51.97 60.55 55.93 51.02 51.89 51.45 58.88 Ranking: Closest 67.96 76.61 72.03 54.07 64.98 59.03 51.45 59.02 54.97 62.01 Ranking: Latent 68.13 76.72 72.17 54.22 66.12 59.58 52.33 59.47 55.67 62.47 Antecedent Trees 65.34 78.12 71.16 50.23 67.36 57.54 49.76 58.43 53.75 60.82 Table 1: Results of different systems and models on CoNLL-2012 English test data. Models below the dashed lines are implemented in our toolkit. scorer (Pradhan et al., 2014), which computes the average of the evaluation metrics MUC (Vilain et al., 1995), B3, (Bagga and Baldwin, 1998) and CEAFe (Luo, 2005). The models are trained on the concatenation of training and development data. The evaluation of the models is shown in Table 1. To put the numbers into context, we compare with Fernandes et al. (2014), the winning system of the CoNLL-2012 shared task, and the state-ofthe-art system of Bj¨orkelund and Kuhn (2014). The mention pair model performs decently, while the antecedent tree model exhibits performance comparable to Fernandes et al. (2014), who use a very similar model. The ranking models outperform Bj¨orkelund and Kuhn (2014), obtaining state-of-the-art performance. 5 Related Work Many </context>
</contexts>
<marker>Luo, 2005</marker>
<rawString>Xiaoqiang Luo. 2005. On coreference resolution performance metrics. In Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing, Vancouver, B.C., Canada, 6–8 October 2005, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Martschat</author>
<author>Michael Strube</author>
</authors>
<title>Recall error analysis for coreference resolution.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>2070--2081</pages>
<location>Doha,</location>
<contexts>
<context position="3886" citStr="Martschat and Strube, 2014" startWordPosition="590" endWordPosition="593">. H and Z are the spaces containing all such structures and mappings. 2.3 Representation For a document x E X, we write Mx = {m1, . . . , mn}for the mentions in x. Following previous work (Chang et al., 2012; Fernandes et al., 2014), we make use of a dummy mention which we denote as mo. If mo is predicted as the 61 Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 61–66, Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP antecedent of a mention mi, we consider mi nonanaphoric. We define M0 x = {m0} U Mx. Inspired by previous work (Bengtson and Roth, 2008; Fernandes et al., 2014; Martschat and Strube, 2014), we adopt a graph-based representation of the latent structures h E &apos;H. In particular, we express structures by labeled directed graphs with vertex set M0x. Figure 1: Latent structure underlying the mention ranking and the antecedent tree approach. The black nodes and arcs represent one substructure for the mention ranking approach. Figure 1 shows a structure underlying the mention ranking and the antecedent tree approach. An arc between two mentions signals coreference. For antecedent trees (Fernandes et al., 2014), the whole structure is considered, while for mention ranking (Denis and Bald</context>
</contexts>
<marker>Martschat, Strube, 2014</marker>
<rawString>Sebastian Martschat and Michael Strube. 2014. Recall error analysis for coreference resolution. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, Doha, Qatar, 25–29 October 2014, pages 2070–2081.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Martschat</author>
<author>Thierry G¨ockel</author>
<author>Michael Strube</author>
</authors>
<title>Analyzing and visualizing coreference resolution errors.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, Denver, Col., 31 May – 5</booktitle>
<pages>6--10</pages>
<marker>Martschat, G¨ockel, Strube, 2015</marker>
<rawString>Sebastian Martschat, Thierry G¨ockel, and Michael Strube. 2015. Analyzing and visualizing coreference resolution errors. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations, Denver, Col., 31 May – 5 June 2015, pages 6–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Ng</author>
<author>Claire Cardie</author>
</authors>
<title>Improving machine learning approaches to coreference resolution.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>104--111</pages>
<location>Philadelphia, Penn., 7–12</location>
<contexts>
<context position="2213" citStr="Ng and Cardie, 2002" startWordPosition="302" endWordPosition="305">e coreference resolution error analysis toolkit cort (Martschat et al., 2015). Hence, this toolkit now provides functionality for devising, implementing, comparing and analyzing approaches to coreference resolution. cort is released as open source1 and is available from the Python Package Index2. 1http://smartschat.de/software 2http://pypi.python.org/pypi. Install it via pip install cort. 2 A Framework for Coreference Resolution In this section we briefly describe a structured prediction framework for coreference resolution. 2.1 Motivation The popular mention pair approach (Soon et al., 2001; Ng and Cardie, 2002) operates on a list of mention pairs. Each mention pair is considered individually for learning and prediction. In contrast, antecedent tree models (Yu and Joachims, 2009; Fernandes et al., 2014; Bj¨orkelund and Kuhn, 2014) operate on a tree which encodes all anaphorantecedent decisions in a document. Conceptually, both approaches have in common that the structures they employ are not annotated in the data (in coreference resolution, the annotation consists of a mapping of mentions to entity identifiers). Hence, we can view both approaches as instantiations of a generic structured prediction a</context>
<context position="10370" citStr="Ng and Cardie, 2002" startWordPosition="1623" endWordPosition="1626">in approach. For the mention ranking model, we employ a cost function that assigns a higher cost to erroneously determining anaphoricity than to selecting a wrong link, similar to the cost functions employed by Durrett and Klein (2013) and Fernandes et al. (2014). The source code is displayed in Listing 3. 3.3.4 Clustering Algorithms The mention ranking model selects one antecedent for each anaphor, therefore there is no need to cluster antecedent decisions. Our toolkit provides clustering algorithms commonly used for mention pair models, such as closestfirst (Soon et al., 2001) or bestfirst (Ng and Cardie, 2002). 3.4 Running cort cort can be used as a Python library, but also provides two command line tools cort-train and cort-predict. 4 Evaluation We implemented a mention pair model with bestfirst clustering (Ng and Cardie, 2002), the mention ranking model with closest (Denis and Baldridge, 2008) and latent (Chang et al., 2012) antecedents, and antecedent trees (Fernandes et al., 2014). Only slight modifications of the source code displayed in Listings 1 and 2 were necessary to implement these approaches. For the ranking models and antecedent trees we use the cost function described in Listing 3. We</context>
</contexts>
<marker>Ng, Cardie, 2002</marker>
<rawString>Vincent Ng and Claire Cardie. 2002. Improving machine learning approaches to coreference resolution. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, Penn., 7–12 July 2002, pages 104–111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Alessandro Moschitti</author>
<author>Nianwen Xue</author>
<author>Olga Uryupina</author>
<author>Yuchen Zhang</author>
</authors>
<title>CoNLL2012 Shared Task: Modeling multilingual unrestricted coreference in OntoNotes.</title>
<date>2012</date>
<booktitle>In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning,</booktitle>
<pages>1--40</pages>
<location>Jeju Island,</location>
<contexts>
<context position="1433" citStr="Pradhan et al., 2012" startWordPosition="196" endWordPosition="199">le binary classification models on mention pairs (Soon et al., 2001) to complex structured prediction approaches (Durrett and Klein, 2013; Fernandes et al., 2014). In this paper, we present a toolkit that implements a framework that unifies these approaches: in the framework, we obtain a unified representation of many coreference approaches by making explicit the latent structures they operate on. Our toolkit provides an interface for defining structures for coreference resolution, which we use to implement several popular approaches. An evaluation of the approaches on CoNLL shared task data (Pradhan et al., 2012) shows that they obtain state-of-the-art results. The toolkit also can perform end-to-end coreference resolution. We implemented this functionality on top of the coreference resolution error analysis toolkit cort (Martschat et al., 2015). Hence, this toolkit now provides functionality for devising, implementing, comparing and analyzing approaches to coreference resolution. cort is released as open source1 and is available from the Python Package Index2. 1http://smartschat.de/software 2http://pypi.python.org/pypi. Install it via pip install cort. 2 A Framework for Coreference Resolution In this</context>
<context position="6387" citStr="Pradhan et al., 2012" startWordPosition="999" endWordPosition="1002">us section. 3.1 Aims By expressing approaches in the framework, researchers can quickly devise, implement, compare and analyze approaches for coreference resolution. To facilitate development, it should be as easy as possible to define a coreference resolution approach. We first describe the general architecture of our toolkit before giving a detailed description of how to implement specific coreference resolution approaches. 3.2 Architecture The toolkit is implemented in Python. It can process raw text and data conforming to the format of the CoNLL-2012 shared task on coreference resolution (Pradhan et al., 2012). The toolkit is organized in four modules: the preprocessing module contains functionality for processing raw text, the core module provides mention extraction and computation of mention properties, the analysis module contains error analysis methods, and the coreference module implements the framework described in the previous section. 3.2.1 preprocessing By making use of NLTK3, this module provides classes and functions for performing the preprocessing tasks necessary for mention extraction and coreference resolution: tokenization, sentence splitting, parsing and named entity recognition. 3</context>
<context position="11107" citStr="Pradhan et al., 2012" startWordPosition="1745" endWordPosition="1748">t-predict. 4 Evaluation We implemented a mention pair model with bestfirst clustering (Ng and Cardie, 2002), the mention ranking model with closest (Denis and Baldridge, 2008) and latent (Chang et al., 2012) antecedents, and antecedent trees (Fernandes et al., 2014). Only slight modifications of the source code displayed in Listings 1 and 2 were necessary to implement these approaches. For the ranking models and antecedent trees we use the cost function described in Listing 3. We evaluate the models on the English test data of the CoNLL-2012 shared task on multilingual coreference resolution (Pradhan et al., 2012). We use the reference implementation of the CoNLL 64 MUC B3 CEAF, Model R P F1 R P F1 R P F1 Average F1 CoNLL-2012 English test data Fernandes et al. (2014) 65.83 75.91 70.51 51.55 65.19 57.58 50.82 57.28 53.86 60.65 Bj¨orkelund and Kuhn (2014) 67.46 74.30 70.72 54.96 62.71 58.58 52.27 59.40 55.61 61.63 Mention Pair 67.16 71.48 69.25 51.97 60.55 55.93 51.02 51.89 51.45 58.88 Ranking: Closest 67.96 76.61 72.03 54.07 64.98 59.03 51.45 59.02 54.97 62.01 Ranking: Latent 68.13 76.72 72.17 54.22 66.12 59.58 52.33 59.47 55.67 62.47 Antecedent Trees 65.34 78.12 71.16 50.23 67.36 57.54 49.76 58.43 53.</context>
</contexts>
<marker>Pradhan, Moschitti, Xue, Uryupina, Zhang, 2012</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Olga Uryupina, and Yuchen Zhang. 2012. CoNLL2012 Shared Task: Modeling multilingual unrestricted coreference in OntoNotes. In Proceedings of the Shared Task of the 16th Conference on Computational Natural Language Learning, Jeju Island, Korea, 12–14 July 2012, pages 1–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Xiaoqiang Luo</author>
<author>Marta Recasens</author>
<author>Eduard Hovy</author>
<author>Vincent Ng</author>
<author>Michael Strube</author>
</authors>
<title>Scoring coreference partitions of predicted mentions: A reference implementation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>30--35</pages>
<location>Baltimore, Md.,</location>
<contexts>
<context position="11889" citStr="Pradhan et al., 2014" startWordPosition="1877" endWordPosition="1880">83 75.91 70.51 51.55 65.19 57.58 50.82 57.28 53.86 60.65 Bj¨orkelund and Kuhn (2014) 67.46 74.30 70.72 54.96 62.71 58.58 52.27 59.40 55.61 61.63 Mention Pair 67.16 71.48 69.25 51.97 60.55 55.93 51.02 51.89 51.45 58.88 Ranking: Closest 67.96 76.61 72.03 54.07 64.98 59.03 51.45 59.02 54.97 62.01 Ranking: Latent 68.13 76.72 72.17 54.22 66.12 59.58 52.33 59.47 55.67 62.47 Antecedent Trees 65.34 78.12 71.16 50.23 67.36 57.54 49.76 58.43 53.75 60.82 Table 1: Results of different systems and models on CoNLL-2012 English test data. Models below the dashed lines are implemented in our toolkit. scorer (Pradhan et al., 2014), which computes the average of the evaluation metrics MUC (Vilain et al., 1995), B3, (Bagga and Baldwin, 1998) and CEAFe (Luo, 2005). The models are trained on the concatenation of training and development data. The evaluation of the models is shown in Table 1. To put the numbers into context, we compare with Fernandes et al. (2014), the winning system of the CoNLL-2012 shared task, and the state-ofthe-art system of Bj¨orkelund and Kuhn (2014). The mention pair model performs decently, while the antecedent tree model exhibits performance comparable to Fernandes et al. (2014), who use a very s</context>
</contexts>
<marker>Pradhan, Luo, Recasens, Hovy, Ng, Strube, 2014</marker>
<rawString>Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Eduard Hovy, Vincent Ng, and Michael Strube. 2014. Scoring coreference partitions of predicted mentions: A reference implementation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Baltimore, Md., 22–27 June 2014, pages 30– 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wee Meng Soon</author>
<author>Hwee Tou Ng</author>
<author>Daniel Chung Yong Lim</author>
</authors>
<title>A machine learning approach to coreference resolution of noun phrases.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>4</issue>
<contexts>
<context position="880" citStr="Soon et al., 2001" startWordPosition="112" endWordPosition="115">)@h-its.org Abstract We present cort, a modular toolkit for devising, implementing, comparing and analyzing approaches to coreference resolution. The toolkit allows for a unified representation of popular coreference resolution approaches by making explicit the structures they operate on. Several of the implemented approaches achieve state-ofthe-art performance. 1 Introduction Coreference resolution is the task of determining which mentions in a text refer to the same entity. Machine learning approaches to coreference resolution range from simple binary classification models on mention pairs (Soon et al., 2001) to complex structured prediction approaches (Durrett and Klein, 2013; Fernandes et al., 2014). In this paper, we present a toolkit that implements a framework that unifies these approaches: in the framework, we obtain a unified representation of many coreference approaches by making explicit the latent structures they operate on. Our toolkit provides an interface for defining structures for coreference resolution, which we use to implement several popular approaches. An evaluation of the approaches on CoNLL shared task data (Pradhan et al., 2012) shows that they obtain state-of-the-art result</context>
<context position="2191" citStr="Soon et al., 2001" startWordPosition="298" endWordPosition="301">nality on top of the coreference resolution error analysis toolkit cort (Martschat et al., 2015). Hence, this toolkit now provides functionality for devising, implementing, comparing and analyzing approaches to coreference resolution. cort is released as open source1 and is available from the Python Package Index2. 1http://smartschat.de/software 2http://pypi.python.org/pypi. Install it via pip install cort. 2 A Framework for Coreference Resolution In this section we briefly describe a structured prediction framework for coreference resolution. 2.1 Motivation The popular mention pair approach (Soon et al., 2001; Ng and Cardie, 2002) operates on a list of mention pairs. Each mention pair is considered individually for learning and prediction. In contrast, antecedent tree models (Yu and Joachims, 2009; Fernandes et al., 2014; Bj¨orkelund and Kuhn, 2014) operate on a tree which encodes all anaphorantecedent decisions in a document. Conceptually, both approaches have in common that the structures they employ are not annotated in the data (in coreference resolution, the annotation consists of a mapping of mentions to entity identifiers). Hence, we can view both approaches as instantiations of a generic s</context>
<context position="4882" citStr="Soon et al., 2001" startWordPosition="747" endWordPosition="750">mention ranking and the antecedent tree approach. An arc between two mentions signals coreference. For antecedent trees (Fernandes et al., 2014), the whole structure is considered, while for mention ranking (Denis and Baldridge, 2008; Chang et al., 2012) only the antecedent decision for one anaphor is examined. This can be expressed via an appropriate segmentation into subgraphs which we refer to as substructures. One such substructure encoding the antecedent decision for m3 is colored black in the figure. Via arc labels we can express additional information. For example, mention pair models (Soon et al., 2001) distinguish between positive and negative instances. This can be modeled by labeling arcs with appropriate labels, such as + and −. 2.4 Inference and Learning As is common in natural language processing, we model the prediction of (h, z) via a linear model. That is, f(x) = fe(x) = arg max (θ, φ(x, h, z)), (h,z)EWXS where θ E Rd is a parameter vector and φ: X x x x i —* Rd is a joint feature representation for inputs and outputs. When employing substructures, one maximization problem has to be solved for each substructure (instead of one maximization problem for the whole structure). To learn </context>
<context position="10335" citStr="Soon et al., 2001" startWordPosition="1617" endWordPosition="1620">ures, which leads to a large margin approach. For the mention ranking model, we employ a cost function that assigns a higher cost to erroneously determining anaphoricity than to selecting a wrong link, similar to the cost functions employed by Durrett and Klein (2013) and Fernandes et al. (2014). The source code is displayed in Listing 3. 3.3.4 Clustering Algorithms The mention ranking model selects one antecedent for each anaphor, therefore there is no need to cluster antecedent decisions. Our toolkit provides clustering algorithms commonly used for mention pair models, such as closestfirst (Soon et al., 2001) or bestfirst (Ng and Cardie, 2002). 3.4 Running cort cort can be used as a Python library, but also provides two command line tools cort-train and cort-predict. 4 Evaluation We implemented a mention pair model with bestfirst clustering (Ng and Cardie, 2002), the mention ranking model with closest (Denis and Baldridge, 2008) and latent (Chang et al., 2012) antecedents, and antecedent trees (Fernandes et al., 2014). Only slight modifications of the source code displayed in Listings 1 and 2 were necessary to implement these approaches. For the ranking models and antecedent trees we use the cost </context>
</contexts>
<marker>Soon, Ng, Lim, 2001</marker>
<rawString>Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veselin Stoyanov</author>
<author>Claire Cardie</author>
<author>Nathan Gilbert</author>
<author>Ellen Riloff</author>
<author>David Buttler</author>
<author>David Hysom</author>
</authors>
<title>Reconcile: A coreference resolution research platform.</title>
<date>2009</date>
<tech>Technical report,</tech>
<institution>Cornell University.</institution>
<contexts>
<context position="13059" citStr="Stoyanov et al., 2009" startWordPosition="2062" endWordPosition="2066">parable to Fernandes et al. (2014), who use a very similar model. The ranking models outperform Bj¨orkelund and Kuhn (2014), obtaining state-of-the-art performance. 5 Related Work Many researchers on coreference resolution release an implementation of the coreference model described in their paper (Lee et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014, inter alia). However, these implementations implement only one approach following one paradigm (such as mention ranking or antecedent trees). Similarly to cort, research toolkits such as BART (Versley et al., 2008) or Reconcile (Stoyanov et al., 2009) provide a framework to implement and compare coreference resolution approaches. In contrast to these toolkits, we make the latent structure underlying coreference approaches explicit, which facilitates development of new approaches and renders the development more transparent. Furthermore, we provide a generic and customizable learning algorithm. 6 Conclusions We presented an implementation of a framework for coreference resolution that represents approaches to coreference resolution by the structures they operate on. In the implementation we placed emphasis on facilitating the definition of </context>
</contexts>
<marker>Stoyanov, Cardie, Gilbert, Riloff, Buttler, Hysom, 2009</marker>
<rawString>Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom. 2009. Reconcile: A coreference resolution research platform. Technical report, Cornell University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xu Sun</author>
<author>Takuya Matsuzaki</author>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Latent variable perceptron algorithm for structured classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1236--1242</pages>
<location>Pasadena, Cal.,</location>
<contexts>
<context position="5589" citStr="Sun et al., 2009" startWordPosition="875" endWordPosition="878"> with appropriate labels, such as + and −. 2.4 Inference and Learning As is common in natural language processing, we model the prediction of (h, z) via a linear model. That is, f(x) = fe(x) = arg max (θ, φ(x, h, z)), (h,z)EWXS where θ E Rd is a parameter vector and φ: X x x x i —* Rd is a joint feature representation for inputs and outputs. When employing substructures, one maximization problem has to be solved for each substructure (instead of one maximization problem for the whole structure). To learn the parameter vector θ E Rd from training data, we employ a latent structured perceptron (Sun et al., 2009) with cost-augmented inference (Crammer et al., 2006) and averaging (Collins, 2002). 3 Implementation We now describe our implementation of the framework presented in the previous section. 3.1 Aims By expressing approaches in the framework, researchers can quickly devise, implement, compare and analyze approaches for coreference resolution. To facilitate development, it should be as easy as possible to define a coreference resolution approach. We first describe the general architecture of our toolkit before giving a detailed description of how to implement specific coreference resolution appro</context>
</contexts>
<marker>Sun, Matsuzaki, Okanohara, Tsujii, 2009</marker>
<rawString>Xu Sun, Takuya Matsuzaki, Daisuke Okanohara, and Jun’ichi Tsujii. 2009. Latent variable perceptron algorithm for structured classification. In Proceedings of the 21th International Joint Conference on Artificial Intelligence, Pasadena, Cal., 14–17 July 2009, pages 1236–1242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yannick Versley</author>
<author>Simone Paolo Ponzetto</author>
<author>Massimo Poesio</author>
<author>Vladimir Eidelman</author>
<author>Alan Jern</author>
<author>Jason Smith</author>
<author>Xiaofeng Yang</author>
<author>Alessandro Moschitti</author>
</authors>
<title>BART: A modular toolkit for coreference resolution.</title>
<date>2008</date>
<booktitle>In Companion Volume to the Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>9--12</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="13022" citStr="Versley et al., 2008" startWordPosition="2056" endWordPosition="2059"> tree model exhibits performance comparable to Fernandes et al. (2014), who use a very similar model. The ranking models outperform Bj¨orkelund and Kuhn (2014), obtaining state-of-the-art performance. 5 Related Work Many researchers on coreference resolution release an implementation of the coreference model described in their paper (Lee et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014, inter alia). However, these implementations implement only one approach following one paradigm (such as mention ranking or antecedent trees). Similarly to cort, research toolkits such as BART (Versley et al., 2008) or Reconcile (Stoyanov et al., 2009) provide a framework to implement and compare coreference resolution approaches. In contrast to these toolkits, we make the latent structure underlying coreference approaches explicit, which facilitates development of new approaches and renders the development more transparent. Furthermore, we provide a generic and customizable learning algorithm. 6 Conclusions We presented an implementation of a framework for coreference resolution that represents approaches to coreference resolution by the structures they operate on. In the implementation we placed emphas</context>
</contexts>
<marker>Versley, Ponzetto, Poesio, Eidelman, Jern, Smith, Yang, Moschitti, 2008</marker>
<rawString>Yannick Versley, Simone Paolo Ponzetto, Massimo Poesio, Vladimir Eidelman, Alan Jern, Jason Smith, Xiaofeng Yang, and Alessandro Moschitti. 2008. BART: A modular toolkit for coreference resolution. In Companion Volume to the Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics, Columbus, Ohio, 15–20 June 2008, pages 9–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Vilain</author>
<author>John Burger</author>
<author>John Aberdeen</author>
<author>Dennis Connolly</author>
<author>Lynette Hirschman</author>
</authors>
<title>A modeltheoretic coreference scoring scheme.</title>
<date>1995</date>
<booktitle>In Proceedings of the 6th Message Understanding Conference (MUC-6),</booktitle>
<pages>45--52</pages>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, Cal.</location>
<contexts>
<context position="11969" citStr="Vilain et al., 1995" startWordPosition="1890" endWordPosition="1894">14) 67.46 74.30 70.72 54.96 62.71 58.58 52.27 59.40 55.61 61.63 Mention Pair 67.16 71.48 69.25 51.97 60.55 55.93 51.02 51.89 51.45 58.88 Ranking: Closest 67.96 76.61 72.03 54.07 64.98 59.03 51.45 59.02 54.97 62.01 Ranking: Latent 68.13 76.72 72.17 54.22 66.12 59.58 52.33 59.47 55.67 62.47 Antecedent Trees 65.34 78.12 71.16 50.23 67.36 57.54 49.76 58.43 53.75 60.82 Table 1: Results of different systems and models on CoNLL-2012 English test data. Models below the dashed lines are implemented in our toolkit. scorer (Pradhan et al., 2014), which computes the average of the evaluation metrics MUC (Vilain et al., 1995), B3, (Bagga and Baldwin, 1998) and CEAFe (Luo, 2005). The models are trained on the concatenation of training and development data. The evaluation of the models is shown in Table 1. To put the numbers into context, we compare with Fernandes et al. (2014), the winning system of the CoNLL-2012 shared task, and the state-ofthe-art system of Bj¨orkelund and Kuhn (2014). The mention pair model performs decently, while the antecedent tree model exhibits performance comparable to Fernandes et al. (2014), who use a very similar model. The ranking models outperform Bj¨orkelund and Kuhn (2014), obtaini</context>
</contexts>
<marker>Vilain, Burger, Aberdeen, Connolly, Hirschman, 1995</marker>
<rawString>Marc Vilain, John Burger, John Aberdeen, Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme. In Proceedings of the 6th Message Understanding Conference (MUC-6), pages 45–52, San Mateo, Cal. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chun-Nam John Yu</author>
<author>Thorsten Joachims</author>
</authors>
<title>Learning structural SVMs with latent variables.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th International Conference on Machine Learning,</booktitle>
<volume>14</volume>
<pages>1169--1176</pages>
<location>Montr´eal, Qu´ebec,</location>
<contexts>
<context position="2383" citStr="Yu and Joachims, 2009" startWordPosition="329" endWordPosition="332">nalyzing approaches to coreference resolution. cort is released as open source1 and is available from the Python Package Index2. 1http://smartschat.de/software 2http://pypi.python.org/pypi. Install it via pip install cort. 2 A Framework for Coreference Resolution In this section we briefly describe a structured prediction framework for coreference resolution. 2.1 Motivation The popular mention pair approach (Soon et al., 2001; Ng and Cardie, 2002) operates on a list of mention pairs. Each mention pair is considered individually for learning and prediction. In contrast, antecedent tree models (Yu and Joachims, 2009; Fernandes et al., 2014; Bj¨orkelund and Kuhn, 2014) operate on a tree which encodes all anaphorantecedent decisions in a document. Conceptually, both approaches have in common that the structures they employ are not annotated in the data (in coreference resolution, the annotation consists of a mapping of mentions to entity identifiers). Hence, we can view both approaches as instantiations of a generic structured prediction approach with latent variables. 2.2 Setting Our aim is to learn a prediction function f that, given an input document x E X, predicts a pair (h, z) E HxZ. h is the (unobse</context>
</contexts>
<marker>Yu, Joachims, 2009</marker>
<rawString>Chun-Nam John Yu and Thorsten Joachims. 2009. Learning structural SVMs with latent variables. In Proceedings of the 26th International Conference on Machine Learning, Montr´eal, Qu´ebec, Canada, 14– 18 June 2009, pages 1169–1176.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>