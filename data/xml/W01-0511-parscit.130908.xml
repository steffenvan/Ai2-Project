<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000132">
<title confidence="0.997791">
Classifying the Semantic Relations in Noun Compounds via a
Domain-Specific Lexical Hierarchy
</title>
<author confidence="0.997878">
Barbara Rosario and Marti Hearst
</author>
<affiliation confidence="0.9991905">
School of Information Management &amp; Systems
University of California, Berkeley
</affiliation>
<address confidence="0.884129">
Berkeley, CA 94720-4600
</address>
<email confidence="0.999392">
{rosario,hearst}@sims.berkeley.edu
</email>
<sectionHeader confidence="0.980153" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999958">
We are developing corpus-based techniques for iden-
tifying semantic relations at an intermediate level of
description (more specific than those used in case
frames, but more general than those used in tra-
ditional knowledge representation systems). In this
paper we describe a classification algorithm for iden-
tifying relationships between two-word noun com-
pounds. We find that a very simple approach using
a machine learning algorithm and a domain-specific
lexical hierarchy successfully generalizes from train-
ing instances, performing better on previously un-
seen words than a baseline consisting of training on
the words themselves.
</bodyText>
<sectionHeader confidence="0.995513" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998456718750001">
We are exploring empirical methods of determin-
ing semantic relationships between constituents in
natural language. Our current project focuses on
biomedical text, both because it poses interesting
challenges, and because it should be possible to make
inferences about propositions that hold between sci-
entific concepts within biomedical texts (Swanson
and Smalheiser, 1994).
One of the important challenges of biomedical
text, along with most other technical text, is the
proliferation of noun compounds. A typical article
title is shown below; it consists a cascade of four
noun phrases linked by prepositions:
Open-labeled long-term study of the effi-
cacy, safety, and tolerability of subcuta-
neous sumatriptan in acute migraine treat-
ment.
The real concern in analyzing such a title is in de-
termining the relationships that hold between differ-
ent concepts, rather than on finding the appropriate
attachments (which is especially difficult given the
lack of a verb). And before we tackle the prepo-
sitional phrase attachment problem, we must find
a way to analyze the meanings of the noun com-
pounds.
Our goal is to extract propositional information
from text, and as a step towards this goal, we clas-
sify constituents according to which semantic rela-
tionships hold between them. For example, we want
to characterize the treatment-for-disease relation-
ship between the words of migraine treatment ver-
sus the method-of-treatment relationship between
the words of aerosol treatment. These relations are
intended to be combined to produce larger proposi-
tions that can then be used in a variety of interpreta-
tion paradigms, such as abductive reasoning (Hobbs
et al., 1993) or inductive logic programming (Ng and
Zelle, 1997).
Note that because we are concerned with the se-
mantic relations that hold between the concepts, as
opposed to the more standard, syntax-driven com-
putational goal of determining left versus right as-
sociation, this has the fortuitous effect of changing
the problem into one of classification, amenable to
standard machine learning classification techniques.
We have found that we can use such algorithms to
classify relationships between two-word noun com-
pounds with a surprising degree of accuracy. A
one-out-of-eighteen classification using a neural net
achieves accuracies as high as 62%. By taking ad-
vantage of lexical ontologies, we achieve strong re-
sults on noun compounds for which neither word is
present in the training set. Thus, we think this is a
promising approach for a variety of semantic label-
ing tasks.
The reminder of this paper is organized as follows:
Section 2 describes related work, Section 3 describes
the semantic relations and how they were chosen,
and Section 4 describes the data collection and on-
tologies. In Section 5 we describe the method for
automatically assigning semantic relations to noun
compounds, and report the results of experiments
using this method. Section 6 concludes the paper
and discusses future work.
</bodyText>
<sectionHeader confidence="0.999228" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.996286362068966">
Several approaches have been proposed for empiri-
cal noun compound interpretation. Lauer and Dras
(1994) point out that there are three components to
the problem: identification of the compound from
within the text, syntactic analysis of the compound
(left versus right association), and the interpreta-
tion of the underlying semantics. Several researchers
have tackled the syntactic analysis (Lauer, 1995;
Pustejovsky et al., 1993; Liberman and Sproat,
1992), usually using a variation of the idea of find-
ing the subconstituents elsewhere in the corpus and
using those to predict how the larger compounds are
structured.
We are interested in the third task, interpretation
of the underlying semantics. Most related work re-
lies on hand-written rules of one kind or another.
Finin (1980) examines the problem of noun com-
pound interpretation in detail, and constructs a
complex set of rules. Vanderwende (1994) uses a so-
phisticated system to extract semantic information
automatically from an on-line dictionary, and then
manipulates a set of hand-written rules with hand-
assigned weights to create an interpretation. Rind-
flesch et al. (2000) use hand-coded rule based sys-
tems to extract the factual assertions from biomed-
ical text. Lapata (2000) classifies nominalizations
according to whether the modifier is the subject or
the object of the underlying verb expressed by the
head noun.1
In the related sub-area of information extraction
(Cardie, 1997; Riloff, 1996), the main goal is to find
every instance of particular entities or events of in-
terest. These systems use empirical techniques to
learn which terms signal entities of interest, in order
to fill in pre-defined templates. Our goals are more
general than those of information extraction, and
so this work should be helpful for that task. How-
ever, our approach will not solve issues surrounding
previously unseen proper nouns, which are often im-
portant for information extraction tasks.
There have been several efforts to incorporate lex-
ical hierarchies into statistical processing, primar-
ily for the problem of prepositional phrase (PP)
attachment. The current standard formulation is:
given a verb followed by a noun and a prepositional
phrase, represented by the tuple v, n1, p, n2, deter-
mine which of v or n1 the PP consisting of p and
n2 attaches to, or is most closely associated with.
Because the data is sparse, empirical methods that
train on word occurrences alone (Hindle and Rooth,
1993) have been supplanted by algorithms that gen-
eralize one or both of the nouns according to class-
membership measures (Resnik, 1993; Resnik and
Hearst, 1993; Brill and Resnik, 1994; Li and Abe,
1998), but the statistics are computed for the par-
ticular preposition and verb.
It is not clear how to use the results of such anal-
ysis after they are found; the semantics of the rela-
</bodyText>
<footnote confidence="0.92806">
1Nominalizations are compounds whose head noun is a
nominalized verb and whose modifier is either the subject or
the object of the verb. We do not distinguish the NCs on the
basis of their formation.
</footnote>
<bodyText confidence="0.99986432">
tionship between the terms must still be determined.
In our framework we would cast this problem as
finding the relationship R(p, n2) that best character-
izes the preposition and the NP that follows it, and
then seeing if the categorization algorithm deter-
mines their exists any relationship R&apos;(n1, R(p, n2))
or R&apos;(v,R(p,n2)).
The algorithms used in the related work reflect the
fact that they condition probabilities on a particular
verb and noun. Resnik (1993; 1995) use classes in
Wordnet (Fellbaum, 1998) and a measure of concep-
tual association to generalize over the nouns. Brill
and Resnik (1994) use Brill’s transformation-based
algorithm along with simple counts within a lexi-
cal hierarchy in order to generalize over individual
words. Li and Abe (1998) use a minimum descrip-
tion length-based algorithm to find an optimal tree
cut over WordNet for each classification problem,
finding improvements over both lexical association
(Hindle and Rooth, 1993) and conceptual associa-
tion, and equaling the transformation-based results.
Our approach differs from these in that we are us-
ing machine learning techniques to determine which
level of the lexical hierarchy is appropriate for gen-
eralizing across nouns.
</bodyText>
<sectionHeader confidence="0.983722" genericHeader="method">
3 Noun Compound Relations
</sectionHeader>
<bodyText confidence="0.99782572826087">
In this work we aim for a representation that is in-
termediate in generality between standard case roles
(such as Agent, Patient, Topic, Instrument), and the
specificity required for information extraction. We
have created a set of relations that are sufficiently
general to cover a significant number of noun com-
pounds, but that can be domain specific enough to
be useful in analysis. We want to support relation-
ships between entities that are shown to be impor-
tant in cognitive linguistics, in particular we intend
to support the kinds of inferences that arise from
Talmy’s force dynamics (Talmy, 1985). It has been
shown that relations of this kind can be combined in
order to determine the “directionality” of a sentence
(e.g., whether or not a politician is in favor of, or op-
posed to, a proposal) (Hearst, 1990). In the medical
domain this translates to, for example, mapping a
sentence into a representation showing that a chem-
ical removes an entity that is blocking the passage
of a fluid through a channel.
The problem remains of determining what the ap-
propriate kinds of relations are. In theoretical lin-
guistics, there are contradictory views regarding the
semantic properties of noun compounds (NCs). Levi
(1978) argues that there exists a small set of se-
mantic relationships that NCs may imply. Downing
(1977) argues that the semantics of NCs cannot be
exhausted by any finite listing of relationships. Be-
tween these two extremes lies Warren’s (1978) tax-
onomy of six major semantic relations organized into
a hierarchical structure.
We have identified the 38 relations shown in Ta-
ble 1. We tried to produce relations that correspond
to the linguistic theories such as those of Levi and
Warren, but in many cases these are inappropriate.
Levi’s classes are too general for our purposes; for
example, she collapses the “location” and “time”
relationships into one single class “In” and there-
fore field mouse and autumnal rain belong to the
same class. Warren’s classification schema is much
more detailed, and there is some overlap between
the top levels of Warren’s hierarchy and our set
of relations. For example, our “Cause (2-1)” for
flu virus corresponds to her “Causer-Result” of hay
fever, and our “Person Afflicted” (migraine patient)
can be thought as Warren’s “Belonging-Possessor”
of gunman. Warren differentiates some classes also
on the basis of the semantics of the constituents,
so that, for example, the “Time” relationship is di-
vided up into “Time-Animate Entity” of weekend
guests and “Time-Inanimate Entity” of Sunday pa-
per. Our classification is based on the kind of re-
lationships that hold between the constituent nouns
rather than on the semantics of the head nouns.
For the automatic classification task, we used only
the 18 relations (indicated in bold in Table 1) for
which an adequate number of examples were found
in the current collection. Many NCs were ambigu-
ous, in that they could be described by more than
one semantic relationship. In these cases, we sim-
ply multi-labeled them: for example, cell growth is
both “Activity” and “Change”, tumor regression is
“Ending/reduction” and “Change” and bladder dys-
function is “Location” and “Defect”. Our approach
handles this kind of multi-labeled classification.
Two relation types are especially problematic.
Some compounds are non-compositional or lexical-
ized, such as vitamin k and e2 protein; others defy
classification because the nouns are subtypes of one
another. This group includes migraine headache,
guinea pig, and hbv carrier. We placed all these NCs
in a catch-all category. We also included a “wrong”
category containing word pairs that were incorrectly
labeled as NCs.2
The relations were found by iterative refinement
based on looking at 2245 extracted compounds (de-
scribed in the next section) and finding commonal-
ities among them. Labeling was done by the au-
thors of this paper and a biology student; the NCs
were classified out of context. We expect to con-
tinue development and refinement of these relation-
ship types, based on what ends up clearly being use-
2The percentage of the word pairs extracted that were not
true NCs was about 6%; some examples are: treat migraine,
ten patient, headache more. We do not know, however, how
many NCs we missed. The errors occurred when the wrong
label was assigned by the tagger (see Section 4).
ful “downstream” in the analysis.
The end goal is to combine these relationships in
NCs with more that two constituent nouns, like in
the example intranasal migraine treatment of Sec-
tion 1.
</bodyText>
<sectionHeader confidence="0.738104" genericHeader="method">
4 Collection and Lexical Resources
</sectionHeader>
<bodyText confidence="0.999964921052632">
To create a collection of noun compounds, we per-
formed searches from MedLine, which contains ref-
erences and abstracts from 4300 biomedical journals.
We used several query terms, intended to span across
different subfields. We retained only the titles and
the abstracts of the retrieved documents. On these
titles and abstracts we ran a part-of-speech tagger
(Cutting et al., 1991) and a program that extracts
only sequences of units tagged as nouns. We ex-
tracted NCs with up to 6 constituents, but for this
paper we consider only NCs with 2 constituents.
The Unified Medical Language System (UMLS)
is a biomedical lexical resource produced and
maintained by the National Library of Medicine
(Humphreys et al., 1998). We use the MetaThe-
saurus component to map lexical items into unique
concept IDs (CUIs).3 The UMLS also has a map-
ping from these CUIs into the MeSH lexical hier-
archy (Lowe and Barnett, 1994); we mapped the
CUIs into MeSH terms. There are about 19,000
unique main terms in MeSH, as well as additional
modifiers. There are 15 main subhierarchies (trees)
in MeSH, each corresponding to a major branch
of medical ontology. For example, tree A cor-
responds to Anatomy, tree B to Organisms, and
so on. The longer the name of the MeSH term,
the longer the path from the root and the more
precise the description. For example migraine is
C10.228.140.546.800.525, that is, C (a disease), C10
(Nervous System Diseases), C10.228 (Central Ner-
vous System Diseases) and so on.
We use the MeSH hierarchy for generalization
across classes of nouns; we use it instead of the other
resources in the UMLS primarily because of MeSH’s
hierarchical structure. For these experiments, we
considered only those noun compounds for which
both nouns can be mapped into MeSH terms, re-
sulting in a total of 2245 NCs.
</bodyText>
<sectionHeader confidence="0.859868" genericHeader="evaluation">
5 Method and Results
</sectionHeader>
<bodyText confidence="0.9996562">
Because we have defined noun compound relation
determination as a classification problem, we can
make use of standard classification algorithms. In
particular, we used neural networks to classify across
all relations simultaneously.
</bodyText>
<footnote confidence="0.94858225">
3In some cases a word maps to more than one CUI; for the
work reported here we arbitrarily chose the first mapping in
all cases. In future work we will explore how to make use of
all of the mapped terms.
</footnote>
<table confidence="0.999949943396227">
Name N Examples
Wrong parse (1) 109 exhibit asthma, ten drugs, measure headache
Subtype (4) 393 headaches migraine, fungus candida, hbv carrier,
giant cell, mexico city, t1 tumour, ht1 receptor
Activity/Physical process (5) 59 bile delivery, virus reproduction, bile drainage,
headache activity, bowel function, tb transmission
Ending/reduction 8 migraine relief, headache resolution
Beginning of activity 2 headache induction, headache onset
Change 26 papilloma growth, headache transformation,
disease development, tissue reinforcement
Produces (on a genetic level) (7) 47 polyomavirus genome, actin mrna, cmv dna, protein gene
Cause (1-2) (20) 116 asthma hospitalizations, aids death, automobile accident
heat shock, university fatigue, food infection
Cause (2-1) 18 flu virus, diarrhoea virus, influenza infection
Characteristic (8) 33 receptor hypersensitivity, cell immunity,
drug toxicity, gene polymorphism, drug susceptibility
Physical property 9 blood pressure, artery diameter, water solubility
Defect (27) 52 hormone deficiency, csf fistulas, gene mutation
Physical Make Up 6 blood plasma, bile vomit
Person afflicted (15) 55 aids patient, bmt children, headache group, polio survivors
Demographic attributes 19 childhood migraine, infant colic, women migraineur
Person/center who treats 20 headache specialist, headache center, diseases physicians,
asthma nurse, children hospital
Research on 11 asthma researchers, headache study, language research
Attribute of clinical study (18) 77 headache parameter, attack study, headache interview,
biology analyses, biology laboratory, influenza epidemiology
Procedure (36) 60 tumor marker, genotype diagnosis, blood culture,
brain biopsy, tissue pathology
Frequency/time of (2-1) (22) 25 headache interval, attack frequency,
football season, headache phase, influenza season
Time of (1-2) 4 morning headache, hour headache, weekend migraine
Measure of (23) 54 relief rate, asthma mortality, asthma morbidity,
cell population, hospital survival
Standard 5 headache criteria, society standard
Instrument (1-2) (33) 121 aciclovir therapy, chloroquine treatment,
laser irradiation, aerosol treatment
Instrument (2-1) 8 vaccine antigen, biopsy needle, medicine ginseng
Instrument (1) 16 heroin use, internet use, drug utilization
Object (35) 30 bowel transplantation, kidney transplant, drug delivery
Misuse 11 drug abuse, acetaminophen overdose, ergotamine abuser
Subject 18 headache presentation, glucose metabolism, heat transfer
Purpose (14) 61 headache drugs, hiv medications, voice therapy,
influenza treatment, polio vaccine
Topic (40) 38 time visualization, headache questionnaire, tobacco history,
vaccination registries, health education, pharmacy database
Location (21) 145 brain artery, tract calculi, liver cell, hospital beds
Modal 14 emergency surgery, trauma method
Material (39) 28 formaldehyde vapor, aloe gel, gelatin powder, latex glove,
Bind 4 receptor ligand, carbohydrate ligand
Activator (1-2) 6 acetylcholine receptor, pain signals
Activator (2-1) 4 headache trigger, headache precipitant
Inhibitor 11 adrenoreceptor blockers, influenza prevention
Defect in Location (21 27) 157 lung abscess, artery aneurysm, brain disorder
</table>
<tableCaption confidence="0.999371">
Table 1: The semantic relations defined via iterative refinement over a set of noun compounds. The relations
</tableCaption>
<bodyText confidence="0.6741134">
shown in boldface are those used in the experiments reported on here. Relation ID numbers are shown in
parentheses by the relation names. The second column shows the number of labeled examples for each class;
the last row shows a class consisting of compounds that exhibit more than one relation. The notation (1-2)
and (2-1) indicates the directionality of the relations. For example, Cause (1-2) indicates that the first noun
causes the second, and Cause (2-1) indicates the converse.
</bodyText>
<table confidence="0.943174066666667">
flu vaccination
Model 2 D 4 G 3
Model 3 D 4 808 G 3 770
Model 4 D 4 808 54 G 3 770
Model 5 D 4 808 54 79 G 3 770 670
Model 6 D 4 808 54 79 429 G 3 770 670 310
Table 2: Different lengths of the MeSH descriptors
for the different models
Model Feature Vector
2 42
3 315
4 687
5 950
6 1111
Lexical 1184
</table>
<tableCaption confidence="0.9294745">
Table 3: Length of the feature vectors for different
models.
</tableCaption>
<bodyText confidence="0.999935454545455">
We ran the experiments creating models that used
different levels of the MeSH hierarchy. For example,
for the NC flu vaccination, flu maps to the MeSH
term D4.808.54.79.429.154.349 and vaccination to
G3.770.670.310.890. Flu vaccination for Model 4
would be represented by a vector consisting of the
concatenation of the two descriptors showing only
the first four levels: D4.808.54.79 G3.770.670.310
(see Table 2). When a word maps to a general MeSH
term (like treatment, Y11) zeros are appended to the
end of the descriptor to stand in place of the missing
values (so, for example, treatment in Model 3 is Y
11 0, and in Model 4 is Y 11 0 0, etc.).
The numbers in the MeSH descriptors are cate-
gorical values; we represented them with indicator
variables. That is, for each variable we calculated
the number of possible categories c and then repre-
sented an observation of the variable as a sequence of
c binary variables in which one binary variable was
one and the remaining c − 1 binary variables were
zero.
We also used a representation in which the words
themselves were used as categorical input variables
(we call this representation “lexical”). For this col-
lection of NCs there were 1184 unique nouns and
therefore the feature vector for each noun had 1184
components. In Table 3 we report the length of the
feature vectors for one noun for each model. The en-
tire NC was described by concatenating the feature
vectors for the two nouns in sequence.
The NCs represented in this fashion were used as
input to a neural network. We used a feed-forward
network trained with conjugate gradient descent.
</bodyText>
<table confidence="0.999361125">
Model Acc1 Acc2 Acc3
Lexical: Log Reg 0.31 0.58 0.62
Lexical: NN 0.62 0.73 0.78
2 0.52 0.65 0.72
3 0.58 0.70 0.76
4 0.60 0.70 0.76
5 0.60 0.72 0.78
6 0.61 0.71 0.76
</table>
<tableCaption confidence="0.999001">
Table 4: Test accuracy for each model, where the model
</tableCaption>
<bodyText confidence="0.905714191489362">
number corresponds to the level of the MeSH hierarchy
used for classification. Lexical NN is Neural Network on
Lexical and Lexical: Log Reg is Logistic Regression on
NN. Acc1 refers to how often the correct relation is the
top-scoring relation, Acc2 refers to how often the correct
relation is one of the top two according to the neural net,
and so on. Guessing would yield a result of 0.077.
The network had one hidden layer, in which a hy-
perbolic tangent function was used, and an output
layer representing the 18 relations. A logistic sig-
moid function was used in the output layer to map
the outputs into the interval (0, 1).
The number of units of the output layer was the
number of relations (18) and therefore fixed. The
network was trained for several choices of numbers of
hidden units; we chose the best-performing networks
based on training set error for each of the models.
We subsequently tested these networks on held-out
testing data.
We compared the results with a baseline in which
logistic regression was used on the lexical features.
Given the indicator variable representation of these
features, this logistic regression essentially forms a
table of log-odds for each lexical item. We also com-
pared to a method in which the lexical indicator vari-
ables were used as input to a neural network. This
approach is of interest to see to what extent, if any,
the MeSH-based features affect performance. Note
also that this lexical neural-network approach is fea-
sible in this setting because the number of unique
words is limited (1184) – such an approach would
not scale to larger problems.
In Table 4 and in Figure 1 we report the results
from these experiments. Neural network using lex-
ical features only yields 62% accuracy on average
across all 18 relations. A neural net trained on
Model 6 using the MeSH terms to represent the
nouns yields an accuracy of 61% on average across
all 18 relations. Note that reasonable performance is
also obtained for Model 2, which is a much more gen-
eral representation. Table 4 shows that both meth-
ods achieve up to 78% accuracy at including the cor-
rect relation among the top three hypothesized.
Multi-class classification is a difficult problem
(Vapnik, 1998). In this problem, a baseline in which
Testing set performance on the best models for each MeSH level
Levels of the MeSH Hierarchy
</bodyText>
<figureCaption confidence="0.721431928571429">
Figure 1: Accuracies on the test sets for all the models.
The dotted line at the bottom is the accuracy of guess-
ing (the inverse of the number of classes). The dash-dot
line above this is the accuracy of logistic regression on
the lexical data. The solid line with asterisks is the ac-
curacy of our representation, when only the maximum
output value from the network is considered. The solid
line with circles if the accuracy of getting the right an-
swer within the two largest output values from the neural
network and the last solid line with diamonds is the ac-
curacy of getting the right answer within the first three
outputs from the network. The three flat dashed lines
are the corresponding performances of the neural net-
work on lexical inputs.
</figureCaption>
<bodyText confidence="0.9999524">
the algorithm guesses yields about 5% accuracy. We
see that our method is a significant improvement
over the tabular logistic-regression-based approach,
which yields an accuracy of only 31 percent. Addi-
tionally, despite the significant reduction in raw in-
formation content as compared to the lexical repre-
sentation, the MeSH-based neural network performs
as well as the lexical-based neural network. (And we
again stress that the lexical-based neural network is
not a viable option for larger domains.)
Figure 2 shows the results for each relation.
MeSH-based generalization does better on some re-
lations (for example 14 and 15) and Lexical on others
(7, 22). It turns out that the test set for relation-
ship 7 (“Produces on a genetic level”) is dominated
by NCs containing the words alleles and mrna and
that all the NCs in the training set containing these
words are assigned relation label 7. A similar situa-
tion is seen for relation 22, “Time(2-1)”. In the test
set examples the second noun is either recurrence,
season or time. In the training set, these nouns ap-
pear only in NCs that have been labeled as belonging
to relation 22.
On the other hand, if we look at relations 14 and
15, we find a wider range of words, and in some cases
</bodyText>
<figure confidence="0.701168">
Performance of each class for the LEXICAL model
Classes
</figure>
<figureCaption confidence="0.9966765">
Figure 2: Accuracies for each class. The numbers at the
bottom refer to the class numbers in Table 1. Note the
very high accuracy for the “mixed” relationship 20-27
(last bar on the right).
</figureCaption>
<bodyText confidence="0.999874085714286">
the words in the test set are not present in the train-
ing set. In relationship 14 (“Purpose”), for example,
vaccine appears 6 times in the test set (e.g., varicella
vaccine). In the training set, NCs with vaccine in
it have also been classified as “Instrument” (anti-
gen vaccine, polysaccharide vaccine), as “Object”
(vaccine development), as “Subtype of” (opv vac-
cine) and as “Wrong” (vaccines using). Other words
in the test set for 14 are varicella which is present
in the trainig set only in varicella serology labeled
as “Attribute of clinical study”, drainage which is
in the training set only as “Location” (gallbladder
drainage and tract drainage) and “Activity” (bile
drainage). Other test set words such as immunisa-
tion and carcinogen do not appear in the training
set at all.
In other words, it seems that the MeSHk-based
categorization does better when generalization is re-
quired. Additionally, this data set is “dense” in the
sense that very few testing words are not present in
the training data. This is of course an unrealistic
situation and we wanted to test the robustness of
the method in a more realistic setting. The results
reported in Table 4 and in Figure 1 were obtained
splitting the data into 50% training and 50% testing
for each relation and we had a total of 855 training
points and 805 test points. Of these, only 75 ex-
amples in the testing set consisted of NCs in which
both words were not present in the training set.
We decided to test the robustness of the MeSH-
based model versus the lexical model in the case of
unseen words; we are also interested in seeing the
relative importance of the first versus the second
noun. Therefore, we split the data into 5% training
(73 data points) and 95% testing (1587 data points)
</bodyText>
<figure confidence="0.971020129032258">
2 3 4 5 6
Accuracy on test set
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
Accuracy for the largest NN output
within 2 largest NN output
within 3 largest NN output
1 4 5 7 8 14 15 18 20 21 22 23 27 33 35 36 39 40 2027
Accuracies on the test set for the best model
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
MeSH
Lexical
</figure>
<table confidence="0.993049142857143">
Model All test 1 2 3 4
Lexical: NN 0.23 0.54 0.14 0.33 0.08
2 0.44 0.62 0.25 0.53 0.38
3 0.41 0.62 0.18 0.47 0.35
4 0.42 0.58 0.26 0.39 0.38
5 0.46 0.64 0.28 0.54 0.40
6 0.44 0.64 0.25 0.50 0.39
</table>
<tableCaption confidence="0.8143545">
Table 5: Test accuracy for the four sub-partitions of
the test set.
</tableCaption>
<bodyText confidence="0.993589333333333">
and partitioned the testing set into 4 subsets as fol-
lows (the numbers in parentheses are the numbers
of points for each case):
</bodyText>
<listItem confidence="0.99737425">
• Case 1: NCs for which the first noun was not
present in the training set (424)
• Case 2: NCs for which the second noun was not
present in the training set (252)
• Case 3: NCs for which both nouns were present
in the training set (101)
• Case 4: NCs for which both nouns were not
present in the training set (810).
</listItem>
<bodyText confidence="0.9997959375">
Table 5 and Figures 3 and 4 present the accuracies
for these test set partitions. Figure 3 shows that
the MeSH-based models are more robust than the
lexical when the number of unseen words is high and
when the size of training set is (very) small. In this
more realistic situation, the MeSH models are able to
generalize over previously unseen words. For unseen
words, lexical reduces to guessing.4
Figure 4 shows the accuracy for the MeSH based-
model for the the four cases of Table 5. It is interest-
ing to note that the accuracy for Case 1 (first noun
not present in the training set) is much higher than
the accuracy for Case 2 (second noun not present in
the training set). This seems to indicate that the
second noun is more important for the classification
that the first one.
</bodyText>
<sectionHeader confidence="0.997818" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.98030125">
We have presented a simple approach to corpus-
based assignment of semantic relations for noun
compounds. The main idea is to define a set of rela-
tions that can hold between the terms and use stan-
dard machine learning techniques and a lexical hi-
erarchy to generalize from training instances to new
examples. The initial results are quite promising.
In this task of multi-class classification (with 18
classes) we achieved an accuracy of about 60%.
These results can be compared with Vanderwende
¢Note that for unseen words, the baseline lexical-based
logistic regression approach, which essentially builds a tabular
representation of the log-odds for each class, also reduces to
random guessing.
Testing set performances for different partitions on the test set
Levels of the MeSH Hierarchy
</bodyText>
<figureCaption confidence="0.923646">
Figure 3: The unbroken lines represent the MeSH mod-
</figureCaption>
<bodyText confidence="0.883630111111111">
els accuracies (for the entire test set and for case 4) and
the dashed lines represent the corresponding lexical ac-
curacies. The accuracies are smaller than the previous
case of Table 4 because the training set is much smaller,
but the point of interest is the difference in the perfor-
mance of MeSH vs. lexical in this more difficult setting.
Note that lexical for case 4 reduces to random guessing.
Testing set performances for different partitions on the test set for the MeSH−based model
Levels of the MeSH Hierarchy
</bodyText>
<figureCaption confidence="0.874722">
Figure 4: Accuracy for the MeSH based-model for the
the four cases. All these curves refer to the case of get-
ting exactly the right answer. Note the difference in
performance between case 1 (first noun not present in
the training set) and case 2 (second noun not present in
training set).
</figureCaption>
<bodyText confidence="0.999456857142857">
(1994) who reports an accuracy of 52% with 13
classes and Lapata (2000) whose algorithm achieves
about 80% accuracy for a much simpler binary clas-
sification.
We have shown that a class-based representation
performes as well as a lexical-based model despite
the reduction of raw information content and de-
</bodyText>
<figure confidence="0.89831975">
2 3 4 5 6
Accuracy on test set
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
Accuracy for MeSH for the entire test
Accuracy for MeSH for Case 4
Accuracy for Lexical for the entire test
Accuracy for Lexical for Case 4
Guessing
2 3 4 5 6
Accuracy on test set
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
Accuracy for the entire test
Case 3
Case 1
Case 2
Case 4
</figure>
<bodyText confidence="0.999807533333333">
spite a somewhat errorful mapping from terms to
concepts. We have also shown that representing the
nouns of the compound by a very general represen-
tation (Model 2) achieves a reasonable performance
of aout 52% accuracy on average. This is particu-
larly important in the case of larger collections with
a much bigger number of unique words for which
the lexical-based model is not a viable option. Our
results seem to indicate that we do not lose much
in terms of accuracy using the more compact MeSH
representation.
We have also shown how MeSH-besed models out
perform a lexical-based approach when the num-
ber of training points is small and when the test
set consists of words unseen in the training data.
This indicates that the MeSH models can generalize
successfully over unseen words. Our approach han-
dles “mixed-class” relations naturally. For the mixed
class Defect in Location, the algorithm achieved an
accuracy around 95% for both “Defect” and “Loca-
tion” simultaneously. Our results also indicate that
the second noun (the head) is more important in
determining the relationships than the first one.
In future we plan to train the algorithm to allow
different levels for each noun in the compound. We
also plan to compare the results to the tree cut algo-
rithm reported in (Li and Abe, 1998), which allows
different levels to be identified for different subtrees.
We also plan to tackle the problem of noun com-
pounds containing more than two terms.
</bodyText>
<sectionHeader confidence="0.995988" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99987375">
We would like to thank Nu Lai for help with the
classification of the noun compound relations. This
work was supported in part by NSF award number
IIS-9817353.
</bodyText>
<sectionHeader confidence="0.998677" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999604514563107">
Eric Brill and Philip Resnik. 1994. A rule-based
approach to prepositional phrase attachment dis-
amibuation. In Proceedings of COLING-94.
Claire Cardie. 1997. Empirical methods in informa-
tion extraction. AI Magazine, 18(4).
Douglass R. Cutting, Julian Kupiec, Jan O. Peder-
sen, and Penelope Sibun. 1991. A practical part-
of-speech tagger. In The 3rd Conference on Ap-
plied Natural Language Processing, Trento, Italy.
P. Downing. 1977. On the creation and use of en-
glish compound nouns. Language, (53):810–842.
Christiane Fellbaum, editor. 1998. WordNet: An
Electronic Lexical Database. MIT Press.
Timothy W. Finin. 1980. The Semantic Interpreta-
tion of Compound Nominals. Ph.d. dissertation,
University of Illinois, Urbana, Illinois.
Marti A. Hearst. 1990. A hybrid approach to re-
stricted text interpretation. In Paul S. Jacobs, ed-
itor, Text-Based Intelligent Systems: Current Re-
search in Text Analysis, Information Extraction,
and Retrieval, pages 38–43. GE Research &amp; De-
velopment Center, TR 90CRD198.
Donald Hindle and Mats Rooth. 1993. Structual
ambiguity and lexical relations. Computational
Linguistics, 19(1).
Jerry R. Hobbs, Mark Stickel, Douglas Appelt, and
Paul Martin. 1993. Interpretation as abduction.
Artificial Intelligence, 63(1-2).
L. Humphreys, D.A.B. Lindberg, H.M. Schoolman,
and G. O. Barnett. 1998. The unified medical
language system: An informatics research collab-
oration. Journal of the American Medical Infor-
matics Assocation, 5(1):1–13.
Maria Lapata. 2000. The automatic interpretation
of nominalizations. In AAAI Proceedings.
Mark Lauer and Mark Dras. 1994. A probabilistic
model of compound nouns. In Proceedings of the
7th Australian Joint Conference on AI.
Mark Lauer. 1995. Corpus statistics meet the com-
pound noun. In Proceedings of the 33rd Meeting
of the Association for Computational Linguistics,
June.
Judith Levi. 1978. The Syntax and Semantics of
Complex Nominals. Academic Press, New York.
Hang Li and Naoki Abe. 1998. Generalizing case
frames using a thesaurus and the MDI principle.
Computational Linguistics, 24(2):217–244.
Mark Liberman and Richard Sproat. 1992. The
stress and structure of modified noun phrases in
english. In I.l Sag and A. Szabolsci, editors, Lex-
ical Matters. CSLI Lecture Notes No. 24, Univer-
sity of Chicago Press.
Henry J. Lowe and G. Octo Barnett. 1994. Un-
derstanding and using the medical subject head-
ings (MeSH) vocabulary to perform literature
searches. Journal of the American Medical Asso-
cation (JAMA), 271(4):1103–1108.
Hwee Tou Ng and John Zelle. 1997. Corpus-based
approaches to semantic interpretation in natural
language processing. AI Magazine, 18(4).
James Pustejovsky, Sabine Bergler, and Peter An-
ick. 1993. Lexical semantic techniques for corpus
analysis. Computational Linguistics, 19(2).
Philip Resnik and Marti A. Hearst. 1993. Structural
ambiguity and conceptual relations. In Proceed-
ings of the ACL Workshop on Very Large Corpora,
Columbus, OH.
Philip Resnik. 1993. Selection and Information:
A Class-Based Approach to Lexical Relationships.
Ph.D. thesis, University of Pennsylvania, Decem-
ber. (Institute for Research in Cognitive Science
report IRCS-93-42).
Philip Resnik. 1995. Disambiguating noun group-
ings with respect to WordNet senses. In Third
Workshop on Very Large Corpora. Association for
Computational Linguistics.
Ellen Riloff. 1996. Automatically generating ex-
traction patterns from untagged text. In Pro-
ceedings of the Thirteenth National Conference on
Artificial Intelligence and the Eighth Innovative
Applications of Artificial Intelligence Conference,
Menlo Park. AAAI Press / MIT Press.
Thomas Rindflesch, Lorraine Tanabe, John N. We-
instein, and Lawrence Hunter. 2000. Extraction
of drugs, genes and relations from the biomedical
literature. Pacific Symposium on Biocomputing,
5(5).
Don R. Swanson and N. R. Smalheiser. 1994. As-
sessing a gap in the biomedical literature: Mag-
nesium deficiency and neurologic disease. Neuro-
science Research Communications, 15:1–9.
Len Talmy. 1985. Force dynamics in language and
thought. In Parasession on Causatives and Agen-
tivity, University of Chicago. Chicago Linguistic
Society (21st Regional Meeting).
Lucy Vanderwende. 1994. Algorithm for automatic
interpretation of noun sequences. In Proceedings
of COLING-94, pages 782–788.
V. Vapnik. 1998. Statistical Learning Theory. Ox-
ford University Press.
Beatrice Warren. 1978. Semantic Patterns of Noun-
Noun Compounds. Acta Universitatis Gothobur-
gensis.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.800358">
<title confidence="0.9061305">Classifying the Semantic Relations in Noun Compounds via Domain-Specific Lexical Hierarchy</title>
<author confidence="0.997893">Barbara Rosario</author>
<author confidence="0.997893">Marti</author>
<affiliation confidence="0.999931">School of Information Management &amp; University of California,</affiliation>
<address confidence="0.99579">Berkeley, CA</address>
<abstract confidence="0.999262857142857">We are developing corpus-based techniques for identifying semantic relations at an intermediate level of description (more specific than those used in case frames, but more general than those used in traditional knowledge representation systems). In this paper we describe a classification algorithm for identifying relationships between two-word noun compounds. We find that a very simple approach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
<author>Philip Resnik</author>
</authors>
<title>A rule-based approach to prepositional phrase attachment disamibuation.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING-94.</booktitle>
<contexts>
<context position="6535" citStr="Brill and Resnik, 1994" startWordPosition="1008" endWordPosition="1011">into statistical processing, primarily for the problem of prepositional phrase (PP) attachment. The current standard formulation is: given a verb followed by a noun and a prepositional phrase, represented by the tuple v, n1, p, n2, determine which of v or n1 the PP consisting of p and n2 attaches to, or is most closely associated with. Because the data is sparse, empirical methods that train on word occurrences alone (Hindle and Rooth, 1993) have been supplanted by algorithms that generalize one or both of the nouns according to classmembership measures (Resnik, 1993; Resnik and Hearst, 1993; Brill and Resnik, 1994; Li and Abe, 1998), but the statistics are computed for the particular preposition and verb. It is not clear how to use the results of such analysis after they are found; the semantics of the rela1Nominalizations are compounds whose head noun is a nominalized verb and whose modifier is either the subject or the object of the verb. We do not distinguish the NCs on the basis of their formation. tionship between the terms must still be determined. In our framework we would cast this problem as finding the relationship R(p, n2) that best characterizes the preposition and the NP that follows it, a</context>
</contexts>
<marker>Brill, Resnik, 1994</marker>
<rawString>Eric Brill and Philip Resnik. 1994. A rule-based approach to prepositional phrase attachment disamibuation. In Proceedings of COLING-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
</authors>
<title>Empirical methods in information extraction.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="5348" citStr="Cardie, 1997" startWordPosition="815" endWordPosition="816">on in detail, and constructs a complex set of rules. Vanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules with handassigned weights to create an interpretation. Rindflesch et al. (2000) use hand-coded rule based systems to extract the factual assertions from biomedical text. Lapata (2000) classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun.1 In the related sub-area of information extraction (Cardie, 1997; Riloff, 1996), the main goal is to find every instance of particular entities or events of interest. These systems use empirical techniques to learn which terms signal entities of interest, in order to fill in pre-defined templates. Our goals are more general than those of information extraction, and so this work should be helpful for that task. However, our approach will not solve issues surrounding previously unseen proper nouns, which are often important for information extraction tasks. There have been several efforts to incorporate lexical hierarchies into statistical processing, primar</context>
</contexts>
<marker>Cardie, 1997</marker>
<rawString>Claire Cardie. 1997. Empirical methods in information extraction. AI Magazine, 18(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglass R Cutting</author>
<author>Julian Kupiec</author>
<author>Jan O Pedersen</author>
<author>Penelope Sibun</author>
</authors>
<title>A practical partof-speech tagger.</title>
<date>1991</date>
<booktitle>In The 3rd Conference on Applied Natural Language Processing,</booktitle>
<location>Trento, Italy.</location>
<contexts>
<context position="13113" citStr="Cutting et al., 1991" startWordPosition="2080" endWordPosition="2083">see Section 4). ful “downstream” in the analysis. The end goal is to combine these relationships in NCs with more that two constituent nouns, like in the example intranasal migraine treatment of Section 1. 4 Collection and Lexical Resources To create a collection of noun compounds, we performed searches from MedLine, which contains references and abstracts from 4300 biomedical journals. We used several query terms, intended to span across different subfields. We retained only the titles and the abstracts of the retrieved documents. On these titles and abstracts we ran a part-of-speech tagger (Cutting et al., 1991) and a program that extracts only sequences of units tagged as nouns. We extracted NCs with up to 6 constituents, but for this paper we consider only NCs with 2 constituents. The Unified Medical Language System (UMLS) is a biomedical lexical resource produced and maintained by the National Library of Medicine (Humphreys et al., 1998). We use the MetaThesaurus component to map lexical items into unique concept IDs (CUIs).3 The UMLS also has a mapping from these CUIs into the MeSH lexical hierarchy (Lowe and Barnett, 1994); we mapped the CUIs into MeSH terms. There are about 19,000 unique main t</context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1991</marker>
<rawString>Douglass R. Cutting, Julian Kupiec, Jan O. Pedersen, and Penelope Sibun. 1991. A practical partof-speech tagger. In The 3rd Conference on Applied Natural Language Processing, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Downing</author>
</authors>
<title>On the creation and use of english compound nouns.</title>
<date>1977</date>
<journal>Language,</journal>
<pages>53--810</pages>
<contexts>
<context position="9491" citStr="Downing (1977)" startWordPosition="1495" endWordPosition="1496">y” of a sentence (e.g., whether or not a politician is in favor of, or opposed to, a proposal) (Hearst, 1990). In the medical domain this translates to, for example, mapping a sentence into a representation showing that a chemical removes an entity that is blocking the passage of a fluid through a channel. The problem remains of determining what the appropriate kinds of relations are. In theoretical linguistics, there are contradictory views regarding the semantic properties of noun compounds (NCs). Levi (1978) argues that there exists a small set of semantic relationships that NCs may imply. Downing (1977) argues that the semantics of NCs cannot be exhausted by any finite listing of relationships. Between these two extremes lies Warren’s (1978) taxonomy of six major semantic relations organized into a hierarchical structure. We have identified the 38 relations shown in Table 1. We tried to produce relations that correspond to the linguistic theories such as those of Levi and Warren, but in many cases these are inappropriate. Levi’s classes are too general for our purposes; for example, she collapses the “location” and “time” relationships into one single class “In” and therefore field mouse and</context>
</contexts>
<marker>Downing, 1977</marker>
<rawString>P. Downing. 1977. On the creation and use of english compound nouns. Language, (53):810–842.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7692" citStr="(1998)" startWordPosition="1205" endWordPosition="1205">zes the preposition and the NP that follows it, and then seeing if the categorization algorithm determines their exists any relationship R&apos;(n1, R(p, n2)) or R&apos;(v,R(p,n2)). The algorithms used in the related work reflect the fact that they condition probabilities on a particular verb and noun. Resnik (1993; 1995) use classes in Wordnet (Fellbaum, 1998) and a measure of conceptual association to generalize over the nouns. Brill and Resnik (1994) use Brill’s transformation-based algorithm along with simple counts within a lexical hierarchy in order to generalize over individual words. Li and Abe (1998) use a minimum description length-based algorithm to find an optimal tree cut over WordNet for each classification problem, finding improvements over both lexical association (Hindle and Rooth, 1993) and conceptual association, and equaling the transformation-based results. Our approach differs from these in that we are using machine learning techniques to determine which level of the lexical hierarchy is appropriate for generalizing across nouns. 3 Noun Compound Relations In this work we aim for a representation that is intermediate in generality between standard case roles (such as Agent, Pa</context>
</contexts>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy W Finin</author>
</authors>
<title>The Semantic Interpretation of Compound Nominals.</title>
<date>1980</date>
<institution>University of Illinois,</institution>
<location>Urbana, Illinois.</location>
<note>Ph.d. dissertation,</note>
<contexts>
<context position="4685" citStr="Finin (1980)" startWordPosition="713" endWordPosition="714">tion of the compound from within the text, syntactic analysis of the compound (left versus right association), and the interpretation of the underlying semantics. Several researchers have tackled the syntactic analysis (Lauer, 1995; Pustejovsky et al., 1993; Liberman and Sproat, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics. Most related work relies on hand-written rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Vanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules with handassigned weights to create an interpretation. Rindflesch et al. (2000) use hand-coded rule based systems to extract the factual assertions from biomedical text. Lapata (2000) classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun.1 </context>
</contexts>
<marker>Finin, 1980</marker>
<rawString>Timothy W. Finin. 1980. The Semantic Interpretation of Compound Nominals. Ph.d. dissertation, University of Illinois, Urbana, Illinois.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>A hybrid approach to restricted text interpretation.</title>
<date>1990</date>
<booktitle>Text-Based Intelligent Systems: Current Research in Text Analysis, Information Extraction, and Retrieval,</booktitle>
<pages>38--43</pages>
<editor>In Paul S. Jacobs, editor,</editor>
<contexts>
<context position="8986" citStr="Hearst, 1990" startWordPosition="1413" endWordPosition="1414">n. We have created a set of relations that are sufficiently general to cover a significant number of noun compounds, but that can be domain specific enough to be useful in analysis. We want to support relationships between entities that are shown to be important in cognitive linguistics, in particular we intend to support the kinds of inferences that arise from Talmy’s force dynamics (Talmy, 1985). It has been shown that relations of this kind can be combined in order to determine the “directionality” of a sentence (e.g., whether or not a politician is in favor of, or opposed to, a proposal) (Hearst, 1990). In the medical domain this translates to, for example, mapping a sentence into a representation showing that a chemical removes an entity that is blocking the passage of a fluid through a channel. The problem remains of determining what the appropriate kinds of relations are. In theoretical linguistics, there are contradictory views regarding the semantic properties of noun compounds (NCs). Levi (1978) argues that there exists a small set of semantic relationships that NCs may imply. Downing (1977) argues that the semantics of NCs cannot be exhausted by any finite listing of relationships. B</context>
</contexts>
<marker>Hearst, 1990</marker>
<rawString>Marti A. Hearst. 1990. A hybrid approach to restricted text interpretation. In Paul S. Jacobs, editor, Text-Based Intelligent Systems: Current Research in Text Analysis, Information Extraction, and Retrieval, pages 38–43. GE Research &amp; Development Center, TR 90CRD198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structual ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="6358" citStr="Hindle and Rooth, 1993" startWordPosition="979" endWordPosition="982">issues surrounding previously unseen proper nouns, which are often important for information extraction tasks. There have been several efforts to incorporate lexical hierarchies into statistical processing, primarily for the problem of prepositional phrase (PP) attachment. The current standard formulation is: given a verb followed by a noun and a prepositional phrase, represented by the tuple v, n1, p, n2, determine which of v or n1 the PP consisting of p and n2 attaches to, or is most closely associated with. Because the data is sparse, empirical methods that train on word occurrences alone (Hindle and Rooth, 1993) have been supplanted by algorithms that generalize one or both of the nouns according to classmembership measures (Resnik, 1993; Resnik and Hearst, 1993; Brill and Resnik, 1994; Li and Abe, 1998), but the statistics are computed for the particular preposition and verb. It is not clear how to use the results of such analysis after they are found; the semantics of the rela1Nominalizations are compounds whose head noun is a nominalized verb and whose modifier is either the subject or the object of the verb. We do not distinguish the NCs on the basis of their formation. tionship between the terms</context>
<context position="7891" citStr="Hindle and Rooth, 1993" startWordPosition="1231" endWordPosition="1234">thms used in the related work reflect the fact that they condition probabilities on a particular verb and noun. Resnik (1993; 1995) use classes in Wordnet (Fellbaum, 1998) and a measure of conceptual association to generalize over the nouns. Brill and Resnik (1994) use Brill’s transformation-based algorithm along with simple counts within a lexical hierarchy in order to generalize over individual words. Li and Abe (1998) use a minimum description length-based algorithm to find an optimal tree cut over WordNet for each classification problem, finding improvements over both lexical association (Hindle and Rooth, 1993) and conceptual association, and equaling the transformation-based results. Our approach differs from these in that we are using machine learning techniques to determine which level of the lexical hierarchy is appropriate for generalizing across nouns. 3 Noun Compound Relations In this work we aim for a representation that is intermediate in generality between standard case roles (such as Agent, Patient, Topic, Instrument), and the specificity required for information extraction. We have created a set of relations that are sufficiently general to cover a significant number of noun compounds, b</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Donald Hindle and Mats Rooth. 1993. Structual ambiguity and lexical relations. Computational Linguistics, 19(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
<author>Mark Stickel</author>
<author>Douglas Appelt</author>
<author>Paul Martin</author>
</authors>
<title>Interpretation as abduction.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--1</pages>
<contexts>
<context position="2584" citStr="Hobbs et al., 1993" startWordPosition="381" endWordPosition="384">a way to analyze the meanings of the noun compounds. Our goal is to extract propositional information from text, and as a step towards this goal, we classify constituents according to which semantic relationships hold between them. For example, we want to characterize the treatment-for-disease relationship between the words of migraine treatment versus the method-of-treatment relationship between the words of aerosol treatment. These relations are intended to be combined to produce larger propositions that can then be used in a variety of interpretation paradigms, such as abductive reasoning (Hobbs et al., 1993) or inductive logic programming (Ng and Zelle, 1997). Note that because we are concerned with the semantic relations that hold between the concepts, as opposed to the more standard, syntax-driven computational goal of determining left versus right association, this has the fortuitous effect of changing the problem into one of classification, amenable to standard machine learning classification techniques. We have found that we can use such algorithms to classify relationships between two-word noun compounds with a surprising degree of accuracy. A one-out-of-eighteen classification using a neur</context>
</contexts>
<marker>Hobbs, Stickel, Appelt, Martin, 1993</marker>
<rawString>Jerry R. Hobbs, Mark Stickel, Douglas Appelt, and Paul Martin. 1993. Interpretation as abduction. Artificial Intelligence, 63(1-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Humphreys</author>
<author>D A B Lindberg</author>
<author>H M Schoolman</author>
<author>G O Barnett</author>
</authors>
<title>The unified medical language system: An informatics research collaboration.</title>
<date>1998</date>
<journal>Journal of the American Medical Informatics Assocation,</journal>
<volume>5</volume>
<issue>1</issue>
<contexts>
<context position="13448" citStr="Humphreys et al., 1998" startWordPosition="2136" endWordPosition="2139"> references and abstracts from 4300 biomedical journals. We used several query terms, intended to span across different subfields. We retained only the titles and the abstracts of the retrieved documents. On these titles and abstracts we ran a part-of-speech tagger (Cutting et al., 1991) and a program that extracts only sequences of units tagged as nouns. We extracted NCs with up to 6 constituents, but for this paper we consider only NCs with 2 constituents. The Unified Medical Language System (UMLS) is a biomedical lexical resource produced and maintained by the National Library of Medicine (Humphreys et al., 1998). We use the MetaThesaurus component to map lexical items into unique concept IDs (CUIs).3 The UMLS also has a mapping from these CUIs into the MeSH lexical hierarchy (Lowe and Barnett, 1994); we mapped the CUIs into MeSH terms. There are about 19,000 unique main terms in MeSH, as well as additional modifiers. There are 15 main subhierarchies (trees) in MeSH, each corresponding to a major branch of medical ontology. For example, tree A corresponds to Anatomy, tree B to Organisms, and so on. The longer the name of the MeSH term, the longer the path from the root and the more precise the descrip</context>
</contexts>
<marker>Humphreys, Lindberg, Schoolman, Barnett, 1998</marker>
<rawString>L. Humphreys, D.A.B. Lindberg, H.M. Schoolman, and G. O. Barnett. 1998. The unified medical language system: An informatics research collaboration. Journal of the American Medical Informatics Assocation, 5(1):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Lapata</author>
</authors>
<title>The automatic interpretation of nominalizations.</title>
<date>2000</date>
<booktitle>In AAAI Proceedings.</booktitle>
<contexts>
<context position="5142" citStr="Lapata (2000)" startWordPosition="784" endWordPosition="785">nterested in the third task, interpretation of the underlying semantics. Most related work relies on hand-written rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Vanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules with handassigned weights to create an interpretation. Rindflesch et al. (2000) use hand-coded rule based systems to extract the factual assertions from biomedical text. Lapata (2000) classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun.1 In the related sub-area of information extraction (Cardie, 1997; Riloff, 1996), the main goal is to find every instance of particular entities or events of interest. These systems use empirical techniques to learn which terms signal entities of interest, in order to fill in pre-defined templates. Our goals are more general than those of information extraction, and so this work should be helpful for that task. However, our approach will not solve issues </context>
<context position="30841" citStr="Lapata (2000)" startWordPosition="5071" endWordPosition="5072">ence in the performance of MeSH vs. lexical in this more difficult setting. Note that lexical for case 4 reduces to random guessing. Testing set performances for different partitions on the test set for the MeSH−based model Levels of the MeSH Hierarchy Figure 4: Accuracy for the MeSH based-model for the the four cases. All these curves refer to the case of getting exactly the right answer. Note the difference in performance between case 1 (first noun not present in the training set) and case 2 (second noun not present in training set). (1994) who reports an accuracy of 52% with 13 classes and Lapata (2000) whose algorithm achieves about 80% accuracy for a much simpler binary classification. We have shown that a class-based representation performes as well as a lexical-based model despite the reduction of raw information content and de2 3 4 5 6 Accuracy on test set 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 1 Accuracy for MeSH for the entire test Accuracy for MeSH for Case 4 Accuracy for Lexical for the entire test Accuracy for Lexical for Case 4 Guessing 2 3 4 5 6 Accuracy on test set 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 1 Accuracy for the entire test Case 3 Case 1 Case 2 Case 4 spite a somewhat er</context>
</contexts>
<marker>Lapata, 2000</marker>
<rawString>Maria Lapata. 2000. The automatic interpretation of nominalizations. In AAAI Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
<author>Mark Dras</author>
</authors>
<title>A probabilistic model of compound nouns.</title>
<date>1994</date>
<booktitle>In Proceedings of the 7th Australian Joint Conference on AI.</booktitle>
<contexts>
<context position="4004" citStr="Lauer and Dras (1994)" startWordPosition="605" endWordPosition="608"> this is a promising approach for a variety of semantic labeling tasks. The reminder of this paper is organized as follows: Section 2 describes related work, Section 3 describes the semantic relations and how they were chosen, and Section 4 describes the data collection and ontologies. In Section 5 we describe the method for automatically assigning semantic relations to noun compounds, and report the results of experiments using this method. Section 6 concludes the paper and discusses future work. 2 Related Work Several approaches have been proposed for empirical noun compound interpretation. Lauer and Dras (1994) point out that there are three components to the problem: identification of the compound from within the text, syntactic analysis of the compound (left versus right association), and the interpretation of the underlying semantics. Several researchers have tackled the syntactic analysis (Lauer, 1995; Pustejovsky et al., 1993; Liberman and Sproat, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics. Mo</context>
</contexts>
<marker>Lauer, Dras, 1994</marker>
<rawString>Mark Lauer and Mark Dras. 1994. A probabilistic model of compound nouns. In Proceedings of the 7th Australian Joint Conference on AI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Corpus statistics meet the compound noun.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Meeting of the Association for Computational Linguistics,</booktitle>
<contexts>
<context position="4304" citStr="Lauer, 1995" startWordPosition="651" endWordPosition="652">e the method for automatically assigning semantic relations to noun compounds, and report the results of experiments using this method. Section 6 concludes the paper and discusses future work. 2 Related Work Several approaches have been proposed for empirical noun compound interpretation. Lauer and Dras (1994) point out that there are three components to the problem: identification of the compound from within the text, syntactic analysis of the compound (left versus right association), and the interpretation of the underlying semantics. Several researchers have tackled the syntactic analysis (Lauer, 1995; Pustejovsky et al., 1993; Liberman and Sproat, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics. Most related work relies on hand-written rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Vanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictio</context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Corpus statistics meet the compound noun. In Proceedings of the 33rd Meeting of the Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judith Levi</author>
</authors>
<title>The Syntax and Semantics of Complex Nominals.</title>
<date>1978</date>
<publisher>Academic Press,</publisher>
<location>New York.</location>
<contexts>
<context position="9393" citStr="Levi (1978)" startWordPosition="1478" endWordPosition="1479">been shown that relations of this kind can be combined in order to determine the “directionality” of a sentence (e.g., whether or not a politician is in favor of, or opposed to, a proposal) (Hearst, 1990). In the medical domain this translates to, for example, mapping a sentence into a representation showing that a chemical removes an entity that is blocking the passage of a fluid through a channel. The problem remains of determining what the appropriate kinds of relations are. In theoretical linguistics, there are contradictory views regarding the semantic properties of noun compounds (NCs). Levi (1978) argues that there exists a small set of semantic relationships that NCs may imply. Downing (1977) argues that the semantics of NCs cannot be exhausted by any finite listing of relationships. Between these two extremes lies Warren’s (1978) taxonomy of six major semantic relations organized into a hierarchical structure. We have identified the 38 relations shown in Table 1. We tried to produce relations that correspond to the linguistic theories such as those of Levi and Warren, but in many cases these are inappropriate. Levi’s classes are too general for our purposes; for example, she collapse</context>
</contexts>
<marker>Levi, 1978</marker>
<rawString>Judith Levi. 1978. The Syntax and Semantics of Complex Nominals. Academic Press, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Li</author>
<author>Naoki Abe</author>
</authors>
<title>Generalizing case frames using a thesaurus and the MDI principle.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>2</issue>
<contexts>
<context position="6554" citStr="Li and Abe, 1998" startWordPosition="1012" endWordPosition="1015">ing, primarily for the problem of prepositional phrase (PP) attachment. The current standard formulation is: given a verb followed by a noun and a prepositional phrase, represented by the tuple v, n1, p, n2, determine which of v or n1 the PP consisting of p and n2 attaches to, or is most closely associated with. Because the data is sparse, empirical methods that train on word occurrences alone (Hindle and Rooth, 1993) have been supplanted by algorithms that generalize one or both of the nouns according to classmembership measures (Resnik, 1993; Resnik and Hearst, 1993; Brill and Resnik, 1994; Li and Abe, 1998), but the statistics are computed for the particular preposition and verb. It is not clear how to use the results of such analysis after they are found; the semantics of the rela1Nominalizations are compounds whose head noun is a nominalized verb and whose modifier is either the subject or the object of the verb. We do not distinguish the NCs on the basis of their formation. tionship between the terms must still be determined. In our framework we would cast this problem as finding the relationship R(p, n2) that best characterizes the preposition and the NP that follows it, and then seeing if t</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Hang Li and Naoki Abe. 1998. Generalizing case frames using a thesaurus and the MDI principle. Computational Linguistics, 24(2):217–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Liberman</author>
<author>Richard Sproat</author>
</authors>
<title>The stress and structure of modified noun phrases in english.</title>
<date>1992</date>
<booktitle>In I.l Sag</booktitle>
<volume>24</volume>
<editor>and A. Szabolsci, editors, Lexical Matters.</editor>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="4358" citStr="Liberman and Sproat, 1992" startWordPosition="657" endWordPosition="660">g semantic relations to noun compounds, and report the results of experiments using this method. Section 6 concludes the paper and discusses future work. 2 Related Work Several approaches have been proposed for empirical noun compound interpretation. Lauer and Dras (1994) point out that there are three components to the problem: identification of the compound from within the text, syntactic analysis of the compound (left versus right association), and the interpretation of the underlying semantics. Several researchers have tackled the syntactic analysis (Lauer, 1995; Pustejovsky et al., 1993; Liberman and Sproat, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics. Most related work relies on hand-written rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Vanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules</context>
</contexts>
<marker>Liberman, Sproat, 1992</marker>
<rawString>Mark Liberman and Richard Sproat. 1992. The stress and structure of modified noun phrases in english. In I.l Sag and A. Szabolsci, editors, Lexical Matters. CSLI Lecture Notes No. 24, University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry J Lowe</author>
<author>G Octo Barnett</author>
</authors>
<title>Understanding and using the medical subject headings (MeSH) vocabulary to perform literature searches.</title>
<date>1994</date>
<journal>Journal of the American Medical Assocation (JAMA),</journal>
<volume>271</volume>
<issue>4</issue>
<contexts>
<context position="13639" citStr="Lowe and Barnett, 1994" startWordPosition="2171" endWordPosition="2174">ed documents. On these titles and abstracts we ran a part-of-speech tagger (Cutting et al., 1991) and a program that extracts only sequences of units tagged as nouns. We extracted NCs with up to 6 constituents, but for this paper we consider only NCs with 2 constituents. The Unified Medical Language System (UMLS) is a biomedical lexical resource produced and maintained by the National Library of Medicine (Humphreys et al., 1998). We use the MetaThesaurus component to map lexical items into unique concept IDs (CUIs).3 The UMLS also has a mapping from these CUIs into the MeSH lexical hierarchy (Lowe and Barnett, 1994); we mapped the CUIs into MeSH terms. There are about 19,000 unique main terms in MeSH, as well as additional modifiers. There are 15 main subhierarchies (trees) in MeSH, each corresponding to a major branch of medical ontology. For example, tree A corresponds to Anatomy, tree B to Organisms, and so on. The longer the name of the MeSH term, the longer the path from the root and the more precise the description. For example migraine is C10.228.140.546.800.525, that is, C (a disease), C10 (Nervous System Diseases), C10.228 (Central Nervous System Diseases) and so on. We use the MeSH hierarchy fo</context>
</contexts>
<marker>Lowe, Barnett, 1994</marker>
<rawString>Henry J. Lowe and G. Octo Barnett. 1994. Understanding and using the medical subject headings (MeSH) vocabulary to perform literature searches. Journal of the American Medical Assocation (JAMA), 271(4):1103–1108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>John Zelle</author>
</authors>
<title>Corpus-based approaches to semantic interpretation in natural language processing.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="2636" citStr="Ng and Zelle, 1997" startWordPosition="389" endWordPosition="392"> Our goal is to extract propositional information from text, and as a step towards this goal, we classify constituents according to which semantic relationships hold between them. For example, we want to characterize the treatment-for-disease relationship between the words of migraine treatment versus the method-of-treatment relationship between the words of aerosol treatment. These relations are intended to be combined to produce larger propositions that can then be used in a variety of interpretation paradigms, such as abductive reasoning (Hobbs et al., 1993) or inductive logic programming (Ng and Zelle, 1997). Note that because we are concerned with the semantic relations that hold between the concepts, as opposed to the more standard, syntax-driven computational goal of determining left versus right association, this has the fortuitous effect of changing the problem into one of classification, amenable to standard machine learning classification techniques. We have found that we can use such algorithms to classify relationships between two-word noun compounds with a surprising degree of accuracy. A one-out-of-eighteen classification using a neural net achieves accuracies as high as 62%. By taking</context>
</contexts>
<marker>Ng, Zelle, 1997</marker>
<rawString>Hwee Tou Ng and John Zelle. 1997. Corpus-based approaches to semantic interpretation in natural language processing. AI Magazine, 18(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Sabine Bergler</author>
<author>Peter Anick</author>
</authors>
<title>Lexical semantic techniques for corpus analysis.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="4330" citStr="Pustejovsky et al., 1993" startWordPosition="653" endWordPosition="656">for automatically assigning semantic relations to noun compounds, and report the results of experiments using this method. Section 6 concludes the paper and discusses future work. 2 Related Work Several approaches have been proposed for empirical noun compound interpretation. Lauer and Dras (1994) point out that there are three components to the problem: identification of the compound from within the text, syntactic analysis of the compound (left versus right association), and the interpretation of the underlying semantics. Several researchers have tackled the syntactic analysis (Lauer, 1995; Pustejovsky et al., 1993; Liberman and Sproat, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics. Most related work relies on hand-written rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Vanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates</context>
</contexts>
<marker>Pustejovsky, Bergler, Anick, 1993</marker>
<rawString>James Pustejovsky, Sabine Bergler, and Peter Anick. 1993. Lexical semantic techniques for corpus analysis. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
<author>Marti A Hearst</author>
</authors>
<title>Structural ambiguity and conceptual relations.</title>
<date>1993</date>
<booktitle>In Proceedings of the ACL Workshop on Very Large Corpora,</booktitle>
<location>Columbus, OH.</location>
<contexts>
<context position="6511" citStr="Resnik and Hearst, 1993" startWordPosition="1004" endWordPosition="1007">rate lexical hierarchies into statistical processing, primarily for the problem of prepositional phrase (PP) attachment. The current standard formulation is: given a verb followed by a noun and a prepositional phrase, represented by the tuple v, n1, p, n2, determine which of v or n1 the PP consisting of p and n2 attaches to, or is most closely associated with. Because the data is sparse, empirical methods that train on word occurrences alone (Hindle and Rooth, 1993) have been supplanted by algorithms that generalize one or both of the nouns according to classmembership measures (Resnik, 1993; Resnik and Hearst, 1993; Brill and Resnik, 1994; Li and Abe, 1998), but the statistics are computed for the particular preposition and verb. It is not clear how to use the results of such analysis after they are found; the semantics of the rela1Nominalizations are compounds whose head noun is a nominalized verb and whose modifier is either the subject or the object of the verb. We do not distinguish the NCs on the basis of their formation. tionship between the terms must still be determined. In our framework we would cast this problem as finding the relationship R(p, n2) that best characterizes the preposition and t</context>
</contexts>
<marker>Resnik, Hearst, 1993</marker>
<rawString>Philip Resnik and Marti A. Hearst. 1993. Structural ambiguity and conceptual relations. In Proceedings of the ACL Workshop on Very Large Corpora, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Selection and Information: A Class-Based Approach to Lexical Relationships.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania, December. (Institute</institution>
<note>for Research in Cognitive Science report IRCS-93-42).</note>
<contexts>
<context position="6486" citStr="Resnik, 1993" startWordPosition="1002" endWordPosition="1003">rts to incorporate lexical hierarchies into statistical processing, primarily for the problem of prepositional phrase (PP) attachment. The current standard formulation is: given a verb followed by a noun and a prepositional phrase, represented by the tuple v, n1, p, n2, determine which of v or n1 the PP consisting of p and n2 attaches to, or is most closely associated with. Because the data is sparse, empirical methods that train on word occurrences alone (Hindle and Rooth, 1993) have been supplanted by algorithms that generalize one or both of the nouns according to classmembership measures (Resnik, 1993; Resnik and Hearst, 1993; Brill and Resnik, 1994; Li and Abe, 1998), but the statistics are computed for the particular preposition and verb. It is not clear how to use the results of such analysis after they are found; the semantics of the rela1Nominalizations are compounds whose head noun is a nominalized verb and whose modifier is either the subject or the object of the verb. We do not distinguish the NCs on the basis of their formation. tionship between the terms must still be determined. In our framework we would cast this problem as finding the relationship R(p, n2) that best characteri</context>
</contexts>
<marker>Resnik, 1993</marker>
<rawString>Philip Resnik. 1993. Selection and Information: A Class-Based Approach to Lexical Relationships. Ph.D. thesis, University of Pennsylvania, December. (Institute for Research in Cognitive Science report IRCS-93-42).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Disambiguating noun groupings with respect to WordNet senses.</title>
<date>1995</date>
<booktitle>In Third Workshop on Very Large Corpora. Association for Computational Linguistics.</booktitle>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Disambiguating noun groupings with respect to WordNet senses. In Third Workshop on Very Large Corpora. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence and the Eighth Innovative Applications of Artificial Intelligence Conference, Menlo Park.</booktitle>
<publisher>AAAI Press / MIT Press.</publisher>
<contexts>
<context position="5363" citStr="Riloff, 1996" startWordPosition="817" endWordPosition="818">and constructs a complex set of rules. Vanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules with handassigned weights to create an interpretation. Rindflesch et al. (2000) use hand-coded rule based systems to extract the factual assertions from biomedical text. Lapata (2000) classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun.1 In the related sub-area of information extraction (Cardie, 1997; Riloff, 1996), the main goal is to find every instance of particular entities or events of interest. These systems use empirical techniques to learn which terms signal entities of interest, in order to fill in pre-defined templates. Our goals are more general than those of information extraction, and so this work should be helpful for that task. However, our approach will not solve issues surrounding previously unseen proper nouns, which are often important for information extraction tasks. There have been several efforts to incorporate lexical hierarchies into statistical processing, primarily for the pro</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>Ellen Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence and the Eighth Innovative Applications of Artificial Intelligence Conference, Menlo Park. AAAI Press / MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Rindflesch</author>
<author>Lorraine Tanabe</author>
<author>John N Weinstein</author>
<author>Lawrence Hunter</author>
</authors>
<title>Extraction of drugs, genes and relations from the biomedical literature. Pacific Symposium on</title>
<date>2000</date>
<journal>Biocomputing,</journal>
<volume>5</volume>
<issue>5</issue>
<contexts>
<context position="5038" citStr="Rindflesch et al. (2000)" startWordPosition="764" endWordPosition="768">ubconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics. Most related work relies on hand-written rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Vanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules with handassigned weights to create an interpretation. Rindflesch et al. (2000) use hand-coded rule based systems to extract the factual assertions from biomedical text. Lapata (2000) classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun.1 In the related sub-area of information extraction (Cardie, 1997; Riloff, 1996), the main goal is to find every instance of particular entities or events of interest. These systems use empirical techniques to learn which terms signal entities of interest, in order to fill in pre-defined templates. Our goals are more general than those of information ex</context>
</contexts>
<marker>Rindflesch, Tanabe, Weinstein, Hunter, 2000</marker>
<rawString>Thomas Rindflesch, Lorraine Tanabe, John N. Weinstein, and Lawrence Hunter. 2000. Extraction of drugs, genes and relations from the biomedical literature. Pacific Symposium on Biocomputing, 5(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Don R Swanson</author>
<author>N R Smalheiser</author>
</authors>
<title>Assessing a gap in the biomedical literature: Magnesium deficiency and neurologic disease.</title>
<date>1994</date>
<journal>Neuroscience Research Communications,</journal>
<pages>15--1</pages>
<contexts>
<context position="1293" citStr="Swanson and Smalheiser, 1994" startWordPosition="175" endWordPosition="178">proach using a machine learning algorithm and a domain-specific lexical hierarchy successfully generalizes from training instances, performing better on previously unseen words than a baseline consisting of training on the words themselves. 1 Introduction We are exploring empirical methods of determining semantic relationships between constituents in natural language. Our current project focuses on biomedical text, both because it poses interesting challenges, and because it should be possible to make inferences about propositions that hold between scientific concepts within biomedical texts (Swanson and Smalheiser, 1994). One of the important challenges of biomedical text, along with most other technical text, is the proliferation of noun compounds. A typical article title is shown below; it consists a cascade of four noun phrases linked by prepositions: Open-labeled long-term study of the efficacy, safety, and tolerability of subcutaneous sumatriptan in acute migraine treatment. The real concern in analyzing such a title is in determining the relationships that hold between different concepts, rather than on finding the appropriate attachments (which is especially difficult given the lack of a verb). And bef</context>
</contexts>
<marker>Swanson, Smalheiser, 1994</marker>
<rawString>Don R. Swanson and N. R. Smalheiser. 1994. Assessing a gap in the biomedical literature: Magnesium deficiency and neurologic disease. Neuroscience Research Communications, 15:1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Len Talmy</author>
</authors>
<title>Force dynamics in language and thought.</title>
<date>1985</date>
<booktitle>In Parasession on Causatives and Agentivity, University of Chicago. Chicago Linguistic Society (21st Regional Meeting).</booktitle>
<contexts>
<context position="8773" citStr="Talmy, 1985" startWordPosition="1374" endWordPosition="1375">elations In this work we aim for a representation that is intermediate in generality between standard case roles (such as Agent, Patient, Topic, Instrument), and the specificity required for information extraction. We have created a set of relations that are sufficiently general to cover a significant number of noun compounds, but that can be domain specific enough to be useful in analysis. We want to support relationships between entities that are shown to be important in cognitive linguistics, in particular we intend to support the kinds of inferences that arise from Talmy’s force dynamics (Talmy, 1985). It has been shown that relations of this kind can be combined in order to determine the “directionality” of a sentence (e.g., whether or not a politician is in favor of, or opposed to, a proposal) (Hearst, 1990). In the medical domain this translates to, for example, mapping a sentence into a representation showing that a chemical removes an entity that is blocking the passage of a fluid through a channel. The problem remains of determining what the appropriate kinds of relations are. In theoretical linguistics, there are contradictory views regarding the semantic properties of noun compound</context>
</contexts>
<marker>Talmy, 1985</marker>
<rawString>Len Talmy. 1985. Force dynamics in language and thought. In Parasession on Causatives and Agentivity, University of Chicago. Chicago Linguistic Society (21st Regional Meeting).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucy Vanderwende</author>
</authors>
<title>Algorithm for automatic interpretation of noun sequences.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING-94,</booktitle>
<pages>782--788</pages>
<contexts>
<context position="4807" citStr="Vanderwende (1994)" startWordPosition="732" endWordPosition="733">he interpretation of the underlying semantics. Several researchers have tackled the syntactic analysis (Lauer, 1995; Pustejovsky et al., 1993; Liberman and Sproat, 1992), usually using a variation of the idea of finding the subconstituents elsewhere in the corpus and using those to predict how the larger compounds are structured. We are interested in the third task, interpretation of the underlying semantics. Most related work relies on hand-written rules of one kind or another. Finin (1980) examines the problem of noun compound interpretation in detail, and constructs a complex set of rules. Vanderwende (1994) uses a sophisticated system to extract semantic information automatically from an on-line dictionary, and then manipulates a set of hand-written rules with handassigned weights to create an interpretation. Rindflesch et al. (2000) use hand-coded rule based systems to extract the factual assertions from biomedical text. Lapata (2000) classifies nominalizations according to whether the modifier is the subject or the object of the underlying verb expressed by the head noun.1 In the related sub-area of information extraction (Cardie, 1997; Riloff, 1996), the main goal is to find every instance of</context>
</contexts>
<marker>Vanderwende, 1994</marker>
<rawString>Lucy Vanderwende. 1994. Algorithm for automatic interpretation of noun sequences. In Proceedings of COLING-94, pages 782–788.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="23186" citStr="Vapnik, 1998" startWordPosition="3701" endWordPosition="3702">ms. In Table 4 and in Figure 1 we report the results from these experiments. Neural network using lexical features only yields 62% accuracy on average across all 18 relations. A neural net trained on Model 6 using the MeSH terms to represent the nouns yields an accuracy of 61% on average across all 18 relations. Note that reasonable performance is also obtained for Model 2, which is a much more general representation. Table 4 shows that both methods achieve up to 78% accuracy at including the correct relation among the top three hypothesized. Multi-class classification is a difficult problem (Vapnik, 1998). In this problem, a baseline in which Testing set performance on the best models for each MeSH level Levels of the MeSH Hierarchy Figure 1: Accuracies on the test sets for all the models. The dotted line at the bottom is the accuracy of guessing (the inverse of the number of classes). The dash-dot line above this is the accuracy of logistic regression on the lexical data. The solid line with asterisks is the accuracy of our representation, when only the maximum output value from the network is considered. The solid line with circles if the accuracy of getting the right answer within the two l</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>V. Vapnik. 1998. Statistical Learning Theory. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beatrice Warren</author>
</authors>
<title>Semantic Patterns of NounNoun Compounds. Acta Universitatis Gothoburgensis.</title>
<date>1978</date>
<marker>Warren, 1978</marker>
<rawString>Beatrice Warren. 1978. Semantic Patterns of NounNoun Compounds. Acta Universitatis Gothoburgensis.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>