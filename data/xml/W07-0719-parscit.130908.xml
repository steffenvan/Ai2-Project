<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000643">
<title confidence="0.99826">
Context-aware Discriminative Phrase Selection
for Statistical Machine Translation
</title>
<author confidence="0.980904">
Jes´us Gim´enez and Lluis M`arquez
</author>
<affiliation confidence="0.950023">
TALP Research Center, LSI Department
</affiliation>
<address confidence="0.740378">
Universitat Polit`ecnica de Catalunya
Jordi Girona Salgado 1–3, E-08034, Barcelona
</address>
<email confidence="0.999694">
{jgimenez,lluism}@lsi.upc.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999594375">
In this work we revise the application
of discriminative learning to the problem
of phrase selection in Statistical Machine
Translation. Inspired by common tech-
niques used in Word Sense Disambiguation,
we train classifiers based on local context
to predict possible phrase translations. Our
work extends that of Vickrey et al. (2005) in
two main aspects. First, we move from word
translation to phrase translation. Second, we
move from the ‘blank-�lling’ task to the ‘full
translation’ task. We report results on a set
of highly frequent source phrases, obtaining
a significant improvement, specially with re-
spect to adequacy, according to a rigorous
process of manual evaluation.
</bodyText>
<sectionHeader confidence="0.999509" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999851545454546">
Translations tables in Phrase-based Statistical Ma-
chine Translation (SMT) are often built on the ba-
sis of Maximum-likelihood Estimation (MLE), be-
ing one of the major limitations of this approach that
the source sentence context in which phrases occur
is completely ignored (Koehn et al., 2003).
In this work, inspired by state-of-the-art Word
Sense Disambiguation (WSD) techniques, we sug-
gest using Discriminative Phrase Translation (DPT)
models which take into account a wider feature
context. Following the approach by Vickrey et al.
(2005), we deal with the ‘phrase translation’ prob-
lem as a classification problem. We use Support
Vector Machines (SVMs) to predict phrase transla-
tions in the context of the whole source sentence.
We extend the work by Vickrey et al. (2005) in two
main aspects. First, we move from ‘word transla-
tion’ to ‘phrase translation’. Second, we move from
the ‘blank-�lling’ task to the ‘full translation’ task.
Our approach is fully described in Section 2. We
apply it to the Spanish-to-English translation of Eu-
ropean Parliament Proceedings. In Section 3, prior
to considering the ‘full translation’ task, we ana-
lyze the impact of using DPT models for the iso-
lated ‘phrase translation’ task. In spite of working
on a very specific domain, a large room for improve-
ment, coherent with WSD performance, and results
by Vickrey et al. (2005), is predicted. Then, in Sec-
tion 4, we tackle the full translation task. DPT mod-
els are integrated in a ‘soft’ manner, by making them
available to the decoder so they can fully interact
with other models. Results using a reduced set of
highly frequent source phrases show a significant
improvement, according to several automatic eval-
uation metrics. Interestingly, the BLEU metric (Pap-
ineni et al., 2001) is not able to reflect this improve-
ment. Through a rigorous process of manual eval-
uation we have verified the gain. We have also ob-
served that it is mainly related to adequacy. These
results confirm that better phrase translation proba-
bilities may be helpful for the full translation task.
However, the fact that no gain in fluency is reported
indicates that the integration of these probabilities
into the statistical framework requires further study.
</bodyText>
<sectionHeader confidence="0.998126" genericHeader="method">
2 Discriminative Phrase Translation
</sectionHeader>
<bodyText confidence="0.996121333333333">
In this section we describe the phrase-based SMT
baseline system and how DPT models are built and
integrated into this system in a ‘soft’ manner.
</bodyText>
<page confidence="0.983554">
159
</page>
<note confidence="0.9421075">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 159–166,
Prague, June 2007. c�2007 Association for Computational Linguistics
</note>
<subsectionHeader confidence="0.947278">
2.1 Baseline System
</subsectionHeader>
<bodyText confidence="0.999987970588235">
The baseline system is a phrase-based SMT sys-
tem (Koehn et al., 2003), built almost entirely us-
ing freely available components. We use the SRI
Language Modeling Toolkit (Stolcke, 2002) for lan-
guage modeling. We build trigram language models
applying linear interpolation and Kneser-Ney dis-
counting for smoothing. Translation models are
built on top of word-aligned parallel corpora linguis-
tically annotated at the level of shallow syntax (i.e.,
lemma, part-of-speech, and base phrase chunks)
as described by Gim´enez and M`arquez (2005).
Text is automatically annotated, using the SVM-
Tool (Gim´enez and M`arquez, 2004), Freeling (Car-
reras et al., 2004), and Phreco (Carreras et al., 2005)
packages. We used the GIZA++ SMT Toolkit1 (Och
and Ney, 2003) to generate word alignments. We
apply the phrase-extract algorithm, as described by
Och (2002), on the Viterbi alignments output by
GIZA++ following the ‘global phrase extraction’
strategy described by Gim´enez and M`arquez (2005)
(i.e., a single phrase translation table is built on top
of the union of alignments corresponding to dif-
ferent linguistic data views). We work with the
union of source-to-target and target-to-source align-
ments, with no heuristic refinement. Phrases up to
length five are considered. Also, phrase pairs ap-
pearing only once are discarded, and phrase pairs
in which the source/target phrase is more than three
times longer than the target/source phrase are ig-
nored. Phrase pairs are scored on the basis of un-
smoothed relative frequency (i.e., MLE). Regard-
ing the argmax search, we used the Pharaoh beam
search decoder (Koehn, 2004), which naturally fits
with the previous tools.
</bodyText>
<subsectionHeader confidence="0.934863">
2.2 DPT for SMT
</subsectionHeader>
<bodyText confidence="0.999781142857143">
Instead of relying on MLE estimation to score the
phrase pairs (fi, ej) in the translation table, we
suggest considering the translation of every source
phrase fi as a multi-class classification problem,
where every possible translation of fi is a class.
We use local linear SVMs 2. Since SVMs are bi-
nary classifiers, the problem must be binarized. We
</bodyText>
<footnote confidence="0.999504333333333">
1http://www.fjoch.com/GIZA++.html
2We use the SVMlight package, which is freely available at
http://svmlight.joachims.org (Joachims, 1999).
</footnote>
<bodyText confidence="0.999753">
have applied a simple one-vs-all binarization, i.e., a
SVM is trained for every possible translation candi-
date ej. Training examples are extracted from the
same training data as in the case of MLE models,
i.e., an aligned parallel corpus, obtained as described
in Section 2.1. We use each sentence pair in which
the source phrase fi occurs to generate a positive ex-
ample for the classifier corresponding to the actual
translation of fi in that sentence, according to the
automatic alignment. This will be as well a negative
example for the classifiers corresponding to the rest
of possible translations of fi.
</bodyText>
<subsectionHeader confidence="0.784082">
2.2.1 Feature Set
</subsectionHeader>
<bodyText confidence="0.9997009">
We consider different kinds of information, al-
ways from the source sentence, based on standard
WSD methods (Yarowsky et al., 2001). As to the
local context, inside the source phrase to disam-
biguate, and 5 tokens to the left and to the right,
we use n-grams (n E {1, 2,3}) of: words, parts-
of-speech, lemmas and base phrase chunking IOB
labels. As to the global context, we collect topical
information by considering the source sentence as a
bag of lemmas.
</bodyText>
<subsubsectionHeader confidence="0.521591">
2.2.2 Decoding. A Trick.
</subsubsectionHeader>
<bodyText confidence="0.99959275">
At translation time, we consider every instance of
fi as a separate case. In each case, for all possi-
ble translations of fi, we collect the SVM score, ac-
cording to the SVM classification rule. We are in
fact modeling P(ej|fi). However, these scores are
not probabilities. We transform them into proba-
bilities by applying the softmax function described
by Bishop (1995). We do not constrain the decoder
to use the translation ej with highest probability. In-
stead, we make all predictions available and let the
decoder choose. We have avoided implementing a
new decoder by pre-computing all the SVM pre-
dictions for all possible translations for all source
phrases appearing in the test set. We input this in-
formation onto the decoder by replicating the entries
in the translation table. In other words, each distinct
occurrence of every single source phrase has a dis-
tinct list of phrase translation candidates with their
corresponding scores. Accordingly, the source sen-
tence is transformed into a sequence of identifiers,
</bodyText>
<page confidence="0.99381">
160
</page>
<bodyText confidence="0.999914615384615">
in our case a sequence of (w, i) pairs3, which allow
us to uniquely identify every distinct instance of ev-
ery word in the test set during decoding, and to re-
trieve DPT predictions in the translation table. For
that purpose, source phrases in the translation table
must comply with the same format.
This imaginative trick4 saved us in the short run
a gigantic amount of work. However, it imposes a
severe limitation on the kind of features which the
DPT system may use. In particular, features from
the target sentence under construction and from
the correspondence between source and target (i.e.,
alignments) can not be used.
</bodyText>
<sectionHeader confidence="0.991221" genericHeader="method">
3 Phrase Translation
</sectionHeader>
<bodyText confidence="0.999976730769231">
Analogously to the word translation’ definition by
Vickrey et al. (2005), rather than predicting the sense
of a word according to a given sense inventory, in
phrase translation’, the goal is to predict the correct
translation of a phrase, for a given target language,
in the context of a sentence. This task is simpler than
the ‘full translation’ task, but provides an insight to
the gain prospectives.
We used the data from the Openlab 2006 Initia-
tive5 promoted by the TC-STAR Consortium6. This
test suite is entirely based on European Parliament
Proceedings. We have focused on the Spanish-to-
English task. The training set consists of 1,281,427
parallel sentences. Performing phrase extraction
over the training data, as described in Section 2.1,
we obtained translation candidates for 1,729,191
source phrases. We built classifiers for all the source
phrases with more than one possible translation and
more than 10 occurrences. 241,234 source phrases
fulfilled this requirement. For each source phrase,
we used 80% of the instances for training, 10% for
development, and 10% for test.
Table 1 shows “phrase translation” results over
the test set. We compare the performance, in terms
of accuracy, of DPT models and the “most fre-
quent translation” baseline (‘MFT’). The MFT base-
</bodyText>
<footnote confidence="0.986939142857143">
3w is a word and i corresponds to the number of instances
of word w seen in the test set before the current instance.
4We have checked that results following this type of decod-
ing when translation tables are estimated on the basis of MLE
are identical to regular decoding results.
5http://tc-star.itc.it/openlab2006/
6http://www.tc-star.org/
</footnote>
<table confidence="0.997301">
phrase set model macro micro
all MFT 0.66 0.70
DPT 0.68 0.76
frequent MFT 0.76 0.75
DPT 0.86 0.86
</table>
<tableCaption confidence="0.999871">
Table 1: “Phrase Translation” Accuracy (test set).
</tableCaption>
<bodyText confidence="0.99873025">
line is equivalent to selecting the translation candi-
date with highest probability according to MLE. The
‘macro’ column shows macro-averaged results over
all phrases, i.e., the accuracy for each phrase counts
equally towards the average. The ‘micro’ column
shows micro-averaged accuracy, where each test ex-
ample counts equally. The ‘all’ set includes results
for the 241,234 phrases, whereas the ‘frequent’ set
includes results for a selection of 41 very frequent
phrases ocurring more than 50,000 times.
A priori, DPT models seem to offer a significant
room for potential improvement. Although phrase
translation differs from WSD in a number of as-
pects, the increase with respect to the MFT baseline
is comparable. Results are also coherent with those
attained by Vickrey et al. (2005).
</bodyText>
<figure confidence="0.994764285714286">
1
0.5
0
-0.5
-1
0 50000 100000 150000 200000 250000 300000
#examples
</figure>
<figureCaption confidence="0.721736166666667">
Figure 1: Analysis of “Phrase Translation” Results
on the development set (Spanish-to-English).
Figure 1 shows the relationship between the accu-
racy7 gain and the number of training examples. In
general, with a sufficient number of examples (over
10,000), DPT outperforms the MFT baseline.
</figureCaption>
<footnote confidence="0.724737">
7We focus on micro-averaged accuracy.
</footnote>
<equation confidence="0.907674">
accuracy(DPT) - accuracy(MLE)
</equation>
<page confidence="0.993297">
161
</page>
<sectionHeader confidence="0.995878" genericHeader="method">
4 Full Translation
</sectionHeader>
<bodyText confidence="0.99999125">
In the “phrase translation” task the predicted phrase
does not interact with the rest of the target sentence.
In this section we analyze the impact of DPT models
when the goal is to translate the whole sentence.
For evaluation purposes we count on a set of 1,008
sentences. Three human references per sentence are
available. We randomly split this set in two halves,
and use them for development and test, respectively.
</bodyText>
<subsectionHeader confidence="0.950058">
4.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.997961787878788">
Evaluating the effects of using DPT predictions, di-
rected towards a better word selection, in the full
translation task presents two serious difficulties.
In first place, the actual room for improvement
caused by a better translation modeling is smaller
than estimated in Section 3. This is mainly due to
the SMT architecture itself which relies on a search
over a probability space in which several models co-
operate. For instance, in many cases errors caused
by a poor translation modeling may be corrected by
the language model. In a recent study, Vilar et al.
(2006) found that only around 25% of the errors are
related to word selection. In half of these cases er-
rors are caused by a wrong word sense disambigua-
tion, and in the other half the word sense is correct
but the lexical choice is wrong.
In second place, most conventional automatic
evaluation metrics have not been designed for this
purpose. For instance, metrics such as BLEU (Pa-
pineni et al., 2001) tend to favour longer n-gram
matchings, and are, thus, biased towards word or-
dering. We might find better suited metrics, such
as METEOR (Banerjee and Lavie, 2005), which is
oriented towards word selection8. However, a new
problem arises. Because different metrics are biased
towards different aspects of quality, scores conferred
by different metrics are often controversial.
In order to cope with evaluation difficulties we
have applied several complementary actions:
1. Based on the results from Section 3, we focus
on a reduced set of 41 very promising phrases
trained on more than 50,000 examples. This
set covers 25.8% of the words in the test set,
</bodyText>
<footnote confidence="0.875949333333333">
8METEOR works at the unigram level, may consider word
stemming and, for the case of English is also able to perform a
lookup for synonymy in WordNet (Fellbaum, 1998).
</footnote>
<bodyText confidence="0.996619357142857">
and exhibits a potential absolute accuracy gain
around 11% (See Table 1).
2. With the purpose of evaluating the changes re-
lated only to this small set of very promis-
ing phrases, we introduce a new measure, Apt,
which computes “phrase translation” accuracy
for a given list of source phrases. For every
test case, Apt counts the proportion of phrases
from the list appearing in the source sentence
which have a valid9 translation both in the tar-
get sentence and in any of the reference trans-
lations. In fact, because in general source-to-
target alignments are not known, Apt calculates
an approximate10 solution.
</bodyText>
<listItem confidence="0.952740777777778">
3. We evaluate overall MT quality on the basis
of ‘Human Likeness’. In particular, we use
the QUEEN11 meta-measure from the QARLA
Framework (Amig´o et al., 2005). QUEEN op-
erates under the assumption that a good trans-
lation must be similar to all human references
according to all metrics. Given a set of auto-
matic translations A, a set of similarity metrics
X, and a set of human references R, QUEEN is
</listItem>
<bodyText confidence="0.671519">
defined as the probability, over R × R × R, that
for every metric in X the automatic translation
a is more similar to a reference r than two other
references r&apos; and r&amp;quot; to each other. Formally:
</bodyText>
<equation confidence="0.952547">
QUEENX,R(a) = Prob(∀x ∈ X : x(a, r) ≥ x(r&apos;, r&apos;&apos;))
</equation>
<bodyText confidence="0.999580727272727">
QUEEN captures the features that are common
to all human references, rewarding those auto-
matic translations which share them, and pe-
nalizing those which do not. Thus, QUEEN pro-
vides a robust means of combining several met-
rics into a single measure of quality. Following
the methodology described by Gim´enez and
Amig´o (2006), we compute the QUEEN mea-
sure over the metric combination with high-
est KING, i.e., discriminative power. We have
considered all the lexical metrics12 provided by
</bodyText>
<footnote confidence="0.995120222222222">
9Valid translations are provided by the translation table.
10Current Apt implementation searches phrases from left to
right in decreasing length order.
11QUEEN is available inside the IQMT package for MT
Evaluation based on ‘Human Likeness’ (Gim´enez and Amig´o,
2006). http://www.lsi.upc.edu/-nlp/IQMT
12Consult the IQMT Technical Manual v1.3 for a detailed de-
scription of the metric set. http://www.lsi.upc.edu/
-nlp/IQMT/IQMT.v1.3.pdf
</footnote>
<page confidence="0.987968">
162
</page>
<table confidence="0.89803825">
QUEEN Apt BLEU METEOR ROUGE
P(e) + PMLE(f|e) 0.43 0.86 0.59 0.77 0.42
P(e) + PMLE(e|f) 0.45 0.87 0.62 0.77 0.43
P(e) + PDPT(e|f) 0.47 0.89 0.62 0.78 0.44
</table>
<tableCaption confidence="0.989476">
Table 2: Automatic evaluation of the ‘full translation’ results on the test set.
</tableCaption>
<bodyText confidence="0.84169625">
IQMT. The optimal set is:
{ METEORwnsyn, ROUGEw 1.2 }
which includes variants of METEOR, and
ROUGE (Lin and Och, 2004).
</bodyText>
<subsectionHeader confidence="0.999363">
4.2 Adjustment of Parameters
</subsectionHeader>
<bodyText confidence="0.802325">
Models are combined in a log-linear fashion:
</bodyText>
<equation confidence="0.999615">
lo9P(e|f) a Almlo9P(e) + Aglo9PMLE(f|e)
+ Adlo9PMLE(e|f) + ADPTlo9PDPT (e|f)
</equation>
<bodyText confidence="0.999580527777778">
P(e) is the language model probability.
PMLE(f|e) corresponds to the MLE-based
generative translation model, whereas PMLE(e|f)
corresponds to the analogous discriminative model.
PDPT (e|f) corresponds to the DPT model which
uses SVM-based predictions in a wider feature
context. In order to perform fair comparisons,
model weights must be adjusted.
Because we have focused on a reduced set of fre-
quent phrases, in order to translate the whole test set
we must provide alternative translation probabilities
for all the source phrases in the vocabulary which
do not have a DPT prediction. We have used MLE
predictions to complete the model. However, inter-
action between DPT and MLE models is problem-
atic. Problems arise when, for a given source phrase,
fi, DPT predictions must compete with MLE pre-
dictions for larger phrases fj overlapping with or
containing fi (See Section 4.3). We have alleviated
these problems by splitting DPT tables in 3 subta-
bles: (1) phrases with DPT prediction, (2) phrases
with DPT prediction only for subphrases of it, and
(3) phrases with no DPT prediction for any sub-
phrase; and separately adjusting their weights.
Counting on a reliable automatic measure of qual-
ity is a crucial issue for system development. Opti-
mal configurations may vary very significantly de-
pending on the metric governing the optimization
process. We optimize the system parameters over
the QUEEN measure, which has proved to lead to
more robust system configurations than BLEU (Lam-
bert et al., 2006). We exhaustively try all possible
parameter configurations, at a resolution of 0.1, over
the development set and select the best one. In order
to keep the optimization process feasible, in terms of
time, the search space is pruned13 during decoding.
</bodyText>
<subsectionHeader confidence="0.686625">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.999970392857143">
We compare the systems using the generative and
discriminative MLE-based translation models to the
discriminative translation model which uses DPT
predictions for the set of 41 very ‘frequent’ source
phrases. Table 2 shows automatic evaluation re-
sults on the test set, according to several metrics.
Phrase translation accuracy (over the ‘frequent’ set
of phrases) and MT quality are evaluated by means
of the Apt and QUEEN measures, respectively. For
the sake of informativeness, BLEU, METEORwnsyn
and ROUGEw 1.2 scores are provided as well.
Interestingly, discriminative models outperform
the (noisy-channel) default generative model. Im-
provement in Apt measure also reveals that DPT pre-
dictions provide a better translation for the set of
‘frequent’ phrases than the MLE models. This im-
provement remains when measuring overall transla-
tion quality via QUEEN. If we take into account that
DPT predictions are available for only 25% of the
words in the test set, we can say that the gain re-
ported by the QUEEN and Apt measures is consistent
with the accuracy prospectives predicted in Table 1.
METEORwnsyn and ROUGEw 1.2 reflect a slight im-
provement as well. However, according to BLEU
there is no difference between both systems. We
suspect that BLEU is unable to accurately reflect the
possible gains attained by a better ‘phrase selection’
over a small set of phrases because of its tendency
</bodyText>
<footnote confidence="0.9903904">
13For each phrase only the 30 top-scoring translations are
used. At all times, only the 100 top-scoring solutions are kept.
We also disabled distortion and word penalty models. There-
fore, translations are monotonic, and source and target tend to
have the same number of words (that is not mandatory).
</footnote>
<page confidence="0.999023">
163
</page>
<bodyText confidence="0.9997755">
to reward long n-gram matchings. In order to clar-
ify this scenario a rigorous process of manual evalu-
ation has been conducted. We have selected a subset
of sentences based on the following criteria:
</bodyText>
<listItem confidence="0.999789666666667">
• sentence length between 10 and 30 words.
• at least 5 words have a DPT prediction.
• DPT and MLE outputs differ.
</listItem>
<bodyText confidence="0.999962739130435">
A total of 114 sentences fulfill these require-
ments. In each translation case, assessors must judge
whether the output by the discriminative ‘MLE’ sys-
tem is better, equal to or worse than the output by
the ‘DPT’ system, with respect to adequacy, fluency,
and overall quality. In order to avoid any bias in the
evaluation, we have randomized the respective posi-
tion in the display of the sentences corresponding to
each system. Four judges participated in the evalua-
tion. Each judge evaluated only half of the cases.
Each case was evaluated by two different judges.
Therefore, we count on 228 human assessments.
Table 3 shows the results of the manual system
comparison. Statistical significance has been deter-
mined using the sign-test (Siegel, 1956). According
to human assessors, the ‘DPT’ system outperforms
the ‘MLE’ system very significantly with respect to
adequacy, whereas for fluency there is a slight ad-
vantage in favor of the ‘MLE’ system. Overall, there
is a slight but significant advantage in favor of the
‘DPT’ system. Manual evaluation confirms our sus-
picion that the BLEU metric is less sensitive than
QUEEN to improvements related to adequacy.
</bodyText>
<subsectionHeader confidence="0.970177">
Error Analysis
</subsectionHeader>
<bodyText confidence="0.999920733333333">
Guided by the QUEEN measure, we carefully inspect
particular cases. We start, in Table 4, by show-
ing a positive case. The three phrases highlighted
in the source sentence (‘tiene’, ‘senora’ and ‘una
cuestion’) find a better translation with the help of
the DPT models: ‘tiene’ translates into ‘has’ instead
of ‘i give’, ‘senora’ into ‘mrs’ instead of ‘lady’, and
‘una cuestion’ into ‘a point’ instead of ‘a ... motion’.
In contrast, Table 5 shows a negative case. The
translation of the Spanish word ‘senora’ as ‘mrs’ is
acceptable. However, it influences very negatively
the translation of the following word ‘diputada’,
whereas the ‘MLE’ system translates the phrase
‘senora diputada’, which does not have a DPT pre-
diction, as a whole. Similarly, the translation of
</bodyText>
<table confidence="0.99853775">
Adequacy Fluency Overall
MLE &gt; DPT 39 84 83
MLE = DPT 100 76 46
MLE &lt; DPT 89 68 99
</table>
<tableCaption confidence="0.933411666666667">
Table 3: Manual evaluation of the ‘full translation’
results on the test set. Counts on the number of
translation cases for which the ‘MLE’ system is bet-
</tableCaption>
<bodyText confidence="0.9588922">
ter than (&gt;), equal to (=), or worse than (&lt;) the
‘DPT’ system, with respect to adequacy, fluency,
and overall MT quality, are presented.
‘cuestidn’ as ‘matter’, although acceptable, is break-
ing the phrase ‘cuestion de orden’ of high cohe-
sion, which is commonly translated as ‘point of or-
der’. The cause underlying these problems is that
DPT predictions are available only for a subset of
phrases. Thus, during decoding, for these cases our
DPT models may be in disadvantage.
</bodyText>
<sectionHeader confidence="0.999923" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.993523461538461">
Recently, there is a growing interest in the appli-
cation of WSD technology to MT. For instance,
Carpuat and Wu (2005b) suggested integrating
WSD predictions into a SMT system in a ‘hard’
manner, either for decoding, by constraining the set
of acceptable translation candidates for each given
source word, or for post-processing the SMT sys-
tem output, by directly replacing the translation of
each selected word with the WSD system predic-
tion. They did not manage to improve MT quality.
They encountered several problems inherent to the
SMT architecture. In particular, they described what
they called the “language model effect” in SMT:
“The lexical choices are made in a way that heav-
ily prefers phrasal cohesion in the output target sen-
tence, as scored by the language model.”. This prob-
lem is a direct consequence of the ‘hard’ interaction
between their WSD and SMT systems. WSD pre-
dictions cannot adapt to the surrounding target con-
text. In a later work, Carpuat and Wu (2005a) ana-
lyzed the converse question, i.e. they measured the
WSD performance of SMT models. They showed
that dedicated WSD models significantly outper-
form current state-of-the-art SMT models. Conse-
quently, SMT should benefit from WSD predictions.
Simultaneously, Vickrey et al. (2005) studied the
</bodyText>
<page confidence="0.995851">
164
</page>
<bodyText confidence="0.818634833333333">
Source tiene la palabra la se˜nora mussolini para una cuesti´on de orden .
Ref 1 mrs mussolini has the floor for a point of order.
Ref 2 you have the floor, missus mussolini , for a question of order.
Ref 3 ms mussolini has now the floor for a point of order.
P(e) + PMLE(e|f) i give the floor to the lady mussolini for a procedural motion.
P(e) + PDPT(e|f) has the floor the mrs mussolini on a point of order.
</bodyText>
<tableCaption confidence="0.982561">
Table 4: Case of Analysis of sentence #422. DPT models help.
</tableCaption>
<bodyText confidence="0.92910875">
Source se˜nora diputada , ´esta no es una cuesti´on de orden .
Ref 1 mrs mussolini , that is not a point of order.
Ref 2 honourable member, this is not a question of order.
Ref 3 my honourable friend, this is not a point of order.
</bodyText>
<equation confidence="0.7374255">
P(e) + PMLE(e|f) honourable member, this is not a point of order.
P(e) + PDPT(e|f) mrs karamanou , this is not a matter of order.
</equation>
<tableCaption confidence="0.972066">
Table 5: Case of Analysis of sentence #434. DPT models fail.
</tableCaption>
<bodyText confidence="0.999953571428571">
application of discriminative models based on WSD
technology to the “blank-filling” task, a simplified
version of the translation task, in which the target
context surrounding the word translation is avail-
able. They did not encounter the “language model
effect” because they approached the task in a ‘soft’
way, i.e., allowing their WSD models to interact
with other models during decoding. Similarly, our
DPT models are, as described in Section 2.2, softly
integrated in the decoding step, and thus do not suf-
fer from the detrimental “language model effect” ei-
ther, in the context of the “full translation” task. Be-
sides, DPT models enforce phrasal cohesion by con-
sidering disambiguation at the level of phrases.
</bodyText>
<sectionHeader confidence="0.997443" genericHeader="conclusions">
6 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.999977205128205">
Despite the fact that measuring improvements in
word selection is a very delicate issue, we have
showed that dedicated discriminative translation
models considering a wider feature context provide
a useful mechanism in order to improve the qual-
ity of current phrase-based SMT systems, specially
with regard to adequacy. However, the fact that no
gain in fluency is reported indicates that the integra-
tion of these probabilities into the statistical frame-
work requires further study.
Moreover, there are several open issues. First, for
practical reasons, we have limited to a reduced set of
‘frequent’ phrases, and we have disabled reordering
and word penalty models. We are currently studying
the impact of a larger set of phrases, covering over
99% of the words in the test set. Experiments with
enabled reordering and word penalty models should
be conducted as well. Second, automatic evalua-
tion of the results revealed a low agreement between
BLEU and other metrics. For system comparison, we
solved this through a process of manual evaluation.
However, this is impractical for the adjustment of
parameters, where hundreds of different configura-
tions are tried. In this work we have relied on auto-
matic evaluation based on ‘Human Likeness’ which
allows for metric combinations and provides a sta-
ble and robust criterion for the metric set selection.
Other alternatives could be tried. The crucial issue,
in our opinion, is that the metric guiding the opti-
mization is able to capture the changes.
Finally, we argue that, if DPT models considered
features from the target side, and from the corre-
spondence between source and target, results could
further improve. However, at the short term, the in-
corporation of these type of features will force us to
either build a new decoder or extend an existing one,
or to move to a new MT architecture, for instance,
in the fashion of the architectures suggested by Till-
mann and Zhang (2006) or Liang et al. (2006).
</bodyText>
<sectionHeader confidence="0.997794" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<footnote confidence="0.780025333333333">
This research has been funded by the Span-
ish Ministry of Education and Science, projects
OpenMT (TIN2006-15307-C03-02) and TRAN-
</footnote>
<page confidence="0.997183">
165
</page>
<bodyText confidence="0.995222666666667">
GRAM (TIN2004-07925-C03-02). We are recog-
nized as a Quality Research Group (2005 SGR-
00130) by DURSI, the Research Department of the
Catalan Government. Authors are thankful to the
TC-STAR Consortium for providing such very valu-
able data sets.
</bodyText>
<sectionHeader confidence="0.998919" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999897494382022">
Enrique Amig´o, Julio Gonzalo, Anselmo Pe˜nas, and Fe-
lisa Verdejo. 2005. QARLA: a Framework for the
Evaluation of Automatic Sumarization. In Proceed-
ings of the 43th Annual Meeting of the Association for
Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Pro-
ceedings of ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for MT and/or Summarization.
Christopher M. Bishop. 1995. 6.4: Modeling conditional
distributions. In Neural Networks for Pattern Recog-
nition, page 215. Oxford University Press.
Marine Carpuat and Dekai Wu. 2005a. Evaluating the
Word Sense Disambiguation Performance of Statisti-
cal Machine Translation. In Proceedings ofIJCNLP.
Marine Carpuat and Dekai Wu. 2005b. Word Sense Dis-
ambiguation vs. Statistical Machine Translation. In
Proceedings ofACL.
Xavier Carreras, Isaac Chao, Lluis Padr´o, and Muntsa
Padr´o. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proceedings of the 4th LREC.
Xavier Carreras, Lluis M´arquez, and Jorge Castro. 2005.
Filtering-ranking perceptron learning for partial pars-
ing. Machine Learning, 59:1–31.
C. Fellbaum, editor. 1998. WordNet. An Electronic Lexi-
cal Database. The MIT Press.
Jes´us Gim´enez and Enrique Amig´o. 2006. IQMT: A
Framework for Automatic Machine Translation Eval-
uation. In Proceedings of the 5th LREC.
Jes´us Gim´enez and Lluis M`arquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In Proceedings of 4th LREC.
Jes´us Gim´enez and Lluis M`arquez. 2005. Combining
Linguistic Data Views for Phrase-based SMT. In Pro-
ceedings of the Workshop on Building and Using Par-
allel Texts, ACL.
T. Joachims. 1999. Making large-Scale SVM Learning
Practical. In B. Sch¨olkopf, C. Burges, and A. Smola,
editors, Advances in Kernel Methods - Support Vector
Learning. The MIT Press.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings ofHLT/NAACL.
Philipp Koehn. 2004. Pharaoh: a Beam Search De-
coder for Phrase-Based Statistical Machine Transla-
tion Models. In Proceedings ofAMTA.
Patrik Lambert, Jes´us Gim´enez, Marta R. Costa-juss´a,
Enrique Amig´o, Rafael E. Banchs, Lluis M´arquez, and
J.A. R. Fonollosa. 2006. Machine Translation Sys-
tem Development based on Human Likeness. In Pro-
ceedings ofIEEE/ACL 2006 Workshop on Spoken Lan-
guage Technology.
Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, , and
Ben Taskar. 2006. An End-to-End Discriminative
Approach to Machine Translation. In Proceedings of
COLING-ACL06.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statics. In Proceedings ofACL.
Franz Josef Och and Hermann Ney. 2003. A Systematic
Comparison of Various Statistical Alignment Models.
Computational Linguistics, 29(1):19–51.
Franz Josef Och. 2002. Statistical Machine Transla-
tion: From Single-Word Models to Alignment Tem-
plates. Ph.D. thesis, RWTH Aachen, Germany.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a method for automatic evalua-
tion of machine translation, rc22176. Technical report,
IBM T.J. Watson Research Center.
Sidney Siegel. 1956. Nonparametric Statistics for the
Behavioral Sciences. McGraw-Hill.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings ofICSLP.
Christoph Tillmann and Tong Zhang. 2006. A Discrim-
inative Global Training Algorithm for Statistical MT.
In Proceedings of COLING-ACL06.
D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005.
Word-Sense Disambiguation for Machine Translation.
In Proceedings ofHLT/EMNLP.
David Vilar, Jia Xu, Luis Fernando D’Haro, and Her-
mann Ney. 2006. Error Analysis of Machine Trans-
lation Output. In Proceedings of the 5th LREC.
David Yarowsky, Silviu Cucerzan, Radu Florian, Charles
Schafer, and Richard Wicentowski. 2001. The Johns
Hopkins Senseval2 System Descriptions. In Proceed-
ings of Senseval-2: Second International Workshop on
Evaluating Word Sense Disambiguation Systems.
</reference>
<page confidence="0.998758">
166
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.313144">
<title confidence="0.9988565">Context-aware Discriminative Phrase for Statistical Machine Translation</title>
<author confidence="0.974941">Gim´enez</author>
<affiliation confidence="0.9749805">TALP Research Center, LSI Universitat Polit`ecnica de</affiliation>
<note confidence="0.343604">Jordi Girona Salgado 1–3, E-08034,</note>
<abstract confidence="0.998880411764706">In this work we revise the application of discriminative learning to the problem of phrase selection in Statistical Machine Translation. Inspired by common techniques used in Word Sense Disambiguation, we train classifiers based on local context to predict possible phrase translations. Our work extends that of Vickrey et al. (2005) in two main aspects. First, we move from word translation to phrase translation. Second, we from the to the We report results on a set of highly frequent source phrases, obtaining a significant improvement, specially with respect to adequacy, according to a rigorous process of manual evaluation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
<author>Anselmo Pe˜nas</author>
<author>Felisa Verdejo</author>
</authors>
<title>QARLA: a Framework for the Evaluation of Automatic Sumarization.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>Amig´o, Gonzalo, Pe˜nas, Verdejo, 2005</marker>
<rawString>Enrique Amig´o, Julio Gonzalo, Anselmo Pe˜nas, and Felisa Verdejo. 2005. QARLA: a Framework for the Evaluation of Automatic Sumarization. In Proceedings of the 43th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</booktitle>
<contexts>
<context position="13077" citStr="Banerjee and Lavie, 2005" startWordPosition="2087" endWordPosition="2090">ted by the language model. In a recent study, Vilar et al. (2006) found that only around 25% of the errors are related to word selection. In half of these cases errors are caused by a wrong word sense disambiguation, and in the other half the word sense is correct but the lexical choice is wrong. In second place, most conventional automatic evaluation metrics have not been designed for this purpose. For instance, metrics such as BLEU (Papineni et al., 2001) tend to favour longer n-gram matchings, and are, thus, biased towards word ordering. We might find better suited metrics, such as METEOR (Banerjee and Lavie, 2005), which is oriented towards word selection8. However, a new problem arises. Because different metrics are biased towards different aspects of quality, scores conferred by different metrics are often controversial. In order to cope with evaluation difficulties we have applied several complementary actions: 1. Based on the results from Section 3, we focus on a reduced set of 41 very promising phrases trained on more than 50,000 examples. This set covers 25.8% of the words in the test set, 8METEOR works at the unigram level, may consider word stemming and, for the case of English is also able to </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>6.4: Modeling conditional distributions.</title>
<date>1995</date>
<booktitle>In Neural Networks for Pattern Recognition,</booktitle>
<pages>215</pages>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="7197" citStr="Bishop (1995)" startWordPosition="1133" endWordPosition="1134">right, we use n-grams (n E {1, 2,3}) of: words, partsof-speech, lemmas and base phrase chunking IOB labels. As to the global context, we collect topical information by considering the source sentence as a bag of lemmas. 2.2.2 Decoding. A Trick. At translation time, we consider every instance of fi as a separate case. In each case, for all possible translations of fi, we collect the SVM score, according to the SVM classification rule. We are in fact modeling P(ej|fi). However, these scores are not probabilities. We transform them into probabilities by applying the softmax function described by Bishop (1995). We do not constrain the decoder to use the translation ej with highest probability. Instead, we make all predictions available and let the decoder choose. We have avoided implementing a new decoder by pre-computing all the SVM predictions for all possible translations for all source phrases appearing in the test set. We input this information onto the decoder by replicating the entries in the translation table. In other words, each distinct occurrence of every single source phrase has a distinct list of phrase translation candidates with their corresponding scores. Accordingly, the source se</context>
</contexts>
<marker>Bishop, 1995</marker>
<rawString>Christopher M. Bishop. 1995. 6.4: Modeling conditional distributions. In Neural Networks for Pattern Recognition, page 215. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Evaluating the Word Sense Disambiguation Performance of Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofIJCNLP.</booktitle>
<contexts>
<context position="22994" citStr="Carpuat and Wu (2005" startWordPosition="3720" endWordPosition="3723">system is better than (&gt;), equal to (=), or worse than (&lt;) the ‘DPT’ system, with respect to adequacy, fluency, and overall MT quality, are presented. ‘cuestidn’ as ‘matter’, although acceptable, is breaking the phrase ‘cuestion de orden’ of high cohesion, which is commonly translated as ‘point of order’. The cause underlying these problems is that DPT predictions are available only for a subset of phrases. Thus, during decoding, for these cases our DPT models may be in disadvantage. 5 Related Work Recently, there is a growing interest in the application of WSD technology to MT. For instance, Carpuat and Wu (2005b) suggested integrating WSD predictions into a SMT system in a ‘hard’ manner, either for decoding, by constraining the set of acceptable translation candidates for each given source word, or for post-processing the SMT system output, by directly replacing the translation of each selected word with the WSD system prediction. They did not manage to improve MT quality. They encountered several problems inherent to the SMT architecture. In particular, they described what they called the “language model effect” in SMT: “The lexical choices are made in a way that heavily prefers phrasal cohesion in</context>
</contexts>
<marker>Carpuat, Wu, 2005</marker>
<rawString>Marine Carpuat and Dekai Wu. 2005a. Evaluating the Word Sense Disambiguation Performance of Statistical Machine Translation. In Proceedings ofIJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marine Carpuat</author>
<author>Dekai Wu</author>
</authors>
<title>Word Sense Disambiguation vs. Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="22994" citStr="Carpuat and Wu (2005" startWordPosition="3720" endWordPosition="3723">system is better than (&gt;), equal to (=), or worse than (&lt;) the ‘DPT’ system, with respect to adequacy, fluency, and overall MT quality, are presented. ‘cuestidn’ as ‘matter’, although acceptable, is breaking the phrase ‘cuestion de orden’ of high cohesion, which is commonly translated as ‘point of order’. The cause underlying these problems is that DPT predictions are available only for a subset of phrases. Thus, during decoding, for these cases our DPT models may be in disadvantage. 5 Related Work Recently, there is a growing interest in the application of WSD technology to MT. For instance, Carpuat and Wu (2005b) suggested integrating WSD predictions into a SMT system in a ‘hard’ manner, either for decoding, by constraining the set of acceptable translation candidates for each given source word, or for post-processing the SMT system output, by directly replacing the translation of each selected word with the WSD system prediction. They did not manage to improve MT quality. They encountered several problems inherent to the SMT architecture. In particular, they described what they called the “language model effect” in SMT: “The lexical choices are made in a way that heavily prefers phrasal cohesion in</context>
</contexts>
<marker>Carpuat, Wu, 2005</marker>
<rawString>Marine Carpuat and Dekai Wu. 2005b. Word Sense Disambiguation vs. Statistical Machine Translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Isaac Chao</author>
<author>Lluis Padr´o</author>
<author>Muntsa Padr´o</author>
</authors>
<title>FreeLing: An Open-Source Suite of Language Analyzers.</title>
<date>2004</date>
<booktitle>In Proceedings of the 4th LREC.</booktitle>
<marker>Carreras, Chao, Padr´o, Padr´o, 2004</marker>
<rawString>Xavier Carreras, Isaac Chao, Lluis Padr´o, and Muntsa Padr´o. 2004. FreeLing: An Open-Source Suite of Language Analyzers. In Proceedings of the 4th LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis M´arquez</author>
<author>Jorge Castro</author>
</authors>
<title>Filtering-ranking perceptron learning for partial parsing.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>59--1</pages>
<marker>Carreras, M´arquez, Castro, 2005</marker>
<rawString>Xavier Carreras, Lluis M´arquez, and Jorge Castro. 2005. Filtering-ranking perceptron learning for partial parsing. Machine Learning, 59:1–31.</rawString>
</citation>
<citation valid="true">
<title>WordNet. An Electronic Lexical Database.</title>
<date>1998</date>
<editor>C. Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>C. Fellbaum, editor. 1998. WordNet. An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Enrique Amig´o</author>
</authors>
<title>IQMT: A Framework for Automatic Machine Translation Evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th LREC.</booktitle>
<marker>Gim´enez, Amig´o, 2006</marker>
<rawString>Jes´us Gim´enez and Enrique Amig´o. 2006. IQMT: A Framework for Automatic Machine Translation Evaluation. In Proceedings of the 5th LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>SVMTool: A general POS tagger generator based on Support Vector Machines.</title>
<date>2004</date>
<booktitle>In Proceedings of 4th LREC.</booktitle>
<marker>Gim´enez, M`arquez, 2004</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2004. SVMTool: A general POS tagger generator based on Support Vector Machines. In Proceedings of 4th LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>Combining Linguistic Data Views for Phrase-based SMT.</title>
<date>2005</date>
<booktitle>In Proceedings of the Workshop on Building and Using Parallel Texts, ACL.</booktitle>
<marker>Gim´enez, M`arquez, 2005</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2005. Combining Linguistic Data Views for Phrase-based SMT. In Proceedings of the Workshop on Building and Using Parallel Texts, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making large-Scale SVM Learning Practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In B. Sch¨olkopf, C. Burges, and A. Smola, editors,</editor>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="5719" citStr="Joachims, 1999" startWordPosition="883" endWordPosition="884">search, we used the Pharaoh beam search decoder (Koehn, 2004), which naturally fits with the previous tools. 2.2 DPT for SMT Instead of relying on MLE estimation to score the phrase pairs (fi, ej) in the translation table, we suggest considering the translation of every source phrase fi as a multi-class classification problem, where every possible translation of fi is a class. We use local linear SVMs 2. Since SVMs are binary classifiers, the problem must be binarized. We 1http://www.fjoch.com/GIZA++.html 2We use the SVMlight package, which is freely available at http://svmlight.joachims.org (Joachims, 1999). have applied a simple one-vs-all binarization, i.e., a SVM is trained for every possible translation candidate ej. Training examples are extracted from the same training data as in the case of MLE models, i.e., an aligned parallel corpus, obtained as described in Section 2.1. We use each sentence pair in which the source phrase fi occurs to generate a positive example for the classifier corresponding to the actual translation of fi in that sentence, according to the automatic alignment. This will be as well a negative example for the classifiers corresponding to the rest of possible translat</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>T. Joachims. 1999. Making large-Scale SVM Learning Practical. In B. Sch¨olkopf, C. Burges, and A. Smola, editors, Advances in Kernel Methods - Support Vector Learning. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT/NAACL.</booktitle>
<contexts>
<context position="1265" citStr="Koehn et al., 2003" startWordPosition="179" endWordPosition="182">from word translation to phrase translation. Second, we move from the ‘blank-�lling’ task to the ‘full translation’ task. We report results on a set of highly frequent source phrases, obtaining a significant improvement, specially with respect to adequacy, according to a rigorous process of manual evaluation. 1 Introduction Translations tables in Phrase-based Statistical Machine Translation (SMT) are often built on the basis of Maximum-likelihood Estimation (MLE), being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored (Koehn et al., 2003). In this work, inspired by state-of-the-art Word Sense Disambiguation (WSD) techniques, we suggest using Discriminative Phrase Translation (DPT) models which take into account a wider feature context. Following the approach by Vickrey et al. (2005), we deal with the ‘phrase translation’ problem as a classification problem. We use Support Vector Machines (SVMs) to predict phrase translations in the context of the whole source sentence. We extend the work by Vickrey et al. (2005) in two main aspects. First, we move from ‘word translation’ to ‘phrase translation’. Second, we move from the ‘blank</context>
<context position="3624" citStr="Koehn et al., 2003" startWordPosition="559" endWordPosition="562">the full translation task. However, the fact that no gain in fluency is reported indicates that the integration of these probabilities into the statistical framework requires further study. 2 Discriminative Phrase Translation In this section we describe the phrase-based SMT baseline system and how DPT models are built and integrated into this system in a ‘soft’ manner. 159 Proceedings of the Second Workshop on Statistical Machine Translation, pages 159–166, Prague, June 2007. c�2007 Association for Computational Linguistics 2.1 Baseline System The baseline system is a phrase-based SMT system (Koehn et al., 2003), built almost entirely using freely available components. We use the SRI Language Modeling Toolkit (Stolcke, 2002) for language modeling. We build trigram language models applying linear interpolation and Kneser-Ney discounting for smoothing. Translation models are built on top of word-aligned parallel corpora linguistically annotated at the level of shallow syntax (i.e., lemma, part-of-speech, and base phrase chunks) as described by Gim´enez and M`arquez (2005). Text is automatically annotated, using the SVMTool (Gim´enez and M`arquez, 2004), Freeling (Carreras et al., 2004), and Phreco (Car</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings ofHLT/NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models.</title>
<date>2004</date>
<booktitle>In Proceedings ofAMTA.</booktitle>
<contexts>
<context position="5165" citStr="Koehn, 2004" startWordPosition="799" endWordPosition="800">phrase translation table is built on top of the union of alignments corresponding to different linguistic data views). We work with the union of source-to-target and target-to-source alignments, with no heuristic refinement. Phrases up to length five are considered. Also, phrase pairs appearing only once are discarded, and phrase pairs in which the source/target phrase is more than three times longer than the target/source phrase are ignored. Phrase pairs are scored on the basis of unsmoothed relative frequency (i.e., MLE). Regarding the argmax search, we used the Pharaoh beam search decoder (Koehn, 2004), which naturally fits with the previous tools. 2.2 DPT for SMT Instead of relying on MLE estimation to score the phrase pairs (fi, ej) in the translation table, we suggest considering the translation of every source phrase fi as a multi-class classification problem, where every possible translation of fi is a class. We use local linear SVMs 2. Since SVMs are binary classifiers, the problem must be binarized. We 1http://www.fjoch.com/GIZA++.html 2We use the SVMlight package, which is freely available at http://svmlight.joachims.org (Joachims, 1999). have applied a simple one-vs-all binarizatio</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a Beam Search Decoder for Phrase-Based Statistical Machine Translation Models. In Proceedings ofAMTA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik Lambert</author>
<author>Jes´us Gim´enez</author>
<author>Marta R Costa-juss´a</author>
<author>Enrique Amig´o</author>
<author>Rafael E Banchs</author>
<author>Lluis M´arquez</author>
<author>J A R Fonollosa</author>
</authors>
<title>Machine Translation System Development based on Human Likeness.</title>
<date>2006</date>
<booktitle>In Proceedings ofIEEE/ACL 2006 Workshop on Spoken Language Technology.</booktitle>
<marker>Lambert, Gim´enez, Costa-juss´a, Amig´o, Banchs, M´arquez, Fonollosa, 2006</marker>
<rawString>Patrik Lambert, Jes´us Gim´enez, Marta R. Costa-juss´a, Enrique Amig´o, Rafael E. Banchs, Lluis M´arquez, and J.A. R. Fonollosa. 2006. Machine Translation System Development based on Human Likeness. In Proceedings ofIEEE/ACL 2006 Workshop on Spoken Language Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Alexandre Bouchard-Cˆot´e</author>
<author>Dan Klein</author>
</authors>
<title>An End-to-End Discriminative Approach to Machine Translation.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL06.</booktitle>
<marker>Liang, Bouchard-Cˆot´e, Klein, 2006</marker>
<rawString>Percy Liang, Alexandre Bouchard-Cˆot´e, Dan Klein, , and Ben Taskar. 2006. An End-to-End Discriminative Approach to Machine Translation. In Proceedings of COLING-ACL06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics.</title>
<date>2004</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="16276" citStr="Lin and Och, 2004" startWordPosition="2618" endWordPosition="2621">de the IQMT package for MT Evaluation based on ‘Human Likeness’ (Gim´enez and Amig´o, 2006). http://www.lsi.upc.edu/-nlp/IQMT 12Consult the IQMT Technical Manual v1.3 for a detailed description of the metric set. http://www.lsi.upc.edu/ -nlp/IQMT/IQMT.v1.3.pdf 162 QUEEN Apt BLEU METEOR ROUGE P(e) + PMLE(f|e) 0.43 0.86 0.59 0.77 0.42 P(e) + PMLE(e|f) 0.45 0.87 0.62 0.77 0.43 P(e) + PDPT(e|f) 0.47 0.89 0.62 0.78 0.44 Table 2: Automatic evaluation of the ‘full translation’ results on the test set. IQMT. The optimal set is: { METEORwnsyn, ROUGEw 1.2 } which includes variants of METEOR, and ROUGE (Lin and Och, 2004). 4.2 Adjustment of Parameters Models are combined in a log-linear fashion: lo9P(e|f) a Almlo9P(e) + Aglo9PMLE(f|e) + Adlo9PMLE(e|f) + ADPTlo9PDPT (e|f) P(e) is the language model probability. PMLE(f|e) corresponds to the MLE-based generative translation model, whereas PMLE(e|f) corresponds to the analogous discriminative model. PDPT (e|f) corresponds to the DPT model which uses SVM-based predictions in a wider feature context. In order to perform fair comparisons, model weights must be adjusted. Because we have focused on a reduced set of frequent phrases, in order to translate the whole test</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statics. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4305" citStr="Och and Ney, 2003" startWordPosition="662" endWordPosition="665">se the SRI Language Modeling Toolkit (Stolcke, 2002) for language modeling. We build trigram language models applying linear interpolation and Kneser-Ney discounting for smoothing. Translation models are built on top of word-aligned parallel corpora linguistically annotated at the level of shallow syntax (i.e., lemma, part-of-speech, and base phrase chunks) as described by Gim´enez and M`arquez (2005). Text is automatically annotated, using the SVMTool (Gim´enez and M`arquez, 2004), Freeling (Carreras et al., 2004), and Phreco (Carreras et al., 2005) packages. We used the GIZA++ SMT Toolkit1 (Och and Ney, 2003) to generate word alignments. We apply the phrase-extract algorithm, as described by Och (2002), on the Viterbi alignments output by GIZA++ following the ‘global phrase extraction’ strategy described by Gim´enez and M`arquez (2005) (i.e., a single phrase translation table is built on top of the union of alignments corresponding to different linguistic data views). We work with the union of source-to-target and target-to-source alignments, with no heuristic refinement. Phrases up to length five are considered. Also, phrase pairs appearing only once are discarded, and phrase pairs in which the s</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Statistical Machine Translation: From Single-Word Models to Alignment Templates.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>RWTH Aachen,</institution>
<contexts>
<context position="4400" citStr="Och (2002)" startWordPosition="678" endWordPosition="679">odels applying linear interpolation and Kneser-Ney discounting for smoothing. Translation models are built on top of word-aligned parallel corpora linguistically annotated at the level of shallow syntax (i.e., lemma, part-of-speech, and base phrase chunks) as described by Gim´enez and M`arquez (2005). Text is automatically annotated, using the SVMTool (Gim´enez and M`arquez, 2004), Freeling (Carreras et al., 2004), and Phreco (Carreras et al., 2005) packages. We used the GIZA++ SMT Toolkit1 (Och and Ney, 2003) to generate word alignments. We apply the phrase-extract algorithm, as described by Och (2002), on the Viterbi alignments output by GIZA++ following the ‘global phrase extraction’ strategy described by Gim´enez and M`arquez (2005) (i.e., a single phrase translation table is built on top of the union of alignments corresponding to different linguistic data views). We work with the union of source-to-target and target-to-source alignments, with no heuristic refinement. Phrases up to length five are considered. Also, phrase pairs appearing only once are discarded, and phrase pairs in which the source/target phrase is more than three times longer than the target/source phrase are ignored. </context>
</contexts>
<marker>Och, 2002</marker>
<rawString>Franz Josef Och. 2002. Statistical Machine Translation: From Single-Word Models to Alignment Templates. Ph.D. thesis, RWTH Aachen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation,</title>
<date>2001</date>
<tech>rc22176. Technical report, IBM</tech>
<institution>T.J. Watson Research Center.</institution>
<contexts>
<context position="2741" citStr="Papineni et al., 2001" startWordPosition="419" endWordPosition="423">he impact of using DPT models for the isolated ‘phrase translation’ task. In spite of working on a very specific domain, a large room for improvement, coherent with WSD performance, and results by Vickrey et al. (2005), is predicted. Then, in Section 4, we tackle the full translation task. DPT models are integrated in a ‘soft’ manner, by making them available to the decoder so they can fully interact with other models. Results using a reduced set of highly frequent source phrases show a significant improvement, according to several automatic evaluation metrics. Interestingly, the BLEU metric (Papineni et al., 2001) is not able to reflect this improvement. Through a rigorous process of manual evaluation we have verified the gain. We have also observed that it is mainly related to adequacy. These results confirm that better phrase translation probabilities may be helpful for the full translation task. However, the fact that no gain in fluency is reported indicates that the integration of these probabilities into the statistical framework requires further study. 2 Discriminative Phrase Translation In this section we describe the phrase-based SMT baseline system and how DPT models are built and integrated i</context>
<context position="12913" citStr="Papineni et al., 2001" startWordPosition="2059" endWordPosition="2063">es on a search over a probability space in which several models cooperate. For instance, in many cases errors caused by a poor translation modeling may be corrected by the language model. In a recent study, Vilar et al. (2006) found that only around 25% of the errors are related to word selection. In half of these cases errors are caused by a wrong word sense disambiguation, and in the other half the word sense is correct but the lexical choice is wrong. In second place, most conventional automatic evaluation metrics have not been designed for this purpose. For instance, metrics such as BLEU (Papineni et al., 2001) tend to favour longer n-gram matchings, and are, thus, biased towards word ordering. We might find better suited metrics, such as METEOR (Banerjee and Lavie, 2005), which is oriented towards word selection8. However, a new problem arises. Because different metrics are biased towards different aspects of quality, scores conferred by different metrics are often controversial. In order to cope with evaluation difficulties we have applied several complementary actions: 1. Based on the results from Section 3, we focus on a reduced set of 41 very promising phrases trained on more than 50,000 exampl</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2001. Bleu: a method for automatic evaluation of machine translation, rc22176. Technical report, IBM T.J. Watson Research Center.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sidney Siegel</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences.</title>
<date>1956</date>
<publisher>McGraw-Hill.</publisher>
<contexts>
<context position="20956" citStr="Siegel, 1956" startWordPosition="3377" endWordPosition="3378">tive ‘MLE’ system is better, equal to or worse than the output by the ‘DPT’ system, with respect to adequacy, fluency, and overall quality. In order to avoid any bias in the evaluation, we have randomized the respective position in the display of the sentences corresponding to each system. Four judges participated in the evaluation. Each judge evaluated only half of the cases. Each case was evaluated by two different judges. Therefore, we count on 228 human assessments. Table 3 shows the results of the manual system comparison. Statistical significance has been determined using the sign-test (Siegel, 1956). According to human assessors, the ‘DPT’ system outperforms the ‘MLE’ system very significantly with respect to adequacy, whereas for fluency there is a slight advantage in favor of the ‘MLE’ system. Overall, there is a slight but significant advantage in favor of the ‘DPT’ system. Manual evaluation confirms our suspicion that the BLEU metric is less sensitive than QUEEN to improvements related to adequacy. Error Analysis Guided by the QUEEN measure, we carefully inspect particular cases. We start, in Table 4, by showing a positive case. The three phrases highlighted in the source sentence (‘</context>
</contexts>
<marker>Siegel, 1956</marker>
<rawString>Sidney Siegel. 1956. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings ofICSLP.</booktitle>
<contexts>
<context position="3739" citStr="Stolcke, 2002" startWordPosition="578" endWordPosition="579"> probabilities into the statistical framework requires further study. 2 Discriminative Phrase Translation In this section we describe the phrase-based SMT baseline system and how DPT models are built and integrated into this system in a ‘soft’ manner. 159 Proceedings of the Second Workshop on Statistical Machine Translation, pages 159–166, Prague, June 2007. c�2007 Association for Computational Linguistics 2.1 Baseline System The baseline system is a phrase-based SMT system (Koehn et al., 2003), built almost entirely using freely available components. We use the SRI Language Modeling Toolkit (Stolcke, 2002) for language modeling. We build trigram language models applying linear interpolation and Kneser-Ney discounting for smoothing. Translation models are built on top of word-aligned parallel corpora linguistically annotated at the level of shallow syntax (i.e., lemma, part-of-speech, and base phrase chunks) as described by Gim´enez and M`arquez (2005). Text is automatically annotated, using the SVMTool (Gim´enez and M`arquez, 2004), Freeling (Carreras et al., 2004), and Phreco (Carreras et al., 2005) packages. We used the GIZA++ SMT Toolkit1 (Och and Ney, 2003) to generate word alignments. We a</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proceedings ofICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillmann</author>
<author>Tong Zhang</author>
</authors>
<title>A Discriminative Global Training Algorithm for Statistical MT.</title>
<date>2006</date>
<booktitle>In Proceedings of COLING-ACL06.</booktitle>
<marker>Tillmann, Zhang, 2006</marker>
<rawString>Christoph Tillmann and Tong Zhang. 2006. A Discriminative Global Training Algorithm for Statistical MT. In Proceedings of COLING-ACL06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Vickrey</author>
<author>L Biewald</author>
<author>M Teyssier</author>
<author>D Koller</author>
</authors>
<title>Word-Sense Disambiguation for Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofHLT/EMNLP.</booktitle>
<contexts>
<context position="1514" citStr="Vickrey et al. (2005)" startWordPosition="215" endWordPosition="218">adequacy, according to a rigorous process of manual evaluation. 1 Introduction Translations tables in Phrase-based Statistical Machine Translation (SMT) are often built on the basis of Maximum-likelihood Estimation (MLE), being one of the major limitations of this approach that the source sentence context in which phrases occur is completely ignored (Koehn et al., 2003). In this work, inspired by state-of-the-art Word Sense Disambiguation (WSD) techniques, we suggest using Discriminative Phrase Translation (DPT) models which take into account a wider feature context. Following the approach by Vickrey et al. (2005), we deal with the ‘phrase translation’ problem as a classification problem. We use Support Vector Machines (SVMs) to predict phrase translations in the context of the whole source sentence. We extend the work by Vickrey et al. (2005) in two main aspects. First, we move from ‘word translation’ to ‘phrase translation’. Second, we move from the ‘blank-�lling’ task to the ‘full translation’ task. Our approach is fully described in Section 2. We apply it to the Spanish-to-English translation of European Parliament Proceedings. In Section 3, prior to considering the ‘full translation’ task, we anal</context>
<context position="8575" citStr="Vickrey et al. (2005)" startWordPosition="1358" endWordPosition="1361">e of every word in the test set during decoding, and to retrieve DPT predictions in the translation table. For that purpose, source phrases in the translation table must comply with the same format. This imaginative trick4 saved us in the short run a gigantic amount of work. However, it imposes a severe limitation on the kind of features which the DPT system may use. In particular, features from the target sentence under construction and from the correspondence between source and target (i.e., alignments) can not be used. 3 Phrase Translation Analogously to the word translation’ definition by Vickrey et al. (2005), rather than predicting the sense of a word according to a given sense inventory, in phrase translation’, the goal is to predict the correct translation of a phrase, for a given target language, in the context of a sentence. This task is simpler than the ‘full translation’ task, but provides an insight to the gain prospectives. We used the data from the Openlab 2006 Initiative5 promoted by the TC-STAR Consortium6. This test suite is entirely based on European Parliament Proceedings. We have focused on the Spanish-toEnglish task. The training set consists of 1,281,427 parallel sentences. Perfo</context>
<context position="11061" citStr="Vickrey et al. (2005)" startWordPosition="1753" endWordPosition="1756">, i.e., the accuracy for each phrase counts equally towards the average. The ‘micro’ column shows micro-averaged accuracy, where each test example counts equally. The ‘all’ set includes results for the 241,234 phrases, whereas the ‘frequent’ set includes results for a selection of 41 very frequent phrases ocurring more than 50,000 times. A priori, DPT models seem to offer a significant room for potential improvement. Although phrase translation differs from WSD in a number of aspects, the increase with respect to the MFT baseline is comparable. Results are also coherent with those attained by Vickrey et al. (2005). 1 0.5 0 -0.5 -1 0 50000 100000 150000 200000 250000 300000 #examples Figure 1: Analysis of “Phrase Translation” Results on the development set (Spanish-to-English). Figure 1 shows the relationship between the accuracy7 gain and the number of training examples. In general, with a sufficient number of examples (over 10,000), DPT outperforms the MFT baseline. 7We focus on micro-averaged accuracy. accuracy(DPT) - accuracy(MLE) 161 4 Full Translation In the “phrase translation” task the predicted phrase does not interact with the rest of the target sentence. In this section we analyze the impact </context>
<context position="24138" citStr="Vickrey et al. (2005)" startWordPosition="3903" endWordPosition="3906">he lexical choices are made in a way that heavily prefers phrasal cohesion in the output target sentence, as scored by the language model.”. This problem is a direct consequence of the ‘hard’ interaction between their WSD and SMT systems. WSD predictions cannot adapt to the surrounding target context. In a later work, Carpuat and Wu (2005a) analyzed the converse question, i.e. they measured the WSD performance of SMT models. They showed that dedicated WSD models significantly outperform current state-of-the-art SMT models. Consequently, SMT should benefit from WSD predictions. Simultaneously, Vickrey et al. (2005) studied the 164 Source tiene la palabra la se˜nora mussolini para una cuesti´on de orden . Ref 1 mrs mussolini has the floor for a point of order. Ref 2 you have the floor, missus mussolini , for a question of order. Ref 3 ms mussolini has now the floor for a point of order. P(e) + PMLE(e|f) i give the floor to the lady mussolini for a procedural motion. P(e) + PDPT(e|f) has the floor the mrs mussolini on a point of order. Table 4: Case of Analysis of sentence #422. DPT models help. Source se˜nora diputada , ´esta no es una cuesti´on de orden . Ref 1 mrs mussolini , that is not a point of ord</context>
</contexts>
<marker>Vickrey, Biewald, Teyssier, Koller, 2005</marker>
<rawString>D. Vickrey, L. Biewald, M. Teyssier, and D. Koller. 2005. Word-Sense Disambiguation for Machine Translation. In Proceedings ofHLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Vilar</author>
<author>Jia Xu</author>
<author>Luis Fernando D’Haro</author>
<author>Hermann Ney</author>
</authors>
<title>Error Analysis of Machine Translation Output.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th LREC.</booktitle>
<marker>Vilar, Xu, D’Haro, Ney, 2006</marker>
<rawString>David Vilar, Jia Xu, Luis Fernando D’Haro, and Hermann Ney. 2006. Error Analysis of Machine Translation Output. In Proceedings of the 5th LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Silviu Cucerzan</author>
<author>Radu Florian</author>
<author>Charles Schafer</author>
<author>Richard Wicentowski</author>
</authors>
<title>The Johns Hopkins Senseval2 System Descriptions.</title>
<date>2001</date>
<booktitle>In Proceedings of Senseval-2: Second International Workshop on Evaluating Word Sense Disambiguation Systems.</booktitle>
<contexts>
<context position="6479" citStr="Yarowsky et al., 2001" startWordPosition="1006" endWordPosition="1009"> extracted from the same training data as in the case of MLE models, i.e., an aligned parallel corpus, obtained as described in Section 2.1. We use each sentence pair in which the source phrase fi occurs to generate a positive example for the classifier corresponding to the actual translation of fi in that sentence, according to the automatic alignment. This will be as well a negative example for the classifiers corresponding to the rest of possible translations of fi. 2.2.1 Feature Set We consider different kinds of information, always from the source sentence, based on standard WSD methods (Yarowsky et al., 2001). As to the local context, inside the source phrase to disambiguate, and 5 tokens to the left and to the right, we use n-grams (n E {1, 2,3}) of: words, partsof-speech, lemmas and base phrase chunking IOB labels. As to the global context, we collect topical information by considering the source sentence as a bag of lemmas. 2.2.2 Decoding. A Trick. At translation time, we consider every instance of fi as a separate case. In each case, for all possible translations of fi, we collect the SVM score, according to the SVM classification rule. We are in fact modeling P(ej|fi). However, these scores a</context>
</contexts>
<marker>Yarowsky, Cucerzan, Florian, Schafer, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Silviu Cucerzan, Radu Florian, Charles Schafer, and Richard Wicentowski. 2001. The Johns Hopkins Senseval2 System Descriptions. In Proceedings of Senseval-2: Second International Workshop on Evaluating Word Sense Disambiguation Systems.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>