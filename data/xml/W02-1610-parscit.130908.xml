<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001694">
<title confidence="0.9982175">
Learning Domain-Specific Transfer Rules:
An Experiment with Korean to English Translation
</title>
<author confidence="0.997097">
Benoit Lavoie, Michael White, and Tanya Korelsky
</author>
<affiliation confidence="0.940902">
CoGenTex, Inc.
</affiliation>
<address confidence="0.95892">
840 Hanshaw Road
Ithaca, NY 14850, USA
</address>
<email confidence="0.99881">
benoit,mike,tanya@cogentex.com
</email>
<sectionHeader confidence="0.995634" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999774888888889">
We describe the design of an MT system that em-
ploys transfer rules induced from parsed bitexts
and present evaluation results. The system learns
lexico-structural transfer rules using syntactic pat-
tern matching, statistical co-occurrence and error-
driven filtering. In an experiment with domain-
specific Korean to English translation, the approach
yielded substantial improvements over three base-
line systems.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999837357142857">
In this paper, we describe the design of an MT
system that employs transfer rules induced from
parsed bitexts and present evaluation results for Ko-
rean to English translation. Our approach is based
on lexico-structural transfer (Nasr et. al., 1997),
and extends recent work reported in (Han et al.,
2000) about Korean to English transfer in particular.
Whereas Han et al. focus on high quality domain-
specific translation using handcrafted transfer rules,
in this work we instead focus on automating the ac-
quisition of such rules.
The proposed approach is inspired by example-
based machine translation (EBMT; Nagao, 1984;
Sato and Nagao, 1990; Maruyama and Watanabe,
1992) and is similar to the recent works of (Mey-
ers et al., 1998) and (Richardson et al., 2001) where
transfer rules are also derived after aligning the
source and target nodes of corresponding parses.
However, while (Meyers et al., 1998) and (Richard-
son et al., 2001) only consider parses and rules
with lexical labels and syntactic roles, our approach
uses parses containing any syntactic information
provided by parsers (lexical labels, syntactic roles,
tense, number, person, etc.), and derives rules con-
sisting of any source and target tree sub-patterns
matching a subset of the parse features. A more de-
tailed description of the differences can be found in
(Lavoie et. al., 2001).
</bodyText>
<sectionHeader confidence="0.926217" genericHeader="method">
2 Overall Runtime System Design
</sectionHeader>
<bodyText confidence="0.9998810625">
Our Korean to English MT runtime system relies on
the following off-the-shelf software components:
Korean parser For parsing, we used the wide cov-
erage syntactic dependency parser for Korean
developed by (Yoon et al., 1997). The parser
was not trained on our corpus.
Transfer component For transfer of the Korean
parses to English structures, we used the same
lexico-structural transfer framework as (Lavoie
et al., 2000).
Realizer For surface realization of the transferred
English syntactic structures, we used the Re-
alPro English realizer (Lavoie and Rambow,
1997).
The training of the system is described in the next
two sections.
</bodyText>
<sectionHeader confidence="0.999166" genericHeader="method">
3 Data Preparation
</sectionHeader>
<subsectionHeader confidence="0.973392">
3.1 Parses for the Bitexts
</subsectionHeader>
<bodyText confidence="0.999969888888889">
In our experiments, we used a parallel corpus de-
rived from bilingual training manuals provided by
the U.S. Defense Language Institute. The corpus
consists of a Korean dialog of 4,183 sentences about
battle scenario message traffic and their English hu-
man translations.
The parses for the Korean sentences were ob-
tained using Yoon’s parser, as in the runtime sys-
tem. The parses for the English human transla-
</bodyText>
<equation confidence="0.996026090909091">
(S1) {i} {Ci-To-Reul} {Ta-Si} {Po-Ra}.
this + map-accusative + again + look-imp
(D1) {po} [class=vbma ente={ra}] (
s1 {ci-to} [class=nnin2 ppca={reul}] (
s1 {i} [class=ande] )
s1 {ta-si} [class=adco2] )
(S2) Look at the map again.
(D2) look [class=verb mood=imp] (
attr at [class=preposition] (
ii map [class=common_noun article=def] )
attr again [class=adverb] )
</equation>
<figureCaption confidence="0.99434">
Figure 1: Syntactic dependency representations for
corresponding Korean and English sentences
</figureCaption>
<table confidence="0.992394666666667">
Korean English
Avg. sentence size 9.08 13.26
Avg. parse size 8.96 10.77
</table>
<tableCaption confidence="0.943135">
Table 1: Average sizes for sentences and parses in
corpus
</tableCaption>
<bodyText confidence="0.999664555555556">
tions were derived from an English Tree Bank de-
veloped in (Han et al., 2000). To enable the sur-
face realization of the English parses via RealPro,
we automatically converted the phrase structures
of the English Tree Bank into deep-syntactic de-
pendency structures (DSyntSs) of the Meaning-Text
Theory (MTT) (Mel’ˇcuk, 1988) using Xia’s con-
verter (Xia and Palmer, 2001) and our own conver-
sion grammars. The realization results of the re-
sulting DSyntSs for our training corpus yielded a
unigram and bigram accuracy (f-score) of approxi-
mately 95% and 90%, respectively.
A DSyntS is an unordered tree where all nodes
are meaning-bearing and lexicalized. Since the out-
put of the Yoon parser is quite similar, we have used
its output as is. The syntactic dependency represen-
tations for two corresponding Korean1 and English
sentences are shown in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999552">
3.2 Training and Test Sets of Parse Pairs
</subsectionHeader>
<bodyText confidence="0.993714555555556">
The average sentence lengths (in words) and parse
sizes (in nodes) for the 4,183 Korean and English
sentences in our corpus are given in Table 1.
In examining the Korean parses, we found that
many of the larger parses, especially those contain-
ing intra-sentential punctuation, had incorrect de-
pendency assignments, incomplete lemmatization
or were incomplete parses. In examining the En-
glish converted parses, we found that many of
</bodyText>
<footnote confidence="0.987906">
1Korean is represented in romanized format in this paper.
</footnote>
<table confidence="0.997970333333333">
Korean English
Avg. sentence size 6.43 9.36
Avg. parse size 6.43 7.34
</table>
<tableCaption confidence="0.9601535">
Table 2: Average sizes for sentences and parses in
training set
</tableCaption>
<table confidence="0.997467666666667">
Korean English
Avg. sentence size 7.04 10.58
Avg. parse size 7.02 8.56
</table>
<tableCaption confidence="0.960738">
Table 3: Average sizes for sentences and parses in
test set
</tableCaption>
<bodyText confidence="0.998216176470588">
the parses containing intra-sentential punctuation
marks other than commas had incorrect dependency
assignments, due to the limitations of our conver-
sion grammars. Consequently, in our experiments
we have primarily focused on a higher quality sub-
set of 1,483 sentence pairs, automatically selected
by eliminating from the corpus all parse pairs where
one of the parses contained more than 11 content
nodes or involved problematic intra-sentential punc-
tuation.
We divided this higher quality subset into training
and test sets. For the test set, we randomly selected
50 parse pairs containing at least 5 nodes each. For
the training set, we used the remaining 1,433 parse
pairs. The average sentence lengths and parse sizes
for the training and test sets are represented in Ta-
bles 2 and 3.
</bodyText>
<subsectionHeader confidence="0.999888">
3.3 Creating the Baseline Transfer Dictionary
</subsectionHeader>
<bodyText confidence="0.999788111111111">
In our system, transfer dictionaries contain Ko-
rean to English lexico-structural transfer rules de-
fined using the formalism described in (Nasr et.
al., 1997), extended to include log likelihood ra-
tios (Manning and Schutze, 1999: 172-175). Sam-
ple transfer rules are illustrated in Section 4. The
simplest transfer rules consist of direct lexical map-
pings, while the most complex may contain source
and target syntactic patterns composed of multiple
nodes defined with lexical and/or syntactic features.
Each transfer rule is assigned a log likelihood ratio
calculated using the training parse set.
To create the baseline transfer dictionary for our
experiments, we had three bilingual dictionary re-
sources at our disposal:
A corpus-based handcrafted dictionary: This
dictionary was manually assembled by (Han et
al., 2000) for the same corpus used here. Note,
</bodyText>
<table confidence="0.388639">
Korean English
</table>
<tableCaption confidence="0.832518">
Table 4: Concurrent lexical coverage of training set
by baseline dictionary
</tableCaption>
<bodyText confidence="0.999876764705882">
however, that it was developed for different
parse representations, and with an emphasis
primarily on the lexical coverage of the source
parses, rather than the source and target parse
pairs.
A corpus-based extracted dictionary: This dic-
tionary was automatically created from our
corpus by the RALI group from the University
of Montreal. Since the extraction heuristics
did not handle the rich morphological suffixes
of Korean, the extraction results contained
inflected words rather than lexemes.
A wide coverage dictionary: This dictionary of
70,300 entries was created by Systran, without
regard to our corpus.
We processed and combined these resources as
follows:
</bodyText>
<listItem confidence="0.927546764705882">
• First, we replaced the inflected words with un-
inflected lexemes using Yoon’s morphological
analyzer and a wide coverage English morpho-
logical database (Karp and Schabes, 1992).
• Second, we merged all morphologically an-
alyzed entries after removing all non-lexical
features, since these features generally did not
match those found in the parses.
• Third, we matched the resulting transfer dictio-
nary entries with the training parse set, in order
to determine for each entry all possible part-of-
speech instantiations and dependency relation-
ships. For each distinct instantiation, we cal-
culated a log likelihood ratio.
• Finally, we created a baseline dictionary us-
ing the instantiated rules whose source patterns
had the best log likelihood ratios.
</listItem>
<bodyText confidence="0.999765714285714">
Table 4 illustrates the concurrent lexical coverage
of the training set using the resulting baseline dictio-
nary, i.e. the percentage of nodes covered by rules
whose source and target patterns both match. Note
that since the baseline dictionary contained some
noise, we allowed induced rules to override ones in
the baseline dictionary where applicable.
</bodyText>
<equation confidence="0.92002125">
@KOREAN:
{po} [class=vbma] (
s1 $X [ppca={reul}] )
@ENGLISH:
look [class=verb] (
attr at [class=preposition] (
ii $X ))
@-2xLOG_LIKELIHOOD: 12.77
</equation>
<figureCaption confidence="0.9945645">
Figure 2: Transfer rule for English lexicalization
and preposition insertion
</figureCaption>
<sectionHeader confidence="0.965828" genericHeader="method">
4 Transfer Rule Induction
</sectionHeader>
<bodyText confidence="0.999819666666667">
The transfer rule induction process has the follow-
ing steps described below (additional details can
also be found in (Lavoie et. al., 2001)):
</bodyText>
<listItem confidence="0.9356923125">
• Nodes of the corresponding source and target
parses are aligned using the baseline transfer
dictionary and some heuristics based on the
similarity of part-of-speech and syntactic con-
text.
• Transfer rule candidates are generated based
on the sub-patterns that contain the corre-
sponding aligned nodes in the source and target
parses.
• The transfer rule candidates are ordered based
on their likelihood ratios.
• The transfer rule candidates are filtered, one at
a time, in the order of the likelihood ratios, by
removing those rule candidates that do not pro-
duce an overall improvement in the accuracy of
the transferred parses.
</listItem>
<bodyText confidence="0.999888272727273">
Figures 2 and 3 show two sample induced rules.
The rule formalism uses notation similar to the
syntactic dependency notation shown in Figure 1,
augmented with variable arguments prefixed with
$ characters. These two lexico-structural rules can
be used to transfer a Korean syntactic representation
for ci-to-reul po-ra to an English syntactic represen-
tation for look at the map. The first rule lexicalizes
the English predicate and inserts the corresponding
preposition while the second rule inserts the English
imperative attribute.
</bodyText>
<subsectionHeader confidence="0.996266">
4.1 Aligning the Parse Nodes
</subsectionHeader>
<bodyText confidence="0.99976">
To align the nodes in the source and target parse
trees, we devised a new dynamic programming
</bodyText>
<table confidence="0.9976725">
Lexical coverage 92.18% 90.17%
@KOREAN: @KOREAN:
$X [class=vbma ente={ra}] $X1 [aid=$1] (
@ENGLISH: $R1 $X2 [aid=$2] )
$X [class=verb mood=imp] @ENGLISH:
@-2xLOG_LIKELIHOOD: 33.37 $Y1 [aid=$1] (
$R2 $Y2 (
$R3 $Y3 [aid=$2] ) )
</table>
<figureCaption confidence="0.85331">
Figure 3: Transfer rule for imperative forms
</figureCaption>
<bodyText confidence="0.99929715">
alignment algorithm that performs a top-down, bidi-
rectional beam search for the least cost mapping be-
tween these nodes. The algorithm is parameterized
by the costs of (1) aligning two nodes whose lex-
emes are not found in the baseline transfer dictio-
nary; (2) aligning two nodes with differing parts of
speech; (3) deleting or inserting a node in the source
or target tree; and (4) aligning two nodes whose rel-
ative locations differ.
To determine an appropriate part of speech cost
measure, we first extracted a small set of parse pairs
that could be reliably aligned using lexical matching
alone, and then based the cost measure on the co-
occurrence counts of the observed parts of speech
pairings. The remaining costs were set by hand.
As a result of the alignment process, alignment id
attributes (aid) are added to the nodes of the parse
pairs. Some nodes may be in alignment with no
other node, such as English prepositions not found
in the Korean DSyntS.
</bodyText>
<subsectionHeader confidence="0.991332">
4.2 Generating Rule Candidates
</subsectionHeader>
<bodyText confidence="0.99972825">
Candidate transfer rules are generated by extracting
source and target tree sub-patterns from the aligned
parse pairs using the two set of constraints described
below.
</bodyText>
<subsectionHeader confidence="0.829924">
4.2.1 Alignment constraints
</subsectionHeader>
<bodyText confidence="0.999972333333334">
Figure 4 shows an example alignment constraint.
This constraint, which matches the structural pat-
terns of the transfer rule illustrated in Figure 2, uses
the aid alignment attribute to indicate that in a Ko-
rean and English parse pair, any source and target
sub-trees matching this alignment constraint (where
$X1 and $Y1 are aligned, i.e. have the same aid at-
tribute values, and where $X2 and $Y3 are aligned)
can be used as a point of departure for generat-
ing transfer rule candidates. We suggest that align-
ment constraints such as this one can be used to de-
fine most of the possible syntactic divergences be-
tween languages (Dorr, 1994), and that only a hand-
ful of them are necessary for two given languages
(we have identified 11 general alignment constraints
</bodyText>
<figureCaption confidence="0.998714">
Figure 4: Alignment constraint
</figureCaption>
<bodyText confidence="0.9212805">
Each node of a candidate transfer rule must have
its relation attribute (relationship with its governor)
specified if it is an internal node, otherwise this relation
must not be specified:
</bodyText>
<equation confidence="0.699777">
e.g.
0 $X1 ( $R $X2 )
</equation>
<figureCaption confidence="0.989339">
Figure 5: Independent attribute constraint
</figureCaption>
<bodyText confidence="0.908283">
necessary for Korean to English transfer so far).
</bodyText>
<subsectionHeader confidence="0.662877">
4.2.2 Attribute constraints
</subsectionHeader>
<bodyText confidence="0.9882195">
Attribute constraints are used to limit the space of
possible transfer rule candidates that can be gen-
erated from the sub-trees satisfying the alignment
constraints. Candidate transfer rules must satisfy all
of the attribute constraints. Attribute constraints can
be divided into two types:
</bodyText>
<listItem confidence="0.707217">
• independent attribute constraints, whose scope
covers only one part of a candidate transfer rule
and which are the same for the source and tar-
get parts;
• concurrent attribute constraints, whose scope
extends to both the source and target parts of a
candidate transfer rule.
</listItem>
<bodyText confidence="0.999882833333333">
Figures 5 and 6 give examples of an indepen-
dent attribute constraint and of a concurrent attribute
constraint. As with the alignment constraints, we
suggest that a relatively small number of attribute
constraints is necessary to generate most of the de-
sired rules for a given language pair.
</bodyText>
<subsectionHeader confidence="0.997391">
4.3 Ordering Rule Candidates
</subsectionHeader>
<bodyText confidence="0.99942925">
In the next step, transfer rule candidates are ordered
as follows. First, they are sorted by their decreasing
log likelihood ratios. Second, if two or more can-
didate transfer rules have the same log likelihood
ratio, ties are broken by a specificity heuristic, with
the result that more general rules are ordered ahead
In a candidate transfer rule, inclusion of the lexemes of
two aligned nodes must be done concurrently:
</bodyText>
<equation confidence="0.793152928571429">
e.g.
$X [aid=$1]
and
$Y [aid=$1]
e.g.
0 [aid=$1]
@KOREAN:
{po} [class=vbma ente={ra}] (
s1 $X [ppca={reul}] )
@ENGLISH:
look [class=verb mood=imp] (
attr at [class=preposition] (
ii $X ) )
@-2xLOG_LIKELIHOOD: 11.40
</equation>
<figure confidence="0.928274">
and
0 [aid=$1] Figure 7: Transfer rule for English imperative with
lexicalization and preposition insertion
</figure>
<figureCaption confidence="0.99986">
Figure 6: Concurrent attribute constraint
</figureCaption>
<bodyText confidence="0.9989325">
of more specific ones. The specificity of a rule is
defined to be the following sum: the number of at-
tributes found in the source and target patterns, plus
1 for each for each lexeme attribute and for each de-
pendency relationship. In our initial experiments,
this simple heuristic has been satisfactory.
</bodyText>
<subsectionHeader confidence="0.999104">
4.4 Filtering Rule Candidates
</subsectionHeader>
<bodyText confidence="0.99999188">
Once the candidate transfer rules have been ordered,
error-driven filtering is used to select those that yield
improvements over the baseline transfer dictionary.
The algorithm works as follows. First, in the initial-
ization step, the set of accepted transfer rules is set
to just those appearing in the baseline transfer dic-
tionary. The current error rate is also established, by
applying these transfer rules to all the source struc-
tures and calculating the overall difference between
the resulting transferred structures and the target
parses, using a tree accuracy recall and precision
measure (determined by comparing the features and
dependency relationships in the transferred parses
and corresponding target parses). Then, in a sin-
gle pass through the ordered list of candidates, each
transfer rule candidate is tested to see if it reduces
the error rate. During each iteration, the candidate
transfer rule is provisionally added to the current set
of accepted rules and the updated set is applied to
all the source structures. If the overall difference be-
tween the transferred structures and the target parses
is lower than the current error rate, then the can-
didate is accepted and the current error rate is up-
dated; otherwise, the candidate is rejected and re-
moved from the current set.
</bodyText>
<subsectionHeader confidence="0.988075">
4.5 Discussion of Induced Rules
</subsectionHeader>
<bodyText confidence="0.9999819">
In our experiments, the alignment constraints
yielded 22,881 source and target sub-tree pairs from
the training set of 1,433 parse pairs. Using the at-
tribute constraints, an initial list of 801,674 trans-
fer rule candidates was then generated from these
sub-tree pairs. The initial list was subsequently re-
duced to 32,877 unique transfer rule candidates by
removing duplicates and by eliminating candidates
that had the same source pattern as another candi-
date with a better log likelihood ratio. After filter-
ing, 2,133 of these transfer rule candidates were ac-
cepted. We expect that the number of accepted rules
per parse pair will decrease with larger training sets,
though this remains to be verified.
The rule illustrated in Figure 3 was accepted as
the 65th best transfer rule with a log likelihood ra-
tio of 33.37, and the rule illustrated in Figure 2 was
accepted as the 189th best transfer rule candidate
with a log likelihood ratio of 12.77. An example
of a candidate transfer rule that was not accepted is
the one that combines the features of the two rules
mentioned above, illustrated in Figure 7. This trans-
fer rule candidate had a lower log likelihood ratio of
11.40; consequently, it is only considered after the
two rules mentioned above, and since it provides no
further improvement upon these two rules, it is fil-
tered out.
In an informal inspection of the top 100 accepted
transfer rules, we found that most of them appear
to be fairly general rules that would normally be
found in a general syntactic-based transfer dictio-
nary. In looking at the remaining rules, we found
that the rules tended to become increasingly corpus-
specific.
The induction results were obtained using a Java
implementation of the induction component. Mi-
crosoft SQL Server was used to count and dedupli-
cate the rule candidates. The data preparation and
induction processes took about 12 hours on a 300
MHz PC with 256 MB RAM.
</bodyText>
<sectionHeader confidence="0.997814" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999774">
5.1 Systems Compared
</subsectionHeader>
<bodyText confidence="0.999336730769231">
Babelfish As a first baseline, we used Babelfish
from Systran, a commercial large coverage MT
system supporting Korean to English transla-
tion. This system was not trained on our cor-
pus.
GIZA++/RW As a second baseline, we used an
off-the-shelf statistical MT system, consisting
of the ISI ReWrite Decoder (Germann et al.,
2001) together with a translation model pro-
duced by GIZA++ (Och and Ney, 2000) and
a language model produced by the CMU Sta-
tistical Language Modeling Toolkit (Clarkson
and Rosenfeld, 1997). This system was trained
on our corpus only.
Lex Only As a third baseline, we used our system
with the baseline transfer dictionary as the sole
transfer resource.
Lex+Induced We compared the three baseline sys-
tems against our complete system, using the
baseline transfer dictionary augmented with
the induced transfer rules.
We ran each of the four systems on the test set
of 50 Korean sentences described in Section 3.2
and compared the resulting translations using the
automatic evaluation and the human evaluation de-
scribed below.
</bodyText>
<subsectionHeader confidence="0.999134">
5.2 Automatic Evaluation Results
</subsectionHeader>
<bodyText confidence="0.99950405">
For the automatic evaluation, we used the Bleu met-
ric from IBM (Papineni et al., 2001). The Bleu
metric combines several modified N-gram precision
measures (N = 1 to 4), and uses brevity penalties to
penalize translations that are shorter than the refer-
ence sentences.
Table 5 shows the Bleu N-gram precision scores
for each of the four systems. Our system
(Lex+Induced) had better precision scores than each
of the baseline systems, except in the case of 4-
grams, where it slightly trailed Babelfish. The sta-
tistical baseline system (GIZA++/RW) performed
poorly, as might have been expected given the small
amount of training data.
Table 6 shows the Bleu overall precision scores.
Our system (Lex+Induced) improved substantially
over both the Lex Only and Babelfish baseline sys-
tems. The score for the statistical baseline system
(GIZA++/RW) is not meaningful, due to the ab-
sence of 3-gram and 4-gram matches.
</bodyText>
<table confidence="0.9969118">
System 1-g Prec 2-g Prec 3-g Prec 4-g Prec
Babelfish 0.3814 0.1207 0.0467 0.0193
GIZA++/RW 0.1894 0.0173 0.0 0.0
Lex Only 0.4234 0.1252 0.0450 0.0145
Lex+Induced 0.4725 0.1618 0.0577 0.0185
</table>
<tableCaption confidence="0.830898">
Table 5: Bleu N-gram precision scores
</tableCaption>
<table confidence="0.9993848">
System Bleu Score
Babelfish 0.0802
GIZA++/RW NA
Lex Only 0.0767
Lex+Induced 0.0950
</table>
<tableCaption confidence="0.998691">
Table 6: Bleu overall precision scores
</tableCaption>
<subsectionHeader confidence="0.981291">
5.3 Human Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999959263157895">
For the human evaluation, we asked two English
native speakers to rank the quality of the transla-
tion results produced by the Babelfish, Lex Only
and Lex+Induced, with preference given to fidelity
over fluency. (The translation results of the statisti-
cal system were not yet available when the evalua-
tion was performed.) A rank of 1 was assigned to
the best translation, a rank of two to the second best
and a rank of 3 to the third, with ties allowed.
Table 7 shows the pairwise comparisons of the
three systems. The top section indicates that the
Babelfish and Lex Only baseline systems are es-
sentially tied, with neither system preferred more
frequently than the other. In contrast, the middle
and bottom sections show that our system improves
substantially over both baseline systems; most strik-
ingly, our system (Lex+Induced) was preferred al-
most 20% more frequently than the Babelfish base-
line (46% to 27%, with ties 27% of the time).
</bodyText>
<note confidence="0.8154599">
System Pair Comparison Result
Babelfish better than Lex Only 37%
Lex Only better than Babelfish 36%
Babelfish same as Lex Only 27%
Babelfish better than Lex+Induced 27%
Lex+Induced better than Babelfish 46%
Babelfish same as Lex+Induced 27%
Lex Only better than Lex+Induced 18%
Lex+Induced better than Lex Only 41%
Lex Only same as Lex+Induced 41%
</note>
<tableCaption confidence="0.992591">
Table 7: Human evaluation results
</tableCaption>
<sectionHeader confidence="0.966276" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99968975">
In this paper we have described the design of an MT
system based on lexico-structural transfer rules in-
duced from parsed bitexts. In a small scale exper-
iment with Korean to English translation, we have
demonstrated a substantial improvement over three
baseline systems, including a nearly 20% improve-
ment in the preference rate for our system over Ba-
belfish (which was not trained on our corpus). Al-
though our experimentation was aimed at Korean
to English translation, we believe that our approach
can be readily applied to other language pairs.
It remains for future work to explore how well
the approach would fare with a much larger train-
ing corpus. One foreseeable problem concerns the
treatment of lengthy training sentences: since the
number of transfer rule candidates generated grows
exponentially with the size of the parse tree pairs,
refinements will be necessary in order to make use
of complex sentences; one option might be to auto-
matically chunk longer sentences into smaller units.
</bodyText>
<sectionHeader confidence="0.994738" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9995157">
We thank Richard Kittredge for helpful discussion, Daryl Mc-
Cullough and Ted Caldwell for their help with evaluation and
Fei Xia for her assistance with the automatic conversion of
the phrase structure parses to syntactic dependency represen-
tations. We also thank Chung-hye Han, Chulwoo Park, Martha
Palmer, and Joseph Rosenzweig for the handcrafted Korean-
English transfer dictionary, and Graham Russell for the corpus-
based extracted transfer dictionary. This work has been par-
tially supported by DARPA TIDES contract no. N66001-00-C-
8009.
</bodyText>
<sectionHeader confidence="0.99857" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999606904761905">
Philip Clarkson and Ronald Rosenfeld. 1997. Statistical Lan-
guage Modeling Using the CMU-Cambridge Toolkit. In
Proceedings ofEurospeech’97.
Bonnie Dorr. 1994. Machine translation divergences: A for-
mal description and proposed solution. Computational Lin-
guistics, 20(4):597–635.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu
and Kenji Yamada. 2001. Fast Decoding and Optimal De-
coding for Machine Translation. In Proceedings ofACL’01,
Toulouse, France.
Chung hye Han, Benoit Lavoie, Martha Palmer, Owen
Rambow, Richard Kittredge, Tanya Korelsky, Nari Kim,
and Myunghee Kim. 2000. Handling Structural Diver-
gences and Recovering Dropped Arguments in a Korean-
English Machine Translation System. In Proceedings of the
Fourth Conference on Machine Translation in the Americas
(AMTA’00), Mision Del Sol, Mexico.
Daniel Karp and Yves Schabes. 1992. A Wide Coverage
Public Domain Morphological Analyzer for English. In
Proceedings of the Fifteenth International Conference on
Computational Linguistics (COLING’92), pages 950–955,
Nantes, France.
Benoit Lavoie and Owen Rambow. 1997. RealPro – A Fast,
Portable Sentence Realizer. In Proceedings of the Confer-
ence on Applied Natural Language Processing (ANLP’97),
Washington, DC.
Benoit Lavoie, Richard Kittredge, Tanya Korelsky, and Owen
Rambow. 2000. A framework for MT and multilingual
NLG systems based on uniform lexico-structural process-
ing. In Proceedings ofANLP/NAACL 2000, Seattle, Wash-
ington.
Benoit Lavoie, Michael White, and Tanya Korelsky. 2001. In-
ducing Lexico-Structural Transfer Rules from Parsed Bi-
texts. In Proceedings of the ACL 2001 Workshop on
Data-driven Machine Translation, pages 17–24, Toulouse,
France.
Christopher D. Manning and Hinrich Schutze. 1999. Foun-
dations of Statistical Natural Language Processing. MIT
Press.
H. Maruyama and H. Watanabe. 1992. Tree cover Search
Algorithm for Example-Based Translation. In Proceedings
of the Fourth International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI’92),
pages 173–184.
Yuji Matsumoto, Hiroyuki Hishimoto, and Takehito Utsuro.
1993. Structural Matching of Parallel Texts. In Proceedings
of the 31st Annual Meetings of the Association for Compu-
tational Linguistics (ACL’93), pages 23–30.
Igor Mel’ˇcuk. 1988. Dependency Syntax. State University of
New York Press, Albany, NY.
Adam Meyers, Roman Yangarber, Ralph Grishman, Catherine
Macleod, and Antonio Moreno-Sandoval. 1998. Deriving
Transfer Rules from Dominance-Preserving Alignments. In
Proceedings of COLING-ACL’98, pages 843–847.
Makoto Nagao. 1984. A framework of a mechanical transla-
tion between Japenese and English by analogy principle. In
A. Elithorn and R. Banerji, editors, Artificial and Human
Intelligence. NATO Publications.
Alexis Nasr, Owen Rambow, Martha Palmer, and Joseph
Rosenzweig. 1997. Enriching lexical transfer with cross-
linguistic semantic features. In Proceedings of the Interlin-
gua Workshop at the MT Summit, San Diego, California.
Franz Josef Och and Hermann Ney. 2000. Improved Statistical
Alignment Models. In Proceedings of ACL’00, pages 440–
447, Hongkong, China.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. Bleu: a Method for Automatic Evaluation of
Machine Translation. In IBM technical report, RC22176.
Stephen D. Richardson, William B. Dolan, Arul Menezes, and
Monica Corston-Oliver. 2001. Overcoming the Customiza-
tion Bottleneck using Example-based MT. In Proceedings
of the ACL 2001 Workshop on Data-driven Machine Trans-
lation, pages 9–16, Toulouse, France.
Satoshi Sato and Makoto Nagao. 1990. Toward Memory-
based Translation. In Proceedings of the 13th International
Conference on Computational Linguistics (COLING’90),
pages 247–252.
Fei Xia and Martha Palmer. 2001. Converting Dependency
Structures to Phrase Structures. In Notes of the First Human
Language Technology Conference, San Diego, California.
Juntae Yoon, Seonho Kim, and Mansuk Song. 1997. New
parsing method using global association table. In Proceed-
ings of the 5th International Workshop on Parsing Technol-
ogy.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.816068">
<title confidence="0.9996735">Learning Domain-Specific Transfer Rules: An Experiment with Korean to English Translation</title>
<author confidence="0.994657">Benoit Lavoie</author>
<author confidence="0.994657">Michael White</author>
<author confidence="0.994657">Tanya</author>
<affiliation confidence="0.841266">CoGenTex,</affiliation>
<address confidence="0.998038">840 Hanshaw Ithaca, NY 14850,</address>
<email confidence="0.999813">benoit,mike,tanya@cogentex.com</email>
<abstract confidence="0.9975532">We describe the design of an MT system that employs transfer rules induced from parsed bitexts and present evaluation results. The system learns lexico-structural transfer rules using syntactic pattern matching, statistical co-occurrence and errordriven filtering. In an experiment with domainspecific Korean to English translation, the approach yielded substantial improvements over three baseline systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Philip Clarkson</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>Statistical Language Modeling Using the CMU-Cambridge Toolkit.</title>
<date>1997</date>
<booktitle>In Proceedings ofEurospeech’97.</booktitle>
<contexts>
<context position="18978" citStr="Clarkson and Rosenfeld, 1997" startWordPosition="3019" endWordPosition="3022">paration and induction processes took about 12 hours on a 300 MHz PC with 256 MB RAM. 5 Evaluation 5.1 Systems Compared Babelfish As a first baseline, we used Babelfish from Systran, a commercial large coverage MT system supporting Korean to English translation. This system was not trained on our corpus. GIZA++/RW As a second baseline, we used an off-the-shelf statistical MT system, consisting of the ISI ReWrite Decoder (Germann et al., 2001) together with a translation model produced by GIZA++ (Och and Ney, 2000) and a language model produced by the CMU Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). This system was trained on our corpus only. Lex Only As a third baseline, we used our system with the baseline transfer dictionary as the sole transfer resource. Lex+Induced We compared the three baseline systems against our complete system, using the baseline transfer dictionary augmented with the induced transfer rules. We ran each of the four systems on the test set of 50 Korean sentences described in Section 3.2 and compared the resulting translations using the automatic evaluation and the human evaluation described below. 5.2 Automatic Evaluation Results For the automatic evaluation, we</context>
</contexts>
<marker>Clarkson, Rosenfeld, 1997</marker>
<rawString>Philip Clarkson and Ronald Rosenfeld. 1997. Statistical Language Modeling Using the CMU-Cambridge Toolkit. In Proceedings ofEurospeech’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
</authors>
<title>Machine translation divergences: A formal description and proposed solution.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>4</issue>
<contexts>
<context position="12692" citStr="Dorr, 1994" startWordPosition="1998" endWordPosition="1999">alignment constraint. This constraint, which matches the structural patterns of the transfer rule illustrated in Figure 2, uses the aid alignment attribute to indicate that in a Korean and English parse pair, any source and target sub-trees matching this alignment constraint (where $X1 and $Y1 are aligned, i.e. have the same aid attribute values, and where $X2 and $Y3 are aligned) can be used as a point of departure for generating transfer rule candidates. We suggest that alignment constraints such as this one can be used to define most of the possible syntactic divergences between languages (Dorr, 1994), and that only a handful of them are necessary for two given languages (we have identified 11 general alignment constraints Figure 4: Alignment constraint Each node of a candidate transfer rule must have its relation attribute (relationship with its governor) specified if it is an internal node, otherwise this relation must not be specified: e.g. 0 $X1 ( $R $X2 ) Figure 5: Independent attribute constraint necessary for Korean to English transfer so far). 4.2.2 Attribute constraints Attribute constraints are used to limit the space of possible transfer rule candidates that can be generated fro</context>
</contexts>
<marker>Dorr, 1994</marker>
<rawString>Bonnie Dorr. 1994. Machine translation divergences: A formal description and proposed solution. Computational Linguistics, 20(4):597–635.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast Decoding and Optimal Decoding for Machine Translation.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL’01,</booktitle>
<location>Toulouse, France.</location>
<contexts>
<context position="18795" citStr="Germann et al., 2001" startWordPosition="2989" endWordPosition="2992">nduction results were obtained using a Java implementation of the induction component. Microsoft SQL Server was used to count and deduplicate the rule candidates. The data preparation and induction processes took about 12 hours on a 300 MHz PC with 256 MB RAM. 5 Evaluation 5.1 Systems Compared Babelfish As a first baseline, we used Babelfish from Systran, a commercial large coverage MT system supporting Korean to English translation. This system was not trained on our corpus. GIZA++/RW As a second baseline, we used an off-the-shelf statistical MT system, consisting of the ISI ReWrite Decoder (Germann et al., 2001) together with a translation model produced by GIZA++ (Och and Ney, 2000) and a language model produced by the CMU Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). This system was trained on our corpus only. Lex Only As a third baseline, we used our system with the baseline transfer dictionary as the sole transfer resource. Lex+Induced We compared the three baseline systems against our complete system, using the baseline transfer dictionary augmented with the induced transfer rules. We ran each of the four systems on the test set of 50 Korean sentences described in Section</context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu and Kenji Yamada. 2001. Fast Decoding and Optimal Decoding for Machine Translation. In Proceedings ofACL’01, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chung hye Han</author>
<author>Benoit Lavoie</author>
<author>Martha Palmer</author>
<author>Owen Rambow</author>
<author>Richard Kittredge</author>
<author>Tanya Korelsky</author>
<author>Nari Kim</author>
<author>Myunghee Kim</author>
</authors>
<title>Handling Structural Divergences and Recovering Dropped Arguments in a KoreanEnglish Machine Translation System.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fourth Conference on Machine Translation in the Americas (AMTA’00), Mision Del Sol,</booktitle>
<location>Mexico.</location>
<contexts>
<context position="960" citStr="Han et al., 2000" startWordPosition="135" endWordPosition="138">luation results. The system learns lexico-structural transfer rules using syntactic pattern matching, statistical co-occurrence and errordriven filtering. In an experiment with domainspecific Korean to English translation, the approach yielded substantial improvements over three baseline systems. 1 Introduction In this paper, we describe the design of an MT system that employs transfer rules induced from parsed bitexts and present evaluation results for Korean to English translation. Our approach is based on lexico-structural transfer (Nasr et. al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular. Whereas Han et al. focus on high quality domainspecific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules. The proposed approach is inspired by examplebased machine translation (EBMT; Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992) and is similar to the recent works of (Meyers et al., 1998) and (Richardson et al., 2001) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, while (Meyers et al., 1998) </context>
<context position="3791" citStr="Han et al., 2000" startWordPosition="584" endWordPosition="587">ap-accusative + again + look-imp (D1) {po} [class=vbma ente={ra}] ( s1 {ci-to} [class=nnin2 ppca={reul}] ( s1 {i} [class=ande] ) s1 {ta-si} [class=adco2] ) (S2) Look at the map again. (D2) look [class=verb mood=imp] ( attr at [class=preposition] ( ii map [class=common_noun article=def] ) attr again [class=adverb] ) Figure 1: Syntactic dependency representations for corresponding Korean and English sentences Korean English Avg. sentence size 9.08 13.26 Avg. parse size 8.96 10.77 Table 1: Average sizes for sentences and parses in corpus tions were derived from an English Tree Bank developed in (Han et al., 2000). To enable the surface realization of the English parses via RealPro, we automatically converted the phrase structures of the English Tree Bank into deep-syntactic dependency structures (DSyntSs) of the Meaning-Text Theory (MTT) (Mel’ˇcuk, 1988) using Xia’s converter (Xia and Palmer, 2001) and our own conversion grammars. The realization results of the resulting DSyntSs for our training corpus yielded a unigram and bigram accuracy (f-score) of approximately 95% and 90%, respectively. A DSyntS is an unordered tree where all nodes are meaning-bearing and lexicalized. Since the output of the Yoo</context>
<context position="7018" citStr="Han et al., 2000" startWordPosition="1095" endWordPosition="1098">nning and Schutze, 1999: 172-175). Sample transfer rules are illustrated in Section 4. The simplest transfer rules consist of direct lexical mappings, while the most complex may contain source and target syntactic patterns composed of multiple nodes defined with lexical and/or syntactic features. Each transfer rule is assigned a log likelihood ratio calculated using the training parse set. To create the baseline transfer dictionary for our experiments, we had three bilingual dictionary resources at our disposal: A corpus-based handcrafted dictionary: This dictionary was manually assembled by (Han et al., 2000) for the same corpus used here. Note, Korean English Table 4: Concurrent lexical coverage of training set by baseline dictionary however, that it was developed for different parse representations, and with an emphasis primarily on the lexical coverage of the source parses, rather than the source and target parse pairs. A corpus-based extracted dictionary: This dictionary was automatically created from our corpus by the RALI group from the University of Montreal. Since the extraction heuristics did not handle the rich morphological suffixes of Korean, the extraction results contained inflected </context>
</contexts>
<marker>Han, Lavoie, Palmer, Rambow, Kittredge, Korelsky, Kim, Kim, 2000</marker>
<rawString>Chung hye Han, Benoit Lavoie, Martha Palmer, Owen Rambow, Richard Kittredge, Tanya Korelsky, Nari Kim, and Myunghee Kim. 2000. Handling Structural Divergences and Recovering Dropped Arguments in a KoreanEnglish Machine Translation System. In Proceedings of the Fourth Conference on Machine Translation in the Americas (AMTA’00), Mision Del Sol, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Karp</author>
<author>Yves Schabes</author>
</authors>
<title>A Wide Coverage Public Domain Morphological Analyzer for English.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Computational Linguistics (COLING’92),</booktitle>
<pages>950--955</pages>
<location>Nantes, France.</location>
<contexts>
<context position="7992" citStr="Karp and Schabes, 1992" startWordPosition="1242" endWordPosition="1245">onary: This dictionary was automatically created from our corpus by the RALI group from the University of Montreal. Since the extraction heuristics did not handle the rich morphological suffixes of Korean, the extraction results contained inflected words rather than lexemes. A wide coverage dictionary: This dictionary of 70,300 entries was created by Systran, without regard to our corpus. We processed and combined these resources as follows: • First, we replaced the inflected words with uninflected lexemes using Yoon’s morphological analyzer and a wide coverage English morphological database (Karp and Schabes, 1992). • Second, we merged all morphologically analyzed entries after removing all non-lexical features, since these features generally did not match those found in the parses. • Third, we matched the resulting transfer dictionary entries with the training parse set, in order to determine for each entry all possible part-ofspeech instantiations and dependency relationships. For each distinct instantiation, we calculated a log likelihood ratio. • Finally, we created a baseline dictionary using the instantiated rules whose source patterns had the best log likelihood ratios. Table 4 illustrates the co</context>
</contexts>
<marker>Karp, Schabes, 1992</marker>
<rawString>Daniel Karp and Yves Schabes. 1992. A Wide Coverage Public Domain Morphological Analyzer for English. In Proceedings of the Fifteenth International Conference on Computational Linguistics (COLING’92), pages 950–955, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Lavoie</author>
<author>Owen Rambow</author>
</authors>
<title>RealPro – A Fast, Portable Sentence Realizer.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference on Applied Natural Language Processing (ANLP’97),</booktitle>
<location>Washington, DC.</location>
<contexts>
<context position="2607" citStr="Lavoie and Rambow, 1997" startWordPosition="395" endWordPosition="398"> (Lavoie et. al., 2001). 2 Overall Runtime System Design Our Korean to English MT runtime system relies on the following off-the-shelf software components: Korean parser For parsing, we used the wide coverage syntactic dependency parser for Korean developed by (Yoon et al., 1997). The parser was not trained on our corpus. Transfer component For transfer of the Korean parses to English structures, we used the same lexico-structural transfer framework as (Lavoie et al., 2000). Realizer For surface realization of the transferred English syntactic structures, we used the RealPro English realizer (Lavoie and Rambow, 1997). The training of the system is described in the next two sections. 3 Data Preparation 3.1 Parses for the Bitexts In our experiments, we used a parallel corpus derived from bilingual training manuals provided by the U.S. Defense Language Institute. The corpus consists of a Korean dialog of 4,183 sentences about battle scenario message traffic and their English human translations. The parses for the Korean sentences were obtained using Yoon’s parser, as in the runtime system. The parses for the English human transla(S1) {i} {Ci-To-Reul} {Ta-Si} {Po-Ra}. this + map-accusative + again + look-imp </context>
</contexts>
<marker>Lavoie, Rambow, 1997</marker>
<rawString>Benoit Lavoie and Owen Rambow. 1997. RealPro – A Fast, Portable Sentence Realizer. In Proceedings of the Conference on Applied Natural Language Processing (ANLP’97), Washington, DC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Lavoie</author>
<author>Richard Kittredge</author>
<author>Tanya Korelsky</author>
<author>Owen Rambow</author>
</authors>
<title>A framework for MT and multilingual NLG systems based on uniform lexico-structural processing.</title>
<date>2000</date>
<booktitle>In Proceedings ofANLP/NAACL 2000,</booktitle>
<location>Seattle, Washington.</location>
<contexts>
<context position="2461" citStr="Lavoie et al., 2000" startWordPosition="374" endWordPosition="377">ny source and target tree sub-patterns matching a subset of the parse features. A more detailed description of the differences can be found in (Lavoie et. al., 2001). 2 Overall Runtime System Design Our Korean to English MT runtime system relies on the following off-the-shelf software components: Korean parser For parsing, we used the wide coverage syntactic dependency parser for Korean developed by (Yoon et al., 1997). The parser was not trained on our corpus. Transfer component For transfer of the Korean parses to English structures, we used the same lexico-structural transfer framework as (Lavoie et al., 2000). Realizer For surface realization of the transferred English syntactic structures, we used the RealPro English realizer (Lavoie and Rambow, 1997). The training of the system is described in the next two sections. 3 Data Preparation 3.1 Parses for the Bitexts In our experiments, we used a parallel corpus derived from bilingual training manuals provided by the U.S. Defense Language Institute. The corpus consists of a Korean dialog of 4,183 sentences about battle scenario message traffic and their English human translations. The parses for the Korean sentences were obtained using Yoon’s parser, </context>
</contexts>
<marker>Lavoie, Kittredge, Korelsky, Rambow, 2000</marker>
<rawString>Benoit Lavoie, Richard Kittredge, Tanya Korelsky, and Owen Rambow. 2000. A framework for MT and multilingual NLG systems based on uniform lexico-structural processing. In Proceedings ofANLP/NAACL 2000, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Lavoie</author>
<author>Michael White</author>
<author>Tanya Korelsky</author>
</authors>
<title>Inducing Lexico-Structural Transfer Rules from Parsed Bitexts.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL 2001 Workshop on Data-driven Machine Translation,</booktitle>
<pages>17--24</pages>
<location>Toulouse, France.</location>
<marker>Lavoie, White, Korelsky, 2001</marker>
<rawString>Benoit Lavoie, Michael White, and Tanya Korelsky. 2001. Inducing Lexico-Structural Transfer Rules from Parsed Bitexts. In Proceedings of the ACL 2001 Workshop on Data-driven Machine Translation, pages 17–24, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Hinrich Schutze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing.</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="6424" citStr="Manning and Schutze, 1999" startWordPosition="1006" endWordPosition="1009">entential punctuation. We divided this higher quality subset into training and test sets. For the test set, we randomly selected 50 parse pairs containing at least 5 nodes each. For the training set, we used the remaining 1,433 parse pairs. The average sentence lengths and parse sizes for the training and test sets are represented in Tables 2 and 3. 3.3 Creating the Baseline Transfer Dictionary In our system, transfer dictionaries contain Korean to English lexico-structural transfer rules defined using the formalism described in (Nasr et. al., 1997), extended to include log likelihood ratios (Manning and Schutze, 1999: 172-175). Sample transfer rules are illustrated in Section 4. The simplest transfer rules consist of direct lexical mappings, while the most complex may contain source and target syntactic patterns composed of multiple nodes defined with lexical and/or syntactic features. Each transfer rule is assigned a log likelihood ratio calculated using the training parse set. To create the baseline transfer dictionary for our experiments, we had three bilingual dictionary resources at our disposal: A corpus-based handcrafted dictionary: This dictionary was manually assembled by (Han et al., 2000) for t</context>
</contexts>
<marker>Manning, Schutze, 1999</marker>
<rawString>Christopher D. Manning and Hinrich Schutze. 1999. Foundations of Statistical Natural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Maruyama</author>
<author>H Watanabe</author>
</authors>
<title>Tree cover Search Algorithm for Example-Based Translation.</title>
<date>1992</date>
<booktitle>In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI’92),</booktitle>
<pages>173--184</pages>
<contexts>
<context position="1326" citStr="Maruyama and Watanabe, 1992" startWordPosition="192" endWordPosition="195">n MT system that employs transfer rules induced from parsed bitexts and present evaluation results for Korean to English translation. Our approach is based on lexico-structural transfer (Nasr et. al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular. Whereas Han et al. focus on high quality domainspecific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules. The proposed approach is inspired by examplebased machine translation (EBMT; Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992) and is similar to the recent works of (Meyers et al., 1998) and (Richardson et al., 2001) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, while (Meyers et al., 1998) and (Richardson et al., 2001) only consider parses and rules with lexical labels and syntactic roles, our approach uses parses containing any syntactic information provided by parsers (lexical labels, syntactic roles, tense, number, person, etc.), and derives rules consisting of any source and target tree sub-patterns matching a subset of the parse features. A mor</context>
</contexts>
<marker>Maruyama, Watanabe, 1992</marker>
<rawString>H. Maruyama and H. Watanabe. 1992. Tree cover Search Algorithm for Example-Based Translation. In Proceedings of the Fourth International Conference on Theoretical and Methodological Issues in Machine Translation (TMI’92), pages 173–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
<author>Hiroyuki Hishimoto</author>
<author>Takehito Utsuro</author>
</authors>
<title>Structural Matching of Parallel Texts.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meetings of the Association for Computational Linguistics (ACL’93),</booktitle>
<pages>23--30</pages>
<marker>Matsumoto, Hishimoto, Utsuro, 1993</marker>
<rawString>Yuji Matsumoto, Hiroyuki Hishimoto, and Takehito Utsuro. 1993. Structural Matching of Parallel Texts. In Proceedings of the 31st Annual Meetings of the Association for Computational Linguistics (ACL’93), pages 23–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax.</title>
<date>1988</date>
<publisher>State University of New York Press,</publisher>
<location>Albany, NY.</location>
<marker>Mel’ˇcuk, 1988</marker>
<rawString>Igor Mel’ˇcuk. 1988. Dependency Syntax. State University of New York Press, Albany, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
<author>Catherine Macleod</author>
<author>Antonio Moreno-Sandoval</author>
</authors>
<title>Deriving Transfer Rules from Dominance-Preserving Alignments.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL’98,</booktitle>
<pages>843--847</pages>
<contexts>
<context position="1386" citStr="Meyers et al., 1998" startWordPosition="204" endWordPosition="208">and present evaluation results for Korean to English translation. Our approach is based on lexico-structural transfer (Nasr et. al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular. Whereas Han et al. focus on high quality domainspecific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules. The proposed approach is inspired by examplebased machine translation (EBMT; Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992) and is similar to the recent works of (Meyers et al., 1998) and (Richardson et al., 2001) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, while (Meyers et al., 1998) and (Richardson et al., 2001) only consider parses and rules with lexical labels and syntactic roles, our approach uses parses containing any syntactic information provided by parsers (lexical labels, syntactic roles, tense, number, person, etc.), and derives rules consisting of any source and target tree sub-patterns matching a subset of the parse features. A more detailed description of the differences can be found in (L</context>
</contexts>
<marker>Meyers, Yangarber, Grishman, Macleod, Moreno-Sandoval, 1998</marker>
<rawString>Adam Meyers, Roman Yangarber, Ralph Grishman, Catherine Macleod, and Antonio Moreno-Sandoval. 1998. Deriving Transfer Rules from Dominance-Preserving Alignments. In Proceedings of COLING-ACL’98, pages 843–847.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Nagao</author>
</authors>
<title>A framework of a mechanical translation between Japenese and English by analogy principle.</title>
<date>1984</date>
<editor>In A. Elithorn and R. Banerji, editors,</editor>
<publisher>NATO Publications.</publisher>
<contexts>
<context position="1274" citStr="Nagao, 1984" startWordPosition="186" endWordPosition="187"> paper, we describe the design of an MT system that employs transfer rules induced from parsed bitexts and present evaluation results for Korean to English translation. Our approach is based on lexico-structural transfer (Nasr et. al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular. Whereas Han et al. focus on high quality domainspecific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules. The proposed approach is inspired by examplebased machine translation (EBMT; Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992) and is similar to the recent works of (Meyers et al., 1998) and (Richardson et al., 2001) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, while (Meyers et al., 1998) and (Richardson et al., 2001) only consider parses and rules with lexical labels and syntactic roles, our approach uses parses containing any syntactic information provided by parsers (lexical labels, syntactic roles, tense, number, person, etc.), and derives rules consisting of any source and target tree sub-pat</context>
</contexts>
<marker>Nagao, 1984</marker>
<rawString>Makoto Nagao. 1984. A framework of a mechanical translation between Japenese and English by analogy principle. In A. Elithorn and R. Banerji, editors, Artificial and Human Intelligence. NATO Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
<author>Martha Palmer</author>
<author>Joseph Rosenzweig</author>
</authors>
<title>Enriching lexical transfer with crosslinguistic semantic features.</title>
<date>1997</date>
<booktitle>In Proceedings of the Interlingua Workshop at the MT Summit,</booktitle>
<location>San Diego, California.</location>
<marker>Nasr, Rambow, Palmer, Rosenzweig, 1997</marker>
<rawString>Alexis Nasr, Owen Rambow, Martha Palmer, and Joseph Rosenzweig. 1997. Enriching lexical transfer with crosslinguistic semantic features. In Proceedings of the Interlingua Workshop at the MT Summit, San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Statistical Alignment Models.</title>
<date>2000</date>
<booktitle>In Proceedings of ACL’00,</booktitle>
<pages>440--447</pages>
<location>Hongkong, China.</location>
<contexts>
<context position="18868" citStr="Och and Ney, 2000" startWordPosition="3002" endWordPosition="3005">component. Microsoft SQL Server was used to count and deduplicate the rule candidates. The data preparation and induction processes took about 12 hours on a 300 MHz PC with 256 MB RAM. 5 Evaluation 5.1 Systems Compared Babelfish As a first baseline, we used Babelfish from Systran, a commercial large coverage MT system supporting Korean to English translation. This system was not trained on our corpus. GIZA++/RW As a second baseline, we used an off-the-shelf statistical MT system, consisting of the ISI ReWrite Decoder (Germann et al., 2001) together with a translation model produced by GIZA++ (Och and Ney, 2000) and a language model produced by the CMU Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997). This system was trained on our corpus only. Lex Only As a third baseline, we used our system with the baseline transfer dictionary as the sole transfer resource. Lex+Induced We compared the three baseline systems against our complete system, using the baseline transfer dictionary augmented with the induced transfer rules. We ran each of the four systems on the test set of 50 Korean sentences described in Section 3.2 and compared the resulting translations using the automatic evaluati</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved Statistical Alignment Models. In Proceedings of ACL’00, pages 440– 447, Hongkong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>Bleu: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2001</date>
<booktitle>In IBM technical report, RC22176.</booktitle>
<contexts>
<context position="19632" citStr="Papineni et al., 2001" startWordPosition="3125" endWordPosition="3128"> corpus only. Lex Only As a third baseline, we used our system with the baseline transfer dictionary as the sole transfer resource. Lex+Induced We compared the three baseline systems against our complete system, using the baseline transfer dictionary augmented with the induced transfer rules. We ran each of the four systems on the test set of 50 Korean sentences described in Section 3.2 and compared the resulting translations using the automatic evaluation and the human evaluation described below. 5.2 Automatic Evaluation Results For the automatic evaluation, we used the Bleu metric from IBM (Papineni et al., 2001). The Bleu metric combines several modified N-gram precision measures (N = 1 to 4), and uses brevity penalties to penalize translations that are shorter than the reference sentences. Table 5 shows the Bleu N-gram precision scores for each of the four systems. Our system (Lex+Induced) had better precision scores than each of the baseline systems, except in the case of 4- grams, where it slightly trailed Babelfish. The statistical baseline system (GIZA++/RW) performed poorly, as might have been expected given the small amount of training data. Table 6 shows the Bleu overall precision scores. Our</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2001</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2001. Bleu: a Method for Automatic Evaluation of Machine Translation. In IBM technical report, RC22176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen D Richardson</author>
<author>William B Dolan</author>
<author>Arul Menezes</author>
<author>Monica Corston-Oliver</author>
</authors>
<title>Overcoming the Customization Bottleneck using Example-based MT.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL 2001 Workshop on Data-driven Machine Translation,</booktitle>
<pages>9--16</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="1416" citStr="Richardson et al., 2001" startWordPosition="210" endWordPosition="213">ults for Korean to English translation. Our approach is based on lexico-structural transfer (Nasr et. al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular. Whereas Han et al. focus on high quality domainspecific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules. The proposed approach is inspired by examplebased machine translation (EBMT; Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992) and is similar to the recent works of (Meyers et al., 1998) and (Richardson et al., 2001) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, while (Meyers et al., 1998) and (Richardson et al., 2001) only consider parses and rules with lexical labels and syntactic roles, our approach uses parses containing any syntactic information provided by parsers (lexical labels, syntactic roles, tense, number, person, etc.), and derives rules consisting of any source and target tree sub-patterns matching a subset of the parse features. A more detailed description of the differences can be found in (Lavoie et. al., 2001). 2 Overal</context>
</contexts>
<marker>Richardson, Dolan, Menezes, Corston-Oliver, 2001</marker>
<rawString>Stephen D. Richardson, William B. Dolan, Arul Menezes, and Monica Corston-Oliver. 2001. Overcoming the Customization Bottleneck using Example-based MT. In Proceedings of the ACL 2001 Workshop on Data-driven Machine Translation, pages 9–16, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sato</author>
<author>Makoto Nagao</author>
</authors>
<title>Toward Memorybased Translation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics (COLING’90),</booktitle>
<pages>247--252</pages>
<contexts>
<context position="1296" citStr="Sato and Nagao, 1990" startWordPosition="188" endWordPosition="191">scribe the design of an MT system that employs transfer rules induced from parsed bitexts and present evaluation results for Korean to English translation. Our approach is based on lexico-structural transfer (Nasr et. al., 1997), and extends recent work reported in (Han et al., 2000) about Korean to English transfer in particular. Whereas Han et al. focus on high quality domainspecific translation using handcrafted transfer rules, in this work we instead focus on automating the acquisition of such rules. The proposed approach is inspired by examplebased machine translation (EBMT; Nagao, 1984; Sato and Nagao, 1990; Maruyama and Watanabe, 1992) and is similar to the recent works of (Meyers et al., 1998) and (Richardson et al., 2001) where transfer rules are also derived after aligning the source and target nodes of corresponding parses. However, while (Meyers et al., 1998) and (Richardson et al., 2001) only consider parses and rules with lexical labels and syntactic roles, our approach uses parses containing any syntactic information provided by parsers (lexical labels, syntactic roles, tense, number, person, etc.), and derives rules consisting of any source and target tree sub-patterns matching a subse</context>
</contexts>
<marker>Sato, Nagao, 1990</marker>
<rawString>Satoshi Sato and Makoto Nagao. 1990. Toward Memorybased Translation. In Proceedings of the 13th International Conference on Computational Linguistics (COLING’90), pages 247–252.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Martha Palmer</author>
</authors>
<title>Converting Dependency Structures to Phrase Structures.</title>
<date>2001</date>
<booktitle>In Notes of the First Human Language Technology Conference,</booktitle>
<location>San Diego, California.</location>
<contexts>
<context position="4082" citStr="Xia and Palmer, 2001" startWordPosition="628" endWordPosition="631">) attr again [class=adverb] ) Figure 1: Syntactic dependency representations for corresponding Korean and English sentences Korean English Avg. sentence size 9.08 13.26 Avg. parse size 8.96 10.77 Table 1: Average sizes for sentences and parses in corpus tions were derived from an English Tree Bank developed in (Han et al., 2000). To enable the surface realization of the English parses via RealPro, we automatically converted the phrase structures of the English Tree Bank into deep-syntactic dependency structures (DSyntSs) of the Meaning-Text Theory (MTT) (Mel’ˇcuk, 1988) using Xia’s converter (Xia and Palmer, 2001) and our own conversion grammars. The realization results of the resulting DSyntSs for our training corpus yielded a unigram and bigram accuracy (f-score) of approximately 95% and 90%, respectively. A DSyntS is an unordered tree where all nodes are meaning-bearing and lexicalized. Since the output of the Yoon parser is quite similar, we have used its output as is. The syntactic dependency representations for two corresponding Korean1 and English sentences are shown in Figure 1. 3.2 Training and Test Sets of Parse Pairs The average sentence lengths (in words) and parse sizes (in nodes) for the </context>
</contexts>
<marker>Xia, Palmer, 2001</marker>
<rawString>Fei Xia and Martha Palmer. 2001. Converting Dependency Structures to Phrase Structures. In Notes of the First Human Language Technology Conference, San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juntae Yoon</author>
<author>Seonho Kim</author>
<author>Mansuk Song</author>
</authors>
<title>New parsing method using global association table.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th International Workshop on Parsing Technology.</booktitle>
<contexts>
<context position="2263" citStr="Yoon et al., 1997" startWordPosition="343" endWordPosition="346">syntactic roles, our approach uses parses containing any syntactic information provided by parsers (lexical labels, syntactic roles, tense, number, person, etc.), and derives rules consisting of any source and target tree sub-patterns matching a subset of the parse features. A more detailed description of the differences can be found in (Lavoie et. al., 2001). 2 Overall Runtime System Design Our Korean to English MT runtime system relies on the following off-the-shelf software components: Korean parser For parsing, we used the wide coverage syntactic dependency parser for Korean developed by (Yoon et al., 1997). The parser was not trained on our corpus. Transfer component For transfer of the Korean parses to English structures, we used the same lexico-structural transfer framework as (Lavoie et al., 2000). Realizer For surface realization of the transferred English syntactic structures, we used the RealPro English realizer (Lavoie and Rambow, 1997). The training of the system is described in the next two sections. 3 Data Preparation 3.1 Parses for the Bitexts In our experiments, we used a parallel corpus derived from bilingual training manuals provided by the U.S. Defense Language Institute. The cor</context>
</contexts>
<marker>Yoon, Kim, Song, 1997</marker>
<rawString>Juntae Yoon, Seonho Kim, and Mansuk Song. 1997. New parsing method using global association table. In Proceedings of the 5th International Workshop on Parsing Technology.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>