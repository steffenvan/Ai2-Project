<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000163">
<title confidence="0.9960125">
Nonparametric Bayesian Inference and Efficient Parsing for
Tree-adjoining Grammars
</title>
<author confidence="0.981993">
Elif Yamangil and Stuart M. Shieber
</author>
<affiliation confidence="0.982223">
Harvard University
</affiliation>
<address confidence="0.706709">
Cambridge, Massachusetts, USA
</address>
<email confidence="0.999463">
{elif,shieber}@seas.harvard.edu
</email>
<sectionHeader confidence="0.993914" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994547368421">
In the line of research extending statis-
tical parsing to more expressive gram-
mar formalisms, we demonstrate for the
first time the use of tree-adjoining gram-
mars (TAG). We present a Bayesian non-
parametric model for estimating a proba-
bilistic TAG from a parsed corpus, along
with novel block sampling methods and
approximation transformations for TAG
that allow efficient parsing. Our work
shows performance improvements on the
Penn Treebank and finds more compact
yet linguistically rich representations of
the data, but more importantly provides
techniques in grammar transformation and
statistical inference that make practical
the use of these more expressive systems,
thereby enabling further experimentation
along these lines.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999938654545455">
There is a deep tension in statistical modeling of
grammatical structure between providing good ex-
pressivity — to allow accurate modeling of the
data with sparse grammars — and low complexity
— making induction of the grammars (say, from
a treebank) and parsing of novel sentences com-
putationally practical. Tree-substitution grammars
(TSG), by expanding the domain of locality of
context-free grammars (CFG), can achieve better
expressivity, and the ability to model more con-
textual dependencies; the payoff would be better
modeling of the data or smaller (sparser) models
or both. For instance, constructions that go across
levels, like the predicate-argument structure of a
verb and its arguments can be modeled by TSGs
(Goodman, 2003).
Recent work that incorporated Dirichlet pro-
cess (DP) nonparametric models into TSGs has
provided an efficient solution to the daunting
model selection problem of segmenting training
data trees into appropriate elementary fragments
to form the grammar (Cohn et al., 2009; Post and
Gildea, 2009). The elementary trees combined in
a TSG are, intuitively, primitives of the language,
yet certain linguistic phenomena (notably various
forms of modification) “split them up”, preventing
their reuse, leading to less sparse grammars than
might be ideal (Yamangil and Shieber, 2012; Chi-
ang, 2000; Resnik, 1992).
TSGs are a special case of the more flexible
grammar formalism of tree adjoining grammar
(TAG) (Joshi et al., 1975). TAG augments TSG
with an adjunction operator and a set of auxil-
iary trees in addition to the substitution operator
and initial trees of TSG, allowing for “splicing in”
of syntactic fragments within trees. This func-
tionality allows for better modeling of linguistic
phenomena such as the distinction between modi-
fiers and arguments (Joshi et al., 1975; XTAG Re-
search Group, 2001). Unfortunately, TAG’s ex-
pressivity comes at the cost of greatly increased
complexity. Parsing complexity for unconstrained
TAG scales as O(n6), impractical as compared to
CFG and TSG’s O(n3). In addition, the model
selection problem for TAG is significantly more
complicated than for TSG since one must reason
about many more combinatorial options with two
types of derivation operators. This has led re-
searchers to resort to manual (Doran et al., 1997)
or heuristic techniques. For example, one can con-
sider “outsourcing” the auxiliary trees (Shieber,
2007), use template rules and a very small num-
ber of grammar categories (Hwa, 1998), or rely
on head-words and force lexicalization in order to
constrain the problem (Xia et al., 2001; Chiang,
</bodyText>
<page confidence="0.96199">
597
</page>
<note confidence="0.915518">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 597–603,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.999215743589744">
2000; Carreras et al., 2008). However a solution
has not been put forward by which a model that
maximizes a principled probabilistic objective is
sought after.
Recent work by Cohn and Blunsom (2010) ar-
gued that under highly expressive grammars such
as TSGs where exponentially many derivations
may be hypothesized of the data, local Gibbs sam-
pling is insufficient for effective inference and
global blocked sampling strategies will be nec-
essary. For TAG, this problem is only more se-
vere due to its mild context-sensitivity and even
richer combinatorial nature. Therefore in previ-
ous work, Shindo et al. (2011) and Yamangil and
Shieber (2012) used tree-insertion grammar (TIG)
as a kind of expressive compromise between TSG
and TAG, as a substrate on which to build nonpara-
metric inference. However TIG has the constraint
of disallowing wrapping adjunction (coordination
between material that falls to the left and right
of the point of adjunction, such as parentheticals
and quotations) as well as left adjunction along the
spine of a right auxiliary tree and vice versa.
In this work we formulate a blocked sampling
strategy for TAG that is effective and efficient, and
prove its superiority against the local Gibbs sam-
pling approach. We show via nonparametric in-
ference that TAG, which contains TSG as a sub-
set, is a better model for treebank data than TSG
and leads to improved parsing performance. TAG
achieves this by using more compact grammars
than TSG and by providing the ability to make
finer-grained linguistic distinctions. We explain
how our parameter refinement scheme for TAG
allows for cubic-time CFG parsing, which is just
as efficient as TSG parsing. Our presentation as-
sumes familiarity with prior work on block sam-
pling of TSG and TIG (Cohn and Blunsom, 2010;
Shindo et al., 2011; Yamangil and Shieber, 2012).
</bodyText>
<sectionHeader confidence="0.994664" genericHeader="method">
2 Probabilistic Model
</sectionHeader>
<bodyText confidence="0.999800444444445">
In the basic nonparametric TSG model, there is
an independent DP for every grammar category
(such as c = NP), each of which uses a base dis-
tribution P0 that generates an initial tree by mak-
ing stepwise decisions and concentration parame-
ter αc that controls the level of sparsity (size) of
the generated grammars: Gc ∼ DP(αc, P0(·  |c))
We extend this model by adding specialized DPs
for auxiliary trees Gaux c∼ DP(αaux
</bodyText>
<equation confidence="0.734683">
c , P aux
0 (·  |c))
</equation>
<bodyText confidence="0.923292666666667">
Therefore, we have an exchangeable process for
generating auxiliary tree aj given j − 1 auxiliary
trees previously generated
</bodyText>
<equation confidence="0.9916534">
aux p0
nc,a; + αc Poa a�  |c)
p(aj  |a&lt;j) = (1)
j − 1 + αaux
c
</equation>
<bodyText confidence="0.995244344827586">
as for initial trees in TSG (Cohn et al., 2009).
We must define base distributions for initial
trees and auxiliary trees. P0 generates an initial
tree with root label c by sampling rules from a
CFG P˜ and making a binary decision at every
node generated whether to leave it as a frontier
node or further expand (with probability βc) (Cohn
et al., 2009). Similarly, our Paux
0 generates an aux-
iliary tree with root label c by sampling a CFG rule
from P˜, flipping an unbiased coin to decide the di-
rection of the spine (if more than a unique child
was generated), making a binary decision at the
spine whether to leave it as a foot node or further
expand (with probability γc), and recurring into P0
or Paux
0 appropriately for the off-spine and spinal
children respectively.
We glue these two processes together via a set
of adjunction parameters µc. In any derivation for
every node labeled c that is not a frontier node
or the root or foot node of an auxiliary tree, we
determine the number (perhaps zero) of simulta-
neous adjunctions (Schabes and Shieber, 1994)
by sampling a Geometric(µc) variable; thus k si-
multaneous adjunctions would have probability
(µc)k(1 − µc). Since we already provide simul-
taneous adjunction we disallow adjunction at the
root of auxiliary trees.
</bodyText>
<sectionHeader confidence="0.999641" genericHeader="method">
3 Inference
</sectionHeader>
<bodyText confidence="0.99765705">
Given this model, our inference task is to ex-
plore posterior derivations underlying the data.
Since TAG derivations are highly structured ob-
jects, we design a blocked Metropolis-Hastings
sampler that samples derivations per entire parse
trees all at once in a joint fashion (Cohn and Blun-
som, 2010; Shindo et al., 2011; Yamangil and
Shieber, 2012). As in previous work, we use a
Goodman-transformed TAG as our proposal dis-
tribution (Goodman, 2003) that incorporates ad-
ditional CFG rules to account for the possibil-
ity of backing off to the infinite base distribution
Paux
0 , and use the parsing algorithm described by
Shieber et al. (1995) for computing inside proba-
bilities under this TAG model.
The algorithm is illustrated in Table 1 along
with Figure 1. Inside probabilities are computed
in a bottom-up fashion and a TAG derivation is
sampled top-down (Johnson et al., 2007). The
</bodyText>
<page confidence="0.980574">
598
</page>
<figure confidence="0.9917545">
6
0
</figure>
<figureCaption confidence="0.999685">
Figure 1: Example used for illustrating blocked
</figureCaption>
<bodyText confidence="0.959803909090909">
sampling with TAG. On the left hand side we have
a partial training tree where we highlight the par-
ticular nodes (with node labels 0, 1, 2, 3, 4) that the
sampling algorithm traverses in post-order. On the
right hand side is the TAG grammar fragment that
is used to parse these particular nodes: one initial
tree and two wrapping auxiliary trees where one
adjoins into the spine of the other for full general-
ity of our illustration. Grammar nodes are labeled
with their Goodman indices (letters i, j, k,l, m).
Greek letters α, β, γ, S denote entire subtrees. We
assume that a subtree in an auxiliary tree (e.g., α)
parses the same subtree in a training tree.
sampler visits every node of the tree in post-order
(O(n) operations, n being the number of nodes),
visits every node below it as a potential foot (an-
other O(n) operations), visits every mid-node in
the path between the original node and the poten-
tial foot (if spine-adjunction is allowed) (O(log n)
operations), and forms the appropriate chart items.
The complexity is O(n2 log n) if spine-adjunction
is allowed, O(n2) otherwise.
</bodyText>
<sectionHeader confidence="0.990849" genericHeader="method">
4 Parameter Refinement
</sectionHeader>
<bodyText confidence="0.999979111111111">
During inference, adjunction probabilities are
treated simplistically to facilitate convergence.
Only two parameters guide adjunction: µc, the
probability of adjunction; and p(aj  |a&lt;j, c) (see
Equation 1), the probability of the particular aux-
iliary tree being adjoined given that there is an
adjunction. In all of this treatment, c, the con-
text of an adjunction, is the grammar category la-
bel such as S or NP, instead of a unique identi-
fier for the node at which the adjunction occurs as
was originally the case in probabilistic TAG liter-
ature. However it is possible to experiment with
further refinement schemes at parsing time. Once
the sampler converges on a grammar, we can re-
estimate its adjunction probabilities. Using the
O(n6) parsing algorithm (Shieber et al., 1995) we
experimented with various refinements schemes
— ranging from full node identifiers, to Goodman
</bodyText>
<figure confidence="0.990079055555556">
Chart item Why made? Inside probability
Ni[4] By assumption. −
Nk[3-4] N∗[4] and β (1 − µc) x π(β)
Nm[2-3] N∗[3] and δ (1 − µc) x π(δ)
Nl[1-3] γ and Nm[2-3] (1 − µc) x π(γ)
xπ(Nm[2-3])
Naux[1-3] Nl[1-3] nc,al /(nc + αaux
c )
xπ(Nl[1-3])
Nk[1-4] Naux[1-3] and Nk[3-4] µc x π(Naux[1-3])
xπ(Nk[3-4])
Nj[0-4] α and Nk[1-4] (1 − µc) x π(α)
xπ(Nk[1-4])
Naux[0-4] Nj[0-4] nc,aj /(nc + αaux
c )
xπ(Nj[0-4])
Ni[0] Naux[0-4] and Ni[4] µc x π(Naux[0-4])
xπ(Ni[4])
</figure>
<tableCaption confidence="0.716563666666667">
Table 1: Computation of inside probabilities for
TAG sampling. We create two types of chart
items: (1) per-node, e.g., NZ[v] denoting the
</tableCaption>
<bodyText confidence="0.99638925">
probability of starting at an initial subtree that
has Goodman index i and generating the subtree
rooted at node v, and (2) per-path, e.g., Nj[v-η]
denoting the probability of starting at an auxiliary
subtree that has Goodman index j and generating
the subtree rooted at v minus the subtree rooted
at η. Above, c denotes the context of adjunction,
which is the nonterminal label of the node of ad-
junction (here, N), µc is the probability of adjunc-
tion, nc,a is the count of the auxiliary tree a, and
nc = Ea nc,a is total number of adjunctions at
context c. The function π(·) retrieves the inside
probability corresponding to an item.
index identifiers of the subtree below the adjunc-
tion (Hwa, 1998), to simple grammar category la-
bels — and find that using Goodman index identi-
fiers as c is the best performing option.
Interestingly, this particular refinement scheme
also allows for fast cubic-time parsing, which we
achieve by approximating the TAG by a TSG with
little loss of coverage (no loss of coverage under
special conditions which we find that are often sat-
isfied) and negligible increase in grammar size, as
discussed in the next section.
</bodyText>
<sectionHeader confidence="0.993071" genericHeader="method">
5 Cubic-time parsing
</sectionHeader>
<bodyText confidence="0.931033083333333">
MCMC training results in a list of sufficient statis-
tics of the final derivation that the TAG sampler
converges upon after a number of iterations. Basi-
cally, these are the list of initial and auxiliary trees,
their cumulative counts over the training data, and
their adjunction statistics. An adjunction statistic
is listed as follows. If α is any elementary tree, and
β is an auxiliary tree that adjoins n times at node v
of α that is uniquely reachable at path p, we write
p
α ← β (n times). We denote v alternatively as
α[p].
</bodyText>
<figure confidence="0.987850833333333">
α
y
J
0
y
α 1
</figure>
<page confidence="0.745224">
599
</page>
<figureCaption confidence="0.9947665">
Figure 2: TAG to TSG transformation algorithm. By removing adjunctions in the correct order we end
up with a larger yet adjunction-free TSG.
</figureCaption>
<figure confidence="0.993518433333333">
Y *
ai
(1) (2) i (3)
P
i
a
i
m
i
j
Yi
Yj
j
i
q
j
Rij i
Ri
i
n
q
P
*
k
R
m
a
Y *
Ri i k
q
</figure>
<bodyText confidence="0.8904835">
Now imagine that we end up with a small gram-
mar that consists of one initial tree α and two aux-
iliary trees β and γ, and the following adjunctions
occurring between them
</bodyText>
<equation confidence="0.898715">
p
α β (n times)
p
</equation>
<bodyText confidence="0.984934030769231">
α γ (m times)
β q γ (k times)
as shown in Figure 2. Assume that α itself occurs
l &gt; n + m times in total so that there is nonzero
probability of no adjunction anywhere within α.
Also assume that the node uniquely identified by
α[p] has Goodman index i, which we denote as
i = G(α[p]).
The general idea of this TAG-TSG approxima-
tion is that, for any auxiliary tree that adjoins at a
node ν with Goodman index i, we create an ini-
tial tree out of it where the root and foot nodes of
the auxiliary tree are both replaced by i. Further,
we split the subtree rooted at ν from its parent and
rename the substitution site that is newly created
at ν as i as well. (See Figure 2.) We can sep-
arate the foot subtree from the rest of the initial
tree since it is completely remembered by any ad-
joined auxiliary trees due to the nature of our re-
finement scheme. However this method fails for
adjunctions that occur at spinal nodes of auxiliary
trees that have foot nodes below them since we
would not know in which order to do the initial
tree creation. However when the spine-adjunction
relation is amenable to a topological sort (as is the
case in Figure 2), we can apply the method by go-
ing in this order and doing some extra bookkeep-
ing: updating the list of Goodman indices and re-
directing adjunctions as we go along. When there
is no such topological sort, we can approximate
the TAG by heuristically dropping low-frequency
adjunctions that introduce cycles.1
The algorithm is illustrated in Figure 2. In (1)
we see the original TAG grammar and its adjunc-
tions (n, m, k are adjunction counts). Note that
the adjunction relation has a topological sort of
α, β, γ. We process auxiliary trees in this order
and iteratively remove their adjunctions by creat-
ing specialized initial tree duplicates. In (2) we
first visit β, which has adjunctions into α at the
node denoted α[p] where p is the unique path from
the root to this node. We retrieve the Goodman in-
dex of this node i = G(α[p]), split the subtree
rooted at this node as a new initial tree αi, relabel
its root as i, and rename the newly-created sub-
stitution site at α[p] as i. Since β has only this
adjunction, we replace it with initial tree version
βi where root/foot labels of β are replaced with
i, and update all adjunctions into β as being into
βi. In (3) we visit γ which now has adjunctions
into α and βi. For the α[p] adjunction we create γi
the same way we created βi but this time we can-
not remove γ as it still has an adjunction into βi.
We retrieve the Goodman index of the node of ad-
junction j = G(βi[q]), split the subtree rooted at
this node as new initial tree βij, relabel its root
as j, and rename the newly-created substitution
site at βi[q] as j. Since γ now has only this ad-
junction left, we remove it by also creating initial
tree version γj where root/foot labels of γ are re-
placed with j. At this point we have an adjunction-
free TSG with elementary trees (and counts)
α(l),αi(l),βi(n),βij(n),γi(m),γj(k) where l is
the count of initial tree α. These counts, when they
are normalized, lead to the appropriate adjunc-
</bodyText>
<footnote confidence="0.9074784">
1We found that, on average, about half of our grammars
have a topological sort of their spine-adjunctions. (On aver-
age fewer than 100 spine adjunctions even exist.) When no
such sort exists, only a few low-frequency adjunctions have
to be removed to eliminate cycles.
</footnote>
<page confidence="0.984406">
600
</page>
<figure confidence="0.996499416666667">
Parsing time (seconds)
40
35
30
25
20
15
10
5
0
0 10 20 30 40 50 60
Sentence length (#tokens)
</figure>
<figureCaption confidence="0.776764333333333">
Figure 3: Nonparametric TAG (blue) parsing is ef-
ficient and incurs only a small increase in parsing
time compared to nonparametric TSG (red).
</figureCaption>
<bodyText confidence="0.992813111111111">
tion probability refinement scheme of µc × p(aj |
a&lt;j, c) where c is the Goodman index.
Although this algorithm increases grammar
size, the sparsity of the nonparametric solution
ensures that the increase is almost negligible: on
average the final Goodman-transformed CFG has
173.9K rules for TSG, 189.2K for TAG. Figure 3
demonstrates the comparable Viterbi parsing times
for TSG and TAG.
</bodyText>
<sectionHeader confidence="0.996399" genericHeader="evaluation">
6 Evaluation
</sectionHeader>
<bodyText confidence="0.999906">
We use the standard Penn treebank methodology
of training on sections 2–21 and testing on section
23. All our data is head-binarized, all hyperpa-
rameters are resampled under appropriate vague
gamma and beta priors. Samplers are run 1000
iterations each; all reported numbers are aver-
ages over 5 runs. For simplicity, parsing results
are based on the maximum probability derivation
(Viterbi algorithm).
In Table 4, we compare TAG inference
schemes and TSG. TAGGibbs operates by locally
adding/removing potential adjunctions, similar to
Cohn et al. (2009). TAG&apos; is the O(n2) algorithm
that disallows spine adjunction. We see that TAG&apos;
has the best parsing performance, while TAG pro-
vides the most compact representation.
</bodyText>
<table confidence="0.867339333333333">
model F measure # initial trees # auxiliary trees
TSG 84.15 69.5K -
TAGGibbs 82.47 69.9K 1.7K
0 84.87 66.4K 1.5K
TAG
TAG 84.82 66.4K 1.4K
</table>
<figureCaption confidence="0.88303">
Figure 4: EVALB results. Note that the Gibbs
sampler for TAG has poor performance and pro-
vides no grammar compaction due to its lack of
convergence.
</figureCaption>
<table confidence="0.998089769230769">
label #adj ave. #lex. #left #right #wrap
(spine adj) depth trees trees trees trees
VP 4532 (23) 1.06 45 22 65 0
NP 2891 (46) 1.71 68 94 13 1
NN 2160 (3) 1.08 85 16 110 0
NNP 1478 (2) 1.12 90 19 90 0
NNS 1217 (1) 1.10 43 9 60 0
VBN 1121 (1) 1.05 6 18 0 0
VBD 976 (0) 1.0 16 25 0 0
NP 937 (0) 3.0 1 5 0 0
VB 870 (0) 1.02 14 31 4 0
S 823 (11) 1.48 42 36 35 3
total 23320 (118) 1.25 824 743 683 9
</table>
<tableCaption confidence="0.793855333333333">
Table 2: Grammar analysis for an estimated TAG,
categorized by label. Only the most common top
10 are shown, binarization variables are denoted
</tableCaption>
<figureCaption confidence="0.8950354">
with overline. A total number of 98 wrapping
adjunctions (9 unique wrapping trees) and 118
spine adjunctions occur.
Figure 5: Example wrapping trees from estimated
TAGs.
</figureCaption>
<sectionHeader confidence="0.986429" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999986631578948">
We described a nonparametric Bayesian inference
scheme for estimating TAG grammars and showed
the power of TAG formalism over TSG for return-
ing rich, generalizable, yet compact representa-
tions of data. The nonparametric inference scheme
presents a principled way of addressing the diffi-
cult model selection problem with TAG. Our sam-
pler has near quadratic-time efficiency, and our
parsing approach remains context-free allowing
for fast cubic-time parsing, so that our overall
parsing framework is highly scalable.2
There are a number of extensions of this
work: Experimenting with automatically in-
duced adjunction refinements as well as in-
corporating substitution refinements can benefit
Bayesian TAG (Shindo et al., 2012; Petrov et al.,
2006). We are also planning to investigate TAG
for more context-sensitive languages, and syn-
chronous TAG for machine translation.
</bodyText>
<footnote confidence="0.66868275">
2An extensive report of our algorithms and experiments
will be provided in the PhD thesis of the first author (Ya-
mangil, 2013). Our code will be made publicly available at
code.seas.harvard.edu/˜elif.
</footnote>
<figure confidence="0.999179558139535">
ADJP
NP
S
S
-LRB- NP
NP* -RRB-
“
“
ADJP
ADJP* ”
-LRB-
S
S* -RRB-
“
S
S* ”
”
-LRB-
“
-RRB-
”
NP
NNP
NP
NP
-LRB- NP
-LRB- NP* -RRB-
-RRB-
NNP
,NNP
NNP* CC
&amp;
“
NP
NP* ”
NP
NP :
NP
NP* PP
,
“
NNP
”
</figure>
<page confidence="0.987986">
601
</page>
<sectionHeader confidence="0.988889" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998994981818182">
Xavier Carreras, Michael Collins, and Terry Koo.
2008. TAG, dynamic programming, and the percep-
tron for efficient, feature-rich parsing. In Proceed-
ings of the Twelfth Conference on Computational
Natural Language Learning, CoNLL ’08, pages 9–
16, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Meeting on Associa-
tion for Computational Linguistics, ACL ’00, pages
456–463, Morristown, NJ, USA. Association for
Computational Linguistics.
Trevor Cohn and Phil Blunsom. 2010. Blocked in-
ference in Bayesian tree substitution grammars. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ’10, pages 225–230, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In NAACL ’09: Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 548–556, Morristown, NJ, USA. Association
for Computational Linguistics.
Christine Doran, Beth Hockey, Philip Hopely, Joseph
Rosenzweig, Anoop Sarkar, B. Srinivas, Fei Xia,
Alexis Nasr, and Owen Rambow. 1997. Maintain-
ing the forest and burning out the underbrush in xtag.
In Proceedings of the ENVGRAM Workshop.
Joshua Goodman. 2003. Efficient parsing of DOP
with PCFG-reductions. In Rens Bod, Remko Scha,
and Khalil Sima’an, editors, Data-Oriented Parsing.
CSLI Publications, Stanford, CA.
Rebecca Hwa. 1998. An empirical evaluation of
probabilistic lexicalized tree insertion grammars. In
Proceedings of the 17th international conference on
Computational linguistics - Volume 1, pages 557–
563, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Mark Johnson, Thomas Griffiths, and Sharon Gold-
water. 2007. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 139–146, Rochester, New York, April.
Association for Computational Linguistics.
Aravind K. Joshi, Leon S. Levy, and Masako Taka-
hashi. 1975. Tree adjunct grammars. Journal of
Computer and System Sciences, 10(1):136–163.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 433–440,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers, pages
45–48, Suntec, Singapore, August. Association for
Computational Linguistics.
Philip Resnik. 1992. Probabilistic tree-adjoining
grammar as a framework for statistical natural lan-
guage processing. In Proceedings of the 14th con-
ference on Computational linguistics - Volume 2,
COLING ’92, pages 418–424, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Yves Schabes and Stuart M. Shieber. 1994. An
alternative conception of tree-adjoining derivation.
Computational Linguistics, 20(1):91–124. Also
available as cmp-lg/9404001.
Stuart M. Shieber, Yves Schabes, and Fernando C. N.
Pereira. 1995. Principles and implementation of de-
ductive parsing. J. Log. Program., 24(1&amp;2):3–36.
Stuart M. Shieber. 2007. Probabilistic synchronous
tree-adjoining grammars for machine translation:
The argument from bilingual dictionaries. In Dekai
Wu and David Chiang, editors, Proceedings of the
Workshop on Syntax and Structure in Statistical
Translation, Rochester, New York, 26 April.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2011. Insertion operator for Bayesian tree substitu-
tion grammars. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies: short pa-
pers - Volume 2, HLT ’11, pages 206–211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined
tree substitution grammars for syntactic parsing. In
Proceedings of the 50th Annual Meeting of the As-
sociation for Computational Linguistics (Volume 1:
Long Papers), pages 440–448, Jeju Island, Korea,
July. Association for Computational Linguistics.
Fei Xia, Chung-hye Han, Martha Palmer, and Aravind
Joshi. 2001. Automatically extracting and compar-
ing lexicalized grammars for different languages. In
Proceedings of the 17th international joint confer-
ence on Artificial intelligence - Volume 2, IJCAI’01,
pages 1321–1326, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
XTAG Research Group. 2001. A lexicalized tree
adjoining grammar for English. Technical Report
IRCS-01-03, IRCS, University of Pennsylvania.
</reference>
<page confidence="0.979602">
602
</page>
<reference confidence="0.998555888888889">
Elif Yamangil and Stuart Shieber. 2012. Estimating
compact yet rich tree insertion grammars. In Pro-
ceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short
Papers), pages 110–114, Jeju Island, Korea, July.
Association for Computational Linguistics.
Elif Yamangil. 2013. Rich Linguistic Structure from
Large-Scale Web Data. Ph.D. thesis, Harvard Uni-
versity. Forthcoming.
</reference>
<page confidence="0.999122">
603
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.891781">
<title confidence="0.999575">Nonparametric Bayesian Inference and Efficient Parsing Tree-adjoining Grammars</title>
<author confidence="0.972064">M Yamangil</author>
<affiliation confidence="0.931817">Harvard</affiliation>
<address confidence="0.974682">Cambridge, Massachusetts,</address>
<abstract confidence="0.999808">In the line of research extending statistical parsing to more expressive grammar formalisms, we demonstrate for the first time the use of tree-adjoining grammars (TAG). We present a Bayesian nonparametric model for estimating a probabilistic TAG from a parsed corpus, along with novel block sampling methods and approximation transformations for TAG that allow efficient parsing. Our work shows performance improvements on the Penn Treebank and finds more compact yet linguistically rich representations of the data, but more importantly provides techniques in grammar transformation and statistical inference that make practical the use of these more expressive systems, thereby enabling further experimentation along these lines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
<author>Terry Koo</author>
</authors>
<title>TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08,</booktitle>
<pages>9--16</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3761" citStr="Carreras et al., 2008" startWordPosition="562" endWordPosition="565">ions with two types of derivation operators. This has led researchers to resort to manual (Doran et al., 1997) or heuristic techniques. For example, one can consider “outsourcing” the auxiliary trees (Shieber, 2007), use template rules and a very small number of grammar categories (Hwa, 1998), or rely on head-words and force lexicalization in order to constrain the problem (Xia et al., 2001; Chiang, 597 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 597–603, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2000; Carreras et al., 2008). However a solution has not been put forward by which a model that maximizes a principled probabilistic objective is sought after. Recent work by Cohn and Blunsom (2010) argued that under highly expressive grammars such as TSGs where exponentially many derivations may be hypothesized of the data, local Gibbs sampling is insufficient for effective inference and global blocked sampling strategies will be necessary. For TAG, this problem is only more severe due to its mild context-sensitivity and even richer combinatorial nature. Therefore in previous work, Shindo et al. (2011) and Yamangil and </context>
</contexts>
<marker>Carreras, Collins, Koo, 2008</marker>
<rawString>Xavier Carreras, Michael Collins, and Terry Koo. 2008. TAG, dynamic programming, and the perceptron for efficient, feature-rich parsing. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08, pages 9– 16, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Statistical parsing with an automatically-extracted tree adjoining grammar.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00,</booktitle>
<pages>456--463</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2283" citStr="Chiang, 2000" startWordPosition="331" endWordPosition="333">by TSGs (Goodman, 2003). Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the daunting model selection problem of segmenting training data trees into appropriate elementary fragments to form the grammar (Cohn et al., 2009; Post and Gildea, 2009). The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) “split them up”, preventing their reuse, leading to less sparse grammars than might be ideal (Yamangil and Shieber, 2012; Chiang, 2000; Resnik, 1992). TSGs are a special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (Joshi et al., 1975). TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for “splicing in” of syntactic fragments within trees. This functionality allows for better modeling of linguistic phenomena such as the distinction between modifiers and arguments (Joshi et al., 1975; XTAG Research Group, 2001). Unfortunately, TAG’s expressivity comes at the cost of greatly increased complexity. P</context>
</contexts>
<marker>Chiang, 2000</marker>
<rawString>David Chiang. 2000. Statistical parsing with an automatically-extracted tree adjoining grammar. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics, ACL ’00, pages 456–463, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
</authors>
<title>Blocked inference in Bayesian tree substitution grammars.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10,</booktitle>
<pages>225--230</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="3931" citStr="Cohn and Blunsom (2010)" startWordPosition="590" endWordPosition="593">tsourcing” the auxiliary trees (Shieber, 2007), use template rules and a very small number of grammar categories (Hwa, 1998), or rely on head-words and force lexicalization in order to constrain the problem (Xia et al., 2001; Chiang, 597 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 597–603, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2000; Carreras et al., 2008). However a solution has not been put forward by which a model that maximizes a principled probabilistic objective is sought after. Recent work by Cohn and Blunsom (2010) argued that under highly expressive grammars such as TSGs where exponentially many derivations may be hypothesized of the data, local Gibbs sampling is insufficient for effective inference and global blocked sampling strategies will be necessary. For TAG, this problem is only more severe due to its mild context-sensitivity and even richer combinatorial nature. Therefore in previous work, Shindo et al. (2011) and Yamangil and Shieber (2012) used tree-insertion grammar (TIG) as a kind of expressive compromise between TSG and TAG, as a substrate on which to build nonparametric inference. However</context>
<context position="5512" citStr="Cohn and Blunsom, 2010" startWordPosition="849" endWordPosition="852">efficient, and prove its superiority against the local Gibbs sampling approach. We show via nonparametric inference that TAG, which contains TSG as a subset, is a better model for treebank data than TSG and leads to improved parsing performance. TAG achieves this by using more compact grammars than TSG and by providing the ability to make finer-grained linguistic distinctions. We explain how our parameter refinement scheme for TAG allows for cubic-time CFG parsing, which is just as efficient as TSG parsing. Our presentation assumes familiarity with prior work on block sampling of TSG and TIG (Cohn and Blunsom, 2010; Shindo et al., 2011; Yamangil and Shieber, 2012). 2 Probabilistic Model In the basic nonparametric TSG model, there is an independent DP for every grammar category (such as c = NP), each of which uses a base distribution P0 that generates an initial tree by making stepwise decisions and concentration parameter αc that controls the level of sparsity (size) of the generated grammars: Gc ∼ DP(αc, P0(· |c)) We extend this model by adding specialized DPs for auxiliary trees Gaux c∼ DP(αaux c , P aux 0 (· |c)) Therefore, we have an exchangeable process for generating auxiliary tree aj given j − 1 </context>
<context position="7794" citStr="Cohn and Blunsom, 2010" startWordPosition="1250" endWordPosition="1254">tree, we determine the number (perhaps zero) of simultaneous adjunctions (Schabes and Shieber, 1994) by sampling a Geometric(µc) variable; thus k simultaneous adjunctions would have probability (µc)k(1 − µc). Since we already provide simultaneous adjunction we disallow adjunction at the root of auxiliary trees. 3 Inference Given this model, our inference task is to explore posterior derivations underlying the data. Since TAG derivations are highly structured objects, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion (Cohn and Blunsom, 2010; Shindo et al., 2011; Yamangil and Shieber, 2012). As in previous work, we use a Goodman-transformed TAG as our proposal distribution (Goodman, 2003) that incorporates additional CFG rules to account for the possibility of backing off to the infinite base distribution Paux 0 , and use the parsing algorithm described by Shieber et al. (1995) for computing inside probabilities under this TAG model. The algorithm is illustrated in Table 1 along with Figure 1. Inside probabilities are computed in a bottom-up fashion and a TAG derivation is sampled top-down (Johnson et al., 2007). The 598 6 0 Figu</context>
</contexts>
<marker>Cohn, Blunsom, 2010</marker>
<rawString>Trevor Cohn and Phil Blunsom. 2010. Blocked inference in Bayesian tree substitution grammars. In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ’10, pages 225–230, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Sharon Goldwater</author>
<author>Phil Blunsom</author>
</authors>
<title>Inducing compact but accurate treesubstitution grammars.</title>
<date>2009</date>
<booktitle>In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>548--556</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1965" citStr="Cohn et al., 2009" startWordPosition="282" endWordPosition="285">e grammars (CFG), can achieve better expressivity, and the ability to model more contextual dependencies; the payoff would be better modeling of the data or smaller (sparser) models or both. For instance, constructions that go across levels, like the predicate-argument structure of a verb and its arguments can be modeled by TSGs (Goodman, 2003). Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the daunting model selection problem of segmenting training data trees into appropriate elementary fragments to form the grammar (Cohn et al., 2009; Post and Gildea, 2009). The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) “split them up”, preventing their reuse, leading to less sparse grammars than might be ideal (Yamangil and Shieber, 2012; Chiang, 2000; Resnik, 1992). TSGs are a special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (Joshi et al., 1975). TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowin</context>
<context position="6257" citStr="Cohn et al., 2009" startWordPosition="988" endWordPosition="991">ependent DP for every grammar category (such as c = NP), each of which uses a base distribution P0 that generates an initial tree by making stepwise decisions and concentration parameter αc that controls the level of sparsity (size) of the generated grammars: Gc ∼ DP(αc, P0(· |c)) We extend this model by adding specialized DPs for auxiliary trees Gaux c∼ DP(αaux c , P aux 0 (· |c)) Therefore, we have an exchangeable process for generating auxiliary tree aj given j − 1 auxiliary trees previously generated aux p0 nc,a; + αc Poa a� |c) p(aj |a&lt;j) = (1) j − 1 + αaux c as for initial trees in TSG (Cohn et al., 2009). We must define base distributions for initial trees and auxiliary trees. P0 generates an initial tree with root label c by sampling rules from a CFG P˜ and making a binary decision at every node generated whether to leave it as a frontier node or further expand (with probability βc) (Cohn et al., 2009). Similarly, our Paux 0 generates an auxiliary tree with root label c by sampling a CFG rule from P˜, flipping an unbiased coin to decide the direction of the spine (if more than a unique child was generated), making a binary decision at the spine whether to leave it as a foot node or further e</context>
<context position="17816" citStr="Cohn et al. (2009)" startWordPosition="3018" endWordPosition="3021">e comparable Viterbi parsing times for TSG and TAG. 6 Evaluation We use the standard Penn treebank methodology of training on sections 2–21 and testing on section 23. All our data is head-binarized, all hyperparameters are resampled under appropriate vague gamma and beta priors. Samplers are run 1000 iterations each; all reported numbers are averages over 5 runs. For simplicity, parsing results are based on the maximum probability derivation (Viterbi algorithm). In Table 4, we compare TAG inference schemes and TSG. TAGGibbs operates by locally adding/removing potential adjunctions, similar to Cohn et al. (2009). TAG&apos; is the O(n2) algorithm that disallows spine adjunction. We see that TAG&apos; has the best parsing performance, while TAG provides the most compact representation. model F measure # initial trees # auxiliary trees TSG 84.15 69.5K - TAGGibbs 82.47 69.9K 1.7K 0 84.87 66.4K 1.5K TAG TAG 84.82 66.4K 1.4K Figure 4: EVALB results. Note that the Gibbs sampler for TAG has poor performance and provides no grammar compaction due to its lack of convergence. label #adj ave. #lex. #left #right #wrap (spine adj) depth trees trees trees trees VP 4532 (23) 1.06 45 22 65 0 NP 2891 (46) 1.71 68 94 13 1 NN 216</context>
</contexts>
<marker>Cohn, Goldwater, Blunsom, 2009</marker>
<rawString>Trevor Cohn, Sharon Goldwater, and Phil Blunsom. 2009. Inducing compact but accurate treesubstitution grammars. In NAACL ’09: Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 548–556, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Doran</author>
<author>Beth Hockey</author>
<author>Philip Hopely</author>
<author>Joseph Rosenzweig</author>
<author>Anoop Sarkar</author>
<author>B Srinivas</author>
<author>Fei Xia</author>
<author>Alexis Nasr</author>
<author>Owen Rambow</author>
</authors>
<title>Maintaining the forest and burning out the underbrush in xtag.</title>
<date>1997</date>
<booktitle>In Proceedings of the ENVGRAM Workshop.</booktitle>
<contexts>
<context position="3249" citStr="Doran et al., 1997" startWordPosition="485" endWordPosition="488">ctionality allows for better modeling of linguistic phenomena such as the distinction between modifiers and arguments (Joshi et al., 1975; XTAG Research Group, 2001). Unfortunately, TAG’s expressivity comes at the cost of greatly increased complexity. Parsing complexity for unconstrained TAG scales as O(n6), impractical as compared to CFG and TSG’s O(n3). In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators. This has led researchers to resort to manual (Doran et al., 1997) or heuristic techniques. For example, one can consider “outsourcing” the auxiliary trees (Shieber, 2007), use template rules and a very small number of grammar categories (Hwa, 1998), or rely on head-words and force lexicalization in order to constrain the problem (Xia et al., 2001; Chiang, 597 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 597–603, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2000; Carreras et al., 2008). However a solution has not been put forward by which a model that maximizes a principl</context>
</contexts>
<marker>Doran, Hockey, Hopely, Rosenzweig, Sarkar, Srinivas, Xia, Nasr, Rambow, 1997</marker>
<rawString>Christine Doran, Beth Hockey, Philip Hopely, Joseph Rosenzweig, Anoop Sarkar, B. Srinivas, Fei Xia, Alexis Nasr, and Owen Rambow. 1997. Maintaining the forest and burning out the underbrush in xtag. In Proceedings of the ENVGRAM Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Efficient parsing of DOP with PCFG-reductions.</title>
<date>2003</date>
<editor>In Rens Bod, Remko Scha, and Khalil Sima’an, editors, Data-Oriented Parsing.</editor>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="1694" citStr="Goodman, 2003" startWordPosition="244" endWordPosition="245">rate modeling of the data with sparse grammars — and low complexity — making induction of the grammars (say, from a treebank) and parsing of novel sentences computationally practical. Tree-substitution grammars (TSG), by expanding the domain of locality of context-free grammars (CFG), can achieve better expressivity, and the ability to model more contextual dependencies; the payoff would be better modeling of the data or smaller (sparser) models or both. For instance, constructions that go across levels, like the predicate-argument structure of a verb and its arguments can be modeled by TSGs (Goodman, 2003). Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the daunting model selection problem of segmenting training data trees into appropriate elementary fragments to form the grammar (Cohn et al., 2009; Post and Gildea, 2009). The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) “split them up”, preventing their reuse, leading to less sparse grammars than might be ideal (Yamangil and Shieber, 2012; Chiang, 2000; Resnik, 1</context>
<context position="7944" citStr="Goodman, 2003" startWordPosition="1277" endWordPosition="1278"> adjunctions would have probability (µc)k(1 − µc). Since we already provide simultaneous adjunction we disallow adjunction at the root of auxiliary trees. 3 Inference Given this model, our inference task is to explore posterior derivations underlying the data. Since TAG derivations are highly structured objects, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion (Cohn and Blunsom, 2010; Shindo et al., 2011; Yamangil and Shieber, 2012). As in previous work, we use a Goodman-transformed TAG as our proposal distribution (Goodman, 2003) that incorporates additional CFG rules to account for the possibility of backing off to the infinite base distribution Paux 0 , and use the parsing algorithm described by Shieber et al. (1995) for computing inside probabilities under this TAG model. The algorithm is illustrated in Table 1 along with Figure 1. Inside probabilities are computed in a bottom-up fashion and a TAG derivation is sampled top-down (Johnson et al., 2007). The 598 6 0 Figure 1: Example used for illustrating blocked sampling with TAG. On the left hand side we have a partial training tree where we highlight the particular</context>
</contexts>
<marker>Goodman, 2003</marker>
<rawString>Joshua Goodman. 2003. Efficient parsing of DOP with PCFG-reductions. In Rens Bod, Remko Scha, and Khalil Sima’an, editors, Data-Oriented Parsing. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
</authors>
<title>An empirical evaluation of probabilistic lexicalized tree insertion grammars.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th international conference on Computational linguistics -</booktitle>
<volume>1</volume>
<pages>557--563</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="3432" citStr="Hwa, 1998" startWordPosition="516" endWordPosition="517">essivity comes at the cost of greatly increased complexity. Parsing complexity for unconstrained TAG scales as O(n6), impractical as compared to CFG and TSG’s O(n3). In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators. This has led researchers to resort to manual (Doran et al., 1997) or heuristic techniques. For example, one can consider “outsourcing” the auxiliary trees (Shieber, 2007), use template rules and a very small number of grammar categories (Hwa, 1998), or rely on head-words and force lexicalization in order to constrain the problem (Xia et al., 2001; Chiang, 597 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 597–603, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2000; Carreras et al., 2008). However a solution has not been put forward by which a model that maximizes a principled probabilistic objective is sought after. Recent work by Cohn and Blunsom (2010) argued that under highly expressive grammars such as TSGs where exponentially many derivations may b</context>
<context position="11829" citStr="Hwa, 1998" startWordPosition="1920" endWordPosition="1921">subtree rooted at node v, and (2) per-path, e.g., Nj[v-η] denoting the probability of starting at an auxiliary subtree that has Goodman index j and generating the subtree rooted at v minus the subtree rooted at η. Above, c denotes the context of adjunction, which is the nonterminal label of the node of adjunction (here, N), µc is the probability of adjunction, nc,a is the count of the auxiliary tree a, and nc = Ea nc,a is total number of adjunctions at context c. The function π(·) retrieves the inside probability corresponding to an item. index identifiers of the subtree below the adjunction (Hwa, 1998), to simple grammar category labels — and find that using Goodman index identifiers as c is the best performing option. Interestingly, this particular refinement scheme also allows for fast cubic-time parsing, which we achieve by approximating the TAG by a TSG with little loss of coverage (no loss of coverage under special conditions which we find that are often satisfied) and negligible increase in grammar size, as discussed in the next section. 5 Cubic-time parsing MCMC training results in a list of sufficient statistics of the final derivation that the TAG sampler converges upon after a num</context>
</contexts>
<marker>Hwa, 1998</marker>
<rawString>Rebecca Hwa. 1998. An empirical evaluation of probabilistic lexicalized tree insertion grammars. In Proceedings of the 17th international conference on Computational linguistics - Volume 1, pages 557– 563, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Bayesian inference for PCFGs via Markov chain Monte Carlo.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>139--146</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="8376" citStr="Johnson et al., 2007" startWordPosition="1348" endWordPosition="1351">a joint fashion (Cohn and Blunsom, 2010; Shindo et al., 2011; Yamangil and Shieber, 2012). As in previous work, we use a Goodman-transformed TAG as our proposal distribution (Goodman, 2003) that incorporates additional CFG rules to account for the possibility of backing off to the infinite base distribution Paux 0 , and use the parsing algorithm described by Shieber et al. (1995) for computing inside probabilities under this TAG model. The algorithm is illustrated in Table 1 along with Figure 1. Inside probabilities are computed in a bottom-up fashion and a TAG derivation is sampled top-down (Johnson et al., 2007). The 598 6 0 Figure 1: Example used for illustrating blocked sampling with TAG. On the left hand side we have a partial training tree where we highlight the particular nodes (with node labels 0, 1, 2, 3, 4) that the sampling algorithm traverses in post-order. On the right hand side is the TAG grammar fragment that is used to parse these particular nodes: one initial tree and two wrapping auxiliary trees where one adjoins into the spine of the other for full generality of our illustration. Grammar nodes are labeled with their Goodman indices (letters i, j, k,l, m). Greek letters α, β, γ, S den</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas Griffiths, and Sharon Goldwater. 2007. Bayesian inference for PCFGs via Markov chain Monte Carlo. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 139–146, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Leon S Levy</author>
<author>Masako Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of Computer and System Sciences,</journal>
<volume>10</volume>
<issue>1</issue>
<contexts>
<context position="2415" citStr="Joshi et al., 1975" startWordPosition="352" endWordPosition="355">efficient solution to the daunting model selection problem of segmenting training data trees into appropriate elementary fragments to form the grammar (Cohn et al., 2009; Post and Gildea, 2009). The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) “split them up”, preventing their reuse, leading to less sparse grammars than might be ideal (Yamangil and Shieber, 2012; Chiang, 2000; Resnik, 1992). TSGs are a special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (Joshi et al., 1975). TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for “splicing in” of syntactic fragments within trees. This functionality allows for better modeling of linguistic phenomena such as the distinction between modifiers and arguments (Joshi et al., 1975; XTAG Research Group, 2001). Unfortunately, TAG’s expressivity comes at the cost of greatly increased complexity. Parsing complexity for unconstrained TAG scales as O(n6), impractical as compared to CFG and TSG’s O(n3). In addition, the model sele</context>
</contexts>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>Aravind K. Joshi, Leon S. Levy, and Masako Takahashi. 1975. Tree adjunct grammars. Journal of Computer and System Sciences, 10(1):136–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>433--440</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433–440, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of a tree substitution grammar.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<pages>45--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Suntec, Singapore,</location>
<contexts>
<context position="1989" citStr="Post and Gildea, 2009" startWordPosition="286" endWordPosition="289">an achieve better expressivity, and the ability to model more contextual dependencies; the payoff would be better modeling of the data or smaller (sparser) models or both. For instance, constructions that go across levels, like the predicate-argument structure of a verb and its arguments can be modeled by TSGs (Goodman, 2003). Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the daunting model selection problem of segmenting training data trees into appropriate elementary fragments to form the grammar (Cohn et al., 2009; Post and Gildea, 2009). The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) “split them up”, preventing their reuse, leading to less sparse grammars than might be ideal (Yamangil and Shieber, 2012; Chiang, 2000; Resnik, 1992). TSGs are a special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (Joshi et al., 1975). TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for “splicing in” of s</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Bayesian learning of a tree substitution grammar. In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 45–48, Suntec, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Probabilistic tree-adjoining grammar as a framework for statistical natural language processing.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguistics - Volume 2, COLING ’92,</booktitle>
<pages>418--424</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2298" citStr="Resnik, 1992" startWordPosition="334" endWordPosition="335">an, 2003). Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the daunting model selection problem of segmenting training data trees into appropriate elementary fragments to form the grammar (Cohn et al., 2009; Post and Gildea, 2009). The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) “split them up”, preventing their reuse, leading to less sparse grammars than might be ideal (Yamangil and Shieber, 2012; Chiang, 2000; Resnik, 1992). TSGs are a special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (Joshi et al., 1975). TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for “splicing in” of syntactic fragments within trees. This functionality allows for better modeling of linguistic phenomena such as the distinction between modifiers and arguments (Joshi et al., 1975; XTAG Research Group, 2001). Unfortunately, TAG’s expressivity comes at the cost of greatly increased complexity. Parsing complexi</context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>Philip Resnik. 1992. Probabilistic tree-adjoining grammar as a framework for statistical natural language processing. In Proceedings of the 14th conference on Computational linguistics - Volume 2, COLING ’92, pages 418–424, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
<author>Stuart M Shieber</author>
</authors>
<title>An alternative conception of tree-adjoining derivation.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>1</issue>
<note>Also available as cmp-lg/9404001.</note>
<contexts>
<context position="7272" citStr="Schabes and Shieber, 1994" startWordPosition="1169" endWordPosition="1172">CFG rule from P˜, flipping an unbiased coin to decide the direction of the spine (if more than a unique child was generated), making a binary decision at the spine whether to leave it as a foot node or further expand (with probability γc), and recurring into P0 or Paux 0 appropriately for the off-spine and spinal children respectively. We glue these two processes together via a set of adjunction parameters µc. In any derivation for every node labeled c that is not a frontier node or the root or foot node of an auxiliary tree, we determine the number (perhaps zero) of simultaneous adjunctions (Schabes and Shieber, 1994) by sampling a Geometric(µc) variable; thus k simultaneous adjunctions would have probability (µc)k(1 − µc). Since we already provide simultaneous adjunction we disallow adjunction at the root of auxiliary trees. 3 Inference Given this model, our inference task is to explore posterior derivations underlying the data. Since TAG derivations are highly structured objects, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion (Cohn and Blunsom, 2010; Shindo et al., 2011; Yamangil and Shieber, 2012). As in previous work, we us</context>
</contexts>
<marker>Schabes, Shieber, 1994</marker>
<rawString>Yves Schabes and Stuart M. Shieber. 1994. An alternative conception of tree-adjoining derivation. Computational Linguistics, 20(1):91–124. Also available as cmp-lg/9404001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>J. Log. Program.,</journal>
<pages>24--1</pages>
<contexts>
<context position="8137" citStr="Shieber et al. (1995)" startWordPosition="1309" endWordPosition="1312">ur inference task is to explore posterior derivations underlying the data. Since TAG derivations are highly structured objects, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion (Cohn and Blunsom, 2010; Shindo et al., 2011; Yamangil and Shieber, 2012). As in previous work, we use a Goodman-transformed TAG as our proposal distribution (Goodman, 2003) that incorporates additional CFG rules to account for the possibility of backing off to the infinite base distribution Paux 0 , and use the parsing algorithm described by Shieber et al. (1995) for computing inside probabilities under this TAG model. The algorithm is illustrated in Table 1 along with Figure 1. Inside probabilities are computed in a bottom-up fashion and a TAG derivation is sampled top-down (Johnson et al., 2007). The 598 6 0 Figure 1: Example used for illustrating blocked sampling with TAG. On the left hand side we have a partial training tree where we highlight the particular nodes (with node labels 0, 1, 2, 3, 4) that the sampling algorithm traverses in post-order. On the right hand side is the TAG grammar fragment that is used to parse these particular nodes: one</context>
<context position="10328" citStr="Shieber et al., 1995" startWordPosition="1677" endWordPosition="1680">f adjunction; and p(aj |a&lt;j, c) (see Equation 1), the probability of the particular auxiliary tree being adjoined given that there is an adjunction. In all of this treatment, c, the context of an adjunction, is the grammar category label such as S or NP, instead of a unique identifier for the node at which the adjunction occurs as was originally the case in probabilistic TAG literature. However it is possible to experiment with further refinement schemes at parsing time. Once the sampler converges on a grammar, we can reestimate its adjunction probabilities. Using the O(n6) parsing algorithm (Shieber et al., 1995) we experimented with various refinements schemes — ranging from full node identifiers, to Goodman Chart item Why made? Inside probability Ni[4] By assumption. − Nk[3, 4] N∗[4] and β (1 − µc) x π(β) Nm[2, 3] N∗[3] and δ (1 − µc) x π(δ) Nl[1, 2, 3] γ and Nm[2, 3] (1 − µc) x π(γ) xπ(Nm[2, 3]) Naux[1, 2, 3] Nl[1, 2, 3] nc,al /(nc + αaux c ) xπ(Nl[1, 2, 3]) Nk[1, 2, 3, 4] Naux[1, 2, 3] and Nk[3, 4] µc x π(Naux[1, 2, 3]) xπ(Nk[3, 4]) Nj[0, 1, 2, 3, 4] α and Nk[1, 2, 3, 4] (1 − µc) x π(α) xπ(Nk[1, 2, 3, 4]) Naux[0, 1, 2, 3, 4] Nj[0, 1, 2, 3, 4] nc,aj /(nc + αaux c ) xπ(Nj[0, 1, 2, 3, 4]) Ni[0] Naux[</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart M. Shieber, Yves Schabes, and Fernando C. N. Pereira. 1995. Principles and implementation of deductive parsing. J. Log. Program., 24(1&amp;2):3–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
</authors>
<title>Probabilistic synchronous tree-adjoining grammars for machine translation: The argument from bilingual dictionaries.</title>
<date>2007</date>
<booktitle>In Dekai Wu and David Chiang, editors, Proceedings of the Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<location>Rochester, New York, 26</location>
<contexts>
<context position="3354" citStr="Shieber, 2007" startWordPosition="502" endWordPosition="503">guments (Joshi et al., 1975; XTAG Research Group, 2001). Unfortunately, TAG’s expressivity comes at the cost of greatly increased complexity. Parsing complexity for unconstrained TAG scales as O(n6), impractical as compared to CFG and TSG’s O(n3). In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators. This has led researchers to resort to manual (Doran et al., 1997) or heuristic techniques. For example, one can consider “outsourcing” the auxiliary trees (Shieber, 2007), use template rules and a very small number of grammar categories (Hwa, 1998), or rely on head-words and force lexicalization in order to constrain the problem (Xia et al., 2001; Chiang, 597 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 597–603, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2000; Carreras et al., 2008). However a solution has not been put forward by which a model that maximizes a principled probabilistic objective is sought after. Recent work by Cohn and Blunsom (2010) argued that under high</context>
</contexts>
<marker>Shieber, 2007</marker>
<rawString>Stuart M. Shieber. 2007. Probabilistic synchronous tree-adjoining grammars for machine translation: The argument from bilingual dictionaries. In Dekai Wu and David Chiang, editors, Proceedings of the Workshop on Syntax and Structure in Statistical Translation, Rochester, New York, 26 April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Shindo</author>
<author>Akinori Fujino</author>
<author>Masaaki Nagata</author>
</authors>
<title>Insertion operator for Bayesian tree substitution grammars.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11,</booktitle>
<pages>206--211</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4343" citStr="Shindo et al. (2011)" startWordPosition="656" endWordPosition="659">nguistics 2000; Carreras et al., 2008). However a solution has not been put forward by which a model that maximizes a principled probabilistic objective is sought after. Recent work by Cohn and Blunsom (2010) argued that under highly expressive grammars such as TSGs where exponentially many derivations may be hypothesized of the data, local Gibbs sampling is insufficient for effective inference and global blocked sampling strategies will be necessary. For TAG, this problem is only more severe due to its mild context-sensitivity and even richer combinatorial nature. Therefore in previous work, Shindo et al. (2011) and Yamangil and Shieber (2012) used tree-insertion grammar (TIG) as a kind of expressive compromise between TSG and TAG, as a substrate on which to build nonparametric inference. However TIG has the constraint of disallowing wrapping adjunction (coordination between material that falls to the left and right of the point of adjunction, such as parentheticals and quotations) as well as left adjunction along the spine of a right auxiliary tree and vice versa. In this work we formulate a blocked sampling strategy for TAG that is effective and efficient, and prove its superiority against the loca</context>
<context position="7815" citStr="Shindo et al., 2011" startWordPosition="1255" endWordPosition="1258">umber (perhaps zero) of simultaneous adjunctions (Schabes and Shieber, 1994) by sampling a Geometric(µc) variable; thus k simultaneous adjunctions would have probability (µc)k(1 − µc). Since we already provide simultaneous adjunction we disallow adjunction at the root of auxiliary trees. 3 Inference Given this model, our inference task is to explore posterior derivations underlying the data. Since TAG derivations are highly structured objects, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion (Cohn and Blunsom, 2010; Shindo et al., 2011; Yamangil and Shieber, 2012). As in previous work, we use a Goodman-transformed TAG as our proposal distribution (Goodman, 2003) that incorporates additional CFG rules to account for the possibility of backing off to the infinite base distribution Paux 0 , and use the parsing algorithm described by Shieber et al. (1995) for computing inside probabilities under this TAG model. The algorithm is illustrated in Table 1 along with Figure 1. Inside probabilities are computed in a bottom-up fashion and a TAG derivation is sampled top-down (Johnson et al., 2007). The 598 6 0 Figure 1: Example used fo</context>
</contexts>
<marker>Shindo, Fujino, Nagata, 2011</marker>
<rawString>Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata. 2011. Insertion operator for Bayesian tree substitution grammars. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers - Volume 2, HLT ’11, pages 206–211, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyuki Shindo</author>
<author>Yusuke Miyao</author>
<author>Akinori Fujino</author>
<author>Masaaki Nagata</author>
</authors>
<title>Bayesian symbol-refined tree substitution grammars for syntactic parsing.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>440--448</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<marker>Shindo, Miyao, Fujino, Nagata, 2012</marker>
<rawString>Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and Masaaki Nagata. 2012. Bayesian symbol-refined tree substitution grammars for syntactic parsing. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 440–448, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Chung-hye Han</author>
<author>Martha Palmer</author>
<author>Aravind Joshi</author>
</authors>
<title>Automatically extracting and comparing lexicalized grammars for different languages.</title>
<date>2001</date>
<booktitle>In Proceedings of the 17th international joint conference on Artificial intelligence - Volume 2, IJCAI’01,</booktitle>
<pages>1321--1326</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="3532" citStr="Xia et al., 2001" startWordPosition="531" endWordPosition="534">ed TAG scales as O(n6), impractical as compared to CFG and TSG’s O(n3). In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators. This has led researchers to resort to manual (Doran et al., 1997) or heuristic techniques. For example, one can consider “outsourcing” the auxiliary trees (Shieber, 2007), use template rules and a very small number of grammar categories (Hwa, 1998), or rely on head-words and force lexicalization in order to constrain the problem (Xia et al., 2001; Chiang, 597 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 597–603, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2000; Carreras et al., 2008). However a solution has not been put forward by which a model that maximizes a principled probabilistic objective is sought after. Recent work by Cohn and Blunsom (2010) argued that under highly expressive grammars such as TSGs where exponentially many derivations may be hypothesized of the data, local Gibbs sampling is insufficient for effective inference and global </context>
</contexts>
<marker>Xia, Han, Palmer, Joshi, 2001</marker>
<rawString>Fei Xia, Chung-hye Han, Martha Palmer, and Aravind Joshi. 2001. Automatically extracting and comparing lexicalized grammars for different languages. In Proceedings of the 17th international joint conference on Artificial intelligence - Volume 2, IJCAI’01, pages 1321–1326, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>XTAG Research Group</author>
</authors>
<title>A lexicalized tree adjoining grammar for English.</title>
<date>2001</date>
<tech>Technical Report IRCS-01-03,</tech>
<institution>IRCS, University of Pennsylvania.</institution>
<contexts>
<context position="2795" citStr="Group, 2001" startWordPosition="416" endWordPosition="417">reuse, leading to less sparse grammars than might be ideal (Yamangil and Shieber, 2012; Chiang, 2000; Resnik, 1992). TSGs are a special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (Joshi et al., 1975). TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for “splicing in” of syntactic fragments within trees. This functionality allows for better modeling of linguistic phenomena such as the distinction between modifiers and arguments (Joshi et al., 1975; XTAG Research Group, 2001). Unfortunately, TAG’s expressivity comes at the cost of greatly increased complexity. Parsing complexity for unconstrained TAG scales as O(n6), impractical as compared to CFG and TSG’s O(n3). In addition, the model selection problem for TAG is significantly more complicated than for TSG since one must reason about many more combinatorial options with two types of derivation operators. This has led researchers to resort to manual (Doran et al., 1997) or heuristic techniques. For example, one can consider “outsourcing” the auxiliary trees (Shieber, 2007), use template rules and a very small num</context>
</contexts>
<marker>Group, 2001</marker>
<rawString>XTAG Research Group. 2001. A lexicalized tree adjoining grammar for English. Technical Report IRCS-01-03, IRCS, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elif Yamangil</author>
<author>Stuart Shieber</author>
</authors>
<title>Estimating compact yet rich tree insertion grammars.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),</booktitle>
<pages>110--114</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2269" citStr="Yamangil and Shieber, 2012" startWordPosition="327" endWordPosition="330">ts arguments can be modeled by TSGs (Goodman, 2003). Recent work that incorporated Dirichlet process (DP) nonparametric models into TSGs has provided an efficient solution to the daunting model selection problem of segmenting training data trees into appropriate elementary fragments to form the grammar (Cohn et al., 2009; Post and Gildea, 2009). The elementary trees combined in a TSG are, intuitively, primitives of the language, yet certain linguistic phenomena (notably various forms of modification) “split them up”, preventing their reuse, leading to less sparse grammars than might be ideal (Yamangil and Shieber, 2012; Chiang, 2000; Resnik, 1992). TSGs are a special case of the more flexible grammar formalism of tree adjoining grammar (TAG) (Joshi et al., 1975). TAG augments TSG with an adjunction operator and a set of auxiliary trees in addition to the substitution operator and initial trees of TSG, allowing for “splicing in” of syntactic fragments within trees. This functionality allows for better modeling of linguistic phenomena such as the distinction between modifiers and arguments (Joshi et al., 1975; XTAG Research Group, 2001). Unfortunately, TAG’s expressivity comes at the cost of greatly increased</context>
<context position="4375" citStr="Yamangil and Shieber (2012)" startWordPosition="661" endWordPosition="664">et al., 2008). However a solution has not been put forward by which a model that maximizes a principled probabilistic objective is sought after. Recent work by Cohn and Blunsom (2010) argued that under highly expressive grammars such as TSGs where exponentially many derivations may be hypothesized of the data, local Gibbs sampling is insufficient for effective inference and global blocked sampling strategies will be necessary. For TAG, this problem is only more severe due to its mild context-sensitivity and even richer combinatorial nature. Therefore in previous work, Shindo et al. (2011) and Yamangil and Shieber (2012) used tree-insertion grammar (TIG) as a kind of expressive compromise between TSG and TAG, as a substrate on which to build nonparametric inference. However TIG has the constraint of disallowing wrapping adjunction (coordination between material that falls to the left and right of the point of adjunction, such as parentheticals and quotations) as well as left adjunction along the spine of a right auxiliary tree and vice versa. In this work we formulate a blocked sampling strategy for TAG that is effective and efficient, and prove its superiority against the local Gibbs sampling approach. We sh</context>
<context position="7844" citStr="Yamangil and Shieber, 2012" startWordPosition="1259" endWordPosition="1262">of simultaneous adjunctions (Schabes and Shieber, 1994) by sampling a Geometric(µc) variable; thus k simultaneous adjunctions would have probability (µc)k(1 − µc). Since we already provide simultaneous adjunction we disallow adjunction at the root of auxiliary trees. 3 Inference Given this model, our inference task is to explore posterior derivations underlying the data. Since TAG derivations are highly structured objects, we design a blocked Metropolis-Hastings sampler that samples derivations per entire parse trees all at once in a joint fashion (Cohn and Blunsom, 2010; Shindo et al., 2011; Yamangil and Shieber, 2012). As in previous work, we use a Goodman-transformed TAG as our proposal distribution (Goodman, 2003) that incorporates additional CFG rules to account for the possibility of backing off to the infinite base distribution Paux 0 , and use the parsing algorithm described by Shieber et al. (1995) for computing inside probabilities under this TAG model. The algorithm is illustrated in Table 1 along with Figure 1. Inside probabilities are computed in a bottom-up fashion and a TAG derivation is sampled top-down (Johnson et al., 2007). The 598 6 0 Figure 1: Example used for illustrating blocked sampli</context>
</contexts>
<marker>Yamangil, Shieber, 2012</marker>
<rawString>Elif Yamangil and Stuart Shieber. 2012. Estimating compact yet rich tree insertion grammars. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 110–114, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elif Yamangil</author>
</authors>
<title>Rich Linguistic Structure from Large-Scale Web Data.</title>
<date>2013</date>
<tech>Ph.D. thesis,</tech>
<institution>Harvard University. Forthcoming.</institution>
<marker>Yamangil, 2013</marker>
<rawString>Elif Yamangil. 2013. Rich Linguistic Structure from Large-Scale Web Data. Ph.D. thesis, Harvard University. Forthcoming.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>