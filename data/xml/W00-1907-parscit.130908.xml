<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000081">
<title confidence="0.690962">
The Detection of Inconsistency in Manually Tagged Text
</title>
<author confidence="0.499922">
Hans van Halteren
</author>
<affiliation confidence="0.56763">
Dept. of Language and Speech
University of Nijmegen
</affiliation>
<address confidence="0.575191333333333">
P.O. Box 9103
6500 HD Nijmegen
The Netherlands
</address>
<email confidence="0.963256">
hvh@let.kun.n1
</email>
<sectionHeader confidence="0.986404" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999894818181818">
This paper proposes a method to detect the
presence of inconsistency in a given manually
tagged corpus. The method consist of gener-
ating an automatic tagger on the basis of the
corpus and then comparing the tagger&apos;s out-
put with the original tagging. It is tested using
the written texts from the BNC sampler and a
WPDV-based tagger generator, and shown to
be both an efficient method to derive a qualita-
tive evaluation of consistency and a useful first
step towards correction.
</bodyText>
<sectionHeader confidence="0.993756" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99994438">
Wordclass tagged corpora are a very popular
resource for both language engineers and lin-
guists. If these corpora are used for inspi-
ration and exemplification, size may be more
important than quality and a fully automati-
cally tagged corpus can suffice. For other uses,
quality is of much higher importance, and here
there will generally be a preference for manu-
ally corrected corpora, even though they may be
smaller. However, manual correction means hu-
man involvement, and that again means a much
higher potential for inconsistency (cf. e.g. Mar-
cus et al. (1993); Baker (1997)).
Before we go and base our NLP systems or
linguistic theories on the wordclass tags found
in a tagged corpus, then, it would certainly be
a good idea to evaluate whether those tags have
indeed been assigned appropriately, and, if not,
possibly correct the situation. This means that
we have to inspect (part of) the corpus and de-
cide whether the tags are consistent with the
tagging manual or, if the tagging manual is not
clear on the subject, whether the tags have at
least been applied consistently throughout the
corpus. In this paper we show, by way of an
experiment, how this task can be made more
efficient with the help of software already in gen-
eral use in wordclass tagging circles, viz, tagger
generators.
The tagged corpus on which we perform our
experiment consists of all the written texts of
the BNC sampler CD. Its size (about 1Mw)
is average for manually corrected corpora, the
tagset is well-developed (C7) and the tagging
process has involved the use of an equally well-
developed automatic tagger (cLAws4) and sub-
sequent correction by a team of experienced an-
notators (cf. Garside and Smith (1997)). We
can assume that the consistency may not be as
high as that of the LOB corpus, which by now
has reached an admirable level of consistency,
but certainly higher than notoriously inconsis-
tent corpora like the Wall Street Journal (cf. van
Halteren et al. (To appear)).
In the following sections, we first examine
the concept of consistency (section 2), then de-
scribe the tagger generator used in the exper-
iment (section 3), evaluate the output of the
experiment (sections 4 and 5), and conclude by
summarising the main findings (section 6).
</bodyText>
<sectionHeader confidence="0.601064" genericHeader="introduction">
2 Consistency and its Evaluation
</sectionHeader>
<bodyText confidence="0.999940416666667">
It is generally agreed that one of the desired
properties of any tagging is consistency, and
that we therefore want to have some means of
evaluating it. An important step towards such
means is an examination of what this property
of &amp;quot;consistency&amp;quot; is supposed to entail, beginning
with a general definition of the concept:
When we say that somebody is con-
sistent, we mean that if the same sit-
uation is encountered more than once,
that person will take the same action
each time.
</bodyText>
<page confidence="0.998407">
48
</page>
<bodyText confidence="0.995508766990292">
With this general definition in place, we can
take a closer look at some aspects of the concept
which are important for the specific activity we
are interested in, viz, the tagging of text.
First of all, we have to distinguish between in-
ternal consistency and consistency with regard
to a defined standard (aka conformance). With
wordclass tagging, there is invariably some kind
of defined standard, e.g. in the form of a tagging
manual. In fact, the importance of the standard
is often taken to be such that deviations from
it are not just called inconsistencies, but that
the stronger term &amp;quot;errors&amp;quot; is used.&apos; It is this
type of consistency which is measured in most
evaluations of the tagged material and which
is referred to with &amp;quot;correctness&amp;quot; or &amp;quot;accuracy&amp;quot;
percentages. However, wordclass tagging is also
assumed to correspond to a general descriptive
linguistic tradition (whether &amp;quot;theory neutral&amp;quot; or
not), which makes it very unlikely that any tag-
ging manual can ever really be complete. The
resulting friction between the (hopefully) clear
but necessarily incomplete tagging manual and
each tagger&apos;s personal conception of the under-
lying linguistic tradition cannot but lead to in-
dividual decisions. In these cases it is impossi-
ble to evaluate the consistency with regard to
the standard, as the standard is partly incom-
plete (the manual) and partly not well-defined
(the linguistic tradition). Instead, we will have
to evaluate the internal consistency, i.e. the de-
gree to which the individual decisions have been
taken consistently.
The problem with the latter kind of evalu-
ation is that, in wordclass tagging, the con-
cept &amp;quot;same situation&amp;quot; can be taken at differ-
ent levels of granularity. When taken only in
the strictest sense, it would mean that the ex-
act same word is occurring in the exact same
context.2 It is this sense which is used when,
during a tagging project, inter-annotator con-
sistency is measured. Several taggers are given
1Below, we will follow this choice of terminology and
use the term &amp;quot;error&amp;quot; for tags which are inconsistent with
regard to the standard, leaving the term &amp;quot;inconsistency&amp;quot;
for those cases where (the description of) the standard
provides no information on a &amp;quot;correct&amp;quot; tag and individ-
ual choices vary.
2Here, we take the context to be that which a human
annotator would use to make decisions. This ought to
be at least the whole sentence, but might well include
the surrounding paragraph or more.
the same text and their taggings are compared.
This is useful for training purposes and improve-
ment of the tagging manual and is also a good
quality control mechanism if quality is seen in
relation to the manual (and possibly the more
exactly defined parts of the linguistic tradition).
It is not, however, very useful in the evalua-
tion of consistency between different parts of the
corpus. Barring exceptional situations, such as
news items which are repeated in several broad-
casts, it is extremely unlikely that there are mul-
tiple occurrences of the same word combined
with the same context. This is unfortunate,
as such occurrences would be extremely easy to
find, and hence compare, automatically.
Internal consistency is much more likely to be
expressed in terms of the same &amp;quot;type&amp;quot; of word
occurring in the same &amp;quot;type&amp;quot; of context. The
question, then, is if and how we can determine
which types of word in which types of context
are tagged differently from occurrence to occur-
rence. The position taken in this paper is that,
just as for the tagging process itself, the best
choice is a combined effort by man and ma-
chine. For the time being, only man has suffi-
cient knowledge of the actual aims of wordclass
tagging and the generalisation skills to deter-
mine which situations are indeed &amp;quot;the same&amp;quot;.
On the other hand, the number of situations to
be examined for inconsistency is much too large
for exhaustive treatment, so that some kind of
sampling is necessary. Seeing that random sam-
pling tends to reveal only the most frequent in-
consistencies (see below), we will have to use
the machine to select situations with a high po-
tential for inconsistency.
Now we may not have any algorithms ready
at hand which detect inconsistency, but there
are quite a number of algorithms which do the
opposite: machine learning algorithms are de-
signed to try to detect consistent behaviour in
order to replicate it. In the context of wordclass
tagging, machine learning algorithms come in
the form of tagger generators, which automat-
ically create tagging programs on the basis of
a tagged training set. If we had the ideal tag-
ger generator and a perfectly consistent training
set, the generated tagger should be able to repli-
cate the tagging in the training set completely.
This means that errors made by a generated
tagger must either be due to inconsistencies in
</bodyText>
<page confidence="0.996742">
49
</page>
<bodyText confidence="0.999941583333333">
the training set or to insufficiency of the learn-
ing algorithm.3 With both causes, the situa-
tions in which errors are made can be assumed
to have a high potential for inconsistency: in
the first case, they are related directly to in-
consistencies; in the second, they are at least
non-trivial and hence possibly more error-prone
for humans as well. It would therefore seem to
be a good idea to focus the human evaluator&apos;s
attention on those tokens for which an auto-
matic tagger&apos;s output and the original tagging
disagree.
</bodyText>
<sectionHeader confidence="0.975541" genericHeader="method">
3 Tagger Generation
</sectionHeader>
<bodyText confidence="0.989458205882353">
For the experiment in which we test this idea,
we use a new tagger generator, which is based
on the Weighted Probability Distribution Vot-
ing algorithm (WPDV; cf. van Halteren (To ap-
pear)). A tagger generated by this system goes
through the following steps:
1. Normally, the first step would be tokenisa-
tion. In our experiment, however, we use
the tokenisation as present in the original
tagging of the corpus, as this makes com-
parison much easier. This means, however,
that the intelligence embedded in the to-
keniser is disabled. The most important
example for the data at hand is that capi-
talised words in headings or at the start of
sentences are not decapitalised but treated
as is.
There is one area where we have to deviate
from the BNC tokenization. In the sam-
pler material, multi-token units, such as &amp;quot;in
front of&amp;quot;, are present as a group of tokens
which together receive one tag. As we want
to detect inconsistency in this grouping as
well, we translate such multi-token units to
sequences of separate tokens, each tagged
with a ditto tag. However, as no special
treatment is present for such sequences in
the tagger generator, they can be expected
to be responsible for a good number of er-
rors in the tagger output.
2. Next, the lexical lookup component at-
taches to each token a list of tags which
were observed with that token in the train-
ing set. Note that, as mentioned above, the
</bodyText>
<footnote confidence="0.8750475">
3The latter obviously in relation to the size of the
training set.
</footnote>
<listItem confidence="0.91151082">
token &amp;quot;The&amp;quot;, e.g. at the start of a sentence,
is different from the token &amp;quot;the&amp;quot;.
3. For those cases where lexical lookup pro-
vides no or insufficient information, we fall
back on lexical similarity lookup. This
means that potential tags are generated by
a WPDV model, using the length of the
token, its pattern of character types (e.g.
&amp;quot;1980s&amp;quot; would be &amp;quot;one or more digits fol-
lowed by one or more lower case charac-
ters&amp;quot;) and its last three actual characters.
The output consists of all tags which, ac-
cording to this model, are at least 0.025
times as probable as the most probable tag
for the token.
4. For tokens which were observed 10 times or
more in the training set, only the output of
the lexical lookup is used. For all other
tokens, the output of the lexical similarity
lookup is added. The resulting list of tags
is used in two ways. Throughout the tag-
ging process, the full list is used as a filter
on the potential tags for a token, i.e. even
if the context provides overwhelming evi-
dence that a specific tag should be used, the
tag is ruled out if it does not occur in the
list. Additionally, the most probable tags
in the list (up to three) are used to define an
ambiguity class (cf. Cutting et al. (1992))
for the token, which is used in the context-
dependent components. The lexical proba-
bilities of the tags are used only to deter-
mine the selection for presence (and rela-
tive position) in the ambiguity class. They
are not used in the context-dependent com-
ponents.
5. In the main context-dependent components,
two WPDV models then determine the
most probable tag for each token on the ba-
sis of the (disambiguated) tags of two pre-
ceding tokens and the ambiguity classes of
the focus and two following tokens. The
difference between the two models is that
one follows the normal order of the tokens,
i.e. tags from left to right, while the other
uses reverse order, i.e. tags from right to
left.
6. The final selection of the tag for each token
is determined by a WPDV model using the
suggestions of the two context-dependent
</listItem>
<page confidence="0.985333">
50
</page>
<bodyText confidence="0.999654705882353">
models for the focus and two tokens on ei-
ther side of it.
There are two reasons for the selection of this
particular tagger generator. First, an evalua-
tion with the same training and test set used by
van Halteren et al. (To appear) has shown this
tagging strategy to compare favourably with
other state-of-the-art tagger generators: 97.82%
agreement with the test set versus 97.55%
for TnT (Brants, 1999), 97.52% for MXPOST
(Ratnaparkhi, 1996), 97.06% for MBT (Daele-
mans et al., 1996) and 96.37% for the Brill tag-
ger (Brill, 1992).4
Furthermore, the use of WPDV allows leave-
one-out5 application for all components6 so that
the tagger can, without any additional effort, be
used in two different modes: a) with the test set
equal to the training set and b) with the test set
disjoint from the training set. In the first mode,
the tagger will have a very large amount of spe-
cific knowledge in each situation. We should ex-
pect errors under these circumstances to show
&amp;quot;hard&amp;quot; inconsistencies, such as the same word
receiving different tags in the company of the
same tags in the direct context. In the second
mode, the tagger is operating &amp;quot;normally&amp;quot;, as if
tagging unseen data. Here, we should expect
&amp;quot;soft&amp;quot; inconsistencies, more to do with types
of words and types of contexts than with ex-
act words and contexts. We should also expect
more errors due to tagger generator learning dis-
abilities here, and the resulting higher error rate
will force us to select a smaller fraction of the
errors for detailed examination.
</bodyText>
<footnote confidence="0.60994985">
4These percentages have been measured on a 115Kw
test set. This means that the 99% confidence intervals
are 97.71-97.93%, 97.43-97.67%, 97.40-97.64%, 96.93-
97.19% and 96.23-96.51% respectively.
5The normal way to test a tagger is by splitting the
available corpus into separate training and test sets, and
then train on the training set and test on the test set. In
this way the test is fair, as the test data has not not been
seen during training. The standard strategy is to split
the corpus into 10 parts, and to repeat the train-test
process 10 times, using each 10% part once as test data.
This is called 10-fold cross-validation. For some machine
learning systems, however, it is possible to (virtually)
remove the information about each individual instance
from the model(s) when that specific instance has to be
classified. This technique, called leave-one-out testing,
in effect allows total cross-validation, e.g. for the case at
hand one-million-fold.
6Even lexical lookup uses the WPDV system, so that
we can use leave-one-out here as well.
</footnote>
<sectionHeader confidence="0.969345" genericHeader="method">
4 Tagger-Corpus Disagreement
</sectionHeader>
<bodyText confidence="0.994266642857143">
When a tagger is generated from the written
text samples found on the BNC sampler CD,
and used to re-tag those samples in the two
modes described, we find an agreement rate of
99.45% when running without special measures
(i.e. test equal to train) and of 96.93% when
running with leave-one-out. In the first case,
there are 6326 errors, in the second 35563. As
we will want to compare the relative efficiency
of using one run or the other, we want to ex-
amine similar numbers of errors in each case.
Therefore, we take every 10th sentence for the
first set (615 errors) and every 50th sentence
for the second set (660 errors). Furthermore,
we choose the two sets in such a way that the
second set is a subset of the first one, so that we
can evaluate the relative recall of the different
runs. For the selected sentences, we examine
all tokens where disagreement occurs.7 In ad-
dition, in order to simulate random sampling,
we take every 1000th sentence of the original
corpus. For these sentences, we examine every
single token (1210 tokens in total) for errors or
inconsistencies in the corpus tagging, but with-
out any reference to automatic tagger output.
Every disagreement (or observed error in the
third group) is classified as to whether tagger
and/or original corpus are right or wrong. Such
a right-or-wrong decision is only taken if the
tagging manual (or, as a backup, the linguis-
tic tradition) is clear on the subject.8 If such
clarity does not exist, the full original corpus
is inspected to determine if one of the possi-
ble tags is chosen in a substantial majority of
instances of the same situation, in which case
that tag is assumed to be the correct one. The
resulting classification makes use of the follow-
ing four classes:
T Tagger error. The original corpus is correct,
the tagger is wrong.
B Benchmark error. The tagger is correct, the
original corpus is wrong.
</bodyText>
<footnote confidence="0.855891">
7We ignore all other tokens. This means that, if there
are tokens which receive the same erroneous tag in both
original corpus and tagger output, these will not be ex-
amined, and the error will not be detected.
8As we are taking the point of view of the average
user, we use only the tagging manual that is found on the
BNC Sampler CD. No reference is made to other manuals
in the CLAWS tradition, such as Johansson (1986).
</footnote>
<page confidence="0.999294">
51
</page>
<tableCaption confidence="0.998664">
Table 1: Assignment of blame for corpus-tagger
</tableCaption>
<table confidence="0.8914768">
disagreement (see text for key).
T BX I
full run 615 416 121 5 73
leave-one-out 660 503 84 6 67
random sample 1210 - 6 - 18
</table>
<bodyText confidence="0.9614598">
X Extreme error. Both the original corpus and
the tagger are wrong.
I Inconsistency. The manual does not indicate
a single correct choice and the practice in
the corpus varies.
The number of times these classes are found in
each of the three examinations are listed in Ta-
ble 1.
Both examinations based on disagreement be-
tween automatic tagger and corpus provide a
high number of inconsistency-linked situations,
certainly much higher than that provided by
random sample examination. Which of the two
tagger runs is more useful depends on what we
intend to do with the results.
The most likely aim is the identification of all
erroneous tags and inconsistencies in the orig-
inal corpus. In this case, we are mostly in-
terested in recall and the leave-one-out run is
preferable. Assuming that the distribution of
classes remains the same throughout the corpus,
examination of all 35563 disagreements found
with the leave-one-out run would yield 4850
(90/660 of 35563) corpus errors and a further
3610 (67/660 of 35563) tokens which are cur-
rently tagged inconsistently and which there-
fore may also have to be adjusted. With the full
run, we would only have to check 6326 disagree-
ments, but this inspection would yield only 1296
errors and 751 inconsistent tags (1 in 3.7 and 1
in 4.8). We see comparable figures when we ex-
amine the part of the corpus which has been
checked for both runs:9 only 25 of the 90 cor-
pus errors which are detected because they are
flagged by the leave-one-out run are also flagged
by the full run (1 in 3.6) and 15 of the 67 incon-
sistencies (1 in 4.5).10 However, even the higher
9Remember that the 1/50 part of the corpus checked
for the leave-one-out run is a subset of the 1/10 part
checked for the full run.
</bodyText>
<footnote confidence="0.777422">
19There is only one inconsistency flagged by the full
</footnote>
<bodyText confidence="0.9992425">
recall of the leave-one-out run is insufficient to
find all erroneous tags and inconsistencies. In
the random sample, we spotted only 6 corpus
errors, but of those 6 only 2 are flagged by ei-
ther tagger run, and of the 18 spotted inconsis-
tencies, 9 escape unflagged.11
However, the unflagged errors and inconsis-
tencies all show similarities in context with
errors and inconsistencies which have been
flagged. Therefore, we can adjust our proposal
and switch to a two-phase inconsistency deter-
mination:
</bodyText>
<listItem confidence="0.98564875">
1. use tagger disagreement to determine con-
texts where inconsistency occurs
2. examine all instances of those contexts in
the full corpus
</listItem>
<bodyText confidence="0.999950625">
With the revised strategy, recall is only interest-
ing with regard to the number of context classes
which are identified, and precision is more im-
portant, as it helps increase the efficiency of
the process. Furthermore, precision is also the
more important property if we do not intend
to identify and correct every individual error
in the corpus, but only want to get a general
impression of tagging quality. From Table 1,
it would seem that the full run has a higher
precision, as it contains 20.5% (126/615) er-
rors and 11.9% (73/615) inconsistencies, versus
13.6% and 10.2% for the leave-one-out run. In
the next section, we will examine whether it also
has sufficient recall as to the context classes we
want to identify.
</bodyText>
<sectionHeader confidence="0.995192" genericHeader="method">
5 Inconsistency Context Classes
</sectionHeader>
<bodyText confidence="0.99991">
Apart from classifying who is to blame for each
disagreement, we have also classified all dis-
agreements for the type of situation they rep-
resent, i.e. their inconsistency context class.
This classification has been done manually, and
it is here that the abovementioned need for
human knowledge and generalisation skills be-
comes very clear. As an example, where &amp;quot;be-
fore&amp;quot; in &amp;quot;just before the film began&amp;quot; is tagged
II (preposition) instead of CS (subordinating
conjunction), we judge that it is a case of
generic preposition-conjunction confusion, and
</bodyText>
<footnote confidence="0.71390325">
run which is missed by the leave-one-out run. There are
no errors for which this is the case.
11These numbers are too small for a statistically sen-
sible extrapolation to the whole corpus.
</footnote>
<page confidence="0.997918">
52
</page>
<bodyText confidence="0.99300902">
that there is no need for subclassification based
on the actual word in question or on the con-
text. However, where the same thing happens
with &amp;quot;as&amp;quot; in &amp;quot;such a stiff fabric as damast&amp;quot;
we decide that this disagreement belongs to
a more specific class (confusion for the word
&amp;quot;as&amp;quot;), since it is the comparison aspect of
&amp;quot;as&amp;quot; which leads to conjunction being prefer-
able to preposition. The creation of classes like
preposition-conjunction confusion could fairly
easily be done automatically, as they correspond
to specific tag (or tag group) confusions and
could be based on confusion lists and numbers
of times the confusion is found. However, finer
distinctions like &amp;quot;as&amp;quot;-confusion, or like confu-
sion for words ending in &amp;quot;-ing&amp;quot; when in noun-
modifying position, can best be decided on man-
ually.
The final result of the classification for the
examined disagreements is a list of 51 classes,
which is shown in Table 2, together with the
number of corresponding disagreements in the
different evaluations.12
For most of the classes, we find corpus er-
rors, sometimes very unexpected ones, e.g. 4
of the 5 &amp;quot;single letter&amp;quot; errors are instances of
the personal pronoun &amp;quot;I&amp;quot; which are erroneously
tagged as proper noun. For some classes, only
tagger errors are found, but even these may be
traced back directly to corpus errors elsewhere,
e.g. the 2 &amp;quot;letter combination&amp;quot; errors are both
tagger errors but are clearly caused by 16 mis-
uses of the ZZ2 tag13 in the corpus, and the
consistent mistagging by the tagger of &amp;quot;in front
of&amp;quot; as preposition-noun-preposition instead of a
multi-token preposition is (at least partly) due
to a single such mistagging in the corpus. Only
rarely, e.g. with the confusion between present
tense verb and infinitive, does it appear that the
blame can be put entirely on the inability of the
tagger generator to learn to make the necessary
distinction.
This means that practically all classes are use-
ful for the strategy proposed above, and the tag-
ger runs hence have to flag instances of as many
121n those cases where a disagreement could be as-
signed to more than one class, the most specific class has
been selected, e.g. a potential location-indicating noun
(NNL) in noun-modifying position is classed as special
noun type rather than generic noun modifier.
</bodyText>
<footnote confidence="0.628487">
13 ZZ2 is meant for plural forms of letters, such as &amp;quot;a&apos;s&amp;quot; ,
but is also found for tokens like &amp;quot;AA&amp;quot;.
</footnote>
<bodyText confidence="0.999850357142857">
classes as possible. Examination of the table
shows that both the full run and the leave-one-
out run provide 49 of the 51 classes. This would
indicate that either run can be used, as similar
numbers of inspected tokens yield similar num-
bers of classes. However, we would advise us-
ing a combination of the two as this is likely to
provide a more varied sample. Whatever sam-
ple of flagged tokens is used, after determining
the inconsistency classes, it will be necessary to
use specific searches on the whole corpus to de-
termine which words (and/or which contexts)
belong to those classes.
As an example, let us look at the &amp;quot;preposition
vs -ing participle&amp;quot; class. The two tagger runs
only show disagreements with &amp;quot;including&amp;quot;, &amp;quot;ex-
cluding&amp;quot; and &amp;quot;following&amp;quot;. However, a full search
shows that &amp;quot;barring&amp;quot;, &amp;quot;concerning&amp;quot;, &amp;quot;consider-
ing&amp;quot; and &amp;quot;regarding&amp;quot; are also tokens which are
sometimes tagged as preposition and sometimes
as participle. At least &amp;quot;concerning&amp;quot; and &amp;quot;bar-
ring&amp;quot; appear to have some corpus errors con-
nected to them. The situation is especially bad
for &amp;quot;barring&amp;quot;, where two of the three examples
are suspect: in &amp;quot;laws barring the manufacture
of cocaine&amp;quot; the tag II is chosen and in &amp;quot;barring
a disaster, the payout will be the same&amp;quot; the tag
VVG (ing-participle).14
</bodyText>
<sectionHeader confidence="0.999339" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999882777777778">
The proposed method, generating a wordclass
tagger from the tagged corpus and comparing
its output with the original corpus, turns out
to be an efficient means of identifying inconsis-
tency in the corpus tagging. In both modes of
operation, without special measures and with
leave-one-out, a substantial percentage of dis-
agreements are linked to inconsistency.
If one intends to eradicate all errors and in-
consistencies, the method will have to be com-
bined with other types of sampling, as not all in-
stances are themselves flagged as disagreements.
However, these other types of sampling can be
based on a classification of contexts underlying
inconsistency. Determination of the classes in-
volved can be done by random sampling, but is
much more efficient when done on the basis of
the tagging disagreements.
</bodyText>
<footnote confidence="0.88486">
141n the third example, at least, the wordplay &amp;quot;an un-
usual example of a gift barring Greeks&amp;quot; we find the cor-
rect tag, VVG.
</footnote>
<page confidence="0.998501">
53
</page>
<bodyText confidence="0.98480776">
Furthermore, if one decides that (some of the)
additional sampling is too labour-intensive,15
inspecting and, where necessary, correcting only
the flagged tokens already provides a substan-
tial consistency improvement. Which type of
run to use for this probably depends on the
available manpower. The leave-one-out run pro-
vides the best recall of errors and inconsisten-
cies, but flags about five times more tokens than
the full run.
With both choices of run type, the reduction
of items to be checked is dependent on the qual-
ity of the generated tagger. For the corpus and
tagger generator used in this paper, the num-
ber of flagged tokens is relatively low, and cer-
tainly low enough to be manually re-checked
completely. For other tagged corpora or tag-
ger generators, the relative number may well be
higher, but we expect the method to be cost-
effective as long as the annotation is limited to
wordclass tagging. Something which has yet to
be investigated is whether the use of the same
tagger generator as has been employed during
the original tagging of the corpus might inter-
fere with the inconsistency detection. While
this is uncertain, it seems wise to alway use a
different type of tagger generator, which should
not be a problem, given the wide choice of avail-
able systems.
For other corpus annotation tasks, such as
word sense tagging or syntactic annotation, the
quality of machine learning systems tends to
be much lower. If the automatic re-annotation
method is to be used here, we strongly sug-
gest the use of several machine learning systems.
Preferably these are then combined, e.g. as de-
scribed by van Halteren et al. (To appear). If
the combination system is still too inaccurate
for a full inspection of all flagged items, the
best items to check will be those where all (or
at least a substantial majority of) the systems
agree, but disagree with corpus annotation. Af-
ter all, a wrong prediction by one or two systems
can easily be blamed on a learning disability on
the part of the systems, but the same wrong pre-
diction by a majority of the systems is a strong
indication that it is probably the corpus anno-
tation that is mistaken.
15E.g. there are 22900 instances of tokens which can
be either preposition or conjunction.
</bodyText>
<sectionHeader confidence="0.965394" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.962695693877551">
J.P. Baker. 1997. Consistency and accuracy
in correcting automatically tagged data. In
Garside, Leech, and McEnery (eds), Corpus
annotation, pages 243-250. Addison Wesley
Longman, London.
Thorsten Brants. 1999. Tagging and Parsing
with Cascaded Markov Models - Automation
of Corpus Annotation. Saarbrficken Disserta-
tions in Computational Linguistics and Lan-
guage Technology. German Research Center
for Artificial Intelligence and Saarland Uni-
versity, Saarbrficken, Germany.
E. Brill. 1992. A simple rule-based part-of-
speech tagger. In Proc. of the Third ACL
Conference on Applied NLP, Trento.
D. Cutting, J. Kupiec, J. Pedersen, and P. Si-
bun. 1992. A practical part-of-speech tagger.
In Proc. of the Third ACL Conference on Ap-
plied NLP, Trento.
W. Daelemans, J. Zavrel, P. Berck, and S. Gillis.
1996. MBT: A memory-based part of speech
tagger generator. In Ejerhed and Dagan
(eds), Proc. of Fourth Workshop on Very
Large Corpora, pages 14-27. ACL SIGDAT.
R. Garside and N. Smith. 1997. A hybrid
grammatical tagger: cLAws4. In Garside,
Leech, and McEnery (eds), Corpus annota-
tion, pages 102-121. Addison Wesley Long-
man, London.
H. van Halteren, J. Zavrel, and W. Dade-
mans. To appear. Improving accuracy in
NLP through combination of machine learn-
ing systems. Computational Linguistics.
H. van Halteren. To appear. Weighted Prob-
ability Distribution Voting, an introduction.
In Computational Linguistics in the Nether-
lands, 1999.
S. Johansson. 1986. The tagged LOB Corpus:
User&apos;s Manual. Norwegian Computing Cen-
tre for the Humanities, Bergen, Norway.
M. Marcus, B. Santorini, and M. Marcinkiewicz.
1993. Building a large annotated corpus of
English: the Penn Treebank. Computational
Linguistics, 19:313-330.
A. Ratnaparkhi. 1996. A maximum entropy
part-of-speech tagger. In Proc. of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing, May 17-18, 1996, Univer-
sity of Pennsylvania.
</reference>
<page confidence="0.999564">
54
</page>
<tableCaption confidence="0.7989105">
Table 2: Classes of inconsistency contexts (errors made in both corpus and tagger are shown as B
and T rather than X, so that the sum of the blame types can be higher than the total count).
</tableCaption>
<figureCaption confidence="0.683005756756757">
Context type Full Leave-one-out Random
special noun direction (ND) 2 (2I) 5 (5I) -
types title (NNA and NNB) 2 (11 1T) 3 (11 2T)
location (NNL) 13 (1B 121) 20 (201) 2 (21)
time (NNT) 2 (2T) 3 (21 1T) 1 (1I)
measure (NNU) 3 (2B 1T) 6 (3B 21 2T) -
day or month (NPD and NPM) 2 (2T)
capitalised word 38 (8B 51 26T) 23 (10B 51 12T) 4 (1B 31)
nominalised adjective 31 (8B 11 21T) 19 (7B 12T) 2 (2I)
nominalised -ing form 18 (5B 41 9T) 17 (4B 13T) 4 (1B 31)
noun modifiers -ing form (JJ vs VVG) 22 (3B 51 14T) 23 (8B 51 11T) 1 (11)
or complements -ed form (JJ vs VVN) 32 (5B 91 18T) 35 (7B 51 23T) 1 (1I)
-ed form of noun 1 (1T) 3 (1B 1I 1T) -
-ist form (JJ vs NN) 1 (11) 3 (31)
capitalised word 36 (5B 51 27T) 21 (5B 51 12T) 3 (31)
other 25 (5B 41 18T) 23 (4B 31 16T) 1 (1B)
quantity-related number of noun 5 (2B 11 3T) 6 (1B 41 1T)
quantification 16 (16T) 5 (5T) -
modifier of number 12 (2B 61 4T) 5 (2B 3T) -
verb tense -ed form (past vs part) 39 (2B 11 36T) 33 (7B 31 23T)
base form (pres vs infin) 39 (39T) 21 (21T) -
other 9 (9T) 8 (1B 7T) -
adverbs adjectives used as (JJ vs R) 19 (1B 18T) 7 (2B 11 4T)
function of (RG vs RR vs RP) 6 (6T) 4 (1B 3T) -
prepositions vs conjunction 24 (5B 21 16T) 13 (8B 5T)
vs verb particle 18 (1B 11 16T) 21 (1B 20T)
vs locative adverb 2 (2T) 3 (3T) -
vs verb participle 2 (1I 1T) 1 (1T) -
difficult words as 10 (1B 9T) 12 (3B 9T) -
his and her 4 (4T) 1 (1T) -
once 3 (1B 2T)
one 7 (1B 11 5T) 1 (1T) 1 (1B)
&apos;s 2 (2T) 4 (2B 2T) 1 (1B)
SO 2 (2B 1T) 3 (1B 2T)
that 6 (2B 4T) 2 (1B 1T)
there 3 (1B 2T) 1 (1B)
to 8 (1B 7T) 10 (1B 9T)
</figureCaption>
<bodyText confidence="0.8943628">
when and where 6 (1B 5T) 8 (4B 4T) -
not English words capitalised foreign word 9 (81 1T)
foreign word 9 (1B 31 5T) 10 (41 6T) -
formula vs digit-letter 7 (5B 3T) 9 (7B 2T) -
single letter (ZZ1) 3 (1B 2T) 5 (4B 1T)
letter combination (ZZ2) 2 (2T)
multi-token units unrecognised 30 (2B 21 26T) 69 (2B 67T) -
falsely recognised 17 (8B 9T) 4 (2B 2T) 2 (21)
impossible tag capitalised words 6 (1B 5T) 7 (2B 5T) -
for token other 30 (4B 26T) 25 (8B 17T) 1 (1B)
miscellaneous untaggable words (FU) 1 (1T) 11 (11T)
strange spelling 8 (1B 8T) 14 (5B 11T) -
capitalised words 18 (18T) 14 (1B 13T)
noun-verb confusion 48 (2B 46T) Si (7B 44T)
other 13 (13T) 12 (3B 9T) -
</bodyText>
<page confidence="0.991355">
55
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.481734">
<title confidence="0.999934">The Detection of Inconsistency in Manually Tagged Text</title>
<author confidence="0.999915">Hans van</author>
<affiliation confidence="0.996849">Dept. of Language and University of</affiliation>
<address confidence="0.8820795">P.O. Box 6500 HD</address>
<note confidence="0.663317">The hvh@let.kun.n1</note>
<abstract confidence="0.99920675">This paper proposes a method to detect the presence of inconsistency in a given manually tagged corpus. The method consist of generating an automatic tagger on the basis of the corpus and then comparing the tagger&apos;s output with the original tagging. It is tested using the written texts from the BNC sampler and a WPDV-based tagger generator, and shown to be both an efficient method to derive a qualitative evaluation of consistency and a useful first step towards correction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J P Baker</author>
</authors>
<title>Consistency and accuracy in correcting automatically tagged data.</title>
<date>1997</date>
<booktitle>In Garside, Leech, and McEnery (eds), Corpus annotation,</booktitle>
<pages>243--250</pages>
<publisher>Addison Wesley Longman,</publisher>
<location>London.</location>
<contexts>
<context position="1265" citStr="Baker (1997)" startWordPosition="206" endWordPosition="207"> towards correction. 1 Introduction Wordclass tagged corpora are a very popular resource for both language engineers and linguists. If these corpora are used for inspiration and exemplification, size may be more important than quality and a fully automatically tagged corpus can suffice. For other uses, quality is of much higher importance, and here there will generally be a preference for manually corrected corpora, even though they may be smaller. However, manual correction means human involvement, and that again means a much higher potential for inconsistency (cf. e.g. Marcus et al. (1993); Baker (1997)). Before we go and base our NLP systems or linguistic theories on the wordclass tags found in a tagged corpus, then, it would certainly be a good idea to evaluate whether those tags have indeed been assigned appropriately, and, if not, possibly correct the situation. This means that we have to inspect (part of) the corpus and decide whether the tags are consistent with the tagging manual or, if the tagging manual is not clear on the subject, whether the tags have at least been applied consistently throughout the corpus. In this paper we show, by way of an experiment, how this task can be made</context>
</contexts>
<marker>Baker, 1997</marker>
<rawString>J.P. Baker. 1997. Consistency and accuracy in correcting automatically tagged data. In Garside, Leech, and McEnery (eds), Corpus annotation, pages 243-250. Addison Wesley Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
</authors>
<title>Tagging and Parsing with Cascaded Markov Models - Automation of Corpus Annotation.</title>
<date>1999</date>
<booktitle>Saarbrficken Dissertations in Computational Linguistics and Language Technology. German Research Center for Artificial Intelligence</booktitle>
<institution>and Saarland University,</institution>
<location>Saarbrficken, Germany.</location>
<contexts>
<context position="12683" citStr="Brants, 1999" startWordPosition="2180" endWordPosition="2181">to right, while the other uses reverse order, i.e. tags from right to left. 6. The final selection of the tag for each token is determined by a WPDV model using the suggestions of the two context-dependent 50 models for the focus and two tokens on either side of it. There are two reasons for the selection of this particular tagger generator. First, an evaluation with the same training and test set used by van Halteren et al. (To appear) has shown this tagging strategy to compare favourably with other state-of-the-art tagger generators: 97.82% agreement with the test set versus 97.55% for TnT (Brants, 1999), 97.52% for MXPOST (Ratnaparkhi, 1996), 97.06% for MBT (Daelemans et al., 1996) and 96.37% for the Brill tagger (Brill, 1992).4 Furthermore, the use of WPDV allows leaveone-out5 application for all components6 so that the tagger can, without any additional effort, be used in two different modes: a) with the test set equal to the training set and b) with the test set disjoint from the training set. In the first mode, the tagger will have a very large amount of specific knowledge in each situation. We should expect errors under these circumstances to show &amp;quot;hard&amp;quot; inconsistencies, such as the sam</context>
</contexts>
<marker>Brants, 1999</marker>
<rawString>Thorsten Brants. 1999. Tagging and Parsing with Cascaded Markov Models - Automation of Corpus Annotation. Saarbrficken Dissertations in Computational Linguistics and Language Technology. German Research Center for Artificial Intelligence and Saarland University, Saarbrficken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A simple rule-based part-ofspeech tagger.</title>
<date>1992</date>
<booktitle>In Proc. of the Third ACL Conference on Applied NLP,</booktitle>
<location>Trento.</location>
<contexts>
<context position="12809" citStr="Brill, 1992" startWordPosition="2202" endWordPosition="2203"> determined by a WPDV model using the suggestions of the two context-dependent 50 models for the focus and two tokens on either side of it. There are two reasons for the selection of this particular tagger generator. First, an evaluation with the same training and test set used by van Halteren et al. (To appear) has shown this tagging strategy to compare favourably with other state-of-the-art tagger generators: 97.82% agreement with the test set versus 97.55% for TnT (Brants, 1999), 97.52% for MXPOST (Ratnaparkhi, 1996), 97.06% for MBT (Daelemans et al., 1996) and 96.37% for the Brill tagger (Brill, 1992).4 Furthermore, the use of WPDV allows leaveone-out5 application for all components6 so that the tagger can, without any additional effort, be used in two different modes: a) with the test set equal to the training set and b) with the test set disjoint from the training set. In the first mode, the tagger will have a very large amount of specific knowledge in each situation. We should expect errors under these circumstances to show &amp;quot;hard&amp;quot; inconsistencies, such as the same word receiving different tags in the company of the same tags in the direct context. In the second mode, the tagger is opera</context>
</contexts>
<marker>Brill, 1992</marker>
<rawString>E. Brill. 1992. A simple rule-based part-ofspeech tagger. In Proc. of the Third ACL Conference on Applied NLP, Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cutting</author>
<author>J Kupiec</author>
<author>J Pedersen</author>
<author>P Sibun</author>
</authors>
<title>A practical part-of-speech tagger.</title>
<date>1992</date>
<booktitle>In Proc. of the Third ACL Conference on Applied NLP,</booktitle>
<location>Trento.</location>
<contexts>
<context position="11455" citStr="Cutting et al. (1992)" startWordPosition="1964" endWordPosition="1967">n. 4. For tokens which were observed 10 times or more in the training set, only the output of the lexical lookup is used. For all other tokens, the output of the lexical similarity lookup is added. The resulting list of tags is used in two ways. Throughout the tagging process, the full list is used as a filter on the potential tags for a token, i.e. even if the context provides overwhelming evidence that a specific tag should be used, the tag is ruled out if it does not occur in the list. Additionally, the most probable tags in the list (up to three) are used to define an ambiguity class (cf. Cutting et al. (1992)) for the token, which is used in the contextdependent components. The lexical probabilities of the tags are used only to determine the selection for presence (and relative position) in the ambiguity class. They are not used in the context-dependent components. 5. In the main context-dependent components, two WPDV models then determine the most probable tag for each token on the basis of the (disambiguated) tags of two preceding tokens and the ambiguity classes of the focus and two following tokens. The difference between the two models is that one follows the normal order of the tokens, i.e. </context>
</contexts>
<marker>Cutting, Kupiec, Pedersen, Sibun, 1992</marker>
<rawString>D. Cutting, J. Kupiec, J. Pedersen, and P. Sibun. 1992. A practical part-of-speech tagger. In Proc. of the Third ACL Conference on Applied NLP, Trento.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>MBT: A memory-based part of speech tagger generator.</title>
<date>1996</date>
<booktitle>In Ejerhed and Dagan (eds), Proc. of Fourth Workshop on Very Large Corpora,</booktitle>
<pages>14--27</pages>
<publisher>ACL SIGDAT.</publisher>
<contexts>
<context position="12763" citStr="Daelemans et al., 1996" startWordPosition="2190" endWordPosition="2194">left. 6. The final selection of the tag for each token is determined by a WPDV model using the suggestions of the two context-dependent 50 models for the focus and two tokens on either side of it. There are two reasons for the selection of this particular tagger generator. First, an evaluation with the same training and test set used by van Halteren et al. (To appear) has shown this tagging strategy to compare favourably with other state-of-the-art tagger generators: 97.82% agreement with the test set versus 97.55% for TnT (Brants, 1999), 97.52% for MXPOST (Ratnaparkhi, 1996), 97.06% for MBT (Daelemans et al., 1996) and 96.37% for the Brill tagger (Brill, 1992).4 Furthermore, the use of WPDV allows leaveone-out5 application for all components6 so that the tagger can, without any additional effort, be used in two different modes: a) with the test set equal to the training set and b) with the test set disjoint from the training set. In the first mode, the tagger will have a very large amount of specific knowledge in each situation. We should expect errors under these circumstances to show &amp;quot;hard&amp;quot; inconsistencies, such as the same word receiving different tags in the company of the same tags in the direct co</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>W. Daelemans, J. Zavrel, P. Berck, and S. Gillis. 1996. MBT: A memory-based part of speech tagger generator. In Ejerhed and Dagan (eds), Proc. of Fourth Workshop on Very Large Corpora, pages 14-27. ACL SIGDAT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Garside</author>
<author>N Smith</author>
</authors>
<title>A hybrid grammatical tagger: cLAws4.</title>
<date>1997</date>
<booktitle>In Garside, Leech, and McEnery (eds), Corpus annotation,</booktitle>
<pages>102--121</pages>
<publisher>Addison Wesley Longman,</publisher>
<location>London.</location>
<contexts>
<context position="2382" citStr="Garside and Smith (1997)" startWordPosition="397" endWordPosition="400">nsistently throughout the corpus. In this paper we show, by way of an experiment, how this task can be made more efficient with the help of software already in general use in wordclass tagging circles, viz, tagger generators. The tagged corpus on which we perform our experiment consists of all the written texts of the BNC sampler CD. Its size (about 1Mw) is average for manually corrected corpora, the tagset is well-developed (C7) and the tagging process has involved the use of an equally welldeveloped automatic tagger (cLAws4) and subsequent correction by a team of experienced annotators (cf. Garside and Smith (1997)). We can assume that the consistency may not be as high as that of the LOB corpus, which by now has reached an admirable level of consistency, but certainly higher than notoriously inconsistent corpora like the Wall Street Journal (cf. van Halteren et al. (To appear)). In the following sections, we first examine the concept of consistency (section 2), then describe the tagger generator used in the experiment (section 3), evaluate the output of the experiment (sections 4 and 5), and conclude by summarising the main findings (section 6). 2 Consistency and its Evaluation It is generally agreed t</context>
</contexts>
<marker>Garside, Smith, 1997</marker>
<rawString>R. Garside and N. Smith. 1997. A hybrid grammatical tagger: cLAws4. In Garside, Leech, and McEnery (eds), Corpus annotation, pages 102-121. Addison Wesley Longman, London.</rawString>
</citation>
<citation valid="false">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Dademans</author>
</authors>
<title>To appear. Improving accuracy in NLP through combination of machine learning systems.</title>
<journal>Computational Linguistics.</journal>
<marker>van Halteren, Zavrel, Dademans, </marker>
<rawString>H. van Halteren, J. Zavrel, and W. Dademans. To appear. Improving accuracy in NLP through combination of machine learning systems. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
</authors>
<title>To appear. Weighted Probability Distribution Voting, an introduction.</title>
<date>1999</date>
<booktitle>In Computational Linguistics in the Netherlands,</booktitle>
<marker>van Halteren, 1999</marker>
<rawString>H. van Halteren. To appear. Weighted Probability Distribution Voting, an introduction. In Computational Linguistics in the Netherlands, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Johansson</author>
</authors>
<title>The tagged LOB Corpus: User&apos;s Manual. Norwegian Computing Centre for the Humanities,</title>
<date>1986</date>
<location>Bergen, Norway.</location>
<contexts>
<context position="17179" citStr="Johansson (1986)" startWordPosition="2954" endWordPosition="2955">assification makes use of the following four classes: T Tagger error. The original corpus is correct, the tagger is wrong. B Benchmark error. The tagger is correct, the original corpus is wrong. 7We ignore all other tokens. This means that, if there are tokens which receive the same erroneous tag in both original corpus and tagger output, these will not be examined, and the error will not be detected. 8As we are taking the point of view of the average user, we use only the tagging manual that is found on the BNC Sampler CD. No reference is made to other manuals in the CLAWS tradition, such as Johansson (1986). 51 Table 1: Assignment of blame for corpus-tagger disagreement (see text for key). T BX I full run 615 416 121 5 73 leave-one-out 660 503 84 6 67 random sample 1210 - 6 - 18 X Extreme error. Both the original corpus and the tagger are wrong. I Inconsistency. The manual does not indicate a single correct choice and the practice in the corpus varies. The number of times these classes are found in each of the three examinations are listed in Table 1. Both examinations based on disagreement between automatic tagger and corpus provide a high number of inconsistency-linked situations, certainly mu</context>
</contexts>
<marker>Johansson, 1986</marker>
<rawString>S. Johansson. 1986. The tagged LOB Corpus: User&apos;s Manual. Norwegian Computing Centre for the Humanities, Bergen, Norway.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--313</pages>
<contexts>
<context position="1251" citStr="Marcus et al. (1993)" startWordPosition="201" endWordPosition="205">nd a useful first step towards correction. 1 Introduction Wordclass tagged corpora are a very popular resource for both language engineers and linguists. If these corpora are used for inspiration and exemplification, size may be more important than quality and a fully automatically tagged corpus can suffice. For other uses, quality is of much higher importance, and here there will generally be a preference for manually corrected corpora, even though they may be smaller. However, manual correction means human involvement, and that again means a much higher potential for inconsistency (cf. e.g. Marcus et al. (1993); Baker (1997)). Before we go and base our NLP systems or linguistic theories on the wordclass tags found in a tagged corpus, then, it would certainly be a good idea to evaluate whether those tags have indeed been assigned appropriately, and, if not, possibly correct the situation. This means that we have to inspect (part of) the corpus and decide whether the tags are consistent with the tagging manual or, if the tagging manual is not clear on the subject, whether the tags have at least been applied consistently throughout the corpus. In this paper we show, by way of an experiment, how this ta</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19:313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy part-of-speech tagger.</title>
<date>1996</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="12722" citStr="Ratnaparkhi, 1996" startWordPosition="2185" endWordPosition="2186">erse order, i.e. tags from right to left. 6. The final selection of the tag for each token is determined by a WPDV model using the suggestions of the two context-dependent 50 models for the focus and two tokens on either side of it. There are two reasons for the selection of this particular tagger generator. First, an evaluation with the same training and test set used by van Halteren et al. (To appear) has shown this tagging strategy to compare favourably with other state-of-the-art tagger generators: 97.82% agreement with the test set versus 97.55% for TnT (Brants, 1999), 97.52% for MXPOST (Ratnaparkhi, 1996), 97.06% for MBT (Daelemans et al., 1996) and 96.37% for the Brill tagger (Brill, 1992).4 Furthermore, the use of WPDV allows leaveone-out5 application for all components6 so that the tagger can, without any additional effort, be used in two different modes: a) with the test set equal to the training set and b) with the test set disjoint from the training set. In the first mode, the tagger will have a very large amount of specific knowledge in each situation. We should expect errors under these circumstances to show &amp;quot;hard&amp;quot; inconsistencies, such as the same word receiving different tags in the </context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy part-of-speech tagger. In Proc. of the Conference on Empirical Methods in Natural Language Processing, May 17-18, 1996, University of Pennsylvania.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>