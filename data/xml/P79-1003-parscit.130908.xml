<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000005">
<sectionHeader confidence="0.37699" genericHeader="method">
WORD EXPERT PARSING&apos;
</sectionHeader>
<author confidence="0.857448">
Steven L. Small
</author>
<affiliation confidence="0.9960425">
Department of Computer Science
University of Maryland
</affiliation>
<address confidence="0.586509">
College Park, Maryland 20742
</address>
<bodyText confidence="0.999023818181818">
This paper describes an approach to conceptual analysis and understanding of natural language in which
linguistic knowledge centers on individual words, and the analysis mechanisms consist of interactions
among distributed procedural experts representing that knowledge. Each word expert models the process
of diagnosing the intended usage of a particular word in context. The Word Expert Parser performs
conceptual analysis through the interactions of the individual experts, which ask questions and
exchange information in converging on a single mutually acceptable sentence meaning. The Word Expert
theory is advanced as a better cognitive model of natural language understanding than the traditional
rule-based approaches. The Word Expert Parser models parts of the theory, and the important issues of
control and representation that arise in developing such a model form the basis of the technical
discussion. An example from the prototype LISP implementation helps explain the theoretical results
presented.
</bodyText>
<sectionHeader confidence="0.934123" genericHeader="method">
I. Introduction
</sectionHeader>
<bodyText confidence="0.999926027522936">
Computational understanding of natural language
requires complex interactions among a variety of distinct
yet redundant mechanisms. The construction of a computer
program to perform such a task begins with the
development of an organizational framework which
inherently incorporates certain assumptions about the
nature of these processes and the environment in which
they take place. Such cognitive premises affect
profoundly the scope and substance of computational
analysis for comprehension as found in the program.
This paper describes a theory of conceptual parsing
which considers knowledge about language to be
distributed across a collection of procedural experts
centered on individual words. Natural language parsing
with word experts entails several new hypotheses about
the organization and representation of linguistic and
pragmatic knowledge for computational language
comprehension. The Word Parser [1] demonstrates
how the word expert Tra127 coupled--111Th certain other
choices based on previous work, affect structure and
process in a cognitive model of parsing.
The Word Expert Parser is a cognitive model of
conceptual language analysis in which the unit of
linguistic knowledge is the word and the focua of
research is the set of processes underlying
comprehension. The model is aimed directly at problems
of word sense ambiguity and idiomatic expressions, and in
greatly generalizing the notion of word sense, promotes
these issues to a central place in the study of language
parsing. Parsing models typically cope unsatisfactorily
with the wide heterogeneity of usages of particular
words. If a sentence contains a standard form of a word,
It can usually be parsed; if it involves a less prevalent
form which has a different part of speech, perhaps it too
can be parsed. Distinguishing among the many senses of a
common verb, adjective, or pronoun, for example, or
correctly translating idioms are rarely possible.
At the source of this difficulty is the reliance on
rule-based formalisms, whether syntactic or semantic
(e.g. cases), which attempt to capture the linguistic
contributions inherent in constituent chunks of sentences
that consist of more than single words. A crucial
assumption underlying work on the Word Expert Parser is
that the fundamental unit of linguistic knowledge is the
word and that understanding its sense or role in a
particular context is the central parsing process. In
the parser to be described, the word expert constitutes
the kernel of linguistic knowledgna its representation
the elemental data structure. It is procedural in nature
and executes directly as a process, cooperating with the
other experts for a given sentence to arrive at a
mutually acceptable sentence meaning.
Certain principles behind the parser do not follow
directly from the view of word primacy but from other
recent theories of parsing. The cognitive processes
involved in language comprehension comprise the fortis of
linguistic study of the word expert approach. Parsing is
viewed as an inferential process where linguistic
knowledge of syntax and semantics and general pragmatic
knowledge are applied in a uniform manner during
&apos;The research described in this report is funded by
the National Aeronautics and Space Administration under
grant number NSG-7251. Their support is gratefully
acknowledged.
interpretation. This methodological position closely
follows that of RiPsbeck (see [2] and [3]) and Schank
[4]. The central concern with word usage and word sense
ambiguity follows similar motivations of Wilks [5]. The
control structure of the Word Expert Parser results fror
agreoment with the hypothesis of Marcus that parsing can
he done deterministically and in a way in which
Information gained through interpretation is permanent
[6]. Rieger&apos;s view of inference as intelligent selection
among a number of competing plausible alternatives [7] of
course forms the cornerstone of the new theory. His
ideas on word sense selection for language analysis ([8]
and [9]) and strategy selection for general problem
solving [10]• constitute a consistent cognitive
perspective.
Any natural language understanding system must
incorporate mechanisms to perform word sense
disambiguation in the context of open-ended world
knowledge. The importance of these mechanisms for word
usage diagnosis derives from the ubiquity of local
ambiguities, and brought about the notion that they be
made the central processes of computational analysis and
understanding. Consideration of almost any English
content word leads to a realization of the scope of the
problem -- with a little time and perhaps help from the
dictionary, many distinct usages can be identified. As a
simple illustration, several usages each for the words
&amp;quot;heavy&amp;quot; and &amp;quot;ice&amp;quot; appear in Figure 1. Each of these
seemingly benign words exhibits a rich depth of
contextual use. An earlier paper contains a list of
almost sixty verbal usages for the word &amp;quot;take&amp;quot; [11].
The representation of all contextual word usages in
an active way that insures their utility for linguistic
diagnosis led to the notion of word experts. Each word
expert is a procedural entity-E8gaz&amp;nc of all possible
contextual interpretations of the word it represents.4
When placed in a context formed by experts for the other
words in a sentence, each expert should be capable of
sufficient context-probing and self-examination to
determine successfully its functional or semantic role,
and further, to realize the nature of that function or
the precise meaning of the word. The representation and
control issues involved in basing a parser on word
experts are discussed below following presentation of an
example execution of the existing Word Expert Parser.
</bodyText>
<sectionHeader confidence="0.897564" genericHeader="method">
2. Model Overview
</sectionHeader>
<bodyText confidence="0.988304857142857">
The Word Expert Parser successfully parses the
sentence
&amp;quot;The deep philosopher throws the peach pit
into the deep pit.&amp;quot;
through cooperation among the appropriate word experts.
Initialization of the parser consists of retrieving thT
experts for &amp;quot;the&amp;quot;, &amp;quot;deep&apos;, &amp;quot;philosopher&amp;quot;, &amp;quot;throw&apos; , s&amp;quot;,
</bodyText>
<footnote confidence="0.968464888888889">
2An important assumption of the word expert viewpoint
is that the set of such contextual word usages is not
only finite, but fairly small as well.
3The perspective of viewing language through lexical
contributions to structure and meaning has naturally led
to the development of word experts for common morphemes
that are not words (and even, experimentally, for
punctwatio9). Especially important is the word expert
for -ing , which aids significantly in helping to
</footnote>
<page confidence="0.996588">
9
</page>
<bodyText confidence="0.933178">
Some word senses of ,&amp;quot;heavy&amp;quot;
</bodyText>
<listItem confidence="0.885326">
1. An overweight pergon is politely called &amp;quot;heavy&amp;quot;:
&amp;quot;He has become quite heavy.&amp;quot;
2. Emotional music is referred to as &amp;quot;heavy&amp;quot;:
&amp;quot;Mahler writes heavy music.&amp;quot;
</listItem>
<bodyText confidence="0.7896384">
i. An intensity of precipitation is &amp;quot;heavy&amp;quot;:
&amp;quot;A heavy snow is expected today.&amp;quot;
Some word senses of &amp;quot;ice&amp;quot;
I. The solid state of water is called &amp;quot;ice&amp;quot;:
&amp;quot;Ice melts at 0°C.&amp;quot;
</bodyText>
<listItem confidence="0.97032">
2. &amp;quot;Ice&amp;quot; participates in an idiomatic nominal
describing a favorite delight:
&amp;quot;Homemade ice cream is delicious.&amp;quot;
3. &amp;quot;Dry ice&amp;quot; is the solid state of carbon dioxide:
&amp;quot;Dry ice will keep that cool all day.&amp;quot;
4. &amp;quot;Ice&amp;quot; or &amp;quot;iced&amp;quot; describes things that have been
cooled (sometimes with ice):
&amp;quot;One iced tea to go please.&amp;quot;
5. &amp;quot;Ice&amp;quot; also describes things made of ice:
&amp;quot;The ice sculptures are beautiful!&amp;quot;
</listItem>
<bodyText confidence="0.840886">
6,7. &amp;quot;Ice hockey&amp;quot; is the name of a popular sport which
has a rule penalizing an action called icing&amp;quot;:
&amp;quot;He iced the puck causing a face-off.&amp;quot;
S. The term &amp;quot;ice box&amp;quot; refers to both a box containing
ice used for cooling foods end a refrigerator:
&amp;quot;This ice box isn&apos;t plugged in!&amp;quot;
</bodyText>
<figureCaption confidence="0.997058">
Figure 1: Example contextual word usages
</figureCaption>
<bodyText confidence="0.996743217391304">
&amp;quot;over&amp;quot;, and go forth, from a disk file, and organizing
them along with data repositories called word bins in a
left to right order in the sentence levut—Wragc.
Note that three copies of tmw—mwrt 1T15--nt i )
copies of each expert for &amp;quot;deep&amp;quot; and &amp;quot;pit&amp;quot; appear in the
workspace. Since each expert executes as a process,4
each process instantiation in the workspace must be put
into an executable state. At this point, the parse is
ready to begin.
The word expert for &amp;quot;the&amp;quot; runs first, and is able to
terminate immediately, creating a new concept designator
(called a concept bin and participating in the concept
level workspace) Wffic1r-will eventually hold the data
7VUUL the intellectual philosopher described in the
input. Next the &amp;quot;deep&amp;quot; expert runs, and since &amp;quot;deep&amp;quot; has
a number of word senses,5 is unable to terminate (i.e:.
complete its discrimination task). Instead, it suspendS
its execution, stating the conditions upon which it
should be resumed. These conditions take the form of
associative trigger patterns, and are referred to as
disambiguate expressions involving gerunds or participles
such as &amp;quot;the man eating tiger&amp;quot;. A full discussion of
this will appear in (121.
</bodyText>
<sectionHeader confidence="0.684638" genericHeader="method">
4 ...
</sectionHeader>
<bodyText confidence="0.979200682242991">
Al6hough I call them &amp;quot;processes&amp;quot;, word experts are
actually coroutines resembling CONNIVER&apos;s generators
(13) and even more so, the stack groups of the MIT LISP
Machine (141.
5It should be clear that the notion of &amp;quot;word sense&amp;quot; as
used here encompasses what might more traditionally be
described as &amp;quot;contextual word usage&amp;quot;. Aspects of a mord
token&apos;s linguistic environment constitute its broadened
&amp;quot;sense&amp;quot;.
restart demons. The &amp;quot;deep&amp;quot; expert creates a restart
demon to wage itup when the sense of the nominal to its
right (i.e., &amp;quot;philosopher&amp;quot;) becomes known. The expert
for &amp;quot;philosopher now runs, observes the control state of
the parser, and contributes the fact that the new concept
refers to a person engaged in the study of philosophy.
As this expert terminates, the expert for &amp;quot;deep&amp;quot; resumes
spontaneously, and, constrained by the fact that &amp;quot;deep&amp;quot;
must describe an entity that can be viewed as a person,
it finally terminates successfully, contributing the fact
that the person is intellectual.
The &amp;quot;throw&amp;quot; expert runs next and successfully prunes
away several usages of &amp;quot;throw&apos; for contextual reasons. A
major reason for the semantic richness of verbs such as
&amp;quot;throw&amp;quot;, &amp;quot;take&amp;quot;, and &apos;jump&amp;quot;, is that in context, each
interacts strongly with a number of succeeding
prepositions and adverbs to form distinct meanings. The
word expert approach easily handles this grouping
together of words to form larger word-like entities. In
the particular case of verbs, the expert for a word like
&amp;quot;throw&amp;quot; simply examines its right lexical neighbor, an4
bases its own sense discrimination on the combination or
what it expects to find there, what it actually finds
there, and what this neighbor tells it (if it does so far
as to ask). No interesting particle follows throw&apos; in
the current example, but it should be easy to conceive of
the basic expert probes to discriminate the sense of
&apos;throw&apos; when followed by away&amp;quot;, up&amp;quot;, out&apos; , &apos;in the
towel&amp;quot;, or other words or word groups. When no such word
follows&apos; throw&amp;quot; as is the case here, its expert simply
waits for the existence of an entire concept to its
right, to determine if it meets any of the requirements
that would make the correct contextual interpretation of
throw&amp;quot; different from the expected &apos;propel by moving
ones arm&amp;quot; (e.g., &amp;quot;throw a party&apos;). Before any such
substantive conceptual activity takes place, however, the
&amp;quot;s&amp;quot; expert runs and Gontrihytes its staA4dard
morphological information to throw s data bin. This
execution of the &apos;s&amp;quot; expert does not, of course, affect
&amp;quot;throw&apos;&apos;s suspended status.
The &amp;quot;the&amp;quot; expert for the second &amp;quot;the&amp;quot; in the
sentence runs next, and as in the previous case, creates
a new concept bin to represent the data about the nominal
and description to come. The &amp;quot;peach&amp;quot; expert realizes
that it could be either a noun or an adjective, and thus
attempts what I call a &amp;quot;pairing&amp;quot; operation with its right
neighbor. It essentially asks the expert for &amp;quot;pit&amp;quot; if
the two of them form a noun-noun pair. To determine the
answer, both pit and &amp;quot;peach&amp;quot; have access to the entire
model of linguistic and pragmatic knowledge. During this
time peach is in a state called &apos;attempting pairing
which is different from the suspended&amp;quot; state of the
&apos;throw&amp;quot; expert. &amp;quot;Pit&amp;quot; answers back that it does pair up
with &amp;quot;peach&apos; (since &amp;quot;pit&amp;quot; is aware of its run-time
context) and enters the &amp;quot;ready&apos; state. &amp;quot;Peach&amp;quot; now
determines its correct sense and terminates. And eince
only one meaningful sense for pit remains, the pit
expert executes quickly,. terminating with the
contextually appropriate &apos;fruit pit&apos; sense. As it
terminates, the pit expert closes off the concept bin
in which it participates, spontaneously resuming the
&amp;quot;throw&amp;quot; expert. An examination of the nature of fruit
pits reveals that they are perfectly suited to propelling
with ones arm, and thus the &apos;throw expert terminates
successfully, contributing its word sense to its event
concept bin.
The &amp;quot;into&amp;quot; expert runs next, opens a concept bin (of
type setting&apos;) for the time, location, or situation
about to be described, and suspends itself. On
suspension, &amp;quot;into&amp;quot;&apos;s expert posts an associative restart
condition that will enable its resumption when a new
picture concept is opened to the right. This initial
action takes place for most prepositions. In certain
cases, if the end of a sentence is reached before an
appropriate expected concept is opened, an expert will
take alternative action. For example, one of the &amp;quot;in&amp;quot;
experts restart trigger patterns consists of control
state data of just this kind -- if the end of a sentence
is reached and no conceptual object for the setting
created by &amp;quot;in has been found, the &apos;in expert will
resume nonetheless, and create a default concept or
perform some kind ot intelligent reference determination.
The sentence &amp;quot;The doctor is in. illustrates this point.
In the current example A the &amp;quot;the&amp;quot; expert chat
executes immediately after into&amp;quot;&apos;s suspension creates
the expected picture concept. The word expert for &amp;quot;deep&amp;quot;
then runs and, as before, cannot immediately discriminate
among its several senses. &amp;quot;Deep&amp;quot; thus suspends, waiting
for the expert for the word to its right to help. At his
point, there are two experts suspended, although the
control flow remains fairly simple. Other examples exist
in which a complex set of conceptual dependencies cause a
number of experts to be suspended simultaneously. These
situations usually resolve themselves with a caecading of
expert resumptions and terminations. In our deep pit&apos;
example, &amp;quot;deep&amp;quot; posts expectations on the central tableau
Of global control state knowledge, and waits for &amp;quot;pit&amp;quot; to
terminate. &amp;quot;Pit&amp;quot;&apos;s expert now runs, and since this
</bodyText>
<page confidence="0.992653">
10
</page>
<bodyText confidence="0.999863523809524">
bulletin board contains &amp;quot;deep&amp;quot;&apos;s expectations of a
person, volT3l1W7-or printed matter, &amp;quot;pit&amp;quot; maps immediately
onto a large hole in the ground. This in turn, causes
both the resumption and termination of the &amp;quot;deep&amp;quot; expert
as well as the closure of the concept bin to which they
belong. At the closing of the concept bin, the &amp;quot;into
expert resumes, marks its concept as a location, and
terminates. With all the word experts completed and all
concept bins closed, the expert for &amp;quot;.&amp;quot; runs and
completes the parse. The concept level workspace now
contains five concepts: a picture concept designating an
intellectual philosopher, an event concept representing
the throwing action, another picture concept describing a
fruit pit which came from a peach, a setting concept
representing a location, and the picture concept which
describes precisely the nature of this location. Work on
the mechanism to determine the schematic roles of the
concepts has just begun, and is described briefly later.
A program trace that shows the actions of the Word Expert
Parser on the example just presented is available on
request.
</bodyText>
<listItem confidence="0.726891">
3. Structure of the Model
</listItem>
<bodyText confidence="0.99742324">
The organization of the parser centers around data
repositories on two levels -- the sentence level
workspace contains a word bin for each word (and
sub-lexical morpheme) of the input and the concept level
workspace contains a concept bin (described above) for
each concept referred to in the input sentence. A third
level of processing, the schema level workspace, while
not yet implemented, will contain a schema for each
conceptual action of the input sentence. All actions
affecting the contents of these data bins are carried out
by the word expert processes, one of which is associated
with each word bin in the workspace. In addition to this
first order information about lexical and conceptual
objects, the parser contains a central tableau of control
d
state escriptions available to any expert that can make
use of self referential knowledge about its own
processing or the states of processing of other model
components. The availability of such control state
information improves considerably both the performance
and the psychological appeal of the model -- each word
expert attempting to disambiguate its contextual usage
knows precisely the progress of its neighbors and the
state of convergence (or the lack thereof) of the entire
parsing process.
</bodyText>
<subsectionHeader confidence="0.802492">
Word Experts
</subsectionHeader>
<bodyText confidence="0.995033230769231">
The principal knowledge structure of the model is
the word sense discrimination expert. A word expert
represents the the linguistic knowledge required to
disambiguate the meaning of a single word in any context.
Although represented computationally as coroutines, these
experts differ considerably from ad hoc LISP programs and
have approximately the same relation to LISP as an
augmented transition network 1151 grammar.° Just as the
graphic representation of an augmented transition network
demonstrates the basic control paradigm of the ATN
parsing approach, a graphic representation for word
experts exists which embodies its functional framework.
Each word expert derives from a branching discrimination
structure called a word sense discrimination network or
sense net. A sense ar-cdffsrgre of an ordefer—T8C- of
Twyrrary (the nodes of the network), and for each one,
the set of possible answers to that question (the
branches emanating from each node)i. Traversal of a sense
network represents the process of converging on a single
contextual usage of a word. The terminal nodes of a
sense net represent distinct word senses of the word
modeled by the network. A sense net for the word &amp;quot;heavy&amp;quot;
appears in part (a) of Figure 2. Examination of this
network reveals that four senses are represented -- the
three adjective usages shown in Figure 1 plus the nominal
sense of &amp;quot;thug- as in &amp;quot;Joe&apos;s heavy told me to beat it.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.890058">
Expert Representation
</subsectionHeader>
<bodyText confidence="0.999351215384616">
The network representation of a word expert leaves
out certain computational necessities of actually using
it for parsing. A word expert has two fundamental
activities. (1) An expert asks questions about the
lexical and conceptual data being amassed by its
neighbors, the control states of various model
components, and more general issues requiring common
sense or knowledge of the physical world. (2) In
addition, at each node an expert performs actions to
affect the lexical and conceptual contents of the
workspaces, the control states of itself, concept bins,
6gn ATN without arbitrarily complex LISP computations
on each arc and at each node, that is.
7/n addition to common sense knowledge of the physical
world, this could include information about the plot,
characters, or focus of a children&apos;s story, or in a
specialized domain such as medical diagnosis [17], could
include highly domain specific knowledge.
and the parser as a whole, and the model&apos;s expectations.
The current procedural representation of the word expert
for &amp;quot;heavy appears as part (b) of Figure 2.
Each word expert process includes three
components -- a declarative header, a start node, and a
body. The header provides a description of the expert&apos;s
behavior for purposes of inter-expert constraint
forwarding. If sense discrimination by a word expert
results in the knowledge that a word to its right, either
not yet executed or suspended, must map to a specific
sense or conceptual category, then it should constrain it
to do so, thus helping it avoid unnecessary processing or
fallacious reasoning. Since word experts are represented
as processes, constraining an expert consists of altering
the pointer to the address at which it expects to
continue execution. Through its descriptive header, an
expert conditions this activity and insures that it takes
place without disastrous consequences.
Each node in the body of the expert has a type
designated by a letter following the node name either Q
(question), A (action), S (suspend), or T (terminal). By
tracing through the question nodes (treating the others
as vacuous except for their goto pointers), a sense
network for each word expert process can be derived. The
graphical framework of a word expert (and thus the
questions it asks) represents its principal linguistic
task of word sense disambiguation. Each question node
has a type, shown following the Q in the node -- MC
(multiple choice), C (conditional), YN (yes/no), and PI
(possible/impossible). In the example expert for
heavy&amp;quot;, node n1 represents a conditional query into the
state of the entire parsing process, and node n12 a
multiple choice question involving the conceptual nature
of the word to &apos;heavy&amp;quot;&apos;s right in the input sentence.
Multiple choice questions typically delve into the
basic relations among objects and actions in the world.
For example, the question asked at node n12 of the
&amp;quot;heavy expert is typical:
&amp;quot;Is the object to my right better described as
&apos; an artistic objectA a form of precipitation, or
a physical object?
Action nodes in the &amp;quot;heavy&amp;quot; expert perform such tasks as
determining the concept bin to which it contributes, and
posting expectations for the word to its right. In terms
of its side effects, the &amp;quot;heavy expert is fairly simple.
A full account of the word expert representation language
will be available next year (12].
</bodyText>
<subsectionHeader confidence="0.957273">
Expert Questions
</subsectionHeader>
<bodyText confidence="0.979411790697674">
The basic structure of the Word Expert Parser
depends principally on the role of individual, word
experts in affecting (1) each other&apos;s actions and (2) the
declarative result of computational analysis. Experts
affect each other by posting expectations on the central
bulletin board, constraining each other, changing control
states of model components (most notably themselves), and
augmenting data structures in the workspaces. ° They
contribute to the conceptual and schematic result of the
parse by contributing object names, descriptions,
schemata, and other useful data to the concept level
workspace. To determine exactly what contributions .to
make, i.e., the accurate ones in the particular run-time
context at hand, the experts ask questions of various
kinds about the processes of the model and the world at
large.
Four types of questions may be asked by an expert,
and whereas some queries can be made in more than one
way, the several question types solicit different kinds
of information. Some questions require fairly involved
inference to be answered adequately, and others demand no
more than simple register lookup. This variety
corresponds well, in my opinion, with human processing
involved in conceptual analysis. Certain contextual
clues to meaning are structural; taking advantage of them
requires solely knowledge of the state of the parsing
process (e.g., &apos;building a noun prase.). Other clues
subtly present themselves through more global evidence,
usually having to do with linking together high order
information about the specific domain at hand. In story
comprehension, this involves the plot, characters, focus
of attention, and general social psychology as well as
common sense knowledge about the world. Understanding
texts dealing with specialized subject matter requires
knowledge about that particular subject, other subjects
related to it, and of course, common sense. The
questions asked by a word expert in arriving at the
correct contextual interpretation of a word probe sources
of both kinds of information, and take different forms.
8The blackboard of the Hearsay speech understanding
system [161 is analogous to the entire workspace of the
parser including the word bins, concept bins, and
bulletin board.
</bodyText>
<page confidence="0.995078">
11
</page>
<figure confidence="0.938505038461538">
(Is the current
concept of type)&amp;quot;picture&amp;quot;?
•
yes
(: Does the word on
my right contribute
to the current
concept?
yes no
[word-expert heavy
(header
[category (PA • h1)1
[sense &lt;descriptors rARGE -PHYSICAL-MASS . ntl)
INTENSE-QUANTITY
SERIOUS-OR-EMOTIONAL . nt2)&gt;1&gt;
&lt;start nO&gt;
&lt;expert
[):A (REFUSE)
NEXT n1)/
[nl:Q parser-state t
(open-picture . n2)
[n2:A CONCEPT nld (10)
NEXT n4)]
(nB:A CONCEPT new PICTURE)
NEXT n4
(n4:A CATEGORYPA)
</figure>
<table confidence="0.885427705882353">
NEXT n10)1
[n10:A 1EXPECT 11 view/PP ART)
EXPECT rw view/PP PRECIPITATION)
EXPECT rw view/PP PHYSOBJ)
Nur nl )1
[nll:S wait-fol.-right-word
rESUME trigger &apos;expert-state (rw) &apos;terminated))
QUEUE f rot)
NEXT n12/1
[n12:O MC view/PP (rw)
(art . nt2)
precipitation , nt3)
physobj . nt1/1
Intl:T PA LARCE-PHYSICAL-MASS1
nt2:T PA SERIOUS-OR-EMOTIONAL]
nt3:T PA INTENSE-AMOUNTD]
(b) Process representation of &amp;quot;heavy&amp;quot; expert
</table>
<figureCaption confidence="0.964621">
Figure 2: Word expert representation
</figureCaption>
<bodyText confidence="0.999639384615385">
The explicit representation of control state and
structural information facilitates its use in parsing --
conditional and yes/no questions perform simple lookup
operations in thePLANNER-like associative data base [18]
that stores the workspace data. Questions about the plot
of a story or its characters, or common sense questions
requiring spacial or temporal simulations are best
phrased as possible/impossible (or yes/no/maybe)
questiono, Sometimes during sense discrimination, the
plausibility of some general fact leans to the pursuit of
different Information than its implausibility. Such
situations occur with enough frequency to justify a
special type of question to deal with them.
</bodyText>
<subsectionHeader confidence="0.918182">
The Importance of Multiple Choice
</subsectionHeader>
<bodyText confidence="0.97414115">
Multiple choice questions comprise the central
inferential component of word experts. They derive from
Ringer&apos;s notion that intelligent selection among
competing alternatives by relative differencing
represents an important aspect of human problem solving
[7]. The Word Expert Parser, unlike certain standardized
tests, prohibits multiple choice questions from
containing a &amp;quot;none of the above&amp;quot; choice. Thus, they
demand the most &apos;reasonable or &amp;quot;consistent&apos; choice ot
potentially unappealing answers. What does a child (or
adult) do when faced with a sentence that seems to state
an implausible proposition or reference implausible
objects? He surely does his best to make sense of the
sentence, no matter what it says. Depending on the
context, certain intelligent and literate people create
metaphorical interpretations for such sentences. The
:lord expert approach interprets metaphor, idiom, and
normal&apos; text with the same mechanism.
Multiple choice questions make this possible but
answering them may require tremendously complex
processing. A substantial knowledge represyntatinn
formalism based on semantic networks, such as KR L (191,
with multiple perspectives, nrocedural attachment, and
intelligent description matching, must be used to
represent in a uniform way both general world knowledge
and knowledge acquired through textual interpretation.
In KRL terms, a multiple choice question such as &apos;Is the
object RAIN more like ARTISTIC-OBJECT, PHYSICAL-OBJECT,
or PRECIPITATION?&amp;quot; must be answered by appeal to the
units representing the four notions involved. Clearly,
RAIN can be viewed as a PHYSICAL-OBJECT; much less so as
an ARTISTIC-OBJECT. However, in almost all contexts,
RAIN is closest conceptually to PRECIPITATION. Thus,
this should be the answer. This multiple choice
mechanism hes many uses in conceptual parsing and
full-scale language comprehension as well as in general
problem solving [201. That any fragment of text (or
other human sensual input) has some interpretation from
the point of view of a particular reader constitutes a
fundamental underlying idea of the word expert approach.
</bodyText>
<subsectionHeader confidence="0.990951">
Expert Side Effects
</subsectionHeader>
<bodyText confidence="0.966496714285714">
Word experts take two kinds of actions -- actions
explicitly intended to affect sense discrimination by
other experts, and actions to augment the conceptual
information that constitutes the result of a parse. Each
path through a sense network represents a distinct usage
of the modeled word, and at each step of the way, the
word expert must update the model to reflect the state of
Ats processing and the extent of its knowledge. The
heavy&apos; expert of Figure 2(b) exhibits several of these
actions. Nodes n2 add re of this word expert process
represent &amp;quot;haavy&amp;quot;&apos;s decision about the concept bin (i.e.,
conceptual notion) in which it participates. /p the
first case it decides to contribute to the same bin as
its left neighbor; in the second, it creates a new one,
eventually to contain the conceptual data provided by
itself and perhaps ocher experts to its right. At node
n10, heavy posts its expectations regarding the word to
its right on the central bulletin board. When it
temporarily suspends execution at node n11, its
&amp;quot;suspended control state description also appears on
this tableau.
Control, state descriptions such as &amp;quot;suspended&amp;quot;,
&amp;quot;terminated&apos;, attempting pairing (see above), and
&amp;quot;ready&amp;quot; are posted on this bulletin board, which contains
a state designation for each expert and concept in the
workspace, as well as a description of the parser state
as a whole. Under restricted conditions, an expert may
affect the state descriptions on this tableau. An expert
that has determined its nominal role, may, for example,
change the state of its concept cthe one to which it
contributes) to &amp;quot;bounded&amp;quot; or &apos;closed&amp;quot;depending on
whether or not all other experts participating in that
concept have terminated. Word experts may post
expectations on the bulletin board to facilitate
handshaking between themselves and subsequently executing
neighbors. In the example parse, the &amp;quot;deep expert
expects an entity that it can describe; by saying so in
detail, it enables the &amp;quot;pit&apos; expert to terminate
successfully on first running, something it would not be
able to do otherwise.
The initial execution of a word expert must
accomplish certain goals of a structural nature. If the
word participates in a noun-noun pair, this must be
determined; in either case, the expert must determine the
concept bin to which it contributes all of its
descriptive data throughout the parse. This concept
9An exception arises when an expert creates a default
concept bin to represent a conceptual notion referenced
in the text, but to which no words in the text
contribute. The automobile in &amp;quot;Joanie parked.&amp;quot; is an
example.
(Is the current
conceptual object
better described
as art, a physobj,
or preci itation?
</bodyText>
<figure confidence="0.974069571428571">
art precipitation physobj
LARGE-PHYSICAL-
MASS
(a) Network representation of &amp;quot;heavy&amp;quot; expert
THUG
SERIOUS-OR- INTENSE-
EMOTIONAL QUANTITY
</figure>
<page confidence="0.983491">
12
</page>
<bodyText confidence="0.98074125">
could either be one that already exists in the workspace
or a new one created by the expert at the time of its
decision. After deciding on a concept, the principal
role of a (content) word expert is to discriminate among
the possibly many remaining senses of the word. Note
that a good deal of this disambiguation may take place
during the initial phase of concept determination. After
asking enough questions to discover some piece of
conceptual data, this data augments what already exists
in the word&apos;s concept bin, including declarative
structures put there both by itself and by the other
lexical participants in that concept. The parse
completes when each word expert in the workspace has
terminated. At this point, the concept level workspace
contains a complete conceptual interpretation of the
input text.
</bodyText>
<subsectionHeader confidence="0.860453">
Conceptual Case Resolution
</subsectionHeader>
<bodyText confidence="0.999905676470588">
Adequate conceptual parsing of input text requires a
stage missing from this discussion and constituting the
current phase of research --- the attachment of each
picture and setting concept (bin) to the appropriate
conceptual case of an event concept. Such a mechanism
can be viewed in an entirely analogous fashion to the
mechanisms just described for performing local
disambiguation of word senses. Rather than word experts,
however, the experts on this level are conceptual in
nature. The concept level thus becomes the main level of
activity and a new level call it the schema level
workspace, turns into the main repository for idterred
intormation. When a concept bin has closed, a concept
expert is retrieved from a disk file, and initialized.
If it is an event concept, its function is to fill its
conceptual cases with settings and pictures; if it is a
setting or picture, it must determine its schematic role.
The activity on this level therefore, involves higher
order processing than sense discrimination, but occurs in
just about the same way. The ambiguities involved in
mapping known concepts into conceptual case schemata
appear identical to those having to do with mapping words
into concepts. Discovering that the word &amp;quot;pit maps in a
certain context to the notion of a &amp;quot;fruit pit&amp;quot; requires
the same abilities and knowledge as realizing that &amp;quot;the
red house&amp;quot; maps in some context to the notion of &amp;quot;a
location for smoking pot and listening to records&amp;quot;. The
implementation of the mechanisms to carry out this next
level of inferential disambiguation has already begun.
It should be quite clear that this schematic level is by
no means the end of the line -- active expert-based plot
following and general text understanding fit nicely into
the word expert framework and constitute its logical
extension.
</bodyText>
<sectionHeader confidence="0.848549" genericHeader="conclusions">
4. Summary and Conclusions
</sectionHeader>
<bodyText confidence="0.999711323529412">
The Word Expert Parser is a theory of organization
and control for a conceptual language analyzer. Thq
control environment is characterized by a collection ot
generator-like coroutines, called word experts, which
cooperatively arrive at a conceptual interpretation of an
input sentence. Many forms of linguistic and
non-linguistic knowledge are available to these experts
in performing their task, including control state
knowledge and knowledge of the world, and by eliminating
all but the most persistent forms of ambiguity, the
parser models human processing.
This new model of parsing claims a number of
theoretical advantages: (1) Its representations of
linguistic knowledge reflect the enormous redundancy in
natural languages -- without this redundancy in the
model, the inter-expert handshaking (seen in many forms
in the example parse) would not be possible. (2) The
model suggests some interesting approaches to language
acquisition. Since much of a word expert&apos;s knowledge is
encoded in a branching discrimination structure, adding
new information about a word involves the addition of a
new branch. This branch would be placed in the expert at
the point where the contextual clues for disambiguating
the new usage differ from those present for a known
usage. (3) Idiosyncratic uses of language are easily
encoded, since the word expert provides a clear way to do
so. These uses are indistinguishable from other uses in
their encodings in the model. (4) The parser represents
a cognitively plausible model of sequential
coroutine-like processing in human language
understanding. The organization of linguistic knowledge
around the word, rather than the rewrite rule, motivates
interesting conjectures about the flow of control in a
human language understander.
</bodyText>
<sectionHeader confidence="0.987134" genericHeader="acknowledgments">
ACKNOWLEDGEMENTS
</sectionHeader>
<bodyText confidence="0.940699">
I would like to thank Chuck Rieger for his insights,
encouragement, and general manner. Many of the ideas
presented here Chuck has graciously allowed me to steal.
In addition, I thank the following people for helping me
with this work through their comments and suggestions:
Phil Agre, Milt Grinberg, Phil London, Jim Reggia, Henan
Samet, Randy Trigg, Rich Wood, and Pamela Zave.
</bodyText>
<sectionHeader confidence="0.981568" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.997716158730159">
(11 Rieger, C. and S. Small Word Expert Parsing,
Proceedings of the 6th International Joint Conference on
Artificial Intelligence, 1979.
[2] Riesbeck, C., Computational Understanding: Analysis
of Sentences and Context, AI-Memo 238, Stanford
University, 1974.
[3] Riesbeck, C. and R. Schank, Comprehension by
Computer: Expectation-based Analysis of Sentences in
Context, Research Report 78, Yale University, 1976.
[41 Schank, R., Conceptual Dependency: A Theory of
Natural Language Understanding, Cognitive Psychology,
vol. 1, no. 4, 1972,
15] Wilke, Y. Makin/ Preferences More Active, Artificial
[61 Marcus, M.,Capturing Linguistic Gereralizations in a
Parser for English Proceedings of the 2nd National
Conference of the tanadian Society for Computational
Studies of Intelligence, 1978.
(7] Rieger, C., The Importance of Multiple Choice,
Proceedings of the 2nd Conference on Theoretical Issues
in Natural Language Processing, 1978.
Discrimination, A Viewing
Parsing Of rclientC1:1,4Di tit
(ed.), Greylock 1J77-
(91 Rieger C., Five Aspects of a Full Scale Story
Comprehension Model, Associative Networks -- The
Representation and Use of Knowledge in Lomputefs, FindrET
(ed.), acacemic—Fren7 /179.
[10] Rieger, C., An Organization of Knowledge for Problem
Solving and Language Comprehension, Artificial
Intelligence, vol. 7, no. 2, 1976.
[11] Small S., Conceptual Language Analysis for Story
Comprehension, Technical Report 663, University of
Maryland, 1978.
[12] Small, S., Word Experts for Conceptual Language
Analysis, Ph.D. Thesis (forthcoming), University of
Maryland, 1980.
[13) McDermott, D. and G. Sussman, The Conniver Reference
Manual, AI-Memo 259a, Massachusetts Institute of
Technology, 1974.
[14] Lisp Machine Group, LISP Machine Progress Report,
AI-Memo 444, Massachusetts Institute of Technology, 1977.
[15] Woods, W., Transition Network Grammars for Natural
Language Analysis, Communications of the ACM, vol. rl,
no. 10, 1970.
[16] Erman, L. and V. Lesser, A Multi-Level Organization
for Problem Solving using Many, Diverse, Cooperating
Sources of Knowledge, Proceedings of the 4th
International Joint Conference on Artificial
Intelligence, 1975.
[17] Reggie, J., Representing and Using Medical Knowledge
for the Neurological Localization Problem (First Report
of the NEUREX Project), Technical Report 693, University
of Maryland, 1978.
[181 Sussman, G., T. Winograd, and E. Charniak,
Micro-Planner Reference Manual, AI-Memo 203a,
Massachusetts Institute of Technology, 1971.
[19] Bobrow, D. and T. Winograd, An Overview of KRL, A
Knowledge Representation Language, Cognitive Science,
vol. 1, no. 1, 1977.
[20] London, P., Dependency Networks as a Representation
for Modeling in General Problem Solvers, Technical Report
698, University of Maryland, 1978.
ntelligence, vol. 1, no. 3, 1978.
</reference>
<page confidence="0.999366">
13
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.011546">
<title confidence="0.999488">WORD EXPERT PARSING&apos;</title>
<author confidence="0.999982">Steven L Small</author>
<affiliation confidence="0.999987">Department of Computer Science University of Maryland</affiliation>
<address confidence="0.999691">College Park, Maryland 20742</address>
<abstract confidence="0.997746418604654">This paper describes an approach to conceptual analysis and understanding of natural language in which linguistic knowledge centers on individual words, and the analysis mechanisms consist of interactions among distributed procedural experts representing that knowledge. Each word expert models the process of diagnosing the intended usage of a particular word in context. The Word Expert Parser performs conceptual analysis through the interactions of the individual experts, which ask questions and exchange information in converging on a single mutually acceptable sentence meaning. The Word Expert theory is advanced as a better cognitive model of natural language understanding than the traditional rule-based approaches. The Word Expert Parser models parts of the theory, and the important issues of control and representation that arise in developing such a model form the basis of the technical discussion. An example from the prototype LISP implementation helps explain the theoretical results presented. Computational understanding of natural language requires complex interactions among a variety of distinct yet redundant mechanisms. The construction of a computer program to perform such a task begins with the development of an organizational framework which inherently incorporates certain assumptions about the nature of these processes and the environment in which they take place. Such cognitive premises affect profoundly the scope and substance of computational analysis for comprehension as found in the program. This paper describes a theory of conceptual parsing which considers knowledge about language to be distributed across a collection of procedural experts centered on individual words. Natural language parsing with word experts entails several new hypotheses about the organization and representation of linguistic and pragmatic knowledge for computational language comprehension. The Word Parser [1] demonstrates the word expert certain other choices based on previous work, affect structure and process in a cognitive model of parsing. The Word Expert Parser is a cognitive model of conceptual language analysis in which the unit of linguistic knowledge is the word and the focua of research is the set of processes underlying comprehension. The model is aimed directly at problems of word sense ambiguity and idiomatic expressions, and in greatly generalizing the notion of word sense, promotes these issues to a central place in the study of language parsing. Parsing models typically cope unsatisfactorily with the wide heterogeneity of usages of particular words. If a sentence contains a standard form of a word, It can usually be parsed; if it involves a less prevalent form which has a different part of speech, perhaps it too can be parsed. Distinguishing among the many senses of a common verb, adjective, or pronoun, for example, or correctly translating idioms are rarely possible. of this difficulty is the reliance on rule-based formalisms, whether syntactic or semantic (e.g. cases), which attempt to capture the linguistic contributions inherent in constituent chunks of sentences that consist of more than single words. A crucial assumption underlying work on the Word Expert Parser is that the fundamental unit of linguistic knowledge is the word and that understanding its sense or role in a particular context is the central parsing process. In the parser to be described, the word expert constitutes the kernel of linguistic knowledgna its representation the elemental data structure. It is procedural in nature and executes directly as a process, cooperating with the other experts for a given sentence to arrive at a mutually acceptable sentence meaning. Certain principles behind the parser do not follow directly from the view of word primacy but from other recent theories of parsing. The cognitive processes involved in language comprehension comprise the fortis of linguistic study of the word expert approach. Parsing is viewed as an inferential process where linguistic knowledge of syntax and semantics and general pragmatic knowledge are applied in a uniform manner during &apos;The research described in this report is funded by the National Aeronautics and Space Administration under grant number NSG-7251. Their support is gratefully acknowledged. interpretation. This methodological position closely that of [2] and [3]) and Schank [4]. The central concern with word usage and word sense ambiguity follows similar motivations of Wilks [5]. The control structure of the Word Expert Parser results fror agreoment with the hypothesis of Marcus that parsing can he done deterministically and in a way in which Information gained through interpretation is permanent [6]. Rieger&apos;s view of inference as intelligent selection among a number of competing plausible alternatives [7] of course forms the cornerstone of the new theory. His ideas on word sense selection for language analysis ([8] and [9]) and strategy selection for general problem solving [10]• constitute a consistent cognitive perspective. Any natural language understanding system must incorporate mechanisms to perform word sense disambiguation in the context of open-ended world knowledge. The importance of these mechanisms for word usage diagnosis derives from the ubiquity of local ambiguities, and brought about the notion that they be made the central processes of computational analysis and understanding. Consideration of almost any English content word leads to a realization of the scope of the problem -with a little time and perhaps help from the dictionary, many distinct usages can be identified. As a simple illustration, several usages each for the words &amp;quot;heavy&amp;quot; and &amp;quot;ice&amp;quot; appear in Figure 1. Each of these seemingly benign words exhibits a rich depth of contextual use. An earlier paper contains a list of almost sixty verbal usages for the word &amp;quot;take&amp;quot; [11]. The representation of all contextual word usages in an active way that insures their utility for linguistic led to the notion of word experts.Each word is a procedural of all possible interpretations of the word it When placed in a context formed by experts for the other words in a sentence, each expert should be capable of sufficient context-probing and self-examination to determine successfully its functional or semantic role, further, to realize the nature of or the precise meaning of the word. The representation and control issues involved in basing a parser on word experts are discussed below following presentation of an example execution of the existing Word Expert Parser. Model Overview The Word Expert Parser successfully parses the sentence &amp;quot;The deep philosopher throws the peach pit into the deep pit.&amp;quot; through cooperation among the appropriate word experts. Initialization of the parser consists of retrieving thT experts for &amp;quot;the&amp;quot;, &amp;quot;deep&apos;, &amp;quot;philosopher&amp;quot;, &amp;quot;throw&apos; , s&amp;quot;, important assumption of the word expert viewpoint is that the set of such contextual word usages is not only finite, but fairly small as well. perspective of viewing language through lexical contributions to structure and meaning has naturally led to the development of word experts for common morphemes that are not words (and even, experimentally, for Especially important is the word expert for -ing , which aids significantly in helping to 9 word senses of 1. An overweight pergon is politely called &amp;quot;heavy&amp;quot;: &amp;quot;He has become quite heavy.&amp;quot; 2. Emotional music is referred to as &amp;quot;heavy&amp;quot;: &amp;quot;Mahler writes heavy music.&amp;quot; i. An intensity of precipitation is &amp;quot;heavy&amp;quot;: &amp;quot;A heavy snow is expected today.&amp;quot; Some word senses of &amp;quot;ice&amp;quot; I. The solid state of water is called &amp;quot;ice&amp;quot;: &amp;quot;Ice melts at 0°C.&amp;quot; 2. &amp;quot;Ice&amp;quot; participates in an idiomatic nominal describing a favorite delight: &amp;quot;Homemade ice cream is delicious.&amp;quot; 3. &amp;quot;Dry ice&amp;quot; is the solid state of carbon dioxide: &amp;quot;Dry ice will keep that cool all day.&amp;quot; 4. &amp;quot;Ice&amp;quot; or &amp;quot;iced&amp;quot; describes things that have been cooled (sometimes with ice): &amp;quot;One iced tea to go please.&amp;quot; 5. &amp;quot;Ice&amp;quot; also describes things made of ice: &amp;quot;The ice sculptures are beautiful!&amp;quot; &amp;quot;Ice is the name of a popular sport which has a rule penalizing an action called icing&amp;quot;: &amp;quot;He iced the puck causing a face-off.&amp;quot; term &amp;quot;ice box&amp;quot; refers to both a box containing used cooling foods end a refrigerator: &amp;quot;This ice box isn&apos;t plugged in!&amp;quot; Figure1: contextualword and go from disk file, and organizing with data repositories called word bins in a to right order in the sentence that three copies of i) of each expert for &amp;quot;deep&amp;quot; and &amp;quot;pit&amp;quot; appear Since each expert executes as a each process instantiation in the workspace must be put into an executable state. At this point, the parse is ready to begin. The word expert for &amp;quot;the&amp;quot; runs first, and is able to terminate immediately, creating a new concept designator a conceptbin and participating in the workspace)eventually hold the data the intellectual philosopher described the the &amp;quot;deep&amp;quot; expert runs, and since &amp;quot;deep&amp;quot; a number of word senses,5 is unable to terminate (i.e:. complete its discrimination task). Instead, it suspendS its execution, stating the conditions upon which it should be resumed. These conditions take the form of associative trigger patterns, and are referred to as disambiguate expressions involving gerunds or participles such as &amp;quot;the man eating tiger&amp;quot;. A full discussion of this will appear in (121. I call them &amp;quot;processes&amp;quot;, word experts actually coroutines resembling CONNIVER&apos;s generators and even more so, the stack groups the MIT LISP Machine (141. should be clear that the notion of &amp;quot;word sense&amp;quot; as used here encompasses what might more traditionally be described as &amp;quot;contextual word usage&amp;quot;. Aspects of a mord token&apos;s linguistic environment constitute its broadened &amp;quot;sense&amp;quot;. demons. The &amp;quot;deep&amp;quot; expert creates a to when the sense of the nominal to its (i.e., &amp;quot;philosopher&amp;quot;) becomes The expert for &amp;quot;philosopher now runs, observes the control state of the parser, and contributes the fact that the new concept refers to a person engaged in the study of philosophy. As this expert terminates, the expert for &amp;quot;deep&amp;quot; resumes spontaneously, and, constrained by the fact that &amp;quot;deep&amp;quot; must describe an entity that can be viewed as a person, it finally terminates successfully, contributing the fact that the person is intellectual. The &amp;quot;throw&amp;quot; expert runs next and successfully prunes away several usages of &amp;quot;throw&apos; for contextual reasons. A major reason for the semantic richness of verbs such as &amp;quot;throw&amp;quot;, &amp;quot;take&amp;quot;, and &apos;jump&amp;quot;, is that in context, each interacts strongly with a number of succeeding prepositions and adverbs to form distinct meanings. The word expert approach easily handles this grouping together of words to form larger word-like entities. In the particular case of verbs, the expert for a word like &amp;quot;throw&amp;quot; simply examines its right lexical neighbor, an4 bases its own sense discrimination on the combination or what it expects to find there, what it actually finds and what this neighbor tells it (if it does as to ask). No interesting particle follows throw&apos; in the current example, but it should be easy to conceive of basic expert probes to sense of when followed by away&amp;quot;, , &apos;in the words or word groups. When no such word follows&apos; throw&amp;quot; as is the case here, its expert simply waits for the existence of an entire concept to its right, to determine if it meets any of the requirements that would make the correct contextual interpretation of throw&amp;quot; different from the expected &apos;propel by moving ones arm&amp;quot; (e.g., &amp;quot;throw a party&apos;). Before any such substantive conceptual activity takes place, however, the &amp;quot;s&amp;quot; expert runs and Gontrihytes its staA4dard information to throw s data bin. execution of the &apos;s&amp;quot; expert does not, of course, affect &amp;quot;throw&apos;&apos;s suspended status. The &amp;quot;the&amp;quot; expert for the second &amp;quot;the&amp;quot; in the sentence runs next, and as in the previous case, creates a new concept bin to represent the data about the nominal and description to come. The &amp;quot;peach&amp;quot; expert realizes that it could be either a noun or an adjective, and thus attempts what I call a &amp;quot;pairing&amp;quot; operation with its right neighbor. It essentially asks the expert for &amp;quot;pit&amp;quot; if the two of them form a noun-noun pair. To determine the answer, both pit and &amp;quot;peach&amp;quot; have access to the entire and pragmatic knowledge. During this time peach is in a state called &apos;attempting pairing is different from the suspended&amp;quot; state of the &apos;throw&amp;quot; expert. &amp;quot;Pit&amp;quot; answers back that it does pair up with &amp;quot;peach&apos; (since &amp;quot;pit&amp;quot; is aware of its run-time context) and enters the &amp;quot;ready&apos; state. &amp;quot;Peach&amp;quot; now determines its correct sense and terminates. And eince only one meaningful sense for pit remains, the pit expert executes quickly,. terminating with the contextually appropriate &apos;fruit pit&apos; sense. As it terminates, the pit expert closes off the concept bin in which it participates, spontaneously resuming the &amp;quot;throw&amp;quot; expert. An examination of the nature of fruit pits reveals that they are perfectly suited to propelling with ones arm, and thus the &apos;throw expert terminates word sense to its event concept bin. The &amp;quot;into&amp;quot; expert runs next, opens a concept bin (of type setting&apos;) for the time, location, or situation about to be described, and suspends itself. On suspension, &amp;quot;into&amp;quot;&apos;s expert posts an associative restart condition that will enable its resumption when a new picture concept is opened to the right. This initial action takes place for most prepositions. In certain cases, if the end of a sentence is reached before an appropriate expected concept is opened, an expert will take alternative action. For example, one of the &amp;quot;in&amp;quot; experts restart trigger patterns consists of control state data of just this kind -if the end of a sentence and no object for the setting created by &amp;quot;in has been found, the &apos;in expert will resume nonetheless, and create a default concept or perform some kind ot intelligent reference determination. The sentence &amp;quot;The doctor is in. illustrates this point. the current example Athe &amp;quot;the&amp;quot; expert chat executes immediately after into&amp;quot;&apos;s suspension creates the expected picture concept. The word expert for &amp;quot;deep&amp;quot; then runs and, as before, cannot immediately discriminate among its several senses. &amp;quot;Deep&amp;quot; thus suspends, waiting the expert for the word to to help. At his point, there are two experts suspended, although the control flow remains fairly simple. Other examples exist in which a complex set of conceptual dependencies cause a number of experts to be suspended simultaneously. These situations usually resolve themselves with a caecading of and terminations. In our deep pit&apos; example, &amp;quot;deep&amp;quot; posts expectations on the central tableau Of global control state knowledge, and waits for &amp;quot;pit&amp;quot; to terminate. &amp;quot;Pit&amp;quot;&apos;s expert now runs, and since this 10 bulletinboard contains &amp;quot;deep&amp;quot;&apos;s expectations of a printed matter, &amp;quot;pit&amp;quot; maps immediately onto a large hole in the ground. This in turn, causes both the resumption and termination of the &amp;quot;deep&amp;quot; expert as well as the closure of the concept bin to which they belong. At the closing of the concept bin, the &amp;quot;into expert resumes, marks its concept as a location, and terminates. With all the word experts completed and all concept bins closed, the expert for &amp;quot;.&amp;quot; runs and completes the parse. The concept level workspace now contains five concepts: a picture concept designating an intellectual philosopher, an event concept representing the throwing action, another picture concept describing a fruit pit which came from a peach, a setting concept representing a location, and the picture concept which describes precisely the nature of this location. Work on the mechanism to determine the schematic roles of the concepts has just begun, and is described briefly later. A program trace that shows the actions of the Word Expert Parser on the example just presented is available on request. Structureof the Model The organization of the parser centers around data repositories on two levels -the sentence level workspace contains a word bin for each word (and sub-lexical morpheme) of the input and the concept level workspace contains a concept bin (described above) for each concept referred to in the input sentence. A third level of processing, the schema level workspace, while not yet implemented, will contain a schema for each conceptual action of the input sentence. All actions affecting the contents of these data bins are carried out by the word expert processes, one of which is associated word in the workspace. In addition to this first order information about lexical and conceptual objects, the parser contains a central tableau of control d state escriptions available to any expert that can make use of self referential knowledge about its own processing or the states of processing of other model components. The availability of such control state information improves considerably both the performance and the psychological appeal of the model -each word expert attempting to disambiguate its contextual usage knows precisely the progress of its neighbors and the state of convergence (or the lack thereof) of the entire parsing process. The principal knowledge structure of the model is the word sense discrimination expert. A word expert represents the the linguistic knowledge required to disambiguate the meaning of a single word in any context. Although represented computationally as coroutines, these experts differ considerably from ad hoc LISP programs and have approximately the same relation to LISP as an augmented transition network 1151 grammar.° Just as the graphic representation of an augmented transition network demonstrates the basic control paradigm of the ATN parsing approach, a graphic representation for word experts exists which embodies its functional framework. Each word expert derives from a branching discrimination called a word sense discriminationnetwork or net. A sense an of nodes of the network), and for each one, the set of possible answers to that question (the branches emanating from each node)i. Traversal of a sense network represents the process of converging on a single contextual usage of a word. The terminal nodes of a sense net represent distinct word senses of the word modeled by the network. A sense net for the word &amp;quot;heavy&amp;quot; appears in part (a) of Figure 2. Examination of this network reveals that four senses are represented -the three adjective usages shown in Figure 1 plus the nominal of as in &amp;quot;Joe&apos;s heavy told me to beat it.&amp;quot; Expert Representation The network representation of a word expert leaves out certain computational necessities of actually using it for parsing. A word expert has two fundamental activities. (1) An expert asks questions about the lexical and conceptual data being amassed by its neighbors, the control states of various model components, and more general issues requiring common sense or knowledge of the physical world. (2) In addition, at each node an expert performs actions to affect the lexical and conceptual contents of the workspaces, the control states of itself, concept bins, 6gn ATN without arbitrarily complex LISP computations on each arc and at each node, that is. addition to common sense knowledge of the physical include information about the plot, characters, or focus of a children&apos;s story, or in a domain such as medical diagnosis include highly domain specific knowledge. and the parser as a whole, and the model&apos;s expectations. The current procedural representation of the word expert for &amp;quot;heavy appears as part (b) of Figure 2. Each word expert process includes three -a declarative header, a start node, and body. The header provides a description of the expert&apos;s behavior for purposes of inter-expert constraint forwarding. If sense discrimination by a word expert results in the knowledge that a word to its right, either not yet executed or suspended, must map to a specific sense or conceptual category, then it should constrain it to do so, thus helping it avoid unnecessary processing or fallacious reasoning. Since word experts are represented as processes, constraining an expert consists of altering the pointer to the address at which it expects to continue execution. Through its descriptive header, an expert conditions this activity and insures that it takes place without disastrous consequences. Each node in the body of the expert has a type by a letter following the node name either (question), A (action), S (suspend), or T (terminal). By tracing through the question nodes (treating the others as vacuous except for their goto pointers), a sense network for each word expert process can be derived. The graphical framework of a word expert (and thus the questions it asks) represents its principal linguistic task of word sense disambiguation. Each question node has a type, shown following the Q in the node -- MC (multiple choice), C (conditional), YN (yes/no), and PI (possible/impossible). In the example expert for heavy&amp;quot;, node n1 represents a conditional query into the the entire process, and node n12 a choice the conceptual nature of the word to &apos;heavy&amp;quot;&apos;s right in the input sentence. Multiple choice questions typically delve into the basic relations among objects and actions in the world. For example, the question asked at node n12 of the &amp;quot;heavy expert is typical: &amp;quot;Is the object to my right better described as an artistic a form of precipitation, or a physical object? Action nodes in the &amp;quot;heavy&amp;quot; expert perform such tasks as determining the concept bin to which it contributes, and posting expectations for the word to its right. In terms of its side effects, the &amp;quot;heavy expert is fairly simple. A full account of the word expert representation language will be available next year (12]. Expert Questions The basic structure of the Word Expert Parser depends principally on the role of individual, word in affecting other&apos;s actions and (2) the declarative result of computational analysis. Experts affect each other by posting expectations on the central bulletin board, constraining each other, changing control states of model components (most notably themselves), and augmenting data structures in the workspaces. ° They contribute to the conceptual and schematic result of the parse by contributing object names, descriptions, schemata, and other useful data to the concept level workspace. To determine exactly what contributions .to make, i.e., the accurate ones in the particular run-time context at hand, the experts ask questions of various about of the model and the world at large. types of questions may be by an expert, and whereas some queries can be made in more than one way, the several question types solicit different kinds of information. Some questions require fairly involved inference to be answered adequately, and others demand no more than simple register lookup. This variety corresponds well, in my opinion, with human processing involved in conceptual analysis. Certain contextual clues to meaning are structural; taking advantage of them requires solely knowledge of the state of the parsing (e.g., &apos;building a noun Other clues subtly present themselves through more global evidence, usually having to do with linking together high order information about the specific domain at hand. In story comprehension, this involves the plot, characters, focus of attention, and general social psychology as well as common sense knowledge about the world. Understanding texts dealing with specialized subject matter requires knowledge about that particular subject, other subjects related to it, and of course, common sense. The asked by a word expert in at the correct contextual interpretation of a word probe sources of both kinds of information, and take different forms. blackboard of the Hearsay speech understanding system [161 is analogous to the entire workspace of the parser including the word bins, concept bins, and bulletin board. 11 the current of • yes (: Does the word on my right contribute to the current concept? yes no [word-expert heavy (header [category (PA • h1)1 [sense &lt;descriptors rARGE -PHYSICAL-MASS . ntl) INTENSE-QUANTITY SERIOUS-OR-EMOTIONAL . nt2)&gt;1&gt; &lt;start nO&gt; &lt;expert NEXT n1)/ [nl:Q parser-state t . n2) [n2:A CONCEPT nld (10) NEXT n4)] (nB:A CONCEPT new PICTURE) NEXT n4 [n10:A 1EXPECT 11 view/PP ART) EXPECT rw view/PP PRECIPITATION) EXPECT rw view/PP PHYSOBJ) [nll:S wait-fol.-right-word trigger &apos;expert-state (rw) &apos;terminated)) QUEUE f rot) NEXT n12/1 [n12:O MC view/PP (rw) . precipitation , nt3) physobj . nt1/1 PA LARCE-PHYSICAL-MASS1 nt2:T PA SERIOUS-OR-EMOTIONAL] nt3:T PA INTENSE-AMOUNTD] (b) Process representation of &amp;quot;heavy&amp;quot; expert Figure2: Word representation The explicit representation of control state and structural information facilitates its use in parsing -conditional and yes/no questions perform simple lookup operations in thePLANNER-like associative data base [18] that stores the workspace data. Questions about the plot a story or or common sense questions requiring spacial or temporal simulations are best phrased as possible/impossible (or yes/no/maybe) questiono, Sometimes during sense discrimination, the plausibility of some general fact leans to the pursuit of different Information than its implausibility. Such situations occur with enough frequency to justify a special type of question to deal with them. The Importance of Multiple Choice Multiple choice questions comprise the central inferential component of word experts. They derive from Ringer&apos;s notion that intelligent selection among competing alternatives by relative differencing represents an important aspect of human problem solving [7]. The Word Expert Parser, unlike certain standardized tests, prohibits multiple choice questions from containing a &amp;quot;none of the above&amp;quot; choice. Thus, they demand the most &apos;reasonable or &amp;quot;consistent&apos; choice ot potentially unappealing answers. What does a child (or adult) do when faced with a sentence that seems to state an implausible proposition or reference implausible objects? He surely does his best to make sense of the sentence, no matter what it says. Depending on the context, certain intelligent and literate people create metaphorical interpretations for such sentences. The :lord expert approach interprets metaphor, idiom, and normal&apos; text with the same mechanism. Multiple choice questions make this possible but answering them may require tremendously complex processing. A substantial knowledge represyntatinn formalism based on semantic networks, such as KR L (191, with multiple perspectives, nrocedural attachment, and intelligent description matching, must be used to represent in a uniform way both general world knowledge and knowledge acquired through textual interpretation. KRL terms, a multiple choice question such as RAIN more like or PRECIPITATION?&amp;quot; must be answered by appeal to the units representing the four notions involved. Clearly, can as a less so as in almost all contexts, RAIN is closest conceptually to PRECIPITATION. Thus, this should be the answer. This multiple choice mechanism hes many uses in conceptual parsing and full-scale language comprehension as well as in general problem solving [201. That any fragment of text (or other human sensual input) has some interpretation from the point of view of a particular reader constitutes a fundamental underlying idea of the word expert approach. Expert Side Effects Word experts take two kinds of actions -actions explicitly intended to affect sense discrimination by other experts, and actions to augment the conceptual information that constitutes the result of a parse. Each path through a sense network represents a distinct usage of the modeled word, and at each step of the way, the word expert must update the model to reflect the state of the extent of its knowledge. The heavy&apos; expert of Figure 2(b) exhibits several of these actions. Nodes n2 add re of this word expert process &amp;quot;haavy&amp;quot;&apos;s decision about the concept bin conceptual notion) in which it participates. /p the first case it decides to contribute to the same bin as its left neighbor; in the second, it creates a new one, eventually to contain the conceptual data provided by itself and perhaps ocher experts to its right. At node n10, heavy posts its expectations regarding the word to its right on the central bulletin board. When it temporarily suspends execution at node n11, its &amp;quot;suspended control state description also appears on this tableau. Control, state descriptions such as &amp;quot;suspended&amp;quot;, &amp;quot;terminated&apos;, attempting pairing (see above), and &amp;quot;ready&amp;quot; are posted on this bulletin board, which contains a state designation for each expert and concept in the workspace, as well as a description of the parser state as a whole. Under restricted conditions, an expert may affect the state descriptions on this tableau. An expert that has determined its nominal role, may, for example, change the state of its concept cthe one to which it to &amp;quot;bounded&amp;quot; or &apos;closed&amp;quot;depending or not all other experts in that concept have terminated. Word experts may post expectations on the bulletin board to facilitate handshaking between themselves and subsequently executing neighbors. In the example parse, the &amp;quot;deep expert expects an entity that it can describe; by saying so in detail, it enables the &amp;quot;pit&apos; expert to terminate successfully on first running, something it would not be able to do otherwise. The initial execution of a word expert must accomplish certain goals of a structural nature. If the word participates in a noun-noun pair, this must be determined; in either case, the expert must determine the concept bin to which it contributes all of its descriptive data throughout the parse. This concept exception arises when an expert creates a default concept bin to represent a conceptual notion referenced in the text, but to which no words in the text contribute. The automobile in &amp;quot;Joanie parked.&amp;quot; is an example. the current conceptual object better described as art, a physobj, or preci itation? art precipitation physobj LARGE-PHYSICAL- MASS (a) Network representation of &amp;quot;heavy&amp;quot; expert THUG INTENSE- EMOTIONAL QUANTITY 12 could either be one that already exists in the workspace or a new one created by the expert at the time of its decision. After deciding on a concept, the principal role of a (content) word expert is to discriminate among the possibly many remaining senses of the word. Note that a good deal of this disambiguation may take place during the initial phase of concept determination. After asking enough questions to discover some piece of conceptual data, this data augments what already exists in the word&apos;s concept bin, including declarative structures put there both by itself and by the other lexical participants in that concept. The parse completes when each word expert in the workspace has terminated. At this point, the concept level workspace contains a complete conceptual interpretation of the input text. Conceptual Case Resolution Adequate conceptual parsing of input text requires a stage missing from this discussion and constituting the current phase of research --the attachment of each picture and setting concept (bin) to the appropriate conceptual case of an event concept. Such a mechanism can be viewed in an entirely analogous fashion to the mechanisms just described for performing local disambiguation of word senses. Rather than word experts, however, the experts on this level are conceptual in nature. The concept level thus becomes the main level of and a new level call it the schema level workspace,turns into the main repository for idterred intormation. When a concept bin has closed, a concept expert is retrieved from a disk file, and initialized. If it is an event concept, its function is to fill its conceptual cases with settings and pictures; if it is a setting or picture, it must determine its schematic role. The activity on this level therefore, involves higher order processing than sense discrimination, but occurs in just about the same way. The ambiguities involved in mapping known concepts into conceptual case schemata appear identical to those having to do with mapping words into concepts. Discovering that the word &amp;quot;pit maps in a certain context to the notion of a &amp;quot;fruit pit&amp;quot; requires the same abilities and knowledge as realizing that &amp;quot;the red house&amp;quot; maps in some context to the notion of &amp;quot;a location for smoking pot and listening to records&amp;quot;. The implementation of the mechanisms to carry out this next level of inferential disambiguation has already begun. It should be quite clear that this schematic level is by no means the end of the line -active expert-based plot following and general text understanding fit nicely into the word expert framework and constitute its logical extension. Summaryand The Word Expert Parser is a theory of organization and control for a conceptual language analyzer. Thq control environment is characterized by a collection ot generator-like coroutines, called word experts, which cooperatively arrive at a conceptual interpretation of an input sentence. Many forms of linguistic and non-linguistic knowledge are available to these experts in performing their task, including control state knowledge and knowledge of the world, and by eliminating all but the most persistent forms of ambiguity, the parser models human processing. This new model of parsing claims a number of theoretical advantages: (1) Its representations of linguistic knowledge reflect the enormous redundancy in natural languages -without this redundancy in the model, the inter-expert handshaking (seen in many forms in the example parse) would not be possible. (2) The model suggests some interesting approaches to language acquisition. Since much of a word expert&apos;s knowledge is encoded in a branching discrimination structure, adding new information about a word involves the addition of a new branch. This branch would be placed in the expert at the point where the contextual clues for disambiguating the new usage differ from those present for a known usage. (3) Idiosyncratic uses of language are easily encoded, since the word expert provides a clear way to do so. These uses are indistinguishable from other uses in their encodings in the model. (4) The parser represents a cognitively plausible model of sequential coroutine-like processing in human language understanding. The organization of linguistic knowledge around the word, rather than the rewrite rule, motivates interesting conjectures about the flow of control in a human language understander. ACKNOWLEDGEMENTS I would like to thank Chuck Rieger for his insights, manner. Many of the ideas presented here Chuck has graciously allowed me to steal. In addition, I thank the following people for helping me with this work through their comments and suggestions:</abstract>
<author confidence="0.8449235">Phil Agre</author>
<author confidence="0.8449235">Milt Grinberg</author>
<author confidence="0.8449235">Phil London</author>
<author confidence="0.8449235">Jim Reggia</author>
<author confidence="0.8449235">Henan Samet</author>
<author confidence="0.8449235">Randy Trigg</author>
<author confidence="0.8449235">Rich Wood</author>
<author confidence="0.8449235">Pamela Zave</author>
<note confidence="0.932985784615385">REFERENCES (11 Rieger, C. and S. Small Word Expert Parsing, Proceedings of the 6th International Joint Conference on Artificial Intelligence, 1979. [2] Riesbeck, C., Computational Understanding: Analysis of Sentences and Context, AI-Memo 238, Stanford University, 1974. [3] Riesbeck, C. and R. Schank, Comprehension by Computer: Expectation-based Analysis of Sentences in Context, Research Report 78, Yale University, 1976. Schank, R., Conceptual Dependency: of Language Understanding, Psychology, vol. 1, no. 4, 1972, Wilke, Preferences More Active, Marcus, M.,Capturing Linguistic Gereralizations in Parser for English Proceedings of the 2nd National Conference of the tanadian Society for Computational Studies of Intelligence, 1978. (7] Rieger, C., The Importance of Multiple Choice, Proceedings of the 2nd Conference on Theoretical Issues in Natural Language Processing, 1978. Discrimination, A Viewing Of tit Greylock (91 Rieger C., Five Aspects of a Full Scale Story Model, Networks-- The Representationand Use of Knowledgein Lomputefs,FindrET /179. [10] Rieger, C., An Organization of Knowledge for Problem and Language Comprehension, Intelligence,vol. 7, no. 2, 1976. [11] Small S., Conceptual Language Analysis for Story Comprehension, Technical Report 663, University of Maryland, 1978. [12] Small, S., Word Experts for Conceptual Language Analysis, Ph.D. Thesis (forthcoming), University of Maryland, 1980. [13) McDermott, D. and G. Sussman, The Conniver Reference Manual, AI-Memo 259a, Massachusetts Institute of Technology, 1974. [14] Lisp Machine Group, LISP Machine Progress Report, AI-Memo 444, Massachusetts Institute of Technology, 1977. [15] Woods, W., Transition Network Grammars for Natural Analysis, Communicationsof the ACM, vol. rl, no. 10, 1970. [16] Erman, L. and V. Lesser, A Multi-Level Organization for Problem Solving using Many, Diverse, Cooperating Sources of Knowledge, Proceedings of the 4th International Joint Conference on Artificial Intelligence, 1975. [17] Reggie, J., Representing and Using Medical Knowledge for the Neurological Localization Problem (First Report of the NEUREX Project), Technical Report 693, University of Maryland, 1978. [181 Sussman, G., T. Winograd, and E. Charniak, Micro-Planner Reference Manual, AI-Memo 203a, Massachusetts Institute of Technology, 1971. [19] Bobrow, D. and T. Winograd, An Overview of KRL, A Representation Language, Science, vol. 1, no. 1, 1977. [20] London, P., Dependency Networks as a Representation for Modeling in General Problem Solvers, Technical Report 698, University of Maryland, 1978. ntelligence,vol. 1, no. 3, 1978. 13</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Word Expert Parsing,</title>
<date>1979</date>
<booktitle>Proceedings of the 6th International Joint Conference on Artificial Intelligence,</booktitle>
<marker>1979</marker>
<rawString> (11 Rieger, C. and S. Small Word Expert Parsing, Proceedings of the 6th International Joint Conference on Artificial Intelligence, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Riesbeck</author>
</authors>
<title>Computational Understanding: Analysis of Sentences and Context, AI-Memo 238,</title>
<date>1974</date>
<institution>Stanford University,</institution>
<contexts>
<context position="4573" citStr="[2]" startWordPosition="678" endWordPosition="678">rimacy but from other recent theories of parsing. The cognitive processes involved in language comprehension comprise the fortis of linguistic study of the word expert approach. Parsing is viewed as an inferential process where linguistic knowledge of syntax and semantics and general pragmatic knowledge are applied in a uniform manner during &apos;The research described in this report is funded by the National Aeronautics and Space Administration under grant number NSG-7251. Their support is gratefully acknowledged. interpretation. This methodological position closely follows that of RiPsbeck (see [2] and [3]) and Schank [4]. The central concern with word usage and word sense ambiguity follows similar motivations of Wilks [5]. The control structure of the Word Expert Parser results fror agreoment with the hypothesis of Marcus that parsing can he done deterministically and in a way in which Information gained through interpretation is permanent [6]. Rieger&apos;s view of inference as intelligent selection among a number of competing plausible alternatives [7] of course forms the cornerstone of the new theory. His ideas on word sense selection for language analysis ([8] and [9]) and strategy sele</context>
</contexts>
<marker>[2]</marker>
<rawString>Riesbeck, C., Computational Understanding: Analysis of Sentences and Context, AI-Memo 238, Stanford University, 1974.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C Riesbeck</author>
<author>R Schank</author>
</authors>
<title>Comprehension by Computer: Expectation-based Analysis of Sentences in Context, Research Report 78, Yale University,</title>
<date>1976</date>
<journal>Cognitive Psychology,</journal>
<booktitle>Preferences More Active, Artificial [61 Marcus, M.,Capturing Linguistic Gereralizations in a Parser for English Proceedings of the 2nd National Conference of the tanadian Society for Computational Studies of Intelligence,</booktitle>
<volume>1</volume>
<pages>41</pages>
<editor>15] Wilke, Y. Makin/</editor>
<contexts>
<context position="4581" citStr="[3]" startWordPosition="680" endWordPosition="680">ut from other recent theories of parsing. The cognitive processes involved in language comprehension comprise the fortis of linguistic study of the word expert approach. Parsing is viewed as an inferential process where linguistic knowledge of syntax and semantics and general pragmatic knowledge are applied in a uniform manner during &apos;The research described in this report is funded by the National Aeronautics and Space Administration under grant number NSG-7251. Their support is gratefully acknowledged. interpretation. This methodological position closely follows that of RiPsbeck (see [2] and [3]) and Schank [4]. The central concern with word usage and word sense ambiguity follows similar motivations of Wilks [5]. The control structure of the Word Expert Parser results fror agreoment with the hypothesis of Marcus that parsing can he done deterministically and in a way in which Information gained through interpretation is permanent [6]. Rieger&apos;s view of inference as intelligent selection among a number of competing plausible alternatives [7] of course forms the cornerstone of the new theory. His ideas on word sense selection for language analysis ([8] and [9]) and strategy selection fo</context>
</contexts>
<marker>[3]</marker>
<rawString>Riesbeck, C. and R. Schank, Comprehension by Computer: Expectation-based Analysis of Sentences in Context, Research Report 78, Yale University, 1976. [41 Schank, R., Conceptual Dependency: A Theory of Natural Language Understanding, Cognitive Psychology, vol. 1, no. 4, 1972, 15] Wilke, Y. Makin/ Preferences More Active, Artificial [61 Marcus, M.,Capturing Linguistic Gereralizations in a Parser for English Proceedings of the 2nd National Conference of the tanadian Society for Computational Studies of Intelligence, 1978. (7] Rieger, C., The Importance of Multiple Choice, Proceedings of the 2nd Conference on Theoretical Issues in Natural Language Processing, 1978. Discrimination, A Viewing Parsing Of rclientC1:1,4Di tit (ed.), Greylock 1J77-(91 Rieger C., Five Aspects of a Full Scale Story Comprehension Model, Associative Networks -- The Representation and Use of Knowledge in Lomputefs, FindrET (ed.), acacemic—Fren7 /179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Rieger</author>
</authors>
<title>An Organization of Knowledge for Problem Solving and Language Comprehension,</title>
<date>1976</date>
<journal>Artificial Intelligence,</journal>
<volume>7</volume>
<contexts>
<context position="5211" citStr="[10]" startWordPosition="778" endWordPosition="778">ral concern with word usage and word sense ambiguity follows similar motivations of Wilks [5]. The control structure of the Word Expert Parser results fror agreoment with the hypothesis of Marcus that parsing can he done deterministically and in a way in which Information gained through interpretation is permanent [6]. Rieger&apos;s view of inference as intelligent selection among a number of competing plausible alternatives [7] of course forms the cornerstone of the new theory. His ideas on word sense selection for language analysis ([8] and [9]) and strategy selection for general problem solving [10]• constitute a consistent cognitive perspective. Any natural language understanding system must incorporate mechanisms to perform word sense disambiguation in the context of open-ended world knowledge. The importance of these mechanisms for word usage diagnosis derives from the ubiquity of local ambiguities, and brought about the notion that they be made the central processes of computational analysis and understanding. Consideration of almost any English content word leads to a realization of the scope of the problem -- with a little time and perhaps help from the dictionary, many distinct us</context>
</contexts>
<marker>[10]</marker>
<rawString>Rieger, C., An Organization of Knowledge for Problem Solving and Language Comprehension, Artificial Intelligence, vol. 7, no. 2, 1976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Small</author>
</authors>
<title>Conceptual Language Analysis for Story Comprehension,</title>
<date>1978</date>
<tech>Technical Report 663,</tech>
<institution>University of Maryland,</institution>
<contexts>
<context position="6098" citStr="[11]" startWordPosition="916" endWordPosition="916">uity of local ambiguities, and brought about the notion that they be made the central processes of computational analysis and understanding. Consideration of almost any English content word leads to a realization of the scope of the problem -- with a little time and perhaps help from the dictionary, many distinct usages can be identified. As a simple illustration, several usages each for the words &amp;quot;heavy&amp;quot; and &amp;quot;ice&amp;quot; appear in Figure 1. Each of these seemingly benign words exhibits a rich depth of contextual use. An earlier paper contains a list of almost sixty verbal usages for the word &amp;quot;take&amp;quot; [11]. The representation of all contextual word usages in an active way that insures their utility for linguistic diagnosis led to the notion of word experts. Each word expert is a procedural entity-E8gaz&amp;nc of all possible contextual interpretations of the word it represents.4 When placed in a context formed by experts for the other words in a sentence, each expert should be capable of sufficient context-probing and self-examination to determine successfully its functional or semantic role, and further, to realize the nature of that function or the precise meaning of the word. The representation </context>
</contexts>
<marker>[11]</marker>
<rawString>Small S., Conceptual Language Analysis for Story Comprehension, Technical Report 663, University of Maryland, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Small</author>
</authors>
<title>Word Experts for Conceptual Language Analysis, Ph.D. Thesis (forthcoming),</title>
<date>1980</date>
<volume>13</volume>
<institution>University of Maryland,</institution>
<marker>[12]</marker>
<rawString>Small, S., Word Experts for Conceptual Language Analysis, Ph.D. Thesis (forthcoming), University of Maryland, 1980. [13) McDermott, D. and G. Sussman, The Conniver Reference Manual, AI-Memo 259a, Massachusetts Institute of Technology, 1974.</rawString>
</citation>
<citation valid="true">
<date>1977</date>
<booktitle>Lisp Machine Group, LISP Machine Progress Report, AI-Memo 444, Massachusetts Institute of Technology,</booktitle>
<marker>[14]</marker>
<rawString>Lisp Machine Group, LISP Machine Progress Report, AI-Memo 444, Massachusetts Institute of Technology, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Woods</author>
</authors>
<title>Transition Network Grammars for Natural Language Analysis,</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>10</volume>
<marker>[15]</marker>
<rawString>Woods, W., Transition Network Grammars for Natural Language Analysis, Communications of the ACM, vol. rl, no. 10, 1970.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Erman</author>
<author>V Lesser</author>
</authors>
<title>A Multi-Level Organization for Problem Solving using Many, Diverse, Cooperating Sources of Knowledge,</title>
<date>1975</date>
<booktitle>Proceedings of the 4th International Joint Conference on Artificial Intelligence,</booktitle>
<marker>[16]</marker>
<rawString>Erman, L. and V. Lesser, A Multi-Level Organization for Problem Solving using Many, Diverse, Cooperating Sources of Knowledge, Proceedings of the 4th International Joint Conference on Artificial Intelligence, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Reggie</author>
</authors>
<title>Representing and Using Medical Knowledge for the Neurological Localization Problem (First Report of the NEUREX Project),</title>
<date>1978</date>
<tech>Technical Report 693,</tech>
<pages>181</pages>
<institution>University of Maryland,</institution>
<contexts>
<context position="20257" citStr="[17]" startWordPosition="3209" endWordPosition="3209">rs, the control states of various model components, and more general issues requiring common sense or knowledge of the physical world. (2) In addition, at each node an expert performs actions to affect the lexical and conceptual contents of the workspaces, the control states of itself, concept bins, 6gn ATN without arbitrarily complex LISP computations on each arc and at each node, that is. 7/n addition to common sense knowledge of the physical world, this could include information about the plot, characters, or focus of a children&apos;s story, or in a specialized domain such as medical diagnosis [17], could include highly domain specific knowledge. and the parser as a whole, and the model&apos;s expectations. The current procedural representation of the word expert for &amp;quot;heavy appears as part (b) of Figure 2. Each word expert process includes three components -- a declarative header, a start node, and a body. The header provides a description of the expert&apos;s behavior for purposes of inter-expert constraint forwarding. If sense discrimination by a word expert results in the knowledge that a word to its right, either not yet executed or suspended, must map to a specific sense or conceptual catego</context>
</contexts>
<marker>[17]</marker>
<rawString>Reggie, J., Representing and Using Medical Knowledge for the Neurological Localization Problem (First Report of the NEUREX Project), Technical Report 693, University of Maryland, 1978. [181 Sussman, G., T. Winograd, and E. Charniak, Micro-Planner Reference Manual, AI-Memo 203a, Massachusetts Institute of Technology, 1971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bobrow</author>
<author>T Winograd</author>
</authors>
<title>An Overview of KRL,</title>
<date>1977</date>
<journal>A Knowledge Representation Language, Cognitive Science,</journal>
<volume>1</volume>
<marker>[19]</marker>
<rawString>Bobrow, D. and T. Winograd, An Overview of KRL, A Knowledge Representation Language, Cognitive Science, vol. 1, no. 1, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P London</author>
</authors>
<title>Dependency Networks as a Representation for Modeling in General Problem Solvers,</title>
<date>1978</date>
<tech>Technical Report 698,</tech>
<volume>1</volume>
<institution>University of Maryland,</institution>
<marker>[20]</marker>
<rawString>London, P., Dependency Networks as a Representation for Modeling in General Problem Solvers, Technical Report 698, University of Maryland, 1978. ntelligence, vol. 1, no. 3, 1978.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>