<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000667">
<title confidence="0.941385">
Learning to Prune: Context-Sensitive Pruning for Syntactic MT
</title>
<author confidence="0.99753">
Wenduan Xu Yue Zhang
</author>
<affiliation confidence="0.9987845">
Computer Laboratory Singapore University of
University of Cambridge Technology and Design
</affiliation>
<email confidence="0.886204">
wenduan.xu@cl.cam.ac.uk yue zhang@sutd.edu.sg
</email>
<author confidence="0.985266">
Philip Williams and Philipp Koehn
</author>
<affiliation confidence="0.997885">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.9412255">
p.j.williams-2@sms.ed.ac.uk
pkoehn@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.994496" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949">
We present a context-sensitive chart prun-
ing method for CKY-style MT decoding.
Source phrases that are unlikely to have
aligned target constituents are identified
using sequence labellers learned from the
parallel corpus, and speed-up is obtained
by pruning corresponding chart cells. The
proposed method is easy to implement, or-
thogonal to cube pruning and additive to
its pruning power. On a full-scale English-
to-German experiment with a string-to-
tree model, we obtain a speed-up of more
than 60% over a strong baseline, with no
loss in BLEU.
</bodyText>
<sectionHeader confidence="0.998104" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948">
Syntactic MT models suffer from decoding effi-
ciency bottlenecks introduced by online n-gram
language model integration and high grammar
complexity. Various efforts have been devoted to
improving decoding efficiency, including hyper-
graph rescoring (Heafield et al., 2013; Huang and
Chiang, 2007), coarse-to-fine processing (Petrov
et al., 2008; Zhang and Gildea, 2008) and gram-
mar transformations (Zhang et al., 2006). For
more expressive, linguistically-motivated syntac-
tic MT models (Galley et al., 2004; Galley et
al., 2006), the grammar complexity has grown
considerably over hierarchical phrase-based mod-
els (Chiang, 2007), and decoding still suffers from
efficiency issues (DeNero et al., 2009).
In this paper, we study a chart pruning method
for CKY-style MT decoding that is orthogonal to
cube pruning (Chiang, 2007) and additive to its
pruning power. The main intuition of our method
is to find those source phrases (i.e. any sequence
of consecutive words) that are unlikely to have any
consistently aligned target counterparts according
to the source context and grammar constraints. We
show that by using highly-efficient sequence la-
belling models learned from the bitext used for
translation model training, such phrases can be ef-
fectively identified prior to MT decoding, and cor-
responding chart cells can be excluded for decod-
ing without affecting translation quality.
We call our method context-sensitive pruning
(CSP); it can be viewed as a bilingual adap-
tation of similar methods in monolingual pars-
ing (Roark and Hollingshead, 2008; Zhang et al.,
2010) which improve parsing efficiency by “clos-
ing” chart cells using binary classifiers. Our con-
tribution is that we demonstrate such methods can
be applied to synchronous-grammar parsing by la-
belling the source-side alone. This is achieved
through a novel training scheme where the la-
belling models are trained over the word-aligned
bitext and gold-standard pruning labels are ob-
tained by projecting target-side constituents to the
source words. To our knowledge, this is the first
work to apply this technique to MT decoding.
The proposed method is easy to implement
and effective in practice. Results on a full-scale
English-to-German experiment show that it gives
more than 60% speed-up over a strong cube prun-
ing baseline, with no loss in BLEU. While we use
a string-to-tree model in this paper, the approach
can be adapted to other syntax-based models.
</bodyText>
<page confidence="0.449461">
352
</page>
<note confidence="0.61221">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 352–357,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.94635225">
y na rmi genon-te
esandlar Tgr ammar
of
synta
ma
n
cti
M
c
nsThea
bstra
ct
rul
b
uc h enNP -S B P PERwir P U
ut wen e e dtha tref o
rm process .TO PS- T
OP KONdenn NP-O APDATd iesenNNR
eformp rozeßVVFINbr a
NC ..eO KON →hb ut ,den nie
dNP- SB → hwe ,wirieeN P-O A → h t
X1., S- TOP1.ie wS-TOP→hbutX 1 needX2, dennN
P -OA 2brauchen NP-SB1i
hatreform process,d ie senRefo rmpro zeßiecTOP→h
</figure>
<figureCaption confidence="0.99964">
Figure 1: Ase
</figureCaption>
<bodyText confidence="0.9192755">
lec tionofgr ammar rulese xtrac tabl efr oman exa
mpleword -aligneds en tencep air.2 T heBasel in e
</bodyText>
<subsectionHeader confidence="0.558961">
String-to-Tree ModelOur basel inetransla tionmod
</subsectionHeader>
<bodyText confidence="0.985770486486487">
eluse s ther ule ex-tract ionalgor it hm ofC hi ang
(2007) a dap tedto astri ng-to- treegra mma r.A fte
rextractin gphrasalp ai r s usingthest an dar dappr
oach ofK oehneta l.( 2003 ),allpair s whos e t a rg e
tphrases aren ot e x-haustivel y domin at edbya c onst
itue ntofthe parse t r ee a re r emo vedan d ea chr
ema in in gpair, a f,e i,toget herwit hitsconstit
uentl abe l,C,forms al ex-icalgram mar ru le:C
p af,e i. Therules ru, r2,and r finF ig ure 1arel exic
alr ules.Non-lex icalrulesare ge nera tedbyel imin
ating one or morep airso ft erm in a
lsu bstrings fromanexi st i ngrulea nd sub
sti tu tingnon -t ermin als.Thispr ocesspro -du cest
heex amplerule sraand r5.Ou rdecodin galgori th
mis a varia ntofCKY and issimilar to othe ra lgor
ith mst ailored fors pe -cifi csynt actictrans lati on
gra mmars(DeNero etal., 2009 ;Ho pkinsandLang mea
d,2010).B yta k-ingth esource-s id eof eachru l
e,pro jecting ontoitthe non-term in a l labelsfromt
heta rget-side ,a ndweighting the gramm aracc
ordingto themodel’ slocalsco ring fe atures ,dec
oding is ast raightf or- wardextensio nofmono
ling ualweig hted char tpars i
n g.Non -localf
eat ures,suchas
n-g ramlan-g uage m ode l scor es,areincorp orat ed
thro ughcubepr un ing(Chia ng, 20 07).3 ChartP
owi
scause tr ans lationp roductstheo fv alu eNP-
TOPNP-AGN NPro duk teAR T derNNWert( a)en-depr od
uct stheof value NPNNe iN PDEG iN Nb)(
runing
vatio ti 1Mo 3.
lsets
b)en-jpFigure2 :T woe xamp leali gnm ents.I n(a)“t
hep roducts”doe snoth avea co nsistentali gnme
ntonth e target sid e,whileitdoe sin( b) .overge
ner ation atthe span lev elandr en derdeco d-ingineffic
ient.Prior wo rkon mono lingualsy n-t ac ticparsi
ngha sd emon st ratedtha tby exc lud -ingchartcell
sthat a re likelyto v iolatec onstituen tco nstr
aints, de codingef fic iencycanbei mprovedwithnol
ossinaccur acy (Roar k and H ollingshe ad,2008).Wec
onsider asimilarmech
a nismforsyn -tactic MT decod in gbypro hib
iting s ubtrans la ti ongenerationfo r chartce
llsv iola tin gsynchro nous- gramm arco nst rain
ts.Amotiv ati ngexample is shown inFi gu re2a,wher
eas egment o fanEngl ish-Ge rma nse nten cepairfro
mt het rain ingdata,along withit swordalignme nt
and t arget-si deparset ree isdepic ted .The
Engli sh phrases“v alue o f”a nd“the pr oduc ts” do
nothav ecorre spo ndin g Germantra nslation sinth
is example. Al thoug hthegra mma rmayhaverul
esto tran sla tethese twophrase s,t hey ca nb esa
felypr unedfort his partic ular sentenc epa ir. Inco
ntra stto cha rtp runi ngfo rmonolingua lp arsing
,our pruni ngd ecisionsa rebased ontheso urcecon
text,itst argettra nslat io nandthemap -pingbe twee
nt hetw o. Thisdisti nct ioni simpor- tantsinc
ethe syntacti ccorres pond encebe twe endiffe re
ntlang uagepair sis differe nt.Sup pose thatw
ewere totr an s latet hesame Engl ish s entencei
ntoJapanese (Figu r e2a);u nlikethe Engl ishto Ge rm
anexample,the English ph rase “thepro d-u c
ts” wil lbe a vali dp hrase tha t h asa Japanese
transl ationu nde r ata rgetconst ituent,sinc eiti
ssy ntactical ly align ed to“t p”(Figu re2b).The key
qu estio nt oco nsider is how t o i njectta rget sy nta
xandworng two sections.
rma tio to
ng models, so hat our pruing labelli decisions cn
ed on the ouce alone, we address tis in e be bas
f oll
</bodyText>
<subsectionHeader confidence="0.995805">
3.2 Pruning by Labelling
</subsectionHeader>
<bodyText confidence="0.989847">
We use binary tags to indicate whether a source
word can start or end a multi-word phrase that has
</bodyText>
<figure confidence="0.846102333333333">
353
1 0 1 1 1 1 1 1 0 1
(a) b-tags (b) e-tags
</figure>
<figureCaption confidence="0.802831666666667">
Figure 3: The pruning effects of two types of bi-
nary tags. The shaded cells are pruned and two
types of tags are assigned independently.
</figureCaption>
<bodyText confidence="0.999384090909091">
a consistently aligned target constituent. We call
these two types the b-tag and the e-tag, respec-
tively, and use the set of values {0, 1} for both.
Under this scheme, a b-tag value of 1 indi-
cates that a source word can be the start of a
source phrase that has a consistently aligned target
phrase; similarly an e-tag of 0 indicates that a word
cannot end a source phrase. If either the b-tag or
the e-tag of an input phrase is 0, the correspond-
ing chart cells will be pruned. The pruning effects
of the two types of tags are illustrated in Figure 3.
In general, 0-valued b-tags prune a whole column
of chart cells and 0-valued e-tags prune a whole
diagonal of cells; and the chart cells on the first
row and the top-most cell are always kept so that
complete translations can always be found.
We build a separate labeller for each tag type us-
ing gold-standard b- and e-tags, respectively. We
train the labellers with maximum-entropy models
(Curran and Clark, 2003; Ratnaparkhi, 1996), us-
ing features similar to those used for suppertag-
ging for CCG parsing (Clark and Curran, 2004).
In each case, features for a pruning tag consist
of word and POS uni-grams extracted from the 5-
word window with the current word in the middle,
POS trigrams ending with the current word, as well
as two previous tags as a bigram and two separate
uni-grams. Our pruning labellers are highly effi-
cient, run in linear time and add little overhead to
decoding. During testing, in order to prevent over-
pruning, a probability cutoff value θ is used. A tag
value of 0 is assigned to a word only if its marginal
probability is greater than θ.
</bodyText>
<subsectionHeader confidence="0.998152">
3.3 Gold-standard Pruning Tags
</subsectionHeader>
<bodyText confidence="0.9974938">
Gold-standard tags are extracted from the word-
aligned bitext used for translation model train-
ing, respecting rule extraction constraints, which
is crucial for the success of our method.
For each training sentence pair, gold-standard
b-tags and e-tags are assigned separately to the
Algorithm 1 Gold-standard Labelling Algorithm
Input forward alignment Ae∼f, backward align-
ment ˆAf∼e and 1-best parse tree τ for f
Output Tag sequences b and e for e
</bodyText>
<listItem confidence="0.974814461538462">
1: procedure TAG(e, f, τ, A, ˆA)
2: l ← |e|
3: for i ← 0 to l − 1 do
4: b[i] ← 0, e[i] ← 0
5: for f[i0, j0] in τ do
6: s ← { ˆA[k]  |k ∈ [i0, j0]}
7: if|s |≤ 1 then continue
8: i ← min(s), j ← max(s)
9: if CONSISTENT(i, j, i0, j0) then
10: b[i0] ← 1, e[j0] ← 1
11: procedure CONSISTENT(i, j, i0, j0)
12: t ← {A[k]  |k ∈ [i, j]}
13: return min(t) ≥ i0 and max(t) ≤ j0
</listItem>
<bodyText confidence="0.999806322580645">
source words. First, we initialize both tags of each
source word to 0s. Then, we iterate through all tar-
get constituent spans, and for each span, we find
its corresponding source phrase, as determined by
the word alignment. If a constituent exists for the
phrase pair, the b-tag of the first word and the e-tag
of the last word in the source phrase are set to 1s,
respectively. Pseudocode is shown in Algorithm 1.
Note that our definition of the gold-standard al-
lows source-side labels to integrate bilingual in-
formation. On line 6, the target-side syntax is
projected to the source; on line 9, consistency is
checked against word alignment.
Consider again the alignment in Figure 2a. Tak-
ing the target constituent span covering “der Pro-
dukte” as an example, the source phrase under a
consistent word alignment is “of the products”.
Thus, the b-tag of “of” and the e-tag of “prod-
ucts” are set to 1s. After considering all target
constituent spans, the complete b- and e-tag se-
quences for the source-side phrase in Figure 2a
are [1, 1, 0, 0] and [0, 0,1,1], respectively. Note
that, since we never prune single-word spans, we
ignore source phrases under consistent one-to-one
or one-to-many alignments.
From the gold standard data, we found 73.69%
of the 54M words do not begin a multi-word
aligned phrase and 77.71% do not end a multi-
word aligned phrase; the 1-best accuracies of the
two labellers tested on a held-out 20K sentences
are 82.50% and 88.78% respectively.
</bodyText>
<page confidence="0.519261">
354
</page>
<figure confidence="0.999939583333333">
0 2 4 6 8 10 12 14 16 18 20
CPU seconds/sentence
(a) time vs. BLEU
0 0.5 1 1.5 2 2.5
Hypothesis Count (x106)
(b) hypo count vs. BLEU
csp
cube pruning
csp
cube pruning
BLEU 0.149
0.1485
0.148
0.1475
0.147
0.1465
0.146
BLEU 0.149
0.1485
0.148
0.1475
0.147
0.1465
0.146
</figure>
<figureCaption confidence="0.999877">
Figure 4: Translation quality comparison with the cube pruning baseline.
</figureCaption>
<sectionHeader confidence="0.996115" genericHeader="introduction">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.983184">
4.1 Setup
</subsectionHeader>
<bodyText confidence="0.999750885714285">
A Moses (Koehn et al., 2007) string-to-tree sys-
tem is used as our baseline. The training cor-
pus consists of the English-German sections of
the Europarl (Koehn, 2005) and the News Com-
mentary corpus. Discarding pairs without target-
side parses, the final training data has 2M sen-
tence pairs, with 54M and 52M words on the
English and German sides, respectively. Word-
alignments are obtained by running GIZA++ (Och
and Ney, 2000) in both directions and refined
with “grow-diag-final-and” (Koehn et al., 2003).
For all experiments, a 5-gram language model
with Kneser-Ney smoothing (Chen and Goodman,
1996) built with the SRILM Toolkit (Stolcke and
others, 2002) is used.
The development and test sets are the 2008
WMT newstest (2,051 sentences) and 2009 WMT
newstest (2,525 sentences) respectively. Feature
weights are tuned with MERT (Och, 2003) on
the development set and output is evaluated us-
ing case-sensitive BLEU (Papineni et al., 2002).
For both rule extraction and decoding, up to seven
terminal/non-terminal symbols on the source-side
are allowed. For decoding, the maximum span-
length is restricted to 15, and the grammar is pre-
filtered to match the entire test set for both the
baseline system and the chart pruning decoder.
We use two labellers to perform b- and e-tag la-
belling independently prior to decoding. Training
of the labelling models is able to complete in un-
der 2.5 hours and the whole test set is labelled in
under 2 seconds. A standard perceptron POS tag-
ger (Collins, 2002) trained on Wall Street Journal
sections 2-21 of the Penn Treebank is used to as-
sign POS tags for both our training and test data.
</bodyText>
<subsectionHeader confidence="0.681281">
4.2 Results
</subsectionHeader>
<bodyText confidence="0.998939121212121">
Figures 4a and 4b compare CSP with the cube
pruning baseline in terms of BLEU. Decoding
speed is measured by the average decoding time
and average number of hypotheses generated per
sentence. We first run the baseline decoder un-
der various beam settings (b = 100 - 2500) un-
til no further increase in BLEU is observed. We
then run the CSP decoder with a range of 0 val-
ues (0 = 0.91 − 0.99), at the default beam size
of 1000 of the baseline decoder. The CSP de-
coder, which considers far fewer chart cells and
generates significantly fewer subtranslations, con-
sistently outperforms the slower baseline. It ulti-
mately achieves a BLEU score of 14.86 at a proba-
bility cutoff value of 0.98, slightly higher than the
highest score of the baseline.
At all levels of comparable translation quality,
our decoder is faster than the baseline. On aver-
age, the speed-up gained is 63.58% as measured
by average decoding time, and comparing on a
point-by-point basis, our decoder always runs over
60% faster. At the 0 value of 0.98, it yields a
speed-up of 57.30%, compared with a beam size
of 400 for the baseline, where both achieved the
highest BLEU.
Figures 5a and 5b demonstrate the pruning
power of CSP (0 = 0.95) in comparison with the
baseline (beam size = 300); across all the cutoff
values and beam sizes, the CSP decoder considers
54.92% fewer translation hypotheses on average
and the minimal reduction achieved is 46.56%.
Figure 6 shows the percentage of spans of dif-
ferent lengths pruned by CSP (0 = 0.98). As ex-
</bodyText>
<figure confidence="0.979138258064516">
355
7
6
5
4
3
2
1
Hypothesis Count (x106)
0
9000
8000
Chart Cell Count
7000
6000
5000
4000
3000
2000
1000
0
0 20 40 60 80 100 120 140
Sentence Length
(a) sentence length vs. hypo count
0 20 40 60 80 100 120 140
Sentence Length
(b) sentence length vs. cell count
csp
cube pruning
csp
cube pruning
</figure>
<figureCaption confidence="0.958421">
Figure 5: Search space comparison with the cube pruning baseline.
</figureCaption>
<figure confidence="0.9992573">
Percentage Pruned (%)
60
50
40
30
20
10
0
0 20 40 60 80 100 120
Span Length
</figure>
<figureCaption confidence="0.99995">
Figure 6: Percentage of spans of different lengths pruned at θ = 0.98.
</figureCaption>
<bodyText confidence="0.999971333333333">
pected, longer spans are pruned more often, as
they are more likely to be at the intersections of
cells pruned by the two types of pruning labels,
thus can be pruned by either type.
We also find CSP does not improve search qual-
ity and it leads to slightly lower model scores,
which shows that some higher scored translation
hypotheses are pruned. This, however, is perfectly
desirable. Since our pruning decisions are based
on independent labellers using contextual infor-
mation, with the objective of eliminating unlikely
subtranslations and rule applications. It may even
offset defects of the translation model (i.e. high-
scored bad translations). The fact that the output
BLEU did not decrease supports this reasoning.
Finally, it is worth noting that our string-to-tree
model does not force complete target parses to be
built during decoding, which is not required in our
pruning method either. We do not use any other
heuristics (other than keeping singleton and the
top-most cells) to make complete translation al-
ways possible. The hypothesis here is that good
labelling models should not affect the derivation
of complete target translations.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999842125">
We presented a novel sequence labelling based,
context-sensitive pruning method for a string-to-
tree MT model. Our method achieves more than
60% speed-up over a state-of-the-art baseline on
a full-scale translation task. In future work, we
plan to adapt our method to models with differ-
ent rule extraction algorithms, such as Hiero and
forest-based translation (Mi and Huang, 2008).
</bodyText>
<sectionHeader confidence="0.99558" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999539">
We thank the anonymous reviewers for comments.
The first author is fully supported by the Carnegie
Trust and receives additional support from the
Cambridge Trusts. Yue Zhang is supported by
SUTD under the grant SRG ISTD 2012-038.
Philip Williams and Philipp Koehn are supported
under EU-FP7-287658 (EU BRIDGE).
</bodyText>
<page confidence="0.85281">
356
</page>
<sectionHeader confidence="0.995016" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999732421686747">
S.F. Chen and J. Goodman. 1996. An empirical study
of smoothing techniques for language modeling. In
Proc. ACL, pages 310–318.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201–228.
S. Clark and J.R. Curran. 2004. The importance of su-
pertagging for wide-coverage ccg parsing. In Proc.
COLING, page 282.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: Theory and experi-
ments with perceptron algorithms. In Proc. EMNLP,
pages 1–8.
J.R. Curran and S. Clark. 2003. Investigating gis and
smoothing for maximum entropy taggers. In Proc.
EACL, pages 91–98.
John DeNero, Mohit Bansal, Adam Pauls, and Dan
Klein. 2009. Efficient parsing for transducer gram-
mars. In Proc. NAACL-HLT, pages 227–235.
M. Galley, M. Hopkins, K. Knight, and D. Marcu.
2004. What’s in a translation rule. In Proc. HLT-
NAACL, pages 273–280.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
COLING and ACL, pages 961–968.
Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2013. Grouping language model boundary words to
speed k–best extraction from hypergraphs. In Proc.
NAACL.
Mark Hopkins and Greg Langmead. 2010. SCFG
decoding without binarization. In Proc. EMNLP,
pages 646–655, October.
L. Huang and D. Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Proc. ACL, volume 45, page 144.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
NAACL-HLT, pages 48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL Demo Sessions, pages 177–180.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In MT Summit, volume 5.
H. Mi and L. Huang. 2008. Forest-based translation
rule extraction. In Proc. EMNLP, pages 206–214.
F. J. Och and H. Ney. 2000. Improved statistical
alignment models. In Proc. ACL, pages 440–447,
Hongkong, China, October.
F.J. Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proc. ACL, pages 160–
167.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. ACL, pages 311–318.
S. Petrov, A. Haghighi, and D. Klein. 2008. Coarse-
to-fine syntactic machine translation using language
projections. In Proc. ACL, pages 108–116.
A. Ratnaparkhi. 1996. A maximum entropy model for
part-of-speech tagging. In Proc. EMNLP, volume 1,
pages 133–142.
Brian Roark and Kristy Hollingshead. 2008. Classify-
ing chart cells for quadratic complexity context-free
inference. In Proc. COLING, pages 745–751.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity context-free parsing pipelines via chart
constraints. In Proc. NAACL, pages 647–655.
A. Stolcke et al. 2002. Srilm-an extensible language
modeling toolkit. In Proc. ICSLP, volume 2, pages
901–904.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proc. ACL.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proc. NAACL, pages 256–263.
Y. Zhang, B.G. Ahn, S. Clark, C. Van Wyk, J.R. Cur-
ran, and L. Rimell. 2010. Chart pruning for fast
lexicalised-grammar parsing. In Proc. COLING,
pages 1471–1479.
</reference>
<page confidence="0.889566">
357
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.179734">
<title confidence="0.999605">Learning to Prune: Context-Sensitive Pruning for Syntactic MT</title>
<author confidence="0.997146">Wenduan Xu Yue Zhang</author>
<affiliation confidence="0.779638666666667">Computer Laboratory Singapore University of University of Cambridge Technology and Design wenduan.xu@cl.cam.ac.uk yue zhang@sutd.edu.sg Williams School of University of</affiliation>
<email confidence="0.98629">pkoehn@inf.ed.ac.uk</email>
<abstract confidence="0.996874866666667">We present a context-sensitive chart prunmethod for MT decoding. Source phrases that are unlikely to have aligned target constituents are identified using sequence labellers learned from the parallel corpus, and speed-up is obtained by pruning corresponding chart cells. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling.</title>
<date>1996</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>310--318</pages>
<contexts>
<context position="12564" citStr="Chen and Goodman, 1996" startWordPosition="2153" endWordPosition="2156"> Experiments 4.1 Setup A Moses (Koehn et al., 2007) string-to-tree system is used as our baseline. The training corpus consists of the English-German sections of the Europarl (Koehn, 2005) and the News Commentary corpus. Discarding pairs without targetside parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,525 sentences) respectively. Feature weights are tuned with MERT (Och, 2003) on the development set and output is evaluated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on the source-side are allowed. For decoding, the maximum spanlength is restricted to 15, and the grammar is prefiltered to match the entire test set for both the baseline system </context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>S.F. Chen and J. Goodman. 1996. An empirical study of smoothing techniques for language modeling. In Proc. ACL, pages 310–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1210" citStr="Chiang, 2007" startWordPosition="168" endWordPosition="169">ained by pruning corresponding chart cells. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU. 1 Introduction Syntactic MT models suffer from decoding efficiency bottlenecks introduced by online n-gram language model integration and high grammar complexity. Various efforts have been devoted to improving decoding efficiency, including hypergraph rescoring (Heafield et al., 2013; Huang and Chiang, 2007), coarse-to-fine processing (Petrov et al., 2008; Zhang and Gildea, 2008) and grammar transformations (Zhang et al., 2006). For more expressive, linguistically-motivated syntactic MT models (Galley et al., 2004; Galley et al., 2006), the grammar complexity has grown considerably over hierarchical phrase-based models (Chiang, 2007), and decoding still suffers from efficiency issues (DeNero et al., 2009). In this paper, we study a chart pruning method for CKY-style MT decoding that is orthogonal to cube pruning (Chiang, 2007) and additive to its pruning power. The main intuition of our method is</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33(2):201–228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>The importance of supertagging for wide-coverage ccg parsing.</title>
<date>2004</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>282</pages>
<contexts>
<context position="8745" citStr="Clark and Curran, 2004" startWordPosition="1471" endWordPosition="1474">ls will be pruned. The pruning effects of the two types of tags are illustrated in Figure 3. In general, 0-valued b-tags prune a whole column of chart cells and 0-valued e-tags prune a whole diagonal of cells; and the chart cells on the first row and the top-most cell are always kept so that complete translations can always be found. We build a separate labeller for each tag type using gold-standard b- and e-tags, respectively. We train the labellers with maximum-entropy models (Curran and Clark, 2003; Ratnaparkhi, 1996), using features similar to those used for suppertagging for CCG parsing (Clark and Curran, 2004). In each case, features for a pruning tag consist of word and POS uni-grams extracted from the 5- word window with the current word in the middle, POS trigrams ending with the current word, as well as two previous tags as a bigram and two separate uni-grams. Our pruning labellers are highly efficient, run in linear time and add little overhead to decoding. During testing, in order to prevent overpruning, a probability cutoff value θ is used. A tag value of 0 is assigned to a word only if its marginal probability is greater than θ. 3.3 Gold-standard Pruning Tags Gold-standard tags are extracte</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>S. Clark and J.R. Curran. 2004. The importance of supertagging for wide-coverage ccg parsing. In Proc. COLING, page 282.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="13458" citStr="Collins, 2002" startWordPosition="2305" endWordPosition="2306">valuated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on the source-side are allowed. For decoding, the maximum spanlength is restricted to 15, and the grammar is prefiltered to match the entire test set for both the baseline system and the chart pruning decoder. We use two labellers to perform b- and e-tag labelling independently prior to decoding. Training of the labelling models is able to complete in under 2.5 hours and the whole test set is labelled in under 2 seconds. A standard perceptron POS tagger (Collins, 2002) trained on Wall Street Journal sections 2-21 of the Penn Treebank is used to assign POS tags for both our training and test data. 4.2 Results Figures 4a and 4b compare CSP with the cube pruning baseline in terms of BLEU. Decoding speed is measured by the average decoding time and average number of hypotheses generated per sentence. We first run the baseline decoder under various beam settings (b = 100 - 2500) until no further increase in BLEU is observed. We then run the CSP decoder with a range of 0 values (0 = 0.91 − 0.99), at the default beam size of 1000 of the baseline decoder. The CSP d</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden Markov models: Theory and experiments with perceptron algorithms. In Proc. EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Curran</author>
<author>S Clark</author>
</authors>
<title>Investigating gis and smoothing for maximum entropy taggers.</title>
<date>2003</date>
<booktitle>In Proc. EACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="8628" citStr="Curran and Clark, 2003" startWordPosition="1452" endWordPosition="1455">rd cannot end a source phrase. If either the b-tag or the e-tag of an input phrase is 0, the corresponding chart cells will be pruned. The pruning effects of the two types of tags are illustrated in Figure 3. In general, 0-valued b-tags prune a whole column of chart cells and 0-valued e-tags prune a whole diagonal of cells; and the chart cells on the first row and the top-most cell are always kept so that complete translations can always be found. We build a separate labeller for each tag type using gold-standard b- and e-tags, respectively. We train the labellers with maximum-entropy models (Curran and Clark, 2003; Ratnaparkhi, 1996), using features similar to those used for suppertagging for CCG parsing (Clark and Curran, 2004). In each case, features for a pruning tag consist of word and POS uni-grams extracted from the 5- word window with the current word in the middle, POS trigrams ending with the current word, as well as two previous tags as a bigram and two separate uni-grams. Our pruning labellers are highly efficient, run in linear time and add little overhead to decoding. During testing, in order to prevent overpruning, a probability cutoff value θ is used. A tag value of 0 is assigned to a wo</context>
</contexts>
<marker>Curran, Clark, 2003</marker>
<rawString>J.R. Curran and S. Clark. 2003. Investigating gis and smoothing for maximum entropy taggers. In Proc. EACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Mohit Bansal</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Efficient parsing for transducer grammars.</title>
<date>2009</date>
<booktitle>In Proc. NAACL-HLT,</booktitle>
<pages>227--235</pages>
<contexts>
<context position="1615" citStr="DeNero et al., 2009" startWordPosition="224" endWordPosition="227">nline n-gram language model integration and high grammar complexity. Various efforts have been devoted to improving decoding efficiency, including hypergraph rescoring (Heafield et al., 2013; Huang and Chiang, 2007), coarse-to-fine processing (Petrov et al., 2008; Zhang and Gildea, 2008) and grammar transformations (Zhang et al., 2006). For more expressive, linguistically-motivated syntactic MT models (Galley et al., 2004; Galley et al., 2006), the grammar complexity has grown considerably over hierarchical phrase-based models (Chiang, 2007), and decoding still suffers from efficiency issues (DeNero et al., 2009). In this paper, we study a chart pruning method for CKY-style MT decoding that is orthogonal to cube pruning (Chiang, 2007) and additive to its pruning power. The main intuition of our method is to find those source phrases (i.e. any sequence of consecutive words) that are unlikely to have any consistently aligned target counterparts according to the source context and grammar constraints. We show that by using highly-efficient sequence labelling models learned from the bitext used for translation model training, such phrases can be effectively identified prior to MT decoding, and correspondi</context>
</contexts>
<marker>DeNero, Bansal, Pauls, Klein, 2009</marker>
<rawString>John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein. 2009. Efficient parsing for transducer grammars. In Proc. NAACL-HLT, pages 227–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>M Hopkins</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>What’s in a translation rule.</title>
<date>2004</date>
<booktitle>In Proc. HLTNAACL,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="1420" citStr="Galley et al., 2004" startWordPosition="196" endWordPosition="199">g-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU. 1 Introduction Syntactic MT models suffer from decoding efficiency bottlenecks introduced by online n-gram language model integration and high grammar complexity. Various efforts have been devoted to improving decoding efficiency, including hypergraph rescoring (Heafield et al., 2013; Huang and Chiang, 2007), coarse-to-fine processing (Petrov et al., 2008; Zhang and Gildea, 2008) and grammar transformations (Zhang et al., 2006). For more expressive, linguistically-motivated syntactic MT models (Galley et al., 2004; Galley et al., 2006), the grammar complexity has grown considerably over hierarchical phrase-based models (Chiang, 2007), and decoding still suffers from efficiency issues (DeNero et al., 2009). In this paper, we study a chart pruning method for CKY-style MT decoding that is orthogonal to cube pruning (Chiang, 2007) and additive to its pruning power. The main intuition of our method is to find those source phrases (i.e. any sequence of consecutive words) that are unlikely to have any consistently aligned target counterparts according to the source context and grammar constraints. We show tha</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>M. Galley, M. Hopkins, K. Knight, and D. Marcu. 2004. What’s in a translation rule. In Proc. HLTNAACL, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proc. COLING and ACL,</booktitle>
<pages>961--968</pages>
<contexts>
<context position="1442" citStr="Galley et al., 2006" startWordPosition="200" endWordPosition="203">tain a speed-up of more than 60% over a strong baseline, with no loss in BLEU. 1 Introduction Syntactic MT models suffer from decoding efficiency bottlenecks introduced by online n-gram language model integration and high grammar complexity. Various efforts have been devoted to improving decoding efficiency, including hypergraph rescoring (Heafield et al., 2013; Huang and Chiang, 2007), coarse-to-fine processing (Petrov et al., 2008; Zhang and Gildea, 2008) and grammar transformations (Zhang et al., 2006). For more expressive, linguistically-motivated syntactic MT models (Galley et al., 2004; Galley et al., 2006), the grammar complexity has grown considerably over hierarchical phrase-based models (Chiang, 2007), and decoding still suffers from efficiency issues (DeNero et al., 2009). In this paper, we study a chart pruning method for CKY-style MT decoding that is orthogonal to cube pruning (Chiang, 2007) and additive to its pruning power. The main intuition of our method is to find those source phrases (i.e. any sequence of consecutive words) that are unlikely to have any consistently aligned target counterparts according to the source context and grammar constraints. We show that by using highly-effi</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proc. COLING and ACL, pages 961–968.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Philipp Koehn</author>
<author>Alon Lavie</author>
</authors>
<title>Grouping language model boundary words to speed k–best extraction from hypergraphs.</title>
<date>2013</date>
<booktitle>In Proc. NAACL.</booktitle>
<contexts>
<context position="1185" citStr="Heafield et al., 2013" startWordPosition="162" endWordPosition="165">allel corpus, and speed-up is obtained by pruning corresponding chart cells. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU. 1 Introduction Syntactic MT models suffer from decoding efficiency bottlenecks introduced by online n-gram language model integration and high grammar complexity. Various efforts have been devoted to improving decoding efficiency, including hypergraph rescoring (Heafield et al., 2013; Huang and Chiang, 2007), coarse-to-fine processing (Petrov et al., 2008; Zhang and Gildea, 2008) and grammar transformations (Zhang et al., 2006). For more expressive, linguistically-motivated syntactic MT models (Galley et al., 2004; Galley et al., 2006), the grammar complexity has grown considerably over hierarchical phrase-based models (Chiang, 2007), and decoding still suffers from efficiency issues (DeNero et al., 2009). In this paper, we study a chart pruning method for CKY-style MT decoding that is orthogonal to cube pruning (Chiang, 2007) and additive to its pruning power. The main i</context>
</contexts>
<marker>Heafield, Koehn, Lavie, 2013</marker>
<rawString>Kenneth Heafield, Philipp Koehn, and Alon Lavie. 2013. Grouping language model boundary words to speed k–best extraction from hypergraphs. In Proc. NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Greg Langmead</author>
</authors>
<title>SCFG decoding without binarization.</title>
<date>2010</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>646--655</pages>
<marker>Hopkins, Langmead, 2010</marker>
<rawString>Mark Hopkins and Greg Langmead. 2010. SCFG decoding without binarization. In Proc. EMNLP, pages 646–655, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
<author>D Chiang</author>
</authors>
<title>Forest rescoring: Faster decoding with integrated language models.</title>
<date>2007</date>
<booktitle>In Proc. ACL,</booktitle>
<volume>45</volume>
<pages>144</pages>
<contexts>
<context position="1210" citStr="Huang and Chiang, 2007" startWordPosition="166" endWordPosition="169">-up is obtained by pruning corresponding chart cells. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU. 1 Introduction Syntactic MT models suffer from decoding efficiency bottlenecks introduced by online n-gram language model integration and high grammar complexity. Various efforts have been devoted to improving decoding efficiency, including hypergraph rescoring (Heafield et al., 2013; Huang and Chiang, 2007), coarse-to-fine processing (Petrov et al., 2008; Zhang and Gildea, 2008) and grammar transformations (Zhang et al., 2006). For more expressive, linguistically-motivated syntactic MT models (Galley et al., 2004; Galley et al., 2006), the grammar complexity has grown considerably over hierarchical phrase-based models (Chiang, 2007), and decoding still suffers from efficiency issues (DeNero et al., 2009). In this paper, we study a chart pruning method for CKY-style MT decoding that is orthogonal to cube pruning (Chiang, 2007) and additive to its pruning power. The main intuition of our method is</context>
</contexts>
<marker>Huang, Chiang, 2007</marker>
<rawString>L. Huang and D. Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In Proc. ACL, volume 45, page 144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. NAACL-HLT,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="12467" citStr="Koehn et al., 2003" startWordPosition="2139" endWordPosition="2142">0.147 0.1465 0.146 Figure 4: Translation quality comparison with the cube pruning baseline. 4 Experiments 4.1 Setup A Moses (Koehn et al., 2007) string-to-tree system is used as our baseline. The training corpus consists of the English-German sections of the Europarl (Koehn, 2005) and the News Commentary corpus. Discarding pairs without targetside parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,525 sentences) respectively. Feature weights are tuned with MERT (Och, 2003) on the development set and output is evaluated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on the source-side are allowed. For decoding, the maximum spanlength is restricted</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. NAACL-HLT, pages 48–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proc. ACL Demo Sessions,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="11992" citStr="Koehn et al., 2007" startWordPosition="2062" endWordPosition="2065"> of the 54M words do not begin a multi-word aligned phrase and 77.71% do not end a multiword aligned phrase; the 1-best accuracies of the two labellers tested on a held-out 20K sentences are 82.50% and 88.78% respectively. 354 0 2 4 6 8 10 12 14 16 18 20 CPU seconds/sentence (a) time vs. BLEU 0 0.5 1 1.5 2 2.5 Hypothesis Count (x106) (b) hypo count vs. BLEU csp cube pruning csp cube pruning BLEU 0.149 0.1485 0.148 0.1475 0.147 0.1465 0.146 BLEU 0.149 0.1485 0.148 0.1475 0.147 0.1465 0.146 Figure 4: Translation quality comparison with the cube pruning baseline. 4 Experiments 4.1 Setup A Moses (Koehn et al., 2007) string-to-tree system is used as our baseline. The training corpus consists of the English-German sections of the Europarl (Koehn, 2005) and the News Commentary corpus. Discarding pairs without targetside parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolki</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. In Proc. ACL Demo Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In MT Summit,</booktitle>
<volume>5</volume>
<contexts>
<context position="12129" citStr="Koehn, 2005" startWordPosition="2086" endWordPosition="2087">bellers tested on a held-out 20K sentences are 82.50% and 88.78% respectively. 354 0 2 4 6 8 10 12 14 16 18 20 CPU seconds/sentence (a) time vs. BLEU 0 0.5 1 1.5 2 2.5 Hypothesis Count (x106) (b) hypo count vs. BLEU csp cube pruning csp cube pruning BLEU 0.149 0.1485 0.148 0.1475 0.147 0.1465 0.146 BLEU 0.149 0.1485 0.148 0.1475 0.147 0.1465 0.146 Figure 4: Translation quality comparison with the cube pruning baseline. 4 Experiments 4.1 Setup A Moses (Koehn et al., 2007) string-to-tree system is used as our baseline. The training corpus consists of the English-German sections of the Europarl (Koehn, 2005) and the News Commentary corpus. Discarding pairs without targetside parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In MT Summit, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Mi</author>
<author>L Huang</author>
</authors>
<title>Forest-based translation rule extraction.</title>
<date>2008</date>
<booktitle>In Proc. EMNLP,</booktitle>
<pages>206--214</pages>
<marker>Mi, Huang, 2008</marker>
<rawString>H. Mi and L. Huang. 2008. Forest-based translation rule extraction. In Proc. EMNLP, pages 206–214.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>440--447</pages>
<location>Hongkong, China,</location>
<contexts>
<context position="12388" citStr="Och and Ney, 2000" startWordPosition="2128" endWordPosition="2131">U 0.149 0.1485 0.148 0.1475 0.147 0.1465 0.146 BLEU 0.149 0.1485 0.148 0.1475 0.147 0.1465 0.146 Figure 4: Translation quality comparison with the cube pruning baseline. 4 Experiments 4.1 Setup A Moses (Koehn et al., 2007) string-to-tree system is used as our baseline. The training corpus consists of the English-German sections of the Europarl (Koehn, 2005) and the News Commentary corpus. Discarding pairs without targetside parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,525 sentences) respectively. Feature weights are tuned with MERT (Och, 2003) on the development set and output is evaluated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on </context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>F. J. Och and H. Ney. 2000. Improved statistical alignment models. In Proc. ACL, pages 440–447, Hongkong, China, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="12805" citStr="Och, 2003" startWordPosition="2193" endWordPosition="2194">de parses, the final training data has 2M sentence pairs, with 54M and 52M words on the English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,525 sentences) respectively. Feature weights are tuned with MERT (Och, 2003) on the development set and output is evaluated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on the source-side are allowed. For decoding, the maximum spanlength is restricted to 15, and the grammar is prefiltered to match the entire test set for both the baseline system and the chart pruning decoder. We use two labellers to perform b- and e-tag labelling independently prior to decoding. Training of the labelling models is able to complete in under 2.5 hours and the whole test set is labelled in under 2 seco</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>F.J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. ACL, pages 160– 167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="12902" citStr="Papineni et al., 2002" startWordPosition="2207" endWordPosition="2210">he English and German sides, respectively. Wordalignments are obtained by running GIZA++ (Och and Ney, 2000) in both directions and refined with “grow-diag-final-and” (Koehn et al., 2003). For all experiments, a 5-gram language model with Kneser-Ney smoothing (Chen and Goodman, 1996) built with the SRILM Toolkit (Stolcke and others, 2002) is used. The development and test sets are the 2008 WMT newstest (2,051 sentences) and 2009 WMT newstest (2,525 sentences) respectively. Feature weights are tuned with MERT (Och, 2003) on the development set and output is evaluated using case-sensitive BLEU (Papineni et al., 2002). For both rule extraction and decoding, up to seven terminal/non-terminal symbols on the source-side are allowed. For decoding, the maximum spanlength is restricted to 15, and the grammar is prefiltered to match the entire test set for both the baseline system and the chart pruning decoder. We use two labellers to perform b- and e-tag labelling independently prior to decoding. Training of the labelling models is able to complete in under 2.5 hours and the whole test set is labelled in under 2 seconds. A standard perceptron POS tagger (Collins, 2002) trained on Wall Street Journal sections 2-2</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proc. ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>A Haghighi</author>
<author>D Klein</author>
</authors>
<title>Coarseto-fine syntactic machine translation using language projections.</title>
<date>2008</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>108--116</pages>
<contexts>
<context position="1258" citStr="Petrov et al., 2008" startWordPosition="172" endWordPosition="175">. The proposed method is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU. 1 Introduction Syntactic MT models suffer from decoding efficiency bottlenecks introduced by online n-gram language model integration and high grammar complexity. Various efforts have been devoted to improving decoding efficiency, including hypergraph rescoring (Heafield et al., 2013; Huang and Chiang, 2007), coarse-to-fine processing (Petrov et al., 2008; Zhang and Gildea, 2008) and grammar transformations (Zhang et al., 2006). For more expressive, linguistically-motivated syntactic MT models (Galley et al., 2004; Galley et al., 2006), the grammar complexity has grown considerably over hierarchical phrase-based models (Chiang, 2007), and decoding still suffers from efficiency issues (DeNero et al., 2009). In this paper, we study a chart pruning method for CKY-style MT decoding that is orthogonal to cube pruning (Chiang, 2007) and additive to its pruning power. The main intuition of our method is to find those source phrases (i.e. any sequence</context>
</contexts>
<marker>Petrov, Haghighi, Klein, 2008</marker>
<rawString>S. Petrov, A. Haghighi, and D. Klein. 2008. Coarseto-fine syntactic machine translation using language projections. In Proc. ACL, pages 108–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A maximum entropy model for part-of-speech tagging.</title>
<date>1996</date>
<booktitle>In Proc. EMNLP,</booktitle>
<volume>1</volume>
<pages>133--142</pages>
<contexts>
<context position="8648" citStr="Ratnaparkhi, 1996" startWordPosition="1456" endWordPosition="1457">hrase. If either the b-tag or the e-tag of an input phrase is 0, the corresponding chart cells will be pruned. The pruning effects of the two types of tags are illustrated in Figure 3. In general, 0-valued b-tags prune a whole column of chart cells and 0-valued e-tags prune a whole diagonal of cells; and the chart cells on the first row and the top-most cell are always kept so that complete translations can always be found. We build a separate labeller for each tag type using gold-standard b- and e-tags, respectively. We train the labellers with maximum-entropy models (Curran and Clark, 2003; Ratnaparkhi, 1996), using features similar to those used for suppertagging for CCG parsing (Clark and Curran, 2004). In each case, features for a pruning tag consist of word and POS uni-grams extracted from the 5- word window with the current word in the middle, POS trigrams ending with the current word, as well as two previous tags as a bigram and two separate uni-grams. Our pruning labellers are highly efficient, run in linear time and add little overhead to decoding. During testing, in order to prevent overpruning, a probability cutoff value θ is used. A tag value of 0 is assigned to a word only if its margi</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A maximum entropy model for part-of-speech tagging. In Proc. EMNLP, volume 1, pages 133–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Classifying chart cells for quadratic complexity context-free inference.</title>
<date>2008</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>745--751</pages>
<contexts>
<context position="2464" citStr="Roark and Hollingshead, 2008" startWordPosition="359" endWordPosition="362">ses (i.e. any sequence of consecutive words) that are unlikely to have any consistently aligned target counterparts according to the source context and grammar constraints. We show that by using highly-efficient sequence labelling models learned from the bitext used for translation model training, such phrases can be effectively identified prior to MT decoding, and corresponding chart cells can be excluded for decoding without affecting translation quality. We call our method context-sensitive pruning (CSP); it can be viewed as a bilingual adaptation of similar methods in monolingual parsing (Roark and Hollingshead, 2008; Zhang et al., 2010) which improve parsing efficiency by “closing” chart cells using binary classifiers. Our contribution is that we demonstrate such methods can be applied to synchronous-grammar parsing by labelling the source-side alone. This is achieved through a novel training scheme where the labelling models are trained over the word-aligned bitext and gold-standard pruning labels are obtained by projecting target-side constituents to the source words. To our knowledge, this is the first work to apply this technique to MT decoding. The proposed method is easy to implement and effective </context>
</contexts>
<marker>Roark, Hollingshead, 2008</marker>
<rawString>Brian Roark and Kristy Hollingshead. 2008. Classifying chart cells for quadratic complexity context-free inference. In Proc. COLING, pages 745–751.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Roark</author>
<author>Kristy Hollingshead</author>
</authors>
<title>Linear complexity context-free parsing pipelines via chart constraints.</title>
<date>2009</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>647--655</pages>
<marker>Roark, Hollingshead, 2009</marker>
<rawString>Brian Roark and Kristy Hollingshead. 2009. Linear complexity context-free parsing pipelines via chart constraints. In Proc. NAACL, pages 647–655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Srilm-an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP,</booktitle>
<volume>2</volume>
<pages>901--904</pages>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke et al. 2002. Srilm-an extensible language modeling toolkit. In Proc. ICSLP, volume 2, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Daniel Gildea</author>
</authors>
<title>Efficient multipass decoding for synchronous context free grammars.</title>
<date>2008</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="1283" citStr="Zhang and Gildea, 2008" startWordPosition="176" endWordPosition="179"> is easy to implement, orthogonal to cube pruning and additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU. 1 Introduction Syntactic MT models suffer from decoding efficiency bottlenecks introduced by online n-gram language model integration and high grammar complexity. Various efforts have been devoted to improving decoding efficiency, including hypergraph rescoring (Heafield et al., 2013; Huang and Chiang, 2007), coarse-to-fine processing (Petrov et al., 2008; Zhang and Gildea, 2008) and grammar transformations (Zhang et al., 2006). For more expressive, linguistically-motivated syntactic MT models (Galley et al., 2004; Galley et al., 2006), the grammar complexity has grown considerably over hierarchical phrase-based models (Chiang, 2007), and decoding still suffers from efficiency issues (DeNero et al., 2009). In this paper, we study a chart pruning method for CKY-style MT decoding that is orthogonal to cube pruning (Chiang, 2007) and additive to its pruning power. The main intuition of our method is to find those source phrases (i.e. any sequence of consecutive words) th</context>
</contexts>
<marker>Zhang, Gildea, 2008</marker>
<rawString>Hao Zhang and Daniel Gildea. 2008. Efficient multipass decoding for synchronous context free grammars. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Liang Huang</author>
<author>Daniel Gildea</author>
<author>Kevin Knight</author>
</authors>
<title>Synchronous binarization for machine translation.</title>
<date>2006</date>
<booktitle>In Proc. NAACL,</booktitle>
<pages>256--263</pages>
<contexts>
<context position="1332" citStr="Zhang et al., 2006" startWordPosition="184" endWordPosition="187"> additive to its pruning power. On a full-scale Englishto-German experiment with a string-totree model, we obtain a speed-up of more than 60% over a strong baseline, with no loss in BLEU. 1 Introduction Syntactic MT models suffer from decoding efficiency bottlenecks introduced by online n-gram language model integration and high grammar complexity. Various efforts have been devoted to improving decoding efficiency, including hypergraph rescoring (Heafield et al., 2013; Huang and Chiang, 2007), coarse-to-fine processing (Petrov et al., 2008; Zhang and Gildea, 2008) and grammar transformations (Zhang et al., 2006). For more expressive, linguistically-motivated syntactic MT models (Galley et al., 2004; Galley et al., 2006), the grammar complexity has grown considerably over hierarchical phrase-based models (Chiang, 2007), and decoding still suffers from efficiency issues (DeNero et al., 2009). In this paper, we study a chart pruning method for CKY-style MT decoding that is orthogonal to cube pruning (Chiang, 2007) and additive to its pruning power. The main intuition of our method is to find those source phrases (i.e. any sequence of consecutive words) that are unlikely to have any consistently aligned </context>
</contexts>
<marker>Zhang, Huang, Gildea, Knight, 2006</marker>
<rawString>Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. 2006. Synchronous binarization for machine translation. In Proc. NAACL, pages 256–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>B G Ahn</author>
<author>S Clark</author>
<author>C Van Wyk</author>
<author>J R Curran</author>
<author>L Rimell</author>
</authors>
<title>Chart pruning for fast lexicalised-grammar parsing.</title>
<date>2010</date>
<booktitle>In Proc. COLING,</booktitle>
<pages>1471--1479</pages>
<marker>Zhang, Ahn, Clark, Van Wyk, Curran, Rimell, 2010</marker>
<rawString>Y. Zhang, B.G. Ahn, S. Clark, C. Van Wyk, J.R. Curran, and L. Rimell. 2010. Chart pruning for fast lexicalised-grammar parsing. In Proc. COLING, pages 1471–1479.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>