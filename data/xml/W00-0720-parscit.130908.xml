<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024017">
<note confidence="0.816178">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 103-106, Lisbon, Portugal, 2000.
</note>
<title confidence="0.999276">
Genetic Algorithms for Feature Relevance Assignment in
Memory-Based Language Processing
</title>
<author confidence="0.98523">
Anne Kool and Walter Daelemans and Jakub Zavrel*
</author>
<affiliation confidence="0.8709365">
CNTS - Language Technology Group
University of Antwerp, UIA, Universiteitsplein 1, 2610 Antwerpen, Belgium
</affiliation>
<email confidence="0.534252">
fkool, daelem, zavrellOuia.ua.ac.be
</email>
<sectionHeader confidence="0.994789" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999826315789474">
We investigate the usefulness of evolutionary al-
gorithms in three incarnations of the problem of
feature relevance assignment in memory-based
language processing (MBLP): feature weight-
ing, feature ordering and feature selection. We
use a simple genetic algorithm (GA) for this
problem on two typical tasks in natural lan-
guage processing: morphological synthesis and
unknown word tagging. We find that GA fea-
ture selection always significantly outperforms
the MBLP variant without selection and that
feature ordering and weighting with GA signifi-
cantly outperforms a situation where no weight-
ing is used. However, GA selection does not sig-
nificantly do better than simple iterative feature
selection methods, and GA weighting and order-
ing reach only similar performance as current
information-theoretic feature weighting meth-
ods.
</bodyText>
<sectionHeader confidence="0.9704855" genericHeader="keywords">
1 Memory-Based Language
Processing
</sectionHeader>
<bodyText confidence="0.98084645">
Memory-Based Language Processing (Daele-
mans, van den Bosch, and Zavrel, 1999) is
based on the idea that language acquisition
should be seen as the incremental storage of
exemplars of specific tasks, and language pro-
cessing as analogical reasoning on the basis of
these stored exemplars. These exemplars take
the form of a vector of, typically, nominal fea-
tures, describing a linguistic problem and its
context, and an associated class symbol repre-
senting the solution to the problem. A new in-
stance is categorized on the basis of its similar-
ity with a memory instance and its associated
* Research funded by CELE, S.AI.L Trust V.Z.W.,
leper, Belgium.
class.
The basic algorithm we use to calculate the dis-
tance between two items is a variant of IB1
(Aha, Kibler, and Albert, 1991). ml does
not solve the problem of modeling the differ-
ence in relevance between the various sources
of information. In an MBLP approach, this
can be overcome by means of feature weighting.
The Isl-IG algorithm uses information gain to
weight the cost of a feature value mismatch dur-
ing comparison. IGTREE is a variant in which an
oblivious decision tree is created with features
as tests, and in which tests are ordered accord-
ing to information gain of the associated fea-
tures. In this case, the accuracy of the trained
system is very much dependent on a good fea-
ture ordering. For all variants of MBLP dis-
cussed here, feature selection can also improve
both accuracy and efficiency by discarding some
features altogether because of their irrelevance
or even counter-productivity in learning to solve
the task. In our experiments we will use a rel-
evance assignment method that radically dif-
fers from information-theoretic measures: ge-
netic algorithms.
</bodyText>
<sectionHeader confidence="0.9971055" genericHeader="method">
2 Genetic Algorithms for Assigning
Relevance
</sectionHeader>
<bodyText confidence="0.999637166666667">
In the experiments, we linked our memory-
based learner TIMBL1 to PGAPACK2. During the
weighting experiments a gene corresponds to a
specific real-valued feature-weight (we will indi-
cate this by including GA in the algorithm name,
i.e. IB1-GA and GATREE, cf. ml-IG and IGTREE).
</bodyText>
<footnote confidence="0.991461666666667">
1TIMBL is available from http://ilk.kub.n1/ and the
algorithms are described in more detail in (Daelemans
et al., 1999).
2A software environment for evolutionary computa-
tion developed by D. Levine, Argonne National Labora-
tory, available from ftp://ftp.mcs.anl.gov/pub/pgapack/
</footnote>
<page confidence="0.999254">
103
</page>
<bodyText confidence="0.9999923125">
In the case of selection the string is composed
of binary values, indicating presence or absence
of a feature (we will call this GASEL). The fit-
ness of the strings is determined by running the
memory-based learner with each string on a val-
idation set, and returning the resulting accuracy
as a fitness value for that string. Hence, both
weighting and selection with the GA is an in-
stance of a wrapper approach as opposed to a
filter approach such as information gain (Ko-
havi and John, 1995).
For comparison, we include two popular
classical wrapper methods: backward elimina-
tion selection (BASEL) and forward selection
(FosEL). Forward selection starts from an
empty set of features and backward selection
begins with a full set of features. At each fur-
ther addition (or deletion, for BASEL) the fea-
ture with the highest accuracy increase (resp.
lowest accuracy decrease) is selected, until im-
provement stalls (resp. performance drops).
During the morphology experiment the pop-
ulation size was 50, but for prediction of un-
known words it was set to 16 because the larger
dataset was computationally more demanding.
The populations were evolved for a maximum
of 200 generations or stopped when no change
had occurred for over 50 generations. Parame-
ter settings for the genetic algorithm were kept
constant: a two-point crossover probability of
0.85, a mutation rate of 0.006, an elitist replace-
ment strategy, and tournament selection.
</bodyText>
<subsectionHeader confidence="0.981941">
2.1 Data
</subsectionHeader>
<bodyText confidence="0.999994714285714">
The first task3 we consider is prediction of what
diminutive suffix a Dutch noun should take on
the basis of its form. There are five different
possible suffix forms (the classes). There are 12
features which contain information (stress and
segmental information) about the structure of
the last three syllables of a noun. The data set
contains 3949 such instances.
The second data set4 is larger and contains
65275 instances, the task we consider here is
part-of-speech (morpho-syntactic category) tag-
ging of unknown words. The features used here
are the coded POS-tags of two words before and
two words after the focus word to be tagged, the
</bodyText>
<footnote confidence="0.95222625">
3Data from the CELEX lexical data base, available
on CD-ROM from the LDC, http://ldc.upenn.edu.
4This dataset is based on the TOSCA tagged LOB
corpus of English.
</footnote>
<bodyText confidence="0.99967275">
last three letters of the focus word, and informa-
tion on hyphenation and capitalisation. There
are 111 possible classes (part of speech tags) to
predict.
</bodyText>
<subsectionHeader confidence="0.99911">
2.2 Method
</subsectionHeader>
<bodyText confidence="0.999531285714286">
We have used 10-fold-cross-validation in all ex-
periments. Because the wrapper methods get
their evaluation feedback directly from accuracy
measurements on the data, we further split the
trainfile for each fold into 2/3 sub-trainset and
a 1/3 validation set. The settings obtained by
this are then tested on the test set of that fold.
</bodyText>
<subsectionHeader confidence="0.646345">
2.3 Results
</subsectionHeader>
<bodyText confidence="0.999993222222222">
In Table 1 we show the results of our exper-
iments (average accuracy and standard devia-
tion over ten folds). We can see that applying
any feature selection scheme when no weights
are used (B31) significantly improves classifi-
cation performance (p&lt;0.01)5. Selection also
improves accuracy when using the B31-IG or
IGTREE algorithm. These differences are sig-
nificant on the morphology dataset (p&lt;0.05),
but for the unknown words dataset only the dif-
ference between (B31) and (iB1-1-, GAsEL) is sig-
nificant (p&lt;0.01). In both cases, however, the
results in Table 1 do not reveal significant dif-
ferences between evolutionary, backward or for-
ward selection.
With respect to feature weighting by means
of a GA the results are much less clear: for the
morphology data, the GA-weights significantly
improve upon B31, refered to as IB1-GA in the
table, (p&lt;0.01) but not IGTREE (GATREE in the
table). For the other dataset GA-weights do not
even improve upon B31. But in general, those
weights found by the genetic algorithm lead to
comparable classification accuracy as with gain
ratio based weighting. The same applies to the
combination of GA-weights with further selec-
tion of irrelevant features (GATREE+GASEL).
</bodyText>
<subsectionHeader confidence="0.99282">
2.4 The Effect of GA Parameters
</subsectionHeader>
<bodyText confidence="0.997204166666667">
We also wanted to test whether the GA would
benefit from optimisation in the crossover and
mutation probabilities. To this end, we used
the morphology dataset, which was split into an
80% trainfile, a 10% validationfile and a held-
out 10% testfile. The mutation rate was var-
</bodyText>
<footnote confidence="0.9946895">
3A11 significance tests in this paper are one-tailed
paired t-tests.
</footnote>
<page confidence="0.965255">
104
</page>
<table confidence="0.999969647058824">
Classifier Morphology Unknown Words
ii31 87.2 (± 1.6) 81.7 (± 0.5)
IBl-FGAsEL 96.5 (± 1.0) 82.8 (± 0.6)
IB1-1-FOSEL 96.6 (± 1.1) 82.9 (± 0.2)
1B 1 ±BAsEL 96.6 (± 1.1) 82.9 (± 0.2)
IB1-IG 96.2 (± 0.8) 82.8 (± 0.3)
IB1-IG+GASEL 97.3 (± 0.9) 83.0 (± 0.3)
IB1-IG-FFOSEL 97.1 (± 0.9) 82.8 (± 0.3)
IB1-IG-FBASEL 97.3 (± 1.0) 82.9 (± 0.3)
IGTREE 96.2 (±0.8) 81.4 (± 0.4)
IGTREE-FGASEL 97.1 (± 0.9) 81.4 (± 0.4)
IGTREE+FOSEL 97.0 (± 0.9) 81.3 (± 0.4)
IGTREE-FBASEL 97.0 (± 1.1) 81.3 (± 0.4)
IB1-GA 95.6 (± 1.0) 81.6 (1 0.8)
IB1-GA+GASEL 97.0 (± 1.1) 82.0 (± 1.2)
GATREE 96.0 (± 1.0) 80.4 (± 1.2)
GATREE±GAsEL 97.1 (± 1.0) 81.0 (± 0.6)
</table>
<tableCaption confidence="0.907821">
Table 1: Accuracy (± standard deviation) results of
the experiments. Boldface marks the best results for
each basic algorithm per data set.
</tableCaption>
<bodyText confidence="0.999986619047619">
ied stepwise adding a value of 0.001 at each ex-
periment, starting at a 0.004 value up to 0.01.
The different values for crossover ranged from
0.65 to 0.95, in steps of 0.05. The effect of
changing crossover and mutation probabilities
was tested for IB1-IG+GA-selection, for ml with
GA weighting, for IGTREE-FGA-selection, and for
IGTREE with GA-weight settings.
These experiments show considerable fluctua-
tion in accuracy within the tested range, but dif-
ferent parameter settings could also yield same
results although they were far apart in value.
Some settings achieved a particularly high accu-
racy in this training regime (e.g. crossover: 0.75,
mutation: 0.009). However, when we used these
in the ten-fold cv setup of our main experi-
ments, this gave a mean score of 97.4 (± 0.9)
for iBl-IG with GA-selection and a mean score
of 97.1 (± 1.1) for IGTREE with GA-selection.
These accuracies are similar to those achieved
with our default parameter settings.
</bodyText>
<subsectionHeader confidence="0.587103">
2.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999971545454545">
Feature selection on the morphology task shows
a significant increase in performance accuracy,
whereas on the unknown words task the differ-
ences are less outspoken. To get some insight
into this phenomenon, we looked at the average
probabilities of the features that were left out
by the evolutionary algorithm and their aver-
age weights.
On the morphology task this reveals that nu-
cleus and coda of the last syllable are highly
relevant, they are always included. The onset
of all three syllables is always left out. Further,
in all partitions the nucleus and coda of the sec-
ond syllable are left out. 6 For part-of-speech
tagging of unknown words all features appear
to be more or less equally relevant. Over the
ten partitions, either no omission is suggested
at all, or the features that carry the pos-tag of
n-2 word before and the n+2 word after the fo-
cus word are deleted. This is comparable to re-
ducing the context window of this classification
task to one word before and one after the focus.
The fact that all features seem to contribute
to the classification when doing POS-tagging
(making selection irrelevant) could also explain
why the IGTREE algorithm seems to benefit less
from the feature orders suggested and why the
non-weighted approach ml already has a high
score on the tagging task. The IGTREE algo-
rithm is more suited for problems where the fea-
tures can be ordered in a straightforward way
because they have significantly different rele-
vance.
</bodyText>
<sectionHeader confidence="0.982441" genericHeader="conclusions">
3 Conclusions and Related Research
</sectionHeader>
<bodyText confidence="0.943000772727273">
The issue of feature-relevance assignment is
well-documented in the machine learning lit-
erature. Excellent comparative surveys are
(Wettschereck, Aha, and Mohri, 1997) and
(Wettschereck and Aha, 1995) or (Blum and
Langley, 1997). Feature subset selection
by means of evolutionary algorithms was in-
vestigated by Skalak (1994), Vafaie and de
Jong (1992), and Yang and Honavar (1997).
Other work deals with evolutionary approaches
for continuous feature weight assignment such
as Wilson and Martinez (1996), or Punch and
Goodman (1993).
The conclusions from these papers are in
agreement with our findings on the natural lan-
guage data, suggesting that feature selection
and weighting with GA&apos;s significantly outper-
form non-weighted approaches. Feature selec-
tion generally improves accuracy with a reduc-
°This fits in with current theory about this morpho-
logical process (e.g. Trommelen (1983), Daelemans et
al. (1997)).
</bodyText>
<page confidence="0.998133">
105
</page>
<bodyText confidence="0.999986384615385">
tion in the number of features used. However,
we have found no results (on these particular
data) that indicate an advantage of evolutionary
feature selection approach over the more classi-
cal iterative methods. Our experiments further
show that there is no evidence that GA weight-
ing is in general competitive with simple filter
methods such as gain ratio. Possibly, a parame-
ter setting for the GA could be found that gives
better results, but searching for such an optimal
parameter setting is at present computationally
unfeasible for typical natural language process-
ing problems.
</bodyText>
<sectionHeader confidence="0.997779" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999196">
Aha, D., D. Kibler, and M. Albert. 1991. Instance-
based learning algorithms. In Machine Learning
Vol. 6, pp 37-66.
Blum, A. and P. Langley. 1997. Selection of rele-
vant features and examples in machine learning.
In Machine Learning: Artificial Intelligence,97,
PP 245-271.
Daelemans, W., P. Berck, and S. Gillis. 1997. Data
mining as a method for linguistic analysis: Dutch
diminutives. In Folia Linguistica , XXXI/1-2, pp
57-75.
Daelemans, W., A. van den Bosch, and J. Zavrel.
1999. Forgetting exceptions is harmful in lan-
guage learning. In Machine Learning, special is-
sue on natural language learning, 34 , pp 11-43-
Daelemans, W., J. Zavrel, K. van der Sloot, and
A. van den Bosch. 1999. Timbl: Tilburg mem-
ory based learner, version 2.0, reference guide. Ilk
technical report 99-01, ILK.
John, G.H., R. Kohavi, and K. Pfleger. 1994. Irrel-
evant features and the subset selection problem.
In Machine Learning: Proceedings of the Eleventh
International Conference, pp 121-129.
Kohavi, R. and G.H. John. 1995. Wrappers for
feature subset selection. In Artificial Intelligence
Journal, Special Issue on Relevance Vol.97, pp
273-324.
Punch, W. F., E.D. Goodman, Lai Chia-Shun
MM Pei, P. Hovland, and R. Enbody. 1993. Fur-
ther research on feature selection and classifica-
tion using genetic algorithms. In Proceedings of
the Fifth International Conference on Genetic Al-
gorithms, pp 557.
Quinlan, J.R. 1993. C4.5: Programs for Machine
Learning. San Mateo: Morgan Kaufmann.
Skalak, D. 1993. Using a genetic algorithm to learn
prototypes for case retrieval and classification.
In Case-Based Reasoning: Papers from the 1993
Workshop, Tech. Report WS-93-01, pp 211-215.
AAAI Press.
Skalak, D. B. 1994. Prototype and feature selec-
tion by sampling and random mutation hill climb-
ing algorithms. In Proceedings of the eleventh In-
ternational Conference on Machine Learning, pp
293-301.
Trommelen, M.T.G. 1983. The Syllable in Dutch,
with special Reference to Diminutive Formation.
Foris: Dordrecht.
Vafaie, H. and K. de Jong. 1992. Genetic algorithms
as a tool for feature selection in machine learn-
ing. In Machine Learning, Proceeding of the 4th
International Conference on Tools with Artificial
Intelligence, pp 200-204.
Wettschereck, D. and D. Aha. 1995. Weighting fea-
tures. In Proceedings of the First International
Conference on Case-Based Reasoning, ICCBR-
95, pp 347-358.
Wettschereck, D., D. Aha, and T. Mohri. 1997. A
review and empirical evaluation of feature weight-
ing methods for a class of lazy learning algorithms.
In Artificial Intelligence Review Vol.11, pp 273-
314.
Wilson, D. and T. Martinez. 1996. Instance-
based learning with genetically derived attribute
weights. In Proceedings of the International Con-
ference on Artificial Intelligence, Expert Systems,
and Neural Networks, pp 11-14.
Yang, J. and V. Honavar. 1997. Feature subset se-
lection using a genetic algorithm. In Genetic Pro-
gramming 1997: Proceedings of the Second An-
nual Conference, pp 380.
</reference>
<page confidence="0.997322">
106
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.001423">
<note confidence="0.959969">of CoNLL-2000 and LLL-2000, 103-106, Lisbon, Portugal, 2000.</note>
<title confidence="0.9979835">Genetic Algorithms for Feature Relevance Assignment Memory-Based Language Processing</title>
<author confidence="0.956679">Anne Kool</author>
<author confidence="0.956679">Walter Daelemans</author>
<author confidence="0.956679">Jakub</author>
<affiliation confidence="0.919265">CNTS - Language Technology University of Antwerp, UIA, Universiteitsplein 1, 2610 Antwerpen,</affiliation>
<email confidence="0.99222">zavrellOuia.ua.ac.be</email>
<abstract confidence="0.992436927586207">We investigate the usefulness of evolutionary algorithms in three incarnations of the problem of feature relevance assignment in memory-based language processing (MBLP): feature weighting, feature ordering and feature selection. We use a simple genetic algorithm (GA) for this problem on two typical tasks in natural language processing: morphological synthesis and word tagging. We find that feature selection always significantly outperforms the MBLP variant without selection and that ordering and weighting with significantly outperforms a situation where no weightis used. However, does not significantly do better than simple iterative feature methods, and and ordering reach only similar performance as current information-theoretic feature weighting methods. 1 Memory-Based Language Processing Memory-Based Language Processing (Daelemans, van den Bosch, and Zavrel, 1999) is based on the idea that language acquisition should be seen as the incremental storage of exemplars of specific tasks, and language processing as analogical reasoning on the basis of these stored exemplars. These exemplars take the form of a vector of, typically, nominal features, describing a linguistic problem and its context, and an associated class symbol representing the solution to the problem. A new instance is categorized on the basis of its similarity with a memory instance and its associated * Research funded by CELE, S.AI.L Trust V.Z.W., leper, Belgium. class. The basic algorithm we use to calculate the distance between two items is a variant of IB1 (Aha, Kibler, and Albert, 1991). ml does not solve the problem of modeling the difference in relevance between the various sources of information. In an MBLP approach, this be overcome by means of weighting. The Isl-IG algorithm uses information gain to weight the cost of a feature value mismatch durcomparison. a variant in which an oblivious decision tree is created with features as tests, and in which tests are ordered according to information gain of the associated features. In this case, the accuracy of the trained is very much dependent on a good feaordering. all variants of MBLP dishere, selection also improve both accuracy and efficiency by discarding some features altogether because of their irrelevance or even counter-productivity in learning to solve the task. In our experiments we will use a relevance assignment method that radically differs from information-theoretic measures: genetic algorithms. 2 Genetic Algorithms for Assigning Relevance In the experiments, we linked our memorylearner to the weighting experiments a gene corresponds to a specific real-valued feature-weight (we will indithis by including the algorithm name, ml-IG and available from http://ilk.kub.n1/ and the algorithms are described in more detail in (Daelemans et al., 1999). software environment for evolutionary computation developed by D. Levine, Argonne National Laboratory, available from ftp://ftp.mcs.anl.gov/pub/pgapack/ 103 In the case of selection the string is composed of binary values, indicating presence or absence a feature (we will call this fitness of the strings is determined by running the memory-based learner with each string on a validation set, and returning the resulting accuracy as a fitness value for that string. Hence, both and selection with the an inof a as opposed to a such as information gain (Kohavi and John, 1995). For comparison, we include two popular classical wrapper methods: backward elimination selection (BASEL) and forward selection (FosEL). Forward selection starts from an empty set of features and backward selection begins with a full set of features. At each furaddition (or deletion, for feature with the highest accuracy increase (resp. lowest accuracy decrease) is selected, until improvement stalls (resp. performance drops). During the morphology experiment the population size was 50, but for prediction of unknown words it was set to 16 because the larger dataset was computationally more demanding. The populations were evolved for a maximum of 200 generations or stopped when no change had occurred for over 50 generations. Parameter settings for the genetic algorithm were kept constant: a two-point crossover probability of 0.85, a mutation rate of 0.006, an elitist replacement strategy, and tournament selection. 2.1 Data first we consider is prediction of what diminutive suffix a Dutch noun should take on the basis of its form. There are five different possible suffix forms (the classes). There are 12 features which contain information (stress and segmental information) about the structure of the last three syllables of a noun. The data set contains 3949 such instances. second data is larger and contains 65275 instances, the task we consider here is part-of-speech (morpho-syntactic category) tagging of unknown words. The features used here the coded two words before and two words after the focus word to be tagged, the from the CELEX lexical data base, available CD-ROM from the LDC, is based on the TOSCA tagged LOB corpus of English. last three letters of the focus word, and information on hyphenation and capitalisation. There are 111 possible classes (part of speech tags) to predict. 2.2 Method We have used 10-fold-cross-validation in all experiments. Because the wrapper methods get their evaluation feedback directly from accuracy measurements on the data, we further split the trainfile for each fold into 2/3 sub-trainset and a 1/3 validation set. The settings obtained by this are then tested on the test set of that fold. 2.3 Results 1 we show the results of our experiments (average accuracy and standard deviation over ten folds). We can see that applying any feature selection scheme when no weights are used (B31) significantly improves classifiperformance Selection also improves accuracy when using the B31-IG or These differences are significant on the morphology dataset (p&lt;0.05), but for the unknown words dataset only the difference between (B31) and (iB1-1-, GAsEL) is significant (p&lt;0.01). In both cases, however, the results in Table 1 do not reveal significant differences between evolutionary, backward or forward selection. respect to feature weighting by a results are much less clear: for the morphology data, the GA-weights significantly upon B31, refered to as the (p&lt;0.01) but not (GATREE the table). For the other dataset GA-weights do not even improve upon B31. But in general, those weights found by the genetic algorithm lead to comparable classification accuracy as with gain ratio based weighting. The same applies to the combination of GA-weights with further selecof irrelevant features 2.4 The Effect of GA Parameters also wanted to test whether the benefit from optimisation in the crossover and mutation probabilities. To this end, we used the morphology dataset, which was split into an 80% trainfile, a 10% validationfile and a held- 10% testfile. The mutation rate was varsignificance tests in this paper are one-tailed paired t-tests. 104 Classifier Morphology Unknown Words ii31 87.2 (± 1.6) 81.7 (± 0.5) IBl-FGAsEL 96.5 (± 1.0) 82.8 (± 0.6) IB1-1-FOSEL 96.6 (± 1.1) 82.9 (± 0.2) 1B 1 ±BAsEL 96.6 (± 1.1) 82.9 (± 0.2) IB1-IG 96.2 (± 0.8) 82.8 (± 0.3) IB1-IG+GASEL 97.3 (± 0.9) 83.0 (± 0.3) IB1-IG-FFOSEL 97.1 (± 0.9) 82.8 (± 0.3) IB1-IG-FBASEL 97.3 (± 1.0) 82.9 (± 0.3) IGTREE 96.2 (±0.8) 81.4 (± 0.4) IGTREE-FGASEL 97.1 (± 0.9) 81.4 (± 0.4) IGTREE+FOSEL 97.0 (± 0.9) 81.3 (± 0.4) IGTREE-FBASEL 97.0 (± 1.1) 81.3 (± 0.4) IB1-GA 95.6 (± 1.0) 81.6 (1 0.8) IB1-GA+GASEL 97.0 (± 1.1) 82.0 (± 1.2) GATREE 96.0 (± 1.0) 80.4 (± 1.2) GATREE±GAsEL 97.1 (± 1.0) 81.0 (± 0.6) 1: (± standard deviation) results of the experiments. Boldface marks the best results for each basic algorithm per data set. ied stepwise adding a value of 0.001 at each experiment, starting at a 0.004 value up to 0.01. The different values for crossover ranged from 0.65 to 0.95, in steps of 0.05. The effect of changing crossover and mutation probabilities tested for ml with for for GA-weight settings. These experiments show considerable fluctuation in accuracy within the tested range, but different parameter settings could also yield same results although they were far apart in value. Some settings achieved a particularly high accuracy in this training regime (e.g. crossover: 0.75, mutation: 0.009). However, when we used these in the ten-fold cv setup of our main experiments, this gave a mean score of 97.4 (± 0.9) for iBl-IG with GA-selection and a mean score 97.1 (± 1.1) for GA-selection. These accuracies are similar to those achieved with our default parameter settings. 2.5 Discussion Feature selection on the morphology task shows a significant increase in performance accuracy, whereas on the unknown words task the differences are less outspoken. To get some insight into this phenomenon, we looked at the average probabilities of the features that were left out by the evolutionary algorithm and their average weights. On the morphology task this reveals that nucleus and coda of the last syllable are highly relevant, they are always included. The onset of all three syllables is always left out. Further, all partitions the nucleus and coda of the secare left out. 6For part-of-speech tagging of unknown words all features appear to be more or less equally relevant. Over the ten partitions, either no omission is suggested at all, or the features that carry the pos-tag of n-2 word before and the n+2 word after the focus word are deleted. This is comparable to reducing the context window of this classification task to one word before and one after the focus. The fact that all features seem to contribute to the classification when doing POS-tagging (making selection irrelevant) could also explain the seems to benefit from the feature orders suggested and why the non-weighted approach ml already has a high on the tagging task. The algorithm is more suited for problems where the features can be ordered in a straightforward way because they have significantly different relevance. 3 Conclusions and Related Research The issue of feature-relevance assignment is well-documented in the machine learning literature. Excellent comparative surveys are (Wettschereck, Aha, and Mohri, 1997) and (Wettschereck and Aha, 1995) or (Blum and Langley, 1997). Feature subset by means of evolutionary algorithms was investigated by Skalak (1994), Vafaie and de Jong (1992), and Yang and Honavar (1997). Other work deals with evolutionary approaches for continuous feature weight assignment such as Wilson and Martinez (1996), or Punch and Goodman (1993). The conclusions from these papers are in agreement with our findings on the natural language data, suggesting that feature selection and weighting with GA&apos;s significantly outperform non-weighted approaches. Feature selecgenerally improves accuracy with a reducfits in with current theory about this morphological process (e.g. Trommelen (1983), Daelemans et al. (1997)). 105 tion in the number of features used. However, we have found no results (on these particular data) that indicate an advantage of evolutionary feature selection approach over the more classical iterative methods. Our experiments further show that there is no evidence that GA weightis in with simple filter methods such as gain ratio. Possibly, a parameter setting for the GA could be found that gives better results, but searching for such an optimal parameter setting is at present computationally unfeasible for typical natural language processing problems.</abstract>
<note confidence="0.829458095238095">References Aha, D., D. Kibler, and M. Albert. 1991. Instancelearning algorithms. In Learning 6, pp Blum, A. and P. Langley. 1997. Selection of relevant features and examples in machine learning. Learning: Artificial Intelligence,97, PP 245-271. Daelemans, W., P. Berck, and S. Gillis. 1997. Data mining as a method for linguistic analysis: Dutch Folia Linguistica , XXXI/1-2, pp 57-75. Daelemans, W., A. van den Bosch, and J. Zavrel. 1999. Forgetting exceptions is harmful in lanlearning. In Learning, special ison natural language learning, 34 , Daelemans, W., J. Zavrel, K. van der Sloot, and A. van den Bosch. 1999. Timbl: Tilburg memory based learner, version 2.0, reference guide. Ilk technical report 99-01, ILK. John, G.H., R. Kohavi, and K. Pfleger. 1994. Irrelevant features and the subset selection problem. Learning: Proceedings of the Eleventh International Conference, pp 121-129. Kohavi, R. and G.H. John. 1995. Wrappers for subset selection. In Intelligence Journal, Special Issue on Relevance Vol.97, pp 273-324. Punch, W. F., E.D. Goodman, Lai Chia-Shun MM Pei, P. Hovland, and R. Enbody. 1993. Further research on feature selection and classificausing genetic algorithms. In of the Fifth International Conference on Genetic Algorithms, pp 557. J.R. 1993. Programs for Machine Mateo: Morgan Kaufmann. Skalak, D. 1993. Using a genetic algorithm to learn prototypes for case retrieval and classification. Reasoning: Papers from the 1993 Workshop, Tech. Report WS-93-01, pp 211-215. AAAI Press. Skalak, D. B. 1994. Prototype and feature selec-</note>
<abstract confidence="0.849531566666667">tion by sampling and random mutation hill climbalgorithms. In of the eleventh International Conference on Machine Learning, pp 293-301. M.T.G. 1983. Syllable in Dutch, with special Reference to Diminutive Formation. Foris: Dordrecht. Vafaie, H. and K. de Jong. 1992. Genetic algorithms as a tool for feature selection in machine learn- In Learning, Proceeding of the 4th International Conference on Tools with Artificial Intelligence, pp 200-204. Wettschereck, D. and D. Aha. 1995. Weighting fea- In of the First International Conference on Case-Based Reasoning, ICCBR- 95, pp 347-358. Wettschereck, D., D. Aha, and T. Mohri. 1997. A review and empirical evaluation of feature weighting methods for a class of lazy learning algorithms. Intelligence Review Vol.11, pp 273- 314. Wilson, D. and T. Martinez. 1996. Instancebased learning with genetically derived attribute In of the International Conference on Artificial Intelligence, Expert Systems, and Neural Networks, pp 11-14. Yang, J. and V. Honavar. 1997. Feature subset seusing a genetic algorithm. In Programming 1997: Proceedings of the Second Annual Conference, pp 380.</abstract>
<intro confidence="0.535139">106</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Aha</author>
<author>D Kibler</author>
<author>M Albert</author>
</authors>
<title>Instancebased learning algorithms.</title>
<date>1991</date>
<journal>In Machine Learning</journal>
<volume>6</volume>
<pages>37--66</pages>
<contexts>
<context position="2015" citStr="Aha, Kibler, and Albert, 1991" startWordPosition="299" endWordPosition="303">remental storage of exemplars of specific tasks, and language processing as analogical reasoning on the basis of these stored exemplars. These exemplars take the form of a vector of, typically, nominal features, describing a linguistic problem and its context, and an associated class symbol representing the solution to the problem. A new instance is categorized on the basis of its similarity with a memory instance and its associated * Research funded by CELE, S.AI.L Trust V.Z.W., leper, Belgium. class. The basic algorithm we use to calculate the distance between two items is a variant of IB1 (Aha, Kibler, and Albert, 1991). ml does not solve the problem of modeling the difference in relevance between the various sources of information. In an MBLP approach, this can be overcome by means of feature weighting. The Isl-IG algorithm uses information gain to weight the cost of a feature value mismatch during comparison. IGTREE is a variant in which an oblivious decision tree is created with features as tests, and in which tests are ordered according to information gain of the associated features. In this case, the accuracy of the trained system is very much dependent on a good feature ordering. For all variants of M</context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>Aha, D., D. Kibler, and M. Albert. 1991. Instancebased learning algorithms. In Machine Learning Vol. 6, pp 37-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>P Langley</author>
</authors>
<title>Selection of relevant features and examples in machine learning.</title>
<date>1997</date>
<booktitle>In Machine Learning: Artificial Intelligence,97, PP</booktitle>
<pages>245--271</pages>
<contexts>
<context position="11381" citStr="Blum and Langley, 1997" startWordPosition="1836" endWordPosition="1839">ion irrelevant) could also explain why the IGTREE algorithm seems to benefit less from the feature orders suggested and why the non-weighted approach ml already has a high score on the tagging task. The IGTREE algorithm is more suited for problems where the features can be ordered in a straightforward way because they have significantly different relevance. 3 Conclusions and Related Research The issue of feature-relevance assignment is well-documented in the machine learning literature. Excellent comparative surveys are (Wettschereck, Aha, and Mohri, 1997) and (Wettschereck and Aha, 1995) or (Blum and Langley, 1997). Feature subset selection by means of evolutionary algorithms was investigated by Skalak (1994), Vafaie and de Jong (1992), and Yang and Honavar (1997). Other work deals with evolutionary approaches for continuous feature weight assignment such as Wilson and Martinez (1996), or Punch and Goodman (1993). The conclusions from these papers are in agreement with our findings on the natural language data, suggesting that feature selection and weighting with GA&apos;s significantly outperform non-weighted approaches. Feature selection generally improves accuracy with a reduc°This fits in with current th</context>
</contexts>
<marker>Blum, Langley, 1997</marker>
<rawString>Blum, A. and P. Langley. 1997. Selection of relevant features and examples in machine learning. In Machine Learning: Artificial Intelligence,97, PP 245-271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>Data mining as a method for linguistic analysis: Dutch diminutives.</title>
<date>1997</date>
<booktitle>In Folia Linguistica ,</booktitle>
<pages>1--2</pages>
<contexts>
<context position="12066" citStr="Daelemans et al. (1997)" startWordPosition="1939" endWordPosition="1942"> was investigated by Skalak (1994), Vafaie and de Jong (1992), and Yang and Honavar (1997). Other work deals with evolutionary approaches for continuous feature weight assignment such as Wilson and Martinez (1996), or Punch and Goodman (1993). The conclusions from these papers are in agreement with our findings on the natural language data, suggesting that feature selection and weighting with GA&apos;s significantly outperform non-weighted approaches. Feature selection generally improves accuracy with a reduc°This fits in with current theory about this morphological process (e.g. Trommelen (1983), Daelemans et al. (1997)). 105 tion in the number of features used. However, we have found no results (on these particular data) that indicate an advantage of evolutionary feature selection approach over the more classical iterative methods. Our experiments further show that there is no evidence that GA weighting is in general competitive with simple filter methods such as gain ratio. Possibly, a parameter setting for the GA could be found that gives better results, but searching for such an optimal parameter setting is at present computationally unfeasible for typical natural language processing problems. References</context>
</contexts>
<marker>Daelemans, Berck, Gillis, 1997</marker>
<rawString>Daelemans, W., P. Berck, and S. Gillis. 1997. Data mining as a method for linguistic analysis: Dutch diminutives. In Folia Linguistica , XXXI/1-2, pp 57-75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
<author>J Zavrel</author>
</authors>
<title>Forgetting exceptions is harmful in language learning.</title>
<date>1999</date>
<booktitle>In Machine Learning, special issue on natural language learning, 34 ,</booktitle>
<pages>11--43</pages>
<marker>Daelemans, van den Bosch, Zavrel, 1999</marker>
<rawString>Daelemans, W., A. van den Bosch, and J. Zavrel. 1999. Forgetting exceptions is harmful in language learning. In Machine Learning, special issue on natural language learning, 34 , pp 11-43-</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>K van der Sloot</author>
<author>A van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory based learner, version 2.0, reference guide. Ilk</title>
<date>1999</date>
<tech>technical report 99-01, ILK.</tech>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 1999</marker>
<rawString>Daelemans, W., J. Zavrel, K. van der Sloot, and A. van den Bosch. 1999. Timbl: Tilburg memory based learner, version 2.0, reference guide. Ilk technical report 99-01, ILK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G H John</author>
<author>R Kohavi</author>
<author>K Pfleger</author>
</authors>
<title>Irrelevant features and the subset selection problem.</title>
<date>1994</date>
<booktitle>In Machine Learning: Proceedings of the Eleventh International Conference,</booktitle>
<pages>121--129</pages>
<marker>John, Kohavi, Pfleger, 1994</marker>
<rawString>John, G.H., R. Kohavi, and K. Pfleger. 1994. Irrelevant features and the subset selection problem. In Machine Learning: Proceedings of the Eleventh International Conference, pp 121-129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kohavi</author>
<author>G H John</author>
</authors>
<title>Wrappers for feature subset selection.</title>
<date>1995</date>
<journal>In Artificial Intelligence Journal, Special Issue on Relevance Vol.97,</journal>
<pages>273--324</pages>
<contexts>
<context position="4060" citStr="Kohavi and John, 1995" startWordPosition="631" endWordPosition="635">evolutionary computation developed by D. Levine, Argonne National Laboratory, available from ftp://ftp.mcs.anl.gov/pub/pgapack/ 103 In the case of selection the string is composed of binary values, indicating presence or absence of a feature (we will call this GASEL). The fitness of the strings is determined by running the memory-based learner with each string on a validation set, and returning the resulting accuracy as a fitness value for that string. Hence, both weighting and selection with the GA is an instance of a wrapper approach as opposed to a filter approach such as information gain (Kohavi and John, 1995). For comparison, we include two popular classical wrapper methods: backward elimination selection (BASEL) and forward selection (FosEL). Forward selection starts from an empty set of features and backward selection begins with a full set of features. At each further addition (or deletion, for BASEL) the feature with the highest accuracy increase (resp. lowest accuracy decrease) is selected, until improvement stalls (resp. performance drops). During the morphology experiment the population size was 50, but for prediction of unknown words it was set to 16 because the larger dataset was computat</context>
</contexts>
<marker>Kohavi, John, 1995</marker>
<rawString>Kohavi, R. and G.H. John. 1995. Wrappers for feature subset selection. In Artificial Intelligence Journal, Special Issue on Relevance Vol.97, pp 273-324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W F Punch</author>
<author>E D Goodman</author>
<author>Lai Chia-Shun MM Pei</author>
<author>P Hovland</author>
<author>R Enbody</author>
</authors>
<title>Further research on feature selection and classification using genetic algorithms.</title>
<date>1993</date>
<booktitle>In Proceedings of the Fifth International Conference on Genetic Algorithms,</booktitle>
<pages>557</pages>
<marker>Punch, Goodman, Pei, Hovland, Enbody, 1993</marker>
<rawString>Punch, W. F., E.D. Goodman, Lai Chia-Shun MM Pei, P. Hovland, and R. Enbody. 1993. Further research on feature selection and classification using genetic algorithms. In Proceedings of the Fifth International Conference on Genetic Algorithms, pp 557.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo:</location>
<marker>Quinlan, 1993</marker>
<rawString>Quinlan, J.R. 1993. C4.5: Programs for Machine Learning. San Mateo: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Skalak</author>
</authors>
<title>Using a genetic algorithm to learn prototypes for case retrieval and classification.</title>
<date>1993</date>
<booktitle>In Case-Based Reasoning: Papers from the 1993 Workshop,</booktitle>
<tech>Tech. Report WS-93-01,</tech>
<pages>211--215</pages>
<publisher>AAAI Press.</publisher>
<marker>Skalak, 1993</marker>
<rawString>Skalak, D. 1993. Using a genetic algorithm to learn prototypes for case retrieval and classification. In Case-Based Reasoning: Papers from the 1993 Workshop, Tech. Report WS-93-01, pp 211-215. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D B Skalak</author>
</authors>
<title>Prototype and feature selection by sampling and random mutation hill climbing algorithms.</title>
<date>1994</date>
<booktitle>In Proceedings of the eleventh International Conference on Machine Learning,</booktitle>
<pages>293--301</pages>
<contexts>
<context position="11477" citStr="Skalak (1994)" startWordPosition="1852" endWordPosition="1853">suggested and why the non-weighted approach ml already has a high score on the tagging task. The IGTREE algorithm is more suited for problems where the features can be ordered in a straightforward way because they have significantly different relevance. 3 Conclusions and Related Research The issue of feature-relevance assignment is well-documented in the machine learning literature. Excellent comparative surveys are (Wettschereck, Aha, and Mohri, 1997) and (Wettschereck and Aha, 1995) or (Blum and Langley, 1997). Feature subset selection by means of evolutionary algorithms was investigated by Skalak (1994), Vafaie and de Jong (1992), and Yang and Honavar (1997). Other work deals with evolutionary approaches for continuous feature weight assignment such as Wilson and Martinez (1996), or Punch and Goodman (1993). The conclusions from these papers are in agreement with our findings on the natural language data, suggesting that feature selection and weighting with GA&apos;s significantly outperform non-weighted approaches. Feature selection generally improves accuracy with a reduc°This fits in with current theory about this morphological process (e.g. Trommelen (1983), Daelemans et al. (1997)). 105 tion</context>
</contexts>
<marker>Skalak, 1994</marker>
<rawString>Skalak, D. B. 1994. Prototype and feature selection by sampling and random mutation hill climbing algorithms. In Proceedings of the eleventh International Conference on Machine Learning, pp 293-301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T G Trommelen</author>
</authors>
<title>The Syllable in Dutch, with special Reference to Diminutive Formation.</title>
<date>1983</date>
<location>Foris: Dordrecht.</location>
<contexts>
<context position="12041" citStr="Trommelen (1983)" startWordPosition="1937" endWordPosition="1938">tionary algorithms was investigated by Skalak (1994), Vafaie and de Jong (1992), and Yang and Honavar (1997). Other work deals with evolutionary approaches for continuous feature weight assignment such as Wilson and Martinez (1996), or Punch and Goodman (1993). The conclusions from these papers are in agreement with our findings on the natural language data, suggesting that feature selection and weighting with GA&apos;s significantly outperform non-weighted approaches. Feature selection generally improves accuracy with a reduc°This fits in with current theory about this morphological process (e.g. Trommelen (1983), Daelemans et al. (1997)). 105 tion in the number of features used. However, we have found no results (on these particular data) that indicate an advantage of evolutionary feature selection approach over the more classical iterative methods. Our experiments further show that there is no evidence that GA weighting is in general competitive with simple filter methods such as gain ratio. Possibly, a parameter setting for the GA could be found that gives better results, but searching for such an optimal parameter setting is at present computationally unfeasible for typical natural language proces</context>
</contexts>
<marker>Trommelen, 1983</marker>
<rawString>Trommelen, M.T.G. 1983. The Syllable in Dutch, with special Reference to Diminutive Formation. Foris: Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Vafaie</author>
<author>K de Jong</author>
</authors>
<title>Genetic algorithms as a tool for feature selection in machine learning.</title>
<date>1992</date>
<booktitle>In Machine Learning, Proceeding of the 4th International Conference on Tools with Artificial Intelligence,</booktitle>
<pages>200--204</pages>
<marker>Vafaie, de Jong, 1992</marker>
<rawString>Vafaie, H. and K. de Jong. 1992. Genetic algorithms as a tool for feature selection in machine learning. In Machine Learning, Proceeding of the 4th International Conference on Tools with Artificial Intelligence, pp 200-204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wettschereck</author>
<author>D Aha</author>
</authors>
<title>Weighting features.</title>
<date>1995</date>
<booktitle>In Proceedings of the First International Conference on Case-Based Reasoning, ICCBR95,</booktitle>
<pages>347--358</pages>
<contexts>
<context position="11353" citStr="Wettschereck and Aha, 1995" startWordPosition="1831" endWordPosition="1834">doing POS-tagging (making selection irrelevant) could also explain why the IGTREE algorithm seems to benefit less from the feature orders suggested and why the non-weighted approach ml already has a high score on the tagging task. The IGTREE algorithm is more suited for problems where the features can be ordered in a straightforward way because they have significantly different relevance. 3 Conclusions and Related Research The issue of feature-relevance assignment is well-documented in the machine learning literature. Excellent comparative surveys are (Wettschereck, Aha, and Mohri, 1997) and (Wettschereck and Aha, 1995) or (Blum and Langley, 1997). Feature subset selection by means of evolutionary algorithms was investigated by Skalak (1994), Vafaie and de Jong (1992), and Yang and Honavar (1997). Other work deals with evolutionary approaches for continuous feature weight assignment such as Wilson and Martinez (1996), or Punch and Goodman (1993). The conclusions from these papers are in agreement with our findings on the natural language data, suggesting that feature selection and weighting with GA&apos;s significantly outperform non-weighted approaches. Feature selection generally improves accuracy with a reduc°</context>
</contexts>
<marker>Wettschereck, Aha, 1995</marker>
<rawString>Wettschereck, D. and D. Aha. 1995. Weighting features. In Proceedings of the First International Conference on Case-Based Reasoning, ICCBR95, pp 347-358.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wettschereck</author>
<author>D Aha</author>
<author>T Mohri</author>
</authors>
<title>A review and empirical evaluation of feature weighting methods for a class of lazy learning algorithms.</title>
<date>1997</date>
<booktitle>In Artificial Intelligence Review Vol.11,</booktitle>
<pages>273--314</pages>
<contexts>
<context position="11319" citStr="Wettschereck, Aha, and Mohri, 1997" startWordPosition="1825" endWordPosition="1829">to contribute to the classification when doing POS-tagging (making selection irrelevant) could also explain why the IGTREE algorithm seems to benefit less from the feature orders suggested and why the non-weighted approach ml already has a high score on the tagging task. The IGTREE algorithm is more suited for problems where the features can be ordered in a straightforward way because they have significantly different relevance. 3 Conclusions and Related Research The issue of feature-relevance assignment is well-documented in the machine learning literature. Excellent comparative surveys are (Wettschereck, Aha, and Mohri, 1997) and (Wettschereck and Aha, 1995) or (Blum and Langley, 1997). Feature subset selection by means of evolutionary algorithms was investigated by Skalak (1994), Vafaie and de Jong (1992), and Yang and Honavar (1997). Other work deals with evolutionary approaches for continuous feature weight assignment such as Wilson and Martinez (1996), or Punch and Goodman (1993). The conclusions from these papers are in agreement with our findings on the natural language data, suggesting that feature selection and weighting with GA&apos;s significantly outperform non-weighted approaches. Feature selection general</context>
</contexts>
<marker>Wettschereck, Aha, Mohri, 1997</marker>
<rawString>Wettschereck, D., D. Aha, and T. Mohri. 1997. A review and empirical evaluation of feature weighting methods for a class of lazy learning algorithms. In Artificial Intelligence Review Vol.11, pp 273-314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wilson</author>
<author>T Martinez</author>
</authors>
<title>Instancebased learning with genetically derived attribute weights.</title>
<date>1996</date>
<booktitle>In Proceedings of the International Conference on Artificial Intelligence, Expert Systems, and Neural Networks,</booktitle>
<pages>11--14</pages>
<contexts>
<context position="11656" citStr="Wilson and Martinez (1996)" startWordPosition="1877" endWordPosition="1880">e ordered in a straightforward way because they have significantly different relevance. 3 Conclusions and Related Research The issue of feature-relevance assignment is well-documented in the machine learning literature. Excellent comparative surveys are (Wettschereck, Aha, and Mohri, 1997) and (Wettschereck and Aha, 1995) or (Blum and Langley, 1997). Feature subset selection by means of evolutionary algorithms was investigated by Skalak (1994), Vafaie and de Jong (1992), and Yang and Honavar (1997). Other work deals with evolutionary approaches for continuous feature weight assignment such as Wilson and Martinez (1996), or Punch and Goodman (1993). The conclusions from these papers are in agreement with our findings on the natural language data, suggesting that feature selection and weighting with GA&apos;s significantly outperform non-weighted approaches. Feature selection generally improves accuracy with a reduc°This fits in with current theory about this morphological process (e.g. Trommelen (1983), Daelemans et al. (1997)). 105 tion in the number of features used. However, we have found no results (on these particular data) that indicate an advantage of evolutionary feature selection approach over the more c</context>
</contexts>
<marker>Wilson, Martinez, 1996</marker>
<rawString>Wilson, D. and T. Martinez. 1996. Instancebased learning with genetically derived attribute weights. In Proceedings of the International Conference on Artificial Intelligence, Expert Systems, and Neural Networks, pp 11-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yang</author>
<author>V Honavar</author>
</authors>
<title>Feature subset selection using a genetic algorithm.</title>
<date>1997</date>
<booktitle>In Genetic Programming 1997: Proceedings of the Second Annual Conference,</booktitle>
<pages>380</pages>
<contexts>
<context position="11533" citStr="Yang and Honavar (1997)" startWordPosition="1860" endWordPosition="1863"> already has a high score on the tagging task. The IGTREE algorithm is more suited for problems where the features can be ordered in a straightforward way because they have significantly different relevance. 3 Conclusions and Related Research The issue of feature-relevance assignment is well-documented in the machine learning literature. Excellent comparative surveys are (Wettschereck, Aha, and Mohri, 1997) and (Wettschereck and Aha, 1995) or (Blum and Langley, 1997). Feature subset selection by means of evolutionary algorithms was investigated by Skalak (1994), Vafaie and de Jong (1992), and Yang and Honavar (1997). Other work deals with evolutionary approaches for continuous feature weight assignment such as Wilson and Martinez (1996), or Punch and Goodman (1993). The conclusions from these papers are in agreement with our findings on the natural language data, suggesting that feature selection and weighting with GA&apos;s significantly outperform non-weighted approaches. Feature selection generally improves accuracy with a reduc°This fits in with current theory about this morphological process (e.g. Trommelen (1983), Daelemans et al. (1997)). 105 tion in the number of features used. However, we have found </context>
</contexts>
<marker>Yang, Honavar, 1997</marker>
<rawString>Yang, J. and V. Honavar. 1997. Feature subset selection using a genetic algorithm. In Genetic Programming 1997: Proceedings of the Second Annual Conference, pp 380.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>