<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003619">
<title confidence="0.983018">
Multilingual semantic parsing with a pipeline of linear classifiers
</title>
<author confidence="0.991192">
Oscar T¨ackstr¨om
</author>
<affiliation confidence="0.994042">
Swedish Institute of Computer Science
</affiliation>
<address confidence="0.85961">
SE-16429, Kista, Sweden
</address>
<email confidence="0.990224">
oscar@sics.se
</email>
<sectionHeader confidence="0.9937" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99975152631579">
I describe a fast multilingual parser for seman-
tic dependencies. The parser is implemented
as a pipeline of linear classifiers trained with
support vector machines. I use only first or-
der features, and no pair-wise feature combi-
nations in order to reduce training and pre-
diction times. Hyper-parameters are carefully
tuned for each language and sub-problem.
The system is evaluated on seven different
languages: Catalan, Chinese, Czech, English,
German, Japanese and Spanish. An analysis
of learning rates and of the reliance on syn-
tactic parsing quality shows that only modest
improvements could be expected for most lan-
guages given more training data; Better syn-
tactic parsing quality, on the other hand, could
greatly improve the results. Individual tun-
ing of hyper-parameters is crucial for obtain-
ing good semantic parsing quality.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999895823529412">
This paper presents my submission for the seman-
tic parsing track of the CoNLL 2009 shared task on
syntactic and semantic dependencies in multiple lan-
guages (Hajiˇc et al., 2009). The submitted parser is
simpler than the submission in which I participated
at the CoNLL 2008 shared task on joint learning of
syntactic and semantic dependencies (Surdeanu et
al., 2008), in which we used a more complex com-
mittee based approach to both syntax and semantics
(Samuelsson et al., 2008). Results are on par with
our previous system, while the parser is orders of
magnitude faster both at training and prediction time
and is able to process natural language text in Cata-
lan, Chinese, Czech, English, German, Japanese and
Spanish. The parser depends on the input to be anno-
tated with part-of-speech tags and syntactic depen-
dencies.
</bodyText>
<sectionHeader confidence="0.929862" genericHeader="method">
2 Semantic parser
</sectionHeader>
<bodyText confidence="0.999961923076923">
The semantic parser is implemented as a pipeline of
linear classifiers and a greedy constraint satisfaction
post-processing step. The implementation is very
similar to the best performing subsystem of the com-
mittee based system in Samuelsson et al. (2008).
Parsing consists of four steps: predicate sense
disambiguation, argument identification, argument
classification and predicate frame constraint satis-
faction. The first three steps are implemented us-
ing linear classifiers, along with heuristic filtering
techniques. Classifiers are trained using the sup-
port vector machine implementation provided by the
LIBLINEAR software (Fan et al., 2008). MALLET
is used as a framework for the system (McCallum,
2002).
For each classifier, the c-parameter of the SVM
is optimised by a one dimensional grid search using
threefold cross validation on the training set. For
the identification step, the c-parameter is optimised
with respect to Fl-score of the positive class, while
for sense disambiguation and argument labelling the
optimisation is with respect to accuracy. The regions
to search were identified by initial runs on the devel-
opment data. Optimising these parameters for each
classification problem individually proved to be cru-
cial for obtaining good results.
</bodyText>
<subsectionHeader confidence="0.997219">
2.1 Predicate sense disambiguation
</subsectionHeader>
<bodyText confidence="0.99996875">
Since disambiguation of predicate sense is a multi-
class problem, I train the classifiers using the method
of Crammer and Singer (2002), using the implemen-
tation provided by LIBLINEAR. Sense labels do not
generalise over predicate lemmas, so one classifier
is trained for each lemma occurring in the training
data. Rare predicates are given the most common
sense of the predicate. Predicates occurring less than
</bodyText>
<page confidence="0.991542">
103
</page>
<note confidence="0.763566">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 103–108,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.99447525">
7 times in the training data were heuristically deter-
mined to be considered rare. Predicates with unseen
lemmas are labelled with the most common sense
tag in the training data.
</bodyText>
<subsectionHeader confidence="0.921717">
2.1.1 Feature templates
</subsectionHeader>
<bodyText confidence="0.9875935">
The following feature templates are used for predi-
cate sense disambiguation:
</bodyText>
<figure confidence="0.645150454545454">
PREDICATEWORD
PREDICATE[POS/FEATS]
PREDICATEWINDOWBAGLEMMAS
PREDICATEWINDOWPOSITION[POS/FEATS]
GOVERNORRELATION
GOVERNOR[WORD/LEMMA]
GOVERNOR[POS/FEATS]
DEPENDENTRELATION
DEPENDENT[WORD/LEMMA]
DEPENDENT[POS/FEATS]
DEPENDENTSUBCAT.
</figure>
<bodyText confidence="0.999075833333333">
The *WINDOW feature templates extract features
from the two preceding and the two following tokens
around the predicate, with respect to the linear order-
ing of the tokens. The *FEATS templates are based
on information in the PFEATS input column for the
languages where this information is provided.
</bodyText>
<subsectionHeader confidence="0.99994">
2.2 Argument identification and labelling
</subsectionHeader>
<bodyText confidence="0.999991242424242">
In line with most previous pipelined systems, iden-
tification and labelling of arguments are performed
as two separate steps. The classifiers in the identi-
fication step are trained with the standard L2-loss
SVM formulation, while the classifiers in the la-
belling step are trained using the method of Cram-
mer and Singer.
In order to reduce the number of candidate argu-
ments in the identification step, I apply the filter-
ing technique of Xue and Palmer (2004), trivially
adopted to the dependency syntax formalism. Fur-
ther, a filtering heuristic is applied in which argu-
ment candidates with rare predicate / argument part-
of-speech combinations are removed; rare meaning
that the argument candidate is actually an argument
in less than 0.05% of the occurrences of the pair.
These heuristics greatly reduce the number of in-
stances in the argument identification step and im-
prove performance by reducing noise from the train-
ing data.
Separate classifiers are trained for verbal pred-
icates and for nominal predicates, both in order
to save computational resources and because the
frame structures do not generalise between verbal
and nominal predicates. For Czech, in order to re-
duce training time I split the argument identification
problem into three sub-problems: verbs, nouns and
others, based on the part-of-speech of the predicate.
In hindsight, after solving a file encoding related bug
which affected the separability of the Czech data
set, a split into verbal and nominal predicates would
have sufficed. Unfortunately I was not able to rerun
the Czech experiments on time.
</bodyText>
<subsectionHeader confidence="0.710373">
2.2.1 Feature templates
</subsectionHeader>
<bodyText confidence="0.9978415">
The following feature templates are used both for
argument identification and argument labelling:
</bodyText>
<equation confidence="0.644202">
PREDICATELEMMASENSE
PREDICATE[POS/FEATS]
POSITION
</equation>
<bodyText confidence="0.823874208333333">
ARGUMENT[POS/FEATS]
ARGUMENT[WORD/LEMMA]
ARGUMENTWINDOWPOSITIONLEMMA
ARGUMENTWINDOWPOSITION[POS/FEATS]
LEFTSIBLINGWORD
LEFTSIBLING[POS/FEATS]
RIGHTSIBLINGWORD
RIGHTSIBLING[POS/FEATS]
LEFTDEPENDENTWORD
RIGHTDEPENDENT[POS/FEATS]
RELATIONPATH
TRIGRAMRELATIONPATH
GOVERNORRELATION
GOVERNORLEMMA
GOVERNOR[POS/FEATS]
Most of these features, introduced by Gildea and Ju-
rafsky (2002), belong to the folklore by now. The
TRIGRAMRELATIONPATH is a ”soft” version of the
RELATIONPATH template, which treats the relation
path as a bag of triplets of directional labelled depen-
dency relations. Initial experiments suggested that
this feature slightly improves performance, by over-
coming local syntactic parse errors and data sparse-
ness in the case of small training sets.
</bodyText>
<subsectionHeader confidence="0.608583">
2.2.2 Predicate frame constraints
</subsectionHeader>
<bodyText confidence="0.9039065">
Following Johansson and Nugues (2008) I impose
the CORE ARGUMENT CONSISTENCY and CON-
</bodyText>
<page confidence="0.995275">
104
</page>
<bodyText confidence="0.999177611111111">
TINUATION CONSISTENCY constraints on the gen-
erated semantic frames. In the cited work, these
constraints are used to filter the candidate frames
for a re-ranker. I instead perform a greedy search
in which only the core argument with the highest
score is kept when the former constraint is violated.
The latter constraint is enforced by simply dropping
any continuation argument lacking its correspond-
ing core argument. Initial experiments on the de-
velopment data indicates that these simple heuristics
slightly improves semantic parsing quality measured
with labelled F1-score. It is possible that the im-
provement could be greater by using L2-regularised
logistic regression scores instead of the SVM scores,
since the latter can not be interpreted as probabili-
ties. However, logistic regression performed consis-
tently worse than the SVM formulation of Crammer
and Singer in the argument labelling step.
</bodyText>
<subsectionHeader confidence="0.992608">
2.2.3 Handling of multi-function arguments
</subsectionHeader>
<bodyText confidence="0.9999945">
In Czech and Japanese an argument can have multi-
ple relations to the same predicate, i.e. the seman-
tic structure needs sometimes be represented by a
multi-graph. I chose the simplest possible solution
and treat these structures as ordinary graphs with
complex labels. This solution is motivated by the
fact that the palette of multi-function arguments is
small, and that the multiple functions mostly are
highly interdependent, such as in the ACT PAT com-
plex which is the most common in Czech.
</bodyText>
<sectionHeader confidence="0.99992" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.998152333333333">
The semantic parser was evaluated on in-domain
data for Catalan, Chinese, Czech, English, German,
Japanese and Spanish, and on out-of-domain data
for Czech, English and German. The respective
data sets are described in Taul´e et al. (2008), Palmer
and Xue (2009), Hajiˇc et al. (2006), Surdeanu et al.
(2008), Burchardt et al. (2006) and Kawahara et al.
(2002).
My official submission scores are given in table
1, together with post submission labelled and un-
labelled F1-scores. The official submissions were
affected by bugs related to file encoding and hyper-
parameter search. After resolving these bugs, I ob-
tained an improvement of mean F1-score of almost
10 absolute points compared to the official scores.
</bodyText>
<table confidence="0.999259461538462">
Lab F1 Lab F1 Unlab F1
Catalan 57.11 67.14 93.31
Chinese 63.41 74.14 82.57
Czech 71.05 78.29 89.20
English 67.64 78.93 88.70
German 53.42 62.98 89.64
Japanese 54.74 61.44 66.01
Spanish 61.51 69.93 93.54
Mean 61.27 70.41 86.14
Czech† 71.59 78.77 87.13
English† 59.82 68.96 86.23
German† 50.43 47.81 79.52
Mean† 60.61 65.18 84.29
</table>
<tableCaption confidence="0.9792644">
Table 1: Semantic labelled and unlabelled F1-scores for
each language and domain. Left column: official labelled
F1-score. Middle column: post submission labelled F1-
score. Right column: post submission unlabelled F1-
score. † indicates out-of-domain test data.
</tableCaption>
<bodyText confidence="0.999926827586207">
Clearly, there is a large difference in performance
for the different languages and domains. As could
be expected the parser performs much better for the
languages for which a large training set is provided.
However, as discussed in the next section, simply
adding more training data does not seem to solve the
problem.
Comparing unlabelled F1-scores with labelled
F1-scores, it seems that argument identification and
labelling errors contribute almost equally to the total
errors for Chinese, Czech and English. For Catalan,
Spanish and German argument identification scores
are high, while labelling scores are in the lower
range. Japanese stands out with exceptionally low
identification scores. Given that the quality of the
predicted syntactic parsing was higher for Japanese
than for any other language, the bottleneck when
performing semantic parsing seems to be the limited
expressivity of the Japanese syntactic dependency
annotation scheme.
Interestingly, for Czech, the result on the out-of-
domain data set is better than the result on the in-
domain data set, even though the unlabelled result
is slightly worse. For English, on the other hand
the performance drop is in the order of 10 absolute
labelled F1 points, while the drop in unlabelled F1-
score is comparably small. The result on German
out-of-domain data seems to be an outlier, with post-
submission results even worse than the official sub-
</bodyText>
<page confidence="0.985245">
105
</page>
<table confidence="0.999915153846154">
10% 25% 50% 75% 100%
Catalan 54.86 60.52 65.22 66.35 67.14
Chinese 72.93 73.40 73.77 74.08 74.14
Czech 75.42 76.90 77.69 78.00 78.29
English 75.75 77.56 78.37 78.71 78.93
German 47.77 54.74 58.94 61.02 62.98
Japanese 59.82 60.34 60.99 61.37 61.44
Spanish 58.80 64.32 68.35 69.34 69.93
Mean 63.62 66.83 69.05 69.84 70.41
Czech† 76.51 77.48 78.41 78.59 78.77
English† 66.04 67.54 68.37 69.00 68.96
German† 41.65 45.94 46.24 47.45 47.81
Mean† 61.40 63.65 64.34 65.01 65.18
</table>
<tableCaption confidence="0.975353">
Table 2: Semantic labelled Fl-scores w.r.t. training set
size. † indicates out-of-domain test data.
</tableCaption>
<bodyText confidence="0.419619">
mission results. I suspect that this is due to a bug.
</bodyText>
<subsectionHeader confidence="0.998059">
3.1 Learning rates
</subsectionHeader>
<bodyText confidence="0.999684206896552">
In order to assess the effect of training set size on
semantic parsing quality, I performed a learning rate
experiment, in which the proportion of the training
set used for training was varied in steps between
10% and 100% of the full training set size.
Learning rates with respect to labelled Fl-scores
are given in table 2. The improvement in scores are
modest for Chinese, Czech, English and Japanese,
while Catalan, German and Spanish stand out by
vast improvements with additional training data.
However, the improvement when going from 75% to
100% of the training data is only modest for all lan-
guages. With the exception for English, for which
the parser achieves the highest score, the relative
labelled Fl-scores follow the relative sizes of the
training sets.
Looking at learning rates with respect to unla-
belled Fl-scores, given in table 3, it is evident that
adding more training data only has a minor effect on
the identification of arguments.
From table 4, one can see that predicate sense dis-
ambiguation is the sub-task that benefits most from
additional training data. This is not surprising, since
the senses does not generalise, and hence we cannot
hope to correctly label the senses of unseen predi-
cates; the only way to improve results with the cur-
rent formalism seems to be by adding more training
data.
The limited power of a pipeline of local classi-
</bodyText>
<table confidence="0.999742">
10% 25% 50% 75% 100%
Catalan 93.12 93.18 93.28 93.35 93.31
Chinese 82.37 82.45 82.54 82.55 82.57
Czech 89.03 89.12 89.17 89.21 89.20
English 87.96 88.38 88.52 88.67 88.70
German 88.23 89.02 89.63 89.53 89.64
Japanese 65.64 65.75 65.88 66.02 66.01
Spanish 93.52 93.49 93.52 93.53 93.54
Mean 85.70 85.91 86.08 86.12 86.14
Czech† 86.76 87.02 87.16 87.08 87.13
English† 85.67 86.14 86.22 86.20 86.23
German† 77.35 78.31 79.09 79.10 79.52
Mean† 83.26 83.82 84.16 84.13 84.29
</table>
<tableCaption confidence="0.9481855">
Table 3: Semantic unlabelled Fl-scores w.r.t. training set
size. † indicates out-of-domain test data.
</tableCaption>
<table confidence="0.999901769230769">
10% 25% 50% 75% 100%
Catalan 30.61 40.29 53.83 55.83 58.95
Chinese 94.06 94.37 94.71 95.10 95.26
Czech 83.24 84.75 85.78 86.21 86.60
English 92.18 93.68 94.83 95.35 95.60
German 34.91 47.27 58.18 62.18 66.55
Japanese 99.07 99.07 99.07 99.07 99.07
Spanish 38.53 50.22 59.59 62.01 66.26
Mean 67.51 72.81 78.00 79.39 81.18
Czech† 89.05 89.88 91.06 91.38 91.56
English† 83.64 84.27 84.83 85.70 85.94
German† 33.64 43.36 42.59 44.44 45.22
Mean† 68.78 72.51 72.83 73.84 74.24
</table>
<tableCaption confidence="0.996329">
Table 4: Predicate sense disambiguation Fl-scores w.r.t.
training set size. † indicates out-of-domain test data.
</tableCaption>
<bodyText confidence="0.998451333333333">
fiers shows itself in the exact match scores, given
in table 5. This problem is clearly not remedied by
additional training data.
</bodyText>
<subsectionHeader confidence="0.999671">
3.2 Dependence on syntactic parsing quality
</subsectionHeader>
<bodyText confidence="0.995847916666667">
Since I only participated in the semantic parsing
task, the results reported above rely on the provided
predicted syntactic dependency parsing. In order to
investigate the effect of parsing quality on the cur-
rent system, I performed the same learning curve
experiments with gold standard parse information.
These results, shown in tables 6 and 7, give an upper
bound on the possible improvement of the current
system by means of improved parsing quality, given
that the same syntactic annotation formalism is used.
Labelled Fl-scores are greatly improved for all
languages except for Japanese, when using gold
</bodyText>
<page confidence="0.990044">
106
</page>
<table confidence="0.999884307692308">
10% 25% 50% 75% 100%
Catalan 6.77 9.08 11.39 11.17 12.24
Chinese 17.02 17.33 17.61 17.76 17.68
Czech 9.33 9.59 9.97 9.95 10.11
English 12.01 12.76 12.96 13.13 13.17
German 76.95 78.50 78.95 79.20 79.50
Japanese 1.20 1.40 1.80 1.60 1.60
Spanish 8.23 10.20 12.93 13.39 13.16
Mean 18.79 19.84 20.80 20.89 21.07
Czech† 2.53 2.79 2.79 2.87 2.87
English† 19.06 19.53 19.76 20.00 20.00
German† 15.98 19.24 17.82 19.94 20.08
Mean† 12.52 13.85 13.46 14.27 14.32
</table>
<tableCaption confidence="0.958173333333333">
Table 5: Percentage of exactly matched predicate-
argument frames w.r.t. training set size. † indicates out-
of-domain test data.
</tableCaption>
<table confidence="0.999962153846154">
10% 25% 50% 75% 100%
Catalan 62.65 72.50 75.39 77.03 78.86
Chinese 82.59 83.23 83.90 83.94 84.03
Czech 79.15 80.62 81.46 81.91 82.24
English 79.84 81.74 82.65 83.01 83.25
German 52.15 60.66 65.12 65.71 68.36
Japanese 60.85 61.76 62.55 62.85 63.23
Spanish 66.40 72.47 75.70 77.73 78.38
Mean 69.09 73.28 75.25 76.03 76.91
Czech† 78.64 80.07 80.77 81.01 81.20
English† 73.05 74.18 74.99 75.28 75.81
German† 52.06 52.77 54.72 56.22 56.35
Mean† 67.92 69.01 70.16 70.84 71.12
</table>
<tableCaption confidence="0.986231">
Table 6: Semantic labelled Fl-scores w.r.t. training set
size, using gold standard syntactic and part-of-speech tag
annotation. † indicates out-of-domain test data.
</tableCaption>
<bodyText confidence="0.996606533333333">
standard syntactic and part-of-speech annotations.
For Catalan, Chinese and Spanish the improvement
is in the order of 10 absolute points. For Japanese
the improvement is a meagre 2 absolute points. This
is not surprising given that the quality of the pro-
vided syntactic parsing was already very high for
Japanese, as discussed previously.
Results with respect to unlabelled Fl-scores fol-
low the same pattern as for labelled Fl-scores.
Again, with Japanese the semantic parsing does not
benefit much from better syntactic parsing quality.
For Catalan and Spanish on the other hand, the iden-
tification of arguments is almost perfect with gold
standard syntax. The poor labelling quality for these
languages can thus not be attributed to the syntactic
</bodyText>
<table confidence="0.999808461538462">
10% 25% 50% 75% 100%
Catalan 99.94 99.98 99.99 99.99 99.99
Chinese 92.55 92.67 92.72 92.63 92.62
Czech 91.21 91.27 91.30 91.30 91.31
English 92.34 92.61 92.85 92.89 92.95
German 93.46 93.59 94.08 93.85 94.14
Japanese 66.98 67.20 67.58 67.62 67.74
Spanish 99.99 99.99 100.00 100.00 100.00
Mean 90.92 91.04 91.22 91.18 91.25
Czech† 89.00 89.22 89.34 89.38 89.36
English† 92.71 92.56 92.91 93.06 93.04
German† 90.54 90.23 90.77 90.86 90.99
Mean† 90.75 90.67 91.01 91.10 91.13
</table>
<tableCaption confidence="0.989314666666667">
Table 7: Semantic unlabelled Fl-scores w.r.t. training set
size, using gold standard syntactic and part-of-speech tag
annotation. † indicates out-of-domain test data.
</tableCaption>
<bodyText confidence="0.819434">
parse quality.
</bodyText>
<subsectionHeader confidence="0.994595">
3.3 Computational requirements
</subsectionHeader>
<bodyText confidence="0.999950875">
Training and prediction times on a 2.3 GHz quad-
core AMD OpteronTMsystem are given in table 8.
Since only linear classifiers and no pair-wise feature
combinations are used, training and prediction times
are quite modest. Verbal and nominal predicates are
trained in parallel, no additional parallelisation is
employed. Most of the training time is spent on op-
timising the c parameter of the SVM. Training times
are roughly ten times as long as compared to training
times with no hyper-parameter optimisation. Czech
stands out as much more computationally demand-
ing, especially in the sense disambiguation training
step. The reason is the vast number of predicates in
Czech compared to the other languages. The ma-
jority of the time in this step is, however, spent on
writing the SVM training problems to disk.
Memory requirements range between approxi-
mately 1 Gigabytes for the smallest data sets and
6 Gigabytes for the largest data set. Memory us-
age could be lowered substantially by using a more
compact feature dictionary. Currently every feature
template / value pair is represented as a string, which
is wasteful since many feature templates share the
same values.
</bodyText>
<sectionHeader confidence="0.999438" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.939048">
I have presented an effective multilingual pipelined
semantic parser, using linear classifiers and a simple
</bodyText>
<page confidence="0.991065">
107
</page>
<table confidence="0.999398">
Sense ArgId ArgLab Tot Pred
Catalan 7m 11m 33m 51m 13s
Chinese 7m 13m 22m 42m 15s
Czech 10h 1h 1.5h 12.5h 34.5m
English 16m 14m 28m 58m 14.5s
German 4m 2m 5m 13m 3.5s
Japanese 1s 1m 4m 5m 4s
Spanish 10m 16m 40m 1.1h 13s
</table>
<tableCaption confidence="0.672871444444444">
Table 8: Training times for each language and sub-
problem and approximate prediction times. Columns:
training times for sense disambiguation (Sense), ar-
gument identification (ArgId), argument labelling (Ar-
gLab), total training time (Tot), and total prediction time
(Pred). Training times are measured w.r.t. to the union
of the official training and development data sets. Predic-
tion times are measured w.r.t. to the official evaluation
data sets.
</tableCaption>
<bodyText confidence="0.99980975">
greedy constraint satisfaction heuristic. While the
semantic parsing results in these experiments fail to
reach the best results given by other experiments, the
parser quickly delivers quite accurate semantic pars-
ing of Catalan, Chinese, Czech, English, German,
Japanese and Spanish.
Optimising the hyper-parameters of each of the
individual classifiers is essential for obtaining good
results with this simple architecture. Syntactic pars-
ing quality has a large impact on the quality of the
semantic parsing; a problem that is not remedied by
adding additional training data.
</bodyText>
<sectionHeader confidence="0.998573" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999850861111111">
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006.
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of the 5th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC-2006), Genoa, Italy.
Koby Crammer and Yoram Singer. 2002. On the learn-
ability and design of output codes for multiclass prob-
lems. Machine Learning, 47(2):201–233, May.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871–1874.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245–288.
Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr
Sgall, Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, Marie
Mikulov´a, and Zdenˇek ˇZabokrtsk´y. 2006. Prague
Dependency Treebank 2.0. CD-ROM, Cat. No.
LDC2006T01, ISBN 1-58563-370-4, Linguistic Data
Consortium, Philadelphia, Pennsylvania, USA.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic depen-
dencies in multiple languages. In Proceedings of
the 13th Conference on Computational Natural Lan-
guage Learning (CoNLL-2009), June 4-5, Boulder,
Colorado, USA.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic–semantic analysis
with PropBank and NomBank. In Proceedings of the
Shared Task Session of CoNLL-2008, Manchester,
UK.
Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of the 3rd International
Conference on Language Resources and Evaluation
(LREC-2002), pages 2008–2013, Las Palmas, Canary
Islands.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143–172.
Yvonne Samuelsson, Oscar T¨ackstr¨om, Sumithra
Velupillai, Johan Eklund, Mark Fishel, and Markus
Saers. 2008. Mixing and blending syntactic and
semantic dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 248–252, Manchester,
England, August. Coling 2008 Organizing Committee.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the 12th Con-
ference on Computational Natural Language Learning
(CoNLL-2008), Manchester, Great Britain.
Mariona Taul´e, Maria Ant`onia Mart´ı, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation (LREC-2008), Marrakesh, Morroco.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Dekang Lin
and Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 88–94, Barcelona, Spain, July. Association for
Computational Linguistics.
</reference>
<page confidence="0.998365">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.834318">
<title confidence="0.998981">Multilingual semantic parsing with a pipeline of linear classifiers</title>
<author confidence="0.95343">Oscar</author>
<affiliation confidence="0.997048">Swedish Institute of Computer</affiliation>
<address confidence="0.92549">SE-16429, Kista,</address>
<email confidence="0.986611">oscar@sics.se</email>
<abstract confidence="0.9972143">I describe a fast multilingual parser for semantic dependencies. The parser is implemented as a pipeline of linear classifiers trained with support vector machines. I use only first order features, and no pair-wise feature combinations in order to reduce training and prediction times. Hyper-parameters are carefully tuned for each language and sub-problem. The system is evaluated on seven different languages: Catalan, Chinese, Czech, English, German, Japanese and Spanish. An analysis of learning rates and of the reliance on syntactic parsing quality shows that only modest improvements could be expected for most languages given more training data; Better syntactic parsing quality, on the other hand, could greatly improve the results. Individual tuning of hyper-parameters is crucial for obtaining good semantic parsing quality.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pad´o</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA corpus: a German corpus resource for lexical semantics.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006),</booktitle>
<location>Genoa, Italy.</location>
<marker>Burchardt, Erk, Frank, Kowalski, Pad´o, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006. The SALSA corpus: a German corpus resource for lexical semantics. In Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006), Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>On the learnability and design of output codes for multiclass problems.</title>
<date>2002</date>
<booktitle>Machine Learning,</booktitle>
<volume>47</volume>
<issue>2</issue>
<contexts>
<context position="3298" citStr="Crammer and Singer (2002)" startWordPosition="502" endWordPosition="505"> threefold cross validation on the training set. For the identification step, the c-parameter is optimised with respect to Fl-score of the positive class, while for sense disambiguation and argument labelling the optimisation is with respect to accuracy. The regions to search were identified by initial runs on the development data. Optimising these parameters for each classification problem individually proved to be crucial for obtaining good results. 2.1 Predicate sense disambiguation Since disambiguation of predicate sense is a multiclass problem, I train the classifiers using the method of Crammer and Singer (2002), using the implementation provided by LIBLINEAR. Sense labels do not generalise over predicate lemmas, so one classifier is trained for each lemma occurring in the training data. Rare predicates are given the most common sense of the predicate. Predicates occurring less than 103 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 103–108, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics 7 times in the training data were heuristically determined to be considered rare. Predicates with unseen lemmas are lab</context>
</contexts>
<marker>Crammer, Singer, 2002</marker>
<rawString>Koby Crammer and Yoram Singer. 2002. On the learnability and design of output codes for multiclass problems. Machine Learning, 47(2):201–233, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="2509" citStr="Fan et al., 2008" startWordPosition="380" endWordPosition="383">nted as a pipeline of linear classifiers and a greedy constraint satisfaction post-processing step. The implementation is very similar to the best performing subsystem of the committee based system in Samuelsson et al. (2008). Parsing consists of four steps: predicate sense disambiguation, argument identification, argument classification and predicate frame constraint satisfaction. The first three steps are implemented using linear classifiers, along with heuristic filtering techniques. Classifiers are trained using the support vector machine implementation provided by the LIBLINEAR software (Fan et al., 2008). MALLET is used as a framework for the system (McCallum, 2002). For each classifier, the c-parameter of the SVM is optimised by a one dimensional grid search using threefold cross validation on the training set. For the identification step, the c-parameter is optimised with respect to Fl-score of the positive class, while for sense disambiguation and argument labelling the optimisation is with respect to accuracy. The regions to search were identified by initial runs on the development data. Optimising these parameters for each classification problem individually proved to be crucial for obta</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="6752" citStr="Gildea and Jurafsky (2002)" startWordPosition="975" endWordPosition="979">sufficed. Unfortunately I was not able to rerun the Czech experiments on time. 2.2.1 Feature templates The following feature templates are used both for argument identification and argument labelling: PREDICATELEMMASENSE PREDICATE[POS/FEATS] POSITION ARGUMENT[POS/FEATS] ARGUMENT[WORD/LEMMA] ARGUMENTWINDOWPOSITIONLEMMA ARGUMENTWINDOWPOSITION[POS/FEATS] LEFTSIBLINGWORD LEFTSIBLING[POS/FEATS] RIGHTSIBLINGWORD RIGHTSIBLING[POS/FEATS] LEFTDEPENDENTWORD RIGHTDEPENDENT[POS/FEATS] RELATIONPATH TRIGRAMRELATIONPATH GOVERNORRELATION GOVERNORLEMMA GOVERNOR[POS/FEATS] Most of these features, introduced by Gildea and Jurafsky (2002), belong to the folklore by now. The TRIGRAMRELATIONPATH is a ”soft” version of the RELATIONPATH template, which treats the relation path as a bag of triplets of directional labelled dependency relations. Initial experiments suggested that this feature slightly improves performance, by overcoming local syntactic parse errors and data sparseness in the case of small training sets. 2.2.2 Predicate frame constraints Following Johansson and Nugues (2008) I impose the CORE ARGUMENT CONSISTENCY and CON104 TINUATION CONSISTENCY constraints on the generated semantic frames. In the cited work, these co</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Jarmila Panevov´a</author>
<author>Eva Hajiˇcov´a</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
<author>Jan ˇStˇep´anek</author>
<author>Jiˇr´ı Havelka</author>
<author>Marie Mikulov´a</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<date>2006</date>
<booktitle>Prague Dependency Treebank 2.0. CD-ROM, Cat. No. LDC2006T01, ISBN 1-58563-370-4, Linguistic Data Consortium,</booktitle>
<location>Philadelphia, Pennsylvania, USA.</location>
<marker>Hajiˇc, Panevov´a, Hajiˇcov´a, Sgall, Pajas, ˇStˇep´anek, Havelka, Mikulov´a, ˇZabokrtsk´y, 2006</marker>
<rawString>Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr Sgall, Petr Pajas, Jan ˇStˇep´anek, Jiˇr´ı Havelka, Marie Mikulov´a, and Zdenˇek ˇZabokrtsk´y. 2006. Prague Dependency Treebank 2.0. CD-ROM, Cat. No. LDC2006T01, ISBN 1-58563-370-4, Linguistic Data Consortium, Philadelphia, Pennsylvania, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009),</booktitle>
<location>Boulder, Colorado, USA.</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009), June 4-5, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based syntactic–semantic analysis with PropBank and NomBank.</title>
<date>2008</date>
<booktitle>In Proceedings of the Shared Task Session of CoNLL-2008,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="7206" citStr="Johansson and Nugues (2008)" startWordPosition="1044" endWordPosition="1047">IGHTDEPENDENT[POS/FEATS] RELATIONPATH TRIGRAMRELATIONPATH GOVERNORRELATION GOVERNORLEMMA GOVERNOR[POS/FEATS] Most of these features, introduced by Gildea and Jurafsky (2002), belong to the folklore by now. The TRIGRAMRELATIONPATH is a ”soft” version of the RELATIONPATH template, which treats the relation path as a bag of triplets of directional labelled dependency relations. Initial experiments suggested that this feature slightly improves performance, by overcoming local syntactic parse errors and data sparseness in the case of small training sets. 2.2.2 Predicate frame constraints Following Johansson and Nugues (2008) I impose the CORE ARGUMENT CONSISTENCY and CON104 TINUATION CONSISTENCY constraints on the generated semantic frames. In the cited work, these constraints are used to filter the candidate frames for a re-ranker. I instead perform a greedy search in which only the core argument with the highest score is kept when the former constraint is violated. The latter constraint is enforced by simply dropping any continuation argument lacking its corresponding core argument. Initial experiments on the development data indicates that these simple heuristics slightly improves semantic parsing quality meas</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic–semantic analysis with PropBank and NomBank. In Proceedings of the Shared Task Session of CoNLL-2008, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Kˆoiti Hasida</author>
</authors>
<title>Construction of a Japanese relevance-tagged corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002),</booktitle>
<pages>2008--2013</pages>
<location>Las Palmas, Canary Islands.</location>
<contexts>
<context position="9068" citStr="Kawahara et al. (2002)" startWordPosition="1338" endWordPosition="1341">aphs with complex labels. This solution is motivated by the fact that the palette of multi-function arguments is small, and that the multiple functions mostly are highly interdependent, such as in the ACT PAT complex which is the most common in Czech. 3 Results The semantic parser was evaluated on in-domain data for Catalan, Chinese, Czech, English, German, Japanese and Spanish, and on out-of-domain data for Czech, English and German. The respective data sets are described in Taul´e et al. (2008), Palmer and Xue (2009), Hajiˇc et al. (2006), Surdeanu et al. (2008), Burchardt et al. (2006) and Kawahara et al. (2002). My official submission scores are given in table 1, together with post submission labelled and unlabelled F1-scores. The official submissions were affected by bugs related to file encoding and hyperparameter search. After resolving these bugs, I obtained an improvement of mean F1-score of almost 10 absolute points compared to the official scores. Lab F1 Lab F1 Unlab F1 Catalan 57.11 67.14 93.31 Chinese 63.41 74.14 82.57 Czech 71.05 78.29 89.20 English 67.64 78.93 88.70 German 53.42 62.98 89.64 Japanese 54.74 61.44 66.01 Spanish 61.51 69.93 93.54 Mean 61.27 70.41 86.14 Czech† 71.59 78.77 87.1</context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida. 2002. Construction of a Japanese relevance-tagged corpus. In Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC-2002), pages 2008–2013, Las Palmas, Canary Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="2572" citStr="McCallum, 2002" startWordPosition="393" endWordPosition="394">satisfaction post-processing step. The implementation is very similar to the best performing subsystem of the committee based system in Samuelsson et al. (2008). Parsing consists of four steps: predicate sense disambiguation, argument identification, argument classification and predicate frame constraint satisfaction. The first three steps are implemented using linear classifiers, along with heuristic filtering techniques. Classifiers are trained using the support vector machine implementation provided by the LIBLINEAR software (Fan et al., 2008). MALLET is used as a framework for the system (McCallum, 2002). For each classifier, the c-parameter of the SVM is optimised by a one dimensional grid search using threefold cross validation on the training set. For the identification step, the c-parameter is optimised with respect to Fl-score of the positive class, while for sense disambiguation and argument labelling the optimisation is with respect to accuracy. The regions to search were identified by initial runs on the development data. Optimising these parameters for each classification problem individually proved to be crucial for obtaining good results. 2.1 Predicate sense disambiguation Since di</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Nianwen Xue</author>
</authors>
<title>Adding semantic roles to the Chinese Treebank.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="8970" citStr="Palmer and Xue (2009)" startWordPosition="1321" endWordPosition="1324">y a multi-graph. I chose the simplest possible solution and treat these structures as ordinary graphs with complex labels. This solution is motivated by the fact that the palette of multi-function arguments is small, and that the multiple functions mostly are highly interdependent, such as in the ACT PAT complex which is the most common in Czech. 3 Results The semantic parser was evaluated on in-domain data for Catalan, Chinese, Czech, English, German, Japanese and Spanish, and on out-of-domain data for Czech, English and German. The respective data sets are described in Taul´e et al. (2008), Palmer and Xue (2009), Hajiˇc et al. (2006), Surdeanu et al. (2008), Burchardt et al. (2006) and Kawahara et al. (2002). My official submission scores are given in table 1, together with post submission labelled and unlabelled F1-scores. The official submissions were affected by bugs related to file encoding and hyperparameter search. After resolving these bugs, I obtained an improvement of mean F1-score of almost 10 absolute points compared to the official scores. Lab F1 Lab F1 Unlab F1 Catalan 57.11 67.14 93.31 Chinese 63.41 74.14 82.57 Czech 71.05 78.29 89.20 English 67.64 78.93 88.70 German 53.42 62.98 89.64 J</context>
</contexts>
<marker>Palmer, Xue, 2009</marker>
<rawString>Martha Palmer and Nianwen Xue. 2009. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1):143–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvonne Samuelsson</author>
<author>Oscar T¨ackstr¨om</author>
<author>Sumithra Velupillai</author>
<author>Johan Eklund</author>
<author>Mark Fishel</author>
<author>Markus Saers</author>
</authors>
<title>Mixing and blending syntactic and semantic dependencies.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,</booktitle>
<pages>248--252</pages>
<location>Manchester, England,</location>
<marker>Samuelsson, T¨ackstr¨om, Velupillai, Eklund, Fishel, Saers, 2008</marker>
<rawString>Yvonne Samuelsson, Oscar T¨ackstr¨om, Sumithra Velupillai, Johan Eklund, Mark Fishel, and Markus Saers. 2008. Mixing and blending syntactic and semantic dependencies. In CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning, pages 248–252, Manchester, England, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llu´ıs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008),</booktitle>
<location>Manchester, Great Britain.</location>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llu´ıs M`arquez, and Joakim Nivre. 2008. The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008), Manchester, Great Britain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taul´e</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Marta Recasens</author>
</authors>
<title>AnCora: Multilevel Annotated Corpora for Catalan and Spanish.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008),</booktitle>
<location>Marrakesh, Morroco.</location>
<marker>Taul´e, Mart´ı, Recasens, 2008</marker>
<rawString>Mariona Taul´e, Maria Ant`onia Mart´ı, and Marta Recasens. 2008. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008), Marrakesh, Morroco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>88--94</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="5086" citStr="Xue and Palmer (2004)" startWordPosition="755" endWordPosition="758">*FEATS templates are based on information in the PFEATS input column for the languages where this information is provided. 2.2 Argument identification and labelling In line with most previous pipelined systems, identification and labelling of arguments are performed as two separate steps. The classifiers in the identification step are trained with the standard L2-loss SVM formulation, while the classifiers in the labelling step are trained using the method of Crammer and Singer. In order to reduce the number of candidate arguments in the identification step, I apply the filtering technique of Xue and Palmer (2004), trivially adopted to the dependency syntax formalism. Further, a filtering heuristic is applied in which argument candidates with rare predicate / argument partof-speech combinations are removed; rare meaning that the argument candidate is actually an argument in less than 0.05% of the occurrences of the pair. These heuristics greatly reduce the number of instances in the argument identification step and improve performance by reducing noise from the training data. Separate classifiers are trained for verbal predicates and for nominal predicates, both in order to save computational resources</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 88–94, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>