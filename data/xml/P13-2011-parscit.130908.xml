<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006605">
<title confidence="0.964233">
An Empirical Study on Uncertainty Identification in Social Media Context
</title>
<author confidence="0.9948115">
Zhongyu Wei&apos;, Junwen Chen&apos;, Wei Gao2,
Binyang Li&apos;, Lanjun Zhou&apos;, Yulan He3, Kam-Fai Wong&apos;
</author>
<affiliation confidence="0.996895333333333">
&apos;The Chinese University of Hong Kong, Shatin, N.T., Hong Kong
2Qatar Computing Research Institute, Qatar Foundation, Doha, Qatar
3School of Engineering &amp; Applied Science, Aston University, Birmingham, UK
</affiliation>
<email confidence="0.9907915">
{zywei,jwchen,byli,ljzhou,kfwong}@se.cuhk.edu.hk
wgao@qf.org.qa, y.he@cantab.net
</email>
<sectionHeader confidence="0.99385" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999575">
Uncertainty text detection is important
to many social-media-based applications
since more and more users utilize social
media platforms (e.g., Twitter, Facebook,
etc.) as information source to produce
or derive interpretations based on them.
However, existing uncertainty cues are in-
effective in social media context because
of its specific characteristics. In this pa-
per, we propose a variant of annotation
scheme for uncertainty identification and
construct the first uncertainty corpus based
on tweets. We then conduct experiments
on the generated tweets corpus to study the
effectiveness of different types of features
for uncertainty text identification.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997730910447762">
Social media is not only a social network tool for
people to communicate but also plays an important
role as information source with more and more
users searching and browsing news on it. People
also utilize information from social media for de-
veloping various applications, such as earthquake
warning systems (Sakaki et al., 2010) and fresh
webpage discovery (Dong et al., 2010). How-
ever, due to its casual and word-of-mouth pecu-
liarities, the quality of information in social me-
dia in terms of factuality becomes a premier con-
cern. Chances are there for uncertain information
or even rumors flooding in such a context of free
form. We analyzed a tweet dataset which includes
326,747 posts (Details are given in Section 3) col-
lected during 2011 London Riots, and result re-
veals that at least 18.91% of these tweets bear un-
certainty characteristics&apos;. Therefore, distinguish-
ing uncertain statements from factual ones is cru-
cial for users to synthesize social media informa-
tion to produce or derive reliable interpretations,
&apos;The preliminary study was done based on a manually de-
fined uncertainty cue-phrase list. Tweets containing at least
one hedge cue were treated as uncertain.
and this is expected helpful for applications like
credibility analysis (Castillo et al., 2011) and ru-
mor detection (Qazvinian et al., 2011) based on
social media.
Although uncertainty has been studied theoret-
ically for a long time as a grammatical phenom-
ena (Seifert and Welte, 1987), the computational
treatment of uncertainty is a newly emerging area
of research. Szarvas et al. (2012) pointed out that
“Uncertainty - in its most general sense - can be
interpreted as lack of information: the receiver of
the information (i.e., the hearer or the reader) can-
not be certain about some pieces of information”.
In recent years, the identification of uncertainty
in formal text, e.g., biomedical text, reviews or
newswire, has attracted lots of attention (Kilicoglu
and Bergler, 2008; Medlock and Briscoe, 2007;
Szarvas, 2008; Light et al., 2004). However, un-
certainty identification in social media context is
rarely explored.
Previous research shows that uncertainty identi-
fication is domain dependent as the usage of hedge
cues varies widely in different domains (Morante
and Sporleder, 2012). Therefore, the employment
of existing out-of-domain corpus to social media
context is ineffective. Furthermore, compared to
the existing uncertainty corpus, the expression of
uncertainty in social media is fairly different from
that in formal text in a sense that people usu-
ally raise questions or refer to external informa-
tion when making uncertain statements. But, nei-
ther of the uncertainty expressions can be repre-
sented based on the existing types of uncertainty
defined in the literature. Therefore, a different un-
certainty classification scheme is needed in social
media context.
In this paper, we propose a novel uncertainty
classification scheme and construct the first uncer-
tainty corpus based on social media data – tweets
in specific here. And then we conduct experi-
ments for uncertainty post identification and study
the effectiveness of different categories of features
based on the generated corpus.
</bodyText>
<page confidence="0.98136">
58
</page>
<note confidence="0.501225">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 58–62,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.998527" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.9999345">
We introduce some popular uncertainty corpora
and methods for uncertainty identification.
</bodyText>
<subsectionHeader confidence="0.994511">
2.1 Uncertainty corpus
</subsectionHeader>
<bodyText confidence="0.999947862068966">
Several text corpora from various domains have
been annotated over the past few years at different
levels (e.g., expression, event, relation, sentence)
with information related to uncertainty.
Sauri and Pustejovsky (2009) presented a cor-
pus annotated with information about the factu-
ality of events, namely Factbank, which is con-
structed based on TimeBank2 containing 3,123 an-
notated sentences from 208 news documents with
8 different levels of uncertainty defined.
Vincze et al. (2008) constructed the BioSocpe
corpus, which consists of medical and biological
texts annotated for negation, uncertainty and their
linguistic scope. This corpus contains 20,924 sen-
tences.
Ganter et al. (2009) generated Wikipedia
Weasels Corpus, where Weasel tags in Wikipedia
articles is adopted readily as labels for uncertainty
annotation. It contains 168,923 unique sentences
with 437 weasel tags in total.
Although several uncertainty corpora exist,
there is not a uniform set of standard for uncer-
tainty annotation. Szarvas et al. (2012) normal-
ized the annotation of the three corpora aforemen-
tioned. However, the context of these corpora
is different from that of social media. Typically,
these documents annotated are grammatically cor-
rect, carefully punctuated, formally structured and
logically expressed.
</bodyText>
<subsectionHeader confidence="0.998655">
2.2 Uncertainty identification
</subsectionHeader>
<bodyText confidence="0.994895458333333">
Previous work on uncertainty identification fo-
cused on classifying sentences into uncertain
or definite categories. Existing approaches are
mainly based on supervised methods (Light et
al., 2004; Medlock and Briscoe, 2007; Medlock,
2008; Szarvas, 2008) using the annotated corpus
with different types of features including Part-Of-
Speech (POS) tags, stems, n-grams, etc..
Classification of uncertain sentences was con-
solidated as a task in the 2010 edition of CoNLL
shared task on learning to detect hedge cues
and their scope in natural language text (Farkas
et al., 2010). The best system for Wikipedia
data (Georgescul, 2010) employed Support Vector
Machine (SVM), and the best system for biolog-
ical data (Tang et al., 2010) adopted Conditional
zhttp://www.timeml.org/site/timebank/
timebank.html
Random Fields (CRF).
In our work, we conduct an empirical study of
uncertainty identification on tweets dataset and ex-
plore the effectiveness of different types of fea-
tures (i.e., content-based, user-based and Twitter-
specific) from social media context.
</bodyText>
<sectionHeader confidence="0.948543" genericHeader="method">
3 Uncertainty corpus for microblogs
</sectionHeader>
<subsectionHeader confidence="0.999684">
3.1 Types of uncertainty in microblogs
</subsectionHeader>
<bodyText confidence="0.947575923076923">
Traditionally, uncertainty can be divided into
two categories, namely Epistemic and Hypothet-
ical (Kiefer, 2005). For Epistemic, there are two
sub-classes Possible and Probable. For Hypotheti-
cal, there are four sub-classes including Investiga-
tion, Condition, Doxastic and Dynamic. The detail
of the classification is described as below (Kiefer,
2005):
Epistemic: On the basis of our world knowledge
we cannot decide at the moment whether the
statement is true or false.
Hypothetical: This type of uncertainty includes
four sub-classes:
</bodyText>
<listItem confidence="0.988273777777778">
• Doxastic: Expresses the speaker’s be-
liefs and hypotheses.
• Investigation: Proposition under inves-
tigation.
• Condition: Proposition under condi-
tion.
• Dynamic: Contains deontic, disposi-
tional, circumstantial and buletic modal-
ity.
</listItem>
<bodyText confidence="0.975317380952381">
Compared to the existing uncertainty corpora,
social media authors enjoy free form of writing.
In order to study the difference, we annotated a
small set of 827 randomly sampled tweets accord-
ing to the scheme of uncertainty types above, in
which we found 65 uncertain tweets. And then,
we manually identified all the possible uncertain
tweets, and found 246 really uncertain ones out of
these 827 tweets, which means that 181 uncertain
tweets are missing based on this scheme. We have
the following three salient observations:
– Firstly, there is no tweet found with the type of
Investigation. We find people seldom use words
like “examine” or “test” (indicative words of In-
vestigation category) when posting tweets. Once
they do this, the statement should be considered
as highly certain. For example, @dobibid I have
tested the link, it is fake!
– Secondly, people frequently raise questions
about some specific topics for confirmation which
expresses uncertainty. For example, @ITVCentral
</bodyText>
<page confidence="0.972029">
59
</page>
<bodyText confidence="0.96820865">
Tweet# 4743
Can you confirm that Birmingham children’s hos-
pital has/hasn’t been attacked by rioters?
– Thirdly, people tend to post message with exter-
nal information (e.g., story from friends) which re-
veals uncertainty. For example, Friend who works
at the children’s hospital in Birmingham says the
riot police are protecting it.
Based on these observations, we propose a vari-
ant of uncertainty types in social media context
by eliminating the category of Investigation and
adding the category of Question and External un-
der Hypothetical, as shown in Table 3.1. Note
that our proposed scheme is based on Kiefer’s
work (2005) which was previously extended to
normalize uncertainty corpora in different genres
by Szarvas et al. (2012). But we did not try these
extended schema for specific genres since even the
most general one (Kiefer, 2005) was proved un-
suitable for social media context.
</bodyText>
<subsectionHeader confidence="0.999221">
3.2 Annotation result
</subsectionHeader>
<bodyText confidence="0.999514741935484">
The dataset we annotated was collected from Twit-
ter using Streaming API during summer riots
in London during August 6-13 2011, including
326,747 tweets in total. Search criteria include
hashtags like #ukriots, #londonriots, #prayforlon-
don, and so on. We further extracted the tweets
relating to seven significant events during the riot
identified by UK newspaper The Guardian from
this set of tweets. We annotated all the 4,743 ex-
tracted tweets for the seven events3.
Two annotators were trained to annotate the
dataset independently. Given a collection of
tweets T = {t1, t2, t3...tnI, the annotation task is
to label each tweet ti as either uncertain or cer-
tain. Uncertainty assertions are to be identified
in terms of the judgements about the author’s in-
tended meaning rather than the presence of uncer-
tain cue-phrase. For those tweets annotated as un-
certain, sub-class labels are also required accord-
ing to the classification indicated in Table 3.1 (i.e.,
multi-label is allowed).
The Kappa coefficient (Carletta, 1996) indi-
cating inter-annotator agreement was 0.9073 for
the certain/uncertain binary classification and was
0.8271 for fine-grained annotation. The conflict
labels from the two annotators were resolved by a
third annotator. Annotation result is displayed in
Table 3.2, where 926 out of 4,743 tweets are la-
beled as uncertain accounting for 19.52%. Ques-
tion is the uncertainty category with most tweets,
followed by External. Only 21 tweets are labeled
</bodyText>
<footnote confidence="0.894983">
3http://www.guardian.co.uk/
uk/interactive/2011/dec/07/
london-riots-twitter
</footnote>
<equation confidence="0.996893555555556">
Uncertainty# 926
Possible# 16
Epistemic
Probable# 129
Condition# 71
Doxastic# 48
Dynamic# 21
External# 208
Question# 488
</equation>
<tableCaption confidence="0.961151">
Table 2: Statistics of annotation result
</tableCaption>
<bodyText confidence="0.999917583333333">
as Dynamic and all of them are buletic modal-
ity4 which shares similarity with Doxastic. There-
fore, we consider Dynamic together with Domes-
tic in the error analysis for simplicity. During
the preliminary annotation, we found that uncer-
tainty cue-phrase is a good indicator for uncer-
tainty tweets since tweets labeled as uncertain al-
ways contain at least one cue-phrase. Therefore,
annotators are also required identify cue-phrases
which trigger the sense of uncertainty in the tweet.
All cue-phrases appearing more than twice are col-
lected to form a uncertainty cue-phrase list.
</bodyText>
<sectionHeader confidence="0.974746" genericHeader="evaluation">
4 Experiment and evaluation
</sectionHeader>
<bodyText confidence="0.999976944444444">
We aim to identify those uncertainty tweets from
tweet collection automatically based on machine
learning approaches. In addition to n-gram fea-
tures, we also explore the effectiveness of three
categories of social media specific features includ-
ing content-based, user-based and Twitter-specific
ones. The description of the three categories of
features is shown in Table 4. Since the length of
tweet is relatively short, we therefore did not carry
out stopwords removal or stemming.
Our preliminary experiments showed that com-
bining unigrams with bigrams and trigrams gave
better performance than using any one or two of
these three features. Therefore, we just report the
result based on the combination of them as n-gram
features. Five-fold cross validation is used for
evaluation. Precision, recall and F-1 score of un-
certainty category are used as the metrics.
</bodyText>
<subsectionHeader confidence="0.997711">
4.1 Overall performance
</subsectionHeader>
<bodyText confidence="0.9999887">
The overall performance of different approaches
is shown in Table 4.1. We used uncertainty cue-
phrase matching approach as baseline, denoted
by CP. For CP, we labeled tweets containing at
least one entry in uncertainty cue-phrase list (de-
scribed in Section 3) as uncertain. All the other
approaches are supervised methods using SVM
based on different feature sets. n-gram stands for
n-gram feature set, C means content-based feature
set, U denotes user-based feature set, T represents
</bodyText>
<page confidence="0.679348">
60 4Proposition expresses plans, intentions or desires.
</page>
<table confidence="0.9855402">
Hypothetical
Category Subtype Cue Phrase Example
Possible, etc. may, etc. It may be raining.
Epistemic
Probable likely, etc. It is probably raining.
Hypothetical Condition if, etc. If it rains, we’ll stay in.
Doxastic believe, etc. He believes that the Earth is flat.
Dynamic hope, etc. fake picture of the london eye on fire... i hope
External someone said, etc. Someone said that London zoo was attacked.
Question seriously?, etc. Birmingham riots are moving to the children hospital?! seriously?
</table>
<tableCaption confidence="0.997889">
Table 1: Classification of uncertainty in social media context
</tableCaption>
<table confidence="0.998091235294117">
Category Name Description
Content-based Length Length of the tweet
Cue Phrase Whether the tweet contains a uncertainty cue
OOV Ratio Ratio of words out of vocabulary
Twitter-specific URL Whether the tweet contains a URL
URL Count Frequency of URLs in corpus
Retweet Count How many times has this tweet been retweeted
Hashtag Whether the tweet contains a hashtag
Hashtag Count Number of Hashtag in tweets
Reply Is the current tweet a reply tweet
Rtweet Is the current tweet a retweet tweet
User-based Follower Count Number of follower the user owns
List Count Number of list the users owns
Friend Count Number of friends the user owns
Favorites Count Number of favorites the user owns
Tweet Count Number of tweets the user published
Verified Whether the user is verified
</table>
<tableCaption confidence="0.991249">
Table 3: Feature list for uncertainty classification
</tableCaption>
<table confidence="0.9999873">
Approach Precision Recall F-1
CP 0.3732 0.9589 0.5373
SVMn−gram 0.7278 0.8259 0.7737
SVMn−gram+C 0.8010 0.8260 0.8133
SVMn−gram+U 0.7708 0.8271 0.7979
SVMn−gram+T 0.7578 0.8266 0.7907
SVMn−gram+ALL 0.8162 0.8269 0.8215
SVMn−gram+Cue Phrase 0.7989 0.8266 0.8125
SVMn−gram+Length 0.7372 0.8216 0.7715
SVMn−gram+OOV Ratio 0.7414 0.8233 0.7802
</table>
<tableCaption confidence="0.999485">
Table 4: Result of uncertainty tweets identification
</tableCaption>
<bodyText confidence="0.988152823529412">
Twitter-specific feature set and ALL is the combi-
nation of C, U and T.
Table 4.1 shows that CP achieves the best recall
but its precision is the lowest. The learning based
methods with different feature sets give some sim-
ilar recalls. Compared to CP, SVMn−gram in-
creases the F-1 score by 43.9% due to the salient
improvement on precision and small drop of re-
call. The performance improves in terms of pre-
cision and F-1 score when the feature set is ex-
panded by adding C, U or T onto n-gram, where
+C brings the highest gain, and SVMn−gram+ALL
performs best in terms of precision and F-1 score.
We then study the effectiveness of the three
content-based features, and result shows that the
presence of uncertain cue-phrase is most indica-
tive for uncertainty tweet identification.
</bodyText>
<table confidence="0.99855975">
Type Poss. Prob. D.&amp;D. Cond. Que. Ext.
Total# 16 129 69 71 488 208
Error# 11 20 18 11 84 40
% 0.69 0.16 0.26 0.15 0.17 0.23
</table>
<tableCaption confidence="0.999235">
Table 5: Error distributions
</tableCaption>
<bodyText confidence="0.999958">
in Table 4.2. Our method performs worst on the
type of Possible and on the combination of Dy-
namic and Doxastic because these two types have
the least number of samples in the corpus and the
classifier tends to be undertrained without enough
samples.
</bodyText>
<sectionHeader confidence="0.97419" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999964928571428">
In this paper, we propose a variant of classification
scheme for uncertainty identification in social me-
dia and construct the first uncertainty corpus based
on tweets. We perform uncertainty identification
experiments on the generated dataset to explore
the effectiveness of different types of features. Re-
sult shows that the three categories of social media
specific features can improve uncertainty identifi-
cation. Furthermore, content-based features bring
the highest improvement among the three and the
presence of uncertain cue-phrase contributes most
for content-based features.
In future, we will explore to use uncertainty
identification for social media applications.
</bodyText>
<subsectionHeader confidence="0.922893">
4.2 Error analysis
</subsectionHeader>
<bodyText confidence="0.999979666666667">
We analyze the prediction errors based on
SVMn−gram+ALL. The distribution of errors in
terms of different types of uncertainty is shown
</bodyText>
<sectionHeader confidence="0.998346" genericHeader="acknowledgments">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.940761">
This work is partially supported by General Re-
</bodyText>
<page confidence="0.976952">
61
</page>
<bodyText confidence="0.548623">
earch Fund of Hong Kong (No. 417112).
</bodyText>
<sectionHeader confidence="0.977187" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993687547619048">
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: the kappa statistic. Computational
linguistics, 22(2):249–254.
Carlos Castillo, Marcelo Mendoza, and Barbara
Poblete. 2011. Information credibility on twitter.
In Proceedings of the 20th International Conference
on World Wide Web, pages 675–684.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010. Time is of the essence: im-
proving recency ranking using twitter data. In Pro-
ceedings of the 19th International Conference on
World Wide Web, pages 331–340. ACM.
Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos
Csirik, and Gy¨orgy Szarvas. 2010. The conll-
2010 shared task: learning to detect hedges and their
scope in natural language text. In Proceedings of
the 14th Conference on Computational Natural Lan-
guage Learning—Shared Task, pages 1–12. Associ-
ation for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding
hedges by chasing weasels: Hedge detection using
wikipedia tags and shallow linguistic features. In
Proceedings of the ACL-IJCNLP 2009, pages 173–
176. Association for Computational Linguistics.
Maria Georgescul. 2010. A hedgehop over a max-
margin framework using hedge cues. In Proceed-
ings of the 14th Conference on Computational Natu-
ral Language Learning—Shared Task, pages 26–31.
Association for Computational Linguistics.
Ferenc Kiefer. 2005. Lehetoseg es szuk-
segszeruseg[Possibility and necessity]. Tinta Kiado,
Budapest.
H. Kilicoglu and S. Bergler. 2008. Recognizing spec-
ulative language in biomedical research articles: a
linguistically motivated perspective. BMC bioinfor-
matics, 9(Suppl 11):S10.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The language of bioscience: Facts, specula-
tions, and statements in between. In Proceedings
of BioLink 2004 workshop on linking biological lit-
erature, ontologies and databases: tools for users,
pages 17–24.
B. Medlock and T. Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific litera-
ture. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
992–999.
Vahed Qazvinian, Emily Rosengren, Dragomir R
Radev, and Qiaozhu Mei. 2011. Rumor has it:
Identifying misinformation in microblogs. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1589–1599.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th International Conference on World Wide
Web, pages 851–860. ACM.
R. Saur´ı and J. Pustejovsky. 2009. Factbank: A cor-
pus annotated with event factuality. Language Re-
sources and Evaluation, 43(3):227–268.
Stephan Seifert and Werner Welte. 1987. A basic bib-
liography on negation in natural language, volume
313. Gunter Narr Verlag.
Gy¨orgy Szarvas, Veronika Vincze, Rich´ard Farkas,
Gy¨orgy M´ora, and Iryna Gurevych. 2012. Cross-
genre and cross-domain detection of semantic uncer-
tainty. Computational Linguistics, 38(2):335–367.
Gy¨orgy Szarvas. 2008. Hedge classification in
biomedical texts with a weakly supervised selection
of keywords. In Proceedings of 46th Annual Meet-
ing of the Association for Computational Linguis-
tics.
Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan,
and Shixi Fan. 2010. A cascade method for detect-
ing hedges and their scope in natural language text.
In Proceedings of the 14th Conference on Compu-
tational Natural Language Learning—Shared Task,
pages 13–17. Association for Computational Lin-
guistics.
V. Vincze, G. Szarvas, R. Farkas, G. M´ora, and
J. Csirik. 2008. The bioscope corpus: biomedical
texts annotated for uncertainty, negation and their
scopes. BMC bioinformatics, 9(Suppl 11):S9.
</reference>
<bodyText confidence="0.8712905">
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Infor-
matics, 41(4):636–654.
Roser Morante and Caroline Sporleder. 2012. Modal-
ity and negation: An introduction to the special is-
sue. Computational Linguistics, 38(2):223–260. 62
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.616587">
<title confidence="0.995994">An Empirical Study on Uncertainty Identification in Social Media Context</title>
<author confidence="0.90664">Junwen Wei Lanjun Yulan Kam-Fai</author>
<affiliation confidence="0.932791333333333">Chinese University of Hong Kong, Shatin, N.T., Hong Computing Research Institute, Qatar Foundation, Doha, of Engineering &amp; Applied Science, Aston University, Birmingham,</affiliation>
<email confidence="0.970933">wgao@qf.org.qa,y.he@cantab.net</email>
<abstract confidence="0.998820882352941">Uncertainty text detection is important to many social-media-based applications since more and more users utilize social media platforms (e.g., Twitter, Facebook, etc.) as information source to produce or derive interpretations based on them. However, existing uncertainty cues are ineffective in social media context because of its specific characteristics. In this paper, we propose a variant of annotation scheme for uncertainty identification and construct the first uncertainty corpus based on tweets. We then conduct experiments on the generated tweets corpus to study the effectiveness of different types of features for uncertainty text identification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Computational linguistics,</journal>
<pages>22--2</pages>
<contexts>
<context position="10794" citStr="Carletta, 1996" startWordPosition="1632" endWordPosition="1633">tated all the 4,743 extracted tweets for the seven events3. Two annotators were trained to annotate the dataset independently. Given a collection of tweets T = {t1, t2, t3...tnI, the annotation task is to label each tweet ti as either uncertain or certain. Uncertainty assertions are to be identified in terms of the judgements about the author’s intended meaning rather than the presence of uncertain cue-phrase. For those tweets annotated as uncertain, sub-class labels are also required according to the classification indicated in Table 3.1 (i.e., multi-label is allowed). The Kappa coefficient (Carletta, 1996) indicating inter-annotator agreement was 0.9073 for the certain/uncertain binary classification and was 0.8271 for fine-grained annotation. The conflict labels from the two annotators were resolved by a third annotator. Annotation result is displayed in Table 3.2, where 926 out of 4,743 tweets are labeled as uncertain accounting for 19.52%. Question is the uncertainty category with most tweets, followed by External. Only 21 tweets are labeled 3http://www.guardian.co.uk/ uk/interactive/2011/dec/07/ london-riots-twitter Uncertainty# 926 Possible# 16 Epistemic Probable# 129 Condition# 71 Doxasti</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification tasks: the kappa statistic. Computational linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Castillo</author>
<author>Marcelo Mendoza</author>
<author>Barbara Poblete</author>
</authors>
<title>Information credibility on twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th International Conference on World Wide Web,</booktitle>
<pages>675--684</pages>
<contexts>
<context position="2408" citStr="Castillo et al., 2011" startWordPosition="354" endWordPosition="357"> dataset which includes 326,747 posts (Details are given in Section 3) collected during 2011 London Riots, and result reveals that at least 18.91% of these tweets bear uncertainty characteristics&apos;. Therefore, distinguishing uncertain statements from factual ones is crucial for users to synthesize social media information to produce or derive reliable interpretations, &apos;The preliminary study was done based on a manually defined uncertainty cue-phrase list. Tweets containing at least one hedge cue were treated as uncertain. and this is expected helpful for applications like credibility analysis (Castillo et al., 2011) and rumor detection (Qazvinian et al., 2011) based on social media. Although uncertainty has been studied theoretically for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical t</context>
</contexts>
<marker>Castillo, Mendoza, Poblete, 2011</marker>
<rawString>Carlos Castillo, Marcelo Mendoza, and Barbara Poblete. 2011. Information credibility on twitter. In Proceedings of the 20th International Conference on World Wide Web, pages 675–684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anlei Dong</author>
<author>Ruiqiang Zhang</author>
<author>Pranam Kolari</author>
<author>Jing Bai</author>
<author>Fernando Diaz</author>
<author>Yi Chang</author>
<author>Zhaohui Zheng</author>
<author>Hongyuan Zha</author>
</authors>
<title>Time is of the essence: improving recency ranking using twitter data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th International Conference on World Wide Web,</booktitle>
<pages>331--340</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1512" citStr="Dong et al., 2010" startWordPosition="210" endWordPosition="213">ification and construct the first uncertainty corpus based on tweets. We then conduct experiments on the generated tweets corpus to study the effectiveness of different types of features for uncertainty text identification. 1 Introduction Social media is not only a social network tool for people to communicate but also plays an important role as information source with more and more users searching and browsing news on it. People also utilize information from social media for developing various applications, such as earthquake warning systems (Sakaki et al., 2010) and fresh webpage discovery (Dong et al., 2010). However, due to its casual and word-of-mouth peculiarities, the quality of information in social media in terms of factuality becomes a premier concern. Chances are there for uncertain information or even rumors flooding in such a context of free form. We analyzed a tweet dataset which includes 326,747 posts (Details are given in Section 3) collected during 2011 London Riots, and result reveals that at least 18.91% of these tweets bear uncertainty characteristics&apos;. Therefore, distinguishing uncertain statements from factual ones is crucial for users to synthesize social media information to </context>
</contexts>
<marker>Dong, Zhang, Kolari, Bai, Diaz, Chang, Zheng, Zha, 2010</marker>
<rawString>Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and Hongyuan Zha. 2010. Time is of the essence: improving recency ranking using twitter data. In Proceedings of the 19th International Conference on World Wide Web, pages 331–340. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Veronika Vincze</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>The conll2010 shared task: learning to detect hedges and their scope in natural language text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 14th Conference on Computational Natural Language Learning—Shared Task,</booktitle>
<pages>1--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Farkas, Vincze, M´ora, Csirik, Szarvas, 2010</marker>
<rawString>Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos Csirik, and Gy¨orgy Szarvas. 2010. The conll2010 shared task: learning to detect hedges and their scope in natural language text. In Proceedings of the 14th Conference on Computational Natural Language Learning—Shared Task, pages 1–12. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viola Ganter</author>
<author>Michael Strube</author>
</authors>
<title>Finding hedges by chasing weasels: Hedge detection using wikipedia tags and shallow linguistic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCNLP 2009,</booktitle>
<pages>173--176</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Ganter, Strube, 2009</marker>
<rawString>Viola Ganter and Michael Strube. 2009. Finding hedges by chasing weasels: Hedge detection using wikipedia tags and shallow linguistic features. In Proceedings of the ACL-IJCNLP 2009, pages 173– 176. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Georgescul</author>
</authors>
<title>A hedgehop over a maxmargin framework using hedge cues.</title>
<date>2010</date>
<booktitle>In Proceedings of the 14th Conference on Computational Natural Language Learning—Shared Task,</booktitle>
<pages>26--31</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6594" citStr="Georgescul, 2010" startWordPosition="983" endWordPosition="984">ork on uncertainty identification focused on classifying sentences into uncertain or definite categories. Existing approaches are mainly based on supervised methods (Light et al., 2004; Medlock and Briscoe, 2007; Medlock, 2008; Szarvas, 2008) using the annotated corpus with different types of features including Part-OfSpeech (POS) tags, stems, n-grams, etc.. Classification of uncertain sentences was consolidated as a task in the 2010 edition of CoNLL shared task on learning to detect hedge cues and their scope in natural language text (Farkas et al., 2010). The best system for Wikipedia data (Georgescul, 2010) employed Support Vector Machine (SVM), and the best system for biological data (Tang et al., 2010) adopted Conditional zhttp://www.timeml.org/site/timebank/ timebank.html Random Fields (CRF). In our work, we conduct an empirical study of uncertainty identification on tweets dataset and explore the effectiveness of different types of features (i.e., content-based, user-based and Twitterspecific) from social media context. 3 Uncertainty corpus for microblogs 3.1 Types of uncertainty in microblogs Traditionally, uncertainty can be divided into two categories, namely Epistemic and Hypothetical (K</context>
</contexts>
<marker>Georgescul, 2010</marker>
<rawString>Maria Georgescul. 2010. A hedgehop over a maxmargin framework using hedge cues. In Proceedings of the 14th Conference on Computational Natural Language Learning—Shared Task, pages 26–31. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferenc Kiefer</author>
</authors>
<title>Lehetoseg es szuksegszeruseg[Possibility and necessity]. Tinta Kiado,</title>
<date>2005</date>
<location>Budapest.</location>
<contexts>
<context position="7206" citStr="Kiefer, 2005" startWordPosition="1068" endWordPosition="1069">) employed Support Vector Machine (SVM), and the best system for biological data (Tang et al., 2010) adopted Conditional zhttp://www.timeml.org/site/timebank/ timebank.html Random Fields (CRF). In our work, we conduct an empirical study of uncertainty identification on tweets dataset and explore the effectiveness of different types of features (i.e., content-based, user-based and Twitterspecific) from social media context. 3 Uncertainty corpus for microblogs 3.1 Types of uncertainty in microblogs Traditionally, uncertainty can be divided into two categories, namely Epistemic and Hypothetical (Kiefer, 2005). For Epistemic, there are two sub-classes Possible and Probable. For Hypothetical, there are four sub-classes including Investigation, Condition, Doxastic and Dynamic. The detail of the classification is described as below (Kiefer, 2005): Epistemic: On the basis of our world knowledge we cannot decide at the moment whether the statement is true or false. Hypothetical: This type of uncertainty includes four sub-classes: • Doxastic: Expresses the speaker’s beliefs and hypotheses. • Investigation: Proposition under investigation. • Condition: Proposition under condition. • Dynamic: Contains deon</context>
<context position="9700" citStr="Kiefer, 2005" startWordPosition="1458" endWordPosition="1459">le, Friend who works at the children’s hospital in Birmingham says the riot police are protecting it. Based on these observations, we propose a variant of uncertainty types in social media context by eliminating the category of Investigation and adding the category of Question and External under Hypothetical, as shown in Table 3.1. Note that our proposed scheme is based on Kiefer’s work (2005) which was previously extended to normalize uncertainty corpora in different genres by Szarvas et al. (2012). But we did not try these extended schema for specific genres since even the most general one (Kiefer, 2005) was proved unsuitable for social media context. 3.2 Annotation result The dataset we annotated was collected from Twitter using Streaming API during summer riots in London during August 6-13 2011, including 326,747 tweets in total. Search criteria include hashtags like #ukriots, #londonriots, #prayforlondon, and so on. We further extracted the tweets relating to seven significant events during the riot identified by UK newspaper The Guardian from this set of tweets. We annotated all the 4,743 extracted tweets for the seven events3. Two annotators were trained to annotate the dataset independe</context>
</contexts>
<marker>Kiefer, 2005</marker>
<rawString>Ferenc Kiefer. 2005. Lehetoseg es szuksegszeruseg[Possibility and necessity]. Tinta Kiado, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kilicoglu</author>
<author>S Bergler</author>
</authors>
<title>Recognizing speculative language in biomedical research articles: a linguistically motivated perspective. BMC bioinformatics, 9(Suppl 11):S10.</title>
<date>2008</date>
<contexts>
<context position="3094" citStr="Kilicoglu and Bergler, 2008" startWordPosition="465" endWordPosition="468">al media. Although uncertainty has been studied theoretically for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise q</context>
</contexts>
<marker>Kilicoglu, Bergler, 2008</marker>
<rawString>H. Kilicoglu and S. Bergler. 2008. Recognizing speculative language in biomedical research articles: a linguistically motivated perspective. BMC bioinformatics, 9(Suppl 11):S10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
<author>Xin Ying Qiu</author>
<author>Padmini Srinivasan</author>
</authors>
<title>The language of bioscience: Facts, speculations, and statements in between.</title>
<date>2004</date>
<booktitle>In Proceedings of BioLink</booktitle>
<pages>17--24</pages>
<contexts>
<context position="3157" citStr="Light et al., 2004" startWordPosition="475" endWordPosition="478">g time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to external information when making uncertain</context>
<context position="6161" citStr="Light et al., 2004" startWordPosition="913" endWordPosition="916">eral uncertainty corpora exist, there is not a uniform set of standard for uncertainty annotation. Szarvas et al. (2012) normalized the annotation of the three corpora aforementioned. However, the context of these corpora is different from that of social media. Typically, these documents annotated are grammatically correct, carefully punctuated, formally structured and logically expressed. 2.2 Uncertainty identification Previous work on uncertainty identification focused on classifying sentences into uncertain or definite categories. Existing approaches are mainly based on supervised methods (Light et al., 2004; Medlock and Briscoe, 2007; Medlock, 2008; Szarvas, 2008) using the annotated corpus with different types of features including Part-OfSpeech (POS) tags, stems, n-grams, etc.. Classification of uncertain sentences was consolidated as a task in the 2010 edition of CoNLL shared task on learning to detect hedge cues and their scope in natural language text (Farkas et al., 2010). The best system for Wikipedia data (Georgescul, 2010) employed Support Vector Machine (SVM), and the best system for biological data (Tang et al., 2010) adopted Conditional zhttp://www.timeml.org/site/timebank/ timebank.</context>
</contexts>
<marker>Light, Qiu, Srinivasan, 2004</marker>
<rawString>Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 2004. The language of bioscience: Facts, speculations, and statements in between. In Proceedings of BioLink 2004 workshop on linking biological literature, ontologies and databases: tools for users, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Medlock</author>
<author>T Briscoe</author>
</authors>
<title>Weakly supervised learning for hedge classification in scientific literature.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>992--999</pages>
<contexts>
<context position="3121" citStr="Medlock and Briscoe, 2007" startWordPosition="469" endWordPosition="472">y has been studied theoretically for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to extern</context>
<context position="6188" citStr="Medlock and Briscoe, 2007" startWordPosition="917" endWordPosition="920">pora exist, there is not a uniform set of standard for uncertainty annotation. Szarvas et al. (2012) normalized the annotation of the three corpora aforementioned. However, the context of these corpora is different from that of social media. Typically, these documents annotated are grammatically correct, carefully punctuated, formally structured and logically expressed. 2.2 Uncertainty identification Previous work on uncertainty identification focused on classifying sentences into uncertain or definite categories. Existing approaches are mainly based on supervised methods (Light et al., 2004; Medlock and Briscoe, 2007; Medlock, 2008; Szarvas, 2008) using the annotated corpus with different types of features including Part-OfSpeech (POS) tags, stems, n-grams, etc.. Classification of uncertain sentences was consolidated as a task in the 2010 edition of CoNLL shared task on learning to detect hedge cues and their scope in natural language text (Farkas et al., 2010). The best system for Wikipedia data (Georgescul, 2010) employed Support Vector Machine (SVM), and the best system for biological data (Tang et al., 2010) adopted Conditional zhttp://www.timeml.org/site/timebank/ timebank.html Random Fields (CRF). I</context>
</contexts>
<marker>Medlock, Briscoe, 2007</marker>
<rawString>B. Medlock and T. Briscoe. 2007. Weakly supervised learning for hedge classification in scientific literature. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 992–999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vahed Qazvinian</author>
<author>Emily Rosengren</author>
<author>Dragomir R Radev</author>
<author>Qiaozhu Mei</author>
</authors>
<title>Rumor has it: Identifying misinformation in microblogs.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1589--1599</pages>
<contexts>
<context position="2453" citStr="Qazvinian et al., 2011" startWordPosition="362" endWordPosition="365">ls are given in Section 3) collected during 2011 London Riots, and result reveals that at least 18.91% of these tweets bear uncertainty characteristics&apos;. Therefore, distinguishing uncertain statements from factual ones is crucial for users to synthesize social media information to produce or derive reliable interpretations, &apos;The preliminary study was done based on a manually defined uncertainty cue-phrase list. Tweets containing at least one hedge cue were treated as uncertain. and this is expected helpful for applications like credibility analysis (Castillo et al., 2011) and rumor detection (Qazvinian et al., 2011) based on social media. Although uncertainty has been studied theoretically for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots </context>
</contexts>
<marker>Qazvinian, Rosengren, Radev, Mei, 2011</marker>
<rawString>Vahed Qazvinian, Emily Rosengren, Dragomir R Radev, and Qiaozhu Mei. 2011. Rumor has it: Identifying misinformation in microblogs. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1589–1599.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takeshi Sakaki</author>
<author>Makoto Okazaki</author>
<author>Yutaka Matsuo</author>
</authors>
<title>Earthquake shakes twitter users: real-time event detection by social sensors.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th International Conference on World Wide Web,</booktitle>
<pages>851--860</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1464" citStr="Sakaki et al., 2010" startWordPosition="202" endWordPosition="205">variant of annotation scheme for uncertainty identification and construct the first uncertainty corpus based on tweets. We then conduct experiments on the generated tweets corpus to study the effectiveness of different types of features for uncertainty text identification. 1 Introduction Social media is not only a social network tool for people to communicate but also plays an important role as information source with more and more users searching and browsing news on it. People also utilize information from social media for developing various applications, such as earthquake warning systems (Sakaki et al., 2010) and fresh webpage discovery (Dong et al., 2010). However, due to its casual and word-of-mouth peculiarities, the quality of information in social media in terms of factuality becomes a premier concern. Chances are there for uncertain information or even rumors flooding in such a context of free form. We analyzed a tweet dataset which includes 326,747 posts (Details are given in Section 3) collected during 2011 London Riots, and result reveals that at least 18.91% of these tweets bear uncertainty characteristics&apos;. Therefore, distinguishing uncertain statements from factual ones is crucial for </context>
</contexts>
<marker>Sakaki, Okazaki, Matsuo, 2010</marker>
<rawString>Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo. 2010. Earthquake shakes twitter users: real-time event detection by social sensors. In Proceedings of the 19th International Conference on World Wide Web, pages 851–860. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Saur´ı</author>
<author>J Pustejovsky</author>
</authors>
<title>Factbank: A corpus annotated with event factuality.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>3</issue>
<marker>Saur´ı, Pustejovsky, 2009</marker>
<rawString>R. Saur´ı and J. Pustejovsky. 2009. Factbank: A corpus annotated with event factuality. Language Resources and Evaluation, 43(3):227–268.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Seifert</author>
<author>Werner Welte</author>
</authors>
<title>A basic bibliography on negation in natural language, volume 313.</title>
<date>1987</date>
<publisher>Gunter Narr Verlag.</publisher>
<contexts>
<context position="2597" citStr="Seifert and Welte, 1987" startWordPosition="386" endWordPosition="389">eristics&apos;. Therefore, distinguishing uncertain statements from factual ones is crucial for users to synthesize social media information to produce or derive reliable interpretations, &apos;The preliminary study was done based on a manually defined uncertainty cue-phrase list. Tweets containing at least one hedge cue were treated as uncertain. and this is expected helpful for applications like credibility analysis (Castillo et al., 2011) and rumor detection (Qazvinian et al., 2011) based on social media. Although uncertainty has been studied theoretically for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in</context>
</contexts>
<marker>Seifert, Welte, 1987</marker>
<rawString>Stephan Seifert and Werner Welte. 1987. A basic bibliography on negation in natural language, volume 313. Gunter Narr Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gy¨orgy Szarvas</author>
<author>Veronika Vincze</author>
<author>Rich´ard Farkas</author>
<author>Gy¨orgy M´ora</author>
<author>Iryna Gurevych</author>
</authors>
<title>Crossgenre and cross-domain detection of semantic uncertainty.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>2</issue>
<marker>Szarvas, Vincze, Farkas, M´ora, Gurevych, 2012</marker>
<rawString>Gy¨orgy Szarvas, Veronika Vincze, Rich´ard Farkas, Gy¨orgy M´ora, and Iryna Gurevych. 2012. Crossgenre and cross-domain detection of semantic uncertainty. Computational Linguistics, 38(2):335–367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>Hedge classification in biomedical texts with a weakly supervised selection of keywords.</title>
<date>2008</date>
<booktitle>In Proceedings of 46th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3136" citStr="Szarvas, 2008" startWordPosition="473" endWordPosition="474">cally for a long time as a grammatical phenomena (Seifert and Welte, 1987), the computational treatment of uncertainty is a newly emerging area of research. Szarvas et al. (2012) pointed out that “Uncertainty - in its most general sense - can be interpreted as lack of information: the receiver of the information (i.e., the hearer or the reader) cannot be certain about some pieces of information”. In recent years, the identification of uncertainty in formal text, e.g., biomedical text, reviews or newswire, has attracted lots of attention (Kilicoglu and Bergler, 2008; Medlock and Briscoe, 2007; Szarvas, 2008; Light et al., 2004). However, uncertainty identification in social media context is rarely explored. Previous research shows that uncertainty identification is domain dependent as the usage of hedge cues varies widely in different domains (Morante and Sporleder, 2012). Therefore, the employment of existing out-of-domain corpus to social media context is ineffective. Furthermore, compared to the existing uncertainty corpus, the expression of uncertainty in social media is fairly different from that in formal text in a sense that people usually raise questions or refer to external information </context>
<context position="6219" citStr="Szarvas, 2008" startWordPosition="923" endWordPosition="924">standard for uncertainty annotation. Szarvas et al. (2012) normalized the annotation of the three corpora aforementioned. However, the context of these corpora is different from that of social media. Typically, these documents annotated are grammatically correct, carefully punctuated, formally structured and logically expressed. 2.2 Uncertainty identification Previous work on uncertainty identification focused on classifying sentences into uncertain or definite categories. Existing approaches are mainly based on supervised methods (Light et al., 2004; Medlock and Briscoe, 2007; Medlock, 2008; Szarvas, 2008) using the annotated corpus with different types of features including Part-OfSpeech (POS) tags, stems, n-grams, etc.. Classification of uncertain sentences was consolidated as a task in the 2010 edition of CoNLL shared task on learning to detect hedge cues and their scope in natural language text (Farkas et al., 2010). The best system for Wikipedia data (Georgescul, 2010) employed Support Vector Machine (SVM), and the best system for biological data (Tang et al., 2010) adopted Conditional zhttp://www.timeml.org/site/timebank/ timebank.html Random Fields (CRF). In our work, we conduct an empir</context>
</contexts>
<marker>Szarvas, 2008</marker>
<rawString>Gy¨orgy Szarvas. 2008. Hedge classification in biomedical texts with a weakly supervised selection of keywords. In Proceedings of 46th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Buzhou Tang</author>
<author>Xiaolong Wang</author>
<author>Xuan Wang</author>
<author>Bo Yuan</author>
<author>Shixi Fan</author>
</authors>
<title>A cascade method for detecting hedges and their scope in natural language text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 14th Conference on Computational Natural Language Learning—Shared Task,</booktitle>
<pages>13--17</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6693" citStr="Tang et al., 2010" startWordPosition="998" endWordPosition="1001">gories. Existing approaches are mainly based on supervised methods (Light et al., 2004; Medlock and Briscoe, 2007; Medlock, 2008; Szarvas, 2008) using the annotated corpus with different types of features including Part-OfSpeech (POS) tags, stems, n-grams, etc.. Classification of uncertain sentences was consolidated as a task in the 2010 edition of CoNLL shared task on learning to detect hedge cues and their scope in natural language text (Farkas et al., 2010). The best system for Wikipedia data (Georgescul, 2010) employed Support Vector Machine (SVM), and the best system for biological data (Tang et al., 2010) adopted Conditional zhttp://www.timeml.org/site/timebank/ timebank.html Random Fields (CRF). In our work, we conduct an empirical study of uncertainty identification on tweets dataset and explore the effectiveness of different types of features (i.e., content-based, user-based and Twitterspecific) from social media context. 3 Uncertainty corpus for microblogs 3.1 Types of uncertainty in microblogs Traditionally, uncertainty can be divided into two categories, namely Epistemic and Hypothetical (Kiefer, 2005). For Epistemic, there are two sub-classes Possible and Probable. For Hypothetical, the</context>
</contexts>
<marker>Tang, Wang, Wang, Yuan, Fan, 2010</marker>
<rawString>Buzhou Tang, Xiaolong Wang, Xuan Wang, Bo Yuan, and Shixi Fan. 2010. A cascade method for detecting hedges and their scope in natural language text. In Proceedings of the 14th Conference on Computational Natural Language Learning—Shared Task, pages 13–17. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vincze</author>
<author>G Szarvas</author>
<author>R Farkas</author>
<author>G M´ora</author>
<author>J Csirik</author>
</authors>
<title>The bioscope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMC bioinformatics, 9(Suppl 11):S9.</title>
<date>2008</date>
<marker>Vincze, Szarvas, Farkas, M´ora, Csirik, 2008</marker>
<rawString>V. Vincze, G. Szarvas, R. Farkas, G. M´ora, and J. Csirik. 2008. The bioscope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMC bioinformatics, 9(Suppl 11):S9.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>