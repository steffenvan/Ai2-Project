<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.187382">
<title confidence="0.978077">
The Role of Pseudo References in MT Evaluation
</title>
<author confidence="0.974662">
Joshua S. Albrecht and Rebecca Hwa
</author>
<affiliation confidence="0.9983165">
Department of Computer Science
University of Pittsburgh
</affiliation>
<email confidence="0.99922">
{jsa8,hwa}@cs.pitt.edu
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999457">
Previous studies have shown automatic evalu-
ation metrics to be more reliable when com-
pared against many human translations. How-
ever, multiple human references may not al-
ways be available. It is more common to have
only a single human reference (extracted from
parallel texts) or no reference at all. Our ear-
lier work suggested that one way to address
this problem is to train a metric to evaluate a
sentence by comparing it against pseudo refer-
ences, or imperfect “references” produced by
off-the-shelf MT systems. In this paper, we
further examine the approach both in terms of
the training methodology and in terms of the
role of the human and pseudo references. Our
expanded experiments show that the approach
generalizes well across multiple years and dif-
ferent source languages.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913315789474">
Standard automatic metrics are reference-based;
that is, they compare system-produced translations
against human-translated references produced for
the same source. Since there is usually no single
best way to translate a sentence, each MT output
should be compared against many references. On
the other hand, creating multiple human references
is itself a costly process. For many naturally occur-
ring datasets (e.g., parallel corpora) only a single ref-
erence is readily available.
The focus of this work is on developing auto-
matic metrics for sentence-level evaluation with at
most one human reference. One way to supple-
ment the single human reference is to use pseudo
references, or sentences produced by off-the-shelf
MT systems, as stand-ins for human references.
However, since pseudo references may be imperfect
translations themselves, the comparisons cannot be
fully trusted. Previously, we have taken a learning-
based approach to develop a composite metric that
combines measurements taken from multiple pseudo
references (Albrecht and Hwa, 2007). Experimental
results suggested the approach to be promising; but
those studies did not consider how well the metric
might generalize across multiple years and different
languages. In this paper, we investigate the appli-
cability of the pseudo-reference metrics under these
more general conditions.
Using the WMT06 Workshop shared-task re-
sults (Koehn and Monz, 2006) as training exam-
ples, we train a metric that evaluates new sentences
by comparing them against pseudo references pro-
duced by three off-the-shelf MT systems. We ap-
ply the learned metric to sentences from the WMT07
shared-task (Callison-Burch et al., 2007b) and com-
pare the metric’s predictions against human judg-
ments. We find that additional pseudo references
improve correlations for automatic metrics.
</bodyText>
<sectionHeader confidence="0.991427" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.9996425">
The ideal evaluation metric reports an accurate dis-
tance between an input instance and its gold stan-
dard, but even when comparing against imperfect
standards, the measured distances may still convey
some useful information – they may help to trian-
gulate the input’s position relative to the true gold
standard.
In the context of sentence-level MT evaluations,
</bodyText>
<page confidence="0.9716">
187
</page>
<note confidence="0.400654">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 187–190,
</note>
<page confidence="0.479515">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999962947368421">
the challenges are two-fold. First, the ideal quantita-
tive distance function between a translation hypoth-
esis and the proper translations is not known; cur-
rent automatic evaluation metrics produce approxi-
mations to the true translational distance. Second,
although we may know the qualitative goodness of
the MT systems that generate the pseudo references,
we do not know how imperfect the pseudo refer-
ences are. These uncertainties make it harder to es-
tablish the true distance between the input hypoth-
esis and the (unobserved) acceptable gold standard
translations.
In order to combine evidence from these uncertain
observations, we take a learning-based approach.
Each hypothesis sentence is compared with multi-
ple pseudo references using multiple metrics. Rep-
resenting the measurements as a set of input features
and using human-assessed MT sentences as training
examples, we train a function that is optimized to
correlate the features with the human assessments in
the training examples. Specifically, for each input
sentence, we compute a set of 18 kinds of reference-
based measurements for each pseudo reference as
well as 26 monolingual fluency measurements. The
full set of measurements then serves as the input fea-
ture vector into the function, which is trained via
support vector regression. The learned function can
then be used as an evaluation metric itself: it takes
the measurements of a new sentence as input and re-
turns a composite score for that sentence.
The approach is considered successful if the met-
ric’s predictions on new test sentences correlate well
with quantitative human assessments. Like other
learned models, the metric is expected to perform
better on data that are more similar to the training
instances. Therefore, a natural question that arises
with a metric developed in this manner is: how well
does it generalize?
</bodyText>
<sectionHeader confidence="0.994569" genericHeader="method">
3 Research Questions
</sectionHeader>
<bodyText confidence="0.999923193548387">
To better understand the capability of metrics that
compare against pseudo-references, we consider the
following aspects:
The role of learning Standard reference-based
metrics can also use pseudo references; however,
they would treat the imperfect references as gold
standard. In contrast, the learning process aims
to determine how much each comparison with a
pseudo reference might be trusted. To observe the
role of learning, we compare trained metrics against
standard reference-based metrics, all using pseudo
references.
The amount vs. types of training data The suc-
cess of any learned model depends on its training ex-
periences. We study the trade-off between the size
of the training set and the specificity of the train-
ing data. We perform experiments comparing a met-
ric trained from a large pool of heterogeneous train-
ing examples that include translated sentences from
multiple languages and individual metrics trained
from particular source languages.
The role of a single human reference Previous
studies have shown the importance of comparing
against multiple references. The approach in this
paper attempts to approximate multiple human ref-
erences with machine-produced sentences. Is a sin-
gle trust-worthy translation more useful than multi-
ple imperfect translations? To answer this question,
we compare three different reference settings: using
just a single human reference, using just the three
pseudo references, and using all four references.
</bodyText>
<sectionHeader confidence="0.999421" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.99999015">
For the experiments reported in this paper, we used
human-evaluated MT sentences from past shared-
tasks of the WMT 2006 and WMT 2007. The data
consists of outputs from German-English, Spanish-
English, and French-English MT systems. The out-
puts are translations from two corpora: Europarl and
news commentary. System outputs have been evalu-
ated by human judges on a 5-point scale (Callison-
Burch et al., 2007a). We have normalized scores
to reduce biases from different judges (Blatz et al.,
2003).
We experimented with using four different sub-
sets of the WMT2006 data as training examples:
only German-English, only Spanish-English, only
French-English, all 06 data. The metrics are trained
using support vector regression with a Gaussian
kernel as implemented in the SVM-Light package
(Joachims, 1999). The SVM parameters are tuned
via grid-search on development data, 20% of the full
training set that has been reserved for this purpose.
</bodyText>
<page confidence="0.994033">
188
</page>
<bodyText confidence="0.999639545454546">
We used three MT systems to generate pseudo ref-
erences: Systran1, GoogleMT 2, and Moses (Koehn
et al., 2007). We chose these three systems because
they are widely accessible and because they take
relatively different approaches. Moreover, although
they have not all been human-evaluated in the past
WMT shared tasks, they are well-known for produc-
ing good translations.
A metric is evaluated based on its Spearman rank
correlation coefficient between the scores it gave to
the evaluative dataset and human assessments for
the same data. The correlation coefficient is a real
number between -1, indicating perfect negative cor-
relations, and +1, indicating perfect positive correla-
tions.
Two standard reference-based metrics, BLEU
(Papineni et al., 2002) and METEOR (Banerjee and
Lavie, 2005), are used for comparisons. BLEU is
smoothed (Lin and Och, 2004), and it considers only
matching up to bigrams because this has higher cor-
relations with human judgments than when higher-
ordered n-grams are included.
</bodyText>
<sectionHeader confidence="0.999954" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.9999836">
The full experimental comparisons are summarized
in Table 1. Each cell shows the correlation coef-
ficient between the human judgments and a metric
(column) that uses a particular kind of references
(row) for some evaluation data set (block row).
The role of learning With the exception of the
German-English data, the learned metrics had higher
correlations with human judges than the baselines,
which used standard metrics with a single human
reference. On the other hand, results suggest that
pseudo references often also improve correlations
for standard metrics. This may seem counter-
intuitive because we can easily think of cases in
which pseudo references hurt standard metrics (e.g.,
use poor outputs as pseudo references). We hypoth-
</bodyText>
<footnote confidence="0.999693666666667">
1Available from http://www.systransoft.com/.
We note that Systran is also a participating system under eval-
uation. Although Sys-Test will be deemed to be identical to
Sys-Ref, it will not automatically receive a high score because
the measurement is weighted by whether Sys-Ref was reliable
during training. Furthermore, measurements between Sys-Test
and other pseudo-references will provide alternative evidences
for the metric to consider.
2http://www.google.com/language tools/
</footnote>
<bodyText confidence="0.99974952">
esize that because the pseudo references came from
high-quality MT systems and because standard met-
rics are based on simple word matches, the chances
for bad judgments (input words matched against
pseudo reference, but both are wrong) are relatively
small compared to chances for good judgments. We
further hypothesize that the learned metrics would
be robust against the qualities of the pseudo refer-
ence MT systems.
The amount vs. types of training data Com-
paring the three metrics trained from single lan-
guage datasets against the metric trained from all
of WMT06 dataset, we see that the learning process
benefitted from the larger quantity of training exam-
ples. It may be the case that the MT systems for the
three language pairs are at a similar stage of maturity
such that the training instances are mutually helpful.
The role of a single human reference Our results
reinforce previous findings that metrics are more re-
liable when they have access to more than a sin-
gle human reference. Our experimental data sug-
gests that a single human reference often may not be
as reliable as using three pseudo references alone.
Finally, the best correlations are achieved by using
both human and pseudo references.
</bodyText>
<sectionHeader confidence="0.996904" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999981666666667">
We have presented an empirical study on automatic
metrics for sentence-level MT evaluation with at
most one human reference. We show that pseudo
references from off-the-shelf MT systems can be
used to augment the single human reference. Be-
cause they are imperfect, it is important to weigh the
trustworthiness of these references through a train-
ing phase. The metric seems robust even when the
applied to sentences from different systems of a later
year. These results suggest that multiple imperfect
translations make informative comparison points in
supplement to human references.
</bodyText>
<sectionHeader confidence="0.998829" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.966374">
This work has been supported by NSF Grants IIS-
0612791.
</bodyText>
<page confidence="0.994499">
189
</page>
<table confidence="0.999781842105263">
Eval. Data Ref Type METEOR BLEU SVM(de06) SVM(es06) SVM(fr06) SVM(wmt06)
de 1HR 0.458 0.471
europarl 3PR 0.521* 0.527* 0.422 0.403 0.480* 0.467
07 1HR+3PR 0.535* 0.547* 0.471 0.480* 0.477* 0.523*
de 1HR 0.290 0.333
news 3PR 0.400* 0.400* 0.262 0.279 0.261 0.261
07 1HR+3PR 0.432* 0.417* 0.298 0.321 0.269 0.330
es 1HR 0.377 0.412
europarl 3PR 0.453* 0.483* 0.336 0.453* 0.432* 0.456*
07 1HR+3PR 0.491* 0.503* 0.405 0.513* 0.483* 0.510*
es 1HR 0.317 0.332
news 3PR 0.320 0.317 0.393* 0.381* 0.426* 0.426*
07 1HR+3PR 0.353* 0.325 0.429* 0.427* 0.380* 0.486*
fr 1HR 0.265 0.246
europarl 3PR 0.196 0.285* 0.270* 0.284* 0.355* 0.366*
07 1HR+3PR 0.221 0.290* 0.277* 0.324* 0.304* 0.381*
fr 1HR 0.226 0.280
news 3PR 0.356* 0.383* 0.237 0.252 0.355* 0.373*
07 1HR+3PR 0.374* 0.394* 0.272 0.339* 0.319* 0.388*
</table>
<tableCaption confidence="0.974834">
Table 1: Correlation comparisons of metrics (columns) using different references (row): a single human reference
(1HR), 3 pseudo references (3PR), or all (1HR+3PR). The type of training used for the regression-trained metrics
are specified in parentheses. For each evaluated corpus, correlations higher than standard metric using one human
reference are marked by an asterisk(*).
</tableCaption>
<sectionHeader confidence="0.997121" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999448038461538">
Joshua S. Albrecht and Rebecca Hwa. 2007. Regression
for sentence-level MT evaluation with pseudo refer-
ences. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics (ACL-
2007).
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for MT evaluation with improved
correlation with human judgments. In ACL 2005
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for Machine Translation and/or Summarization,
June.
John Blatz, Erin Fitzgerald, George Foster, Simona Gan-
drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,
and Nicola Ueffing. 2003. Confidence estimation
for machine translation. Technical Report Natural
Language Engineering Workshop Final Report, Johns
Hopkins University.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007a. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136–158, Prague, Czech Republic, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Cameron Shaw
Fordyce, and Christof Monz, editors. 2007b. Proceed-
ings of the Second Workshop on Statistical Machine
Translation. Association for Computational Linguis-
tics, Prague, Czech Republic, June.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In Bernhard Sch¨oelkopf, Christo-
pher Burges, and Alexander Smola, editors, Advances
in Kernel Methods - Support Vector Learning. MIT
Press.
Philipp Koehn and Christof Monz, editors. 2006. Pro-
ceedings on the Workshop on Statistical Machine
Translation. Association for Computational Linguis-
tics, New York City, June.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, et al. 2007. Moses: Open source
toolkit for statistical machine translation. Proceedings
ofACL, Demonstration Session.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING 2004), August.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, Philadelphia, PA.
</reference>
<page confidence="0.997863">
190
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.979883">
<title confidence="0.999936">The Role of Pseudo References in MT Evaluation</title>
<author confidence="0.999942">S Albrecht</author>
<affiliation confidence="0.998153">Department of Computer University of</affiliation>
<abstract confidence="0.999124789473684">Previous studies have shown automatic evaluation metrics to be more reliable when compared against many human translations. However, multiple human references may not always be available. It is more common to have only a single human reference (extracted from parallel texts) or no reference at all. Our earlier work suggested that one way to address this problem is to train a metric to evaluate a by comparing it against referor imperfect “references” produced by off-the-shelf MT systems. In this paper, we further examine the approach both in terms of the training methodology and in terms of the role of the human and pseudo references. Our expanded experiments show that the approach generalizes well across multiple years and different source languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Joshua S Albrecht</author>
<author>Rebecca Hwa</author>
</authors>
<title>Regression for sentence-level MT evaluation with pseudo references.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL2007).</booktitle>
<contexts>
<context position="2023" citStr="Albrecht and Hwa, 2007" startWordPosition="307" endWordPosition="310">ora) only a single reference is readily available. The focus of this work is on developing automatic metrics for sentence-level evaluation with at most one human reference. One way to supplement the single human reference is to use pseudo references, or sentences produced by off-the-shelf MT systems, as stand-ins for human references. However, since pseudo references may be imperfect translations themselves, the comparisons cannot be fully trusted. Previously, we have taken a learningbased approach to develop a composite metric that combines measurements taken from multiple pseudo references (Albrecht and Hwa, 2007). Experimental results suggested the approach to be promising; but those studies did not consider how well the metric might generalize across multiple years and different languages. In this paper, we investigate the applicability of the pseudo-reference metrics under these more general conditions. Using the WMT06 Workshop shared-task results (Koehn and Monz, 2006) as training examples, we train a metric that evaluates new sentences by comparing them against pseudo references produced by three off-the-shelf MT systems. We apply the learned metric to sentences from the WMT07 shared-task (Calliso</context>
</contexts>
<marker>Albrecht, Hwa, 2007</marker>
<rawString>Joshua S. Albrecht and Rebecca Hwa. 2007. Regression for sentence-level MT evaluation with pseudo references. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<contexts>
<context position="8426" citStr="Banerjee and Lavie, 2005" startWordPosition="1290" endWordPosition="1293">cessible and because they take relatively different approaches. Moreover, although they have not all been human-evaluated in the past WMT shared tasks, they are well-known for producing good translations. A metric is evaluated based on its Spearman rank correlation coefficient between the scores it gave to the evaluative dataset and human assessments for the same data. The correlation coefficient is a real number between -1, indicating perfect negative correlations, and +1, indicating perfect positive correlations. Two standard reference-based metrics, BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), are used for comparisons. BLEU is smoothed (Lin and Och, 2004), and it considers only matching up to bigrams because this has higher correlations with human judgments than when higherordered n-grams are included. 5 Results The full experimental comparisons are summarized in Table 1. Each cell shows the correlation coefficient between the human judgments and a metric (column) that uses a particular kind of references (row) for some evaluation data set (block row). The role of learning With the exception of the German-English data, the learned metrics had higher correlations with human judges </context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for MT evaluation with improved correlation with human judgments. In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2003</date>
<tech>Technical Report Natural Language Engineering Workshop Final Report,</tech>
<institution>Johns Hopkins University.</institution>
<contexts>
<context position="7188" citStr="Blatz et al., 2003" startWordPosition="1101" endWordPosition="1104">ng just a single human reference, using just the three pseudo references, and using all four references. 4 Experimental Setup For the experiments reported in this paper, we used human-evaluated MT sentences from past sharedtasks of the WMT 2006 and WMT 2007. The data consists of outputs from German-English, SpanishEnglish, and French-English MT systems. The outputs are translations from two corpora: Europarl and news commentary. System outputs have been evaluated by human judges on a 5-point scale (CallisonBurch et al., 2007a). We have normalized scores to reduce biases from different judges (Blatz et al., 2003). We experimented with using four different subsets of the WMT2006 data as training examples: only German-English, only Spanish-English, only French-English, all 06 data. The metrics are trained using support vector regression with a Gaussian kernel as implemented in the SVM-Light package (Joachims, 1999). The SVM parameters are tuned via grid-search on development data, 20% of the full training set that has been reserved for this purpose. 188 We used three MT systems to generate pseudo references: Systran1, GoogleMT 2, and Moses (Koehn et al., 2007). We chose these three systems because they </context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2003</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. 2003. Confidence estimation for machine translation. Technical Report Natural Language Engineering Workshop Final Report, Johns Hopkins University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>136--158</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2643" citStr="Callison-Burch et al., 2007" startWordPosition="402" endWordPosition="405">, 2007). Experimental results suggested the approach to be promising; but those studies did not consider how well the metric might generalize across multiple years and different languages. In this paper, we investigate the applicability of the pseudo-reference metrics under these more general conditions. Using the WMT06 Workshop shared-task results (Koehn and Monz, 2006) as training examples, we train a metric that evaluates new sentences by comparing them against pseudo references produced by three off-the-shelf MT systems. We apply the learned metric to sentences from the WMT07 shared-task (Callison-Burch et al., 2007b) and compare the metric’s predictions against human judgments. We find that additional pseudo references improve correlations for automatic metrics. 2 Background The ideal evaluation metric reports an accurate distance between an input instance and its gold standard, but even when comparing against imperfect standards, the measured distances may still convey some useful information – they may help to triangulate the input’s position relative to the true gold standard. In the context of sentence-level MT evaluations, 187 Proceedings of the Third Workshop on Statistical Machine Translation, pa</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2007a. (meta-) evaluation of machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 136–158, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<date></date>
<booktitle>2007b. Proceedings of the Second Workshop on Statistical Machine Translation. Association for Computational Linguistics,</booktitle>
<editor>Chris Callison-Burch, Philipp Koehn, Cameron Shaw Fordyce, and Christof Monz, editors.</editor>
<location>Prague, Czech Republic,</location>
<marker></marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Cameron Shaw Fordyce, and Christof Monz, editors. 2007b. Proceedings of the Second Workshop on Statistical Machine Translation. Association for Computational Linguistics, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making large-scale SVM learning practical.</title>
<date>1999</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning.</booktitle>
<editor>In Bernhard Sch¨oelkopf, Christopher Burges, and Alexander Smola, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="7494" citStr="Joachims, 1999" startWordPosition="1147" endWordPosition="1148">panishEnglish, and French-English MT systems. The outputs are translations from two corpora: Europarl and news commentary. System outputs have been evaluated by human judges on a 5-point scale (CallisonBurch et al., 2007a). We have normalized scores to reduce biases from different judges (Blatz et al., 2003). We experimented with using four different subsets of the WMT2006 data as training examples: only German-English, only Spanish-English, only French-English, all 06 data. The metrics are trained using support vector regression with a Gaussian kernel as implemented in the SVM-Light package (Joachims, 1999). The SVM parameters are tuned via grid-search on development data, 20% of the full training set that has been reserved for this purpose. 188 We used three MT systems to generate pseudo references: Systran1, GoogleMT 2, and Moses (Koehn et al., 2007). We chose these three systems because they are widely accessible and because they take relatively different approaches. Moreover, although they have not all been human-evaluated in the past WMT shared tasks, they are well-known for producing good translations. A metric is evaluated based on its Spearman rank correlation coefficient between the sco</context>
</contexts>
<marker>Joachims, 1999</marker>
<rawString>Thorsten Joachims. 1999. Making large-scale SVM learning practical. In Bernhard Sch¨oelkopf, Christopher Burges, and Alexander Smola, editors, Advances in Kernel Methods - Support Vector Learning. MIT Press.</rawString>
</citation>
<citation valid="true">
<date>2006</date>
<booktitle>Proceedings on the Workshop on Statistical Machine Translation. Association for Computational Linguistics,</booktitle>
<editor>Philipp Koehn and Christof Monz, editors.</editor>
<location>New York City,</location>
<marker>2006</marker>
<rawString>Philipp Koehn and Christof Monz, editors. 2006. Proceedings on the Workshop on Statistical Machine Translation. Association for Computational Linguistics, New York City, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
<author>H Hoang</author>
<author>A Birch</author>
<author>C Callison-Burch</author>
<author>M Federico</author>
<author>N Bertoldi</author>
<author>B Cowan</author>
<author>W Shen</author>
<author>C Moran</author>
<author>R Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>Proceedings ofACL, Demonstration Session.</booktitle>
<contexts>
<context position="7744" citStr="Koehn et al., 2007" startWordPosition="1188" endWordPosition="1191">res to reduce biases from different judges (Blatz et al., 2003). We experimented with using four different subsets of the WMT2006 data as training examples: only German-English, only Spanish-English, only French-English, all 06 data. The metrics are trained using support vector regression with a Gaussian kernel as implemented in the SVM-Light package (Joachims, 1999). The SVM parameters are tuned via grid-search on development data, 20% of the full training set that has been reserved for this purpose. 188 We used three MT systems to generate pseudo references: Systran1, GoogleMT 2, and Moses (Koehn et al., 2007). We chose these three systems because they are widely accessible and because they take relatively different approaches. Moreover, although they have not all been human-evaluated in the past WMT shared tasks, they are well-known for producing good translations. A metric is evaluated based on its Spearman rank correlation coefficient between the scores it gave to the evaluative dataset and human assessments for the same data. The correlation coefficient is a real number between -1, indicating perfect negative correlations, and +1, indicating perfect positive correlations. Two standard reference</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, et al. 2007. Moses: Open source toolkit for statistical machine translation. Proceedings ofACL, Demonstration Session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Orange: a method for evaluating automatic evaluation metrics for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING</booktitle>
<contexts>
<context position="8490" citStr="Lin and Och, 2004" startWordPosition="1301" endWordPosition="1304">r, although they have not all been human-evaluated in the past WMT shared tasks, they are well-known for producing good translations. A metric is evaluated based on its Spearman rank correlation coefficient between the scores it gave to the evaluative dataset and human assessments for the same data. The correlation coefficient is a real number between -1, indicating perfect negative correlations, and +1, indicating perfect positive correlations. Two standard reference-based metrics, BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), are used for comparisons. BLEU is smoothed (Lin and Och, 2004), and it considers only matching up to bigrams because this has higher correlations with human judgments than when higherordered n-grams are included. 5 Results The full experimental comparisons are summarized in Table 1. Each cell shows the correlation coefficient between the human judgments and a metric (column) that uses a particular kind of references (row) for some evaluation data set (block row). The role of learning With the exception of the German-English data, the learned metrics had higher correlations with human judges than the baselines, which used standard metrics with a single hu</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin and Franz Josef Och. 2004. Orange: a method for evaluating automatic evaluation metrics for machine translation. In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004), August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="8388" citStr="Papineni et al., 2002" startWordPosition="1284" endWordPosition="1287"> systems because they are widely accessible and because they take relatively different approaches. Moreover, although they have not all been human-evaluated in the past WMT shared tasks, they are well-known for producing good translations. A metric is evaluated based on its Spearman rank correlation coefficient between the scores it gave to the evaluative dataset and human assessments for the same data. The correlation coefficient is a real number between -1, indicating perfect negative correlations, and +1, indicating perfect positive correlations. Two standard reference-based metrics, BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), are used for comparisons. BLEU is smoothed (Lin and Och, 2004), and it considers only matching up to bigrams because this has higher correlations with human judgments than when higherordered n-grams are included. 5 Results The full experimental comparisons are summarized in Table 1. Each cell shows the correlation coefficient between the human judgments and a metric (column) that uses a particular kind of references (row) for some evaluation data set (block row). The role of learning With the exception of the German-English data, the learned metrics had </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Philadelphia, PA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>