<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000042">
<title confidence="0.96063">
Uncertainty Detection as Approximate Max-Margin Sequence Labelling
</title>
<author confidence="0.690995">
Oscar T¨ackstr¨om
</author>
<affiliation confidence="0.474163">
SICS / Uppsala University
Kista / Uppsala, Sweden
</affiliation>
<email confidence="0.857286">
oscar@sics.se
</email>
<author confidence="0.696622">
Gunnar Eriksson
</author>
<affiliation confidence="0.559491">
SICS
</affiliation>
<address confidence="0.537955">
Kista, Sweden
</address>
<email confidence="0.98746">
guer@sics.se
</email>
<author confidence="0.965449">
Sumithra Velupillai
</author>
<affiliation confidence="0.798361">
DSV, Stockholm University
Kista, Sweden
</affiliation>
<email confidence="0.967267">
sumithra@dsv.su.se
</email>
<author confidence="0.988089">
Hercules Dalianis
</author>
<affiliation confidence="0.802801">
DSV, Stockholm University
Kista, Sweden
</affiliation>
<email confidence="0.989441">
hercules@dsv.su.se
</email>
<author confidence="0.994315">
Martin Hassel
</author>
<affiliation confidence="0.8141665">
DSV, Stockholm University
Kista, Sweden
</affiliation>
<email confidence="0.959622">
xmartin@dsv.su.se
</email>
<author confidence="0.809976">
Jussi Karlgren
</author>
<affiliation confidence="0.646007">
SICS
</affiliation>
<address confidence="0.546607">
Kista, Sweden
</address>
<email confidence="0.984412">
jussi@sics.se
</email>
<sectionHeader confidence="0.993593" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995839">
This paper reports experiments for the
CoNLL-2010 shared task on learning to
detect hedges and their scope in natu-
ral language text. We have addressed
the experimental tasks as supervised lin-
ear maximum margin prediction prob-
lems. For sentence level hedge detection
in the biological domain we use an L1-
regularised binary support vector machine,
while for sentence level weasel detection
in the Wikipedia domain, we use an L2-
regularised approach. We model the in-
sentence uncertainty cue and scope de-
tection task as an L2-regularised approxi-
mate maximum margin sequence labelling
problem, using the BIO-encoding. In ad-
dition to surface level features, we use a
variety of linguistic features based on a
functional dependency analysis. A greedy
forward selection strategy is used in ex-
ploring the large set of potential features.
Our official results for Task 1 for the bio-
logical domain are 85.2 F1-score, for the
Wikipedia set 55.4 F1-score. For Task 2,
our official results are 2.1 for the entire
task with a score of 62.5 for cue detec-
tion. After resolving errors and final bugs,
our final results are for Task 1, biologi-
cal: 86.0, Wikipedia: 58.2; Task 2, scopes:
39.6 and cues: 78.5.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999823765957447">
This paper reports experiments to detect uncer-
tainty in text. The experiments are part of the two
shared tasks given by CoNLL-2010 (Farkas et al.,
2010). The first task is to identify uncertain sen-
tences; the second task is to detect the cue phrase
which makes the sentence uncertain and to mark
its scope or span in the sentence.
Uncertainty as a target category needs to be ad-
dressed with some care. Sentences, utterances,
statements are not uncertain – their producer, the
speaker or author, is. Statements may explicitly
indicate this uncertainty, employing several differ-
ent linguistic and textual mechanisms to encode
the speaker’s attitude with respect to the verac-
ity of an utterance. The absence of such markers
does not necessarily indicate certainty – the oppo-
sition between certain and uncertain is not clearly
demarkable, but more of a dimensional measure.
Uncertainty on the part of the speaker may be dif-
ficult to differentiate from a certain assessment of
an uncertain situation, It is unclear whether this
specimen is an X or a Y vs. The difference between
X and Y is unclear.
In this task, the basis for identifying uncertainty
in utterances is almost entirely lexical. Hedges,
the main target of this experiment, are an estab-
lished category in lexical grammar analyses - see
e.g. Quirk et al. (1985), for examples of English
language constructions. Most languages use vari-
ous verbal markers or modifiers for indicating the
speaker’s beliefs in what is being said, most proto-
typically using conditional or optative verb forms,
Six Parisiens seraient morts, or auxiliaries, This
mushroom may be edible, but aspectual markers
may also be recruited for this purpose, more indi-
rectly, I’m hoping you will help vs. I hope you will
help; Do you want to see me now vs. Did you want
to see me now. Besides verbs, there are classes
of terms that through their presence, typically in
an adverbial role, in an utterance make explicit
its tentativeness: possibly, perhaps... and more
complex constructions with some reservation, es-
pecially such that explicitly mention the speaker
and the speaker’s beliefs or doubts, I suspect that
X.
Weasels, the other target of this experiment,
on the other hand, do not indicate uncertainty.
</bodyText>
<page confidence="0.980958">
84
</page>
<bodyText confidence="0.955788222222222">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 84–91,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
Weasels are employed when speakers attempt to
convince the listener of something they most likely
are certain of themselves, by anchoring the truth-
fulness of the utterance to some outside fact or au-
thority (Most linguists believe in the existence of
an autonomous linguistic processing component),
but where the authority in question is so unspecific
as not to be verifiable when scrutinised.
We address both CoNLL-2010 shared tasks
(Farkas et al., 2010). The first, detecting uncer-
tain information on a sentence level, we solve by
using an L1-regularised support vector machine
with hinge loss for the biological domain, and
an L2-regularised maximum margin model for the
Wikipedia domain. The second task, resolution of
in-sentence scopes of hedge cues, we approach as
an approximate L2-regularized maximum margin
structured prediction problem. Our official results
for Task 1 for the biological domain are 85.2 F1-
score, for the Wikipedia set 55.4 F1-score. For
Task 2, our official results were 2.1 for the entire
task with a score of 62.5 for cue detection. After
resolving errors and unfortunate bugs, our final re-
sults are for Task 1, biological: 86.0, Wikipedia:
58.2; Task 2: 39.6 and 78.5 for cues.
</bodyText>
<sectionHeader confidence="0.889944" genericHeader="method">
2 Detecting Sentence Level Uncertainty
</sectionHeader>
<bodyText confidence="0.999929028571428">
On the sentence level, word- and lemma-based
features have been shown to be useful for uncer-
tainty detection (see e.g. Light et al. (2004), Med-
lock and Briscoe (2007), Medlock (2008), and
Szarvas (2008)). Medlock (2008) and Szarvas
(2008) employ probabilistic, weakly supervised
methods, where in the former, a stemmed single
term and bigram representation achieved best re-
sults (0.82 BEP), and in the latter, a more complex
n-gram feature selection procedure was applied
using a Maximum Entropy classifier, achieving
best results when adding reliable keywords from
an external hedge keyword dictionary (0.85 BEP,
85.08 F1-score on biomedical articles). More lin-
guistically motivated features are used by Kil-
icoglu and Bergler (2008), such as negated “un-
hedging” verbs and nouns and that preceded by
epistemic verbs and nouns. On the fruit-fly dataset
(Medlock and Briscoe, 2007) they achieve 0.85
BEP, and on the BMC dataset (Szarvas, 2008) they
achieve 0.82 BEP. Light et al. (2004) also found
that most of the uncertain sentences appeared to-
wards the end of the abstract, indicating that the
position of an uncertain sentence might be a use-
ful feature.
Ganter and Strube (2009) consider weasel tags
in Wikipedia articles as hedge cues, and achieve
results of 0.70 BEP using word- and distance
based features on a test set automatically derived
from Wikipedia, and 0.69 BEP on a manually an-
notated test set using syntactic patterns as fea-
tures. These results suggest that syntactic features
are useful for identifying weasels that ought to be
tagged. However, evaluation is performed on bal-
anced test sets, which gives a higher baseline.
</bodyText>
<subsectionHeader confidence="0.998757">
2.1 Learning and Optimization Framework
</subsectionHeader>
<bodyText confidence="0.999955763157894">
A guiding principle in our approach to this shared
task has been to focus on highly computationally
efficient models, both in terms of training and pre-
diction times. Although kernel based non-linear
separators may sometimes obtain better predic-
tion performance, compared to linear models, the
speed penalty at prediction time is often substan-
tial, since the number of support patterns often
grows linearly with the size of the training set. We
therefore restrict ourselves to linear models, but
allow for a restricted family of explicit non-linear
mappings by feature combinations.
For sentence level hedge detection in the bio-
logical domain, we employ an L1-regularised sup-
port vector machine with hinge loss, as provided
by the library implemented by Fan et al. (2008),
while for weasel detection in the Wikipedia do-
main, we instead use the L2-regularised maximum
margin model described in more detail in section
3.1. In both cases, we approximately optimise the
F1-measure by weighting each class by the inverse
of its proportion in the training data.
The reason for using L1-regularisation in the bi-
ological domain is that the annotation is heavily
biased towards a rather small number of lexical
cues, making most of the potential surface features
irrelevant. The Wikipedia weasel annotation, on
the other hand, is much more noisy and less de-
termined by specific lexical markers. Regularising
with respect to the L1-norm is known to give pref-
erence to sparse models and for the special case
of logistic regression, Ng (2004) proved that the
sample complexity grows only logarithmically in
the number of irrelevant features, instead of lin-
early as when regularising with respect to the L2-
norm. Our preliminary experiments indicated that
L1-regularisation is superior to L2-regularisation
in the biological domain, while slightly inferior in
</bodyText>
<page confidence="0.998503">
85
</page>
<bodyText confidence="0.809586">
the Wikipedia domain.
</bodyText>
<subsectionHeader confidence="0.995501">
2.2 Feature Definitions
</subsectionHeader>
<bodyText confidence="0.991821132075472">
The asymmetric relationship between certain and
uncertain sentences becomes evident when one
tries to learn this distinction based on surface level
cues. While the UNCERTAIN category is to a large
extent explicitly anchored in lexical markers, the
CERTAIN category is more or less defined implic-
itly as the complement of the UNCERTAIN cate-
gory. To handle this situation, we use a bias fea-
ture to model the weight of the CERTAIN category,
while explicit features are used to model the UN-
CERTAIN category.
The following list describes the feature tem-
plates explored for sentence level uncertainty de-
tection. Some features are based on a linguistic
analysis by the Connexor Functional Dependency
(FDG) parser (Tapanainen and J¨arvinen, 1997).
SENLEN Preliminary experiments indicated that taking sen-
tence length into account is beneficial. We incorporate
this by using three different bias terms, according to the
length (in tokens) of the sentences. This feature takes
the following values: S &lt; 18 &lt; M &lt; 32 &lt; L.
DOCPT Document part, e.g., TITLE, ABSTRACT and BODY
TEXT, allowing for different models for different docu-
ment parts.
TOKEN, LEMMA Tokens in most cases equals words, but
may in some special cases also be multiword units, e.g.
of course, as defined by the FDG tokenisation. Lemmas
are base forms of words, with some special features
introduced for numeric tokens, e.g., year, short number,
and long number.
QUANT Syntactic function of a noun phrase with a quanti-
fier head (at least some of the isoforms are conserved
between mouse and humans), or a modifying quantifier
(Recently, many investigators have been interested in
the study on eosinophil biology).
HEAD, DEPREL Functional dependency head of the token,
and the type of dependency relation between the head
and the token, respectively.
SYN Phrase-level and clause-level syntactic functions of a
word.
MORPH Part-of-speech and morphological traits of a word.
Each feature template defines a set of features
when applied to data. The TOKEN, LEMMA,
QUANT, HEAD, DEPREL templates yield single-
ton sets of features for each token, while the SYN
and MORPH templates extends to sets consisting
of several features for each token. A sentence is
represented as the union of all active token level
features and the SENLEN and DOCPT, if active.
In addition to the linear combination of concrete
features, we allow combined features by the Carte-
sian product of the feature set extensions of two or
more feature templates.
</bodyText>
<subsectionHeader confidence="0.999744">
2.3 Feature Template Selection
</subsectionHeader>
<bodyText confidence="0.999989388888889">
Although regularised maximum margin models
often cope well even in the presence of irrelevant
features, it is a good idea to search the large set of
potential features for an optimal subset.
In order to make this search feasible we make
two simplifications. First, we do not explore the
full set of individual features, but instead the set of
feature templates, as defined above. Second, we
perform a greedy search in which we iteratively
add the feature template that gives the largest per-
formance improvement, when added to the cur-
rent optimal set of templates. The performance of
a feature set for sentence level detection is mea-
sured as the mean Fl-score, with respect to the
UNCERTAIN class, minus one standard deviation
– the mean and standard deviation are computed
by three fold cross-validation on the training set.
We subtract one standard deviation from the mean
in order to promote stable solutions over unstable
ones.
Of course, these simplifications do not come for
free. The solution of the optimisation problem
might be quite unstable with respect to the optimal
hyper-parameters of the learning algorithm, which
in turn may depend on the feature set used. This
risk could be reduced by conducting a more thor-
ough parameter search for each candidate feature
set, however, this was simply too time consuming
for the present work. A further risk of using for-
ward selection is that feature interactions are ig-
nored. This issue is handled better with backward
elimination, but that is also more time consuming.
The full set of explored feature templates is too
large to be listed here; instead we list the features
selected in each iteration of the search, together
with their corresponding scores, in Table 1.
</bodyText>
<sectionHeader confidence="0.978257" genericHeader="method">
3 Detecting In-sentence Uncertainty
</sectionHeader>
<bodyText confidence="0.999889">
When it comes to the automatic identification of
hedge cues and their linguistic scopes, Morante
and Daelemans (2009) and ¨Ozg¨ur and Radev
(2009) report experiments on the BioScope cor-
pus (Vincze et al., 2008), achieving best results
(10-fold cross evaluation) on the identification of
hedge cues of 71.59 F-score (using IGTree with
current, preceding and subsequent word and cur-
</bodyText>
<page confidence="0.991412">
86
</page>
<table confidence="0.96042075">
Task Template set Dev F1 Test F1
SENLEN - -
U LEMMA 88.9 (.25) 78.79
U LEMMABI 90.3 (.19) 85.86
U LEMMA®QUANT 90.3 (.07) 85.97
SENLEN - -
Wiki U TOKEN®DOCPT 59.0 (.76) 60.12
U TOKENBI®SENLEN 59.9 (.09) 58.26
</table>
<tableCaption confidence="0.9645005">
Table 1: Top feature templates for sentence level
hedge and weasel detection.
</tableCaption>
<bodyText confidence="0.999641071428572">
rent lemma as features) and 82.82 F-score (using a
Support Vector Machine classifier and a complex
feature set including keyword and dependency re-
lation information), respectively. On the task of
automatic scope resolution, best results are re-
ported as 59.66 (F-score) and 61.13 (accuracy),
respectively, on the full paper subset. ¨Ozg¨ur and
Radev (2009) use a rule-based method for this sub-
task, while Morante and Daelemans (2009) use
three different classifiers as input to a CRF-based
meta-learner, with a complex set of features, in-
cluding hedge cue information, current and sur-
rounding token information, distance information
and location information.
</bodyText>
<subsectionHeader confidence="0.999272">
3.1 Learning and Optimisation Framework
</subsectionHeader>
<bodyText confidence="0.999972368421053">
In recent years, a wide range of different ap-
proaches to general structured prediction prob-
lems, of which sequence labelling is a special
case, have been suggested. Among others, Con-
ditional Random Fields (Lafferty et al., 2001),
Max-Margin Markov Networks (Taskar et al.,
2003), and Structured Support Vector Machines
(Tsochantaridis et al., 2005). A drawback of
these approaches is that they are all quite com-
putationally demanding. As an alternative, we
propose a much more computationally lenient ap-
proach based on the regularised margin-rescaling
formulation of Taskar et al. (2003), which we in-
stead optimise by stochastic subgradient descent
as suggested by Ratliff et al. (2007). In addi-
tion we only perform approximate decoding, us-
ing beam search, which allows arbitrary complex
joint feature maps to be employed, without sacri-
ficing speed.
</bodyText>
<subsectionHeader confidence="0.753125">
3.1.1 Technical Details
</subsectionHeader>
<bodyText confidence="0.991117">
Let X denote the pattern set and let Y denote the
set of structured labels. Let A denote the set of
atomic labels and let each label y E Y consist of
an indexed sequence of atomic labels yi E A. De-
note by Yx C Y the set of possible label assign-
ments to pattern x E X and by yx E Yx its cor-
rect label. In the specific case of BIO-sequence
labelling, A = {BEGIN, INSIDE, OUTSIDE} and
Yx = A|x|, where IxI is the length of the sequence
xEX.
A structured classification problem amounts
to learning a mapping from patterns to labels,
f : X H Y, such that the expected loss
EX×Y[A(yx, f(x))] is minimised. The prediction
loss, A : Y x Y H R+, measures the loss of
predicting label y = f(x) when the correct la-
bel is yx, with A(yx, yx) = 0. Here we assume
the Hamming loss, AH(y, y0) = E|y|
i=1 δ(yi, y0i),
where δ(yi, y0i) = 1 if yi =� y0i and 0 otherwise.
The idea of the margin-rescaling approach is to
let the structured margin between the correct label
yx and a hypothesis y E Yx scale linearly with the
prediction loss A(yx, y) (Taskar et al., 2003). The
structured margin is defined in terms of a score
function S : X x Y H R, in our case the linear
score function S(x, y) = w&apos;Φ(x, y), where w E
R&apos; is a vector of parameters and Φ : X xY H R&apos;
is a joint feature function. The learning problem
then amounts to finding parameters w such that
S(x, yx) &gt; S(x, y) + A(yx, y) for all y E Yx \
{yx} over the training data D. In other words, we
want the score of the correct label to be higher than
the score plus the loss, of all other labels, for each
instance. In order to balance margin maximisation
and margin violation, we add the L2-regularisation
term 11w112.
By making use of the loss augmented decoding
function
</bodyText>
<equation confidence="0.96933275">
fΔ(x, yx) = argmax [S(x, y) + A(yx, y)], (1)
y∈Y.
we get the following regularised risk functional:
SΔ(x(i), yx(i)) + λ2 11w112, (2)
</equation>
<bodyText confidence="0.822194">
where
</bodyText>
<equation confidence="0.8957575">
SΔ(x, yx) = max [S(x, y) + A(yx, y)]−S(x, yx)
y∈Y.
</equation>
<bodyText confidence="0.942017285714286">
(3)
We optimise (2) by stochastic approximate subgra-
dient descent with step size sequence [η0/1/t]∞�=1
(Ratliff et al., 2007). The initial step size η0
and the regularisation factor λ are data depen-
dent hyper-parameters, which we tune by cross-
validation.
</bodyText>
<equation confidence="0.94363">
Bio
QA,D(w) = � |D|
i=1
</equation>
<page confidence="0.98423">
87
</page>
<bodyText confidence="0.999913125">
This framework is highly efficient both at learn-
ing and prediction time. Training cues and scopes
on the biological data, takes about a minute, while
prediction times are in the order of seconds, using
a Java based implementation on a standard laptop;
the absolute majority of that time is spent on read-
ing and extracting features from an inefficient in-
ternal JSON-based format.
</bodyText>
<subsectionHeader confidence="0.908685">
3.1.2 Hashed Feature Functions
</subsectionHeader>
<bodyText confidence="0.9999506875">
Joint feature functions enable encoding of depen-
dencies between labels and relations between pat-
tern and label. Most feature templates are de-
fined based on input only, while some are de-
fined with respect to output features as well. Let
Ψ(x, y1:i_1, i) E R&apos; denote the joint feature func-
tion corresponding to the application of all active
feature templates to pattern x E X and partially
decoded label y1:i_1 E Ai_1 when decoding at
position i. The feature mapping used in scoring
candidate label yi E A is then computed as the
Cartesian product Φ(x, y, i) = Ψ(x, y1:i_1, i) �
A(yi), where A(yi) E R&apos; is a unique unitary fea-
ture vector representation of label yi. The feature
representation for a complete sequence x and its
associated label y is then computed as
</bodyText>
<equation confidence="0.998626">
Φ(x, y) = � |x |Φ(x, y, i)
i=1
</equation>
<bodyText confidence="0.999809791666667">
When employing joint feature functions and com-
bined features, the number of unique features may
grow very large. This is a problem when the
amount of internal memory is limited. Feature
hashing, as described by Weinberger et al. (2009),
is a simple trick to circumvent this problem. As-
sume that we have an original feature function
O : X x Y H R&apos;, where m might be arbitrar-
ily large. Let h : N+ H [1, n] be a hash function
and let h_1(i) C [1, m] be the set of integers such
that j E h_1(i) iff h(j) = i. We now use this
hash function to map the index of each feature in
O(x, y) to its corresponding index in Φ(x, y), as
Φi(x, y) = Ejlh−1(i) Oj(x, y). The features in Φ
are thus unions of multisets of features in O. Given
a hash function with good collision properties, we
can expect that the subset of features mapped to
any index in Φ(x, y) is small and composed of ele-
ments drawn at random from O(x, y). Weinberger
et al. (2009) contains proofs of bounds on these
distributions. Furthermore, by using a k-valued
hash function h : Nk H [1, n], the Cartesian prod-
uct of k feature sets can be computed much more
efficiently, compared to using a dictionary.
</bodyText>
<subsectionHeader confidence="0.999724">
3.2 Position Based Feature Definitions
</subsectionHeader>
<bodyText confidence="0.999957838709677">
For in-sentence cue and scope prediction we make
use of the same token level feature templates as
for sentence level detection. An additional level
of expressivity is added in that each token level
template is associated with a token position. A
template is addressed either relative to the token
currently being decoded, or by the dependency arc
of a token, which in turn is addressed by a relative
position. The addressing can be either to a single
position, or a range of positions. Feature templates
may further be defined with respect to features of
the input pattern, the token level labels predicted
so far, or with respect to combinations of input
and label features. Joint features, just as complex
feature combinations, are created by forming the
Cartesian product of an input feature set and a la-
bel feature set.
The feature templates are instantiated by pre-
fixing the template name to each member of the
feature set. To exemplify, the single position tem-
plate TOKENi, given that the token currently be-
ing decoded at position i is suggests, is instanti-
ated as the singleton set {TOKENi = suggests}.
The range template TOKENi,i+1, given that the
current token is suggests and the next token is
that, is instantiated as the set {TOKENi,i+1 =
suggests, TOKENi,i+1 = that}; i.e. each member
of the set is prefixed by the range template name.
In addition to the token level templates used for
sentence level prediction, the following templates
were explored:
</bodyText>
<subsectionHeader confidence="0.694822">
LABEL Label predicted so far at the addressed position(s).
</subsectionHeader>
<bodyText confidence="0.99977">
HEAD.X An arbitrary feature, X, addressed by follow-
ing the dependency arc(s) from the addressed posi-
tion(s). For example, HEAD.LEMMAi corresponds to
the lemma found by looking at the dependency head of
the current token.
CUE, CUESCOPE Whether the token(s) addressed is re-
spectively, a cue marker, or within the syntactic scope
of the current cue, following the definition of scope
provided by Vincze et al. (2008).
</bodyText>
<subsectionHeader confidence="0.999332">
3.3 Feature Template Selection
</subsectionHeader>
<bodyText confidence="0.9998154">
Just as with sentence level detection, we used a
greedy forward selection strategy when searching
for the optimal subset of feature templates. The
cue and scope detection subtasks were optimised
separately.
</bodyText>
<page confidence="0.99739">
88
</page>
<bodyText confidence="0.999963333333333">
The scoring measures used in the search for
cue and scope detection features differ. In order
to match the official scoring measure for cue de-
tection, we optimise the Fl-score of labels cor-
responding to cue tags, i.e. we treat the BEGIN
and INSIDE cue tags as an equivalence class. The
official scoring measure for scope prediction, on
the other hand, corresponds to the exact match
of scope boundaries. Unfortunately using exact
match performance turned out to be not very well
suited for use in greedy forward selection. This
is because before a sufficient per token accuracy
has been reached, and even when it has, the ex-
act match score may fluctuate wildly. Therefore,
as a substitute, we instead guide the search by to-
ken level accuracy. This discrepancy between the
search criterion and the official scoring metric is
unfortunate.
Again, when taking into account position ad-
dressing, joint features and combined features, the
complete set of explored templates is too large to
fit in the current experiment. The selected features
together with their corresponding scores are found
in Table 2.
</bodyText>
<table confidence="0.998313125">
Task Template set Dev F1 Test F1
TOKENi 74.0 (1.5) -
U TOKENi−1 81.0 (.30) 68.78
Cue U MORPHi 83.6 (.10) 74.06
U LEMMAi ® LEMMAi+1 85.6 (.20) 78.41
U SYNi 86.5 (.41) 78.28
U LEMMAi−1 ® LEMMAi 86.7 (.42) 78.52
CueScopei 66.9 (.92) -
U LABELi−2,i−1 79.5 (.67) 34.80
U LEMMAi 82.4 (1.1) 33.18
U MORPHi 83.1 (.35) 35.70
Scope U CUEi−2,i−1 83.4 (.13) 40.14
U CUEi,i+1,i+2 83.6 (.11) 41.15
U LEMMAi−1 84.1 (.16) 40.04
U MORPHi 84.4 (.33) 40.04
U TOKENi+1 84.5 (.09) 39.64
</table>
<tableCaption confidence="0.977443">
Table 2: Top feature templates for in-sentence de-
tection of hedge cues and scopes.
</tableCaption>
<sectionHeader confidence="0.997997" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999953965517241">
Our final F,-score results for the corrected system
are, in Task 1 for the biological domain 85.97, for
the Wikipedia domain 58.25; for Task 2, our re-
sults are 39.64 for the entire task with a score of
78.52 for cue detection.
Any gold standard-based shared experiment un-
avoidably invites discussion on the reliability of
the gold standard. It is easy to find borderline ex-
amples in the evaluation corpus, e.g. sentences
that may just as well be labeled “certain” rather
than “uncertain”. This gives an indication of the
true complexity of assessing the hidden variable of
uncertainty and coercing it to a binary judgment
rather than a dimensional one. It is unlikely that
everyone will agree on a binary judgment every
time.
To improve experimental results and the gen-
eralisability of the results for the task of detect-
ing uncertain information on a sentence level, we
would need to break reliance on the purely lexical
cues. For instance, we now have identified possi-
ble and putative as markers for uncertainty, but in
many instances they are not (Finally, we wish to
ensure that others can use and evaluate the GREC
as simply as possible). This would be avoidable
through either a deeper analysis of the sentence
to note that possible in this case does not modify
anything of substance in the sentence, or alterna-
tively through a multi-word term preprocessor to
identify as simply as possible as an analysis unit.
In the Wikipedia experiment, where the objec-
tive is to identify weasel phrases, the judicious en-
coding of quantifiers such as “some of the most
well-known researchers say that X” would be
likely to identify the sought-for sentences when
the quantified NP is in subject position. In our
experiment we find that our dependency analysis
did not distinguish between the various syntactic
roles of quantified NPs. As a result, we marked
several sentences with a quantifier as a “weasel”
sentence, even where the quantified NP was in a
non-subject role – leading to overly many weasel
sentences. An example is given in Table 3.
If certainty can be identified separately, not as
absence of overt uncertainty, identifying uncer-
tainty can potentially be aided through the iden-
tification of explicit certainty together with nega-
tion, as found by Kilicoglu and Bergler (2008). In
keeping with their results, we found negations in a
sizeable proportion of the annotated training mate-
rial. Currently we capture negation as a lexical cue
in immediate bigrams, but with longer range nega-
tions, we will miss some clear cases: Table 3 gives
two examples. To avoid these misses, we will both
need to identify overt expressions of certainty and
to identify and track the scope of negation – the
first challenge is unexplored but would not seem
to be overly complex; the second is a well-known
</bodyText>
<page confidence="0.999297">
89
</page>
<bodyText confidence="0.999992897727273">
and established challenge for NLP systems in gen-
eral.
In the task of detecting in-sentence uncertainty
– identification of hedge cues and their scopes –
we find that an evaluation method based on ex-
act match of a token sequence is overly unforgiv-
ing. There are many cases where the marginal to-
kens of a sequence are less than central or irrele-
vant for the understanding of the hedge cue and its
scope: moving the boundary by one position over
an uninteresting token may completely invalidate
an otherwise arguably correct analysis. A token-
by-token scoring would be a more functional eval-
uation criterion, or perhaps a fuzzy match, allow-
ing for a certain amount of erroneous characters.
For our experiments, this has posed some chal-
lenges. While we model the in-sentence un-
certainty detection as a sequence labelling prob-
lem in the BIO-representation (BEGIN, INSIDE,
OUTSIDE), the provided corpus uses an XML-
representation. Moreover, the official scoring tool
requires that the predictions are well formed XML,
necessitating a conversion from XML to BIO prior
to training and from BIO to XML after prediction.
Consistent tokenisation is important, but the syn-
tactic analysis components used by us distorted the
original tokenisation and restoring the exact same
token sequence proved problematic.
Conversion from BIO to XML is straightforward
for cues, while some care must be taken when an-
notating scopes, since erroneous scope predictions
may result in malformed XML. When adding the
scope annotation, we use a stack based algorithm.
For each sentence, we simultaneously traverse the
scope-sequence corresponding to each cue, left to
right, token by token. The stack is used to en-
sure that scopes are either separated or nested and
an additional restriction ensures that scopes may
never start or end inside a cue. In case the al-
gorithm fails to place a scope according to these
restrictions, we fall back and let the scope cover
the whole sentence. Several of the more frequent
errors in our analyses are scoping errors, many
likely to do with the fallback solution. Our analy-
sis quite frequently fails also to assign the subject
of a sentence to the scope of a hedging verb. Ta-
ble 3 shows one example each of these errors –
overextended scope and missing subject.
Unfortunately, the tokenisation output by our
analysis components is not always consistent with
the tokenisation assumed by the BioScope annota-
tion. A post-processing step was therefore added
in which each, possibly complex, token in the pre-
dicted BIO-sequence is heuristically mapped to its
corresponding position in the XML structure. This
post-processing is not perfect and scopes and cues
at non-word token boundaries, such as parenthe-
ses, are quite often misplaced with respect to the
BioScope annotation. Table 3 gives one example
which is scored “erroneous” since the token “(63)”
is in scope, where the “correct” solution has it out-
side the scope. These errors are not important to
address, but are quite frequent in our results – ap-
proximately 80 errors are of this type.
To achieve more general and effective methods
to detect uncertainty in an argument, we should
note that uncertainty is signalled in a text through
many mechanisms, and that the purely lexical and
explicit signal found through the present experi-
ments in hedge identification is effective and use-
ful, but will not catch everything we might want to
find. Lexical approaches are also domain depen-
dent. For instance, Szarvas (2008) and Morante
and Daelemans (2009) report loss in performance,
when applying the same methods developed on bi-
ological data, on clinical text. Using the systems
developed for scientific text elsewhere poses a mi-
gration challenge. It would be desirable both to
automatically learn a hedging lexicon from a gen-
eral seed set and to have features on a higher level
of abstraction.
Our main result is that casting this task as a se-
quence labelling problem affords us the possibility
to combine linguistic analyses with a highly effi-
cient implementation of a max-margin prediction
algorithm. Our framework processes the data sets
in minutes for training and seconds for prediction
on a standard personal computer.
</bodyText>
<sectionHeader confidence="0.998888" genericHeader="conclusions">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9949484">
The authors would like to thank Joakim Nivre
for feedback in earlier stages of this work. This
work was funded by The Swedish National Grad-
uate School of Language Technology and by the
Swedish Research Council.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9968346">
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library
for large linear classification. Journal of Machine Learn-
ing Research, 9:1871–1874.
Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos
</reference>
<page confidence="0.98167">
90
</page>
<reference confidence="0.982924569892474">
Neg + certain However, how IFN--y and IL-4 inhibit IL-17 production is not yet known.
Neg + certain The mechanism by which Tregs preserve peripheral tolerance is still not entirely clear.
“some”: not weasel Tourist folks usually visit this peaceful paradise to enjoy some leisurenonsubj.
“some”: weasel Somesubj suggest that the origin of music likely stems from naturally occurring sounds and rhythms.
Prediction dRas85DV12 &lt;xcope .1&gt;&lt;cue .1&gt;may&lt;/cue&gt; be more potent than dEGFRA because
dRas85DV12 can activate endogenous PI3K signaling [16]&lt;/xcope&gt;.
Gold standard dRas85DV12 &lt;xcope .1&gt;&lt;cue .1&gt;may&lt;/cue&gt; be more potent than dEGFRA&lt;/xcope&gt; because
dRas85DV12 can activate endogenous PI3K signaling [16].
Prediction However, the precise molecular mechanisms of Stat3-mediated expression of ROR-yt
&lt;xcope .1&gt;are still &lt;cue .1&gt;unclear&lt;/cue&gt;&lt;/xcope&gt;.
Gold standard However, &lt;xcope .1&gt;the precise molecular mechanisms of Stat3-mediated expression of ROR-yt
are still &lt;cue .1&gt;unclear&lt;/cue&gt;&lt;/xcope&gt;.
Prediction Interestingly, Foxp3 &lt;xcope .1&gt;&lt;cue .1&gt;may&lt;/cue&gt; inhibit ROR-yt
activity on its target genes, at least in par,t through direct interaction with ROR-yt (63)&lt;/xcope&gt;.
Gold standard Interestingly, Foxp3 &lt;xcope .1&gt;&lt;cue .1&gt;may&lt;/cue&gt; inhibit RORt
activity on its target genes, at least in par,t through direct interaction with RORt&lt;/xcope&gt; (63).
Table 3: Examples of erroneous analyses.
Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL-2010
Shared Task: Learning to Detect Hedges and their Scope
in Natural Language Text. In Proceedings of the 14th
Conference on Computational Natural Language Learn-
ing (CoNLL-2010): Shared Task, pages 1–12, Uppsala,
Sweden, July. Association for Computational Linguistics.
Viola Ganter and Michael Strube. 2009. Finding hedges
by chasing weasels: hedge detection using Wikipedia tags
and shallow linguistic features. In ACL-IJCNLP ’09: Pro-
ceedings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Halil Kilicoglu and Sabine Bergler. 2008. Recognizing spec-
ulative language in biomedical research articles: a linguis-
tically motivated perspective. BMC Bioinformatics, 9.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional random fields: Probabilis-
tic models for segmenting and labeling sequence data. In
Proc. 18th Int. Conf. on Machine Learning. Morgan Kauf-
mann Publishers.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 2004.
The language of bioscience: Facts, speculations, and state-
ments in between. In Lynette Hirschman and James
Pustejovsky, editors, HLT-NAACL 2004 Workshop: Bi-
oLINK 2004, Linking Biological Literature, Ontologies
and Databases, Boston, USA. ACL.
Ben Medlock and Ted Briscoe. 2007. Weakly supervised
learning for hedge classification in scientific literature. In
Proceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, Prague, Czech Repub-
lic. Association for Computational Linguistics.
Ben Medlock. 2008. Exploring hedge identification in
biomedical literature. Journal of Biomedical Informatics,
41(4):636–654.
Roser Morante and Walter Daelemans. 2009. Learning the
scope of hedge cues in biomedical texts. In BioNLP ’09:
Proceedings of Workshop on BioNLP, Morristown, NJ,
USA. ACL.
Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regulariza-
tion, and rotational invariance. In ICML ’04: Proceedings
of the 21st International Conference on Machine learning,
page 78, New York, NY, USA. ACM.
Arzucan ¨Ozg¨ur and Dragomir R. Radev. 2009. Detecting
speculations and their scopes in scientific text. In Pro-
ceedings of 2009 Conference on Empirical Methods in
Natural Language Processing, Singapore. ACL.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and
Jan Svartvik. 1985. A comprehensive grammar of the
English language. Longman.
Nathan D. Ratliff, Andrew J. Bagnell, and Martin A. Zinke-
vich. 2007. (Online) subgradient methods for structured
prediction. In Eleventh International Conference on Arti-
ficial Intelligence and Statistics (AIStats).
Gy¨orgy Szarvas. 2008. Hedge classification in biomedical
texts with a weakly supervised selection of keywords. In
Proceedings of ACL-08: HLT, Columbus, Ohio. ACL.
Pasi Tapanainen and Timo J¨arvinen. 1997. A non-projective
dependency parser. In Proceedings of the 5th Conference
on Applied Natural Language Processing.
Benjamin Taskar, Carlos Guestrin, and Daphne Koller. 2003.
Max-margin Markov networks. In Sebastian Thrun,
Lawrence K. Saul, and Bernhard Sch¨olkopf, editors,
NIPS. MIT Press.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin methods
for structured and interdependent output variables. Jour-
nal of Machine Learning Research, 6:1453–1484.
Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas, Gy¨orgy
M´ora, and J´anos Csirik. 2008. The BioScope corpus:
biomedical texts annotated for uncertainty, negation and
their scopes. BMC Bioinformatics, 9(S-11).
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex
Smola, and Josh Attenberg. 2009. Feature hashing for
large scale multitask learning. In ICML ’09: Proceedings
of the 26th Annual International Conference on Machine
Learning, New York, NY, USA. ACM.
</reference>
<page confidence="0.99917">
91
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000072">
<title confidence="0.998737">Uncertainty Detection as Approximate Max-Margin Sequence Labelling</title>
<author confidence="0.851836">Oscar</author>
<affiliation confidence="0.817505">SICS / Uppsala Kista / Uppsala,</affiliation>
<email confidence="0.964303">oscar@sics.se</email>
<note confidence="0.3719845">Gunnar Kista,</note>
<email confidence="0.840648">guer@sics.se</email>
<affiliation confidence="0.154294">Sumithra</affiliation>
<address confidence="0.649378">DSV, Stockholm</address>
<email confidence="0.7924535">Kista,sumithra@dsv.su.se</email>
<affiliation confidence="0.514635">Hercules</affiliation>
<address confidence="0.826044">DSV, Stockholm</address>
<email confidence="0.837845">Kista,hercules@dsv.su.se</email>
<affiliation confidence="0.46505">Martin</affiliation>
<address confidence="0.779158">DSV, Stockholm</address>
<email confidence="0.8217005">Kista,xmartin@dsv.su.se</email>
<author confidence="0.323415">Jussi</author>
<email confidence="0.6541035">Kista,jussi@sics.se</email>
<abstract confidence="0.985345724137931">This paper reports experiments for the CoNLL-2010 shared task on learning to detect hedges and their scope in natural language text. We have addressed the experimental tasks as supervised linear maximum margin prediction problems. For sentence level hedge detection the biological domain we use an regularised binary support vector machine, while for sentence level weasel detection the Wikipedia domain, we use an regularised approach. We model the insentence uncertainty cue and scope detask as an approximate maximum margin sequence labelling using the In addition to surface level features, we use a variety of linguistic features based on a functional dependency analysis. A greedy forward selection strategy is used in exploring the large set of potential features. Our official results for Task 1 for the biodomain are 85.2 for the set 55.4 For Task 2, our official results are 2.1 for the entire task with a score of 62.5 for cue detection. After resolving errors and final bugs, our final results are for Task 1, biologi-</abstract>
<note confidence="0.587561">cal: 86.0, Wikipedia: 58.2; Task 2, scopes: 39.6 and cues: 78.5.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="7821" citStr="Fan et al. (2008)" startWordPosition="1236" endWordPosition="1239">ction times. Although kernel based non-linear separators may sometimes obtain better prediction performance, compared to linear models, the speed penalty at prediction time is often substantial, since the number of support patterns often grows linearly with the size of the training set. We therefore restrict ourselves to linear models, but allow for a restricted family of explicit non-linear mappings by feature combinations. For sentence level hedge detection in the biological domain, we employ an L1-regularised support vector machine with hinge loss, as provided by the library implemented by Fan et al. (2008), while for weasel detection in the Wikipedia domain, we instead use the L2-regularised maximum margin model described in more detail in section 3.1. In both cases, we approximately optimise the F1-measure by weighting each class by the inverse of its proportion in the training data. The reason for using L1-regularisation in the biological domain is that the annotation is heavily biased towards a rather small number of lexical cues, making most of the potential surface features irrelevant. The Wikipedia weasel annotation, on the other hand, is much more noisy and less determined by specific le</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Rich´ard Farkas</author>
<author>Veronika Vincze</author>
<author>Gy¨orgy M´ora</author>
</authors>
<title>J´anos Neg + certain However, how IFN--y and IL-4 inhibit IL-17 production is not yet known. Neg + certain The mechanism by which Tregs preserve peripheral tolerance is still not entirely clear. “some”: not weasel Tourist folks usually visit this peaceful paradise to enjoy some leisurenonsubj.</title>
<marker>Farkas, Vincze, M´ora, </marker>
<rawString>Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos Neg + certain However, how IFN--y and IL-4 inhibit IL-17 production is not yet known. Neg + certain The mechanism by which Tregs preserve peripheral tolerance is still not entirely clear. “some”: not weasel Tourist folks usually visit this peaceful paradise to enjoy some leisurenonsubj.</rawString>
</citation>
<citation valid="false">
<title>some”: weasel Somesubj suggest that the origin of music likely stems from naturally occurring sounds and rhythms. Prediction dRas85DV12 .1&gt;&lt;cue .1&gt;may&lt;/cue&gt; be more potent than dEGFRA because dRas85DV12 can activate endogenous PI3K signaling [16]&lt;/xcope&gt;.</title>
<marker></marker>
<rawString>“some”: weasel Somesubj suggest that the origin of music likely stems from naturally occurring sounds and rhythms. Prediction dRas85DV12 &lt;xcope .1&gt;&lt;cue .1&gt;may&lt;/cue&gt; be more potent than dEGFRA because dRas85DV12 can activate endogenous PI3K signaling [16]&lt;/xcope&gt;.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Gold standard</author>
</authors>
<title>dRas85DV12 .1&gt;&lt;cue .1&gt;may&lt;/cue&gt; be more potent than dEGFRA&lt;/xcope&gt; because dRas85DV12 can activate endogenous PI3K signaling [16].</title>
<marker>standard, </marker>
<rawString>Gold standard dRas85DV12 &lt;xcope .1&gt;&lt;cue .1&gt;may&lt;/cue&gt; be more potent than dEGFRA&lt;/xcope&gt; because dRas85DV12 can activate endogenous PI3K signaling [16].</rawString>
</citation>
<citation valid="false">
<authors>
<author>Prediction However</author>
</authors>
<title>the precise molecular mechanisms of Stat3-mediated expression of ROR-yt .1&gt;are still .1&gt;unclear&lt;/cue&gt;&lt;/xcope&gt;.</title>
<marker>However, </marker>
<rawString>Prediction However, the precise molecular mechanisms of Stat3-mediated expression of ROR-yt &lt;xcope .1&gt;are still &lt;cue .1&gt;unclear&lt;/cue&gt;&lt;/xcope&gt;.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Gold standard</author>
</authors>
<title>However, .1&gt;the precise molecular mechanisms of Stat3-mediated expression of ROR-yt are still .1&gt;unclear&lt;/cue&gt;&lt;/xcope&gt;.</title>
<marker>standard, </marker>
<rawString>Gold standard However, &lt;xcope .1&gt;the precise molecular mechanisms of Stat3-mediated expression of ROR-yt are still &lt;cue .1&gt;unclear&lt;/cue&gt;&lt;/xcope&gt;.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Prediction Interestingly</author>
</authors>
<title>Foxp3 .1&gt;&lt;cue .1&gt;may&lt;/cue&gt; inhibit ROR-yt activity on its target genes, at least in par,t through direct interaction with ROR-yt (63)&lt;/xcope&gt;.</title>
<marker>Interestingly, </marker>
<rawString>Prediction Interestingly, Foxp3 &lt;xcope .1&gt;&lt;cue .1&gt;may&lt;/cue&gt; inhibit ROR-yt activity on its target genes, at least in par,t through direct interaction with ROR-yt (63)&lt;/xcope&gt;.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Gold standard Interestingly</author>
</authors>
<title>Foxp3 .1&gt;&lt;cue .1&gt;may&lt;/cue&gt; inhibit RORt activity on its target genes, at least in par,t through direct interaction with RORt&lt;/xcope&gt; (63). Table 3: Examples of erroneous analyses.</title>
<marker>Interestingly, </marker>
<rawString>Gold standard Interestingly, Foxp3 &lt;xcope .1&gt;&lt;cue .1&gt;may&lt;/cue&gt; inhibit RORt activity on its target genes, at least in par,t through direct interaction with RORt&lt;/xcope&gt; (63). Table 3: Examples of erroneous analyses.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Csirik</author>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text.</title>
<date>2010</date>
<booktitle>In Proceedings of the 14th Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task,</booktitle>
<pages>1--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<marker>Csirik, Szarvas, 2010</marker>
<rawString>Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of the 14th Conference on Computational Natural Language Learning (CoNLL-2010): Shared Task, pages 1–12, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Viola Ganter</author>
<author>Michael Strube</author>
</authors>
<title>Finding hedges by chasing weasels: hedge detection using Wikipedia tags and shallow linguistic features.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP ’09: Proceedings of the ACL-IJCNLP 2009 Conference Short Papers,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="6551" citStr="Ganter and Strube (2009)" startWordPosition="1034" endWordPosition="1037">ords from an external hedge keyword dictionary (0.85 BEP, 85.08 F1-score on biomedical articles). More linguistically motivated features are used by Kilicoglu and Bergler (2008), such as negated “unhedging” verbs and nouns and that preceded by epistemic verbs and nouns. On the fruit-fly dataset (Medlock and Briscoe, 2007) they achieve 0.85 BEP, and on the BMC dataset (Szarvas, 2008) they achieve 0.82 BEP. Light et al. (2004) also found that most of the uncertain sentences appeared towards the end of the abstract, indicating that the position of an uncertain sentence might be a useful feature. Ganter and Strube (2009) consider weasel tags in Wikipedia articles as hedge cues, and achieve results of 0.70 BEP using word- and distance based features on a test set automatically derived from Wikipedia, and 0.69 BEP on a manually annotated test set using syntactic patterns as features. These results suggest that syntactic features are useful for identifying weasels that ought to be tagged. However, evaluation is performed on balanced test sets, which gives a higher baseline. 2.1 Learning and Optimization Framework A guiding principle in our approach to this shared task has been to focus on highly computationally </context>
</contexts>
<marker>Ganter, Strube, 2009</marker>
<rawString>Viola Ganter and Michael Strube. 2009. Finding hedges by chasing weasels: hedge detection using Wikipedia tags and shallow linguistic features. In ACL-IJCNLP ’09: Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
<author>Sabine Bergler</author>
</authors>
<title>Recognizing speculative language in biomedical research articles: a linguistically motivated perspective.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<contexts>
<context position="6104" citStr="Kilicoglu and Bergler (2008)" startWordPosition="956" endWordPosition="960">ion (see e.g. Light et al. (2004), Medlock and Briscoe (2007), Medlock (2008), and Szarvas (2008)). Medlock (2008) and Szarvas (2008) employ probabilistic, weakly supervised methods, where in the former, a stemmed single term and bigram representation achieved best results (0.82 BEP), and in the latter, a more complex n-gram feature selection procedure was applied using a Maximum Entropy classifier, achieving best results when adding reliable keywords from an external hedge keyword dictionary (0.85 BEP, 85.08 F1-score on biomedical articles). More linguistically motivated features are used by Kilicoglu and Bergler (2008), such as negated “unhedging” verbs and nouns and that preceded by epistemic verbs and nouns. On the fruit-fly dataset (Medlock and Briscoe, 2007) they achieve 0.85 BEP, and on the BMC dataset (Szarvas, 2008) they achieve 0.82 BEP. Light et al. (2004) also found that most of the uncertain sentences appeared towards the end of the abstract, indicating that the position of an uncertain sentence might be a useful feature. Ganter and Strube (2009) consider weasel tags in Wikipedia articles as hedge cues, and achieve results of 0.70 BEP using word- and distance based features on a test set automati</context>
<context position="26163" citStr="Kilicoglu and Bergler (2008)" startWordPosition="4347" endWordPosition="4350"> the quantified NP is in subject position. In our experiment we find that our dependency analysis did not distinguish between the various syntactic roles of quantified NPs. As a result, we marked several sentences with a quantifier as a “weasel” sentence, even where the quantified NP was in a non-subject role – leading to overly many weasel sentences. An example is given in Table 3. If certainty can be identified separately, not as absence of overt uncertainty, identifying uncertainty can potentially be aided through the identification of explicit certainty together with negation, as found by Kilicoglu and Bergler (2008). In keeping with their results, we found negations in a sizeable proportion of the annotated training material. Currently we capture negation as a lexical cue in immediate bigrams, but with longer range negations, we will miss some clear cases: Table 3 gives two examples. To avoid these misses, we will both need to identify overt expressions of certainty and to identify and track the scope of negation – the first challenge is unexplored but would not seem to be overly complex; the second is a well-known 89 and established challenge for NLP systems in general. In the task of detecting in-sente</context>
</contexts>
<marker>Kilicoglu, Bergler, 2008</marker>
<rawString>Halil Kilicoglu and Sabine Bergler. 2008. Recognizing speculative language in biomedical research articles: a linguistically motivated perspective. BMC Bioinformatics, 9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proc. 18th Int. Conf. on Machine Learning.</booktitle>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="14808" citStr="Lafferty et al., 2001" startWordPosition="2359" endWordPosition="2362"> on the full paper subset. ¨Ozg¨ur and Radev (2009) use a rule-based method for this subtask, while Morante and Daelemans (2009) use three different classifiers as input to a CRF-based meta-learner, with a complex set of features, including hedge cue information, current and surrounding token information, distance information and location information. 3.1 Learning and Optimisation Framework In recent years, a wide range of different approaches to general structured prediction problems, of which sequence labelling is a special case, have been suggested. Among others, Conditional Random Fields (Lafferty et al., 2001), Max-Margin Markov Networks (Taskar et al., 2003), and Structured Support Vector Machines (Tsochantaridis et al., 2005). A drawback of these approaches is that they are all quite computationally demanding. As an alternative, we propose a much more computationally lenient approach based on the regularised margin-rescaling formulation of Taskar et al. (2003), which we instead optimise by stochastic subgradient descent as suggested by Ratliff et al. (2007). In addition we only perform approximate decoding, using beam search, which allows arbitrary complex joint feature maps to be employed, witho</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th Int. Conf. on Machine Learning. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
<author>Xin Ying Qiu</author>
<author>Padmini Srinivasan</author>
</authors>
<title>The language of bioscience: Facts, speculations, and statements in between.</title>
<date>2004</date>
<booktitle>In Lynette Hirschman and James Pustejovsky, editors, HLT-NAACL 2004 Workshop: BioLINK 2004, Linking Biological Literature, Ontologies and Databases,</booktitle>
<publisher>ACL.</publisher>
<location>Boston, USA.</location>
<contexts>
<context position="5509" citStr="Light et al. (2004)" startWordPosition="869" endWordPosition="872">as an approximate L2-regularized maximum margin structured prediction problem. Our official results for Task 1 for the biological domain are 85.2 F1- score, for the Wikipedia set 55.4 F1-score. For Task 2, our official results were 2.1 for the entire task with a score of 62.5 for cue detection. After resolving errors and unfortunate bugs, our final results are for Task 1, biological: 86.0, Wikipedia: 58.2; Task 2: 39.6 and 78.5 for cues. 2 Detecting Sentence Level Uncertainty On the sentence level, word- and lemma-based features have been shown to be useful for uncertainty detection (see e.g. Light et al. (2004), Medlock and Briscoe (2007), Medlock (2008), and Szarvas (2008)). Medlock (2008) and Szarvas (2008) employ probabilistic, weakly supervised methods, where in the former, a stemmed single term and bigram representation achieved best results (0.82 BEP), and in the latter, a more complex n-gram feature selection procedure was applied using a Maximum Entropy classifier, achieving best results when adding reliable keywords from an external hedge keyword dictionary (0.85 BEP, 85.08 F1-score on biomedical articles). More linguistically motivated features are used by Kilicoglu and Bergler (2008), suc</context>
</contexts>
<marker>Light, Qiu, Srinivasan, 2004</marker>
<rawString>Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 2004. The language of bioscience: Facts, speculations, and statements in between. In Lynette Hirschman and James Pustejovsky, editors, HLT-NAACL 2004 Workshop: BioLINK 2004, Linking Biological Literature, Ontologies and Databases, Boston, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Medlock</author>
<author>Ted Briscoe</author>
</authors>
<title>Weakly supervised learning for hedge classification in scientific literature.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5537" citStr="Medlock and Briscoe (2007)" startWordPosition="873" endWordPosition="877">regularized maximum margin structured prediction problem. Our official results for Task 1 for the biological domain are 85.2 F1- score, for the Wikipedia set 55.4 F1-score. For Task 2, our official results were 2.1 for the entire task with a score of 62.5 for cue detection. After resolving errors and unfortunate bugs, our final results are for Task 1, biological: 86.0, Wikipedia: 58.2; Task 2: 39.6 and 78.5 for cues. 2 Detecting Sentence Level Uncertainty On the sentence level, word- and lemma-based features have been shown to be useful for uncertainty detection (see e.g. Light et al. (2004), Medlock and Briscoe (2007), Medlock (2008), and Szarvas (2008)). Medlock (2008) and Szarvas (2008) employ probabilistic, weakly supervised methods, where in the former, a stemmed single term and bigram representation achieved best results (0.82 BEP), and in the latter, a more complex n-gram feature selection procedure was applied using a Maximum Entropy classifier, achieving best results when adding reliable keywords from an external hedge keyword dictionary (0.85 BEP, 85.08 F1-score on biomedical articles). More linguistically motivated features are used by Kilicoglu and Bergler (2008), such as negated “unhedging” ver</context>
</contexts>
<marker>Medlock, Briscoe, 2007</marker>
<rawString>Ben Medlock and Ted Briscoe. 2007. Weakly supervised learning for hedge classification in scientific literature. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Medlock</author>
</authors>
<title>Exploring hedge identification in biomedical literature.</title>
<date>2008</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>41</volume>
<issue>4</issue>
<contexts>
<context position="5553" citStr="Medlock (2008)" startWordPosition="878" endWordPosition="879">tructured prediction problem. Our official results for Task 1 for the biological domain are 85.2 F1- score, for the Wikipedia set 55.4 F1-score. For Task 2, our official results were 2.1 for the entire task with a score of 62.5 for cue detection. After resolving errors and unfortunate bugs, our final results are for Task 1, biological: 86.0, Wikipedia: 58.2; Task 2: 39.6 and 78.5 for cues. 2 Detecting Sentence Level Uncertainty On the sentence level, word- and lemma-based features have been shown to be useful for uncertainty detection (see e.g. Light et al. (2004), Medlock and Briscoe (2007), Medlock (2008), and Szarvas (2008)). Medlock (2008) and Szarvas (2008) employ probabilistic, weakly supervised methods, where in the former, a stemmed single term and bigram representation achieved best results (0.82 BEP), and in the latter, a more complex n-gram feature selection procedure was applied using a Maximum Entropy classifier, achieving best results when adding reliable keywords from an external hedge keyword dictionary (0.85 BEP, 85.08 F1-score on biomedical articles). More linguistically motivated features are used by Kilicoglu and Bergler (2008), such as negated “unhedging” verbs and nouns and</context>
</contexts>
<marker>Medlock, 2008</marker>
<rawString>Ben Medlock. 2008. Exploring hedge identification in biomedical literature. Journal of Biomedical Informatics, 41(4):636–654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roser Morante</author>
<author>Walter Daelemans</author>
</authors>
<title>Learning the scope of hedge cues in biomedical texts.</title>
<date>2009</date>
<booktitle>In BioNLP ’09: Proceedings of Workshop on BioNLP,</booktitle>
<publisher>ACL.</publisher>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="13330" citStr="Morante and Daelemans (2009)" startWordPosition="2125" endWordPosition="2128">or each candidate feature set, however, this was simply too time consuming for the present work. A further risk of using forward selection is that feature interactions are ignored. This issue is handled better with backward elimination, but that is also more time consuming. The full set of explored feature templates is too large to be listed here; instead we list the features selected in each iteration of the search, together with their corresponding scores, in Table 1. 3 Detecting In-sentence Uncertainty When it comes to the automatic identification of hedge cues and their linguistic scopes, Morante and Daelemans (2009) and ¨Ozg¨ur and Radev (2009) report experiments on the BioScope corpus (Vincze et al., 2008), achieving best results (10-fold cross evaluation) on the identification of hedge cues of 71.59 F-score (using IGTree with current, preceding and subsequent word and cur86 Task Template set Dev F1 Test F1 SENLEN - - U LEMMA 88.9 (.25) 78.79 U LEMMABI 90.3 (.19) 85.86 U LEMMA®QUANT 90.3 (.07) 85.97 SENLEN - - Wiki U TOKEN®DOCPT 59.0 (.76) 60.12 U TOKENBI®SENLEN 59.9 (.09) 58.26 Table 1: Top feature templates for sentence level hedge and weasel detection. rent lemma as features) and 82.82 F-score (using</context>
<context position="30183" citStr="Morante and Daelemans (2009)" startWordPosition="5012" endWordPosition="5015">correct” solution has it outside the scope. These errors are not important to address, but are quite frequent in our results – approximately 80 errors are of this type. To achieve more general and effective methods to detect uncertainty in an argument, we should note that uncertainty is signalled in a text through many mechanisms, and that the purely lexical and explicit signal found through the present experiments in hedge identification is effective and useful, but will not catch everything we might want to find. Lexical approaches are also domain dependent. For instance, Szarvas (2008) and Morante and Daelemans (2009) report loss in performance, when applying the same methods developed on biological data, on clinical text. Using the systems developed for scientific text elsewhere poses a migration challenge. It would be desirable both to automatically learn a hedging lexicon from a general seed set and to have features on a higher level of abstraction. Our main result is that casting this task as a sequence labelling problem affords us the possibility to combine linguistic analyses with a highly efficient implementation of a max-margin prediction algorithm. Our framework processes the data sets in minutes </context>
</contexts>
<marker>Morante, Daelemans, 2009</marker>
<rawString>Roser Morante and Walter Daelemans. 2009. Learning the scope of hedge cues in biomedical texts. In BioNLP ’09: Proceedings of Workshop on BioNLP, Morristown, NJ, USA. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Y Ng</author>
</authors>
<title>Feature selection, l1 vs. l2 regularization, and rotational invariance.</title>
<date>2004</date>
<booktitle>In ICML ’04: Proceedings of the 21st International Conference on Machine learning,</booktitle>
<pages>78</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8580" citStr="Ng (2004)" startWordPosition="1362" endWordPosition="1363">In both cases, we approximately optimise the F1-measure by weighting each class by the inverse of its proportion in the training data. The reason for using L1-regularisation in the biological domain is that the annotation is heavily biased towards a rather small number of lexical cues, making most of the potential surface features irrelevant. The Wikipedia weasel annotation, on the other hand, is much more noisy and less determined by specific lexical markers. Regularising with respect to the L1-norm is known to give preference to sparse models and for the special case of logistic regression, Ng (2004) proved that the sample complexity grows only logarithmically in the number of irrelevant features, instead of linearly as when regularising with respect to the L2- norm. Our preliminary experiments indicated that L1-regularisation is superior to L2-regularisation in the biological domain, while slightly inferior in 85 the Wikipedia domain. 2.2 Feature Definitions The asymmetric relationship between certain and uncertain sentences becomes evident when one tries to learn this distinction based on surface level cues. While the UNCERTAIN category is to a large extent explicitly anchored in lexica</context>
</contexts>
<marker>Ng, 2004</marker>
<rawString>Andrew Y. Ng. 2004. Feature selection, l1 vs. l2 regularization, and rotational invariance. In ICML ’04: Proceedings of the 21st International Conference on Machine learning, page 78, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arzucan ¨Ozg¨ur</author>
<author>Dragomir R Radev</author>
</authors>
<title>Detecting speculations and their scopes in scientific text.</title>
<date>2009</date>
<booktitle>In Proceedings of 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>ACL.</publisher>
<marker>¨Ozg¨ur, Radev, 2009</marker>
<rawString>Arzucan ¨Ozg¨ur and Dragomir R. Radev. 2009. Detecting speculations and their scopes in scientific text. In Proceedings of 2009 Conference on Empirical Methods in Natural Language Processing, Singapore. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Randolph Quirk</author>
<author>Sidney Greenbaum</author>
<author>Geoffrey Leech</author>
<author>Jan Svartvik</author>
</authors>
<title>A comprehensive grammar of the English language.</title>
<date>1985</date>
<publisher>Longman.</publisher>
<contexts>
<context position="3007" citStr="Quirk et al. (1985)" startWordPosition="472" endWordPosition="475">sence of such markers does not necessarily indicate certainty – the opposition between certain and uncertain is not clearly demarkable, but more of a dimensional measure. Uncertainty on the part of the speaker may be difficult to differentiate from a certain assessment of an uncertain situation, It is unclear whether this specimen is an X or a Y vs. The difference between X and Y is unclear. In this task, the basis for identifying uncertainty in utterances is almost entirely lexical. Hedges, the main target of this experiment, are an established category in lexical grammar analyses - see e.g. Quirk et al. (1985), for examples of English language constructions. Most languages use various verbal markers or modifiers for indicating the speaker’s beliefs in what is being said, most prototypically using conditional or optative verb forms, Six Parisiens seraient morts, or auxiliaries, This mushroom may be edible, but aspectual markers may also be recruited for this purpose, more indirectly, I’m hoping you will help vs. I hope you will help; Do you want to see me now vs. Did you want to see me now. Besides verbs, there are classes of terms that through their presence, typically in an adverbial role, in an u</context>
</contexts>
<marker>Quirk, Greenbaum, Leech, Svartvik, 1985</marker>
<rawString>Randolph Quirk, Sidney Greenbaum, Geoffrey Leech, and Jan Svartvik. 1985. A comprehensive grammar of the English language. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan D Ratliff</author>
<author>Andrew J Bagnell</author>
<author>Martin A Zinkevich</author>
</authors>
<title>(Online) subgradient methods for structured prediction.</title>
<date>2007</date>
<booktitle>In Eleventh International Conference on Artificial Intelligence and Statistics (AIStats).</booktitle>
<contexts>
<context position="15266" citStr="Ratliff et al. (2007)" startWordPosition="2428" endWordPosition="2431">structured prediction problems, of which sequence labelling is a special case, have been suggested. Among others, Conditional Random Fields (Lafferty et al., 2001), Max-Margin Markov Networks (Taskar et al., 2003), and Structured Support Vector Machines (Tsochantaridis et al., 2005). A drawback of these approaches is that they are all quite computationally demanding. As an alternative, we propose a much more computationally lenient approach based on the regularised margin-rescaling formulation of Taskar et al. (2003), which we instead optimise by stochastic subgradient descent as suggested by Ratliff et al. (2007). In addition we only perform approximate decoding, using beam search, which allows arbitrary complex joint feature maps to be employed, without sacrificing speed. 3.1.1 Technical Details Let X denote the pattern set and let Y denote the set of structured labels. Let A denote the set of atomic labels and let each label y E Y consist of an indexed sequence of atomic labels yi E A. Denote by Yx C Y the set of possible label assignments to pattern x E X and by yx E Yx its correct label. In the specific case of BIO-sequence labelling, A = {BEGIN, INSIDE, OUTSIDE} and Yx = A|x|, where IxI is the le</context>
<context position="17483" citStr="Ratliff et al., 2007" startWordPosition="2856" endWordPosition="2859">g data D. In other words, we want the score of the correct label to be higher than the score plus the loss, of all other labels, for each instance. In order to balance margin maximisation and margin violation, we add the L2-regularisation term 11w112. By making use of the loss augmented decoding function fΔ(x, yx) = argmax [S(x, y) + A(yx, y)], (1) y∈Y. we get the following regularised risk functional: SΔ(x(i), yx(i)) + λ2 11w112, (2) where SΔ(x, yx) = max [S(x, y) + A(yx, y)]−S(x, yx) y∈Y. (3) We optimise (2) by stochastic approximate subgradient descent with step size sequence [η0/1/t]∞�=1 (Ratliff et al., 2007). The initial step size η0 and the regularisation factor λ are data dependent hyper-parameters, which we tune by crossvalidation. Bio QA,D(w) = � |D| i=1 87 This framework is highly efficient both at learning and prediction time. Training cues and scopes on the biological data, takes about a minute, while prediction times are in the order of seconds, using a Java based implementation on a standard laptop; the absolute majority of that time is spent on reading and extracting features from an inefficient internal JSON-based format. 3.1.2 Hashed Feature Functions Joint feature functions enable en</context>
</contexts>
<marker>Ratliff, Bagnell, Zinkevich, 2007</marker>
<rawString>Nathan D. Ratliff, Andrew J. Bagnell, and Martin A. Zinkevich. 2007. (Online) subgradient methods for structured prediction. In Eleventh International Conference on Artificial Intelligence and Statistics (AIStats).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>Hedge classification in biomedical texts with a weakly supervised selection of keywords.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<publisher>ACL.</publisher>
<location>Columbus, Ohio.</location>
<contexts>
<context position="5573" citStr="Szarvas (2008)" startWordPosition="881" endWordPosition="882"> problem. Our official results for Task 1 for the biological domain are 85.2 F1- score, for the Wikipedia set 55.4 F1-score. For Task 2, our official results were 2.1 for the entire task with a score of 62.5 for cue detection. After resolving errors and unfortunate bugs, our final results are for Task 1, biological: 86.0, Wikipedia: 58.2; Task 2: 39.6 and 78.5 for cues. 2 Detecting Sentence Level Uncertainty On the sentence level, word- and lemma-based features have been shown to be useful for uncertainty detection (see e.g. Light et al. (2004), Medlock and Briscoe (2007), Medlock (2008), and Szarvas (2008)). Medlock (2008) and Szarvas (2008) employ probabilistic, weakly supervised methods, where in the former, a stemmed single term and bigram representation achieved best results (0.82 BEP), and in the latter, a more complex n-gram feature selection procedure was applied using a Maximum Entropy classifier, achieving best results when adding reliable keywords from an external hedge keyword dictionary (0.85 BEP, 85.08 F1-score on biomedical articles). More linguistically motivated features are used by Kilicoglu and Bergler (2008), such as negated “unhedging” verbs and nouns and that preceded by ep</context>
<context position="30150" citStr="Szarvas (2008)" startWordPosition="5009" endWordPosition="5010"> scope, where the “correct” solution has it outside the scope. These errors are not important to address, but are quite frequent in our results – approximately 80 errors are of this type. To achieve more general and effective methods to detect uncertainty in an argument, we should note that uncertainty is signalled in a text through many mechanisms, and that the purely lexical and explicit signal found through the present experiments in hedge identification is effective and useful, but will not catch everything we might want to find. Lexical approaches are also domain dependent. For instance, Szarvas (2008) and Morante and Daelemans (2009) report loss in performance, when applying the same methods developed on biological data, on clinical text. Using the systems developed for scientific text elsewhere poses a migration challenge. It would be desirable both to automatically learn a hedging lexicon from a general seed set and to have features on a higher level of abstraction. Our main result is that casting this task as a sequence labelling problem affords us the possibility to combine linguistic analyses with a highly efficient implementation of a max-margin prediction algorithm. Our framework pr</context>
</contexts>
<marker>Szarvas, 2008</marker>
<rawString>Gy¨orgy Szarvas. 2008. Hedge classification in biomedical texts with a weakly supervised selection of keywords. In Proceedings of ACL-08: HLT, Columbus, Ohio. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo J¨arvinen</author>
</authors>
<title>A non-projective dependency parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing.</booktitle>
<marker>Tapanainen, J¨arvinen, 1997</marker>
<rawString>Pasi Tapanainen and Timo J¨arvinen. 1997. A non-projective dependency parser. In Proceedings of the 5th Conference on Applied Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Taskar</author>
<author>Carlos Guestrin</author>
<author>Daphne Koller</author>
</authors>
<title>Max-margin Markov networks.</title>
<date>2003</date>
<editor>In Sebastian Thrun, Lawrence K. Saul, and Bernhard Sch¨olkopf, editors, NIPS.</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="14858" citStr="Taskar et al., 2003" startWordPosition="2366" endWordPosition="2369">use a rule-based method for this subtask, while Morante and Daelemans (2009) use three different classifiers as input to a CRF-based meta-learner, with a complex set of features, including hedge cue information, current and surrounding token information, distance information and location information. 3.1 Learning and Optimisation Framework In recent years, a wide range of different approaches to general structured prediction problems, of which sequence labelling is a special case, have been suggested. Among others, Conditional Random Fields (Lafferty et al., 2001), Max-Margin Markov Networks (Taskar et al., 2003), and Structured Support Vector Machines (Tsochantaridis et al., 2005). A drawback of these approaches is that they are all quite computationally demanding. As an alternative, we propose a much more computationally lenient approach based on the regularised margin-rescaling formulation of Taskar et al. (2003), which we instead optimise by stochastic subgradient descent as suggested by Ratliff et al. (2007). In addition we only perform approximate decoding, using beam search, which allows arbitrary complex joint feature maps to be employed, without sacrificing speed. 3.1.1 Technical Details Let </context>
<context position="16502" citStr="Taskar et al., 2003" startWordPosition="2668" endWordPosition="2671">ence xEX. A structured classification problem amounts to learning a mapping from patterns to labels, f : X H Y, such that the expected loss EX×Y[A(yx, f(x))] is minimised. The prediction loss, A : Y x Y H R+, measures the loss of predicting label y = f(x) when the correct label is yx, with A(yx, yx) = 0. Here we assume the Hamming loss, AH(y, y0) = E|y| i=1 δ(yi, y0i), where δ(yi, y0i) = 1 if yi =� y0i and 0 otherwise. The idea of the margin-rescaling approach is to let the structured margin between the correct label yx and a hypothesis y E Yx scale linearly with the prediction loss A(yx, y) (Taskar et al., 2003). The structured margin is defined in terms of a score function S : X x Y H R, in our case the linear score function S(x, y) = w&apos;Φ(x, y), where w E R&apos; is a vector of parameters and Φ : X xY H R&apos; is a joint feature function. The learning problem then amounts to finding parameters w such that S(x, yx) &gt; S(x, y) + A(yx, y) for all y E Yx \ {yx} over the training data D. In other words, we want the score of the correct label to be higher than the score plus the loss, of all other labels, for each instance. In order to balance margin maximisation and margin violation, we add the L2-regularisation t</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>Benjamin Taskar, Carlos Guestrin, and Daphne Koller. 2003. Max-margin Markov networks. In Sebastian Thrun, Lawrence K. Saul, and Bernhard Sch¨olkopf, editors, NIPS. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thorsten Joachims</author>
<author>Thomas Hofmann</author>
<author>Yasemin Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>6--1453</pages>
<contexts>
<context position="14928" citStr="Tsochantaridis et al., 2005" startWordPosition="2375" endWordPosition="2378">elemans (2009) use three different classifiers as input to a CRF-based meta-learner, with a complex set of features, including hedge cue information, current and surrounding token information, distance information and location information. 3.1 Learning and Optimisation Framework In recent years, a wide range of different approaches to general structured prediction problems, of which sequence labelling is a special case, have been suggested. Among others, Conditional Random Fields (Lafferty et al., 2001), Max-Margin Markov Networks (Taskar et al., 2003), and Structured Support Vector Machines (Tsochantaridis et al., 2005). A drawback of these approaches is that they are all quite computationally demanding. As an alternative, we propose a much more computationally lenient approach based on the regularised margin-rescaling formulation of Taskar et al. (2003), which we instead optimise by stochastic subgradient descent as suggested by Ratliff et al. (2007). In addition we only perform approximate decoding, using beam search, which allows arbitrary complex joint feature maps to be employed, without sacrificing speed. 3.1.1 Technical Details Let X denote the pattern set and let Y denote the set of structured labels</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. Journal of Machine Learning Research, 6:1453–1484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>Gy¨orgy Szarvas</author>
<author>Rich´ard Farkas</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
</authors>
<title>The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<marker>Vincze, Szarvas, Farkas, M´ora, Csirik, 2008</marker>
<rawString>Veronika Vincze, Gy¨orgy Szarvas, Rich´ard Farkas, Gy¨orgy M´ora, and J´anos Csirik. 2008. The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes. BMC Bioinformatics, 9(S-11).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Weinberger</author>
<author>Anirban Dasgupta</author>
<author>John Langford</author>
<author>Alex Smola</author>
<author>Josh Attenberg</author>
</authors>
<title>Feature hashing for large scale multitask learning.</title>
<date>2009</date>
<booktitle>In ICML ’09: Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="19079" citStr="Weinberger et al. (2009)" startWordPosition="3135" endWordPosition="3138">_1 E Ai_1 when decoding at position i. The feature mapping used in scoring candidate label yi E A is then computed as the Cartesian product Φ(x, y, i) = Ψ(x, y1:i_1, i) � A(yi), where A(yi) E R&apos; is a unique unitary feature vector representation of label yi. The feature representation for a complete sequence x and its associated label y is then computed as Φ(x, y) = � |x |Φ(x, y, i) i=1 When employing joint feature functions and combined features, the number of unique features may grow very large. This is a problem when the amount of internal memory is limited. Feature hashing, as described by Weinberger et al. (2009), is a simple trick to circumvent this problem. Assume that we have an original feature function O : X x Y H R&apos;, where m might be arbitrarily large. Let h : N+ H [1, n] be a hash function and let h_1(i) C [1, m] be the set of integers such that j E h_1(i) iff h(j) = i. We now use this hash function to map the index of each feature in O(x, y) to its corresponding index in Φ(x, y), as Φi(x, y) = Ejlh−1(i) Oj(x, y). The features in Φ are thus unions of multisets of features in O. Given a hash function with good collision properties, we can expect that the subset of features mapped to any index in</context>
</contexts>
<marker>Weinberger, Dasgupta, Langford, Smola, Attenberg, 2009</marker>
<rawString>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. 2009. Feature hashing for large scale multitask learning. In ICML ’09: Proceedings of the 26th Annual International Conference on Machine Learning, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>