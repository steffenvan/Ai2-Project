<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.876019">
Information Extraction over Structured Data:
Question Answering with Freebase
</title>
<author confidence="0.719602">
Xuchen Yao 1 and Benjamin Van Durme 1,2
</author>
<affiliation confidence="0.870127">
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
</affiliation>
<address confidence="0.725814">
Baltimore, MD, USA
</address>
<sectionHeader confidence="0.951469" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999952714285714">
Answering natural language questions us-
ing the Freebase knowledge base has re-
cently been explored as a platform for ad-
vancing the state of the art in open do-
main semantic parsing. Those efforts map
questions to sophisticated meaning repre-
sentations that are then attempted to be
matched against viable answer candidates
in the knowledge base. Here we show
that relatively modest information extrac-
tion techniques, when paired with a web-
scale corpus, can outperform these sophis-
ticated approaches by roughly 34% rela-
tive gain.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999871523809524">
Question answering (QA) from a knowledge base
(KB) has a long history within natural language
processing, going back to the 1960s and 1970s,
with systems such as Baseball (Green Jr et al.,
1961) and Lunar (Woods, 1977). These systems
were limited to closed-domains due to a lack of
knowledge resources, computing power, and abil-
ity to robustly understand natural language. With
the recent growth in KBs such as DBPedia (Auer
et al., 2007), Freebase (Bollacker et al., 2008)
and Yago2 (Hoffart et al., 2011), it has be-
come more practical to consider answering ques-
tions across wider domains, with commercial sys-
tems including Google Now, based on Google’s
Knowledge Graph, and Facebook Graph
Search, based on social network connections.
The AI community has tended to approach this
problem with a focus on first understanding the in-
tent of the question, via shallow or deep forms of
semantic parsing (c.f. §3 for a discussion). Typ-
ically questions are converted into some mean-
ing representation (e.g., the lambda calculus), then
mapped to database queries. Performance is thus
bounded by the accuracy of the original seman-
tic parsing, and the well-formedness of resultant
database queries.1
The Information Extraction (IE) community ap-
proaches QA differently: first performing rela-
tively coarse information retrieval as a way to
triage the set of possible answer candidates, and
only then attempting to perform deeper analysis.
Researchers in semantic parsing have recently
explored QA over Freebase as a way of moving
beyond closed domains such as GeoQuery (Tang
and Mooney, 2001). While making semantic pars-
ing more robust is a laudable goal, here we provide
a more rigorous IE baseline against which those
efforts should be compared: we show that “tradi-
tional” IE methodology can significantly outper-
form prior state-of-the-art as reported in the se-
mantic parsing literature, with a relative gain of
34% F1 as compared to Berant et al. (2013).
</bodyText>
<sectionHeader confidence="0.99366" genericHeader="introduction">
2 Approach
</sectionHeader>
<bodyText confidence="0.999127625">
We will view a KB as an interlinked collection of
“topics”. When given a question about one or sev-
eral topics, we can select a “view” of the KB con-
cerning only involved topics, then inspect every
related node within a few hops of relations to the
topic node in order to extract the answer. We call
such a view a topic graph and assume answers can
be found within the graph. We aim to maximally
automate the answer extraction process, by mas-
sively combining discriminative features for both
the question and the topic graph. With a high per-
formance learner we have found that a system with
millions of features can be trained within hours,
leading to intuitive, human interpretable features.
For example, we learn that given a question con-
cerning money, such as: what money is used in
</bodyText>
<footnote confidence="0.892755666666667">
1As an example, 50% of errors of the CCG-backed
(Kwiatkowski et al., 2013) system were contributed by pars-
ing or structural matching failure.
</footnote>
<page confidence="0.926229">
956
</page>
<note confidence="0.8329355">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 956–966,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.99997475862069">
ukraine, the expected answer type is likely cur-
rency. We formalize this approach in §4.
One challenge for natural language querying
against a KB is the relative informality of queries
as compared to the grammar of a KB. For exam-
ple, for the question: who cheated on celebrity
A, answers can be retrieved via the Freebase rela-
tion celebrity.infidelity.participant, but the con-
nection between the phrase cheated on and the
formal KB relation is not explicit. To allevi-
ate this problem, the best attempt so far is to
map from ReVerb (Fader et al., 2011) predicate-
argument triples to Freebase relation triples (Cai
and Yates, 2013; Berant et al., 2013). Note that
to boost precision, ReVerb has already pruned
down less frequent or credible triples, yielding not
as much coverage as its text source, ClueWeb.
Here we instead directly mine relation mappings
from ClueWeb and show that both direct relation
mapping precision and indirect QA F1 improve by
a large margin. Details in §5.
Finally, we tested our system, jacana-
freebase,2 on a realistic dataset generously
contributed by Berant et al. (2013), who collected
thousands of commonly asked questions by
crawling the Google Suggest service. Our
method achieves state-of-the-art performance
with F1 at 42.0%, a 34% relative increase from
the previous F1 of 31.4%.
</bodyText>
<sectionHeader confidence="0.998445" genericHeader="method">
3 Background
</sectionHeader>
<bodyText confidence="0.999945368421053">
QA from a KB faces two prominent challenges:
model and data. The model challenge involves
finding the best meaning representation for the
question, converting it into a query and exe-
cuting the query on the KB. Most work ap-
proaches this via the bridge of various interme-
diate representations, including combinatory cat-
egorial grammar (Zettlemoyer and Collins, 2005,
2007, 2009; Kwiatkowski et al., 2010, 2011,
2013), synchronous context-free grammars (Wong
and Mooney, 2007), dependency trees (Liang et
al., 2011; Berant et al., 2013), string kernels (Kate
and Mooney, 2006; Chen and Mooney, 2011),
and tree transducers (Jones et al., 2012). These
works successfully showed their effectiveness in
QA, despite the fact that most of them require
hand-labeled logic annotations. More recent re-
search started to minimize this direct supervision
by using latent meaning representations (Berant et
</bodyText>
<footnote confidence="0.751463">
2https://code.google.com/p/jacana
</footnote>
<bodyText confidence="0.99561756097561">
al., 2013; Kwiatkowski et al., 2013) or distant su-
pervision (Krishnamurthy and Mitchell, 2012).
We instead attack the problem of QA from a KB
from an IE perspective: we learn directly the pat-
tern of QA pairs, represented by the dependency
parse of questions and the Freebase structure of
answer candidates, without the use of intermedi-
ate, general purpose meaning representations.
The data challenge is more formally framed as
ontology or (textual) schema matching (Hobbs,
1985; Rahm and Bernstein, 2001; Euzenat and
Shvaiko, 2007): matching structure of two on-
tologies/databases or (in extension) mapping be-
tween KB relations and NL text. In terms of
the latter, Cai and Yates (2013) and Berant et al.
(2013) applied pattern matching and relation inter-
section between Freebase relations and predicate-
argument triples from the ReVerb OpenIE sys-
tem (Fader et al., 2011). Kwiatkowski et al.
(2013) expanded their CCG lexicon with Wik-
tionary word tags towards more domain indepen-
dence. Fader et al. (2013) learned question para-
phrases from aligning multiple questions with the
same answers generated by WikiAnswers. The
key factor to their success is to have a huge text
source. Our work pushes the data challenge to the
limit by mining directly from ClueWeb, a 5TB
collection of web data.
Finally, the KB community has developed other
means for QA without semantic parsing (Lopez et
al., 2005; Frank et al., 2007; Unger et al., 2012;
Yahya et al., 2012; Shekarpour et al., 2013). Most
of these work executed SPARQL queries on in-
terlinked data represented by RDF (Resource De-
scription Framework) triples, or simply performed
triple matching. Heuristics and manual templates
were also commonly used (Chu-Carroll et al.,
2012). We propose instead to learn discriminative
features from the data with shallow question anal-
ysis. The final system captures intuitive patterns
of QA pairs automatically.
</bodyText>
<sectionHeader confidence="0.988983" genericHeader="method">
4 Graph Features
</sectionHeader>
<bodyText confidence="0.9980278">
Our model is inspired by an intuition on how ev-
eryday people search for answers. If you asked
someone: what is the name of justin bieber
brother,3 and gave them access to Freebase, that
person might first determine that the question
</bodyText>
<footnote confidence="0.938392666666667">
3All examples used in this paper come from the train-
ing data crawled from Google Suggest. They are low-
ercased and some contain typos.
</footnote>
<page confidence="0.997251">
957
</page>
<bodyText confidence="0.999881285714286">
is about Justin Bieber (or his brother), go to
Justin Bieber’s Freebase page, and search for his
brother’s name. Unfortunately Freebase does not
contain an exact relation called brother, but in-
stead sibling. Thus further inference (i.e., brother
H male sibling) has to be made. In the following
we describe how we represent this process.
</bodyText>
<subsectionHeader confidence="0.999547">
4.1 Question Graph
</subsectionHeader>
<bodyText confidence="0.99996675">
In answering our example query a person might
take into consideration multiple constraints. With
regards to the question, we know we are looking
for the name of a person based on the following:
</bodyText>
<listItem confidence="0.902692909090909">
• the dependency relation nsubj(what, name)
and prep of(name, brother) indicates that the
question seeks the information of a name;4
• the dependency relation prep of(name,
brother) indicates that the name is about a
brother (but we do not know whether it is a
person name yet);
• the dependency relation nn(brother, bieber)
and the facts that, (i) Bieber is a person and (ii)
a person’s brother should also be a person, indi-
cate that the name is about a person.
</listItem>
<bodyText confidence="0.97685875">
This motivates the design of dependency-based
features. We show one example in Figure 1(a),
left side. The following linguistic information is
of interest:
</bodyText>
<listItem confidence="0.960281076923077">
• question word (qword), such as what/who/how
many. We use a list of 9 common qwords. 5
• question focus (qfocus), a cue of expected an-
swer types, such as name/money/time. We
keep our analysis simple and do not use a ques-
tion classifier, but simply extract the noun de-
pendent of qword as qfocus.
• question verb (qverb), such as is/play/take, ex-
tracted from the main verb of the question.
Question verbs are also good hints of answer
types. For instance, play is likely to be followed
by an instrument, a movie or a sports team.
• question topic (qtopic). The topic of the ques-
</listItem>
<bodyText confidence="0.752484">
tion helps us find relevant Freebase pages. We
simply apply a named entity recognizer to find
the question topic. Note that there can be more
than one topic in the question.
Then we convert the dependency parse into a more
generic question graph, in the following steps:
</bodyText>
<footnote confidence="0.959181666666667">
4We use the Stanford collapsed dependency form.
5who, when, what, where, how, which, why, whom,
whose.
</footnote>
<listItem confidence="0.946362222222222">
1. if a node was tagged with a question feature,
then replace this node with its question feature,
e.g., what → qword=what;
2. (special case) if a qtopic node was tagged as
a named entity, then replace this node with
its its named entity form, e.g., bieber →
qtopic=person;
3. drop any leaf node that is a determiner, prepo-
sition or punctuation.
</listItem>
<bodyText confidence="0.999796842105263">
The converted graph is shown in Figure 1(a),
right side. We call this a question feature graph,
with every node and relation a potential feature
for this question. Then features are extracted
in the following form: with s the source and
t the target node, for every edge e(s, t) in the
graph, extract s, t, s  |t and s  |e  |t as
features. For the edge, prep of(qfocus=name,
brother), this would mean the following features:
qfocus=name, brother, qfocus=name|brother,
and qfocus=name|prep of|brother.
We show with examples why these features
make sense later in §6 Table 6. Furthermore, the
reason that we have kept some lexical features,
such as brother, is that we hope to learn from
training a high correlation between brother and
some Freebase relations and properties (such as
sibling and male) if we do not possess an exter-
nal resource to help us identify such a correlation.
</bodyText>
<subsectionHeader confidence="0.996087">
4.2 Freebase Topic Graph
</subsectionHeader>
<bodyText confidence="0.999869476190476">
Given a topic, we selectively roll out the Free-
base graph by choosing those nodes within a few
hops of relationship to the topic node, and form
a topic graph. Besides incoming and/or outgo-
ing relationships, nodes also have properties: a
string that describes the attribute of a node, for
instance, node type, gender or height (for a per-
son). One major difference between relations and
properties is that both arguments of a relation are
nodes, while only one argument of a property is a
node, the other a string. Arguments of relations are
usually interconnected, e.g., London can be the
place of birth for Justin Bieber, or capital of
the UK. Arguments of properties are attributes that
are only “attached” to certain nodes and have no
outgoing edges. Figure 1(b) shows an example.
Both relationship and property of a node are
important to identifying the answer. They con-
nect the nodes with the question and describe
some unique characteristics. For instance, with-
out the properties type:person and gender:male,
</bodyText>
<page confidence="0.980921">
958
</page>
<figure confidence="0.534305">
(a) Dependence parse with annotated question features in dashed boxes (left) and converted feature graph (right) with
only relevant and general information about the original question kept. Note that the left is a real but incorrect parse.
(b) A view of Freebase graph on the Justin Bieber topic with nodes in solid boxes and properties in
</figure>
<figureCaption confidence="0.826671">
dashed boxes. The hatching node, Jaxon Bieber, is the answer. Freebase uses a dummy parent node
for a list of nodes with the same relation.
Figure 1: Dependency parse and excerpted Freebase topic graph on the question what is the name of
justin bieber brother.
</figureCaption>
<figure confidence="0.999846057692308">
cop
cop
nsubj
nsubj
prep_of
det
prep_of
qword
what
qword=
what
qverb
qfocus
is name
qverb=
be
qfocus=
name
the brother
brother
nn
nn
nn
nn
qtopic
justin bieber
qtopic
qtopic=
person
qtopic=
person
type
person.sibling_s
Jazmyn Bieber
gender
sibling sibling
type
Jaxon Bieber
type
gender
female
person
male
person
dummy node
place_of_birth
awards_won
Justin Bieber
gender
person
male
London
</figure>
<page confidence="0.992137">
959
</page>
<bodyText confidence="0.999981862068966">
we would not have known the node Jaxon Bieber
represents a male person. These properties, along
with the sibling relationship to the topic node, are
important cues for answering the question. Thus
for the Freebase graph, we use relations (with di-
rections) and properties as features for each node.
Additionally, we have analyzed how Freebase
relations map back to the question. Some of the
mapping can be simply detected as paraphras-
ing or lexical overlap. For example, the per-
son.parents relationship helps answering ques-
tions about parenthood. However, most Freebase
relations are framed in a way that is not com-
monly addressed in natural language questions.
For instance, for common celebrity gossip ques-
tions like who cheated on celebrity A, it is
hard for a system to find the Freebase relation
celebrity.infidelity.participant as the target rela-
tion if it had not observed this pattern in training.
Thus assuming there is an alignment model that
is able to tell how likely one relation maps to the
original question, we add extra alignment-based
features for the incoming and outgoing relation of
each node. Specifically, for each relation rel in
a topic graph, we compute P(rel  |question) to
rank the relations. Finally the ranking (e.g., top
1/2/5/10/100 and beyond) of each relation is used
as features instead of a pure probability. We de-
scribe such an alignment model in § 5.
</bodyText>
<subsectionHeader confidence="0.997719">
4.3 Feature Production
</subsectionHeader>
<bodyText confidence="0.999069285714286">
We combine question features and Freebase fea-
tures (per node) by doing a pairwise concatena-
tion. In this way we hope to capture the associa-
tion between question patterns and answer nodes.
For instance, in a loglinear model setting, we ex-
pect to learn a high feature weight for features like:
qfocus=money|node type=currency
and a very low weight for:
qfocus=money|node type=person.
This combination greatly enlarges the total
number of features, but owing to progress in large-
scale machine learning such feature spaces are less
of a concern than they once were (concrete num-
bers in § 6 Model Tuning).
</bodyText>
<sectionHeader confidence="0.995831" genericHeader="method">
5 Relation Mapping
</sectionHeader>
<bodyText confidence="0.9970425">
In this section we describe a “translation” table be-
tween Freebase relations and NL words was built.
</bodyText>
<subsectionHeader confidence="0.916216">
5.1 Formula
</subsectionHeader>
<bodyText confidence="0.998892095238095">
The objective is to find the most likely rela-
tion a question prompts. For instance, for the
question who is the father of King George
VI, the most likely relation we look for is peo-
ple.person.parents. To put it more formally,
given a question Q of a word vector w, we want
to find out the relation R that maximizes the prob-
ability P(R  |Q).
More interestingly, for the question who is
the father of the Periodic Table, the ac-
tual relation that encodes its original mean-
ing is law.invention.inventor, rather than peo-
ple.person.parents. This simple example points
out that every part of the question could change
what the question inquires eventually. Thus we
need to count for each word w in Q. Due to the
bias and incompleteness of any data source, we
approximate the true probability of P with P˜ un-
der our specific model. For the simplicity of com-
putation, we assume conditional independence be-
tween words and apply Naive Bayes:
</bodyText>
<equation confidence="0.99655375">
P˜(R  |Q) ∝ P˜(Q  |R) P˜(R)
≈ P˜(w  |R) P˜(R)
11 ≈ P˜(w  |R) P˜(R)
w
</equation>
<bodyText confidence="0.99952775">
where P˜(R) is the prior probability of a relation
R and P˜(w  |R) is the conditional probability of
word w given R.
It is possible that we do not observe a certain
relation R when computing the above equation.
In this case we back off to the “sub-relations”: a
relation R is a concatenation of a series of sub-
relations R = r = r1.r2.r3..... For instance, the
sub-relations of people.person.parents are peo-
ple, person, and parents. Again, we assume con-
ditional independence between sub-relations and
apply Naive Bayes:
</bodyText>
<equation confidence="0.999754666666667">
˜Pbackoff(R  |Q) ≈ P˜(r  |Q)
11 ≈
r P˜(r  |Q)
11 ∝
r
P˜(w  |r) P˜(r)
</equation>
<bodyText confidence="0.6569895">
One other reason that we estimated
P˜(w  |r) and P˜(r) for sub-relations is
that Freebase relations share some com-
mon structures in between them. For in-
stance, both people.person.parents and
fictional universe.fictional character.parents
</bodyText>
<equation confidence="0.966092333333333">
P˜(Q  |r) P˜(r)
11 ≈ ri
r w
</equation>
<page confidence="0.924667">
960
</page>
<bodyText confidence="0.9999879">
indicate the parent relationship but the latter is
much less commonly annotated. We hope that the
shared sub-relation, parents, can help better esti-
mate for the less annotated. Note that the backoff
model would have a much smaller value than the
original, due to double multiplication H, Hw. In
practice we normalize it by the sub-relations size
to keep it at the same scale with P˜(R  |Q).
Finally, to estimate the prior and conditional
probability, we need a massive data collection.
</bodyText>
<subsectionHeader confidence="0.99948">
5.2 Steps
</subsectionHeader>
<bodyText confidence="0.999690047619048">
The ClueWeb096 dataset is a collection of 1 billion
webpages (5TB compressed in raw HTML) in 10
languages by Carnegie Mellon University in 2009.
FACC1, the Freebase Annotation of the ClueWeb
Corpus version 1 (Gabrilovich et al., 2013), con-
tains index and offset of Freebase entities within
the English portion of ClueWeb. Out of all 500
million English documents, 340 million were au-
tomatically annotated with at least one entity, with
an average of 15 entity mentions per document.
The precision and recall of annotation were esti-
mated at 80−85% and 70−85% (Orr et al., 2013).
Given these two resources, for each binary Free-
base relation, we can find a collection of sentences
each of which contains both of its arguments, then
simply learn how words in these sentences are as-
sociated with this relation, i.e., P˜(w  |R) and
P˜(w  |r). By counting how many times each rela-
tion R was annotated, we can estimate P˜(R) and
P˜(r). The learning task can be framed in the fol-
lowing short steps:
</bodyText>
<listItem confidence="0.974834636363636">
1. We split each HTML document by sentences
(Kiss and Strunk, 2006) using NLTK (Bird and
Loper, 2004) and extracted those with at least
two Freebase entities which has at least one di-
rect established relation according to Freebase.
2. The extraction formed two parallel corpora,
one with “relation - sentence” pairs (for esti-
mating P˜(w  |R) and P˜(R)) and the other with
“subrelations - sentence” pairs (for P˜(w  |r)
and P˜(r)). Each corpus has 1.2 billion pairs.
3. The tricky part was to align these 1.2 billion
</listItem>
<bodyText confidence="0.800926">
pairs. Since the relations on one side of these
pairs are not natural sentences, we ran the
most simple IBM alignment Model 1 (Brown
et al., 1993) to estimate the translation proba-
bility with GIZA++ (Och and Ney, 2003). To
speed up, the 1.2 billion pairs were split into
</bodyText>
<footnote confidence="0.964203">
6http://lemurproject.org/clueweb09/
</footnote>
<table confidence="0.9896895">
0 G 10 G 102 G 103 G 104 &gt; 104
7.0% 0.7% 1.2% 0.4% 1.3% 89.5%
</table>
<tableCaption confidence="0.997835">
Table 1: Percentage of answer relations (the in-
</tableCaption>
<bodyText confidence="0.847006888888889">
coming relation connected to the answer node)
with respect to how many sentences we learned
this relation from in CluewebMapping. For in-
stance, the first column says there are 7% of an-
swer relations for which we cannot find a mapping
(so we had to use the backoff probability estima-
tion); the last column says there are 89.5% of an-
swer relations that we were able to learn the map-
ping between this relation and text based on more
than 10 thousand relation-sentence pairs. The total
number of answer relations is 7886.
100 even chunks. We ran 5 iterations of EM on
each one and finally aligned the 1.2 billion pairs
from both directions. To symmetrize the align-
ment, common MT heuristics INTERSECTION,
UNION, GROW-DIAG-FINAL, and GROW-DIAG-
FINAL-AND (Koehn, 2010) were separately ap-
plied and evaluated later.
</bodyText>
<listItem confidence="0.930324428571429">
4. Treating the aligned pairs as observation, the
co-occurrence matrix between aligning rela-
tions and words was computed. There were
10,484 relations and sub-relations in all, and we
kept the top 20,000 words.
5. From the co-occurrence matrix we computed
P˜(w  |R), P˜(R), P˜(w  |r) and P˜(r).
</listItem>
<bodyText confidence="0.999977777777778">
Hand-checking the learned probabilities shows
both success, failure and some bias. For in-
stance, for the film.actor.film relation (mapping
from film names to actor names), the top words
given by P˜(w  |R) are won, star, among, show.
For the film.film.directed by relation, some im-
portant stop words that could indicate this re-
lation, such as by and with, rank directly after
director and direct. However, due to signifi-
cant popular interest in certain news categories,
and the resultant catering by websites to those
information desires, then for example we also
learned a heavily correlated connection between
Jennifer Aniston and celebrity.infidelity.victim,
and between some other you-know-who names
and celebrity.infidelity.participant.
We next formally evaluate how the learned map-
ping help predict relations from words.
</bodyText>
<page confidence="0.994889">
961
</page>
<subsectionHeader confidence="0.933644">
5.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.997473166666667">
Both ClueWeb and its Freebase annotation has a
bias. Thus we were firstly interested in the cov-
erage of mined relation mappings. As a com-
parison, we used a dataset of relation mapping
contributed by Berant et al. (2013) and Lin et al.
(2012). The idea is very similar: they intersected
Freebase relations with predicates in (arg1, predi-
cate, arg2) triples extracted from ReVerb to learn
the mapping between Freebase relations and triple
predicates. Note the scale difference: although
ReVerb was also extracted from ClueWeb09,
there were only 15 million triples to intersect with
the relations, while we had 1.2 billion alignment
pairs. We call this dataset ReverbMapping and
ours CluewebMapping.
The evaluation dataset, WEBQUESTIONS, was
also contributed by Berant et al. (2013). It con-
tains 3778 training and 2032 test questions col-
lected from the Google Suggest service. All ques-
tions were annotated with answers from Freebase.
Some questions have more than one answer, such
as what to see near sedona arizona?.
We evaluated on the training set in two aspects:
coverage and prediction performance. We define
answer node as the node that is the answer and
answer relation as the relation from the answer
node to its direct parent. Then we computed how
much and how well the answer relation was trig-
gered by ReverbMapping and CluewebMapping.
Thus for the question, who is the father of King
George VI, we ask two questions: does the map-
ping, 1. (coverage) contain the answer relation
people.person.parents? 2. (precision) predict
the answer relation from the question?
Table 1 shows the coverage of CluewebMap-
ping, which covers 93.0% of all answer rela-
tions. Among them, we were able to learn the rule
mapping using more than 10 thousand relation-
sentence pairs for each of the 89.5% of all an-
swer relations. In contrast, ReverbMapping covers
89.7% of the answer relations.
Next we evaluated the prediction performance,
using the evaluation metrics of information re-
trieval. For each question, we extracted all rela-
tions in its corresponding topic graph, and ranked
each relation with whether it is the answer re-
lation. For instance, for the previous exam-
ple question, we want to rank the relation peo-
ple.person.parents as number 1. We com-
puted standard MAP (Mean Average Precision)
and MRR (Mean Reciprocal Rank), shown in Ta-
ble 2(a). As a simple baseline, “word overlap”
counts the overlap between relations and the ques-
tion. CluewebMapping ranks each relation by
P˜(R Q). ReverbMapping does the same, ex-
cept that we took a uniform distribution on P˜(w
R) and P˜(R) since the contributed dataset did
not include co-occurrence counts to estimate these
probabilities.7 Note that the median rank from
CluewebMapping is only 12, indicating that half
of all answer relations are ranked in the top 12.
Table 2(b) further shows the percentage of
answer relations with respect to their rank-
ing. CluewebMapping successfully ranked 19%
of answer relations as top 1. A sample
of these includes person.place of birth, loca-
tion.containedby, country.currency used, reg-
ular tv appearance.actor, etc. These percentage
numbers are good clue for feature design: for in-
stance, we may be confident in a relation if it is
ranked top 5 or 10 by CluewebMapping.
To conclude, we found that CluewebMapping
provides satisfying coverage on the 3778 training
questions: only 7% were missing, despite the bi-
ased nature of web data. Also, CluewebMapping
gives reasonably good precision on its prediction,
despite the noisy nature of web data. We move on
to fully evaluate the final QA F1.
</bodyText>
<sectionHeader confidence="0.999511" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999985588235294">
We evaluate the final F1 in this section. The sys-
tem of comparison is that of Berant et al. (2013).
Data We re-used WEBQUESTIONS, a dataset
collected by Berant et al. (2013). It contains 5810
questions crawled from the Google Suggest ser-
vice, with answers annotated on Amazon Mechan-
ical Turk. All questions contain at least one an-
swer from Freebase. This dataset has been split by
65%/35% into TRAIN-ALL and TEST. We further
randomly divided TRAIN-ALL by 80%/20% to a
smaller TRAIN and development set DEV. Note
that our DEV set is different from that of Berant
et al. (2013), but the final result on TEST is di-
rectly comparable. Results are reported in terms
of macro F1 with partial credit (following Berant
et al. (2013)) if a predicted answer list does not
have a perfect match with all gold answers, as a
</bodyText>
<footnote confidence="0.9997586">
7The way we used ReverbMapping was not how Berant et
al. (2013) originally used it: they employed a discriminative
log-linear model to judge relations and that might yield better
performance. As a fair comparison, ranking of CluewebMap-
ping under uniform distribution is also included in Table 2(a).
</footnote>
<page confidence="0.981381">
962
</page>
<table confidence="0.876403944444445">
Median Rank MAP MRR
word overlap 471 0.0380 0.0590
ReverbMapping 60 0.0691 0.0829
CluewebMapping 12 0.2074 0.2900
with uniform dist. 61 0.0544 0.0561
(a) Ranking on answer relations. Best result on
CluewebMapping was under the GROW-DIAG-FINAL-AND
heuristics (row 3) when symmetrizing alignment from both
directions. The last row shows ranking of CluewebMapping
under uniform distribution (assuming counting on words and
relations is not known).
1 &lt; 5 &lt; 10 &lt; 50 &lt; 100 &gt; 100
w. o. 3.5 4.7 2.5 3.9 4.1 81.3
R.M. 2.6 9.1 8.6 26.0 13.0 40.7
C.M. 19.0 19.9 8.9 22.3 7.5 22.4
(b) Percentage of answer relations w.r.t. ranking number
(header). w.o.: word overlap; R.M.: ReverbMapping; C.M.:
CluewebMapping.
</table>
<tableCaption confidence="0.936992">
Table 2: Evaluation on answer relation ranking
prediction on 3778 training questions.
</tableCaption>
<bodyText confidence="0.984916275862069">
lot of questions in WEBQUESTIONS contain more
than one answer.
Search With an Information Retrieval (IR)
front-end, we need to locate the exact Freebase
topic node a question is about. For this pur-
pose we used the Freebase Search API (Freebase,
2013a).All named entities 8 in a question were sent
to this API, which returned a ranked list of rele-
vant topics. We also evaluated how well the search
API served the IR purpose. WEBQUESTIONS not
only has answers annotated, but also which Free-
base topic nodes the answers come from. Thus
we evaluated the ranking of retrieval with the gold
standard annotation on TRAIN-ALL, shown in Ta-
ble 3. The top 2 results of the Search API con-
tain gold standard topics for more than 90% of the
questions and the top 10 results contain more than
95%. We took this as a “good enough” IR front-
end and used it on TEST.
Once a topic is obtained we query the Freebase
Topic API (Freebase, 2013b) to retrieve all rele-
vant information, resulting in a topic graph. The
API returns almost identical information as dis-
played via a web browser to a user viewing this
topic. Given that turkers annotated answers based
on the topic page via a browser, this supports the
assumption that the same answer would be located
in the topic graph, which is then passed to the QA
engine for feature extraction and classification.
</bodyText>
<footnote confidence="0.9388195">
8When no named entities are detected, we fall back to
noun phrases.
</footnote>
<table confidence="0.968086333333333">
top 1 2 3 5 10
# 3263 3456 3532 3574 3604
% 86.4 91.5 93.5 94.6 95.4
</table>
<tableCaption confidence="0.8445316">
Table 3: Evaluation on the Freebase Search API:
how many questions’ top n retrieved results con-
tain the gold standard topic. Total number of ques-
tions is 3778 (size of TRAIN-ALL). There were
only 5 questions with no retrieved results.
</tableCaption>
<table confidence="0.9998222">
P R F1
basic 57.3 30.1 39.5
+ word overlap 56.0 31.4 40.2
+ CluewebMapping 59.9 35.4 44.5
+both 59.0 35.4 44.3
</table>
<tableCaption confidence="0.999504">
Table 4: Fi on DEV with different feature settings.
</tableCaption>
<bodyText confidence="0.999282294117647">
Model Tuning We treat QA on Freebase as a
binary classification task: for each node in the
topic graph, we extract features and judge whether
it is the answer node. Every question was pro-
cessed by the Stanford CoreNLP suite with the
caseless model. Then the question features (§4.1)
and node features (§4.2) were combined (§4.3)
for each node. The learning problem is chal-
lenging: for about 3000 questions in TRAIN,
there are 3 million nodes (1000 nodes per topic
graph), and 7 million feature types. We em-
ployed a high-performance machine learning tool,
Classias (Okazaki, 2009). Training usually
took around 4 hours. We experimented with vari-
ous discriminative learners on DEV, including lo-
gistic regression, perceptron and SVM, and found
L1 regularized logistic regression to give the best
result. The L1 regularization encourages sparse
features by driving feature weights towards zero,
which was ideal for the over-generated feature
space. After training, we had around 30 thousand
features with non-zero weights, a 200 fold reduc-
tion from the original features.
Also, we did an ablation test on DEV about
how additional features on the mapping between
Freebase relations and the original questions help,
with three feature settings: 1) “basic” features in-
clude feature productions read off from the fea-
ture graph (Figure 1); 2) “+ word overlap” adds
additional features on whether sub-relations have
overlap with the question; and 3) “+ CluewebMap-
ping” adds the ranking of relation prediction given
the question according to CluewebMapping. Ta-
ble 4 shows that the additional CluewebMapping
</bodyText>
<page confidence="0.99663">
963
</page>
<table confidence="0.9995955">
P R F1
Gold Retrieval 45.4 52.2 48.6
Freebase Search API 38.8 45.8 42.0
Berant et al. (2013) - - 31.4
</table>
<tableCaption confidence="0.951429">
Table 5: F1 on TEST with Gold Retrieval and
</tableCaption>
<bodyText confidence="0.9981168">
Freebase Search API as the IR front end. Berant
et al. (2013) actually reported accuracy on this
dataset. However, since their system predicted an-
swers for almost every question (p.c.), it is roughly
that precision=recall=F1=accuracy for them.
features improved overall F1 by 5%, a 13% rel-
ative improvement: a remarkable gain given that
the model already learned a strong correlation be-
tween question types and answer types (explained
more in discussion and Table 6 later).
Finally, the ratio of positive vs. negative exam-
ples affect final F1: the more positive examples,
the lower the precision and the higher the recall.
Under the original setting, this ratio was about
1 : 275. This produced precision around 60%
and recall around 35% (c.f. Table 4). To optimize
for F1, we down-sampled the negative examples to
20%, i.e., a new ratio of 1 : 55. This boosted the
final F1 on DEV to 48%. We report the final TEST
result under this down-sampled training. In prac-
tice the precision/recall balance can be adjusted by
the positive/negative ratio.
Test Results Table 5 gives the final F1 on TEST.
“Gold Retrieval” always ranked the correct topic
node top 1, a perfect IR front-end assumption. In
a more realistic scenario, we had already evaluated
that the Freebase Search API returned the correct
topic node 95% of the time in its top 10 results (c.f.
Table 3), thus we also tested on the top 10 results
returned by the Search API. To keep things sim-
ple, we did not perform answer voting, but sim-
ply extracted answers from the first (ranked by the
Search API) topic node with predicted answer(s)
found. The final F1 of 42.0% gives a relative im-
provement over previous best result (Berant et al.,
2013) of 31.4% by one third.
One question of interest is whether our system,
aided by the massive web data, can be fairly com-
pared to the semantic parsing approaches (note
that Berant et al. (2013) also used ClueWeb in-
directly through ReVerb). Thus we took out
the word overlapping and CluewebMapping based
features, and the new F1 on TEST was 36.9%.
The other question of interest is that whether
our system has acquired some level of “machine
</bodyText>
<figure confidence="0.834905222222222">
wgt. feature
5.56 qfocus=money|type=Currency
5.35 qverb=die|type=Cause Of Death
5.11 qword=when|type=datetime
4.56 qverb=border|rel=location.adjoins
3.90 qword=why|incoming relation rank=top 3
2.94 qverb=go|qtopic=location|type=Tourist attraction
-3.94 qtopic=location|rel=location.imports exports.date
-2.93 qtopic=person|rel=education.end date
</figure>
<tableCaption confidence="0.8643085">
Table 6: A sample of the top 50 most positive/neg-
ative features. Features are production between
</tableCaption>
<bodyText confidence="0.978513428571429">
question and node features (c.f. Figure 1).
intelligence”: how much does it know what the
question inquires? We discuss it below through
feature and error analysis.
Discussion The combination between questions
and Freebase nodes captures some real gist of QA
pattern typing, shown in Table 6 with sampled fea-
tures and weights. Our system learned, for in-
stance, when the question asks for geographic ad-
jacency information (qverb=border), the correct
answer relation to look for is location.adjoins.
Detailed comparison with the output from Berant
et al. (2013) is a work in progress and will be pre-
sented in a follow-up report.
</bodyText>
<sectionHeader confidence="0.998451" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999984272727273">
We proposed an automatic method for Question
Answering from structured data source (Free-
base). Our approach associates question features
with answer patterns described by Freebase and
has achieved state-of-the-art results on a balanced
and realistic QA corpus. To compensate for the
problem of domain mismatch or overfitting, we
exploited ClueWeb, mined mappings between KB
relations and natural language text, and showed
that it helped both relation prediction and an-
swer extraction. Our method employs relatively
lightweight machinery but has good performance.
We hope that this result establishes a new baseline
against which semantic parsing researchers can
measure their progress towards deeper language
understanding and answering of human questions.
Acknowledgments We thank the Allen Institute
for Artificial Intelligence for funding this work.
We are also grateful to Jonathan Berant, Tom
Kwiatkowski, Qingqing Cai, Adam Lopez, Chris
Callison-Burch and Peter Clark for helpful discus-
sion and to the reviewers for insightful comments.
</bodyText>
<page confidence="0.996784">
964
</page>
<sectionHeader confidence="0.982761" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999788769230769">
S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens
Lehmann, Richard Cyganiak, and Zachary Ives.
2007. DBPedia: A nucleus for a web of open data.
In The semantic web, pages 722–735. Springer.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic Parsing on Freebase from
Question-Answer Pairs. In Proceedings of EMNLP.
Steven Bird and Edward Loper. 2004. NLTK: The Nat-
ural Language Toolkit. In Proceedings of the ACL
Workshop on Effective Tools and Methodologies for
Teaching Natural Language Processing and Compu-
tational Linguistics.
Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim
Sturge, and Jamie Taylor. 2008. Freebase: a col-
laboratively created graph database for structuring
human knowledge. In Proceedings of the 2008 ACM
SIGMOD international conference on Management
of data, pages 1247–1250. ACM.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263–311.
Qingqing Cai and Alexander Yates. 2013. Large-scale
semantic parsing via schema matching and lexicon
extension. In Proceedings of ACL.
David L Chen and Raymond J Mooney. 2011. Learn-
ing to Interpret Natural Language Navigation In-
structions from Observations. In AAAI, volume 2,
pages 1–2.
J. Chu-Carroll, J. Fan, B. K. Boguraev, D. Carmel,
D. Sheinwald, and C. Welty. 2012. Finding needles
in the haystack: Search and candidate generation.
IBM Journal of Research and Development.
J´erˆome Euzenat and Pavel Shvaiko. 2007. Ontology
matching. Springer.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In Proceedings of EMNLP.
Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.
2013. Paraphrase-Driven Learning for Open Ques-
tion Answering. In Proceedings of ACL.
Anette Frank, Hans-Ulrich Krieger, Feiyu Xu, Hans
Uszkoreit, Berthold Crysmann, Brigitte J¨org, and
Ulrich Sch¨afer. 2007. Question answering from
structured knowledge sources. Journal of Applied
Logic, 5(1):20–48.
Freebase. 2013a. Freebase Search API.
https://developers.google.com/freebase/v1/search-
overview.
Freebase. 2013b. Freebase Topic API.
https://developers.google.com/freebase/v1/topic-
overview.
Evgeniy Gabrilovich, Michael Ringgaard, , and Amar-
nag Subramanya. 2013. FACC1: Freebase anno-
tation of ClueWeb corpora, Version 1 (Release date
2013-06-26, Format version 1, Correction level 0).
http://lemurproject.org/clueweb09/FACC1/, June.
Bert F Green Jr, Alice K Wolf, Carol Chomsky, and
Kenneth Laughery. 1961. Baseball: an automatic
question-answerer. In Papers presented at the May
9-11, 1961, western joint IRE-AIEE-ACM computer
conference, pages 219–224. ACM.
Jerry R Hobbs. 1985. Ontological promiscuity. In
Proceedings ofACL.
Johannes Hoffart, Fabian M Suchanek, Klaus
Berberich, Edwin Lewis-Kelham, Gerard De Melo,
and Gerhard Weikum. 2011. Yago2: exploring and
querying world knowledge in time, space, context,
and many languages. In Proceedings of the 20th
international conference companion on World Wide
Web, pages 229–232. ACM.
Bevan Keeley Jones, Mark Johnson, and Sharon Gold-
water. 2012. Semantic parsing with bayesian tree
transducers. In Proceedings of ACL.
Rohit J Kate and Raymond J Mooney. 2006. Using
string-kernels for learning semantic parsers. In Pro-
ceedings of ACL.
Tibor Kiss and Jan Strunk. 2006. Unsupervised mul-
tilingual sentence boundary detection. Computa-
tional Linguistics, 32(4):485–525.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press, New York, NY, USA.
Jayant Krishnamurthy and Tom M Mitchell. 2012.
Weakly supervised training of semantic parsers. In
Proceedings of EMNLP-CoNLL.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing probabilis-
tic CCG grammars from logical form with higher-
order unification. In Proceedings of EMNLP, pages
1223–1233.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2011. Lexical generaliza-
tion in CCG grammar induction for semantic pars-
ing. In Proceedings of EMNLP.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling Semantic Parsers with
On-the-fly Ontology Matching. In Proceedings of
EMNLP.
Percy Liang, Michael I. Jordan, and Dan Klein.
2011. Learning Dependency-Based Compositional
Semantics. In Proceedings of ACL.
Thomas Lin, Oren Etzioni, et al. 2012. Entity Linking
at Web Scale. In Proceedings of Knowledge Extrac-
tion Workshop (AKBC-WEKEX), pages 84–88.
</reference>
<page confidence="0.984503">
965
</page>
<reference confidence="0.999814962962963">
Vanessa Lopez, Michele Pasin, and Enrico Motta.
2005. Aqualog: An ontology-portable question an-
swering system for the semantic web. In The Seman-
tic Web: Research and Applications, pages 546–562.
Springer.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19–51.
Naoaki Okazaki. 2009. Classias: a collection of
machine-learning algorithms for classification.
Dave Orr, Amar Subramanya, Evgeniy Gabrilovich,
and Michael Ringgaard. 2013. 11 billion
clues in 800 million documents: A web re-
search corpus annotated with freebase concepts.
http://googleresearch.blogspot.com/2013/07/11-
billion-clues-in-800-million.html, July.
Erhard Rahm and Philip A Bernstein. 2001. A survey
of approaches to automatic schema matching. the
VLDB Journal, 10(4):334–350.
Saeedeh Shekarpour, Axel-Cyrille Ngonga Ngomo,
and S¨oren Auer. 2013. Question answering on in-
terlinked data. In Proceedings of WWW.
Lappoon R Tang and Raymond J Mooney. 2001. Us-
ing multiple clause constructors in inductive logic
programming for semantic parsing. In Machine
Learning: ECML 2001, pages 466–477. Springer.
Christina Unger, Lorenz B¨uhmann, Jens Lehmann,
Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and
Philipp Cimiano. 2012. Template-based question
answering over RDF data. In Proceedings of the
21st international conference on World Wide Web.
Yuk Wah Wong and Raymond J Mooney. 2007.
Learning synchronous grammars for semantic pars-
ing with lambda calculus. In Proceedings of ACL.
William A Woods. 1977. Lunar rocks in natural en-
glish: Explorations in natural language question an-
swering. Linguistic structures processing, 5:521–
569.
Mohamed Yahya, Klaus Berberich, Shady Elbas-
suoni, Maya Ramanath, Volker Tresp, and Gerhard
Weikum. 2012. Natural language questions for the
web of data. In Proceedings of EMNLP.
Luke S Zettlemoyer and Michael Collins. 2005.
Learning to map sentences to logical form: Struc-
tured classification with probabilistic categorial
grammars. Uncertainty in Artificial Intelligence
(UAI).
Luke S Zettlemoyer and Michael Collins. 2007. On-
line learning of relaxed CCG grammars for parsing
to logical form. In Proceedings of EMNLP-CoNLL.
Luke S Zettlemoyer and Michael Collins. 2009.
Learning context-dependent mappings from sen-
tences to logical form. In Proceedings of ACL-
CoNLL.
</reference>
<page confidence="0.998666">
966
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.525811">
<title confidence="0.9985325">Information Extraction over Structured Question Answering with Freebase</title>
<author confidence="0.982409">Yao</author>
<author confidence="0.982409">Van_Durme</author>
<affiliation confidence="0.764766666666667">for Language and Speech Language Technology Center of Johns Hopkins</affiliation>
<address confidence="0.998896">Baltimore, MD, USA</address>
<abstract confidence="0.996361733333333">Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing. Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base. Here we show that relatively modest information extraction techniques, when paired with a webscale corpus, can outperform these sophisticated approaches by roughly 34% relative gain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S¨oren Auer</author>
<author>Christian Bizer</author>
<author>Georgi Kobilarov</author>
<author>Jens Lehmann</author>
<author>Richard Cyganiak</author>
<author>Zachary Ives</author>
</authors>
<title>DBPedia: A nucleus for a web of open data.</title>
<date>2007</date>
<booktitle>In The semantic web,</booktitle>
<pages>722--735</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1241" citStr="Auer et al., 2007" startWordPosition="192" endWordPosition="195">relatively modest information extraction techniques, when paired with a webscale corpus, can outperform these sophisticated approaches by roughly 34% relative gain. 1 Introduction Question answering (QA) from a knowledge base (KB) has a long history within natural language processing, going back to the 1960s and 1970s, with systems such as Baseball (Green Jr et al., 1961) and Lunar (Woods, 1977). These systems were limited to closed-domains due to a lack of knowledge resources, computing power, and ability to robustly understand natural language. With the recent growth in KBs such as DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), it has become more practical to consider answering questions across wider domains, with commercial systems including Google Now, based on Google’s Knowledge Graph, and Facebook Graph Search, based on social network connections. The AI community has tended to approach this problem with a focus on first understanding the intent of the question, via shallow or deep forms of semantic parsing (c.f. §3 for a discussion). Typically questions are converted into some meaning representation (e.g., the lambda calculus), then mapped to </context>
</contexts>
<marker>Auer, Bizer, Kobilarov, Lehmann, Cyganiak, Ives, 2007</marker>
<rawString>S¨oren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. DBPedia: A nucleus for a web of open data. In The semantic web, pages 722–735. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic Parsing on Freebase from Question-Answer Pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="2747" citStr="Berant et al. (2013)" startWordPosition="432" endWordPosition="435"> to triage the set of possible answer candidates, and only then attempting to perform deeper analysis. Researchers in semantic parsing have recently explored QA over Freebase as a way of moving beyond closed domains such as GeoQuery (Tang and Mooney, 2001). While making semantic parsing more robust is a laudable goal, here we provide a more rigorous IE baseline against which those efforts should be compared: we show that “traditional” IE methodology can significantly outperform prior state-of-the-art as reported in the semantic parsing literature, with a relative gain of 34% F1 as compared to Berant et al. (2013). 2 Approach We will view a KB as an interlinked collection of “topics”. When given a question about one or several topics, we can select a “view” of the KB concerning only involved topics, then inspect every related node within a few hops of relations to the topic node in order to extract the answer. We call such a view a topic graph and assume answers can be found within the graph. We aim to maximally automate the answer extraction process, by massively combining discriminative features for both the question and the topic graph. With a high performance learner we have found that a system wit</context>
<context position="4534" citStr="Berant et al., 2013" startWordPosition="733" endWordPosition="736"> type is likely currency. We formalize this approach in §4. One challenge for natural language querying against a KB is the relative informality of queries as compared to the grammar of a KB. For example, for the question: who cheated on celebrity A, answers can be retrieved via the Freebase relation celebrity.infidelity.participant, but the connection between the phrase cheated on and the formal KB relation is not explicit. To alleviate this problem, the best attempt so far is to map from ReVerb (Fader et al., 2011) predicateargument triples to Freebase relation triples (Cai and Yates, 2013; Berant et al., 2013). Note that to boost precision, ReVerb has already pruned down less frequent or credible triples, yielding not as much coverage as its text source, ClueWeb. Here we instead directly mine relation mappings from ClueWeb and show that both direct relation mapping precision and indirect QA F1 improve by a large margin. Details in §5. Finally, we tested our system, jacanafreebase,2 on a realistic dataset generously contributed by Berant et al. (2013), who collected thousands of commonly asked questions by crawling the Google Suggest service. Our method achieves state-of-the-art performance with F1 </context>
<context position="6846" citStr="Berant et al. (2013)" startWordPosition="1091" endWordPosition="1094">2012). We instead attack the problem of QA from a KB from an IE perspective: we learn directly the pattern of QA pairs, represented by the dependency parse of questions and the Freebase structure of answer candidates, without the use of intermediate, general purpose meaning representations. The data challenge is more formally framed as ontology or (textual) schema matching (Hobbs, 1985; Rahm and Bernstein, 2001; Euzenat and Shvaiko, 2007): matching structure of two ontologies/databases or (in extension) mapping between KB relations and NL text. In terms of the latter, Cai and Yates (2013) and Berant et al. (2013) applied pattern matching and relation intersection between Freebase relations and predicateargument triples from the ReVerb OpenIE system (Fader et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the same answers generated by WikiAnswers. The key factor to their success is to have a huge text source. Our work pushes the data challenge to the limit by mining directly from ClueWeb, a 5TB collection of web data. Finally, the KB commun</context>
<context position="22365" citStr="Berant et al. (2013)" startWordPosition="3702" endWordPosition="3705">certain news categories, and the resultant catering by websites to those information desires, then for example we also learned a heavily correlated connection between Jennifer Aniston and celebrity.infidelity.victim, and between some other you-know-who names and celebrity.infidelity.participant. We next formally evaluate how the learned mapping help predict relations from words. 961 5.3 Evaluation Both ClueWeb and its Freebase annotation has a bias. Thus we were firstly interested in the coverage of mined relation mappings. As a comparison, we used a dataset of relation mapping contributed by Berant et al. (2013) and Lin et al. (2012). The idea is very similar: they intersected Freebase relations with predicates in (arg1, predicate, arg2) triples extracted from ReVerb to learn the mapping between Freebase relations and triple predicates. Note the scale difference: although ReVerb was also extracted from ClueWeb09, there were only 15 million triples to intersect with the relations, while we had 1.2 billion alignment pairs. We call this dataset ReverbMapping and ours CluewebMapping. The evaluation dataset, WEBQUESTIONS, was also contributed by Berant et al. (2013). It contains 3778 training and 2032 tes</context>
<context position="25819" citStr="Berant et al. (2013)" startWordPosition="4267" endWordPosition="4270">regular tv appearance.actor, etc. These percentage numbers are good clue for feature design: for instance, we may be confident in a relation if it is ranked top 5 or 10 by CluewebMapping. To conclude, we found that CluewebMapping provides satisfying coverage on the 3778 training questions: only 7% were missing, despite the biased nature of web data. Also, CluewebMapping gives reasonably good precision on its prediction, despite the noisy nature of web data. We move on to fully evaluate the final QA F1. 6 Experiments We evaluate the final F1 in this section. The system of comparison is that of Berant et al. (2013). Data We re-used WEBQUESTIONS, a dataset collected by Berant et al. (2013). It contains 5810 questions crawled from the Google Suggest service, with answers annotated on Amazon Mechanical Turk. All questions contain at least one answer from Freebase. This dataset has been split by 65%/35% into TRAIN-ALL and TEST. We further randomly divided TRAIN-ALL by 80%/20% to a smaller TRAIN and development set DEV. Note that our DEV set is different from that of Berant et al. (2013), but the final result on TEST is directly comparable. Results are reported in terms of macro F1 with partial credit (follo</context>
<context position="31185" citStr="Berant et al. (2013)" startWordPosition="5175" endWordPosition="5178">, we did an ablation test on DEV about how additional features on the mapping between Freebase relations and the original questions help, with three feature settings: 1) “basic” features include feature productions read off from the feature graph (Figure 1); 2) “+ word overlap” adds additional features on whether sub-relations have overlap with the question; and 3) “+ CluewebMapping” adds the ranking of relation prediction given the question according to CluewebMapping. Table 4 shows that the additional CluewebMapping 963 P R F1 Gold Retrieval 45.4 52.2 48.6 Freebase Search API 38.8 45.8 42.0 Berant et al. (2013) - - 31.4 Table 5: F1 on TEST with Gold Retrieval and Freebase Search API as the IR front end. Berant et al. (2013) actually reported accuracy on this dataset. However, since their system predicted answers for almost every question (p.c.), it is roughly that precision=recall=F1=accuracy for them. features improved overall F1 by 5%, a 13% relative improvement: a remarkable gain given that the model already learned a strong correlation between question types and answer types (explained more in discussion and Table 6 later). Finally, the ratio of positive vs. negative examples affect final F1: th</context>
<context position="32938" citStr="Berant et al., 2013" startWordPosition="5479" endWordPosition="5482">al F1 on TEST. “Gold Retrieval” always ranked the correct topic node top 1, a perfect IR front-end assumption. In a more realistic scenario, we had already evaluated that the Freebase Search API returned the correct topic node 95% of the time in its top 10 results (c.f. Table 3), thus we also tested on the top 10 results returned by the Search API. To keep things simple, we did not perform answer voting, but simply extracted answers from the first (ranked by the Search API) topic node with predicted answer(s) found. The final F1 of 42.0% gives a relative improvement over previous best result (Berant et al., 2013) of 31.4% by one third. One question of interest is whether our system, aided by the massive web data, can be fairly compared to the semantic parsing approaches (note that Berant et al. (2013) also used ClueWeb indirectly through ReVerb). Thus we took out the word overlapping and CluewebMapping based features, and the new F1 on TEST was 36.9%. The other question of interest is that whether our system has acquired some level of “machine wgt. feature 5.56 qfocus=money|type=Currency 5.35 qverb=die|type=Cause Of Death 5.11 qword=when|type=datetime 4.56 qverb=border|rel=location.adjoins 3.90 qword=</context>
<context position="34380" citStr="Berant et al. (2013)" startWordPosition="5685" endWordPosition="5688"> positive/negative features. Features are production between question and node features (c.f. Figure 1). intelligence”: how much does it know what the question inquires? We discuss it below through feature and error analysis. Discussion The combination between questions and Freebase nodes captures some real gist of QA pattern typing, shown in Table 6 with sampled features and weights. Our system learned, for instance, when the question asks for geographic adjacency information (qverb=border), the correct answer relation to look for is location.adjoins. Detailed comparison with the output from Berant et al. (2013) is a work in progress and will be presented in a follow-up report. 7 Conclusion We proposed an automatic method for Question Answering from structured data source (Freebase). Our approach associates question features with answer patterns described by Freebase and has achieved state-of-the-art results on a balanced and realistic QA corpus. To compensate for the problem of domain mismatch or overfitting, we exploited ClueWeb, mined mappings between KB relations and natural language text, and showed that it helped both relation prediction and answer extraction. Our method employs relatively ligh</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Edward Loper</author>
</authors>
<title>NLTK: The Natural Language Toolkit.</title>
<date>2004</date>
<booktitle>In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics.</booktitle>
<contexts>
<context position="19374" citStr="Bird and Loper, 2004" startWordPosition="3209" endWordPosition="3212"> per document. The precision and recall of annotation were estimated at 80−85% and 70−85% (Orr et al., 2013). Given these two resources, for each binary Freebase relation, we can find a collection of sentences each of which contains both of its arguments, then simply learn how words in these sentences are associated with this relation, i.e., P˜(w |R) and P˜(w |r). By counting how many times each relation R was annotated, we can estimate P˜(R) and P˜(r). The learning task can be framed in the following short steps: 1. We split each HTML document by sentences (Kiss and Strunk, 2006) using NLTK (Bird and Loper, 2004) and extracted those with at least two Freebase entities which has at least one direct established relation according to Freebase. 2. The extraction formed two parallel corpora, one with “relation - sentence” pairs (for estimating P˜(w |R) and P˜(R)) and the other with “subrelations - sentence” pairs (for P˜(w |r) and P˜(r)). Each corpus has 1.2 billion pairs. 3. The tricky part was to align these 1.2 billion pairs. Since the relations on one side of these pairs are not natural sentences, we ran the most simple IBM alignment Model 1 (Brown et al., 1993) to estimate the translation probability </context>
</contexts>
<marker>Bird, Loper, 2004</marker>
<rawString>Steven Bird and Edward Loper. 2004. NLTK: The Natural Language Toolkit. In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kurt Bollacker</author>
<author>Colin Evans</author>
<author>Praveen Paritosh</author>
<author>Tim Sturge</author>
<author>Jamie Taylor</author>
</authors>
<title>Freebase: a collaboratively created graph database for structuring human knowledge.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,</booktitle>
<pages>1247--1250</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1276" citStr="Bollacker et al., 2008" startWordPosition="197" endWordPosition="200">extraction techniques, when paired with a webscale corpus, can outperform these sophisticated approaches by roughly 34% relative gain. 1 Introduction Question answering (QA) from a knowledge base (KB) has a long history within natural language processing, going back to the 1960s and 1970s, with systems such as Baseball (Green Jr et al., 1961) and Lunar (Woods, 1977). These systems were limited to closed-domains due to a lack of knowledge resources, computing power, and ability to robustly understand natural language. With the recent growth in KBs such as DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), it has become more practical to consider answering questions across wider domains, with commercial systems including Google Now, based on Google’s Knowledge Graph, and Facebook Graph Search, based on social network connections. The AI community has tended to approach this problem with a focus on first understanding the intent of the question, via shallow or deep forms of semantic parsing (c.f. §3 for a discussion). Typically questions are converted into some meaning representation (e.g., the lambda calculus), then mapped to database queries. Performance is th</context>
</contexts>
<marker>Bollacker, Evans, Paritosh, Sturge, Taylor, 2008</marker>
<rawString>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247–1250. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="19933" citStr="Brown et al., 1993" startWordPosition="3306" endWordPosition="3309">es (Kiss and Strunk, 2006) using NLTK (Bird and Loper, 2004) and extracted those with at least two Freebase entities which has at least one direct established relation according to Freebase. 2. The extraction formed two parallel corpora, one with “relation - sentence” pairs (for estimating P˜(w |R) and P˜(R)) and the other with “subrelations - sentence” pairs (for P˜(w |r) and P˜(r)). Each corpus has 1.2 billion pairs. 3. The tricky part was to align these 1.2 billion pairs. Since the relations on one side of these pairs are not natural sentences, we ran the most simple IBM alignment Model 1 (Brown et al., 1993) to estimate the translation probability with GIZA++ (Och and Ney, 2003). To speed up, the 1.2 billion pairs were split into 6http://lemurproject.org/clueweb09/ 0 G 10 G 102 G 103 G 104 &gt; 104 7.0% 0.7% 1.2% 0.4% 1.3% 89.5% Table 1: Percentage of answer relations (the incoming relation connected to the answer node) with respect to how many sentences we learned this relation from in CluewebMapping. For instance, the first column says there are 7% of answer relations for which we cannot find a mapping (so we had to use the backoff probability estimation); the last column says there are 89.5% of a</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Large-scale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="4512" citStr="Cai and Yates, 2013" startWordPosition="729" endWordPosition="732">, the expected answer type is likely currency. We formalize this approach in §4. One challenge for natural language querying against a KB is the relative informality of queries as compared to the grammar of a KB. For example, for the question: who cheated on celebrity A, answers can be retrieved via the Freebase relation celebrity.infidelity.participant, but the connection between the phrase cheated on and the formal KB relation is not explicit. To alleviate this problem, the best attempt so far is to map from ReVerb (Fader et al., 2011) predicateargument triples to Freebase relation triples (Cai and Yates, 2013; Berant et al., 2013). Note that to boost precision, ReVerb has already pruned down less frequent or credible triples, yielding not as much coverage as its text source, ClueWeb. Here we instead directly mine relation mappings from ClueWeb and show that both direct relation mapping precision and indirect QA F1 improve by a large margin. Details in §5. Finally, we tested our system, jacanafreebase,2 on a realistic dataset generously contributed by Berant et al. (2013), who collected thousands of commonly asked questions by crawling the Google Suggest service. Our method achieves state-of-the-ar</context>
<context position="6821" citStr="Cai and Yates (2013)" startWordPosition="1086" endWordPosition="1089">shnamurthy and Mitchell, 2012). We instead attack the problem of QA from a KB from an IE perspective: we learn directly the pattern of QA pairs, represented by the dependency parse of questions and the Freebase structure of answer candidates, without the use of intermediate, general purpose meaning representations. The data challenge is more formally framed as ontology or (textual) schema matching (Hobbs, 1985; Rahm and Bernstein, 2001; Euzenat and Shvaiko, 2007): matching structure of two ontologies/databases or (in extension) mapping between KB relations and NL text. In terms of the latter, Cai and Yates (2013) and Berant et al. (2013) applied pattern matching and relation intersection between Freebase relations and predicateargument triples from the ReVerb OpenIE system (Fader et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the same answers generated by WikiAnswers. The key factor to their success is to have a huge text source. Our work pushes the data challenge to the limit by mining directly from ClueWeb, a 5TB collection of web dat</context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013. Large-scale semantic parsing via schema matching and lexicon extension. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning to Interpret Natural Language Navigation Instructions from Observations.</title>
<date>2011</date>
<booktitle>In AAAI,</booktitle>
<volume>2</volume>
<pages>1--2</pages>
<contexts>
<context position="5808" citStr="Chen and Mooney, 2011" startWordPosition="932" endWordPosition="935">F1 of 31.4%. 3 Background QA from a KB faces two prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question, converting it into a query and executing the query on the KB. Most work approaches this via the bridge of various intermediate representations, including combinatory categorial grammar (Zettlemoyer and Collins, 2005, 2007, 2009; Kwiatkowski et al., 2010, 2011, 2013), synchronous context-free grammars (Wong and Mooney, 2007), dependency trees (Liang et al., 2011; Berant et al., 2013), string kernels (Kate and Mooney, 2006; Chen and Mooney, 2011), and tree transducers (Jones et al., 2012). These works successfully showed their effectiveness in QA, despite the fact that most of them require hand-labeled logic annotations. More recent research started to minimize this direct supervision by using latent meaning representations (Berant et 2https://code.google.com/p/jacana al., 2013; Kwiatkowski et al., 2013) or distant supervision (Krishnamurthy and Mitchell, 2012). We instead attack the problem of QA from a KB from an IE perspective: we learn directly the pattern of QA pairs, represented by the dependency parse of questions and the Freeb</context>
</contexts>
<marker>Chen, Mooney, 2011</marker>
<rawString>David L Chen and Raymond J Mooney. 2011. Learning to Interpret Natural Language Navigation Instructions from Observations. In AAAI, volume 2, pages 1–2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chu-Carroll</author>
<author>J Fan</author>
<author>B K Boguraev</author>
<author>D Carmel</author>
<author>D Sheinwald</author>
<author>C Welty</author>
</authors>
<title>Finding needles in the haystack: Search and candidate generation.</title>
<date>2012</date>
<journal>IBM Journal of Research and Development.</journal>
<contexts>
<context position="7858" citStr="Chu-Carroll et al., 2012" startWordPosition="1254" endWordPosition="1257"> by WikiAnswers. The key factor to their success is to have a huge text source. Our work pushes the data challenge to the limit by mining directly from ClueWeb, a 5TB collection of web data. Finally, the KB community has developed other means for QA without semantic parsing (Lopez et al., 2005; Frank et al., 2007; Unger et al., 2012; Yahya et al., 2012; Shekarpour et al., 2013). Most of these work executed SPARQL queries on interlinked data represented by RDF (Resource Description Framework) triples, or simply performed triple matching. Heuristics and manual templates were also commonly used (Chu-Carroll et al., 2012). We propose instead to learn discriminative features from the data with shallow question analysis. The final system captures intuitive patterns of QA pairs automatically. 4 Graph Features Our model is inspired by an intuition on how everyday people search for answers. If you asked someone: what is the name of justin bieber brother,3 and gave them access to Freebase, that person might first determine that the question 3All examples used in this paper come from the training data crawled from Google Suggest. They are lowercased and some contain typos. 957 is about Justin Bieber (or his brother),</context>
</contexts>
<marker>Chu-Carroll, Fan, Boguraev, Carmel, Sheinwald, Welty, 2012</marker>
<rawString>J. Chu-Carroll, J. Fan, B. K. Boguraev, D. Carmel, D. Sheinwald, and C. Welty. 2012. Finding needles in the haystack: Search and candidate generation. IBM Journal of Research and Development.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J´erˆome Euzenat</author>
<author>Pavel Shvaiko</author>
</authors>
<title>Ontology matching.</title>
<date>2007</date>
<publisher>Springer.</publisher>
<contexts>
<context position="6668" citStr="Euzenat and Shvaiko, 2007" startWordPosition="1060" endWordPosition="1063">pervision by using latent meaning representations (Berant et 2https://code.google.com/p/jacana al., 2013; Kwiatkowski et al., 2013) or distant supervision (Krishnamurthy and Mitchell, 2012). We instead attack the problem of QA from a KB from an IE perspective: we learn directly the pattern of QA pairs, represented by the dependency parse of questions and the Freebase structure of answer candidates, without the use of intermediate, general purpose meaning representations. The data challenge is more formally framed as ontology or (textual) schema matching (Hobbs, 1985; Rahm and Bernstein, 2001; Euzenat and Shvaiko, 2007): matching structure of two ontologies/databases or (in extension) mapping between KB relations and NL text. In terms of the latter, Cai and Yates (2013) and Berant et al. (2013) applied pattern matching and relation intersection between Freebase relations and predicateargument triples from the ReVerb OpenIE system (Fader et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the same answers generated by WikiAnswers. The key factor to </context>
</contexts>
<marker>Euzenat, Shvaiko, 2007</marker>
<rawString>J´erˆome Euzenat and Pavel Shvaiko. 2007. Ontology matching. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4436" citStr="Fader et al., 2011" startWordPosition="718" endWordPosition="721">A, June 23-25 2014. c�2014 Association for Computational Linguistics ukraine, the expected answer type is likely currency. We formalize this approach in §4. One challenge for natural language querying against a KB is the relative informality of queries as compared to the grammar of a KB. For example, for the question: who cheated on celebrity A, answers can be retrieved via the Freebase relation celebrity.infidelity.participant, but the connection between the phrase cheated on and the formal KB relation is not explicit. To alleviate this problem, the best attempt so far is to map from ReVerb (Fader et al., 2011) predicateargument triples to Freebase relation triples (Cai and Yates, 2013; Berant et al., 2013). Note that to boost precision, ReVerb has already pruned down less frequent or credible triples, yielding not as much coverage as its text source, ClueWeb. Here we instead directly mine relation mappings from ClueWeb and show that both direct relation mapping precision and indirect QA F1 improve by a large margin. Details in §5. Finally, we tested our system, jacanafreebase,2 on a realistic dataset generously contributed by Berant et al. (2013), who collected thousands of commonly asked questions</context>
<context position="7005" citStr="Fader et al., 2011" startWordPosition="1115" endWordPosition="1118">uestions and the Freebase structure of answer candidates, without the use of intermediate, general purpose meaning representations. The data challenge is more formally framed as ontology or (textual) schema matching (Hobbs, 1985; Rahm and Bernstein, 2001; Euzenat and Shvaiko, 2007): matching structure of two ontologies/databases or (in extension) mapping between KB relations and NL text. In terms of the latter, Cai and Yates (2013) and Berant et al. (2013) applied pattern matching and relation intersection between Freebase relations and predicateargument triples from the ReVerb OpenIE system (Fader et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the same answers generated by WikiAnswers. The key factor to their success is to have a huge text source. Our work pushes the data challenge to the limit by mining directly from ClueWeb, a 5TB collection of web data. Finally, the KB community has developed other means for QA without semantic parsing (Lopez et al., 2005; Frank et al., 2007; Unger et al., 2012; Yahya et al., 2012; Shekarpour et al</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Luke Zettlemoyer</author>
<author>Oren Etzioni</author>
</authors>
<title>Paraphrase-Driven Learning for Open Question Answering.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="7139" citStr="Fader et al. (2013)" startWordPosition="1137" endWordPosition="1140">e data challenge is more formally framed as ontology or (textual) schema matching (Hobbs, 1985; Rahm and Bernstein, 2001; Euzenat and Shvaiko, 2007): matching structure of two ontologies/databases or (in extension) mapping between KB relations and NL text. In terms of the latter, Cai and Yates (2013) and Berant et al. (2013) applied pattern matching and relation intersection between Freebase relations and predicateargument triples from the ReVerb OpenIE system (Fader et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the same answers generated by WikiAnswers. The key factor to their success is to have a huge text source. Our work pushes the data challenge to the limit by mining directly from ClueWeb, a 5TB collection of web data. Finally, the KB community has developed other means for QA without semantic parsing (Lopez et al., 2005; Frank et al., 2007; Unger et al., 2012; Yahya et al., 2012; Shekarpour et al., 2013). Most of these work executed SPARQL queries on interlinked data represented by RDF (Resource Description Framework) triples, </context>
</contexts>
<marker>Fader, Zettlemoyer, Etzioni, 2013</marker>
<rawString>Anthony Fader, Luke Zettlemoyer, and Oren Etzioni. 2013. Paraphrase-Driven Learning for Open Question Answering. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anette Frank</author>
<author>Hans-Ulrich Krieger</author>
<author>Feiyu Xu</author>
<author>Hans Uszkoreit</author>
<author>Berthold Crysmann</author>
<author>Brigitte J¨org</author>
<author>Ulrich Sch¨afer</author>
</authors>
<title>Question answering from structured knowledge sources.</title>
<date>2007</date>
<journal>Journal of Applied Logic,</journal>
<volume>5</volume>
<issue>1</issue>
<marker>Frank, Krieger, Xu, Uszkoreit, Crysmann, J¨org, Sch¨afer, 2007</marker>
<rawString>Anette Frank, Hans-Ulrich Krieger, Feiyu Xu, Hans Uszkoreit, Berthold Crysmann, Brigitte J¨org, and Ulrich Sch¨afer. 2007. Question answering from structured knowledge sources. Journal of Applied Logic, 5(1):20–48.</rawString>
</citation>
<citation valid="false">
<authors>
<author>2013a</author>
</authors>
<title>Freebase Search API.</title>
<note>https://developers.google.com/freebase/v1/searchoverview.</note>
<contexts>
<context position="2747" citStr="(2013)" startWordPosition="435" endWordPosition="435"> set of possible answer candidates, and only then attempting to perform deeper analysis. Researchers in semantic parsing have recently explored QA over Freebase as a way of moving beyond closed domains such as GeoQuery (Tang and Mooney, 2001). While making semantic parsing more robust is a laudable goal, here we provide a more rigorous IE baseline against which those efforts should be compared: we show that “traditional” IE methodology can significantly outperform prior state-of-the-art as reported in the semantic parsing literature, with a relative gain of 34% F1 as compared to Berant et al. (2013). 2 Approach We will view a KB as an interlinked collection of “topics”. When given a question about one or several topics, we can select a “view” of the KB concerning only involved topics, then inspect every related node within a few hops of relations to the topic node in order to extract the answer. We call such a view a topic graph and assume answers can be found within the graph. We aim to maximally automate the answer extraction process, by massively combining discriminative features for both the question and the topic graph. With a high performance learner we have found that a system wit</context>
<context position="4983" citStr="(2013)" startWordPosition="808" endWordPosition="808">t attempt so far is to map from ReVerb (Fader et al., 2011) predicateargument triples to Freebase relation triples (Cai and Yates, 2013; Berant et al., 2013). Note that to boost precision, ReVerb has already pruned down less frequent or credible triples, yielding not as much coverage as its text source, ClueWeb. Here we instead directly mine relation mappings from ClueWeb and show that both direct relation mapping precision and indirect QA F1 improve by a large margin. Details in §5. Finally, we tested our system, jacanafreebase,2 on a realistic dataset generously contributed by Berant et al. (2013), who collected thousands of commonly asked questions by crawling the Google Suggest service. Our method achieves state-of-the-art performance with F1 at 42.0%, a 34% relative increase from the previous F1 of 31.4%. 3 Background QA from a KB faces two prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question, converting it into a query and executing the query on the KB. Most work approaches this via the bridge of various intermediate representations, including combinatory categorial grammar (Zettlemoyer and Collins, 2005, 2007, </context>
<context position="6821" citStr="(2013)" startWordPosition="1089" endWordPosition="1089"> Mitchell, 2012). We instead attack the problem of QA from a KB from an IE perspective: we learn directly the pattern of QA pairs, represented by the dependency parse of questions and the Freebase structure of answer candidates, without the use of intermediate, general purpose meaning representations. The data challenge is more formally framed as ontology or (textual) schema matching (Hobbs, 1985; Rahm and Bernstein, 2001; Euzenat and Shvaiko, 2007): matching structure of two ontologies/databases or (in extension) mapping between KB relations and NL text. In terms of the latter, Cai and Yates (2013) and Berant et al. (2013) applied pattern matching and relation intersection between Freebase relations and predicateargument triples from the ReVerb OpenIE system (Fader et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the same answers generated by WikiAnswers. The key factor to their success is to have a huge text source. Our work pushes the data challenge to the limit by mining directly from ClueWeb, a 5TB collection of web dat</context>
<context position="22365" citStr="(2013)" startWordPosition="3705" endWordPosition="3705">ategories, and the resultant catering by websites to those information desires, then for example we also learned a heavily correlated connection between Jennifer Aniston and celebrity.infidelity.victim, and between some other you-know-who names and celebrity.infidelity.participant. We next formally evaluate how the learned mapping help predict relations from words. 961 5.3 Evaluation Both ClueWeb and its Freebase annotation has a bias. Thus we were firstly interested in the coverage of mined relation mappings. As a comparison, we used a dataset of relation mapping contributed by Berant et al. (2013) and Lin et al. (2012). The idea is very similar: they intersected Freebase relations with predicates in (arg1, predicate, arg2) triples extracted from ReVerb to learn the mapping between Freebase relations and triple predicates. Note the scale difference: although ReVerb was also extracted from ClueWeb09, there were only 15 million triples to intersect with the relations, while we had 1.2 billion alignment pairs. We call this dataset ReverbMapping and ours CluewebMapping. The evaluation dataset, WEBQUESTIONS, was also contributed by Berant et al. (2013). It contains 3778 training and 2032 tes</context>
<context position="25819" citStr="(2013)" startWordPosition="4270" endWordPosition="4270">earance.actor, etc. These percentage numbers are good clue for feature design: for instance, we may be confident in a relation if it is ranked top 5 or 10 by CluewebMapping. To conclude, we found that CluewebMapping provides satisfying coverage on the 3778 training questions: only 7% were missing, despite the biased nature of web data. Also, CluewebMapping gives reasonably good precision on its prediction, despite the noisy nature of web data. We move on to fully evaluate the final QA F1. 6 Experiments We evaluate the final F1 in this section. The system of comparison is that of Berant et al. (2013). Data We re-used WEBQUESTIONS, a dataset collected by Berant et al. (2013). It contains 5810 questions crawled from the Google Suggest service, with answers annotated on Amazon Mechanical Turk. All questions contain at least one answer from Freebase. This dataset has been split by 65%/35% into TRAIN-ALL and TEST. We further randomly divided TRAIN-ALL by 80%/20% to a smaller TRAIN and development set DEV. Note that our DEV set is different from that of Berant et al. (2013), but the final result on TEST is directly comparable. Results are reported in terms of macro F1 with partial credit (follo</context>
<context position="31185" citStr="(2013)" startWordPosition="5178" endWordPosition="5178">lation test on DEV about how additional features on the mapping between Freebase relations and the original questions help, with three feature settings: 1) “basic” features include feature productions read off from the feature graph (Figure 1); 2) “+ word overlap” adds additional features on whether sub-relations have overlap with the question; and 3) “+ CluewebMapping” adds the ranking of relation prediction given the question according to CluewebMapping. Table 4 shows that the additional CluewebMapping 963 P R F1 Gold Retrieval 45.4 52.2 48.6 Freebase Search API 38.8 45.8 42.0 Berant et al. (2013) - - 31.4 Table 5: F1 on TEST with Gold Retrieval and Freebase Search API as the IR front end. Berant et al. (2013) actually reported accuracy on this dataset. However, since their system predicted answers for almost every question (p.c.), it is roughly that precision=recall=F1=accuracy for them. features improved overall F1 by 5%, a 13% relative improvement: a remarkable gain given that the model already learned a strong correlation between question types and answer types (explained more in discussion and Table 6 later). Finally, the ratio of positive vs. negative examples affect final F1: th</context>
<context position="33130" citStr="(2013)" startWordPosition="5517" endWordPosition="5517">e correct topic node 95% of the time in its top 10 results (c.f. Table 3), thus we also tested on the top 10 results returned by the Search API. To keep things simple, we did not perform answer voting, but simply extracted answers from the first (ranked by the Search API) topic node with predicted answer(s) found. The final F1 of 42.0% gives a relative improvement over previous best result (Berant et al., 2013) of 31.4% by one third. One question of interest is whether our system, aided by the massive web data, can be fairly compared to the semantic parsing approaches (note that Berant et al. (2013) also used ClueWeb indirectly through ReVerb). Thus we took out the word overlapping and CluewebMapping based features, and the new F1 on TEST was 36.9%. The other question of interest is that whether our system has acquired some level of “machine wgt. feature 5.56 qfocus=money|type=Currency 5.35 qverb=die|type=Cause Of Death 5.11 qword=when|type=datetime 4.56 qverb=border|rel=location.adjoins 3.90 qword=why|incoming relation rank=top 3 2.94 qverb=go|qtopic=location|type=Tourist attraction -3.94 qtopic=location|rel=location.imports exports.date -2.93 qtopic=person|rel=education.end date Table </context>
<context position="34380" citStr="(2013)" startWordPosition="5688" endWordPosition="5688">tive features. Features are production between question and node features (c.f. Figure 1). intelligence”: how much does it know what the question inquires? We discuss it below through feature and error analysis. Discussion The combination between questions and Freebase nodes captures some real gist of QA pattern typing, shown in Table 6 with sampled features and weights. Our system learned, for instance, when the question asks for geographic adjacency information (qverb=border), the correct answer relation to look for is location.adjoins. Detailed comparison with the output from Berant et al. (2013) is a work in progress and will be presented in a follow-up report. 7 Conclusion We proposed an automatic method for Question Answering from structured data source (Freebase). Our approach associates question features with answer patterns described by Freebase and has achieved state-of-the-art results on a balanced and realistic QA corpus. To compensate for the problem of domain mismatch or overfitting, we exploited ClueWeb, mined mappings between KB relations and natural language text, and showed that it helped both relation prediction and answer extraction. Our method employs relatively ligh</context>
</contexts>
<marker>2013a, </marker>
<rawString>Freebase. 2013a. Freebase Search API. https://developers.google.com/freebase/v1/searchoverview.</rawString>
</citation>
<citation valid="false">
<authors>
<author>2013b</author>
</authors>
<title>Freebase Topic API.</title>
<note>https://developers.google.com/freebase/v1/topicoverview.</note>
<contexts>
<context position="2747" citStr="(2013)" startWordPosition="435" endWordPosition="435"> set of possible answer candidates, and only then attempting to perform deeper analysis. Researchers in semantic parsing have recently explored QA over Freebase as a way of moving beyond closed domains such as GeoQuery (Tang and Mooney, 2001). While making semantic parsing more robust is a laudable goal, here we provide a more rigorous IE baseline against which those efforts should be compared: we show that “traditional” IE methodology can significantly outperform prior state-of-the-art as reported in the semantic parsing literature, with a relative gain of 34% F1 as compared to Berant et al. (2013). 2 Approach We will view a KB as an interlinked collection of “topics”. When given a question about one or several topics, we can select a “view” of the KB concerning only involved topics, then inspect every related node within a few hops of relations to the topic node in order to extract the answer. We call such a view a topic graph and assume answers can be found within the graph. We aim to maximally automate the answer extraction process, by massively combining discriminative features for both the question and the topic graph. With a high performance learner we have found that a system wit</context>
<context position="4983" citStr="(2013)" startWordPosition="808" endWordPosition="808">t attempt so far is to map from ReVerb (Fader et al., 2011) predicateargument triples to Freebase relation triples (Cai and Yates, 2013; Berant et al., 2013). Note that to boost precision, ReVerb has already pruned down less frequent or credible triples, yielding not as much coverage as its text source, ClueWeb. Here we instead directly mine relation mappings from ClueWeb and show that both direct relation mapping precision and indirect QA F1 improve by a large margin. Details in §5. Finally, we tested our system, jacanafreebase,2 on a realistic dataset generously contributed by Berant et al. (2013), who collected thousands of commonly asked questions by crawling the Google Suggest service. Our method achieves state-of-the-art performance with F1 at 42.0%, a 34% relative increase from the previous F1 of 31.4%. 3 Background QA from a KB faces two prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question, converting it into a query and executing the query on the KB. Most work approaches this via the bridge of various intermediate representations, including combinatory categorial grammar (Zettlemoyer and Collins, 2005, 2007, </context>
<context position="6821" citStr="(2013)" startWordPosition="1089" endWordPosition="1089"> Mitchell, 2012). We instead attack the problem of QA from a KB from an IE perspective: we learn directly the pattern of QA pairs, represented by the dependency parse of questions and the Freebase structure of answer candidates, without the use of intermediate, general purpose meaning representations. The data challenge is more formally framed as ontology or (textual) schema matching (Hobbs, 1985; Rahm and Bernstein, 2001; Euzenat and Shvaiko, 2007): matching structure of two ontologies/databases or (in extension) mapping between KB relations and NL text. In terms of the latter, Cai and Yates (2013) and Berant et al. (2013) applied pattern matching and relation intersection between Freebase relations and predicateargument triples from the ReVerb OpenIE system (Fader et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the same answers generated by WikiAnswers. The key factor to their success is to have a huge text source. Our work pushes the data challenge to the limit by mining directly from ClueWeb, a 5TB collection of web dat</context>
<context position="22365" citStr="(2013)" startWordPosition="3705" endWordPosition="3705">ategories, and the resultant catering by websites to those information desires, then for example we also learned a heavily correlated connection between Jennifer Aniston and celebrity.infidelity.victim, and between some other you-know-who names and celebrity.infidelity.participant. We next formally evaluate how the learned mapping help predict relations from words. 961 5.3 Evaluation Both ClueWeb and its Freebase annotation has a bias. Thus we were firstly interested in the coverage of mined relation mappings. As a comparison, we used a dataset of relation mapping contributed by Berant et al. (2013) and Lin et al. (2012). The idea is very similar: they intersected Freebase relations with predicates in (arg1, predicate, arg2) triples extracted from ReVerb to learn the mapping between Freebase relations and triple predicates. Note the scale difference: although ReVerb was also extracted from ClueWeb09, there were only 15 million triples to intersect with the relations, while we had 1.2 billion alignment pairs. We call this dataset ReverbMapping and ours CluewebMapping. The evaluation dataset, WEBQUESTIONS, was also contributed by Berant et al. (2013). It contains 3778 training and 2032 tes</context>
<context position="25819" citStr="(2013)" startWordPosition="4270" endWordPosition="4270">earance.actor, etc. These percentage numbers are good clue for feature design: for instance, we may be confident in a relation if it is ranked top 5 or 10 by CluewebMapping. To conclude, we found that CluewebMapping provides satisfying coverage on the 3778 training questions: only 7% were missing, despite the biased nature of web data. Also, CluewebMapping gives reasonably good precision on its prediction, despite the noisy nature of web data. We move on to fully evaluate the final QA F1. 6 Experiments We evaluate the final F1 in this section. The system of comparison is that of Berant et al. (2013). Data We re-used WEBQUESTIONS, a dataset collected by Berant et al. (2013). It contains 5810 questions crawled from the Google Suggest service, with answers annotated on Amazon Mechanical Turk. All questions contain at least one answer from Freebase. This dataset has been split by 65%/35% into TRAIN-ALL and TEST. We further randomly divided TRAIN-ALL by 80%/20% to a smaller TRAIN and development set DEV. Note that our DEV set is different from that of Berant et al. (2013), but the final result on TEST is directly comparable. Results are reported in terms of macro F1 with partial credit (follo</context>
<context position="31185" citStr="(2013)" startWordPosition="5178" endWordPosition="5178">lation test on DEV about how additional features on the mapping between Freebase relations and the original questions help, with three feature settings: 1) “basic” features include feature productions read off from the feature graph (Figure 1); 2) “+ word overlap” adds additional features on whether sub-relations have overlap with the question; and 3) “+ CluewebMapping” adds the ranking of relation prediction given the question according to CluewebMapping. Table 4 shows that the additional CluewebMapping 963 P R F1 Gold Retrieval 45.4 52.2 48.6 Freebase Search API 38.8 45.8 42.0 Berant et al. (2013) - - 31.4 Table 5: F1 on TEST with Gold Retrieval and Freebase Search API as the IR front end. Berant et al. (2013) actually reported accuracy on this dataset. However, since their system predicted answers for almost every question (p.c.), it is roughly that precision=recall=F1=accuracy for them. features improved overall F1 by 5%, a 13% relative improvement: a remarkable gain given that the model already learned a strong correlation between question types and answer types (explained more in discussion and Table 6 later). Finally, the ratio of positive vs. negative examples affect final F1: th</context>
<context position="33130" citStr="(2013)" startWordPosition="5517" endWordPosition="5517">e correct topic node 95% of the time in its top 10 results (c.f. Table 3), thus we also tested on the top 10 results returned by the Search API. To keep things simple, we did not perform answer voting, but simply extracted answers from the first (ranked by the Search API) topic node with predicted answer(s) found. The final F1 of 42.0% gives a relative improvement over previous best result (Berant et al., 2013) of 31.4% by one third. One question of interest is whether our system, aided by the massive web data, can be fairly compared to the semantic parsing approaches (note that Berant et al. (2013) also used ClueWeb indirectly through ReVerb). Thus we took out the word overlapping and CluewebMapping based features, and the new F1 on TEST was 36.9%. The other question of interest is that whether our system has acquired some level of “machine wgt. feature 5.56 qfocus=money|type=Currency 5.35 qverb=die|type=Cause Of Death 5.11 qword=when|type=datetime 4.56 qverb=border|rel=location.adjoins 3.90 qword=why|incoming relation rank=top 3 2.94 qverb=go|qtopic=location|type=Tourist attraction -3.94 qtopic=location|rel=location.imports exports.date -2.93 qtopic=person|rel=education.end date Table </context>
<context position="34380" citStr="(2013)" startWordPosition="5688" endWordPosition="5688">tive features. Features are production between question and node features (c.f. Figure 1). intelligence”: how much does it know what the question inquires? We discuss it below through feature and error analysis. Discussion The combination between questions and Freebase nodes captures some real gist of QA pattern typing, shown in Table 6 with sampled features and weights. Our system learned, for instance, when the question asks for geographic adjacency information (qverb=border), the correct answer relation to look for is location.adjoins. Detailed comparison with the output from Berant et al. (2013) is a work in progress and will be presented in a follow-up report. 7 Conclusion We proposed an automatic method for Question Answering from structured data source (Freebase). Our approach associates question features with answer patterns described by Freebase and has achieved state-of-the-art results on a balanced and realistic QA corpus. To compensate for the problem of domain mismatch or overfitting, we exploited ClueWeb, mined mappings between KB relations and natural language text, and showed that it helped both relation prediction and answer extraction. Our method employs relatively ligh</context>
</contexts>
<marker>2013b, </marker>
<rawString>Freebase. 2013b. Freebase Topic API. https://developers.google.com/freebase/v1/topicoverview.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Michael Ringgaard</author>
</authors>
<title>FACC1: Freebase annotation of ClueWeb corpora,</title>
<date>2013</date>
<journal>Version</journal>
<volume>1</volume>
<pages>2013--06</pages>
<location>http://lemurproject.org/clueweb09/FACC1/,</location>
<marker>Gabrilovich, Ringgaard, 2013</marker>
<rawString>Evgeniy Gabrilovich, Michael Ringgaard, , and Amarnag Subramanya. 2013. FACC1: Freebase annotation of ClueWeb corpora, Version 1 (Release date 2013-06-26, Format version 1, Correction level 0). http://lemurproject.org/clueweb09/FACC1/, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bert F Green Jr</author>
<author>Alice K Wolf</author>
<author>Carol Chomsky</author>
<author>Kenneth Laughery</author>
</authors>
<title>Baseball: an automatic question-answerer.</title>
<date>1961</date>
<booktitle>In Papers presented at the May 9-11, 1961, western joint IRE-AIEE-ACM computer conference,</booktitle>
<pages>219--224</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="997" citStr="Jr et al., 1961" startWordPosition="152" endWordPosition="155">ancing the state of the art in open domain semantic parsing. Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base. Here we show that relatively modest information extraction techniques, when paired with a webscale corpus, can outperform these sophisticated approaches by roughly 34% relative gain. 1 Introduction Question answering (QA) from a knowledge base (KB) has a long history within natural language processing, going back to the 1960s and 1970s, with systems such as Baseball (Green Jr et al., 1961) and Lunar (Woods, 1977). These systems were limited to closed-domains due to a lack of knowledge resources, computing power, and ability to robustly understand natural language. With the recent growth in KBs such as DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), it has become more practical to consider answering questions across wider domains, with commercial systems including Google Now, based on Google’s Knowledge Graph, and Facebook Graph Search, based on social network connections. The AI community has tended to approach this problem with </context>
</contexts>
<marker>Jr, Wolf, Chomsky, Laughery, 1961</marker>
<rawString>Bert F Green Jr, Alice K Wolf, Carol Chomsky, and Kenneth Laughery. 1961. Baseball: an automatic question-answerer. In Papers presented at the May 9-11, 1961, western joint IRE-AIEE-ACM computer conference, pages 219–224. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry R Hobbs</author>
</authors>
<title>Ontological promiscuity.</title>
<date>1985</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="6614" citStr="Hobbs, 1985" startWordPosition="1054" endWordPosition="1055">arch started to minimize this direct supervision by using latent meaning representations (Berant et 2https://code.google.com/p/jacana al., 2013; Kwiatkowski et al., 2013) or distant supervision (Krishnamurthy and Mitchell, 2012). We instead attack the problem of QA from a KB from an IE perspective: we learn directly the pattern of QA pairs, represented by the dependency parse of questions and the Freebase structure of answer candidates, without the use of intermediate, general purpose meaning representations. The data challenge is more formally framed as ontology or (textual) schema matching (Hobbs, 1985; Rahm and Bernstein, 2001; Euzenat and Shvaiko, 2007): matching structure of two ontologies/databases or (in extension) mapping between KB relations and NL text. In terms of the latter, Cai and Yates (2013) and Berant et al. (2013) applied pattern matching and relation intersection between Freebase relations and predicateargument triples from the ReVerb OpenIE system (Fader et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the sam</context>
</contexts>
<marker>Hobbs, 1985</marker>
<rawString>Jerry R Hobbs. 1985. Ontological promiscuity. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Fabian M Suchanek</author>
<author>Klaus Berberich</author>
<author>Edwin Lewis-Kelham</author>
<author>Gerard De Melo</author>
<author>Gerhard Weikum</author>
</authors>
<title>Yago2: exploring and querying world knowledge in time, space, context, and many languages.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th international conference companion on World Wide Web,</booktitle>
<pages>229--232</pages>
<publisher>ACM.</publisher>
<marker>Hoffart, Suchanek, Berberich, Lewis-Kelham, De Melo, Weikum, 2011</marker>
<rawString>Johannes Hoffart, Fabian M Suchanek, Klaus Berberich, Edwin Lewis-Kelham, Gerard De Melo, and Gerhard Weikum. 2011. Yago2: exploring and querying world knowledge in time, space, context, and many languages. In Proceedings of the 20th international conference companion on World Wide Web, pages 229–232. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bevan Keeley Jones</author>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Semantic parsing with bayesian tree transducers.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5851" citStr="Jones et al., 2012" startWordPosition="939" endWordPosition="942">wo prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question, converting it into a query and executing the query on the KB. Most work approaches this via the bridge of various intermediate representations, including combinatory categorial grammar (Zettlemoyer and Collins, 2005, 2007, 2009; Kwiatkowski et al., 2010, 2011, 2013), synchronous context-free grammars (Wong and Mooney, 2007), dependency trees (Liang et al., 2011; Berant et al., 2013), string kernels (Kate and Mooney, 2006; Chen and Mooney, 2011), and tree transducers (Jones et al., 2012). These works successfully showed their effectiveness in QA, despite the fact that most of them require hand-labeled logic annotations. More recent research started to minimize this direct supervision by using latent meaning representations (Berant et 2https://code.google.com/p/jacana al., 2013; Kwiatkowski et al., 2013) or distant supervision (Krishnamurthy and Mitchell, 2012). We instead attack the problem of QA from a KB from an IE perspective: we learn directly the pattern of QA pairs, represented by the dependency parse of questions and the Freebase structure of answer candidates, without</context>
</contexts>
<marker>Jones, Johnson, Goldwater, 2012</marker>
<rawString>Bevan Keeley Jones, Mark Johnson, and Sharon Goldwater. 2012. Semantic parsing with bayesian tree transducers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rohit J Kate</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using string-kernels for learning semantic parsers.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5784" citStr="Kate and Mooney, 2006" startWordPosition="928" endWordPosition="931">ease from the previous F1 of 31.4%. 3 Background QA from a KB faces two prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question, converting it into a query and executing the query on the KB. Most work approaches this via the bridge of various intermediate representations, including combinatory categorial grammar (Zettlemoyer and Collins, 2005, 2007, 2009; Kwiatkowski et al., 2010, 2011, 2013), synchronous context-free grammars (Wong and Mooney, 2007), dependency trees (Liang et al., 2011; Berant et al., 2013), string kernels (Kate and Mooney, 2006; Chen and Mooney, 2011), and tree transducers (Jones et al., 2012). These works successfully showed their effectiveness in QA, despite the fact that most of them require hand-labeled logic annotations. More recent research started to minimize this direct supervision by using latent meaning representations (Berant et 2https://code.google.com/p/jacana al., 2013; Kwiatkowski et al., 2013) or distant supervision (Krishnamurthy and Mitchell, 2012). We instead attack the problem of QA from a KB from an IE perspective: we learn directly the pattern of QA pairs, represented by the dependency parse of</context>
</contexts>
<marker>Kate, Mooney, 2006</marker>
<rawString>Rohit J Kate and Raymond J Mooney. 2006. Using string-kernels for learning semantic parsers. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tibor Kiss</author>
<author>Jan Strunk</author>
</authors>
<title>Unsupervised multilingual sentence boundary detection.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="19340" citStr="Kiss and Strunk, 2006" startWordPosition="3203" endWordPosition="3206">th an average of 15 entity mentions per document. The precision and recall of annotation were estimated at 80−85% and 70−85% (Orr et al., 2013). Given these two resources, for each binary Freebase relation, we can find a collection of sentences each of which contains both of its arguments, then simply learn how words in these sentences are associated with this relation, i.e., P˜(w |R) and P˜(w |r). By counting how many times each relation R was annotated, we can estimate P˜(R) and P˜(r). The learning task can be framed in the following short steps: 1. We split each HTML document by sentences (Kiss and Strunk, 2006) using NLTK (Bird and Loper, 2004) and extracted those with at least two Freebase entities which has at least one direct established relation according to Freebase. 2. The extraction formed two parallel corpora, one with “relation - sentence” pairs (for estimating P˜(w |R) and P˜(R)) and the other with “subrelations - sentence” pairs (for P˜(w |r) and P˜(r)). Each corpus has 1.2 billion pairs. 3. The tricky part was to align these 1.2 billion pairs. Since the relations on one side of these pairs are not natural sentences, we ran the most simple IBM alignment Model 1 (Brown et al., 1993) to est</context>
</contexts>
<marker>Kiss, Strunk, 2006</marker>
<rawString>Tibor Kiss and Jan Strunk. 2006. Unsupervised multilingual sentence boundary detection. Computational Linguistics, 32(4):485–525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Machine Translation.</title>
<date>2010</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="20964" citStr="Koehn, 2010" startWordPosition="3486" endWordPosition="3487">irst column says there are 7% of answer relations for which we cannot find a mapping (so we had to use the backoff probability estimation); the last column says there are 89.5% of answer relations that we were able to learn the mapping between this relation and text based on more than 10 thousand relation-sentence pairs. The total number of answer relations is 7886. 100 even chunks. We ran 5 iterations of EM on each one and finally aligned the 1.2 billion pairs from both directions. To symmetrize the alignment, common MT heuristics INTERSECTION, UNION, GROW-DIAG-FINAL, and GROW-DIAGFINAL-AND (Koehn, 2010) were separately applied and evaluated later. 4. Treating the aligned pairs as observation, the co-occurrence matrix between aligning relations and words was computed. There were 10,484 relations and sub-relations in all, and we kept the top 20,000 words. 5. From the co-occurrence matrix we computed P˜(w |R), P˜(R), P˜(w |r) and P˜(r). Hand-checking the learned probabilities shows both success, failure and some bias. For instance, for the film.actor.film relation (mapping from film names to actor names), the top words given by P˜(w |R) are won, star, among, show. For the film.film.directed by </context>
</contexts>
<marker>Koehn, 2010</marker>
<rawString>Philipp Koehn. 2010. Statistical Machine Translation. Cambridge University Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jayant Krishnamurthy</author>
<author>Tom M Mitchell</author>
</authors>
<title>Weakly supervised training of semantic parsers.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<contexts>
<context position="6231" citStr="Krishnamurthy and Mitchell, 2012" startWordPosition="990" endWordPosition="993">ki et al., 2010, 2011, 2013), synchronous context-free grammars (Wong and Mooney, 2007), dependency trees (Liang et al., 2011; Berant et al., 2013), string kernels (Kate and Mooney, 2006; Chen and Mooney, 2011), and tree transducers (Jones et al., 2012). These works successfully showed their effectiveness in QA, despite the fact that most of them require hand-labeled logic annotations. More recent research started to minimize this direct supervision by using latent meaning representations (Berant et 2https://code.google.com/p/jacana al., 2013; Kwiatkowski et al., 2013) or distant supervision (Krishnamurthy and Mitchell, 2012). We instead attack the problem of QA from a KB from an IE perspective: we learn directly the pattern of QA pairs, represented by the dependency parse of questions and the Freebase structure of answer candidates, without the use of intermediate, general purpose meaning representations. The data challenge is more formally framed as ontology or (textual) schema matching (Hobbs, 1985; Rahm and Bernstein, 2001; Euzenat and Shvaiko, 2007): matching structure of two ontologies/databases or (in extension) mapping between KB relations and NL text. In terms of the latter, Cai and Yates (2013) and Beran</context>
</contexts>
<marker>Krishnamurthy, Mitchell, 2012</marker>
<rawString>Jayant Krishnamurthy and Tom M Mitchell. 2012. Weakly supervised training of semantic parsers. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing probabilistic CCG grammars from logical form with higherorder unification.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1223--1233</pages>
<contexts>
<context position="5613" citStr="Kwiatkowski et al., 2010" startWordPosition="903" endWordPosition="906">collected thousands of commonly asked questions by crawling the Google Suggest service. Our method achieves state-of-the-art performance with F1 at 42.0%, a 34% relative increase from the previous F1 of 31.4%. 3 Background QA from a KB faces two prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question, converting it into a query and executing the query on the KB. Most work approaches this via the bridge of various intermediate representations, including combinatory categorial grammar (Zettlemoyer and Collins, 2005, 2007, 2009; Kwiatkowski et al., 2010, 2011, 2013), synchronous context-free grammars (Wong and Mooney, 2007), dependency trees (Liang et al., 2011; Berant et al., 2013), string kernels (Kate and Mooney, 2006; Chen and Mooney, 2011), and tree transducers (Jones et al., 2012). These works successfully showed their effectiveness in QA, despite the fact that most of them require hand-labeled logic annotations. More recent research started to minimize this direct supervision by using latent meaning representations (Berant et 2https://code.google.com/p/jacana al., 2013; Kwiatkowski et al., 2013) or distant supervision (Krishnamurthy a</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic CCG grammars from logical form with higherorder unification. In Proceedings of EMNLP, pages 1223–1233.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Lexical generalization in CCG grammar induction for semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2011</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2011. Lexical generalization in CCG grammar induction for semantic parsing. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling Semantic Parsers with On-the-fly Ontology Matching.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3618" citStr="Kwiatkowski et al., 2013" startWordPosition="587" endWordPosition="590">ations to the topic node in order to extract the answer. We call such a view a topic graph and assume answers can be found within the graph. We aim to maximally automate the answer extraction process, by massively combining discriminative features for both the question and the topic graph. With a high performance learner we have found that a system with millions of features can be trained within hours, leading to intuitive, human interpretable features. For example, we learn that given a question concerning money, such as: what money is used in 1As an example, 50% of errors of the CCG-backed (Kwiatkowski et al., 2013) system were contributed by parsing or structural matching failure. 956 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 956–966, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics ukraine, the expected answer type is likely currency. We formalize this approach in §4. One challenge for natural language querying against a KB is the relative informality of queries as compared to the grammar of a KB. For example, for the question: who cheated on celebrity A, answers can be retrieved via the Freebase relation ce</context>
<context position="6173" citStr="Kwiatkowski et al., 2013" startWordPosition="982" endWordPosition="985">ttlemoyer and Collins, 2005, 2007, 2009; Kwiatkowski et al., 2010, 2011, 2013), synchronous context-free grammars (Wong and Mooney, 2007), dependency trees (Liang et al., 2011; Berant et al., 2013), string kernels (Kate and Mooney, 2006; Chen and Mooney, 2011), and tree transducers (Jones et al., 2012). These works successfully showed their effectiveness in QA, despite the fact that most of them require hand-labeled logic annotations. More recent research started to minimize this direct supervision by using latent meaning representations (Berant et 2https://code.google.com/p/jacana al., 2013; Kwiatkowski et al., 2013) or distant supervision (Krishnamurthy and Mitchell, 2012). We instead attack the problem of QA from a KB from an IE perspective: we learn directly the pattern of QA pairs, represented by the dependency parse of questions and the Freebase structure of answer candidates, without the use of intermediate, general purpose meaning representations. The data challenge is more formally framed as ontology or (textual) schema matching (Hobbs, 1985; Rahm and Bernstein, 2001; Euzenat and Shvaiko, 2007): matching structure of two ontologies/databases or (in extension) mapping between KB relations and NL te</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling Semantic Parsers with On-the-fly Ontology Matching. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael I Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning Dependency-Based Compositional Semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5723" citStr="Liang et al., 2011" startWordPosition="918" endWordPosition="921">-the-art performance with F1 at 42.0%, a 34% relative increase from the previous F1 of 31.4%. 3 Background QA from a KB faces two prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question, converting it into a query and executing the query on the KB. Most work approaches this via the bridge of various intermediate representations, including combinatory categorial grammar (Zettlemoyer and Collins, 2005, 2007, 2009; Kwiatkowski et al., 2010, 2011, 2013), synchronous context-free grammars (Wong and Mooney, 2007), dependency trees (Liang et al., 2011; Berant et al., 2013), string kernels (Kate and Mooney, 2006; Chen and Mooney, 2011), and tree transducers (Jones et al., 2012). These works successfully showed their effectiveness in QA, despite the fact that most of them require hand-labeled logic annotations. More recent research started to minimize this direct supervision by using latent meaning representations (Berant et 2https://code.google.com/p/jacana al., 2013; Kwiatkowski et al., 2013) or distant supervision (Krishnamurthy and Mitchell, 2012). We instead attack the problem of QA from a KB from an IE perspective: we learn directly th</context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael I. Jordan, and Dan Klein. 2011. Learning Dependency-Based Compositional Semantics. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Lin</author>
<author>Oren Etzioni</author>
</authors>
<title>Entity Linking at Web Scale.</title>
<date>2012</date>
<booktitle>In Proceedings of Knowledge Extraction Workshop (AKBC-WEKEX),</booktitle>
<pages>84--88</pages>
<marker>Lin, Etzioni, 2012</marker>
<rawString>Thomas Lin, Oren Etzioni, et al. 2012. Entity Linking at Web Scale. In Proceedings of Knowledge Extraction Workshop (AKBC-WEKEX), pages 84–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vanessa Lopez</author>
<author>Michele Pasin</author>
<author>Enrico Motta</author>
</authors>
<title>Aqualog: An ontology-portable question answering system for the semantic web. In The Semantic Web: Research and Applications,</title>
<date>2005</date>
<pages>546--562</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="7527" citStr="Lopez et al., 2005" startWordPosition="1202" endWordPosition="1205">ebase relations and predicateargument triples from the ReVerb OpenIE system (Fader et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the same answers generated by WikiAnswers. The key factor to their success is to have a huge text source. Our work pushes the data challenge to the limit by mining directly from ClueWeb, a 5TB collection of web data. Finally, the KB community has developed other means for QA without semantic parsing (Lopez et al., 2005; Frank et al., 2007; Unger et al., 2012; Yahya et al., 2012; Shekarpour et al., 2013). Most of these work executed SPARQL queries on interlinked data represented by RDF (Resource Description Framework) triples, or simply performed triple matching. Heuristics and manual templates were also commonly used (Chu-Carroll et al., 2012). We propose instead to learn discriminative features from the data with shallow question analysis. The final system captures intuitive patterns of QA pairs automatically. 4 Graph Features Our model is inspired by an intuition on how everyday people search for answers.</context>
</contexts>
<marker>Lopez, Pasin, Motta, 2005</marker>
<rawString>Vanessa Lopez, Michele Pasin, and Enrico Motta. 2005. Aqualog: An ontology-portable question answering system for the semantic web. In The Semantic Web: Research and Applications, pages 546–562. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="20005" citStr="Och and Ney, 2003" startWordPosition="3318" endWordPosition="3321">d those with at least two Freebase entities which has at least one direct established relation according to Freebase. 2. The extraction formed two parallel corpora, one with “relation - sentence” pairs (for estimating P˜(w |R) and P˜(R)) and the other with “subrelations - sentence” pairs (for P˜(w |r) and P˜(r)). Each corpus has 1.2 billion pairs. 3. The tricky part was to align these 1.2 billion pairs. Since the relations on one side of these pairs are not natural sentences, we ran the most simple IBM alignment Model 1 (Brown et al., 1993) to estimate the translation probability with GIZA++ (Och and Ney, 2003). To speed up, the 1.2 billion pairs were split into 6http://lemurproject.org/clueweb09/ 0 G 10 G 102 G 103 G 104 &gt; 104 7.0% 0.7% 1.2% 0.4% 1.3% 89.5% Table 1: Percentage of answer relations (the incoming relation connected to the answer node) with respect to how many sentences we learned this relation from in CluewebMapping. For instance, the first column says there are 7% of answer relations for which we cannot find a mapping (so we had to use the backoff probability estimation); the last column says there are 89.5% of answer relations that we were able to learn the mapping between this rela</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoaki Okazaki</author>
</authors>
<title>Classias: a collection of machine-learning algorithms for classification. Dave Orr, Amar Subramanya, Evgeniy Gabrilovich, and Michael Ringgaard.</title>
<date>2009</date>
<tech>http://googleresearch.blogspot.com/2013/07/11-billion-clues-in-800-million.html,</tech>
<contexts>
<context position="30072" citStr="Okazaki, 2009" startWordPosition="5002" endWordPosition="5003">V with different feature settings. Model Tuning We treat QA on Freebase as a binary classification task: for each node in the topic graph, we extract features and judge whether it is the answer node. Every question was processed by the Stanford CoreNLP suite with the caseless model. Then the question features (§4.1) and node features (§4.2) were combined (§4.3) for each node. The learning problem is challenging: for about 3000 questions in TRAIN, there are 3 million nodes (1000 nodes per topic graph), and 7 million feature types. We employed a high-performance machine learning tool, Classias (Okazaki, 2009). Training usually took around 4 hours. We experimented with various discriminative learners on DEV, including logistic regression, perceptron and SVM, and found L1 regularized logistic regression to give the best result. The L1 regularization encourages sparse features by driving feature weights towards zero, which was ideal for the over-generated feature space. After training, we had around 30 thousand features with non-zero weights, a 200 fold reduction from the original features. Also, we did an ablation test on DEV about how additional features on the mapping between Freebase relations an</context>
</contexts>
<marker>Okazaki, 2009</marker>
<rawString>Naoaki Okazaki. 2009. Classias: a collection of machine-learning algorithms for classification. Dave Orr, Amar Subramanya, Evgeniy Gabrilovich, and Michael Ringgaard. 2013. 11 billion clues in 800 million documents: A web research corpus annotated with freebase concepts. http://googleresearch.blogspot.com/2013/07/11-billion-clues-in-800-million.html, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erhard Rahm</author>
<author>Philip A Bernstein</author>
</authors>
<title>A survey of approaches to automatic schema matching.</title>
<date>2001</date>
<journal>the VLDB Journal,</journal>
<volume>10</volume>
<issue>4</issue>
<contexts>
<context position="6640" citStr="Rahm and Bernstein, 2001" startWordPosition="1056" endWordPosition="1059">to minimize this direct supervision by using latent meaning representations (Berant et 2https://code.google.com/p/jacana al., 2013; Kwiatkowski et al., 2013) or distant supervision (Krishnamurthy and Mitchell, 2012). We instead attack the problem of QA from a KB from an IE perspective: we learn directly the pattern of QA pairs, represented by the dependency parse of questions and the Freebase structure of answer candidates, without the use of intermediate, general purpose meaning representations. The data challenge is more formally framed as ontology or (textual) schema matching (Hobbs, 1985; Rahm and Bernstein, 2001; Euzenat and Shvaiko, 2007): matching structure of two ontologies/databases or (in extension) mapping between KB relations and NL text. In terms of the latter, Cai and Yates (2013) and Berant et al. (2013) applied pattern matching and relation intersection between Freebase relations and predicateargument triples from the ReVerb OpenIE system (Fader et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the same answers generated by Wik</context>
</contexts>
<marker>Rahm, Bernstein, 2001</marker>
<rawString>Erhard Rahm and Philip A Bernstein. 2001. A survey of approaches to automatic schema matching. the VLDB Journal, 10(4):334–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saeedeh Shekarpour</author>
<author>Axel-Cyrille Ngonga Ngomo</author>
<author>S¨oren Auer</author>
</authors>
<title>Question answering on interlinked data.</title>
<date>2013</date>
<booktitle>In Proceedings of WWW.</booktitle>
<contexts>
<context position="7613" citStr="Shekarpour et al., 2013" startWordPosition="1218" endWordPosition="1221">er et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the same answers generated by WikiAnswers. The key factor to their success is to have a huge text source. Our work pushes the data challenge to the limit by mining directly from ClueWeb, a 5TB collection of web data. Finally, the KB community has developed other means for QA without semantic parsing (Lopez et al., 2005; Frank et al., 2007; Unger et al., 2012; Yahya et al., 2012; Shekarpour et al., 2013). Most of these work executed SPARQL queries on interlinked data represented by RDF (Resource Description Framework) triples, or simply performed triple matching. Heuristics and manual templates were also commonly used (Chu-Carroll et al., 2012). We propose instead to learn discriminative features from the data with shallow question analysis. The final system captures intuitive patterns of QA pairs automatically. 4 Graph Features Our model is inspired by an intuition on how everyday people search for answers. If you asked someone: what is the name of justin bieber brother,3 and gave them acces</context>
</contexts>
<marker>Shekarpour, Ngomo, Auer, 2013</marker>
<rawString>Saeedeh Shekarpour, Axel-Cyrille Ngonga Ngomo, and S¨oren Auer. 2013. Question answering on interlinked data. In Proceedings of WWW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lappoon R Tang</author>
<author>Raymond J Mooney</author>
</authors>
<title>Using multiple clause constructors in inductive logic programming for semantic parsing.</title>
<date>2001</date>
<booktitle>In Machine Learning: ECML</booktitle>
<pages>466--477</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2383" citStr="Tang and Mooney, 2001" startWordPosition="371" endWordPosition="374">ed into some meaning representation (e.g., the lambda calculus), then mapped to database queries. Performance is thus bounded by the accuracy of the original semantic parsing, and the well-formedness of resultant database queries.1 The Information Extraction (IE) community approaches QA differently: first performing relatively coarse information retrieval as a way to triage the set of possible answer candidates, and only then attempting to perform deeper analysis. Researchers in semantic parsing have recently explored QA over Freebase as a way of moving beyond closed domains such as GeoQuery (Tang and Mooney, 2001). While making semantic parsing more robust is a laudable goal, here we provide a more rigorous IE baseline against which those efforts should be compared: we show that “traditional” IE methodology can significantly outperform prior state-of-the-art as reported in the semantic parsing literature, with a relative gain of 34% F1 as compared to Berant et al. (2013). 2 Approach We will view a KB as an interlinked collection of “topics”. When given a question about one or several topics, we can select a “view” of the KB concerning only involved topics, then inspect every related node within a few h</context>
</contexts>
<marker>Tang, Mooney, 2001</marker>
<rawString>Lappoon R Tang and Raymond J Mooney. 2001. Using multiple clause constructors in inductive logic programming for semantic parsing. In Machine Learning: ECML 2001, pages 466–477. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Unger</author>
<author>Lorenz B¨uhmann</author>
<author>Jens Lehmann</author>
<author>Axel-Cyrille Ngonga Ngomo</author>
<author>Daniel Gerber</author>
<author>Philipp Cimiano</author>
</authors>
<title>Template-based question answering over RDF data.</title>
<date>2012</date>
<booktitle>In Proceedings of the 21st international conference on World Wide Web.</booktitle>
<marker>Unger, B¨uhmann, Lehmann, Ngomo, Gerber, Cimiano, 2012</marker>
<rawString>Christina Unger, Lorenz B¨uhmann, Jens Lehmann, Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and Philipp Cimiano. 2012. Template-based question answering over RDF data. In Proceedings of the 21st international conference on World Wide Web.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning synchronous grammars for semantic parsing with lambda calculus.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="5685" citStr="Wong and Mooney, 2007" startWordPosition="912" endWordPosition="915">gest service. Our method achieves state-of-the-art performance with F1 at 42.0%, a 34% relative increase from the previous F1 of 31.4%. 3 Background QA from a KB faces two prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question, converting it into a query and executing the query on the KB. Most work approaches this via the bridge of various intermediate representations, including combinatory categorial grammar (Zettlemoyer and Collins, 2005, 2007, 2009; Kwiatkowski et al., 2010, 2011, 2013), synchronous context-free grammars (Wong and Mooney, 2007), dependency trees (Liang et al., 2011; Berant et al., 2013), string kernels (Kate and Mooney, 2006; Chen and Mooney, 2011), and tree transducers (Jones et al., 2012). These works successfully showed their effectiveness in QA, despite the fact that most of them require hand-labeled logic annotations. More recent research started to minimize this direct supervision by using latent meaning representations (Berant et 2https://code.google.com/p/jacana al., 2013; Kwiatkowski et al., 2013) or distant supervision (Krishnamurthy and Mitchell, 2012). We instead attack the problem of QA from a KB from a</context>
</contexts>
<marker>Wong, Mooney, 2007</marker>
<rawString>Yuk Wah Wong and Raymond J Mooney. 2007. Learning synchronous grammars for semantic parsing with lambda calculus. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Woods</author>
</authors>
<title>Lunar rocks in natural english: Explorations in natural language question answering. Linguistic structures processing,</title>
<date>1977</date>
<pages>5--521</pages>
<contexts>
<context position="1021" citStr="Woods, 1977" startWordPosition="158" endWordPosition="159">in open domain semantic parsing. Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base. Here we show that relatively modest information extraction techniques, when paired with a webscale corpus, can outperform these sophisticated approaches by roughly 34% relative gain. 1 Introduction Question answering (QA) from a knowledge base (KB) has a long history within natural language processing, going back to the 1960s and 1970s, with systems such as Baseball (Green Jr et al., 1961) and Lunar (Woods, 1977). These systems were limited to closed-domains due to a lack of knowledge resources, computing power, and ability to robustly understand natural language. With the recent growth in KBs such as DBPedia (Auer et al., 2007), Freebase (Bollacker et al., 2008) and Yago2 (Hoffart et al., 2011), it has become more practical to consider answering questions across wider domains, with commercial systems including Google Now, based on Google’s Knowledge Graph, and Facebook Graph Search, based on social network connections. The AI community has tended to approach this problem with a focus on first underst</context>
</contexts>
<marker>Woods, 1977</marker>
<rawString>William A Woods. 1977. Lunar rocks in natural english: Explorations in natural language question answering. Linguistic structures processing, 5:521– 569.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Yahya</author>
<author>Klaus Berberich</author>
<author>Shady Elbassuoni</author>
<author>Maya Ramanath</author>
<author>Volker Tresp</author>
<author>Gerhard Weikum</author>
</authors>
<title>Natural language questions for the web of data.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="7587" citStr="Yahya et al., 2012" startWordPosition="1214" endWordPosition="1217">b OpenIE system (Fader et al., 2011). Kwiatkowski et al. (2013) expanded their CCG lexicon with Wiktionary word tags towards more domain independence. Fader et al. (2013) learned question paraphrases from aligning multiple questions with the same answers generated by WikiAnswers. The key factor to their success is to have a huge text source. Our work pushes the data challenge to the limit by mining directly from ClueWeb, a 5TB collection of web data. Finally, the KB community has developed other means for QA without semantic parsing (Lopez et al., 2005; Frank et al., 2007; Unger et al., 2012; Yahya et al., 2012; Shekarpour et al., 2013). Most of these work executed SPARQL queries on interlinked data represented by RDF (Resource Description Framework) triples, or simply performed triple matching. Heuristics and manual templates were also commonly used (Chu-Carroll et al., 2012). We propose instead to learn discriminative features from the data with shallow question analysis. The final system captures intuitive patterns of QA pairs automatically. 4 Graph Features Our model is inspired by an intuition on how everyday people search for answers. If you asked someone: what is the name of justin bieber bro</context>
</contexts>
<marker>Yahya, Berberich, Elbassuoni, Ramanath, Tresp, Weikum, 2012</marker>
<rawString>Mohamed Yahya, Klaus Berberich, Shady Elbassuoni, Maya Ramanath, Volker Tresp, and Gerhard Weikum. 2012. Natural language questions for the web of data. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>Uncertainty in Artificial Intelligence (UAI).</booktitle>
<contexts>
<context position="5575" citStr="Zettlemoyer and Collins, 2005" startWordPosition="897" endWordPosition="900">y contributed by Berant et al. (2013), who collected thousands of commonly asked questions by crawling the Google Suggest service. Our method achieves state-of-the-art performance with F1 at 42.0%, a 34% relative increase from the previous F1 of 31.4%. 3 Background QA from a KB faces two prominent challenges: model and data. The model challenge involves finding the best meaning representation for the question, converting it into a query and executing the query on the KB. Most work approaches this via the bridge of various intermediate representations, including combinatory categorial grammar (Zettlemoyer and Collins, 2005, 2007, 2009; Kwiatkowski et al., 2010, 2011, 2013), synchronous context-free grammars (Wong and Mooney, 2007), dependency trees (Liang et al., 2011; Berant et al., 2013), string kernels (Kate and Mooney, 2006; Chen and Mooney, 2011), and tree transducers (Jones et al., 2012). These works successfully showed their effectiveness in QA, despite the fact that most of them require hand-labeled logic annotations. More recent research started to minimize this direct supervision by using latent meaning representations (Berant et 2https://code.google.com/p/jacana al., 2013; Kwiatkowski et al., 2013) o</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke S Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. Uncertainty in Artificial Intelligence (UAI).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Online learning of relaxed CCG grammars for parsing to logical form.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP-CoNLL.</booktitle>
<marker>Zettlemoyer, Collins, 2007</marker>
<rawString>Luke S Zettlemoyer and Michael Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. In Proceedings of EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke S Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical form.</title>
<date>2009</date>
<booktitle>In Proceedings of ACLCoNLL.</booktitle>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Luke S Zettlemoyer and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical form. In Proceedings of ACLCoNLL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>