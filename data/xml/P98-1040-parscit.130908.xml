<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006245">
<title confidence="0.992785">
Dialogue Management in Vector-Based Call Routing
</title>
<author confidence="0.971236">
Jennifer Chu-Carroll and Bob Carpenter
</author>
<affiliation confidence="0.911438">
Lucent Technologies Bell Laboratories
</affiliation>
<address confidence="0.9119445">
600 Mountain Avenue
Murray Hill, NJ 07974, U.S.A.
</address>
<email confidence="0.999824">
E-mail: {jencc,carp} @research.bell-labs.com
</email>
<sectionHeader confidence="0.993924" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999987578947369">
This paper describes a domain independent, automat-
ically trained call router which directs customer calls
based on their response to an open-ended &amp;quot;How may I di-
rect your call?&amp;quot; query. Routing behavior is trained from
a corpus of transcribed and hand-routed calls and then
carried out using vector-based information retrieval tech-
niques. Based on the statistical discriminating power of
the n-gram terms extracted from the caller&apos;s request, the
caller is 1) routed to the appropriate destination, 2) trans-
ferred to a human operator, or 3) asked a disambigua-
tion question. In the last case, the system dynamically
generates queries tailored to the caller&apos;s request and the
destinations with which it is consistent. Our approach
is domain independent and the training process is fully
automatic. Evaluations over a financial services call cen-
ter handling hundreds of activities with dozens of desti-
nations demonstrate a substantial improvement on exist-
ing systems by correctly routing 93.8% of the calls after
punting 10.2% of the calls to a human operator.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999951333333333">
The call routing task involves directing a user&apos;s call to
the appropriate destination within a call center or pro-
viding some simple information, such as loan rates. In
current systems, the user&apos;s goals are typically gleaned
via a touch-tone system employing a rigid hierarchical
menu. The primary disadvantages of navigating menus
for users are the time it takes to listen to all the options
and the difficulty of matching their goals to the options;
these problems are compounded by the necessity of de-
scending a nested hierarchy of choices to zero in on a
particular activity. Even simple requests such as &amp;quot;I&apos;d like
my savings account balance&amp;quot; may require users to nav-
igate as many as four or five nested menus with four or
five options each. We have developed an alternative to
touch-tone menus that allows users to interact with a call
router in natural spoken English dialogues just as they
would with a human operator.
Human operators respond to a caller request by 1)
routing the call to an appropriate destination, or 2) query-
ing the caller for further information to determine where
to route the call. Our automatic call router has these two
options as well as a third option of sending the call to a
human operator. The rest of this paper provides both a
description and an evaluation of an automatic call router
driven by vector-based information retrieval techniques.
After introducing our fundamental routing technique, we
focus on the disambiguation query generation module.
Our disambiguation module is based on the same sta-
tistical training as routing, and dynamically generates
queries tailored to the caller&apos;s request and the destina-
tions with which it is consistent. The main advantages
of our system are that 1) it is domain independent, 2) it
is trained fully automatically to both route and disam-
biguate requests, and 3) its performance is sufficient for
use in the field, substantially improving on that of previ-
ous systems.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999839727272727">
Call routing is similar to topic identification (Mc-
Donough et al., 1994) and document routing (Harman,
1995) in identifying which one of n topics (destinations)
most closely matches a caller&apos;s request. Call routing is
distinguished from these activities by requiring a single
destination, but allowing a request to be refined in an in-
teractive dialogue. We are further interested in carrying
out the routing using natural, conversational language.
The only work on call routing to date that we are
aware of is that by Gorin et al. (to appear). They se-
lect salient phrase fragments from caller requests, such as
made a long distance and the area code for. These phrase
fragments are used to determine the most likely destina-
tion(s), which they refer to as call type(s), for the request
either by computing the a posteriori probability for each
call type or by passing the fragments through a neural
network classifier. Abella and Gorin (1997) utilized the
Boolean formula minimization algorithm for combining
the resulting set of call types based on a hand-coded hi-
erarchy of call types. Their intention is to utilize the out-
come of this algorithm to select from a set of dialogue
strategies for response generation.
</bodyText>
<sectionHeader confidence="0.992339" genericHeader="method">
3 Corpus Analysis
</sectionHeader>
<bodyText confidence="0.999818">
To examine human-human dialogue behavior in call
routing, we analyzed a set of 4497 transcribed telephone
calls involving customers interacting with human opera-
tors, looking at both the semantics of caller requests and
</bodyText>
<page confidence="0.997158">
256
</page>
<table confidence="0.995849333333333">
Name Activity Indirect
# of calls 949 3271 277
% of all calls 21.1% 72.7% 6.2%
</table>
<tableCaption confidence="0.999801">
Table 1: Semantic Types of Caller Requests
</tableCaption>
<bodyText confidence="0.9995816">
dialogue actions for response generation. The call cen-
ter provides financial services in hundreds of categories
in the general areas of banking, credit cards, loans, insur-
ance and investments; we concentrated on the 23 desti-
nations for which we had at least 10 calls in the corpus.
</bodyText>
<subsectionHeader confidence="0.999973">
3.1 Semantics of Caller Requests
</subsectionHeader>
<bodyText confidence="0.99997134375">
The operator provides an open-ended prompt of &amp;quot;How
may I direct your call?&amp;quot; We classified user responses
into three categories. First, callers may explicitly pro-
vide a destination name, either by itself or embedded
in a complete sentence, such as &amp;quot;may I have consumer
lending?&amp;quot; Second, callers may describe the activity they
would like to perform. Such requests may be unambigu-
ous, such as &amp;quot;I&apos;d like my checking account balance&amp;quot;, or
ambiguous, such as &amp;quot;car loans please&amp;quot;, which in our call
center can be resolved to either consumer lending, which
handles new car loans, or to loan services, which handles
existing car loans. Third, a caller can provide an indirect
request, in which they describe their goal in a round-
about way, often including irrelevant information. This
often occurs when the caller either is unfamiliar with the
call center hierarchy or does not have a concrete idea of
how to achieve the goal, as in &amp;quot;oh I&apos;m calling &apos;cuz ah a
friend gave me this number and ah she told me ah with
this number I can buy some cars or whatever but she
didn&apos;t know how to explain it to me so I just called you
you know to get that information.&amp;quot;
Table 1 shows the distribution of caller requests in our
corpus with respect to these semantic types. Our analysis
shows that in the vast majority of calls, the request was
based on destination name or activity. Since there is a
fairly small number (dozens to hundreds) of activities be-
ing handled by each destination, requests based on name
and activity are expected to be more predictable and thus
more suitable for handling by an automatic call router.
Thus, our goal is to automatically route those calls based
on name and activity, while leaving the indirect or inap-
propriate requests to human call operators.
</bodyText>
<subsectionHeader confidence="0.999331">
3.2 Dialogue Actions for Response Generation
</subsectionHeader>
<bodyText confidence="0.998709833333333">
We also analyzed the operator&apos;s responses to caller re-
quests to determine the dialogue actions needed for re-
sponse generation in our automatic call router. We found
that in the call routing task, the call operator either no-
tifies the customer of the routing destination or asks a
disambiguating query.&apos;
</bodyText>
<footnote confidence="0.977561666666667">
&apos;In cases where the operator generates an acknowledgment, such
as uh-huh, midway through the caller&apos;s request, we analyzed the next
operator utterance.
</footnote>
<table confidence="0.999653">
Notification Query
NP Others
# of calls 3608 657 232
% of all calls 80.2% 14.6% 5.2%
</table>
<tableCaption confidence="0.997785">
Table 2: Call Operator Dialogue Actions
</tableCaption>
<figure confidence="0.643473">
Caller Request
</figure>
<figureCaption confidence="0.999367">
Figure 1: Call Router Architecture
</figureCaption>
<bodyText confidence="0.999971857142857">
Table 2 shows the frequency that each dialogue ac-
tion should be employed based strictly on the presence
of ambiguity in the caller requests in our corpus. We fur-
ther analyzed those calls considered ambiguous within
our call center and noted that 75% of such ambiguous re-
quests involve underspecified noun phrases, such as re-
questing car loans without specifying whether it is an
existing or new car loan. The remaining 25% of the
ambiguous requests involve underspecified verb phrases,
such as asking to transfer funds without specifying the
types of accounts to and from which the transfer will oc-
cur, or missing verb phrases, such as asking for direct
deposit without specifying whether the caller wants to
set up or change an existing direct deposit.
</bodyText>
<sectionHeader confidence="0.970819" genericHeader="method">
4 Dialogue Management in Call Routing
</sectionHeader>
<bodyText confidence="0.999766090909091">
Our call router consists of two components: the rout-
ing module and the disambiguation module. The rout-
ing module takes a caller request and determines a set of
destinations to which the call can reasonably be routed.
If there is exactly one such destination, the call is routed
there and the customer notified; if there are multiple des-
tinations, the disambiguation module is invoked in an at-
tempt to formulate a query; and if there is no appropriate
destination or if a reasonable disambiguation query can-
not be generated, the call is routed to an operator. Fig-
ure 1 shows a diagram outlining this process.
</bodyText>
<subsectionHeader confidence="0.996966">
4.1 The Routing Module
</subsectionHeader>
<bodyText confidence="0.9999325">
Our approach is novel in its application of information
retrieval techniques to select candidate destinations for
a call. We treat call routing as an instance of document
routing, where a collection of judged documents is used
for training and the task is to judge the relevance of a set
of test documents (Schiitze et al., 1995). More specifi-
</bodyText>
<figure confidence="0.998752857142857">
Candidate Destinations
Routing
Notification
Caller
Response
Di ambiguating
Query
</figure>
<page confidence="0.980001">
257
</page>
<bodyText confidence="0.999833">
cally, each destination in our call center is represented as
a collection of documents (transcriptions of calls routed
to that destination), and given a caller request, we judge
the relevance of the request to each destination.
</bodyText>
<subsubsectionHeader confidence="0.62486">
4.1.1 The Training Process
</subsubsectionHeader>
<bodyText confidence="0.999893083333333">
Document Construction Our training corpus consists
of 3753 calls each of which is hand-routed to one of
23 destinations.2 Our first step is to create one (virtual)
document per destination, which contains the text of the
callers&apos; contributions to all calls routed to that destina-
tion.
Morphological Filtering We filter each (virtual) doc-
ument through the morphological processor of the Bell
Labs&apos; Text-to-Speech synthesizer (Sproat, 1997) to ex-
tract the root form of each word in the corpus. Next,
the root forms of caller utterances are filtered through
two lists, the ignore list and the stop list, in order to
build a better n-gram model. The ignore list consists
of noise words, such as uh and um, which sometimes
get in the way of proper n-gram extraction, as in &amp;quot;I&apos;d
like to speak to someone about a car uh loan&amp;quot;. With
noise word filtering, we can properly extract the bigram
&amp;quot;car,loan&amp;quot;. The stop list enumerates words that do not
discriminate between destinations, such as the, be, and
afternoon. We modified the standard stop list distributed
with the SMART information retrieval system (Salton,
1971) to include domain specific terms and proper names
that occurred in the training corpus. Note that when a
stop word is filtered out of the caller utterance, a place-
holder is inserted to prevent the words preceding and fol-
lowing the stop word to form n-grams. For instance, af-
ter filtering the stop words out of &amp;quot;I want to check on an
account&amp;quot;, the utterance becomes &amp;quot;&lt;sw&gt; &lt;sw&gt; &lt;sw&gt;
check &lt;sw&gt; &lt;sw&gt; account&amp;quot;. Without the placeholders,
we would extract the bigram &amp;quot;check,account&amp;quot;, just as if
the caller had used the term checking account.
Term Extraction We extract the n-gram terms that oc-
cur more frequently than a pre-determined threshold and
do not contain any stop words. Our current system uses
unigrams that occurred at least twice and bigrams and
trigrams that occurred at least three times in the corpus.
No 4-grams occurred three times.
Term-Document Matrix Once the set of relevant
terms is determined, we construct an m x n term-
document frequency matrix A whose rows represent the
m terms, whose columns represent the n destinations,
and where an entry At,d is the frequency with which term
t occurs in calls to destination d.
It is often advantageous to weight the raw counts
to fine tune the contribution of each term to routing.
We begin by normalizing the row vectors representing
terms by making them each of unit length. Thus we di-
vide each row At in the original matrix by its length,
</bodyText>
<footnote confidence="0.991989333333333">
2These 3753 calls are a subset of the corpus of 4497 calls used in
our corpus analysis. We excluded those ambiguous calls that were not
resolved by the operator.
</footnote>
<bodyText confidence="0.999722818181818">
(El&lt;e&lt;n 4,e)1/2. Our second weighting is based on
the tiotron that a term that only occurs in a few docu-
ments is more important in discriminating among docu-
ments than a term that occurs in nearly every document.
We use the inverse document frequency (IDF) weighting
scheme (Sparck Jones, 1972) whereby a term is weighted
inversely to the number of documents in which it occurs,
by means of IDF(t) = log2 n/d(t) where t is a term, n is
the total number of documents in the corpus, and d(t) is
the number of documents containing the term t. Thus we
obtain a weighted matrix B, whose elements are given
</bodyText>
<equation confidence="0.947043666666667">
by Bt,c1= At,d X /DF(t)/(E \ 1/2
1&lt;e&lt;n A2 t,e/ •
— —
</equation>
<bodyText confidence="0.999824875">
Vector Representation To reduce the dimensional-
ity of our vector representations for terms and doc-
uments, we applied the singular value decomposition
(Deerwester et al., 1990) to the m x n matrix B of
weighted term-document frequencies. Specifically, we
take B = U SVT , where U is an m x r orthonormal ma-
trix (where r is the rank of B), V is an n x r orthonor-
mal matrix, and S is an r x r diagonal matrix such that
si,i &gt; s2,2 &gt; • • • &gt; Sr,r &gt; 0.
We can think of each row in U as an r-dimensional
vector that represents a term, whereas each row in V is
an r-dimensional vector representing a document. With
appropriate scaling of the axes by the singular values
on the diagonal of S, we can compare documents to
documents and terms to terms using their corresponding
points in this new r-dimensional space (Deerwester et
al., 1990). For instance, to employ the dot product of
two vectors as a measure of their similarity as is com-
mon in information retrieval (Salton, 1971), we have the
matrix BTB whose elements contain the dot product of
document vectors. Because S is diagonal and U is or-
thonormal, BTB = VS2VT = VS(VS)T . Thus, ele-
ment i, j in BTB, representing the dot product between
document vectors i and j, can be computed by taking
the dot product between the i and j rows of the matrix
VS. In other words, we can consider rows in the matrix
VS as vectors representing documents for the purpose
of document/document comparison. An element of the
original matrix Be,,,representing the degree of associa-
tion between the ith term and the jth document, can be
recovered by multiplying the ith term vector by the jth
scaled document vector, namely Bi,i = Ui((VS)i )T.
</bodyText>
<subsubsectionHeader confidence="0.631207">
4.1.2 Call Routing
</subsubsectionHeader>
<bodyText confidence="0.999354090909091">
Given the vector representations of terms and documents
(destinations) in r-dimensional space, how do we deter-
mine to which destination a new call should be routed?
Our process for vector-based call routing consists of the
following four steps:
Term Extraction Given a transcription of the caller&apos;s
utterance (either from a keyboard interface or from the
output of a speech recognizer), the first step is to extract
the relevant n-gram terms from the utterance. For in-
stance, term extraction on the request &amp;quot;I want to check
the balance in my savings account&amp;quot; would result in
</bodyText>
<page confidence="0.986137">
258
</page>
<bodyText confidence="0.998536017857143">
one bigram term, &amp;quot;saving,account&amp;quot;, and two unigrams,
&amp;quot;check&amp;quot; and &amp;quot;balance&amp;quot;.
Pseudo-Document Generation Given the extracted
terms from a caller request, we can represent the request
as an 7n-dimensional vector Q where each component Qi
represents the number of times that the ith term occurred
in the caller&apos;s request. We then create an r-dimensional
pseudo-document vector D = QU, following the stan-
dard methodology of vector-based information retrieval
(see (Deerwester et al., 1990)). Note that D is simply
the sum of the term vectors Ui for all terms occurring in
the caller&apos;s request, weighted by their frequency of oc-
currence in the request, and is scaled properly for docu-
ment/document comparison.
Scoring Once the vector D for the pseudo-document is
determined, we compare it with the document vectors by
computing the cosine between D and each scaled docu-
ment vectors in VS. Next, we transform the cosine score
for each destination using a sigmoid function specifically
fitted for that destination to obtain a confidence score that
represents the router&apos;s confidence that the call should be
routed to that destination.
The reason for the mapping from cosine scores to con-
fidence scores is because the absolute degree of similar-
ity between a request and a destination, as given by the
cosine value between their vector representations, does
not translate directly into the likelihood for correct rout-
ing. Instead, some destinations may require a higher co-
sine value, i.e., a closer degree of similarity, than others
in order for a request to be correctly associated with those
destinations. Thus we collected, for each destination,
a set of cosine value/routing value pairs over all calls
in the training data, where the routing value is 1 if the
call should be routed to that destination and 0 otherwise.
Then for each destination, we used the least squared error
method in fitting a sigmoid function, 1/(1 + e—(&amp;quot;+b)),
to the set of cosine/routing pairs.
We tested the routing performance using cosine vs.
confidence values on 307 unseen unambiguous requests.
In each case, we selected the destination with the high-
est cosine/confidence score to be the target destination.
Using strict cosine scores, 92.2% of the calls are routed
to the correct destination. On the other hand, using sig-
moid confidence fitting, 93.5% of the calls are correctly
routed. This yields a relative reduction in error rate of
16.7%.
Decision Making The outcome of the routing module
is a set of destinations whose confidence scores are above
a pre-determined threshold. These candidate destinations
represent those to which the caller&apos;s request can reason-
ably be routed. If there is only one such destination, then
the call is routed and the caller notified; if there are two
or more possible destinations, the disambiguation mod-
ule is invoked in an attempt to formulate a query; other-
wise, the the call is routed to an operator.
To determine the optimal value for the threshold, we
</bodyText>
<figure confidence="0.9972164">
0.8
0.6
(./
1.&apos;4 0.0
0.2
</figure>
<figureCaption confidence="0.999827">
Figure 2: Router Performance vs. Threshold
</figureCaption>
<bodyText confidence="0.99993">
ran a series of experiments to compute the upperbound
and lowerbound of the router&apos;s performance varying the
threshold from 0 to 0.9 at 0.1 intervals. The lowerbound
represents the percentage of calls that are routed cor-
rectly, while the upperbound indicates the percentage of
calls that have the potential to be routed correctly after
disambiguation (see section 5 for details on upperbound
and lowerbound measures). The results in Figure 2 show
0.2 to be the threshold that yields optimal performance.
</bodyText>
<subsectionHeader confidence="0.982021">
4.2 The Disambiguation Module
</subsectionHeader>
<bodyText confidence="0.99999">
The disambiguation module attempts to formulate an ap-
propriate query to solicit further information from the
caller in order to determine a unique destination to which
the call should be routed. To generate an appropriate
query, the caller&apos;s request and the candidate destinations
must both be taken into account. We have developed
a vector-based method for dynamically generating dis-
ambiguation queries by first selecting a set of terms and
then forming a wh or yes-no question from these selected
terms.
The terms selected by the disambiguation mechanism
are those terms related to the original request that can
likely be used to disambiguate among the candidate des-
tinations. These terms are chosen by filtering all terms
based on the following three criteria:
</bodyText>
<listItem confidence="0.947136176470588">
1. Closeness: We choose terms that are close (by
the cosine measure) to the differences between the
scaled pseudo-document query vector, D, and vec-
tors representing the candidate destinations in VS.
The intuition is that adding terms close to the differ-
ences will disambiguate the original query.
2. Relevance: From the close terms, we construct a
set of relevant tenns which are terms that further
specify a term in the original request. A close term
is considered relevant if it can be combined with a
term in the request to form a valid n-gram term, and
the relevant term will be the resulting n-gram term.
For instance, if &amp;quot;car loan&amp;quot; is in the original request,
then both &amp;quot;new&amp;quot; and &amp;quot;new,car&amp;quot; would produce the
relevant term &amp;quot;new,car,loan&amp;quot;.
3. Disambiguating power: Finally, we restrict at-
tention to relevant terms that can be added to the
</listItem>
<figure confidence="0.997601571428572">
0 0.1
..
0.2 0.3 0.0 0.5
Threshold
Upperhound
. Lowelhound.
0.6 0.7 0.8 09
</figure>
<page confidence="0.99595">
259
</page>
<bodyText confidence="0.999951891891892">
original request to result in an unambiguous rout-
ing using the routing mechanism described in Sec-
tion 4.1.2. If none of the relevant terms satisfy this
criterion, then we include all relevant terms in the
set of disambiguating terms. Thus, instead of giving
up the disambiguation process when no one term is
predicted to resolve the ambiguity, the system may
pose a question which further specifies the request
and then select a disambiguating term based on this
refined (although still ambiguous) request.
The result of this filtering process is a finite set of
terms which are relevant to the original ambiguous query
and, when added to it, are likely to resolve the ambigu-
ity. If a significant number of these terms share a head
word, such as loan, the system asks the wh-question &amp;quot;for
what type of loan?&amp;quot; Otherwise, the term that occurred
most frequently in the training data is selected, based on
the heuristic that a more common term is likely to be
relevant than an obscure term, and a yes-no question is
formed based on this term. A third alternative would be
to ask a disjunctive question, but we have not yet ex-
plored this possibility. Figure 1 shows that after the sys-
tem poses its query, it attempts to route the refined re-
quest, which is the original request augmented with the
caller response to the system&apos;s query. In the case of wh-
questions, n-gram terms are extracted from the caller&apos;s
response. In the case of yes-no questions, the system de-
termines whether a yes or no answer is given.3 In the for-
mer case, the disambiguating term used to form the query
is considered the caller response, while in the latter case,
the response is treated as in responses to wh-questions.
Note that our disambiguation mechanism, like our ba-
sic routing technique, is fully domain-independent. It
utilizes a set of n-gram terms, as well as term and doc-
ument vectors that were obtained by the training of the
call router. Thus, porting the call router to a new domain
requires no change in the disambiguation module.
</bodyText>
<subsectionHeader confidence="0.996194">
4.3 Example
</subsectionHeader>
<bodyText confidence="0.99999175">
To illustrate our call router, consider the request &amp;quot;loans
please.&amp;quot; This request is ambiguous because our call
center handles mortgage loans separately from all other
types of loans, and for all other loans, existing loans and
new loans are again handled by different departments.
Given this request, the call router first extracts the rel-
evant n-gram terms, which in this case results in the uni-
gram &amp;quot;loan&amp;quot;. It then computes a pseudo-document vec-
tor that represents this request, which is compared in turn
with the 23 vectors representing all destinations in the
call center. The cosine values between the request and
each destination are then mapped into confidence values.
</bodyText>
<footnote confidence="0.972036">
3ln our current system, a response is considered a yes response only
if it explicitly contains the word yes. However, as discussed in (Green
and Carberry, 1994; Hockey et al., 1997), responses to yes-no questions
may not explicitly contain a yes or no term. We leave incorporating a
more sophisticated response understanding model, such as (Green and
Carberry, 1994), into our system for future work.
</footnote>
<bodyText confidence="0.998936142857143">
Using a confidence threshold of 0.2, we have two can-
didate destinations, Loan Servicing and Consumer Lend-
ing; thus the disambiguation module is invoked.
Our disambiguation module first selects from all n-
gram terms those whose term vectors are close to the dif-
ference between the request vector and either of the two
candidate destination vectors. This results in a list of 60
close terms, the vast majority of which are semantically
close to &amp;quot;loan&amp;quot;, such as &amp;quot;auto,loan&amp;quot;, &amp;quot;payoff&amp;quot;, and
&amp;quot;owe&amp;quot;. Next, the relevant terms are constructed from
the set of close terms. This results in a list of 27 relevant
terms, including &amp;quot;auto, loan&amp;quot; and &amp;quot;loan,payoff&amp;quot;, but ex-
cluding owe, since neither &amp;quot;loan,owe&amp;quot; nor &amp;quot;owe,loan&amp;quot;
constitutes a valid bigram. The third step is to select
those relevant terms with disambiguation power, result-
ing in 18 disambiguating terms. Since 11 of these disam-
biguating terms share a head noun loan, a wh-question is
generated based on this head word, resulting in the query
&amp;quot;for what type of loan?&amp;quot;
Suppose in response to the system&apos;s query, the user
answers &amp;quot;car loan&amp;quot;. The router then adds the new bi-
gram &amp;quot;car; loan&amp;quot; to the original request and attempts to
route the refined request. This refined request is again
ambiguous between Loan Servicing and Consumer Lend-
ing since the caller did not specify whether it was an ex-
isting or new car loan. Again, the disambiguation mod-
ule selects the close, relevant, and disambiguating terms,
resulting in a unique term &amp;quot;exist,car,loan&amp;quot;. Thus, the
system generates the yes-no question &amp;quot;is this about an
existing car loan?&amp;quot;4 If the user responds &amp;quot;yes&amp;quot;, then the
trigram term &amp;quot;exist,car,loan&amp;quot; is added to the refined re-
quest and the call routed to Loan Servicing; if the user
says &amp;quot;no, it&apos;s a new car loan&amp;quot;, then &amp;quot;new,car,loan&amp;quot; is
extracted from the response and the call routed to Con-
sumer Lending.
</bodyText>
<sectionHeader confidence="0.999547" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.99986">
5.1 The Routing Module
</subsectionHeader>
<bodyText confidence="0.9999635">
We performed an evaluation of the routing module of our
call router on a fresh set of 389 calls to a human opera-
tor.5 Out of the 389 requests, 307 are unambiguous and
routed to their correct destinations, and 82 were ambigu-
ous and annotated with a list of candidate destinations.
Unfortunately, in this test set, only the caller&apos;s first ut-
terance was transcribed. Thus we have no information
about where the ambiguous calls were eventually routed.
The routing decision made for each call is classified
into one of 8 groups, as shown in Figure 3. For instance,
</bodyText>
<footnote confidence="0.997239333333333">
4Our current system uses simple template filling for response gener-
ation by utilizing a manually constructed mappings from n-gram terms
to their inflected forms, such as from &amp;quot;exist,carloan&amp;quot; to &amp;quot;an existing
car loan&amp;quot;.
5The calls in the test set were recorded separately from our training
corpus. In this paper, we focus on evaluation based on transcriptions of
the calls. A companion paper compares call performance on transcrip-
tions to the output of a speech recognizer (Carpenter and Chu-Carroll,
submitted).
</footnote>
<page confidence="0.985809">
260
</page>
<figure confidence="0.346081833333333">
Is request actually unambiguous?
Is call routed by router? Is call routed by router?
ye/ \up yes no
correct? contains correct? one of possible? overlaps with possible?
Yes/ No yes/ No
3a 3b 4a 4b
</figure>
<figureCaption confidence="0.8646">
Figure 3: Classification of Router Outcome
</figureCaption>
<table confidence="0.99928675">
Unambiguous Ambiguous All
Requests Requests Requests
LB la/(1+2) 4a/(3+4) (la+4a)/all
UB (la+2a)/(1+2) (3a+4a)/(3+4) (la+2a+3a+4a)/all
</table>
<tableCaption confidence="0.999874">
Table 3: Calculation of Upperbounds and Lowerbounds
</tableCaption>
<bodyText confidence="0.998620967741936">
group la contains those calls which are 1) actually unam-
biguous, 2) considered unambiguous by the router, and
3) routed to the correct destination. On the other hand,
group 3b contains those calls which are 1) actually am-
biguous, 2) considered by the router to be unambiguous,
and 3) routed to a destination which is not one of the
potential destinations.
We evaluated the router&apos;s performance on three sub-
sets of our test data, unambiguous requests alone, am-
biguous requests alone, and all requests combined. For
each set of data, we calculated a lowerbound perfor-
mance, which measures the percentage of calls that are
correctly routed, and an upperbound performance, which
measures the percentage of calls that are either correctly
routed or have the potential to be correctly routed. Ta-
ble 3 shows how the upperbounds and lowerbounds are
computed based on the classification in Figure 3 for each
of the three data sets. For instance, for unambiguous re-
quests (classes 1 and 2), the lowerbound is the number
of calls actually routed to the correct destination (I a)
divided by the number of total unambiguous requests,
while the upperbound is the number of calls actually
routed to the correct destination ( I a) plus the number of
calls which the router finds to be ambiguous between the
correct destination and some other destination(s) (2a), di-
vided by the number of unambiguous queries. The calls
in category 2a are considered to be potentially correct be-
cause it is likely that the call will be routed to the correct
destination after disambiguation.
Table 4 shows the upperbound and lowerbound perfor-
mance for each of the three test sets. These results show
</bodyText>
<table confidence="0.86319525">
Unambiguous Ambiguous All
Requests Requests Requests
LB 80.1% 58.5% 75.6%
UB 96.7% 98.8% 97.2%
</table>
<tableCaption confidence="0.999709">
Table 4: Router Performance with Threshold = 0.2
</tableCaption>
<bodyText confidence="0.999920470588235">
that the system&apos;s overall performance will fall some-
where between 75.6% and 97.2%. The actual perfor-
mance of the system is determined by two factors: 1) the
performance of the disambiguation module, which de-
termines the correct routing rate of the 16.6% of the un-
ambiguous calls that were considered ambiguous by the
router (class 2a), and 2) the percentage of calls that were
routed correctly out of the 40.4% ambiguous calls that
were considered unambiguous and routed by the router
(class 3a). Note that the performance figures in Table 4
are the result of 100% automatic routing, since no re-
quest in our test set failed to evoke at least one candidate
destination. In the next sections, we discuss the perfor-
mance of the disambiguation module, which determines
the overall system performance, and show how allowing
calls to be punted to operators affects the system&apos;s per-
formance.
</bodyText>
<subsectionHeader confidence="0.985853">
5.2 The Disambiguation Module
</subsectionHeader>
<bodyText confidence="0.999979">
To evaluate our disambiguation module, we needed dia-
logues which satisfy two criteria: 1) the caller&apos;s first ut-
terance is ambiguous, and 2) the operator asked a follow-
up question to disambiguate the query and subsequently
routed the call to the appropriate destination. We used
157 calls that meet these two criteria as our test set. Note
that this test set is disjoint from the test set used in the
evaluation of the router (Section 5.1), since none of the
transcribed calls in the latter test set satisfied criterion
(2).
For each ambiguous call, the first user utterance was
given to the router as input. The outcome of the router
was classified as follows:
</bodyText>
<listItem confidence="0.973679285714286">
1. Unambiguous: in this case the call was routed to the
selected destination. This routing was considered
correct if the selected destination was the same as
the actual destination and incorrect otherwise.
2. Ambiguous: in this case the router attempted to ini-
tiate disambiguation. The outcome of the routing of
these calls were determined as follows:
(a) Correct, if a disambiguation query was gener-
ated which, when answered, led to the correct
destination.
(b) Incorrect, if a disambiguation query was gen-
erated which, when answered, could not lead
to a correct destination.
(c) Reject, if the router could not form a sensi-
</listItem>
<bodyText confidence="0.926299583333333">
ble query or was unable to gather sufficient in-
formation from the user after its queries and
routed the call to an operator.
Table 5 shows the number of calls that fall into each
of the 5 categories. Out of the 157 calls, the router au-
tomatically routed 115 of them either with or without
disambiguation (73.2%). Furthermore, 87.0% of these
routed calls were routed to the correct destination. No-
tice that out of the 52 ambiguous calls that the router con-
sidered unambiguous, 40 were routed correctly (76.9%).
yes/&apos; \\\0 yes// \\\r
is lb 2a 2b
</bodyText>
<page confidence="0.73955">
261
</page>
<table confidence="0.968023666666667">
Routed As Unambiguous Routed As Ambiguous
Correct Incorrect Correct Incorrect Reject
40 12 60 3 42
</table>
<tableCaption confidence="0.9592885">
Table 5: Performance of Disambiguation Module on
Ambiguous Calls
</tableCaption>
<table confidence="0.999814">
Correct Incorrect Reject
Class 1 63.2% 1.3% 0%
Class 2 7.5% 1.7% 5.3%
Class 3 6.5% 2.2% 0%
Class 4 7.0% 0.4% 4.9%
Total 84.2% 5.6% 10.2%
</table>
<tableCaption confidence="0.998848">
Table 6: Overall Performance of Call Router
</tableCaption>
<bodyText confidence="0.9999664">
This is simply because our vector-based router is able to
distinguish between cases where an ambiguous query is
equally likely to be routed to more than one destination,
and situations where the likelihood of one potential desti-
nation overwhelms that of the other(s). In the latter case,
the router routes the call to the most likely destination in-
stead of initiating disambiguation, which has been shown
to be an effective strategy; not surprisingly, human op-
erators are also prone to guess the destination based on
likelihood and route callers without disambiguation.
</bodyText>
<subsectionHeader confidence="0.991747">
5.3 Overall Performance
</subsectionHeader>
<bodyText confidence="0.999944882352941">
Combining results from Section 5.2 for ambiguous calls
with results from Section 5.1 for unambiguous calls leads
to the overall performance of the call router in Table 6.
The table shows the number of calls that will be correctly
routed, incorrectly routed, and rejected, if we apply the
performance of the disambiguation module (Table 5) to
the calls that fall into each class in the evaluation of
the routing module (Section 5.1). Our results show that
out of the 389 calls in our test set, 89.8% of the calls
will be automatically routed by the call router. Of these
calls, 93.8% (which constitutes 84.2% of all calls) will
be routed to their correct destinations. This is substan-
tially better than the results obtained by Gorin et al., who
report an 84% correct routing rate with a 10% false rejec-
tion rate (routed to an operator when the call could have
been automatically routed) on 14 destinations (Gorin et
al., to appear).6
</bodyText>
<sectionHeader confidence="0.999839" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.974512538461538">
We described and evaluated a domain independent, au-
tomatically trained call router that takes one of three ac-
tions in response to a caller&apos;s request. It can route the
call to a destination within the call center, attempt to
6Gorin et al.&apos;s results are measured without the possibility of system
queries. To provide a fair comparison, we evaluated our routing module
on all 389 calls in our test set using the scoring method described in
(Gorin etal., to appear) (which corresponds roughly to our upperbound
measure), and achieved a 94.1% correct routing rate to 23 destinations
when all calls are automatically routed (no false rejection), a substantial
improvement over their system.
formulate a disambiguating query, or route the call to a
human operator. The routing module of the call router
selects a set of candidate destinations based on n-gram
terms extracted from the caller request and a vector-
based comparison between these n-gram terms and each
possible destination. If disambiguation is necessary, a
yes-no question or wh-question is dynamically generated
from among known n-gram terms in the domain based
on closeness, relevance, and disambiguating power, thus
tailoring the disambiguating query to the original request
and the candidate destinations. Finally, our system per-
forms substantially better than the best previously exist-
ing system, achieving an overall 93.8% correct routing
rate for automatically routed calls when rejecting 10.2%
of all calls.
</bodyText>
<sectionHeader confidence="0.999506" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999980666666667">
We would like to thank Christer Samuelsson and Jim Hi-
eronymus for helpful discussions, and Diane Litman for
comments on an earlier draft of this paper.
</bodyText>
<sectionHeader confidence="0.999679" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998821542857143">
A. Abella and A. Gorin. 1997. Generating semantically
consistent inputs to a dialog manager. In Proc. EU-
ROSPEECH, pages 1879-1882.
B. Carpenter and J. Chu-Carroll. submitted. Natural
language call routing: A robust, self-organizing ap-
proach.
S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and
R. Harshman. 1990. Indexing by latent semantic anal-
ysis. Journal of the American Society for Information
Science, 41:391-407.
A. Gorin, G. Riccardi, and J. Wright. to appear. How
may I help you? Speech Communication.
N. Green and S. Carberry. 1994. A hybrid reasoning
model for indirect answers. In Proc. ACL, pages 58-
65.
D. Harman. 1995. Overview of the fourth Text REtrieval
Conference. In Proc. TREC.
B. Hockey, D. Rossen-Knill, B. Spejewski, M. Stone,
and S. Isard. 1997. Can you predict responses to
yes/no questions? yes, no, and stuff. In Proc. EU-
ROSPEECH, pages 2267-2270.
J. McDonough, K. Ng, P. Jeanrenaud, H. Gish, and J. R.
Rohlicek. 1994. Approaches to topic identification on
the switchboard corpus. In Proc. ICASSP, pages 385-
388.
G. Salton. 1971. The SMART Retrieval System. Prentice
Hall.
H. Schiitze, D. Hull, and J. Pedersen. 1995. A compari-
son of classifiers and document representations for the
routing problem. In Proc. SIGIR.
K. Sparck Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 28:11-20.
R. Sproat, editor. 1997. Multilingual Text-to-Speech
Synthesis: The Bell Labs Approach. Kluwer.
</reference>
<page confidence="0.99715">
262
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.871150">
<title confidence="0.998205">Dialogue Management in Vector-Based Call Routing</title>
<author confidence="0.917724">Chu-Carroll Carpenter</author>
<affiliation confidence="0.984833">Lucent Technologies Bell Laboratories</affiliation>
<address confidence="0.9996335">600 Mountain Avenue Murray Hill, NJ 07974, U.S.A.</address>
<email confidence="0.999603">E-mail:{jencc,carp}@research.bell-labs.com</email>
<abstract confidence="0.99737455">This paper describes a domain independent, automatically trained call router which directs customer calls on their response to an open-ended may I diyour call?&amp;quot; Routing behavior is trained from a corpus of transcribed and hand-routed calls and then carried out using vector-based information retrieval techniques. Based on the statistical discriminating power of the n-gram terms extracted from the caller&apos;s request, the caller is 1) routed to the appropriate destination, 2) transferred to a human operator, or 3) asked a disambiguation question. In the last case, the system dynamically generates queries tailored to the caller&apos;s request and the destinations with which it is consistent. Our approach is domain independent and the training process is fully automatic. Evaluations over a financial services call center handling hundreds of activities with dozens of destinations demonstrate a substantial improvement on existing systems by correctly routing 93.8% of the calls after punting 10.2% of the calls to a human operator.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Abella</author>
<author>A Gorin</author>
</authors>
<title>Generating semantically consistent inputs to a dialog manager.</title>
<date>1997</date>
<booktitle>In Proc. EUROSPEECH,</booktitle>
<pages>1879--1882</pages>
<contexts>
<context position="4204" citStr="Abella and Gorin (1997)" startWordPosition="674" endWordPosition="677">refined in an interactive dialogue. We are further interested in carrying out the routing using natural, conversational language. The only work on call routing to date that we are aware of is that by Gorin et al. (to appear). They select salient phrase fragments from caller requests, such as made a long distance and the area code for. These phrase fragments are used to determine the most likely destination(s), which they refer to as call type(s), for the request either by computing the a posteriori probability for each call type or by passing the fragments through a neural network classifier. Abella and Gorin (1997) utilized the Boolean formula minimization algorithm for combining the resulting set of call types based on a hand-coded hierarchy of call types. Their intention is to utilize the outcome of this algorithm to select from a set of dialogue strategies for response generation. 3 Corpus Analysis To examine human-human dialogue behavior in call routing, we analyzed a set of 4497 transcribed telephone calls involving customers interacting with human operators, looking at both the semantics of caller requests and 256 Name Activity Indirect # of calls 949 3271 277 % of all calls 21.1% 72.7% 6.2% Table</context>
</contexts>
<marker>Abella, Gorin, 1997</marker>
<rawString>A. Abella and A. Gorin. 1997. Generating semantically consistent inputs to a dialog manager. In Proc. EUROSPEECH, pages 1879-1882.</rawString>
</citation>
<citation valid="false">
<authors>
<author>submitted</author>
</authors>
<title>Natural language call routing: A robust, self-organizing approach.</title>
<marker>submitted, </marker>
<rawString>B. Carpenter and J. Chu-Carroll. submitted. Natural language call routing: A robust, self-organizing approach.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S Dumais</author>
<author>G Furnas</author>
<author>T Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by latent semantic analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>41--391</pages>
<contexts>
<context position="13200" citStr="Deerwester et al., 1990" startWordPosition="2198" endWordPosition="2201">cument. We use the inverse document frequency (IDF) weighting scheme (Sparck Jones, 1972) whereby a term is weighted inversely to the number of documents in which it occurs, by means of IDF(t) = log2 n/d(t) where t is a term, n is the total number of documents in the corpus, and d(t) is the number of documents containing the term t. Thus we obtain a weighted matrix B, whose elements are given by Bt,c1= At,d X /DF(t)/(E \ 1/2 1&lt;e&lt;n A2 t,e/ • — — Vector Representation To reduce the dimensionality of our vector representations for terms and documents, we applied the singular value decomposition (Deerwester et al., 1990) to the m x n matrix B of weighted term-document frequencies. Specifically, we take B = U SVT , where U is an m x r orthonormal matrix (where r is the rank of B), V is an n x r orthonormal matrix, and S is an r x r diagonal matrix such that si,i &gt; s2,2 &gt; • • • &gt; Sr,r &gt; 0. We can think of each row in U as an r-dimensional vector that represents a term, whereas each row in V is an r-dimensional vector representing a document. With appropriate scaling of the axes by the singular values on the diagonal of S, we can compare documents to documents and terms to terms using their corresponding points </context>
<context position="15781" citStr="Deerwester et al., 1990" startWordPosition="2651" endWordPosition="2654">ms from the utterance. For instance, term extraction on the request &amp;quot;I want to check the balance in my savings account&amp;quot; would result in 258 one bigram term, &amp;quot;saving,account&amp;quot;, and two unigrams, &amp;quot;check&amp;quot; and &amp;quot;balance&amp;quot;. Pseudo-Document Generation Given the extracted terms from a caller request, we can represent the request as an 7n-dimensional vector Q where each component Qi represents the number of times that the ith term occurred in the caller&apos;s request. We then create an r-dimensional pseudo-document vector D = QU, following the standard methodology of vector-based information retrieval (see (Deerwester et al., 1990)). Note that D is simply the sum of the term vectors Ui for all terms occurring in the caller&apos;s request, weighted by their frequency of occurrence in the request, and is scaled properly for document/document comparison. Scoring Once the vector D for the pseudo-document is determined, we compare it with the document vectors by computing the cosine between D and each scaled document vectors in VS. Next, we transform the cosine score for each destination using a sigmoid function specifically fitted for that destination to obtain a confidence score that represents the router&apos;s confidence that the </context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S. Dumais, G. Furnas, T. Landauer, and R. Harshman. 1990. Indexing by latent semantic analysis. Journal of the American Society for Information Science, 41:391-407.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Gorin</author>
<author>G Riccardi</author>
<author>J Wright</author>
</authors>
<title>to appear. How may I help you? Speech Communication.</title>
<marker>Gorin, Riccardi, Wright, </marker>
<rawString>A. Gorin, G. Riccardi, and J. Wright. to appear. How may I help you? Speech Communication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Green</author>
<author>S Carberry</author>
</authors>
<title>A hybrid reasoning model for indirect answers.</title>
<date>1994</date>
<booktitle>In Proc. ACL,</booktitle>
<pages>58--65</pages>
<contexts>
<context position="23416" citStr="Green and Carberry, 1994" startWordPosition="3926" endWordPosition="3929">sting loans and new loans are again handled by different departments. Given this request, the call router first extracts the relevant n-gram terms, which in this case results in the unigram &amp;quot;loan&amp;quot;. It then computes a pseudo-document vector that represents this request, which is compared in turn with the 23 vectors representing all destinations in the call center. The cosine values between the request and each destination are then mapped into confidence values. 3ln our current system, a response is considered a yes response only if it explicitly contains the word yes. However, as discussed in (Green and Carberry, 1994; Hockey et al., 1997), responses to yes-no questions may not explicitly contain a yes or no term. We leave incorporating a more sophisticated response understanding model, such as (Green and Carberry, 1994), into our system for future work. Using a confidence threshold of 0.2, we have two candidate destinations, Loan Servicing and Consumer Lending; thus the disambiguation module is invoked. Our disambiguation module first selects from all ngram terms those whose term vectors are close to the difference between the request vector and either of the two candidate destination vectors. This result</context>
</contexts>
<marker>Green, Carberry, 1994</marker>
<rawString>N. Green and S. Carberry. 1994. A hybrid reasoning model for indirect answers. In Proc. ACL, pages 58-65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Harman</author>
</authors>
<title>Overview of the fourth Text REtrieval Conference. In</title>
<date>1995</date>
<booktitle>Proc. TREC.</booktitle>
<contexts>
<context position="3371" citStr="Harman, 1995" startWordPosition="537" endWordPosition="538">ation query generation module. Our disambiguation module is based on the same statistical training as routing, and dynamically generates queries tailored to the caller&apos;s request and the destinations with which it is consistent. The main advantages of our system are that 1) it is domain independent, 2) it is trained fully automatically to both route and disambiguate requests, and 3) its performance is sufficient for use in the field, substantially improving on that of previous systems. 2 Related Work Call routing is similar to topic identification (McDonough et al., 1994) and document routing (Harman, 1995) in identifying which one of n topics (destinations) most closely matches a caller&apos;s request. Call routing is distinguished from these activities by requiring a single destination, but allowing a request to be refined in an interactive dialogue. We are further interested in carrying out the routing using natural, conversational language. The only work on call routing to date that we are aware of is that by Gorin et al. (to appear). They select salient phrase fragments from caller requests, such as made a long distance and the area code for. These phrase fragments are used to determine the most</context>
</contexts>
<marker>Harman, 1995</marker>
<rawString>D. Harman. 1995. Overview of the fourth Text REtrieval Conference. In Proc. TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hockey</author>
<author>D Rossen-Knill</author>
<author>B Spejewski</author>
<author>M Stone</author>
<author>S Isard</author>
</authors>
<title>Can you predict responses to yes/no questions? yes, no, and stuff.</title>
<date>1997</date>
<booktitle>In Proc. EUROSPEECH,</booktitle>
<pages>2267--2270</pages>
<contexts>
<context position="23438" citStr="Hockey et al., 1997" startWordPosition="3930" endWordPosition="3933">are again handled by different departments. Given this request, the call router first extracts the relevant n-gram terms, which in this case results in the unigram &amp;quot;loan&amp;quot;. It then computes a pseudo-document vector that represents this request, which is compared in turn with the 23 vectors representing all destinations in the call center. The cosine values between the request and each destination are then mapped into confidence values. 3ln our current system, a response is considered a yes response only if it explicitly contains the word yes. However, as discussed in (Green and Carberry, 1994; Hockey et al., 1997), responses to yes-no questions may not explicitly contain a yes or no term. We leave incorporating a more sophisticated response understanding model, such as (Green and Carberry, 1994), into our system for future work. Using a confidence threshold of 0.2, we have two candidate destinations, Loan Servicing and Consumer Lending; thus the disambiguation module is invoked. Our disambiguation module first selects from all ngram terms those whose term vectors are close to the difference between the request vector and either of the two candidate destination vectors. This results in a list of 60 clos</context>
</contexts>
<marker>Hockey, Rossen-Knill, Spejewski, Stone, Isard, 1997</marker>
<rawString>B. Hockey, D. Rossen-Knill, B. Spejewski, M. Stone, and S. Isard. 1997. Can you predict responses to yes/no questions? yes, no, and stuff. In Proc. EUROSPEECH, pages 2267-2270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J McDonough</author>
<author>K Ng</author>
<author>P Jeanrenaud</author>
<author>H Gish</author>
<author>J R Rohlicek</author>
</authors>
<title>Approaches to topic identification on the switchboard corpus.</title>
<date>1994</date>
<booktitle>In Proc. ICASSP,</booktitle>
<pages>385--388</pages>
<contexts>
<context position="3335" citStr="McDonough et al., 1994" startWordPosition="529" endWordPosition="533">l routing technique, we focus on the disambiguation query generation module. Our disambiguation module is based on the same statistical training as routing, and dynamically generates queries tailored to the caller&apos;s request and the destinations with which it is consistent. The main advantages of our system are that 1) it is domain independent, 2) it is trained fully automatically to both route and disambiguate requests, and 3) its performance is sufficient for use in the field, substantially improving on that of previous systems. 2 Related Work Call routing is similar to topic identification (McDonough et al., 1994) and document routing (Harman, 1995) in identifying which one of n topics (destinations) most closely matches a caller&apos;s request. Call routing is distinguished from these activities by requiring a single destination, but allowing a request to be refined in an interactive dialogue. We are further interested in carrying out the routing using natural, conversational language. The only work on call routing to date that we are aware of is that by Gorin et al. (to appear). They select salient phrase fragments from caller requests, such as made a long distance and the area code for. These phrase frag</context>
</contexts>
<marker>McDonough, Ng, Jeanrenaud, Gish, Rohlicek, 1994</marker>
<rawString>J. McDonough, K. Ng, P. Jeanrenaud, H. Gish, and J. R. Rohlicek. 1994. Approaches to topic identification on the switchboard corpus. In Proc. ICASSP, pages 385-388.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>The SMART Retrieval System.</title>
<date>1971</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="10808" citStr="Salton, 1971" startWordPosition="1779" endWordPosition="1780">, the root forms of caller utterances are filtered through two lists, the ignore list and the stop list, in order to build a better n-gram model. The ignore list consists of noise words, such as uh and um, which sometimes get in the way of proper n-gram extraction, as in &amp;quot;I&apos;d like to speak to someone about a car uh loan&amp;quot;. With noise word filtering, we can properly extract the bigram &amp;quot;car,loan&amp;quot;. The stop list enumerates words that do not discriminate between destinations, such as the, be, and afternoon. We modified the standard stop list distributed with the SMART information retrieval system (Salton, 1971) to include domain specific terms and proper names that occurred in the training corpus. Note that when a stop word is filtered out of the caller utterance, a placeholder is inserted to prevent the words preceding and following the stop word to form n-grams. For instance, after filtering the stop words out of &amp;quot;I want to check on an account&amp;quot;, the utterance becomes &amp;quot;&lt;sw&gt; &lt;sw&gt; &lt;sw&gt; check &lt;sw&gt; &lt;sw&gt; account&amp;quot;. Without the placeholders, we would extract the bigram &amp;quot;check,account&amp;quot;, just as if the caller had used the term checking account. Term Extraction We extract the n-gram terms that occur more fre</context>
<context position="13999" citStr="Salton, 1971" startWordPosition="2358" endWordPosition="2359">al matrix, and S is an r x r diagonal matrix such that si,i &gt; s2,2 &gt; • • • &gt; Sr,r &gt; 0. We can think of each row in U as an r-dimensional vector that represents a term, whereas each row in V is an r-dimensional vector representing a document. With appropriate scaling of the axes by the singular values on the diagonal of S, we can compare documents to documents and terms to terms using their corresponding points in this new r-dimensional space (Deerwester et al., 1990). For instance, to employ the dot product of two vectors as a measure of their similarity as is common in information retrieval (Salton, 1971), we have the matrix BTB whose elements contain the dot product of document vectors. Because S is diagonal and U is orthonormal, BTB = VS2VT = VS(VS)T . Thus, element i, j in BTB, representing the dot product between document vectors i and j, can be computed by taking the dot product between the i and j rows of the matrix VS. In other words, we can consider rows in the matrix VS as vectors representing documents for the purpose of document/document comparison. An element of the original matrix Be,,,representing the degree of association between the ith term and the jth document, can be recover</context>
</contexts>
<marker>Salton, 1971</marker>
<rawString>G. Salton. 1971. The SMART Retrieval System. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schiitze</author>
<author>D Hull</author>
<author>J Pedersen</author>
</authors>
<title>A comparison of classifiers and document representations for the routing problem.</title>
<date>1995</date>
<booktitle>In Proc. SIGIR.</booktitle>
<contexts>
<context position="9345" citStr="Schiitze et al., 1995" startWordPosition="1542" endWordPosition="1545">nations, the disambiguation module is invoked in an attempt to formulate a query; and if there is no appropriate destination or if a reasonable disambiguation query cannot be generated, the call is routed to an operator. Figure 1 shows a diagram outlining this process. 4.1 The Routing Module Our approach is novel in its application of information retrieval techniques to select candidate destinations for a call. We treat call routing as an instance of document routing, where a collection of judged documents is used for training and the task is to judge the relevance of a set of test documents (Schiitze et al., 1995). More specifiCandidate Destinations Routing Notification Caller Response Di ambiguating Query 257 cally, each destination in our call center is represented as a collection of documents (transcriptions of calls routed to that destination), and given a caller request, we judge the relevance of the request to each destination. 4.1.1 The Training Process Document Construction Our training corpus consists of 3753 calls each of which is hand-routed to one of 23 destinations.2 Our first step is to create one (virtual) document per destination, which contains the text of the callers&apos; contributions to</context>
</contexts>
<marker>Schiitze, Hull, Pedersen, 1995</marker>
<rawString>H. Schiitze, D. Hull, and J. Pedersen. 1995. A comparison of classifiers and document representations for the routing problem. In Proc. SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>Journal of Documentation,</journal>
<pages>28--11</pages>
<contexts>
<context position="12665" citStr="Jones, 1972" startWordPosition="2102" endWordPosition="2103">begin by normalizing the row vectors representing terms by making them each of unit length. Thus we divide each row At in the original matrix by its length, 2These 3753 calls are a subset of the corpus of 4497 calls used in our corpus analysis. We excluded those ambiguous calls that were not resolved by the operator. (El&lt;e&lt;n 4,e)1/2. Our second weighting is based on the tiotron that a term that only occurs in a few documents is more important in discriminating among documents than a term that occurs in nearly every document. We use the inverse document frequency (IDF) weighting scheme (Sparck Jones, 1972) whereby a term is weighted inversely to the number of documents in which it occurs, by means of IDF(t) = log2 n/d(t) where t is a term, n is the total number of documents in the corpus, and d(t) is the number of documents containing the term t. Thus we obtain a weighted matrix B, whose elements are given by Bt,c1= At,d X /DF(t)/(E \ 1/2 1&lt;e&lt;n A2 t,e/ • — — Vector Representation To reduce the dimensionality of our vector representations for terms and documents, we applied the singular value decomposition (Deerwester et al., 1990) to the m x n matrix B of weighted term-document frequencies. Spe</context>
</contexts>
<marker>Jones, 1972</marker>
<rawString>K. Sparck Jones. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28:11-20.</rawString>
</citation>
<citation valid="true">
<title>Multilingual Text-to-Speech Synthesis: The Bell Labs Approach.</title>
<date>1997</date>
<editor>R. Sproat, editor.</editor>
<publisher>Kluwer.</publisher>
<contexts>
<context position="4204" citStr="(1997)" startWordPosition="677" endWordPosition="677">eractive dialogue. We are further interested in carrying out the routing using natural, conversational language. The only work on call routing to date that we are aware of is that by Gorin et al. (to appear). They select salient phrase fragments from caller requests, such as made a long distance and the area code for. These phrase fragments are used to determine the most likely destination(s), which they refer to as call type(s), for the request either by computing the a posteriori probability for each call type or by passing the fragments through a neural network classifier. Abella and Gorin (1997) utilized the Boolean formula minimization algorithm for combining the resulting set of call types based on a hand-coded hierarchy of call types. Their intention is to utilize the outcome of this algorithm to select from a set of dialogue strategies for response generation. 3 Corpus Analysis To examine human-human dialogue behavior in call routing, we analyzed a set of 4497 transcribed telephone calls involving customers interacting with human operators, looking at both the semantics of caller requests and 256 Name Activity Indirect # of calls 949 3271 277 % of all calls 21.1% 72.7% 6.2% Table</context>
</contexts>
<marker>1997</marker>
<rawString>R. Sproat, editor. 1997. Multilingual Text-to-Speech Synthesis: The Bell Labs Approach. Kluwer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>