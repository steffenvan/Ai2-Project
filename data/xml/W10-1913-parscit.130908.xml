<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9988285">
Identifying the Information Structure of Scientific Abstracts: An
Investigation of Three Different Schemes
</title>
<author confidence="0.997667">
Yufan Guo
</author>
<affiliation confidence="0.998407">
University of Cambridge, UK
</affiliation>
<email confidence="0.919763">
yg244@cam.ac.uk
</email>
<author confidence="0.580003">
Ilona Silins
</author>
<affiliation confidence="0.527355">
Karolinska Institutet, SWEDEN
</affiliation>
<email confidence="0.924741">
Ilona.Silins@ki.se
</email>
<author confidence="0.989945">
Anna Korhonen
</author>
<affiliation confidence="0.996528">
University of Cambridge, UK
</affiliation>
<email confidence="0.939861">
alk23@cam.ac.uk
</email>
<author confidence="0.996779">
Lin Sun
</author>
<affiliation confidence="0.998343">
University of Cambridge, UK
</affiliation>
<email confidence="0.972294">
ls418@cam.ac.uk
</email>
<author confidence="0.936483">
Maria Liakata
</author>
<affiliation confidence="0.917027">
Aberystwyth University, UK
</affiliation>
<email confidence="0.963821">
mal@aber.ac.uk
</email>
<author confidence="0.75359">
Ulla Stenius
</author>
<affiliation confidence="0.667494">
Karolinska Institutet, SWEDEN
</affiliation>
<email confidence="0.964941">
Ulla.Stenius@ki.se
</email>
<sectionHeader confidence="0.996942" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995125">
Many practical tasks require accessing
specific types of information in scientific
literature; e.g. information about the ob-
jective, methods, results or conclusions of
the study in question. Several schemes
have been developed to characterize such
information in full journal papers. Yet
many tasks focus on abstracts instead. We
take three schemes of different type and
granularity (those based on section names,
argumentative zones and conceptual struc-
ture of documents) and investigate their
applicability to biomedical abstracts. We
show that even for the finest-grained of
these schemes, the majority of categories
appear in abstracts and can be identified
relatively reliably using machine learning.
We discuss the impact of our results and
the need for subsequent task-based evalu-
ation of the schemes.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999757254237288">
Scientific abstracts tend to be very similar in terms
of their information structure. For example, many
abstracts provide some background information
before defining the precise objective of the study,
and the conclusions are typically preceded by the
description of the results obtained.
Many readers of scientific abstracts are inter-
ested in specific types of information only, e.g.
the general background of the study, the methods
used in the study, or the results obtained. Accord-
ingly, many text mining tasks focus on the ex-
traction of information from certain parts of ab-
stracts only. Therefore classification of abstracts
(or full articles) according to the categories of in-
formation structure can support both the manual
study of scientific literature as well as its auto-
matic analysis, e.g. information extraction, sum-
marization and information retrieval (Teufel and
Moens, 2002; Mizuta et al., 2005; Tbahriti et al.,
2006; Ruch et al., 2007).
To date, a number of different schemes and
techniques have been proposed for sentence-based
classification of scientific literature according to
information structure, e.g. (Teufel and Moens,
2002; Mizuta et al., 2005; Lin et al., 2006; Hi-
rohata et al., 2008; Teufel et al., 2009; Shatkay
et al., 2008; Liakata et al., 2010). Some of the
schemes are coarse-grained and merely classify
sentences according to typical section names seen
in scientific documents (Lin et al., 2006; Hirohata
et al., 2008). Others are finer-grained and based
e.g. on argumentative zones (Teufel and Moens,
2002; Mizuta et al., 2005; Teufel et al., 2009),
qualitative dimensions (Shatkay et al., 2008) or
conceptual structure (Liakata et al., 2010) of doc-
uments.
The majority of such schemes have been de-
veloped for full scientific journal articles which
are richer in information and also considered to
be more in need of the definition of information
structure (Lin, 2009). However, many practical
tasks currently focus on abstracts. As a distilled
summary of key information in full articles, ab-
stracts may exhibit an entirely different distribu-
tion of scheme categories than full articles. For
tasks involving abstracts, it would be useful to
know which schemes are applicable to abstracts
and which can be automatically identified in them
with reasonable accuracy.
In this paper, we will compare the applicabil-
ity of three different schemes – those based on
section names, argumentative zones and concep-
tual structure of documents – to a collection of
biomedical abstracts used for cancer risk assess-
ment (CRA). CRA is an example of a real-world
task which could greatly benefit from knowledge
about the information structure of abstracts since
cancer risk assessors look for a variety of infor-
mation in them ranging from specific methods to
</bodyText>
<page confidence="0.983385">
99
</page>
<note confidence="0.979208">
Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 99–107,
Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999894727272727">
results concerning different chemicals (Korhonen
et al., 2009). We report work on the annotation
of CRA abstracts according to each scheme and
investigate the schemes in terms of their distri-
bution, mutual overlap, and the success of iden-
tifying them automatically using machine learn-
ing. Our investigation provides an initial idea of
the practical usefulness of the schemes for tasks
involving abstracts. We discuss the impact of our
results and the further task-based evaluation which
we intend to conduct in the context of CRA.
</bodyText>
<sectionHeader confidence="0.881554" genericHeader="introduction">
2 The three schemes
</sectionHeader>
<bodyText confidence="0.985959333333333">
We investigate three different schemes – those
based on Section Names (S1), Argumentative
Zones (S2) and Core Scientific Concepts (S3):
S1: The first scheme differs from the others in the
sense that it is actually designed for abstracts. It
is based on section names found in some scientific
abstracts. We use the 4-way classification from
(Hirohata et al., 2008) where abstracts are divided
into objective, method, results and conclusions.
Table 1 provides a short description of each cate-
gory for this and other schemes (see also this table
for any category abbreviations used in this paper).
</bodyText>
<listItem confidence="0.994553807692308">
S2: The second scheme is based on Argumenta-
tive Zoning (AZ) of documents. The idea of AZ
is to follow the knowledge claims made by au-
thors. Teufel and Moens (2002) introduced AZ
and applied it to computational linguistics papers.
Mizuta et al. (2005) modified the scheme for biol-
ogy papers. More recently, Teufel et al. (2009) in-
troduced a refined version of AZ and applied it to
chemistry papers. As these schemes are too fine-
grained for abstracts (some of the categories do
not appear in abstracts at all), we adopt a reduced
version of AZ which integrates seven categories
from (Teufel and Moens, 2002) and (Mizuta et al.,
2005) - those which actually appear in abstracts.
S3: The third scheme is concept-driven and
ontology-motivated (Liakata et al., 2010). It treats
scientific papers as humanly-readable representa-
tions of scientific investigations and seeks to re-
trieve the structure of the investigation from the
paper as generic high-level Core Scientific Con-
cepts (CoreSC). The CoreSC is a 3-layer annota-
tion scheme but we only consider the first layer
in the current work. The second layer pertains to
properties of the categories (e.g. “advantage” vs.
“disadvantage” of METH, “new” vs. “old” METH
or OBJT). Such level of granularity is rare in ab-
</listItem>
<bodyText confidence="0.999932571428571">
stracts. The 3rd layer involves coreference iden-
tification between the same instances of each cat-
egory, which is also not of concern in abstracts.
With eleven categories, S3 is the most fine-grained
of our schemes. CoreSC has been previously ap-
plied to chemistry papers (Liakata et al., 2010,
2009).
</bodyText>
<sectionHeader confidence="0.969273" genericHeader="method">
3 Data: cancer risk assessment abstracts
</sectionHeader>
<bodyText confidence="0.999913777777778">
We used as our data the corpus of CRA ab-
stracts described in (Korhonen et al., 2009) which
contains MedLine abstracts from different sub-
domains of biomedicine. The abstracts were se-
lected so that they provide rich information about
various scientific data (human, animal and cellu-
lar) used for CRA. We selected 1000 abstracts (in
random) from this corpus. The resulting data in-
cludes 7,985 sentences and 225,785 words in total.
</bodyText>
<sectionHeader confidence="0.805323" genericHeader="method">
4 Annotation of abstracts
</sectionHeader>
<bodyText confidence="0.999962774193548">
Annotation guidelines. We used the guidelines of
Liakata for S3 (Liakata and Soldatova, 2008), and
developed the guidelines for S1 and S2 (15 pages
each). The guidelines define the unit (a sentence)
and the categories of annotation and provide ad-
vice for conflict resolution (e.g. which categories
to prefer when two or several are possible within
the same sentence), as well as examples of anno-
tated abstracts.
Annotation tool. We modified the annotation tool
of Korhonen et al. (2009) so that it could be used to
annotate abstracts according to the schemes. This
tool was originally developed for the annotation of
CRA abstracts according to the scientific evidence
they contain. The tool works as a Firefox plug-in.
Figure 1 shows an example of an abstract anno-
tated according to the three schemes.
Description of annotation. Using the guidelines
and the tool, the CRA corpus was annotated ac-
cording to each of the schemes. The annotation
proceeded scheme by scheme, independently, so
that annotations of one scheme were not based on
any of the other two. One annotator (a computa-
tional linguist) annotated all the abstracts accord-
ing to the three schemes, starting from the coarse-
grained S1, then proceeding to S2 and finally to
the finest-grained S3. It took 45, 50 and 90 hours
in total for S1, S2 and S3, respectively.
The resulting corpus. Table 2 shows the distri-
bution of sentences per scheme category in the re-
sulting corpus.
</bodyText>
<page confidence="0.996604">
100
</page>
<tableCaption confidence="0.99928">
Table 1: The Three Schemes
</tableCaption>
<figure confidence="0.736577692307692">
S1 Objective OBJ
Method METH
The background and the aim of the research
The way to achieve the goal
Result RES
Conclusion CON
The principle findings
Analysis, discussion and the main conclusions
S2 Background BKG
Objective OBJ
Method METH
Result RES
Conclusion CON
</figure>
<figureCaption confidence="0.3689385">
Related work REL
Future work FUT
</figureCaption>
<bodyText confidence="0.950698846153846">
The circumstances pertaining to the current work, situation, or its causes, history, etc.
A thing aimed at or sought, a target or goal
A way of doing research, esp. according to a defined and regular plan; a special form of proce-
dure or characteristic set of procedures employed in a field of study as a mode of investigation
and inquiry
The effect, consequence, issue or outcome of an experiment; the quantity, formula, etc. obtained
by calculation
A judgment or statement arrived at by any reasoning process; an inference, deduction, induc-
tion; a proposition deduced by reasoning from other propositions; the result of a discussion,
or examination of a question, final determination, decision, resolution, final arrangement or
agreement
A comparison between the current work and the related work
The work that needs to be done in the future
</bodyText>
<figure confidence="0.980383090909091">
S3 Hypothesis HYP
Motivation MOT
Background BKG
Goal GOAL
Object OBJT
Experiment EXP
Model MOD
Method METH
Observation OBS
Result RES
Conclusion CON
</figure>
<bodyText confidence="0.994101705882353">
A statement that has not been yet confirmed rather than a factual statement
The reason for carrying out the investigation
Description of generally accepted background knowledge and previous work
The target state of the investigation where intended discoveries are made
An entity which is a product or main theme of the investigation
Experiment details
A statement about a theoretical model or framework
The means by which the authors seek to achieve a goal of the investigation
The data/phenomena recorded within an investigation
Factual statements about the outputs of an investigation
Statements inferred from observations and results, relating to research hypothesis
Inter-annotator agreement. We measured the
inter-annotator agreement on 300 abstracts (i.e. a
third of the corpus) using three annotators (one lin-
guist, one expert in CRA, and the computational
linguist who annotated all the corpus). Accord-
ing to Cohen’s Kappa (Cohen, 1960), the inter-
annotator agreement for S1, S2, and S3 was κ =
0.84, κ = 0.85, and κ = 0.50, respectively. Ac-
cording to (Landis and Koch, 1977), the agree-
ment 0.81-1.00 is perfect and 0.41-0.60 is mod-
erate. Our results indicate that S1 and S2 are
the easiest schemes for the annotators and S3 the
most challenging. This is not surprising as S3 is
the scheme with the finest granularity. Its reliable
identification may require a longer period of train-
ing and possibly improved guidelines. Moreover,
previous annotation efforts using S3 have used do-
main experts for annotation (Liakata et al., 2009,
2010). In our case the domain expert and the lin-
guist agreed the most on S3 (κ = 0.60). For S1
and S2 the best agreement was between the lin-
guist and the computational linguist (κ = 0.87 and
κ = 0.88, respectively).
</bodyText>
<tableCaption confidence="0.541892">
Table 2: Distribution of sentences in the scheme-
annotated CRA corpus
</tableCaption>
<sectionHeader confidence="0.8436765" genericHeader="method">
5 Comparison of the schemes in terms of
annotations
</sectionHeader>
<bodyText confidence="0.999812846153846">
The three schemes we have used to annotate ab-
stracts were developed independently and have
separate guidelines. Thus, even though they seem
to have some categories in common (e.g. METH,
RES, CON) this does not necessarily guarantee that
the latter cover the same information across all
three schemes. We therefore wanted to investigate
the relation between the schemes and the extent of
overlap or complementarity between them.
We used the annotations obtained with each
scheme to create three contingency matrices for
pairwise comparison. We calculated the chi-
squared Pearson statistic, the chi-squared like-
</bodyText>
<table confidence="0.987433666666667">
S1 OBJ METH RES CON
61483 39163 89575 35564 Words
2145 1396 3203 1241 Sentences
27% 17% 40% 16% Sentences
S2 BKG OBJ METH RES CON REL FUT
36828 23493 41544 89538 30752 2456 1174 Words
1429 674 1473 3185 1082 95 47 Sentences
18% 8% 18% 40% 14% 1% 1% Sentences
S3 HYP MOT BKG GOAL OBJT EXP MOD METH OBS RES CON
2676 4277 28028 10612 15894 22444 1157 17982 17402 75951 29362 Words
99 172 1088 294 474 805 41 637 744 2582 1049 Sentences
1% 2% 14% 4% 6% 10% 1% 8% 9% 32% 13% Sentences
</table>
<page confidence="0.992334">
101
</page>
<figureCaption confidence="0.9698">
Figure 1: An example of an abstract annotated ac-
cording to the three schemes
</figureCaption>
<bodyText confidence="0.962419235294118">
S1
lihood ratio, the contingency coefficient and
Cramer’s V (Table 3)1, all of which showed a def-
inite correlation between rows and columns for the
pairwise comparison of all three schemes.
However, none of the above measures give an
indication of the differential association between
schemes, i.e. whether it goes both directions and
to what extent. For this reason we calculated the
Goodman-Kruskal lambda L statistic (Siegel and
Castellan, 1988), which gives us the reduction in
error for predicting the categories of one annota-
tion scheme, if we know the categories assigned
according to the other. When using the categories
of S1 as the independent variables, we obtained a
lambda of over 0.72 which suggests a 72% reduc-
tion in error in predicting S2 categories and 47% in
</bodyText>
<footnote confidence="0.776834">
1These are association measures for r x c tables. We used
the implementation in the vcd package of R (http://www.r-
project.org/).
</footnote>
<bodyText confidence="0.999836591836735">
predicting S3 categories. With S2 categories being
the independent variables, we obtained a reduction
in error of 88% when predicting S1 and 55% when
predicting S3 categories. The lower lambdas for
predicting S3 are hardly surprising as S3 has 11
categories as opposed to 4 and 7 for S1 and S2 re-
spectively. S3 on the other hand has strong predic-
tive power in predicting the categories of S1 and
S2 with lambdas of 0.86 and 0.84 respectively. In
terms of association, S1 and S2 seem to be more
strongly associated, followed by S1 and S3 and
then S2 and S3.
We were then interested in the correspondence
between the actual categories of the three schemes,
which is visualized in Figure 2. Looking at the
categories of S1, OBJ maps mostly to BKG and OBJ
in S2 (with a small percentage in METH and REL).
S1 OBJ maps to BKG, GOAL, HYP, MOT and OBJT
in S3 (with a small percentage in METH and MOD).
S1 METH maps to METH in S2 (with a small per-
centage in S2 OBJ) while it maps to EXP, METH
and MOD in S3 (with a small percentage in GOAL
and OBJT). S1 RES covers S2 RES and 40% REL,
whereas in S3 it covers RES, OBS and 20% MOD.
S1 CON covers S2 CON, FUT, 45% REL and a small
percentage of RES. In terms of the S2 vs S3 com-
parison, S2 BKG maps to S3 BKG, HYP, MOT and a
small percentage of OBJT and MOD. S2 CON maps
to S3 CON, with a small percentage in RES, OBS
and HYP. S2 FUT maps entirely to S3 CON. S2
METH maps to S3 METH, EXP, MOD, 20% OBJT
and a small percentage of GOAL. S2 OBJ maps
to S3 GOAL and OBJT, with 15% HYP, MOD and
MOT and a small percentage in METH. S2 REL
spans across S3 CON, RES, MOT and OBJT, albeit
in very small percentages. Finally, S2 RES maps to
S3 RES and OBS, with 25% in MOD and small per-
centages in METH, CON, OBJT. Thus, it appears
that each category in S1 maps to a couple of cate-
gories in S2 and several in S3, which in turn seem
to elaborate on the S2 categories.
Based on the above analysis of the categories,
it is reasonable to assume a subsumption relation
between the categories of the type S1 &gt; S2 &gt;
S3, with REL cutting across several of the S3 cat-
egories and FUT branching off S3 CON. This is
an interesting and exciting outcome given that the
three different schemes have such a different ori-
gin.
</bodyText>
<equation confidence="0.6059595">
S3
S2
</equation>
<page confidence="0.989168">
102
</page>
<tableCaption confidence="0.999797">
Table 3: Association measures between schemes S1, S2, S3
</tableCaption>
<table confidence="0.999418666666667">
S1 vs S2 S1 vs S3 S2 vs S3
X2 df P X2 df P X2 df P
Likelihood Ratio 5577.1 18 0 5363.6 30 0 6293.4 60 0
Pearson 6613.0 18 0 6371.0 30 0 8554.7 60 0
Contingency Coeff 0.842 0.837 0.871
Cramer’s V 0.901 0.885 0.725
</table>
<figureCaption confidence="0.5041355">
Figure 2: Pairwise interpretation of categories of
one scheme in terms of the categories of the other.
</figureCaption>
<sectionHeader confidence="0.9934795" genericHeader="method">
6 Automatic identification of information
structure
</sectionHeader>
<subsectionHeader confidence="0.980741">
6.1 Features
</subsectionHeader>
<bodyText confidence="0.9998138">
The first step in automatic identification of infor-
mation structure is feature extraction. We chose
a number of general purpose features suitable for
all the three schemes. With the exception of our
novel verb class feature, the features are similar to
those employed in related works, e.g. (Teufel and
Moens, 2002; Mullen et al., 2005; Hirohata et al.,
2008):
History. There are typical patterns in the infor-
mation structure, e.g. RES tends to be followed
by CON rather than by BKG. Therefore, we used
the category assigned to the previous sentence as
a feature.
Location. Categories tend to appear in typical po-
sitions in a document, e.g. BKG occurs often in the
beginning and CON at the end of the abstract. We
divided each abstract into ten equal parts (1-10),
measured by the number of words, and defined the
location (of a sentence) feature by the parts where
the sentence begins and ends.
Word. Like many text classification tasks, we em-
ployed all the words in the corpus as features.
Bi-gram. We considered each bi-gram (combina-
tion of two word features) as a feature.
Verb. Verbs are central to the meaning of sen-
tences, and can vary from one category to another.
For example, experiment is frequent in METH and
conclude in CON. Previous works have used the
matrix verb of each sentence as a feature. Because
the matrix verb is not the only meaningful verb,
we used all the verbs instead.
Verb Class. Because individual verbs can result in
sparse data problems, we also experimented with a
novel feature: verb class (e.g. the class of EXPERI-
MENT verbs for verbs such as measure and inject).
We obtained 60 classes by clustering verbs appear-
ing in full cancer risk assessment articles using the
approach of Sun and Korhonen (2009).
POS. Tense tends to vary from one category to an-
other, e.g. past is common in RES and past partici-
</bodyText>
<page confidence="0.998743">
103
</page>
<bodyText confidence="0.999742666666667">
ple in CON. We used the part-of-speech (POS) tag
of each verb assigned by the C&amp;C tagger (Curran
et al., 2007) as a feature.
GR. Structural information about heads and de-
pendents has proved useful in text classification.
We used grammatical relations (GRs) returned by
the C&amp;C parser as features. They consist of a
named relation, a head and a dependent, and pos-
sibly extra parameters depending on the relation
involved, e.g. (dobj investigate mouse). We cre-
ated features for each subject (ncsubj), direct ob-
ject (dobj), indirect object (iobj) and second object
(obj2) relation in the corpus.
Subj and Obj. As some GR features may suf-
fer from data sparsity, we collected all the subjects
and objects (appearing with any verbs) from GRs
and used them as features.
Voice. There may be a correspondence between
the active and passive voice and categories (e.g.
passive is frequent in METH). We therefore used
voice as a feature.
</bodyText>
<subsectionHeader confidence="0.99299">
6.2 Methods
</subsectionHeader>
<bodyText confidence="0.975981777777778">
We used Naive Bayes (NB) and Support Vector
Machines (SVM) for classification. NB is a sim-
ple and fast method while SVM has yielded high
performance in many text classification tasks.
NB applies Bayes’ rule and Maximum Like-
lihood estimation with strong independence as-
sumptions. It aims to select the class c with maxi-
mum probability given the feature set F:
arg maxc P(c|f)=arg maxc P(cP F()
</bodyText>
<equation confidence="0.715857333333333">
c�
=arg maxc P(c)·P(f|c) {
=arg maxc P(c) · H f IF P(J Ic)
</equation>
<bodyText confidence="0.889011294117647">
SVM constructs hyperplanes in a multidimen-
sional space that separates data points of different
classes. Good separation is achieved by the hyper-
plane that has the largest distance from the nearest
data points of any class. The hyperplane has the
form w · x − b = 0, where w is the normal vector
to the hyperplane. We want to maximize the dis-
tance from the hyperplane to the data points, or the
distance between two parallel hyperplanes each of
which separates the data. The parallel hyperplanes
can be written as:
w·x−b = 1 and w·x−b = −1, and the distance
between the two is 2
|�|. The problem reduces to:
Minimize |w|
Subject to w · xi − b &gt; 1 for xi of one class,
and w · xi − b &lt; −1 for xi of the other.
</bodyText>
<sectionHeader confidence="0.983619" genericHeader="method">
7 Experimental evaluation
</sectionHeader>
<subsectionHeader confidence="0.986986">
7.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999984166666667">
We developed a tokenizer to detect the bound-
aries of sentences and to perform basic tokenisa-
tion, such as separating punctuation from adjacent
words e.g. in tricky biomedical terms such as 2-
amino-3,8-diethylimidazo[4,5-f]quinoxaline. We
used the C&amp;C tools (Curran et al., 2007) for POS
tagging, lemmatization and parsing. The lemma
output was used for extracting Word, Bi-gram and
Verb features. The parser produced GRs for each
sentence from which we extracted the GR, Subj,
Obj and Voice features. We only considered the
GRs relating to verbs. The ”obj” marker in a sub-
ject relation indicates a verb in passive voice (e.g.
(ncsubj observed 14 difference 5 obj)). To control
the number of features we removed the words and
GRs with fewer than 2 occurrences and bi-grams
with fewer than 5 occurrences, and lemmatized the
lexical items for all the features.
</bodyText>
<subsectionHeader confidence="0.603691">
7.2 Evaluation methods
</subsectionHeader>
<bodyText confidence="0.999994">
We used Weka (Witten, 2008) for the classifica-
tion, employing its NB and SVM linear kernel. The
results were measured in terms of accuracy (the
percentage of correctly classified sentences), pre-
cision, recall, and F-Measure. We used 10-fold
cross validation to avoid the possible bias intro-
duced by relying on any one particular split of the
data. The data were randomly divided into ten
parts of approximately the same size. Each indi-
vidual part was retained as test data and the re-
maining nine parts were used as training data. The
process was repeated ten times with each part used
once as the test data. The resulting ten estimates
were then combined to give a final score. We
compare our classifiers against a baseline method
based on random sampling of category labels from
training data and their assignment to sentences on
the basis of their observed distribution.
</bodyText>
<sectionHeader confidence="0.52266" genericHeader="evaluation">
7.3 Results
</sectionHeader>
<bodyText confidence="0.999934444444445">
Table 4 shows F-measure results when using each
individual feature alone, and Table 5 when using
all the features but the individual feature in ques-
tion. In these two tables, we only report the results
for SVM which performed considerably better than
NB. Although we have results for most scheme
categories, the results for some are missing due to
the lack of sufficient training data (see Table 2), or
due to a small feature set (e.g. History alone).
</bodyText>
<page confidence="0.998335">
104
</page>
<tableCaption confidence="0.933891">
Table 4: F-Measure results when using each in-
</tableCaption>
<table confidence="0.968720928571429">
dividual feature alone Table 5: F-Measure results using all the features and
all but one of the features
a b c d e f g h i j k
S1 OBJ .39 .83 .71 .69 .52 .45 .45 .45 .54 .39 -
METH - .47 .81 .74 .63 .49 - .46 .03 .42 .51
RES - .76 .85 .86 .76 .70 .72 .69 .70 .68 .54
CON - .72 .70 .65 .63 .53 .49 .57 .68 .20 -
S2 BKG .26 .73 .69 .67 .45 .38 .56 .33 .33 .29 -
OBJ - .13 .72 .68 .54 .63 - .49 .48 .20 -
METH - .50 .81 .72 .64 .47 - .47 .03 .42 .51
RES - .76 .85 .87 .76 .72 .72 .70 .69 .68 .54
CON - .70 .73 .71 .62 .51 .40 .61 .67 .23 -
REL _ _ _ _ _ _ _ _ _ _ _
FUT _ _ _ _ _ _ _ _ _ _ _
S3 HYP - - - - .67 - - - - - -
MOT .18 .57 .70 .49 .39 .13 .36 .33 .30 .40 -
BKG - - .54 .40 .21 - - .11 .06 .06 -
GOAL - - .53 .33 .22 - .19 .31 - .25 -
OBJT - - .73 .63 .60 .10 - .26 .32 --
EXP - .22 .63 .46 .33 .30 - .31 .07 .44 .25
MOD _ _ _ _ _ _ _ _ _ _
_
METH - - .82 .61 .39 .39 - .50 - .37 -
OBS - .59 .75 .71 .63 .56 .56 .54 .48 .52 .47
RES - - .87 .73 .41 .34 - .38 .24 .35 -
CON - .74 .68 .65 .65 .50 .48 .49 .55 .21 -
a-k: History, Location, Word, Bi-gram, Verb, Verb Class, POS, GR,
Subj, Obj, Voice
</table>
<bodyText confidence="0.9995135">
Looking at individual features alone, Word,
Bi-gram and Verb perform the best for all the
schemes, and History and Voice perform the worst.
In fact History performs very well on the training
data, but for the test data we can only use esti-
mates rather than the actual labels. The Voice fea-
ture works only for RES and METH for S1 and S2,
and for OBS for S3. This feature is probably only
meaningful for some of the categories.
When using all but one of the features, S1 and
S2 suffer the most from the absence of Location,
while S3 from the absence of Word/POS. Verb
Class on its own performs worse than Verb, how-
ever when combined with other features it per-
forms better: leave-Verb-out outperforms leave-
Verb Class-out.
After comparing the various combinations of
features, we found that the best selection of fea-
tures was all but the Verb for all the schemes. Ta-
ble 6 shows the results for the baseline (BL), and
the best results for NB and SVM. NB and SVM per-
form clearly better than BL for all the schemes.
The results for SVM are the best. NB yields the
highest performance with S1. Being sensitive to
sparse data, it does not perform equally well on S2
and S3 which have a higher number of categories,
some of which are low in frequency (see Table 2).
For S1, SVM finds all the four scheme categories
with the accuracy of 89%. F-measure is 90 for
OBJ, RES and CON and 81 for METH. For S2,
the classifier finds six of the seven categories, with
the accuracy of 90% and the average F-measure of
</bodyText>
<table confidence="0.993587307692308">
ALL A B C D E F G H I J K
S1 OBJ .90 .89 .87 .92 .90 .90 .91 .91 .91 .92 .91 .88
METH .80 .81 .80 .80 .79 .81 .79 .80 .80 .80 .81 .81
RES .88 .90 .88 .90 .88 .90 .88 .88 .88 .89 .89 .90
CON .86 .85 .82 .87 .88 .90 .90 .88 .89 .88 .88 .90
S2 BKG .91 .94 .90 .90 .93 .94 .94 .91 .93 .94 .92 .94
OBJ .72 .78 .84 .78 .83 .88 .84 .81 .83 .84 .78 .83
METH .81 .83 .80 .81 .80 .85 .80 .78 .81 .81 .82 .83
RES .88 .90 .88 .89 .88 .91 .89 .89 .90 .90 .90 .89
CON .84 .83 .77 .83 .86 .88 .86 .87 .88 .89 .88 .81
REL _ _ _ _ _ _ _ _ _ _ _ _
FUT - 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0
S3 HYP _ _ _ _ _ _ _ _ _ _ _ _
MOT .82 .84 .80 .76 .82 .82 .83 .78 .83 .83 .83 .83
BKG .59 .60 .60 .54 .67 .62 .62 .59 .61 .61 .62 .61
GOAL .62 .67 .67 .62 .71 .62 .67 .43 .67 .67 .67 .62
OBJT .88 .85 .83 .74 .83 .85 .83 .74 .83 .83 .83 .85
EXP .72 .68 .72 .53 .65 .70 .72 .73 .74 .74 .72 .68
MOD _ _ _ _ _ _ _ _ _ _ _
_
METH .87 .86 .87 .66 .85 .89 .87 .88 .86 .86 .87 .86
OBS .82 .81 .84 .72 .80 .82 .81 .80 .82 .82 .81 .81
RES .87 .87 .88 .74 .87 .86 .87 .86 .87 .87 .87 .88
CON .88 .88 .82 .88 .83 .87 .87 .84 .87 .88 .87 .86
A-K: History, Location, Word, Bi-gram, Verb, Verb Class, POS, GR, Subj,
Obj, Voice
</table>
<bodyText confidence="0.994281944444445">
We have 1.0 for FUT in S2 probably because the size of the training data is
just right, and the model doesn’t overfit the data. We make this assumption
because we have 1.0 for almost all the categories in the training data, but only
for FUT on the test data.
91 for the six categories. As with S2, METH has
the lowest performance (at 85 F-measure). The
one missing category (REL) appears in our abstract
data with very low frequency (see Table 2).
For S3, SVM uncovers as many as nine of the
11 categories with accuracy of 81%. Six cate-
gories perform well, with F-measure higher than
80. EXP, BKG and GOAL have F-measure of 70,
62 and 62, respectively. Like the missing cate-
gories HYP and MOD, GOAL is very low in fre-
quency. The lower performance of the higher fre-
quency EXP and BKG is probably due to low pre-
cision in distinguishing between EXP and METH,
and BKG and other categories, respectively.
</bodyText>
<figure confidence="0.991901515151515">
.25
.76
S2
BL
NB
Table 6: Baseline and best NB and SVM results
Acc. F-Measure
OBJ METH RES CON
.23 .23 .39 .18
.85 .75 .85 .71
.90 .81 .90 .90
S1
BL
NB
.29
.82
SVM .89
Acc. F-Measure
BKG OBJ METH RES CON REL FUT
.13 .08 .22 .40 .13 - -
.79 .25 .70 .83 .66 - -
SVM .90
.94 .88 .85 .91 .88 - 1.0
Acc. F-Measure
HYP MOT BKG GOAL OBJT EXP MODMETH OBS RES CON
- .10 .06 .04 .06 .11 - .13 .24 .15 .17
- .56 - - - .30 - .32 .61 .59 .62
S3
BL
NB
.15
.53
SVM .81 - .82 .62 .62 .85 .70 - .89 .82 .86 .87
</figure>
<page confidence="0.993824">
105
</page>
<sectionHeader confidence="0.989274" genericHeader="discussions">
8 Discussion and conclusions
</sectionHeader>
<bodyText confidence="0.99998053164557">
The results from our corpus annotation (see Ta-
ble 2) show that for the coarse-grained S1, all the
four categories appear frequently in biomedical
abstracts (this is not surprising because S1 was ac-
tually designed for abstracts). All of them can be
identified using machine learning. For S2 and S3,
the majority of categories appear in abstracts with
high enough frequency that we can conclude that
also these two schemes are applicable to abstracts.
For S2 we identified six categories using machine
learning, and for S3 as many as nine, indicating
that automatic identification of the schemes in ab-
stracts is realistic.
Our analysis in section 5 showed that there is
a subsumption relation between the categories of
the schemes. S2 and S3 provide finer-grained in-
formation about the information structure of ab-
stracts than S1, even with their 2-3 low frequency
(or missing) categories. They can be useful for
practical tasks requiring such information. For ex-
ample, considering S3, there may be tasks where
one needs to distinguish between EXP, MOD and
METH, between HYP, MOT and GOAL, or between
OBS and RES.
Ultimately, the optimal scheme will depend on
the level of detail required by the application at
hand. Therefore, in the future, we plan to conduct
task-based evaluation of the schemes in the con-
text of CRA and to evaluate the usefulness of S1-
S3 for tasks cancer risk assessors perform on ab-
stracts (Korhonen et al., 2009). Now that we have
annotated the CRA corpus for S1-S3 and have a
machine learning approach available, we are in an
excellent position to conduct this evaluation.
A key question for real-world tasks is the level
of machine learning performance required. We
plan to investigate this in the context of our task-
based evaluation. Although we employed fairly
standard text classification methodology in our ex-
periments, we obtained high performance for S1
and S2. Due to the higher number of categories
(and less training data for each of them), the over-
all performance was not equally impressive for S3
(although still quite high at 81% accuracy).
Hirohata et al. (2008) have showed that the
amount of training data can have a big impact
on our task. They used c. 50,000 Medline ab-
stracts annotated (by the authors of the Medline
abstracts) as training data for S1. When using a
small set of standard text classification features
and Conditional Random Fields (CRF) (Lafferty
et al., 2001) for classification, they obtained 95.5%
per-sentence accuracy on 1000 abstracts. How-
ever, when only 1000 abstracts were used for train-
ing the accuracy was considerably worse; their re-
ported per-abstract accuracy dropped from 68.8%
to less than 50%. Although it would be difficult to
obtain similarly huge training data for S2 and S3,
this result suggests that one key to improved per-
formance is larger training data, and this is what
we plan to explore especially for S3.
In addition we plan to improve our method. We
showed that our schemes partly overlap and that
similar features and methods tend to perform the
best / worst for each of the schemes. It is therefore
unlikely that considerable scheme specific tuning
will be necessary. However, we plan to develop
our features further and to make better use of the
sequential nature of information structure. Cur-
rently this is only represented as the History fea-
ture, which provides a narrow window view to the
category of the previous sentence. Also we plan to
compare SVM against methods such as CRF and
Maximum Entropy which have proved successful
in recent related works (Hirohata et al., 2008; Mer-
ity et al., 2009). The resulting models will be eval-
uated both directly and in the context of CRA to
provide an indication of their practical usefulness
for real-world tasks.
</bodyText>
<sectionHeader confidence="0.999196" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999729666666667">
The work reported in this paper was funded by the
Royal Society (UK), the Swedish Research Coun-
cil, FAS (Sweden), and JISC (UK) which is fund-
ing the SAPIENT Automation project. YG was
funded by the Cambridge International Scholar-
ship.
</bodyText>
<page confidence="0.99781">
106
</page>
<sectionHeader confidence="0.998342" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999610155555555">
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20:37–46.
J. R. Curran, S. Clark, and J. Bos. 2007. Linguistically
motivated large-scale nlp with c&amp;c and boxer. In
Proceedings of the ACL 2007 Demonstrations Ses-
sion, pages 33–36.
K. Hirohata, N. Okazaki, S. Ananiadou, and
M. Ishizuka. 2008. Identifying sections in scien-
tific abstracts using conditional random fields. In
Proc. of 3rd International Joint Conference on Nat-
ural Language Processing.
A. Korhonen, L. Sun, I. Silins, and U. Stenius. 2009.
The first step in the development of text mining tech-
nology for cancer risk assessment: Identifying and
organizing scientific evidence in risk assessment lit-
erature. BMC Bioinformatics, 10:303.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditionl random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proc. of
ICML.
J. R. Landis and G. G. Koch. 1977. The measurement
of observer agreement for categorical data. Biomet-
rics, 33:159–174.
M. Liakata and L.N. Soldatova. 2008. Guide-
lines for the annotation of general scientific con-
cepts. Aberystwyth University, JISC Project Report
http://ie-repository.jisc.ac.uk/88/.
M. Liakata, Claire Q, and L.N. Soldatova. 2009. Se-
mantic annotation of papers: Interface &amp; enrichment
tool (sapient). In Proceedings of BioNLP-09, pages
193–200, Boulder, Colorado.
M. Liakata, S. Teufel, A. Siddharthan, and C. Batch-
elor. 2010. Corpora for the conceptualisation and
zoning of scientific papers. To appear in the 7th In-
ternational Conference on Language Resources and
Evaluation.
J. Lin, D. Karakos, D. Demner-Fushman, and S. Khu-
danpur. 2006. Generative content models for struc-
tural analysis of medical abstracts. In Proceedings
of BioNLP-06, pages 65–72, New York, USA.
J. Lin. 2009. Is searching full text more effective than
searching abstracts? BMC Bioinformatics, 10:46.
S. Merity, T. Murphy, and J. R. Curran. 2009. Ac-
curate argumentative zoning with maximum entropy
models. In Proceedings of the 2009 Workshop
on Text and Citation Analysis for Scholarly Digital
Libraries, pages 19–26. Association for Computa-
tional Linguistics.
Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier.
2005. Zone analysis in biology articles as a basis
for information extraction. International Journal of
Medical Informatics on Natural Language Process-
ing in Biomedicine and Its Applications.
T. Mullen, Y. Mizuta, and N. Collier. 2005. A baseline
feature set for learning rhetorical zones using full ar-
ticles in the biomedical domain. Natural language
processing and text mining, 7:52–58.
P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geiss-
buhler, P. Fabry, J. Gobeill, V. Pillet, D. Rebholz-
Schuhmann, C. Lovis, and A. L. Veuthey. 2007.
Using argumentation to extract key sentences from
biomedical abstracts. Int J Med Inform, 76:195–
200.
H. Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur.
2008. Multi-dimensional classification of biomed-
ical text: Toward automated, practical provision of
high-utility text to diverse users. Bioinformatics,
18:2086–2093.
S. Siegel and N. J. Jr. Castellan. 1988. Nonparamet-
ric Statistics for the Behavioral Sciences. McGraw-
Hill, Berkeley, CA, 2nd edition.
L. Sun and A. Korhonen. 2009. Improving verb clus-
tering with automatically acquired selectional pref-
erence. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
I. Tbahriti, C. Chichester, Frederique Lisacek, and
P. Ruch. 2006. Using argumentation to retrieve
articles with similar citations. Int J Med Inform,
75:488–495.
S. Teufel and M. Moens. 2002. Summarizing scientific
articles: Experiments with relevance and rhetorical
status. Computational Linguistics, 28:409–445.
S. Teufel, A. Siddharthan, and C. Batchelor. 2009. To-
wards domain-independent argumentative zoning:
Evidence from chemistry and computational linguis-
tics. In Proc. of EMNLP.
I. H. Witten, 2008. Data mining: practical machine
learning tools and techniques with Java Implemen-
tations. http://www.cs.waikato.ac.nz/ml/weka/.
</reference>
<page confidence="0.998659">
107
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.026209">
<title confidence="0.998493">Identifying the Information Structure of Scientific Abstracts: Investigation of Three Different Schemes</title>
<author confidence="0.999948">Yufan Guo</author>
<affiliation confidence="0.997457">of Cambridge,</affiliation>
<email confidence="0.991119">yg244@cam.ac.uk</email>
<title confidence="0.321773666666667">Ilona Silins Institutet, Ilona.Silins@ki.se</title>
<author confidence="0.999937">Anna Korhonen</author>
<affiliation confidence="0.99711">of Cambridge,</affiliation>
<email confidence="0.985567">alk23@cam.ac.uk</email>
<author confidence="0.999979">Lin Sun</author>
<affiliation confidence="0.996435">of Cambridge,</affiliation>
<email confidence="0.983543">ls418@cam.ac.uk</email>
<author confidence="0.585141">Maria</author>
<affiliation confidence="0.963834">University,</affiliation>
<email confidence="0.985903">mal@aber.ac.uk</email>
<author confidence="0.635518">Ulla Stenius</author>
<affiliation confidence="0.855221">Institutet,</affiliation>
<email confidence="0.399589">Ulla.Stenius@ki.se</email>
<abstract confidence="0.999564285714286">Many practical tasks require accessing specific types of information in scientific literature; e.g. information about the objective, methods, results or conclusions of the study in question. Several schemes have been developed to characterize such information in full journal papers. Yet many tasks focus on abstracts instead. We take three schemes of different type and granularity (those based on section names, argumentative zones and conceptual structure of documents) and investigate their applicability to biomedical abstracts. We show that even for the finest-grained of these schemes, the majority of categories appear in abstracts and can be identified relatively reliably using machine learning. We discuss the impact of our results and the need for subsequent task-based evaluation of the schemes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--37</pages>
<contexts>
<context position="11145" citStr="Cohen, 1960" startWordPosition="1758" endWordPosition="1759"> Experiment details A statement about a theoretical model or framework The means by which the authors seek to achieve a goal of the investigation The data/phenomena recorded within an investigation Factual statements about the outputs of an investigation Statements inferred from observations and results, relating to research hypothesis Inter-annotator agreement. We measured the inter-annotator agreement on 300 abstracts (i.e. a third of the corpus) using three annotators (one linguist, one expert in CRA, and the computational linguist who annotated all the corpus). According to Cohen’s Kappa (Cohen, 1960), the interannotator agreement for S1, S2, and S3 was κ = 0.84, κ = 0.85, and κ = 0.50, respectively. According to (Landis and Koch, 1977), the agreement 0.81-1.00 is perfect and 0.41-0.60 is moderate. Our results indicate that S1 and S2 are the easiest schemes for the annotators and S3 the most challenging. This is not surprising as S3 is the scheme with the finest granularity. Its reliable identification may require a longer period of training and possibly improved guidelines. Moreover, previous annotation efforts using S3 have used domain experts for annotation (Liakata et al., 2009, 2010).</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20:37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Curran</author>
<author>S Clark</author>
<author>J Bos</author>
</authors>
<title>Linguistically motivated large-scale nlp with c&amp;c and boxer.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL 2007 Demonstrations Session,</booktitle>
<pages>33--36</pages>
<contexts>
<context position="18789" citStr="Curran et al., 2007" startWordPosition="3148" endWordPosition="3151">b is not the only meaningful verb, we used all the verbs instead. Verb Class. Because individual verbs can result in sparse data problems, we also experimented with a novel feature: verb class (e.g. the class of EXPERIMENT verbs for verbs such as measure and inject). We obtained 60 classes by clustering verbs appearing in full cancer risk assessment articles using the approach of Sun and Korhonen (2009). POS. Tense tends to vary from one category to another, e.g. past is common in RES and past partici103 ple in CON. We used the part-of-speech (POS) tag of each verb assigned by the C&amp;C tagger (Curran et al., 2007) as a feature. GR. Structural information about heads and dependents has proved useful in text classification. We used grammatical relations (GRs) returned by the C&amp;C parser as features. They consist of a named relation, a head and a dependent, and possibly extra parameters depending on the relation involved, e.g. (dobj investigate mouse). We created features for each subject (ncsubj), direct object (dobj), indirect object (iobj) and second object (obj2) relation in the corpus. Subj and Obj. As some GR features may suffer from data sparsity, we collected all the subjects and objects (appearing</context>
<context position="21100" citStr="Curran et al., 2007" startWordPosition="3553" endWordPosition="3556">two parallel hyperplanes each of which separates the data. The parallel hyperplanes can be written as: w·x−b = 1 and w·x−b = −1, and the distance between the two is 2 |�|. The problem reduces to: Minimize |w| Subject to w · xi − b &gt; 1 for xi of one class, and w · xi − b &lt; −1 for xi of the other. 7 Experimental evaluation 7.1 Preprocessing We developed a tokenizer to detect the boundaries of sentences and to perform basic tokenisation, such as separating punctuation from adjacent words e.g. in tricky biomedical terms such as 2- amino-3,8-diethylimidazo[4,5-f]quinoxaline. We used the C&amp;C tools (Curran et al., 2007) for POS tagging, lemmatization and parsing. The lemma output was used for extracting Word, Bi-gram and Verb features. The parser produced GRs for each sentence from which we extracted the GR, Subj, Obj and Voice features. We only considered the GRs relating to verbs. The ”obj” marker in a subject relation indicates a verb in passive voice (e.g. (ncsubj observed 14 difference 5 obj)). To control the number of features we removed the words and GRs with fewer than 2 occurrences and bi-grams with fewer than 5 occurrences, and lemmatized the lexical items for all the features. 7.2 Evaluation metho</context>
</contexts>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>J. R. Curran, S. Clark, and J. Bos. 2007. Linguistically motivated large-scale nlp with c&amp;c and boxer. In Proceedings of the ACL 2007 Demonstrations Session, pages 33–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Hirohata</author>
<author>N Okazaki</author>
<author>S Ananiadou</author>
<author>M Ishizuka</author>
</authors>
<title>Identifying sections in scientific abstracts using conditional random fields.</title>
<date>2008</date>
<booktitle>In Proc. of 3rd International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="2495" citStr="Hirohata et al., 2008" startWordPosition="358" endWordPosition="362"> Therefore classification of abstracts (or full articles) according to the categories of information structure can support both the manual study of scientific literature as well as its automatic analysis, e.g. information extraction, summarization and information retrieval (Teufel and Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for full scientific journal articles which are richer in information </context>
<context position="5158" citStr="Hirohata et al., 2008" startWordPosition="781" endWordPosition="784"> investigation provides an initial idea of the practical usefulness of the schemes for tasks involving abstracts. We discuss the impact of our results and the further task-based evaluation which we intend to conduct in the context of CRA. 2 The three schemes We investigate three different schemes – those based on Section Names (S1), Argumentative Zones (S2) and Core Scientific Concepts (S3): S1: The first scheme differs from the others in the sense that it is actually designed for abstracts. It is based on section names found in some scientific abstracts. We use the 4-way classification from (Hirohata et al., 2008) where abstracts are divided into objective, method, results and conclusions. Table 1 provides a short description of each category for this and other schemes (see also this table for any category abbreviations used in this paper). S2: The second scheme is based on Argumentative Zoning (AZ) of documents. The idea of AZ is to follow the knowledge claims made by authors. Teufel and Moens (2002) introduced AZ and applied it to computational linguistics papers. Mizuta et al. (2005) modified the scheme for biology papers. More recently, Teufel et al. (2009) introduced a refined version of AZ and ap</context>
<context position="17196" citStr="Hirohata et al., 2008" startWordPosition="2866" endWordPosition="2869"> 18 0 6371.0 30 0 8554.7 60 0 Contingency Coeff 0.842 0.837 0.871 Cramer’s V 0.901 0.885 0.725 Figure 2: Pairwise interpretation of categories of one scheme in terms of the categories of the other. 6 Automatic identification of information structure 6.1 Features The first step in automatic identification of information structure is feature extraction. We chose a number of general purpose features suitable for all the three schemes. With the exception of our novel verb class feature, the features are similar to those employed in related works, e.g. (Teufel and Moens, 2002; Mullen et al., 2005; Hirohata et al., 2008): History. There are typical patterns in the information structure, e.g. RES tends to be followed by CON rather than by BKG. Therefore, we used the category assigned to the previous sentence as a feature. Location. Categories tend to appear in typical positions in a document, e.g. BKG occurs often in the beginning and CON at the end of the abstract. We divided each abstract into ten equal parts (1-10), measured by the number of words, and defined the location (of a sentence) feature by the parts where the sentence begins and ends. Word. Like many text classification tasks, we employed all the </context>
<context position="30409" citStr="Hirohata et al. (2008)" startWordPosition="5435" endWordPosition="5438"> for S1-S3 and have a machine learning approach available, we are in an excellent position to conduct this evaluation. A key question for real-world tasks is the level of machine learning performance required. We plan to investigate this in the context of our taskbased evaluation. Although we employed fairly standard text classification methodology in our experiments, we obtained high performance for S1 and S2. Due to the higher number of categories (and less training data for each of them), the overall performance was not equally impressive for S3 (although still quite high at 81% accuracy). Hirohata et al. (2008) have showed that the amount of training data can have a big impact on our task. They used c. 50,000 Medline abstracts annotated (by the authors of the Medline abstracts) as training data for S1. When using a small set of standard text classification features and Conditional Random Fields (CRF) (Lafferty et al., 2001) for classification, they obtained 95.5% per-sentence accuracy on 1000 abstracts. However, when only 1000 abstracts were used for training the accuracy was considerably worse; their reported per-abstract accuracy dropped from 68.8% to less than 50%. Although it would be difficult </context>
</contexts>
<marker>Hirohata, Okazaki, Ananiadou, Ishizuka, 2008</marker>
<rawString>K. Hirohata, N. Okazaki, S. Ananiadou, and M. Ishizuka. 2008. Identifying sections in scientific abstracts using conditional random fields. In Proc. of 3rd International Joint Conference on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Korhonen</author>
<author>L Sun</author>
<author>I Silins</author>
<author>U Stenius</author>
</authors>
<title>The first step in the development of text mining technology for cancer risk assessment: Identifying and organizing scientific evidence in risk assessment literature.</title>
<date>2009</date>
<journal>BMC Bioinformatics,</journal>
<pages>10--303</pages>
<contexts>
<context position="4306" citStr="Korhonen et al., 2009" startWordPosition="642" endWordPosition="645"> names, argumentative zones and conceptual structure of documents – to a collection of biomedical abstracts used for cancer risk assessment (CRA). CRA is an example of a real-world task which could greatly benefit from knowledge about the information structure of abstracts since cancer risk assessors look for a variety of information in them ranging from specific methods to 99 Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 99–107, Uppsala, Sweden, 15 July 2010. c�2010 Association for Computational Linguistics results concerning different chemicals (Korhonen et al., 2009). We report work on the annotation of CRA abstracts according to each scheme and investigate the schemes in terms of their distribution, mutual overlap, and the success of identifying them automatically using machine learning. Our investigation provides an initial idea of the practical usefulness of the schemes for tasks involving abstracts. We discuss the impact of our results and the further task-based evaluation which we intend to conduct in the context of CRA. 2 The three schemes We investigate three different schemes – those based on Section Names (S1), Argumentative Zones (S2) and Core S</context>
<context position="7073" citStr="Korhonen et al., 2009" startWordPosition="1100" endWordPosition="1103">r in the current work. The second layer pertains to properties of the categories (e.g. “advantage” vs. “disadvantage” of METH, “new” vs. “old” METH or OBJT). Such level of granularity is rare in abstracts. The 3rd layer involves coreference identification between the same instances of each category, which is also not of concern in abstracts. With eleven categories, S3 is the most fine-grained of our schemes. CoreSC has been previously applied to chemistry papers (Liakata et al., 2010, 2009). 3 Data: cancer risk assessment abstracts We used as our data the corpus of CRA abstracts described in (Korhonen et al., 2009) which contains MedLine abstracts from different subdomains of biomedicine. The abstracts were selected so that they provide rich information about various scientific data (human, animal and cellular) used for CRA. We selected 1000 abstracts (in random) from this corpus. The resulting data includes 7,985 sentences and 225,785 words in total. 4 Annotation of abstracts Annotation guidelines. We used the guidelines of Liakata for S3 (Liakata and Soldatova, 2008), and developed the guidelines for S1 and S2 (15 pages each). The guidelines define the unit (a sentence) and the categories of annotatio</context>
<context position="29744" citStr="Korhonen et al., 2009" startWordPosition="5325" endWordPosition="5328">racts than S1, even with their 2-3 low frequency (or missing) categories. They can be useful for practical tasks requiring such information. For example, considering S3, there may be tasks where one needs to distinguish between EXP, MOD and METH, between HYP, MOT and GOAL, or between OBS and RES. Ultimately, the optimal scheme will depend on the level of detail required by the application at hand. Therefore, in the future, we plan to conduct task-based evaluation of the schemes in the context of CRA and to evaluate the usefulness of S1- S3 for tasks cancer risk assessors perform on abstracts (Korhonen et al., 2009). Now that we have annotated the CRA corpus for S1-S3 and have a machine learning approach available, we are in an excellent position to conduct this evaluation. A key question for real-world tasks is the level of machine learning performance required. We plan to investigate this in the context of our taskbased evaluation. Although we employed fairly standard text classification methodology in our experiments, we obtained high performance for S1 and S2. Due to the higher number of categories (and less training data for each of them), the overall performance was not equally impressive for S3 (a</context>
</contexts>
<marker>Korhonen, Sun, Silins, Stenius, 2009</marker>
<rawString>A. Korhonen, L. Sun, I. Silins, and U. Stenius. 2009. The first step in the development of text mining technology for cancer risk assessment: Identifying and organizing scientific evidence in risk assessment literature. BMC Bioinformatics, 10:303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditionl random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML.</booktitle>
<contexts>
<context position="30728" citStr="Lafferty et al., 2001" startWordPosition="5490" endWordPosition="5493">text classification methodology in our experiments, we obtained high performance for S1 and S2. Due to the higher number of categories (and less training data for each of them), the overall performance was not equally impressive for S3 (although still quite high at 81% accuracy). Hirohata et al. (2008) have showed that the amount of training data can have a big impact on our task. They used c. 50,000 Medline abstracts annotated (by the authors of the Medline abstracts) as training data for S1. When using a small set of standard text classification features and Conditional Random Fields (CRF) (Lafferty et al., 2001) for classification, they obtained 95.5% per-sentence accuracy on 1000 abstracts. However, when only 1000 abstracts were used for training the accuracy was considerably worse; their reported per-abstract accuracy dropped from 68.8% to less than 50%. Although it would be difficult to obtain similarly huge training data for S2 and S3, this result suggests that one key to improved performance is larger training data, and this is what we plan to explore especially for S3. In addition we plan to improve our method. We showed that our schemes partly overlap and that similar features and methods tend</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditionl random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Landis</author>
<author>G G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<pages>33--159</pages>
<contexts>
<context position="11283" citStr="Landis and Koch, 1977" startWordPosition="1784" endWordPosition="1787">the investigation The data/phenomena recorded within an investigation Factual statements about the outputs of an investigation Statements inferred from observations and results, relating to research hypothesis Inter-annotator agreement. We measured the inter-annotator agreement on 300 abstracts (i.e. a third of the corpus) using three annotators (one linguist, one expert in CRA, and the computational linguist who annotated all the corpus). According to Cohen’s Kappa (Cohen, 1960), the interannotator agreement for S1, S2, and S3 was κ = 0.84, κ = 0.85, and κ = 0.50, respectively. According to (Landis and Koch, 1977), the agreement 0.81-1.00 is perfect and 0.41-0.60 is moderate. Our results indicate that S1 and S2 are the easiest schemes for the annotators and S3 the most challenging. This is not surprising as S3 is the scheme with the finest granularity. Its reliable identification may require a longer period of training and possibly improved guidelines. Moreover, previous annotation efforts using S3 have used domain experts for annotation (Liakata et al., 2009, 2010). In our case the domain expert and the linguist agreed the most on S3 (κ = 0.60). For S1 and S2 the best agreement was between the linguis</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. R. Landis and G. G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33:159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Liakata</author>
<author>L N Soldatova</author>
</authors>
<title>Guidelines for the annotation of general scientific concepts. Aberystwyth University,</title>
<date>2008</date>
<tech>JISC Project Report http://ie-repository.jisc.ac.uk/88/.</tech>
<contexts>
<context position="7536" citStr="Liakata and Soldatova, 2008" startWordPosition="1172" endWordPosition="1175">ry papers (Liakata et al., 2010, 2009). 3 Data: cancer risk assessment abstracts We used as our data the corpus of CRA abstracts described in (Korhonen et al., 2009) which contains MedLine abstracts from different subdomains of biomedicine. The abstracts were selected so that they provide rich information about various scientific data (human, animal and cellular) used for CRA. We selected 1000 abstracts (in random) from this corpus. The resulting data includes 7,985 sentences and 225,785 words in total. 4 Annotation of abstracts Annotation guidelines. We used the guidelines of Liakata for S3 (Liakata and Soldatova, 2008), and developed the guidelines for S1 and S2 (15 pages each). The guidelines define the unit (a sentence) and the categories of annotation and provide advice for conflict resolution (e.g. which categories to prefer when two or several are possible within the same sentence), as well as examples of annotated abstracts. Annotation tool. We modified the annotation tool of Korhonen et al. (2009) so that it could be used to annotate abstracts according to the schemes. This tool was originally developed for the annotation of CRA abstracts according to the scientific evidence they contain. The tool wo</context>
</contexts>
<marker>Liakata, Soldatova, 2008</marker>
<rawString>M. Liakata and L.N. Soldatova. 2008. Guidelines for the annotation of general scientific concepts. Aberystwyth University, JISC Project Report http://ie-repository.jisc.ac.uk/88/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Liakata</author>
<author>Q Claire</author>
<author>L N Soldatova</author>
</authors>
<title>Semantic annotation of papers: Interface &amp; enrichment tool (sapient).</title>
<date>2009</date>
<booktitle>In Proceedings of BioNLP-09,</booktitle>
<pages>193--200</pages>
<location>Boulder, Colorado.</location>
<contexts>
<context position="11737" citStr="Liakata et al., 2009" startWordPosition="1859" endWordPosition="1862"> Cohen’s Kappa (Cohen, 1960), the interannotator agreement for S1, S2, and S3 was κ = 0.84, κ = 0.85, and κ = 0.50, respectively. According to (Landis and Koch, 1977), the agreement 0.81-1.00 is perfect and 0.41-0.60 is moderate. Our results indicate that S1 and S2 are the easiest schemes for the annotators and S3 the most challenging. This is not surprising as S3 is the scheme with the finest granularity. Its reliable identification may require a longer period of training and possibly improved guidelines. Moreover, previous annotation efforts using S3 have used domain experts for annotation (Liakata et al., 2009, 2010). In our case the domain expert and the linguist agreed the most on S3 (κ = 0.60). For S1 and S2 the best agreement was between the linguist and the computational linguist (κ = 0.87 and κ = 0.88, respectively). Table 2: Distribution of sentences in the schemeannotated CRA corpus 5 Comparison of the schemes in terms of annotations The three schemes we have used to annotate abstracts were developed independently and have separate guidelines. Thus, even though they seem to have some categories in common (e.g. METH, RES, CON) this does not necessarily guarantee that the latter cover the sam</context>
</contexts>
<marker>Liakata, Claire, Soldatova, 2009</marker>
<rawString>M. Liakata, Claire Q, and L.N. Soldatova. 2009. Semantic annotation of papers: Interface &amp; enrichment tool (sapient). In Proceedings of BioNLP-09, pages 193–200, Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Liakata</author>
<author>S Teufel</author>
<author>A Siddharthan</author>
<author>C Batchelor</author>
</authors>
<title>Corpora for the conceptualisation and zoning of scientific papers.</title>
<date>2010</date>
<booktitle>the 7th International Conference on Language Resources and Evaluation.</booktitle>
<note>To appear in</note>
<contexts>
<context position="2561" citStr="Liakata et al., 2010" startWordPosition="371" endWordPosition="374">g to the categories of information structure can support both the manual study of scientific literature as well as its automatic analysis, e.g. information extraction, summarization and information retrieval (Teufel and Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for full scientific journal articles which are richer in information and also considered to be more in need of the definition of inform</context>
<context position="6150" citStr="Liakata et al., 2010" startWordPosition="948" endWordPosition="951"> Teufel and Moens (2002) introduced AZ and applied it to computational linguistics papers. Mizuta et al. (2005) modified the scheme for biology papers. More recently, Teufel et al. (2009) introduced a refined version of AZ and applied it to chemistry papers. As these schemes are too finegrained for abstracts (some of the categories do not appear in abstracts at all), we adopt a reduced version of AZ which integrates seven categories from (Teufel and Moens, 2002) and (Mizuta et al., 2005) - those which actually appear in abstracts. S3: The third scheme is concept-driven and ontology-motivated (Liakata et al., 2010). It treats scientific papers as humanly-readable representations of scientific investigations and seeks to retrieve the structure of the investigation from the paper as generic high-level Core Scientific Concepts (CoreSC). The CoreSC is a 3-layer annotation scheme but we only consider the first layer in the current work. The second layer pertains to properties of the categories (e.g. “advantage” vs. “disadvantage” of METH, “new” vs. “old” METH or OBJT). Such level of granularity is rare in abstracts. The 3rd layer involves coreference identification between the same instances of each category</context>
</contexts>
<marker>Liakata, Teufel, Siddharthan, Batchelor, 2010</marker>
<rawString>M. Liakata, S. Teufel, A. Siddharthan, and C. Batchelor. 2010. Corpora for the conceptualisation and zoning of scientific papers. To appear in the 7th International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
<author>D Karakos</author>
<author>D Demner-Fushman</author>
<author>S Khudanpur</author>
</authors>
<title>Generative content models for structural analysis of medical abstracts.</title>
<date>2006</date>
<booktitle>In Proceedings of BioNLP-06,</booktitle>
<pages>65--72</pages>
<location>New York, USA.</location>
<contexts>
<context position="2472" citStr="Lin et al., 2006" startWordPosition="354" endWordPosition="357">of abstracts only. Therefore classification of abstracts (or full articles) according to the categories of information structure can support both the manual study of scientific literature as well as its automatic analysis, e.g. information extraction, summarization and information retrieval (Teufel and Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for full scientific journal articles which are</context>
</contexts>
<marker>Lin, Karakos, Demner-Fushman, Khudanpur, 2006</marker>
<rawString>J. Lin, D. Karakos, D. Demner-Fushman, and S. Khudanpur. 2006. Generative content models for structural analysis of medical abstracts. In Proceedings of BioNLP-06, pages 65–72, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lin</author>
</authors>
<title>Is searching full text more effective than searching abstracts?</title>
<date>2009</date>
<journal>BMC Bioinformatics,</journal>
<pages>10--46</pages>
<contexts>
<context position="3188" citStr="Lin, 2009" startWordPosition="472" endWordPosition="473">emes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for full scientific journal articles which are richer in information and also considered to be more in need of the definition of information structure (Lin, 2009). However, many practical tasks currently focus on abstracts. As a distilled summary of key information in full articles, abstracts may exhibit an entirely different distribution of scheme categories than full articles. For tasks involving abstracts, it would be useful to know which schemes are applicable to abstracts and which can be automatically identified in them with reasonable accuracy. In this paper, we will compare the applicability of three different schemes – those based on section names, argumentative zones and conceptual structure of documents – to a collection of biomedical abstra</context>
</contexts>
<marker>Lin, 2009</marker>
<rawString>J. Lin. 2009. Is searching full text more effective than searching abstracts? BMC Bioinformatics, 10:46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Merity</author>
<author>T Murphy</author>
<author>J R Curran</author>
</authors>
<title>Accurate argumentative zoning with maximum entropy models.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries,</booktitle>
<pages>pages</pages>
<marker>Merity, Murphy, Curran, 2009</marker>
<rawString>S. Merity, T. Murphy, and J. R. Curran. 2009. Accurate argumentative zoning with maximum entropy models. In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, pages 19–26. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Mizuta</author>
<author>A Korhonen</author>
<author>T Mullen</author>
<author>N Collier</author>
</authors>
<title>Zone analysis in biology articles as a basis for information extraction.</title>
<date>2005</date>
<journal>International Journal of Medical Informatics on Natural</journal>
<contexts>
<context position="2192" citStr="Mizuta et al., 2005" startWordPosition="311" endWordPosition="314">Many readers of scientific abstracts are interested in specific types of information only, e.g. the general background of the study, the methods used in the study, or the results obtained. Accordingly, many text mining tasks focus on the extraction of information from certain parts of abstracts only. Therefore classification of abstracts (or full articles) according to the categories of information structure can support both the manual study of scientific literature as well as its automatic analysis, e.g. information extraction, summarization and information retrieval (Teufel and Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentat</context>
<context position="5640" citStr="Mizuta et al. (2005)" startWordPosition="862" endWordPosition="865">r abstracts. It is based on section names found in some scientific abstracts. We use the 4-way classification from (Hirohata et al., 2008) where abstracts are divided into objective, method, results and conclusions. Table 1 provides a short description of each category for this and other schemes (see also this table for any category abbreviations used in this paper). S2: The second scheme is based on Argumentative Zoning (AZ) of documents. The idea of AZ is to follow the knowledge claims made by authors. Teufel and Moens (2002) introduced AZ and applied it to computational linguistics papers. Mizuta et al. (2005) modified the scheme for biology papers. More recently, Teufel et al. (2009) introduced a refined version of AZ and applied it to chemistry papers. As these schemes are too finegrained for abstracts (some of the categories do not appear in abstracts at all), we adopt a reduced version of AZ which integrates seven categories from (Teufel and Moens, 2002) and (Mizuta et al., 2005) - those which actually appear in abstracts. S3: The third scheme is concept-driven and ontology-motivated (Liakata et al., 2010). It treats scientific papers as humanly-readable representations of scientific investigat</context>
</contexts>
<marker>Mizuta, Korhonen, Mullen, Collier, 2005</marker>
<rawString>Y. Mizuta, A. Korhonen, T. Mullen, and N. Collier. 2005. Zone analysis in biology articles as a basis for information extraction. International Journal of Medical Informatics on Natural Language Processing in Biomedicine and Its Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mullen</author>
<author>Y Mizuta</author>
<author>N Collier</author>
</authors>
<title>A baseline feature set for learning rhetorical zones using full articles in the biomedical domain. Natural language processing and text mining,</title>
<date>2005</date>
<pages>7--52</pages>
<contexts>
<context position="17172" citStr="Mullen et al., 2005" startWordPosition="2862" endWordPosition="2865">4 60 0 Pearson 6613.0 18 0 6371.0 30 0 8554.7 60 0 Contingency Coeff 0.842 0.837 0.871 Cramer’s V 0.901 0.885 0.725 Figure 2: Pairwise interpretation of categories of one scheme in terms of the categories of the other. 6 Automatic identification of information structure 6.1 Features The first step in automatic identification of information structure is feature extraction. We chose a number of general purpose features suitable for all the three schemes. With the exception of our novel verb class feature, the features are similar to those employed in related works, e.g. (Teufel and Moens, 2002; Mullen et al., 2005; Hirohata et al., 2008): History. There are typical patterns in the information structure, e.g. RES tends to be followed by CON rather than by BKG. Therefore, we used the category assigned to the previous sentence as a feature. Location. Categories tend to appear in typical positions in a document, e.g. BKG occurs often in the beginning and CON at the end of the abstract. We divided each abstract into ten equal parts (1-10), measured by the number of words, and defined the location (of a sentence) feature by the parts where the sentence begins and ends. Word. Like many text classification tas</context>
</contexts>
<marker>Mullen, Mizuta, Collier, 2005</marker>
<rawString>T. Mullen, Y. Mizuta, and N. Collier. 2005. A baseline feature set for learning rhetorical zones using full articles in the biomedical domain. Natural language processing and text mining, 7:52–58.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ruch</author>
<author>C Boyer</author>
<author>C Chichester</author>
<author>I Tbahriti</author>
<author>A Geissbuhler</author>
<author>P Fabry</author>
<author>J Gobeill</author>
<author>V Pillet</author>
<author>D RebholzSchuhmann</author>
<author>C Lovis</author>
<author>A L Veuthey</author>
</authors>
<title>Using argumentation to extract key sentences from biomedical abstracts.</title>
<date>2007</date>
<journal>Int J Med Inform,</journal>
<volume>76</volume>
<pages>200</pages>
<contexts>
<context position="2235" citStr="Ruch et al., 2007" startWordPosition="319" endWordPosition="322">erested in specific types of information only, e.g. the general background of the study, the methods used in the study, or the results obtained. Accordingly, many text mining tasks focus on the extraction of information from certain parts of abstracts only. Therefore classification of abstracts (or full articles) according to the categories of information structure can support both the manual study of scientific literature as well as its automatic analysis, e.g. information extraction, summarization and information retrieval (Teufel and Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta e</context>
</contexts>
<marker>Ruch, Boyer, Chichester, Tbahriti, Geissbuhler, Fabry, Gobeill, Pillet, RebholzSchuhmann, Lovis, Veuthey, 2007</marker>
<rawString>P. Ruch, C. Boyer, C. Chichester, I. Tbahriti, A. Geissbuhler, P. Fabry, J. Gobeill, V. Pillet, D. RebholzSchuhmann, C. Lovis, and A. L. Veuthey. 2007. Using argumentation to extract key sentences from biomedical abstracts. Int J Med Inform, 76:195– 200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Shatkay</author>
<author>F Pan</author>
<author>A Rzhetsky</author>
<author>W J Wilbur</author>
</authors>
<title>Multi-dimensional classification of biomedical text: Toward automated, practical provision of high-utility text to diverse users.</title>
<date>2008</date>
<journal>Bioinformatics,</journal>
<pages>18--2086</pages>
<contexts>
<context position="2538" citStr="Shatkay et al., 2008" startWordPosition="367" endWordPosition="370">ull articles) according to the categories of information structure can support both the manual study of scientific literature as well as its automatic analysis, e.g. information extraction, summarization and information retrieval (Teufel and Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for full scientific journal articles which are richer in information and also considered to be more in need of t</context>
</contexts>
<marker>Shatkay, Pan, Rzhetsky, Wilbur, 2008</marker>
<rawString>H. Shatkay, F. Pan, A. Rzhetsky, and W. J. Wilbur. 2008. Multi-dimensional classification of biomedical text: Toward automated, practical provision of high-utility text to diverse users. Bioinformatics, 18:2086–2093.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Castellan</author>
</authors>
<title>Nonparametric Statistics for the Behavioral Sciences.</title>
<date>1988</date>
<location>McGrawHill, Berkeley, CA,</location>
<note>2nd edition.</note>
<contexts>
<context position="13693" citStr="Castellan, 1988" startWordPosition="2202" endWordPosition="2203"> 474 805 41 637 744 2582 1049 Sentences 1% 2% 14% 4% 6% 10% 1% 8% 9% 32% 13% Sentences 101 Figure 1: An example of an abstract annotated according to the three schemes S1 lihood ratio, the contingency coefficient and Cramer’s V (Table 3)1, all of which showed a definite correlation between rows and columns for the pairwise comparison of all three schemes. However, none of the above measures give an indication of the differential association between schemes, i.e. whether it goes both directions and to what extent. For this reason we calculated the Goodman-Kruskal lambda L statistic (Siegel and Castellan, 1988), which gives us the reduction in error for predicting the categories of one annotation scheme, if we know the categories assigned according to the other. When using the categories of S1 as the independent variables, we obtained a lambda of over 0.72 which suggests a 72% reduction in error in predicting S2 categories and 47% in 1These are association measures for r x c tables. We used the implementation in the vcd package of R (http://www.rproject.org/). predicting S3 categories. With S2 categories being the independent variables, we obtained a reduction in error of 88% when predicting S1 and </context>
</contexts>
<marker>Castellan, 1988</marker>
<rawString>S. Siegel and N. J. Jr. Castellan. 1988. Nonparametric Statistics for the Behavioral Sciences. McGrawHill, Berkeley, CA, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Sun</author>
<author>A Korhonen</author>
</authors>
<title>Improving verb clustering with automatically acquired selectional preference.</title>
<date>2009</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="18575" citStr="Sun and Korhonen (2009)" startWordPosition="3106" endWordPosition="3109"> of sentences, and can vary from one category to another. For example, experiment is frequent in METH and conclude in CON. Previous works have used the matrix verb of each sentence as a feature. Because the matrix verb is not the only meaningful verb, we used all the verbs instead. Verb Class. Because individual verbs can result in sparse data problems, we also experimented with a novel feature: verb class (e.g. the class of EXPERIMENT verbs for verbs such as measure and inject). We obtained 60 classes by clustering verbs appearing in full cancer risk assessment articles using the approach of Sun and Korhonen (2009). POS. Tense tends to vary from one category to another, e.g. past is common in RES and past partici103 ple in CON. We used the part-of-speech (POS) tag of each verb assigned by the C&amp;C tagger (Curran et al., 2007) as a feature. GR. Structural information about heads and dependents has proved useful in text classification. We used grammatical relations (GRs) returned by the C&amp;C parser as features. They consist of a named relation, a head and a dependent, and possibly extra parameters depending on the relation involved, e.g. (dobj investigate mouse). We created features for each subject (ncsubj</context>
</contexts>
<marker>Sun, Korhonen, 2009</marker>
<rawString>L. Sun and A. Korhonen. 2009. Improving verb clustering with automatically acquired selectional preference. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tbahriti</author>
<author>C Chichester</author>
<author>Frederique Lisacek</author>
<author>P Ruch</author>
</authors>
<title>Using argumentation to retrieve articles with similar citations.</title>
<date>2006</date>
<journal>Int J Med Inform,</journal>
<pages>75--488</pages>
<contexts>
<context position="2215" citStr="Tbahriti et al., 2006" startWordPosition="315" endWordPosition="318">tific abstracts are interested in specific types of information only, e.g. the general background of the study, the methods used in the study, or the results obtained. Accordingly, many text mining tasks focus on the extraction of information from certain parts of abstracts only. Therefore classification of abstracts (or full articles) according to the categories of information structure can support both the manual study of scientific literature as well as its automatic analysis, e.g. information extraction, summarization and information retrieval (Teufel and Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and M</context>
</contexts>
<marker>Tbahriti, Chichester, Lisacek, Ruch, 2006</marker>
<rawString>I. Tbahriti, C. Chichester, Frederique Lisacek, and P. Ruch. 2006. Using argumentation to retrieve articles with similar citations. Int J Med Inform, 75:488–495.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>M Moens</author>
</authors>
<title>Summarizing scientific articles: Experiments with relevance and rhetorical status.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--409</pages>
<contexts>
<context position="2171" citStr="Teufel and Moens, 2002" startWordPosition="307" endWordPosition="310">f the results obtained. Many readers of scientific abstracts are interested in specific types of information only, e.g. the general background of the study, the methods used in the study, or the results obtained. Accordingly, many text mining tasks focus on the extraction of information from certain parts of abstracts only. Therefore classification of abstracts (or full articles) according to the categories of information structure can support both the manual study of scientific literature as well as its automatic analysis, e.g. information extraction, summarization and information retrieval (Teufel and Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and bas</context>
<context position="5553" citStr="Teufel and Moens (2002)" startWordPosition="849" endWordPosition="852"> S1: The first scheme differs from the others in the sense that it is actually designed for abstracts. It is based on section names found in some scientific abstracts. We use the 4-way classification from (Hirohata et al., 2008) where abstracts are divided into objective, method, results and conclusions. Table 1 provides a short description of each category for this and other schemes (see also this table for any category abbreviations used in this paper). S2: The second scheme is based on Argumentative Zoning (AZ) of documents. The idea of AZ is to follow the knowledge claims made by authors. Teufel and Moens (2002) introduced AZ and applied it to computational linguistics papers. Mizuta et al. (2005) modified the scheme for biology papers. More recently, Teufel et al. (2009) introduced a refined version of AZ and applied it to chemistry papers. As these schemes are too finegrained for abstracts (some of the categories do not appear in abstracts at all), we adopt a reduced version of AZ which integrates seven categories from (Teufel and Moens, 2002) and (Mizuta et al., 2005) - those which actually appear in abstracts. S3: The third scheme is concept-driven and ontology-motivated (Liakata et al., 2010). I</context>
<context position="17151" citStr="Teufel and Moens, 2002" startWordPosition="2858" endWordPosition="2861">1 18 0 5363.6 30 0 6293.4 60 0 Pearson 6613.0 18 0 6371.0 30 0 8554.7 60 0 Contingency Coeff 0.842 0.837 0.871 Cramer’s V 0.901 0.885 0.725 Figure 2: Pairwise interpretation of categories of one scheme in terms of the categories of the other. 6 Automatic identification of information structure 6.1 Features The first step in automatic identification of information structure is feature extraction. We chose a number of general purpose features suitable for all the three schemes. With the exception of our novel verb class feature, the features are similar to those employed in related works, e.g. (Teufel and Moens, 2002; Mullen et al., 2005; Hirohata et al., 2008): History. There are typical patterns in the information structure, e.g. RES tends to be followed by CON rather than by BKG. Therefore, we used the category assigned to the previous sentence as a feature. Location. Categories tend to appear in typical positions in a document, e.g. BKG occurs often in the beginning and CON at the end of the abstract. We divided each abstract into ten equal parts (1-10), measured by the number of words, and defined the location (of a sentence) feature by the parts where the sentence begins and ends. Word. Like many te</context>
</contexts>
<marker>Teufel, Moens, 2002</marker>
<rawString>S. Teufel and M. Moens. 2002. Summarizing scientific articles: Experiments with relevance and rhetorical status. Computational Linguistics, 28:409–445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Teufel</author>
<author>A Siddharthan</author>
<author>C Batchelor</author>
</authors>
<title>Towards domain-independent argumentative zoning: Evidence from chemistry and computational linguistics.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<contexts>
<context position="2516" citStr="Teufel et al., 2009" startWordPosition="363" endWordPosition="366">on of abstracts (or full articles) according to the categories of information structure can support both the manual study of scientific literature as well as its automatic analysis, e.g. information extraction, summarization and information retrieval (Teufel and Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for full scientific journal articles which are richer in information and also considered t</context>
<context position="5716" citStr="Teufel et al. (2009)" startWordPosition="875" endWordPosition="878">. We use the 4-way classification from (Hirohata et al., 2008) where abstracts are divided into objective, method, results and conclusions. Table 1 provides a short description of each category for this and other schemes (see also this table for any category abbreviations used in this paper). S2: The second scheme is based on Argumentative Zoning (AZ) of documents. The idea of AZ is to follow the knowledge claims made by authors. Teufel and Moens (2002) introduced AZ and applied it to computational linguistics papers. Mizuta et al. (2005) modified the scheme for biology papers. More recently, Teufel et al. (2009) introduced a refined version of AZ and applied it to chemistry papers. As these schemes are too finegrained for abstracts (some of the categories do not appear in abstracts at all), we adopt a reduced version of AZ which integrates seven categories from (Teufel and Moens, 2002) and (Mizuta et al., 2005) - those which actually appear in abstracts. S3: The third scheme is concept-driven and ontology-motivated (Liakata et al., 2010). It treats scientific papers as humanly-readable representations of scientific investigations and seeks to retrieve the structure of the investigation from the paper</context>
</contexts>
<marker>Teufel, Siddharthan, Batchelor, 2009</marker>
<rawString>S. Teufel, A. Siddharthan, and C. Batchelor. 2009. Towards domain-independent argumentative zoning: Evidence from chemistry and computational linguistics. In Proc. of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I H Witten</author>
</authors>
<title>Data mining: practical machine learning tools and techniques with Java Implementations.</title>
<date>2008</date>
<note>http://www.cs.waikato.ac.nz/ml/weka/.</note>
<contexts>
<context position="21730" citStr="Witten, 2008" startWordPosition="3661" endWordPosition="3662"> lemmatization and parsing. The lemma output was used for extracting Word, Bi-gram and Verb features. The parser produced GRs for each sentence from which we extracted the GR, Subj, Obj and Voice features. We only considered the GRs relating to verbs. The ”obj” marker in a subject relation indicates a verb in passive voice (e.g. (ncsubj observed 14 difference 5 obj)). To control the number of features we removed the words and GRs with fewer than 2 occurrences and bi-grams with fewer than 5 occurrences, and lemmatized the lexical items for all the features. 7.2 Evaluation methods We used Weka (Witten, 2008) for the classification, employing its NB and SVM linear kernel. The results were measured in terms of accuracy (the percentage of correctly classified sentences), precision, recall, and F-Measure. We used 10-fold cross validation to avoid the possible bias introduced by relying on any one particular split of the data. The data were randomly divided into ten parts of approximately the same size. Each individual part was retained as test data and the remaining nine parts were used as training data. The process was repeated ten times with each part used once as the test data. The resulting ten e</context>
</contexts>
<marker>Witten, 2008</marker>
<rawString>I. H. Witten, 2008. Data mining: practical machine learning tools and techniques with Java Implementations. http://www.cs.waikato.ac.nz/ml/weka/.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>