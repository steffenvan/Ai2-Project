<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000137">
<title confidence="0.997443">
An Architecture for Data-to-Text systems
</title>
<author confidence="0.980867">
Ehud Reiter
</author>
<affiliation confidence="0.7336355">
University of Aberdeen
Aberdeen, UK
</affiliation>
<email confidence="0.545787">
ereiter@csd. abdn . ac . uk
</email>
<sectionHeader confidence="0.988157" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999202230769231">
I present an architecture for data-to-text systems,
that is NLG systems which produce texts from
non-linguistic input data; this essentially extends
the architecture of Reiter and Dale (2000) to ys-
tems1 whose input is raw data instead of AT knowl-
edge bases. This larchitecture is being used in the
BABYTALK project, and is based on experiences in
several projects at Aberdeen; it also seems to b
compatible with many data-to-text systems devel-
oped elsewhere. It consists of four stages which ar
organised in a pipeline: Signal Analysis, Data Inter-
pretation, Document Planning, and Microplannin
and Realisation.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.939256263157895">
Data-to-text systems are Natural Language Genera-
tion (NLG) system Which generate texts from non-
linguistic input data, such as sensor data and event
logs. Such systems need to perform data analysis as
well as linguistic processing. In this paper I present
a 4-stage pipeline architecture which I believe is
suitable for many data-to-text systems. It is being
used in a new project at Aberdeen, BABYTALK,
and is partially based on our experiences with other
data-to-text systems developed at Aberdeen.
Very briefly, the four stages are
11 tgna na ysis: Analysing numerical iñd
other input data looking for patterns nd
trends.
2 DilInterpretation: Identifying more com-
plex (and domain-specific) messages from the
patterns iñd trends detected in Signal Analy-
sis ã1ô identifying causal and other relations
between messages.
</bodyText>
<listItem confidence="0.943091714285714">
3. Document planning: peciding which of the
above messages should be mentioned in the
generated text, and creating a document and
rhetorical structure around these messages.
4. Microplanning and Realisation: Creating an
actual text which communicates the document
plan.
</listItem>
<bodyText confidence="0.999542875">
In the rest of this paper, I describe the above
stages. I then explain why I think this architec-
ture is an appropriate one for data-to-text systems;
look at how well it fits existing data-to-text sys-
tems; discuss intermediate representations between
the stages; and briefly describe software resources
which we are (slowly) developing to support the ar-
lchitecture.
</bodyText>
<sectionHeader confidence="0.941491" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.787146">
2.1 Data-to-text
</subsectionHeader>
<bodyText confidence="0.999597111111111">
Data-to-text systems generate texts from non-
linguistic input data, which is typically numeri-
cal. For example, S cmTimE (Reiter et al., 2005)
and FOG (Goldberg et al., 1994) generate textual
weather forecasts from numerical weather predic-
tion data; ANA (Kukich, 1983) generates textual
stock market reports from numerical stock market
data; and VITRA (Herzog and Wazinski, 1994) and
DESCRIBER (Roy, 2002) generate natural descrip-
tions of visual scenes. Some work has also been
done on generating texts from lists of events; for
example PLANDoc (McKeown et al., 1994) gen-
erates summaries based on trace files from a simula-
tor,l land Hallett and Scott (2005) describe a system
Which generates ummaries of events in a medical
iecord.
Perhaps -t-hbiggest difference between data-to-I
text systems and NLG ystems whose input is a
</bodyText>
<page confidence="0.999213">
97
</page>
<bodyText confidence="0.999728444444445">
knowledge base is that data-to-text Systems Imist
analyse and interpret their input data as well as de-
cide ho mA td inguistically comraimicate it. While
there is of course a substantial literature on data
analysis, thisl primarily focuses on hypothesiS test-
ing and data mining; there are difference § betweenl
this kind of data analysis and data analysis for the
purposes of generating la textual summary (Sripada
et al., 2003)
</bodyText>
<subsectionHeader confidence="0.969979">
2.2 NLG Architectures
</subsectionHeader>
<bodyText confidence="0.999983423076923">
Perhaps the best-known larchitecture for general
NLG system S is the three-stage Pipeline inodel
(Reiter and pale, 2000), Which divides NLG into
document planning microplanning, and realisation
stages This larchitecture focused on NLG systems
whose input was a knowledge base. The architec-
ture proposed here essentially extends the Reiter
and Dale architecture by adding two new stages,
Signal AnalysiS and Data Interpretation, which are
placed before document planning in the pipeline;
this extension allows the inputs to the system to be
data instead of (or in addition to) knowledge
The RAGS (Mellish et al., 2006) project con-
ducted a survey of NLG systems, and concluded that
they were architecturally quite diverse in terms of
how they were modularised; it proposed a fairly
abstract NLG architecture which described different
representations which might be used in an NLG sys-
tem, but did not lcommit to specific Stages or mod-
ules1
The only proposaf for an larchitecturel which is
specifically for data-to-text systems (which I am
aware of) is Yu et al. (2004). The architecture given
in that paper is quite detailed, and based on one sys-
tem; thel architecture presented here, in contrast is
higher-level and applies to many systems
</bodyText>
<sectionHeader confidence="0.972339" genericHeader="method">
3 The Architecture
</sectionHeader>
<bodyText confidence="0.999786153846154">
lh this section I butline my proposed architecture,
which isl summarised in Figures 1 and 2. Note that
some data-to-text systems only have some bf the
stages Figure 2 explains when a stage iS needed in
a system
I also explain how thiS architecture is being used
in BABYTALK (Portet et al. 2007). BABYTALK is
a new project at Aberdeen whose goal is to generate
summaries of medical data about babies in a neona-
tal intensive care unit. BABYTALK ystems have
two types of inputs: 1) numerical sensor data about
the 157I53 recording information ii1E as hé1i1 rate,
blood pressure and temperature; land (2) record § of
</bodyText>
<figure confidence="0.966049777777778">
Messages,
Relations
Document Planning
Selected msgs,
doc/rhet struct
•
Microplanning and
Realisation
Text
</figure>
<figureCaption confidence="0.999464">
[Figure 1: The Architecture
</figureCaption>
<bodyText confidence="0.999694782608696">
medical actions and observations such as medica-
tion administered td the baby and blood test re-
sults1 Different BABYTALK systems generate dif-
ferent kinds bf texts from thisl input data; here I
vill focusl on thel BABYTALK BT45 system, which
generate S summariesl df about 45 Minutes Worth df
medical data which are intended to help doctors and
nurses make appropriate treatment decisions (Law
et al., 2005).
Figures 3 and 4 Ishowl example S of numerid and
levent inputs to BT45; Figure 5 shows the text that a
doctor wrote to describe this data. This is an extract
from a corpus text, not a generated text; it illustrates
the type of text BT45 is trying to generate
BABYTALK is a multi-person project, involving
People with diverse backgrounds, ranging from sigH
nal analysis to computational linguistics. From thisl
perspective, the larchitecture need sl to nottd117bi
intellectually sensible, it alsd needs t5inodularise
the system in a way which allows people to develop
modules in their area of expertise without needing to
become experts in all of the research areas involved
in BABYTALK.
</bodyText>
<figure confidence="0.998361714285714">
Event
Input Data
Numeric Input Data
Signal Analysis
Patterns
•
Data Interpretation
</figure>
<page confidence="0.998692">
98
</page>
<table confidence="0.9936358">
Stage input output needed if . . .
Signal Analysis numeric datal discrete patterns in data there is
numeric input data
Data Interpretation basic patterns higher-level messages, text communicates
land events relations between messages
more than basic patterns
Document Planning messages messages to be mentioned, only some messages are
relations document and rhetorical struct mentioned in text
Microplanning and irnessages text users want fluent texts
Realisation structure
</table>
<figureCaption confidence="0.993771">
Figure 2: Stages in the Architecture
</figureCaption>
<figure confidence="0.999523166666667">
200.0
HR
0.0
SO
38.0
TC.
TP
32.0
40.0
8N1
20.0
03 Jan 01 o:38 10:39 10:40
</figure>
<figureCaption confidence="0.999939">
Figure 3: Example of BT45 numeric input data
</figureCaption>
<table confidence="0.9911386">
time action parameters
10.38 Fi02 changed new-value = 35%
10.39 morphine given amt=5Oug, route=IV
10.41 incubator opened agent = doctor
10.51 baby intubated agent = doctor
</table>
<figureCaption confidence="0.985217">
Figure 4 Example of BT45 levent input data
</figureCaption>
<subsectionHeader confidence="0.973002">
3.1 Signal Analysis
</subsectionHeader>
<bodyText confidence="0.9999899">
Thel first stag in the architecture is td tr ö detect
basic patterns in the numerical input data J Patterns
are often organised into a taxonomy or ontology, al-
though this is not required by the architecture J From
In preparation for re-intubation, a bolusl of 5Oug
of morphine is given at 1039 when the Fi02 =
35%J There is a momentary bradycardia and then
thel mean BP increase § to 40. The bats go down to
79 land take 2 Mins td come back up. The toe/core
temperature gap increases to 1.6 degrees.
</bodyText>
<figureCaption confidence="0.988455">
Figure 5: Extract from BT45 porpus text
</figureCaption>
<bodyText confidence="0.999974586206896">
a high-level perspective, the goal of ignal analyH
sis is to replace numerical data by a set of discrete
patterns; this lallows the remainder of thel ystem to
reason symbolically instead of numerically.
Signa ll analysis inust lalso distinguish &apos;real&apos; data
from noise. or example, RR (Pianesi et al., 2007),
which summarises the behaviour of participants in a
meeting inust analyse audio ignals and determine
when a participant is talking (and hence contribut-I
ing o thel meeting) and when he or she is making
non-communicative noises, such as coughingJ
Input data which is already structured as discrete
levents, uch as iog files or records of medical acH
tions, can bypass signal analysis. If all input data
is of this form (as in PLANDoc (McKeown et
1994), for example), then the data-to-text system
does not need to perform any signal analysis.
In my experience, signal analysis in data-to-text
systems can usually be done with existing signal
analysis algorithms. There is a substantial litera-
ture on signal analysis and pattern detection, includ-
ing specialist journals such as Pattern Recognition
Quite sophisticated algorithms can be used in data
to-text systems; for example TREND (Boyd, 1998)
used wavelet analysis. The basic challenge for the
system developer is to understand the algorithms,
the domain, and the way humans talk about the do-
main well enough to identify Which algorithms are
appropriate for a particular system.
</bodyText>
<figure confidence="0.93827">
100.0
60.0
</figure>
<page confidence="0.962608">
99
</page>
<bodyText confidence="0.878224">
BT45&apos;s1 signa1 analysiS ifiodule uses three algo-
rithms to detect patterns:
</bodyText>
<listItem confidence="0.993649111111111">
• Short-term changes in channels, such as spikes
and steps, are detected using a simplified ver-
sion of the algorithm proposed by Yu et al.
(2007).
• Longer-term ehanges in the data, such as sen-
sor values increasing or decreasing over time,
are detected using linear segmentation (Keogh
et al., 2001).
• artefacts that is data which is eorrupted or
</listItem>
<bodyText confidence="0.892219833333333">
otherwise should be ignored (for example be-
cause a sensor has fallen Off the baby) are de-
tected as described by Portet et al. (2007)
To take a concrete example, one pattern type in
BT45 is SPIKE; this indicates that the values in
sensor channel have temporarily increased or de-
creased, but then reverted to their previous value.
Thel SPIKE Iclas sl has several parameters, including
channel, direction time and lextremeValue. One
particular spike which is detected in the Figure 3
data ha sl parameter S channe1=HR (Heart Rate) di-
rection=Down, time=1039, extremeValue=90
</bodyText>
<subsectionHeader confidence="0.998337">
3.2 Data Interpretation
</subsectionHeader>
<bodyText confidence="0.979891870967742">
The second stage of the architecture analyses the
patterns detected by signal analysis, and also any
events which are directly specified in the input data,
and infers more complex (and domain-specific)
messages about the data set from these pattern S land
events. It may lalso infer relationships (such as
causality) between patterns, events, and messages
From a high-level perspective, the goal of data inter-
pretation is to map basic patterns and events into the
messages and relationships that human sl use when
discussing this domain
In some applications, such as inarine weather
forecasts (Reiter a a—E, 2005), humans only idd
to basic pattern S when discussing the domain, iñd
do not refer W. relationships. Systems in such do-
mains dd not d to perform data interpretation
1311i in other domains, humans al;) eommumcate
about higher-level events or patterns. For exam-
ple, SCUBATEXT (Sripada and O&apos;a2., 2007), Which
generates safety-oriented summaries of scuba dives
from l depth profile S land other dive computer data,
looks for patterns that suggest potentially dangerous
activitiesl in a dive. For instance, one Eik activity
is a &apos;sawtooth&apos; di W where the diver descends as-
cends, and then descends again. SCUBATEXT cre-
ales sawtooth messages wherever it see S an increas-
ing trend in depth, followed by a decreasing trend in
depth, followed by another increasing trend; this is
an example of data interpretation.
BT45 performs three basic activities in data inter
pretati on:
</bodyText>
<listItem confidence="0.997358923076923">
• Create messages: This is similar td SCUBA-
TEXT. For example, a BRADYCARDIA (heart
rate is temporarily too low) message is created
from downward Spikes in heart rate Which go
belowl 1001
• Decide howl important event si are: BT45 as
Signs an importance to every message; this is
needed by the document planner. For example,
the importance of a BRADYCARDIA depends
primarily on how long it lasts for (5 seconds is
unimportant, 5 minutes is very important), an
secondarily on how low heart rate goes.
• Detect relationship s1 between events: BT45
</listItem>
<bodyText confidence="0.843210047619048">
looks for three kinds of relationships between
messages: eausality (for example, blood oxy-
gen increases because the nurse increased oxyH
gen levels in the ventilator); part of a procedure
(for example, giving morphine is part of a re-
intubation procedure); and other (for example,
an increase in blood oxygen is associated with
a decrease in blood CO2).
Currently, data interpretation in [urr45 is primarily
done by production rules Written in JESSI, which
are based on knowledge-acquisition activities con-
ducted with experienced doctors. Wel may in the
future use KBTA (Shahar, 1997) for some bf thisl
reasoning.
To take a Iconcrete example in BT45 the SPIKE
event Mentioned at the end Of the Signal y-
section is interpreted in Data Interpretation as a
BRADYCARDIA. It has moderate importance (15 on
a scale of 0 to 100), land has an associated with link
td a drop in blood oxygen saturation (SO) which
happens lat about the same time.
</bodyText>
<subsectionHeader confidence="0.997901">
3.3 Document Planning
</subsectionHeader>
<bodyText confidence="0.9818742">
The third stage bf the architecture decide § which
events to mention in the text, and also bn the text&apos;s1
rhetorical and document (e.g., paragraph break)
structure. This is the same as thel Document Plan-
MT* stage in -t-harchitecture a Reiter iñd Dale
</bodyText>
<footnote confidence="0.421623">
http://herzberg.ca.sandia.gov/jess/
</footnote>
<page confidence="0.958755">
100
</page>
<bodyText confidence="0.999791819444445">
(2000)J From a high-level perspective, signal analy-
sis and data interpretation can produce a large num-
bei J of messages patterns, and event S (often hun-
dreds or thousands), but texts lusually are limited
to only describing a small number of message (the
actual number depends on the genre, but often is
between 5 and 25 events) Th 6 Document Planner
must decide which messages are actually communi-
cated in thel text; thisl decision is based on the do-
main and genre It must lalso try td Icommunicate
howl thel message mentioned in the text relate to
each other; thisl can partially be done using rhetori-
cal and document structureJ
Ir i some data-to-text applications, such as pollen
forecasts (Turner et al., 2006) all thel input data is
communicated to the user, so document planning is
a trivial taskJ But thisl is Unusual, lusually some se-
lection isl needed. Indeed, La wl (2005), who
found that doctors Irnade better decisions from l tex-
tual summaries than from graphical displays, spec-
ulate that this happened precisely because the texts
only communicated the most relevant information.
Document planning is perhaps the least-
understood aspect of data-text systems, and indeed
of NLG in general; in fact Evans et al. (2002) argue
that document planning should not be considered
part of NLG. Of course document planning still
needs to be done in data-to-text systems regardless
of whether it is regarded as an &apos;NLG&apos; task.
Yu et al. (2007) treat document planning in data-
to-text systems as the task of finding &apos;interesting
patterns&apos;, and use two mechanisms to do this: rules
acquired from domain experts, and novelty (how of-
ten a pattern has been seen before). They use simple
schemas to specify document structure.
Hallett and Scott (2005), who summarise medi-
cal events, use a different approach. The input
their document planner i graph of events and re-
lations between events. [Their system works by par-
titioning this graph into inter-connected clusters of
events; dropping small clusters (unless they contain
information which domain knowledge says must be
reported); and then mapping clusters Onto a spe=
cific report spine for the target type of report, which
specifies which events are central for this type of re-
port Non-spinal Non-spinal events are linked to spinal events
using rhetorical relations Which are based on event
relations; they may be dropped if they are too fr
away from a spinal event in the graph.
Document planning in BT45 E currently done in
a similar fashion t5 Hallett and Scott, except that
many decisions take into account the importance
of messages (calculated lbyl Data Interpretation).
BT45&apos;s document planner decides l on paragraph
boundaries, but not Sentence boundaries (sentence
boundaries are chosen by the microplanner/realiser,
as part of aggregation).
For example, the BRADYCARDIA message men-1
tionedl in the last section is included in a cluster
of messages which happen at about the same time
(these essentially are the messages mentioned in the
text of Figure 5) This cluster is overall the second-
most important message cluster, so it is included in
the text; and the BRADYCARDIA message is moder-I
ately important so it is included in the text describ-I
ing the cluster1 The cluster is realised at the doc-
ument level as a single paragraph (this decision is
based on its size). A simple SEQUENCE rhetorical
Irelation is used to relate thel BRADYCARDIA mes-
sage to other messages, because it only has generic
Iassociated with link sl to other messages in its Iclus-I
ter.
</bodyText>
<sectionHeader confidence="0.745652" genericHeader="method">
.4 Microplanning and Realisation
</sectionHeader>
<bodyText confidence="0.999940586206897">
The fourth stage of the architecture generates ac-
ual texts based on the content and structure cho-
sen by Document Planning. It corresponds to the
wo stages of Microplanning and Realisation in Re-
*ter and Dale (2000). From a high-level perspective,
microplanning and realisation must decide how to
actually express in language the concepts and struc-
ure selected by earlier stages.
It is possible to perform microplanning and real-
isation using simple templates. Whether this is ac-
ceptable depends on what is appropriate for users.
or example, IGRAPH (Ferres et al., 2006) uses tem-
I° lates to generate (spoken) descriptions of graphs
for visually-impaired users] 1GRAPH&apos;s textS look
Clumsy and repetitive on paper, and probably would
not be acceptable as textual Summaries for people
Who can read. But visually impaired users listening
to a speech synthesiser have different requirements,
and in some cases may prefer simple repetitive texts.
Microplanning and realisation in data-to-text are
iiiI3 similai td microplanning iñd realisation in
other NLG Systems, so I Will not describe them in
detail here. There are a few differences in empha-1
sis, though.
One difference is that most (although not all)
data-to-text system produce Simple language from
a syntactic perspective, because their users prefer
thisl Syntactic realisation in iii systems E ithi
tively straightforward.
</bodyText>
<page confidence="0.997841">
101
</page>
<bodyText confidence="0.999961">
Another difference is that data-to-text Systems
must deal with difference § in how reader § interpret
words that communicate data. For example, Reiter
et_aLI (2005) found considerable differences in how
different readers and writers of weather forecasts in-
terpreted time phrases such as by late evening land
later and Roy (2002) found inconsistencies l in the
way that different people mapped colour terms such
as pink intO numerical RGB coloui pecifications.
There is no clear solution to this problem, it is a maH
jor open research issue.
Finally, data-to-text system may need to commu-
nicate uncertainty about about the reliability of the
input data or the system&apos;s analysis. Again this is an
open research issue. There is a substantial litera-
ture in linguistic S and psychology on Icommunicat-
ing uncertainty, but I am not aware bf attemptS
incorporate these findings into data-to-text systems
The current Version of BT45 lexicalises the
BRADYCARDIA message mentioned above as There
is a bradycardia to 90; ie, it uses a there-iS sentence
and mentions the extreme value but not the duration1
The corpus text shown in Figure 5, in contrast, uses
the phrase There is a momentary bradycardia&apos;mo-
mentary l 4 qualitative description of the duration
of the bradycardia; determining when it can be used
is not easy, in part because different people use it in
different ways.
</bodyText>
<sectionHeader confidence="0.99799" genericHeader="method">
4 Justification for Architecture
</sectionHeader>
<bodyText confidence="0.999539136363636">
The architecture presented here divide S the data-to-
text generation process into 4 stages. Obviously
we could form an architecture lwith more stages
by splitting the stages Presented here into Smaller
stage S (foil example split Microplanning and Re-
alisation into twO separate stages) we could also
form l an architecture with fewer tages by combin-
nig stage S (for example, combine Signal Analysis
and Data Interpretation into one stage). The stages
of thel architecture described here are based on the
following criteria.
Type si of processing performed and knowledge
required Signa analysis uses numerical pattern-
recognition algorithms and is t5 some degree do-
main independent; data interpretation uses sym-
Mi reasoning and relies on domain knowledge;
document Planning uses Symbolic reasoning land
relies on domain communication knowledge (Kit-
tredge et al. 1991); and microplanning
sation are based on linguistic reasoning and are par-
tially domain-independent. This consideration is es-
pecially important in projectS such as BABYTALK
which involve developers from diverse backgrounds
(as mentioned above).
Intermediate representations. Modules needl
dear API&apos;s which will remain fairly stable even if
a module is re-implemented using different tech
niques One of thel Primary reasons foil Icombin-
ing Microplanning and Realisation into one stage iS
that it is difficult to define a generic API between
a Microplanner and a Realiser because the inputS
expected by a Realiser depend on the Syntactic for-
malism it is based on.
Perhaps the hardest decision to justify is the sep-
aration of Data Interpretation and Document Plan-
ning, as these both involve domain-dependent sym-
bolic AT reasoning, and they tend td eitheil both
be present or both be absent in individual sys-
tems. My inain reason foil Separating these is that
they emerge from two very different research com-I
munities Data Interpretation has been studies by
researchers in knowledge-based (expert) Systems,
While Document planning has been studied by re-
searchers in the NLG community.
</bodyText>
<sectionHeader confidence="0.985989" genericHeader="method">
5 Applicability
</sectionHeader>
<bodyText confidence="0.999849346153846">
Perhaps not surprisingly, the architecture described
here fits many data-to-text systems developed at Ab-
erdeen by the author and his colleagues, includ-
ing SUMTIME (Reiter et al., 2005) (generates ma-
rine weather forecasts for offshore oil rigs); pollen
forecast generator (Turner et al., 2006); SUMTIME-
TURBINE (Yu et al., 2007) (generates Summaries Of
sensor data from a gas turbine); land SCUBATEXT
(Sripada and Gao 2007) (generate afety-oriented
summaries of scuba dives) It also seems to fit many
data-to-text systems developed elsewhere.
For example, ANA (Kuloch 1983), which gener-
a7t Stock market Summaries, is described as hav-I
ing 4 modules in a pipeline: fact generator, mes-
sage generator, discourse organiser, and text gener-
ator. These correspond WI the ignal analysis, dYta
interpretation, document planning, land microplan-
ning/realisation modules described here.
Anotheil example is PLANDoc (McKeown
al., 1994), which generates summaries of what hapH
ened in a simulation. li is described as having 5
modules Message Generator, Ontologiser, Content
Planner, Lexicaliser, and Surface Generator. The
Message Generator is essentially an interface to the
simulation package the Ontologiser performs dYta
interpretation; thel Content Planner does document
</bodyText>
<page confidence="0.99754">
102
</page>
<bodyText confidence="0.999969222222222">
planning and the Lexicaliser and Surface Genera-
tor perform microplanning and realisation (respec-
tively). As mentioned above, PLANDoc does not
need to perform signal analysis, because its input is
already in the form of discrete events
However, thel architecture described here does
not fit all data-to-text systems. For example DE-
SCRIBER (Roy, 2002) takes a more integrated ap-
proach based on machine learning
</bodyText>
<sectionHeader confidence="0.996086" genericHeader="method">
6 Representations
</sectionHeader>
<bodyText confidence="0.999625272727273">
The above description of the architecture is of
course very high-level. To make it more con-
crete, we need to specify intermediate representa-
tions (APIs) between the modules. As Mellish et al.
(2006) point out, it is difficult to do this for NLG as
a whole, because the field is very diverse. However,
we believe this problem is more tractable (although
still hard) in the more limited area of data-to-text.
In particular, we use the following intermediate
representations in BT45, and believe these could be
used in other systems as well:
</bodyText>
<listItem confidence="0.971381">
• output of Signal Analysis is a set of pat-
terns. Patterns are represented as objects in
Protege2 ontology, which includes types such
as SPIKE and INCREASING TREND. Objects
have parameters (feature values), such as the
channel they occurred in and the time they oc-
curred at.
• output of Data Interpretation is a set of mes-
sages which are again represented as objects in
a Protégé ontology; all messages have an im-
portance parameter. Data Interpretation also
produce S a Set of relations between messages;
these are represented as (RelationType, Mes-
sagel, Message2) triples
• output of Document Planning is a document
</listItem>
<bodyText confidence="0.908045083333333">
plan. This is represented as a tree. Nodes in
the tree can specify messages; they can also be
annotated with document structure level (e.g.,
paragraph). Parent-child links can be anno-
tated with rhetorical relations (which are finer-
grained than the domain relations produced by
Data Interpretation e.g., [you) IONAL-CAUS
instead of CAUSES). Ii RAGS (Mellish et a—E,
2006) terminology, a document pia&apos;) E a tree
which represents both document and rhetorical
structure and whose nodes can specify con-
ceptual structure S De, messages)
</bodyText>
<listItem confidence="0.58972">
• output of Microplanning and Realisation is a
text, which may include HTML mark-ups.
</listItem>
<sectionHeader confidence="0.977483" genericHeader="evaluation">
7 Software
</sectionHeader>
<bodyText confidence="0.9921366">
An architecture should lalso be supported by Soft-
ware resources Which help developers create sys-
tems based on the architecture. We are trying to creH
ate such resources for building data-to-text systems.
1n particular
</bodyText>
<listItem confidence="0.9607234">
• Signal Analysis: The TSNET (Hunter, 2006)
system, which has been developed at Aberdeen
over a number of years, includes many signal
analysis algorithms for time series data. We
are adapting TSNET so that it can be used to
perform Signal Analysis in BABYTALK, and
believe it could be used in other data-to-text
systems as well.
• Microplanning and Realisation: We are devel-
oping a Java library called SIMPLENLG3 to per-
</listItem>
<bodyText confidence="0.994343222222222">
form these tasks; again this is based on several
previous projects. SIMPLENLG currently per-
forms morphological processing, realisation of
simple syntactic structures, and simple lexical-
isation; we hope to add support for simple mi-
croplanning soon.
We hope over the next few years to expand the
above, and in particular also provide software tools
for Data Interpretation and Document Planning.
</bodyText>
<sectionHeader confidence="0.997692" genericHeader="conclusions">
8 Conclusioti
</sectionHeader>
<bodyText confidence="0.9999685">
There is growing interest in data-to-text Systems.
Such systems are both practically useful (all fielded
NLG systems that I am aware of are data-to-text sysH
tems) and scientifically interesting (in part because
they help uS study how language relates to the nonH
linguistic world). I have tried in this paper to out-
line the kinds of processing that data-to-text systems
must perform, and show how this processing can fit
into an architecture which is an extension of (not a
replacement of) existing NLG architectures.
</bodyText>
<sectionHeader confidence="0.978654" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.995880333333333">
Many thanks to the reviewers and to my colleagueS
a Aberdeen for their comments on this paper All
opinions expressed here are bf course mine lalone.
BABYTALK E supported by grant EP/D049520/1
from the UK Engineering and Physical Sciences Re-
search Council (EPSRC).
</bodyText>
<footnote confidence="0.879884">
2 http://protege.stanford.e0 3http://www.csd.abdn.ac.uk/,--ereiter/simp1en1g/
</footnote>
<page confidence="0.998443">
103
</page>
<sectionHeader confidence="0.900657" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.97812301980198">
Sarah Boyd. 1998. TREND: a system for gener-
ating intelligent descriptions of time-series data.
In Proceedings of the IEEE International Confer-
ence on Intelligent Processing Systems (ICIPS-
1998).
Roger Evans, Paul Piwek, and Lynne Cahill. 2002.]
What is NLG? In Proceedings of the Second
International Conference on Natural Language
Generation, page 144-151.
Leo Ferres, Avi Parush, Shelley Roberts, and Gitte
Lindgaard. 2006. Helping people with visual im-
pairments gain access to graphical informationl
through natural 1anguage: The iGraph system.
In Proceedingslof the 10th International Confer-
ence on Computers Helping People with Special
Needs.
Eli Goldberg l Norbert Driedger, land Richard Kit-I
tredgel 1994. Using natural-language process-
ing td produce Weather forecasts. IEEE Expert,
9(2):45-53.
Catalina Hallett land Donia Scott. 2005. Struc-
tural variation in generated health reports. In
Third International Workshop on Paraphrasing
(IWP2005).
Gerd Herzog land Peter Wazinski. 1994. Visual
TRAnslator: Linking perceptions and natural lan-
guage] descriptions. Artificial Intelligenc0Re
view, 8(2-3): 175-1871
Jim Hunter1 2006. TSNet a distributed architec-
ture foil time series analysis. In Proceeding si of
IDA MAP 2006, pages 85-92.
Eamonn Keogh, Selina Chu, David Hart, land
Michael Pazzani1 2001. An online algorithm for
segmenting time series. In Proceedings of IEEE
International Conferenc0 on Data Mining pages
289-296.
Richard Kittredge, Tanya Korelsky, and Owen Ram-
19911 On rhd need f4 domain commu-
nication language l Computational Intelligence,
7(4):305-314l
Karen Kukich. f9-83-. Design land implementation
of a knowledge-based reit. generator l In Pro-
ceedings of ACL-1983, pages 145-150.
Anna Law l Yvonne Freer, Jim l Hunter l Robert Lo-I
gie, Neil McIntosh l and John Quinn. 2005. Gen-
erating textual summaries of graphical time series
datalt5 support inedical decision making in the
neonatal intensive care unit. Journal of Clinical
Monitoring and Computing, 19:183-194.
Kathleen McKeown, Karen Kukich, and James
Shawl 1994 Practical issues in automatid docu-I
ment generation. In Proceedings of ANLP-1994,
pages 7-14.
Chris Mellish, Donia Scott, Lynn Cahill, Daniel
Paiva, Roger Evans, and Mike Reape. 2006. A
reference architecture for natural language gen-
eration systems. Natural Language Engineering,
12:1-34.
Fabio Pianesil Massimo Zancanaro, Elena Not
Chiaral Leonardi, Vera Falcon l and Bruno [Lepri.
2007. Multimodal support to group dynamics.
Personal and Ubiquitous Computing, 11. In
press.
Francoi Portet, Ehud Reiter, Jim Hunter and So-
mayajulu Sripada l 2007. Automatic generation
bf textual summaries from neonatal intensive care
data. In Proceedings of AIME 2007. Forthcom-I
ing.
Ehud Reiter and Robert Dale. 2000. Building NatH
ural4,anguage Generation Systems Cambridge
University Press.
Ehud Reiter, Somayajulu Sripada l Jim Hunter, and
Jin Yu. 2005. Choosing words in computer-
generated Weather forecasts. Artificial Intelli-
gence, 167:137-1691
Deb Roy l 2002. Learning visually grounded word
land syntax foil a scene description task. Com-
puter Speech and Language, 16:353-385.
Yuval Shahar. 1997. A framework for knowledge-]
based temporal abstraction. Artificial Intelli-
gence, 90:79-133.
Somayajulh Sripada and Feng Gaol 20071 Sum-
marizing dive pomputer data. In Proceedings of
Ithe Workshop Ion Multimodal Output Generation
(MOG-2007), page 149-157.
Somayajulu Sripada, Ehud Reiter l Jim Hunter, and
Jin Yu. 20031 Generating English summariesIbf
time Senes data ]using the Gncean maxims In
Proceedings of KDD-2003, pages 187-1961
Ross Turner, Somayajulu Sripada, Ehud Reiter, and
Ian Davy. 2006. Generating spatio-temporal de-
scriptions in W-1&amp;1 forecasts. In Proceedings
EACL-2006 Poster Session, pages 163-166.
Jin YuT, Ehuld Reiter, Rim Hunter and Somayajulu
Sripada l 2004-. A new architecture for summaris-I
ing time seriesl data. In Proceedings of INLG-04
Poster Session, pages 47-50.
Jin Yu, Ehud Reiter, Jim Hunter, and Chris Mellish.
20071 Choosing the content of textual summan0
Of large] time-series data sets. Natural Language
Engineering, 13:25-49.
</reference>
<page confidence="0.998733">
104
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.423490">
<title confidence="0.937476">Architecture for Data-to-Textsystems</title>
<affiliation confidence="0.99808">University of</affiliation>
<address confidence="0.834652">Aberdeen,</address>
<email confidence="0.809432">ereiter@csd.abdn.ac.uk</email>
<abstract confidence="0.967142">I present an architecture for data-to-text systems, is which produce texts from non-linguistic input data; this essentially extends the architecture of Reiter and Dale (2000) to ystems1 whose input is raw data instead of AT knowledge bases. This larchitecture is being used in the and is based on experiences in several projects at Aberdeen; it also seems to b compatible with many data-to-text systems developed elsewhere. It consists of four stages which ar organised in a pipeline: Signal Analysis, Data Interpretation, Document Planning, and Microplannin and Realisation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sarah Boyd</author>
</authors>
<title>TREND: a system for generating intelligent descriptions of time-series data.</title>
<date>1998</date>
<booktitle>In Proceedings of the IEEE International Conference on Intelligent Processing Systems (ICIPS1998).</booktitle>
<contexts>
<context position="9303" citStr="Boyd, 1998" startWordPosition="1493" endWordPosition="1494">ed as discrete levents, uch as iog files or records of medical acH tions, can bypass signal analysis. If all input data is of this form (as in PLANDoc (McKeown et 1994), for example), then the data-to-text system does not need to perform any signal analysis. In my experience, signal analysis in data-to-text systems can usually be done with existing signal analysis algorithms. There is a substantial literature on signal analysis and pattern detection, including specialist journals such as Pattern Recognition Quite sophisticated algorithms can be used in data to-text systems; for example TREND (Boyd, 1998) used wavelet analysis. The basic challenge for the system developer is to understand the algorithms, the domain, and the way humans talk about the domain well enough to identify Which algorithms are appropriate for a particular system. 100.0 60.0 99 BT45&apos;s1 signa1 analysiS ifiodule uses three algorithms to detect patterns: • Short-term changes in channels, such as spikes and steps, are detected using a simplified version of the algorithm proposed by Yu et al. (2007). • Longer-term ehanges in the data, such as sensor values increasing or decreasing over time, are detected using linear segmenta</context>
</contexts>
<marker>Boyd, 1998</marker>
<rawString>Sarah Boyd. 1998. TREND: a system for generating intelligent descriptions of time-series data. In Proceedings of the IEEE International Conference on Intelligent Processing Systems (ICIPS1998).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Evans</author>
<author>Paul Piwek</author>
<author>Lynne Cahill</author>
</authors>
<title>What is NLG?</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Natural Language Generation,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="15072" citStr="Evans et al. (2002)" startWordPosition="2441" endWordPosition="2444">nd document structureJ Ir i some data-to-text applications, such as pollen forecasts (Turner et al., 2006) all thel input data is communicated to the user, so document planning is a trivial taskJ But thisl is Unusual, lusually some selection isl needed. Indeed, La wl (2005), who found that doctors Irnade better decisions from l textual summaries than from graphical displays, speculate that this happened precisely because the texts only communicated the most relevant information. Document planning is perhaps the leastunderstood aspect of data-text systems, and indeed of NLG in general; in fact Evans et al. (2002) argue that document planning should not be considered part of NLG. Of course document planning still needs to be done in data-to-text systems regardless of whether it is regarded as an &apos;NLG&apos; task. Yu et al. (2007) treat document planning in datato-text systems as the task of finding &apos;interesting patterns&apos;, and use two mechanisms to do this: rules acquired from domain experts, and novelty (how often a pattern has been seen before). They use simple schemas to specify document structure. Hallett and Scott (2005), who summarise medical events, use a different approach. The input their document pl</context>
</contexts>
<marker>Evans, Piwek, Cahill, 2002</marker>
<rawString>Roger Evans, Paul Piwek, and Lynne Cahill. 2002.] What is NLG? In Proceedings of the Second International Conference on Natural Language Generation, page 144-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Ferres</author>
<author>Avi Parush</author>
<author>Shelley Roberts</author>
<author>Gitte Lindgaard</author>
</authors>
<title>Helping people with visual impairments gain access to graphical informationl through natural 1anguage: The iGraph system.</title>
<date>2006</date>
<booktitle>In Proceedingslof the 10th International Conference on Computers Helping People with Special</booktitle>
<location>Needs.</location>
<contexts>
<context position="17962" citStr="Ferres et al., 2006" startWordPosition="2913" endWordPosition="2916">in its Iclus-I ter. .4 Microplanning and Realisation The fourth stage of the architecture generates acual texts based on the content and structure chosen by Document Planning. It corresponds to the wo stages of Microplanning and Realisation in Re*ter and Dale (2000). From a high-level perspective, microplanning and realisation must decide how to actually express in language the concepts and strucure selected by earlier stages. It is possible to perform microplanning and realisation using simple templates. Whether this is acceptable depends on what is appropriate for users. or example, IGRAPH (Ferres et al., 2006) uses temI° lates to generate (spoken) descriptions of graphs for visually-impaired users] 1GRAPH&apos;s textS look Clumsy and repetitive on paper, and probably would not be acceptable as textual Summaries for people Who can read. But visually impaired users listening to a speech synthesiser have different requirements, and in some cases may prefer simple repetitive texts. Microplanning and realisation in data-to-text are iiiI3 similai td microplanning iñd realisation in other NLG Systems, so I Will not describe them in detail here. There are a few differences in empha-1 sis, though. One difference</context>
</contexts>
<marker>Ferres, Parush, Roberts, Lindgaard, 2006</marker>
<rawString>Leo Ferres, Avi Parush, Shelley Roberts, and Gitte Lindgaard. 2006. Helping people with visual impairments gain access to graphical informationl through natural 1anguage: The iGraph system. In Proceedingslof the 10th International Conference on Computers Helping People with Special Needs.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Eli Goldberg</author>
</authors>
<title>l Norbert Driedger, land Richard Kit-I tredgel 1994. Using natural-language processing td produce Weather forecasts.</title>
<journal>IEEE Expert,</journal>
<pages>9--2</pages>
<marker>Goldberg, </marker>
<rawString>Eli Goldberg l Norbert Driedger, land Richard Kit-I tredgel 1994. Using natural-language processing td produce Weather forecasts. IEEE Expert, 9(2):45-53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Catalina Hallett land Donia Scott</author>
</authors>
<title>Structural variation in generated health reports.</title>
<date>2005</date>
<booktitle>In Third International Workshop on Paraphrasing (IWP2005).</booktitle>
<contexts>
<context position="2891" citStr="Scott (2005)" startWordPosition="447" endWordPosition="448">nlinguistic input data, which is typically numerical. For example, S cmTimE (Reiter et al., 2005) and FOG (Goldberg et al., 1994) generate textual weather forecasts from numerical weather prediction data; ANA (Kukich, 1983) generates textual stock market reports from numerical stock market data; and VITRA (Herzog and Wazinski, 1994) and DESCRIBER (Roy, 2002) generate natural descriptions of visual scenes. Some work has also been done on generating texts from lists of events; for example PLANDoc (McKeown et al., 1994) generates summaries based on trace files from a simulator,l land Hallett and Scott (2005) describe a system Which generates ummaries of events in a medical iecord. Perhaps -t-hbiggest difference between data-to-I text systems and NLG ystems whose input is a 97 knowledge base is that data-to-text Systems Imist analyse and interpret their input data as well as decide ho mA td inguistically comraimicate it. While there is of course a substantial literature on data analysis, thisl primarily focuses on hypothesiS testing and data mining; there are difference § betweenl this kind of data analysis and data analysis for the purposes of generating la textual summary (Sripada et al., 2003) </context>
<context position="15587" citStr="Scott (2005)" startWordPosition="2529" endWordPosition="2530">understood aspect of data-text systems, and indeed of NLG in general; in fact Evans et al. (2002) argue that document planning should not be considered part of NLG. Of course document planning still needs to be done in data-to-text systems regardless of whether it is regarded as an &apos;NLG&apos; task. Yu et al. (2007) treat document planning in datato-text systems as the task of finding &apos;interesting patterns&apos;, and use two mechanisms to do this: rules acquired from domain experts, and novelty (how often a pattern has been seen before). They use simple schemas to specify document structure. Hallett and Scott (2005), who summarise medical events, use a different approach. The input their document planner i graph of events and relations between events. [Their system works by partitioning this graph into inter-connected clusters of events; dropping small clusters (unless they contain information which domain knowledge says must be reported); and then mapping clusters Onto a spe= cific report spine for the target type of report, which specifies which events are central for this type of report Non-spinal Non-spinal events are linked to spinal events using rhetorical relations Which are based on event relatio</context>
</contexts>
<marker>Scott, 2005</marker>
<rawString>Catalina Hallett land Donia Scott. 2005. Structural variation in generated health reports. In Third International Workshop on Paraphrasing (IWP2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerd Herzog land Peter Wazinski</author>
</authors>
<title>Visual TRAnslator: Linking perceptions and natural language] descriptions.</title>
<date>1994</date>
<booktitle>Artificial Intelligenc0Re view,</booktitle>
<pages>8--2</pages>
<contexts>
<context position="2613" citStr="Wazinski, 1994" startWordPosition="400" endWordPosition="401">how well it fits existing data-to-text systems; discuss intermediate representations between the stages; and briefly describe software resources which we are (slowly) developing to support the arlchitecture. 2 Background 2.1 Data-to-text Data-to-text systems generate texts from nonlinguistic input data, which is typically numerical. For example, S cmTimE (Reiter et al., 2005) and FOG (Goldberg et al., 1994) generate textual weather forecasts from numerical weather prediction data; ANA (Kukich, 1983) generates textual stock market reports from numerical stock market data; and VITRA (Herzog and Wazinski, 1994) and DESCRIBER (Roy, 2002) generate natural descriptions of visual scenes. Some work has also been done on generating texts from lists of events; for example PLANDoc (McKeown et al., 1994) generates summaries based on trace files from a simulator,l land Hallett and Scott (2005) describe a system Which generates ummaries of events in a medical iecord. Perhaps -t-hbiggest difference between data-to-I text systems and NLG ystems whose input is a 97 knowledge base is that data-to-text Systems Imist analyse and interpret their input data as well as decide ho mA td inguistically comraimicate it. Whi</context>
</contexts>
<marker>Wazinski, 1994</marker>
<rawString>Gerd Herzog land Peter Wazinski. 1994. Visual TRAnslator: Linking perceptions and natural language] descriptions. Artificial Intelligenc0Re view, 8(2-3): 175-1871</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jim Hunter1</author>
</authors>
<title>TSNet a distributed architecture foil time series analysis.</title>
<date>2006</date>
<booktitle>In Proceeding si of IDA MAP</booktitle>
<pages>85--92</pages>
<marker>Hunter1, 2006</marker>
<rawString>Jim Hunter1 2006. TSNet a distributed architecture foil time series analysis. In Proceeding si of IDA MAP 2006, pages 85-92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eamonn Keogh</author>
<author>Selina Chu</author>
<author>David Hart</author>
<author>land Michael Pazzani1</author>
</authors>
<title>An online algorithm for segmenting time series.</title>
<date>2001</date>
<booktitle>In Proceedings of IEEE International Conferenc0 on Data Mining</booktitle>
<pages>289--296</pages>
<marker>Keogh, Chu, Hart, Pazzani1, 2001</marker>
<rawString>Eamonn Keogh, Selina Chu, David Hart, land Michael Pazzani1 2001. An online algorithm for segmenting time series. In Proceedings of IEEE International Conferenc0 on Data Mining pages 289-296.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Richard Kittredge</author>
</authors>
<title>Tanya Korelsky, and Owen</title>
<booktitle>Ram19911 On rhd need f4 domain communication language l Computational Intelligence,</booktitle>
<pages>7--4</pages>
<marker>Kittredge, </marker>
<rawString>Richard Kittredge, Tanya Korelsky, and Owen Ram19911 On rhd need f4 domain communication language l Computational Intelligence, 7(4):305-314l</rawString>
</citation>
<citation valid="false">
<authors>
<author>Karen Kukich</author>
</authors>
<title>f9-83-. Design land implementation of a knowledge-based reit. generator l</title>
<booktitle>In Proceedings of ACL-1983,</booktitle>
<pages>145--150</pages>
<marker>Kukich, </marker>
<rawString>Karen Kukich. f9-83-. Design land implementation of a knowledge-based reit. generator l In Proceedings of ACL-1983, pages 145-150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anna Law l Yvonne Freer</author>
</authors>
<title>Jim l Hunter l Robert Lo-I gie, Neil McIntosh l and</title>
<date>2005</date>
<journal>Journal of Clinical Monitoring and Computing,</journal>
<pages>19--183</pages>
<marker>Freer, 2005</marker>
<rawString>Anna Law l Yvonne Freer, Jim l Hunter l Robert Lo-I gie, Neil McIntosh l and John Quinn. 2005. Generating textual summaries of graphical time series datalt5 support inedical decision making in the neonatal intensive care unit. Journal of Clinical Monitoring and Computing, 19:183-194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen McKeown</author>
<author>Karen Kukich</author>
<author>James Shawl</author>
</authors>
<title>Practical issues in automatid docu-I ment generation.</title>
<date>1994</date>
<booktitle>In Proceedings of ANLP-1994,</booktitle>
<pages>7--14</pages>
<contexts>
<context position="2801" citStr="McKeown et al., 1994" startWordPosition="429" endWordPosition="432">upport the arlchitecture. 2 Background 2.1 Data-to-text Data-to-text systems generate texts from nonlinguistic input data, which is typically numerical. For example, S cmTimE (Reiter et al., 2005) and FOG (Goldberg et al., 1994) generate textual weather forecasts from numerical weather prediction data; ANA (Kukich, 1983) generates textual stock market reports from numerical stock market data; and VITRA (Herzog and Wazinski, 1994) and DESCRIBER (Roy, 2002) generate natural descriptions of visual scenes. Some work has also been done on generating texts from lists of events; for example PLANDoc (McKeown et al., 1994) generates summaries based on trace files from a simulator,l land Hallett and Scott (2005) describe a system Which generates ummaries of events in a medical iecord. Perhaps -t-hbiggest difference between data-to-I text systems and NLG ystems whose input is a 97 knowledge base is that data-to-text Systems Imist analyse and interpret their input data as well as decide ho mA td inguistically comraimicate it. While there is of course a substantial literature on data analysis, thisl primarily focuses on hypothesiS testing and data mining; there are difference § betweenl this kind of data analysis a</context>
</contexts>
<marker>McKeown, Kukich, Shawl, 1994</marker>
<rawString>Kathleen McKeown, Karen Kukich, and James Shawl 1994 Practical issues in automatid docu-I ment generation. In Proceedings of ANLP-1994, pages 7-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Mellish</author>
<author>Donia Scott</author>
<author>Lynn Cahill</author>
<author>Daniel Paiva</author>
<author>Roger Evans</author>
<author>Mike Reape</author>
</authors>
<title>A reference architecture for natural language generation systems.</title>
<date>2006</date>
<journal>Natural Language Engineering,</journal>
<pages>12--1</pages>
<contexts>
<context position="4130" citStr="Mellish et al., 2006" startWordPosition="640" endWordPosition="643">res Perhaps the best-known larchitecture for general NLG system S is the three-stage Pipeline inodel (Reiter and pale, 2000), Which divides NLG into document planning microplanning, and realisation stages This larchitecture focused on NLG systems whose input was a knowledge base. The architecture proposed here essentially extends the Reiter and Dale architecture by adding two new stages, Signal AnalysiS and Data Interpretation, which are placed before document planning in the pipeline; this extension allows the inputs to the system to be data instead of (or in addition to) knowledge The RAGS (Mellish et al., 2006) project conducted a survey of NLG systems, and concluded that they were architecturally quite diverse in terms of how they were modularised; it proposed a fairly abstract NLG architecture which described different representations which might be used in an NLG system, but did not lcommit to specific Stages or modules1 The only proposaf for an larchitecturel which is specifically for data-to-text systems (which I am aware of) is Yu et al. (2004). The architecture given in that paper is quite detailed, and based on one system; thel architecture presented here, in contrast is higher-level and app</context>
<context position="24128" citStr="Mellish et al. (2006)" startWordPosition="3863" endWordPosition="3866">ning and the Lexicaliser and Surface Generator perform microplanning and realisation (respectively). As mentioned above, PLANDoc does not need to perform signal analysis, because its input is already in the form of discrete events However, thel architecture described here does not fit all data-to-text systems. For example DESCRIBER (Roy, 2002) takes a more integrated approach based on machine learning 6 Representations The above description of the architecture is of course very high-level. To make it more concrete, we need to specify intermediate representations (APIs) between the modules. As Mellish et al. (2006) point out, it is difficult to do this for NLG as a whole, because the field is very diverse. However, we believe this problem is more tractable (although still hard) in the more limited area of data-to-text. In particular, we use the following intermediate representations in BT45, and believe these could be used in other systems as well: • output of Signal Analysis is a set of patterns. Patterns are represented as objects in Protege2 ontology, which includes types such as SPIKE and INCREASING TREND. Objects have parameters (feature values), such as the channel they occurred in and the time th</context>
</contexts>
<marker>Mellish, Scott, Cahill, Paiva, Evans, Reape, 2006</marker>
<rawString>Chris Mellish, Donia Scott, Lynn Cahill, Daniel Paiva, Roger Evans, and Mike Reape. 2006. A reference architecture for natural language generation systems. Natural Language Engineering, 12:1-34.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Pianesil Massimo Zancanaro</author>
</authors>
<title>Elena Not Chiaral Leonardi, Vera Falcon l and Bruno [Lepri.</title>
<date>2007</date>
<journal>Personal and Ubiquitous Computing,</journal>
<volume>11</volume>
<note>In press.</note>
<marker>Zancanaro, 2007</marker>
<rawString>Fabio Pianesil Massimo Zancanaro, Elena Not Chiaral Leonardi, Vera Falcon l and Bruno [Lepri. 2007. Multimodal support to group dynamics. Personal and Ubiquitous Computing, 11. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Francoi Portet</author>
</authors>
<title>Ehud Reiter, Jim Hunter and Somayajulu Sripada l 2007. Automatic generation bf textual summaries from neonatal intensive care data.</title>
<date>2007</date>
<booktitle>In Proceedings of AIME</booktitle>
<note>Forthcom-I ing.</note>
<marker>Portet, 2007</marker>
<rawString>Francoi Portet, Ehud Reiter, Jim Hunter and Somayajulu Sripada l 2007. Automatic generation bf textual summaries from neonatal intensive care data. In Proceedings of AIME 2007. Forthcom-I ing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<date>2000</date>
<booktitle>Building NatH ural4,anguage Generation Systems</booktitle>
<publisher>Cambridge University Press.</publisher>
<marker>Reiter, Dale, 2000</marker>
<rawString>Ehud Reiter and Robert Dale. 2000. Building NatH ural4,anguage Generation Systems Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>Somayajulu Sripada l</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<pages>167--137</pages>
<marker>Reiter, 2005</marker>
<rawString>Ehud Reiter, Somayajulu Sripada l Jim Hunter, and Jin Yu. 2005. Choosing words in computergenerated Weather forecasts. Artificial Intelligence, 167:137-1691</rawString>
</citation>
<citation valid="false">
<authors>
<author>Deb Roy</author>
</authors>
<title>l 2002. Learning visually grounded word land syntax foil a scene description task.</title>
<journal>Computer Speech and Language,</journal>
<pages>16--353</pages>
<marker>Roy, </marker>
<rawString>Deb Roy l 2002. Learning visually grounded word land syntax foil a scene description task. Computer Speech and Language, 16:353-385.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Shahar</author>
</authors>
<title>A framework for knowledge-] based temporal abstraction.</title>
<date>1997</date>
<journal>Artificial Intelligence,</journal>
<pages>90--79</pages>
<contexts>
<context position="13181" citStr="Shahar, 1997" startWordPosition="2123" endWordPosition="2124">nship s1 between events: BT45 looks for three kinds of relationships between messages: eausality (for example, blood oxygen increases because the nurse increased oxyH gen levels in the ventilator); part of a procedure (for example, giving morphine is part of a reintubation procedure); and other (for example, an increase in blood oxygen is associated with a decrease in blood CO2). Currently, data interpretation in [urr45 is primarily done by production rules Written in JESSI, which are based on knowledge-acquisition activities conducted with experienced doctors. Wel may in the future use KBTA (Shahar, 1997) for some bf thisl reasoning. To take a Iconcrete example in BT45 the SPIKE event Mentioned at the end Of the Signal ysection is interpreted in Data Interpretation as a BRADYCARDIA. It has moderate importance (15 on a scale of 0 to 100), land has an associated with link td a drop in blood oxygen saturation (SO) which happens lat about the same time. 3.3 Document Planning The third stage bf the architecture decide § which events to mention in the text, and also bn the text&apos;s1 rhetorical and document (e.g., paragraph break) structure. This is the same as thel Document PlanMT* stage in -t-harchit</context>
</contexts>
<marker>Shahar, 1997</marker>
<rawString>Yuval Shahar. 1997. A framework for knowledge-] based temporal abstraction. Artificial Intelligence, 90:79-133.</rawString>
</citation>
<citation valid="false">
<title>Somayajulh Sripada and Feng Gaol 20071 Summarizing dive pomputer data.</title>
<booktitle>In Proceedings of Ithe Workshop Ion Multimodal Output Generation (MOG-2007),</booktitle>
<pages>149--157</pages>
<marker></marker>
<rawString>Somayajulh Sripada and Feng Gaol 20071 Summarizing dive pomputer data. In Proceedings of Ithe Workshop Ion Multimodal Output Generation (MOG-2007), page 149-157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Somayajulu Sripada</author>
</authors>
<title>Ehud Reiter l</title>
<date>2003</date>
<booktitle>In Proceedings of KDD-2003,</booktitle>
<pages>187--1961</pages>
<marker>Sripada, 2003</marker>
<rawString>Somayajulu Sripada, Ehud Reiter l Jim Hunter, and Jin Yu. 20031 Generating English summariesIbf time Senes data ]using the Gncean maxims In Proceedings of KDD-2003, pages 187-1961</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ross Turner</author>
<author>Somayajulu Sripada</author>
<author>Ehud Reiter</author>
<author>Ian Davy</author>
</authors>
<title>Generating spatio-temporal descriptions in W-1&amp;1 forecasts.</title>
<date>2006</date>
<booktitle>In Proceedings EACL-2006 Poster Session,</booktitle>
<pages>163--166</pages>
<contexts>
<context position="14559" citStr="Turner et al., 2006" startWordPosition="2357" endWordPosition="2360">large numbei J of messages patterns, and event S (often hundreds or thousands), but texts lusually are limited to only describing a small number of message (the actual number depends on the genre, but often is between 5 and 25 events) Th 6 Document Planner must decide which messages are actually communicated in thel text; thisl decision is based on the domain and genre It must lalso try td Icommunicate howl thel message mentioned in the text relate to each other; thisl can partially be done using rhetorical and document structureJ Ir i some data-to-text applications, such as pollen forecasts (Turner et al., 2006) all thel input data is communicated to the user, so document planning is a trivial taskJ But thisl is Unusual, lusually some selection isl needed. Indeed, La wl (2005), who found that doctors Irnade better decisions from l textual summaries than from graphical displays, speculate that this happened precisely because the texts only communicated the most relevant information. Document planning is perhaps the leastunderstood aspect of data-text systems, and indeed of NLG in general; in fact Evans et al. (2002) argue that document planning should not be considered part of NLG. Of course document </context>
<context position="22529" citStr="Turner et al., 2006" startWordPosition="3625" endWordPosition="3628"> both be absent in individual systems. My inain reason foil Separating these is that they emerge from two very different research com-I munities Data Interpretation has been studies by researchers in knowledge-based (expert) Systems, While Document planning has been studied by researchers in the NLG community. 5 Applicability Perhaps not surprisingly, the architecture described here fits many data-to-text systems developed at Aberdeen by the author and his colleagues, including SUMTIME (Reiter et al., 2005) (generates marine weather forecasts for offshore oil rigs); pollen forecast generator (Turner et al., 2006); SUMTIMETURBINE (Yu et al., 2007) (generates Summaries Of sensor data from a gas turbine); land SCUBATEXT (Sripada and Gao 2007) (generate afety-oriented summaries of scuba dives) It also seems to fit many data-to-text systems developed elsewhere. For example, ANA (Kuloch 1983), which genera7t Stock market Summaries, is described as hav-I ing 4 modules in a pipeline: fact generator, message generator, discourse organiser, and text generator. These correspond WI the ignal analysis, dYta interpretation, document planning, land microplanning/realisation modules described here. Anotheil example i</context>
</contexts>
<marker>Turner, Sripada, Reiter, Davy, 2006</marker>
<rawString>Ross Turner, Somayajulu Sripada, Ehud Reiter, and Ian Davy. 2006. Generating spatio-temporal descriptions in W-1&amp;1 forecasts. In Proceedings EACL-2006 Poster Session, pages 163-166.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jin YuT</author>
</authors>
<title>Ehuld Reiter, Rim Hunter and Somayajulu Sripada l 2004-. A new architecture for summaris-I ing time seriesl data.</title>
<booktitle>In Proceedings of INLG-04 Poster Session,</booktitle>
<pages>47--50</pages>
<marker>YuT, </marker>
<rawString>Jin YuT, Ehuld Reiter, Rim Hunter and Somayajulu Sripada l 2004-. A new architecture for summaris-I ing time seriesl data. In Proceedings of INLG-04 Poster Session, pages 47-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Yu</author>
<author>Ehud Reiter</author>
<author>Jim Hunter</author>
<author>Chris Mellish</author>
</authors>
<title>Choosing the content of textual summan0 Of large] time-series data sets.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<pages>13--25</pages>
<contexts>
<context position="9774" citStr="Yu et al. (2007)" startWordPosition="1569" endWordPosition="1572">pecialist journals such as Pattern Recognition Quite sophisticated algorithms can be used in data to-text systems; for example TREND (Boyd, 1998) used wavelet analysis. The basic challenge for the system developer is to understand the algorithms, the domain, and the way humans talk about the domain well enough to identify Which algorithms are appropriate for a particular system. 100.0 60.0 99 BT45&apos;s1 signa1 analysiS ifiodule uses three algorithms to detect patterns: • Short-term changes in channels, such as spikes and steps, are detected using a simplified version of the algorithm proposed by Yu et al. (2007). • Longer-term ehanges in the data, such as sensor values increasing or decreasing over time, are detected using linear segmentation (Keogh et al., 2001). • artefacts that is data which is eorrupted or otherwise should be ignored (for example because a sensor has fallen Off the baby) are detected as described by Portet et al. (2007) To take a concrete example, one pattern type in BT45 is SPIKE; this indicates that the values in sensor channel have temporarily increased or decreased, but then reverted to their previous value. Thel SPIKE Iclas sl has several parameters, including channel, direc</context>
<context position="15286" citStr="Yu et al. (2007)" startWordPosition="2478" endWordPosition="2481"> lusually some selection isl needed. Indeed, La wl (2005), who found that doctors Irnade better decisions from l textual summaries than from graphical displays, speculate that this happened precisely because the texts only communicated the most relevant information. Document planning is perhaps the leastunderstood aspect of data-text systems, and indeed of NLG in general; in fact Evans et al. (2002) argue that document planning should not be considered part of NLG. Of course document planning still needs to be done in data-to-text systems regardless of whether it is regarded as an &apos;NLG&apos; task. Yu et al. (2007) treat document planning in datato-text systems as the task of finding &apos;interesting patterns&apos;, and use two mechanisms to do this: rules acquired from domain experts, and novelty (how often a pattern has been seen before). They use simple schemas to specify document structure. Hallett and Scott (2005), who summarise medical events, use a different approach. The input their document planner i graph of events and relations between events. [Their system works by partitioning this graph into inter-connected clusters of events; dropping small clusters (unless they contain information which domain kn</context>
<context position="22563" citStr="Yu et al., 2007" startWordPosition="3631" endWordPosition="3634"> My inain reason foil Separating these is that they emerge from two very different research com-I munities Data Interpretation has been studies by researchers in knowledge-based (expert) Systems, While Document planning has been studied by researchers in the NLG community. 5 Applicability Perhaps not surprisingly, the architecture described here fits many data-to-text systems developed at Aberdeen by the author and his colleagues, including SUMTIME (Reiter et al., 2005) (generates marine weather forecasts for offshore oil rigs); pollen forecast generator (Turner et al., 2006); SUMTIMETURBINE (Yu et al., 2007) (generates Summaries Of sensor data from a gas turbine); land SCUBATEXT (Sripada and Gao 2007) (generate afety-oriented summaries of scuba dives) It also seems to fit many data-to-text systems developed elsewhere. For example, ANA (Kuloch 1983), which genera7t Stock market Summaries, is described as hav-I ing 4 modules in a pipeline: fact generator, message generator, discourse organiser, and text generator. These correspond WI the ignal analysis, dYta interpretation, document planning, land microplanning/realisation modules described here. Anotheil example is PLANDoc (McKeown al., 1994), whi</context>
</contexts>
<marker>Yu, Reiter, Hunter, Mellish, 2007</marker>
<rawString>Jin Yu, Ehud Reiter, Jim Hunter, and Chris Mellish. 20071 Choosing the content of textual summan0 Of large] time-series data sets. Natural Language Engineering, 13:25-49.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>