<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.996136">
Efficient Inference and Structured Learning
for Semantic Role Labeling
</title>
<author confidence="0.572609">
Oscar Täckström Kuzman Ganchev Dipanjan Das
</author>
<affiliation confidence="0.511022">
Google Google Google
</affiliation>
<address confidence="0.810636">
New York New York New York
</address>
<email confidence="0.997308">
oscart@google.com kuzman@google.com dipanjand@google.com
</email>
<sectionHeader confidence="0.994767" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995979058823529">
We present a dynamic programming algorithm
for efficient constrained inference in semantic
role labeling. The algorithm tractably captures
a majority of the structural constraints exam-
ined by prior work in this area, which has re-
sorted to either approximate methods or off-the-
shelf integer linear programming solvers. In ad-
dition, it allows training a globally-normalized
log-linear model with respect to constrained
conditional likelihood. We show that the dy-
namic program is several times faster than an
off-the-shelf integer linear programming solver,
while reaching the same solution. Furthermore,
we show that our structured model results in
significant improvements over its local counter-
part, achieving state-of-the-art results on both
PropBank- and FrameNet-annotated corpora.
</bodyText>
<sectionHeader confidence="0.998808" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945096153847">
Semantic role labeling (henceforth, SRL) is the task
of identifying the semantic arguments of predicates
in natural language text. Pioneered by Gildea and
Jurafsky (2002), this task has been widely investi-
gated by the NLP community. There have been two
shared tasks at CoNLL 2004 and 2005 focusing on
this problem, using PropBank conventions to identify
the phrasal arguments of verbal predicates (Palmer et
al., 2005; Carreras and Màrquez, 2004, 2005). Since
then, there has been work on SRL for nominal pred-
icates (Meyers et al., 2004; Gerber and Chai, 2010)
and variants that investigated the prediction of se-
mantic dependencies rather than phrasal arguments
(Surdeanu et al., 2008; Hajiˇc et al., 2009).
Here, we present an inference method for SRL,
addressing the problem of phrasal argument structure
prediction (as opposed to semantic dependencies). In
contrast to most prior semantic role labeling work
focusing on PropBank conventions, barring notable
exceptions such as Meza-Ruiz and Riedel (2009), our
framework first performs frame identification, the
subtask of disambiguating the predicate frame; this
makes our analysis more interpretable. The focus of
this paper, however, is the subtask of semantic role
labeling, wherein we take a set of (potentially over-
lapping) candidate sentential phrases and identify and
label them with the semantic roles associated with the
predicted frame. This treatment is commonly used in
frame semantic parsing (Das et al., 2014; Hermann
et al., 2014) and our two-stage framework is able to
model both PropBank and FrameNet conventions.
Previous work focusing on semantic role labeling
imposed several structural constraints warranted by
the annotation conventions of the task and other lin-
guistic considerations, such as avoiding overlapping
arguments and repeated core roles in the final predic-
tion. Such global inference often leads to improved
results and more meaningful predictions compared to
local unconstrained methods (Màrquez et al., 2008).
A popular framework for imposing these constraints
has been integer linear programming (ILP), wherein
the inference problem is specified declaratively (Pun-
yakanok et al., 2008). However, ILP-based inference
methods often rely on generic off-the-shelf solvers
that fail to exploit problem-specific structure (Martins
et al., 2011). Instead, we present a dynamic program
(DP) that exactly enforces most of the constraints
examined by Punyakanok et al. (2008); remaining
constraints are enforced by reverting to k-best infer-
ence if needed. We show that this technique solves
the inference problem more than four times faster
than a state-of-the-art off-the-shelf ILP solver, while
</bodyText>
<page confidence="0.994454">
29
</page>
<note confidence="0.2184205">
Transactions of the Association for Computational Linguistics, vol. 3, pp. 29–41, 2015. Action Editor: Kristina Toutanova.
Submission batch: 9/2014; Revision batch 1/2015; Published 1/2015. c�2015 Association for Computational Linguistics.
</note>
<figure confidence="0.794948888888889">
(wanter) (thing wanted) (thing expected)
A0 want.01 A1 A1 expect.01 C-A1
I want to hold your hand .
It is expected to rain .
I want to hold your hand .
The spy who knew me .
A0 hold.01 A1 A0 R-A0 know.01 A1
(holder) (thing held) (knower) (thing known
or thought)
</figure>
<figureCaption confidence="0.968036142857143">
Figure 1: Example semantic role annotations for the
two verbs in the sentence “I want to hold your hand.”,
according to PropBank. The annotations on top show the
frame structure corresponding to want, while the ones
below reflect the annotations for hold. Note that the agent
role (A0) is realized as the same word (“I”), but with the
meaning wanter in one case and holder in the other.
</figureCaption>
<bodyText confidence="0.996931416666667">
being guaranteed to achieve identical results.
In addition to being relatively slow, ILP-based
methods only solve the maximum a posteriori (MAP)
inference problem, which prevents the computation
of marginals and feature expectations. The proposed
DP, on the other hand, allows us to train a globally-
normalized log-linear model, enforcing the structural
constraints during training. Empirically, we show
that such a structured model consistently performs
better than training separate classifiers and incorpo-
rating the constraints only at inference time. We
present results on the Wall Street Journal develop-
ment and test sets, as well as the Brown test set from
the CoNLL 2005 shared task for verbal SRL; these
show that our structured model — which uses a single
dependency parse and no model averaging or rerank-
ing — outperforms other strong single-model sys-
tems and rivals state-of-the-art ensemble-based meth-
ods. We further present results on the OntoNotes 5.0
corpora annotated with semantic roles for both verbal
and nominal predicates (Weischedel et al., 2011) and
strongly outperform the prior state of the art (Pradhan
et al., 2013). Finally, we present results on FrameNet
1.5 data, again achieving state-of-the-art results.
</bodyText>
<sectionHeader confidence="0.984957" genericHeader="method">
2 Task Overview
</sectionHeader>
<bodyText confidence="0.998755857142857">
We seek to predict the semantic argument structure
of predicates in text. For brevity and practical rea-
sons, the exposition and empirical study is primarily
focused on PropBank-style annotations (Palmer et
al., 2005). However, our approach applies directly
to FrameNet-style annotations as well (Baker et al.,
1998) and as shown empirically in §6, a similar trend
</bodyText>
<figureCaption confidence="0.9986615">
Figure 2: Examples showing continuation and reference
roles according to PropBank. The role prefix C- indicates
continuation of an argument, while the prefix R- indicates
reference to another overt argument of the same predicate.
</figureCaption>
<bodyText confidence="0.9997842">
holds across both types of annotation.
In both cases, we are provided with a frame lexicon
that contains type-level information for lexical units
(a lemma conjoined with a coarse-grained part-of-
speech tag).1 For each lexical unit, a list of senses,
or frames, are provided, where each frame comes
with a set of semantic roles that constitute the various
participants in the frame. These roles can be either
core or non-core to the frame.
In PropBank, a set of seven generic core role labels
are defined (A0-A5 and AA) that take on different
semantics for each frame; each frame associates with
a subset of these core roles. In addition there are
21 non-core role labels that serve as adjuncts, such
as the temporal role AM-TMP and the locative role
AM-LOC; these are shared across frames and assume
similar meaning.
FrameNet similarly specifies a set of frames and
roles, with two key differences. First, the semantics
of the small set of core role labels in PropBank are
local to each frame. In contrast, the several hundred
role labels in FrameNet are shared across frames and
they take on similar semantics in the frames in which
they participate. Second, while frames in PropBank
are just coarse-grained lemma-specific senses, the
frame repository in FrameNet is shared across lem-
mas. See Hermann et al. (2014) for examples of these
differences.
Both PropBank- and FrameNet annotated data con-
sist of sentence-level annotations that instantiate the
respective frame lexicon with each predicate disam-
biguated to its frame, as well as the phrasal argu-
ments of each predicate labeled with their semantic
roles. Figure 1 shows an example sentence with two
verbs annotated according to PropBank conventions.
</bodyText>
<footnote confidence="0.986282">
1The CoNLL 2005 dataset is restricted to verbal predicates.
</footnote>
<page confidence="0.999107">
30
</page>
<bodyText confidence="0.999983888888889">
In addition to such basic semantic role annotation,
the PropBank-annotated data sets from the CoNLL
2004 and 2005 shared tasks and OntoNotes 5.0, repre-
sent discontiguous arguments across multiple spans.
These are annotated such that the first span is labeled
with one of the 28 semantic role labels, while subse-
quent spans have the continuation prefix C- attached
to the role. The first sentence in Figure 2 shows such
an annotation. Moreover, these data sets feature refer-
ence roles for arguments, primarily relative pronouns,
that refer to other overt arguments of the predicate.
These roles are annotated by attaching the prefix R-
to the role of the co-referent argument. For exam-
ple, in the second sentence of Figure 2, the relative
pronoun who refers to the argument The spy and is
labeled R-A0. FrameNet annotations, on the other
hand, contain neither continuation or reference roles
according to conventions adopted by prior work.
</bodyText>
<sectionHeader confidence="0.995125" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.99992">
Before delving into the details of the structural con-
straints enforced in the SRL task, we describe its two
subtasks. Akin to most previous work, these subtasks
are solved as separate steps in a cascaded fashion.
</bodyText>
<subsectionHeader confidence="0.998385">
3.1 Classifier Cascade
</subsectionHeader>
<bodyText confidence="0.985685142857143">
To predict annotations such as those described in the
previous section, we take a preprocessed sentence
and first attempt to disambiguate the frame of each
predicate (frame identification). In this work, as part
of preprocessing, we use a part-of-speech tagger and
a dependency parser to syntactically analyze the sen-
tence; this diverges from most prior work on seman-
tic argument prediction, which rely on constituency
parses. Next, we take each disambiguated frame and
look up the core and non-core (or adjunct) roles that
can associate with the frame. Given the predicate
token, we (over-)generate a set of candidate spans in
the sentence, that are then labeled with roles from
the set of core roles, from the set of adjunct roles,
or with the null role 0 (role labeling).2 Our system
thus comprises a cascade of two statistical models.
Note that most prior work on PropBank data only
considered the latter task, remaining agnostic to the
2This setup differs from the related line of work that only
predicts semantic dependencies between the predicate and the
head words of semantic arguments; the latter task is arguably
more straightforward (Surdeanu et al., 2008; Hajiˇc et al., 2009).
frame. Moreover, the semantic role labeling step has
typically been divided into two stages: first identify-
ing the spans that serve as semantic arguments and
then labeling them with their roles (Màrquez et al.,
2008). In contrast, we approach the semantic role
labeling subproblem using a single statistical model.
</bodyText>
<subsectionHeader confidence="0.999633">
3.2 Frame Identification
</subsectionHeader>
<bodyText confidence="0.9998435">
Given a preprocessed sentence x and a marked pred-
icate t with lemma f, we seek to predict the frame
f instantiated by the predicate. To this end, we use
different models in the PropBank and FrameNet set-
tings. In case of PropBank, we define the probability
of a frame f under a conditional log-linear model:
</bodyText>
<equation confidence="0.659478">
p(f  |x, t, f) a exp (ψ · h(f, x, t, f)) ,
</equation>
<bodyText confidence="0.999945090909091">
where ψ denotes the model parameters and h(·) is
the feature function (see Table 1 for details on the
features employed). The model’s partition function
sums over all frames for the lemma f in the lexicon
and we estimate the model parameters by maximizing
regularized conditional log-likelihood.
In the case of FrameNet, to make our results di-
rectly comparable to the recent state-of-the-art re-
sults of Hermann et al. (2014), we instead use their
embeddings-based WSABIE model (Weston et al.,
2011) for the frame identification step.
</bodyText>
<subsectionHeader confidence="0.997424">
3.3 Unconstrained Semantic Role Labeling
</subsectionHeader>
<bodyText confidence="0.979348388888889">
Given an identified frame f in a sentence x of n
words (wl, ... , wn), we seek to predict a set of ar-
gument spans labeled with their semantic roles. We
assume that there is a set of candidate spans S that
could potentially serve as arguments of t. Specifi-
cally, we derive S with a high-recall rule-based algo-
rithm that looks at the (dependency) syntactic context
of the predicate word t, as described in §6.3.
Let one candidate span be s E S. The set of
possible roles R is composed of core roles RC asso-
ciating with f, adjunct roles RA and the null role 0.
In addition, in the PropBank setting, we have a set of
continuation roles RN and reference roles RR; thus,
R = RC ∪ RA ∪ RN ∪ RR ∪ {0}. We assume a
model that assigns a real-valued compatibility score
g(s, r) to each pair of span and role (s, r) E S x R;
the precise nature of the model and its estimation
is described in §5. With no consistency constraints
</bodyText>
<page confidence="0.999784">
31
</page>
<bodyText confidence="0.999334666666667">
between the span-role pairs, prediction amounts to
selecting the optimal role for each span. This gives
us a global score which is a sum over all spans:
</bodyText>
<equation confidence="0.9912465">
max g(s,r), (1)
r∈R
</equation>
<bodyText confidence="0.970758">
with the solution being the corresponding arg max.
</bodyText>
<subsectionHeader confidence="0.993156">
3.4 Semantic Role Labeling as an ILP
</subsectionHeader>
<bodyText confidence="0.99977">
We can represent any prediction for the individual
classifiers with a set of indicator variables z = {zs,r}
with one variable for each span s and role r. An
equivalent formulation to Equation (1) is then:
</bodyText>
<equation confidence="0.99863625">
zs,r × g(s, r)
s.t. z ∈ {0,1}|S||R |(2)
X zs,r = 1 ∀s ∈ S ,
r∈R
</equation>
<bodyText confidence="0.995692555555556">
where we have constrained the indicator variables to
take on binary values, and required that we choose
exactly one role (including the 0 role) for each span.
To further guide the inference, we add the following
constraints to the ILP in Equation (2), as originally
proposed by Punyakanok et al. (2008):3
No Span Overlap Let Si be the set of spans cover-
ing token wi. We want to ensure that at most one of
the spans in Si have an overt role assignment:
</bodyText>
<equation confidence="0.96955">
∀i ∈ [1, n] , X X zs,r ≤ 1 .
s∈Si r6=0
</equation>
<bodyText confidence="0.9132725">
Unique Core Roles Each core role r ∈ RC can be
overt in at most one of the spans in S:
</bodyText>
<equation confidence="0.929103">
X∀r ∈ RC , zs,r ≤ 1 .
s∈S
</equation>
<bodyText confidence="0.816789375">
Continuation Roles A continuation role, may only
be assigned if the corresponding base (i.e. non-
continuation, non-reference) role is assigned to an
earlier span. To express this, we define s ≤ s0 to
mean that s starts before s0. For a continuation role
r ∈ RN, let base(r) ∈ RC ∪ RA be the correspond-
ing base role. Then the constraint is:
X∀r ∈ RN , ∀s ∈ S , zs,r ≤ zs&apos;,base(r) .
s&apos;≤s
3Note that the continuation roles and reference roles con-
straints below are only applicable to PropBank annotations, as
these roles are not present in FrameNet annotations.
Reference Roles Similar to continuation roles, a
span can only be labeled with a reference role r ∈
RR if another span is labeled with the corresponding
base role, base(r) ∈ RC ∪ RA:
</bodyText>
<equation confidence="0.630277">
X∀r ∈ RR , ∀s ∈ S , zs,r ≤ zs&apos;,base(r) .
s&apos;∈S
</equation>
<sectionHeader confidence="0.988992" genericHeader="method">
4 Dynamic Program Formulation
</sectionHeader>
<bodyText confidence="0.999983206896552">
An advantage of the formulation in the previous sec-
tion is that the constrained MAP inference problem
can be solved with an off-the-shelf ILP solver. Un-
fortunately, these solvers typically fail to exploit the
problem-specific structure of the set of admissible
solutions, which often leads to slow inference. As
an alternative, we propose a dynamic program that
takes advantage of the sequential and local nature of
the problem, while directly enforcing all but the non-
core continuation roles constraint and the reference
roles constraint; the remaining constraints can be ef-
ficiently enforced by a straightforward search over
the k-best solutions of the dynamic program. The
resulting inference procedure is guaranteed to find
the same optimal solution as the corresponding ILP
(modulo rounding and tie breaking), while being sub-
stantially faster. In addition, the forward-backward
algorithm can be applied to compute marginals over
the indicator variables, taking the constraints into
account. This facilitates computation of confidence
scores, as well as learning with a constrained globally
normalized log-linear model, as described in §5.
We encode the dynamic program as a weighted
lattice G = (V, E), where V is the set of vertices and
E is the set of (weighted) edges, such that the shortest
path through the lattice corresponds to the optimal
ILP solution. The core of the lattice is the encoding of
the no span overlap constraint; additional constraints
are later added on top of this backbone.
</bodyText>
<subsectionHeader confidence="0.995533">
4.1 No Span Overlap
</subsectionHeader>
<bodyText confidence="0.99998175">
We first describe the structure and then the weights
of the dynamic program lattice. For ease of exposi-
tion, Figure 3 shows an example sentence with three
argument candidates corresponding to “It”, “to rain”
and “rain”, with the possible span-role assignments:
“It”:A1/0, “to rain”:A0/C-A1/0 and “rain”:A0/0.
Our goal is to construct a dynamic program such that
the length of the optimal path is equal to the score
</bodyText>
<equation confidence="0.930948875">
X
s∈S
X
max
Z
s∈S
X
r∈R
</equation>
<page confidence="0.98031">
32
</page>
<figureCaption confidence="0.969091666666667">
Figure 3: Lattice corresponding to the dynamic program
for the no span overlap constraint. The path of the correct
argument assignment is indicated with dashed edges.
</figureCaption>
<bodyText confidence="0.971547923076923">
of mapping “It” to A1, “to rain” to C-A1 and “rain”
to ∅. The dynamic program needs to ensure that re-
gardless of the scores, either “to rain” or “rain” must
be labeled with ∅ since they overlap in the sentence.
We satisfy the latter by using a semi-Markov model
formally described below. In order to ensure that the
∅ role assignment scores are included correctly, they
are given a special treatment in the scoring function.4
Lattice Structure The set of vertices V = {vj :
j E [0, n + 1]} contains a vertex between every pair
of consecutive words. The edges E are divided into
null edges (between consecutive vertices) and argu-
ment edges (which connect the vertices correspond-
ing to the argument span endpoints). We will use the
notation ej,j+1,∅ for the null edge from vj to vj+1.
For each span and non-null role pair (s, r), r =� ∅,
we add an argument edge es,r between vi_1 and vj
where the span s is from word i to j. Figure 3 illus-
trates the structure of the lattice. In this example, we
assume that there are two possible roles (A0/C-A1)
for the phrase “to rain”; consequently, there are two
argument edges corresponding to this phrase.
A path through the lattice corresponds to a global
assignment of roles to spans by assigning role r to
span s for every edge es,r in the path, and assigning
the ∅ role to all other spans. The length of a path is
given by the sum of the weights of its edges.
Lattice Weights The idea behind our weighting
scheme is to include all the null scores g(s, ∅) at the
start, and then subtract them whenever we assign a
role to a candidate span. Let us augment the lattice
4The resulting lattice corresponds to that of an aggressively
pruned semi-Markov sequence model, modulo the special care
given to the ∅ role in our case.
described above with a special node v_1 and a special
edge e*,∅ between v_1 and v0. Set the weight of e*,∅
to c∅ * = KIES g(s, ∅). We then set the weight of
the null edges ej,j+1,∅ to 0 and the weight of the
argument edges es,r to crs = g(s, r) − g(s, ∅).
</bodyText>
<figureCaption confidence="0.911747">
Proposition 1. There is a one-to-one correspon-
dence between paths in the lattice and global role
assignments with non-overlapping arguments. Fur-
thermore, the length of a path is equivalent to the
ILP score of the corresponding assignment.
</figureCaption>
<bodyText confidence="0.999222592592593">
Proof Sketch We already described how to con-
struct an assignment from any path through the lat-
tice. For any role assignment without overlaps we
can include all these edges in a single left-to-right
path, and complete the path with null edges. Since
there are no overlaps, we will not need to include
incompatible edges. So there is a one-to-one corre-
spondence between paths and valid assignments. To
see that the score is the same as the path length, we
can use induction on the number of non-null edges
in the path. Base case: If there are no selected argu-
ments, then the length of the path is just c∅* which
is exactly the ILP score. Inductive step: We add an
overt argument to the solution. In the path, we re-
place a sequence of null edges with an edge es,r. The
change in path length is crs = g(s, r) − g(s, ∅). In
the assignment, we need to change zs,∅ from 1 to 0
and zs,r from 0 to 1. Thus, the change in ILP score
is also g(s, r) − g(s, ∅).
The above construction can be further simplified.
Note that while the special edge e*,∅ is needed for the
direct correspondence with the ILP score, its weight
is constant across variable assignments. Thus, this
edge only adds a constant offset of c∅* to the ILP
solution. For the same reason, its presence has no
influence on the arg max or marginal computations
and we therefore drop it in our implementation.
</bodyText>
<subsectionHeader confidence="0.995755">
4.2 Unique Core Roles
</subsectionHeader>
<bodyText confidence="0.999709375">
To incorporate the unique core roles constraint, we
add state signatures to the vertices in the lattice and
restrict the edges accordingly. This increases the size
of the lattice by O(2|RC|), where RC is the set of
core roles. Our approach is similar to that of Tromble
and Eisner (2006), but whereas they suggest incorpo-
rating the uniqueness constraints incrementally, we
apply them all at once. This is necessary since we
</bodyText>
<figure confidence="0.989980363636364">
A0
expected.01
C-A1
A0
A1
0
0 0 0 0
0 0
A0
A1 A0/C-A1
It is expected to rain .
</figure>
<page confidence="0.991473">
33
</page>
<bodyText confidence="0.999103414634147">
seek to train a structured probabilistic model, which
requires the marginals with respect to the full set of
constraints.5 While the number of signatures is expo-
nential in |RC|, in practice this is a modest constant
as each frame only has a small number of possible
core roles (two or three for many frames).6 Further-
more, since many of the potential edges are pruned
by the constraints, as described below, the added
computational complexity is further reduced.
Lattice Structure The set of vertices are now V =
{v0, vn+1, vkj : j E [1, n] , k E {0,11|RC|1, where
v0 and vn+1 are the start and end vertices. The re-
maining vertices vkj are analogous to the ones in §4.1
but are annotated with a bit vector encoding the sub-
set of core roles that have been used so far. The rth
bit in the superscript k is set iff the rth core role has
been assigned at vkj . The null edges ekj,j+1,∅ connect
each node vkj to its successorvkj+1. Since a null edge
does not affect the core role assignment, the signature
k remains unchanged between vkj and vkj+1.
Figure 4 shows an example lattice, which in ad-
dition to the no span overlap and unique core roles
constraints encodes the core continuation roles con-
straint (see §4.3). For efficiency, we exclude vertices
and edges not on any path from v0 to vn+1. For exam-
ple, vk1 exist only for |k |G 1, since v0 corresponds to
no core roles being selected and a single span can add
at most one core role. Argument edges eks,r connect-
ing vertices vki−1and vk�
j corresponds to assigning
role r to the span s = wi, ... , wj. If r E RC then
k =� k0, otherwise k = k0. The edge is only included
if the role r is non-core, or if kr =� 1, to guarantee
uniqueness of core roles. By this construction, once
a core role has been assigned at a vertex vkj , it cannot
be assigned on any future path reachable from vkj .
Lattice Weights The edges are weighted in the
same way as in §4.1. It is easy to verify that the
structure enforces unique core roles, but is otherwise
equivalent to that in §4.1. Since the weights are iden-
tical, the proof of Proposition 1 carries over directly.
</bodyText>
<footnote confidence="0.9987376">
5We note that the approach of Riedel and Smith (2010) could
potentially be used to compute the marginals in an incremental
fashion similar to Tromble and Eisner (2006).
6 In the OntoNotes 5.0 development set, there are on average
10.4 core-role combinations per predicate frame.
</footnote>
<figureCaption confidence="0.8856865">
Figure 4: Lattice corresponding to the no span overlap,
unique core roles and core continuation roles constraints.
</figureCaption>
<bodyText confidence="0.8687575">
Each vertex is labeled with its signature k E {0,11|RC|;
in this example, “0, 1” equals {A0}. This represents the
subset of core-roles assigned on the path up to and includ-
ing the vertex. Dashed edges indicate the correct path.
</bodyText>
<subsectionHeader confidence="0.992013">
4.3 Core Continuation Roles
</subsectionHeader>
<bodyText confidence="0.9996633">
Recall that the constraint for continuation roles is
that they must occur after their corresponding base
role. We enforce this constraint for core roles by not
including argument edges eks,r with r E RN from a
configuration k which does not have the correspond-
ing base role set (kbase(r) =� 1). Figure 4 shows an
example; here the edge corresponding to “to rain”
with label C-A1 is included since the vertex signature
k = {1, 01 has kA1 = 1, but there is no correspond-
ing edge for k0 = {0, 01 since k0A1 = 0.
</bodyText>
<subsectionHeader confidence="0.995463">
4.4 Remaining Constraints
</subsectionHeader>
<bodyText confidence="0.990407466666667">
Unfortunately, enforcing the reference roles con-
straint, and the continuation roles constraint for non-
core roles, directly in the dynamic program is not
practical, due to combinatorial explosion. First, while
the continuation roles constraint almost only applies
to core roles,7 every role in RC U RA may have a
corresponding reference role. Second, even if we
restrict the constraints to core reference roles, the
lack of ordering between the spans in the constraint
means that we would have to represent all subsets of
RC x {r  |r E RR,base(r) E RC}.
However, these constraints are rarely violated in
practice. As we will see in §6, these remaining con-
straints can be enforced efficiently with k-best in-
ference in the constrained dynamic program from
</bodyText>
<footnote confidence="0.7327885">
7Less than 2% of continuation roles correspond to non-core
roles in the OntoNotes 5.0 development set.
</footnote>
<figure confidence="0.983805761904762">
0
A0
1,1 1,1
A0
0 0
0
0 0
1,0 1,0 1,0 1,0 1,0 1,0
C-A1
A1
0,1 0,1
0
A0
A0
0 0 0 0 0 0
0,0
0,0 0,0 0,0
0,0
0,0
0,0 0,0
It is expected to rain .
</figure>
<page confidence="0.871146">
34
</page>
<subsectionHeader confidence="0.244088">
Frame identification features
</subsectionHeader>
<listItem confidence="0.9965955">
• the predicate t • tag of t
• the lemma 2 • children words of t
• tag of t’s children • tag of t’s parent
• parent word of t • subcat. frame of t
• dep. label of t • dep. labels of t’s children
• word to the left of t • word to the right of t
• tag to the left of t • tag to the right of t
• word cluster of t • word clusters of t’s children
</listItem>
<tableCaption confidence="0.933020333333333">
Table 1: Frame identification features. By subcategoriza-
tion frame, we refer to the sequence of dependency labels
of t’s children in the dependency tree.
</tableCaption>
<bodyText confidence="0.966667333333333">
the previous section, using the algorithm of Huang
and Chiang (2005) and picking the best solution that
satisfies all the constraints.
</bodyText>
<sectionHeader confidence="0.980673" genericHeader="method">
5 Local and Structured Learning
</sectionHeader>
<bodyText confidence="0.99064396">
To train our models, we assume a training set where
each predicate t (with lemma f) in sentence x has
been identified and labeled with its semantic frame
f, as well as with each candidate span and role pair
(s, r) E S x R. We first consider a local log-linear
model. Let the local score of span s and role r be
given by g(s, r) = 0 · f(r, s, x, t, f, f), where 0 de-
notes the vector of model parameters and f(·) the
feature function (see Table 2 for the specific features
employed). We treat the local scores as the poten-
tials in a multiclass logistic regression model, such
that p(r  |s, x, t, f, f) a exp (g(s, r)), and estimate
the parameters by maximizing the regularized condi-
tional likelihood of the training set.
A downside of estimating the parameters locally
is that it “wastes” model capacity, in the sense that
the learning seeks to move probability mass away
from annotations that violate structural constraints
but can never be predicted at inference time. With
the dynamic program formulation from the previous
section, we can instead use a globally normalized
probabilistic model that takes the constraints from
§4.1-§4.3 into account during learning. To achieve
this, we model the probability of a joint assignment
Features additionally conjoined with the frame
</bodyText>
<listItem confidence="0.996603888888889">
• starting word of s • tag of the starting word of s
• ending word of s • tag of the ending word of s
• head word of s • tag of the head word of s
• bag of words in s • bag of tags in s
• a bias feature • cluster of s’s head
• dependency path between s’s head and t
• the set of dependency labels of t’s children
• dependency path conjoined with the tag of s’s head
• dep. path conjoined with the cluster of s’s head
• position of s w.r.t. t (before, after, overlap or same)
• position conjoined with distance from s to t
• subcategorization frame of s
• predicate use voice (active, passive, or unknown)
• whether the subject of t is missing (missingsubj)
• missingsubj, conjoined with the dependency path
between s’s head and t
• missingsubj, conjoined with the dependency path
between s’s head and the verb dominating t
</listItem>
<bodyText confidence="0.453401">
Features only conjoined with the role
</bodyText>
<listItem confidence="0.9935414">
• cluster of s’s head conjoined with cluster of t
• dep. path conjoined with the cluster of the head of s
• word of s’s head conj. with words of its children
• tag of s’s head conj. with words of its children
• cluster of s’s head conj. with cluster of its children
• cluster of t’s head conj. with cluster of s’s head
• word, tag, dependency label and cluster of the words
immediately to the left and right of s
• six features that each conjoin position and distance
with one of the following:
</listItem>
<bodyText confidence="0.787358">
tag, dependency label and cluster of s’s head,
tag, dependency label and cluster of t’s head
</bodyText>
<tableCaption confidence="0.720364">
Table 2: Semantic role labeling features. The argument
</tableCaption>
<bodyText confidence="0.65630425">
span is denoted by s, while t denotes the predicate token.
All features are conjoined with the role r. Features in the
top part have two versions, one conjoined with the role r
and one conjoined with both the role r and the frame f.
</bodyText>
<equation confidence="0.81860425">
z, subject to the constraints, as
p(z  |x, t, �, f ) a exp (1:
sES
s.t. z E {0,1}|S||R |, Az &lt; b,
</equation>
<bodyText confidence="0.999965">
where Az &lt; b encodes the subset of linear con-
straints from §3.4 that can be tractably enforced in
the dynamic program. In effect, p(z  |x, t, f, f) = 0
for any z that violates the constraints. We estimate
the parameters of this globally normalized model by
maximizing the regularized conditional likelihood of
</bodyText>
<equation confidence="0.914475666666667">
�g(s, r) x zs,r
1:
rER
</equation>
<page confidence="0.991897">
35
</page>
<bodyText confidence="0.999980954545455">
the training set, using the standard forward-backward
algorithm on the dynamic program lattice to compute
the required normalizer and feature expectations.
There have been several studies of the use of con-
strained MAP inference for semantic role labeling
on top of the predictions of local classifiers (Tromble
and Eisner, 2006; Punyakanok et al., 2008; Das et
al., 2012), as well as on ensembles for combining the
predictions of separate systems using integer linear
programming (Surdeanu et al., 2007; Punyakanok
et al., 2008).8 Meza-Ruiz and Riedel (2009) fur-
ther used a Markov Logic Network formulation to
incorporate a subset of these constraints during learn-
ing. Another popular approach has been to apply a
reranking model, which can incorporate soft struc-
tural constraints in the form of features, on top of
the k-best output of local classifiers (Toutanova et
al., 2008; Johansson and Nugues, 2008). However,
none of these methods provide any means to perform
efficient marginal inference and this work is the first
to use a globally normalized probabilistic model with
structural constraints for this task.
</bodyText>
<sectionHeader confidence="0.9856" genericHeader="method">
6 Empirical Study
</sectionHeader>
<bodyText confidence="0.999917">
We next present our experimental setup, datasets
used, preprocessing details and empirical results.
</bodyText>
<subsectionHeader confidence="0.989492">
6.1 Datasets and Evaluation
</subsectionHeader>
<bodyText confidence="0.98648818">
We measure experimental results on three datasets.
First, we use the CoNLL 2005 shared task data an-
notated according to PropBank conventions with the
standard training, development and test splits (Car-
reras and Màrquez, 2005). These were originally
constructed from sections 02-21, section 24 and sec-
tion 23 of the Wall Street Journal (WSJ) portion of
the Penn Treebank (Marcus et al., 1993). The Prop-
Bank I resource was used to construct the verb frame
lexicon for the CoNLL 2005 experiments.
Second, we perform experiments on a substan-
tially larger data set annotated according to PropBank
conventions, using the recent OntoNotes 5.0 corpus
(Weischedel et al., 2011), with the CoNLL 2012 train-
ing, development and test splits from Pradhan et al.
(2013). The frame lexicon for these experiments is
8While the dynamic program in §4 could be used to effi-
ciently implement such ensembles, since it solves the equivalent
ILP, our focus in this work is on learning a single accurate model.
derived from the OntoNotes frame files. This corpus
consists of nominal predicate-argument structure an-
notations in addition to verbs. Specifically, we use
version 12 downloaded from http://cemantix.
org/data/ontonotes.html, for which some
errors from the initial release used by Pradhan et al.
(2013) have been corrected.
Finally, we present results on FrameNet-annotated
data, where our setup mirrors that of Hermann et
al. (2014), who used the full-text annotations of the
FrameNet 1.5 release.9 We use the same training,
development and test splits as Hermann et al., which
consists of 39, 16 and 23 documents, respectively.
For evaluation on PropBank, we use the script
from the CoNLL 2005 shared task that measures role
labeling precision, recall and F1-score, as well as the
full argument structure accuracy.10 In the FrameNet
setting, we use a reimplementation of the SemEval
2007 shared task evaluation script that measures joint
frame-argument precision, recall and F1-score (Baker
et al., 2007). For consistency, we use a stricter mea-
sure of full structure accuracy than with PropBank
that gives credit only when both the predicted frame
and all of its arguments are correct.
The statistical significance of the observed differ-
ences between our different models is assessed with
a paired bootstrap test (Efron and Tibshirani, 1994),
using 1000 bootstrap samples. For brevity, we only
provide the p-values for the difference between our
best and second best models on the test set, as well
as between our second and third best models.
</bodyText>
<subsectionHeader confidence="0.998338">
6.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.99947225">
All corpora were preprocessed with a part-of-speech
tagger and a syntactic dependency parser, both of
which were trained on the CoNLL 2012 training split
extracted from OntoNotes 5.0 (Pradhan et al., 2013);
this training data has no overlap with any of the de-
velopment or test corpora used in our experiments.
The constituency trees in OntoNotes were converted
to Stanford dependencies before training our parser
(de Marneffe and Manning, 2013).
The part-of-speech tagger employs a second-order
conditional random field (Lafferty et al., 2001) with
the following features. Emission features: bias, the
</bodyText>
<footnote confidence="0.99778">
9http://framenet.icsi.berkeley.edu.
10http://www.lsi.upc.edu/~srlconll/srl-eval.pl
</footnote>
<page confidence="0.999035">
36
</page>
<bodyText confidence="0.999941">
word, the cluster of the word, suffixes of lengths 1 to
4, the capitalization shape of the word, whether the
word contains a hyphen and the identity of the last
word in the sentence. Transition features: the tag
bigram, the tag bigram conjoined with, respectively,
the clusters of the current and the previous words,
the tag trigram and the tag trigram conjoined with,
respectively, the clusters of the current and previous
word, as well as with the word two positions back.
For syntactic dependencies, we use the parser and
features described by Zhang and McDonald (2014),
which exploits structural diversity in cube-pruning
to improve higher-order graph-based inference. On
the WSJ development set (section 22), the labeled
attachment score of the parser is 90.9% while the
part-of-speech tagger achieves an accuracy of 97.2%
on the same dataset. On the OntoNotes development
set, the corresponding scores are 90.2% and 97.3%.
Both the tagger and the parser, as well as the frame
identification and role labeling models (see Tables 1
and 2), have features based on word clusters. Specifi-
cally, we use the clusters with 1000 classes described
by Turian et al. (2010), which are induced with the
Brown algorithm (Brown et al., 1992).
</bodyText>
<subsectionHeader confidence="0.998768">
6.3 Candidate Argument Extraction
</subsectionHeader>
<bodyText confidence="0.996030232142857">
We use a rule-based heuristic to extract candidate
arguments for role labeling. Most prior work on
PropBank-style semantic role labeling have relied on
constituency syntax for candidate argument extrac-
tion. Instead, we rely on dependency syntax, which
allows faster preprocessing and potential extension
to the many languages for which only dependency
annotations are available. To this end, we adapt the
constituency-based candidate argument extraction
method of Xue and Palmer (2004) to dependencies.
In gold PropBank annotations, syntactic con-
stituents serve as arguments in all constructions.
However, extracting constituents from a dependency
tree is not straightforward. The full dependency sub-
tree under a particular head word often merges syn-
tactic constituents. For example, in the tree fragment
The man who knew too much
the dependency tree has the full clause as the subtree
headed by man, making it non-trivial to extract a
partial subtree underneath it that could serve as a
valid argument (for example, The man).
In our candidate argument extraction algorithm,
first, we select all the children subtrees of a given
predicate as potential arguments; if a child word
is connected via the conj (conjunction) or the prep
(preposition) label, we also select the corresponding
grand-children subtrees. Next, we climb up to the
predicate’s syntactic parent and add any partial sub-
trees headed by it that could serve as constituents in
the corresponding phrase-structure tree. To capture
such constructions, we select partial subtrees for a
head word by first adding the head word, then adding
contiguous child subtrees from the head word’s right-
most left child towards the leftmost left child until we
either reach the predicate word or an offensive depen-
dency label.11 This procedure is then symmetrically
applied to the head word’s right children. Once a par-
tial subtree has been added, we add the parent word’s
children subtrees — and potentially grandchildren
subtrees in case of children labeled as conj or prep —
to the candidate list, akin to the first step. We apply
this parent operation recursively for all the ancestors
of the predicate. Finally, we consider the predicate’s
syntactic parent word as a candidate argument if the
predicate is connected to it via the amod label.
The candidates are further filtered to only keep
those where the role of the argument, conjoined with
the path from its head to the predicate, has been ob-
served in the training data. This algorithm obtains an
unlabeled argument recall of 88.2% on the OntoNotes
5.0 development data, with a precision of 38.2%.
For FrameNet, we use the extraction method of
Hermann et al. (2014, §5.4), which is also inspired by
Xue and Palmer (2004). On the FrameNet develop-
ment data, this method obtains an unlabeled argument
recall of 72.6%, with a precision of 25.1%.12
</bodyText>
<subsectionHeader confidence="0.998352">
6.4 Baseline Systems
</subsectionHeader>
<bodyText confidence="0.968153666666666">
We compare our local and structured models to the
top performing constituency-based systems from the
11All but the following labels are treated as offensive: advmod,
amod, appos, aux, auxpass, cc, conj, dep, det, mwe, neg, nn,
npadvmod, num, number, poss, preconj, predet, prep, prt, ps,
quantmod and tmod.
12The low recall on FrameNet suggests that a deeper analysis
of missed arguments is necessary. However, to allow a fair
comparison with prior work, we leave this for future work.
</bodyText>
<figure confidence="0.961239833333333">
det
root
rcmod
nsubj
dobj
advmod
</figure>
<page confidence="0.995508">
37
</page>
<table confidence="0.9999005625">
Method Development WSJ Test Brown Test
Prec. Recall F1 Comp. Prec. Recall F1 Comp. Prec. Recall F1 Comp.
Local/Local 80.0 75.2 77.5 51.5 81.6 76.6 79.0 53.1 73.7 68.1 70.8 ** 39.1
Local/DP 81.3 74.8 77.9 52.4 82.6 76.4 79.3* 54.3* 74.0 66.8 70.2 38.4
Structured/DP 76.2 78.6 54.4 82.3 77.6 79.9* 56.0* 74.3 68.6 71.3 39.8
81.2
Prior work Prec. Recall F1 Comp. Prec. Recall F1 Comp. Prec. Recall F1 Comp.
Surdeanu – – – – 79.7 74.9 77.2 52.0 – – – –
Punyakanok – – – – 77.1 75.5 76.3 – – – – –
Toutanova – – 77.9 57.2 – – 79.7 58.7 – – 67.8 39.4
Ensembles Prec. Recall F1 Comp. Prec. Recall F1 Comp. Prec. Recall F1 Comp.
Surdeanu – – – – 87.5 74.7 80.6 51.7 81.8 61.3 70.1 34.3
Punyakanok 80.1 74.8 77.4 50.7 76.8 53.8 62.9 67.8 32.3
Toutanova – – 78.6 58.7 78.8 60.1 – 68.8 40.8
82.3 79.4 73.4
81.9 80.3 –
</table>
<tableCaption confidence="0.900721">
Table 3: Semantic role labeling results on the CoNLL 2005 data set. The method labels are training/inference. For
example, Local/DP means training with the local model, but inference with the dynamic program. Bold font indicates
the best system using a single model and a single parse, while the best scores among all systems are underlined.
Statistical significance was assessed for F1 and Comp. on the WSJ and Brown test sets with p &lt; 0.01* and p &lt; 0.05**.
</tableCaption>
<bodyText confidence="0.999927628571429">
literature on the CoNLL 2005 datasets. To facilitate
a more nuanced comparison, we distinguish between
prior work based on single systems, which use a
single input parse and no model combination, and
ensemble-based systems. For single systems, our
first baseline is the strongest non-ensemble system
presented by Surdeanu et al. (2007) that treats the
SRL problem as a sequential tagging task (see §4.1 of
the cited paper). Next, we consider the non-ensemble
system presented by Punyakanok et al. (2008) that
trains local classifiers and uses an ILP to satisfy the
structural constraints; this system is most similar to
our approach, but is trained locally. Finally, our third
single system baseline is the model of Toutanova et
al. (2008) that uses a tree structured dynamic pro-
gram that assumes that all candidate spans are nested;
this system relies on global features in a reranking
framework (see row 2 of Figure 19 of the cited paper).
These authors also report ensemble-based variants
that combine the outputs of multiple SRL systems in
various ways; as observed in other NLP problems, the
ensemble systems outperformed the single-system
counterparts, and are state of the art. To situate our
models with these ensemble-based approaches, we
include them in Table 3.
For the OntoNotes datasets, we compare our mod-
els to Pradhan et al. (2013), who report results with a
variant of the (non-ensemble) ASSERT system (Prad-
han et al., 2005). These are the only previously re-
ported results for the SRL problem on this dataset.
Finally, for the FrameNet experiments, our base-
line is the state-of-the-art system of Hermann et al.
(2014), which combines a frame-identification model
based on WSABIE (Weston et al., 2011) with a log-
linear role labeling model.
</bodyText>
<subsectionHeader confidence="0.989155">
6.5 Hyperparameters
</subsectionHeader>
<bodyText confidence="0.999990166666667">
The l1 and l2 regularization weights for the frame
identification and role labeling models for all experi-
ments were tuned on the OntoNotes development
data. For frame identification, the regularization
weights are set to 0 and 0.1, while for semantic role
labeling they are set to 0.1 and 1.0, respectively.
</bodyText>
<subsectionHeader confidence="0.727614">
6.6 Results
</subsectionHeader>
<bodyText confidence="0.998350818181818">
Table 3 shows our results on the CoNLL 2005 de-
velopment set as well as the WSJ and Brown test
sets.13 Our structured model achieves the highest
F1-score among the non-ensemble systems, outper-
forming even the ensemble systems on the Brown
13We also experimented with a parser trained only on the WSJ
training set. This results in a drop in role labeling F1-score of
0.3% (absolute) averaged across models on the CoNLL 2005
development set. The corresponding drop for the structured
model is 0.6%, which suggests that it benefits more from parser
improvements compared to the local models.
</bodyText>
<page confidence="0.995988">
38
</page>
<table confidence="0.999677083333333">
Development
Method Prec. Recall F1 Comp.
Local/Local 79.5 77.0 78.2 57.4
Local/DP 80.6 77.1 78.8 59.0
Structured/DP 80.5 77.8 79.1 60.1
CoNLL 2012 Test
Method Prec. Recall F1 Comp.
Local/Local 79.8 77.7 78.7 59.5
Local/DP 80.9 77.7 79.2* 60.9*
Structured/DP 80.6 78.2 79.4* 61.8*
Pradhan 81.3 70.5 75.5 51.7
Pradhan (revised) 78.5 76.6 77.5 55.8
</table>
<tableCaption confidence="0.891147">
Table 4: Semantic role labeling results on the OntoNotes
5.0 development and test sets from CoNLL 2012. “Prad-
</tableCaption>
<figureCaption confidence="0.753337">
han” is the Overall results from Table 5 of Pradhan et al.
(2013). “Pradhan (revised)” are corrected results from per-
sonal communication with Pradhan et al. (see footnote 14
for details). Statistical significance was assessed for F1
and Comp. on the test set with p &lt; 0.01*.
</figureCaption>
<bodyText confidence="0.999342038461538">
test set, while performing at par on the development
set. Overall, using structured learning improves re-
call at a slight expense of precision when compared
to local learning. This leads to a higher F1-score and
a substantial increase in complete argument structure
accuracy (Comp. in the tables). The increase in recall
is to be expected, since during training the structured
model can rely on the constraints to eliminate some
hypotheses. This has the effect of alleviating some of
the label imbalance seen in the training data (recall
that the model encounters roughly four times as many
null roles as non-null role assignments). While the
results on the WSJ test set are highly statistically sig-
nificant, the small size of the Brown test set give rise
to a larger variance; results here are only significant
at a level of p ≈ 0.1 for F1 and p ≈ 0.2 for Comp.
Table 4 shows the semantic role labeling results on
the OntoNotes data. We observe the same trend as we
did on the CoNLL 2005 data from Table 3. Adding
constraints at inference time notably improves pre-
cision at virtually no cost to recall. Structured learn-
ing additionally increases recall at a small cost to
precision and yields the best results both in terms
of F1- and complete analysis scores. These results
are all highly statistically significant. Compared to
the results of Pradhan et al. (2013), our structured
</bodyText>
<table confidence="0.99952125">
Method Development
Prec. Recall F1 Comp.
Local/Local 80.5 62.7 70.5 31.3
Local/DP 80.7 62.9 70.7 31.2
Structured/DP 79.6 64.1 71.0 32.6
Hermann 78.3 64.5 70.8 –
Test
Method Prec. Recall F1 Comp.
Local/Local 75.9 64.5 69.7 32.8
Local/DP 76.1 64.9 70.1* 33.0
Structured/DP 75.4 65.8 70.3** 33.8***
Hermann 74.3 66.0 69.9 –
</table>
<tableCaption confidence="0.85697">
Table 5: Full structure prediction results (joint frame
identification and semantic role labeling performance)
</tableCaption>
<bodyText confidence="0.944448461538462">
for FrameNet. All systems use the WSABIE model from
Hermann et al. (2014) for the frame identification step.
“Hermann” is the Wsabie Embedding results from Table
3 of Hermann et al. (2014). Statistical significance was
assessed for F1 and Comp. on the test set with p &lt; 0.01*,
p &lt; 0.05** and p &lt; 0.075***.
model yields a 15% relative error reduction in terms
of F1-score and a 20% reduction in terms of com-
plete analysis score.14 The frame identification accu-
racies on the OntoNotes development and test set are
94.5% and 94.9%, respectively, whereas Pradhan et
al. (2013) report an accuracy of 92.8% on the test set;
this represents almost a 30% relative error reduction.
Finally, Table 5 shows the results on the FrameNet
data. While structured learning helps less here com-
pared to the PropBank setting, our model outper-
forms the prior state-of-the-art model of Hermann
et al. (2014) and we obtain a modest improvement
in complete analysis score compared to local train-
ing. Due to the small size of the FrameNet test set,
similarly to the Brown test set, we observe a larger
variance across bootstrap samples, but in this case the
results are statistically significant to a larger degree.
Table 6 relates the speed of the various inference
algorithms to the number of constraint violations.
The time is relative to local inference; it excludes the
</bodyText>
<footnote confidence="0.5481956">
14Unfortunately, these results are not strictly comparable, due
to errors in the original release of the data that was used by
Pradhan et al. (2013). Results with Pradhan et al.’s system on the
corrected release, obtained from personal communication with
Pradhan et al., are included in Table 4 as “Pradhan (revised)”.
</footnote>
<page confidence="0.998702">
39
</page>
<bodyText confidence="0.999990517241379">
time of feature extraction and computation of g(s, r),
which is the same across inference methods. Similar
to Tromble and Eisner (2006), for all algorithms, we
first use the local solution without constraints and
only apply the constraints in the case of a violation.
Removing this optimization results in a slowdown
across the board by a factor of about 5 and does
not change the ranking of the methods. Since the
structured model has identical parameterization to
the local model, optimality is guaranteed even when
using this scheme with the former. We report the re-
sults of two ILP solvers: SCIP15 and Gurobi.16 SCIP
is a factor of 8 slower than Gurobi for this problem,
while Gurobi is a further factor of about 4 slower
than our dynamic program. The penultimate line of
Table 6 shows the result of using an LP-relaxation in-
stead of the ILP. This does not come with optimality
guarantees, but is included for completeness.
Finally, when using k-best inference to satisfy the
reference roles and non-core continuation roles con-
straints in the dynamic program (§4.4), the maximum
value of k is 80 on the OntoNotes development set.
Across data points for which such k-best inference
is necessary, the average k is found to be 1.8. If we
allow ourselves to ignore these constraints, we can
avoid k-best inference and achieve a further speedup,
as shown in the last line of Table 6. The heuristics of
Toutanova et al. (2008) could potentially be used as
an alternative way of satisfying these constraints.
</bodyText>
<sectionHeader confidence="0.999125" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999985571428572">
We described a dynamic program for constrained
inference in semantic role labeling that efficiently
enforces a majority of structural constraints, given
potentially overlapping candidate arguments. The dy-
namic program provably finds the optimal solutions
of a corresponding ILP and in practice requires a frac-
tion of the computational cost compared to an highly
optimized off-the-shelf ILP solver, which has typi-
cally been used for this problem. Furthermore, the
dynamic program facilitates learning with a globally
normalized log-linear model and provides a proba-
bilistic measure of confidence in predictions. Empir-
ically, we showed a four-fold speedup in inference
time compared to a state-of-the-art ILP solver and
</bodyText>
<footnote confidence="0.9986445">
15http://scip.zib.de/
16http://www.gurobi.com/
</footnote>
<table confidence="0.98784075">
Number of constraint violations
Method Time overlap unique cont. ref.
ILP-SCIP 198.7 0 0 0 0
ILP-Gurobi 25.0 0 0 0 0
DP k-best 6.2 0 0 0 0
Local 1.0 162 1725 63 297
LP-Gurobi 23.0 6 0 0 0
DP no k-best 4.0 0 0 0 272
</table>
<tableCaption confidence="0.992366666666667">
Table 6: Speed and constraint violation results on the
OntoNotes 5.0 development set. Exact and approximate
methods are shown above and below the line, respectively.
</tableCaption>
<bodyText confidence="0.994645">
by using structured learning our model outperforms
all comparable non-ensemble baselines on both Prop-
Bank and FrameNet data sets.
</bodyText>
<sectionHeader confidence="0.99902" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999765">
We thank Ryan McDonald, Emily Pitler, Slav Petrov
and Fernando Pereira for their detailed comments. In
particular, Ryan pointed out a simplification that im-
proved on our original dynamic program formulation.
We also thank Sameer Pradhan for his corrections to
the OntoNotes data. Finally, we thank André Mar-
tins for numerous discussions on this subject and the
anonymous reviewers for their insightful comments.
</bodyText>
<sectionHeader confidence="0.998988" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99982695">
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceedings
of ACL.
Collin Baker, Michael Ellsworth, and Katrin Erk. 2007.
Semeval-2007 task 19: Frame semantic structure ex-
traction. In Proceedings of SemEval.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4).
Xavier Carreras and Lluís Màrquez. 2004. Introduction to
the CoNLL-2004 shared task: Semantic role labeling.
In Proceedings of CoNLL.
Xavier Carreras and Lluís Màrquez. 2005. Introduction to
the CoNLL-2005 shared task: Semantic role labeling.
In Proceedings of CoNLL.
Dipanjan Das, André F. T. Martins, and Noah A. Smith.
2012. An exact dual decomposition algorithm for shal-
low semantic parsing with constraints. In Proceedings
of *SEM.
</reference>
<page confidence="0.966983">
40
</page>
<reference confidence="0.999902371428572">
Dipanjan Das, Desai Chen, André F. T. Martins, Nathan
Schneider, and Noah A. Smith. 2014. Frame-semantic
parsing. Computational Linguistics, 40(1):9–56.
Marie-Catherine de Marneffe and Christopher D. Man-
ning, 2013. Stanford typed dependencies manual.
Bradley Efron and Robert J Tibshirani. 1994. An intro-
duction to the bootstrap. CRC press.
Matthew Gerber and Joyce Y. Chai. 2010. Beyond Nom-
Bank: A study of implicit arguments for nominal predi-
cates. In Proceedings of ACL.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic
labeling of semantic roles. Computational Linguistics,
28(3):245–288.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Antònia Martí, Lluís
Màrquez, Adam Meyers, Joakim Nivre, Sebastian Padó,
Jan Štˇepánek, Pavel Straˇnák, Mihai Surdeanu, Nianwen
Xue, and Yi Zhang. 2009. The CoNLL-2009 shared
task: Syntactic and semantic dependencies in multiple
languages. In Proceedings of CoNLL.
Karl Moritz Hermann, Dipanjan Das, Jason Weston, and
Kuzman Ganchev. 2014. Semantic frame identification
with distributed word representations. In Proceedings
of ACL.
Liang Huang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of PropBank.
In Proceedings of EMNLP.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proceed-
ings of ICML.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated corpus
of English: the Penn treebank. Computational Linguis-
tics, 19(2):313–330.
Lluís Màrquez, Xavier Carreras, Kenneth C. Litkowski,
and Suzanne Stevenson. 2008. Semantic role labeling:
An introduction to the special issue. Computational
Linguistics, 34(2):145–159.
André F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar,
and Mário A. T. Figueiredo. 2011. Dual decomposition
with many overlapping components. In Proceedings of
EMNLP.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In Proceedings of NAACL/HLT Workshop on
Frontiers in Corpus Annotation.
Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly iden-
tifying predicates, arguments and senses using Markov
Logic. In Proceedings of NAACL-HLT.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005.
The Proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71–106.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2005.
Support vector learning for semantic argument classifi-
cation. Machine Learning, 60(1-3):11–39.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Tou Hwee Ng, Anders Björkelund, Olga Uryupina,
Yuchen Zhang, and Zhi Zhong. 2013. Towards robust
linguistic analysis using OntoNotes. In Proceedings of
CoNLL.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2):257–287.
Sebastian Riedel and David A. Smith. 2010. Relaxed
marginal inference and its application to dependency
parsing. In Proceedings of NAACL-HLT.
Mihai Surdeanu, Lluís Màrquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination strategies for
semantic role labeling. Journal ofArtificial Intelligence
Research, 29(1):105–151.
Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluís
Màrquez, and Joakim Nivre. 2008. The CoNLL 2008
shared task on joint parsing of syntactic and semantic
dependencies. In Proceedings of CoNLL.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic role
labeling. Computational Linguistics, 34(2):161–191.
Roy W. Tromble and Jason Eisner. 2006. A fast finite-
state relaxation method for enforcing global constraints
on sequence decoding. In Proceedings of NAACL-HLT.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: A simple and general method for
semi-supervised learning. In Proceedings of ACL.
Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch
Marcus, Robert Belvin, Sameer Pradhan, Lance
Ramshaw, and Nianwen Xue. 2011. OntoNotes: A
large training corpus for enhanced processing. In
J. Olive, C. Christianson, and J. McCary, editors, Hand-
book of Natural Language Processing and Machine
Translation. Springer.
Jason Weston, Samy Bengio, and Nicolas Usunier. 2011.
WSABIE: Scaling up to large vocabulary image anno-
tation. In Proceedings of IJCAI.
Nianwen Xue and Martha Palmer. 2004. Calibrating
features for semantic role labeling. In Proceedings of
EMNLP.
Hao Zhang and Ryan McDonald. 2014. Enforcing struc-
tural diversity in cube-pruned dependency parsing. In
Proceedings of ACL.
</reference>
<page confidence="0.9997065">
41
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.574325">
<title confidence="0.96441325">Efficient Inference and Structured for Semantic Role Labeling Oscar Täckström Kuzman Ganchev Dipanjan Das Google Google Google</title>
<author confidence="0.967641">New York New York New</author>
<email confidence="0.99817">oscart@google.comkuzman@google.comdipanjand@google.com</email>
<abstract confidence="0.980269611111111">We present a dynamic programming algorithm for efficient constrained inference in semantic role labeling. The algorithm tractably captures a majority of the structural constraints examined by prior work in this area, which has resorted to either approximate methods or off-theshelf integer linear programming solvers. In addition, it allows training a globally-normalized log-linear model with respect to constrained conditional likelihood. We show that the dynamic program is several times faster than an off-the-shelf integer linear programming solver, while reaching the same solution. Furthermore, we show that our structured model results in significant improvements over its local counterpart, achieving state-of-the-art results on both PropBankand FrameNet-annotated corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Collin F Baker</author>
<author>Charles J Fillmore</author>
<author>John B Lowe</author>
</authors>
<title>The Berkeley FrameNet project.</title>
<date>1998</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="6165" citStr="Baker et al., 1998" startWordPosition="926" endWordPosition="929">the OntoNotes 5.0 corpora annotated with semantic roles for both verbal and nominal predicates (Weischedel et al., 2011) and strongly outperform the prior state of the art (Pradhan et al., 2013). Finally, we present results on FrameNet 1.5 data, again achieving state-of-the-art results. 2 Task Overview We seek to predict the semantic argument structure of predicates in text. For brevity and practical reasons, the exposition and empirical study is primarily focused on PropBank-style annotations (Palmer et al., 2005). However, our approach applies directly to FrameNet-style annotations as well (Baker et al., 1998) and as shown empirically in §6, a similar trend Figure 2: Examples showing continuation and reference roles according to PropBank. The role prefix C- indicates continuation of an argument, while the prefix R- indicates reference to another overt argument of the same predicate. holds across both types of annotation. In both cases, we are provided with a frame lexicon that contains type-level information for lexical units (a lemma conjoined with a coarse-grained part-ofspeech tag).1 For each lexical unit, a list of senses, or frames, are provided, where each frame comes with a set of semantic r</context>
</contexts>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Collin Baker</author>
<author>Michael Ellsworth</author>
<author>Katrin Erk</author>
</authors>
<title>Semeval-2007 task 19: Frame semantic structure extraction.</title>
<date>2007</date>
<booktitle>In Proceedings of SemEval.</booktitle>
<contexts>
<context position="32679" citStr="Baker et al., 2007" startWordPosition="5611" endWordPosition="5614">ur setup mirrors that of Hermann et al. (2014), who used the full-text annotations of the FrameNet 1.5 release.9 We use the same training, development and test splits as Hermann et al., which consists of 39, 16 and 23 documents, respectively. For evaluation on PropBank, we use the script from the CoNLL 2005 shared task that measures role labeling precision, recall and F1-score, as well as the full argument structure accuracy.10 In the FrameNet setting, we use a reimplementation of the SemEval 2007 shared task evaluation script that measures joint frame-argument precision, recall and F1-score (Baker et al., 2007). For consistency, we use a stricter measure of full structure accuracy than with PropBank that gives credit only when both the predicted frame and all of its arguments are correct. The statistical significance of the observed differences between our different models is assessed with a paired bootstrap test (Efron and Tibshirani, 1994), using 1000 bootstrap samples. For brevity, we only provide the p-values for the difference between our best and second best models on the test set, as well as between our second and third best models. 6.2 Preprocessing All corpora were preprocessed with a part-</context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>Collin Baker, Michael Ellsworth, and Katrin Erk. 2007. Semeval-2007 task 19: Frame semantic structure extraction. In Proceedings of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V deSouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Classbased n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="35157" citStr="Brown et al., 1992" startWordPosition="5997" endWordPosition="6000">cube-pruning to improve higher-order graph-based inference. On the WSJ development set (section 22), the labeled attachment score of the parser is 90.9% while the part-of-speech tagger achieves an accuracy of 97.2% on the same dataset. On the OntoNotes development set, the corresponding scores are 90.2% and 97.3%. Both the tagger and the parser, as well as the frame identification and role labeling models (see Tables 1 and 2), have features based on word clusters. Specifically, we use the clusters with 1000 classes described by Turian et al. (2010), which are induced with the Brown algorithm (Brown et al., 1992). 6.3 Candidate Argument Extraction We use a rule-based heuristic to extract candidate arguments for role labeling. Most prior work on PropBank-style semantic role labeling have relied on constituency syntax for candidate argument extraction. Instead, we rely on dependency syntax, which allows faster preprocessing and potential extension to the many languages for which only dependency annotations are available. To this end, we adapt the constituency-based candidate argument extraction method of Xue and Palmer (2004) to dependencies. In gold PropBank annotations, syntactic constituents serve as</context>
</contexts>
<marker>Brown, deSouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Classbased n-gram models of natural language. Computational Linguistics, 18(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluís Màrquez</author>
</authors>
<title>Introduction to the CoNLL-2004 shared task: Semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="1474" citStr="Carreras and Màrquez, 2004" startWordPosition="207" endWordPosition="210">structured model results in significant improvements over its local counterpart, achieving state-of-the-art results on both PropBank- and FrameNet-annotated corpora. 1 Introduction Semantic role labeling (henceforth, SRL) is the task of identifying the semantic arguments of predicates in natural language text. Pioneered by Gildea and Jurafsky (2002), this task has been widely investigated by the NLP community. There have been two shared tasks at CoNLL 2004 and 2005 focusing on this problem, using PropBank conventions to identify the phrasal arguments of verbal predicates (Palmer et al., 2005; Carreras and Màrquez, 2004, 2005). Since then, there has been work on SRL for nominal predicates (Meyers et al., 2004; Gerber and Chai, 2010) and variants that investigated the prediction of semantic dependencies rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Here, we present an inference method for SRL, addressing the problem of phrasal argument structure prediction (as opposed to semantic dependencies). In contrast to most prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and Riedel (2009), our framework first performs frame i</context>
</contexts>
<marker>Carreras, Màrquez, 2004</marker>
<rawString>Xavier Carreras and Lluís Màrquez. 2004. Introduction to the CoNLL-2004 shared task: Semantic role labeling. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluís Màrquez</author>
</authors>
<title>Introduction to the CoNLL-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="30913" citStr="Carreras and Màrquez, 2005" startWordPosition="5331" endWordPosition="5335">nova et al., 2008; Johansson and Nugues, 2008). However, none of these methods provide any means to perform efficient marginal inference and this work is the first to use a globally normalized probabilistic model with structural constraints for this task. 6 Empirical Study We next present our experimental setup, datasets used, preprocessing details and empirical results. 6.1 Datasets and Evaluation We measure experimental results on three datasets. First, we use the CoNLL 2005 shared task data annotated according to PropBank conventions with the standard training, development and test splits (Carreras and Màrquez, 2005). These were originally constructed from sections 02-21, section 24 and section 23 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993). The PropBank I resource was used to construct the verb frame lexicon for the CoNLL 2005 experiments. Second, we perform experiments on a substantially larger data set annotated according to PropBank conventions, using the recent OntoNotes 5.0 corpus (Weischedel et al., 2011), with the CoNLL 2012 training, development and test splits from Pradhan et al. (2013). The frame lexicon for these experiments is 8While the dynamic program</context>
</contexts>
<marker>Carreras, Màrquez, 2005</marker>
<rawString>Xavier Carreras and Lluís Màrquez. 2005. Introduction to the CoNLL-2005 shared task: Semantic role labeling. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>André F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>An exact dual decomposition algorithm for shallow semantic parsing with constraints.</title>
<date>2012</date>
<booktitle>In Proceedings of *SEM.</booktitle>
<contexts>
<context position="29796" citStr="Das et al., 2012" startWordPosition="5160" endWordPosition="5163">orced in the dynamic program. In effect, p(z |x, t, f, f) = 0 for any z that violates the constraints. We estimate the parameters of this globally normalized model by maximizing the regularized conditional likelihood of �g(s, r) x zs,r 1: rER 35 the training set, using the standard forward-backward algorithm on the dynamic program lattice to compute the required normalizer and feature expectations. There have been several studies of the use of constrained MAP inference for semantic role labeling on top of the predictions of local classifiers (Tromble and Eisner, 2006; Punyakanok et al., 2008; Das et al., 2012), as well as on ensembles for combining the predictions of separate systems using integer linear programming (Surdeanu et al., 2007; Punyakanok et al., 2008).8 Meza-Ruiz and Riedel (2009) further used a Markov Logic Network formulation to incorporate a subset of these constraints during learning. Another popular approach has been to apply a reranking model, which can incorporate soft structural constraints in the form of features, on top of the k-best output of local classifiers (Toutanova et al., 2008; Johansson and Nugues, 2008). However, none of these methods provide any means to perform ef</context>
</contexts>
<marker>Das, Martins, Smith, 2012</marker>
<rawString>Dipanjan Das, André F. T. Martins, and Noah A. Smith. 2012. An exact dual decomposition algorithm for shallow semantic parsing with constraints. In Proceedings of *SEM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Desai Chen</author>
<author>André F T Martins</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Frame-semantic parsing.</title>
<date>2014</date>
<journal>Computational Linguistics,</journal>
<volume>40</volume>
<issue>1</issue>
<contexts>
<context position="2504" citStr="Das et al., 2014" startWordPosition="364" endWordPosition="367"> contrast to most prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and Riedel (2009), our framework first performs frame identification, the subtask of disambiguating the predicate frame; this makes our analysis more interpretable. The focus of this paper, however, is the subtask of semantic role labeling, wherein we take a set of (potentially overlapping) candidate sentential phrases and identify and label them with the semantic roles associated with the predicted frame. This treatment is commonly used in frame semantic parsing (Das et al., 2014; Hermann et al., 2014) and our two-stage framework is able to model both PropBank and FrameNet conventions. Previous work focusing on semantic role labeling imposed several structural constraints warranted by the annotation conventions of the task and other linguistic considerations, such as avoiding overlapping arguments and repeated core roles in the final prediction. Such global inference often leads to improved results and more meaningful predictions compared to local unconstrained methods (Màrquez et al., 2008). A popular framework for imposing these constraints has been integer linear p</context>
</contexts>
<marker>Das, Chen, Martins, Schneider, Smith, 2014</marker>
<rawString>Dipanjan Das, Desai Chen, André F. T. Martins, Nathan Schneider, and Noah A. Smith. 2014. Frame-semantic parsing. Computational Linguistics, 40(1):9–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<date>2013</date>
<note>Stanford typed dependencies manual.</note>
<marker>de Marneffe, Manning, 2013</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning, 2013. Stanford typed dependencies manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Efron</author>
<author>Robert J Tibshirani</author>
</authors>
<title>An introduction to the bootstrap.</title>
<date>1994</date>
<publisher>CRC press.</publisher>
<contexts>
<context position="33016" citStr="Efron and Tibshirani, 1994" startWordPosition="5665" endWordPosition="5668">sures role labeling precision, recall and F1-score, as well as the full argument structure accuracy.10 In the FrameNet setting, we use a reimplementation of the SemEval 2007 shared task evaluation script that measures joint frame-argument precision, recall and F1-score (Baker et al., 2007). For consistency, we use a stricter measure of full structure accuracy than with PropBank that gives credit only when both the predicted frame and all of its arguments are correct. The statistical significance of the observed differences between our different models is assessed with a paired bootstrap test (Efron and Tibshirani, 1994), using 1000 bootstrap samples. For brevity, we only provide the p-values for the difference between our best and second best models on the test set, as well as between our second and third best models. 6.2 Preprocessing All corpora were preprocessed with a part-of-speech tagger and a syntactic dependency parser, both of which were trained on the CoNLL 2012 training split extracted from OntoNotes 5.0 (Pradhan et al., 2013); this training data has no overlap with any of the development or test corpora used in our experiments. The constituency trees in OntoNotes were converted to Stanford depend</context>
</contexts>
<marker>Efron, Tibshirani, 1994</marker>
<rawString>Bradley Efron and Robert J Tibshirani. 1994. An introduction to the bootstrap. CRC press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Gerber</author>
<author>Joyce Y Chai</author>
</authors>
<title>Beyond NomBank: A study of implicit arguments for nominal predicates.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="1589" citStr="Gerber and Chai, 2010" startWordPosition="228" endWordPosition="231">both PropBank- and FrameNet-annotated corpora. 1 Introduction Semantic role labeling (henceforth, SRL) is the task of identifying the semantic arguments of predicates in natural language text. Pioneered by Gildea and Jurafsky (2002), this task has been widely investigated by the NLP community. There have been two shared tasks at CoNLL 2004 and 2005 focusing on this problem, using PropBank conventions to identify the phrasal arguments of verbal predicates (Palmer et al., 2005; Carreras and Màrquez, 2004, 2005). Since then, there has been work on SRL for nominal predicates (Meyers et al., 2004; Gerber and Chai, 2010) and variants that investigated the prediction of semantic dependencies rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Here, we present an inference method for SRL, addressing the problem of phrasal argument structure prediction (as opposed to semantic dependencies). In contrast to most prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and Riedel (2009), our framework first performs frame identification, the subtask of disambiguating the predicate frame; this makes our analysis more interpretable. The f</context>
</contexts>
<marker>Gerber, Chai, 2010</marker>
<rawString>Matthew Gerber and Joyce Y. Chai. 2010. Beyond NomBank: A study of implicit arguments for nominal predicates. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1199" citStr="Gildea and Jurafsky (2002)" startWordPosition="162" endWordPosition="165">ining a globally-normalized log-linear model with respect to constrained conditional likelihood. We show that the dynamic program is several times faster than an off-the-shelf integer linear programming solver, while reaching the same solution. Furthermore, we show that our structured model results in significant improvements over its local counterpart, achieving state-of-the-art results on both PropBank- and FrameNet-annotated corpora. 1 Introduction Semantic role labeling (henceforth, SRL) is the task of identifying the semantic arguments of predicates in natural language text. Pioneered by Gildea and Jurafsky (2002), this task has been widely investigated by the NLP community. There have been two shared tasks at CoNLL 2004 and 2005 focusing on this problem, using PropBank conventions to identify the phrasal arguments of verbal predicates (Palmer et al., 2005; Carreras and Màrquez, 2004, 2005). Since then, there has been work on SRL for nominal predicates (Meyers et al., 2004; Gerber and Chai, 2010) and variants that investigated the prediction of semantic dependencies rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Here, we present an inference method for SRL, addressing the p</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
</authors>
<title>Maria Antònia Martí, Lluís Màrquez, Adam Meyers, Joakim Nivre, Sebastian Padó, Jan Štˇepánek, Pavel Straˇnák, Mihai Surdeanu,</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<location>Nianwen Xue, and</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Antònia Martí, Lluís Màrquez, Adam Meyers, Joakim Nivre, Sebastian Padó, Jan Štˇepánek, Pavel Straˇnák, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Dipanjan Das</author>
<author>Jason Weston</author>
<author>Kuzman Ganchev</author>
</authors>
<title>Semantic frame identification with distributed word representations.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2527" citStr="Hermann et al., 2014" startWordPosition="368" endWordPosition="371">prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and Riedel (2009), our framework first performs frame identification, the subtask of disambiguating the predicate frame; this makes our analysis more interpretable. The focus of this paper, however, is the subtask of semantic role labeling, wherein we take a set of (potentially overlapping) candidate sentential phrases and identify and label them with the semantic roles associated with the predicted frame. This treatment is commonly used in frame semantic parsing (Das et al., 2014; Hermann et al., 2014) and our two-stage framework is able to model both PropBank and FrameNet conventions. Previous work focusing on semantic role labeling imposed several structural constraints warranted by the annotation conventions of the task and other linguistic considerations, such as avoiding overlapping arguments and repeated core roles in the final prediction. Such global inference often leads to improved results and more meaningful predictions compared to local unconstrained methods (Màrquez et al., 2008). A popular framework for imposing these constraints has been integer linear programming (ILP), where</context>
<context position="7758" citStr="Hermann et al. (2014)" startWordPosition="1189" endWordPosition="1192"> such as the temporal role AM-TMP and the locative role AM-LOC; these are shared across frames and assume similar meaning. FrameNet similarly specifies a set of frames and roles, with two key differences. First, the semantics of the small set of core role labels in PropBank are local to each frame. In contrast, the several hundred role labels in FrameNet are shared across frames and they take on similar semantics in the frames in which they participate. Second, while frames in PropBank are just coarse-grained lemma-specific senses, the frame repository in FrameNet is shared across lemmas. See Hermann et al. (2014) for examples of these differences. Both PropBank- and FrameNet annotated data consist of sentence-level annotations that instantiate the respective frame lexicon with each predicate disambiguated to its frame, as well as the phrasal arguments of each predicate labeled with their semantic roles. Figure 1 shows an example sentence with two verbs annotated according to PropBank conventions. 1The CoNLL 2005 dataset is restricted to verbal predicates. 30 In addition to such basic semantic role annotation, the PropBank-annotated data sets from the CoNLL 2004 and 2005 shared tasks and OntoNotes 5.0,</context>
<context position="11698" citStr="Hermann et al. (2014)" startWordPosition="1836" endWordPosition="1839"> different models in the PropBank and FrameNet settings. In case of PropBank, we define the probability of a frame f under a conditional log-linear model: p(f |x, t, f) a exp (ψ · h(f, x, t, f)) , where ψ denotes the model parameters and h(·) is the feature function (see Table 1 for details on the features employed). The model’s partition function sums over all frames for the lemma f in the lexicon and we estimate the model parameters by maximizing regularized conditional log-likelihood. In the case of FrameNet, to make our results directly comparable to the recent state-of-the-art results of Hermann et al. (2014), we instead use their embeddings-based WSABIE model (Weston et al., 2011) for the frame identification step. 3.3 Unconstrained Semantic Role Labeling Given an identified frame f in a sentence x of n words (wl, ... , wn), we seek to predict a set of argument spans labeled with their semantic roles. We assume that there is a set of candidate spans S that could potentially serve as arguments of t. Specifically, we derive S with a high-recall rule-based algorithm that looks at the (dependency) syntactic context of the predicate word t, as described in §6.3. Let one candidate span be s E S. The se</context>
<context position="32106" citStr="Hermann et al. (2014)" startWordPosition="5521" endWordPosition="5524">s 8While the dynamic program in §4 could be used to efficiently implement such ensembles, since it solves the equivalent ILP, our focus in this work is on learning a single accurate model. derived from the OntoNotes frame files. This corpus consists of nominal predicate-argument structure annotations in addition to verbs. Specifically, we use version 12 downloaded from http://cemantix. org/data/ontonotes.html, for which some errors from the initial release used by Pradhan et al. (2013) have been corrected. Finally, we present results on FrameNet-annotated data, where our setup mirrors that of Hermann et al. (2014), who used the full-text annotations of the FrameNet 1.5 release.9 We use the same training, development and test splits as Hermann et al., which consists of 39, 16 and 23 documents, respectively. For evaluation on PropBank, we use the script from the CoNLL 2005 shared task that measures role labeling precision, recall and F1-score, as well as the full argument structure accuracy.10 In the FrameNet setting, we use a reimplementation of the SemEval 2007 shared task evaluation script that measures joint frame-argument precision, recall and F1-score (Baker et al., 2007). For consistency, we use a</context>
<context position="37864" citStr="Hermann et al. (2014" startWordPosition="6428" endWordPosition="6431">in to the first step. We apply this parent operation recursively for all the ancestors of the predicate. Finally, we consider the predicate’s syntactic parent word as a candidate argument if the predicate is connected to it via the amod label. The candidates are further filtered to only keep those where the role of the argument, conjoined with the path from its head to the predicate, has been observed in the training data. This algorithm obtains an unlabeled argument recall of 88.2% on the OntoNotes 5.0 development data, with a precision of 38.2%. For FrameNet, we use the extraction method of Hermann et al. (2014, §5.4), which is also inspired by Xue and Palmer (2004). On the FrameNet development data, this method obtains an unlabeled argument recall of 72.6%, with a precision of 25.1%.12 6.4 Baseline Systems We compare our local and structured models to the top performing constituency-based systems from the 11All but the following labels are treated as offensive: advmod, amod, appos, aux, auxpass, cc, conj, dep, det, mwe, neg, nn, npadvmod, num, number, poss, preconj, predet, prep, prt, ps, quantmod and tmod. 12The low recall on FrameNet suggests that a deeper analysis of missed arguments is necessar</context>
<context position="41487" citStr="Hermann et al. (2014)" startWordPosition="7053" endWordPosition="7056">uts of multiple SRL systems in various ways; as observed in other NLP problems, the ensemble systems outperformed the single-system counterparts, and are state of the art. To situate our models with these ensemble-based approaches, we include them in Table 3. For the OntoNotes datasets, we compare our models to Pradhan et al. (2013), who report results with a variant of the (non-ensemble) ASSERT system (Pradhan et al., 2005). These are the only previously reported results for the SRL problem on this dataset. Finally, for the FrameNet experiments, our baseline is the state-of-the-art system of Hermann et al. (2014), which combines a frame-identification model based on WSABIE (Weston et al., 2011) with a loglinear role labeling model. 6.5 Hyperparameters The l1 and l2 regularization weights for the frame identification and role labeling models for all experiments were tuned on the OntoNotes development data. For frame identification, the regularization weights are set to 0 and 0.1, while for semantic role labeling they are set to 0.1 and 1.0, respectively. 6.6 Results Table 3 shows our results on the CoNLL 2005 development set as well as the WSJ and Brown test sets.13 Our structured model achieves the hi</context>
<context position="45157" citStr="Hermann et al. (2014)" startWordPosition="7660" endWordPosition="7663"> results are all highly statistically significant. Compared to the results of Pradhan et al. (2013), our structured Method Development Prec. Recall F1 Comp. Local/Local 80.5 62.7 70.5 31.3 Local/DP 80.7 62.9 70.7 31.2 Structured/DP 79.6 64.1 71.0 32.6 Hermann 78.3 64.5 70.8 – Test Method Prec. Recall F1 Comp. Local/Local 75.9 64.5 69.7 32.8 Local/DP 76.1 64.9 70.1* 33.0 Structured/DP 75.4 65.8 70.3** 33.8*** Hermann 74.3 66.0 69.9 – Table 5: Full structure prediction results (joint frame identification and semantic role labeling performance) for FrameNet. All systems use the WSABIE model from Hermann et al. (2014) for the frame identification step. “Hermann” is the Wsabie Embedding results from Table 3 of Hermann et al. (2014). Statistical significance was assessed for F1 and Comp. on the test set with p &lt; 0.01*, p &lt; 0.05** and p &lt; 0.075***. model yields a 15% relative error reduction in terms of F1-score and a 20% reduction in terms of complete analysis score.14 The frame identification accuracies on the OntoNotes development and test set are 94.5% and 94.9%, respectively, whereas Pradhan et al. (2013) report an accuracy of 92.8% on the test set; this represents almost a 30% relative error reduction. </context>
</contexts>
<marker>Hermann, Das, Weston, Ganchev, 2014</marker>
<rawString>Karl Moritz Hermann, Dipanjan Das, Jason Weston, and Kuzman Ganchev. 2014. Semantic frame identification with distributed word representations. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context position="25881" citStr="Huang and Chiang (2005)" startWordPosition="4439" endWordPosition="4442">It is expected to rain . 34 Frame identification features • the predicate t • tag of t • the lemma 2 • children words of t • tag of t’s children • tag of t’s parent • parent word of t • subcat. frame of t • dep. label of t • dep. labels of t’s children • word to the left of t • word to the right of t • tag to the left of t • tag to the right of t • word cluster of t • word clusters of t’s children Table 1: Frame identification features. By subcategorization frame, we refer to the sequence of dependency labels of t’s children in the dependency tree. the previous section, using the algorithm of Huang and Chiang (2005) and picking the best solution that satisfies all the constraints. 5 Local and Structured Learning To train our models, we assume a training set where each predicate t (with lemma f) in sentence x has been identified and labeled with its semantic frame f, as well as with each candidate span and role pair (s, r) E S x R. We first consider a local log-linear model. Let the local score of span s and role r be given by g(s, r) = 0 · f(r, s, x, t, f, f), where 0 denotes the vector of model parameters and f(·) the feature function (see Table 2 for the specific features employed). We treat the local </context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best parsing. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based semantic role labeling of PropBank.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="30332" citStr="Johansson and Nugues, 2008" startWordPosition="5245" endWordPosition="5248"> of local classifiers (Tromble and Eisner, 2006; Punyakanok et al., 2008; Das et al., 2012), as well as on ensembles for combining the predictions of separate systems using integer linear programming (Surdeanu et al., 2007; Punyakanok et al., 2008).8 Meza-Ruiz and Riedel (2009) further used a Markov Logic Network formulation to incorporate a subset of these constraints during learning. Another popular approach has been to apply a reranking model, which can incorporate soft structural constraints in the form of features, on top of the k-best output of local classifiers (Toutanova et al., 2008; Johansson and Nugues, 2008). However, none of these methods provide any means to perform efficient marginal inference and this work is the first to use a globally normalized probabilistic model with structural constraints for this task. 6 Empirical Study We next present our experimental setup, datasets used, preprocessing details and empirical results. 6.1 Datasets and Evaluation We measure experimental results on three datasets. First, we use the CoNLL 2005 shared task data annotated according to PropBank conventions with the standard training, development and test splits (Carreras and Màrquez, 2005). These were origin</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based semantic role labeling of PropBank. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML.</booktitle>
<contexts>
<context position="33780" citStr="Lafferty et al., 2001" startWordPosition="5785" endWordPosition="5788">st set, as well as between our second and third best models. 6.2 Preprocessing All corpora were preprocessed with a part-of-speech tagger and a syntactic dependency parser, both of which were trained on the CoNLL 2012 training split extracted from OntoNotes 5.0 (Pradhan et al., 2013); this training data has no overlap with any of the development or test corpora used in our experiments. The constituency trees in OntoNotes were converted to Stanford dependencies before training our parser (de Marneffe and Manning, 2013). The part-of-speech tagger employs a second-order conditional random field (Lafferty et al., 2001) with the following features. Emission features: bias, the 9http://framenet.icsi.berkeley.edu. 10http://www.lsi.upc.edu/~srlconll/srl-eval.pl 36 word, the cluster of the word, suffixes of lengths 1 to 4, the capitalization shape of the word, whether the word contains a hyphen and the identity of the last word in the sentence. Transition features: the tag bigram, the tag bigram conjoined with, respectively, the clusters of the current and the previous words, the tag trigram and the tag trigram conjoined with, respectively, the clusters of the current and previous word, as well as with the word </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="31079" citStr="Marcus et al., 1993" startWordPosition="5360" endWordPosition="5363">globally normalized probabilistic model with structural constraints for this task. 6 Empirical Study We next present our experimental setup, datasets used, preprocessing details and empirical results. 6.1 Datasets and Evaluation We measure experimental results on three datasets. First, we use the CoNLL 2005 shared task data annotated according to PropBank conventions with the standard training, development and test splits (Carreras and Màrquez, 2005). These were originally constructed from sections 02-21, section 24 and section 23 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993). The PropBank I resource was used to construct the verb frame lexicon for the CoNLL 2005 experiments. Second, we perform experiments on a substantially larger data set annotated according to PropBank conventions, using the recent OntoNotes 5.0 corpus (Weischedel et al., 2011), with the CoNLL 2012 training, development and test splits from Pradhan et al. (2013). The frame lexicon for these experiments is 8While the dynamic program in §4 could be used to efficiently implement such ensembles, since it solves the equivalent ILP, our focus in this work is on learning a single accurate model. deriv</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluís Màrquez</author>
<author>Xavier Carreras</author>
<author>Kenneth C Litkowski</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Semantic role labeling: An introduction to the special issue.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="3026" citStr="Màrquez et al., 2008" startWordPosition="440" endWordPosition="443">th the predicted frame. This treatment is commonly used in frame semantic parsing (Das et al., 2014; Hermann et al., 2014) and our two-stage framework is able to model both PropBank and FrameNet conventions. Previous work focusing on semantic role labeling imposed several structural constraints warranted by the annotation conventions of the task and other linguistic considerations, such as avoiding overlapping arguments and repeated core roles in the final prediction. Such global inference often leads to improved results and more meaningful predictions compared to local unconstrained methods (Màrquez et al., 2008). A popular framework for imposing these constraints has been integer linear programming (ILP), wherein the inference problem is specified declaratively (Punyakanok et al., 2008). However, ILP-based inference methods often rely on generic off-the-shelf solvers that fail to exploit problem-specific structure (Martins et al., 2011). Instead, we present a dynamic program (DP) that exactly enforces most of the constraints examined by Punyakanok et al. (2008); remaining constraints are enforced by reverting to k-best inference if needed. We show that this technique solves the inference problem more</context>
<context position="10801" citStr="Màrquez et al., 2008" startWordPosition="1680" endWordPosition="1683"> thus comprises a cascade of two statistical models. Note that most prior work on PropBank data only considered the latter task, remaining agnostic to the 2This setup differs from the related line of work that only predicts semantic dependencies between the predicate and the head words of semantic arguments; the latter task is arguably more straightforward (Surdeanu et al., 2008; Hajiˇc et al., 2009). frame. Moreover, the semantic role labeling step has typically been divided into two stages: first identifying the spans that serve as semantic arguments and then labeling them with their roles (Màrquez et al., 2008). In contrast, we approach the semantic role labeling subproblem using a single statistical model. 3.2 Frame Identification Given a preprocessed sentence x and a marked predicate t with lemma f, we seek to predict the frame f instantiated by the predicate. To this end, we use different models in the PropBank and FrameNet settings. In case of PropBank, we define the probability of a frame f under a conditional log-linear model: p(f |x, t, f) a exp (ψ · h(f, x, t, f)) , where ψ denotes the model parameters and h(·) is the feature function (see Table 1 for details on the features employed). The m</context>
</contexts>
<marker>Màrquez, Carreras, Litkowski, Stevenson, 2008</marker>
<rawString>Lluís Màrquez, Xavier Carreras, Kenneth C. Litkowski, and Suzanne Stevenson. 2008. Semantic role labeling: An introduction to the special issue. Computational Linguistics, 34(2):145–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>André F T Martins</author>
<author>Noah A Smith</author>
<author>Pedro M Q Aguiar</author>
<author>Mário A T Figueiredo</author>
</authors>
<title>Dual decomposition with many overlapping components.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="3357" citStr="Martins et al., 2011" startWordPosition="485" endWordPosition="488">ions of the task and other linguistic considerations, such as avoiding overlapping arguments and repeated core roles in the final prediction. Such global inference often leads to improved results and more meaningful predictions compared to local unconstrained methods (Màrquez et al., 2008). A popular framework for imposing these constraints has been integer linear programming (ILP), wherein the inference problem is specified declaratively (Punyakanok et al., 2008). However, ILP-based inference methods often rely on generic off-the-shelf solvers that fail to exploit problem-specific structure (Martins et al., 2011). Instead, we present a dynamic program (DP) that exactly enforces most of the constraints examined by Punyakanok et al. (2008); remaining constraints are enforced by reverting to k-best inference if needed. We show that this technique solves the inference problem more than four times faster than a state-of-the-art off-the-shelf ILP solver, while 29 Transactions of the Association for Computational Linguistics, vol. 3, pp. 29–41, 2015. Action Editor: Kristina Toutanova. Submission batch: 9/2014; Revision batch 1/2015; Published 1/2015. c�2015 Association for Computational Linguistics. (wanter)</context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>André F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar, and Mário A. T. Figueiredo. 2011. Dual decomposition with many overlapping components. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Meyers</author>
<author>Ruth Reeves</author>
<author>Catherine Macleod</author>
<author>Rachel Szekely</author>
<author>Veronika Zielinska</author>
<author>Brian Young</author>
<author>Ralph Grishman</author>
</authors>
<title>The NomBank project: An interim report.</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL/HLT Workshop on Frontiers in Corpus Annotation.</booktitle>
<contexts>
<context position="1565" citStr="Meyers et al., 2004" startWordPosition="224" endWordPosition="227">f-the-art results on both PropBank- and FrameNet-annotated corpora. 1 Introduction Semantic role labeling (henceforth, SRL) is the task of identifying the semantic arguments of predicates in natural language text. Pioneered by Gildea and Jurafsky (2002), this task has been widely investigated by the NLP community. There have been two shared tasks at CoNLL 2004 and 2005 focusing on this problem, using PropBank conventions to identify the phrasal arguments of verbal predicates (Palmer et al., 2005; Carreras and Màrquez, 2004, 2005). Since then, there has been work on SRL for nominal predicates (Meyers et al., 2004; Gerber and Chai, 2010) and variants that investigated the prediction of semantic dependencies rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Here, we present an inference method for SRL, addressing the problem of phrasal argument structure prediction (as opposed to semantic dependencies). In contrast to most prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and Riedel (2009), our framework first performs frame identification, the subtask of disambiguating the predicate frame; this makes our analysis m</context>
</contexts>
<marker>Meyers, Reeves, Macleod, Szekely, Zielinska, Young, Grishman, 2004</marker>
<rawString>Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The NomBank project: An interim report. In Proceedings of NAACL/HLT Workshop on Frontiers in Corpus Annotation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Meza-Ruiz</author>
<author>Sebastian Riedel</author>
</authors>
<title>Jointly identifying predicates, arguments and senses using Markov Logic.</title>
<date>2009</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="2036" citStr="Meza-Ruiz and Riedel (2009)" startWordPosition="293" endWordPosition="296">erbal predicates (Palmer et al., 2005; Carreras and Màrquez, 2004, 2005). Since then, there has been work on SRL for nominal predicates (Meyers et al., 2004; Gerber and Chai, 2010) and variants that investigated the prediction of semantic dependencies rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Here, we present an inference method for SRL, addressing the problem of phrasal argument structure prediction (as opposed to semantic dependencies). In contrast to most prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and Riedel (2009), our framework first performs frame identification, the subtask of disambiguating the predicate frame; this makes our analysis more interpretable. The focus of this paper, however, is the subtask of semantic role labeling, wherein we take a set of (potentially overlapping) candidate sentential phrases and identify and label them with the semantic roles associated with the predicted frame. This treatment is commonly used in frame semantic parsing (Das et al., 2014; Hermann et al., 2014) and our two-stage framework is able to model both PropBank and FrameNet conventions. Previous work focusing </context>
<context position="29983" citStr="Meza-Ruiz and Riedel (2009)" startWordPosition="5188" endWordPosition="5191"> the regularized conditional likelihood of �g(s, r) x zs,r 1: rER 35 the training set, using the standard forward-backward algorithm on the dynamic program lattice to compute the required normalizer and feature expectations. There have been several studies of the use of constrained MAP inference for semantic role labeling on top of the predictions of local classifiers (Tromble and Eisner, 2006; Punyakanok et al., 2008; Das et al., 2012), as well as on ensembles for combining the predictions of separate systems using integer linear programming (Surdeanu et al., 2007; Punyakanok et al., 2008).8 Meza-Ruiz and Riedel (2009) further used a Markov Logic Network formulation to incorporate a subset of these constraints during learning. Another popular approach has been to apply a reranking model, which can incorporate soft structural constraints in the form of features, on top of the k-best output of local classifiers (Toutanova et al., 2008; Johansson and Nugues, 2008). However, none of these methods provide any means to perform efficient marginal inference and this work is the first to use a globally normalized probabilistic model with structural constraints for this task. 6 Empirical Study We next present our exp</context>
</contexts>
<marker>Meza-Ruiz, Riedel, 2009</marker>
<rawString>Ivan Meza-Ruiz and Sebastian Riedel. 2009. Jointly identifying predicates, arguments and senses using Markov Logic. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The Proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="1446" citStr="Palmer et al., 2005" startWordPosition="203" endWordPosition="206">re, we show that our structured model results in significant improvements over its local counterpart, achieving state-of-the-art results on both PropBank- and FrameNet-annotated corpora. 1 Introduction Semantic role labeling (henceforth, SRL) is the task of identifying the semantic arguments of predicates in natural language text. Pioneered by Gildea and Jurafsky (2002), this task has been widely investigated by the NLP community. There have been two shared tasks at CoNLL 2004 and 2005 focusing on this problem, using PropBank conventions to identify the phrasal arguments of verbal predicates (Palmer et al., 2005; Carreras and Màrquez, 2004, 2005). Since then, there has been work on SRL for nominal predicates (Meyers et al., 2004; Gerber and Chai, 2010) and variants that investigated the prediction of semantic dependencies rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Here, we present an inference method for SRL, addressing the problem of phrasal argument structure prediction (as opposed to semantic dependencies). In contrast to most prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and Riedel (2009), our fram</context>
<context position="6066" citStr="Palmer et al., 2005" startWordPosition="912" endWordPosition="915">gle-model systems and rivals state-of-the-art ensemble-based methods. We further present results on the OntoNotes 5.0 corpora annotated with semantic roles for both verbal and nominal predicates (Weischedel et al., 2011) and strongly outperform the prior state of the art (Pradhan et al., 2013). Finally, we present results on FrameNet 1.5 data, again achieving state-of-the-art results. 2 Task Overview We seek to predict the semantic argument structure of predicates in text. For brevity and practical reasons, the exposition and empirical study is primarily focused on PropBank-style annotations (Palmer et al., 2005). However, our approach applies directly to FrameNet-style annotations as well (Baker et al., 1998) and as shown empirically in §6, a similar trend Figure 2: Examples showing continuation and reference roles according to PropBank. The role prefix C- indicates continuation of an argument, while the prefix R- indicates reference to another overt argument of the same predicate. holds across both types of annotation. In both cases, we are provided with a frame lexicon that contains type-level information for lexical units (a lemma conjoined with a coarse-grained part-ofspeech tag).1 For each lexic</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="41294" citStr="Pradhan et al., 2005" startWordPosition="7020" endWordPosition="7024">s are nested; this system relies on global features in a reranking framework (see row 2 of Figure 19 of the cited paper). These authors also report ensemble-based variants that combine the outputs of multiple SRL systems in various ways; as observed in other NLP problems, the ensemble systems outperformed the single-system counterparts, and are state of the art. To situate our models with these ensemble-based approaches, we include them in Table 3. For the OntoNotes datasets, we compare our models to Pradhan et al. (2013), who report results with a variant of the (non-ensemble) ASSERT system (Pradhan et al., 2005). These are the only previously reported results for the SRL problem on this dataset. Finally, for the FrameNet experiments, our baseline is the state-of-the-art system of Hermann et al. (2014), which combines a frame-identification model based on WSABIE (Weston et al., 2011) with a loglinear role labeling model. 6.5 Hyperparameters The l1 and l2 regularization weights for the frame identification and role labeling models for all experiments were tuned on the OntoNotes development data. For frame identification, the regularization weights are set to 0 and 0.1, while for semantic role labeling </context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2005</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2005. Support vector learning for semantic argument classification. Machine Learning, 60(1-3):11–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
</authors>
<title>Alessandro Moschitti, Nianwen Xue, Tou Hwee Ng, Anders Björkelund,</title>
<date>2013</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<location>Olga Uryupina, Yuchen Zhang, and</location>
<marker>Pradhan, 2013</marker>
<rawString>Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, Tou Hwee Ng, Anders Björkelund, Olga Uryupina, Yuchen Zhang, and Zhi Zhong. 2013. Towards robust linguistic analysis using OntoNotes. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="3204" citStr="Punyakanok et al., 2008" startWordPosition="464" endWordPosition="468">opBank and FrameNet conventions. Previous work focusing on semantic role labeling imposed several structural constraints warranted by the annotation conventions of the task and other linguistic considerations, such as avoiding overlapping arguments and repeated core roles in the final prediction. Such global inference often leads to improved results and more meaningful predictions compared to local unconstrained methods (Màrquez et al., 2008). A popular framework for imposing these constraints has been integer linear programming (ILP), wherein the inference problem is specified declaratively (Punyakanok et al., 2008). However, ILP-based inference methods often rely on generic off-the-shelf solvers that fail to exploit problem-specific structure (Martins et al., 2011). Instead, we present a dynamic program (DP) that exactly enforces most of the constraints examined by Punyakanok et al. (2008); remaining constraints are enforced by reverting to k-best inference if needed. We show that this technique solves the inference problem more than four times faster than a state-of-the-art off-the-shelf ILP solver, while 29 Transactions of the Association for Computational Linguistics, vol. 3, pp. 29–41, 2015. Action </context>
<context position="13600" citStr="Punyakanok et al. (2008)" startWordPosition="2190" endWordPosition="2193">onding arg max. 3.4 Semantic Role Labeling as an ILP We can represent any prediction for the individual classifiers with a set of indicator variables z = {zs,r} with one variable for each span s and role r. An equivalent formulation to Equation (1) is then: zs,r × g(s, r) s.t. z ∈ {0,1}|S||R |(2) X zs,r = 1 ∀s ∈ S , r∈R where we have constrained the indicator variables to take on binary values, and required that we choose exactly one role (including the 0 role) for each span. To further guide the inference, we add the following constraints to the ILP in Equation (2), as originally proposed by Punyakanok et al. (2008):3 No Span Overlap Let Si be the set of spans covering token wi. We want to ensure that at most one of the spans in Si have an overt role assignment: ∀i ∈ [1, n] , X X zs,r ≤ 1 . s∈Si r6=0 Unique Core Roles Each core role r ∈ RC can be overt in at most one of the spans in S: X∀r ∈ RC , zs,r ≤ 1 . s∈S Continuation Roles A continuation role, may only be assigned if the corresponding base (i.e. noncontinuation, non-reference) role is assigned to an earlier span. To express this, we define s ≤ s0 to mean that s starts before s0. For a continuation role r ∈ RN, let base(r) ∈ RC ∪ RA be the correspo</context>
<context position="29777" citStr="Punyakanok et al., 2008" startWordPosition="5156" endWordPosition="5159">that can be tractably enforced in the dynamic program. In effect, p(z |x, t, f, f) = 0 for any z that violates the constraints. We estimate the parameters of this globally normalized model by maximizing the regularized conditional likelihood of �g(s, r) x zs,r 1: rER 35 the training set, using the standard forward-backward algorithm on the dynamic program lattice to compute the required normalizer and feature expectations. There have been several studies of the use of constrained MAP inference for semantic role labeling on top of the predictions of local classifiers (Tromble and Eisner, 2006; Punyakanok et al., 2008; Das et al., 2012), as well as on ensembles for combining the predictions of separate systems using integer linear programming (Surdeanu et al., 2007; Punyakanok et al., 2008).8 Meza-Ruiz and Riedel (2009) further used a Markov Logic Network formulation to incorporate a subset of these constraints during learning. Another popular approach has been to apply a reranking model, which can incorporate soft structural constraints in the form of features, on top of the k-best output of local classifiers (Toutanova et al., 2008; Johansson and Nugues, 2008). However, none of these methods provide any </context>
<context position="40356" citStr="Punyakanok et al. (2008)" startWordPosition="6866" endWordPosition="6869">ed. Statistical significance was assessed for F1 and Comp. on the WSJ and Brown test sets with p &lt; 0.01* and p &lt; 0.05**. literature on the CoNLL 2005 datasets. To facilitate a more nuanced comparison, we distinguish between prior work based on single systems, which use a single input parse and no model combination, and ensemble-based systems. For single systems, our first baseline is the strongest non-ensemble system presented by Surdeanu et al. (2007) that treats the SRL problem as a sequential tagging task (see §4.1 of the cited paper). Next, we consider the non-ensemble system presented by Punyakanok et al. (2008) that trains local classifiers and uses an ILP to satisfy the structural constraints; this system is most similar to our approach, but is trained locally. Finally, our third single system baseline is the model of Toutanova et al. (2008) that uses a tree structured dynamic program that assumes that all candidate spans are nested; this system relies on global features in a reranking framework (see row 2 of Figure 19 of the cited paper). These authors also report ensemble-based variants that combine the outputs of multiple SRL systems in various ways; as observed in other NLP problems, the ensemb</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2):257–287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Riedel</author>
<author>David A Smith</author>
</authors>
<title>Relaxed marginal inference and its application to dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="23153" citStr="Riedel and Smith (2010)" startWordPosition="3936" endWordPosition="3939">an s = wi, ... , wj. If r E RC then k =� k0, otherwise k = k0. The edge is only included if the role r is non-core, or if kr =� 1, to guarantee uniqueness of core roles. By this construction, once a core role has been assigned at a vertex vkj , it cannot be assigned on any future path reachable from vkj . Lattice Weights The edges are weighted in the same way as in §4.1. It is easy to verify that the structure enforces unique core roles, but is otherwise equivalent to that in §4.1. Since the weights are identical, the proof of Proposition 1 carries over directly. 5We note that the approach of Riedel and Smith (2010) could potentially be used to compute the marginals in an incremental fashion similar to Tromble and Eisner (2006). 6 In the OntoNotes 5.0 development set, there are on average 10.4 core-role combinations per predicate frame. Figure 4: Lattice corresponding to the no span overlap, unique core roles and core continuation roles constraints. Each vertex is labeled with its signature k E {0,11|RC|; in this example, “0, 1” equals {A0}. This represents the subset of core-roles assigned on the path up to and including the vertex. Dashed edges indicate the correct path. 4.3 Core Continuation Roles Rec</context>
</contexts>
<marker>Riedel, Smith, 2010</marker>
<rawString>Sebastian Riedel and David A. Smith. 2010. Relaxed marginal inference and its application to dependency parsing. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Lluís Màrquez</author>
<author>Xavier Carreras</author>
<author>Pere R Comas</author>
</authors>
<title>Combination strategies for semantic role labeling.</title>
<date>2007</date>
<journal>Journal ofArtificial Intelligence Research,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="29927" citStr="Surdeanu et al., 2007" startWordPosition="5180" endWordPosition="5183">rs of this globally normalized model by maximizing the regularized conditional likelihood of �g(s, r) x zs,r 1: rER 35 the training set, using the standard forward-backward algorithm on the dynamic program lattice to compute the required normalizer and feature expectations. There have been several studies of the use of constrained MAP inference for semantic role labeling on top of the predictions of local classifiers (Tromble and Eisner, 2006; Punyakanok et al., 2008; Das et al., 2012), as well as on ensembles for combining the predictions of separate systems using integer linear programming (Surdeanu et al., 2007; Punyakanok et al., 2008).8 Meza-Ruiz and Riedel (2009) further used a Markov Logic Network formulation to incorporate a subset of these constraints during learning. Another popular approach has been to apply a reranking model, which can incorporate soft structural constraints in the form of features, on top of the k-best output of local classifiers (Toutanova et al., 2008; Johansson and Nugues, 2008). However, none of these methods provide any means to perform efficient marginal inference and this work is the first to use a globally normalized probabilistic model with structural constraints </context>
<context position="40188" citStr="Surdeanu et al. (2007)" startWordPosition="6838" endWordPosition="6841"> inference with the dynamic program. Bold font indicates the best system using a single model and a single parse, while the best scores among all systems are underlined. Statistical significance was assessed for F1 and Comp. on the WSJ and Brown test sets with p &lt; 0.01* and p &lt; 0.05**. literature on the CoNLL 2005 datasets. To facilitate a more nuanced comparison, we distinguish between prior work based on single systems, which use a single input parse and no model combination, and ensemble-based systems. For single systems, our first baseline is the strongest non-ensemble system presented by Surdeanu et al. (2007) that treats the SRL problem as a sequential tagging task (see §4.1 of the cited paper). Next, we consider the non-ensemble system presented by Punyakanok et al. (2008) that trains local classifiers and uses an ILP to satisfy the structural constraints; this system is most similar to our approach, but is trained locally. Finally, our third single system baseline is the model of Toutanova et al. (2008) that uses a tree structured dynamic program that assumes that all candidate spans are nested; this system relies on global features in a reranking framework (see row 2 of Figure 19 of the cited p</context>
</contexts>
<marker>Surdeanu, Màrquez, Carreras, Comas, 2007</marker>
<rawString>Mihai Surdeanu, Lluís Màrquez, Xavier Carreras, and Pere R. Comas. 2007. Combination strategies for semantic role labeling. Journal ofArtificial Intelligence Research, 29(1):105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Lluís Màrquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL.</booktitle>
<contexts>
<context position="1713" citStr="Surdeanu et al., 2008" startWordPosition="246" endWordPosition="249">ying the semantic arguments of predicates in natural language text. Pioneered by Gildea and Jurafsky (2002), this task has been widely investigated by the NLP community. There have been two shared tasks at CoNLL 2004 and 2005 focusing on this problem, using PropBank conventions to identify the phrasal arguments of verbal predicates (Palmer et al., 2005; Carreras and Màrquez, 2004, 2005). Since then, there has been work on SRL for nominal predicates (Meyers et al., 2004; Gerber and Chai, 2010) and variants that investigated the prediction of semantic dependencies rather than phrasal arguments (Surdeanu et al., 2008; Hajiˇc et al., 2009). Here, we present an inference method for SRL, addressing the problem of phrasal argument structure prediction (as opposed to semantic dependencies). In contrast to most prior semantic role labeling work focusing on PropBank conventions, barring notable exceptions such as Meza-Ruiz and Riedel (2009), our framework first performs frame identification, the subtask of disambiguating the predicate frame; this makes our analysis more interpretable. The focus of this paper, however, is the subtask of semantic role labeling, wherein we take a set of (potentially overlapping) ca</context>
<context position="10561" citStr="Surdeanu et al., 2008" startWordPosition="1641" endWordPosition="1644"> frame. Given the predicate token, we (over-)generate a set of candidate spans in the sentence, that are then labeled with roles from the set of core roles, from the set of adjunct roles, or with the null role 0 (role labeling).2 Our system thus comprises a cascade of two statistical models. Note that most prior work on PropBank data only considered the latter task, remaining agnostic to the 2This setup differs from the related line of work that only predicts semantic dependencies between the predicate and the head words of semantic arguments; the latter task is arguably more straightforward (Surdeanu et al., 2008; Hajiˇc et al., 2009). frame. Moreover, the semantic role labeling step has typically been divided into two stages: first identifying the spans that serve as semantic arguments and then labeling them with their roles (Màrquez et al., 2008). In contrast, we approach the semantic role labeling subproblem using a single statistical model. 3.2 Frame Identification Given a preprocessed sentence x and a marked predicate t with lemma f, we seek to predict the frame f instantiated by the predicate. To this end, we use different models in the PropBank and FrameNet settings. In case of PropBank, we def</context>
</contexts>
<marker>Surdeanu, Johansson, Meyers, Màrquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluís Màrquez, and Joakim Nivre. 2008. The CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>A global joint model for semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="30303" citStr="Toutanova et al., 2008" startWordPosition="5241" endWordPosition="5244">n top of the predictions of local classifiers (Tromble and Eisner, 2006; Punyakanok et al., 2008; Das et al., 2012), as well as on ensembles for combining the predictions of separate systems using integer linear programming (Surdeanu et al., 2007; Punyakanok et al., 2008).8 Meza-Ruiz and Riedel (2009) further used a Markov Logic Network formulation to incorporate a subset of these constraints during learning. Another popular approach has been to apply a reranking model, which can incorporate soft structural constraints in the form of features, on top of the k-best output of local classifiers (Toutanova et al., 2008; Johansson and Nugues, 2008). However, none of these methods provide any means to perform efficient marginal inference and this work is the first to use a globally normalized probabilistic model with structural constraints for this task. 6 Empirical Study We next present our experimental setup, datasets used, preprocessing details and empirical results. 6.1 Datasets and Evaluation We measure experimental results on three datasets. First, we use the CoNLL 2005 shared task data annotated according to PropBank conventions with the standard training, development and test splits (Carreras and Màrq</context>
<context position="40592" citStr="Toutanova et al. (2008)" startWordPosition="6905" endWordPosition="6908">sed on single systems, which use a single input parse and no model combination, and ensemble-based systems. For single systems, our first baseline is the strongest non-ensemble system presented by Surdeanu et al. (2007) that treats the SRL problem as a sequential tagging task (see §4.1 of the cited paper). Next, we consider the non-ensemble system presented by Punyakanok et al. (2008) that trains local classifiers and uses an ILP to satisfy the structural constraints; this system is most similar to our approach, but is trained locally. Finally, our third single system baseline is the model of Toutanova et al. (2008) that uses a tree structured dynamic program that assumes that all candidate spans are nested; this system relies on global features in a reranking framework (see row 2 of Figure 19 of the cited paper). These authors also report ensemble-based variants that combine the outputs of multiple SRL systems in various ways; as observed in other NLP problems, the ensemble systems outperformed the single-system counterparts, and are state of the art. To situate our models with these ensemble-based approaches, we include them in Table 3. For the OntoNotes datasets, we compare our models to Pradhan et al</context>
<context position="48180" citStr="Toutanova et al. (2008)" startWordPosition="8172" endWordPosition="8175">of using an LP-relaxation instead of the ILP. This does not come with optimality guarantees, but is included for completeness. Finally, when using k-best inference to satisfy the reference roles and non-core continuation roles constraints in the dynamic program (§4.4), the maximum value of k is 80 on the OntoNotes development set. Across data points for which such k-best inference is necessary, the average k is found to be 1.8. If we allow ourselves to ignore these constraints, we can avoid k-best inference and achieve a further speedup, as shown in the last line of Table 6. The heuristics of Toutanova et al. (2008) could potentially be used as an alternative way of satisfying these constraints. 7 Conclusions We described a dynamic program for constrained inference in semantic role labeling that efficiently enforces a majority of structural constraints, given potentially overlapping candidate arguments. The dynamic program provably finds the optimal solutions of a corresponding ILP and in practice requires a fraction of the computational cost compared to an highly optimized off-the-shelf ILP solver, which has typically been used for this problem. Furthermore, the dynamic program facilitates learning with</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2008</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2008. A global joint model for semantic role labeling. Computational Linguistics, 34(2):161–191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy W Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>A fast finitestate relaxation method for enforcing global constraints on sequence decoding.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL-HLT.</booktitle>
<contexts>
<context position="20779" citStr="Tromble and Eisner (2006)" startWordPosition="3486" endWordPosition="3489">ed for the direct correspondence with the ILP score, its weight is constant across variable assignments. Thus, this edge only adds a constant offset of c∅* to the ILP solution. For the same reason, its presence has no influence on the arg max or marginal computations and we therefore drop it in our implementation. 4.2 Unique Core Roles To incorporate the unique core roles constraint, we add state signatures to the vertices in the lattice and restrict the edges accordingly. This increases the size of the lattice by O(2|RC|), where RC is the set of core roles. Our approach is similar to that of Tromble and Eisner (2006), but whereas they suggest incorporating the uniqueness constraints incrementally, we apply them all at once. This is necessary since we A0 expected.01 C-A1 A0 A1 0 0 0 0 0 0 0 A0 A1 A0/C-A1 It is expected to rain . 33 seek to train a structured probabilistic model, which requires the marginals with respect to the full set of constraints.5 While the number of signatures is exponential in |RC|, in practice this is a modest constant as each frame only has a small number of possible core roles (two or three for many frames).6 Furthermore, since many of the potential edges are pruned by the constr</context>
<context position="23267" citStr="Tromble and Eisner (2006)" startWordPosition="3954" endWordPosition="3957">re, or if kr =� 1, to guarantee uniqueness of core roles. By this construction, once a core role has been assigned at a vertex vkj , it cannot be assigned on any future path reachable from vkj . Lattice Weights The edges are weighted in the same way as in §4.1. It is easy to verify that the structure enforces unique core roles, but is otherwise equivalent to that in §4.1. Since the weights are identical, the proof of Proposition 1 carries over directly. 5We note that the approach of Riedel and Smith (2010) could potentially be used to compute the marginals in an incremental fashion similar to Tromble and Eisner (2006). 6 In the OntoNotes 5.0 development set, there are on average 10.4 core-role combinations per predicate frame. Figure 4: Lattice corresponding to the no span overlap, unique core roles and core continuation roles constraints. Each vertex is labeled with its signature k E {0,11|RC|; in this example, “0, 1” equals {A0}. This represents the subset of core-roles assigned on the path up to and including the vertex. Dashed edges indicate the correct path. 4.3 Core Continuation Roles Recall that the constraint for continuation roles is that they must occur after their corresponding base role. We enf</context>
<context position="29752" citStr="Tromble and Eisner, 2006" startWordPosition="5152" endWordPosition="5155">ear constraints from §3.4 that can be tractably enforced in the dynamic program. In effect, p(z |x, t, f, f) = 0 for any z that violates the constraints. We estimate the parameters of this globally normalized model by maximizing the regularized conditional likelihood of �g(s, r) x zs,r 1: rER 35 the training set, using the standard forward-backward algorithm on the dynamic program lattice to compute the required normalizer and feature expectations. There have been several studies of the use of constrained MAP inference for semantic role labeling on top of the predictions of local classifiers (Tromble and Eisner, 2006; Punyakanok et al., 2008; Das et al., 2012), as well as on ensembles for combining the predictions of separate systems using integer linear programming (Surdeanu et al., 2007; Punyakanok et al., 2008).8 Meza-Ruiz and Riedel (2009) further used a Markov Logic Network formulation to incorporate a subset of these constraints during learning. Another popular approach has been to apply a reranking model, which can incorporate soft structural constraints in the form of features, on top of the k-best output of local classifiers (Toutanova et al., 2008; Johansson and Nugues, 2008). However, none of t</context>
<context position="46891" citStr="Tromble and Eisner (2006)" startWordPosition="7950" endWordPosition="7953"> degree. Table 6 relates the speed of the various inference algorithms to the number of constraint violations. The time is relative to local inference; it excludes the 14Unfortunately, these results are not strictly comparable, due to errors in the original release of the data that was used by Pradhan et al. (2013). Results with Pradhan et al.’s system on the corrected release, obtained from personal communication with Pradhan et al., are included in Table 4 as “Pradhan (revised)”. 39 time of feature extraction and computation of g(s, r), which is the same across inference methods. Similar to Tromble and Eisner (2006), for all algorithms, we first use the local solution without constraints and only apply the constraints in the case of a violation. Removing this optimization results in a slowdown across the board by a factor of about 5 and does not change the ranking of the methods. Since the structured model has identical parameterization to the local model, optimality is guaranteed even when using this scheme with the former. We report the results of two ILP solvers: SCIP15 and Gurobi.16 SCIP is a factor of 8 slower than Gurobi for this problem, while Gurobi is a further factor of about 4 slower than our </context>
</contexts>
<marker>Tromble, Eisner, 2006</marker>
<rawString>Roy W. Tromble and Jason Eisner. 2006. A fast finitestate relaxation method for enforcing global constraints on sequence decoding. In Proceedings of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: A simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="35092" citStr="Turian et al. (2010)" startWordPosition="5986" endWordPosition="5989">Zhang and McDonald (2014), which exploits structural diversity in cube-pruning to improve higher-order graph-based inference. On the WSJ development set (section 22), the labeled attachment score of the parser is 90.9% while the part-of-speech tagger achieves an accuracy of 97.2% on the same dataset. On the OntoNotes development set, the corresponding scores are 90.2% and 97.3%. Both the tagger and the parser, as well as the frame identification and role labeling models (see Tables 1 and 2), have features based on word clusters. Specifically, we use the clusters with 1000 classes described by Turian et al. (2010), which are induced with the Brown algorithm (Brown et al., 1992). 6.3 Candidate Argument Extraction We use a rule-based heuristic to extract candidate arguments for role labeling. Most prior work on PropBank-style semantic role labeling have relied on constituency syntax for candidate argument extraction. Instead, we rely on dependency syntax, which allows faster preprocessing and potential extension to the many languages for which only dependency annotations are available. To this end, we adapt the constituency-based candidate argument extraction method of Xue and Palmer (2004) to dependenci</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised learning. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<title>OntoNotes: A large training corpus for enhanced processing.</title>
<date>2011</date>
<booktitle>Handbook of Natural Language Processing and Machine Translation.</booktitle>
<editor>Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch Marcus, Robert Belvin, Sameer Pradhan, Lance Ramshaw, and Nianwen Xue.</editor>
<publisher>Springer.</publisher>
<marker>2011</marker>
<rawString>Ralph Weischedel, Eduard Hovy, Martha Palmer, Mitch Marcus, Robert Belvin, Sameer Pradhan, Lance Ramshaw, and Nianwen Xue. 2011. OntoNotes: A large training corpus for enhanced processing. In J. Olive, C. Christianson, and J. McCary, editors, Handbook of Natural Language Processing and Machine Translation. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Weston</author>
<author>Samy Bengio</author>
<author>Nicolas Usunier</author>
</authors>
<title>WSABIE: Scaling up to large vocabulary image annotation.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCAI.</booktitle>
<contexts>
<context position="11772" citStr="Weston et al., 2011" startWordPosition="1847" endWordPosition="1850">k, we define the probability of a frame f under a conditional log-linear model: p(f |x, t, f) a exp (ψ · h(f, x, t, f)) , where ψ denotes the model parameters and h(·) is the feature function (see Table 1 for details on the features employed). The model’s partition function sums over all frames for the lemma f in the lexicon and we estimate the model parameters by maximizing regularized conditional log-likelihood. In the case of FrameNet, to make our results directly comparable to the recent state-of-the-art results of Hermann et al. (2014), we instead use their embeddings-based WSABIE model (Weston et al., 2011) for the frame identification step. 3.3 Unconstrained Semantic Role Labeling Given an identified frame f in a sentence x of n words (wl, ... , wn), we seek to predict a set of argument spans labeled with their semantic roles. We assume that there is a set of candidate spans S that could potentially serve as arguments of t. Specifically, we derive S with a high-recall rule-based algorithm that looks at the (dependency) syntactic context of the predicate word t, as described in §6.3. Let one candidate span be s E S. The set of possible roles R is composed of core roles RC associating with f, adj</context>
<context position="41570" citStr="Weston et al., 2011" startWordPosition="7065" endWordPosition="7068">ensemble systems outperformed the single-system counterparts, and are state of the art. To situate our models with these ensemble-based approaches, we include them in Table 3. For the OntoNotes datasets, we compare our models to Pradhan et al. (2013), who report results with a variant of the (non-ensemble) ASSERT system (Pradhan et al., 2005). These are the only previously reported results for the SRL problem on this dataset. Finally, for the FrameNet experiments, our baseline is the state-of-the-art system of Hermann et al. (2014), which combines a frame-identification model based on WSABIE (Weston et al., 2011) with a loglinear role labeling model. 6.5 Hyperparameters The l1 and l2 regularization weights for the frame identification and role labeling models for all experiments were tuned on the OntoNotes development data. For frame identification, the regularization weights are set to 0 and 0.1, while for semantic role labeling they are set to 0.1 and 1.0, respectively. 6.6 Results Table 3 shows our results on the CoNLL 2005 development set as well as the WSJ and Brown test sets.13 Our structured model achieves the highest F1-score among the non-ensemble systems, outperforming even the ensemble syst</context>
</contexts>
<marker>Weston, Bengio, Usunier, 2011</marker>
<rawString>Jason Weston, Samy Bengio, and Nicolas Usunier. 2011. WSABIE: Scaling up to large vocabulary image annotation. In Proceedings of IJCAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="35678" citStr="Xue and Palmer (2004)" startWordPosition="6071" endWordPosition="6074">s described by Turian et al. (2010), which are induced with the Brown algorithm (Brown et al., 1992). 6.3 Candidate Argument Extraction We use a rule-based heuristic to extract candidate arguments for role labeling. Most prior work on PropBank-style semantic role labeling have relied on constituency syntax for candidate argument extraction. Instead, we rely on dependency syntax, which allows faster preprocessing and potential extension to the many languages for which only dependency annotations are available. To this end, we adapt the constituency-based candidate argument extraction method of Xue and Palmer (2004) to dependencies. In gold PropBank annotations, syntactic constituents serve as arguments in all constructions. However, extracting constituents from a dependency tree is not straightforward. The full dependency subtree under a particular head word often merges syntactic constituents. For example, in the tree fragment The man who knew too much the dependency tree has the full clause as the subtree headed by man, making it non-trivial to extract a partial subtree underneath it that could serve as a valid argument (for example, The man). In our candidate argument extraction algorithm, first, we </context>
<context position="37920" citStr="Xue and Palmer (2004)" startWordPosition="6438" endWordPosition="6441">cursively for all the ancestors of the predicate. Finally, we consider the predicate’s syntactic parent word as a candidate argument if the predicate is connected to it via the amod label. The candidates are further filtered to only keep those where the role of the argument, conjoined with the path from its head to the predicate, has been observed in the training data. This algorithm obtains an unlabeled argument recall of 88.2% on the OntoNotes 5.0 development data, with a precision of 38.2%. For FrameNet, we use the extraction method of Hermann et al. (2014, §5.4), which is also inspired by Xue and Palmer (2004). On the FrameNet development data, this method obtains an unlabeled argument recall of 72.6%, with a precision of 25.1%.12 6.4 Baseline Systems We compare our local and structured models to the top performing constituency-based systems from the 11All but the following labels are treated as offensive: advmod, amod, appos, aux, auxpass, cc, conj, dep, det, mwe, neg, nn, npadvmod, num, number, poss, preconj, predet, prep, prt, ps, quantmod and tmod. 12The low recall on FrameNet suggests that a deeper analysis of missed arguments is necessary. However, to allow a fair comparison with prior work, </context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Ryan McDonald</author>
</authors>
<title>Enforcing structural diversity in cube-pruned dependency parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="34497" citStr="Zhang and McDonald (2014)" startWordPosition="5892" endWordPosition="5895">u. 10http://www.lsi.upc.edu/~srlconll/srl-eval.pl 36 word, the cluster of the word, suffixes of lengths 1 to 4, the capitalization shape of the word, whether the word contains a hyphen and the identity of the last word in the sentence. Transition features: the tag bigram, the tag bigram conjoined with, respectively, the clusters of the current and the previous words, the tag trigram and the tag trigram conjoined with, respectively, the clusters of the current and previous word, as well as with the word two positions back. For syntactic dependencies, we use the parser and features described by Zhang and McDonald (2014), which exploits structural diversity in cube-pruning to improve higher-order graph-based inference. On the WSJ development set (section 22), the labeled attachment score of the parser is 90.9% while the part-of-speech tagger achieves an accuracy of 97.2% on the same dataset. On the OntoNotes development set, the corresponding scores are 90.2% and 97.3%. Both the tagger and the parser, as well as the frame identification and role labeling models (see Tables 1 and 2), have features based on word clusters. Specifically, we use the clusters with 1000 classes described by Turian et al. (2010), whi</context>
</contexts>
<marker>Zhang, McDonald, 2014</marker>
<rawString>Hao Zhang and Ryan McDonald. 2014. Enforcing structural diversity in cube-pruned dependency parsing. In Proceedings of ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>