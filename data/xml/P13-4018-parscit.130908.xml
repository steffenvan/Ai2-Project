<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000131">
<title confidence="0.995538">
A Java Framework for Multilingual Definition and Hypernym Extraction
</title>
<author confidence="0.806936">
Stefano Faralli and Roberto Navigli
</author>
<affiliation confidence="0.493752">
Dipartimento di Informatica
</affiliation>
<address confidence="0.375258">
Sapienza Universit`a di Roma
</address>
<email confidence="0.942016">
{faralli,navigli}@di.uniroma1.it
</email>
<sectionHeader confidence="0.992223" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998862384615385">
In this paper we present a demonstra-
tion of a multilingual generalization of
Word-Class Lattices (WCLs), a super-
vised lattice-based model used to identify
textual definitions and extract hypernyms
from them. Lattices are learned from a
dataset of automatically-annotated defini-
tions from Wikipedia. We release a Java
API for the programmatic use of multilin-
gual WCLs in three languages (English,
French and Italian), as well as a Web ap-
plication for definition and hypernym ex-
traction from user-provided sentences.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997475">
Electronic dictionaries and domain glossaries are
definition repositories which prove very useful not
only for lookup purposes, but also for automatic
tasks such as Question Answering (Cui et al.,
2007; Saggion, 2004), taxonomy learning (Navigli
et al., 2011; Velardi et al., 2013), domain Word
Sense Disambiguation (Duan and Yates, 2010;
Faralli and Navigli, 2012), automatic acquisition
of semantic predicates (Flati and Navigli, 2013),
relation extraction (Yap and Baldwin, 2009) and,
more in general, knowledge acquisition (Hovy et
al., 2013). Unfortunately, constructing and updat-
ing such resources requires the effort of a team of
experts. Moreover, they are of no help when deal-
ing with new words or usages, or, even worse, new
domains. Nonetheless, raw text often contains
several definitional sentences, that is, it provides
within itself formal explanations for terms of inter-
est. Whilst it is not feasible to search texts manu-
ally for definitions in several languages, the task of
extracting definitional information can be autom-
atized by means of Machine Learning (ML) and
Natural Language Processing (NLP) techniques.
Many approaches (Snow et al., 2004; Kozareva
and Hovy, 2010, inter alia) build upon lexico-
syntactic patterns, inspired by the seminal work
of Hearst (1992). However, these methods suf-
fer from two signifiicant drawbacks: on the one
hand, low recall (as definitional sentences occur in
highly variable syntactic structures), and, on the
other hand, noise (because the most frequent def-
initional pattern – X is a Y – is inherently very
noisy). A recent approach to definition and hyper-
nym extraction, called Word-Class Lattices (Nav-
igli and Velardi, 2010, WCLs), overcomes these
issues by addressing the variability of definitional
sentences and providing a flexible way of automat-
ically extracting hypernyms from them. To do so,
lattice-based classifiers are learned from a training
set of textual definitions. Training sentences are
automatically clustered by similarity and, for each
such cluster, a lattice classifier is learned which
models the variants of the definition template de-
tected. A lattice is a directed acyclic graph, a
subclass of non-deterministic finite state automata.
The purpose of the lattice structure is to preserve
(in a compact form) the salient differences among
distinct sequences.
In this paper we present a demonstration of
Word-Class Lattices by providing a Java API and
a Web application for online usage. Since multi-
linguality is a key need in today’s information so-
ciety, and because WCLs have been tested over-
whelmingly only with the English language, we
provide experiments for three different languages,
namely English, French and Italian. To do so, in
contrast to Navigli and Velardi (2010), who cre-
ated a manually annotated training set of defini-
tions, we provide a heuristic method for the au-
tomatic acquisition of reliable training sets from
Wikipedia, and use them to determine the robust-
ness and generalization power of WCLs. We show
high performance in definition and hypernym ex-
traction for our three languages.
</bodyText>
<sectionHeader confidence="0.988925" genericHeader="method">
2 Word-Class Lattices
</sectionHeader>
<bodyText confidence="0.999222">
In this section we briefly summarize Word-Class
Lattices, originally introduced by Navigli and Ve-
lardi (2010).
</bodyText>
<subsectionHeader confidence="0.991442">
2.1 Definitional Sentence Generalization
</subsectionHeader>
<bodyText confidence="0.996881">
WCL relies on a formal notion of textual defi-
nition. Specifically, given a definition, e.g.: “In
computer science, a closure is a first-class func-
tion with free variables that are bound in the lex-
ical environment”, we assume that it contains the
</bodyText>
<page confidence="0.991111">
103
</page>
<bodyText confidence="0.6797262">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 103–108,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
[In geography, a country]DF [is]VF [apolitical division]GF .
[In finance, a bond]DF [is]VF [a negotiable certificate]GF [that that acknowledges... ]REST .
[In poetry, a foot]DF [is]V F [a measure]GF [, consisting... ]REST .
</bodyText>
<tableCaption confidence="0.99936">
Table 1: Example definitions (defined terms are marked in bold face, their hypernyms in italics).
</tableCaption>
<figure confidence="0.983011333333333">
country political
bond negotiable
foot
</figure>
<figureCaption confidence="0.999815">
Figure 1: The DF and GF Word-Class Lattices for the sentences in Table 1.
</figureCaption>
<figure confidence="0.993058363636364">
geography
finance
poetry
division
certificate
measure
In
NN, , a (TARGET)
a
JJ
NNz
</figure>
<bodyText confidence="0.995921384615385">
following fields (Storrer and Wellinghoff, 2006):
definiendum (DF), definitor (VF), definiens (GF)
and rest (REST), where DF is the part of the
definition including the word being defined (e.g.,
“In computer science, a closure”), VF is the verb
phrase used to introduce the definition (e.g., “is”),
GF usually includes the hypernym (e.g., “a first-
class function”, hypernym marked in italics) and
RF includes additional clauses (e.g., “with free
variables that are bound in the lexical environ-
ment”).
Consider a set of training sentences T, each
of which is automatically part-of-speech tagged
and manually bracketed with the DF, VF, GF and
REST fields (examples are shown in Table 1). We
first identify the set of most frequent words F
(e.g., the, a, is, of, refer, etc.). Then we add
the symbol (TARGET) to F and replace in T the
terms being defined with (TARGET). We then use
the set of frequent words F to generalize words to
“word classes”.
We define a word class as either a word itself
or its part of speech. Given a sentence s =
w1, w2, ... , w|s|, where wi is the i-th word of s,
we generalize its words wi to word classes ωi as
follows:
</bodyText>
<equation confidence="0.67939">
ωi =
</equation>
<bodyText confidence="0.665488111111111">
that is, a word wi is left unchanged if it occurs fre-
quently in the training corpus (i.e., wi E F) or is
transformed to its part of speech tag (POS(wi))
otherwise. As a result, we obtain a generalized
sentence s&apos; = ω1, ω2, ... , ω|s|. For instance,
given the first sentence in Table 1, we obtain the
corresponding generalized sentence: “In NN, a
(TARGET) is a JJ NN”, where NN and JJ indicate
the noun and adjective classes, respectively.
</bodyText>
<subsectionHeader confidence="0.999206">
2.2 Learning
</subsectionHeader>
<bodyText confidence="0.991833">
The WCL learning algorithm consists of 3 steps:
</bodyText>
<listItem confidence="0.714603727272727">
• Star patterns: each sentence in the training
set is preprocessed and generalized to a star
pattern by replacing with * all the words wi E�
F, i.e., non-frequent words. For instance, “In
geography, a country is a political division”
is transformed to “In *, a (TARGET) is a *”;
• Sentence clustering: the training sentences
are then clustered based on the star patterns
they belong to;
• Word-Class Lattice construction: for each
sentence cluster, a WCL is created separately
for each DF, VF and GF field by means of a
greedy alignment algorithm. In Figure 1 we
show the resulting lattices for the DF and GF
fields built for the cluster of sentences of Ta-
ble 1. Note that during the construction of the
lattice the nodes associated with the hyper-
nym words in the learning sentences (i.e., di-
vision, certificate and measure) are marked as
hypernyms in order to determine the hyper-
nym of a test sentence at classification time
(see (Navigli and Velardi, 2010) for details).
</listItem>
<subsectionHeader confidence="0.99677">
2.3 Classification
</subsectionHeader>
<bodyText confidence="0.999984578947368">
Once the learning process is over, a set of WCLs
is produced for the DF, VF and GF fields. Given
a test sentence s, we consider all possible combi-
nations of definiendum, definitor and definiens lat-
tices and select the combination of the three WCLs
that best fits the sentence, if such a combination
exists. In fact, choosing the most appropriate
combination of lattices impacts the performance
of hypernym extraction. The best combination
of WCLs is selected by maximizing the follow-
ing confidence score: score(s, lDF, lV F, lGF) =
coverage · log(support + 1) where s is the candi-
date sentence, lDF, lV F and lGF are three lattices
one for each definition field, coverage is the frac-
tion of words of the input sentence covered by the
three lattices, and support is the sum of the num-
ber of sentences in the star patterns corresponding
to the GF lattice. Finally, when a sentence is clas-
sified as a definition, its hypernym is extracted by
</bodyText>
<equation confidence="0.5752615">
�wi if wi E F
POS(wi) otherwise
</equation>
<page confidence="0.994247">
104
</page>
<table confidence="0.99926725">
# Wikipedia pages # definitions extracted
English (EN) 3,904,360 1,552,493
French (FR) 1,617,359 447,772
Italian (IT) 1,008,044 291,259
</table>
<tableCaption confidence="0.902455">
Table 2: The number of Wikipedia pages and the
resulting automatically annotated definitions.
</tableCaption>
<bodyText confidence="0.98448">
selecting the words in the input sentence that are
marked as hypernyms in the WCL selected for GF.
</bodyText>
<sectionHeader confidence="0.916489" genericHeader="method">
3 Multilingual Word-Class Lattices
</sectionHeader>
<bodyText confidence="0.999874444444445">
In order to enable multilinguality, thereby extract-
ing definitions and hypernyms in many languages,
we provide here a heuristic method for the creation
of multilingual training datasets from Wikipedia,
that we apply to three languages: English, French
and Italian. As a result, we are able to fully au-
tomatize the definition and hypernym extraction
by utilizing collaboratively-curated encyclopedia
content.
</bodyText>
<subsectionHeader confidence="0.9897895">
3.1 Automatic Learning of Multilingual
WCLs
</subsectionHeader>
<bodyText confidence="0.99808">
The method consists of four steps:
</bodyText>
<listItem confidence="0.814705">
1. candidate definition extraction: we iterate
</listItem>
<bodyText confidence="0.910918666666667">
through the collection of Wikipedia pages for
the language of interest. For each article we
extract the first paragraph, which usually, but
not always, contains a definitional sentence
for the concept expressed by the page title.
We discard all those pages for which the title
corresponds to a special page (i.e., title in the
form “List of [... ]”, “Index of [. . . ]”, “[. . . ]
(disambiguation)” etc.).
</bodyText>
<listItem confidence="0.665389666666667">
2. part-of-speech tagging and phrase chunk-
ing: for each candidate definition we per-
form part-of-speech tagging and chunking,
thus automatically identifying noun, verb,
and prepositional phrases (we use TreeTag-
ger (Schmid, 1997)).
3. automatic annotation: we replace all the oc-
currences in the candidate definition of the
target term (i.e., the title of the page) with
the marker (TARGET), we then tag as hyper-
nym the words associated with the first hy-
perlink occurring to the right of (TARGET).
Then we tag as VF (i.e., definitor field,
see Section 2.1) the verb phrase found be-
tween (TARGET) and the hypernym, if such
a phrase exists. Next we tag as GF (i.e.,
definiens field) the phrase which contains the
hypernym and as DF (i.e., definiendum field)
the phrase which starts at the beginning of
the sentence and ends right before the start
of the VP tag. Finally we mark as REST the
</listItem>
<bodyText confidence="0.960180571428571">
remaining phrases after the phrase already
tagged as GF. For example, given the sen-
tence “Albert Einstein was a German-born
theoretical physicist.”, we produce the fol-
lowing sentence annotation: “[Albert Ein-
stein]DF [was]VF [a German-born theoreti-
cal physicist]GF .” (target term marked in
bold and hypernym in italics).
4. filtering: we finally discard all the candidate
definitions for which not all fields could be
found during the previous step (i.e., either the
(TARGET), hypernym or any DF, VF, GF,
REST tag is missing).
We applied the above four steps to the En-
glish, French and Italian dumps of Wikipedia1.
The numbers are shown in Table 2: starting with
3,904,360 Wikipedia pages for English, 1,617,359
for French and 1,008,044 for Italian (first column),
we obtained 1,552,493, 447,772, and 291,259 au-
tomatically tagged sentences, respectively, for the
three languages (second column in the Table).
Since we next had to use these sentences for train-
ing our WCLs, we took out a random sample
of 1000 sentences for each language which we
used for testing purposes. We manually annotated
each of these sentences as definitional or non-
definitional2 and, in the case of the former, also
with the correct hypernym.
</bodyText>
<subsectionHeader confidence="0.992918">
3.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999957727272727">
We tested the newly acquired training dataset
against two test datasets. The first dataset was
our random sampling of 1000 Wikipedia test sen-
tences which we had set aside for each language
(no intersection with the training set, see previous
section). The second dataset was the same one
used in Navigli and Velardi (2010), made up of
sentences from the ukWaC Web corpus (Ferraresi
et al., 2008) and used to estimate the definition and
hypernym extraction performance on an open text
corpus.
</bodyText>
<subsectionHeader confidence="0.780642">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.9997363">
Table 3 shows the results obtained on definition
(column 2-4) and hypernym extraction (column 5-
7) in terms of precision (P), recall (R) and accu-
racy (A) on our first dataset. Note that accuracy
also takes into account candidate definitions in
the test set which were tagged as non-definitional
(see Section 3.1). In the Table we compare the
performance of our English WCL trained from
Wikipedia sentences using our automatic proce-
dure against the original performance of WCL
</bodyText>
<footnote confidence="0.9998068">
1We used the 21-09-2012 (EN), 17-09-2012 (FR), 21-09-
2012 (IT) dumps.
2Note that the first sentence of a Wikipedia page might
seldom be non-definitional, such as “Basmo fortress is lo-
cated in the north-western part ...”.
</footnote>
<page confidence="0.985616">
105
</page>
<table confidence="0.999968333333333">
Definition Extraction Hypernym Extraction
P R A P R A
EN 98.5 78.3 81.0 98.5 77.4 80.0
FR 98.7 83.3 84.0 98.6 78.0 79.0
IT 98.8 87.3 87.0 98.7 83.2 83.0
EN (2010) 100.0 59.0 66.0 100.0 58.3 65.0
</table>
<tableCaption confidence="0.998013">
Table 3: Precision (P), recall (R) and accuracy
</tableCaption>
<bodyText confidence="0.804761875">
(A) of definition and hypernym extraction when
testing on our dataset of 1000 randomly sam-
pled Wikipedia first-paragraph sentences. EN
(2010) refers to the WCL learned from the origi-
nal manually-curated training set from Navigli and
Velardi (2010), while EN, FR and IT refer to WCL
trained, respectively, with one of the three training
sets automatically acquired from Wikipedia.
</bodyText>
<table confidence="0.991745">
P R
EN 98.9 57.6
EN (2010) 94.8 56.5
</table>
<tableCaption confidence="0.873796">
Table 4: Estimated WCL definition extraction
precision (P) and recall (R), testing a sample of
ukWaC sentences.
</tableCaption>
<bodyText confidence="0.99872225">
trained on 1,908 manually-selected training sen-
tences3. It can be seen that the automatically ac-
quired training set considerably improves the per-
formance, as it covers higher variability. We note
that the recall in both definition and hypernym ex-
traction is higher for French and Italian. We at-
tribute this behavior to the higher complexity and,
again, variability of English Wikipedia pages, and
specifically first-sentence definitions. We remark
that the presented results were obtained without
any human effort, except for the independent col-
laborative editing and hyperlinking of Wikipedia
pages, and that the overall performances can be
improved by manually checking the automatically
annotated training datasets.
We also replicated the experiment carried out
by Navigli and Velardi (2010), testing WCLs with
a subset (over 300,000 sentences) of the ukWaC
Web corpus. As can be seen in Table 4, the
estimated precision and recall for WCL defini-
tion extraction with the 2010 training set were
94.8% and 56.5%, respectively, while with our au-
tomatically acquired English training set we ob-
tained a higher precision of 98.9% and a recall of
57.6%. This second experiment shows that learn-
ing WCLs from hundreds of thousands of defini-
tion candidates does not overfit to Wikipedia-style
definitional sentences.
After looking at the automatically acquired
training datasets, we noted some erroneous an-
notations mainly due to the following factors: i)
some Wikipedia pages do not start with defini-
</bodyText>
<footnote confidence="0.914207">
3Available from http://lcl.uniroma1.it/wcl
</footnote>
<figure confidence="0.973845947368421">
1 // select the language of interest
2 Language targetLanguage = Language.EN;
3 // open the training set
4 Dataset ts = new AnnotatedDataset(
5 trainingDatasetFile,
6 targetLanguage);
7 // obtain an instance of the WCL classifier
8 WCLClassifier c = new WCLClassifier(targetLanguage);
9 c.train(ts);
10 // create a sentence to be tested
11 Sentence sentence = Sentence.createFromString(
12 &amp;quot;WCL&amp;quot;,
13 &amp;quot;WCL is a kind of classifier.&amp;quot;,
14 targetLanguage);
15 // test the sentence
16 SentenceAnnotation sa = c.test(sentence);
17 // print the hypernym
18 if (sa.isDefinition())
19 System.out.println(sa.getHyper());
</figure>
<figureCaption confidence="0.999536">
Figure 2: An example of WCL API usage.
</figureCaption>
<bodyText confidence="0.999873769230769">
tional sentences; ii) they may contain more than
one verbal phrase between the defined term and
the hypernym; iii) the first link after the verbal
phrase does not cover, or partially covers, the
correct hypernym. The elimination of the above
wrongly acquired definitional patterns can be im-
plemented with some language-dependent heuris-
tics or can be done by human annotators. In any
case, given the presence of a high number of cor-
rect annotated sentences, these wrong definitional
patterns have a very low impact on the definition
and hypernym extraction precision as shown in the
above experiments (see Table 3 and Table 4).
</bodyText>
<sectionHeader confidence="0.991364" genericHeader="method">
4 Multilingual WCL API
</sectionHeader>
<bodyText confidence="0.99998052631579">
Together with the training and test sets of the
above experiments, we also release here our im-
plementation of Word-Class Lattices, available as
a Java API. As a result the WCL classifier can eas-
ily be used programmatically in any Java project.
In Figure 2 we show an example of the API usage.
After the selection of the target language (line 2),
we load the training dataset for the target language
(line 4). Then an instance of WCLClassifier is
created (line 8) and the training phase is launched
on the input training corpora (line 9). Now the
classifier is ready to be tested on any given sen-
tence in the target language (lines 11-16). If the
classifier output is positive (line 18) we can print
the extracted hypernym (line 19). The output of
the presented code is the string “classifier” which
corresponds to the hypernym extracted by WCL
for the input sentence “WCL is a kind of classi-
fier”.
</bodyText>
<subsectionHeader confidence="0.999889">
4.1 Web user interface
</subsectionHeader>
<bodyText confidence="0.99997875">
We also release a Web interface to enable online
usage of our WCLs for the English, French and
Italian languages. In Figure 3 we show a screen-
shot of our Web interface. The user can type the
</bodyText>
<page confidence="0.997385">
106
</page>
<figureCaption confidence="0.999795">
Figure 3: A screenshot of the WCL Web interface.
</figureCaption>
<bodyText confidence="0.999939777777778">
term of interest, the candidate definition, select
the language of interest and, after submission, in
the case of positive response from WCL, obtain
the corresponding hypernym and a graphical rep-
resentation of the lattices matching the given sen-
tence, as shown in the bottom part of the Figure.
The graphical representation shows the concate-
nation of the learned lattices which match the DF,
VF, GF parts of the given sentence (see Section
2). We also allow the user not to provide the term
of interest: in this case all the nouns in the sen-
tence are considered as candidate defined terms.
The Web user interface is part of a client-server ap-
plication, created with the JavaServer Pages tech-
nology. The server side produces an HTML page
(like the one shown in Figure 3), using the WCL
API (see Section 4) to process and test the submit-
ted definition candidate.
</bodyText>
<sectionHeader confidence="0.999855" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999845263157895">
A great deal of work is concerned with the lan-
guage independent extraction of definitions. Much
recent work uses symbolic methods that depend
on lexico-syntactic patterns or features, which are
manually created or semi-automatically learned as
recently done in (Zhang and Jiang, 2009; Wester-
hout, 2009). A fully automated method is, instead,
proposed by Borg et al. (2009), where higher
performance (around 60-70% F1-measure) is ob-
tained only for specific domains and patterns. Ve-
lardi et al. (2008), in order to improve precision
while keeping pattern generality, prune candidates
using more refined stylistic patterns and lexical fil-
ters. Cui et al. (2007) propose the use of prob-
abilistic lexico-semantic patterns, for definitional
question answering in the TREC contest4. How-
ever, the TREC evaluation datasets cannot be con-
sidered true definitions, but rather text fragments
providing some relevant fact about a target term.
</bodyText>
<footnote confidence="0.9661545">
4Text REtrieval Conferences: http://trec.nist.
gov
</footnote>
<bodyText confidence="0.9976836">
Hypernym extraction methods vary from simple
lexical patterns (Hearst, 1992; Oakes, 2005) to sta-
tistical and machine learning techniques (Agirre
et al., 2000; Caraballo, 1999; Dolan et al., 1993;
Sanfilippo and Poznanski, 1992; Ritter et al.,
2009). Extraction heuristics can be adopted in
many languages (De Benedictis et al., 2013),
where given a definitional sentence the hypernym
is identified as the first occuring noun after the
defined term. One of the highest-coverage meth-
ods is proposed by Snow et al. (2004). They first
search sentences that contain two terms which are
known to be in a taxonomic relation (term pairs are
taken from WordNet (Miller et al., 1990)); then
they parse the sentences, and automatically ex-
tract patterns from the parse trees. Finally, they
train a hypernym classifier based on these features.
Lexico-syntactic patterns are generated for each
sentence relating a term to its hypernym, and a de-
pendency parser is used to represent them.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999943333333333">
In this demonstration we provide three main con-
tributions: 1) a general method for obtaining large
training sets of annotated definitional sentences
for many languages from Wikipedia, thanks to
which we can release three new training sets for
English, French and Italian; 2) an API to program-
matically use WCLs in Java projects; 3) a Web ap-
plication which enables online use of multilingual
WCLs:http://lcl.uniroma1.it/wcl/.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998529666666667">
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
</bodyText>
<page confidence="0.998323">
107
</page>
<sectionHeader confidence="0.990124" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999705483870967">
Eneko Agirre, Olatz Ansa, Eduard H. Hovy, and David
Martinez. 2000. Enriching very large ontologies using the
WWW. In ECAI Workshop on Ontology Learning, Berlin,
Germany.
Claudia Borg, Mike Rosner, and Gordon Pace. 2009. Evo-
lutionary algorithms for definition extraction. In Proceed-
ings of the 1st Workshop on Definition Extraction, pages
26–32, Borovets, Bulgaria.
Sharon A. Caraballo. 1999. Automatic construction of a
hypernym-labeled noun hierarchy from text. In Proceed-
ings of the 37th Annual Meeting of the Association for
Computational Linguistics: Proceedings of the Confer-
ence, pages 120–126, Maryland, USA.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007. Soft pat-
tern matching models for definitional question answering.
ACM Transactions on Information Systems, 25(2):1–30.
Flavio De Benedictis, Stefano Faralli, and Roberto Navigli.
2013. GlossBoot: Bootstrapping Multilingual Domain
Glossaries from the Web. In Proceedings of 51st Annual
Meeting of the Association for Computational Linguistics,
Sofia, Bulgaria.
William Dolan, Lucy Vanderwende, and Stephen D. Richard-
son. 1993. Automatically deriving structured knowledge
bases from on-line dictionaries. In Proceedings of the
First Conference of the Pacific Association for Computa-
tional Linguistics, pages 5–14, Vancouver, Canada.
Weisi Duan and Alexander Yates. 2010. Extracting glosses
to disambiguate word senses. In Proceedings of Human
Language Technologies: The 11th Annual Conference of
the North American Chapter of the Association for Com-
putational Linguistics, pages 627–635, Los Angeles, CA,
USA.
Stefano Faralli and Roberto Navigli. 2012. A new
minimally-supervised framework for Domain Word Sense
Disambiguation. In Proceedings of the 2012 Joint Con-
ference on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learning,
pages 1411–1422, Jeju, Korea.
Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia
Bernardini. 2008. Introducing and evaluating ukWaC, a
very large web-derived corpus of English. In Proceedings
of the 4th Web as Corpus Workshop (WAC-4), pages 47–
54, Marrakech, Morocco.
Tiziano Flati and Roberto Navigli. 2013. SPred: Large-scale
Harvesting of Semantic Predicates. In Proceedings of 51st
Annual Meeting of the Association for Computational Lin-
guistics, Sofia, Bulgaria.
Marti A. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 15th Inter-
national Conference on Computational Linguistics, pages
539–545, Nantes, France.
Eduard Hovy, Roberto Navigli, and Simone Paolo Ponzetto.
2013. Collaboratively built semi-structured content and
artificial intelligence: The story so far. Artificial Intelli-
gence, 194:2–27.
Zornitsa Kozareva and Eduard Hovy. 2010. Learning argu-
ments and supertypes of semantic relations using recur-
sive patterns. In Proceedings of the 48th Annual Meeting
of the Association for Computational Linguistics (ACL),
Uppsala, Sweden, pages 1482–1491, Uppsala, Sweden.
George A. Miller, R.T. Beckwith, Christiane D. Fellbaum,
D. Gross, and K. Miller. 1990. WordNet: an online
lexical database. International Journal of Lexicography,
3(4):235–244.
Roberto Navigli and Paola Velardi. 2010. Learning Word-
Class Lattices for definition and hypernym extraction. In
Proceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1318–1327, Up-
psala, Sweden.
Roberto Navigli, Paola Velardi, and Stefano Faralli. 2011.
A graph-based algorithm for inducing lexical taxonomies
from scratch. In Proceedings of the 22th International
Joint Conference on Artificial Intelligence, pages 1872–
1877, Barcelona, Spain.
Michael P. Oakes. 2005. Using Hearst’s rules for the auto-
matic acquisition of hyponyms for mining a pharmaceu-
tical corpus. In RANLP Text Mining Workshop’05, pages
63–67, Borovets, Bulgaria.
Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discovery.
In Proceedings of the 2009 AAAI Spring Symposium on
Learning by Reading and Learning to Read, pages 88–93,
Palo Alto, California.
Horacio Saggion. 2004. Identifying definitions in text col-
lections for question answering. In Proceedings of the
Fourth International Conference on Language Resources
and Evaluation, pages 1927–1930, Lisbon, Portugal.
Antonio Sanfilippo and Victor Poznanski. 1992. The ac-
quisition of lexical knowledge from combined machine-
readable dictionary sources. In Proceedings of the third
Conference on Applied Natural Language Processing,
pages 80–87, Trento, Italy.
Helmut Schmid. 1997. Probabilistic part-of-speech tagging
using decision trees. In Daniel Jones and Harold Somers,
editors, New Methods in Language Processing, Studies in
Computational Linguistics, pages 154–164. UCL Press,
London, GB.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004.
Learning syntactic patterns for automatic hypernym dis-
covery. In Lawrence K. Saul, Yair Weiss, and L´eon Bot-
tou, editors, Proc. of NIPS 2004, pages 1297–1304, Cam-
bridge, Mass. MIT Press.
Angelika Storrer and Sandra Wellinghoff. 2006. Automated
detection and annotation of term definitions in German
text corpora. In LREC 2006, pages 275–295, Genoa, Italy.
Paola Velardi, Roberto Navigli, and Pierluigi D’Amadio.
2008. Mining the Web to create specialized glossaries.
IEEE Intelligent Systems, 23(5):18–25.
Paola Velardi, Stefano Faralli, and Roberto Navigli. 2013.
OntoLearn Reloaded: A graph-based algorithm for taxon-
omy induction. Computational Linguistics, 39(3).
Eline Westerhout. 2009. Definition extraction using linguis-
tic and structural features. In Proceedings of the RANLP
2009 Workshop on Definition Extraction, page 61–67,
Borovets, Bulgaria.
Willy Yap and Timothy Baldwin. 2009. Experiments on
pattern-based relation learning. In Proceedings of the 18th
ACM Conference on Information and Knowledge Man-
agement (CIKM 2009), pages 1657–1660, Hong Kong,
China, 2009.
Chunxia Zhang and Peng Jiang. 2009. Automatic extraction
of definitions. In Proceedings of 2nd IEEE International
Conference on Computer Science and Information Tech-
nology, pages 364–368, Beijing, China.
</reference>
<page confidence="0.998367">
108
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.582688">
<title confidence="0.8909615">A Java Framework for Multilingual Definition and Hypernym Extraction Faralli</title>
<author confidence="0.821844">Dipartimento di_Sapienza Universit`a di</author>
<abstract confidence="0.998207142857143">In this paper we present a demonstration of a multilingual generalization of Word-Class Lattices (WCLs), a supervised lattice-based model used to identify textual definitions and extract hypernyms from them. Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Olatz Ansa</author>
<author>Eduard H Hovy</author>
<author>David Martinez</author>
</authors>
<title>Enriching very large ontologies using the WWW.</title>
<date>2000</date>
<booktitle>In ECAI Workshop on Ontology Learning,</booktitle>
<location>Berlin, Germany.</location>
<contexts>
<context position="19984" citStr="Agirre et al., 2000" startWordPosition="3244" endWordPosition="3247">ve precision while keeping pattern generality, prune candidates using more refined stylistic patterns and lexical filters. Cui et al. (2007) propose the use of probabilistic lexico-semantic patterns, for definitional question answering in the TREC contest4. However, the TREC evaluation datasets cannot be considered true definitions, but rather text fragments providing some relevant fact about a target term. 4Text REtrieval Conferences: http://trec.nist. gov Hypernym extraction methods vary from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Caraballo, 1999; Dolan et al., 1993; Sanfilippo and Poznanski, 1992; Ritter et al., 2009). Extraction heuristics can be adopted in many languages (De Benedictis et al., 2013), where given a definitional sentence the hypernym is identified as the first occuring noun after the defined term. One of the highest-coverage methods is proposed by Snow et al. (2004). They first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)); then they parse the sentences, and automatically extract patterns from the parse </context>
</contexts>
<marker>Agirre, Ansa, Hovy, Martinez, 2000</marker>
<rawString>Eneko Agirre, Olatz Ansa, Eduard H. Hovy, and David Martinez. 2000. Enriching very large ontologies using the WWW. In ECAI Workshop on Ontology Learning, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Borg</author>
<author>Mike Rosner</author>
<author>Gordon Pace</author>
</authors>
<title>Evolutionary algorithms for definition extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 1st Workshop on Definition Extraction,</booktitle>
<pages>26--32</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="19219" citStr="Borg et al. (2009)" startWordPosition="3132" endWordPosition="3135">nt-server application, created with the JavaServer Pages technology. The server side produces an HTML page (like the one shown in Figure 3), using the WCL API (see Section 4) to process and test the submitted definition candidate. 5 Related Work A great deal of work is concerned with the language independent extraction of definitions. Much recent work uses symbolic methods that depend on lexico-syntactic patterns or features, which are manually created or semi-automatically learned as recently done in (Zhang and Jiang, 2009; Westerhout, 2009). A fully automated method is, instead, proposed by Borg et al. (2009), where higher performance (around 60-70% F1-measure) is obtained only for specific domains and patterns. Velardi et al. (2008), in order to improve precision while keeping pattern generality, prune candidates using more refined stylistic patterns and lexical filters. Cui et al. (2007) propose the use of probabilistic lexico-semantic patterns, for definitional question answering in the TREC contest4. However, the TREC evaluation datasets cannot be considered true definitions, but rather text fragments providing some relevant fact about a target term. 4Text REtrieval Conferences: http://trec.ni</context>
</contexts>
<marker>Borg, Rosner, Pace, 2009</marker>
<rawString>Claudia Borg, Mike Rosner, and Gordon Pace. 2009. Evolutionary algorithms for definition extraction. In Proceedings of the 1st Workshop on Definition Extraction, pages 26–32, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon A Caraballo</author>
</authors>
<title>Automatic construction of a hypernym-labeled noun hierarchy from text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics: Proceedings of the Conference,</booktitle>
<pages>120--126</pages>
<location>Maryland, USA.</location>
<contexts>
<context position="20001" citStr="Caraballo, 1999" startWordPosition="3248" endWordPosition="3249">eping pattern generality, prune candidates using more refined stylistic patterns and lexical filters. Cui et al. (2007) propose the use of probabilistic lexico-semantic patterns, for definitional question answering in the TREC contest4. However, the TREC evaluation datasets cannot be considered true definitions, but rather text fragments providing some relevant fact about a target term. 4Text REtrieval Conferences: http://trec.nist. gov Hypernym extraction methods vary from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Caraballo, 1999; Dolan et al., 1993; Sanfilippo and Poznanski, 1992; Ritter et al., 2009). Extraction heuristics can be adopted in many languages (De Benedictis et al., 2013), where given a definitional sentence the hypernym is identified as the first occuring noun after the defined term. One of the highest-coverage methods is proposed by Snow et al. (2004). They first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)); then they parse the sentences, and automatically extract patterns from the parse trees. Finally, t</context>
</contexts>
<marker>Caraballo, 1999</marker>
<rawString>Sharon A. Caraballo. 1999. Automatic construction of a hypernym-labeled noun hierarchy from text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics: Proceedings of the Conference, pages 120–126, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Soft pattern matching models for definitional question answering.</title>
<date>2007</date>
<journal>ACM Transactions on Information Systems,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="935" citStr="Cui et al., 2007" startWordPosition="130" endWordPosition="133">ed lattice-based model used to identify textual definitions and extract hypernyms from them. Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences. 1 Introduction Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (Cui et al., 2007; Saggion, 2004), taxonomy learning (Navigli et al., 2011; Velardi et al., 2013), domain Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012), automatic acquisition of semantic predicates (Flati and Navigli, 2013), relation extraction (Yap and Baldwin, 2009) and, more in general, knowledge acquisition (Hovy et al., 2013). Unfortunately, constructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional</context>
<context position="19505" citStr="Cui et al. (2007)" startWordPosition="3176" endWordPosition="3179">e language independent extraction of definitions. Much recent work uses symbolic methods that depend on lexico-syntactic patterns or features, which are manually created or semi-automatically learned as recently done in (Zhang and Jiang, 2009; Westerhout, 2009). A fully automated method is, instead, proposed by Borg et al. (2009), where higher performance (around 60-70% F1-measure) is obtained only for specific domains and patterns. Velardi et al. (2008), in order to improve precision while keeping pattern generality, prune candidates using more refined stylistic patterns and lexical filters. Cui et al. (2007) propose the use of probabilistic lexico-semantic patterns, for definitional question answering in the TREC contest4. However, the TREC evaluation datasets cannot be considered true definitions, but rather text fragments providing some relevant fact about a target term. 4Text REtrieval Conferences: http://trec.nist. gov Hypernym extraction methods vary from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Caraballo, 1999; Dolan et al., 1993; Sanfilippo and Poznanski, 1992; Ritter et al., 2009). Extraction heuristics can be</context>
</contexts>
<marker>Cui, Kan, Chua, 2007</marker>
<rawString>Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007. Soft pattern matching models for definitional question answering. ACM Transactions on Information Systems, 25(2):1–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Flavio De Benedictis</author>
<author>Stefano Faralli</author>
<author>Roberto Navigli</author>
</authors>
<title>GlossBoot: Bootstrapping Multilingual Domain Glossaries from the Web. In</title>
<date>2013</date>
<booktitle>Proceedings of 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sofia, Bulgaria.</location>
<marker>De Benedictis, Faralli, Navigli, 2013</marker>
<rawString>Flavio De Benedictis, Stefano Faralli, and Roberto Navigli. 2013. GlossBoot: Bootstrapping Multilingual Domain Glossaries from the Web. In Proceedings of 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Dolan</author>
<author>Lucy Vanderwende</author>
<author>Stephen D Richardson</author>
</authors>
<title>Automatically deriving structured knowledge bases from on-line dictionaries.</title>
<date>1993</date>
<booktitle>In Proceedings of the First Conference of the Pacific Association for Computational Linguistics,</booktitle>
<pages>5--14</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="20021" citStr="Dolan et al., 1993" startWordPosition="3250" endWordPosition="3253">erality, prune candidates using more refined stylistic patterns and lexical filters. Cui et al. (2007) propose the use of probabilistic lexico-semantic patterns, for definitional question answering in the TREC contest4. However, the TREC evaluation datasets cannot be considered true definitions, but rather text fragments providing some relevant fact about a target term. 4Text REtrieval Conferences: http://trec.nist. gov Hypernym extraction methods vary from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Caraballo, 1999; Dolan et al., 1993; Sanfilippo and Poznanski, 1992; Ritter et al., 2009). Extraction heuristics can be adopted in many languages (De Benedictis et al., 2013), where given a definitional sentence the hypernym is identified as the first occuring noun after the defined term. One of the highest-coverage methods is proposed by Snow et al. (2004). They first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)); then they parse the sentences, and automatically extract patterns from the parse trees. Finally, they train a hypernym</context>
</contexts>
<marker>Dolan, Vanderwende, Richardson, 1993</marker>
<rawString>William Dolan, Lucy Vanderwende, and Stephen D. Richardson. 1993. Automatically deriving structured knowledge bases from on-line dictionaries. In Proceedings of the First Conference of the Pacific Association for Computational Linguistics, pages 5–14, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weisi Duan</author>
<author>Alexander Yates</author>
</authors>
<title>Extracting glosses to disambiguate word senses.</title>
<date>2010</date>
<booktitle>In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>627--635</pages>
<location>Los Angeles, CA, USA.</location>
<contexts>
<context position="1071" citStr="Duan and Yates, 2010" startWordPosition="150" endWordPosition="153">automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences. 1 Introduction Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (Cui et al., 2007; Saggion, 2004), taxonomy learning (Navigli et al., 2011; Velardi et al., 2013), domain Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012), automatic acquisition of semantic predicates (Flati and Navigli, 2013), relation extraction (Yap and Baldwin, 2009) and, more in general, knowledge acquisition (Hovy et al., 2013). Unfortunately, constructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of interest. Whilst it is not feasible to search texts manu</context>
</contexts>
<marker>Duan, Yates, 2010</marker>
<rawString>Weisi Duan and Alexander Yates. 2010. Extracting glosses to disambiguate word senses. In Proceedings of Human Language Technologies: The 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 627–635, Los Angeles, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefano Faralli</author>
<author>Roberto Navigli</author>
</authors>
<title>A new minimally-supervised framework for Domain Word Sense Disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>1411--1422</pages>
<location>Jeju,</location>
<contexts>
<context position="1099" citStr="Faralli and Navigli, 2012" startWordPosition="154" endWordPosition="157">d definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences. 1 Introduction Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (Cui et al., 2007; Saggion, 2004), taxonomy learning (Navigli et al., 2011; Velardi et al., 2013), domain Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012), automatic acquisition of semantic predicates (Flati and Navigli, 2013), relation extraction (Yap and Baldwin, 2009) and, more in general, knowledge acquisition (Hovy et al., 2013). Unfortunately, constructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of interest. Whilst it is not feasible to search texts manually for definitions in seve</context>
</contexts>
<marker>Faralli, Navigli, 2012</marker>
<rawString>Stefano Faralli and Roberto Navigli. 2012. A new minimally-supervised framework for Domain Word Sense Disambiguation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 1411–1422, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
<author>Marco Baroni</author>
<author>Silvia Bernardini</author>
</authors>
<title>Introducing and evaluating ukWaC, a very large web-derived corpus of English.</title>
<date>2008</date>
<booktitle>In Proceedings of the 4th Web as Corpus Workshop (WAC-4),</booktitle>
<pages>47--54</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="12386" citStr="Ferraresi et al., 2008" startWordPosition="2009" endWordPosition="2012">00 sentences for each language which we used for testing purposes. We manually annotated each of these sentences as definitional or nondefinitional2 and, in the case of the former, also with the correct hypernym. 3.2 Evaluation We tested the newly acquired training dataset against two test datasets. The first dataset was our random sampling of 1000 Wikipedia test sentences which we had set aside for each language (no intersection with the training set, see previous section). The second dataset was the same one used in Navigli and Velardi (2010), made up of sentences from the ukWaC Web corpus (Ferraresi et al., 2008) and used to estimate the definition and hypernym extraction performance on an open text corpus. 3.3 Results Table 3 shows the results obtained on definition (column 2-4) and hypernym extraction (column 5- 7) in terms of precision (P), recall (R) and accuracy (A) on our first dataset. Note that accuracy also takes into account candidate definitions in the test set which were tagged as non-definitional (see Section 3.1). In the Table we compare the performance of our English WCL trained from Wikipedia sentences using our automatic procedure against the original performance of WCL 1We used the 2</context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, Bernardini, 2008</marker>
<rawString>Adriano Ferraresi, Eros Zanchetta, Marco Baroni, and Silvia Bernardini. 2008. Introducing and evaluating ukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus Workshop (WAC-4), pages 47– 54, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tiziano Flati</author>
<author>Roberto Navigli</author>
</authors>
<title>SPred: Large-scale Harvesting of Semantic Predicates.</title>
<date>2013</date>
<booktitle>In Proceedings of 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="1171" citStr="Flati and Navigli, 2013" startWordPosition="163" endWordPosition="166">se of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences. 1 Introduction Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (Cui et al., 2007; Saggion, 2004), taxonomy learning (Navigli et al., 2011; Velardi et al., 2013), domain Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012), automatic acquisition of semantic predicates (Flati and Navigli, 2013), relation extraction (Yap and Baldwin, 2009) and, more in general, knowledge acquisition (Hovy et al., 2013). Unfortunately, constructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of interest. Whilst it is not feasible to search texts manually for definitions in several languages, the task of extracting definitional information can be au</context>
</contexts>
<marker>Flati, Navigli, 2013</marker>
<rawString>Tiziano Flati and Roberto Navigli. 2013. SPred: Large-scale Harvesting of Semantic Predicates. In Proceedings of 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 15th International Conference on Computational Linguistics,</booktitle>
<pages>539--545</pages>
<location>Nantes, France.</location>
<contexts>
<context position="2020" citStr="Hearst (1992)" startWordPosition="298" endWordPosition="299">lp when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of interest. Whilst it is not feasible to search texts manually for definitions in several languages, the task of extracting definitional information can be automatized by means of Machine Learning (ML) and Natural Language Processing (NLP) techniques. Many approaches (Snow et al., 2004; Kozareva and Hovy, 2010, inter alia) build upon lexicosyntactic patterns, inspired by the seminal work of Hearst (1992). However, these methods suffer from two signifiicant drawbacks: on the one hand, low recall (as definitional sentences occur in highly variable syntactic structures), and, on the other hand, noise (because the most frequent definitional pattern – X is a Y – is inherently very noisy). A recent approach to definition and hypernym extraction, called Word-Class Lattices (Navigli and Velardi, 2010, WCLs), overcomes these issues by addressing the variability of definitional sentences and providing a flexible way of automatically extracting hypernyms from them. To do so, lattice-based classifiers ar</context>
<context position="19902" citStr="Hearst, 1992" startWordPosition="3233" endWordPosition="3234">for specific domains and patterns. Velardi et al. (2008), in order to improve precision while keeping pattern generality, prune candidates using more refined stylistic patterns and lexical filters. Cui et al. (2007) propose the use of probabilistic lexico-semantic patterns, for definitional question answering in the TREC contest4. However, the TREC evaluation datasets cannot be considered true definitions, but rather text fragments providing some relevant fact about a target term. 4Text REtrieval Conferences: http://trec.nist. gov Hypernym extraction methods vary from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Caraballo, 1999; Dolan et al., 1993; Sanfilippo and Poznanski, 1992; Ritter et al., 2009). Extraction heuristics can be adopted in many languages (De Benedictis et al., 2013), where given a definitional sentence the hypernym is identified as the first occuring noun after the defined term. One of the highest-coverage methods is proposed by Snow et al. (2004). They first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990));</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 15th International Conference on Computational Linguistics, pages 539–545, Nantes, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Collaboratively built semi-structured content and artificial intelligence: The story so far.</title>
<date>2013</date>
<journal>Artificial Intelligence,</journal>
<pages>194--2</pages>
<contexts>
<context position="1280" citStr="Hovy et al., 2013" startWordPosition="179" endWordPosition="182">on and hypernym extraction from user-provided sentences. 1 Introduction Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (Cui et al., 2007; Saggion, 2004), taxonomy learning (Navigli et al., 2011; Velardi et al., 2013), domain Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012), automatic acquisition of semantic predicates (Flati and Navigli, 2013), relation extraction (Yap and Baldwin, 2009) and, more in general, knowledge acquisition (Hovy et al., 2013). Unfortunately, constructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of interest. Whilst it is not feasible to search texts manually for definitions in several languages, the task of extracting definitional information can be automatized by means of Machine Learning (ML) and Natural Language Processing (NLP) techniques. Many approaches</context>
</contexts>
<marker>Hovy, Navigli, Ponzetto, 2013</marker>
<rawString>Eduard Hovy, Roberto Navigli, and Simone Paolo Ponzetto. 2013. Collaboratively built semi-structured content and artificial intelligence: The story so far. Artificial Intelligence, 194:2–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zornitsa Kozareva</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning arguments and supertypes of semantic relations using recursive patterns.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1482--1491</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1924" citStr="Kozareva and Hovy, 2010" startWordPosition="281" endWordPosition="284">tructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of interest. Whilst it is not feasible to search texts manually for definitions in several languages, the task of extracting definitional information can be automatized by means of Machine Learning (ML) and Natural Language Processing (NLP) techniques. Many approaches (Snow et al., 2004; Kozareva and Hovy, 2010, inter alia) build upon lexicosyntactic patterns, inspired by the seminal work of Hearst (1992). However, these methods suffer from two signifiicant drawbacks: on the one hand, low recall (as definitional sentences occur in highly variable syntactic structures), and, on the other hand, noise (because the most frequent definitional pattern – X is a Y – is inherently very noisy). A recent approach to definition and hypernym extraction, called Word-Class Lattices (Navigli and Velardi, 2010, WCLs), overcomes these issues by addressing the variability of definitional sentences and providing a flex</context>
</contexts>
<marker>Kozareva, Hovy, 2010</marker>
<rawString>Zornitsa Kozareva and Eduard Hovy. 2010. Learning arguments and supertypes of semantic relations using recursive patterns. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), Uppsala, Sweden, pages 1482–1491, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>R T Beckwith</author>
<author>Christiane D Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>WordNet: an online lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="20500" citStr="Miller et al., 1990" startWordPosition="3329" endWordPosition="3332">patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Caraballo, 1999; Dolan et al., 1993; Sanfilippo and Poznanski, 1992; Ritter et al., 2009). Extraction heuristics can be adopted in many languages (De Benedictis et al., 2013), where given a definitional sentence the hypernym is identified as the first occuring noun after the defined term. One of the highest-coverage methods is proposed by Snow et al. (2004). They first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)); then they parse the sentences, and automatically extract patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. 6 Conclusion In this demonstration we provide three main contributions: 1) a general method for obtaining large training sets of annotated definitional sentences for many languages from Wikipedia, thanks to which we can release three new training sets for English, French and Italian; 2) an API to pr</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, R.T. Beckwith, Christiane D. Fellbaum, D. Gross, and K. Miller. 1990. WordNet: an online lexical database. International Journal of Lexicography, 3(4):235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Learning WordClass Lattices for definition and hypernym extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1318--1327</pages>
<location>Uppsala,</location>
<contexts>
<context position="2416" citStr="Navigli and Velardi, 2010" startWordPosition="359" endWordPosition="363">f Machine Learning (ML) and Natural Language Processing (NLP) techniques. Many approaches (Snow et al., 2004; Kozareva and Hovy, 2010, inter alia) build upon lexicosyntactic patterns, inspired by the seminal work of Hearst (1992). However, these methods suffer from two signifiicant drawbacks: on the one hand, low recall (as definitional sentences occur in highly variable syntactic structures), and, on the other hand, noise (because the most frequent definitional pattern – X is a Y – is inherently very noisy). A recent approach to definition and hypernym extraction, called Word-Class Lattices (Navigli and Velardi, 2010, WCLs), overcomes these issues by addressing the variability of definitional sentences and providing a flexible way of automatically extracting hypernyms from them. To do so, lattice-based classifiers are learned from a training set of textual definitions. Training sentences are automatically clustered by similarity and, for each such cluster, a lattice classifier is learned which models the variants of the definition template detected. A lattice is a directed acyclic graph, a subclass of non-deterministic finite state automata. The purpose of the lattice structure is to preserve (in a compac</context>
<context position="3952" citStr="Navigli and Velardi (2010)" startWordPosition="598" endWordPosition="602">th the English language, we provide experiments for three different languages, namely English, French and Italian. To do so, in contrast to Navigli and Velardi (2010), who created a manually annotated training set of definitions, we provide a heuristic method for the automatic acquisition of reliable training sets from Wikipedia, and use them to determine the robustness and generalization power of WCLs. We show high performance in definition and hypernym extraction for our three languages. 2 Word-Class Lattices In this section we briefly summarize Word-Class Lattices, originally introduced by Navigli and Velardi (2010). 2.1 Definitional Sentence Generalization WCL relies on a formal notion of textual definition. Specifically, given a definition, e.g.: “In computer science, a closure is a first-class function with free variables that are bound in the lexical environment”, we assume that it contains the 103 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 103–108, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics [In geography, a country]DF [is]VF [apolitical division]GF . [In finance, a bond]DF [is]VF [a negotiable certificate]GF </context>
<context position="7559" citStr="Navigli and Velardi, 2010" startWordPosition="1213" endWordPosition="1216">n clustered based on the star patterns they belong to; • Word-Class Lattice construction: for each sentence cluster, a WCL is created separately for each DF, VF and GF field by means of a greedy alignment algorithm. In Figure 1 we show the resulting lattices for the DF and GF fields built for the cluster of sentences of Table 1. Note that during the construction of the lattice the nodes associated with the hypernym words in the learning sentences (i.e., division, certificate and measure) are marked as hypernyms in order to determine the hypernym of a test sentence at classification time (see (Navigli and Velardi, 2010) for details). 2.3 Classification Once the learning process is over, a set of WCLs is produced for the DF, VF and GF fields. Given a test sentence s, we consider all possible combinations of definiendum, definitor and definiens lattices and select the combination of the three WCLs that best fits the sentence, if such a combination exists. In fact, choosing the most appropriate combination of lattices impacts the performance of hypernym extraction. The best combination of WCLs is selected by maximizing the following confidence score: score(s, lDF, lV F, lGF) = coverage · log(support + 1) where </context>
<context position="12313" citStr="Navigli and Velardi (2010)" startWordPosition="1996" endWordPosition="1999">use these sentences for training our WCLs, we took out a random sample of 1000 sentences for each language which we used for testing purposes. We manually annotated each of these sentences as definitional or nondefinitional2 and, in the case of the former, also with the correct hypernym. 3.2 Evaluation We tested the newly acquired training dataset against two test datasets. The first dataset was our random sampling of 1000 Wikipedia test sentences which we had set aside for each language (no intersection with the training set, see previous section). The second dataset was the same one used in Navigli and Velardi (2010), made up of sentences from the ukWaC Web corpus (Ferraresi et al., 2008) and used to estimate the definition and hypernym extraction performance on an open text corpus. 3.3 Results Table 3 shows the results obtained on definition (column 2-4) and hypernym extraction (column 5- 7) in terms of precision (P), recall (R) and accuracy (A) on our first dataset. Note that accuracy also takes into account candidate definitions in the test set which were tagged as non-definitional (see Section 3.1). In the Table we compare the performance of our English WCL trained from Wikipedia sentences using our a</context>
<context position="13688" citStr="Navigli and Velardi (2010)" startWordPosition="2226" endWordPosition="2229">t sentence of a Wikipedia page might seldom be non-definitional, such as “Basmo fortress is located in the north-western part ...”. 105 Definition Extraction Hypernym Extraction P R A P R A EN 98.5 78.3 81.0 98.5 77.4 80.0 FR 98.7 83.3 84.0 98.6 78.0 79.0 IT 98.8 87.3 87.0 98.7 83.2 83.0 EN (2010) 100.0 59.0 66.0 100.0 58.3 65.0 Table 3: Precision (P), recall (R) and accuracy (A) of definition and hypernym extraction when testing on our dataset of 1000 randomly sampled Wikipedia first-paragraph sentences. EN (2010) refers to the WCL learned from the original manually-curated training set from Navigli and Velardi (2010), while EN, FR and IT refer to WCL trained, respectively, with one of the three training sets automatically acquired from Wikipedia. P R EN 98.9 57.6 EN (2010) 94.8 56.5 Table 4: Estimated WCL definition extraction precision (P) and recall (R), testing a sample of ukWaC sentences. trained on 1,908 manually-selected training sentences3. It can be seen that the automatically acquired training set considerably improves the performance, as it covers higher variability. We note that the recall in both definition and hypernym extraction is higher for French and Italian. We attribute this behavior to</context>
</contexts>
<marker>Navigli, Velardi, 2010</marker>
<rawString>Roberto Navigli and Paola Velardi. 2010. Learning WordClass Lattices for definition and hypernym extraction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1318–1327, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
<author>Stefano Faralli</author>
</authors>
<title>A graph-based algorithm for inducing lexical taxonomies from scratch.</title>
<date>2011</date>
<booktitle>In Proceedings of the 22th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1872--1877</pages>
<location>Barcelona,</location>
<contexts>
<context position="992" citStr="Navigli et al., 2011" startWordPosition="138" endWordPosition="141">itions and extract hypernyms from them. Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences. 1 Introduction Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (Cui et al., 2007; Saggion, 2004), taxonomy learning (Navigli et al., 2011; Velardi et al., 2013), domain Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012), automatic acquisition of semantic predicates (Flati and Navigli, 2013), relation extraction (Yap and Baldwin, 2009) and, more in general, knowledge acquisition (Hovy et al., 2013). Unfortunately, constructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal exp</context>
</contexts>
<marker>Navigli, Velardi, Faralli, 2011</marker>
<rawString>Roberto Navigli, Paola Velardi, and Stefano Faralli. 2011. A graph-based algorithm for inducing lexical taxonomies from scratch. In Proceedings of the 22th International Joint Conference on Artificial Intelligence, pages 1872– 1877, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael P Oakes</author>
</authors>
<title>Using Hearst’s rules for the automatic acquisition of hyponyms for mining a pharmaceutical corpus.</title>
<date>2005</date>
<booktitle>In RANLP Text Mining Workshop’05,</booktitle>
<pages>63--67</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="19916" citStr="Oakes, 2005" startWordPosition="3235" endWordPosition="3236">omains and patterns. Velardi et al. (2008), in order to improve precision while keeping pattern generality, prune candidates using more refined stylistic patterns and lexical filters. Cui et al. (2007) propose the use of probabilistic lexico-semantic patterns, for definitional question answering in the TREC contest4. However, the TREC evaluation datasets cannot be considered true definitions, but rather text fragments providing some relevant fact about a target term. 4Text REtrieval Conferences: http://trec.nist. gov Hypernym extraction methods vary from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Caraballo, 1999; Dolan et al., 1993; Sanfilippo and Poznanski, 1992; Ritter et al., 2009). Extraction heuristics can be adopted in many languages (De Benedictis et al., 2013), where given a definitional sentence the hypernym is identified as the first occuring noun after the defined term. One of the highest-coverage methods is proposed by Snow et al. (2004). They first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)); then they par</context>
</contexts>
<marker>Oakes, 2005</marker>
<rawString>Michael P. Oakes. 2005. Using Hearst’s rules for the automatic acquisition of hyponyms for mining a pharmaceutical corpus. In RANLP Text Mining Workshop’05, pages 63–67, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>What is this, anyway: Automatic hypernym discovery.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 AAAI Spring Symposium on Learning by Reading and Learning to Read,</booktitle>
<pages>88--93</pages>
<location>Palo Alto, California.</location>
<contexts>
<context position="20075" citStr="Ritter et al., 2009" startWordPosition="3258" endWordPosition="3261">ic patterns and lexical filters. Cui et al. (2007) propose the use of probabilistic lexico-semantic patterns, for definitional question answering in the TREC contest4. However, the TREC evaluation datasets cannot be considered true definitions, but rather text fragments providing some relevant fact about a target term. 4Text REtrieval Conferences: http://trec.nist. gov Hypernym extraction methods vary from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Caraballo, 1999; Dolan et al., 1993; Sanfilippo and Poznanski, 1992; Ritter et al., 2009). Extraction heuristics can be adopted in many languages (De Benedictis et al., 2013), where given a definitional sentence the hypernym is identified as the first occuring noun after the defined term. One of the highest-coverage methods is proposed by Snow et al. (2004). They first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)); then they parse the sentences, and automatically extract patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic </context>
</contexts>
<marker>Ritter, Soderland, Etzioni, 2009</marker>
<rawString>Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009. What is this, anyway: Automatic hypernym discovery. In Proceedings of the 2009 AAAI Spring Symposium on Learning by Reading and Learning to Read, pages 88–93, Palo Alto, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Saggion</author>
</authors>
<title>Identifying definitions in text collections for question answering.</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth International Conference on Language Resources and Evaluation,</booktitle>
<pages>1927--1930</pages>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="951" citStr="Saggion, 2004" startWordPosition="134" endWordPosition="135">odel used to identify textual definitions and extract hypernyms from them. Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences. 1 Introduction Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (Cui et al., 2007; Saggion, 2004), taxonomy learning (Navigli et al., 2011; Velardi et al., 2013), domain Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012), automatic acquisition of semantic predicates (Flati and Navigli, 2013), relation extraction (Yap and Baldwin, 2009) and, more in general, knowledge acquisition (Hovy et al., 2013). Unfortunately, constructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that</context>
</contexts>
<marker>Saggion, 2004</marker>
<rawString>Horacio Saggion. 2004. Identifying definitions in text collections for question answering. In Proceedings of the Fourth International Conference on Language Resources and Evaluation, pages 1927–1930, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Sanfilippo</author>
<author>Victor Poznanski</author>
</authors>
<title>The acquisition of lexical knowledge from combined machinereadable dictionary sources.</title>
<date>1992</date>
<booktitle>In Proceedings of the third Conference on Applied Natural Language Processing,</booktitle>
<pages>80--87</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="20053" citStr="Sanfilippo and Poznanski, 1992" startWordPosition="3254" endWordPosition="3257">dates using more refined stylistic patterns and lexical filters. Cui et al. (2007) propose the use of probabilistic lexico-semantic patterns, for definitional question answering in the TREC contest4. However, the TREC evaluation datasets cannot be considered true definitions, but rather text fragments providing some relevant fact about a target term. 4Text REtrieval Conferences: http://trec.nist. gov Hypernym extraction methods vary from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Caraballo, 1999; Dolan et al., 1993; Sanfilippo and Poznanski, 1992; Ritter et al., 2009). Extraction heuristics can be adopted in many languages (De Benedictis et al., 2013), where given a definitional sentence the hypernym is identified as the first occuring noun after the defined term. One of the highest-coverage methods is proposed by Snow et al. (2004). They first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)); then they parse the sentences, and automatically extract patterns from the parse trees. Finally, they train a hypernym classifier based on these featu</context>
</contexts>
<marker>Sanfilippo, Poznanski, 1992</marker>
<rawString>Antonio Sanfilippo and Victor Poznanski. 1992. The acquisition of lexical knowledge from combined machinereadable dictionary sources. In Proceedings of the third Conference on Applied Natural Language Processing, pages 80–87, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1997</date>
<booktitle>New Methods in Language Processing, Studies in Computational Linguistics,</booktitle>
<pages>154--164</pages>
<editor>In Daniel Jones and Harold Somers, editors,</editor>
<publisher>UCL Press,</publisher>
<location>London, GB.</location>
<contexts>
<context position="10099" citStr="Schmid, 1997" startWordPosition="1626" endWordPosition="1627">on of Wikipedia pages for the language of interest. For each article we extract the first paragraph, which usually, but not always, contains a definitional sentence for the concept expressed by the page title. We discard all those pages for which the title corresponds to a special page (i.e., title in the form “List of [... ]”, “Index of [. . . ]”, “[. . . ] (disambiguation)” etc.). 2. part-of-speech tagging and phrase chunking: for each candidate definition we perform part-of-speech tagging and chunking, thus automatically identifying noun, verb, and prepositional phrases (we use TreeTagger (Schmid, 1997)). 3. automatic annotation: we replace all the occurrences in the candidate definition of the target term (i.e., the title of the page) with the marker (TARGET), we then tag as hypernym the words associated with the first hyperlink occurring to the right of (TARGET). Then we tag as VF (i.e., definitor field, see Section 2.1) the verb phrase found between (TARGET) and the hypernym, if such a phrase exists. Next we tag as GF (i.e., definiens field) the phrase which contains the hypernym and as DF (i.e., definiendum field) the phrase which starts at the beginning of the sentence and ends right be</context>
</contexts>
<marker>Schmid, 1997</marker>
<rawString>Helmut Schmid. 1997. Probabilistic part-of-speech tagging using decision trees. In Daniel Jones and Harold Somers, editors, New Methods in Language Processing, Studies in Computational Linguistics, pages 154–164. UCL Press, London, GB.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery. In</title>
<date>2004</date>
<booktitle>Proc. of NIPS 2004,</booktitle>
<pages>1297--1304</pages>
<editor>Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors,</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="1899" citStr="Snow et al., 2004" startWordPosition="277" endWordPosition="280">Unfortunately, constructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of interest. Whilst it is not feasible to search texts manually for definitions in several languages, the task of extracting definitional information can be automatized by means of Machine Learning (ML) and Natural Language Processing (NLP) techniques. Many approaches (Snow et al., 2004; Kozareva and Hovy, 2010, inter alia) build upon lexicosyntactic patterns, inspired by the seminal work of Hearst (1992). However, these methods suffer from two signifiicant drawbacks: on the one hand, low recall (as definitional sentences occur in highly variable syntactic structures), and, on the other hand, noise (because the most frequent definitional pattern – X is a Y – is inherently very noisy). A recent approach to definition and hypernym extraction, called Word-Class Lattices (Navigli and Velardi, 2010, WCLs), overcomes these issues by addressing the variability of definitional sente</context>
<context position="20345" citStr="Snow et al. (2004)" startWordPosition="3302" endWordPosition="3305">roviding some relevant fact about a target term. 4Text REtrieval Conferences: http://trec.nist. gov Hypernym extraction methods vary from simple lexical patterns (Hearst, 1992; Oakes, 2005) to statistical and machine learning techniques (Agirre et al., 2000; Caraballo, 1999; Dolan et al., 1993; Sanfilippo and Poznanski, 1992; Ritter et al., 2009). Extraction heuristics can be adopted in many languages (De Benedictis et al., 2013), where given a definitional sentence the hypernym is identified as the first occuring noun after the defined term. One of the highest-coverage methods is proposed by Snow et al. (2004). They first search sentences that contain two terms which are known to be in a taxonomic relation (term pairs are taken from WordNet (Miller et al., 1990)); then they parse the sentences, and automatically extract patterns from the parse trees. Finally, they train a hypernym classifier based on these features. Lexico-syntactic patterns are generated for each sentence relating a term to its hypernym, and a dependency parser is used to represent them. 6 Conclusion In this demonstration we provide three main contributions: 1) a general method for obtaining large training sets of annotated defini</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2004</marker>
<rawString>Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2004. Learning syntactic patterns for automatic hypernym discovery. In Lawrence K. Saul, Yair Weiss, and L´eon Bottou, editors, Proc. of NIPS 2004, pages 1297–1304, Cambridge, Mass. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angelika Storrer</author>
<author>Sandra Wellinghoff</author>
</authors>
<title>Automated detection and annotation of term definitions in German text corpora.</title>
<date>2006</date>
<booktitle>In LREC</booktitle>
<pages>275--295</pages>
<location>Genoa, Italy.</location>
<contexts>
<context position="4998" citStr="Storrer and Wellinghoff, 2006" startWordPosition="757" endWordPosition="760">gust 4-9 2013. c�2013 Association for Computational Linguistics [In geography, a country]DF [is]VF [apolitical division]GF . [In finance, a bond]DF [is]VF [a negotiable certificate]GF [that that acknowledges... ]REST . [In poetry, a foot]DF [is]V F [a measure]GF [, consisting... ]REST . Table 1: Example definitions (defined terms are marked in bold face, their hypernyms in italics). country political bond negotiable foot Figure 1: The DF and GF Word-Class Lattices for the sentences in Table 1. geography finance poetry division certificate measure In NN, , a (TARGET) a JJ NNz following fields (Storrer and Wellinghoff, 2006): definiendum (DF), definitor (VF), definiens (GF) and rest (REST), where DF is the part of the definition including the word being defined (e.g., “In computer science, a closure”), VF is the verb phrase used to introduce the definition (e.g., “is”), GF usually includes the hypernym (e.g., “a firstclass function”, hypernym marked in italics) and RF includes additional clauses (e.g., “with free variables that are bound in the lexical environment”). Consider a set of training sentences T, each of which is automatically part-of-speech tagged and manually bracketed with the DF, VF, GF and REST fie</context>
</contexts>
<marker>Storrer, Wellinghoff, 2006</marker>
<rawString>Angelika Storrer and Sandra Wellinghoff. 2006. Automated detection and annotation of term definitions in German text corpora. In LREC 2006, pages 275–295, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Velardi</author>
<author>Roberto Navigli</author>
<author>Pierluigi D’Amadio</author>
</authors>
<title>Mining the Web to create specialized glossaries.</title>
<date>2008</date>
<journal>IEEE Intelligent Systems,</journal>
<volume>23</volume>
<issue>5</issue>
<marker>Velardi, Navigli, D’Amadio, 2008</marker>
<rawString>Paola Velardi, Roberto Navigli, and Pierluigi D’Amadio. 2008. Mining the Web to create specialized glossaries. IEEE Intelligent Systems, 23(5):18–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Velardi</author>
<author>Stefano Faralli</author>
<author>Roberto Navigli</author>
</authors>
<title>OntoLearn Reloaded: A graph-based algorithm for taxonomy induction.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="1015" citStr="Velardi et al., 2013" startWordPosition="142" endWordPosition="145">ernyms from them. Lattices are learned from a dataset of automatically-annotated definitions from Wikipedia. We release a Java API for the programmatic use of multilingual WCLs in three languages (English, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences. 1 Introduction Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (Cui et al., 2007; Saggion, 2004), taxonomy learning (Navigli et al., 2011; Velardi et al., 2013), domain Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012), automatic acquisition of semantic predicates (Flati and Navigli, 2013), relation extraction (Yap and Baldwin, 2009) and, more in general, knowledge acquisition (Hovy et al., 2013). Unfortunately, constructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of </context>
</contexts>
<marker>Velardi, Faralli, Navigli, 2013</marker>
<rawString>Paola Velardi, Stefano Faralli, and Roberto Navigli. 2013. OntoLearn Reloaded: A graph-based algorithm for taxonomy induction. Computational Linguistics, 39(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eline Westerhout</author>
</authors>
<title>Definition extraction using linguistic and structural features.</title>
<date>2009</date>
<booktitle>In Proceedings of the RANLP 2009 Workshop on Definition Extraction,</booktitle>
<pages>61--67</pages>
<location>Borovets, Bulgaria.</location>
<contexts>
<context position="19149" citStr="Westerhout, 2009" startWordPosition="3121" endWordPosition="3123"> as candidate defined terms. The Web user interface is part of a client-server application, created with the JavaServer Pages technology. The server side produces an HTML page (like the one shown in Figure 3), using the WCL API (see Section 4) to process and test the submitted definition candidate. 5 Related Work A great deal of work is concerned with the language independent extraction of definitions. Much recent work uses symbolic methods that depend on lexico-syntactic patterns or features, which are manually created or semi-automatically learned as recently done in (Zhang and Jiang, 2009; Westerhout, 2009). A fully automated method is, instead, proposed by Borg et al. (2009), where higher performance (around 60-70% F1-measure) is obtained only for specific domains and patterns. Velardi et al. (2008), in order to improve precision while keeping pattern generality, prune candidates using more refined stylistic patterns and lexical filters. Cui et al. (2007) propose the use of probabilistic lexico-semantic patterns, for definitional question answering in the TREC contest4. However, the TREC evaluation datasets cannot be considered true definitions, but rather text fragments providing some relevant</context>
</contexts>
<marker>Westerhout, 2009</marker>
<rawString>Eline Westerhout. 2009. Definition extraction using linguistic and structural features. In Proceedings of the RANLP 2009 Workshop on Definition Extraction, page 61–67, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Willy Yap</author>
<author>Timothy Baldwin</author>
</authors>
<title>Experiments on pattern-based relation learning.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM</booktitle>
<pages>1657--1660</pages>
<location>Hong Kong, China,</location>
<contexts>
<context position="1216" citStr="Yap and Baldwin, 2009" startWordPosition="169" endWordPosition="172">lish, French and Italian), as well as a Web application for definition and hypernym extraction from user-provided sentences. 1 Introduction Electronic dictionaries and domain glossaries are definition repositories which prove very useful not only for lookup purposes, but also for automatic tasks such as Question Answering (Cui et al., 2007; Saggion, 2004), taxonomy learning (Navigli et al., 2011; Velardi et al., 2013), domain Word Sense Disambiguation (Duan and Yates, 2010; Faralli and Navigli, 2012), automatic acquisition of semantic predicates (Flati and Navigli, 2013), relation extraction (Yap and Baldwin, 2009) and, more in general, knowledge acquisition (Hovy et al., 2013). Unfortunately, constructing and updating such resources requires the effort of a team of experts. Moreover, they are of no help when dealing with new words or usages, or, even worse, new domains. Nonetheless, raw text often contains several definitional sentences, that is, it provides within itself formal explanations for terms of interest. Whilst it is not feasible to search texts manually for definitions in several languages, the task of extracting definitional information can be automatized by means of Machine Learning (ML) a</context>
</contexts>
<marker>Yap, Baldwin, 2009</marker>
<rawString>Willy Yap and Timothy Baldwin. 2009. Experiments on pattern-based relation learning. In Proceedings of the 18th ACM Conference on Information and Knowledge Management (CIKM 2009), pages 1657–1660, Hong Kong, China, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunxia Zhang</author>
<author>Peng Jiang</author>
</authors>
<title>Automatic extraction of definitions.</title>
<date>2009</date>
<booktitle>In Proceedings of 2nd IEEE International Conference on Computer Science and Information Technology,</booktitle>
<pages>364--368</pages>
<location>Beijing, China.</location>
<contexts>
<context position="19130" citStr="Zhang and Jiang, 2009" startWordPosition="3117" endWordPosition="3120">sentence are considered as candidate defined terms. The Web user interface is part of a client-server application, created with the JavaServer Pages technology. The server side produces an HTML page (like the one shown in Figure 3), using the WCL API (see Section 4) to process and test the submitted definition candidate. 5 Related Work A great deal of work is concerned with the language independent extraction of definitions. Much recent work uses symbolic methods that depend on lexico-syntactic patterns or features, which are manually created or semi-automatically learned as recently done in (Zhang and Jiang, 2009; Westerhout, 2009). A fully automated method is, instead, proposed by Borg et al. (2009), where higher performance (around 60-70% F1-measure) is obtained only for specific domains and patterns. Velardi et al. (2008), in order to improve precision while keeping pattern generality, prune candidates using more refined stylistic patterns and lexical filters. Cui et al. (2007) propose the use of probabilistic lexico-semantic patterns, for definitional question answering in the TREC contest4. However, the TREC evaluation datasets cannot be considered true definitions, but rather text fragments prov</context>
</contexts>
<marker>Zhang, Jiang, 2009</marker>
<rawString>Chunxia Zhang and Peng Jiang. 2009. Automatic extraction of definitions. In Proceedings of 2nd IEEE International Conference on Computer Science and Information Technology, pages 364–368, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>