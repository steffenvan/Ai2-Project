<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021858">
<title confidence="0.9912085">
An Ordering of Terms Based on Semantic
Relatedness
</title>
<author confidence="0.992409">
Peter Wittek
</author>
<affiliation confidence="0.9846024">
Department of Computer Science
National University of Singapore
S´andor Dar´anyi
Swedish School of Library and Information Science
G¨oteborg University
</affiliation>
<email confidence="0.927738">
Sandor.Daranyi@hb.se
</email>
<author confidence="0.952448">
Chew Lim Tan
</author>
<affiliation confidence="0.9998065">
Department of Computer Science
National University of Singapore
</affiliation>
<email confidence="0.969242">
tancl@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.947368" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998630111111111">
Term selection methods typically employ a statistical measure to
filter or weight terms. Term expansion for IR may also depend on
statistics, or use some other, non-metric method based on a lexical
resource. At the same time, a wide range of semantic similarity mea-
sures have been developed to support natural language processing tasks
such as word sense disambiguation. This paper combines the two ap-
proaches and proposes an algorithm that provides a semantic order of
terms based on a semantic relatedness measure. This semantic order
can be exploited by term weighting and term expansion methods.
</bodyText>
<sectionHeader confidence="0.997332" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999802833333333">
Since the early days of the vector space model, it has been debated whether
it is a proper carrier of meaning of texts [23], arguing if distributional sim-
ilarity is an adequate proxy for lexical semantic relatedness [3]. With the
statistical, i.e. devoid of word semantics approaches there is generally no
way to improve both precision and recall at the same time, increasing one
is done at the expense of the other. For example, casting a wider net of
</bodyText>
<page confidence="0.973446">
235
</page>
<bodyText confidence="0.966793071428571">
Proceedings of the 8th International Conference on Computational Semantics, pages 235–247,
Tilburg, January 2009. c�2009 International Conference on Computational Semantics
search terms to improve recall of relevant items will also bring in an even
greater proportion of irrelevant items, lowering precision. In the meantime,
practical applications in information retrieval and text classification have
been proliferating, especially with developments in kernel methods in the
last decade [9, 4].
Ordering of terms based on semantic relatedness seeks an answer to the
simple question, can statistical term weighting be eclipsed? Namely, vari-
ants of weighting schemes based on term occurrences and co-occurrences
dominate the information retrieval and text classification scenes. However,
they also have a number of limitations. The connection between statistics
and word semantics is in general not understood very well. In other words,
a systematic discussion of mappings between theories of word meaning and
modeling them by mathematical objects is missing for the time being. Fur-
ther, enriching weighting schemes by importing their sense content from
lexical resources such as WordNet lacks a theoretical interpretation in terms
of lexical semantics. Combining co-occurrence and lexical resource-based
approaches for term weighting and term expansion may offer further theo-
retical insights, as well as performance benefits.
Using vectors in the vector space model as such mathematical objects
for the representation of term, document or query meaning necessarily ex-
presses content mapped on form as a set of coordinates. These coordinates,
at least in the case of the tfidf scheme, are corpus-specific, i.e. term weights
are neither constant over time nor database independent. Introducing a se-
mantic ordering of terms, hence loading a coordinate with semantic content,
reduces the dependence on a specific corpus.
In what follows, we will argue that:
</bodyText>
<listItem confidence="0.9994197">
• By assigning specific scalar values to terms in an ontology, terms rep-
resented by sets of geometric coordinates can be outdone;
• Such values result from a one-dimensional ordering based on the idea
of a sense-preserving distance between terms in a conceptual hierarchy;
• Sense-preserving distances mapped onto a line condense lexical rela-
tions and express them as a kind of within-language referential mean-
ing pertinent to individual terms, quasi charging their occurrences
independent of their occurrence rates, i.e. from the outside;
• This linear order can be used to assist term expansion and term weight-
ing.
</listItem>
<page confidence="0.99778">
236
</page>
<bodyText confidence="0.999685166666667">
This paper is organized as follows. Section 2 discusses the most impor-
tant measures for semantic relatedness with regard to the major linguistic
theories. Section 3 introduces an algorithm that creates a linear semantic
order of terms of a corpus, and Section 4 both offers first results in text
classification and discusses some implications. Finally, Section 5 concludes
the paper.
</bodyText>
<sectionHeader confidence="0.871906" genericHeader="method">
2 Measuring Semantic Relatedness
</sectionHeader>
<bodyText confidence="0.998908357142857">
Several methods have been proposed for measuring similarity. One of such
early proposals was the semantic differential which analyzes the affective
meaning of terms into a range of different dimensions with the opposed
adjectives at both ends, and locates the terms within semantic space [20].
Semantic similarity as proposed by Miller and Charles is a continuous
variable that describes the degree of synonymy between two terms [16]. They
argue that native speakers can order pairs of terms by semantic similarity,
for example ship-vessel, ship-watercraft, ship-riverboat, ship-sail, ship-house,
ship-dog, ship-sun. This concept may be extended to quantify relations
between non-synonymous but closely related terms, for example airplane-
wing. Semantic distance is the inverse of semantic similarity [17].
Semantic relatedness is defined between senses of terms. Given a relat-
edness formula rel(s1, s2) between two senses s1 and s2, term relatedness
between two terms t1 and t2 can be calculated as
</bodyText>
<equation confidence="0.9389965">
rel(t1, t2) = max rel(s1,s2),
81∈sen(t1),82∈sen(t2)
</equation>
<bodyText confidence="0.96157175">
where sen(t) is a set of senses of term t [3].
Automated systems assign a score of semantic relatedness to a given pair
of terms calculated from a relatedness measure. The absolute score itself
is typically irrelevant on its own, what is important is that the measure
assigns a higher score to term pairs which humans think are more related
and comparatively lower scores to term pairs that are less related [17].
The best known theories of word semantics fall in three major groups:
1. “Meaning is use” [30]: habitual usage provides indirect contextual in-
terpretation of any term. In accord with Carnap, frequency of use
expresses aspects of a conceptual hierarchy. In terms of logical seman-
tics, one regards document groups as value extensions (classes) and
index terms as value intensions (properties) of a (semantic) function
</bodyText>
<page confidence="0.977534">
237
</page>
<bodyText confidence="0.998860333333333">
’f’. Extensions and intensions are inverse proportional: the more prop-
erties defined, the less entities they apply to - there are more flowers
in general than tulips in particular, for instance.
</bodyText>
<listItem confidence="0.9869122">
2. “Meaning is change”: the stimulus-response theory by Bloomfield and
the biological theory of meaning by von Uexk¨ull both stress that the
meaning of any action is its consequences.
3. “Meaning is equivalence”: referential or ostensional theories of mean-
ing suggest that ’X = Y for/as long as Z’ [22].
</listItem>
<bodyText confidence="0.99990224137931">
Point 2 refers to theories which assign a temporal structure to word
meaning, they are not discussed here. Measures that rely on distributional
measures (Point 1) and those that use knowledge-rich resources (Point 3)
both exist, and they have been individually shown to good quantifiers of
term similarity each [17], These theories have been individually shown to be
good, therefore their combination must be a valid research alternative.
A lexical resource in computer science is a structure that captures se-
mantic relations among terms. Such a resource necessarily entails some sort
of world view with respect to a given domain. This is often conceived as a set
of concepts, their definitions and their inter-relationships; this is referred to
as a conceptualization. The following types of resources are commonly used
in measuring semantic similarity between terms: dictionary [12], semantic
networks, such as WordNet [5], thesauri modeled on Roget’s Thesaurus [19].
All approaches to measuring semantic relatedness that use a lexical re-
source regard the resource as a network or a directed graph, making use of
the structural information embedded in the graph [8, 3].
Distributional similarity, as studied by language technology, covers an
important kind of theories of word meaning and can be hence seen as con-
tributing to semantic document indexing and retrieval. Its predecessors go
back a long way, building on the notion of term dependence and structures
derived therefrom [2, 18]. Also called the contextual theory of meaning (see
[15] for the historical development of the concept), the underlying distri-
butional hypothesis is often cited for explaining how word meaning enters
information processing [10], and basically equals the claim “meaning is use”
in language philosophy. Before attempts to utilize lexical resources for the
same purpose, this used to be the sole source of word semantics in informa-
tion retrieval, inherent in the exploitation of term occurrences (tfidf) and
term co-occurrences [7, 21, 27], including multiple-level term co-occurrences
[11].
</bodyText>
<page confidence="0.984967">
238
</page>
<bodyText confidence="0.9702683">
Statistical techniques typically suffer from the sparse data problem: they
perform poorly when the terms are relatively rare, due to the scarcity of data.
Hybrid approaches attempt to address this problem by supplementing sparse
data with information from a lexical database [24, 8]. In a semantic network,
to differentiate between the weights of edges connecting a node and all its
child nodes, one needs to consider the link strength of each specific child
link. This is a situation in which corpus statistics can contribute. Ideally
the method chosen should be both theoretically sound and computationally
efficient [8].
Following the notation in information theory, the information content
(IC) of a concept c can be quantified as follows.
log P(c).
where P(c) is the probability of encountering an instance of concept c. In the
case of the hierarchical structure, where a concept in the hierarchy subsumes
those ones below it, this implies that P(c) is monotonic as one moves up in
the hierarchy. As the node’s probability increases, its information content or
its informativeness decreases. If there is a unique top node in the hierarchy,
then its probability is 1, hence its information content is 0. Given the
monotonic feature of the information content value, the similarity of two
concepts can be formally defined as follows.
</bodyText>
<equation confidence="0.820993">
sim(c1, c2) = max IC(c) = max − log p(c)
c∈Sup(c1,c2) c∈Sup(c1,c2)
</equation>
<bodyText confidence="0.9999315">
where Sup(c1, c2) is the set of concepts that subsume both c1 and c2. To
maximize the representativeness, the similarity value is the information con-
tent value of the node whose IC value is the largest among those higher order
classes.
The information content method requires less information on the detailed
structure of a lexical resource and it is insensitive to varying link types [24].
On the other hand, it does not differentiate between the similarity values of
any pair of concepts in a sub-hierarchy as long as their lowest super-ordinate
class is the same. Moreover, in the calculation of information content, a
polysemous term will have a large content value if only term frequency data
are used.
The distance function between two terms can be written as follows:
</bodyText>
<equation confidence="0.998701666666667">
d(t1, t2) = IC(c1) + IC(c2) − 2IC(LSuper(c1, c2)),
1
IC(c) =
</equation>
<page confidence="0.980404">
239
</page>
<bodyText confidence="0.999266333333333">
where LSuper(c1, c2) denotes the lowest super-ordinate of c1 and c2 in a
lexical resource. This distance measure also satisfies the properties of a
metric [8].
</bodyText>
<sectionHeader confidence="0.921522" genericHeader="method">
3 A Semantic Ordering of Terms
</sectionHeader>
<bodyText confidence="0.999977757575758">
Traditional distributional term clustering methods do not provide signifi-
cantly improved text representation [13]. Distributional clustering has also
been employed to compress the feature space while compromising document
classification accuracy [1]. Applying the information bottleneck method to
find term clusters that preserve the information about document categories
has been shown to increase text classification accuracy in certain cases [28].
On the other hand, term expansion has been widely researched, with
varying results [21]. These methods generate new features for each docu-
ment in the data set. These new features can be synonyms or homonyms of
document terms [26], or expanded features for terms, sentences and docu-
ments as in [6]. Several distributional criteria have been used to select terms
related to the query. For instance, [25] proposed the principle that the se-
lected terms should have a higher probability in the relevant documents than
in the irrelevant documents. Others examined the impact of determining ex-
pansion terms using a minimum spanning tree and some simple linguistic
analysis [29].
This section proposes an algorithm that connects term clustering and
term expansion. It employs a pairwise comparison between the terms to
find a linear order, instead of finding clusters. In this order, the transition
from a term to an adjacent one is “smooth” if the semantic distance between
two neighboring terms is small. The dimension of the feature space is not
compressed, yet, groups of adjacent terms can be regarded as semantic clus-
ters. Hence, following the idea of term expansion, adjacent terms can help
to improve the effectiveness of any vector space-based language technology.
Let V denote a set of terms {t1, t2, ... , tn} and let d(ti, tj) denote the
semantic distance between the terms ti and tj.
Let G = (V, E) denote a weighted undirected graph, where the weights
on the set E are defined by the distances between the terms.
Finding a semantic ordering of terms can be translated to a graph prob-
lem: a minimum-weight Hamiltonian path S of G gives the ordering by
reading the nodes from one end of the path to the other. G is a complete
graph, therefore such a path always exists, but finding it is an NP-complete
problem. The following greedy algorithm is similar to the nearest neighbor
</bodyText>
<page confidence="0.986876">
240
</page>
<bodyText confidence="0.996694">
heuristic for the solution of the traveling salesman problem. It creates a
graph G0 = (S, T), where S = V and T C E. This G0 graph is a span-
ning tree of G in which the maximum degree of a node is two, that is, the
minimum spanning tree is a path between two nodes.
Step 1 Find the term at the highest stage of the hierarchy in a lexical
resource.
</bodyText>
<equation confidence="0.920299">
ts = argminti∈V depth(ti).
</equation>
<bodyText confidence="0.984424">
This seed term is the first element of V 0, V 0 = {ts}. Remove it from
the set V :
</bodyText>
<equation confidence="0.996224">
V := V \{ts}.
</equation>
<bodyText confidence="0.983163">
Step 2 Let tl denote the leftmost term of the ordering and tr the rightmost
one. Find the next two elements of the ordering:
</bodyText>
<equation confidence="0.9959905">
t0l = argminti∈V d(ti, tl),
t0r = argminti∈V \{t�}d(ti, tr).
Step 3 If d(tl, t0l) &lt; d(tr, t0r) then add t0l to V 0, E0 := E0 U {e(tl, t0l)}, and
V := V \{t0l}. Else add t0r to V 0, E0 := E0U{e(tr, t0r)} and V := V \{t0r}.
</equation>
<bodyText confidence="0.9702752">
Step 4 Repeat from Step 2 until V = ∅.
The computational cost of the algorithm is O(n2). The above algorithm
can be thought of as a modified Prim’s algorithm, but it does not find the
optimal minimum-weight spanning tree.
The validity of the ordering algorithm is discussed as follows.
</bodyText>
<listItem confidence="0.9698336">
1. The ordering is possible. Starting from the seed term, the candidate
sets will always contain elements, which either share the same hyper-
nym or are hypernyms of each other.
2. The ordering is good enough. The quality will also depend on the
lexical resource in question. Further, the complexity of human lan-
guages makes the creation of even a near perfect semantic network of
its concepts impossible. Thus in many ways the lexical resource-based
measures are as good as the networks on which they are based.
3. The distance between adjacent terms is uniform. By the construction
of the ordering, it is obvious that the distances will not be uniform.
</listItem>
<page confidence="0.998548">
241
</page>
<sectionHeader confidence="0.998679" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999717">
We were interested in how the distances of consecutive index terms change if
we apply the semantic ordering. We indexed the ModApte split of Reuters-
21578 benchmark corpus with a WordNet-based stemmer. The indexing
found 12643 individual terms. Prior to the semantic ordering, terms were
assumed to be in an arbitrary order. Measuring the Jiang-Conrath distance
between the arbitrarily ordered terms, the average distance was 1.68. Note
that the Jiang-Conrath distance was normalized to the interval [0, 2]. Fig-
ure 1 shows the distribution of distances. The histogram has a high peak
at the maximum distance, indicating that the original arrangment had little
to do with semantic distance. However, there were few terms with zero or
little distance between them. This is due to terms which are related and
start with the same word or stem. For example, account, account execu-
tive, account for, accountable, accountant, accounting principle, accounting
standard, accounting system, accounts payable, accounts receivable.
</bodyText>
<figureCaption confidence="0.8959165">
Figure 1: Distribution of Distances Between Adjacent Terms in an Arbitrary
Order
</figureCaption>
<bodyText confidence="0.999864">
After the semantic ordering of the term by the proposed algorithm, both
the average distance and the Jiang-Conrath distance were 0.56. About one
third of the terms had very little distance between each other (see Figure 2).
Nevertheless, over 10 % of the total terms still had the maximum distance.
This is due to the non-optimal nature of the proposed term-ordering algo-
rithm. These terms add noise to the classification. The noisy terms occur
</bodyText>
<page confidence="0.993039">
242
</page>
<bodyText confidence="0.999854428571429">
typically at the two sides of the scale, being the leftmost and the rightmost
ones. While it is easy to find terms close to each other in the beginning, as
the algorithm proceeds, fewer terms remain in the pool to be chosen. For
instance, brand, brand name, trade name, label are in the 33rd, 34th, 35th
and 36th position on the left side counting from the seed respectively, while
windy, widespread, willingly, whatsoever, worried, worthwhile close the left
side, apparently sharing little in common.
</bodyText>
<figureCaption confidence="0.8944155">
Figure 2: Distribution of Distances Between Adjacent Terms in a Semantic
Order Based on Jiang-Conrath Distance
</figureCaption>
<bodyText confidence="0.999975222222222">
We conducted experiments on the ten most common categories of the
ModApte split of Reuters-21578. We trained support vector machines with
a linear kernel to compare the micro- and macro-average Fl measures for
different methods. Table 1 summarizes the results. The baseline vector
space model has zero expansion terms. Neighboring terms of the semantic
order were chosen as expansion terms. We found that increasing the number
of expansion terms also increases the effectiveness of classification, however,
effectiveness decreases after 4 expansions for micro-F1 and after 6 expansions
for macro-F1.
</bodyText>
<sectionHeader confidence="0.998313" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.970994">
Terms can be corpus- or genre-specific. Manually constructed general-purpose
lexical resources include many usages that are infrequent in a particular cor-
</bodyText>
<page confidence="0.996211">
243
</page>
<table confidence="0.996636875">
Number of
Expansion Micro-F1 Macro-F1
Terms
0 0.900 0.826
2 0.901 0.826
4 0.905 0.828
6 0.898 0.830
8 0.896 0.827
</table>
<tableCaption confidence="0.999567">
Table 1: Micro-Average and Macro F1-measure, Reuters-21578
</tableCaption>
<bodyText confidence="0.999972357142857">
pus or genre of documents, and therefore of little use. For example, one of
the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym
of person [14]. This usage of the term is practically never used in newspaper
articles, hence distributional attributes should be taken into consideration
when creating a linear ordering of terms.
Integrating lexical resources into an upgraded semantic weighting scheme
that could augment statistical term weighting is a prospect that cannot be
overlooked in information retrieval and text categorization. Our first results
with such a scheme in text categorization. At the same time, the results
also raise the question, does assigning specific scalar values to terms in an
ontology, this far represented by their geometric coordinates only, turn them
metaphorically into band lines of elements in a conceptual spectrum. We
anticipate that applying other types of kernels to the task may bring a new
set of challenging results.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999048285714286">
[1] L.D. Baker and A.K. McCallum. Distributional clustering of words
for text classification. In Proceedings of SIGIR-98, 21st ACM Inter-
national Conference on Research and Development in Information Re-
trieval, pages 96–103, Melbourne, Australia, August 1998. ACM Press,
New York, NY, USA.
[2] M.A. B¨artschi. Term dependence in information retrieval models. Mas-
ter’s thesis, Swiss Federal Institute of Technology, 1984.
</reference>
<page confidence="0.990764">
244
</page>
<reference confidence="0.999306939393939">
[3] A. Budanitsky and G. Hirst. Evaluating WordNet-based measures of
lexical semantic relatedness. Computational Linguistics, 32(1):13–47,
2006.
[4] N. Cristianini, J. Shawe-Taylor, and H. Lodhi. Latent semantic kernels.
Journal of Intelligent Information Systems, 18(2):127–152, 2002.
[5] C. Fellbaum. WordNet: An Electronic Lexical Database. MIT Press,
Cambridge, MA, USA, 1998.
[6] E. Gabrilovich and S. Markovitch. Feature generation for text cate-
gorization using world knowledge. In Proceedings of IJCAI-05, 19th
International Joint Conference on Artificial Intelligence, volume 19,
Edinburgh, UK, 2005. Lawrence Erlbaum Associates Ltd.
[7] S. I. Gallant. A practical approach for representing context and for
performing word sense disambiguation using neural networks. Neural
Computation, 3:293–309, 1991.
[8] J.J. Jiang and D.W. Conrath. Semantic similarity based on corpus
statistics and lexical taxonomy. In Proceedings of the International Con-
ference on Research in Computational Linguistics, pages 19–33, Taipei,
Taiwan, 1997.
[9] T. Joachims. Text categorization with support vector machines: Learn-
ing with many relevant features. In Proceedings of ECML-98, 10th
European Conference on Machine Learning, pages 137–142, Chemnitz,
Germany, April 1998. Springer-Verlag, London, UK.
[10] J. Karlgren and M. Sahlgren. From words to understanding. Founda-
tions of Real-World Intelligence, pages 294–308, 2001.
[11] A. Kontostathis and W.M. Pottenger. A framework for understanding
latent semantic indexing (LSI) performance. Information Processing
and Management, 42(1):56–73, 2006.
[12] M. Lesk. Automatic sense disambiguation using machine readable dic-
tionaries: How to tell a pine cone from an ice cream cone? In Proceed-
ings of SIGDOC-86, 5th Annual International Conference on Systems
Documentation, pages 24–26, New York, NY, USA, 1986. ACM Press.
[13] D.D. Lewis. An evaluation of phrasal and clustered representations
on a text categorization task. In Proceedings of SIGIR-92, 15th ACM
</reference>
<page confidence="0.990902">
245
</page>
<reference confidence="0.997746774193548">
International Conference on Research and Development in Information
Retrieval, pages 37–50, Copenhagen, Denmark, June 1992. ACM Press,
New York, NY, USA.
[14] D. Lin. Automatic retrieval and clustering of similar words. In Pro-
ceedings of COLING-ACL Workshop on Usage of WordNet in Natu-
ral Language Processing Systems, volume 98, pages 768–773, Montr´eal,
Qu´ebec, Canada, August 1998. ACL, Morristown, NJ, USA.
[15] J. Lyons. Semantics. Cambridge University Press, New York, NY,
USA, 1977.
[16] G. Miller and W. Charles. Contextual correlates of semantic similarity.
Language and Cognitive Processes, 6(1):1–28, 1991.
[17] S. Mohammad and G. Hirst. Distributional measures as proxies for
semantic relatedness. Submitted for publication, 2005.
[18] J. Morris, C. Beghtol, and G. Hirst. Term relationships and their contri-
bution to text semantics and information literacy through lexical cohe-
sion. In Proceedings of the 31st Annual Conference of the Canadian As-
sociation for Information Science, Halifax, Nova Scotia, Canada, May
2003.
[19] J. Morris and G. Hirst. Lexical cohesion computed by thesaural rela-
tions as an indicator of the structure of text. Computational Linguistics,
17(1):21–48, 1991.
[20] C.E. Osgood. The nature and measurement of meaning. Psychological
Bulletin, 49(3):197–237, 1952.
[21] H.J. Peat and P. Willett. The limitations of term co-occurrence data for
query expansion in document retrieval systems. Journal of the Ameri-
can Society for Information Science, 42(5):378–383, 1991.
[22] C.S. Peirce. Logic as semiotic: The theory of signs. Philosophical
Writings of Peirce, pages 98–119, 1955.
[23] V.V. Raghavan and S.K.M. Wong. A critical analysis of vector space
model for information retrieval. Journal of the American Society for
Information Science, 37(5):279–287, 1986.
</reference>
<page confidence="0.978544">
246
</page>
<reference confidence="0.9999495">
[24] P. Resnik. Using information content to evaluate semantic similarity in
a taxonomy. In Proceedings of IJCAI-95, 14th International Joint Con-
ference on Artificial Intelligence, volume 1, pages 448–453, Montr´eal,
Qu´ebec, Canada, August 1995.
[25] S.E. Robertson. On term selection for query expansion. Journal of
Documentation, 46(4):359–364, 1990.
[26] M.D.E.B. Rodriguez and J.M.G. Hidalgo. Using WordNet to com-
plement training information in text categorisation. In Procedings of
RANLP-97, 2nd International Conference on Recent Advances in Natu-
ral Language Processing. John Benjamins Publishing, Amsterdam, The
Netherlands, 1997.
[27] H. Schutze and T. Pedersen. A co-occurrence-based thesaurus and
two applications to information retrieval. Information Processing and
Management, 3(33):307–318, 1997.
[28] N. Slonim and N. Tishby. The power of word clusters for text clas-
sification. In Proceedings of ECIR-01, 23rd European Colloquium on
Information Retrieval Research, Darmstadt, Germany, 2001.
[29] A.F. Smeaton and C.J. van Rijsbergen. The retrieval effects of query
expansion on a feedback document retrieval system. The Computer
Journal, 26(3):239–246, 1983.
[30] L. Wittgenstein. Philosophical Investigations. Blackwell Publishing,
Oxford, UK, 1967.
</reference>
<page confidence="0.997985">
247
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.494839">
<title confidence="0.9985165">An Ordering of Terms Based on Relatedness</title>
<author confidence="0.999755">Peter Wittek</author>
<affiliation confidence="0.998105">Department of Computer National University of Singapore</affiliation>
<author confidence="0.930965">S´andor Dar´anyi</author>
<affiliation confidence="0.9902535">Swedish School of Library and Information G¨oteborg University</affiliation>
<title confidence="0.552061">Sandor.Daranyi@hb.se</title>
<author confidence="0.99899">Chew Lim Tan</author>
<affiliation confidence="0.999703">Department of Computer National University of Singapore</affiliation>
<email confidence="0.992163">tancl@comp.nus.edu.sg</email>
<abstract confidence="0.9995491">Term selection methods typically employ a statistical measure to filter or weight terms. Term expansion for IR may also depend on statistics, or use some other, non-metric method based on a lexical resource. At the same time, a wide range of semantic similarity measures have been developed to support natural language processing tasks such as word sense disambiguation. This paper combines the two approaches and proposes an algorithm that provides a semantic order of terms based on a semantic relatedness measure. This semantic order can be exploited by term weighting and term expansion methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L D Baker</author>
<author>A K McCallum</author>
</authors>
<title>Distributional clustering of words for text classification.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGIR-98, 21st ACM International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>96--103</pages>
<publisher>ACM Press,</publisher>
<location>Melbourne, Australia,</location>
<contexts>
<context position="11560" citStr="[1]" startWordPosition="1795" endWordPosition="1795"> term frequency data are used. The distance function between two terms can be written as follows: d(t1, t2) = IC(c1) + IC(c2) − 2IC(LSuper(c1, c2)), 1 IC(c) = 239 where LSuper(c1, c2) denotes the lowest super-ordinate of c1 and c2 in a lexical resource. This distance measure also satisfies the properties of a metric [8]. 3 A Semantic Ordering of Terms Traditional distributional term clustering methods do not provide significantly improved text representation [13]. Distributional clustering has also been employed to compress the feature space while compromising document classification accuracy [1]. Applying the information bottleneck method to find term clusters that preserve the information about document categories has been shown to increase text classification accuracy in certain cases [28]. On the other hand, term expansion has been widely researched, with varying results [21]. These methods generate new features for each document in the data set. These new features can be synonyms or homonyms of document terms [26], or expanded features for terms, sentences and documents as in [6]. Several distributional criteria have been used to select terms related to the query. For instance, [</context>
</contexts>
<marker>[1]</marker>
<rawString>L.D. Baker and A.K. McCallum. Distributional clustering of words for text classification. In Proceedings of SIGIR-98, 21st ACM International Conference on Research and Development in Information Retrieval, pages 96–103, Melbourne, Australia, August 1998. ACM Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A B¨artschi</author>
</authors>
<title>Term dependence in information retrieval models.</title>
<date>1984</date>
<tech>Master’s thesis,</tech>
<institution>Swiss Federal Institute of Technology,</institution>
<contexts>
<context position="8283" citStr="[2, 18]" startWordPosition="1275" endWordPosition="1276">2], semantic networks, such as WordNet [5], thesauri modeled on Roget’s Thesaurus [19]. All approaches to measuring semantic relatedness that use a lexical resource regard the resource as a network or a directed graph, making use of the structural information embedded in the graph [8, 3]. Distributional similarity, as studied by language technology, covers an important kind of theories of word meaning and can be hence seen as contributing to semantic document indexing and retrieval. Its predecessors go back a long way, building on the notion of term dependence and structures derived therefrom [2, 18]. Also called the contextual theory of meaning (see [15] for the historical development of the concept), the underlying distributional hypothesis is often cited for explaining how word meaning enters information processing [10], and basically equals the claim “meaning is use” in language philosophy. Before attempts to utilize lexical resources for the same purpose, this used to be the sole source of word semantics in information retrieval, inherent in the exploitation of term occurrences (tfidf) and term co-occurrences [7, 21, 27], including multiple-level term co-occurrences [11]. 238 Statist</context>
<context position="15927" citStr="[0, 2]" startWordPosition="2561" endWordPosition="2562">construction of the ordering, it is obvious that the distances will not be uniform. 241 4 Discussion We were interested in how the distances of consecutive index terms change if we apply the semantic ordering. We indexed the ModApte split of Reuters21578 benchmark corpus with a WordNet-based stemmer. The indexing found 12643 individual terms. Prior to the semantic ordering, terms were assumed to be in an arbitrary order. Measuring the Jiang-Conrath distance between the arbitrarily ordered terms, the average distance was 1.68. Note that the Jiang-Conrath distance was normalized to the interval [0, 2]. Figure 1 shows the distribution of distances. The histogram has a high peak at the maximum distance, indicating that the original arrangment had little to do with semantic distance. However, there were few terms with zero or little distance between them. This is due to terms which are related and start with the same word or stem. For example, account, account executive, account for, accountable, accountant, accounting principle, accounting standard, accounting system, accounts payable, accounts receivable. Figure 1: Distribution of Distances Between Adjacent Terms in an Arbitrary Order After</context>
</contexts>
<marker>[2]</marker>
<rawString>M.A. B¨artschi. Term dependence in information retrieval models. Master’s thesis, Swiss Federal Institute of Technology, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="1179" citStr="[3]" startWordPosition="178" endWordPosition="178">e range of semantic similarity measures have been developed to support natural language processing tasks such as word sense disambiguation. This paper combines the two approaches and proposes an algorithm that provides a semantic order of terms based on a semantic relatedness measure. This semantic order can be exploited by term weighting and term expansion methods. 1 Introduction Since the early days of the vector space model, it has been debated whether it is a proper carrier of meaning of texts [23], arguing if distributional similarity is an adequate proxy for lexical semantic relatedness [3]. With the statistical, i.e. devoid of word semantics approaches there is generally no way to improve both precision and recall at the same time, increasing one is done at the expense of the other. For example, casting a wider net of 235 Proceedings of the 8th International Conference on Computational Semantics, pages 235–247, Tilburg, January 2009. c�2009 International Conference on Computational Semantics search terms to improve recall of relevant items will also bring in an even greater proportion of irrelevant items, lowering precision. In the meantime, practical applications in informatio</context>
<context position="5506" citStr="[3]" startWordPosition="835" endWordPosition="835">y semantic similarity, for example ship-vessel, ship-watercraft, ship-riverboat, ship-sail, ship-house, ship-dog, ship-sun. This concept may be extended to quantify relations between non-synonymous but closely related terms, for example airplanewing. Semantic distance is the inverse of semantic similarity [17]. Semantic relatedness is defined between senses of terms. Given a relatedness formula rel(s1, s2) between two senses s1 and s2, term relatedness between two terms t1 and t2 can be calculated as rel(t1, t2) = max rel(s1,s2), 81∈sen(t1),82∈sen(t2) where sen(t) is a set of senses of term t [3]. Automated systems assign a score of semantic relatedness to a given pair of terms calculated from a relatedness measure. The absolute score itself is typically irrelevant on its own, what is important is that the measure assigns a higher score to term pairs which humans think are more related and comparatively lower scores to term pairs that are less related [17]. The best known theories of word semantics fall in three major groups: 1. “Meaning is use” [30]: habitual usage provides indirect contextual interpretation of any term. In accord with Carnap, frequency of use expresses aspects of a </context>
<context position="7964" citStr="[8, 3]" startWordPosition="1225" endWordPosition="1226">ly entails some sort of world view with respect to a given domain. This is often conceived as a set of concepts, their definitions and their inter-relationships; this is referred to as a conceptualization. The following types of resources are commonly used in measuring semantic similarity between terms: dictionary [12], semantic networks, such as WordNet [5], thesauri modeled on Roget’s Thesaurus [19]. All approaches to measuring semantic relatedness that use a lexical resource regard the resource as a network or a directed graph, making use of the structural information embedded in the graph [8, 3]. Distributional similarity, as studied by language technology, covers an important kind of theories of word meaning and can be hence seen as contributing to semantic document indexing and retrieval. Its predecessors go back a long way, building on the notion of term dependence and structures derived therefrom [2, 18]. Also called the contextual theory of meaning (see [15] for the historical development of the concept), the underlying distributional hypothesis is often cited for explaining how word meaning enters information processing [10], and basically equals the claim “meaning is use” in l</context>
</contexts>
<marker>[3]</marker>
<rawString>A. Budanitsky and G. Hirst. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Cristianini</author>
<author>J Shawe-Taylor</author>
<author>H Lodhi</author>
</authors>
<title>Latent semantic kernels.</title>
<date>2002</date>
<journal>Journal of Intelligent Information Systems,</journal>
<volume>18</volume>
<issue>2</issue>
<contexts>
<context position="1912" citStr="[9, 4]" startWordPosition="286" endWordPosition="287"> at the same time, increasing one is done at the expense of the other. For example, casting a wider net of 235 Proceedings of the 8th International Conference on Computational Semantics, pages 235–247, Tilburg, January 2009. c�2009 International Conference on Computational Semantics search terms to improve recall of relevant items will also bring in an even greater proportion of irrelevant items, lowering precision. In the meantime, practical applications in information retrieval and text classification have been proliferating, especially with developments in kernel methods in the last decade [9, 4]. Ordering of terms based on semantic relatedness seeks an answer to the simple question, can statistical term weighting be eclipsed? Namely, variants of weighting schemes based on term occurrences and co-occurrences dominate the information retrieval and text classification scenes. However, they also have a number of limitations. The connection between statistics and word semantics is in general not understood very well. In other words, a systematic discussion of mappings between theories of word meaning and modeling them by mathematical objects is missing for the time being. Further, enrichi</context>
</contexts>
<marker>[4]</marker>
<rawString>N. Cristianini, J. Shawe-Taylor, and H. Lodhi. Latent semantic kernels. Journal of Intelligent Information Systems, 18(2):127–152, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA, USA,</location>
<contexts>
<context position="7718" citStr="[5]" startWordPosition="1186" endWordPosition="1186">e theories have been individually shown to be good, therefore their combination must be a valid research alternative. A lexical resource in computer science is a structure that captures semantic relations among terms. Such a resource necessarily entails some sort of world view with respect to a given domain. This is often conceived as a set of concepts, their definitions and their inter-relationships; this is referred to as a conceptualization. The following types of resources are commonly used in measuring semantic similarity between terms: dictionary [12], semantic networks, such as WordNet [5], thesauri modeled on Roget’s Thesaurus [19]. All approaches to measuring semantic relatedness that use a lexical resource regard the resource as a network or a directed graph, making use of the structural information embedded in the graph [8, 3]. Distributional similarity, as studied by language technology, covers an important kind of theories of word meaning and can be hence seen as contributing to semantic document indexing and retrieval. Its predecessors go back a long way, building on the notion of term dependence and structures derived therefrom [2, 18]. Also called the contextual theory</context>
</contexts>
<marker>[5]</marker>
<rawString>C. Fellbaum. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, MA, USA, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Feature generation for text categorization using world knowledge.</title>
<date>2005</date>
<booktitle>In Proceedings of IJCAI-05, 19th International Joint Conference on Artificial Intelligence,</booktitle>
<volume>19</volume>
<location>Edinburgh, UK,</location>
<contexts>
<context position="12058" citStr="[6]" startWordPosition="1874" endWordPosition="1874"> also been employed to compress the feature space while compromising document classification accuracy [1]. Applying the information bottleneck method to find term clusters that preserve the information about document categories has been shown to increase text classification accuracy in certain cases [28]. On the other hand, term expansion has been widely researched, with varying results [21]. These methods generate new features for each document in the data set. These new features can be synonyms or homonyms of document terms [26], or expanded features for terms, sentences and documents as in [6]. Several distributional criteria have been used to select terms related to the query. For instance, [25] proposed the principle that the selected terms should have a higher probability in the relevant documents than in the irrelevant documents. Others examined the impact of determining expansion terms using a minimum spanning tree and some simple linguistic analysis [29]. This section proposes an algorithm that connects term clustering and term expansion. It employs a pairwise comparison between the terms to find a linear order, instead of finding clusters. In this order, the transition from </context>
</contexts>
<marker>[6]</marker>
<rawString>E. Gabrilovich and S. Markovitch. Feature generation for text categorization using world knowledge. In Proceedings of IJCAI-05, 19th International Joint Conference on Artificial Intelligence, volume 19, Edinburgh, UK, 2005. Lawrence Erlbaum Associates Ltd.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S I Gallant</author>
</authors>
<title>A practical approach for representing context and for performing word sense disambiguation using neural networks.</title>
<date>1991</date>
<journal>Neural Computation,</journal>
<volume>3</volume>
<contexts>
<context position="8819" citStr="[7, 21, 27]" startWordPosition="1356" endWordPosition="1358">g on the notion of term dependence and structures derived therefrom [2, 18]. Also called the contextual theory of meaning (see [15] for the historical development of the concept), the underlying distributional hypothesis is often cited for explaining how word meaning enters information processing [10], and basically equals the claim “meaning is use” in language philosophy. Before attempts to utilize lexical resources for the same purpose, this used to be the sole source of word semantics in information retrieval, inherent in the exploitation of term occurrences (tfidf) and term co-occurrences [7, 21, 27], including multiple-level term co-occurrences [11]. 238 Statistical techniques typically suffer from the sparse data problem: they perform poorly when the terms are relatively rare, due to the scarcity of data. Hybrid approaches attempt to address this problem by supplementing sparse data with information from a lexical database [24, 8]. In a semantic network, to differentiate between the weights of edges connecting a node and all its child nodes, one needs to consider the link strength of each specific child link. This is a situation in which corpus statistics can contribute. Ideally the met</context>
</contexts>
<marker>[7]</marker>
<rawString>S. I. Gallant. A practical approach for representing context and for performing word sense disambiguation using neural networks. Neural Computation, 3:293–309, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date></date>
<booktitle>In Proceedings of the International Conference on Research in Computational Linguistics,</booktitle>
<pages>pages</pages>
<location>Taipei, Taiwan,</location>
<contexts>
<context position="7964" citStr="[8, 3]" startWordPosition="1225" endWordPosition="1226">ly entails some sort of world view with respect to a given domain. This is often conceived as a set of concepts, their definitions and their inter-relationships; this is referred to as a conceptualization. The following types of resources are commonly used in measuring semantic similarity between terms: dictionary [12], semantic networks, such as WordNet [5], thesauri modeled on Roget’s Thesaurus [19]. All approaches to measuring semantic relatedness that use a lexical resource regard the resource as a network or a directed graph, making use of the structural information embedded in the graph [8, 3]. Distributional similarity, as studied by language technology, covers an important kind of theories of word meaning and can be hence seen as contributing to semantic document indexing and retrieval. Its predecessors go back a long way, building on the notion of term dependence and structures derived therefrom [2, 18]. Also called the contextual theory of meaning (see [15] for the historical development of the concept), the underlying distributional hypothesis is often cited for explaining how word meaning enters information processing [10], and basically equals the claim “meaning is use” in l</context>
<context position="9498" citStr="[8]" startWordPosition="1461" endWordPosition="1461">ues typically suffer from the sparse data problem: they perform poorly when the terms are relatively rare, due to the scarcity of data. Hybrid approaches attempt to address this problem by supplementing sparse data with information from a lexical database [24, 8]. In a semantic network, to differentiate between the weights of edges connecting a node and all its child nodes, one needs to consider the link strength of each specific child link. This is a situation in which corpus statistics can contribute. Ideally the method chosen should be both theoretically sound and computationally efficient [8]. Following the notation in information theory, the information content (IC) of a concept c can be quantified as follows. log P(c). where P(c) is the probability of encountering an instance of concept c. In the case of the hierarchical structure, where a concept in the hierarchy subsumes those ones below it, this implies that P(c) is monotonic as one moves up in the hierarchy. As the node’s probability increases, its information content or its informativeness decreases. If there is a unique top node in the hierarchy, then its probability is 1, hence its information content is 0. Given the mono</context>
<context position="11278" citStr="[8]" startWordPosition="1758" endWordPosition="1758">he other hand, it does not differentiate between the similarity values of any pair of concepts in a sub-hierarchy as long as their lowest super-ordinate class is the same. Moreover, in the calculation of information content, a polysemous term will have a large content value if only term frequency data are used. The distance function between two terms can be written as follows: d(t1, t2) = IC(c1) + IC(c2) − 2IC(LSuper(c1, c2)), 1 IC(c) = 239 where LSuper(c1, c2) denotes the lowest super-ordinate of c1 and c2 in a lexical resource. This distance measure also satisfies the properties of a metric [8]. 3 A Semantic Ordering of Terms Traditional distributional term clustering methods do not provide significantly improved text representation [13]. Distributional clustering has also been employed to compress the feature space while compromising document classification accuracy [1]. Applying the information bottleneck method to find term clusters that preserve the information about document categories has been shown to increase text classification accuracy in certain cases [28]. On the other hand, term expansion has been widely researched, with varying results [21]. These methods generate new </context>
</contexts>
<marker>[8]</marker>
<rawString>J.J. Jiang and D.W. Conrath. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the International Conference on Research in Computational Linguistics, pages 19–33, Taipei, Taiwan, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text categorization with support vector machines: Learning with many relevant features.</title>
<date>1998</date>
<booktitle>In Proceedings of ECML-98, 10th European Conference on Machine Learning,</booktitle>
<pages>137--142</pages>
<publisher>Springer-Verlag,</publisher>
<location>Chemnitz, Germany,</location>
<contexts>
<context position="1912" citStr="[9, 4]" startWordPosition="286" endWordPosition="287"> at the same time, increasing one is done at the expense of the other. For example, casting a wider net of 235 Proceedings of the 8th International Conference on Computational Semantics, pages 235–247, Tilburg, January 2009. c�2009 International Conference on Computational Semantics search terms to improve recall of relevant items will also bring in an even greater proportion of irrelevant items, lowering precision. In the meantime, practical applications in information retrieval and text classification have been proliferating, especially with developments in kernel methods in the last decade [9, 4]. Ordering of terms based on semantic relatedness seeks an answer to the simple question, can statistical term weighting be eclipsed? Namely, variants of weighting schemes based on term occurrences and co-occurrences dominate the information retrieval and text classification scenes. However, they also have a number of limitations. The connection between statistics and word semantics is in general not understood very well. In other words, a systematic discussion of mappings between theories of word meaning and modeling them by mathematical objects is missing for the time being. Further, enrichi</context>
</contexts>
<marker>[9]</marker>
<rawString>T. Joachims. Text categorization with support vector machines: Learning with many relevant features. In Proceedings of ECML-98, 10th European Conference on Machine Learning, pages 137–142, Chemnitz, Germany, April 1998. Springer-Verlag, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Karlgren</author>
<author>M Sahlgren</author>
</authors>
<title>From words to understanding. Foundations of Real-World Intelligence,</title>
<date>2001</date>
<pages>294--308</pages>
<contexts>
<context position="8510" citStr="[10]" startWordPosition="1309" endWordPosition="1309">of the structural information embedded in the graph [8, 3]. Distributional similarity, as studied by language technology, covers an important kind of theories of word meaning and can be hence seen as contributing to semantic document indexing and retrieval. Its predecessors go back a long way, building on the notion of term dependence and structures derived therefrom [2, 18]. Also called the contextual theory of meaning (see [15] for the historical development of the concept), the underlying distributional hypothesis is often cited for explaining how word meaning enters information processing [10], and basically equals the claim “meaning is use” in language philosophy. Before attempts to utilize lexical resources for the same purpose, this used to be the sole source of word semantics in information retrieval, inherent in the exploitation of term occurrences (tfidf) and term co-occurrences [7, 21, 27], including multiple-level term co-occurrences [11]. 238 Statistical techniques typically suffer from the sparse data problem: they perform poorly when the terms are relatively rare, due to the scarcity of data. Hybrid approaches attempt to address this problem by supplementing sparse data </context>
</contexts>
<marker>[10]</marker>
<rawString>J. Karlgren and M. Sahlgren. From words to understanding. Foundations of Real-World Intelligence, pages 294–308, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kontostathis</author>
<author>W M Pottenger</author>
</authors>
<title>A framework for understanding latent semantic indexing (LSI) performance.</title>
<date>2006</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="8870" citStr="[11]" startWordPosition="1363" endWordPosition="1363">therefrom [2, 18]. Also called the contextual theory of meaning (see [15] for the historical development of the concept), the underlying distributional hypothesis is often cited for explaining how word meaning enters information processing [10], and basically equals the claim “meaning is use” in language philosophy. Before attempts to utilize lexical resources for the same purpose, this used to be the sole source of word semantics in information retrieval, inherent in the exploitation of term occurrences (tfidf) and term co-occurrences [7, 21, 27], including multiple-level term co-occurrences [11]. 238 Statistical techniques typically suffer from the sparse data problem: they perform poorly when the terms are relatively rare, due to the scarcity of data. Hybrid approaches attempt to address this problem by supplementing sparse data with information from a lexical database [24, 8]. In a semantic network, to differentiate between the weights of edges connecting a node and all its child nodes, one needs to consider the link strength of each specific child link. This is a situation in which corpus statistics can contribute. Ideally the method chosen should be both theoretically sound and c</context>
</contexts>
<marker>[11]</marker>
<rawString>A. Kontostathis and W.M. Pottenger. A framework for understanding latent semantic indexing (LSI) performance. Information Processing and Management, 42(1):56–73, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone?</title>
<date>1986</date>
<booktitle>In Proceedings of SIGDOC-86, 5th Annual International Conference on Systems Documentation,</booktitle>
<pages>24--26</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="7678" citStr="[12]" startWordPosition="1180" endWordPosition="1180">ifiers of term similarity each [17], These theories have been individually shown to be good, therefore their combination must be a valid research alternative. A lexical resource in computer science is a structure that captures semantic relations among terms. Such a resource necessarily entails some sort of world view with respect to a given domain. This is often conceived as a set of concepts, their definitions and their inter-relationships; this is referred to as a conceptualization. The following types of resources are commonly used in measuring semantic similarity between terms: dictionary [12], semantic networks, such as WordNet [5], thesauri modeled on Roget’s Thesaurus [19]. All approaches to measuring semantic relatedness that use a lexical resource regard the resource as a network or a directed graph, making use of the structural information embedded in the graph [8, 3]. Distributional similarity, as studied by language technology, covers an important kind of theories of word meaning and can be hence seen as contributing to semantic document indexing and retrieval. Its predecessors go back a long way, building on the notion of term dependence and structures derived therefrom [2</context>
</contexts>
<marker>[12]</marker>
<rawString>M. Lesk. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone? In Proceedings of SIGDOC-86, 5th Annual International Conference on Systems Documentation, pages 24–26, New York, NY, USA, 1986. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
</authors>
<title>An evaluation of phrasal and clustered representations on a text categorization task.</title>
<date>1992</date>
<booktitle>In Proceedings of SIGIR-92, 15th ACM International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>37--50</pages>
<publisher>ACM Press,</publisher>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="11424" citStr="[13]" startWordPosition="1778" endWordPosition="1778">rdinate class is the same. Moreover, in the calculation of information content, a polysemous term will have a large content value if only term frequency data are used. The distance function between two terms can be written as follows: d(t1, t2) = IC(c1) + IC(c2) − 2IC(LSuper(c1, c2)), 1 IC(c) = 239 where LSuper(c1, c2) denotes the lowest super-ordinate of c1 and c2 in a lexical resource. This distance measure also satisfies the properties of a metric [8]. 3 A Semantic Ordering of Terms Traditional distributional term clustering methods do not provide significantly improved text representation [13]. Distributional clustering has also been employed to compress the feature space while compromising document classification accuracy [1]. Applying the information bottleneck method to find term clusters that preserve the information about document categories has been shown to increase text classification accuracy in certain cases [28]. On the other hand, term expansion has been widely researched, with varying results [21]. These methods generate new features for each document in the data set. These new features can be synonyms or homonyms of document terms [26], or expanded features for terms,</context>
</contexts>
<marker>[13]</marker>
<rawString>D.D. Lewis. An evaluation of phrasal and clustered representations on a text categorization task. In Proceedings of SIGIR-92, 15th ACM International Conference on Research and Development in Information Retrieval, pages 37–50, Copenhagen, Denmark, June 1992. ACM Press, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL Workshop on Usage of WordNet in Natural Language Processing Systems,</booktitle>
<volume>98</volume>
<pages>768--773</pages>
<publisher>ACL,</publisher>
<location>Montr´eal, Qu´ebec, Canada,</location>
<contexts>
<context position="18696" citStr="[14]" startWordPosition="2999" endWordPosition="2999">ffectiveness decreases after 4 expansions for micro-F1 and after 6 expansions for macro-F1. 5 Conclusions Terms can be corpus- or genre-specific. Manually constructed general-purpose lexical resources include many usages that are infrequent in a particular cor243 Number of Expansion Micro-F1 Macro-F1 Terms 0 0.900 0.826 2 0.901 0.826 4 0.905 0.828 6 0.898 0.830 8 0.896 0.827 Table 1: Micro-Average and Macro F1-measure, Reuters-21578 pus or genre of documents, and therefore of little use. For example, one of the 8 senses of company in WordNet is a visitor/visitant, which is a hyponym of person [14]. This usage of the term is practically never used in newspaper articles, hence distributional attributes should be taken into consideration when creating a linear ordering of terms. Integrating lexical resources into an upgraded semantic weighting scheme that could augment statistical term weighting is a prospect that cannot be overlooked in information retrieval and text categorization. Our first results with such a scheme in text categorization. At the same time, the results also raise the question, does assigning specific scalar values to terms in an ontology, this far represented by their</context>
</contexts>
<marker>[14]</marker>
<rawString>D. Lin. Automatic retrieval and clustering of similar words. In Proceedings of COLING-ACL Workshop on Usage of WordNet in Natural Language Processing Systems, volume 98, pages 768–773, Montr´eal, Qu´ebec, Canada, August 1998. ACL, Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lyons</author>
</authors>
<title>Semantics.</title>
<date>1977</date>
<publisher>Cambridge University Press,</publisher>
<location>New York, NY, USA,</location>
<contexts>
<context position="8339" citStr="[15]" startWordPosition="1285" endWordPosition="1285">d on Roget’s Thesaurus [19]. All approaches to measuring semantic relatedness that use a lexical resource regard the resource as a network or a directed graph, making use of the structural information embedded in the graph [8, 3]. Distributional similarity, as studied by language technology, covers an important kind of theories of word meaning and can be hence seen as contributing to semantic document indexing and retrieval. Its predecessors go back a long way, building on the notion of term dependence and structures derived therefrom [2, 18]. Also called the contextual theory of meaning (see [15] for the historical development of the concept), the underlying distributional hypothesis is often cited for explaining how word meaning enters information processing [10], and basically equals the claim “meaning is use” in language philosophy. Before attempts to utilize lexical resources for the same purpose, this used to be the sole source of word semantics in information retrieval, inherent in the exploitation of term occurrences (tfidf) and term co-occurrences [7, 21, 27], including multiple-level term co-occurrences [11]. 238 Statistical techniques typically suffer from the sparse data pr</context>
</contexts>
<marker>[15]</marker>
<rawString>J. Lyons. Semantics. Cambridge University Press, New York, NY, USA, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
<author>W Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<journal>Language and Cognitive Processes,</journal>
<volume>6</volume>
<issue>1</issue>
<contexts>
<context position="4843" citStr="[16]" startWordPosition="736" endWordPosition="736">, and Section 4 both offers first results in text classification and discusses some implications. Finally, Section 5 concludes the paper. 2 Measuring Semantic Relatedness Several methods have been proposed for measuring similarity. One of such early proposals was the semantic differential which analyzes the affective meaning of terms into a range of different dimensions with the opposed adjectives at both ends, and locates the terms within semantic space [20]. Semantic similarity as proposed by Miller and Charles is a continuous variable that describes the degree of synonymy between two terms [16]. They argue that native speakers can order pairs of terms by semantic similarity, for example ship-vessel, ship-watercraft, ship-riverboat, ship-sail, ship-house, ship-dog, ship-sun. This concept may be extended to quantify relations between non-synonymous but closely related terms, for example airplanewing. Semantic distance is the inverse of semantic similarity [17]. Semantic relatedness is defined between senses of terms. Given a relatedness formula rel(s1, s2) between two senses s1 and s2, term relatedness between two terms t1 and t2 can be calculated as rel(t1, t2) = max rel(s1,s2), 81∈s</context>
</contexts>
<marker>[16]</marker>
<rawString>G. Miller and W. Charles. Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Mohammad</author>
<author>G Hirst</author>
</authors>
<title>Distributional measures as proxies for semantic relatedness.</title>
<date>2005</date>
<note>Submitted for publication,</note>
<contexts>
<context position="5214" citStr="[17]" startWordPosition="785" endWordPosition="785">s with the opposed adjectives at both ends, and locates the terms within semantic space [20]. Semantic similarity as proposed by Miller and Charles is a continuous variable that describes the degree of synonymy between two terms [16]. They argue that native speakers can order pairs of terms by semantic similarity, for example ship-vessel, ship-watercraft, ship-riverboat, ship-sail, ship-house, ship-dog, ship-sun. This concept may be extended to quantify relations between non-synonymous but closely related terms, for example airplanewing. Semantic distance is the inverse of semantic similarity [17]. Semantic relatedness is defined between senses of terms. Given a relatedness formula rel(s1, s2) between two senses s1 and s2, term relatedness between two terms t1 and t2 can be calculated as rel(t1, t2) = max rel(s1,s2), 81∈sen(t1),82∈sen(t2) where sen(t) is a set of senses of term t [3]. Automated systems assign a score of semantic relatedness to a given pair of terms calculated from a relatedness measure. The absolute score itself is typically irrelevant on its own, what is important is that the measure assigns a higher score to term pairs which humans think are more related and comparat</context>
<context position="7109" citStr="[17]" startWordPosition="1093" endWordPosition="1093">change”: the stimulus-response theory by Bloomfield and the biological theory of meaning by von Uexk¨ull both stress that the meaning of any action is its consequences. 3. “Meaning is equivalence”: referential or ostensional theories of meaning suggest that ’X = Y for/as long as Z’ [22]. Point 2 refers to theories which assign a temporal structure to word meaning, they are not discussed here. Measures that rely on distributional measures (Point 1) and those that use knowledge-rich resources (Point 3) both exist, and they have been individually shown to good quantifiers of term similarity each [17], These theories have been individually shown to be good, therefore their combination must be a valid research alternative. A lexical resource in computer science is a structure that captures semantic relations among terms. Such a resource necessarily entails some sort of world view with respect to a given domain. This is often conceived as a set of concepts, their definitions and their inter-relationships; this is referred to as a conceptualization. The following types of resources are commonly used in measuring semantic similarity between terms: dictionary [12], semantic networks, such as Wo</context>
</contexts>
<marker>[17]</marker>
<rawString>S. Mohammad and G. Hirst. Distributional measures as proxies for semantic relatedness. Submitted for publication, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>C Beghtol</author>
<author>G Hirst</author>
</authors>
<title>Term relationships and their contribution to text semantics and information literacy through lexical cohesion.</title>
<date>2003</date>
<booktitle>In Proceedings of the 31st Annual Conference of the Canadian Association for Information Science,</booktitle>
<location>Halifax, Nova Scotia, Canada,</location>
<contexts>
<context position="8283" citStr="[2, 18]" startWordPosition="1275" endWordPosition="1276">2], semantic networks, such as WordNet [5], thesauri modeled on Roget’s Thesaurus [19]. All approaches to measuring semantic relatedness that use a lexical resource regard the resource as a network or a directed graph, making use of the structural information embedded in the graph [8, 3]. Distributional similarity, as studied by language technology, covers an important kind of theories of word meaning and can be hence seen as contributing to semantic document indexing and retrieval. Its predecessors go back a long way, building on the notion of term dependence and structures derived therefrom [2, 18]. Also called the contextual theory of meaning (see [15] for the historical development of the concept), the underlying distributional hypothesis is often cited for explaining how word meaning enters information processing [10], and basically equals the claim “meaning is use” in language philosophy. Before attempts to utilize lexical resources for the same purpose, this used to be the sole source of word semantics in information retrieval, inherent in the exploitation of term occurrences (tfidf) and term co-occurrences [7, 21, 27], including multiple-level term co-occurrences [11]. 238 Statist</context>
</contexts>
<marker>[18]</marker>
<rawString>J. Morris, C. Beghtol, and G. Hirst. Term relationships and their contribution to text semantics and information literacy through lexical cohesion. In Proceedings of the 31st Annual Conference of the Canadian Association for Information Science, Halifax, Nova Scotia, Canada, May 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="7762" citStr="[19]" startWordPosition="1192" endWordPosition="1192">be good, therefore their combination must be a valid research alternative. A lexical resource in computer science is a structure that captures semantic relations among terms. Such a resource necessarily entails some sort of world view with respect to a given domain. This is often conceived as a set of concepts, their definitions and their inter-relationships; this is referred to as a conceptualization. The following types of resources are commonly used in measuring semantic similarity between terms: dictionary [12], semantic networks, such as WordNet [5], thesauri modeled on Roget’s Thesaurus [19]. All approaches to measuring semantic relatedness that use a lexical resource regard the resource as a network or a directed graph, making use of the structural information embedded in the graph [8, 3]. Distributional similarity, as studied by language technology, covers an important kind of theories of word meaning and can be hence seen as contributing to semantic document indexing and retrieval. Its predecessors go back a long way, building on the notion of term dependence and structures derived therefrom [2, 18]. Also called the contextual theory of meaning (see [15] for the historical dev</context>
</contexts>
<marker>[19]</marker>
<rawString>J. Morris and G. Hirst. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17(1):21–48, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Osgood</author>
</authors>
<title>The nature and measurement of meaning.</title>
<date>1952</date>
<journal>Psychological Bulletin,</journal>
<volume>49</volume>
<issue>3</issue>
<contexts>
<context position="4702" citStr="[20]" startWordPosition="714" endWordPosition="714">ess with regard to the major linguistic theories. Section 3 introduces an algorithm that creates a linear semantic order of terms of a corpus, and Section 4 both offers first results in text classification and discusses some implications. Finally, Section 5 concludes the paper. 2 Measuring Semantic Relatedness Several methods have been proposed for measuring similarity. One of such early proposals was the semantic differential which analyzes the affective meaning of terms into a range of different dimensions with the opposed adjectives at both ends, and locates the terms within semantic space [20]. Semantic similarity as proposed by Miller and Charles is a continuous variable that describes the degree of synonymy between two terms [16]. They argue that native speakers can order pairs of terms by semantic similarity, for example ship-vessel, ship-watercraft, ship-riverboat, ship-sail, ship-house, ship-dog, ship-sun. This concept may be extended to quantify relations between non-synonymous but closely related terms, for example airplanewing. Semantic distance is the inverse of semantic similarity [17]. Semantic relatedness is defined between senses of terms. Given a relatedness formula r</context>
</contexts>
<marker>[20]</marker>
<rawString>C.E. Osgood. The nature and measurement of meaning. Psychological Bulletin, 49(3):197–237, 1952.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H J Peat</author>
<author>P Willett</author>
</authors>
<title>The limitations of term co-occurrence data for query expansion in document retrieval systems.</title>
<date>1991</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>42</volume>
<issue>5</issue>
<contexts>
<context position="8819" citStr="[7, 21, 27]" startWordPosition="1356" endWordPosition="1358">g on the notion of term dependence and structures derived therefrom [2, 18]. Also called the contextual theory of meaning (see [15] for the historical development of the concept), the underlying distributional hypothesis is often cited for explaining how word meaning enters information processing [10], and basically equals the claim “meaning is use” in language philosophy. Before attempts to utilize lexical resources for the same purpose, this used to be the sole source of word semantics in information retrieval, inherent in the exploitation of term occurrences (tfidf) and term co-occurrences [7, 21, 27], including multiple-level term co-occurrences [11]. 238 Statistical techniques typically suffer from the sparse data problem: they perform poorly when the terms are relatively rare, due to the scarcity of data. Hybrid approaches attempt to address this problem by supplementing sparse data with information from a lexical database [24, 8]. In a semantic network, to differentiate between the weights of edges connecting a node and all its child nodes, one needs to consider the link strength of each specific child link. This is a situation in which corpus statistics can contribute. Ideally the met</context>
<context position="11849" citStr="[21]" startWordPosition="1837" endWordPosition="1837">es the properties of a metric [8]. 3 A Semantic Ordering of Terms Traditional distributional term clustering methods do not provide significantly improved text representation [13]. Distributional clustering has also been employed to compress the feature space while compromising document classification accuracy [1]. Applying the information bottleneck method to find term clusters that preserve the information about document categories has been shown to increase text classification accuracy in certain cases [28]. On the other hand, term expansion has been widely researched, with varying results [21]. These methods generate new features for each document in the data set. These new features can be synonyms or homonyms of document terms [26], or expanded features for terms, sentences and documents as in [6]. Several distributional criteria have been used to select terms related to the query. For instance, [25] proposed the principle that the selected terms should have a higher probability in the relevant documents than in the irrelevant documents. Others examined the impact of determining expansion terms using a minimum spanning tree and some simple linguistic analysis [29]. This section pr</context>
</contexts>
<marker>[21]</marker>
<rawString>H.J. Peat and P. Willett. The limitations of term co-occurrence data for query expansion in document retrieval systems. Journal of the American Society for Information Science, 42(5):378–383, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Peirce</author>
</authors>
<title>Logic as semiotic: The theory of signs. Philosophical Writings of Peirce,</title>
<date>1955</date>
<pages>98--119</pages>
<contexts>
<context position="6792" citStr="[22]" startWordPosition="1043" endWordPosition="1043">s value extensions (classes) and index terms as value intensions (properties) of a (semantic) function 237 ’f’. Extensions and intensions are inverse proportional: the more properties defined, the less entities they apply to - there are more flowers in general than tulips in particular, for instance. 2. “Meaning is change”: the stimulus-response theory by Bloomfield and the biological theory of meaning by von Uexk¨ull both stress that the meaning of any action is its consequences. 3. “Meaning is equivalence”: referential or ostensional theories of meaning suggest that ’X = Y for/as long as Z’ [22]. Point 2 refers to theories which assign a temporal structure to word meaning, they are not discussed here. Measures that rely on distributional measures (Point 1) and those that use knowledge-rich resources (Point 3) both exist, and they have been individually shown to good quantifiers of term similarity each [17], These theories have been individually shown to be good, therefore their combination must be a valid research alternative. A lexical resource in computer science is a structure that captures semantic relations among terms. Such a resource necessarily entails some sort of world view</context>
</contexts>
<marker>[22]</marker>
<rawString>C.S. Peirce. Logic as semiotic: The theory of signs. Philosophical Writings of Peirce, pages 98–119, 1955.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V V Raghavan</author>
<author>S K M Wong</author>
</authors>
<title>A critical analysis of vector space model for information retrieval.</title>
<date>1986</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>37</volume>
<issue>5</issue>
<contexts>
<context position="1083" citStr="[23]" startWordPosition="164" endWordPosition="164">istics, or use some other, non-metric method based on a lexical resource. At the same time, a wide range of semantic similarity measures have been developed to support natural language processing tasks such as word sense disambiguation. This paper combines the two approaches and proposes an algorithm that provides a semantic order of terms based on a semantic relatedness measure. This semantic order can be exploited by term weighting and term expansion methods. 1 Introduction Since the early days of the vector space model, it has been debated whether it is a proper carrier of meaning of texts [23], arguing if distributional similarity is an adequate proxy for lexical semantic relatedness [3]. With the statistical, i.e. devoid of word semantics approaches there is generally no way to improve both precision and recall at the same time, increasing one is done at the expense of the other. For example, casting a wider net of 235 Proceedings of the 8th International Conference on Computational Semantics, pages 235–247, Tilburg, January 2009. c�2009 International Conference on Computational Semantics search terms to improve recall of relevant items will also bring in an even greater proportio</context>
</contexts>
<marker>[23]</marker>
<rawString>V.V. Raghavan and S.K.M. Wong. A critical analysis of vector space model for information retrieval. Journal of the American Society for Information Science, 37(5):279–287, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of IJCAI-95, 14th International Joint Conference on Artificial Intelligence,</booktitle>
<volume>1</volume>
<pages>448--453</pages>
<location>Montr´eal, Qu´ebec, Canada,</location>
<contexts>
<context position="9158" citStr="[24, 8]" startWordPosition="1406" endWordPosition="1407"> is use” in language philosophy. Before attempts to utilize lexical resources for the same purpose, this used to be the sole source of word semantics in information retrieval, inherent in the exploitation of term occurrences (tfidf) and term co-occurrences [7, 21, 27], including multiple-level term co-occurrences [11]. 238 Statistical techniques typically suffer from the sparse data problem: they perform poorly when the terms are relatively rare, due to the scarcity of data. Hybrid approaches attempt to address this problem by supplementing sparse data with information from a lexical database [24, 8]. In a semantic network, to differentiate between the weights of edges connecting a node and all its child nodes, one needs to consider the link strength of each specific child link. This is a situation in which corpus statistics can contribute. Ideally the method chosen should be both theoretically sound and computationally efficient [8]. Following the notation in information theory, the information content (IC) of a concept c can be quantified as follows. log P(c). where P(c) is the probability of encountering an instance of concept c. In the case of the hierarchical structure, where a conce</context>
<context position="10669" citStr="[24]" startWordPosition="1655" endWordPosition="1655">tion content is 0. Given the monotonic feature of the information content value, the similarity of two concepts can be formally defined as follows. sim(c1, c2) = max IC(c) = max − log p(c) c∈Sup(c1,c2) c∈Sup(c1,c2) where Sup(c1, c2) is the set of concepts that subsume both c1 and c2. To maximize the representativeness, the similarity value is the information content value of the node whose IC value is the largest among those higher order classes. The information content method requires less information on the detailed structure of a lexical resource and it is insensitive to varying link types [24]. On the other hand, it does not differentiate between the similarity values of any pair of concepts in a sub-hierarchy as long as their lowest super-ordinate class is the same. Moreover, in the calculation of information content, a polysemous term will have a large content value if only term frequency data are used. The distance function between two terms can be written as follows: d(t1, t2) = IC(c1) + IC(c2) − 2IC(LSuper(c1, c2)), 1 IC(c) = 239 where LSuper(c1, c2) denotes the lowest super-ordinate of c1 and c2 in a lexical resource. This distance measure also satisfies the properties of a m</context>
</contexts>
<marker>[24]</marker>
<rawString>P. Resnik. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of IJCAI-95, 14th International Joint Conference on Artificial Intelligence, volume 1, pages 448–453, Montr´eal, Qu´ebec, Canada, August 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S E Robertson</author>
</authors>
<title>On term selection for query expansion.</title>
<date>1990</date>
<journal>Journal of Documentation,</journal>
<volume>46</volume>
<issue>4</issue>
<contexts>
<context position="12163" citStr="[25]" startWordPosition="1890" endWordPosition="1890">]. Applying the information bottleneck method to find term clusters that preserve the information about document categories has been shown to increase text classification accuracy in certain cases [28]. On the other hand, term expansion has been widely researched, with varying results [21]. These methods generate new features for each document in the data set. These new features can be synonyms or homonyms of document terms [26], or expanded features for terms, sentences and documents as in [6]. Several distributional criteria have been used to select terms related to the query. For instance, [25] proposed the principle that the selected terms should have a higher probability in the relevant documents than in the irrelevant documents. Others examined the impact of determining expansion terms using a minimum spanning tree and some simple linguistic analysis [29]. This section proposes an algorithm that connects term clustering and term expansion. It employs a pairwise comparison between the terms to find a linear order, instead of finding clusters. In this order, the transition from a term to an adjacent one is “smooth” if the semantic distance between two neighboring terms is small. Th</context>
</contexts>
<marker>[25]</marker>
<rawString>S.E. Robertson. On term selection for query expansion. Journal of Documentation, 46(4):359–364, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M D E B Rodriguez</author>
<author>J M G Hidalgo</author>
</authors>
<title>Using WordNet to complement training information in text categorisation.</title>
<date>1997</date>
<booktitle>In Procedings of RANLP-97, 2nd International Conference on Recent Advances in Natural Language Processing. John Benjamins Publishing,</booktitle>
<location>Amsterdam, The Netherlands,</location>
<contexts>
<context position="11991" citStr="[26]" startWordPosition="1862" endWordPosition="1862">tly improved text representation [13]. Distributional clustering has also been employed to compress the feature space while compromising document classification accuracy [1]. Applying the information bottleneck method to find term clusters that preserve the information about document categories has been shown to increase text classification accuracy in certain cases [28]. On the other hand, term expansion has been widely researched, with varying results [21]. These methods generate new features for each document in the data set. These new features can be synonyms or homonyms of document terms [26], or expanded features for terms, sentences and documents as in [6]. Several distributional criteria have been used to select terms related to the query. For instance, [25] proposed the principle that the selected terms should have a higher probability in the relevant documents than in the irrelevant documents. Others examined the impact of determining expansion terms using a minimum spanning tree and some simple linguistic analysis [29]. This section proposes an algorithm that connects term clustering and term expansion. It employs a pairwise comparison between the terms to find a linear orde</context>
</contexts>
<marker>[26]</marker>
<rawString>M.D.E.B. Rodriguez and J.M.G. Hidalgo. Using WordNet to complement training information in text categorisation. In Procedings of RANLP-97, 2nd International Conference on Recent Advances in Natural Language Processing. John Benjamins Publishing, Amsterdam, The Netherlands, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schutze</author>
<author>T Pedersen</author>
</authors>
<title>A co-occurrence-based thesaurus and two applications to information retrieval.</title>
<date>1997</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>3</volume>
<issue>33</issue>
<contexts>
<context position="8819" citStr="[7, 21, 27]" startWordPosition="1356" endWordPosition="1358">g on the notion of term dependence and structures derived therefrom [2, 18]. Also called the contextual theory of meaning (see [15] for the historical development of the concept), the underlying distributional hypothesis is often cited for explaining how word meaning enters information processing [10], and basically equals the claim “meaning is use” in language philosophy. Before attempts to utilize lexical resources for the same purpose, this used to be the sole source of word semantics in information retrieval, inherent in the exploitation of term occurrences (tfidf) and term co-occurrences [7, 21, 27], including multiple-level term co-occurrences [11]. 238 Statistical techniques typically suffer from the sparse data problem: they perform poorly when the terms are relatively rare, due to the scarcity of data. Hybrid approaches attempt to address this problem by supplementing sparse data with information from a lexical database [24, 8]. In a semantic network, to differentiate between the weights of edges connecting a node and all its child nodes, one needs to consider the link strength of each specific child link. This is a situation in which corpus statistics can contribute. Ideally the met</context>
</contexts>
<marker>[27]</marker>
<rawString>H. Schutze and T. Pedersen. A co-occurrence-based thesaurus and two applications to information retrieval. Information Processing and Management, 3(33):307–318, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Slonim</author>
<author>N Tishby</author>
</authors>
<title>The power of word clusters for text classification.</title>
<date>2001</date>
<booktitle>In Proceedings of ECIR-01, 23rd European Colloquium on Information Retrieval Research,</booktitle>
<location>Darmstadt, Germany,</location>
<contexts>
<context position="11760" citStr="[28]" startWordPosition="1823" endWordPosition="1823">est super-ordinate of c1 and c2 in a lexical resource. This distance measure also satisfies the properties of a metric [8]. 3 A Semantic Ordering of Terms Traditional distributional term clustering methods do not provide significantly improved text representation [13]. Distributional clustering has also been employed to compress the feature space while compromising document classification accuracy [1]. Applying the information bottleneck method to find term clusters that preserve the information about document categories has been shown to increase text classification accuracy in certain cases [28]. On the other hand, term expansion has been widely researched, with varying results [21]. These methods generate new features for each document in the data set. These new features can be synonyms or homonyms of document terms [26], or expanded features for terms, sentences and documents as in [6]. Several distributional criteria have been used to select terms related to the query. For instance, [25] proposed the principle that the selected terms should have a higher probability in the relevant documents than in the irrelevant documents. Others examined the impact of determining expansion term</context>
</contexts>
<marker>[28]</marker>
<rawString>N. Slonim and N. Tishby. The power of word clusters for text classification. In Proceedings of ECIR-01, 23rd European Colloquium on Information Retrieval Research, Darmstadt, Germany, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A F Smeaton</author>
<author>C J van Rijsbergen</author>
</authors>
<title>The retrieval effects of query expansion on a feedback document retrieval system.</title>
<date>1983</date>
<journal>The Computer Journal,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="12432" citStr="[29]" startWordPosition="1932" endWordPosition="1932"> varying results [21]. These methods generate new features for each document in the data set. These new features can be synonyms or homonyms of document terms [26], or expanded features for terms, sentences and documents as in [6]. Several distributional criteria have been used to select terms related to the query. For instance, [25] proposed the principle that the selected terms should have a higher probability in the relevant documents than in the irrelevant documents. Others examined the impact of determining expansion terms using a minimum spanning tree and some simple linguistic analysis [29]. This section proposes an algorithm that connects term clustering and term expansion. It employs a pairwise comparison between the terms to find a linear order, instead of finding clusters. In this order, the transition from a term to an adjacent one is “smooth” if the semantic distance between two neighboring terms is small. The dimension of the feature space is not compressed, yet, groups of adjacent terms can be regarded as semantic clusters. Hence, following the idea of term expansion, adjacent terms can help to improve the effectiveness of any vector space-based language technology. Let </context>
</contexts>
<marker>[29]</marker>
<rawString>A.F. Smeaton and C.J. van Rijsbergen. The retrieval effects of query expansion on a feedback document retrieval system. The Computer Journal, 26(3):239–246, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Wittgenstein</author>
</authors>
<title>Philosophical Investigations.</title>
<date>1967</date>
<publisher>Blackwell Publishing,</publisher>
<location>Oxford, UK,</location>
<contexts>
<context position="5969" citStr="[30]" startWordPosition="913" endWordPosition="913">en two terms t1 and t2 can be calculated as rel(t1, t2) = max rel(s1,s2), 81∈sen(t1),82∈sen(t2) where sen(t) is a set of senses of term t [3]. Automated systems assign a score of semantic relatedness to a given pair of terms calculated from a relatedness measure. The absolute score itself is typically irrelevant on its own, what is important is that the measure assigns a higher score to term pairs which humans think are more related and comparatively lower scores to term pairs that are less related [17]. The best known theories of word semantics fall in three major groups: 1. “Meaning is use” [30]: habitual usage provides indirect contextual interpretation of any term. In accord with Carnap, frequency of use expresses aspects of a conceptual hierarchy. In terms of logical semantics, one regards document groups as value extensions (classes) and index terms as value intensions (properties) of a (semantic) function 237 ’f’. Extensions and intensions are inverse proportional: the more properties defined, the less entities they apply to - there are more flowers in general than tulips in particular, for instance. 2. “Meaning is change”: the stimulus-response theory by Bloomfield and the biol</context>
</contexts>
<marker>[30]</marker>
<rawString>L. Wittgenstein. Philosophical Investigations. Blackwell Publishing, Oxford, UK, 1967.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>