<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001055">
<title confidence="0.98903">
Textual Entailment Features for Machine Translation Evaluation
</title>
<author confidence="0.999383">
Sebastian Pad´o, Michel Galley, Dan Jurafsky, Christopher D. Manning*
</author>
<affiliation confidence="0.989479">
Stanford University
</affiliation>
<email confidence="0.997487">
{pado,mgalley,jurafsky,manning}@stanford.edu
</email>
<sectionHeader confidence="0.997371" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999348583333333">
We present two regression models for the prediction
of pairwise preference judgments among MT hy-
potheses. Both models are based on feature sets that
are motivated by textual entailment and incorporate
lexical similarity as well as local syntactic features
and specific semantic phenomena. One model pre-
dicts absolute scores; the other one direct pairwise
judgments. We find that both models are compet-
itive with regression models built over the scores
of established MT evaluation metrics. Further data
analysis clarifies the complementary behavior of the
two feature sets.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999872846153846">
Automatic metrics to assess the quality of machine trans-
lations have been a major enabler in improving the per-
formance of MT systems, leading to many varied ap-
proaches to develop such metrics. Initially, most metrics
judged the quality of MT hypotheses by token sequence
match (cf. BLEU (Papineni et al., 2002), NIST (Dod-
dington, 2002). These measures rate systems hypothe-
ses by measuring the overlap in surface word sequences
shared between hypothesis and reference translation.
With improvements in the state-of-the-art in machine
translation, the effectiveness of purely surface-oriented
measures has been questioned (see e.g., Callison-Burch
et al. (2006)). In response, metrics have been proposed
that attempt to integrate more linguistic information
into the matching process to distinguish linguistically li-
censed from unwanted variation (Gim´enez and M`arquez,
2008). However, there is little agreement on what types
of knowledge are helpful: Some suggestions concen-
trate on lexical information, e.g., by the integration of
word similarity information as in Meteor (Banerjee and
Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other
proposals use structural information such as dependency
edges (Owczarzak et al., 2007).
In this paper, we investigate an MT evaluation metric
that is inspired by the similarity between this task and
the textual entailment task (Dagan et al., 2005), which
</bodyText>
<footnote confidence="0.97442275">
*This paper is based on work funded by the Defense Ad-
vanced Research Projects Agency through IBM. The content
does not necessarily reflect the views of the U.S. Government,
and no official endorsement should be inferred..
</footnote>
<figure confidence="0.356305">
HYP: The virus did not infect anybody.
HYP: Virus was infected.
no entailment no entailment
REF: No one was infected by the virus.
</figure>
<figureCaption confidence="0.692093666666667">
Figure 1: Entailment status between an MT system hy-
pothesis and a reference translation for good translations
(above) and bad translations (below).
</figureCaption>
<bodyText confidence="0.999859714285714">
suggests that the quality of an MT hypothesis should be
predictable by a combination of lexical and structural
features that model the matches and mismatches be-
tween system output and reference translation. We use
supervised regression models to combine these features
and analyze feature weights to obtain further insights
into the usefulness of different feature types.
</bodyText>
<sectionHeader confidence="0.979872" genericHeader="method">
2 Textual Entailment for MT Evaluation
</sectionHeader>
<subsectionHeader confidence="0.936617">
2.1 Textual Entailment vs. MT Evaluation
</subsectionHeader>
<bodyText confidence="0.999942095238095">
Textual entailment (TE) was introduced by Dagan et
al. (2005) as a concept that corresponds more closely
to “common sense” reasoning than classical, categorical
entailment. Textual entailment is defined as a relation
between two natural language sentences (a premise P
and a hypothesis H) that holds if a human reading P
would infer that H is most likely true.
Information about the presence or absence of entail-
ment between two sentences has been found to be ben-
eficial for a range of NLP tasks such as Word Sense
Disambiguation or Question Answering (Dagan et al.,
2006; Harabagiu and Hickl, 2006). Our intuition is that
this idea can also be fruitful in MT Evaluation, as illus-
trated in Figure 1. Very good MT output should entail
the reference translation. In contrast, missing hypothesis
material breaks forward entailment; additional material
breaks backward entailment; and for bad translations,
entailment fails in both directions.
Work on the recognition of textual entailment (RTE)
has consistently found that the integration of more syn-
tactic and semantic knowledge can yield gains over
</bodyText>
<figure confidence="0.355617">
REF: No one was infected by the virus.
entailment entailment
</figure>
<footnote confidence="0.1702965">
Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 37–41,
Athens, Greece, 30 March – 31 March 2009. c�2009 Association for Computational Linguistics
</footnote>
<page confidence="0.999446">
37
</page>
<bodyText confidence="0.9998605625">
surface-based methods, provided that the linguistic anal-
ysis was sufficiently robust. Thus, for RTE, “deep”
matching outperforms surface matching. The reason is
that linguistic representation makes it considerably eas-
ier to distinguish admissible variation (i.e., paraphrase)
from true, meaning-changing divergence. Admissible
variation may be lexical (synonymy), structural (word
and phrase placement), or both (diathesis alternations).
The working hypothesis of this paper is that the ben-
efits of deeper analysis carry over to MT evaluation.
More specifically, we test whether the features that al-
low good performance on the RTE task can also predict
human judgments for MT output. Analogously to RTE,
these features should help us to differentiate meaning
preserving translation variants from bad translations.
Nevertheless, there are also substantial differences
between TE and MT evaluation. Crucially, TE assumes
the premise and hypothesis to be well-formed sentences,
which is not true in MT evaluation. Thus, a possible crit-
icism to the use of TE methods is that the features could
become unreliable for ill-formed MT output. However,
there is a second difference between the tasks that works
to our advantage. Due to its strict compositional nature,
TE requires an accurate semantic analysis of all sentence
parts, since, for example, one misanalysed negation or
counterfactual embedding can invert the entailment sta-
tus (MacCartney and Manning, 2008). In contrast, hu-
man MT judgments behave more additively: failure of a
translation with respect to a single semantic dimension
(e.g., polarity or tense) degrades its quality, but usually
not crucially so. We therefore expect that even noisy
entailment features can be predictive in MT evaluation.
</bodyText>
<subsectionHeader confidence="0.99124">
2.2 Entailment-based prediction of MT quality
</subsectionHeader>
<bodyText confidence="0.999958583333334">
Regression-based prediction. Experiences from the
annotation of MT quality judgments show that human
raters have difficulty in consistently assigning absolute
scores to MT system output, due to the number of ways
in which MT output can deviate. Thus, the human an-
notation for the WMT 2008 dataset was collected in
the form of binary pairwise preferences that are con-
siderably easier to make (Callison-Burch et al., 2008).
This section presents two models for the prediction of
pairwise preferences.
The first model (ABS) is a regularized linear regres-
sion model over entailment-motivated features (see be-
low) that predicts an absolute score for each reference-
hypothesis pair. Pairwise preferences are created simply
by comparing the absolute predicted scores. This model
is more general, since it can also be used where absolute
score predictions are desirable; furthermore, the model
is efficient with a runtime linear in the number of sys-
tems and corpus size. On the downside, this model is
not optimized for the prediction of pairwise judgments.
The second model we consider is a regularized logis-
tic regression model (PAIR) that is directly optimized to
predict a weighted binary preference for each hypothe-
sis pair. This model is less efficient since its runtime is
</bodyText>
<table confidence="0.933220125">
Alignment score(3) Unaligned material (10)
Adjuncts (7) Apposition (2)
Modality (5) Factives (8)
Polarity (5) Quantors (4)
Tense (2) Dates (6)
Root (2) Semantic Relations (4)
Semantic relatedness (7) Structural Match (5)
Compatibility of locations and entities (4)
</table>
<tableCaption confidence="0.907268">
Table 1: Entailment feature groups provided by the
Stanford RTE system, with number of features
</tableCaption>
<bodyText confidence="0.999739478260869">
quadratic in the number of systems. On the other hand,
it can be trained on more reliable pairwise preference
judgments. In a second step, we combine the individ-
ual decisions to compute the highest-likelihood total
ordering of hypotheses. The construction of an optimal
ordering from weighted pairwise preferences is an NP-
hard problem (via reduction of CYCLIC-ORDERING;
Barzilay and Elhadad, 2002), but a greedy search yields
a close approximation (Cohen et al., 1999).
Both models can be used to predict system-level
scores from sentence-level scores. Again, we have two
method for doing this. The basic method (BASIC) pre-
dicts the quality of each system directly as the percent-
age of sentences for which its output was rated best
among all systems. However, we noticed that the man-
ual rankings for the WMT 2007 dataset show a tie for
best system for almost 30% of sentences. BASIC is
systematically unable to account for these ties. We
therefore implemented a “tie-aware” prediction method
(WITHTIES) that uses the same sentence-level output as
BASIC, but computes system-level quality differently,
as the percentage of sentences where the system’s hy-
pothesis was scored better or at most a worse than the
best system, for some global “tie interval” E.
Features. We use the Stanford RTE system (MacCart-
ney et al., 2006) to generate a set of entailment features
(RTE) for each pair of MT hypothesis and reference
translation. Features are generated in both directions
to avoid biases towards short or long translations. The
Stanford RTE system uses a three-stage architecture.
It (a) constructs a robust, dependency-based linguistic
analysis of the two sentences; (b) identifies the best
alignment between the two dependency graphs given
similarity scores from a range of lexical resources, us-
ing a Markov Chain Monte Carlo sampling strategy;
and (c) computes roughly 75 features over the aligned
pair of dependency graphs. The different feature groups
are shown in Table 1. A small number features are
real-valued, measuring different quality aspects of the
alignment. The other features are binary, indicating
matches and mismatches of different types (e.g., align-
ment between predicates embedded under compatible
or incompatible modals, respectively).
To judge to what extent the entailment-based model
delivers improvements that cannot be obtained with es-
tablished methods, we also experiment with a feature set
</bodyText>
<page confidence="0.996027">
38
</page>
<bodyText confidence="0.999866">
formed from a set of established MT evaluation metrics
(TRADMT). We combine different parametrization of
(smoothed) BLEU (Papineni et al., 2002), NIST (Dod-
dington, 2002), and TER (Snover et al., 2006), to give
a total of roughly 100 features. Finally, we consider a
combination of both feature sets (COMB).
</bodyText>
<sectionHeader confidence="0.998388" genericHeader="method">
3 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.99989106122449">
Setup. To assess and compare the performance of our
models, we use corpora that were created by past in-
stances of the WMT workshop. We optimize the feature
weights for the ABS models on the WMT 2006 and
2007 absolute score annotations, and correspondingly
for the PAIR models on the WMT 2007 absolute score
and ranking annotations. All models are evaluated on
WMT 2008 to compare against the published results.
Finally, we need to set the tie interval ε. Since we
did not want to optimize ε, we simply assumed that the
percentage of ties observed on WMT 2007 generalizes
to test sets such as the 2008 dataset. We set ε so that
there are ties for first place on 30% of the sentences,
with good practical success (see below).
Results. Table 2 shows our results. The first results
column (Cons) shows consistency, i.e., accuracy in pre-
dicting human pairwise preference judgments. Note that
the performance of a random baseline is not at 50%, but
substantially lower. This is due to (a) the presence of
contradictions and ties in the human judgments, which
cannot be predicted; and (b) WMT’s requirement to
compute a total ordering of all translations for a given
sentence (rather than independent binary judgments),
which introduces transitivity constraints. See Callison-
Burch et al. (2008) for details. Among our models, PAIR
shows a somewhat better consistency than ABS, as can
be expected from a model directly optimized on pair-
wise judgments. Across feature sets, COMB works best
with a consistency of 0.53, competitive with published
WMT 2008 results.
The two final columns (BASIC and WITHTIES) show
Spearman’s ρ for the correlation between human judg-
ments and the two types of system-level predictions.
For BASIC system-level predictions, we find that
PAIR performs considerably worse than ABS, by a mar-
gin of up to ρ = 0.1. Recall that the system-level analy-
sis considers only the top-ranked hypotheses; apparently,
a model optimized on pairwise judgments has a harder
time choosing the best among the top-ranked hypothe-
ses. This interpretation is supported by the large benefit
that PAIR derives from explicit tie modeling. ABS gains
as well, although not as much, so that the correlation of
the tie-aware predictions is similar for ABS and PAIR.
Comparing different feature sets, BASIC show a simi-
lar pattern to the consistency figures. There is no clear
winner between RTE and TRADMT. The performance
of TRADMT is considerably better than the performance
of BLEU and TER in the WMT 2008 evaluation, where
ρ &lt; 0.55. RTE is able to match the performance of an
</bodyText>
<tableCaption confidence="0.8031995">
Table 2: Evaluation on the WMT 2008 dataset for our
regression models, compared to results from WMT 2008
</tableCaption>
<bodyText confidence="0.998821837209303">
ensemble of state-of-the-art metrics, which validates our
hope that linguistically motivated entailment features
are sufficiently robust to make a positive contribution
in MT evaluation. Furthermore, the two individual fea-
ture sets are outperformed by the combined feature set
COMB. We interpret this as support for our regression-
based combination approach.
Moving to WITHTIES, we see the best results from
the RTE model which improves by Δρ = 0.06 for ABS
and Δρ = 0.11 for PAIR. There is less improvement for
the other feature sets, in particular COMB. We submitted
the two overall best models, ABS-RTE and PAIR-RTE
with tie-aware prediction, to the WMT 2009 challenge.
Data Analysis. We analyzed at the models’ predic-
tions to gain a better understanding of the differences in
the behavior of TRADMT-based and RTE-based mod-
els. As a first step, we computed consistency numbers
for the set of “top” translations (hypotheses that were
ranked highest for a given reference) and for the set
of “bottom” translations (hypotheses that were ranked
worst for a given reference). We found small but con-
sistent differences between the models: RTE performs
about 1.5 percent better on the top hypotheses than on
the bottom translations. We found the inverse effect for
the TRADMT model, which performs 2 points worse on
the top hypotheses than on the bottom hypotheses. Re-
visiting our initial concern that the entailment features
are too noisy for very bad translations, this finding indi-
cates some ungrammaticality-induced degradation for
the entailment features, but not much. Conversely, these
numbers also provide support for our initial hypothesis
that surface-based features are good at detecting very
deviant translations, but can have trouble dealing with
legitimate linguistic variation.
Next, we analyzed the average size of the score dif-
ferences between the best and second-best hypotheses
for correct and incorrect predictions. We found that the
RTE-based model predicted on average almost twice the
difference for correct predictions (Δ = 0.30) than for
incorrect predictions (Δ = 0.16), while the difference
was considerably smaller for the TRADMT-based model
(Δ = 0.17 for correct vs. Δ = 0.13 for incorrect). We
believe it is this better discrimination on the top hypothe-
</bodyText>
<figure confidence="0.998948534883721">
Model
Feature set
BASIC
(ρ)
WITHTIES
(ρ)
Cons
(Acc.)
TRADMT
RTE
COMB
TRADMT
PAIR
COMB
WMT 2008 (worst)
WMT 2008 (best)
0.50 0.74
0.72
0.74
0.52
0.51
0.53
0.44
0.56
0.74
0.78
0.74
0.73
0.77
0.77
0.37
0.83
PAIR
RTE
PAIR
0.51
0.51
0.63
0.66
0.70
ABS
ABS
ABS
</figure>
<page confidence="0.996241">
39
</page>
<table confidence="0.9714082">
Segment TRADMT RTE COMB Gold
REF: Scottish NHS boards need to improve criminal records checks for Rank: 3 Rank: 1 Rank: 2 Rank: 1
employees outside Europe, a watchdog has said.
HYP: The Scottish health ministry should improve the controls on extra-
community employees to check whether they have criminal precedents,
said the monitoring committee. [1357, lium-systran]
REF: Arguments, bullying and fights between the pupils have extended Rank: 5 Rank: 2 Rank: 4 Rank: 5
to the relations between their parents.
HYP: Disputes, chicane and fights between the pupils transposed in
relations between the parents. [686, rbmt4]
</table>
<tableCaption confidence="0.9889905">
Table 3: Examples of reference translations and MT output from the WMT 2008 French-English News dataset.
Rank judgments are out of five (smaller is better).
</tableCaption>
<bodyText confidence="0.999969214285715">
ses that explains the increased benefit the RTE-based
model obtains from tie-aware predictions: if the best
hypothesis is wrong, chances are much better than for
the TRADMT-based model that counting the second-
best hypothesis as “best” is correct. Unfortunately, this
property is not shared by COMB to the same degree, and
it does not improve as much as RTE.
Table 3 illustrates the difference between RTE and
TRADMT. In the first example, RTE makes a more ac-
curate prediction than TRADMT. The human rater’s
favorite translation deviates considerably from the ref-
erence translation in lexical choice, syntactic structure,
and word order, for which it is punished by TRADMT.
In contrast, RTE determines correctly that the propo-
sitional content of the reference is almost completely
preserved. The prediction of COMB is between the two
extremes. The second example shows a sentence where
RTE provides a worse prediction. This sentence was
rated as bad by the judge, presumably due to the inap-
propriate translation of the main verb. This problem,
together with the reformulation of the subject, leads
TRADMT to correctly predict a low score (rank 5/5).
RTE’s deeper analysis comes up with a high score (rank
2/5), based on the existing semantic overlap. The com-
bined model is closer to the truth, predicting rank 4.
Feature Weights. Finally, we assessed the impor-
tance of the different entailment feature groups in the
RTE model.1 Since the presence of correlated features
makes the weights difficult to interpret, we restrict our-
selves to two general observations.
First, we find high weights not only for the score of
the alignment between hypothesis and reference, but
also for a number of syntacto-semantic match and mis-
match features. This means that we do get an additional
benefit from the presence of these features. For example,
features with a negative effect include dropping adjuncts,
unaligned root nodes, incompatible modality between
the main clauses, person and location mismatches (as
opposed to general mismatches) and wrongly handled
passives. Conversely, some factors that increase the
prediction are good alignment, matching embeddings
under factive verbs, and matches between appositions.
</bodyText>
<footnote confidence="0.779626">
1The feature weights are similar for the COMB model.
</footnote>
<bodyText confidence="0.999885076923077">
Second, we find clear differences in the usefulness
of feature groups between MT evaluation and the RTE
task. Some of them, in particular structural features,
can be linked to the generally lower grammaticality of
MT hypotheses. A case in point is a feature that fires
for mismatches between dependents of predicates and
which is too unreliable on the SMT data. Other differ-
ences simply reflect that the two tasks have different
profiles, as sketched in Section 2.1. RTE exhibits high
feature weights for quantifier and polarity features, both
of which have the potential to influence entailment deci-
sions, but are relatively unimportant for MT evaluation,
at least at the current state of the art.
</bodyText>
<sectionHeader confidence="0.999533" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999983620689655">
In this paper, we have investigated an approach to MT
evaluation that is inspired by the similarity between
this task and textual entailment. Our two models – one
predicting absolute scores and one predicting pairwise
preference judgments – use entailment features to pre-
dict the quality of MT hypotheses, thus replacing sur-
face matching with syntacto-semantic matching. Both
models perform similarly, showing sufficient robustness
and coverage to attain comparable performance to a
committee of established MT evaluation metrics.
We have described two refinements: (1) combining
the features into a superior joint model; and (2) adding a
confidence interval around the best hypothesis to model
ties for first place. Both strategies improve correlation;
however, unfortunately the benefits do not currently
combine. Our feature weight analysis indicates that
syntacto-semantic features do play an important role in
score prediction in the RTE model. We plan to assess
the additional benefit of the full entailment feature set
against the TRADMT feature set extended by a proper
lexical similarity metric, such as METEOR.
The computation of entailment features is more
heavyweight than traditional MT evaluation metrics.
We found the speed (about 6 s per hypothesis on a cur-
rent PC) to be sufficient for easily judging the quality of
datasets of the size conventionally used for MT evalua-
tion. However, this may still be too expensive as part of
an MT model that directly optimizes some performance
measure, e.g., minimum error rate training (Och, 2003).
</bodyText>
<page confidence="0.997812">
40
</page>
<sectionHeader confidence="0.988661" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.874724">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Translation
and Summarization, pages 65–72, Ann Arbor, MI.
R. Barzilay and N. Elhadad. 2002. Inferring strategies
for sentence ordering in multidocument news summa-
</bodyText>
<reference confidence="0.996703787878788">
rization. Journal of Artificial Intelligence Research,
17:35–55.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in ma-
chine translation research. In Proceedings of EACL,
pages 249–256, Trento, Italy.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the ACL Workshop on Statistical Machine
Translation, pages 70–106, Columbus, OH.
Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A
maximum similarity metric for machine translation
evaluation. In Proceedings of ACL-08: HLT, pages
55–62, Columbus, Ohio.
William W. Cohen, Robert E. Schapire, and Yoram
Singer. 1999. Learning to order things. Journal
of Artificial Intelligence Research, 10:243–270.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The PASCAL recognising textual entailment
challenge. In Proceedings of the PASCAL Chal-
lenges Workshop on Recognising Textual Entailment,
Southampton, UK.
Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Mar-
morshtein, and Carlo Strapparava. 2006. Direct word
sense matching for lexical substitution. In Proceed-
ings of ACL, Sydney, Australia.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram cooccurrence
statistics. In Proceedings of HLT, pages 128–132,
San Diego, CA.
Jes´us Gim´enez and Llu´ıs M`arquez. 2008. A smorgas-
bord of features for automatic MT evaluation. In
Proceedings of the Third Workshop on Statistical Ma-
chine Translation, pages 195–198, Columbus, Ohio.
Sanda Harabagiu and Andrew Hickl. 2006. Methods
for using textual entailment in open-domain question
answering. In Proceedings of ACL, pages 905–912,
Sydney, Australia.
Bill MacCartney and Christopher D. Manning. 2008.
Modeling semantic containment and exclusion in nat-
ural language inference. In Proceedings of Coling,
pages 521–528, Manchester, UK.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of NAACL, pages
41–48, New York City, NY.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
ACL, pages 160–167, Sapporo, Japan.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. 2007. Dependency-based automatic evalu-
ation for machine translation. In Proceedings of
the NAACL-HLT /AMTA Workshop on Syntax and
Structure in Statistical Translation, pages 80–87,
Rochester, NY.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings of
ACL, pages 311–318, Philadelphia, PA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings ofAMTA, pages 223–231, Cambridge,
MA.
</reference>
<page confidence="0.999437">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.850292">
<title confidence="0.991491">Textual Entailment Features for Machine Translation Evaluation</title>
<author confidence="0.853685">Michel Galley Pad´o</author>
<author confidence="0.853685">Dan Jurafsky</author>
<author confidence="0.853685">D Christopher</author>
<affiliation confidence="0.999095">Stanford University</affiliation>
<abstract confidence="0.999804615384615">We present two regression models for the prediction of pairwise preference judgments among MT hypotheses. Both models are based on feature sets that motivated by entailment incorporate lexical similarity as well as local syntactic features and specific semantic phenomena. One model predicts absolute scores; the other one direct pairwise judgments. We find that both models are competitive with regression models built over the scores of established MT evaluation metrics. Further data analysis clarifies the complementary behavior of the two feature sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>rization</author>
</authors>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>17--35</pages>
<marker>rization, </marker>
<rawString>rization. Journal of Artificial Intelligence Research, 17:35–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>249--256</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="1455" citStr="Callison-Burch et al. (2006)" startWordPosition="204" endWordPosition="207"> quality of machine translations have been a major enabler in improving the performance of MT systems, leading to many varied approaches to develop such metrics. Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf. BLEU (Papineni et al., 2002), NIST (Doddington, 2002). These measures rate systems hypotheses by measuring the overlap in surface word sequences shared between hypothesis and reference translation. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an M</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the role of BLEU in machine translation research. In Proceedings of EACL, pages 249–256, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL Workshop on Statistical Machine Translation,</booktitle>
<pages>70--106</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="6701" citStr="Callison-Burch et al., 2008" startWordPosition="1007" endWordPosition="1010">olarity or tense) degrades its quality, but usually not crucially so. We therefore expect that even noisy entailment features can be predictive in MT evaluation. 2.2 Entailment-based prediction of MT quality Regression-based prediction. Experiences from the annotation of MT quality judgments show that human raters have difficulty in consistently assigning absolute scores to MT system output, due to the number of ways in which MT output can deviate. Thus, the human annotation for the WMT 2008 dataset was collected in the form of binary pairwise preferences that are considerably easier to make (Callison-Burch et al., 2008). This section presents two models for the prediction of pairwise preferences. The first model (ABS) is a regularized linear regression model over entailment-motivated features (see below) that predicts an absolute score for each referencehypothesis pair. Pairwise preferences are created simply by comparing the absolute predicted scores. This model is more general, since it can also be used where absolute score predictions are desirable; furthermore, the model is efficient with a runtime linear in the number of systems and corpus size. On the downside, this model is not optimized for the predi</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. 2008. Further meta-evaluation of machine translation. In Proceedings of the ACL Workshop on Statistical Machine Translation, pages 70–106, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Seng Chan</author>
<author>Hwee Tou Ng</author>
</authors>
<title>MAXSIM: A maximum similarity metric for machine translation evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>55--62</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="1925" citStr="Chan and Ng, 2008" startWordPosition="273" endWordPosition="276">of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an MT evaluation metric that is inspired by the similarity between this task and the textual entailment task (Dagan et al., 2005), which *This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred.. HYP: The virus did not infect anybody. HYP: Virus was infected. no entailment no entailment REF: No one was infecte</context>
</contexts>
<marker>Chan, Ng, 2008</marker>
<rawString>Yee Seng Chan and Hwee Tou Ng. 2008. MAXSIM: A maximum similarity metric for machine translation evaluation. In Proceedings of ACL-08: HLT, pages 55–62, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Robert E Schapire</author>
<author>Yoram Singer</author>
</authors>
<title>Learning to order things.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>10--243</pages>
<contexts>
<context position="8381" citStr="Cohen et al., 1999" startWordPosition="1268" endWordPosition="1271"> Match (5) Compatibility of locations and entities (4) Table 1: Entailment feature groups provided by the Stanford RTE system, with number of features quadratic in the number of systems. On the other hand, it can be trained on more reliable pairwise preference judgments. In a second step, we combine the individual decisions to compute the highest-likelihood total ordering of hypotheses. The construction of an optimal ordering from weighted pairwise preferences is an NPhard problem (via reduction of CYCLIC-ORDERING; Barzilay and Elhadad, 2002), but a greedy search yields a close approximation (Cohen et al., 1999). Both models can be used to predict system-level scores from sentence-level scores. Again, we have two method for doing this. The basic method (BASIC) predicts the quality of each system directly as the percentage of sentences for which its output was rated best among all systems. However, we noticed that the manual rankings for the WMT 2007 dataset show a tie for best system for almost 30% of sentences. BASIC is systematically unable to account for these ties. We therefore implemented a “tie-aware” prediction method (WITHTIES) that uses the same sentence-level output as BASIC, but computes s</context>
</contexts>
<marker>Cohen, Schapire, Singer, 1999</marker>
<rawString>William W. Cohen, Robert E. Schapire, and Yoram Singer. 1999. Learning to order things. Journal of Artificial Intelligence Research, 10:243–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Southampton, UK.</location>
<contexts>
<context position="2180" citStr="Dagan et al., 2005" startWordPosition="313" endWordPosition="316">matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowledge are helpful: Some suggestions concentrate on lexical information, e.g., by the integration of word similarity information as in Meteor (Banerjee and Lavie, 2005) or MaxSim (Chan and Ng, 2008). Other proposals use structural information such as dependency edges (Owczarzak et al., 2007). In this paper, we investigate an MT evaluation metric that is inspired by the similarity between this task and the textual entailment task (Dagan et al., 2005), which *This paper is based on work funded by the Defense Advanced Research Projects Agency through IBM. The content does not necessarily reflect the views of the U.S. Government, and no official endorsement should be inferred.. HYP: The virus did not infect anybody. HYP: Virus was infected. no entailment no entailment REF: No one was infected by the virus. Figure 1: Entailment status between an MT system hypothesis and a reference translation for good translations (above) and bad translations (below). suggests that the quality of an MT hypothesis should be predictable by a combination of lex</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The PASCAL recognising textual entailment challenge. In Proceedings of the PASCAL Challenges Workshop on Recognising Textual Entailment, Southampton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Alfio Gliozzo</author>
<author>Efrat Marmorshtein</author>
<author>Carlo Strapparava</author>
</authors>
<title>Direct word sense matching for lexical substitution.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="3712" citStr="Dagan et al., 2006" startWordPosition="559" endWordPosition="562">ation 2.1 Textual Entailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if a human reading P would infer that H is most likely true. Information about the presence or absence of entailment between two sentences has been found to be beneficial for a range of NLP tasks such as Word Sense Disambiguation or Question Answering (Dagan et al., 2006; Harabagiu and Hickl, 2006). Our intuition is that this idea can also be fruitful in MT Evaluation, as illustrated in Figure 1. Very good MT output should entail the reference translation. In contrast, missing hypothesis material breaks forward entailment; additional material breaks backward entailment; and for bad translations, entailment fails in both directions. Work on the recognition of textual entailment (RTE) has consistently found that the integration of more syntactic and semantic knowledge can yield gains over REF: No one was infected by the virus. entailment entailment Proceedings </context>
</contexts>
<marker>Dagan, Glickman, Gliozzo, Marmorshtein, Strapparava, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, Alfio Gliozzo, Efrat Marmorshtein, and Carlo Strapparava. 2006. Direct word sense matching for lexical substitution. In Proceedings of ACL, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of HLT,</booktitle>
<pages>128--132</pages>
<location>San Diego, CA.</location>
<contexts>
<context position="1131" citStr="Doddington, 2002" startWordPosition="162" endWordPosition="164"> absolute scores; the other one direct pairwise judgments. We find that both models are competitive with regression models built over the scores of established MT evaluation metrics. Further data analysis clarifies the complementary behavior of the two feature sets. 1 Introduction Automatic metrics to assess the quality of machine translations have been a major enabler in improving the performance of MT systems, leading to many varied approaches to develop such metrics. Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf. BLEU (Papineni et al., 2002), NIST (Doddington, 2002). These measures rate systems hypotheses by measuring the overlap in surface word sequences shared between hypothesis and reference translation. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement on what types of knowled</context>
<context position="10502" citStr="Doddington, 2002" startWordPosition="1600" endWordPosition="1602"> 1. A small number features are real-valued, measuring different quality aspects of the alignment. The other features are binary, indicating matches and mismatches of different types (e.g., alignment between predicates embedded under compatible or incompatible modals, respectively). To judge to what extent the entailment-based model delivers improvements that cannot be obtained with established methods, we also experiment with a feature set 38 formed from a set of established MT evaluation metrics (TRADMT). We combine different parametrization of (smoothed) BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006), to give a total of roughly 100 features. Finally, we consider a combination of both feature sets (COMB). 3 Experimental Evaluation Setup. To assess and compare the performance of our models, we use corpora that were created by past instances of the WMT workshop. We optimize the feature weights for the ABS models on the WMT 2006 and 2007 absolute score annotations, and correspondingly for the PAIR models on the WMT 2007 absolute score and ranking annotations. All models are evaluated on WMT 2008 to compare against the published results. Finally, we need to set t</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of HLT, pages 128–132, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>A smorgasbord of features for automatic MT evaluation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Third Workshop on Statistical Machine Translation,</booktitle>
<pages>195--198</pages>
<location>Columbus, Ohio.</location>
<marker>Gim´enez, M`arquez, 2008</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2008. A smorgasbord of features for automatic MT evaluation. In Proceedings of the Third Workshop on Statistical Machine Translation, pages 195–198, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Andrew Hickl</author>
</authors>
<title>Methods for using textual entailment in open-domain question answering.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>905--912</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="3740" citStr="Harabagiu and Hickl, 2006" startWordPosition="563" endWordPosition="566">tailment vs. MT Evaluation Textual entailment (TE) was introduced by Dagan et al. (2005) as a concept that corresponds more closely to “common sense” reasoning than classical, categorical entailment. Textual entailment is defined as a relation between two natural language sentences (a premise P and a hypothesis H) that holds if a human reading P would infer that H is most likely true. Information about the presence or absence of entailment between two sentences has been found to be beneficial for a range of NLP tasks such as Word Sense Disambiguation or Question Answering (Dagan et al., 2006; Harabagiu and Hickl, 2006). Our intuition is that this idea can also be fruitful in MT Evaluation, as illustrated in Figure 1. Very good MT output should entail the reference translation. In contrast, missing hypothesis material breaks forward entailment; additional material breaks backward entailment; and for bad translations, entailment fails in both directions. Work on the recognition of textual entailment (RTE) has consistently found that the integration of more syntactic and semantic knowledge can yield gains over REF: No one was infected by the virus. entailment entailment Proceedings of the Fourth Workshop on St</context>
</contexts>
<marker>Harabagiu, Hickl, 2006</marker>
<rawString>Sanda Harabagiu and Andrew Hickl. 2006. Methods for using textual entailment in open-domain question answering. In Proceedings of ACL, pages 905–912, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Modeling semantic containment and exclusion in natural language inference.</title>
<date>2008</date>
<booktitle>In Proceedings of Coling,</booktitle>
<pages>521--528</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="5938" citStr="MacCartney and Manning, 2008" startWordPosition="888" endWordPosition="891">re are also substantial differences between TE and MT evaluation. Crucially, TE assumes the premise and hypothesis to be well-formed sentences, which is not true in MT evaluation. Thus, a possible criticism to the use of TE methods is that the features could become unreliable for ill-formed MT output. However, there is a second difference between the tasks that works to our advantage. Due to its strict compositional nature, TE requires an accurate semantic analysis of all sentence parts, since, for example, one misanalysed negation or counterfactual embedding can invert the entailment status (MacCartney and Manning, 2008). In contrast, human MT judgments behave more additively: failure of a translation with respect to a single semantic dimension (e.g., polarity or tense) degrades its quality, but usually not crucially so. We therefore expect that even noisy entailment features can be predictive in MT evaluation. 2.2 Entailment-based prediction of MT quality Regression-based prediction. Experiences from the annotation of MT quality judgments show that human raters have difficulty in consistently assigning absolute scores to MT system output, due to the number of ways in which MT output can deviate. Thus, the hu</context>
</contexts>
<marker>MacCartney, Manning, 2008</marker>
<rawString>Bill MacCartney and Christopher D. Manning. 2008. Modeling semantic containment and exclusion in natural language inference. In Proceedings of Coling, pages 521–528, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill MacCartney</author>
<author>Trond Grenager</author>
<author>Marie-Catherine de Marneffe</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Learning to recognize features of valid textual entailments.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>41--48</pages>
<location>New York City, NY.</location>
<marker>MacCartney, Grenager, de Marneffe, Cer, Manning, 2006</marker>
<rawString>Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D. Manning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of NAACL, pages 41–48, New York City, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Dependency-based automatic evaluation for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the NAACL-HLT /AMTA Workshop on Syntax and Structure in Statistical Translation,</booktitle>
<pages>80--87</pages>
<location>Rochester, NY.</location>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007. Dependency-based automatic evaluation for machine translation. In Proceedings of the NAACL-HLT /AMTA Workshop on Syntax and Structure in Statistical Translation, pages 80–87, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="1106" citStr="Papineni et al., 2002" startWordPosition="157" endWordPosition="160"> phenomena. One model predicts absolute scores; the other one direct pairwise judgments. We find that both models are competitive with regression models built over the scores of established MT evaluation metrics. Further data analysis clarifies the complementary behavior of the two feature sets. 1 Introduction Automatic metrics to assess the quality of machine translations have been a major enabler in improving the performance of MT systems, leading to many varied approaches to develop such metrics. Initially, most metrics judged the quality of MT hypotheses by token sequence match (cf. BLEU (Papineni et al., 2002), NIST (Doddington, 2002). These measures rate systems hypotheses by measuring the overlap in surface word sequences shared between hypothesis and reference translation. With improvements in the state-of-the-art in machine translation, the effectiveness of purely surface-oriented measures has been questioned (see e.g., Callison-Burch et al. (2006)). In response, metrics have been proposed that attempt to integrate more linguistic information into the matching process to distinguish linguistically licensed from unwanted variation (Gim´enez and M`arquez, 2008). However, there is little agreement</context>
<context position="10477" citStr="Papineni et al., 2002" startWordPosition="1595" endWordPosition="1598">ture groups are shown in Table 1. A small number features are real-valued, measuring different quality aspects of the alignment. The other features are binary, indicating matches and mismatches of different types (e.g., alignment between predicates embedded under compatible or incompatible modals, respectively). To judge to what extent the entailment-based model delivers improvements that cannot be obtained with established methods, we also experiment with a feature set 38 formed from a set of established MT evaluation metrics (TRADMT). We combine different parametrization of (smoothed) BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006), to give a total of roughly 100 features. Finally, we consider a combination of both feature sets (COMB). 3 Experimental Evaluation Setup. To assess and compare the performance of our models, we use corpora that were created by past instances of the WMT workshop. We optimize the feature weights for the ABS models on the WMT 2006 and 2007 absolute score annotations, and correspondingly for the PAIR models on the WMT 2007 absolute score and ranking annotations. All models are evaluated on WMT 2008 to compare against the published results. </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAMTA,</booktitle>
<pages>223--231</pages>
<location>Cambridge, MA.</location>
<contexts>
<context position="10533" citStr="Snover et al., 2006" startWordPosition="1605" endWordPosition="1608">are real-valued, measuring different quality aspects of the alignment. The other features are binary, indicating matches and mismatches of different types (e.g., alignment between predicates embedded under compatible or incompatible modals, respectively). To judge to what extent the entailment-based model delivers improvements that cannot be obtained with established methods, we also experiment with a feature set 38 formed from a set of established MT evaluation metrics (TRADMT). We combine different parametrization of (smoothed) BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and TER (Snover et al., 2006), to give a total of roughly 100 features. Finally, we consider a combination of both feature sets (COMB). 3 Experimental Evaluation Setup. To assess and compare the performance of our models, we use corpora that were created by past instances of the WMT workshop. We optimize the feature weights for the ABS models on the WMT 2006 and 2007 absolute score annotations, and correspondingly for the PAIR models on the WMT 2007 absolute score and ranking annotations. All models are evaluated on WMT 2008 to compare against the published results. Finally, we need to set the tie interval ε. Since we did</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings ofAMTA, pages 223–231, Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>