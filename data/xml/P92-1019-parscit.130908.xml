<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003460">
<title confidence="0.992819">
A CONNECTIONIST PARSER
FOR STRUCTURE UNIFICATION GRAMMAR
</title>
<author confidence="0.996239">
James B. Henderson*
</author>
<affiliation confidence="0.998405">
Department of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.9309165">
200 South 33rd
Philadelphia, PA 19104, USA
</address>
<email confidence="0.918199">
(henders©linc.cis.upenn.edu)
</email>
<sectionHeader confidence="0.994044" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.9998626">
This paper presents a connectionist syntactic
parser which uses Structure Unification Grammar
as its grammatical framework. The parser is im-
plemented in a connectionist architecture which
stores and dynamically manipulates symbolic rep-
resentations, but which can&apos;t represent arbitrary
disjunction and has bounded memory. These
problems can be overcome with Structure Unifica-
tion Grammar&apos;s extensive use of partial descrip-
tions.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.99990632">
The similarity between connectionist models of
computation and neuron computation suggests
that a study of syntactic parsing in a connection-
ist computational architecture could lead to sig-
nificant insights into ways natural language can
be parsed efficiently. Unfortunately, previous in-
vestigations into connectionist parsing (Cottrell,
1989, Fanty, 1985, Selman and Hirst, 1987) have
not been very successful. They cannot parse arbi-
trarily long sentences and have inadequate gram-
mar representations. However, the difficulties with
connectionist parsing can be overcome by adopt-
ing a different connectionist model of computa-
tion, namely that proposed by Shastri and Ajjana-
gadde (1990). This connectionist computational
architecture differs from others in that it directly
manifests the symbolic interpretation of the infor-
mation it stores and manipulates. It also shares
the massive parallelism, evidential reasoning abil-
ity, and neurological plausibility of other connec-
tionist architectures. Since virtually all charac-
terizations of natural language syntax have relied
heavily on symbolic representations, this architec-
ture is ideally suited for the investigation of syn-
tactic parsing.
</bodyText>
<footnote confidence="0.874462">
*This research was supported by ARO grant
</footnote>
<note confidence="0.883265333333333">
DAAL 03-89-C-0031, DARPA grant N00014-90-J-
1863, NSF grant IRI 90-16592, and Ben Franklin grant
91S.3078C-1.
</note>
<bodyText confidence="0.999952363636364">
The computational architecture proposed by
Shastri and Ajjanagadde (1990) provides a rather
general purpose computing framework, but it does
have significant limitations. A computing mod-
ule can represent entities, store predications over
those entities, and use pattern—action rules to ma-
nipulate this stored information. This form of rep-
resentation is very expressive, and pattern—action
rules are a general purpose way to do compu-
tation. However, this architecture has two lim-
itations which pose difficult problems for pars-
ing natural language. First, only a conjunction
of predications can be stored. The architecture
cannot represent arbitrary disjunction. This lim-
itation implies that the parser&apos;s representation of
syntactic structure must be able to leave unspec-
ified the information which the input has not yet
determined, rather than having a disjunction of
more completely specified possibilities for com-
pleting the sentence. Second, the memory ca-
pacity of any module is bounded. The number
of entities which can be stored is bounded by a
small constant, and the number of predications
per predicate is also bounded. These bounds pose
problems for parsing because the syntactic struc-
tures which need to be recovered can be arbitrarily
large. This problem can be solved by allowing the
parser to output the syntactic structure incremen-
tally, thus allowing the parser to forget the infor-
mation which it has already output and which it
no longer needs to complete the parse. This tech-
nique requires that the representation of syntactic
structure be able to leave unspecified the informa-
tion which has already been determined but which
is no longer needed for the completion of the parse.
Thus the limitations of the architecture mean that
the parser&apos;s representation of syntactic structure
must be able to leave unspecified both the infor-
mation which the input has not yet determined
and the information which is no longer needed.
In order to comply with these requirements,
the parser uses Structure Unification Grammar
(Henderson, 1990) as its grammatical framework.
SUG is a formalization of accumulating informa-
</bodyText>
<page confidence="0.997646">
144
</page>
<bodyText confidence="0.999969444444445">
tion about the phrase structure of a sentence un-
til a complete description of the sentence&apos;s phrase
structure tree is constructed. Its extensive use
of partial descriptions makes it ideally suited for
dealing with the limitations of the architecture.
This paper focuses on the parser&apos;s represen-
tation of phrase structure information and on the
way the parser accumulates this information dur-
ing a parse. Brief descriptions of the grammar
formalism and the implementation in the connec-
tionist architecture are also given. Except where
otherwise noted, a simulation of the implementa-
tion has been written, and its grammar supports
a small set of examples. A more extensive gram-
mar is under development. SUG is clearly an ade-
quate grammatical framework, due to its ability
to straightforwardly simulate Feature Structure
Based Tree Adjoining Grammar (Vijay—Shanker,
1987), as well as other formalisms (Henderson,
1990). Initial investigations suggest that the con-
straints imposed by the parser do not interfere
with this linguistic adequacy, and more extensive
empirical verification of this claim is in progress.
The remainder of this paper will first give an
overview of Structure Unification Grammar, then
present the parser design, and finally a sketch of
its implementation.
</bodyText>
<sectionHeader confidence="0.9760805" genericHeader="method">
STRUCTURE UNIFICATION
GRAMMAR
</sectionHeader>
<bodyText confidence="0.999835588235294">
Structure Unification Grammar is a formaliza-
tion of accumulating information about the phrase
structure of a sentence until this structure is com-
pletely described. This information is specified in
partial descriptions of phrase structure trees. An
SUG grammar is simply a set of these descriptions.
The descriptions cannot use disjunction or nega-
tion, but their partiality makes them both flexi-
ble enough and powerful enough to state what is
known and only what is known where it is known.
There is also a simple abstraction operation for
SUG descriptions which allows unneeded informa-
tion to be forgotten, as will be discussed in the
section on the parser design. In an SUG deriva-
tion, descriptions are combined by equating nodes.
This way of combining descriptions is extremely
flexible, thus allowing the parser to take full ad-
vantage of the flexibility of SUG descriptions, and
also providing for efficient parsing strategies. The
final description produced by a derivation must
completely describe some phrase structure tree.
This tree is the result of the derivation. The de-
sign of SUG incorporates ideas from Tree Adjoin-
ing Grammar, Description Theory (Marcus et aL,
1983), Combinatory Categorial Grammar, Lexi-
cal Functional Grammar, and Head—driven Phrase
Structure Grammar.
An SUG grammar is a set of partial descrip-
tions of phrase structure trees. Each SUG gram-
mar entry simply specifies an allowable grouping
of information, thus expressing the information in-
terdependencies. The language which SUG pro-
vides for specifying these descriptions allows par-
tiality both in the information about individual
nodes, and (crucially) in the information about
the structural relations between nodes. As in
many formalisms, nodes are described with fea-
ture structures. The use of feature structures al-
lows unknown characteristics of a node to be left
unspecified. Nodes are divided into nonterminals,
which are arbitrary feature structures, and termi-
nals, which are atomic instances of strings. Unlike
most formalisms, SUG allows the specification of
the structural relations to be equally partial. For
example, if a description specifies children for a
node, this does not preclude that node from ac-
quiring other children, such as modifiers. This
partiality also allows grammar entries to under-
specify ordering constraints between nodes, thus
allowing for variations in word order. This partial-
ity in structural information is imperative to allow
incremental parsing without disjunction (Marcus
et at, 1983). In addition to the immediate domi-
nance relation for specifying parent—child relation-
ships and linear precedence for specifying ordering
constraints, SUG allows chains of immediate dom-
inance relationships to be partially specified using
the dominance relation. A dominance constraint
between two nodes specifies that there must be a
chain of zero or more immediate dominance con-
straints between the two nodes, but it does not
say anything about the chain. This relation is
necessary to express long distance dependencies in
a single grammar entry. Some examples of SUG
phrase structure descriptions are given in figure 1,
and will be discussed below.
A complete description of a phrase structure
tree is constructed from the partial descriptions in
an SUG grammar by conjoining a set of grammar
entries and specifying how these descriptions share
nodes. More formally, an SUG derivation starts
with descriptions from the grammar, and in each
step conjoins a set of one or more descriptions and
adds zero or more statements of equality between
nonterminal nodes. The description which results
from a derivation step must be satisfiable, so the
feature structures of any two equated nodes must
unify and the resulting structural constraints must
be consistent with some phrase structure tree. The
final description produced by a derivation must
be a complete description of some phrase struc-
ture tree. This tree is the result of the derivation.
The sentences generated by a derivation are all
those terminal strings which are consistent with
the ordering constraints on the resulting tree. Fig-
</bodyText>
<page confidence="0.989729">
145
</page>
<figure confidence="0.757992">
pizza t
</figure>
<figureCaption confidence="0.796975333333333">
Figure 1: Example grammar entries. They can
be combined to form a structure for the sentence
&amp;quot;Who ate white pizza?&amp;quot;.
</figureCaption>
<bodyText confidence="0.99998912">
ure 2 shows an example derivation with one step
in which all grammar entries are combined and
all equations are done. This definition of deriva-
tions provides a very flexible framework for investi-
gating various parsing strategies. Any ordering of
combining grammar entries and doing equations is
a valid derivation. The only constraints on deriva-
tions come from the meanings of the description
primitives and from the need to have a unique re-
sulting tree. This flexibility is crucial to allow the
parser to compensate for the connectionist archi-
tecture&apos;s limitations and to parse efficiently.
Because the resulting description of an SUG
derivation must be both a consistent description
and a complete description of some tree, an SUG
grammar entry can state both what is true about
the phrase structure tree and what needs to be
true. For a description to be complete it must
specify a single immediate dominance tree and all
terminals mentioned in the description must have
some (possibly empty) string specified for them.
Otherwise there would be no way to determine the
exact tree structure or the word for each terminal
in the resulting tree. A grammar entry can express
grammatical requirements by not satisfying these
completion requirements locally. For example, in
figure 1 the structure for &amp;quot;ate&amp;quot; has a subject node
with category NP and with a terminal as the val-
ues of its head feature. Because this terminal does
not have its word specified, this NP must equate
with another NP node which does have a word for
the value of its head feature. The unification of the
two NP&apos;s feature structures will cause the equation
of the two head terminals. In this way the struc-
ture for &amp;quot;ate&amp;quot; expresses the fact that it obligatorily
subcategorizes for a subject NP. The structure for
&amp;quot;ate&amp;quot; also expresses its subcategorization for an
object NP, but this object is not obligatory since
it does not have an underspecified terminal head.
Like the subject of &amp;quot;ate&amp;quot;, the root of the structure
for &amp;quot;white&amp;quot; in figure 1 has an underspecified ter-
minal head. This expresses the fact that &amp;quot;white&amp;quot;
obligatorily modifies N&apos;s. The need to construct
a single immediate dominance tree is used in the
structure for &amp;quot;who&amp;quot; to express the need for the
subcategorized S to have an NP gap. Because the
dominated NP node does not have an immediate
parent, it must equate with some node which has
an immediate parent. The site of this equation is
the gap associated with &amp;quot;who&amp;quot;.
</bodyText>
<subsectionHeader confidence="0.465249">
THE PARSER
</subsectionHeader>
<bodyText confidence="0.999981954545455">
The parser presented in this paper accumulates
phrase structure information in the same way as
does Structure Unification Grammar. It calcu-
lates SUG derivation steps using a small set of
operations, and incrementally outputs the deriva-
tion as it parses. The parser is implemented in
the connectionist architecture proposed by Shastri
and Ajjanagadde (1990) as a special purpose mod-
ule for syntactic constituent structure parsing. An
SUG description is stored in the module&apos;s mem-
ory by representing nonterminal nodes as entities
and all other needed information as predications
over these nodes. If the parser starts to run out
of memory space, then it can remove some nodes
from the memory, thus forgetting all information
about those nodes. The parser operations are im-
plemented in pattern—action rules. As each word
is input to the parser, one of these rules combines
one of the word&apos;s grammar entries with the current
description. When the parse is finished the parser
checks to make sure it has produced a complete
description of some phrase structure tree.
</bodyText>
<sectionHeader confidence="0.636366" genericHeader="method">
THE GRAMMARS
</sectionHeader>
<bodyText confidence="0.999983866666666">
The grammars which are supported by the parser
are a subset of those for Structure Unification
Grammar. These grammars are for the most part
lexicalized. Each lexicalized grammar entry is a
rooted tree fragment with exactly one phoneti-
cally realized terminal, which is the word of the
entry. Such grammar entries specify what infor-
mation is known about the phrase structure of
the sentence given the presence of the word, and
can be used (Henderson, 1990) to simulate Lexi-
calized Tree Adjoining Grammar (Schabes, 1990).
Nonlexical grammar entries are rooted tree frag-
ments with no words. They can be used to ex-
press constructions like reduced relative clauses,
for which no lexical information is necessary. The
</bodyText>
<figure confidence="0.986938391304348">
key: x
y
x immediately
dominates y
x dominates y
x precedes y
Xh y is the head
feature value
of x
x is a terminal
empty feature
structure
xt
146
did t Barbie t see t
w
1 9.11
/ lit
pictures of yesterdays
who t
key:
x x is equated
with y
</figure>
<figureCaption confidence="0.999699">
Figure 2: A derivation for the sentence &amp;quot;Who did Barbie see a picture of yesterday&amp;quot;.
</figureCaption>
<bodyText confidence="0.99997514893617">
current mechanism the parser uses to find possible
long distance dependencies requires some informa-
tion about possible extractions to be specified in
grammar entries, despite the fact that this infor-
mation currently only has meaning at the level of
the parser.
The primary limitations on the parser&apos;s abil-
ity to parse the sentences derivable with a gram-
mar are due to the architecture&apos;s lack of disjunc-
tion and limited memory capacity. Technically,
constraints on long distance dependencies are en-
forced by the parser&apos;s limited ability to calcu-
late dominance relationships, but the definition
of an SUG derivation could be changed to man-
ifest these constraints. This new definition would
be necessary to maintain the traditional split be-
tween competence and performance phenomena.
The remaining constraints imposed at the level of
the parser are traditionally treated as performance
constraints. For example, the parser&apos;s bounded
memory prevents it from being able to parse arbi-
trarily center embedded sentences or from allow-
ing arbitrarily many phrases on the right frontier
of a sentence to be modified. These are well es-
tablished performance constraints on natural lan-
guage (Chomsky, 1959, and many others). The
lack of a disjunction operator limits the parser&apos;s
ability to represent local ambiguities. This re-
sults in some locally ambiguous grammatical sen-
tences being unparsable. The existence of such
sentences for the human parser, called garden path
sentences, is also well documented (Bever, 1970,
among others). The representations currently
used for handling local ambiguities appear to be
adequate for building the constituent structure of
any non—garden path sentences. The full verifi-
cation of this claim awaits a study of how effec-
tively probabilistic constraints can be used to re-
solve ambiguities. The work presented in this pa-
per does not directly address the question of how
ambiguities between possible predicate—argument
structures are resolved. Also, the current parser
is not intended to be a model of performance phe-
nomena, although since the parser is intended to
be computationally adequate, all limitations im-
posed by the parser must fall within the set of
performance constraints on natural language.
</bodyText>
<sectionHeader confidence="0.930287" genericHeader="method">
THE PARSER DESIGN
</sectionHeader>
<bodyText confidence="0.99994825">
The parser follows SUG derivations, incrementally
combining a grammar entry for each word with the
description built from the previous words of the
sentence. Like in SUG the intermediate descrip-
tions can specify multiple rooted tree fragments,
but the parser represents such a set as a list in or-
der to represent the ordering between terminals in
the fragments. The parser begins with a descrip-
tion containing only an S node which needs a head.
This description expresses the parser&apos;s expectation
for a sentence. As each word is read, a gram-
mar entry for that word is chosen and combined
</bodyText>
<page confidence="0.961529">
147
</page>
<figure confidence="0.999151178571429">
dominance
instantiating
at z
current grammar
description: entry:
ric attaching
at x
current grammar
description: entry:
Li
current grammar
description: entry:
equationless
combining
current
current grammar description:
description: entry:
,A leftward /‘
attaching
at y
\K\
Or current internal key:
description: equation x is the
rx(i) Yr host of y
I xis
xi ,vii)
equatable
with y
</figure>
<figureCaption confidence="0.999993">
Figure 3: The operations of the parser.
</figureCaption>
<bodyText confidence="0.980925541666667">
with the current description using one of four com-
bination operations. Nonlexical grammar entries
can be combined with the current description at
any time using the same operations. There is also
an internal operation which equates two nodes al-
ready in the current description without using a
grammar entry. The parser outputs each opera-
tion it does as it does them, thus providing incre-
mental output to other language modules. After
each operation the parser&apos;s representation of the
current description is updated so that it fully re-
flects the new information added by the operation.
The five operations used by the parser are
shown in figure 3. The first combination opera-
tion, called attaching, adds the grammar entry to
the current description and equates the root of the
grammar entry with some node already in the cur-
rent description. The second, called dominance in-
stantiating, equates a node without a parent in the
current description with a node in the grammar
entry, and equates the host of the unparented node
with the root of the grammar entry. The host func-
tion is used in the parser&apos;s mechanism for enforc-
ing dominance constraints, and represents the fact
that the unparented node is potentially dominated
by its current host. In the case of long distance
dependencies, a node&apos;s host is changed to nodes
further and further down in the tree in a man-
ner similar to slash passing in Generalized Phrase
Structure Grammar, but the resulting domain of
possible extractions is more similar to that of Tree
Adjoining Grammar. The equationless combining
operation simply adds a grammar entry to the end
of the tree fragment list. This operation is some-
times necessary in order to delay attachment de-
cisions long enough to make the right choice. The
leftward attaching operation equates the root of
the tree fragment on the end of the list with some
node in the grammar entry, as long as this root is
not the initializing matrix S1. The one parser op-
eration which does not involve a grammar entry is
called internal equating. When the parser&apos;s rep-
resentation of the current description is updated
so that it fully reflects newly added information,
some potential equations are calculated for nodes
which do not yet have immediate parents. The
internal equating operation executes one of these
potential equations. There are two cases when this
can occur, equating fillers with gaps and equating
a root of a tree fragment with a node in the next
earlier tree fragment on the list. The later is how
tree fragments are removed from the list.
The bound on the number of entities which
can be stored in the parser&apos;s memory requires that
the parser be able to forget entities. The imple-
mentation of the parser only represents nontermi-
nal nodes as entities. The number of nontermi-
nals in the memory is kept low simply by forget-
ting nodes when the memory starts getting full,
thereby also forgetting the predications over the
nodes. This forgetting operation abstracts away
from the existence of the forgotten node in the
phrase structure. Once a node is forgotten it can
no longer be equated with, so nodes which must
be equated with in order for the total descrip-
tion to be complete can not be forgotten. Forget-
ting nodes may eliminate some otherwise possible
parses, but it will never allow parses which violate
1 As of this writing the implementation of the tree
fragment list and these later two combination opera-
tions has been designed, but not coded in the simula-
tion of the parser&apos;s implementation.
</bodyText>
<page confidence="0.985444">
148
</page>
<figure confidence="0.999006611111111">
parser state: grammar entries:
lit
P
Barbie t Barbie t
s-1 attaching
nt
dresses t
11, t
ifresses
t
NP
I
Barbie t dominance
instantiating
h,(11P 17-4;i4p.
- VP
Barbie t dresses: forgetting
VP
Barbie dresses t eq
uationless
combining
h vp
AP-Nh
I t
fashionablyt
VP(1)
,NP
Barbie t drgsest
vp
ft
I V
fashionably: internal
equating
VP
0,NP AP-v
Barbie t rei&apos;ses t fashionablYt
</figure>
<figureCaption confidence="0.999953">
Figure 4: An example parse of &amp;quot;Barbie dresses fashionably&amp;quot;.
</figureCaption>
<bodyText confidence="0.999864704545455">
the forgotten constraints. Any forgetting strategy
can be used as long as the only eliminated parses
are for readings which people do not get. Several
such strategies have been proposed in the litera-
ture.
As a simple example parse consider the parse
of &amp;quot;Barbie dresses fashionably&amp;quot; sketched in fig-
ure 4. The parser begins with an S which needs
a head, and receives the word &amp;quot;Barbie&amp;quot;. The un-
derlined grammar entry is chosen because it can
attach to the S in the current description using
the attaching operation. The next word input is
&amp;quot;dresses&amp;quot;, and its verb grammar entry is chosen
and combined with the current description using
the dominance instantiating operation. In the re-
sulting description the subject NP is no longer on
the right frontier, so it will not be involved in any
future equations and thus can be forgotten. Re-
member that the output of the parser is incremen-
tal, so forgetting the subject will not interfere with
semantic interpretation. The next word input is
&amp;quot;fashionably&amp;quot;, which is a VP modifier. The parser
could simply attach &amp;quot;fashionably&amp;quot;, but for the pur-
poses of exposition assume the parser is not sure
where to attach this modifier, so it simply adds
this grammar entry to the end of the tree frag-
ment list using equationless combining. The up-
dating rules of the parser then calculate that the
VP root of this tree fragment could equate with
the VP for &amp;quot;dresses&amp;quot;, and it records this fact. The
internal equating operation can then apply to do
this equation, thereby choosing this attachment
site for &amp;quot;fashionably&amp;quot;. This technique can be used
to delay resolving any attachment ambiguity. At
this point the end of the sentence has been reached
and the current description is complete, so a suc-
cessful parse is signaled.
Another example which illustrates the parser&apos;s
ability to use underspecification to delay disam-
biguation decisions is given in figure 5. The feature
decomposition ±A,±V is used for the major cate-
gories (N, V, A, and P) in order to allow the object
of &amp;quot;know&amp;quot; to be underspecified as to whether it is
of category N ([—A,—V]) or V ([—A,-I-V]). When
</bodyText>
<page confidence="0.995168">
149
</page>
<figure confidence="0.381914">
grammar entry:
</figure>
<figureCaption confidence="0.994091333333333">
Figure 5: Delaying the resolution of the ambigu-
ity between &amp;quot;Barbie knows a man.&amp;quot; and &amp;quot;Barbie
knows a man left.&amp;quot;
</figureCaption>
<bodyText confidence="0.985693111111111">
&amp;quot;a man&amp;quot; is input the parser is not sure if it is the
object of &amp;quot;know&amp;quot; or the subject of this object, so
the structure for &amp;quot;a man&amp;quot; is simply added to the
parser state using equationless combining. This
underspecification can be maintained for as long
as necessary, provided there are resources available
to maintain it. If no verb is subsequently input
then the NP can be equated with the —A node
using internal equation, thus making &amp;quot;a man&amp;quot; the
object of &amp;quot;know&amp;quot;. If, as shown, a verb is input
then leftward attaching can be used to attach &amp;quot;a
man&amp;quot; as the subject of the verb, and then the
verb&apos;s S node can be equated with the —A node to
make it the object of &amp;quot;know&amp;quot;. Since this parser is
only concerned with constituent structure and not
with predicate—argument structure, the fact that
the —A node plays two different semantic roles in
the two cases is not a problem.
</bodyText>
<sectionHeader confidence="0.9964875" genericHeader="method">
THE CONNECTIONIST
IMPLEMENTATION
</sectionHeader>
<bodyText confidence="0.99957335">
The above parser is implemented using the con-
nectionist computational architecture proposed by
Shastri and Ajjanagadde (1990). This architecture
solves the variable binding problem2 by using units
which pulse periodically, and representing differ-
ent entities in different phases. Units which are
storing predications about the same entity pulse
synchronously, and units which are storing pred-
ications about different entities pulse in different
phases. The number of distinct entities which can
be stored in a module&apos;s memory at one time is
determined by the width of a pulse spike and the
time between periodic firings (the period). Neuro-
logically plausible estimates of these values put the
maximum number of entities in the general vicin-
ity of 7±2. The architecture does computation
with sets of units which implement pattern—action
rules. When such a set of units finds its pattern
in the predications in the memory, it modifies the
memory contents in accordance with its action and
</bodyText>
<footnote confidence="0.969592333333333">
3The variable binding problem is keeping track of
what predications are for what variables when more
than one variable is being used.
</footnote>
<figureCaption confidence="0.875036">
Figure 6: The architecture of the parser.
the entity(s) which matched.
</figureCaption>
<bodyText confidence="0.999986961538461">
This connectionist computational architecture
is used to implement a special purpose module
for syntactic constituent structure parsing. A di-
agram of the parser&apos;s architecture is shown in fig-
ure 6. This parsing module uses its memory to
store information about the phrase structure de-
scription being built. Nonterminals are the enti-
ties in the memory, and predications over nonter-
minals are used to represent all the information
the parser needs about the current description.
Pattern—action rules are used to make changes to
this information. Most of these rules implement
the grammar. For each grammar entry there is
a rule for each way of using that grammar en-
try in a combination operation. The patterns for
these rules look for nodes in the current descrip-
tion where their grammar entry can be combined
in their way. The actions for these rules add in-
formation to the memory so as to represent the
changes to the current description which result
from their combination. If the grammar entry is
lexical then its rules are only activated when its
word is the next word in the sentence. A general
purpose connectionist arbitrator is used to choose
between multiple rule pattern matches, as with
other disambiguation decisions3. This arbitrator
</bodyText>
<footnote confidence="0.585585">
3Because a rule&apos;s pattern matches must be commu-
nicated to the rule&apos;s action through an arbitrator, the
existence and quality of a match must be specified in
a single node&apos;s phase. For rules which involve more
than one node, information about one of the nodes
must be represented in the phase of the other node for
the purposes of testing patterns. This is the purpose
</footnote>
<figure confidence="0.996976512195122">
) next word
* 1r_
grammar
operation
patterns
parser state
internal
operation
patterns
I signal
[generation
w matching operations w
and nodes
higher level
preference4
chosen
A grammar
entry
arbitrator
chosen operations
and nodes
grammar
operation
actions
internal
operation
actions
updating
forgetting
new
information
parser memory
internal
&amp;quot;equations syntactic
derivation
parser state:
-A 6)
Barbie t knows t
rNPt
Cl)114h
at mant
</figure>
<page confidence="0.986887">
150
</page>
<bodyText confidence="0.999985692307692">
weighs the preferences for the possible choices and
makes a decision. This mechanism for doing dis-
ambiguation allows higher level components of the
language system to influence disambiguation by
adding to the preferences of the arbitrator4. It
also allows probabilistic constraints such as lexi-
cal preferences and structural biases to be used,
although these aspects of the parser design have
not yet been adequately investigated. Because the
parser&apos;s grammar is implemented in rules which
all compute in parallel, the speed of the parser
is independent of the size of the grammar. The
internal equating operation is implemented with
a rule that looks for pairs of nodes which have
been specified as possible equations, and equates
them, provided that that equation is chosen by
the arbitrator. Equation is done by translating
all predications for one node to the phase of the
other node, then forgetting the first node. The for-
getting operation is implemented with links which
suppress all predications stored for the node to be
forgotten. The only other rules update the parser
state to fully reflects any new information added
by a grammar rule. These rules act whenever they
apply, and include the calculation of equatability
and host relationships.
</bodyText>
<sectionHeader confidence="0.994691" genericHeader="conclusions">
CONCLUSION
</sectionHeader>
<bodyText confidence="0.999487219512195">
This paper has given an overview of a connection-
ist syntactic constituent structure parser which
uses Structure Unification Grammar as its gram-
matical framework. The connectionist computa-
tional architecture which is used stores and dy-
namically manipulates symbolic representations,
thus making it ideally suited for syntactic parsing.
However, the architecture&apos;s inability to represent
arbitrary disjunction and its bounded memory ca-
pacity pose problems for parsing. These difficul-
ties can be overcome by using Structure Unifica-
tion Grammar as the grammatical framework, due
to SUG&apos;s extensive use of partial descriptions.
This investigation has indeed led to insights
into efficient natural language parsing. This
parser&apos;s speed is independent of the size of its
grammar. It only uses a bounded amount of mem-
ory. Its output is incremental, monotonic, and
does not include disjunction. Its disambiguation
of the signal generation box in figure 6. For all such
rules, the identity of one of the nodes can be deter-
mined uniquely given the other node and the parser
state. For example in the dominance instantiating op-
eration, given the unparented node, the host of that
node can be found because host is a function. This
constraint on parser operations seems to have signifi-
cant linguistic import, but more investigation of this
possibility is necessary.
&apos;In the current simulation of the parser implemen-
tation the arbitrators are controlled by the user.
mechanism provides a parallel interface for the in-
fluence of higher level language modules. Assum-
ing neurologically plausible timing characteristics
for the computing units of the connectionist archi-
tecture, the parser&apos;s speed is roughly compatible
with the speed of human speech. In the future the
ability of this architecture to do evidential reason-
ing should allow the use of statistical information
in the parser, thus making use of both grammat-
ical and statistical approaches to language in a
single framework.
</bodyText>
<sectionHeader confidence="0.999118" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999473780487805">
Bever, Thomas G (1970). The cognitive basis for
linguistic structures. In J. R. Hayes, editor,
Cognition and the Development of Language.
John Wiley, New York, NY.
Chomsky, Noam (1959). On certain formal prop-
erties of grammars. Information and Control,
2: 137-167.
Cottrell, Garrison Weeks (1989). A Connectionist
Approach to Word Sense Disambiguation. Mor-
gan Kaufmann Publishers, Los Altos, CA.
Fanty, Mark (1985). Context-free parsing in con-
nectionist networks. Technical Report TR174,
University of Rochester, Rochester, NY.
Henderson, James (1990). Structure unifica-
tion grammar: A unifying framework for in-
vestigating natural language. Technical Re-
port MS-CIS-90-94, University of Pennsylvania,
Philadelphia, PA.
Marcus, Mitchell; Hindle, Donald; and Fleck,
Margaret (1983). D-theory: Talking about talk-
ing about trees. In Proceedings of the 21st An-
nual Meeting of the ACL, Cambridge, MA.
Schabes, Yves (1990). Mathematical and Compu-
tational Aspects of Lezicalized Grammars. PhD
thesis, University of Pennsylvania, Philadelphia,
PA. •
Selman, Bart and Hirst, Graeme (1987). Pars-
ing as an energy minimization problem. In
Lawrence Davis, editor, Genetic Algorithms and
Simulated Annealing, chapter 11, pages 141-
154. Morgan Kaufmann Publishers, Los Altos,
CA.
Shastri, Lokendra and Ajjanagadde, Venkat
(1990). From simple associations to system-
atic reasoning: A connectionist representation
of rules, variables and dynamic bindings. Tech-
nical Report MS-CIS-90-05, University of Penn-
sylvania, Philadelphia, PA. Revised Jan 1992.
Vijay—Shanker, K. (1987). A Study of Tree Ad-
joining Grammars. PhD thesis, University of
Pennsylvania, Philadelphia, PA.
</reference>
<page confidence="0.998349">
151
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.846473">
<title confidence="0.9989355">A CONNECTIONIST PARSER FOR STRUCTURE UNIFICATION GRAMMAR</title>
<author confidence="0.999979">James B Henderson</author>
<affiliation confidence="0.9996275">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.9995295">200 South 33rd Philadelphia, PA 19104, USA</address>
<email confidence="0.985508">(henders©linc.cis.upenn.edu)</email>
<abstract confidence="0.986481727272727">This paper presents a connectionist syntactic parser which uses Structure Unification Grammar as its grammatical framework. The parser is implemented in a connectionist architecture which stores and dynamically manipulates symbolic representations, but which can&apos;t represent arbitrary disjunction and has bounded memory. These problems can be overcome with Structure Unification Grammar&apos;s extensive use of partial descriptions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thomas G Bever</author>
</authors>
<title>The cognitive basis for linguistic structures.</title>
<date>1970</date>
<booktitle>Cognition and the Development of Language.</booktitle>
<editor>In J. R. Hayes, editor,</editor>
<publisher>John Wiley,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="15747" citStr="Bever, 1970" startWordPosition="2476" endWordPosition="2477">. For example, the parser&apos;s bounded memory prevents it from being able to parse arbitrarily center embedded sentences or from allowing arbitrarily many phrases on the right frontier of a sentence to be modified. These are well established performance constraints on natural language (Chomsky, 1959, and many others). The lack of a disjunction operator limits the parser&apos;s ability to represent local ambiguities. This results in some locally ambiguous grammatical sentences being unparsable. The existence of such sentences for the human parser, called garden path sentences, is also well documented (Bever, 1970, among others). The representations currently used for handling local ambiguities appear to be adequate for building the constituent structure of any non—garden path sentences. The full verification of this claim awaits a study of how effectively probabilistic constraints can be used to resolve ambiguities. The work presented in this paper does not directly address the question of how ambiguities between possible predicate—argument structures are resolved. Also, the current parser is not intended to be a model of performance phenomena, although since the parser is intended to be computational</context>
</contexts>
<marker>Bever, 1970</marker>
<rawString>Bever, Thomas G (1970). The cognitive basis for linguistic structures. In J. R. Hayes, editor, Cognition and the Development of Language. John Wiley, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>On certain formal properties of grammars.</title>
<date>1959</date>
<journal>Information and Control,</journal>
<volume>2</volume>
<pages>137--167</pages>
<contexts>
<context position="15433" citStr="Chomsky, 1959" startWordPosition="2428" endWordPosition="2429">the definition of an SUG derivation could be changed to manifest these constraints. This new definition would be necessary to maintain the traditional split between competence and performance phenomena. The remaining constraints imposed at the level of the parser are traditionally treated as performance constraints. For example, the parser&apos;s bounded memory prevents it from being able to parse arbitrarily center embedded sentences or from allowing arbitrarily many phrases on the right frontier of a sentence to be modified. These are well established performance constraints on natural language (Chomsky, 1959, and many others). The lack of a disjunction operator limits the parser&apos;s ability to represent local ambiguities. This results in some locally ambiguous grammatical sentences being unparsable. The existence of such sentences for the human parser, called garden path sentences, is also well documented (Bever, 1970, among others). The representations currently used for handling local ambiguities appear to be adequate for building the constituent structure of any non—garden path sentences. The full verification of this claim awaits a study of how effectively probabilistic constraints can be used </context>
</contexts>
<marker>Chomsky, 1959</marker>
<rawString>Chomsky, Noam (1959). On certain formal properties of grammars. Information and Control, 2: 137-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Garrison Weeks Cottrell</author>
</authors>
<title>A Connectionist Approach to Word Sense Disambiguation.</title>
<date>1989</date>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>Los Altos, CA.</location>
<contexts>
<context position="1014" citStr="Cottrell, 1989" startWordPosition="132" endWordPosition="133"> architecture which stores and dynamically manipulates symbolic representations, but which can&apos;t represent arbitrary disjunction and has bounded memory. These problems can be overcome with Structure Unification Grammar&apos;s extensive use of partial descriptions. INTRODUCTION The similarity between connectionist models of computation and neuron computation suggests that a study of syntactic parsing in a connectionist computational architecture could lead to significant insights into ways natural language can be parsed efficiently. Unfortunately, previous investigations into connectionist parsing (Cottrell, 1989, Fanty, 1985, Selman and Hirst, 1987) have not been very successful. They cannot parse arbitrarily long sentences and have inadequate grammar representations. However, the difficulties with connectionist parsing can be overcome by adopting a different connectionist model of computation, namely that proposed by Shastri and Ajjanagadde (1990). This connectionist computational architecture differs from others in that it directly manifests the symbolic interpretation of the information it stores and manipulates. It also shares the massive parallelism, evidential reasoning ability, and neurologica</context>
</contexts>
<marker>Cottrell, 1989</marker>
<rawString>Cottrell, Garrison Weeks (1989). A Connectionist Approach to Word Sense Disambiguation. Morgan Kaufmann Publishers, Los Altos, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Fanty</author>
</authors>
<title>Context-free parsing in connectionist networks.</title>
<date>1985</date>
<tech>Technical Report TR174,</tech>
<institution>University of Rochester,</institution>
<location>Rochester, NY.</location>
<contexts>
<context position="1027" citStr="Fanty, 1985" startWordPosition="134" endWordPosition="135">ich stores and dynamically manipulates symbolic representations, but which can&apos;t represent arbitrary disjunction and has bounded memory. These problems can be overcome with Structure Unification Grammar&apos;s extensive use of partial descriptions. INTRODUCTION The similarity between connectionist models of computation and neuron computation suggests that a study of syntactic parsing in a connectionist computational architecture could lead to significant insights into ways natural language can be parsed efficiently. Unfortunately, previous investigations into connectionist parsing (Cottrell, 1989, Fanty, 1985, Selman and Hirst, 1987) have not been very successful. They cannot parse arbitrarily long sentences and have inadequate grammar representations. However, the difficulties with connectionist parsing can be overcome by adopting a different connectionist model of computation, namely that proposed by Shastri and Ajjanagadde (1990). This connectionist computational architecture differs from others in that it directly manifests the symbolic interpretation of the information it stores and manipulates. It also shares the massive parallelism, evidential reasoning ability, and neurological plausibilit</context>
</contexts>
<marker>Fanty, 1985</marker>
<rawString>Fanty, Mark (1985). Context-free parsing in connectionist networks. Technical Report TR174, University of Rochester, Rochester, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Henderson</author>
</authors>
<title>Structure unification grammar: A unifying framework for investigating natural language.</title>
<date>1990</date>
<tech>Technical Report MS-CIS-90-94,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="4056" citStr="Henderson, 1990" startWordPosition="595" endWordPosition="596"> and which it no longer needs to complete the parse. This technique requires that the representation of syntactic structure be able to leave unspecified the information which has already been determined but which is no longer needed for the completion of the parse. Thus the limitations of the architecture mean that the parser&apos;s representation of syntactic structure must be able to leave unspecified both the information which the input has not yet determined and the information which is no longer needed. In order to comply with these requirements, the parser uses Structure Unification Grammar (Henderson, 1990) as its grammatical framework. SUG is a formalization of accumulating informa144 tion about the phrase structure of a sentence until a complete description of the sentence&apos;s phrase structure tree is constructed. Its extensive use of partial descriptions makes it ideally suited for dealing with the limitations of the architecture. This paper focuses on the parser&apos;s representation of phrase structure information and on the way the parser accumulates this information during a parse. Brief descriptions of the grammar formalism and the implementation in the connectionist architecture are also given</context>
<context position="13663" citStr="Henderson, 1990" startWordPosition="2133" endWordPosition="2134">urrent description. When the parse is finished the parser checks to make sure it has produced a complete description of some phrase structure tree. THE GRAMMARS The grammars which are supported by the parser are a subset of those for Structure Unification Grammar. These grammars are for the most part lexicalized. Each lexicalized grammar entry is a rooted tree fragment with exactly one phonetically realized terminal, which is the word of the entry. Such grammar entries specify what information is known about the phrase structure of the sentence given the presence of the word, and can be used (Henderson, 1990) to simulate Lexicalized Tree Adjoining Grammar (Schabes, 1990). Nonlexical grammar entries are rooted tree fragments with no words. They can be used to express constructions like reduced relative clauses, for which no lexical information is necessary. The key: x y x immediately dominates y x dominates y x precedes y Xh y is the head feature value of x x is a terminal empty feature structure xt 146 did t Barbie t see t w 1 9.11 / lit pictures of yesterdays who t key: x x is equated with y Figure 2: A derivation for the sentence &amp;quot;Who did Barbie see a picture of yesterday&amp;quot;. current mechanism the</context>
</contexts>
<marker>Henderson, 1990</marker>
<rawString>Henderson, James (1990). Structure unification grammar: A unifying framework for investigating natural language. Technical Report MS-CIS-90-94, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Marcus</author>
<author>Donald Hindle</author>
<author>Margaret Fleck</author>
</authors>
<title>D-theory: Talking about talking about trees.</title>
<date>1983</date>
<booktitle>In Proceedings of the 21st Annual Meeting of the ACL,</booktitle>
<location>Cambridge, MA.</location>
<marker>Marcus, Hindle, Fleck, 1983</marker>
<rawString>Marcus, Mitchell; Hindle, Donald; and Fleck, Margaret (1983). D-theory: Talking about talking about trees. In Proceedings of the 21st Annual Meeting of the ACL, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Schabes</author>
</authors>
<title>Mathematical and Computational Aspects of Lezicalized Grammars.</title>
<date>1990</date>
<tech>PhD thesis,</tech>
<publisher></publisher>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<contexts>
<context position="13726" citStr="Schabes, 1990" startWordPosition="2142" endWordPosition="2143">to make sure it has produced a complete description of some phrase structure tree. THE GRAMMARS The grammars which are supported by the parser are a subset of those for Structure Unification Grammar. These grammars are for the most part lexicalized. Each lexicalized grammar entry is a rooted tree fragment with exactly one phonetically realized terminal, which is the word of the entry. Such grammar entries specify what information is known about the phrase structure of the sentence given the presence of the word, and can be used (Henderson, 1990) to simulate Lexicalized Tree Adjoining Grammar (Schabes, 1990). Nonlexical grammar entries are rooted tree fragments with no words. They can be used to express constructions like reduced relative clauses, for which no lexical information is necessary. The key: x y x immediately dominates y x dominates y x precedes y Xh y is the head feature value of x x is a terminal empty feature structure xt 146 did t Barbie t see t w 1 9.11 / lit pictures of yesterdays who t key: x x is equated with y Figure 2: A derivation for the sentence &amp;quot;Who did Barbie see a picture of yesterday&amp;quot;. current mechanism the parser uses to find possible long distance dependencies requir</context>
</contexts>
<marker>Schabes, 1990</marker>
<rawString>Schabes, Yves (1990). Mathematical and Computational Aspects of Lezicalized Grammars. PhD thesis, University of Pennsylvania, Philadelphia, PA. •</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Selman</author>
<author>Graeme Hirst</author>
</authors>
<title>Parsing as an energy minimization problem.</title>
<date>1987</date>
<booktitle>Genetic Algorithms and Simulated Annealing, chapter 11,</booktitle>
<pages>141--154</pages>
<editor>In Lawrence Davis, editor,</editor>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>Los Altos, CA.</location>
<contexts>
<context position="1052" citStr="Selman and Hirst, 1987" startWordPosition="136" endWordPosition="139">d dynamically manipulates symbolic representations, but which can&apos;t represent arbitrary disjunction and has bounded memory. These problems can be overcome with Structure Unification Grammar&apos;s extensive use of partial descriptions. INTRODUCTION The similarity between connectionist models of computation and neuron computation suggests that a study of syntactic parsing in a connectionist computational architecture could lead to significant insights into ways natural language can be parsed efficiently. Unfortunately, previous investigations into connectionist parsing (Cottrell, 1989, Fanty, 1985, Selman and Hirst, 1987) have not been very successful. They cannot parse arbitrarily long sentences and have inadequate grammar representations. However, the difficulties with connectionist parsing can be overcome by adopting a different connectionist model of computation, namely that proposed by Shastri and Ajjanagadde (1990). This connectionist computational architecture differs from others in that it directly manifests the symbolic interpretation of the information it stores and manipulates. It also shares the massive parallelism, evidential reasoning ability, and neurological plausibility of other connectionist </context>
</contexts>
<marker>Selman, Hirst, 1987</marker>
<rawString>Selman, Bart and Hirst, Graeme (1987). Parsing as an energy minimization problem. In Lawrence Davis, editor, Genetic Algorithms and Simulated Annealing, chapter 11, pages 141-154. Morgan Kaufmann Publishers, Los Altos, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lokendra Shastri</author>
<author>Venkat Ajjanagadde</author>
</authors>
<title>From simple associations to systematic reasoning: A connectionist representation of rules, variables and dynamic bindings.</title>
<date>1990</date>
<tech>Technical Report MS-CIS-90-05,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA. Revised</location>
<contexts>
<context position="1357" citStr="Shastri and Ajjanagadde (1990)" startWordPosition="181" endWordPosition="185">tion and neuron computation suggests that a study of syntactic parsing in a connectionist computational architecture could lead to significant insights into ways natural language can be parsed efficiently. Unfortunately, previous investigations into connectionist parsing (Cottrell, 1989, Fanty, 1985, Selman and Hirst, 1987) have not been very successful. They cannot parse arbitrarily long sentences and have inadequate grammar representations. However, the difficulties with connectionist parsing can be overcome by adopting a different connectionist model of computation, namely that proposed by Shastri and Ajjanagadde (1990). This connectionist computational architecture differs from others in that it directly manifests the symbolic interpretation of the information it stores and manipulates. It also shares the massive parallelism, evidential reasoning ability, and neurological plausibility of other connectionist architectures. Since virtually all characterizations of natural language syntax have relied heavily on symbolic representations, this architecture is ideally suited for the investigation of syntactic parsing. *This research was supported by ARO grant DAAL 03-89-C-0031, DARPA grant N00014-90-J1863, NSF gr</context>
<context position="12489" citStr="Shastri and Ajjanagadde (1990)" startWordPosition="1936" endWordPosition="1939">re for &amp;quot;who&amp;quot; to express the need for the subcategorized S to have an NP gap. Because the dominated NP node does not have an immediate parent, it must equate with some node which has an immediate parent. The site of this equation is the gap associated with &amp;quot;who&amp;quot;. THE PARSER The parser presented in this paper accumulates phrase structure information in the same way as does Structure Unification Grammar. It calculates SUG derivation steps using a small set of operations, and incrementally outputs the derivation as it parses. The parser is implemented in the connectionist architecture proposed by Shastri and Ajjanagadde (1990) as a special purpose module for syntactic constituent structure parsing. An SUG description is stored in the module&apos;s memory by representing nonterminal nodes as entities and all other needed information as predications over these nodes. If the parser starts to run out of memory space, then it can remove some nodes from the memory, thus forgetting all information about those nodes. The parser operations are implemented in pattern—action rules. As each word is input to the parser, one of these rules combines one of the word&apos;s grammar entries with the current description. When the parse is fini</context>
<context position="24694" citStr="Shastri and Ajjanagadde (1990)" startWordPosition="3988" endWordPosition="3991">sing internal equation, thus making &amp;quot;a man&amp;quot; the object of &amp;quot;know&amp;quot;. If, as shown, a verb is input then leftward attaching can be used to attach &amp;quot;a man&amp;quot; as the subject of the verb, and then the verb&apos;s S node can be equated with the —A node to make it the object of &amp;quot;know&amp;quot;. Since this parser is only concerned with constituent structure and not with predicate—argument structure, the fact that the —A node plays two different semantic roles in the two cases is not a problem. THE CONNECTIONIST IMPLEMENTATION The above parser is implemented using the connectionist computational architecture proposed by Shastri and Ajjanagadde (1990). This architecture solves the variable binding problem2 by using units which pulse periodically, and representing different entities in different phases. Units which are storing predications about the same entity pulse synchronously, and units which are storing predications about different entities pulse in different phases. The number of distinct entities which can be stored in a module&apos;s memory at one time is determined by the width of a pulse spike and the time between periodic firings (the period). Neurologically plausible estimates of these values put the maximum number of entities in th</context>
</contexts>
<marker>Shastri, Ajjanagadde, 1990</marker>
<rawString>Shastri, Lokendra and Ajjanagadde, Venkat (1990). From simple associations to systematic reasoning: A connectionist representation of rules, variables and dynamic bindings. Technical Report MS-CIS-90-05, University of Pennsylvania, Philadelphia, PA. Revised Jan 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay—Shanker</author>
</authors>
<title>A Study of Tree Adjoining Grammars.</title>
<date>1987</date>
<tech>PhD thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<marker>Vijay—Shanker, 1987</marker>
<rawString>Vijay—Shanker, K. (1987). A Study of Tree Adjoining Grammars. PhD thesis, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>