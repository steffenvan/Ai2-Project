<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011565">
<title confidence="0.987468">
Multilingual Dependency Learning:
Exploiting Rich Features for Tagging Syntactic and Semantic Dependencies
</title>
<author confidence="0.922943">
Hai Zhao(赵海)†, Wenliang Chen(陈文亮)‡,
Jun’ichi Kazama‡, Kiyotaka Uchimoto‡, and Kentaro Torisawa‡†Department of Chinese, Translation and Linguistics
</author>
<affiliation confidence="0.990401">
City University of Hong Kong
</affiliation>
<address confidence="0.949279">
83 Tat Chee Avenue, Kowloon, Hong Kong, China
</address>
<affiliation confidence="0.822698">
‡Language Infrastructure Group, MASTAR Project
National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.950824">
3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan, 619-0289
</address>
<email confidence="0.998284">
haizhao@cityu.edu.hk, chenwl@nict.go.jp
</email>
<sectionHeader confidence="0.995621" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9735874375">
This paper describes our system about mul-
tilingual syntactic and semantic dependency
parsing for our participation in the joint task
of CoNLL-2009 shared tasks. Our system
uses rich features and incorporates various in-
tegration technologies. The system is evalu-
ated on in-domain and out-of-domain evalu-
ation data of closed challenge of joint task.
For in-domain evaluation, our system ranks
the second for the average macro labeled F1 of
all seven languages, 82.52% (only about 0.1%
worse than the best system), and the first for
English with macro labeled F1 87.69%. And
for out-of-domain evaluation, our system also
achieves the second for average score of all
three languages.
</bodyText>
<sectionHeader confidence="0.999097" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999901636363636">
This paper describes the system of National In-
stitute of Information and Communications Tech-
nology (NICT) and City University of Hong Kong
(CityU) for the joint learning task of CoNLL-2009
shared task (Hajiˇc et al., 2009)1. The system is ba-
sically a pipeline of syntactic parser and semantic
parser. We use a syntactic parser that uses very rich
features and integrates graph- and transition-based
methods. As for the semantic parser, a group of well
selected feature templates are used with n-best syn-
tactic features.
</bodyText>
<footnote confidence="0.981965666666667">
1Our thanks give to the following corpus providers, (Taul´e
et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu
et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002).
</footnote>
<page confidence="0.996208">
61
</page>
<bodyText confidence="0.999863142857143">
The rest of the paper is organized as follows. The
next section presents the technical details of our syn-
tactic dependency parsing. Section 3 describes the
details of the semantic dependency parsing. Section
4 shows the evaluation results. Section 5 looks into a
few issues concerning our forthcoming work for this
shared task, and Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.971279" genericHeader="method">
2 Syntactic Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999993666666667">
Basically, we build our syntactic dependency parsers
based on the MSTParser, a freely available imple-
mentation2, whose details are presented in the paper
of McDonald and Pereira (2006). Moreover, we ex-
ploit rich features for the parsers. We represent fea-
tures by following the work of Chen et al. (2008) and
Koo et al. (2008) and use features based on depen-
dency relations predicted by transition-based parsers
(Nivre and McDonald, 2008). Chen et al. (2008) and
Koo et al. (2008) proposed the methods to obtain
new features from large-scale unlabeled data. In our
system, we perform their methods on training data
because the closed challenge does not allow to use
unlabeled data. In this paper, we call these new ad-
ditional features rich features.
</bodyText>
<subsectionHeader confidence="0.996702">
2.1 Basic Features
</subsectionHeader>
<bodyText confidence="0.999915714285714">
Firstly, we use all the features presented by McDon-
ald et al. (2006), if they are available in data. Then
we add new features for the languages having FEAT
information (Hajiˇc et al., 2009). FEAT is a set of
morphological-features, e.g. more detailed part of
speech, number, gender, etc. We try to align differ-
ent types of morphological-features. For example,
</bodyText>
<footnote confidence="0.979382">
2http://mstparser.sourceforge.net
</footnote>
<note confidence="0.8811735">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 61–66,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.99912">
we can obtain a sequence of gender tags of all words
from a head h to its dependent d. Then we represent
the features based on the obtained sequences.
Based on the results of development data, we per-
form non-projective parsing for Czech and German
and perform projective parsing for Catalan, Chinese,
English, Japanese, and Spanish.
</bodyText>
<subsectionHeader confidence="0.977034">
2.2 Features Based on Dependency Pairs
</subsectionHeader>
<bodyText confidence="0.610225">
I see a beautiful bird .
</bodyText>
<figureCaption confidence="0.999589">
Figure 1: Example dependency graph.
</figureCaption>
<bodyText confidence="0.9998126">
Chen et al. (2008) presented a method of extract-
ing short dependency pairs from large-scale auto-
parsed data. Here, we extract all dependency pairs
rather than short dependency pairs from training
data because we believe that training data are reli-
able. In a parsed sentence, if two words have de-
pendency relation, we add this word pair into a list
named L and count its frequency. We consider the
direction. For example, in figure 1, a and bird have
dependency relation in the sentence “I see a beauti-
ful bird.”. Then we add word pair “a-bird:HEAD”3
into list L and accumulate its frequency.
We remove the pairs which occur only once in
training data. According to frequency, we then
group word pairs into different buckets, with bucket
LOW for frequencies 2-7, bucket MID for frequen-
cies 8-14, and bucket HIGH for frequencies 15+.
We set these threshold values by following the set-
ting of Chen et al. (2008). For example, the fre-
quency of pair “a-bird:HEAD” is 5. Then it is
grouped into bucket “LOW”. We also add a vir-
tual bucket “ZERO” to represent the pairs that are
not included in the list. So we have four buckets.
“ZERO”, “LOW”, “MID”, and “HIGH” are used as
bucket IDs.
Based on the buckets, we represent new features
for a head h and its dependent d. We check word
pairs surrounding h and d. Table 1 shows the word
pairs, where h-word refers to the head word, d-word
refers to the dependent word, h-word-1 refers to
</bodyText>
<footnote confidence="0.851875">
3HEAD means that bird is the head of the pair.
</footnote>
<bodyText confidence="0.9972089375">
the word to the left of the head in the sentence, h-
word+1 refers to the word to the right of the head,
d-word-1 refers to the word to the left of the depen-
dent, and d-word+1 refers the word to the right of
the dependent. Then we obtain the bucket IDs of
these word pairs from L.
We generate new features consisting of indicator
functions for bucket IDs of word pairs. We call these
features word-pair-based features. We also generate
combined features involving bucket IDs and part-of-
speech tags of heads.
h-word, d-word
h-word-1, d-word
h-word+1, d-word
h-word, d-word-1
h-word, d-word+1
</bodyText>
<tableCaption confidence="0.997441">
Table 1: Word pairs for feature representation
</tableCaption>
<subsectionHeader confidence="0.972172">
2.3 Features Based on Word Clusters
</subsectionHeader>
<bodyText confidence="0.999786333333333">
Koo et al. (2008) presented new features based on
word clusters obtained from large-scale unlabeled
data and achieved large improvement for English
and Czech. Here, word clusters are generated only
from the training data for all the languages. We per-
form word clustering by using the clustering tool4,
which also was used by Koo et al. (2008). The
cluster-based features are the same as the ones used
by Koo et al. (2008).
</bodyText>
<subsectionHeader confidence="0.988354">
2.4 Features Based on Predicted Relations
</subsectionHeader>
<bodyText confidence="0.999986111111111">
Nivre and McDonald (2008) presented an integrat-
ing method to provide additional information for
graph-based and transition-based parsers. Here, we
represent features based on dependency relations
predicted by transition-based parsers for graph-
based parser. Based on the results on development
data, we choose the MaltParser for Catalan, Czech,
German, and Spanish, and choose another MaxEnt-
based parser for Chinese, English, and Japanese.
</bodyText>
<subsectionHeader confidence="0.921611">
2.4.1 A Transition-based Parser: MaltParser
</subsectionHeader>
<bodyText confidence="0.973051">
For Catalan, Czech, German, and Spanish, we
use the MaltParser, a freely available implementa-
</bodyText>
<footnote confidence="0.983219">
4http://www.cs.berkeley.edu/˜pliang/software/brown-
cluster-1.2.zip
</footnote>
<page confidence="0.998787">
62
</page>
<bodyText confidence="0.999944222222222">
tion5, whose details are presented in the paper of
Nivre (2003). More information about the parser can
be available in the paper (Nivre, 2003).
Due to computational cost, we do not select new
feature templates for the MaltParser. Following the
features settings of Hall et al. (2007), we use their
Czech feature file and Catalan feature file. To sim-
ply, we apply Czech feature file for German too, and
apply Catalan feature file for Spanish.
</bodyText>
<subsectionHeader confidence="0.9813755">
2.4.2 Another Transition-based Parser:
MaxEnt-based Parser
</subsectionHeader>
<bodyText confidence="0.999989555555555">
In three highly projective language, Chinese,
English and Japanese, we use the maximum en-
tropy syntactic dependency parser as in Zhao and
Kit (2008). We still use the similar feature notations
of that work. We use the same greedy feature selec-
tion of Zhao et al. (2009) to determine an optimal
feature template set for each language. Full feature
sets for the three languages can be found at website,
http://bcmi.sjtu.edu.cn/˜zhaohai.
</bodyText>
<subsectionHeader confidence="0.911402">
2.4.3 Feature Representation
</subsectionHeader>
<bodyText confidence="0.868571571428572">
For training data, we use 2-way jackknifing to
generate predicted dependency parsing trees by two
transition-based parsers. Following the features of
Nivre and McDonald (2008), we define features for
a head h and its dependent d with label l as shown in
table 2, where GTran refers to dependency parsing
trees generated by the MaltParser or MaxEnt-base
Parser and * refers to any label. All features are
conjoined with the part-of-speech tags of the words
involved in the dependency.
Is (h, d, *) in GTran?
Is (h, d, l) in GTran?
Is (h, d, *) not in GTran?
Is (h, d, l) not in GTran?
</bodyText>
<tableCaption confidence="0.982498">
Table 2: Features set based on predicted labels
</tableCaption>
<sectionHeader confidence="0.849713" genericHeader="method">
3 n-best Syntactic Features for Semantic
Dependency Parsing
</sectionHeader>
<bodyText confidence="0.912254">
Due to the limited computational resource that we
have, we used the the similar learning framework as
our participant in semantic-only task (Zhao et al.,
</bodyText>
<footnote confidence="0.944369">
5http://w3.msi.vxu.se/˜nivre/research/MaltParser.html
</footnote>
<table confidence="0.99981925">
Normal n-best Matched
Ca 53 54 50
Ch 75 65 55
En 73 70 63
</table>
<tableCaption confidence="0.99854">
Table 3: Feature template sets:n-best vs. non-n-best
</tableCaption>
<bodyText confidence="0.999945975609756">
2009). Namely, three languages, a single maximum
entropy model is used for all identification and clas-
sification tasks of predicate senses or argument la-
bels in four languages, Catalan, Czech, Japanese, or
Spanish. For the rest three languages, an individual
sense classifier still using maximum entropy is ad-
ditionally used to output the predicate sense previ-
ously. More details about argument candidate prun-
ing strategies and feature template set selection are
described in Zhao et al. (2009).
The same feature template sets as the semantic-
only task are used for three languages, Czech, Ger-
man and Japanese. For the rest four languages, we
further use n-best syntactic features to strengthen
semantic dependency parsing upon those automati-
cally discovered feature template sets. However, we
cannot obtain an obvious performance improvement
in Spanish by using n-best syntactic features. There-
fore, only Catalan, Chinese and English semantic
parsing adopted these types of features at last.
Our work about n-best syntactic features still
starts from the feature template set that is originally
selected for the semantic-only task. The original fea-
ture template set is hereafter referred to ’the normal’
or ’non-n-best’. In practice, only 2nd-best syntactic
outputs are actually adopted by our system for the
joint task.
To generate helpful feature templates from the
2nd-best syntactic tree, we simply let all feature tem-
plates in the normal feature set that are based on
the 1st-best syntactic tree now turn to the 2nd-best
one. Using the same notations for feature template
representation as in Zhao et al. (2009), we take an
example to show how the original n-best features
are produced. Assuming a.children.dprel.bag is
one of syntactic feature templates in the normal
set, this feature means that all syntactic children of
the argument candidate (a) are chosen, and their
dependant labels are collected, the duplicated la-
bels are removed and then sorted, finally all these
strings are concatenated as a feature. The cor-
</bodyText>
<page confidence="0.98566">
63
</page>
<figure confidence="0.987451578947368">
Language Features
Catalan p:2.lm.dprel
a.lemma + a:2.h.form
a.lemma + a:2.pphead.form
(a:2:p:2|dpPath.dprel.seq) + p.FEAT1
Chinese a:2.h.pos
a:2.children.pos.seq + p:2.children.pos.seq
a:2:p:2|dpPath.dprel.bag
a:2:p:2|dpPathPred.form.seq
a:2:p:2|dpPath.pos.bag
(a:2:p:2|dpTreeRelation) + p.pos
(a:2:p:2|dpPath.dprel.seq) + a.pos
English a:2:p:2|dpPathPred.lemma.bag
a:2:p:2|dpPathPred.pos.bag
a:2:p:2|dpTreeRelation
a:2:p:2|dpPath.dprel.seq
a:2:p:2|dpPathPred.dprel.seq
a.lemma + a:2.dprel + a:2.h.lemma
(a:2:p:2|dpTreeRelation) + p.pos
</figure>
<tableCaption confidence="0.910814">
Table 4: Features for n-best syntactic tree
</tableCaption>
<bodyText confidence="0.9997039">
responding 2nd-best syntactic feature will be a :
2.children.dprel.bag. As all operations to gener-
ate the feature for a.children.dprel.bag is within
the 1st-best syntactic tree, while those for a :
2.children.dprel.bag is within the 2nd-best one. As
all these 2nd-best syntactic features are generated,
we use the same greedy feature selection procedure
as in Zhao et al. (2009) to determine the best fit fea-
ture template set according to the evaluation results
in the development set.
For Catalan, Chinese and English, three opti-
mal n-best feature sets are obtained, respectively.
Though dozens of n-best features are initially gen-
erated for selection, only few of them survive af-
ter the greedy selection. A feature number statis-
tics is in Table 3, and those additionally selected
n-best features for three languages are in Table
4. Full feature lists and their explanation for
all languages will be available at the website,
http://bcmi.sjtu.edu.cn/˜zhaohai.
</bodyText>
<sectionHeader confidence="0.995615" genericHeader="method">
4 Evaluation Results
</sectionHeader>
<bodyText confidence="0.9974244">
Two tracks (closed and open challenges) are pro-
vided for joint task of CoNLL2009 shared task.
We participated in the closed challenge and evalu-
ated our system on the in-domain and out-of-domain
evaluation data.
</bodyText>
<table confidence="0.992962">
avg. Cz En Gr
Syntactic (LAS) 77.96 75.58 82.38 75.93
Semantic (Labeled F1) 75.01 82.66 74.58 67.78
Joint (Macro F1) 76.51 79.12 78.51 71.89
</table>
<tableCaption confidence="0.9920485">
Table 7: The effect of rich features for syntactic depen-
dency parsing
</tableCaption>
<subsectionHeader confidence="0.991325">
4.1 Official Results
</subsectionHeader>
<bodyText confidence="0.9999925">
The official results for the joint task are in Table 5,
and the out-of-domain task in Table 6, where num-
bers in bold stand for the best performances for the
specific language. For out-of-domain (OOD) eval-
uation, we did not perform any domain adaptation.
For both in-domain and out-of-domain evaluation,
our system achieved the second best performance
for the average Macro F1 scores of all the languages.
And our system provided the first best performance
for the average Semantic Labeled F1 score and the
forth for the average Labeled Syntactic Accuracy
score for in-domain evaluation.
</bodyText>
<subsectionHeader confidence="0.989436">
4.2 Further results
</subsectionHeader>
<bodyText confidence="0.999606636363636">
At first, we check the effect of rich features for syn-
tactic dependency parsing. Table 7 shows the com-
parative results of basic features and all features on
test and development data, where “Basic” refers to
the system with basic features and “ALL” refers to
the system with basic features plus rich features. We
found that the additional features provided improve-
ment of 1.72% for test data and 1.90% for develop-
ment data.
Then we investigate the effect of different train-
ing data size for semantic parsing. The learning
</bodyText>
<tableCaption confidence="0.895915">
Table 6: The official results of our submission for out-of-
</tableCaption>
<table confidence="0.965017588235294">
domain task(%)
Test Dev
Basic ALL Basic ALL
Catalan 82.91 85.88 83.15 85.98
Chinese 74.28 75.67 73.36 75.64
Czech 77.21 79.70 77.91 80.22
English 88.63 89.19 86.35 87.40
German 84.61 86.24 83.99 85.44
Japanese 92.31 92.32 92.01 92.85
Spanish 83.59 86.29 83.73 86.22
Average 83.32 85.04 82.92 84.82
(+1.72) (+1.90)
64
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 85.04 85.88 75.67 79.70 89.19 86.24 92.32 86.29
Semantic (Labeled F1) 79.96 80.10 76.77 82.04 86.15 76.19 78.17 80.29
Joint (Macro F1) 82.52 83.01 76.23 80.87 87.69 81.22 85.28 83.31
</table>
<tableCaption confidence="0.99417">
Table 5: The official results of our joint submission (%)
</tableCaption>
<table confidence="0.999874666666667">
Data Czech Chinese English
normal n-best normal n-best
25% 80.71 75.12 75.24 82.02 82.06
50% 81.52 76.50 76.59 83.52 83.42
75% 81.90 76.92 77.01 84.21 84.30
100% 82.24 77.35 77.34 84.73 84.80
</table>
<tableCaption confidence="0.9960945">
Table 8: The performance in development set (semantic
labeled F1) vs. training corpus size
</tableCaption>
<bodyText confidence="0.9996305">
curves are drawn for Czech, Chinese and English.
We use 25%, 50% and 75% training corpus, respec-
tively. The results in development sets are given in
Table 8. Note that in this table the differences be-
tween normal and n-best feature template sets are
also given for Chinese and English. The results
in the table show that n-best features help improve
Chinese semantic parsing as the training corpus is
smaller, while it works for English as the training
corpus is larger.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999982142857143">
This work shows our further endeavor in syntactic
and semantic dependency parsing, based on our pre-
vious work (Chen et al., 2008; Zhao and Kit, 2008).
Chen et al. (Chen et al., 2008) and Koo et al. (Koo
et al., 2008) used large-scale unlabeled data to im-
prove syntactic dependency parsing performance.
Here, we just performed their method on training
data. From the results, we found that the new fea-
tures provided better performance. In future work,
we can try these methods on large-scale unlabeled
data for other languages besides Chinese and En-
glish.
In Zhao and Kit (2008), we addressed that seman-
tic parsing should benefit from cross-validated train-
ing corpus and n-best syntactic output. These two
issues have been implemented during this shared
task. Though existing work show that re-ranking for
semantic-only or syntactic-semantic joint tasks may
bring higher performance, the limited computational
resources does not permit us to do this for multiple
languages.
To analyze the advantage and the weakness of our
system, the ranks for every languages of our sys-
tem’s outputs are given in Table 9, and the perfor-
mance differences between our system and the best
one in Table 106. The comparisons in these two ta-
bles indicate that our system is slightly weaker in the
syntactic parsing part, this may be due to the reason
that syntactic parsing in our system does not ben-
efit from semantic parsing as the other joint learn-
ing systems. However, considering that the seman-
tic parsing in our system simply follows the output
of the syntactic parsing and the semantic part of our
system still ranks the first for the average score, the
semantic part of our system does output robust and
stable results. It is worth noting that semantic la-
beled F1 in Czech given by our system is 4.47%
worse than the best one. This forby gap in this lan-
guage further indicates the advantage of our system
in the other six languages and some latent bugs or
learning framework misuse in Czech semantic pars-
ing.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99993625">
We describe the system that uses rich features and
incorporates integrating technology for joint learn-
ing task of syntactic and semantic dependency pars-
ing in multiple languages. The evaluation results
show that our system is good at both syntactic and
semantic parsing, which suggests that a feature-
oriented method is effective in multiple language
processing.
</bodyText>
<sectionHeader confidence="0.997779" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.886896">
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006.
</reference>
<footnote confidence="0.9855665">
6The difference for Chinese in the latter table is actually
computed between ours and the second best system.
</footnote>
<page confidence="0.996118">
65
</page>
<table confidence="0.995609">
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 4 4 4 4 2 3 3 4
Semantic (Labeled F1) 1 1 3 4 1 2 2 1
Joint (Macro F1) 2 1 3 4 1 3 2 1
</table>
<tableCaption confidence="0.980031">
Table 9: Our system’s rank within the joint task according to three main measures
</tableCaption>
<table confidence="0.998544">
average Catalan Chinese Czech English German Japanese Spanish
Syntactic (LAS) 0.73 1.98 0.84 0.68 0.69 1.24 0.25 1.35
Semantic (Labeled F1) - - 0.38 4.47 - 2.42 0.09 -
Joint (Macro F1) 0.12 - 0.15 2.40 - 1.22 0.37 -
</table>
<tableCaption confidence="0.9913425">
Table 10: The performance differences between our system and the best one within the joint task according to three
main measures
</tableCaption>
<reference confidence="0.997872408450705">
The SALSA corpus: a German corpus resource for
lexical semantics. In Proceedings of LREC-2006,
Genoa, Italy.
Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto,
Yujie Zhang, and Hitoshi Isahara. 2008. Dependency
parsing with short dependency relations in unlabeled
data. In Proceedings of IJCNLP-2008, Hyderabad, In-
dia, January 8-10.
Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr
Sgall, Petr Pajas, Jan ˇStˇep´anek, JiˇrfHavelka, Marie
Mikulov´a, and Zdenˇek ˇZabokrtsk´y. 2006. Prague De-
pendency Treebank 2.0.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Martf, Llufs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of CoNLL-
2009, Boulder, Colorado, USA.
Johan Hall, Jens Nilsson, Joakim Nivre, G¨ulsen Eryiˇgit,
Be´ata Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? a study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933–939, Prague, Czech, June.
Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida.
2002. Construction of a Japanese relevance-tagged
corpus. In Proceedings of LREC-2002, pages 2008–
2013, Las Palmas, Canary Islands.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple semi-supervised dependency parsing. In Pro-
ceedings ofACL-08: HLT, pages 595–603, Columbus,
Ohio, USA, June.
Ryan McDonald and Fernando Pereira. 2006. On-
line learning of approximate dependency parsing algo-
rithms. In Proceedings of EACL-2006, pages 81–88,
Trento, Italy, April.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proceedings of CoNLL-
X, New York City, June.
Joakim Nivre and Ryan McDonald. 2008. Integrating
graph-based and transition-based dependency parsers.
In Proceedings of ACL-08: HLT, pages 950–958,
Columbus, Ohio, June.
Joakim Nivre. 2003. An efficient algorithm for projec-
tive dependency parsing. In Proceedings of the 8th In-
ternational Workshop on Parsing Technologies (IWPT
03), pages 149–160, Nancy, France, April 23-25.
Martha Palmer and Nianwen Xue. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143–172.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llufs M`arquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proceedings of the CoNLL-
2008.
Mariona Taul´e, Maria Ant`onia Martf, and Marta Re-
casens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. In Proceedings of the LREC-
2008, Marrakesh, Morroco.
Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and
semantic dependencies with two single-stage maxi-
mum entropy models. In Proceedings of CoNLL-2008,
pages 203–207, Manchester, UK, August 16-17.
Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong
Zhou. 2009. Multilingual dependency learning: A
huge feature engineering method to semantic depen-
dency parsing. In Proceedings of CoNLL-2009, Boul-
der, Colorado, USA.
</reference>
<page confidence="0.98902">
66
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.685232">
<title confidence="0.999454">Multilingual Dependency Learning: Exploiting Rich Features for Tagging Syntactic and Semantic Dependencies</title>
<author confidence="0.951691">Wenliang</author>
<affiliation confidence="0.9939795">and of Chinese, Translation and City University of Hong</affiliation>
<address confidence="0.808647">83 Tat Chee Avenue, Kowloon, Hong Kong,</address>
<affiliation confidence="0.9665575">Infrastructure Group, MASTAR National Institute of Information and Communications</affiliation>
<address confidence="0.995643">3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, Japan,</address>
<email confidence="0.983817">haizhao@cityu.edu.hk,chenwl@nict.go.jp</email>
<abstract confidence="0.999423352941176">This paper describes our system about multilingual syntactic and semantic dependency parsing for our participation in the joint task of CoNLL-2009 shared tasks. Our system uses rich features and incorporates various integration technologies. The system is evaluated on in-domain and out-of-domain evaluation data of closed challenge of joint task. For in-domain evaluation, our system ranks the second for the average macro labeled F1 of all seven languages, 82.52% (only about 0.1% worse than the best system), and the first for English with macro labeled F1 87.69%. And for out-of-domain evaluation, our system also achieves the second for average score of all three languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Pad´o</author>
<author>Manfred Pinkal</author>
</authors>
<date>2006</date>
<marker>Burchardt, Erk, Frank, Kowalski, Pad´o, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Pad´o, and Manfred Pinkal. 2006.</rawString>
</citation>
<citation valid="false">
<title>The SALSA corpus: a German corpus resource for lexical semantics.</title>
<booktitle>In Proceedings of LREC-2006,</booktitle>
<location>Genoa, Italy.</location>
<marker></marker>
<rawString>The SALSA corpus: a German corpus resource for lexical semantics. In Proceedings of LREC-2006, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenliang Chen</author>
<author>Daisuke Kawahara</author>
<author>Kiyotaka Uchimoto</author>
<author>Yujie Zhang</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Dependency parsing with short dependency relations in unlabeled data.</title>
<date>2008</date>
<booktitle>In Proceedings of IJCNLP-2008,</booktitle>
<location>Hyderabad, India,</location>
<contexts>
<context position="2659" citStr="Chen et al. (2008)" startWordPosition="399" endWordPosition="402">chnical details of our syntactic dependency parsing. Section 3 describes the details of the semantic dependency parsing. Section 4 shows the evaluation results. Section 5 looks into a few issues concerning our forthcoming work for this shared task, and Section 6 concludes the paper. 2 Syntactic Dependency Parsing Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2, whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the l</context>
<context position="4167" citStr="Chen et al. (2008)" startWordPosition="638" endWordPosition="641">onference on Computational Natural Language Learning (CoNLL): Shared Task, pages 61–66, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics we can obtain a sequence of gender tags of all words from a head h to its dependent d. Then we represent the features based on the obtained sequences. Based on the results of development data, we perform non-projective parsing for Czech and German and perform projective parsing for Catalan, Chinese, English, Japanese, and Spanish. 2.2 Features Based on Dependency Pairs I see a beautiful bird . Figure 1: Example dependency graph. Chen et al. (2008) presented a method of extracting short dependency pairs from large-scale autoparsed data. Here, we extract all dependency pairs rather than short dependency pairs from training data because we believe that training data are reliable. In a parsed sentence, if two words have dependency relation, we add this word pair into a list named L and count its frequency. We consider the direction. For example, in figure 1, a and bird have dependency relation in the sentence “I see a beautiful bird.”. Then we add word pair “a-bird:HEAD”3 into list L and accumulate its frequency. We remove the pairs which </context>
<context position="16132" citStr="Chen et al., 2008" startWordPosition="2540" endWordPosition="2543">size curves are drawn for Czech, Chinese and English. We use 25%, 50% and 75% training corpus, respectively. The results in development sets are given in Table 8. Note that in this table the differences between normal and n-best feature template sets are also given for Chinese and English. The results in the table show that n-best features help improve Chinese semantic parsing as the training corpus is smaller, while it works for English as the training corpus is larger. 5 Discussion This work shows our further endeavor in syntactic and semantic dependency parsing, based on our previous work (Chen et al., 2008; Zhao and Kit, 2008). Chen et al. (Chen et al., 2008) and Koo et al. (Koo et al., 2008) used large-scale unlabeled data to improve syntactic dependency parsing performance. Here, we just performed their method on training data. From the results, we found that the new features provided better performance. In future work, we can try these methods on large-scale unlabeled data for other languages besides Chinese and English. In Zhao and Kit (2008), we addressed that semantic parsing should benefit from cross-validated training corpus and n-best syntactic output. These two issues have been implem</context>
</contexts>
<marker>Chen, Kawahara, Uchimoto, Zhang, Isahara, 2008</marker>
<rawString>Wenliang Chen, Daisuke Kawahara, Kiyotaka Uchimoto, Yujie Zhang, and Hitoshi Isahara. 2008. Dependency parsing with short dependency relations in unlabeled data. In Proceedings of IJCNLP-2008, Hyderabad, India, January 8-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Jarmila Panevov´a</author>
<author>Eva Hajiˇcov´a</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
<author>Jan ˇStˇep´anek</author>
<author>Marie Mikulov´a JiˇrfHavelka</author>
<author>Zdenˇek ˇZabokrtsk´y</author>
</authors>
<date>2006</date>
<journal>Prague Dependency Treebank</journal>
<volume>2</volume>
<marker>Hajiˇc, Panevov´a, Hajiˇcov´a, Sgall, Pajas, ˇStˇep´anek, JiˇrfHavelka, ˇZabokrtsk´y, 2006</marker>
<rawString>Jan Hajiˇc, Jarmila Panevov´a, Eva Hajiˇcov´a, Petr Sgall, Petr Pajas, Jan ˇStˇep´anek, JiˇrfHavelka, Marie Mikulov´a, and Zdenˇek ˇZabokrtsk´y. 2006. Prague Dependency Treebank 2.0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Martf</author>
<author>Llufs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL2009,</booktitle>
<location>Boulder, Colorado, USA.</location>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Martf, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Martf, Llufs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of CoNLL2009, Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Hall</author>
<author>Jens Nilsson</author>
<author>Joakim Nivre</author>
<author>G¨ulsen Eryiˇgit</author>
<author>Be´ata Megyesi</author>
<author>Mattias Nilsson</author>
<author>Markus Saers</author>
</authors>
<title>Single malt or blended? a study in multilingual parser optimization.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>933--939</pages>
<location>Prague, Czech,</location>
<marker>Hall, Nilsson, Nivre, Eryiˇgit, Megyesi, Nilsson, Saers, 2007</marker>
<rawString>Johan Hall, Jens Nilsson, Joakim Nivre, G¨ulsen Eryiˇgit, Be´ata Megyesi, Mattias Nilsson, and Markus Saers. 2007. Single malt or blended? a study in multilingual parser optimization. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 933–939, Prague, Czech, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Kˆoiti Hasida</author>
</authors>
<title>Construction of a Japanese relevance-tagged corpus.</title>
<date>2002</date>
<booktitle>In Proceedings of LREC-2002,</booktitle>
<pages>pages</pages>
<location>Las Palmas, Canary Islands.</location>
<contexts>
<context position="1957" citStr="Kawahara et al., 2002" startWordPosition="285" endWordPosition="288">echnology (NICT) and City University of Hong Kong (CityU) for the joint learning task of CoNLL-2009 shared task (Hajiˇc et al., 2009)1. The system is basically a pipeline of syntactic parser and semantic parser. We use a syntactic parser that uses very rich features and integrates graph- and transition-based methods. As for the semantic parser, a group of well selected feature templates are used with n-best syntactic features. 1Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 The rest of the paper is organized as follows. The next section presents the technical details of our syntactic dependency parsing. Section 3 describes the details of the semantic dependency parsing. Section 4 shows the evaluation results. Section 5 looks into a few issues concerning our forthcoming work for this shared task, and Section 6 concludes the paper. 2 Syntactic Dependency Parsing Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2, whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we ex</context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi, and Kˆoiti Hasida. 2002. Construction of a Japanese relevance-tagged corpus. In Proceedings of LREC-2002, pages 2008– 2013, Las Palmas, Canary Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>595--603</pages>
<location>Columbus, Ohio, USA,</location>
<contexts>
<context position="2681" citStr="Koo et al. (2008)" startWordPosition="404" endWordPosition="407">syntactic dependency parsing. Section 3 describes the details of the semantic dependency parsing. Section 4 shows the evaluation results. Section 5 looks into a few issues concerning our forthcoming work for this shared task, and Section 6 concludes the paper. 2 Syntactic Dependency Parsing Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2, whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT i</context>
<context position="6311" citStr="Koo et al. (2008)" startWordPosition="1018" endWordPosition="1021">word to the right of the head, d-word-1 refers to the word to the left of the dependent, and d-word+1 refers the word to the right of the dependent. Then we obtain the bucket IDs of these word pairs from L. We generate new features consisting of indicator functions for bucket IDs of word pairs. We call these features word-pair-based features. We also generate combined features involving bucket IDs and part-ofspeech tags of heads. h-word, d-word h-word-1, d-word h-word+1, d-word h-word, d-word-1 h-word, d-word+1 Table 1: Word pairs for feature representation 2.3 Features Based on Word Clusters Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech. Here, word clusters are generated only from the training data for all the languages. We perform word clustering by using the clustering tool4, which also was used by Koo et al. (2008). The cluster-based features are the same as the ones used by Koo et al. (2008). 2.4 Features Based on Predicted Relations Nivre and McDonald (2008) presented an integrating method to provide additional information for graph-based and transition-based parsers. Here, we repre</context>
<context position="16220" citStr="Koo et al., 2008" startWordPosition="2559" endWordPosition="2562">orpus, respectively. The results in development sets are given in Table 8. Note that in this table the differences between normal and n-best feature template sets are also given for Chinese and English. The results in the table show that n-best features help improve Chinese semantic parsing as the training corpus is smaller, while it works for English as the training corpus is larger. 5 Discussion This work shows our further endeavor in syntactic and semantic dependency parsing, based on our previous work (Chen et al., 2008; Zhao and Kit, 2008). Chen et al. (Chen et al., 2008) and Koo et al. (Koo et al., 2008) used large-scale unlabeled data to improve syntactic dependency parsing performance. Here, we just performed their method on training data. From the results, we found that the new features provided better performance. In future work, we can try these methods on large-scale unlabeled data for other languages besides Chinese and English. In Zhao and Kit (2008), we addressed that semantic parsing should benefit from cross-validated training corpus and n-best syntactic output. These two issues have been implemented during this shared task. Though existing work show that re-ranking for semantic-on</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings ofACL-08: HLT, pages 595–603, Columbus, Ohio, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL-2006,</booktitle>
<pages>81--88</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="2540" citStr="McDonald and Pereira (2006)" startWordPosition="377" endWordPosition="380">dt et al., 2006) and (Kawahara et al., 2002). 61 The rest of the paper is organized as follows. The next section presents the technical details of our syntactic dependency parsing. Section 3 describes the details of the semantic dependency parsing. Section 4 shows the evaluation results. Section 5 looks into a few issues concerning our forthcoming work for this shared task, and Section 6 concludes the paper. 2 Syntactic Dependency Parsing Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2, whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use </context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In Proceedings of EACL-2006, pages 81–88, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual dependency analysis with a twostage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLLX,</booktitle>
<location>New York City,</location>
<contexts>
<context position="3192" citStr="McDonald et al. (2006)" startWordPosition="488" endWordPosition="492">atures for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT information (Hajiˇc et al., 2009). FEAT is a set of morphological-features, e.g. more detailed part of speech, number, gender, etc. We try to align different types of morphological-features. For example, 2http://mstparser.sourceforge.net Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 61–66, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics we can obtain a sequence of gender tags of all words from a head h to its de</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a twostage discriminative parser. In Proceedings of CoNLLX, New York City, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Ryan McDonald</author>
</authors>
<title>Integrating graph-based and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>950--958</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2793" citStr="Nivre and McDonald, 2008" startWordPosition="420" endWordPosition="423">n 4 shows the evaluation results. Section 5 looks into a few issues concerning our forthcoming work for this shared task, and Section 6 concludes the paper. 2 Syntactic Dependency Parsing Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2, whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT information (Hajiˇc et al., 2009). FEAT is a set of morphological-features, e.g. more detailed part of speech, nu</context>
<context position="6784" citStr="Nivre and McDonald (2008)" startWordPosition="1096" endWordPosition="1099"> h-word+1, d-word h-word, d-word-1 h-word, d-word+1 Table 1: Word pairs for feature representation 2.3 Features Based on Word Clusters Koo et al. (2008) presented new features based on word clusters obtained from large-scale unlabeled data and achieved large improvement for English and Czech. Here, word clusters are generated only from the training data for all the languages. We perform word clustering by using the clustering tool4, which also was used by Koo et al. (2008). The cluster-based features are the same as the ones used by Koo et al. (2008). 2.4 Features Based on Predicted Relations Nivre and McDonald (2008) presented an integrating method to provide additional information for graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for graphbased parser. Based on the results on development data, we choose the MaltParser for Catalan, Czech, German, and Spanish, and choose another MaxEntbased parser for Chinese, English, and Japanese. 2.4.1 A Transition-based Parser: MaltParser For Catalan, Czech, German, and Spanish, we use the MaltParser, a freely available implementa4http://www.cs.berkeley.edu/˜pliang/software/brow</context>
<context position="8544" citStr="Nivre and McDonald (2008)" startWordPosition="1355" endWordPosition="1358">y projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use the similar feature notations of that work. We use the same greedy feature selection of Zhao et al. (2009) to determine an optimal feature template set for each language. Full feature sets for the three languages can be found at website, http://bcmi.sjtu.edu.cn/˜zhaohai. 2.4.3 Feature Representation For training data, we use 2-way jackknifing to generate predicted dependency parsing trees by two transition-based parsers. Following the features of Nivre and McDonald (2008), we define features for a head h and its dependent d with label l as shown in table 2, where GTran refers to dependency parsing trees generated by the MaltParser or MaxEnt-base Parser and * refers to any label. All features are conjoined with the part-of-speech tags of the words involved in the dependency. Is (h, d, *) in GTran? Is (h, d, l) in GTran? Is (h, d, *) not in GTran? Is (h, d, l) not in GTran? Table 2: Features set based on predicted labels 3 n-best Syntactic Features for Semantic Dependency Parsing Due to the limited computational resource that we have, we used the the similar lea</context>
</contexts>
<marker>Nivre, McDonald, 2008</marker>
<rawString>Joakim Nivre and Ryan McDonald. 2008. Integrating graph-based and transition-based dependency parsers. In Proceedings of ACL-08: HLT, pages 950–958, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>An efficient algorithm for projective dependency parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03),</booktitle>
<pages>149--160</pages>
<location>Nancy, France,</location>
<contexts>
<context position="7467" citStr="Nivre (2003)" startWordPosition="1189" endWordPosition="1190">graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for graphbased parser. Based on the results on development data, we choose the MaltParser for Catalan, Czech, German, and Spanish, and choose another MaxEntbased parser for Chinese, English, and Japanese. 2.4.1 A Transition-based Parser: MaltParser For Catalan, Czech, German, and Spanish, we use the MaltParser, a freely available implementa4http://www.cs.berkeley.edu/˜pliang/software/browncluster-1.2.zip 62 tion5, whose details are presented in the paper of Nivre (2003). More information about the parser can be available in the paper (Nivre, 2003). Due to computational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use</context>
</contexts>
<marker>Nivre, 2003</marker>
<rawString>Joakim Nivre. 2003. An efficient algorithm for projective dependency parsing. In Proceedings of the 8th International Workshop on Parsing Technologies (IWPT 03), pages 149–160, Nancy, France, April 23-25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Nianwen Xue</author>
</authors>
<title>Adding semantic roles to the Chinese Treebank.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<volume>15</volume>
<issue>1</issue>
<contexts>
<context position="1860" citStr="Palmer and Xue, 2009" startWordPosition="268" endWordPosition="271">ction This paper describes the system of National Institute of Information and Communications Technology (NICT) and City University of Hong Kong (CityU) for the joint learning task of CoNLL-2009 shared task (Hajiˇc et al., 2009)1. The system is basically a pipeline of syntactic parser and semantic parser. We use a syntactic parser that uses very rich features and integrates graph- and transition-based methods. As for the semantic parser, a group of well selected feature templates are used with n-best syntactic features. 1Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 The rest of the paper is organized as follows. The next section presents the technical details of our syntactic dependency parsing. Section 3 describes the details of the semantic dependency parsing. Section 4 shows the evaluation results. Section 5 looks into a few issues concerning our forthcoming work for this shared task, and Section 6 concludes the paper. 2 Syntactic Dependency Parsing Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implemen</context>
</contexts>
<marker>Palmer, Xue, 2009</marker>
<rawString>Martha Palmer and Nianwen Xue. 2009. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1):143–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
<author>Llufs M`arquez</author>
<author>Joakim Nivre</author>
</authors>
<title>The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies.</title>
<date>2008</date>
<booktitle>In Proceedings of the CoNLL2008.</booktitle>
<marker>Surdeanu, Johansson, Meyers, M`arquez, Nivre, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Llufs M`arquez, and Joakim Nivre. 2008. The CoNLL2008 shared task on joint parsing of syntactic and semantic dependencies. In Proceedings of the CoNLL2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taul´e</author>
<author>Maria Ant`onia Martf</author>
<author>Marta Recasens</author>
</authors>
<title>AnCora: Multilevel Annotated Corpora for Catalan and Spanish.</title>
<date>2008</date>
<booktitle>In Proceedings of the LREC2008,</booktitle>
<location>Marrakesh, Morroco.</location>
<marker>Taul´e, Martf, Recasens, 2008</marker>
<rawString>Mariona Taul´e, Maria Ant`onia Martf, and Marta Recasens. 2008. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. In Proceedings of the LREC2008, Marrakesh, Morroco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chunyu Kit</author>
</authors>
<title>Parsing syntactic and semantic dependencies with two single-stage maximum entropy models.</title>
<date>2008</date>
<booktitle>In Proceedings of CoNLL-2008,</booktitle>
<pages>203--207</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="8053" citStr="Zhao and Kit (2008)" startWordPosition="1281" endWordPosition="1284">nted in the paper of Nivre (2003). More information about the parser can be available in the paper (Nivre, 2003). Due to computational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use the similar feature notations of that work. We use the same greedy feature selection of Zhao et al. (2009) to determine an optimal feature template set for each language. Full feature sets for the three languages can be found at website, http://bcmi.sjtu.edu.cn/˜zhaohai. 2.4.3 Feature Representation For training data, we use 2-way jackknifing to generate predicted dependency parsing trees by two transition-based parsers. Following the features of Nivre and McDonald (2008), we define features for a head h and its dependent d with label l as shown in table 2, where GTran refers to</context>
<context position="16153" citStr="Zhao and Kit, 2008" startWordPosition="2544" endWordPosition="2547">wn for Czech, Chinese and English. We use 25%, 50% and 75% training corpus, respectively. The results in development sets are given in Table 8. Note that in this table the differences between normal and n-best feature template sets are also given for Chinese and English. The results in the table show that n-best features help improve Chinese semantic parsing as the training corpus is smaller, while it works for English as the training corpus is larger. 5 Discussion This work shows our further endeavor in syntactic and semantic dependency parsing, based on our previous work (Chen et al., 2008; Zhao and Kit, 2008). Chen et al. (Chen et al., 2008) and Koo et al. (Koo et al., 2008) used large-scale unlabeled data to improve syntactic dependency parsing performance. Here, we just performed their method on training data. From the results, we found that the new features provided better performance. In future work, we can try these methods on large-scale unlabeled data for other languages besides Chinese and English. In Zhao and Kit (2008), we addressed that semantic parsing should benefit from cross-validated training corpus and n-best syntactic output. These two issues have been implemented during this sha</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Hai Zhao and Chunyu Kit. 2008. Parsing syntactic and semantic dependencies with two single-stage maximum entropy models. In Proceedings of CoNLL-2008, pages 203–207, Manchester, UK, August 16-17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Wenliang Chen</author>
<author>Chunyu Kit</author>
<author>Guodong Zhou</author>
</authors>
<title>Multilingual dependency learning: A huge feature engineering method to semantic dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of CoNLL-2009,</booktitle>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="8174" citStr="Zhao et al. (2009)" startWordPosition="1304" endWordPosition="1307">omputational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use the similar feature notations of that work. We use the same greedy feature selection of Zhao et al. (2009) to determine an optimal feature template set for each language. Full feature sets for the three languages can be found at website, http://bcmi.sjtu.edu.cn/˜zhaohai. 2.4.3 Feature Representation For training data, we use 2-way jackknifing to generate predicted dependency parsing trees by two transition-based parsers. Following the features of Nivre and McDonald (2008), we define features for a head h and its dependent d with label l as shown in table 2, where GTran refers to dependency parsing trees generated by the MaltParser or MaxEnt-base Parser and * refers to any label. All features are c</context>
<context position="9874" citStr="Zhao et al. (2009)" startWordPosition="1576" endWordPosition="1579">tParser.html Normal n-best Matched Ca 53 54 50 Ch 75 65 55 En 73 70 63 Table 3: Feature template sets:n-best vs. non-n-best 2009). Namely, three languages, a single maximum entropy model is used for all identification and classification tasks of predicate senses or argument labels in four languages, Catalan, Czech, Japanese, or Spanish. For the rest three languages, an individual sense classifier still using maximum entropy is additionally used to output the predicate sense previously. More details about argument candidate pruning strategies and feature template set selection are described in Zhao et al. (2009). The same feature template sets as the semanticonly task are used for three languages, Czech, German and Japanese. For the rest four languages, we further use n-best syntactic features to strengthen semantic dependency parsing upon those automatically discovered feature template sets. However, we cannot obtain an obvious performance improvement in Spanish by using n-best syntactic features. Therefore, only Catalan, Chinese and English semantic parsing adopted these types of features at last. Our work about n-best syntactic features still starts from the feature template set that is originally</context>
<context position="12367" citStr="Zhao et al. (2009)" startWordPosition="1924" endWordPosition="1927">p:2|dpPathPred.lemma.bag a:2:p:2|dpPathPred.pos.bag a:2:p:2|dpTreeRelation a:2:p:2|dpPath.dprel.seq a:2:p:2|dpPathPred.dprel.seq a.lemma + a:2.dprel + a:2.h.lemma (a:2:p:2|dpTreeRelation) + p.pos Table 4: Features for n-best syntactic tree responding 2nd-best syntactic feature will be a : 2.children.dprel.bag. As all operations to generate the feature for a.children.dprel.bag is within the 1st-best syntactic tree, while those for a : 2.children.dprel.bag is within the 2nd-best one. As all these 2nd-best syntactic features are generated, we use the same greedy feature selection procedure as in Zhao et al. (2009) to determine the best fit feature template set according to the evaluation results in the development set. For Catalan, Chinese and English, three optimal n-best feature sets are obtained, respectively. Though dozens of n-best features are initially generated for selection, only few of them survive after the greedy selection. A feature number statistics is in Table 3, and those additionally selected n-best features for three languages are in Table 4. Full feature lists and their explanation for all languages will be available at the website, http://bcmi.sjtu.edu.cn/˜zhaohai. 4 Evaluation Resu</context>
</contexts>
<marker>Zhao, Chen, Kit, Zhou, 2009</marker>
<rawString>Hai Zhao, Wenliang Chen, Chunyu Kit, and Guodong Zhou. 2009. Multilingual dependency learning: A huge feature engineering method to semantic dependency parsing. In Proceedings of CoNLL-2009, Boulder, Colorado, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>