<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.894925">
An Agreement Measure for Determining Inter-Annotator Reliability of
Human Judgements on Affective Text
</title>
<author confidence="0.991815">
Plaban Kr. Bhowmick, Pabitra Mitra, Anupam Basu
</author>
<affiliation confidence="0.9995585">
Department of Computer Science and Engineering,
Indian Institute of Technology, Kharagpur, India – 721302
</affiliation>
<email confidence="0.996681">
{plaban,pabitra,anupam}@cse.iitkgp.ernet.in
</email>
<sectionHeader confidence="0.99474" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980941176471">
An affective text may be judged to be-
long to multiple affect categories as it may
evoke different affects with varying degree
of intensity. For affect classification of
text, it is often required to annotate text
corpus with affect categories. This task
is often performed by a number of hu-
man judges. This paper presents a new
agreement measure inspired by Kappa co-
efficient to compute inter-annotator relia-
bility when the annotators have freedom
to categorize a text into more than one
class. The extended reliability coefficient
has been applied to measure the quality of
an affective text corpus. An analysis of
the factors that influence corpus quality has
been provided.
</bodyText>
<sectionHeader confidence="0.998784" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962214285714">
The accuracy of a supervised machine learning
task primarily depends on the annotation quality of
the data, that is used for training and cross valida-
tion. Reliability of annotation is a key requirement
for the usability of an annotated corpus. Inconsis-
tency or noisy annotation may lead to the degrada-
tion of performances of supervised learning algo-
rithms. The data annotated by a single annotator
may be prone to error and hence an unreliable one.
This also holds for annotating an affective corpus,
which is highly dependent on the mental state of
the subject. The recent trend in corpus develop-
ment in NLP is to annotate corpus by more than
one annotators independently. In corpus statistics,
</bodyText>
<note confidence="0.723272">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.967213">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999622578947369">
the corpus reliability is measured by coefficient of
agreement. The coefficients of agreement are ap-
plied to corpus for various goals like measuring re-
liability, validity and stability of corpus (Artstein
and Poesio, 2008).
Jacob Cohen (Cohen, 1960) introduced Kappa
statistics as a coefficient of agreement for nom-
inal scales. The Kappa coefficient measures the
proportion of observed agreement over the agree-
ment by chance and the maximum agreement at-
tainable over chance agreement considering pair-
wise agreement. Later Fleiss (Fleiss, 1981) pro-
posed an extension to measure agreement in ordi-
nal scale data.
Cohen’s Kappa has been widely used in vari-
ous research areas. Because of its simplicity and
robustness, it has become a popular approach for
agreement measurement in the area of electron-
ics (Jung, 2003), geographical informatics (Hagen,
2003), medical (Hripcsak and Heitjan, 2002), and
many more domains.
There are other variants of Kappa like agree-
ment measures (Carletta, 1996). Scott’s 7r (Scott,
1955) was introduced to measure agreement in sur-
vey research. Kappa and 7r measures differ in the
way they determine the chance related agreements.
7r-like coefficients determine the chance agreement
among arbitrary coders, while r.-like coefficients
treats the chance of agreement among the coders
who produced the reliability data (Artstein and
Poesio, 2008).
One of the drawbacks of 7r and Kappa like coef-
ficients except Fleiss’ Kappa (Fleiss, 1981) is that
they treat all kinds of disagreements in the same
manner. Krippendorff’s α (Krippendorff, 1980) is
a reliability measure which treats different kind of
disagreements separately by introducing a notion
of distance between two categories. It offers a way
</bodyText>
<page confidence="0.984415">
58
</page>
<note confidence="0.849411">
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 58–65
Manchester, August 2008
</note>
<bodyText confidence="0.999754107142857">
to measure agreement in nominal, interval, ordinal
and ratio scale data.
Reliability assessment of corpus is an impor-
tant issue in corpus driven natural language pro-
cessing and the existing reliability measures have
been used in various corpus development tasks.
For example, Kappa coefficient has been used
in developing parts of speech corpus (Mieskes
and Strube, 2006), dialogue act tagging efforts
like MapTask (Carletta et al., 1997) and Switch-
board (Stolke et al., 1997), subjectivity tagging
task (Bruce and Wiebe, 1999) and many more.
The 7r and r. coefficients measure the reliabil-
ity of the annotation task where a data item can
be annotated with one category. (Rosenberg and
Binkowski, 2004) puts an effort towards measur-
ing corpus reliability for multiply labeled data
points. In this measure, the annotators are allowed
to mark one data point with at most two classes,
one of which is primary and other is secondary.
This measure was used to determine the reliability
of a email corpus where emails are assigned with
primary and secondary labels from a set of email
types.
Affect recognition from text is a recent and
promising subarea of natural language process-
ing. The task is to classify text segments into ap-
propriate affect categories. The supervised ma-
chine learning techniques, which requires a reli-
able annotated corpus, may be applied for solv-
ing the problem. In general, a blend of emotions
is common in both verbal and non-verbal com-
munication. Unlike conventional annotation tasks
like POS corpus development, where one data item
may belong to only one category, in affective text
corpus, a data item may be fuzzy and may belong
to multiple affect categories. For example, the fol-
lowing sentence may belong to disgust and sad
category since it may evoke both the emotions to
different degrees of intensity.
A young married woman was burnt to
death allegedly by her in-laws for dowry.
This property makes the existing agreement mea-
sures inapplicable for determining agreement in
emotional corpus. Craggs and Wood (2004)
adopted a categorical scheme for annotating emo-
tion in affective text dialogue. They claimed to ad-
dress the problem of agreement measurement for
the data set where one data item may belong to
more than one category using an extension of Krip-
pendorff’s α. But the details of the extension is yet
to be disseminated.
In this paper, we propose a new agreement mea-
sure for multiclass annotation which we denote by
Am. The new measure is then applied to an affec-
tive text corpus to
</bodyText>
<listItem confidence="0.9823195">
• Assess Reliability: To test whether the corpus
can be used for developing computational af-
fect recognizer.
• Determine Gold Standard: To define a gold
standard that will be used to test the accuracy
of the affect recognizer.
</listItem>
<bodyText confidence="0.999619125">
In section 2, we describe the affective text cor-
pus and the annotation scheme. In section 3, we
propose a new reliability measure (Am) for mul-
ticlass annotated data. In section 4, we provide
an algorithm to determine gold standard data from
the annotation and in section 5, we discuss about
applying Am measure to the corpus developed by
us and some observations related to the annotation.
</bodyText>
<sectionHeader confidence="0.8029225" genericHeader="method">
2 Affective Text Corpus and Annotation
Scheme
</sectionHeader>
<bodyText confidence="0.997008375">
The affective text corpus collected by us consists
of 1000 sentences extracted from Times of India
news archive1. The sentences were collected from
headlines as well as articles belonging to political,
social, sports and entertainment domain.
Selection of affect categories is a very crucial
and important decision problem due to the follow-
ing reasons.
</bodyText>
<listItem confidence="0.9963308">
• The affect categories should be applicable to
the considered genre.
• The affect categories should be identifiable
from language.
• The categories should be unambiguous.
</listItem>
<bodyText confidence="0.999772777777778">
We shall try to validate these points based on the
results obtained, after applying the our extended
measure on the text corpus with respect to a set of
selected basic emotional categories.
Basic emotions are those for which the respec-
tive expressions across culture, ethnicity, age, sex,
social structure are invariant (Ortony and Turner,
1990). But unfortunately, there is a long per-
sistent debate among the psychologists regarding
</bodyText>
<footnote confidence="0.988796">
1http://timesofindia.indiatimes.com/archive.cms
</footnote>
<page confidence="0.999401">
59
</page>
<bodyText confidence="0.999567210526316">
the number of basic emotional categories (Ortony
and Turner, 1990). One of the theories behind
the basic emotions is that they are biologically
primitive because they possess evolutionary signif-
icance related to the basic needs for the survival of
the species (Plutchik, 1980). The universality of
recognition of emotions from distinctive facial ex-
pressions is an indirect technique to establish the
basic emotions (Darwin, 1965).
Six basic affect categories (Ekman, Friesen and
Ellsworth, 1982) have been considered in emotion
recognition from speech (Song et al., 2004), fa-
cial expression (Pantic and Rothkrantz, 2000). Our
annotation scheme considers six basic emotions,
namely, Anger, Disgust, Fear, Happiness, Sadness,
Surprise as specified by Ekman for affect recogni-
tion in text corpus.
The annotation scheme considers the following
points:
</bodyText>
<listItem confidence="0.91000425">
• Two types are sentences are collected for an-
notation.
– Direct Affective Sentence: Here, the
agent present in the sentence is experi-
</listItem>
<bodyText confidence="0.887597444444444">
encing a set of emotions, which are ex-
plicit in the sentence. For example, in
the following sentence Indian support-
ers are the agents experiencing a disgust
emotion.
Indian supporters are disgusted
about players’ performances in
the World Cup.
– Indirect Affective Sentence: Here, the
reader of the sentence is experiencing a
set of emotions. In the following sen-
tence, the reader is experiencing a dis-
gust emotion because the event of ac-
cepting bribe, is an indecent act carried
out by responsible agents like Top offi-
cials.
Top officials are held for accept-
ing bribe from a poor villager.
</bodyText>
<listItem confidence="0.995550375">
• A sentence may trigger multiple emotions si-
multaneously. So, one annotator may classify
a sentence to more than one affective cate-
gories.
• For each emotion, the keywords that trigger
the particular emotion are marked.
• For each emotion, the events or objects that
trigger the concerned emotion are marked.
</listItem>
<bodyText confidence="0.99990125">
Here, we aim at measuring the agreement in an-
notation. The focus is to measure the agreement
in annotation pattern rather than the agreement in
individual emotional classes.
</bodyText>
<sectionHeader confidence="0.976893" genericHeader="method">
3 Proposed Agreement Measure
</sectionHeader>
<bodyText confidence="0.9999916">
To overcome the shortcomings of existing relia-
bility measures mentioned earlier, we propose Am
measure, which is an agreement measure for cor-
pus annotation task considering multiclass classifi-
cation. We present the notion of agreement below.
</bodyText>
<subsectionHeader confidence="0.999892">
3.1 Notion of Paired Agreement
</subsectionHeader>
<bodyText confidence="0.998814307692308">
In order to allow for multiple labels, we calculate
agreement between all the pairs of possible labels.
Let C1 and C2 be two affect categories, e.g., anger
and disgust. Let &lt;C1, C2&gt; denote the category
pair. An annotator’s assignment of labels can be
represented as apair of binary choices for each cat-
egory pair &lt;C1, C2&gt;, namely, &lt; 0, 0 &gt;, &lt; 0,1 &gt;,
&lt; 1, 0 &gt;, and &lt; 1,1 &gt;. It should be noted that the
proposed metric considers the non-inclusion in a
category by an annotator pair as an agreement.
For an item, two annotators U1 and U2 are said
to agree on &lt;C1, C2&gt; if the following conditions
hold.
</bodyText>
<equation confidence="0.9996105">
U1.C1 = U2.C1
U1.C2 = U2.C2
</equation>
<bodyText confidence="0.999881428571429">
where Ui.Cj signifies that the value for Cj for an-
notator Ui and the value may either be 1 or 0. For
example, if one coder marks an item with anger
and another with disgust, they would disagree on
the pairs that include these labels, but still agree
that the item does not express happiness and sad-
ness.
</bodyText>
<subsectionHeader confidence="0.999001">
3.2 Am Agreement Measure
</subsectionHeader>
<bodyText confidence="0.9999365">
With the notion of paired agreement discussed ear-
lier, the observed agreement(Po) is the proportion
of items the annotators agreed on the category
pairs and the expected agreement(Pe) is the pro-
portion of items for which agreement is expected
by chance when the items are randomly. Follow-
ing the line of Cohen’s Kappa (Cohen, 1960), Am
is defined as the proportion of agreement after ex-
pected or chance agreement is removed from con-
sideration and is given by
</bodyText>
<equation confidence="0.937443">
Po − Pe
Am = (1) 1 − Pe
</equation>
<page confidence="0.926854">
60
</page>
<bodyText confidence="0.999041333333333">
When Po equals Pe, Am value is computed to
be 0, which signifies no non-random agreement
among the annotators. An Am value of 1, the
upper limit of Am, indicates a perfect agreement
among the annotators. We define Po and Pe as
follows.
</bodyText>
<subsectionHeader confidence="0.758622">
Observed Agreement (Po):
</subsectionHeader>
<bodyText confidence="0.9967557">
Let I be the number of items, C is the number of
categories and U is the number of annotators and
S be the set of all category pairs with cardinality
. The total agreement on a category pair p
for an item i is nip, the number of annotator pairs
who agree on p for i.
The average agreement on a category pair p for
an item i is nip divided by the total number of an-
notator pairs and is given by
The average agreement for the item i is the mean
</bodyText>
<equation confidence="0.961547375">
Pi = Cl 1 (U
nip
X
(3 )
µ2/ \2¶ pES
The observed agreement is the average agreement
given by
G = {[0 0], [0 1], [1 1]}.
</equation>
<bodyText confidence="0.9963895">
It is to be noted that the combinations [0 1] and [1
0] are clubbed to one element as they are symmet-
ric to each other. Let Pˆ(pg|u) be the overall pro-
portion of items assigned with assignment combi-
nation g E G to category pair p E S by annotator
u and npgu be the total number of assignments of
items by annotator u with assignment combination
g to category pair p. Then Pˆ(pg|u) is given by
=
For an item, the probability that two arbitrary
coders agree with the same assignment combina-
tion in a category pair is the joint probability of
individual coders making this assignments inde-
pendently. For two annotators
</bodyText>
<equation confidence="0.967178666666667">
Pˆ(pg|u)
npgu (5)
I
</equation>
<bodyText confidence="0.620110666666667">
ux and uy the joint
Pˆ(pg|uy). The
d is given by
</bodyText>
<equation confidence="0.982585707692307">
P
(p
) = 1
U
P
(pg
u
) P
(p
2
,
y
ˆ
g
X
ˆ
|
x
ˆ
g|
µ
¶
ux
u
)EW
µ 2
C¶
1
µU ¶nip (2)
Pip =
2
probability is given by
Pˆ(pg|ux)
on a category
pair for all assignment combinations
is given by
Pˆ(p) = X Pˆ(pg) (7 )
pgEG
pairs.
d is given by
1
Po = I
1
I µ2¶ µ2¶
Pi
XI
i=1
(4)
X
pES
XI
i=1
XI
i=1
X
pES
nip
1
Pe =
CXPˆ(p)
µ2¶ pES
(8)
4
IC(C − 1)U(U − 1)
The
</equation>
<bodyText confidence="0.936556833333333">
measure may be calculated based on the
expressions of Po and Pe as given in Equation 4
an
Am
d Equation 8 to compute the reliability of anno-
tation with respect to multiclass annotation.
</bodyText>
<sectionHeader confidence="0.993886" genericHeader="method">
4 Gold Standard Determination
</sectionHeader>
<bodyText confidence="0.964590842105263">
Gold standard data is used as a reference data set
for various goals like
a category pair. For a category
pair, four possible
assignment combinations constitute a set which is
of Pip over all category pairs and is given by
over all the item an
nip
Expected Agreement (Pe):
The expected agreement is defined as the agree-
ment among the annotators when they assign the
items to a set of categories randomly. However,
since we are considering the agreement on cate-
gory pairs, we consider the expected agreement
to be the expectation that the annotators agree on
probability that two arbitrary annotators agree on
a category pair p with assignment combination g
is the average over all annotator pairs belonging to
W, the set of annotator pairs an
</bodyText>
<equation confidence="0.8569645">
uy)
(6)
</equation>
<bodyText confidence="0.989697333333333">
The probability that two arbitrary annotators agree
The chance agreement is calculated by taking
average over all category
</bodyText>
<listItem confidence="0.980281">
• Building reliable classifier
</listItem>
<page confidence="0.938962">
61
</page>
<listItem confidence="0.972948">
• Determine the performance of a classifier
</listItem>
<bodyText confidence="0.987969">
To attach a set of labels to a data item in the gold
standard data, we assign the majority decision
label to an item. Let no be the number of annota-
tors, who have assigned an item i into category C
and no annotators have decided not to assign the
same item into that category. Then i is assigned to
C if no &gt; no; otherwise it is not assigned to that
category.
</bodyText>
<table confidence="0.378172">
Algorithm 1: Algorithm for determining gold
standard data
Input: Set of I items annotated into C
categories by U annotators
Output: Gold standard data
</table>
<equation confidence="0.866877333333333">
foreach annotator u E U do
�u &lt;-- 0;
end
foreach item i E I do
foreach category c E C do
O = set of annotators who have
assigned i in category c;
0 = set of annotators who have not
assigned i in category c;
if cardinality(0)&gt;cardinality(0) then
assign label c to i;
�j &lt;-- �j + 1 where j E O;
end
else if cardinality(0)&lt;cardinality(0)
then
do not assign label c to i;
�j &lt;-- �j + 1 where j E 0;
end
</equation>
<bodyText confidence="0.5440235">
else if �� &gt; �� then
assign label c to i;
</bodyText>
<listItem confidence="0.63835">
end
end
end
</listItem>
<bodyText confidence="0.9967735">
If no = no, then we resolve the tie based on the
performances of the annotators in previous assign-
ments. We assign an expert coder index(ξ) to each
annotator and it is updated based on the agreement
of their judgments over the corpus. There are two
cases when the � values are incremented
</bodyText>
<listItem confidence="0.924921666666667">
• If the item is assigned to a category in the gold
standard data, the � values are incremented
for those annotators who have assigned the
item into that category.
• If the item is not assigned to a category in
the gold standard data, the � values are in-
</listItem>
<bodyText confidence="0.999469916666667">
cremented for those annotators who have not
assigned the item into that category.
If no and no are equal for an item, we make use
of the � values for deciding upon the assignment of
the item to the category in concern. We assign the
item into that category if the combined � values of
the annotators who have assigned the item into that
category is greater than the combined � values of
the annotators who have not assigned the item into
that category, i.e.,
The algorithm for determining gold standard
data is given in Algorithm 1.
</bodyText>
<sectionHeader confidence="0.992319" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.9999585">
We applied the proposed An measure to estimate
the quality of the affective corpus described in sec-
tion 2. Below we present the annotation experi-
ment followed by some relevant analysis.
</bodyText>
<subsectionHeader confidence="0.998352">
5.1 Annotation Experiment
</subsectionHeader>
<bodyText confidence="0.999972866666667">
Ten human judges with the same social back-
ground participated in the study, assigning affec-
tive categories to sentences independently of one
another. The annotators were provided with the
annotation instructions and they were trained with
some sentences not belonging to the corpus. The
annotation was performed with the help of a web
based annotation interface2. The corpus consists
of 1000 sentences. Three of judges were able to
complete the task within 20 days. In this paper,
we report the result of applying the measure with
data provided by three annotators without consid-
ering the incomplete annotations. Distribution of
the sentences across the affective categories for the
three judges is given in Figure 1.
</bodyText>
<subsectionHeader confidence="0.999969">
5.2 Analysis of Corpus Quality
</subsectionHeader>
<bodyText confidence="0.990708">
The corpus was evaluated in terms of the proposed
measure. Some of the relevant observations are
presented below.
</bodyText>
<listItem confidence="0.9913745">
• Agreement Value: Different agreement val-
ues related to An measure are given in Ta-
ble 1. We present An values for all the anno-
tator pairs in Table 2.
</listItem>
<footnote confidence="0.69276">
2http://www.mla.iitkgp.ernet.in/Annotation/index.php
</footnote>
<figure confidence="0.862072666666667">
Sj
Si &gt;
�nO
i=1
no
j=1
</figure>
<page confidence="0.860903">
62
</page>
<figureCaption confidence="0.9934725">
Figure 1: Distribution of sentences for three
judges.
</figureCaption>
<table confidence="0.99570925">
Agreement A,,t Value
Observed Agreement(Po) 0.878
Chance Agreement(Pe) 0.534
A,,t 0.738
</table>
<tableCaption confidence="0.988898">
Table 1: Agreement values for the affective text
corpus.
</tableCaption>
<table confidence="0.999736">
Annotator Pair Po Pe A,,t Value
1-2 0.858 0.526 0.702
1-3 0.868 0.54 0.713
2-3 0.884 0.531 0.752
</table>
<tableCaption confidence="0.993219">
Table 2: Annotator pairwise A,,t values.
</tableCaption>
<listItem confidence="0.999218333333333">
• Agreement Study: Table 3 provides the dis-
tribution of the sentences against individual
observed agreement values. It is observed
</listItem>
<figure confidence="0.8676478">
Observed Agreement No. of Sentences
0.0 &lt; A0 &lt; 0.2 14
0.2 &lt; A0 &lt; 0.4 73
0.4 &lt; A0 &lt; 0.7 198
0.7 &lt; A0 &lt; 1.0 715
</figure>
<tableCaption confidence="0.7532685">
Table 3: Distribution of the sentences over ob-
served agreement.
</tableCaption>
<bodyText confidence="0.9883929">
that 71.5% of the corpus belongs to [0.7 1.0]
range of observed agreement and among this
bulk portion of the corpus, the annotators as-
sign 78.6% of the sentences into a single cat-
egory. This is due to the existence of a domi-
nant emotion in a sentence and in most of the
cases, the sentence contains enough clues to
decode it. For the non-dominant emotions in
a sentence, ambiguity has been found while
decoding.
</bodyText>
<listItem confidence="0.946077">
• Disagreement Study: In Table 4, we present
</listItem>
<bodyText confidence="0.871249692307692">
the category wise disagreement for all the an-
notator pairs. From the disagreement table it
is evident that the categories with maximum
number of disagreements are anger, disgust
and fear. The emotions which are close to
each other in the evaluation-activation space
are inherently ambiguous. For example,
anger and disgust are close to each other in
the evaluation-activation space. So, ambigu-
ity between these categories will be higher
compared to other pairs. If [a b] is the pair, we
count the number of cases where one annota-
tor categorized one item into [a -] pattern and
other annotator classified the same item into
[- b] pattern. In Table 5, we provide the con-
fusion between two affective categories for all
annotator pairs. This confusion matrix is a
symmetric one. So, we have provided only
the upper triangular matrix.
In Figure 2, we provide ambiguity counts of
the affective category pairs. It can be ob-
Figure 2: Category pair wise disagreement
(A=Anger, D=Disgust, F=Fear, H=Happiness,
S=Sadness and Su=Surprise).
served that anger, disgust and fear are asso-
ciated with three topmost ambiguous pairs.
</bodyText>
<subsectionHeader confidence="0.994114">
5.3 Gold Standard for Affective Text Corpus
</subsectionHeader>
<bodyText confidence="0.999967">
To determine the gold standard corpus, we have
applied majority decision label based approach
discussed in section 4 on the judgements provided
by only three annotators. However, as the num-
ber of annotators is much less in the current study,
the determined gold standard corpus may not have
</bodyText>
<page confidence="0.997428">
63
</page>
<table confidence="0.9993038">
Anger Disgust Fear Happiness Sadness Surprise
1-2 68 94 74 64 74 45
1-3 74 86 105 57 54 45
2-3 65 49 58 22 50 20
Total 207 229 273 143 178 110
</table>
<tableCaption confidence="0.988355">
Table 4: Categorywise disagreement for the annotator pairs.
</tableCaption>
<table confidence="0.999765428571428">
Anger Disgust Fear Happiness Sadness Surprise
Anger - 39 28 11 22 7
Disgust - - 28 6 24 13
Fear - - - 2 24 12
Happiness - - - - 18 8
Sadness - - - - - 9
Surprise - - - - - -
</table>
<tableCaption confidence="0.999865">
Table 5: Confusion matrix for category pairs.
</tableCaption>
<bodyText confidence="0.957662">
much significance. Here, we report the result
of applying the gold standard determination algo-
rithm on the data provided by three annotators.
The distribution of sentences over the affective cat-
egories is depicted in Figure 3.
</bodyText>
<figureCaption confidence="0.995594">
Figure 3: Distribution of sentences in gold stan-
dard corpus.
</figureCaption>
<sectionHeader confidence="0.993935" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999976962962963">
Measuring the reliability of the affective text cor-
pus where one single item may be classified into
more than one single category is a complex task.
In this paper, we have provided a new coefficient
to measure reliability in multiclass annotation task
by incorporating pairwise agreement in affective
class pairs. The measure yields an agreement value
0.72, when applied to an annotated corpus pro-
vided by three users. This considerable agreement
value indicates that the affect categories consid-
ered for annotation may be applicable to the news
genre.
We are in process of collecting annotated corpus
from more annotators which will ensure a statisti-
cally significant result. According to the disagree-
ment study presented in section 5.2, confusions
between specific emotions is most likely between
categories which are adjacent in the activation-
evaluation space. The models of annotator agree-
ment which use weights for different types of dis-
agreement will be interesting for future study. The
direct and indirect affective sentences have not
been treated separately in this study. The algo-
rithm for determination of gold standard requires
more details investigation as simple majority vot-
ing may not be sufficient for highly subjective data
like emotion.
</bodyText>
<sectionHeader confidence="0.964961" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99554">
Plaban Kr. Bhowmick is partially supported by
Microsoft Corporation, USA and Media Lab Asia,
India. The authors are thankful to the reviewers for
their detailed suggestions regarding the work.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99757025">
Artstein, Ron and Massimo Poesio. 2008. Inter-coder
Agreement for Computational Linguistics. Compu-
tational Linguistics.
Bruce, Rebecca F. and Janyce M. Wiebe 1999. Rec-
</reference>
<page confidence="0.986437">
64
</page>
<reference confidence="0.999838546666667">
ognizing Subjectivity: A Case Study of Manual Tag-
ging. Natural Language Engineering. 1(1):1-16.
Carletta, Jean. 1996. Assessing Agreement on Classi-
fication Tasks: The Kappa Statistic. Computational
Linguistics. 22(21):249-254.
Carletta, Jean, Isard .A, Isard S., Jacqueline C. Kowtko,
Gwyneth D. Sneddon, and Anne H. Anderson. 1997.
The Reliability of a Dialogue Structure Coding
Scheme. Computational Linguistics. 23(1):13-32.
Cohen, Jacob. 1960. A Coefficient of Agreement
for Nominal Scales. Educational and Psychological
Measurement. 20(1):37-46.
Craggs Richard and Mary M. Wood. 2004. A Categori-
cal Annotation Scheme for Emotion in the Linguistic
Content of Dialogue. Tutorial and Research Work-
shop, Affective Dialogue Systems. Kloster Irsee, 89-
100.
Darwin, Charles. 1965. The Expression of Emotions in
Man and Animals.. Chicago: University of Chicago
Press. (Original work published 1872)
Ekman, Paul., Friesen W. V., and Ellsworth P. 1982.
What Emotion Categories or Dimensions can Ob-
servers Judge from Facial Behavior? Emotion in
the human face, Cambridge University Press. pages
39-55, New York.
Fleiss, Joseph L. 1981. Statistical Methods for Rates
and Proportions. Wiley. second ed., New York.
Hagen-Zanker, Alex. 2003. Fuzzy Set Approach to
Assessing Similarity of Categorical Maps. Interna-
tional Journal for Geographical Information Science.
17(3):235-249.
Hripcsak, George and Daniel F. Heitjan. 2002. Mea-
suring Agreement in Medical Informatics Reliabil-
ity Studies. Journal of Biomedical Informatics.
35(2):99-110.
Jung, Ho-Won. 2003. Evaluating Interrater Agreement
in SPICE-based Assessments. Computer Standards
&amp; Interfaces. 25(5):477-499.
Krippendorff, Klaus 1980. Content Analysis: An Intro-
duction to its Methodology. Sage Publications. Bev-
erley Hills, CA.
Mieskes, Margot and Michael Strube. 2006. Part-of-
Speech Tagging of Transcribed Speech. Proceedings
of International Conference on Language Resources
and Evaluation. GENOA
Ortony, Andrew and Terence J. Turner. 1990. What’s
Basic About Basic Emotions?. Psychological Re-
view. 97(3):315-331.
Pantic, Maja and Leon Rothkrantz. 2000. Automatic
Analysis ofFacial Expressions: The State of the Art.
IEEE Transactions on Pattern Analysis and Machine
Intelligence. 22(12):1424-1445.
Plutchik, Robert 1980. A General Psychoevolutionary
Theory of Emotion. Emotion: Theory, research, and
experience: Vol. 1. Theories of emotion. Academic
Press, New York, 3-33.
Rosenberg, Andrew, and Ed Binkowski. 2004. Aug-
menting the Kappa Statistic to Determine Interanno-
tator Reliability for Multiply Labeled Data Points.
In Proceedings of North American Chapter of the
Association for Computational Linguistics. Boston,
77-80.
Scott, William A. 1955. Reliability of Content Anal-
ysis: The Case of Nominal Scale Coding. Public
Opinion Quarterly. 19(3):321-325.
Song, Mingli, Chun Chen, Jiajun Bu, and Mingyu You.
2004. Speech Emotion Recognition and Intensity Es-
timation. Internation Conference on Computational
Science and its Applications. Perugia, 406-413.
Stolcke A., Ries K., Coccaro N., Shriberg E., Bates R.,
Jurafsky .D, Taylor P., Martin C. Van-Ess-Dykema,
and Meteer .M. 1997. Dialogue Act Modeling
for Automatic Tagging and Recognition of Con-
versational Speech. Computational Linguistics.
26(3):339-371.
</reference>
<page confidence="0.999614">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.915427">
<title confidence="0.97042">An Agreement Measure for Determining Inter-Annotator Reliability Human Judgements on Affective Text</title>
<author confidence="0.997246">Pabitra Mitra Bhowmick</author>
<author confidence="0.997246">Anupam</author>
<affiliation confidence="0.994958">Department of Computer Science and Indian Institute of Technology, Kharagpur, India –</affiliation>
<abstract confidence="0.999153333333333">An affective text may be judged to belong to multiple affect categories as it may evoke different affects with varying degree of intensity. For affect classification of text, it is often required to annotate text corpus with affect categories. This task is often performed by a number of human judges. This paper presents a new agreement measure inspired by Kappa coefficient to compute inter-annotator reliability when the annotators have freedom to categorize a text into more than one class. The extended reliability coefficient has been applied to measure the quality of an affective text corpus. An analysis of the factors that influence corpus quality has been provided.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder Agreement for Computational Linguistics. Computational Linguistics.</title>
<date>2008</date>
<contexts>
<context position="2099" citStr="Artstein and Poesio, 2008" startWordPosition="312" endWordPosition="315">nnotating an affective corpus, which is highly dependent on the mental state of the subject. The recent trend in corpus development in NLP is to annotate corpus by more than one annotators independently. In corpus statistics, © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. the corpus reliability is measured by coefficient of agreement. The coefficients of agreement are applied to corpus for various goals like measuring reliability, validity and stability of corpus (Artstein and Poesio, 2008). Jacob Cohen (Cohen, 1960) introduced Kappa statistics as a coefficient of agreement for nominal scales. The Kappa coefficient measures the proportion of observed agreement over the agreement by chance and the maximum agreement attainable over chance agreement considering pairwise agreement. Later Fleiss (Fleiss, 1981) proposed an extension to measure agreement in ordinal scale data. Cohen’s Kappa has been widely used in various research areas. Because of its simplicity and robustness, it has become a popular approach for agreement measurement in the area of electronics (Jung, 2003), geograph</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Artstein, Ron and Massimo Poesio. 2008. Inter-coder Agreement for Computational Linguistics. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca F Bruce</author>
<author>Janyce M Wiebe</author>
</authors>
<title>Recognizing Subjectivity: A Case Study of Manual Tagging.</title>
<date>1999</date>
<journal>Natural Language Engineering.</journal>
<pages>1--1</pages>
<contexts>
<context position="4258" citStr="Bruce and Wiebe, 1999" startWordPosition="642" endWordPosition="645">workshop on Human Judgements in Computational Linguistics, pages 58–65 Manchester, August 2008 to measure agreement in nominal, interval, ordinal and ratio scale data. Reliability assessment of corpus is an important issue in corpus driven natural language processing and the existing reliability measures have been used in various corpus development tasks. For example, Kappa coefficient has been used in developing parts of speech corpus (Mieskes and Strube, 2006), dialogue act tagging efforts like MapTask (Carletta et al., 1997) and Switchboard (Stolke et al., 1997), subjectivity tagging task (Bruce and Wiebe, 1999) and many more. The 7r and r. coefficients measure the reliability of the annotation task where a data item can be annotated with one category. (Rosenberg and Binkowski, 2004) puts an effort towards measuring corpus reliability for multiply labeled data points. In this measure, the annotators are allowed to mark one data point with at most two classes, one of which is primary and other is secondary. This measure was used to determine the reliability of a email corpus where emails are assigned with primary and secondary labels from a set of email types. Affect recognition from text is a recent </context>
</contexts>
<marker>Bruce, Wiebe, 1999</marker>
<rawString>Bruce, Rebecca F. and Janyce M. Wiebe 1999. Recognizing Subjectivity: A Case Study of Manual Tagging. Natural Language Engineering. 1(1):1-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing Agreement on Classification Tasks: The Kappa Statistic. Computational Linguistics.</title>
<date>1996</date>
<pages>22--21</pages>
<contexts>
<context position="2866" citStr="Carletta, 1996" startWordPosition="433" endWordPosition="434">of observed agreement over the agreement by chance and the maximum agreement attainable over chance agreement considering pairwise agreement. Later Fleiss (Fleiss, 1981) proposed an extension to measure agreement in ordinal scale data. Cohen’s Kappa has been widely used in various research areas. Because of its simplicity and robustness, it has become a popular approach for agreement measurement in the area of electronics (Jung, 2003), geographical informatics (Hagen, 2003), medical (Hripcsak and Heitjan, 2002), and many more domains. There are other variants of Kappa like agreement measures (Carletta, 1996). Scott’s 7r (Scott, 1955) was introduced to measure agreement in survey research. Kappa and 7r measures differ in the way they determine the chance related agreements. 7r-like coefficients determine the chance agreement among arbitrary coders, while r.-like coefficients treats the chance of agreement among the coders who produced the reliability data (Artstein and Poesio, 2008). One of the drawbacks of 7r and Kappa like coefficients except Fleiss’ Kappa (Fleiss, 1981) is that they treat all kinds of disagreements in the same manner. Krippendorff’s α (Krippendorff, 1980) is a reliability measu</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Carletta, Jean. 1996. Assessing Agreement on Classification Tasks: The Kappa Statistic. Computational Linguistics. 22(21):249-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
<author>A Isard</author>
<author>S Isard</author>
<author>Jacqueline C Kowtko</author>
<author>Gwyneth D Sneddon</author>
<author>Anne H Anderson</author>
</authors>
<title>The Reliability of a Dialogue Structure Coding Scheme. Computational Linguistics.</title>
<date>1997</date>
<pages>23--1</pages>
<contexts>
<context position="4169" citStr="Carletta et al., 1997" startWordPosition="628" endWordPosition="631">n of distance between two categories. It offers a way 58 Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 58–65 Manchester, August 2008 to measure agreement in nominal, interval, ordinal and ratio scale data. Reliability assessment of corpus is an important issue in corpus driven natural language processing and the existing reliability measures have been used in various corpus development tasks. For example, Kappa coefficient has been used in developing parts of speech corpus (Mieskes and Strube, 2006), dialogue act tagging efforts like MapTask (Carletta et al., 1997) and Switchboard (Stolke et al., 1997), subjectivity tagging task (Bruce and Wiebe, 1999) and many more. The 7r and r. coefficients measure the reliability of the annotation task where a data item can be annotated with one category. (Rosenberg and Binkowski, 2004) puts an effort towards measuring corpus reliability for multiply labeled data points. In this measure, the annotators are allowed to mark one data point with at most two classes, one of which is primary and other is secondary. This measure was used to determine the reliability of a email corpus where emails are assigned with primary </context>
</contexts>
<marker>Carletta, Isard, Isard, Kowtko, Sneddon, Anderson, 1997</marker>
<rawString>Carletta, Jean, Isard .A, Isard S., Jacqueline C. Kowtko, Gwyneth D. Sneddon, and Anne H. Anderson. 1997. The Reliability of a Dialogue Structure Coding Scheme. Computational Linguistics. 23(1):13-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement.</title>
<date>1960</date>
<pages>20--1</pages>
<contexts>
<context position="2126" citStr="Cohen, 1960" startWordPosition="318" endWordPosition="319">ighly dependent on the mental state of the subject. The recent trend in corpus development in NLP is to annotate corpus by more than one annotators independently. In corpus statistics, © 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. the corpus reliability is measured by coefficient of agreement. The coefficients of agreement are applied to corpus for various goals like measuring reliability, validity and stability of corpus (Artstein and Poesio, 2008). Jacob Cohen (Cohen, 1960) introduced Kappa statistics as a coefficient of agreement for nominal scales. The Kappa coefficient measures the proportion of observed agreement over the agreement by chance and the maximum agreement attainable over chance agreement considering pairwise agreement. Later Fleiss (Fleiss, 1981) proposed an extension to measure agreement in ordinal scale data. Cohen’s Kappa has been widely used in various research areas. Because of its simplicity and robustness, it has become a popular approach for agreement measurement in the area of electronics (Jung, 2003), geographical informatics (Hagen, 20</context>
<context position="11572" citStr="Cohen, 1960" startWordPosition="1852" endWordPosition="1853">annotator Ui and the value may either be 1 or 0. For example, if one coder marks an item with anger and another with disgust, they would disagree on the pairs that include these labels, but still agree that the item does not express happiness and sadness. 3.2 Am Agreement Measure With the notion of paired agreement discussed earlier, the observed agreement(Po) is the proportion of items the annotators agreed on the category pairs and the expected agreement(Pe) is the proportion of items for which agreement is expected by chance when the items are randomly. Following the line of Cohen’s Kappa (Cohen, 1960), Am is defined as the proportion of agreement after expected or chance agreement is removed from consideration and is given by Po − Pe Am = (1) 1 − Pe 60 When Po equals Pe, Am value is computed to be 0, which signifies no non-random agreement among the annotators. An Am value of 1, the upper limit of Am, indicates a perfect agreement among the annotators. We define Po and Pe as follows. Observed Agreement (Po): Let I be the number of items, C is the number of categories and U is the number of annotators and S be the set of all category pairs with cardinality . The total agreement on a categor</context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen, Jacob. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement. 20(1):37-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craggs Richard</author>
<author>Mary M Wood</author>
</authors>
<title>A Categorical Annotation Scheme for Emotion in the Linguistic Content of Dialogue. Tutorial and Research Workshop, Affective Dialogue Systems. Kloster Irsee,</title>
<date>2004</date>
<pages>89--100</pages>
<marker>Richard, Wood, 2004</marker>
<rawString>Craggs Richard and Mary M. Wood. 2004. A Categorical Annotation Scheme for Emotion in the Linguistic Content of Dialogue. Tutorial and Research Workshop, Affective Dialogue Systems. Kloster Irsee, 89-100.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Darwin</author>
</authors>
<title>The Expression of Emotions in Man and Animals..</title>
<date>1965</date>
<institution>University of Chicago Press.</institution>
<location>Chicago:</location>
<note>(Original work published 1872)</note>
<contexts>
<context position="8355" citStr="Darwin, 1965" startWordPosition="1309" endWordPosition="1310">ture are invariant (Ortony and Turner, 1990). But unfortunately, there is a long persistent debate among the psychologists regarding 1http://timesofindia.indiatimes.com/archive.cms 59 the number of basic emotional categories (Ortony and Turner, 1990). One of the theories behind the basic emotions is that they are biologically primitive because they possess evolutionary significance related to the basic needs for the survival of the species (Plutchik, 1980). The universality of recognition of emotions from distinctive facial expressions is an indirect technique to establish the basic emotions (Darwin, 1965). Six basic affect categories (Ekman, Friesen and Ellsworth, 1982) have been considered in emotion recognition from speech (Song et al., 2004), facial expression (Pantic and Rothkrantz, 2000). Our annotation scheme considers six basic emotions, namely, Anger, Disgust, Fear, Happiness, Sadness, Surprise as specified by Ekman for affect recognition in text corpus. The annotation scheme considers the following points: • Two types are sentences are collected for annotation. – Direct Affective Sentence: Here, the agent present in the sentence is experiencing a set of emotions, which are explicit in</context>
</contexts>
<marker>Darwin, 1965</marker>
<rawString>Darwin, Charles. 1965. The Expression of Emotions in Man and Animals.. Chicago: University of Chicago Press. (Original work published 1872)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Ekman</author>
<author>W V Friesen</author>
<author>P Ellsworth</author>
</authors>
<title>What Emotion Categories or Dimensions can Observers Judge from Facial Behavior? Emotion in the human face, Cambridge</title>
<date>1982</date>
<pages>39--55</pages>
<publisher>University Press.</publisher>
<location>New York.</location>
<marker>Ekman, Friesen, Ellsworth, 1982</marker>
<rawString>Ekman, Paul., Friesen W. V., and Ellsworth P. 1982. What Emotion Categories or Dimensions can Observers Judge from Facial Behavior? Emotion in the human face, Cambridge University Press. pages 39-55, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Statistical Methods for Rates and Proportions.</title>
<date>1981</date>
<editor>Wiley. second ed.,</editor>
<location>New York.</location>
<contexts>
<context position="2420" citStr="Fleiss, 1981" startWordPosition="362" endWordPosition="363">://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. the corpus reliability is measured by coefficient of agreement. The coefficients of agreement are applied to corpus for various goals like measuring reliability, validity and stability of corpus (Artstein and Poesio, 2008). Jacob Cohen (Cohen, 1960) introduced Kappa statistics as a coefficient of agreement for nominal scales. The Kappa coefficient measures the proportion of observed agreement over the agreement by chance and the maximum agreement attainable over chance agreement considering pairwise agreement. Later Fleiss (Fleiss, 1981) proposed an extension to measure agreement in ordinal scale data. Cohen’s Kappa has been widely used in various research areas. Because of its simplicity and robustness, it has become a popular approach for agreement measurement in the area of electronics (Jung, 2003), geographical informatics (Hagen, 2003), medical (Hripcsak and Heitjan, 2002), and many more domains. There are other variants of Kappa like agreement measures (Carletta, 1996). Scott’s 7r (Scott, 1955) was introduced to measure agreement in survey research. Kappa and 7r measures differ in the way they determine the chance relat</context>
</contexts>
<marker>Fleiss, 1981</marker>
<rawString>Fleiss, Joseph L. 1981. Statistical Methods for Rates and Proportions. Wiley. second ed., New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Hagen-Zanker</author>
</authors>
<title>Fuzzy Set Approach to Assessing Similarity of Categorical Maps.</title>
<date>2003</date>
<journal>International Journal for Geographical Information Science.</journal>
<pages>17--3</pages>
<marker>Hagen-Zanker, 2003</marker>
<rawString>Hagen-Zanker, Alex. 2003. Fuzzy Set Approach to Assessing Similarity of Categorical Maps. International Journal for Geographical Information Science. 17(3):235-249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Hripcsak</author>
<author>Daniel F Heitjan</author>
</authors>
<title>Measuring Agreement in Medical Informatics Reliability Studies.</title>
<date>2002</date>
<journal>Journal of Biomedical Informatics.</journal>
<pages>35--2</pages>
<contexts>
<context position="2767" citStr="Hripcsak and Heitjan, 2002" startWordPosition="415" endWordPosition="418">ppa statistics as a coefficient of agreement for nominal scales. The Kappa coefficient measures the proportion of observed agreement over the agreement by chance and the maximum agreement attainable over chance agreement considering pairwise agreement. Later Fleiss (Fleiss, 1981) proposed an extension to measure agreement in ordinal scale data. Cohen’s Kappa has been widely used in various research areas. Because of its simplicity and robustness, it has become a popular approach for agreement measurement in the area of electronics (Jung, 2003), geographical informatics (Hagen, 2003), medical (Hripcsak and Heitjan, 2002), and many more domains. There are other variants of Kappa like agreement measures (Carletta, 1996). Scott’s 7r (Scott, 1955) was introduced to measure agreement in survey research. Kappa and 7r measures differ in the way they determine the chance related agreements. 7r-like coefficients determine the chance agreement among arbitrary coders, while r.-like coefficients treats the chance of agreement among the coders who produced the reliability data (Artstein and Poesio, 2008). One of the drawbacks of 7r and Kappa like coefficients except Fleiss’ Kappa (Fleiss, 1981) is that they treat all kind</context>
</contexts>
<marker>Hripcsak, Heitjan, 2002</marker>
<rawString>Hripcsak, George and Daniel F. Heitjan. 2002. Measuring Agreement in Medical Informatics Reliability Studies. Journal of Biomedical Informatics. 35(2):99-110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ho-Won Jung</author>
</authors>
<title>Evaluating Interrater Agreement in SPICE-based Assessments.</title>
<date>2003</date>
<journal>Computer Standards &amp; Interfaces.</journal>
<pages>25--5</pages>
<contexts>
<context position="2689" citStr="Jung, 2003" startWordPosition="408" endWordPosition="409">ein and Poesio, 2008). Jacob Cohen (Cohen, 1960) introduced Kappa statistics as a coefficient of agreement for nominal scales. The Kappa coefficient measures the proportion of observed agreement over the agreement by chance and the maximum agreement attainable over chance agreement considering pairwise agreement. Later Fleiss (Fleiss, 1981) proposed an extension to measure agreement in ordinal scale data. Cohen’s Kappa has been widely used in various research areas. Because of its simplicity and robustness, it has become a popular approach for agreement measurement in the area of electronics (Jung, 2003), geographical informatics (Hagen, 2003), medical (Hripcsak and Heitjan, 2002), and many more domains. There are other variants of Kappa like agreement measures (Carletta, 1996). Scott’s 7r (Scott, 1955) was introduced to measure agreement in survey research. Kappa and 7r measures differ in the way they determine the chance related agreements. 7r-like coefficients determine the chance agreement among arbitrary coders, while r.-like coefficients treats the chance of agreement among the coders who produced the reliability data (Artstein and Poesio, 2008). One of the drawbacks of 7r and Kappa lik</context>
</contexts>
<marker>Jung, 2003</marker>
<rawString>Jung, Ho-Won. 2003. Evaluating Interrater Agreement in SPICE-based Assessments. Computer Standards &amp; Interfaces. 25(5):477-499.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to its Methodology. Sage Publications.</title>
<date>1980</date>
<location>Beverley Hills, CA.</location>
<contexts>
<context position="3443" citStr="Krippendorff, 1980" startWordPosition="521" endWordPosition="522">a like agreement measures (Carletta, 1996). Scott’s 7r (Scott, 1955) was introduced to measure agreement in survey research. Kappa and 7r measures differ in the way they determine the chance related agreements. 7r-like coefficients determine the chance agreement among arbitrary coders, while r.-like coefficients treats the chance of agreement among the coders who produced the reliability data (Artstein and Poesio, 2008). One of the drawbacks of 7r and Kappa like coefficients except Fleiss’ Kappa (Fleiss, 1981) is that they treat all kinds of disagreements in the same manner. Krippendorff’s α (Krippendorff, 1980) is a reliability measure which treats different kind of disagreements separately by introducing a notion of distance between two categories. It offers a way 58 Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 58–65 Manchester, August 2008 to measure agreement in nominal, interval, ordinal and ratio scale data. Reliability assessment of corpus is an important issue in corpus driven natural language processing and the existing reliability measures have been used in various corpus development tasks. For example, Kappa coefficient has been used in d</context>
</contexts>
<marker>Krippendorff, 1980</marker>
<rawString>Krippendorff, Klaus 1980. Content Analysis: An Introduction to its Methodology. Sage Publications. Beverley Hills, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margot Mieskes</author>
<author>Michael Strube</author>
</authors>
<title>Part-ofSpeech Tagging of Transcribed Speech.</title>
<date>2006</date>
<booktitle>Proceedings of International Conference on Language Resources and Evaluation. GENOA</booktitle>
<contexts>
<context position="4102" citStr="Mieskes and Strube, 2006" startWordPosition="618" endWordPosition="621">eats different kind of disagreements separately by introducing a notion of distance between two categories. It offers a way 58 Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 58–65 Manchester, August 2008 to measure agreement in nominal, interval, ordinal and ratio scale data. Reliability assessment of corpus is an important issue in corpus driven natural language processing and the existing reliability measures have been used in various corpus development tasks. For example, Kappa coefficient has been used in developing parts of speech corpus (Mieskes and Strube, 2006), dialogue act tagging efforts like MapTask (Carletta et al., 1997) and Switchboard (Stolke et al., 1997), subjectivity tagging task (Bruce and Wiebe, 1999) and many more. The 7r and r. coefficients measure the reliability of the annotation task where a data item can be annotated with one category. (Rosenberg and Binkowski, 2004) puts an effort towards measuring corpus reliability for multiply labeled data points. In this measure, the annotators are allowed to mark one data point with at most two classes, one of which is primary and other is secondary. This measure was used to determine the re</context>
</contexts>
<marker>Mieskes, Strube, 2006</marker>
<rawString>Mieskes, Margot and Michael Strube. 2006. Part-ofSpeech Tagging of Transcribed Speech. Proceedings of International Conference on Language Resources and Evaluation. GENOA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Ortony</author>
<author>Terence J Turner</author>
</authors>
<title>What’s Basic About Basic Emotions?. Psychological Review.</title>
<date>1990</date>
<pages>97--3</pages>
<contexts>
<context position="7786" citStr="Ortony and Turner, 1990" startWordPosition="1227" endWordPosition="1230">ection of affect categories is a very crucial and important decision problem due to the following reasons. • The affect categories should be applicable to the considered genre. • The affect categories should be identifiable from language. • The categories should be unambiguous. We shall try to validate these points based on the results obtained, after applying the our extended measure on the text corpus with respect to a set of selected basic emotional categories. Basic emotions are those for which the respective expressions across culture, ethnicity, age, sex, social structure are invariant (Ortony and Turner, 1990). But unfortunately, there is a long persistent debate among the psychologists regarding 1http://timesofindia.indiatimes.com/archive.cms 59 the number of basic emotional categories (Ortony and Turner, 1990). One of the theories behind the basic emotions is that they are biologically primitive because they possess evolutionary significance related to the basic needs for the survival of the species (Plutchik, 1980). The universality of recognition of emotions from distinctive facial expressions is an indirect technique to establish the basic emotions (Darwin, 1965). Six basic affect categories (</context>
</contexts>
<marker>Ortony, Turner, 1990</marker>
<rawString>Ortony, Andrew and Terence J. Turner. 1990. What’s Basic About Basic Emotions?. Psychological Review. 97(3):315-331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Pantic</author>
<author>Leon Rothkrantz</author>
</authors>
<title>Automatic Analysis ofFacial Expressions: The State of the Art.</title>
<date>2000</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence.</journal>
<pages>22--12</pages>
<contexts>
<context position="8546" citStr="Pantic and Rothkrantz, 2000" startWordPosition="1335" endWordPosition="1338">.cms 59 the number of basic emotional categories (Ortony and Turner, 1990). One of the theories behind the basic emotions is that they are biologically primitive because they possess evolutionary significance related to the basic needs for the survival of the species (Plutchik, 1980). The universality of recognition of emotions from distinctive facial expressions is an indirect technique to establish the basic emotions (Darwin, 1965). Six basic affect categories (Ekman, Friesen and Ellsworth, 1982) have been considered in emotion recognition from speech (Song et al., 2004), facial expression (Pantic and Rothkrantz, 2000). Our annotation scheme considers six basic emotions, namely, Anger, Disgust, Fear, Happiness, Sadness, Surprise as specified by Ekman for affect recognition in text corpus. The annotation scheme considers the following points: • Two types are sentences are collected for annotation. – Direct Affective Sentence: Here, the agent present in the sentence is experiencing a set of emotions, which are explicit in the sentence. For example, in the following sentence Indian supporters are the agents experiencing a disgust emotion. Indian supporters are disgusted about players’ performances in the World</context>
</contexts>
<marker>Pantic, Rothkrantz, 2000</marker>
<rawString>Pantic, Maja and Leon Rothkrantz. 2000. Automatic Analysis ofFacial Expressions: The State of the Art. IEEE Transactions on Pattern Analysis and Machine Intelligence. 22(12):1424-1445.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Plutchik</author>
</authors>
<title>A General Psychoevolutionary Theory of Emotion. Emotion: Theory, research, and experience:</title>
<date>1980</date>
<volume>1</volume>
<pages>3--33</pages>
<publisher>Academic Press,</publisher>
<location>New York,</location>
<contexts>
<context position="8202" citStr="Plutchik, 1980" startWordPosition="1287" endWordPosition="1288">set of selected basic emotional categories. Basic emotions are those for which the respective expressions across culture, ethnicity, age, sex, social structure are invariant (Ortony and Turner, 1990). But unfortunately, there is a long persistent debate among the psychologists regarding 1http://timesofindia.indiatimes.com/archive.cms 59 the number of basic emotional categories (Ortony and Turner, 1990). One of the theories behind the basic emotions is that they are biologically primitive because they possess evolutionary significance related to the basic needs for the survival of the species (Plutchik, 1980). The universality of recognition of emotions from distinctive facial expressions is an indirect technique to establish the basic emotions (Darwin, 1965). Six basic affect categories (Ekman, Friesen and Ellsworth, 1982) have been considered in emotion recognition from speech (Song et al., 2004), facial expression (Pantic and Rothkrantz, 2000). Our annotation scheme considers six basic emotions, namely, Anger, Disgust, Fear, Happiness, Sadness, Surprise as specified by Ekman for affect recognition in text corpus. The annotation scheme considers the following points: • Two types are sentences ar</context>
</contexts>
<marker>Plutchik, 1980</marker>
<rawString>Plutchik, Robert 1980. A General Psychoevolutionary Theory of Emotion. Emotion: Theory, research, and experience: Vol. 1. Theories of emotion. Academic Press, New York, 3-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Rosenberg</author>
<author>Ed Binkowski</author>
</authors>
<title>Augmenting the Kappa Statistic to Determine Interannotator Reliability for Multiply Labeled Data Points.</title>
<date>2004</date>
<booktitle>In Proceedings of North American Chapter of the Association for Computational Linguistics.</booktitle>
<pages>77--80</pages>
<location>Boston,</location>
<contexts>
<context position="4433" citStr="Rosenberg and Binkowski, 2004" startWordPosition="672" endWordPosition="675"> Reliability assessment of corpus is an important issue in corpus driven natural language processing and the existing reliability measures have been used in various corpus development tasks. For example, Kappa coefficient has been used in developing parts of speech corpus (Mieskes and Strube, 2006), dialogue act tagging efforts like MapTask (Carletta et al., 1997) and Switchboard (Stolke et al., 1997), subjectivity tagging task (Bruce and Wiebe, 1999) and many more. The 7r and r. coefficients measure the reliability of the annotation task where a data item can be annotated with one category. (Rosenberg and Binkowski, 2004) puts an effort towards measuring corpus reliability for multiply labeled data points. In this measure, the annotators are allowed to mark one data point with at most two classes, one of which is primary and other is secondary. This measure was used to determine the reliability of a email corpus where emails are assigned with primary and secondary labels from a set of email types. Affect recognition from text is a recent and promising subarea of natural language processing. The task is to classify text segments into appropriate affect categories. The supervised machine learning techniques, whi</context>
</contexts>
<marker>Rosenberg, Binkowski, 2004</marker>
<rawString>Rosenberg, Andrew, and Ed Binkowski. 2004. Augmenting the Kappa Statistic to Determine Interannotator Reliability for Multiply Labeled Data Points. In Proceedings of North American Chapter of the Association for Computational Linguistics. Boston, 77-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Scott</author>
</authors>
<title>Reliability of Content Analysis: The Case of Nominal Scale Coding. Public Opinion Quarterly.</title>
<date>1955</date>
<contexts>
<context position="2892" citStr="Scott, 1955" startWordPosition="437" endWordPosition="438">e agreement by chance and the maximum agreement attainable over chance agreement considering pairwise agreement. Later Fleiss (Fleiss, 1981) proposed an extension to measure agreement in ordinal scale data. Cohen’s Kappa has been widely used in various research areas. Because of its simplicity and robustness, it has become a popular approach for agreement measurement in the area of electronics (Jung, 2003), geographical informatics (Hagen, 2003), medical (Hripcsak and Heitjan, 2002), and many more domains. There are other variants of Kappa like agreement measures (Carletta, 1996). Scott’s 7r (Scott, 1955) was introduced to measure agreement in survey research. Kappa and 7r measures differ in the way they determine the chance related agreements. 7r-like coefficients determine the chance agreement among arbitrary coders, while r.-like coefficients treats the chance of agreement among the coders who produced the reliability data (Artstein and Poesio, 2008). One of the drawbacks of 7r and Kappa like coefficients except Fleiss’ Kappa (Fleiss, 1981) is that they treat all kinds of disagreements in the same manner. Krippendorff’s α (Krippendorff, 1980) is a reliability measure which treats different </context>
</contexts>
<marker>Scott, 1955</marker>
<rawString>Scott, William A. 1955. Reliability of Content Analysis: The Case of Nominal Scale Coding. Public Opinion Quarterly. 19(3):321-325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mingli Song</author>
<author>Chun Chen</author>
<author>Jiajun Bu</author>
<author>Mingyu You</author>
</authors>
<date>2004</date>
<booktitle>Speech Emotion Recognition and Intensity Estimation. Internation Conference on Computational Science and its Applications.</booktitle>
<pages>406--413</pages>
<location>Perugia,</location>
<contexts>
<context position="8497" citStr="Song et al., 2004" startWordPosition="1328" endWordPosition="1331">p://timesofindia.indiatimes.com/archive.cms 59 the number of basic emotional categories (Ortony and Turner, 1990). One of the theories behind the basic emotions is that they are biologically primitive because they possess evolutionary significance related to the basic needs for the survival of the species (Plutchik, 1980). The universality of recognition of emotions from distinctive facial expressions is an indirect technique to establish the basic emotions (Darwin, 1965). Six basic affect categories (Ekman, Friesen and Ellsworth, 1982) have been considered in emotion recognition from speech (Song et al., 2004), facial expression (Pantic and Rothkrantz, 2000). Our annotation scheme considers six basic emotions, namely, Anger, Disgust, Fear, Happiness, Sadness, Surprise as specified by Ekman for affect recognition in text corpus. The annotation scheme considers the following points: • Two types are sentences are collected for annotation. – Direct Affective Sentence: Here, the agent present in the sentence is experiencing a set of emotions, which are explicit in the sentence. For example, in the following sentence Indian supporters are the agents experiencing a disgust emotion. Indian supporters are d</context>
</contexts>
<marker>Song, Chen, Bu, You, 2004</marker>
<rawString>Song, Mingli, Chun Chen, Jiajun Bu, and Mingyu You. 2004. Speech Emotion Recognition and Intensity Estimation. Internation Conference on Computational Science and its Applications. Perugia, 406-413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
<author>K Ries</author>
<author>N Coccaro</author>
<author>E Shriberg</author>
<author>R Bates</author>
<author>D Jurafsky</author>
<author>P Taylor</author>
<author>Martin C Van-Ess-Dykema</author>
<author>M Meteer</author>
</authors>
<title>Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech. Computational Linguistics.</title>
<date>1997</date>
<pages>26--3</pages>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Van-Ess-Dykema, Meteer, 1997</marker>
<rawString>Stolcke A., Ries K., Coccaro N., Shriberg E., Bates R., Jurafsky .D, Taylor P., Martin C. Van-Ess-Dykema, and Meteer .M. 1997. Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech. Computational Linguistics. 26(3):339-371.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>