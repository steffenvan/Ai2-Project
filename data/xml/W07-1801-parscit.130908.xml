<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005547">
<title confidence="0.982696">
Speech Recognition Grammar Compilation in Grammatical Framework
</title>
<author confidence="0.997398">
Bj¨orn Bringert
</author>
<affiliation confidence="0.999655">
Department of Computer Science and Engineering
Chalmers University of Technology and G¨oteborg University
</affiliation>
<address confidence="0.973964">
SE-412 96 G¨oteborg, Sweden
</address>
<email confidence="0.999285">
bringert@cs.chalmers.se
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999907333333333">
This paper describes how grammar-based
language models for speech recognition sys-
tems can be generated from Grammatical
Framework (GF) grammars. Context-free
grammars and finite-state models can be
generated in several formats: GSL, SRGS,
JSGF, and HTK SLF. In addition, semantic
interpretation code can be embedded in the
generated context-free grammars. This en-
ables rapid development of portable, multi-
lingual and easily modifiable speech recog-
nition applications.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99983408">
Speech recognition grammars are used for guid-
ing speech recognizers in many applications. How-
ever, there are a number of problems associated
with writing grammars in the low-level, system-
specific formats required by speech recognizers.
This work addresses these problems by generat-
ing speech recognition grammars and semantic in-
terpretation components from grammars written in
Grammatical Framework (GF), a high-level, type-
theoretical grammar formalism. Compared to exist-
ing work on compiling unification grammars, such
as Regulus (Rayner et al., 2006), our work uses a
type-theoretical grammar formalism with a focus on
multilinguality and modular grammar development,
and supports multiple speech recognition grammar
formalisms, including finite-state models.
We first outline some existing problems in the de-
velopment and maintenance of speech recognition
grammars, and describe how our work attempts to
address these problems. In the following two sec-
tions we introduce speech recognition grammars and
Grammatical Framework. The bulk of the paper
then describes how we generate context-free speech
recognition grammars, finite-state language models
and semantic interpretation code from GF gram-
mars. We conclude by giving references to a number
of experimental dialogue systems which already use
our grammar compiler for generating speech recog-
nition grammars.
Expressivity Speech recognition grammars are
written in simple formalisms which do not have
the powerful constructs of high-level grammar for-
malisms. This makes speech recognition grammar
writing labor-intensive and error prone, especially
for languages with more inflection and agreement
than English.
This is solved by using a high-level grammar for-
malism with powerful constructs and a grammar
library which implements the domain-independent
linguistic details.
Duplicated work When speech recognition gram-
mars are written directly in the low-level format re-
quired by the speech recognizer, other parts of the
system, such as semantic interpretation components,
must often be constructed separately.
This duplicated work can be avoided by gener-
ating all the components from a single declarative
source, such as a GF grammar.
Consistency Because of the lack of abstraction
mechanisms and consistency checks, it is difficult
</bodyText>
<page confidence="0.830703">
1
</page>
<note confidence="0.59109">
Proceedings of SPEECHGRAM 2007, pages 1–8,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999873666666666">
to modify a system which uses hand-written speech
recognition grammars. The problem is multiplied
when the system is multilingual. The developer
has to modify the speech recognition grammar and
the semantic interpretation component manually for
each language. A simple change may require touch-
ing many parts of the grammar, and there are no au-
tomatic consistency checks.
The strong typing of the GF language enforces
consistency between the semantics and the concrete
representation in each language.
Localization With hand-written grammars, it is
about as difficult to add support for a new language
as it is to write the grammar and semantic interpre-
tation for the first language.
GF’s support for multilingual grammars and the
common interface implemented by all grammars in
the GF resource grammar library makes it easier to
translate a grammar to a new language.
Portability A grammar in any given speech recog-
nition grammar format cannot be used with a speech
recognizer which uses another format.
In our approach, a GF grammar is used as the
canonical representation which the developer works
with, and speech recognition grammars in many for-
mats can be generated automatically from this rep-
resentation.
</bodyText>
<sectionHeader confidence="0.964199" genericHeader="introduction">
2 Speech Recognition Grammars
</sectionHeader>
<bodyText confidence="0.999955538461539">
To achieve acceptable accuracy, speech recognition
software is guided by a language model which de-
fines the language which can be recognized. A lan-
guage model may also assign different probabilities
to different strings in the language. A language
model can either be a statistical language model
(SLM), such as an n-gram model, or a grammar-
based language model, for example a context-free
grammar (CFG) or a finite-state automaton (FSA).
In this paper, we use the term speech recogni-
tion grammar (SRG) to refer to all grammar-based
language models, including context-free grammars,
regular grammars and finite-state automata.
</bodyText>
<sectionHeader confidence="0.998013" genericHeader="method">
3 Grammatical Framework
</sectionHeader>
<bodyText confidence="0.998305777777778">
Grammatical Framework (GF) (Ranta, 2004) is a
grammar formalism based on constructive type the-
ory. In GF, an abstract syntax defines a seman-
tic representation. A concrete syntax declares how
terms in an abstract syntax are linearized, that is,
how they are mapped to concrete representations.
GF grammars can be made multilingual by having
multiple concrete syntaxes for a single abstract syn-
tax.
</bodyText>
<subsectionHeader confidence="0.999378">
3.1 The Resource Grammar Library
</subsectionHeader>
<bodyText confidence="0.999951551724138">
The GF Resource Grammar Library (Ranta et al.,
2006) currently implements the morphological and
syntactic details of 10 languages. This library is in-
tended to make it possible to write grammars with-
out caring about the linguistic details of particular
languages. It is inspired by library-based software
engineering, where complex functionality is imple-
mented in reusable software libraries with simple in-
terfaces.
The resource grammar library is used through
GF’s facility for grammar composition, where the
abstract syntax of one grammar is used in the imple-
mentation of the concrete syntax of another gram-
mar. Thus, an application grammar writer who uses
a resource grammar uses its abstract syntax terms
to implement the linearizations in the application
grammar.
The resource grammars for the different lan-
guages implement a common interface, i.e. they
all have a common abstract syntax. This means
that grammars which are implemented using re-
source grammars can be easily localized to other
languages. Localization normally consists of trans-
lating the application-specific lexical items, and ad-
justing any linearizations which turn out to be uni-
diomatic in the language in question. For example,
when the GoTGoDiS (Ericsson et al., 2006) appli-
cation was localized to Finnish, only 3 out of 180
linearization rules had to be changed.
</bodyText>
<subsectionHeader confidence="0.9998">
3.2 An Example GF Grammar
</subsectionHeader>
<bodyText confidence="0.99840125">
Figure 1 contains a small example GF abstract syn-
tax. Figure 2 defines an English concrete syntax
for it, using the resource grammar library. We will
use this grammar when we show examples of speech
recognition grammar generation later.
In the abstract syntax, cat judgements introduce
syntactic categories, and fun judgements declare
constructors in those categories. For example, the
</bodyText>
<page confidence="0.82009">
2
</page>
<equation confidence="0.809805111111111">
abstract Food = {
cat Order;Items;Item;Number;Size;
fun order: Items → Order;
and : Items → Items → Items;
items: Item → Number → Size → Items;
pizza,beer : Item;
one,two : Number;
small,large: Size;
}
</equation>
<figureCaption confidence="0.768325">
Figure 1: Food.gf: A GF abstract syntax module.
</figureCaption>
<bodyText confidence="0.751182">
concrete FoodEng of Food = open English in {
</bodyText>
<equation confidence="0.963312142857143">
flags startcat = Order;
lincat Order = Utt;Items = NP;
Item = CN;Number = Det;
Size = AP;
lin order x = mkUtt x;
and x y = mkNP and Conj x y;
items x n s = mkNP n (mkCN s x);
pizza = mkCN (regN “pizza”);
beer = mkCN (regN “beer”);
one = mkDet one Quant;
two = mkDet n2;
small = mkAP (regA “small”);
large = mkAP (regA “large”);
}
</equation>
<figureCaption confidence="0.909682">
Figure 2: FoodEng.gf: English concrete syntax for
the abstract syntax in Figure 1.
</figureCaption>
<bodyText confidence="0.999969571428571">
items constructor makes an Items term from an Item,
a Number and a Size. The term items pizza two small
is an example of a term in this abstract syntax.
In the concrete syntax, a lincat judgement de-
clares the type of the concrete terms generated from
the abstract syntax terms in a given category. The
linearization of each constructor is declared with a
lin judgement. In the concrete syntax in Figure 2,
library functions from the English resource gram-
mar are used for the linearizations, but it is also pos-
sible to write concrete syntax terms directly. The
linearization of the term items pizza two small is
{s = “two small pizzas”}, a record containing a sin-
gle string field.
By changing the imports and the four lexical
items, this grammar can be translated to any other
language for which there is a resource grammar.
For example, in the German version, we replace
(regN “beer”) with (reg2N “Bier” “Biere” neuter)
and so on. The functions regN and reg2N implement
paradigms for regular English and German nouns,
respectively. This replacement can be formalized
using GF’s parameterized modules, which lets one
write a common implementation that can be instan-
tiated with the language-specific parts. Note that the
application grammar does not deal with details such
as agreement, as this is taken care of by the resource
grammar.
</bodyText>
<sectionHeader confidence="0.97294" genericHeader="method">
4 Generating Context-free Grammars
</sectionHeader>
<subsectionHeader confidence="0.958314">
4.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.999973">
GF grammars are converted to context-free speech
recognition grammars in a number of steps. An
overview of the compilation pipeline is show in Fig-
ure 3. The figure also includes compilation to finite-
state automata, as described in Section 5. Each step
of the compilation is described in more detail in the
sections below.
Conversion to CFG The GF grammar is first
converted into a context-free grammar annotated
with functions and profiles, as described by
Ljungl¨of (2004).
Cycle elimination All directly and indirectly
cyclic productions are removed, since they cannot be
handled gracefully by the subsequent left-recursion
elimination. Such productions do not contribute to
the coverage to the grammar, only to the set of pos-
sible semantic results.
Bottom-up filtering Productions whose right-
hand sides use categories for which there are no pro-
ductions are removed, since these will never match
any input.
Top-down filtering Only productions for cate-
gories which can be reached from the start category
are kept. This is mainly used to remove parts of the
grammar which are unused because of the choice
of start category. One example where this is useful
is when a speech recognition grammar is generated
from a multimodal grammar (Bringert et al., 2005).
In this case, the start category is different from the
start category used by the parser, in that its lineariza-
tion only contains the speech component of the in-
</bodyText>
<page confidence="0.970769">
3
</page>
<figure confidence="0.89749">
GF grammar
</figure>
<figureCaption confidence="0.999794">
Figure 3: Grammar compilation pipeline.
</figureCaption>
<bodyText confidence="0.999933787234042">
put. Top-down filtering then has the effect of ex-
cluding the non-speech modalities from the speech
recognition grammar.
The bottom-up and top-down filtering steps are it-
erated until a fixed point is reached, since both these
steps may produce new filtering opportunities.
Left-recursion elimination All direct and indi-
rect left-recursion is removed using the LCLR trans-
form described by Moore (2000). We have modi-
fied the LCLR transform to avoid adding productions
which use a category A−X when there are no pro-
ductions for A−X.
Identical category elimination In this step, the
categories are grouped into equivalence classes by
their right-hand sides and semantic annotations. The
categories A1 ...An in each class are replaced by a
single category A1+...+An throughout the grammar,
discarding any duplicate productions. This has the
effect of replacing all categories which have identi-
cal sets of productions with a single category. Con-
crete syntax parameters which do not affect inflec-
tion is one source of such redundancy; the LCLR
transform is another.
EBNF compaction The resulting context-free
grammar is compacted into an Extended Backus-
Naur Form (EBNF) representation. This reduces the
size and improves the readability of the final gram-
mar. The compaction is done by, for each cate-
gory, grouping all the productions which have the
same semantic interpretation, and the same sequence
of non-terminals on their right-hand sides, ignoring
any terminals. The productions in each group are
merged into one EBNF production, where the ter-
minal sequences between the non-terminals are con-
verted to regular expressions which are the unions of
the original terminal sequences. These regular ex-
pressions are then minimized.
Conversion to output format The resulting non-
left-recursive grammar is converted to SRGS, JSGF
or Nuance GSL format.
A fragment of a SRGS ABNF grammar generated
from the GF grammar in Figure 2 is shown below.
The left-recursive and rule was removed from the
grammar before compilation, as the left-recursion
elimination step makes it difficult to read the gen-
erated grammar. The fragment shown here is for the
singular part of the items rule.
</bodyText>
<equation confidence="0.99987425">
$FE1 = $FE6 $FE9 $FE4;
$FE6 = one;
$FE9 = large  |small;
$FE4 = beer  |pizza;
</equation>
<bodyText confidence="0.99994675">
The corresponding fragment generated from the
German version of the grammar is more complex,
since the numeral and the adjective must agree with
the gender of the noun.
</bodyText>
<equation confidence="0.9997348">
$FG1 = $FG10 $FG13 $FG6  |$FG9 $FG12 $FG4;
$FG9 = eine; $FG10 = ein;
$FG12 = große  |kleine;
$FG13 = großes  |kleines;
$FG4 = Pizza; $FG6 = Bier;
</equation>
<subsectionHeader confidence="0.599423">
4.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999983">
The generated grammar is an overgenerating ap-
proximation of the original GF grammar. This is
inevitable, since the GF formalism is stronger than
</bodyText>
<figure confidence="0.996578285714286">
SRGS/JSGF/GSL SLF
EBNF compaction
Identical category
elimination
Left-recursion
elimination
CFG conversion
Cycle elimination
Bottom-up filtering
Top-down filtering
FSA compilation
Regular
approximation
Minimization
</figure>
<page confidence="0.958716">
4
</page>
<bodyText confidence="0.99997408">
context-free grammars, for example through its sup-
port for reduplication. GF’s support for dependently
typed and higher-order abstract syntax is also not
yet carried over to the generated speech recogni-
tion grammars. This could be handled in a subse-
quent semantic interpretation step. However, that
requires that the speech recognizer considers mul-
tiple hypotheses, since some may be discarded by
the semantic interpretation. Currently, if the abstract
syntax types are only dependent on finite types, the
grammar can be expanded to remove the dependen-
cies. This appears to be sufficient for many realistic
applications.
In some cases, empty productions in the gener-
ated grammar could cause problems for the cycle
and left-recursion elimination, though we have yet
to encounter this in practice. Empty productions can
be removed by transforming the grammar, though
this has not yet been implemented.
For some grammars, the initial CFG generation
can generate a very large number of productions.
While the resulting speech recognition grammars
are of a reasonable size, the large intermediate gram-
mars can cause memory problems. Further opti-
mization is needed to address this problem.
</bodyText>
<sectionHeader confidence="0.999421" genericHeader="method">
5 Finite-State Models
</sectionHeader>
<subsectionHeader confidence="0.904692">
5.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.99189984">
Some speech recognition systems use finite-state au-
tomata rather than context-free grammars as lan-
guage models. GF grammars can be compiled to
finite-state automata using the procedure shown in
Figure 3. The initial part of the compilation to
a finite-state model is shared with the context-free
SRG compilation, and is described in Section 4.
Regular approximation The context-free gram-
mar is approximated with a regular grammar, us-
ing the algorithm described by Mohri and Neder-
hof (2001).
Compilation to finite-state automata The reg-
ular grammar is transformed into a set of non-
deterministic finite automata (NFA) using a modi-
fied version of the make fa algorithm described by
Nederhof (2000). For realistic grammars, applying
the original make fa algorithm to the whole gram-
mar generates a very large automaton, since a copy
of the sub-automaton corresponding to a given cate-
gory is made for every use of the category.
Instead, one automaton is generated for each cat-
egory in the regular grammar. All categories which
are not in the same mutually recursive set as the
category for which the automaton is generated are
treated as terminal symbols. This results in a set
of automata with edges labeled with either terminal
symbols or the names of other automata.
If desired, the set of automata can be con-
verted into a single automaton by substituting each
category-labeled edge with a copy of the corre-
sponding automaton. Note that this always termi-
nates, since the sub-automata do not have edges la-
beled with the categories from the same mutually re-
cursive set.
Minimization Each of the automata is turned into
a minimal deterministic finite automaton (DFA) by
using Brzozowski’s (1962) algorithm, which min-
imizes the automaton by performing two deter-
minizations and reversals.
Conversion to output format The resulting finite
automaton can be output in HTK Standard Lattice
Format (SLF). SLF supports sub-lattices, which al-
lows us to convert our set of automata directly into a
set of lattices. Since SLF uses labeled nodes, rather
than labeled edges, we move the labels to the nodes.
This is done by first introducing a new labeled node
for each edge, and then eliminating all internal un-
labeled nodes. Figure 4 shows the SLF model gen-
erated from the example grammar. For clarity, the
sub-lattices have been inlined.
</bodyText>
<figureCaption confidence="0.9451155">
Figure 4: SLF model generated from the grammar
in Figure 2.
</figureCaption>
<figure confidence="0.9989281">
START
and one
two
small beer
large pizza
small
large
pizzas
beers
END
</figure>
<page confidence="0.796231">
5
</page>
<subsectionHeader confidence="0.920672">
5.2 Discussion
</subsectionHeader>
<bodyText confidence="0.999806">
Finite-state models are even more restrictive than
context-free grammars. This problem is handled
by approximating the context-free grammar with
an overgenerating finite-state automaton. This may
lead to failure in a subsequent parsing step, which,
as in the context-free case, is acceptable if the rec-
ognizer can return all hypotheses.
</bodyText>
<sectionHeader confidence="0.995605" genericHeader="method">
6 Semantic Interpretation
</sectionHeader>
<bodyText confidence="0.9960424">
Semantic interpretation can be done as a separate
parsing step after speech recognition, or it can be
done with semantic information embedded in the
speech recognition grammar. The latter approach re-
sembles the semantic actions used by parser genera-
tors for programming languages. One formalism for
semantic interpretation is the proposed Semantic In-
terpretation for Speech Recognition (SISR) standard.
SISR tags are pieces of ECMAScript code embed-
ded in the speech recognition grammar.
</bodyText>
<subsectionHeader confidence="0.996818">
6.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.99998080952381">
The GF system can include SISR tags when gen-
erating speech recognitions grammars in SRGS
and JSGF format. The SISR tags are generated
from the semantic information in the annotated
CFG (Ljungl¨of, 2004). The result of the semantic
interpretation is an abstract syntax term.
The left-recursion elimination step makes it
somewhat challenging to produce correct abstract
syntax trees. We have extended Moore’s (2000)
LCLR transform to preserve the semantic interpreta-
tion. The LCLR transform introduces new categories
of the form A−X where X is a proper left corner of
a category A. The new category A−X can be under-
stood as “the category A, but missing an initial X”.
Thus the semantic interpretation for a production in
A−X is the semantic interpretation for the original A-
production, abstracted (in the X-calculus sense) over
the semantic interpretation of the missing X. Con-
versely, where-ever a category A−X is used, its re-
sult is applied to the interpretation of the occurrence
of X.
</bodyText>
<subsectionHeader confidence="0.985848">
6.2 Discussion
</subsectionHeader>
<bodyText confidence="0.99972925">
As discussed in Section 4.2, the semantic interpre-
tation code could be used to implement the non-
context-free features of GF, but this is not yet done.
The slot-filling mechanism in the GSL format
could also be used to build semantic representations,
by returning program code which can then be ex-
ecuted. The UNIANCE grammar compiler (Bos,
2002) uses that approach.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="method">
7 Related Work
</sectionHeader>
<subsectionHeader confidence="0.999915">
7.1 Unification Grammar Compilation
</subsectionHeader>
<bodyText confidence="0.999976791666667">
Compilation of unification grammars to speech
recognition grammars is well described in the liter-
ature (Moore, 1999; Dowding et al., 2001). Regu-
lus (Rayner et al., 2006) is perhaps the most ambi-
tious such system. Like GF, Regulus uses a general
grammar for each language, which is specialized to a
domain-specific one. Ljungl¨of (Ljungl¨of, 2007b) re-
lates GF and Regulus by showing how to convert GF
grammars to Regulus grammars. We carry composi-
tional semantic interpretation through left-recursion
elimination using the same idea as the UNIANCE
grammar compiler (Bos, 2002), though our version
handles both direct and indirect left-recursion.
The main difference between our work and the
existing compilers is that we work with type-
theoretical grammars rather than unification gram-
mars. While the existing work focuses on GSL
as the output language, we also support a number
of other formats, including finite-state models. By
using the GF resource grammars, speech recogni-
tion language models can be produced for more lan-
guages than with previous systems. One shortcom-
ing of our system is that it does not yet have support
for weighted grammars.
</bodyText>
<subsectionHeader confidence="0.999443">
7.2 Generating SLMs from GF Grammars
</subsectionHeader>
<bodyText confidence="0.999960857142857">
Jonson (2006) has shown that in addition to gener-
ating grammar-based language models, GF can be
used to build statistical language models (SLMs). It
was found that compared to our grammar-based ap-
proach, use of generated SLMs improved the recog-
nition performance for out-of-grammar utterances
significantly.
</bodyText>
<sectionHeader confidence="0.999807" genericHeader="evaluation">
8 Results
</sectionHeader>
<bodyText confidence="0.996036">
Speech recognition grammars generated from GF
grammars have already been used in a number of
research dialogue systems.
</bodyText>
<page confidence="0.998547">
6
</page>
<bodyText confidence="0.999613512820513">
GOTTIS (Bringert et al., 2005; Ericsson et al.,
2006), an experimental multimodal and multilingual
dialogue system for public transportation queries,
uses GF grammars for parsing multimodal input.
For speech recognition, it uses GSL grammars gen-
erated from the speech modality part of the GF
grammars.
DJ-GoDiS, GoDiS-deLUX, and GoTGoDiS (Er-
icsson et al., 2006) are three applications which use
GF grammars for speech recognition and parsing
together with the GoDiS implementation of issue-
based dialogue management (Larsson, 2002). GoT-
GoDiS has been translated to 7 languages using the
GF resource grammar library, with each new transla-
tion taking less than one day (Ericsson et al., 2006).
The DICO (Villing and Larsson, 2006) dialogue
system for trucks has recently been modified to
use GF grammars for speech recognition and pars-
ing (Ljungl¨of, 2007a).
DUDE (Lemon and Liu, 2006) and its extension
REALL-DUDE (Lemon et al., 2006b) are environ-
ments where non-experts can develop dialogue sys-
tems based on Business Process Models describing
the applications. From keywords, prompts and an-
swer sets defined by the developer, the system gen-
erates a GF grammar. This grammar is used for pars-
ing input, and for generating a language model in
SLF or GSL format.
The Voice Programming system by Georgila and
Lemon (Georgila and Lemon, 2006; Lemon et al.,
2006a) uses an SLF language model generated from
a GF grammar.
Perera and Ranta (2007) have studied how GF
grammars can be used for localization of dialogue
systems. A GF grammar was developed and local-
ized to 4 other languages in significantly less time
than an equivalent GSL grammar. They also found
the GSL grammar generated by GF to be much
smaller than the hand-written GSL grammar.
</bodyText>
<sectionHeader confidence="0.998511" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.99997976">
We have shown how GF grammars can be compiled
to several common speech recognition grammar for-
mats. This has helped decrease development time,
improve modifiability, aid localization and enable
portability in a number of experimental dialogue
systems.
Several systems developed in the TALK and
DICO projects use the same GF grammars for
speech recognition, parsing and multimodal fu-
sion (Ericsson et al., 2006). Using the same gram-
mar for multiple system components reduces devel-
opment and modification costs, and makes it easier
to maintain consistency within the system.
The feasibility of rapid localization of dialogue
systems which use GF grammars has been demon-
strated in the GoTGoDiS (Ericsson et al., 2006) sys-
tem, and in experiments by Perera and Ranta (2007).
Using speech recognition grammars generated by
GF makes it easy to support different speech rec-
ognizers. For example, by using the GF grammar
compiler, the DUDE (Lemon and Liu, 2006) system
can support both the ATK and Nuance recognizers.
Implementations of the methods described in this
paper are freely available as part of the GF distribu-
tion1.
</bodyText>
<sectionHeader confidence="0.998638" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.960466166666667">
Aarne Ranta, Peter Ljungl¨of, Rebecca Jonson,
David Hjelm, Ann-Charlotte Forslund, H˚akan Bur-
den, Xingkun Liu, Oliver Lemon, and the anony-
mous referees have contributed valuable comments
on the grammar compiler implementation and/or
this article. We would like to thank Nuance Com-
munications, Inc., OptimSys, s.r.o., and Opera Soft-
ware ASA for software licenses and technical sup-
port. The code in this paper has been typeset using
lhs2TeX, with help from Andres L¨oh. This work has
been partly funded by the EU TALK project, IST-
507802.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.983159090909091">
Johan Bos. 2002. Compilation of unification grammars
with compositional semantics to speech recognition
packages. In Proceedings of the 19th international
conference on Computational linguistics, pages 1–7,
Morristown, NJ, USA. Association for Computational
Linguistics.
Bj¨orn Bringert, Robin Cooper, Peter Ljungl¨of, and Aarne
Ranta. 2005. Multimodal Dialogue System Gram-
mars. In Proceedings of DIALOR’05, Ninth Workshop
on the Semantics and Pragmatics of Dialogue, pages
53–60.
</reference>
<footnote confidence="0.967289">
1http://www.cs.chalmers.se/—aarne/GF/
</footnote>
<page confidence="0.998629">
7
</page>
<reference confidence="0.999618544444445">
Janusz A. Brzozowski. 1962. Canonical regular expres-
sions and minimal state graphs for definite events. In
Mathematical theory of Automata, Volume 12 of MRI
Symposia Series, pages 529–561. Polytechnic Press,
Polytechnic Institute of Brooklyn, N.Y.
John Dowding, Beth A. Hockey, Jean M. Gawron, and
Christopher Culy. 2001. Practical issues in compil-
ing typed unification grammars for speech recognition.
In ACL ’01: Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics, pages
164–171, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Stina Ericsson, Gabriel Amores, Bj¨orn Bringert, H˚akan
Burden, Ann C. Forslund, David Hjelm, Rebecca Jon-
son, Staffan Larsson, Peter Ljungl¨of, Pilar Manch´on,
David Milward, Guillermo P´erez, and Mikael Sandin.
2006. Software illustrating a unified approach to mul-
timodality and multilinguality in the in-home domain.
Technical Report 1.6, TALK Project.
Kallirroi Georgila and Oliver Lemon. 2006. Program-
ming by Voice: enhancing adaptivity and robustness
of spoken dialogue systems. In BRANDIAL’06, Pro-
ceedings of the 10th Workshop on the Semantics and
Pragmatics of Dialogue, pages 199–200.
Rebecca Jonson. 2006. Generating Statistical Lan-
guage Models from Interpretation Grammars in Dia-
logue Systems. In Proceedings of EACL’06.
Staffan Larsson. 2002. Issue-based Dialogue Manage-
ment. Ph.D. thesis, G¨oteborg University.
Oliver Lemon and Xingkun Liu. 2006. DUDE: a
Dialogue and Understanding Development Environ-
ment, mapping Business Process Models to Informa-
tion State Update dialogue systems. In EACL 2006,
11st Conference of the European Chapter of the Asso-
ciation for Computational Linguistics.
Oliver Lemon, Kallirroi Georgila, David Milward, and
Tommy Herbert. 2006a. Programming Devices and
Services. Technical Report 2.3, TALK Project.
Oliver Lemon, Xingkun Liu, Daniel Shapiro, and Carl
Tollander. 2006b. Hierarchical Reinforcement Learn-
ing of Dialogue Policies in a development environment
for dialogue systems: REALL-DUDE. In BRAN-
DIAL’06, Proceedings of the 10th Workshop on the Se-
mantics and Pragmatics of Dialogue, pages 185–186,
September.
Peter Ljungl¨of. 2004. Expressivity and Complexity of
the Grammatical Framework. Ph.D. thesis, G¨oteborg
University, G¨oteborg, Sweden.
Peter Ljungl¨of. 2007a. Personal communication, March.
Peter Ljungl¨of. 2007b. Converting Grammatical Frame-
work to Regulus.
Mehryar Mohri and Mark J. Nederhof. 2001. Regu-
lar Approximation of Context-Free Grammars through
Transformation. In Jean C. Junqua and Gertjan van
Noord, editors, Robustness in Language and Speech
Technology, pages 153–163. Kluwer Academic Pub-
lishers, Dordrecht.
Robert C. Moore. 1999. Using Natural-Language
Knowledge Sources in Speech Recognition. In K. M.
Ponting, editor, Computational Models of Speech Pat-
tern Processing, pages 304–327. Springer.
Robert C. Moore. 2000. Removing left recursion from
context-free grammars. In Proceedings of the first
conference on North American chapter of the Associ-
ation for Computational Linguistics, pages 249–255,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Mark J. Nederhof. 2000. Regular Approximation of
CFLs: A Grammatical View. In Harry Bunt and Anton
Nijholt, editors, Advances in Probabilistic and other
Parsing Technologies, pages 221–241. Kluwer Aca-
demic Publishers.
Nadine Perera and Aarne Ranta. 2007. An Experiment in
Dialogue System Localization with the GF Resource
Grammar Library.
Aarne Ranta, Ali El Dada, and Janna Khegai. 2006. The
GF Resource Grammar Library, June.
Aarne Ranta. 2004. Grammatical Framework: A Type-
Theoretical Grammar Formalism. Journal of Func-
tional Programming, 14(2):145–189, March.
Manny Rayner, Beth A. Hockey, and Pierrette Bouil-
lon. 2006. Putting Linguistics into Speech Recogni-
tion: The Regulus Grammar Compiler. CSLI Publica-
tions, Ventura Hall, Stanford University, Stanford, CA
94305, USA, July.
Jessica Villing and Staffan Larsson. 2006. Dico: A
Multimodal Menu-based In-vehicle Dialogue System.
In BRANDIAL’06, Proceedings of the 10th Workshop
on the Semantics and Pragmatics of Dialogue, pages
187–188.
</reference>
<page confidence="0.998478">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.778768">
<title confidence="0.999913">Speech Recognition Grammar Compilation in Grammatical Framework</title>
<author confidence="0.998789">Bj¨orn</author>
<affiliation confidence="0.976189">Department of Computer Science and Chalmers University of Technology and G¨oteborg</affiliation>
<address confidence="0.848871">SE-412 96 G¨oteborg,</address>
<email confidence="0.981729">bringert@cs.chalmers.se</email>
<abstract confidence="0.996134615384615">This paper describes how grammar-based language models for speech recognition systems can be generated from Grammatical Framework (GF) grammars. Context-free grammars and finite-state models can be generated in several formats: GSL, SRGS, JSGF, and HTK SLF. In addition, semantic interpretation code can be embedded in the generated context-free grammars. This enables rapid development of portable, multilingual and easily modifiable speech recognition applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Compilation of unification grammars with compositional semantics to speech recognition packages.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="19618" citStr="Bos, 2002" startWordPosition="3094" endWordPosition="3095">ic interpretation for the original Aproduction, abstracted (in the X-calculus sense) over the semantic interpretation of the missing X. Conversely, where-ever a category A−X is used, its result is applied to the interpretation of the occurrence of X. 6.2 Discussion As discussed in Section 4.2, the semantic interpretation code could be used to implement the noncontext-free features of GF, but this is not yet done. The slot-filling mechanism in the GSL format could also be used to build semantic representations, by returning program code which can then be executed. The UNIANCE grammar compiler (Bos, 2002) uses that approach. 7 Related Work 7.1 Unification Grammar Compilation Compilation of unification grammars to speech recognition grammars is well described in the literature (Moore, 1999; Dowding et al., 2001). Regulus (Rayner et al., 2006) is perhaps the most ambitious such system. Like GF, Regulus uses a general grammar for each language, which is specialized to a domain-specific one. Ljungl¨of (Ljungl¨of, 2007b) relates GF and Regulus by showing how to convert GF grammars to Regulus grammars. We carry compositional semantic interpretation through left-recursion elimination using the same i</context>
</contexts>
<marker>Bos, 2002</marker>
<rawString>Johan Bos. 2002. Compilation of unification grammars with compositional semantics to speech recognition packages. In Proceedings of the 19th international conference on Computational linguistics, pages 1–7, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bj¨orn Bringert</author>
<author>Robin Cooper</author>
<author>Peter Ljungl¨of</author>
<author>Aarne Ranta</author>
</authors>
<title>Multimodal Dialogue System Grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of DIALOR’05, Ninth Workshop on the Semantics and Pragmatics of Dialogue,</booktitle>
<pages>53--60</pages>
<marker>Bringert, Cooper, Ljungl¨of, Ranta, 2005</marker>
<rawString>Bj¨orn Bringert, Robin Cooper, Peter Ljungl¨of, and Aarne Ranta. 2005. Multimodal Dialogue System Grammars. In Proceedings of DIALOR’05, Ninth Workshop on the Semantics and Pragmatics of Dialogue, pages 53–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janusz A Brzozowski</author>
</authors>
<title>Canonical regular expressions and minimal state graphs for definite events.</title>
<date>1962</date>
<booktitle>In Mathematical theory of Automata, Volume 12 of MRI Symposia Series,</booktitle>
<pages>529--561</pages>
<publisher>Polytechnic Press,</publisher>
<institution>Polytechnic Institute of Brooklyn, N.Y.</institution>
<marker>Brzozowski, 1962</marker>
<rawString>Janusz A. Brzozowski. 1962. Canonical regular expressions and minimal state graphs for definite events. In Mathematical theory of Automata, Volume 12 of MRI Symposia Series, pages 529–561. Polytechnic Press, Polytechnic Institute of Brooklyn, N.Y.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Dowding</author>
<author>Beth A Hockey</author>
<author>Jean M Gawron</author>
<author>Christopher Culy</author>
</authors>
<title>Practical issues in compiling typed unification grammars for speech recognition.</title>
<date>2001</date>
<booktitle>In ACL ’01: Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>164--171</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="19828" citStr="Dowding et al., 2001" startWordPosition="3123" endWordPosition="3126">plied to the interpretation of the occurrence of X. 6.2 Discussion As discussed in Section 4.2, the semantic interpretation code could be used to implement the noncontext-free features of GF, but this is not yet done. The slot-filling mechanism in the GSL format could also be used to build semantic representations, by returning program code which can then be executed. The UNIANCE grammar compiler (Bos, 2002) uses that approach. 7 Related Work 7.1 Unification Grammar Compilation Compilation of unification grammars to speech recognition grammars is well described in the literature (Moore, 1999; Dowding et al., 2001). Regulus (Rayner et al., 2006) is perhaps the most ambitious such system. Like GF, Regulus uses a general grammar for each language, which is specialized to a domain-specific one. Ljungl¨of (Ljungl¨of, 2007b) relates GF and Regulus by showing how to convert GF grammars to Regulus grammars. We carry compositional semantic interpretation through left-recursion elimination using the same idea as the UNIANCE grammar compiler (Bos, 2002), though our version handles both direct and indirect left-recursion. The main difference between our work and the existing compilers is that we work with typetheo</context>
</contexts>
<marker>Dowding, Hockey, Gawron, Culy, 2001</marker>
<rawString>John Dowding, Beth A. Hockey, Jean M. Gawron, and Christopher Culy. 2001. Practical issues in compiling typed unification grammars for speech recognition. In ACL ’01: Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pages 164–171, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stina Ericsson</author>
<author>Gabriel Amores</author>
<author>Bj¨orn Bringert</author>
<author>H˚akan Burden</author>
<author>Ann C Forslund</author>
<author>David Hjelm</author>
<author>Rebecca Jonson</author>
<author>Staffan Larsson</author>
<author>Peter Ljungl¨of</author>
<author>Pilar Manch´on</author>
<author>David Milward</author>
<author>Guillermo P´erez</author>
<author>Mikael Sandin</author>
</authors>
<title>Software illustrating a unified approach to multimodality and multilinguality in the in-home domain.</title>
<date>2006</date>
<tech>Technical Report 1.6, TALK Project.</tech>
<marker>Ericsson, Amores, Bringert, Burden, Forslund, Hjelm, Jonson, Larsson, Ljungl¨of, Manch´on, Milward, P´erez, Sandin, 2006</marker>
<rawString>Stina Ericsson, Gabriel Amores, Bj¨orn Bringert, H˚akan Burden, Ann C. Forslund, David Hjelm, Rebecca Jonson, Staffan Larsson, Peter Ljungl¨of, Pilar Manch´on, David Milward, Guillermo P´erez, and Mikael Sandin. 2006. Software illustrating a unified approach to multimodality and multilinguality in the in-home domain. Technical Report 1.6, TALK Project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kallirroi Georgila</author>
<author>Oliver Lemon</author>
</authors>
<title>Programming by Voice: enhancing adaptivity and robustness of spoken dialogue systems.</title>
<date>2006</date>
<booktitle>In BRANDIAL’06, Proceedings of the 10th Workshop on the Semantics and Pragmatics of Dialogue,</booktitle>
<pages>199--200</pages>
<contexts>
<context position="22650" citStr="Georgila and Lemon, 2006" startWordPosition="3570" endWordPosition="3573">nd Larsson, 2006) dialogue system for trucks has recently been modified to use GF grammars for speech recognition and parsing (Ljungl¨of, 2007a). DUDE (Lemon and Liu, 2006) and its extension REALL-DUDE (Lemon et al., 2006b) are environments where non-experts can develop dialogue systems based on Business Process Models describing the applications. From keywords, prompts and answer sets defined by the developer, the system generates a GF grammar. This grammar is used for parsing input, and for generating a language model in SLF or GSL format. The Voice Programming system by Georgila and Lemon (Georgila and Lemon, 2006; Lemon et al., 2006a) uses an SLF language model generated from a GF grammar. Perera and Ranta (2007) have studied how GF grammars can be used for localization of dialogue systems. A GF grammar was developed and localized to 4 other languages in significantly less time than an equivalent GSL grammar. They also found the GSL grammar generated by GF to be much smaller than the hand-written GSL grammar. 9 Conclusions We have shown how GF grammars can be compiled to several common speech recognition grammar formats. This has helped decrease development time, improve modifiability, aid localizatio</context>
</contexts>
<marker>Georgila, Lemon, 2006</marker>
<rawString>Kallirroi Georgila and Oliver Lemon. 2006. Programming by Voice: enhancing adaptivity and robustness of spoken dialogue systems. In BRANDIAL’06, Proceedings of the 10th Workshop on the Semantics and Pragmatics of Dialogue, pages 199–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Jonson</author>
</authors>
<title>Generating Statistical Language Models from Interpretation Grammars in Dialogue Systems.</title>
<date>2006</date>
<booktitle>In Proceedings of EACL’06.</booktitle>
<contexts>
<context position="20889" citStr="Jonson (2006)" startWordPosition="3295" endWordPosition="3296">r version handles both direct and indirect left-recursion. The main difference between our work and the existing compilers is that we work with typetheoretical grammars rather than unification grammars. While the existing work focuses on GSL as the output language, we also support a number of other formats, including finite-state models. By using the GF resource grammars, speech recognition language models can be produced for more languages than with previous systems. One shortcoming of our system is that it does not yet have support for weighted grammars. 7.2 Generating SLMs from GF Grammars Jonson (2006) has shown that in addition to generating grammar-based language models, GF can be used to build statistical language models (SLMs). It was found that compared to our grammar-based approach, use of generated SLMs improved the recognition performance for out-of-grammar utterances significantly. 8 Results Speech recognition grammars generated from GF grammars have already been used in a number of research dialogue systems. 6 GOTTIS (Bringert et al., 2005; Ericsson et al., 2006), an experimental multimodal and multilingual dialogue system for public transportation queries, uses GF grammars for pa</context>
</contexts>
<marker>Jonson, 2006</marker>
<rawString>Rebecca Jonson. 2006. Generating Statistical Language Models from Interpretation Grammars in Dialogue Systems. In Proceedings of EACL’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Staffan Larsson</author>
</authors>
<title>Issue-based Dialogue Management.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>G¨oteborg University.</institution>
<contexts>
<context position="21846" citStr="Larsson, 2002" startWordPosition="3438" endWordPosition="3439">from GF grammars have already been used in a number of research dialogue systems. 6 GOTTIS (Bringert et al., 2005; Ericsson et al., 2006), an experimental multimodal and multilingual dialogue system for public transportation queries, uses GF grammars for parsing multimodal input. For speech recognition, it uses GSL grammars generated from the speech modality part of the GF grammars. DJ-GoDiS, GoDiS-deLUX, and GoTGoDiS (Ericsson et al., 2006) are three applications which use GF grammars for speech recognition and parsing together with the GoDiS implementation of issuebased dialogue management (Larsson, 2002). GoTGoDiS has been translated to 7 languages using the GF resource grammar library, with each new translation taking less than one day (Ericsson et al., 2006). The DICO (Villing and Larsson, 2006) dialogue system for trucks has recently been modified to use GF grammars for speech recognition and parsing (Ljungl¨of, 2007a). DUDE (Lemon and Liu, 2006) and its extension REALL-DUDE (Lemon et al., 2006b) are environments where non-experts can develop dialogue systems based on Business Process Models describing the applications. From keywords, prompts and answer sets defined by the developer, the s</context>
</contexts>
<marker>Larsson, 2002</marker>
<rawString>Staffan Larsson. 2002. Issue-based Dialogue Management. Ph.D. thesis, G¨oteborg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
<author>Xingkun Liu</author>
</authors>
<title>DUDE: a Dialogue and Understanding Development Environment, mapping Business Process Models to Information State Update dialogue systems.</title>
<date>2006</date>
<booktitle>In EACL 2006, 11st Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="22198" citStr="Lemon and Liu, 2006" startWordPosition="3495" endWordPosition="3498">peech modality part of the GF grammars. DJ-GoDiS, GoDiS-deLUX, and GoTGoDiS (Ericsson et al., 2006) are three applications which use GF grammars for speech recognition and parsing together with the GoDiS implementation of issuebased dialogue management (Larsson, 2002). GoTGoDiS has been translated to 7 languages using the GF resource grammar library, with each new translation taking less than one day (Ericsson et al., 2006). The DICO (Villing and Larsson, 2006) dialogue system for trucks has recently been modified to use GF grammars for speech recognition and parsing (Ljungl¨of, 2007a). DUDE (Lemon and Liu, 2006) and its extension REALL-DUDE (Lemon et al., 2006b) are environments where non-experts can develop dialogue systems based on Business Process Models describing the applications. From keywords, prompts and answer sets defined by the developer, the system generates a GF grammar. This grammar is used for parsing input, and for generating a language model in SLF or GSL format. The Voice Programming system by Georgila and Lemon (Georgila and Lemon, 2006; Lemon et al., 2006a) uses an SLF language model generated from a GF grammar. Perera and Ranta (2007) have studied how GF grammars can be used for </context>
<context position="24021" citStr="Lemon and Liu, 2006" startWordPosition="3793" endWordPosition="3796"> for speech recognition, parsing and multimodal fusion (Ericsson et al., 2006). Using the same grammar for multiple system components reduces development and modification costs, and makes it easier to maintain consistency within the system. The feasibility of rapid localization of dialogue systems which use GF grammars has been demonstrated in the GoTGoDiS (Ericsson et al., 2006) system, and in experiments by Perera and Ranta (2007). Using speech recognition grammars generated by GF makes it easy to support different speech recognizers. For example, by using the GF grammar compiler, the DUDE (Lemon and Liu, 2006) system can support both the ATK and Nuance recognizers. Implementations of the methods described in this paper are freely available as part of the GF distribution1. Acknowledgments Aarne Ranta, Peter Ljungl¨of, Rebecca Jonson, David Hjelm, Ann-Charlotte Forslund, H˚akan Burden, Xingkun Liu, Oliver Lemon, and the anonymous referees have contributed valuable comments on the grammar compiler implementation and/or this article. We would like to thank Nuance Communications, Inc., OptimSys, s.r.o., and Opera Software ASA for software licenses and technical support. The code in this paper has been t</context>
</contexts>
<marker>Lemon, Liu, 2006</marker>
<rawString>Oliver Lemon and Xingkun Liu. 2006. DUDE: a Dialogue and Understanding Development Environment, mapping Business Process Models to Information State Update dialogue systems. In EACL 2006, 11st Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
<author>Kallirroi Georgila</author>
<author>David Milward</author>
<author>Tommy Herbert</author>
</authors>
<title>Programming Devices and Services.</title>
<date>2006</date>
<tech>Technical Report 2.3, TALK Project.</tech>
<contexts>
<context position="22247" citStr="Lemon et al., 2006" startWordPosition="3503" endWordPosition="3506">oDiS-deLUX, and GoTGoDiS (Ericsson et al., 2006) are three applications which use GF grammars for speech recognition and parsing together with the GoDiS implementation of issuebased dialogue management (Larsson, 2002). GoTGoDiS has been translated to 7 languages using the GF resource grammar library, with each new translation taking less than one day (Ericsson et al., 2006). The DICO (Villing and Larsson, 2006) dialogue system for trucks has recently been modified to use GF grammars for speech recognition and parsing (Ljungl¨of, 2007a). DUDE (Lemon and Liu, 2006) and its extension REALL-DUDE (Lemon et al., 2006b) are environments where non-experts can develop dialogue systems based on Business Process Models describing the applications. From keywords, prompts and answer sets defined by the developer, the system generates a GF grammar. This grammar is used for parsing input, and for generating a language model in SLF or GSL format. The Voice Programming system by Georgila and Lemon (Georgila and Lemon, 2006; Lemon et al., 2006a) uses an SLF language model generated from a GF grammar. Perera and Ranta (2007) have studied how GF grammars can be used for localization of dialogue systems. A GF grammar wa</context>
</contexts>
<marker>Lemon, Georgila, Milward, Herbert, 2006</marker>
<rawString>Oliver Lemon, Kallirroi Georgila, David Milward, and Tommy Herbert. 2006a. Programming Devices and Services. Technical Report 2.3, TALK Project.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Lemon</author>
<author>Xingkun Liu</author>
<author>Daniel Shapiro</author>
<author>Carl Tollander</author>
</authors>
<title>Hierarchical Reinforcement Learning of Dialogue Policies in a development environment for dialogue systems: REALL-DUDE.</title>
<date>2006</date>
<booktitle>In BRANDIAL’06, Proceedings of the 10th Workshop on the Semantics and Pragmatics of Dialogue,</booktitle>
<pages>185--186</pages>
<contexts>
<context position="22247" citStr="Lemon et al., 2006" startWordPosition="3503" endWordPosition="3506">oDiS-deLUX, and GoTGoDiS (Ericsson et al., 2006) are three applications which use GF grammars for speech recognition and parsing together with the GoDiS implementation of issuebased dialogue management (Larsson, 2002). GoTGoDiS has been translated to 7 languages using the GF resource grammar library, with each new translation taking less than one day (Ericsson et al., 2006). The DICO (Villing and Larsson, 2006) dialogue system for trucks has recently been modified to use GF grammars for speech recognition and parsing (Ljungl¨of, 2007a). DUDE (Lemon and Liu, 2006) and its extension REALL-DUDE (Lemon et al., 2006b) are environments where non-experts can develop dialogue systems based on Business Process Models describing the applications. From keywords, prompts and answer sets defined by the developer, the system generates a GF grammar. This grammar is used for parsing input, and for generating a language model in SLF or GSL format. The Voice Programming system by Georgila and Lemon (Georgila and Lemon, 2006; Lemon et al., 2006a) uses an SLF language model generated from a GF grammar. Perera and Ranta (2007) have studied how GF grammars can be used for localization of dialogue systems. A GF grammar wa</context>
</contexts>
<marker>Lemon, Liu, Shapiro, Tollander, 2006</marker>
<rawString>Oliver Lemon, Xingkun Liu, Daniel Shapiro, and Carl Tollander. 2006b. Hierarchical Reinforcement Learning of Dialogue Policies in a development environment for dialogue systems: REALL-DUDE. In BRANDIAL’06, Proceedings of the 10th Workshop on the Semantics and Pragmatics of Dialogue, pages 185–186, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Ljungl¨of</author>
</authors>
<title>Expressivity and Complexity of the Grammatical Framework.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>G¨oteborg University, G¨oteborg, Sweden.</institution>
<marker>Ljungl¨of, 2004</marker>
<rawString>Peter Ljungl¨of. 2004. Expressivity and Complexity of the Grammatical Framework. Ph.D. thesis, G¨oteborg University, G¨oteborg, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Ljungl¨of</author>
</authors>
<date>2007</date>
<tech>Personal communication,</tech>
<marker>Ljungl¨of, 2007</marker>
<rawString>Peter Ljungl¨of. 2007a. Personal communication, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Ljungl¨of</author>
</authors>
<title>Converting Grammatical Framework to Regulus.</title>
<date>2007</date>
<marker>Ljungl¨of, 2007</marker>
<rawString>Peter Ljungl¨of. 2007b. Converting Grammatical Framework to Regulus.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Mark J Nederhof</author>
</authors>
<title>Regular Approximation of Context-Free Grammars through Transformation.</title>
<date>2001</date>
<booktitle>Robustness in Language and Speech Technology,</booktitle>
<pages>153--163</pages>
<editor>In Jean C. Junqua and Gertjan van Noord, editors,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="15428" citStr="Mohri and Nederhof (2001)" startWordPosition="2411" endWordPosition="2415">mediate grammars can cause memory problems. Further optimization is needed to address this problem. 5 Finite-State Models 5.1 Algorithm Some speech recognition systems use finite-state automata rather than context-free grammars as language models. GF grammars can be compiled to finite-state automata using the procedure shown in Figure 3. The initial part of the compilation to a finite-state model is shared with the context-free SRG compilation, and is described in Section 4. Regular approximation The context-free grammar is approximated with a regular grammar, using the algorithm described by Mohri and Nederhof (2001). Compilation to finite-state automata The regular grammar is transformed into a set of nondeterministic finite automata (NFA) using a modified version of the make fa algorithm described by Nederhof (2000). For realistic grammars, applying the original make fa algorithm to the whole grammar generates a very large automaton, since a copy of the sub-automaton corresponding to a given category is made for every use of the category. Instead, one automaton is generated for each category in the regular grammar. All categories which are not in the same mutually recursive set as the category for which</context>
</contexts>
<marker>Mohri, Nederhof, 2001</marker>
<rawString>Mehryar Mohri and Mark J. Nederhof. 2001. Regular Approximation of Context-Free Grammars through Transformation. In Jean C. Junqua and Gertjan van Noord, editors, Robustness in Language and Speech Technology, pages 153–163. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Using Natural-Language Knowledge Sources in Speech Recognition. In</title>
<date>1999</date>
<booktitle>Computational Models of Speech Pattern Processing,</booktitle>
<pages>304--327</pages>
<editor>K. M. Ponting, editor,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="19805" citStr="Moore, 1999" startWordPosition="3121" endWordPosition="3122"> result is applied to the interpretation of the occurrence of X. 6.2 Discussion As discussed in Section 4.2, the semantic interpretation code could be used to implement the noncontext-free features of GF, but this is not yet done. The slot-filling mechanism in the GSL format could also be used to build semantic representations, by returning program code which can then be executed. The UNIANCE grammar compiler (Bos, 2002) uses that approach. 7 Related Work 7.1 Unification Grammar Compilation Compilation of unification grammars to speech recognition grammars is well described in the literature (Moore, 1999; Dowding et al., 2001). Regulus (Rayner et al., 2006) is perhaps the most ambitious such system. Like GF, Regulus uses a general grammar for each language, which is specialized to a domain-specific one. Ljungl¨of (Ljungl¨of, 2007b) relates GF and Regulus by showing how to convert GF grammars to Regulus grammars. We carry compositional semantic interpretation through left-recursion elimination using the same idea as the UNIANCE grammar compiler (Bos, 2002), though our version handles both direct and indirect left-recursion. The main difference between our work and the existing compilers is tha</context>
</contexts>
<marker>Moore, 1999</marker>
<rawString>Robert C. Moore. 1999. Using Natural-Language Knowledge Sources in Speech Recognition. In K. M. Ponting, editor, Computational Models of Speech Pattern Processing, pages 304–327. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>Removing left recursion from context-free grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics,</booktitle>
<pages>249--255</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="11187" citStr="Moore (2000)" startWordPosition="1749" endWordPosition="1750">In this case, the start category is different from the start category used by the parser, in that its linearization only contains the speech component of the in3 GF grammar Figure 3: Grammar compilation pipeline. put. Top-down filtering then has the effect of excluding the non-speech modalities from the speech recognition grammar. The bottom-up and top-down filtering steps are iterated until a fixed point is reached, since both these steps may produce new filtering opportunities. Left-recursion elimination All direct and indirect left-recursion is removed using the LCLR transform described by Moore (2000). We have modified the LCLR transform to avoid adding productions which use a category A−X when there are no productions for A−X. Identical category elimination In this step, the categories are grouped into equivalence classes by their right-hand sides and semantic annotations. The categories A1 ...An in each class are replaced by a single category A1+...+An throughout the grammar, discarding any duplicate productions. This has the effect of replacing all categories which have identical sets of productions with a single category. Concrete syntax parameters which do not affect inflection is one</context>
</contexts>
<marker>Moore, 2000</marker>
<rawString>Robert C. Moore. 2000. Removing left recursion from context-free grammars. In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, pages 249–255, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark J Nederhof</author>
</authors>
<title>Regular Approximation of CFLs: A Grammatical View.</title>
<date>2000</date>
<booktitle>Advances in Probabilistic and other Parsing Technologies,</booktitle>
<pages>221--241</pages>
<editor>In Harry Bunt and Anton Nijholt, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="15633" citStr="Nederhof (2000)" startWordPosition="2447" endWordPosition="2448">ree grammars as language models. GF grammars can be compiled to finite-state automata using the procedure shown in Figure 3. The initial part of the compilation to a finite-state model is shared with the context-free SRG compilation, and is described in Section 4. Regular approximation The context-free grammar is approximated with a regular grammar, using the algorithm described by Mohri and Nederhof (2001). Compilation to finite-state automata The regular grammar is transformed into a set of nondeterministic finite automata (NFA) using a modified version of the make fa algorithm described by Nederhof (2000). For realistic grammars, applying the original make fa algorithm to the whole grammar generates a very large automaton, since a copy of the sub-automaton corresponding to a given category is made for every use of the category. Instead, one automaton is generated for each category in the regular grammar. All categories which are not in the same mutually recursive set as the category for which the automaton is generated are treated as terminal symbols. This results in a set of automata with edges labeled with either terminal symbols or the names of other automata. If desired, the set of automat</context>
</contexts>
<marker>Nederhof, 2000</marker>
<rawString>Mark J. Nederhof. 2000. Regular Approximation of CFLs: A Grammatical View. In Harry Bunt and Anton Nijholt, editors, Advances in Probabilistic and other Parsing Technologies, pages 221–241. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadine Perera</author>
<author>Aarne Ranta</author>
</authors>
<date>2007</date>
<booktitle>An Experiment in Dialogue System Localization with the GF Resource Grammar Library.</booktitle>
<contexts>
<context position="22752" citStr="Perera and Ranta (2007)" startWordPosition="3588" endWordPosition="3591">ecognition and parsing (Ljungl¨of, 2007a). DUDE (Lemon and Liu, 2006) and its extension REALL-DUDE (Lemon et al., 2006b) are environments where non-experts can develop dialogue systems based on Business Process Models describing the applications. From keywords, prompts and answer sets defined by the developer, the system generates a GF grammar. This grammar is used for parsing input, and for generating a language model in SLF or GSL format. The Voice Programming system by Georgila and Lemon (Georgila and Lemon, 2006; Lemon et al., 2006a) uses an SLF language model generated from a GF grammar. Perera and Ranta (2007) have studied how GF grammars can be used for localization of dialogue systems. A GF grammar was developed and localized to 4 other languages in significantly less time than an equivalent GSL grammar. They also found the GSL grammar generated by GF to be much smaller than the hand-written GSL grammar. 9 Conclusions We have shown how GF grammars can be compiled to several common speech recognition grammar formats. This has helped decrease development time, improve modifiability, aid localization and enable portability in a number of experimental dialogue systems. Several systems developed in th</context>
</contexts>
<marker>Perera, Ranta, 2007</marker>
<rawString>Nadine Perera and Aarne Ranta. 2007. An Experiment in Dialogue System Localization with the GF Resource Grammar Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aarne Ranta</author>
<author>Ali El Dada</author>
<author>Janna Khegai</author>
</authors>
<date>2006</date>
<booktitle>The GF Resource Grammar Library,</booktitle>
<marker>Ranta, El Dada, Khegai, 2006</marker>
<rawString>Aarne Ranta, Ali El Dada, and Janna Khegai. 2006. The GF Resource Grammar Library, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aarne Ranta</author>
</authors>
<title>Grammatical Framework: A TypeTheoretical Grammar Formalism.</title>
<date>2004</date>
<journal>Journal of Functional Programming,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="5082" citStr="Ranta, 2004" startWordPosition="742" endWordPosition="743">uage model which defines the language which can be recognized. A language model may also assign different probabilities to different strings in the language. A language model can either be a statistical language model (SLM), such as an n-gram model, or a grammarbased language model, for example a context-free grammar (CFG) or a finite-state automaton (FSA). In this paper, we use the term speech recognition grammar (SRG) to refer to all grammar-based language models, including context-free grammars, regular grammars and finite-state automata. 3 Grammatical Framework Grammatical Framework (GF) (Ranta, 2004) is a grammar formalism based on constructive type theory. In GF, an abstract syntax defines a semantic representation. A concrete syntax declares how terms in an abstract syntax are linearized, that is, how they are mapped to concrete representations. GF grammars can be made multilingual by having multiple concrete syntaxes for a single abstract syntax. 3.1 The Resource Grammar Library The GF Resource Grammar Library (Ranta et al., 2006) currently implements the morphological and syntactic details of 10 languages. This library is intended to make it possible to write grammars without caring a</context>
</contexts>
<marker>Ranta, 2004</marker>
<rawString>Aarne Ranta. 2004. Grammatical Framework: A TypeTheoretical Grammar Formalism. Journal of Functional Programming, 14(2):145–189, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manny Rayner</author>
<author>Beth A Hockey</author>
<author>Pierrette Bouillon</author>
</authors>
<title>Putting Linguistics into Speech Recognition: The Regulus Grammar Compiler.</title>
<date>2006</date>
<publisher>CSLI Publications,</publisher>
<location>Ventura Hall, Stanford University, Stanford, CA 94305, USA,</location>
<contexts>
<context position="1282" citStr="Rayner et al., 2006" startWordPosition="172" endWordPosition="175">d easily modifiable speech recognition applications. 1 Introduction Speech recognition grammars are used for guiding speech recognizers in many applications. However, there are a number of problems associated with writing grammars in the low-level, systemspecific formats required by speech recognizers. This work addresses these problems by generating speech recognition grammars and semantic interpretation components from grammars written in Grammatical Framework (GF), a high-level, typetheoretical grammar formalism. Compared to existing work on compiling unification grammars, such as Regulus (Rayner et al., 2006), our work uses a type-theoretical grammar formalism with a focus on multilinguality and modular grammar development, and supports multiple speech recognition grammar formalisms, including finite-state models. We first outline some existing problems in the development and maintenance of speech recognition grammars, and describe how our work attempts to address these problems. In the following two sections we introduce speech recognition grammars and Grammatical Framework. The bulk of the paper then describes how we generate context-free speech recognition grammars, finite-state language models</context>
<context position="19859" citStr="Rayner et al., 2006" startWordPosition="3129" endWordPosition="3132">he occurrence of X. 6.2 Discussion As discussed in Section 4.2, the semantic interpretation code could be used to implement the noncontext-free features of GF, but this is not yet done. The slot-filling mechanism in the GSL format could also be used to build semantic representations, by returning program code which can then be executed. The UNIANCE grammar compiler (Bos, 2002) uses that approach. 7 Related Work 7.1 Unification Grammar Compilation Compilation of unification grammars to speech recognition grammars is well described in the literature (Moore, 1999; Dowding et al., 2001). Regulus (Rayner et al., 2006) is perhaps the most ambitious such system. Like GF, Regulus uses a general grammar for each language, which is specialized to a domain-specific one. Ljungl¨of (Ljungl¨of, 2007b) relates GF and Regulus by showing how to convert GF grammars to Regulus grammars. We carry compositional semantic interpretation through left-recursion elimination using the same idea as the UNIANCE grammar compiler (Bos, 2002), though our version handles both direct and indirect left-recursion. The main difference between our work and the existing compilers is that we work with typetheoretical grammars rather than un</context>
</contexts>
<marker>Rayner, Hockey, Bouillon, 2006</marker>
<rawString>Manny Rayner, Beth A. Hockey, and Pierrette Bouillon. 2006. Putting Linguistics into Speech Recognition: The Regulus Grammar Compiler. CSLI Publications, Ventura Hall, Stanford University, Stanford, CA 94305, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jessica Villing</author>
<author>Staffan Larsson</author>
</authors>
<title>Dico: A Multimodal Menu-based In-vehicle Dialogue System. In</title>
<date>2006</date>
<booktitle>BRANDIAL’06, Proceedings of the 10th Workshop on the Semantics and Pragmatics of Dialogue,</booktitle>
<pages>187--188</pages>
<contexts>
<context position="22043" citStr="Villing and Larsson, 2006" startWordPosition="3470" endWordPosition="3473">alogue system for public transportation queries, uses GF grammars for parsing multimodal input. For speech recognition, it uses GSL grammars generated from the speech modality part of the GF grammars. DJ-GoDiS, GoDiS-deLUX, and GoTGoDiS (Ericsson et al., 2006) are three applications which use GF grammars for speech recognition and parsing together with the GoDiS implementation of issuebased dialogue management (Larsson, 2002). GoTGoDiS has been translated to 7 languages using the GF resource grammar library, with each new translation taking less than one day (Ericsson et al., 2006). The DICO (Villing and Larsson, 2006) dialogue system for trucks has recently been modified to use GF grammars for speech recognition and parsing (Ljungl¨of, 2007a). DUDE (Lemon and Liu, 2006) and its extension REALL-DUDE (Lemon et al., 2006b) are environments where non-experts can develop dialogue systems based on Business Process Models describing the applications. From keywords, prompts and answer sets defined by the developer, the system generates a GF grammar. This grammar is used for parsing input, and for generating a language model in SLF or GSL format. The Voice Programming system by Georgila and Lemon (Georgila and Lemo</context>
</contexts>
<marker>Villing, Larsson, 2006</marker>
<rawString>Jessica Villing and Staffan Larsson. 2006. Dico: A Multimodal Menu-based In-vehicle Dialogue System. In BRANDIAL’06, Proceedings of the 10th Workshop on the Semantics and Pragmatics of Dialogue, pages 187–188.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>