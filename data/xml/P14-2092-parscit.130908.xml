<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001890">
<title confidence="0.997918">
A Hybrid Approach to Skeleton-based Translation
</title>
<author confidence="0.979846">
Tong Xiao†‡, Jingbo Zhu†‡, Chunliang Zhang†‡† Northeastern University, Shenyang 110819, China
</author>
<affiliation confidence="0.887785">
‡ Hangzhou YaTuo Company, 358 Wener Rd., Hangzhou 310012, China
</affiliation>
<email confidence="0.988362">
{xiaotong,zhujingbo,zhangcl}@mail.neu.edu.cn
</email>
<sectionHeader confidence="0.993691" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998985166666667">
In this paper we explicitly consider sen-
tence skeleton information for Machine
Translation (MT). The basic idea is that
we translate the key elements of the input
sentence using a skeleton translation mod-
el, and then cover the remain segments us-
ing a full translation model. We apply our
approach to a state-of-the-art phrase-based
system and demonstrate very promising
BLEU improvements and TER reductions
on the NIST Chinese-English MT evalua-
tion data.
</bodyText>
<sectionHeader confidence="0.998679" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.992817931818182">
Current Statistical Machine Translation (SMT) ap-
proaches model the translation problem as a pro-
cess of generating a derivation of atomic transla-
tion units, assuming that every unit is drawn out
of the same model. The simplest of these is the
phrase-based approach (Och et al., 1999; Koehn
et al., 2003) which employs a global model to
process any sub-strings of the input sentence. In
this way, all we need is to increasingly translate
a sequence of source words each time until the
entire sentence is covered. Despite good result-
s in many tasks, such a method ignores the roles
of each source word and is somewhat differen-
t from the way used by translators. For exam-
ple, an important-first strategy is generally adopt-
ed in human translation - we translate the key ele-
ments/structures (or skeleton) of the sentence first,
and then translate the remaining parts. This es-
pecially makes sense for some languages, such as
Chinese, where complex structures are usually in-
volved.
Note that the source-language structural infor-
mation has been intensively investigated in recent
studies of syntactic translation models. Some of
them developed syntax-based models on complete
syntactic trees with Treebank annotations (Liu et
al., 2006; Huang et al., 2006; Zhang et al., 2008),
and others used source-language syntax as soft
constraints (Marton and Resnik, 2008; Chiang,
2010). However, these approaches suffer from
the same problem as the phrase-based counterpart
and use the single global model to handle differ-
ent translation units, no matter they are from the
skeleton of the input tree/sentence or other not-so-
important sub-structures.
In this paper we instead explicitly model the
translation problem with sentence skeleton infor-
mation. In particular,
• We develop a skeleton-based model which
divides translation into two sub-models: a
skeleton translation model (i.e., translating
the key elements) and a full translation model
(i.e., translating the remaining source words
and generating the complete translation).
</bodyText>
<listItem confidence="0.5980235">
• We develop a skeletal language model to de-
scribe the possibility of translation skeleton
and handle some of the long-distance word
dependencies.
• We apply the proposed model to Chinese-
English phrase-based MT and demonstrate
promising BLEU improvements and TER re-
ductions on the NIST evaluation data.
</listItem>
<sectionHeader confidence="0.837393" genericHeader="introduction">
2 A Skeleton-based Approach to NIT
</sectionHeader>
<subsectionHeader confidence="0.985214">
2.1 Skeleton Identification
</subsectionHeader>
<bodyText confidence="0.9986338">
The first issue that arises is how to identify the
skeleton for a given source sentence. Many ways
are available. E.g., we can start with a full syntac-
tic tree and transform it into a simpler form (e.g.,
removing a sub-tree). Here we choose a simple
and straightforward method: a skeleton is obtained
by dropping all unimportant words in the origi-
nal sentence, while preserving the grammaticali-
ty. See the following for an example skeleton of a
Chinese sentence.
</bodyText>
<page confidence="0.980075">
563
</page>
<bodyText confidence="0.563845333333333">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 563–568,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
Original Sentence (subscripts represent indices):
</bodyText>
<equation confidence="0.6589176">
*[1] n[2] *1Aft[3]
per ton seawater desalination
)Y*[6] 4E[7] 5[g] 7�[9] M[10]
the cost 5 yuan of
A_y[13] TR[14] o[15]
</equation>
<bodyText confidence="0.7024055">
has been further reduced .
(The cost of seawater desalination treatment has
been further reduced from 5 yuan per ton.)
Sentence Skeleton (subscripts represent indices):
</bodyText>
<equation confidence="0.681918">
)Y*[6] ��y[13] TR[14] o[15]
</equation>
<bodyText confidence="0.98186365">
the cost has been further reduced .
(The cost has been further reduced.)
Obviously the skeleton used in this work can be
viewed as a simplified sentence. Thus the prob-
lem is in principle the same as sentence simpli-
fication/compression. The motivations of defin-
ing the problem in this way are two-fold. First,
as the skeleton is a well-formed (but simple) sen-
tence, all current MT approaches are applicable
to the skeleton translation problem. Second, ob-
taining simplified sentences by word deletion is
a well-studied issue (Knight and Marcu, 2000;
Clarke and Lapata, 2006; Galley and McKeown,
2007; Cohn and Lapata, 2008; Yamangil and
Shieber, 2010; Yoshikawa et al., 2012). Many
good sentence simpliciation/compression methods
are available to our work. Due to the lack of space,
we do not go deep into this problem. In Section
3.1 we describe the corpus and system employed
for automatic generation of sentence skeletons.
</bodyText>
<subsectionHeader confidence="0.997356">
2.2 Base Model
</subsectionHeader>
<bodyText confidence="0.999972428571429">
Next we describe our approach to integrating
skeleton information into MT models. We start
with an assumption that the 1-best skeleton is pro-
vided by the skeleton identification system. Then
we define skeleton-based translation as a task of
searching for the best target string tˆ given the
source string and its skeleton τ:
</bodyText>
<equation confidence="0.999799">
tˆ = arg max
t P(t|τ, s) (1)
</equation>
<bodyText confidence="0.994502">
As is standard in SMT, we further assume that
1) the translation process can be decomposed in-
to a derivation of phrase-pairs (for phrase-based
models) or translation rules (for syntax-based
models); 2) and a linear function g(·) is used to
assign a model score to each derivation. Let ds,r,t
(or d for short) denote a translation derivation. The
above problem can be redefined in a Viterbi fash-
ion - we find the derivation dˆ with the highest mod-
el score given s and τ:
</bodyText>
<equation confidence="0.8777665">
dˆ = arg max g(d) (2)
d
</equation>
<bodyText confidence="0.9925498">
In this way, the MT output can be regarded as the
target-string encoded in ˆd.
To compute g(d), we use a linear combination
of a skeleton translation model gskel(d) and a full
translation model gfull(d):
</bodyText>
<equation confidence="0.999415">
g(d) = gskel(d) + gfull(d) (3)
</equation>
<bodyText confidence="0.999994296296296">
where the skeleton translation model handles the
translation of the sentence skeleton, while the full
translation model is the baseline model and han-
dles the original problem of translating the whole
sentence. The motivation here is straightforward:
we use an additional score gskel(d) to model the
problem of skeleton translation and interpolate it
with the baseline model. See Figure 1 for an exam-
ple of applying the above model to phrase-based
MT. In the figure, each source phrase is translated
into a target phrase, which is represented by linked
rectangles. The skeleton translation model focus-
es on the translation of the sentence skeleton, i.e.,
the solid (red) rectangles; while the full transla-
tion model computes the model score for all those
phrase-pairs, i.e., all solid and dashed rectangles.
Another note on the model. Eq. (3) provides a
very flexible way for model selection. While we
will restrict ourself to phrase-based translation in
the following description and experiments, we can
choose different models/features for gskel(d) and
gfull(d). E.g., one may introduce syntactic fea-
tures into gskel(d) due to their good ability in cap-
turing structural information; and employ a stan-
dard phrase-based model for gfull(d) in which not
all segments of the sentence need to respect syn-
tactic constraints.
</bodyText>
<subsectionHeader confidence="0.999655">
2.3 Model Score Computation
</subsectionHeader>
<bodyText confidence="0.9999698">
In this work both the skeleton translation model
gskel(d) and full translation model gfull(d) resem-
ble the usual forms used in phrase-based MT, i.e.,
the model score is computed by a linear combina-
tion of a group of phrase-based features and lan-
guage models. In phrase-based MT, the transla-
tion problem is modeled by a derivation of phrase-
pairs. Given a translation model m, a language
model lm and a vector of feature weights w, the
model score of a derivation d is computed by
</bodyText>
<figure confidence="0.565175071428571">
tF9-[4] M[5]
treatment of
44[11] --�—[12]
from
564
���z ?� � AA* 3 5 0 Ä: -h A—Ú F*
phrase 1
the cost
P1 g(d&apos;; w&apos;, m, lm&apos;) = w&apos; m · fm(p1) + w&apos;lm · lm&apos;(”the cost”)
Skeleton: g(d; w, m, lm) = wm · fm(p1) + wlm · lm(”the cost”)
Full:
g(d&apos;; w&apos;, m, lm&apos;) = w&apos;m · fm(p1 ◦ p4 ◦ p5 ◦ p9)+
w&apos;lm · lm&apos; (”the cost X has been further reduced X .”)
Full: g(d; w, m, lm) = wm · fm(p1 ◦ p2 ◦ ... ◦ p9) + wlm · lm(”the cost of seawater ... per ton .”)
</figure>
<figureCaption confidence="0.991718333333333">
Figure 1: Example derivation and model scores for a sentence in LDC2006E38. The solid (red) rect-
angles represent the sentence skeleton, and the dashed (blue) rectangles represent the non-skeleton seg-
ments. X represents a slot in the translation skeleton. o represents composition of phrase-pairs.
</figureCaption>
<figure confidence="0.97515035">
��
3 5 A 0 Ä: -h
Skeleton:
g(d&apos;; w&apos;, m, lm&apos;) = w&apos;m · fm(p1 ◦ p4 ◦ p5)+
w&apos;lm · lm&apos; (”the cost X has been further reduced”)
Full: g(d; w, m, lm) = wm · fm(p1 ◦ p2 ◦ ... ◦ p5) + wlm · lm(”the cost of seawater ... further reduced”)
g(d&apos;; w&apos;, m, lm&apos;) = w&apos;m · fm(p1) + w&apos;lm · lm&apos;(”the cost X”)
g(d; w, m, lm) = wm · fm(p1 ◦ p2 ◦ p3) + wlm · lm(”the cost of seawater desalination treatment”)
phrases 2 &amp; 3
Skeleton:
Full:
P1 P2 P3 P4 P5
the cost
has been further
reduced
of seawater desalination treatment
�ë ���z ?� �
A_Ú
-F*
phrases 4 &amp; 5
�ë ���z ?� �
3 5 A 0 Ä: -h A—Ú F*
��
the cost
of seawater desalination treatment
P1 P2 P3
A_Ú
-F*
��
�ë ���z ?� �
3 5 A 0 Ä: -h
has been further
the cost
reduced
.
of seawater desalination treatment
from 5 yuan per ton
P1 P2 P3 P4 P5 P6 P7 P8 Pa
phrases 6-9
Skeleton:
</figure>
<figureCaption confidence="0.386226">
g(d; w, m, lm) = wm · fm(d) + wlm · lm(d) (4)
</figureCaption>
<bodyText confidence="0.999989111111111">
where fm(d) is a vector of feature values defined
on d, and wm is the corresponding weight vector.
lm(d) and wlm are the score and weight of the lan-
guage model, respectively.
To ease modeling, we only consider skeleton-
consistent derivations in this work. A deriva-
tion d is skeleton-consistent if no phrases in d
cross skeleton boundaries (e.g., a phrase where t-
wo of the source words are in the skeleton and
one is outside). Obviously, from any skeleton-
consistent derivation d we can extract a skeleton
derivation dT which covers the sentence skeleton
exactly. For example, in Figure 1, the deriva-
tion of phrase-pairs {p1, p2, ..., p9} is skeleton-
consistent, and the skeleton derivation is formed
by {p1, p4, p5, p9}.
Then, we can simply define gskel(d) and
gfull(d) as the model scores of dT and d:
</bodyText>
<equation confidence="0.9987345">
gskel(d) g(dT; wT, m, lmT) (5)
gfull(d) °= g(d; w, m, lm) (6)
</equation>
<bodyText confidence="0.999774666666667">
This model makes the skeleton translation and
full translation much simpler because they per-
form in the same way of string translation in
phrase-based MT. Both gskel(d) and gfull(d) share
the same translation model m which can easily
learned from the bilingual data1. On the other
hand, it has different feature weight vectors for in-
dividual models (i.e., w and wT).
For language modeling, lm is the standard n-
gram language model adopted in the baseline sys-
tem. lmT is a skeletal language for estimating the
well-formedness of the translation skeleton. Here
a translation skeleton is a target string where all
segments of non-skeleton translation are general-
ized to a symbol X. E.g., in Figure 1, the trans-
</bodyText>
<footnote confidence="0.938999">
1In gskel(d), we compute the reordering model score on
the skeleton though it is learned from the full sentences. In
this way the reordering problems in skeleton translation and
full translation are distinguished and handled separately.
</footnote>
<page confidence="0.99817">
565
</page>
<bodyText confidence="0.999366176470589">
lation skeleton is ’the cost X has been further re-
duced X .’, where two Xs represent non-skeleton
segments in the translation. In such a way of string
representation, the skeletal language model can be
implemented as a standard n-gram language mod-
el, that is, a string probability is calculated by a
product of a sequence of n-gram probabilities (in-
volving normal words and X). To learn the skele-
tal language model, we replace non-skeleton parts
of the target sentences in the bilingual corpus to
Xs using the source sentence skeletons and word
alignments. The skeletal language model is then
trained on these generalized strings in a standard
way of n-gram language modeling.
By substituting Eq. (4) into Eqs. (5) and (6),
and then Eqs. (3) and (2), we have the final model
used in this work:
</bodyText>
<equation confidence="0.872467333333333">
dˆ = arg max (wm · fm(d) + wlm · lm(d) +
d
)wτ m · fm(dτ) + wτ lm · lmτ(dτ) (7)
</equation>
<bodyText confidence="0.997431571428571">
Figure 1 shows the translation process and as-
sociated model scores for the example sentence.
Note that this method does not require any new
translation models for implementation. Given a
baseline phrase-based system, all we need is to
learn the feature weights w and wτ on the devel-
opment set (with source-language skeleton anno-
tation) and the skeletal language model lmτ on
the target-language side of the bilingual corpus.
To implement Eq. (7), we can perform standard
decoding while ”doubly weighting” the phrases
which cover a skeletal section of the sentence, and
combining the two language models and the trans-
lation model in a linear fashion.
</bodyText>
<sectionHeader confidence="0.999795" genericHeader="background">
3 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.99936">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999991075">
We experimented with our approach on Chinese-
English translation using the NiuTrans open-
source MT toolkit (Xiao et al., 2012). Our bilin-
gual corpus consists of 2.7M sentence pairs. Al-
l these sentences were aligned in word level us-
ing the GIZA++ system and the ”grow-diag-final-
and” heuristics. A 5-gram language model was
trained on the Xinhua portion of the English Gi-
gaword corpus in addition to the target-side of the
bilingual data. This language model was used
in both the baseline and our improved system-
s. For our skeletal language model, we trained a
5-gram language model on the target-side of the
bilingual data by generalizing non-skeleton seg-
ments to Xs. We used the newswire portion of the
NIST MT06 evaluation data as our developmen-
t set, and used the evaluation data of MT04 and
MT05 as our test sets. We chose the default fea-
ture set of the NiuTrans.Phrase engine for building
the baseline, including phrase translation proba-
bilities, lexical weights, a 5-gram language mod-
el, word and phrase bonuses, a ME-based lexical-
ized reordering model. All feature weights were
learned using minimum error rate training (Och,
2003).
Our skeleton identification system was built
using the t3 toolkit2 which implements a state-
of-the-art sentence simplification system. We
used the NEU Chinese sentence simplification
(NEUCSS) corpus as our training data (Zhang
et al., 2013). It contains the annotation of sen-
tence skeleton on the Chinese-language side of
the Penn Parallel Chinese-English Treebank (LD-
C2003E07). We trained our system using the Parts
1-8 of the NEUCSS corpus and obtained a 65.2%
relational F1 score and 63.1% compression rate in
held-out test (Part 10). For comparison, we also
manually annotated the MT development and test
data with skeleton information according to the
annotation standard provided within NEUCSS.
</bodyText>
<subsectionHeader confidence="0.957097">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.999924761904762">
Table 1 shows the case-insensitive IBM-version
BLEU and TER scores of different systems. We
see, first of all, that the MT system benefits from
our approach in most cases. In both the manual
and automatic identification of sentence skeleton
(rows 2 and 4), there is a significant improvemen-
t on the ”All” data set. However, using different
skeleton identification results for training and in-
ference (row 3) does not show big improvements
due to the data inconsistency problem.
Another interesting question is whether the
skeletal language model really contributes to the
improvements. To investigate it, we removed the
skeletal language model from our skeleton-based
translation system (with automatic skeleton iden-
tification on both the development and test sets).
Seen from row −lmτ of Table 1, the removal of
the skeletal language model results in a significan-
t drop in both BLEU and TER performance. It
indicates that this language model is very benefi-
cial to our system. For comparison, we removed
</bodyText>
<footnote confidence="0.978565">
2http://staffwww.dcs.shef.ac.uk/people/T.Cohn/t3/
</footnote>
<page confidence="0.989425">
566
</page>
<table confidence="0.9998022">
system Entry test-skel MT06 (Dev) MT04 MT05 All
dev-skel BLEU TER BLEU TER BLEU TER BLEU TER
baseline - - 35.06 60.54 38.53 61.15 34.32 62.82 36.64 61.54
SBMT manual manual 35.71 59.60 38.99 60.67 35.35 61.60 37.30 60.73
SBMT manual auto 35.72 59.62 38.75 61.16 35.02 62.20 37.03 61.19
SBMT auto auto 35.57 59.66 39.21 60.59 35.29 61.89 37.33 60.80
−lmτ auto auto 35.23 60.17 38.86 60.78 34.82 62.46 36.99 61.16
−mτ auto auto 35.50 59.69 39.00 60.69 35.10 62.03 37.12 60.90
s-space - - 35.00 60.50 38.39 61.20 34.33 62.90 36.57 61.58
s-feat. - - 35.16 60.50 38.60 61.17 34.25 62.88 36.70 61.58
</table>
<tableCaption confidence="0.995708">
Table 1: BLEU4[%] and TER[%] scores of different systems. Boldface means a significant improvement
</tableCaption>
<bodyText confidence="0.999665384615385">
(p &lt; 0.05). SBMT means our skeleton-based MT system. −lmτ (or −mτ) means that we remove the
skeletal language model (or translation model) from our proposed approach. s-space means that we
restrict the baseline system to the search space of skeleton-consistent derivations. s-feat. means that we
introduce an indicator feature for skeleton-consistent derivations into the baseline system.
the skeleton-based translation model from our sys-
tem as well. Row −mτ of Table 1 shows that the
skeleton-based translation model can contribute to
the overall improvement but there is no big differ-
ences between baseline and −mτ.
Apart from showing the effects of the skeleton-
based model, we also studied the behavior of the
MT system under the different settings of search
space. Row s-space of Table 1 shows the BLEU
and TER results of restricting the baseline sys-
tem to the space of skeleton-consistent derivation-
s, i.e., we remove both the skeleton-based trans-
lation model and language model from the SBMT
system. We see that the limited search space is a
little harmful to the baseline system. Further, we
regarded skeleton-consistent derivations as an in-
dicator feature and introduced it into the baseline
system. Seen from row s-feat., this feature does
not show promising improvements. These results
indicate that the real improvements are due to the
skeleton-based model/features used in this work,
rather than the ”well-formed” derivations.
</bodyText>
<sectionHeader confidence="0.99998" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999921863636364">
Skeleton is a concept that has been used in several
sub-areas in MT for years. For example, in confu-
sion network-based system combination it refer-
s to the backbone hypothesis for building confu-
sion networks (Rosti et al., 2007; Rosti et al.,
2008); Liu et al. (2011) regard skeleton as a short-
ened sentence after removing some of the function
words for better word deletion. In contrast, we de-
fine sentence skeleton as the key segments of a
sentence and develop a new MT approach based
on this information.
There are some previous studies on the use of
sentence skeleton or related information in MT
(Mellebeek et al., 2006a; Mellebeek et al., 2006b;
Owczarzak et al., 2006). In spite of their good
ideas of using skeleton skeleton information, they
did not model the skeleton-based translation prob-
lem in modern SMT pipelines. Our work is a fur-
ther step towards the use of sentence skeleton in
MT. More importantly, we develop a complete ap-
proach to this issue and show its effectiveness in a
state-of-the-art MT system.
</bodyText>
<sectionHeader confidence="0.990526" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999990692307692">
We have presented a simple but effective approach
to integrating the sentence skeleton information
into a phrase-based system. The experimental re-
sults show that the proposed approach achieves
very promising BLEU improvements and TER re-
ductions on the NIST evaluation data. In our fu-
ture work we plan to investigate methods of inte-
grating both syntactic models (for skeleton trans-
lation) and phrasal models (for full translation) in
our system. We also plan to study sophisticated
reordering models for skeleton translation, rather
than reusing the baseline reordering model which
is learned on the full sentences.
</bodyText>
<sectionHeader confidence="0.996514" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999693666666667">
This work was supported in part by the Nation-
al Science Foundation of China (Grants 61272376
and 61300097), and the China Postdoctoral Sci-
ence Foundation (Grant 2013M530131). The au-
thors would like to thank the anonymous reviewers
for their pertinent and insightful comments.
</bodyText>
<page confidence="0.995115">
567
</page>
<sectionHeader confidence="0.990232" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999944744444444">
David Chiang. 2010. Learning to Translate with
Source and Target Syntax. In Proc. of ACL 2010,
pages 1443-1452.
James Clarke and Mirella Lapata. 2006. Models for
Sentence Compression: A Comparison across Do-
mains, Training Requirements and Evaluation Mea-
sures. In Proc. of ACL/COLING 2006, pages 377-
384.
Trevor Cohn and Mirella Lapata. 2008. Sentence
Compression Beyond Word Deletion. In Proc. of
COLING 2008, pages 137-144.
Jason Eisner. 2003. Learning Non-Isomorphic Tree
Mappings for Machine Translation. In Proc. of ACL
2003, pages 205-208.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov Grammars for Sentence Compres-
sion. In Proc. of HLT:NAACL 2007, pages 180-187.
Liang Huang, Kevin Knight and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proc. of AMTA 2006, pages
66-73.
Kevin Knight and Daniel Marcu. 2000. Statistical-
based summarization-step one: sentence compres-
sion. In Proc. of AAAI 2000, pages 703-710.
Philipp Koehn, Franz J. Och and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proc. of
NAACL 2003, pages 48-54.
Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-to-
String Alignment Template for Statistical Machine
Translation. In Proc. of ACL/COLING 2006, pages
609-616.
Shujie Liu, Chi-Ho Li and Ming Zhou. 2011. Statistic
Machine Translation Boosted with Spurious Word
Deletion. In Proc. of Machine Translation Summit
XIII, pages 72-79.
Yuval Marton and Philip Resnik. 2008. Soft Syntactic
Constraints for Hierarchical Phrased-Based Transla-
tion. In Proc. of ACL:HLT 2008, pages 1003-1011.
Bart Mellebeek, Karolina Owczarzak, Josef van Gen-
abith and Andy Way. 2006. Multi-Engine Machine
Translation by Recursive Sentence Decomposition.
In Proc. of AMTA 2006, pages 110-118.
Bart Mellebeek, Karolina Owczarzak, Declan Groves,
Josef Van Genabith and Andy Way. 2006. A Syn-
tactic Skeleton for Statistical Machine Translation.
In Proc. of EAMT 2006, pages 195-202.
Franz J. Och, Christoph Tillmann and Hermann Ney.
1999. Improved Alignment Models for Statistical
Machine Translation. In Proc. of EMNLP/VLC
1999, pages 20-28.
Franz J. Och. 2003. Minimum error rate training in s-
tatistical machine translation. In Proc. of ACL 2003,
pages 160-167.
Karolina Owczarzak, Bart Mellebeek, Declan Groves,
Josef van Genabith and Andy Way. 2006. Wrapper
Syntax for Example-Based Machine Translation. In
Proc. of AMTA2006, pages 148-155.
Antti-Veikko I. Rosti, Spyros Matsoukas and Richard
Schwartz. 2007. Improved Word-Level System
Combination for Machine Translation. In Proc. of
ACL 2007, pages 312-319.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2008. Incremental hypothe-
sis alignment for building confusion networks with
application to machine translation system combina-
tion. In Proc. of Third Workshop on Statistical Ma-
chine Translation, pages 183õ186.
Tong Xiao, Jingbo Zhu, Hao Zhang and Qiang Li
2012. NiuTrans: An Open Source Toolkit for
Phrase-based and Syntax-based Machine Transla-
tion. In Proc. of ACL 2012, system demonstrations,
pages 19-24.
Elif Yamangil and Stuart M. Shieber. 2010. Bayesian
Synchronous Tree-Substitution Grammar Induction
and Its Application to Sentence Compression. In
Proc. of ACL 2010, pages 937-947.
Katsumasa Yoshikawa, Ryu Iida, Tsutomu Hirao and
Manabu Okumura. 2012. Sentence Compression
with Semantic Role Constraints. In Proc. of ACL
2012, pages 349-353.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew
Lim Tan and Sheng Li. 2008. A Tree Sequence
Alignment-based Tree-to-Tree Translation Model.
In Proc. of ACL:HLT 2008, pages 559-567.
Chunliang Zhang, Minghan Hu, Tong Xiao, Xue Jiang,
Lixin Shi and Jingbo Zhu. 2013. Chinese Sentence
Compression: Corpus and Evaluation. In Proc.
of Chinese Computational Linguistics and Natural
Language Processing Based on Naturally Annotated
Big Data, pages 257-267.
</reference>
<page confidence="0.996743">
568
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.332828">
<title confidence="0.995214">A Hybrid Approach to Skeleton-based Translation</title>
<author confidence="0.71861">Jingbo Chunliang University</author>
<author confidence="0.71861">Shenyang</author>
<address confidence="0.530508">YaTuo Company, 358 Wener Rd., Hangzhou 310012,</address>
<abstract confidence="0.997145846153846">In this paper we explicitly consider sentence skeleton information for Machine Translation (MT). The basic idea is that we translate the key elements of the input sentence using a skeleton translation model, and then cover the remain segments using a full translation model. We apply our approach to a state-of-the-art phrase-based system and demonstrate very promising BLEU improvements and TER reductions on the NIST Chinese-English MT evaluation data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Learning to Translate with Source and Target Syntax.</title>
<date>2010</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>1443--1452</pages>
<contexts>
<context position="2097" citStr="Chiang, 2010" startWordPosition="323" endWordPosition="324">te the key elements/structures (or skeleton) of the sentence first, and then translate the remaining parts. This especially makes sense for some languages, such as Chinese, where complex structures are usually involved. Note that the source-language structural information has been intensively investigated in recent studies of syntactic translation models. Some of them developed syntax-based models on complete syntactic trees with Treebank annotations (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and others used source-language syntax as soft constraints (Marton and Resnik, 2008; Chiang, 2010). However, these approaches suffer from the same problem as the phrase-based counterpart and use the single global model to handle different translation units, no matter they are from the skeleton of the input tree/sentence or other not-soimportant sub-structures. In this paper we instead explicitly model the translation problem with sentence skeleton information. In particular, • We develop a skeleton-based model which divides translation into two sub-models: a skeleton translation model (i.e., translating the key elements) and a full translation model (i.e., translating the remaining source </context>
</contexts>
<marker>Chiang, 2010</marker>
<rawString>David Chiang. 2010. Learning to Translate with Source and Target Syntax. In Proc. of ACL 2010, pages 1443-1452.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Mirella Lapata</author>
</authors>
<title>Models for Sentence Compression: A Comparison across Domains, Training Requirements and Evaluation Measures.</title>
<date>2006</date>
<booktitle>In Proc. of ACL/COLING</booktitle>
<pages>377--384</pages>
<contexts>
<context position="4726" citStr="Clarke and Lapata, 2006" startWordPosition="727" endWordPosition="730">pts represent indices): )Y*[6] ��y[13] TR[14] o[15] the cost has been further reduced . (The cost has been further reduced.) Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 Base Model Next we describe our approach to integrating skeleton information into MT models. We start with an assumption that the 1-best skeleton is provided by the skeleton identification system. Then we define skeleton-based translation as a</context>
</contexts>
<marker>Clarke, Lapata, 2006</marker>
<rawString>James Clarke and Mirella Lapata. 2006. Models for Sentence Compression: A Comparison across Domains, Training Requirements and Evaluation Measures. In Proc. of ACL/COLING 2006, pages 377-384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Mirella Lapata</author>
</authors>
<title>Sentence Compression Beyond Word Deletion.</title>
<date>2008</date>
<booktitle>In Proc. of COLING</booktitle>
<pages>137--144</pages>
<contexts>
<context position="4775" citStr="Cohn and Lapata, 2008" startWordPosition="735" endWordPosition="738"> the cost has been further reduced . (The cost has been further reduced.) Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 Base Model Next we describe our approach to integrating skeleton information into MT models. We start with an assumption that the 1-best skeleton is provided by the skeleton identification system. Then we define skeleton-based translation as a task of searching for the best target string tˆ </context>
</contexts>
<marker>Cohn, Lapata, 2008</marker>
<rawString>Trevor Cohn and Mirella Lapata. 2008. Sentence Compression Beyond Word Deletion. In Proc. of COLING 2008, pages 137-144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning Non-Isomorphic Tree Mappings for Machine Translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>205--208</pages>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning Non-Isomorphic Tree Mappings for Machine Translation. In Proc. of ACL 2003, pages 205-208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
</authors>
<title>Lexicalized Markov Grammars for Sentence Compression.</title>
<date>2007</date>
<booktitle>In Proc. of HLT:NAACL</booktitle>
<pages>180--187</pages>
<contexts>
<context position="4752" citStr="Galley and McKeown, 2007" startWordPosition="731" endWordPosition="734">Y*[6] ��y[13] TR[14] o[15] the cost has been further reduced . (The cost has been further reduced.) Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 Base Model Next we describe our approach to integrating skeleton information into MT models. We start with an assumption that the 1-best skeleton is provided by the skeleton identification system. Then we define skeleton-based translation as a task of searching for the</context>
</contexts>
<marker>Galley, McKeown, 2007</marker>
<rawString>Michel Galley and Kathleen McKeown. 2007. Lexicalized Markov Grammars for Sentence Compression. In Proc. of HLT:NAACL 2007, pages 180-187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>Kevin Knight</author>
<author>Aravind Joshi</author>
</authors>
<title>Statistical syntax-directed translation with extended domain of locality.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA</booktitle>
<pages>66--73</pages>
<contexts>
<context position="1976" citStr="Huang et al., 2006" startWordPosition="303" endWordPosition="306">m the way used by translators. For example, an important-first strategy is generally adopted in human translation - we translate the key elements/structures (or skeleton) of the sentence first, and then translate the remaining parts. This especially makes sense for some languages, such as Chinese, where complex structures are usually involved. Note that the source-language structural information has been intensively investigated in recent studies of syntactic translation models. Some of them developed syntax-based models on complete syntactic trees with Treebank annotations (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and others used source-language syntax as soft constraints (Marton and Resnik, 2008; Chiang, 2010). However, these approaches suffer from the same problem as the phrase-based counterpart and use the single global model to handle different translation units, no matter they are from the skeleton of the input tree/sentence or other not-soimportant sub-structures. In this paper we instead explicitly model the translation problem with sentence skeleton information. In particular, • We develop a skeleton-based model which divides translation into two sub-models: a skeleton tra</context>
</contexts>
<marker>Huang, Knight, Joshi, 2006</marker>
<rawString>Liang Huang, Kevin Knight and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain of locality. In Proc. of AMTA 2006, pages 66-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>Statisticalbased summarization-step one: sentence compression.</title>
<date>2000</date>
<booktitle>In Proc. of AAAI</booktitle>
<pages>703--710</pages>
<contexts>
<context position="4701" citStr="Knight and Marcu, 2000" startWordPosition="723" endWordPosition="726">ntence Skeleton (subscripts represent indices): )Y*[6] ��y[13] TR[14] o[15] the cost has been further reduced . (The cost has been further reduced.) Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 Base Model Next we describe our approach to integrating skeleton information into MT models. We start with an assumption that the 1-best skeleton is provided by the skeleton identification system. Then we define skelet</context>
</contexts>
<marker>Knight, Marcu, 2000</marker>
<rawString>Kevin Knight and Daniel Marcu. 2000. Statisticalbased summarization-step one: sentence compression. In Proc. of AAAI 2000, pages 703-710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz J Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL</booktitle>
<pages>48--54</pages>
<contexts>
<context position="1032" citStr="Koehn et al., 2003" startWordPosition="150" endWordPosition="153">the input sentence using a skeleton translation model, and then cover the remain segments using a full translation model. We apply our approach to a state-of-the-art phrase-based system and demonstrate very promising BLEU improvements and TER reductions on the NIST Chinese-English MT evaluation data. 1 Introduction Current Statistical Machine Translation (SMT) approaches model the translation problem as a process of generating a derivation of atomic translation units, assuming that every unit is drawn out of the same model. The simplest of these is the phrase-based approach (Och et al., 1999; Koehn et al., 2003) which employs a global model to process any sub-strings of the input sentence. In this way, all we need is to increasingly translate a sequence of source words each time until the entire sentence is covered. Despite good results in many tasks, such a method ignores the roles of each source word and is somewhat different from the way used by translators. For example, an important-first strategy is generally adopted in human translation - we translate the key elements/structures (or skeleton) of the sentence first, and then translate the remaining parts. This especially makes sense for some lan</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz J. Och and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proc. of NAACL 2003, pages 48-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Tree-toString Alignment Template for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL/COLING</booktitle>
<pages>609--616</pages>
<contexts>
<context position="1956" citStr="Liu et al., 2006" startWordPosition="299" endWordPosition="302">what different from the way used by translators. For example, an important-first strategy is generally adopted in human translation - we translate the key elements/structures (or skeleton) of the sentence first, and then translate the remaining parts. This especially makes sense for some languages, such as Chinese, where complex structures are usually involved. Note that the source-language structural information has been intensively investigated in recent studies of syntactic translation models. Some of them developed syntax-based models on complete syntactic trees with Treebank annotations (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and others used source-language syntax as soft constraints (Marton and Resnik, 2008; Chiang, 2010). However, these approaches suffer from the same problem as the phrase-based counterpart and use the single global model to handle different translation units, no matter they are from the skeleton of the input tree/sentence or other not-soimportant sub-structures. In this paper we instead explicitly model the translation problem with sentence skeleton information. In particular, • We develop a skeleton-based model which divides translation into two sub-mo</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu and Shouxun Lin. 2006. Tree-toString Alignment Template for Statistical Machine Translation. In Proc. of ACL/COLING 2006, pages 609-616.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shujie Liu</author>
<author>Chi-Ho Li</author>
<author>Ming Zhou</author>
</authors>
<title>Statistic Machine Translation Boosted with Spurious Word Deletion.</title>
<date>2011</date>
<booktitle>In Proc. of Machine Translation Summit XIII,</booktitle>
<pages>72--79</pages>
<contexts>
<context position="18272" citStr="Liu et al. (2011)" startWordPosition="3057" endWordPosition="3060">rded skeleton-consistent derivations as an indicator feature and introduced it into the baseline system. Seen from row s-feat., this feature does not show promising improvements. These results indicate that the real improvements are due to the skeleton-based model/features used in this work, rather than the ”well-formed” derivations. 4 Related Work Skeleton is a concept that has been used in several sub-areas in MT for years. For example, in confusion network-based system combination it refers to the backbone hypothesis for building confusion networks (Rosti et al., 2007; Rosti et al., 2008); Liu et al. (2011) regard skeleton as a shortened sentence after removing some of the function words for better word deletion. In contrast, we define sentence skeleton as the key segments of a sentence and develop a new MT approach based on this information. There are some previous studies on the use of sentence skeleton or related information in MT (Mellebeek et al., 2006a; Mellebeek et al., 2006b; Owczarzak et al., 2006). In spite of their good ideas of using skeleton skeleton information, they did not model the skeleton-based translation problem in modern SMT pipelines. Our work is a further step towards the</context>
</contexts>
<marker>Liu, Li, Zhou, 2011</marker>
<rawString>Shujie Liu, Chi-Ho Li and Ming Zhou. 2011. Statistic Machine Translation Boosted with Spurious Word Deletion. In Proc. of Machine Translation Summit XIII, pages 72-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Soft Syntactic Constraints for Hierarchical Phrased-Based Translation.</title>
<date>2008</date>
<booktitle>In Proc. of ACL:HLT</booktitle>
<pages>1003--1011</pages>
<contexts>
<context position="2082" citStr="Marton and Resnik, 2008" startWordPosition="319" endWordPosition="322"> translation - we translate the key elements/structures (or skeleton) of the sentence first, and then translate the remaining parts. This especially makes sense for some languages, such as Chinese, where complex structures are usually involved. Note that the source-language structural information has been intensively investigated in recent studies of syntactic translation models. Some of them developed syntax-based models on complete syntactic trees with Treebank annotations (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and others used source-language syntax as soft constraints (Marton and Resnik, 2008; Chiang, 2010). However, these approaches suffer from the same problem as the phrase-based counterpart and use the single global model to handle different translation units, no matter they are from the skeleton of the input tree/sentence or other not-soimportant sub-structures. In this paper we instead explicitly model the translation problem with sentence skeleton information. In particular, • We develop a skeleton-based model which divides translation into two sub-models: a skeleton translation model (i.e., translating the key elements) and a full translation model (i.e., translating the re</context>
</contexts>
<marker>Marton, Resnik, 2008</marker>
<rawString>Yuval Marton and Philip Resnik. 2008. Soft Syntactic Constraints for Hierarchical Phrased-Based Translation. In Proc. of ACL:HLT 2008, pages 1003-1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Mellebeek</author>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Multi-Engine Machine Translation by Recursive Sentence Decomposition.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA</booktitle>
<pages>110--118</pages>
<marker>Mellebeek, Owczarzak, van Genabith, Way, 2006</marker>
<rawString>Bart Mellebeek, Karolina Owczarzak, Josef van Genabith and Andy Way. 2006. Multi-Engine Machine Translation by Recursive Sentence Decomposition. In Proc. of AMTA 2006, pages 110-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bart Mellebeek</author>
<author>Karolina Owczarzak</author>
<author>Declan Groves</author>
<author>Josef Van Genabith</author>
<author>Andy Way</author>
</authors>
<title>A Syntactic Skeleton for Statistical Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. of EAMT</booktitle>
<pages>195--202</pages>
<marker>Mellebeek, Owczarzak, Groves, Van Genabith, Way, 2006</marker>
<rawString>Bart Mellebeek, Karolina Owczarzak, Declan Groves, Josef Van Genabith and Andy Way. 2006. A Syntactic Skeleton for Statistical Machine Translation. In Proc. of EAMT 2006, pages 195-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Christoph Tillmann</author>
<author>Hermann Ney</author>
</authors>
<title>Improved Alignment Models for Statistical Machine Translation.</title>
<date>1999</date>
<booktitle>In Proc. of EMNLP/VLC</booktitle>
<pages>20--28</pages>
<contexts>
<context position="1011" citStr="Och et al., 1999" startWordPosition="146" endWordPosition="149">e key elements of the input sentence using a skeleton translation model, and then cover the remain segments using a full translation model. We apply our approach to a state-of-the-art phrase-based system and demonstrate very promising BLEU improvements and TER reductions on the NIST Chinese-English MT evaluation data. 1 Introduction Current Statistical Machine Translation (SMT) approaches model the translation problem as a process of generating a derivation of atomic translation units, assuming that every unit is drawn out of the same model. The simplest of these is the phrase-based approach (Och et al., 1999; Koehn et al., 2003) which employs a global model to process any sub-strings of the input sentence. In this way, all we need is to increasingly translate a sequence of source words each time until the entire sentence is covered. Despite good results in many tasks, such a method ignores the roles of each source word and is somewhat different from the way used by translators. For example, an important-first strategy is generally adopted in human translation - we translate the key elements/structures (or skeleton) of the sentence first, and then translate the remaining parts. This especially mak</context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz J. Och, Christoph Tillmann and Hermann Ney. 1999. Improved Alignment Models for Statistical Machine Translation. In Proc. of EMNLP/VLC 1999, pages 20-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>160--167</pages>
<contexts>
<context position="14088" citStr="Och, 2003" startWordPosition="2393" endWordPosition="2394">al language model, we trained a 5-gram language model on the target-side of the bilingual data by generalizing non-skeleton segments to Xs. We used the newswire portion of the NIST MT06 evaluation data as our development set, and used the evaluation data of MT04 and MT05 as our test sets. We chose the default feature set of the NiuTrans.Phrase engine for building the baseline, including phrase translation probabilities, lexical weights, a 5-gram language model, word and phrase bonuses, a ME-based lexicalized reordering model. All feature weights were learned using minimum error rate training (Och, 2003). Our skeleton identification system was built using the t3 toolkit2 which implements a stateof-the-art sentence simplification system. We used the NEU Chinese sentence simplification (NEUCSS) corpus as our training data (Zhang et al., 2013). It contains the annotation of sentence skeleton on the Chinese-language side of the Penn Parallel Chinese-English Treebank (LDC2003E07). We trained our system using the Parts 1-8 of the NEUCSS corpus and obtained a 65.2% relational F1 score and 63.1% compression rate in held-out test (Part 10). For comparison, we also manually annotated the MT development</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz J. Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL 2003, pages 160-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Bart Mellebeek</author>
<author>Declan Groves</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Wrapper Syntax for Example-Based Machine Translation.</title>
<date>2006</date>
<booktitle>In Proc. of AMTA2006,</booktitle>
<pages>148--155</pages>
<marker>Owczarzak, Mellebeek, Groves, van Genabith, Way, 2006</marker>
<rawString>Karolina Owczarzak, Bart Mellebeek, Declan Groves, Josef van Genabith and Andy Way. 2006. Wrapper Syntax for Example-Based Machine Translation. In Proc. of AMTA2006, pages 148-155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved Word-Level System Combination for Machine Translation.</title>
<date>2007</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>312--319</pages>
<contexts>
<context position="18232" citStr="Rosti et al., 2007" startWordPosition="3049" endWordPosition="3052"> to the baseline system. Further, we regarded skeleton-consistent derivations as an indicator feature and introduced it into the baseline system. Seen from row s-feat., this feature does not show promising improvements. These results indicate that the real improvements are due to the skeleton-based model/features used in this work, rather than the ”well-formed” derivations. 4 Related Work Skeleton is a concept that has been used in several sub-areas in MT for years. For example, in confusion network-based system combination it refers to the backbone hypothesis for building confusion networks (Rosti et al., 2007; Rosti et al., 2008); Liu et al. (2011) regard skeleton as a shortened sentence after removing some of the function words for better word deletion. In contrast, we define sentence skeleton as the key segments of a sentence and develop a new MT approach based on this information. There are some previous studies on the use of sentence skeleton or related information in MT (Mellebeek et al., 2006a; Mellebeek et al., 2006b; Owczarzak et al., 2006). In spite of their good ideas of using skeleton skeleton information, they did not model the skeleton-based translation problem in modern SMT pipelines</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko I. Rosti, Spyros Matsoukas and Richard Schwartz. 2007. Improved Word-Level System Combination for Machine Translation. In Proc. of ACL 2007, pages 312-319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Incremental hypothesis alignment for building confusion networks with application to machine translation system combination.</title>
<date>2008</date>
<booktitle>In Proc. of Third Workshop on Statistical Machine Translation,</booktitle>
<pages>183--186</pages>
<contexts>
<context position="18253" citStr="Rosti et al., 2008" startWordPosition="3053" endWordPosition="3056">tem. Further, we regarded skeleton-consistent derivations as an indicator feature and introduced it into the baseline system. Seen from row s-feat., this feature does not show promising improvements. These results indicate that the real improvements are due to the skeleton-based model/features used in this work, rather than the ”well-formed” derivations. 4 Related Work Skeleton is a concept that has been used in several sub-areas in MT for years. For example, in confusion network-based system combination it refers to the backbone hypothesis for building confusion networks (Rosti et al., 2007; Rosti et al., 2008); Liu et al. (2011) regard skeleton as a shortened sentence after removing some of the function words for better word deletion. In contrast, we define sentence skeleton as the key segments of a sentence and develop a new MT approach based on this information. There are some previous studies on the use of sentence skeleton or related information in MT (Mellebeek et al., 2006a; Mellebeek et al., 2006b; Owczarzak et al., 2006). In spite of their good ideas of using skeleton skeleton information, they did not model the skeleton-based translation problem in modern SMT pipelines. Our work is a furth</context>
</contexts>
<marker>Rosti, Zhang, Matsoukas, Schwartz, 2008</marker>
<rawString>Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas, and Richard Schwartz. 2008. Incremental hypothesis alignment for building confusion networks with application to machine translation system combination. In Proc. of Third Workshop on Statistical Machine Translation, pages 183õ186.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tong Xiao</author>
<author>Jingbo Zhu</author>
</authors>
<title>Hao Zhang and Qiang Li 2012. NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation.</title>
<booktitle>In Proc. of ACL 2012, system demonstrations,</booktitle>
<pages>19--24</pages>
<marker>Xiao, Zhu, </marker>
<rawString>Tong Xiao, Jingbo Zhu, Hao Zhang and Qiang Li 2012. NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation. In Proc. of ACL 2012, system demonstrations, pages 19-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elif Yamangil</author>
<author>Stuart M Shieber</author>
</authors>
<title>Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression.</title>
<date>2010</date>
<booktitle>In Proc. of ACL</booktitle>
<pages>937--947</pages>
<contexts>
<context position="4803" citStr="Yamangil and Shieber, 2010" startWordPosition="739" endWordPosition="742">her reduced . (The cost has been further reduced.) Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 Base Model Next we describe our approach to integrating skeleton information into MT models. We start with an assumption that the 1-best skeleton is provided by the skeleton identification system. Then we define skeleton-based translation as a task of searching for the best target string tˆ given the source string and </context>
</contexts>
<marker>Yamangil, Shieber, 2010</marker>
<rawString>Elif Yamangil and Stuart M. Shieber. 2010. Bayesian Synchronous Tree-Substitution Grammar Induction and Its Application to Sentence Compression. In Proc. of ACL 2010, pages 937-947.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katsumasa Yoshikawa</author>
<author>Ryu Iida</author>
<author>Tsutomu Hirao</author>
<author>Manabu Okumura</author>
</authors>
<title>Sentence Compression with Semantic Role Constraints.</title>
<date>2012</date>
<booktitle>In Proc. of ACL 2012,</booktitle>
<pages>349--353</pages>
<contexts>
<context position="4828" citStr="Yoshikawa et al., 2012" startWordPosition="743" endWordPosition="746">been further reduced.) Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 Base Model Next we describe our approach to integrating skeleton information into MT models. We start with an assumption that the 1-best skeleton is provided by the skeleton identification system. Then we define skeleton-based translation as a task of searching for the best target string tˆ given the source string and its skeleton τ: tˆ = arg </context>
</contexts>
<marker>Yoshikawa, Iida, Hirao, Okumura, 2012</marker>
<rawString>Katsumasa Yoshikawa, Ryu Iida, Tsutomu Hirao and Manabu Okumura. 2012. Sentence Compression with Semantic Role Constraints. In Proc. of ACL 2012, pages 349-353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Hongfei Jiang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
<author>Chew Lim Tan</author>
<author>Sheng Li</author>
</authors>
<title>A Tree Sequence Alignment-based Tree-to-Tree Translation Model.</title>
<date>2008</date>
<booktitle>In Proc. of ACL:HLT</booktitle>
<pages>559--567</pages>
<contexts>
<context position="1997" citStr="Zhang et al., 2008" startWordPosition="307" endWordPosition="310">anslators. For example, an important-first strategy is generally adopted in human translation - we translate the key elements/structures (or skeleton) of the sentence first, and then translate the remaining parts. This especially makes sense for some languages, such as Chinese, where complex structures are usually involved. Note that the source-language structural information has been intensively investigated in recent studies of syntactic translation models. Some of them developed syntax-based models on complete syntactic trees with Treebank annotations (Liu et al., 2006; Huang et al., 2006; Zhang et al., 2008), and others used source-language syntax as soft constraints (Marton and Resnik, 2008; Chiang, 2010). However, these approaches suffer from the same problem as the phrase-based counterpart and use the single global model to handle different translation units, no matter they are from the skeleton of the input tree/sentence or other not-soimportant sub-structures. In this paper we instead explicitly model the translation problem with sentence skeleton information. In particular, • We develop a skeleton-based model which divides translation into two sub-models: a skeleton translation model (i.e.,</context>
</contexts>
<marker>Zhang, Jiang, Aw, Li, Tan, Li, 2008</marker>
<rawString>Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, Chew Lim Tan and Sheng Li. 2008. A Tree Sequence Alignment-based Tree-to-Tree Translation Model. In Proc. of ACL:HLT 2008, pages 559-567.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunliang Zhang</author>
<author>Minghan Hu</author>
<author>Tong Xiao</author>
<author>Xue Jiang</author>
<author>Lixin Shi</author>
<author>Jingbo Zhu</author>
</authors>
<title>Chinese Sentence Compression: Corpus and Evaluation.</title>
<date>2013</date>
<booktitle>In Proc. of Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data,</booktitle>
<pages>257--267</pages>
<contexts>
<context position="14329" citStr="Zhang et al., 2013" startWordPosition="2426" endWordPosition="2429">ed the evaluation data of MT04 and MT05 as our test sets. We chose the default feature set of the NiuTrans.Phrase engine for building the baseline, including phrase translation probabilities, lexical weights, a 5-gram language model, word and phrase bonuses, a ME-based lexicalized reordering model. All feature weights were learned using minimum error rate training (Och, 2003). Our skeleton identification system was built using the t3 toolkit2 which implements a stateof-the-art sentence simplification system. We used the NEU Chinese sentence simplification (NEUCSS) corpus as our training data (Zhang et al., 2013). It contains the annotation of sentence skeleton on the Chinese-language side of the Penn Parallel Chinese-English Treebank (LDC2003E07). We trained our system using the Parts 1-8 of the NEUCSS corpus and obtained a 65.2% relational F1 score and 63.1% compression rate in held-out test (Part 10). For comparison, we also manually annotated the MT development and test data with skeleton information according to the annotation standard provided within NEUCSS. 3.2 Results Table 1 shows the case-insensitive IBM-version BLEU and TER scores of different systems. We see, first of all, that the MT syst</context>
</contexts>
<marker>Zhang, Hu, Xiao, Jiang, Shi, Zhu, 2013</marker>
<rawString>Chunliang Zhang, Minghan Hu, Tong Xiao, Xue Jiang, Lixin Shi and Jingbo Zhu. 2013. Chinese Sentence Compression: Corpus and Evaluation. In Proc. of Chinese Computational Linguistics and Natural Language Processing Based on Naturally Annotated Big Data, pages 257-267.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>