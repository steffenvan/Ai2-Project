<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000210">
<title confidence="0.966422">
SEMAFOR: Frame Argument Resolution with Log-Linear Models
</title>
<author confidence="0.992617">
Desai Chen Nathan Schneider Dipanjan Das Noah A. Smith
</author>
<affiliation confidence="0.985086">
School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.812416">
{desaic@andrew,dipanjan@cs,nschneid@cs,nasmith@cs}.cmu.edu
</email>
<sectionHeader confidence="0.989696" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996591384615384">
This paper describes the SEMAFOR sys-
tem’s performance in the SemEval 2010
task on linking events and their partici-
pants in discourse. Our entry is based
upon SEMAFOR 1.0 (Das et al., 2010a),
a frame-semantic probabilistic parser built
from log-linear models. The extended sys-
tem models null instantiations, including
non-local argument reference. Performance
is evaluated on the task data with and with-
out gold-standard overt arguments. In both
settings, it fares the best of the submitted
systems with respect to recall and Fl.
</bodyText>
<sectionHeader confidence="0.999241" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999175777777778">
The theory of frame semantics (Fillmore, 1982)
holds that meaning is largely structured by holis-
tic units of knowledge, called frames. Each frame
encodes a conventionalized gestalt event or sce-
nario, often with conceptual dependents (partic-
ipants, props, or attributes) filling roles to elab-
orate the specific instance of the frame. In the
FrameNet lexicon (Fillmore et al., 2003), each
frame defines core roles tightly coupled with
the particular meaning of the frame, as well as
more generic non-core roles (Ruppenhofer et al.,
2006). Frames can be evoked with linguistic pred-
icates, known as lexical units (LUs); role fillers
can be expressed overtly and linked to the frame
via (morpho)syntactic constructions. However, a
great deal of conceptually-relevant content is left
unexpressed or is not explicitly linked to the frame
via linguistic conventions; rather, it is expected
that the listener will be able to infer the appro-
priate relationships pragmatically. Certain types
of implicit content and implicit reference are for-
malized in the theory of null instantiations (NIs)
(Fillmore, 1986; Ruppenhofer, 2005). A complete
frame-semantic analysis of text thus incorporates
covert and overt predicate-argument information.
In this paper, we describe a system for frame-
semantic analysis, evaluated on a semantic role
labeling task for explicit and implicit arguments
(§2). Extending the SEMAFOR 1.0 frame-
semantic parser (Das et al., 2010a; outlined in §3),
we detect null instantiations via a simple two-stage
pipeline: the first stage predicts whether a given
role is null-instantiated, and the second stage (§4)
predicts how it is null-instantiated, if it is not overt.
We report performance on the SemEval 2010 test
set under the full-SRL and NI-only conditions.
</bodyText>
<sectionHeader confidence="0.992873" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999840833333333">
The SemEval 2007 task on frame-semantic pars-
ing (Baker et al., 2007) provided a small (about
50,000 words and 2,000 sentences) dataset of
news text, travel guides, and bureaucratic accounts
of weapons stockpiles. Sentences in this dataset
were fully annotated with frames and their argu-
ments. The SemEval 2010 task (Ruppenhofer et
al., 2010) adds annotated data in the fiction do-
main: parts of two Sherlock Holmes stories by
Arthur Conan Doyle. The SemEval 2010 train-
ing set consists of the SemEval 2007 data plus
one document from the new domain. This doc-
ument has about 7800 words in 438 sentences;
it has 1492 annotated frame instances, including
3169 (overt and null-instantiated) argument anno-
tations. The test set consists of two chapters from
another story: Chapter 13 contains about 4000
words, 249 sentences, and 791 frames; Chapter 14
contains about 5000 words, 276 sentences, and
941 frames (see also Table 3). Figure 1 shows
two annotated test sentences. All data released for
the 2010 task include part-of-speech tags, lemmas,
and phrase-structure trees from a parser, with head
annotations for constituents.
</bodyText>
<sectionHeader confidence="0.981452" genericHeader="method">
3 Argument identification
</sectionHeader>
<bodyText confidence="0.993617875">
Our starting point is SEMAFOR 1.0 (Das et
al., 2010a), a discriminative probabilistic frame-
semantic parsing model that operates in three
stages: (a) rule-based target selection, (b) proba-
bilistic disambiguation that resolves each target to
a FrameNet frame, and (c) joint selection of text
spans to fill the roles of each target through a sec-
ond probabilistic model.1
</bodyText>
<footnote confidence="0.981123">
1Das et al. (2010a) report the performance of this system
on the complete SemEval 2007 task at 46.49% Fl.
</footnote>
<page confidence="0.919338">
264
</page>
<note confidence="0.572794">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 264–267,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figure confidence="0.999581058823529">
Cognizer Opinion
think.v
`` I THINK that I shall be in a position to MAKE the situation rather more CLEAR to you before long .
OPINION
Experiencer
Actor Effect
make.v
It has been an exceedingly DIFFICULT and most complicated business .
CAUSATION
Degree Activity
Phenomenon
DIFFICULTY
difficult.a
Degree Attribute
OBVIOUSNESS
clear.n
Experiencer
</figure>
<figureCaption confidence="0.998351">
Figure 1. Two consecutive sentences
</figureCaption>
<bodyText confidence="0.847636777777778">
in the test set, with frame-semantic an-
notations. Shaded regions represent
frames: they include the target word in
the sentence, the corresponding frame
name and lexical unit, and arguments.
Horizontal bars mark gold argument
spans—white bars are gold annotations
and black bars show mistakes of our
NI-only system.
</bodyText>
<table confidence="0.9195098">
Chapter 13 Chapter 14
Training Data Prec. Rec. Fl Prec. Rec. Fl
SemEval 2010 data (includes SemEval 2007 data) 0.69 0.50 0.58 0.66 0.48 0.56
SemEval 2007 data + 50% new, in-domain data 0.68 0.47 0.55 0.66 0.45 0.54
SemEval 2007 data only 0.67 0.41 0.50 0.64 0.40 0.50
</table>
<tableCaption confidence="0.685591">
Table 1. Overt
argument labeling
performance.
</tableCaption>
<bodyText confidence="0.999985454545454">
Stage (c), known as argument identification or
SRL, is most relevant here. In this step, the system
takes the target (frame-evoking) phrase t and cor-
responding frame type f predicted by the previous
stages, and independently fills each role of f with
a word or phrase from the sentence, or the sym-
bol OTHER to indicate that the role has no (local)
overt argument. Features used to inform this de-
cision include aspects of the syntactic dependency
parse (e.g. the path in the parse from the target
to the argument); voice; word overlap of the argu-
ment with respect to the target; and part-of-speech
tags within and around the argument. SEMAFOR
as described in (Das et al., 2010a) does not dis-
tinguish between different types of null instantia-
tions or find non-local referents. Given perfect
input to stage (c), the system achieved 68.5% F1
on the SemEval 2007 data (exact match, evaluat-
ing overt arguments only). The only difference
in our use of SEMAFOR’s argument identification
module is in preprocessing the training data: we
use dependency parses transformed from the head-
augmented phrase-structure parses in the task data.
Table 1 shows the performance of our argument
identification model on this task’s test data. The
SRL systems compared in (Ruppenhofer et al.,
2010) all achieved precision in the mid 60% range,
but SEMAFOR achieved substantially higher re-
call, F1, and label accuracy on this subtask. (The
table also shows how performance of our model
degrades when half or all of the new data are not
used for training; the 9% difference in recall sug-
gests the importance of in-domain training data.)
</bodyText>
<sectionHeader confidence="0.989945" genericHeader="method">
4 Null instantiation detection
</sectionHeader>
<bodyText confidence="0.9657634">
In this subtask, which follows the argument iden-
tification subtask (§3), our system seeks to char-
acterize non-overt core roles given gold standard
local frame-argument annotations. Consider the
following passage from the test data:
“That’s lucky for him—in fact, it’s lucky for all
of you, since you are all on the wrong side of the
law in this matter. I am not sure that as a consci-
entious detective [Authorities my] first duty is not to
arrest [Suspect the whole household]. [DNI
</bodyText>
<equation confidence="0.426279">
Charges∅]
</equation>
<bodyText confidence="0.999636821428571">
The frame we are interested in, ARREST, has four
core roles, two of which (Authorities and Sus-
pect) have overt (local) arguments. The third core
role, Charges, is annotated as having anaphoric
or definite null instantiation (DNI). “Definite”
means that the discourse implies a specific referent
that should be recoverable from context, without
marking that referent linguistically. Some DNIs in
the data are linked to phrases in syntactically non-
local positions, such as in another sentence (see
Figure 1). This one is not (though our model in-
correctly labels this matter from the previous sen-
tence as a DNI referent for this role). The fourth
core role, Offense, is not annotated as a null in-
stantiation because it belongs to the same CoreSet
as Charges—which is to say they are relevant in
a similar way to the frame as a whole (both pertain
to the rationale for the arrest) and only one is typ-
ically expressed.2 We will use the term masked
to refer to any non-overt core role which does not
need to be specified as null-instantiated due to a
structural connection to another role in its frame.
The typology of NIs given in Ruppenhofer
(2005) and employed in the annotation distin-
guishes anaphoric/definite NIs from existential or
indefinite null instantiations (INIs). Rather than
having a specific referent accessible in the dis-
course, INIs are left vague or deemphasized, as in
</bodyText>
<footnote confidence="0.94983225">
2If the FrameNet lexicon marks a pair of roles within a
frame as being in a CoreSet or Excludes relationship, then
filling one of them satisfies the requirement that the other be
(expressly or implicitly) present in the use of the frame.
</footnote>
<page confidence="0.978933">
265
</page>
<table confidence="0.97720225">
NI-only
Full
Chapter 13 Chapter 14
Training Data Prec. Rec. Fl Prec. Rec. Fl
SemEval 2010 new: 100% 0.40 0.64 0.50 0.53 0.60 0.56
SemEval 2010 new: 75% 0.66 0.37 0.50 0.70 0.37 0.48
SemEval 2010 new: 50% 0.73 0.38 0.51 0.75 0.35 0.48
All 0.35 0.55 0.43 0.56 0.49 0.52
</table>
<tableCaption confidence="0.999813">
Table 2. Performance on the
</tableCaption>
<bodyText confidence="0.985497857142857">
full task and the NI-only task.
The NI model was trained on the
new SemEval 2010 document, “The
Tiger of San Pedro” (data from the
2007 task was excluded because
none of the null instantiations in that
data had annotated referents).
</bodyText>
<figure confidence="0.993934375">
overt
DNI
INI
masked
inc.
total
Gold
Predicted
</figure>
<table confidence="0.621718714285714">
overt DNI INI masked inc. total
2068 (1630) 5 362 327 0 2762
64 12 (3) 182 90 0 348
41 2 214 96 0 353
73 0 240 1394 0 1707
12 2 55 2 0 71
2258 21 1053 1909 0 3688 correct
</table>
<tableCaption confidence="0.750680666666667">
Table 3. Instantiation type confusion ma-
trix for the full model (argument identifi-
cation plus NI detection). Parenthesized
</tableCaption>
<bodyText confidence="0.8942525">
numbers count the predictions of the cor-
rect type which also predicted the same
(argument or referent) span. On the NI-
only task, our system has a similar distri-
bution of NI detection errors.
the thing(s) eaten in the sentence We ate.
The problem can be decomposed into two steps:
(a) classifying each null instantiation as definite,
indefinite, or masked; and (b) resolving the DNIs,
which entails finding referents in the non-local
context. Instead, our model makes a single NI pre-
diction for any role that received no local argument
(OTHER) in the argument identification phase (§3),
thereby combining classification and resolution.3
</bodyText>
<subsectionHeader confidence="0.989024">
4.1 Model
</subsectionHeader>
<bodyText confidence="0.99994944">
Our model for this subtask is analogous to the ar-
gument identification model: it chooses one from
among many possible fillers for each role. How-
ever, whereas the argument identification model
considers parse constituents as potential local
fillers (which might constitute an overt argument
within the sentence) along with a special category,
OTHER, here the set of candidate fillers consists of
phrases from outside the sentence, along with spe-
cial categories INI or MASKED. When selected, a
non-local phrase will be interpreted as a non-local
argument and labeled as a DNI referent.
These non-local candidate fillers are handled
differently from candidates within the sentence
considered in the argument identification model:
they are selected using more restrictive criteria,
and are associated with a different set of features.
Restricted search space for DNI referents. We
consider nouns, pronouns, and noun phrases from
the previous three sentences as candidate DNI ref-
erents. This narrows the search space considerably
to make learning tractable, but at a cost: many
gold DNI referents will not even be considered.
In the training data, there are about 250 DNI in-
stances with explicit referents; their distribution is
</bodyText>
<subsectionHeader confidence="0.502132">
3Investigation of separate modeling is left to future work.
</subsectionHeader>
<bodyText confidence="0.989929">
chaotic.4 Judging by the training data, our heuris-
tics thus limit oracle recall to about 20% of DNIs.5
Modified feature set. Since it is not obvious how
to calculate a syntactic path between two words
in different sentences, we replaced dependency
path features with simpler features derived from
FrameNet’s lexicographic exemplar annotations.
For each candidate span, we use two types of fea-
tures to model the affinity between the head word
and the role. The first indicates whether the head
word is used as a filler for this role in at least
one of the lexicographic exemplars. The second
encodes the maximum distributional similarity to
any word heading a filler of that role in the ex-
emplars.6 In practice, we found that these fea-
tures received negligible weight and had virtually
no effect on performance, possibly due to data
sparseness. An additional change in the feature
set is that ordering/distance features (Das et al.,
2010b, p. 13) were replaced with a feature indicat-
ing the number of sentences away the candidate
is from the target.7 Otherwise, the null identifica-
491 DNI referents are found no more than three sentences
prior; another 90 are in the same sentence as the target. 20
DNIs have referents which are not noun phrases. Six appear
after the sentence containing its frame target; 28 appear at
least 25 sentences prior. 60 have no referent.
5Our system ignores DNIs with no referent or with a ref-
erent in the same sentence as the target. Experiments with
variants on these assumptions show that the larger the search
space (i.e. the more candidate DNI referents are under con-
sideration), the worse the trained model performs at distin-
guishing NIs from non-NIs (though DNI vs. INI precision
improves). This suggests that data sparseness is hindering
our system’s ability to learn useful generalizations about NIs.
</bodyText>
<footnote confidence="0.959327571428571">
6Distributional similarity scores are obtained
from D. Lin’s Proximity-based Thesaurus (http:
//webdocs.cs.ualberta.ca/~lindek/
Downloads/sims.lsp.gz) and quantized into bi-
nary features for intervals: [0, .03), [.03, .06), [.06, .08),
[.08, ∞).
7All of the new features are instantiated in three forms:
</footnote>
<page confidence="0.997315">
266
</page>
<bodyText confidence="0.9999712">
tion model uses the same features as the argument
identification model.
The theory of null instantiations holds that the
grammaticality of lexically-licensed NI for a role
in a given frame depends on the LU: for exam-
ple, the verbs buy and sell share the same frame
but differ as to whether the Buyer or Seller role
may be lexically null-instantiated. Our model’s
feature set is rich enough to capture this in a soft
way, with lexicalized features that fire, e.g., when
the Seller role is null-instantiated and the target
is buy. Moreover, (Ruppenhofer, 2005) hypoth-
esizes that each role has a strong preference for
one interpretation (INI or DNI) when it is lexically
null-instantiated, regardless of LU. This, too, is
modeled in our feature set. In theory these trends
should be learnable given sufficient data, though it
is doubtful that there are enough examples of null
instantiations in the currently available dataset for
this learning to take place.
</bodyText>
<subsectionHeader confidence="0.872352">
4.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.98577519047619">
We trained the model on the non-overt arguments
in the new SemEval 2010 training document,
which has 580 null instantiations—303 DNIs and
277 INIs.8,9 Then we used the task scoring proce-
dure to evaluate the NI detection subtask in isola-
tion (given gold-standard overt arguments) as well
as the full task (when this module is combined in a
pipeline with argument identification). Results are
shown in Table 2.10
Table 3 provides a breakdown of our sys-
tem’s predictions on the test data by instantiation
type: overt local arguments, DNIs, INIs, and the
MASKED category (marking the role as redundant
or irrelevant for the particular use of the frame,
given the other arguments). It also shows counts
for incorporated (“inc.”) roles, which are filled by
the frame-evoking target, e.g. clear in Figure 1.11
This table shows that the system is reasonably ef-
fective at discriminating NIs from masked roles,
one specific to the frame and the role, one specific to the role
name only, and one to learn the overall bias of the data.
</bodyText>
<footnote confidence="0.907631636363637">
8For feature engineering we held out the last 25% of sen-
tences from the new training document as development data,
retraining on the full training set for final evaluation.
9We used Nils Reiter’s FrameNet API, version 0.4
(http://www.cl.uni-heidelberg.de/trac/
FrameNetAPI) in processing the data.
10The other system participating in the NI-only subtask
had much lower NI recall of 8% (Ruppenhofer et al., 2010).
11We do not predict any DNIs without referents or in-
corporated roles, though the evaluation script gives us credit
when we predict INI for these cases.
</footnote>
<bodyText confidence="0.999244181818182">
but DNI identification suffers from low recall and
INI identification from low precision. Data sparse-
ness is likely the biggest obstacle here. To put this
in perspective, there are over 20,000 training ex-
amples of overt arguments, but fewer than 600 ex-
amples of null instantiations, two thirds of which
do not have referents. Without an order of mag-
nitude more NI data (at least), it is unlikely that
a supervised learner could generalize well enough
to recognize on new data null instantiations of the
over 7000 roles in the lexicon.
</bodyText>
<sectionHeader confidence="0.999198" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999968777777778">
We have described a system that implements a
clean probabilistic model of frame-semantic struc-
ture, considering overt arguments as well as var-
ious forms of null instantion of roles. The sys-
tem was evaluated on SemEval 2010 data, with
mixed success at detecting null instantiations. We
believe in-domain data sparseness is the predom-
inant factor limiting the robustness of our super-
vised model.
</bodyText>
<sectionHeader confidence="0.998214" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.990102">
This work was supported by DARPA grant
NBCH-1080004 and computational resources
provided by Yahoo. We thank the task organizers for
providing data and conducting the evaluation, and two
reviewers for their comments.
</bodyText>
<sectionHeader confidence="0.998556" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999807111111111">
C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-2007
Task 19: Frame Semantic Structure Extraction. In Proc.
of SemEval.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010a.
Probabilistic frame-semantic parsing. In Proc. of NAACL-
HLT.
D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010b.
SEMAFOR 1.0: A probabilistic frame-semantic parser.
Technical Report CMU-LTI-10-001, Carnegie Mellon
University.
C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003.
Background to FrameNet. International Journal of Lexi-
cography, 16(3).
C. J. Fillmore. 1982. Frame semantics. In Linguistics in the
Morning Calm, pages 111–137. Hanshin Publishing Co.,
Seoul, South Korea.
C. J. Fillmore. 1986. Pragmatically controlled zero
anaphora. In Proc. of Berkeley Linguistics Society, pages
95–107, Berkeley, CA.
J. Ruppenhofer, M. Ellsworth, M. R.L. Petruck, C. R. John-
son, and J. Scheffczyk. 2006. FrameNet II: extended the-
ory and practice.
J. Ruppenhofer, C. Sporleder, R. Morante, C. Baker, and
M. Palmer. 2010. SemEval-2010 Task 10: Linking
Events and Their Participants in Discourse. In Proc. of
SemEval.
J. Ruppenhofer. 2005. Regularities in null instantiation.
</reference>
<page confidence="0.99729">
267
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.752076">
<title confidence="0.999709">SEMAFOR: Frame Argument Resolution with Log-Linear Models</title>
<author confidence="0.999966">Desai Chen Nathan Schneider Dipanjan Das Noah A Smith</author>
<affiliation confidence="0.989461">School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA</affiliation>
<email confidence="0.999612">desaic@andrew.cmu.edu</email>
<email confidence="0.999612">dipanjan@cs.cmu.edu</email>
<email confidence="0.999612">nschneid@cs.cmu.edu</email>
<email confidence="0.999612">nasmith@cs.cmu.edu</email>
<abstract confidence="0.982833785714286">This paper describes the SEMAFOR system’s performance in the SemEval 2010 task on linking events and their participants in discourse. Our entry is based upon SEMAFOR 1.0 (Das et al., 2010a), a frame-semantic probabilistic parser built from log-linear models. The extended sysmodels including non-local argument reference. Performance is evaluated on the task data with and without gold-standard overt arguments. In both settings, it fares the best of the submitted with respect to recall and</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Baker</author>
<author>M Ellsworth</author>
<author>K Erk</author>
</authors>
<date>2007</date>
<booktitle>SemEval-2007 Task 19: Frame Semantic Structure Extraction. In Proc. of SemEval.</booktitle>
<contexts>
<context position="2653" citStr="Baker et al., 2007" startWordPosition="395" endWordPosition="398">. In this paper, we describe a system for framesemantic analysis, evaluated on a semantic role labeling task for explicit and implicit arguments (§2). Extending the SEMAFOR 1.0 framesemantic parser (Das et al., 2010a; outlined in §3), we detect null instantiations via a simple two-stage pipeline: the first stage predicts whether a given role is null-instantiated, and the second stage (§4) predicts how it is null-instantiated, if it is not overt. We report performance on the SemEval 2010 test set under the full-SRL and NI-only conditions. 2 Data The SemEval 2007 task on frame-semantic parsing (Baker et al., 2007) provided a small (about 50,000 words and 2,000 sentences) dataset of news text, travel guides, and bureaucratic accounts of weapons stockpiles. Sentences in this dataset were fully annotated with frames and their arguments. The SemEval 2010 task (Ruppenhofer et al., 2010) adds annotated data in the fiction domain: parts of two Sherlock Holmes stories by Arthur Conan Doyle. The SemEval 2010 training set consists of the SemEval 2007 data plus one document from the new domain. This document has about 7800 words in 438 sentences; it has 1492 annotated frame instances, including 3169 (overt and nu</context>
</contexts>
<marker>Baker, Ellsworth, Erk, 2007</marker>
<rawString>C. Baker, M. Ellsworth, and K. Erk. 2007. SemEval-2007 Task 19: Frame Semantic Structure Extraction. In Proc. of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>N Schneider</author>
<author>D Chen</author>
<author>N A Smith</author>
</authors>
<title>Probabilistic frame-semantic parsing.</title>
<date>2010</date>
<booktitle>In Proc. of NAACLHLT.</booktitle>
<contexts>
<context position="2249" citStr="Das et al., 2010" startWordPosition="329" endWordPosition="332">rame via linguistic conventions; rather, it is expected that the listener will be able to infer the appropriate relationships pragmatically. Certain types of implicit content and implicit reference are formalized in the theory of null instantiations (NIs) (Fillmore, 1986; Ruppenhofer, 2005). A complete frame-semantic analysis of text thus incorporates covert and overt predicate-argument information. In this paper, we describe a system for framesemantic analysis, evaluated on a semantic role labeling task for explicit and implicit arguments (§2). Extending the SEMAFOR 1.0 framesemantic parser (Das et al., 2010a; outlined in §3), we detect null instantiations via a simple two-stage pipeline: the first stage predicts whether a given role is null-instantiated, and the second stage (§4) predicts how it is null-instantiated, if it is not overt. We report performance on the SemEval 2010 test set under the full-SRL and NI-only conditions. 2 Data The SemEval 2007 task on frame-semantic parsing (Baker et al., 2007) provided a small (about 50,000 words and 2,000 sentences) dataset of news text, travel guides, and bureaucratic accounts of weapons stockpiles. Sentences in this dataset were fully annotated with</context>
<context position="3784" citStr="Das et al., 2010" startWordPosition="576" endWordPosition="579">in 438 sentences; it has 1492 annotated frame instances, including 3169 (overt and null-instantiated) argument annotations. The test set consists of two chapters from another story: Chapter 13 contains about 4000 words, 249 sentences, and 791 frames; Chapter 14 contains about 5000 words, 276 sentences, and 941 frames (see also Table 3). Figure 1 shows two annotated test sentences. All data released for the 2010 task include part-of-speech tags, lemmas, and phrase-structure trees from a parser, with head annotations for constituents. 3 Argument identification Our starting point is SEMAFOR 1.0 (Das et al., 2010a), a discriminative probabilistic framesemantic parsing model that operates in three stages: (a) rule-based target selection, (b) probabilistic disambiguation that resolves each target to a FrameNet frame, and (c) joint selection of text spans to fill the roles of each target through a second probabilistic model.1 1Das et al. (2010a) report the performance of this system on the complete SemEval 2007 task at 46.49% Fl. 264 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 264–267, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguis</context>
<context position="6075" citStr="Das et al., 2010" startWordPosition="945" endWordPosition="948"> relevant here. In this step, the system takes the target (frame-evoking) phrase t and corresponding frame type f predicted by the previous stages, and independently fills each role of f with a word or phrase from the sentence, or the symbol OTHER to indicate that the role has no (local) overt argument. Features used to inform this decision include aspects of the syntactic dependency parse (e.g. the path in the parse from the target to the argument); voice; word overlap of the argument with respect to the target; and part-of-speech tags within and around the argument. SEMAFOR as described in (Das et al., 2010a) does not distinguish between different types of null instantiations or find non-local referents. Given perfect input to stage (c), the system achieved 68.5% F1 on the SemEval 2007 data (exact match, evaluating overt arguments only). The only difference in our use of SEMAFOR’s argument identification module is in preprocessing the training data: we use dependency parses transformed from the headaugmented phrase-structure parses in the task data. Table 1 shows the performance of our argument identification model on this task’s test data. The SRL systems compared in (Ruppenhofer et al., 2010) </context>
<context position="12895" citStr="Das et al., 2010" startWordPosition="2084" endWordPosition="2087">c exemplar annotations. For each candidate span, we use two types of features to model the affinity between the head word and the role. The first indicates whether the head word is used as a filler for this role in at least one of the lexicographic exemplars. The second encodes the maximum distributional similarity to any word heading a filler of that role in the exemplars.6 In practice, we found that these features received negligible weight and had virtually no effect on performance, possibly due to data sparseness. An additional change in the feature set is that ordering/distance features (Das et al., 2010b, p. 13) were replaced with a feature indicating the number of sentences away the candidate is from the target.7 Otherwise, the null identifica491 DNI referents are found no more than three sentences prior; another 90 are in the same sentence as the target. 20 DNIs have referents which are not noun phrases. Six appear after the sentence containing its frame target; 28 appear at least 25 sentences prior. 60 have no referent. 5Our system ignores DNIs with no referent or with a referent in the same sentence as the target. Experiments with variants on these assumptions show that the larger the se</context>
</contexts>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010a. Probabilistic frame-semantic parsing. In Proc. of NAACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Das</author>
<author>N Schneider</author>
<author>D Chen</author>
<author>N A Smith</author>
</authors>
<title>SEMAFOR 1.0: A probabilistic frame-semantic parser.</title>
<date>2010</date>
<tech>Technical Report CMU-LTI-10-001,</tech>
<institution>Carnegie Mellon University.</institution>
<contexts>
<context position="2249" citStr="Das et al., 2010" startWordPosition="329" endWordPosition="332">rame via linguistic conventions; rather, it is expected that the listener will be able to infer the appropriate relationships pragmatically. Certain types of implicit content and implicit reference are formalized in the theory of null instantiations (NIs) (Fillmore, 1986; Ruppenhofer, 2005). A complete frame-semantic analysis of text thus incorporates covert and overt predicate-argument information. In this paper, we describe a system for framesemantic analysis, evaluated on a semantic role labeling task for explicit and implicit arguments (§2). Extending the SEMAFOR 1.0 framesemantic parser (Das et al., 2010a; outlined in §3), we detect null instantiations via a simple two-stage pipeline: the first stage predicts whether a given role is null-instantiated, and the second stage (§4) predicts how it is null-instantiated, if it is not overt. We report performance on the SemEval 2010 test set under the full-SRL and NI-only conditions. 2 Data The SemEval 2007 task on frame-semantic parsing (Baker et al., 2007) provided a small (about 50,000 words and 2,000 sentences) dataset of news text, travel guides, and bureaucratic accounts of weapons stockpiles. Sentences in this dataset were fully annotated with</context>
<context position="3784" citStr="Das et al., 2010" startWordPosition="576" endWordPosition="579">in 438 sentences; it has 1492 annotated frame instances, including 3169 (overt and null-instantiated) argument annotations. The test set consists of two chapters from another story: Chapter 13 contains about 4000 words, 249 sentences, and 791 frames; Chapter 14 contains about 5000 words, 276 sentences, and 941 frames (see also Table 3). Figure 1 shows two annotated test sentences. All data released for the 2010 task include part-of-speech tags, lemmas, and phrase-structure trees from a parser, with head annotations for constituents. 3 Argument identification Our starting point is SEMAFOR 1.0 (Das et al., 2010a), a discriminative probabilistic framesemantic parsing model that operates in three stages: (a) rule-based target selection, (b) probabilistic disambiguation that resolves each target to a FrameNet frame, and (c) joint selection of text spans to fill the roles of each target through a second probabilistic model.1 1Das et al. (2010a) report the performance of this system on the complete SemEval 2007 task at 46.49% Fl. 264 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 264–267, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguis</context>
<context position="6075" citStr="Das et al., 2010" startWordPosition="945" endWordPosition="948"> relevant here. In this step, the system takes the target (frame-evoking) phrase t and corresponding frame type f predicted by the previous stages, and independently fills each role of f with a word or phrase from the sentence, or the symbol OTHER to indicate that the role has no (local) overt argument. Features used to inform this decision include aspects of the syntactic dependency parse (e.g. the path in the parse from the target to the argument); voice; word overlap of the argument with respect to the target; and part-of-speech tags within and around the argument. SEMAFOR as described in (Das et al., 2010a) does not distinguish between different types of null instantiations or find non-local referents. Given perfect input to stage (c), the system achieved 68.5% F1 on the SemEval 2007 data (exact match, evaluating overt arguments only). The only difference in our use of SEMAFOR’s argument identification module is in preprocessing the training data: we use dependency parses transformed from the headaugmented phrase-structure parses in the task data. Table 1 shows the performance of our argument identification model on this task’s test data. The SRL systems compared in (Ruppenhofer et al., 2010) </context>
<context position="12895" citStr="Das et al., 2010" startWordPosition="2084" endWordPosition="2087">c exemplar annotations. For each candidate span, we use two types of features to model the affinity between the head word and the role. The first indicates whether the head word is used as a filler for this role in at least one of the lexicographic exemplars. The second encodes the maximum distributional similarity to any word heading a filler of that role in the exemplars.6 In practice, we found that these features received negligible weight and had virtually no effect on performance, possibly due to data sparseness. An additional change in the feature set is that ordering/distance features (Das et al., 2010b, p. 13) were replaced with a feature indicating the number of sentences away the candidate is from the target.7 Otherwise, the null identifica491 DNI referents are found no more than three sentences prior; another 90 are in the same sentence as the target. 20 DNIs have referents which are not noun phrases. Six appear after the sentence containing its frame target; 28 appear at least 25 sentences prior. 60 have no referent. 5Our system ignores DNIs with no referent or with a referent in the same sentence as the target. Experiments with variants on these assumptions show that the larger the se</context>
</contexts>
<marker>Das, Schneider, Chen, Smith, 2010</marker>
<rawString>D. Das, N. Schneider, D. Chen, and N. A. Smith. 2010b. SEMAFOR 1.0: A probabilistic frame-semantic parser. Technical Report CMU-LTI-10-001, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
<author>C R Johnson</author>
<author>M R L Petruck</author>
</authors>
<title>Background to FrameNet.</title>
<date>2003</date>
<journal>International Journal of Lexicography,</journal>
<volume>16</volume>
<issue>3</issue>
<contexts>
<context position="1187" citStr="Fillmore et al., 2003" startWordPosition="169" endWordPosition="172">ons, including non-local argument reference. Performance is evaluated on the task data with and without gold-standard overt arguments. In both settings, it fares the best of the submitted systems with respect to recall and Fl. 1 Introduction The theory of frame semantics (Fillmore, 1982) holds that meaning is largely structured by holistic units of knowledge, called frames. Each frame encodes a conventionalized gestalt event or scenario, often with conceptual dependents (participants, props, or attributes) filling roles to elaborate the specific instance of the frame. In the FrameNet lexicon (Fillmore et al., 2003), each frame defines core roles tightly coupled with the particular meaning of the frame, as well as more generic non-core roles (Ruppenhofer et al., 2006). Frames can be evoked with linguistic predicates, known as lexical units (LUs); role fillers can be expressed overtly and linked to the frame via (morpho)syntactic constructions. However, a great deal of conceptually-relevant content is left unexpressed or is not explicitly linked to the frame via linguistic conventions; rather, it is expected that the listener will be able to infer the appropriate relationships pragmatically. Certain types</context>
</contexts>
<marker>Fillmore, Johnson, Petruck, 2003</marker>
<rawString>C. J. Fillmore, C. R. Johnson, and M. R.L. Petruck. 2003. Background to FrameNet. International Journal of Lexicography, 16(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
</authors>
<title>Frame semantics.</title>
<date>1982</date>
<booktitle>In Linguistics in the Morning Calm,</booktitle>
<pages>111--137</pages>
<publisher>Hanshin Publishing Co.,</publisher>
<location>Seoul, South</location>
<contexts>
<context position="853" citStr="Fillmore, 1982" startWordPosition="119" endWordPosition="120">ith@cs}.cmu.edu Abstract This paper describes the SEMAFOR system’s performance in the SemEval 2010 task on linking events and their participants in discourse. Our entry is based upon SEMAFOR 1.0 (Das et al., 2010a), a frame-semantic probabilistic parser built from log-linear models. The extended system models null instantiations, including non-local argument reference. Performance is evaluated on the task data with and without gold-standard overt arguments. In both settings, it fares the best of the submitted systems with respect to recall and Fl. 1 Introduction The theory of frame semantics (Fillmore, 1982) holds that meaning is largely structured by holistic units of knowledge, called frames. Each frame encodes a conventionalized gestalt event or scenario, often with conceptual dependents (participants, props, or attributes) filling roles to elaborate the specific instance of the frame. In the FrameNet lexicon (Fillmore et al., 2003), each frame defines core roles tightly coupled with the particular meaning of the frame, as well as more generic non-core roles (Ruppenhofer et al., 2006). Frames can be evoked with linguistic predicates, known as lexical units (LUs); role fillers can be expressed </context>
</contexts>
<marker>Fillmore, 1982</marker>
<rawString>C. J. Fillmore. 1982. Frame semantics. In Linguistics in the Morning Calm, pages 111–137. Hanshin Publishing Co., Seoul, South Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Fillmore</author>
</authors>
<title>Pragmatically controlled zero anaphora.</title>
<date>1986</date>
<booktitle>In Proc. of Berkeley Linguistics Society,</booktitle>
<pages>95--107</pages>
<location>Berkeley, CA.</location>
<contexts>
<context position="1904" citStr="Fillmore, 1986" startWordPosition="281" endWordPosition="282">re generic non-core roles (Ruppenhofer et al., 2006). Frames can be evoked with linguistic predicates, known as lexical units (LUs); role fillers can be expressed overtly and linked to the frame via (morpho)syntactic constructions. However, a great deal of conceptually-relevant content is left unexpressed or is not explicitly linked to the frame via linguistic conventions; rather, it is expected that the listener will be able to infer the appropriate relationships pragmatically. Certain types of implicit content and implicit reference are formalized in the theory of null instantiations (NIs) (Fillmore, 1986; Ruppenhofer, 2005). A complete frame-semantic analysis of text thus incorporates covert and overt predicate-argument information. In this paper, we describe a system for framesemantic analysis, evaluated on a semantic role labeling task for explicit and implicit arguments (§2). Extending the SEMAFOR 1.0 framesemantic parser (Das et al., 2010a; outlined in §3), we detect null instantiations via a simple two-stage pipeline: the first stage predicts whether a given role is null-instantiated, and the second stage (§4) predicts how it is null-instantiated, if it is not overt. We report performanc</context>
</contexts>
<marker>Fillmore, 1986</marker>
<rawString>C. J. Fillmore. 1986. Pragmatically controlled zero anaphora. In Proc. of Berkeley Linguistics Society, pages 95–107, Berkeley, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ruppenhofer</author>
<author>M Ellsworth</author>
<author>M R L Petruck</author>
<author>C R Johnson</author>
<author>J Scheffczyk</author>
</authors>
<title>FrameNet II: extended theory and practice.</title>
<date>2006</date>
<contexts>
<context position="1342" citStr="Ruppenhofer et al., 2006" startWordPosition="194" endWordPosition="197"> it fares the best of the submitted systems with respect to recall and Fl. 1 Introduction The theory of frame semantics (Fillmore, 1982) holds that meaning is largely structured by holistic units of knowledge, called frames. Each frame encodes a conventionalized gestalt event or scenario, often with conceptual dependents (participants, props, or attributes) filling roles to elaborate the specific instance of the frame. In the FrameNet lexicon (Fillmore et al., 2003), each frame defines core roles tightly coupled with the particular meaning of the frame, as well as more generic non-core roles (Ruppenhofer et al., 2006). Frames can be evoked with linguistic predicates, known as lexical units (LUs); role fillers can be expressed overtly and linked to the frame via (morpho)syntactic constructions. However, a great deal of conceptually-relevant content is left unexpressed or is not explicitly linked to the frame via linguistic conventions; rather, it is expected that the listener will be able to infer the appropriate relationships pragmatically. Certain types of implicit content and implicit reference are formalized in the theory of null instantiations (NIs) (Fillmore, 1986; Ruppenhofer, 2005). A complete frame</context>
</contexts>
<marker>Ruppenhofer, Ellsworth, Petruck, Johnson, Scheffczyk, 2006</marker>
<rawString>J. Ruppenhofer, M. Ellsworth, M. R.L. Petruck, C. R. Johnson, and J. Scheffczyk. 2006. FrameNet II: extended theory and practice.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ruppenhofer</author>
<author>C Sporleder</author>
<author>R Morante</author>
<author>C Baker</author>
<author>M Palmer</author>
</authors>
<date>2010</date>
<booktitle>SemEval-2010 Task 10: Linking Events and Their Participants in Discourse. In Proc. of SemEval.</booktitle>
<contexts>
<context position="2926" citStr="Ruppenhofer et al., 2010" startWordPosition="437" endWordPosition="440">a a simple two-stage pipeline: the first stage predicts whether a given role is null-instantiated, and the second stage (§4) predicts how it is null-instantiated, if it is not overt. We report performance on the SemEval 2010 test set under the full-SRL and NI-only conditions. 2 Data The SemEval 2007 task on frame-semantic parsing (Baker et al., 2007) provided a small (about 50,000 words and 2,000 sentences) dataset of news text, travel guides, and bureaucratic accounts of weapons stockpiles. Sentences in this dataset were fully annotated with frames and their arguments. The SemEval 2010 task (Ruppenhofer et al., 2010) adds annotated data in the fiction domain: parts of two Sherlock Holmes stories by Arthur Conan Doyle. The SemEval 2010 training set consists of the SemEval 2007 data plus one document from the new domain. This document has about 7800 words in 438 sentences; it has 1492 annotated frame instances, including 3169 (overt and null-instantiated) argument annotations. The test set consists of two chapters from another story: Chapter 13 contains about 4000 words, 249 sentences, and 791 frames; Chapter 14 contains about 5000 words, 276 sentences, and 941 frames (see also Table 3). Figure 1 shows two </context>
<context position="6674" citStr="Ruppenhofer et al., 2010" startWordPosition="1039" endWordPosition="1042">ribed in (Das et al., 2010a) does not distinguish between different types of null instantiations or find non-local referents. Given perfect input to stage (c), the system achieved 68.5% F1 on the SemEval 2007 data (exact match, evaluating overt arguments only). The only difference in our use of SEMAFOR’s argument identification module is in preprocessing the training data: we use dependency parses transformed from the headaugmented phrase-structure parses in the task data. Table 1 shows the performance of our argument identification model on this task’s test data. The SRL systems compared in (Ruppenhofer et al., 2010) all achieved precision in the mid 60% range, but SEMAFOR achieved substantially higher recall, F1, and label accuracy on this subtask. (The table also shows how performance of our model degrades when half or all of the new data are not used for training; the 9% difference in recall suggests the importance of in-domain training data.) 4 Null instantiation detection In this subtask, which follows the argument identification subtask (§3), our system seeks to characterize non-overt core roles given gold standard local frame-argument annotations. Consider the following passage from the test data: </context>
<context position="16510" citStr="Ruppenhofer et al., 2010" startWordPosition="2669" endWordPosition="2672">ble shows that the system is reasonably effective at discriminating NIs from masked roles, one specific to the frame and the role, one specific to the role name only, and one to learn the overall bias of the data. 8For feature engineering we held out the last 25% of sentences from the new training document as development data, retraining on the full training set for final evaluation. 9We used Nils Reiter’s FrameNet API, version 0.4 (http://www.cl.uni-heidelberg.de/trac/ FrameNetAPI) in processing the data. 10The other system participating in the NI-only subtask had much lower NI recall of 8% (Ruppenhofer et al., 2010). 11We do not predict any DNIs without referents or incorporated roles, though the evaluation script gives us credit when we predict INI for these cases. but DNI identification suffers from low recall and INI identification from low precision. Data sparseness is likely the biggest obstacle here. To put this in perspective, there are over 20,000 training examples of overt arguments, but fewer than 600 examples of null instantiations, two thirds of which do not have referents. Without an order of magnitude more NI data (at least), it is unlikely that a supervised learner could generalize well en</context>
</contexts>
<marker>Ruppenhofer, Sporleder, Morante, Baker, Palmer, 2010</marker>
<rawString>J. Ruppenhofer, C. Sporleder, R. Morante, C. Baker, and M. Palmer. 2010. SemEval-2010 Task 10: Linking Events and Their Participants in Discourse. In Proc. of SemEval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ruppenhofer</author>
</authors>
<date>2005</date>
<note>Regularities in null instantiation.</note>
<contexts>
<context position="1924" citStr="Ruppenhofer, 2005" startWordPosition="283" endWordPosition="284">ore roles (Ruppenhofer et al., 2006). Frames can be evoked with linguistic predicates, known as lexical units (LUs); role fillers can be expressed overtly and linked to the frame via (morpho)syntactic constructions. However, a great deal of conceptually-relevant content is left unexpressed or is not explicitly linked to the frame via linguistic conventions; rather, it is expected that the listener will be able to infer the appropriate relationships pragmatically. Certain types of implicit content and implicit reference are formalized in the theory of null instantiations (NIs) (Fillmore, 1986; Ruppenhofer, 2005). A complete frame-semantic analysis of text thus incorporates covert and overt predicate-argument information. In this paper, we describe a system for framesemantic analysis, evaluated on a semantic role labeling task for explicit and implicit arguments (§2). Extending the SEMAFOR 1.0 framesemantic parser (Das et al., 2010a; outlined in §3), we detect null instantiations via a simple two-stage pipeline: the first stage predicts whether a given role is null-instantiated, and the second stage (§4) predicts how it is null-instantiated, if it is not overt. We report performance on the SemEval 201</context>
<context position="8678" citStr="Ruppenhofer (2005)" startWordPosition="1383" endWordPosition="1384">ough our model incorrectly labels this matter from the previous sentence as a DNI referent for this role). The fourth core role, Offense, is not annotated as a null instantiation because it belongs to the same CoreSet as Charges—which is to say they are relevant in a similar way to the frame as a whole (both pertain to the rationale for the arrest) and only one is typically expressed.2 We will use the term masked to refer to any non-overt core role which does not need to be specified as null-instantiated due to a structural connection to another role in its frame. The typology of NIs given in Ruppenhofer (2005) and employed in the annotation distinguishes anaphoric/definite NIs from existential or indefinite null instantiations (INIs). Rather than having a specific referent accessible in the discourse, INIs are left vague or deemphasized, as in 2If the FrameNet lexicon marks a pair of roles within a frame as being in a CoreSet or Excludes relationship, then filling one of them satisfies the requirement that the other be (expressly or implicitly) present in the use of the frame. 265 NI-only Full Chapter 13 Chapter 14 Training Data Prec. Rec. Fl Prec. Rec. Fl SemEval 2010 new: 100% 0.40 0.64 0.50 0.53</context>
<context position="14661" citStr="Ruppenhofer, 2005" startWordPosition="2370" endWordPosition="2371">ll of the new features are instantiated in three forms: 266 tion model uses the same features as the argument identification model. The theory of null instantiations holds that the grammaticality of lexically-licensed NI for a role in a given frame depends on the LU: for example, the verbs buy and sell share the same frame but differ as to whether the Buyer or Seller role may be lexically null-instantiated. Our model’s feature set is rich enough to capture this in a soft way, with lexicalized features that fire, e.g., when the Seller role is null-instantiated and the target is buy. Moreover, (Ruppenhofer, 2005) hypothesizes that each role has a strong preference for one interpretation (INI or DNI) when it is lexically null-instantiated, regardless of LU. This, too, is modeled in our feature set. In theory these trends should be learnable given sufficient data, though it is doubtful that there are enough examples of null instantiations in the currently available dataset for this learning to take place. 4.2 Evaluation We trained the model on the non-overt arguments in the new SemEval 2010 training document, which has 580 null instantiations—303 DNIs and 277 INIs.8,9 Then we used the task scoring proce</context>
</contexts>
<marker>Ruppenhofer, 2005</marker>
<rawString>J. Ruppenhofer. 2005. Regularities in null instantiation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>