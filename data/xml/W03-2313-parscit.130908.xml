<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000146">
<title confidence="0.9911">
Generation of Video Documentaries from Discourse Structures
</title>
<author confidence="0.7885035">
Cesare Rocchi Massimo Zancanaro
ITC Irst ITC Irst
</author>
<affiliation confidence="0.812052">
Trento Italy Trento Italy
</affiliation>
<email confidence="0.983904">
rocchi@itc.it zancana@itc.it
</email>
<sectionHeader confidence="0.982186" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999835130434782">
Recent interests in the use of multime-
dia presentations and multimodal inter-
faces have raised the need for the au-
tomatic generation of graphics and es-
pecially temporal media. This paper
presents an engine to build video doc-
umentaries from annotated audio com-
mentaries. The engine, taking into
consideration the discourse structure of
the commentary, plans the segmentation
in shots as well as the camera move-
ments and decides the transition effects
among shots. The output is a complete
script of a &amp;quot;video presentation&amp;quot;, with in-
structions for synchronizing images and
movements with the playing of the au-
dio commentary. The language of cine-
matography and a set of strategies sim-
ilar to those used in documentaries are
the basic resources to plan the anima-
tion. Strategies encompass constraints
and conventions normally used in build-
ing video documentaries.
</bodyText>
<sectionHeader confidence="0.997739" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999292">
In the last decade there has been an increasing
interest in the generation of multimedia presen-
tations and a growing tendency towards the use
of multi-modal interfaces (Wahlster et al., 1993;
Maybury, 1993). These interests have raised the
need for automatic generation not only of natural
language, but also graphics and especially tempo-
ral media (Andr√©, 2000).
In this paper, an engine to build video sequences
of images starting from an audio commentary is
described. The input for the engine is a represen-
tation of (possibly automatically) generated ver-
bal commentary. The engine, taking into consid-
eration the discourse structure of the commentary,
retrieves the most appropriate set of images from
an annotated database, plans the segmentation in
shots as well as the camera movements and fi-
nally decides the transition effects among shots.
The output of the engine is a complete script of
a &amp;quot;video presentation&amp;quot;, with instructions for syn-
chronizing images and movements with the play-
ing of the audio commentary.
The language of cinematography (Metz, 1974),
including shot segmentation, camera movements
and transition effects, is the basic resource to plan
the animation and to synchronize the visual and
the verbal parts of the presentation. In generat-
ing animations, a set of strategies similar to those
used in documentaries are employed. Two broad
classes of strategies have been identified. The
first class encompasses constraints imposed by
the grammar of cinematography, while the second
deals with conventions normally used in guiding
camera movements in the production of documen-
taries.
After a short discussion on related works, rele-
vant concepts and terminology of cinematography
are introduced in section 3. Section 4 briefly sum-
marizes the Rhetorical Structure Theory (RST) for
the analysis of discourse structure. In section 5 we
</bodyText>
<page confidence="0.997575">
95
</page>
<bodyText confidence="0.999649666666667">
present some of the heuristics that we have bor-
rowed from the field of cinematography. In sec-
tion 6 we illustrate the architecture of the engine
and its parts. In section 7 we give some examples
of how the engine works. Finally, in section 8, we
outline conclusions and future work.
</bodyText>
<sectionHeader confidence="0.999929" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999771068965517">
One of the first case studies of the generation of
&amp;quot;motion presentations&amp;quot; is the work of (Karp and
Feiner, 1993). Their system generates scripts for
animation using top-down hierarchical planning
techniques. (Christianson et al., 1996) presents a
successful attempt to encode several of the princi-
ples of cinematography in the Declarative camera
control language.
Similar systems are BETTY (Butz, 1994) and
CATHI (Butz, 1997). BETTY is an animation
planner, which generates scripts for animated pre-
sentations. The CATHY system generates on-line
descriptions of 3D animated clips for the illustra-
tion of technical devices, in the context of a coor-
dinated multimedia document.
Animated presentations have been successfully
employed also in multimodal frameworks for the
generation of explanations (Daniel et al., 1999)
and in learning environments (Bares and Lester,
1997).
The novelty of our approach lies in the use
of rhetorical structure of the accompanying audio
commentary in planning the video. In particular,
knowledge of rhetorical structure is extremely use-
ful in taking decisions related to the punctuation of
the video, in order to reflect the rhythm of the au-
dio commentary and its communicative goals. In
our view, the verbal part of the documentary al-
ways drives the generation of the visual part.
</bodyText>
<sectionHeader confidence="0.905193" genericHeader="method">
3 Relevant concepts and terminology
</sectionHeader>
<bodyText confidence="0.9998915">
According to Metz (1974), cinematic representa-
tion is not like a human language, which is defined
by a set of grammatical rules. It is nevertheless
guided by a set of generally accepted conventions.
These guidelines may be used for developing mul-
timedia presentations that can be best perceived by
the viewer. Following, we briefly summarize the
basic terminology of cinematography.
</bodyText>
<subsectionHeader confidence="0.999695">
3.1 Shot and camera movements
</subsectionHeader>
<bodyText confidence="0.999898">
The shot is the basic unit of a video sequence. In
the field of cinematography a shot is defined as a
continuous view from single camera without inter-
ruption. Since we only deal with still images, we
define a shot as a sequence of camera movements
applied to the same image.
The basic camera movements are pan, from
&amp;quot;panorama&amp;quot;, a rotation of the camera along the x-
axis, tilt a rotation along the y-axis and dolly, a
movement along the z-axis.
</bodyText>
<subsectionHeader confidence="0.999617">
3.2 Transition effects
</subsectionHeader>
<bodyText confidence="0.999960166666667">
Transitions among shots are considered as the
punctuation symbols of cinematography; they af-
fect the rhythm of the discourse and the message
conveyed by the video. The main effects are cut -
the first frame of the shot to be displayed immedi-
ately replaces the last frame of the shot currently
on display; fade - a shot is gradually replaced by
(fade out) or gradually replaces (fade in) a black
screen or another shot and cross fade (or dissolve)
which is the composition of a fade out on the dis-
played shot and a fade in applied to the shot to be
shown.
</bodyText>
<sectionHeader confidence="0.987229" genericHeader="method">
4 Rhetorical Structure Theory
</sectionHeader>
<bodyText confidence="0.980425392857143">
Rhetorical Structure Theory (Mann and Thomp-
son, 1987) allows the analysis of discourse struc-
ture in terms of dependency trees, with each node
of the tree being a text span. Each branch of the
tree represents a relationship between two nodes.
One node is called the nucleus and the other is
called the satellite. The information in the satel-
lite relates to that found in the nucleus in that it
expresses an idea related to what is said in the nu-
cleus. For example, a background relation holds
when a satellite provides a context to the infor-
mation expressed in the nucleus. Figure 1 shows
an example of a portion of a rhetorical tree. The
second paragraph provides details with respect to
the content expressed in the first paragraph. This
additional information acts as a sort of reinforce-
ment for what has been previously said in the first
paragraph and consequently facilitates the absorp-
tion of information. In the original formulation
by Mann and Thompson the theory posited twenty
96
At the bottom on the tight is a The choice of a tournament for the
blacksmith&apos;s workshop, a plebeian month of February is related to the
antithesis to the tournament going jousts and revelries that took place in
on in the upper part of the painting carnival time.
which is chiefly an aristocratic
activity.
2
</bodyText>
<figureCaption confidence="0.998665">
Figure 1: An example of Rhetorical Tree.
</figureCaption>
<bodyText confidence="0.987892774647887">
different rhetorical relations. From this original
repository we borrowed a set of relations (elabo-
ration, background, sequence and circumstance),
which are commonly used in descriptive text, like
those we have analyzed (see Section 6.1).
5 Heuristics and constraints of
cinematography
Directors and film critics have identified several
heuristics for making good movies. In design-
ing a shot, it is important to consider the message
that it has to convey and the (semantic) relations
with the previous and following messages. Cam-
era movements can be used to signal some of these
semantic relations. For example, according to Ar-
ijon (1976), panning and tilting can be used to re-
veal spatial relations among objects and to move
the watcher&apos;s attention from one center of interest
to another; dollying can be employed to focus the
attention on a particular zone or object previously
displayed. For example, if an object is currently
displayed and the following message deepens one
aspect of it, a zoom on that aspect can be chosen.
Besides rules for movement selection, cine-
matographers have also identified a set of con-
straints on possible camera movement combina-
tions, in order to ensure a pleasant presentation. In
particular, each camera movement has to be &amp;quot;con-
sistent&amp;quot; with respect to the previous movements.
The watcher, looking at a movie in which camera
moves to one side and then to the opposite one, can
misunderstand the underlying message and expe-
rience some difficulties in following the stream
of the presentation. For example, if the previous
move is a pan towards the right the following ef-
fect cannot be a pan towards the left neither along
the same path nor along similar paths. In general
when a camera movement is chosen it constrains
the choice of the following movements.
Another important feature of a movie is cohe-
sion. A video sequence has to be a continuum, an
uninterrupted stream in which each piece is con-
nected to the others and is part of a whole. To
achieve cohesion in designing the visual part of a
presentation it is worth considering the relations
among the new information to be delivered and
those already given (discourse history) and to pro-
vide rhetorical strategies to build the presentation.
The combination of rules and constraints encode
some of the basic &amp;quot;principles of cinematography&amp;quot;,
which have been identified with the help of an
expert director of documentaries (see also Arijon
1976).
Rules and constraints are the core on which the
system relies. They encode the rhetorical strate-
gies that are the basic resource for: (i) selecting
appropriate images, (ii) designing the presenta-
tion structure, (iii) completing each shot, (iv) syn-
chronizing the visual part with the audio commen-
tary and avoiding the &amp;quot;seasickness&amp;quot; effect. Rules
are formalized in a context sensitive &amp;quot;presentation
grammar&amp;quot;, are fired by a forward chaining mech-
anism and are relative to: (i) rhetorical relations
among the text spans; (ii) the geometric properties
of images selected from the information repository
and (iii) the topics matching among segments and
images. An example of rule is given in Figure 2.
The rule applies when a segment has a relation of
type background or circumstance; in that case the
segment is assigned to a new shot.
Constraints are conditions that forbid particular
combinations of camera movements and are tested
</bodyText>
<figure confidence="0.934132571428571">
(defrule split (segment)
(conditions
(or (has-relation segment background)
(has-relation segment circumstance)))
(actions
(init-shot shot)
(add-segment segment shot)))
</figure>
<figureCaption confidence="0.9863645">
Figure 2: An example of rule for shot initializa-
tion.
</figureCaption>
<page confidence="0.944381">
97
</page>
<figure confidence="0.652461625">
(defconstraint zoom an
(var my (get-previous-movement))
(var my 2 (get-previous-movement my))
(and
(not (equal my zoom our)
|XML |xmlLoc_0 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_no bi_xmlSFBIA_new bi_xmlPara_continue
(not (equal my2 zoom out))))
|XML |xmlLoc_0 xmlBold_no xmlItalic_no xmlFontSize_smaller xmlPic_no xmlTable_no xmlBullet_no bi_xmlSFBIA_new bi_xmlPara_continue
</figure>
<figureCaption confidence="0.994154">
Figure 3: An example of constraint.
</figureCaption>
<bodyText confidence="0.99991125">
according to the type of movement proposed by
the engine and the sequence of past movements.
An example of constraint is shown in Figure 3.
Potentially each camera movement can lead to an
inconsistent sequence. To select a zoom-in move-
ment it is worth considering whether the previous
move or the penultimate one is a zoom-out; if not,
then a zoom-in applies.
</bodyText>
<sectionHeader confidence="0.984338" genericHeader="method">
6 The Video Planner Engine
</sectionHeader>
<bodyText confidence="0.850649">
The engine is structured as in Figure 4. When a
</bodyText>
<figureCaption confidence="0.999544">
Figure 4: The system architecture.
</figureCaption>
<bodyText confidence="0.977029333333333">
video for a given commentary is requested, the en-
gine analyses the discourse structure of the com-
mentary and selects an appropriate set of images
to be presented. The generation chain consists of
four phases:
Detail Association: a detail is associated with
each segment of the commentary;
Shot initialization and structure planning: a
candidate structure for the final presentation
</bodyText>
<figure confidence="0.995775689655172">
&lt;movie id=&amp;quot;january&amp;quot;‚Ä∫
&lt;shots&gt;
&lt;shot id=&amp;quot;shot603&amp;quot; image=&amp;quot;det01&amp;quot;‚Ä∫
&lt;video-track&gt;
pa ice duration =&amp;quot;2&amp;quot;/&gt;
&lt;/video-track&gt;
&lt;audio-track&gt;
&lt;play audio=&amp;quot;january.wav&amp;quot;/&gt;
&lt;/audio-track&gt;
&lt;/shot&gt;
&lt;shot id=&amp;quot;snot605&amp;quot; image=&amp;quot;det01&amp;quot;‚Ä∫
&lt;video-track&gt;
&lt;pause duration=&amp;quot;1&amp;quot;/&gt;
&lt;zoom duration &amp;quot;4&amp;quot; scale=&amp;quot;4&amp;quot;/&gt;
&lt;pause duration =&amp;quot;2&amp;quot;/&gt;
&lt;/video-track&gt;
&lt;audio-track&gt;
&lt;audio-pause duration =&amp;quot;3&amp;quot;/&gt;
&lt;play audio=&amp;quot;snowball-fight.wav&amp;quot;/&gt;
&lt;audio-pause duration &amp;quot;1/&gt;
&lt;play audio=&amp;quot;castle.wav&amp;quot;/&gt;
&lt;/audio-track&gt;
&lt;/shot&gt;
&lt;/shots&gt;
&lt;editing&gt;
&lt;d splay shot=&amp;quot;shot603&amp;quot;/&gt;
&lt;crossfade shot=&amp;quot;shot605&amp;quot; duration=&amp;quot;1&amp;quot;/&gt;
ed ng
&lt;/movie&gt;
</figure>
<figureCaption confidence="0.99999">
Figure 5: Example of script.
</figureCaption>
<bodyText confidence="0.966826545454545">
is elaborated, taking into consideration the
rhetorical structure of the commentary;
Shot Completion: camera movements between
details are planned. Constraints are consid-
ered in order to avoid &amp;quot;inconsistencies&amp;quot;;
Editing: transitions among effects are selected
according to the rhetorical structure of the
commentary.
The output is a complete script for the video
and the audio channels encoded in a renderer-
independent markup language (see Figure 5).
</bodyText>
<subsectionHeader confidence="0.952401">
6.1 Resources
</subsectionHeader>
<bodyText confidence="0.99995">
The video engine requires access to information
about the structure of the data and a certain amount
of knowledge about the domain.
As domain of application we have chosen the
Cycle of the Months of Torre Aquila at the B uon-
consiglio Castle in the city of Trento (Italy). This
fresco is composed of eleven panels (each one rep-
resenting a month) painted during the 1400s and
illustrates the activities of aristocrats and peasants
throughout the year.
As a case study we have collected a set of text
that have been annotated by means of RST (see be-
low). The nature of these texts, taken from a guide
</bodyText>
<figure confidence="0.997487545454545">
Annotated
text
Image
repository
Detail association
Rules
Shot initialization
Shot completion
Constraints
Editing
Script
</figure>
<page confidence="0.997698">
98
</page>
<bodyText confidence="0.944818785714286">
&lt;segment Id &apos;01&amp;quot; parent &amp;quot;root&amp;quot; relname=&amp;quot;none&amp;quot;
topic=&amp;quot;tournament&amp;quot; audio=&amp;quot;castle.wav&amp;quot;
duration=&amp;quot;3&amp;quot; &gt;
At the bottom on the right is a blacksmith&apos;s
workshop, a plebeian antithesis to the
tournament going on in the upper part of the
painting which is chiefly an aristocratic
activity. &lt;/segment&gt;
&lt;segment id=&amp;quot;02&amp;quot; parent=&amp;quot;01&amp;quot;
relname=&amp;quot;elaboration&amp;quot; topics=&amp;quot;castle&amp;quot;
audio=&amp;quot;windows.wav&amp;quot; duration =&amp;quot;2&amp;quot; 7&gt;
The differences between the various styles of
construction have been reproduced extremely
carefully.
</bodyText>
<figure confidence="0.53269">
&lt;/segment&gt;
</figure>
<figureCaption confidence="0.999409">
Figure 6: Enriched RST annotation of a text.
</figureCaption>
<bodyText confidence="0.999864194444445">
of Torre Aquila, is descriptive and the prevailing
rhetorical relations are elaboration, sequence, cir-
cumstance and background. At the moment we
have favoured a sentence-by-sentence segmenta-
tion and the average size of the resulting trees
ranges from seven to ten nodes.
The domain knowledge is encoded in a simple
taxonomy, that is a set of keywords - called topics
- representing entities, such as characters and an-
imals, and processes, such as hunting and leisure
activities. At this phase of the work, only one rela-
tion between topics is defined, the member-of rela-
tion, that denotes that a topic belongs to a particu-
lar class. For instance, the topic fox_ hunting is in a
member-of relation with the topic hunting, which
means thatfox_hunting is a form of hunting. At the
moment, even if simple, knowledge representation
is rich enough to accomplish our purposes.
The main input of the engine is a textual
representation of the commentary annotated ac-
cording to its rhetorical structure (see Figure 6).
Additionally, the main concept of each segment is
specified as well as the duration in milliseconds
of the segment when played (although in Figure 6
the transcription of the commentary is shown, it
is never used). Finally, the engine employs a
database of images. For each images, the relevant
details depicted have to be specified both in terms
of their bounding boxes and in terms of the topics
they represent. For example, Figure 7 illustrates
the details for the panel of the month of January,
annotated as in Figure 8. This picture consists of
three main details: the snowball fight at the bottom
(1), the castle at the top on the right (2) and the
hunting scene (3), beside the castle. Within each
detail it is possible to identify further details, as in
</bodyText>
<figureCaption confidence="0.998484">
Figure 7: Details for the picture of January.
</figureCaption>
<bodyText confidence="0.644931">
the case of the castle, which contains the detail of
windows (a).
</bodyText>
<subsectionHeader confidence="0.992824">
6.2 Phase 1: Detail association
</subsectionHeader>
<bodyText confidence="0.718222875">
In this phase the system assigns one or more de-
tails to each segment of the commentary. This op-
eration is performed by searching the image repos-
itory for details with the same topic of the seg-
ment.
&lt;db month=&amp;quot;january&amp;quot;‚Ä∫
&lt;image id=&amp;quot;jan img&amp;quot; source=&amp;quot;january full.jpg&amp;quot;
height=&amp;quot;713&amp;quot; width=&amp;quot;500&amp;quot;/&gt;
&lt;detail Id &amp;quot;01&amp;quot; topic=&amp;quot;january&amp;quot; parent &amp;quot;root&amp;quot;
img=&amp;quot;jan amp&amp;quot; coords=&amp;quot;0,0,500,713&amp;quot;/&gt;
de au id-02&amp;quot; topic=&amp;quot;snowball-fight&amp;quot;
parent=&amp;quot;01&amp;quot; img=&amp;quot;january img&amp;quot;
coords=&amp;quot;20,430,460,650&amp;quot;/&gt;
&lt;detail Id &apos;03&amp;quot; topic &amp;quot;castle&amp;quot; parent &amp;quot;01&amp;quot;
img=&amp;quot;january img&amp;quot; coords=&amp;quot;12,50,330,430&amp;quot;/&gt;
&lt;detail id-&amp;quot;03a&amp;quot; topic=&amp;quot;windowl&amp;quot; parent=&amp;quot;03&amp;quot;
</bodyText>
<figure confidence="0.4979315">
img=&amp;quot;january img&amp;quot; coords=&amp;quot;190,55,315,300&amp;quot;/&gt;
&lt;detail id &amp;quot;04&amp;quot; topic &amp;quot;hunters&amp;quot; parent &amp;quot;0
img=&amp;quot;january_img&amp;quot; coords=&amp;quot;300,105,475,400&amp;quot;/&gt;
&lt;/db&gt;
</figure>
<figureCaption confidence="0.999623">
Figure 8: Annotation of the image in figure 7.
</figureCaption>
<page confidence="0.99404">
99
</page>
<subsectionHeader confidence="0.989646">
6.3 Phase 2: Shot initialization
</subsectionHeader>
<bodyText confidence="0.999968964285714">
In this phase, shots are initialized taking into con-
sideration the rhetorical structure of the commen-
tary. At the moment the nucleus/satellite distinc-
tion is not taken into account. The result of phase
2 is a candidate structure for the final presentation.
The processing is guided by a set of rules, which
are fired when particular configurations of rhetor-
ical relations are matched (see Figure 2). For ex-
ample a relation of type elaboration or sequence
signals a smooth transition from the current topic
to new information that is strictly related to it; it is
thus preferable to aggregate segments in the same
shot and to exploit camera movements. Back-
ground and circumstance tend to highlight the in-
troduction of new information that provides a con-
text in which the following or the previous mes-
sages can be interpreted. They tend to break the
flow of the discourse. It is thus preferable to split
the segments in two different shots so that, in the
next phase, it is possible to exploit proper tran-
sition effects in order to emphasize that change
of rhythm. There are cases in which the struc-
ture planned in this phase is revised during succes-
sive stages of computation. For example, to avoid
the &amp;quot;seasickness&amp;quot; effect the system can apply con-
straints and then modify the previously planned
structure by adding new shots (see examples in
section 7).
</bodyText>
<subsectionHeader confidence="0.996503">
6.4 Phase 3: Shot completion
</subsectionHeader>
<bodyText confidence="0.998631333333333">
This is the phase in which the engine incremen-
tally completes each shot by illustrating each of its
segments. In performing this task the engine traces
the camera movements already planned. When
a candidate move is proposed the system verifies
whether it is suitable or not according to the list
of past camera movements and the constraints im-
posed over that type of movement. Constraints en-
code the cinematographer&apos;s expertise in selecting
and applying camera movements in order to ob-
tain &amp;quot;well-formed&amp;quot; shots. For instance, when a
panning movement is proposed where the previ-
ous movement is also a panning, the system has to
check if the resulting sequence is suitable. Simple
constraints include:
</bodyText>
<listItem confidence="0.939177777777778">
‚Ä¢ When the previous movement is a dolly-out
a dolly-in cannot be applied;
‚Ä¢ When the previous movement is a dolly-in
a dolly-out cannot be the subsequent move-
ment;
‚Ä¢ When a panning or a tilting is along a similar
path and in the opposite direction of the pre-
vious movement
that panning or tilting cannot be applied.
</listItem>
<bodyText confidence="0.999872">
Constraints encode schemes of forbidden move-
ments and when one of them is not satisfied the
proposed move is rejected. In this case the engine
initializes a new shot, declares the previous one
completed and associates the remaining segments
to the new shot.
</bodyText>
<subsectionHeader confidence="0.992791">
6.5 Phase 4: Movie Editing
</subsectionHeader>
<bodyText confidence="0.9998418">
This is the phase in which the engine chooses the
&amp;quot;punctuation&amp;quot; of the presentation. Movie edit-
ing is achieved by selecting appropriate transitions
among shots. In order to reflect the rhythm of the
discourse, the choice of transition effects is guided
by the rhetorical structure of the commentary. The
system retrieves the last segment of the shot dis-
played and the first segment of the shot to be pre-
sented and plans the transition according to the
following rules:
</bodyText>
<listItem confidence="0.938698416666667">
‚Ä¢ If two segments are linked by a relation of
type elaboration
a short cross fade applies;
‚Ä¢ If two segments are linked by a relation of
type background or circumstance
a long cross fade applies.
‚Ä¢ If two segments are linked by a relation of
type sequence
a cut applies.
‚Ä¢ If a relation of type enumeration holds among
two or more segments
a rapid sequence of cut applies.
</listItem>
<bodyText confidence="0.9999514">
These rules have been selected according to the
observations about the usual employment of tran-
sition effects in the field of cinematography (Ari-
jon, 1976). Fade effects are fit for smooth transi-
tion, when there is a topic shift or when the center
</bodyText>
<page confidence="0.979773">
100
</page>
<figureCaption confidence="0.999944">
Figure 9: The &amp;quot;Tournament&amp;quot; example.
</figureCaption>
<bodyText confidence="0.999973">
of interest changes but the new topic is related to
the old one, as in the case of elaboration or back-
ground. Cut is more appropriate for abrupt and
rapid changes, to emphasize the introduction of a
new concept, as in the case of sequence. A spe-
cial case holds when the verbal commentary enu-
merates a set of subjects or different aspects of
the same object; in those cases a rapid sequence
of cuts can be used to visually enumerate the ele-
ments described.
</bodyText>
<sectionHeader confidence="0.995484" genericHeader="method">
7 Examples
</sectionHeader>
<bodyText confidence="0.999966588235294">
The first example concerns the rhythm of the dis-
course (Figure 9). Since the topic of both seg-
ments is the same, the text could be visually rep-
resented by displaying the same image during the
playing of both the first and the second audio com-
mentary. In this case a cross fade effect helps
the user to understand that background informa-
tion is going to be provided. In fact, the second
segment provides contextual information to sup-
port the user in understanding the information pre-
sented in the first paragraph. The first image is
thus presented while the audio of the first segment
is played; then, when the audio switches to the
second segment, the image is enlarged to cover
the entire panel and finally refocused on the de-
tail once the audio has stopped. By adopting this
strategy the system generates a movie that reflects
</bodyText>
<figureCaption confidence="0.899">
Figure 10: The &amp;quot;Castle&amp;quot; example.
</figureCaption>
<bodyText confidence="0.999591">
the discourse structure of the text and the rhythm
of the discourse, supporting the same communica-
tive goals of the verbal part of the presentation.
The second example concerns the application of
constraints in order to avoid an inconsistent se-
quence of camera movements (Figure 10). The
text first describes the castle on the left. In this
case the system, after a brief pause on the whole
scene, selects a dolly-in movement, magnifying
the detail of the castle (1). Then a second dolly-in
is applied to focus on the castle&apos;s windows (2). Fi-
nally, in order to focus on the hunting scene (3) the
camera should dolly out and then move towards
right, but this combination is forbidden by the con-
straint on dolly-out. In this case the engine revises
the structure of the movie. It declares completed
the current shot, initializes a new shot and asso-
ciates the remaining segments with it.
</bodyText>
<sectionHeader confidence="0.965281" genericHeader="conclusions">
8 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.944455">
In this paper we have presented an engine to gen-
erate video sequences starting from an audio com-
mentary. First, we have identified a set of cine-
</bodyText>
<figure confidence="0.966384230769231">
Background
At the bottom on the nght iso blacksmith&apos;s The choice of a tournament for
workshop, a plebeian antithesis to the the month of February is related
tournament going coin the upper part of to the jousts and revelries that
the painting which is chiefly an aristocratic took place in carnival time.
activity.
Sequence
Elaboration
The scene is dominated by The windows are divided On the right them are
a castle that has been by a stone cross with two hunters coming
identified as Stenico glass panes. through the snow.
castle. 2 3
MI‚ñ†
</figure>
<page confidence="0.992321">
101
</page>
<bodyText confidence="0.999936464285714">
matic techniques that are the basic resources to
plan the presentation. Second, we have shown
how the resources (knowledge on the rhetorical
structure of the commentary, knowledge about the
domain and the repository of images) are anno-
tated. Third, we have illustrated the architec-
ture of the engine and the four steps of computa-
tion. Finally we have presented some examples,
which show how the system employs rules and
constraints to generate engaging presentations.
At the moment the system relies on a set of
fifteen rules and ten constraints. Improvements
are envisaged in particular to take into consider-
ation the time needed to complete the movements
(in this moment we assume a constant speed of
the camera in movements) and more elaborated
strategies to replan forbidden sequences of camera
movements.
We have noted that the annotation of the re-
sources (especially text) is time-consuming In the
future, in order to speed-up this task, we intend
to investigate the possibility of a (semi-)automatic
annotation of the discourse structure.
The application of the video clips in a mobile
museum guide is currently under study (Zanca-
naro et al., 2003) and we are now experimenting
with the techniques described here to automati-
cally produce user-tailored videos.
</bodyText>
<sectionHeader confidence="0.994338" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997995">
This work has been supported by the PEACH
and TICCA projects, funded by the Autonomous
Province of Trento.
</bodyText>
<sectionHeader confidence="0.998441" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99991741509434">
Elisabeth Andr√©. 2000. The Generation of Multime-
dia Documents. In A Handbook of Natural Lan-
guage Processing: Techniques and Applications for
the Processing of Language as Text, pages 305-327.
Marcel Dekker Inc., New York.
Daniel Arijon. 1976. Grammar of the Film Language.
Silman-James Press, Los Angeles, CA.
William. H. Bares and James. C. Lester. 1997.
Realtime Generation of Customized 3d Animated
Explanations for Knowledge-based Learning Envi-
ronments. In AAA/97 Proceedings of the Four-
teenth National Conference on Artificial Intelli-
gence, pages 347-354, Rhode Island, July 27-31.
Anreas Butz. 1994. BETTY: Planning and Generat-
ing Animations for the Visualization of Movements
and Spatial Relations. In Proceedings of Advanced
Visual Interfaces, Bary Italy.
Anreas Butz. 1997. Anymation with CATHY. In
Fourteenth National Conference on Artificial Intelli-
gence and Ninth Innovative Applications of Artificial
Intelligence Conference, volume 1, pages 957-962,
Providence, Rhode Island, July 27-31.
David B. Christianson, Sean E. Anderson, Li We He,
David Salesin, Daniel S. Weld, and Michael F. Co-
hen. 1996. Declarative Camera Control for Auto-
matic Cinematography. In Proceedings of the Thir-
teenth National Conference on Artificial Intelligence
and Eighth Innovative Applications of Artificial In-
telligence Conference, volume 1, pages 148-155,
Portland, Oregon, August 4-8.
Brent H. Daniel, Charles B. Callaway, William H.
Bares, and James C. Lester. 1999. Student-sensitive
Multimodal Explanation Generation for 3d Learn-
ing Environments. In Proceedings of the Sixteenth
National Conference on Artificial Intelligence, vol-
ume 1, pages 114-120, Orlando, Florida, July 18-22.
Peter Karp and Steve Feiner. 1993. Automated Presen-
tation Planning of Animation using Task Decompo-
sition with Heuristic Reasoning. In Proceedings of
Graphics Interface, pages 118-127.
William C. Mann and Sandra Thompson. 1987.
Rhetorical Structure Theory: a Theory of Text Or-
ganization. In The Structure of Discourse. Ablex
Publishing Corporation.
Mark T. Maybury. 1993. Intelligent Multimedia Inter-
faces. AAAI Press.
Christian Metz. 1974. Film Language: a Semiotics of
the Cinema. Oxford University Press.
Wolfgang Wahlster, Elisabeth Andr√©, Wolfgang Fin-
kler, Hans-Jrgen Profitilich, and Thomas Rist.
1993. Plan-based Integration of Natural Language
and Graphics Generation. Artificial Intelligence,
63:387-427.
</reference>
<page confidence="0.998618">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.640311">
<title confidence="0.999849">Generation of Video Documentaries from Discourse Structures</title>
<author confidence="0.999835">Cesare Rocchi Massimo Zancanaro</author>
<affiliation confidence="0.992758">Irst ITC</affiliation>
<address confidence="0.668517">Italy Italy</address>
<email confidence="0.975231">rocchi@itc.itzancana@itc.it</email>
<abstract confidence="0.99910775">Recent interests in the use of multimedia presentations and multimodal interfaces have raised the need for the automatic generation of graphics and especially temporal media. This paper presents an engine to build video docfrom annotated audio commentaries. The engine, taking consideration the discourse structure of the commentary, plans the segmentation in shots as well as the camera movements and decides the transition effects among shots. The output is a complete script of a &amp;quot;video presentation&amp;quot;, with instructions for synchronizing images and movements with the playing of the audio commentary. The language of cinematography and a set of strategies similar to those used in documentaries are the basic resources to plan the animation. Strategies encompass constraints and conventions normally used in building video documentaries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Elisabeth Andr√©</author>
</authors>
<title>The Generation of Multimedia Documents.</title>
<date>2000</date>
<booktitle>In A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text,</booktitle>
<pages>305--327</pages>
<publisher>Marcel Dekker Inc.,</publisher>
<location>New York.</location>
<contexts>
<context position="1407" citStr="Andr√©, 2000" startWordPosition="217" endWordPosition="218">he language of cinematography and a set of strategies similar to those used in documentaries are the basic resources to plan the animation. Strategies encompass constraints and conventions normally used in building video documentaries. 1 Introduction In the last decade there has been an increasing interest in the generation of multimedia presentations and a growing tendency towards the use of multi-modal interfaces (Wahlster et al., 1993; Maybury, 1993). These interests have raised the need for automatic generation not only of natural language, but also graphics and especially temporal media (Andr√©, 2000). In this paper, an engine to build video sequences of images starting from an audio commentary is described. The input for the engine is a representation of (possibly automatically) generated verbal commentary. The engine, taking into consideration the discourse structure of the commentary, retrieves the most appropriate set of images from an annotated database, plans the segmentation in shots as well as the camera movements and finally decides the transition effects among shots. The output of the engine is a complete script of a &amp;quot;video presentation&amp;quot;, with instructions for synchronizing image</context>
</contexts>
<marker>Andr√©, 2000</marker>
<rawString>Elisabeth Andr√©. 2000. The Generation of Multimedia Documents. In A Handbook of Natural Language Processing: Techniques and Applications for the Processing of Language as Text, pages 305-327. Marcel Dekker Inc., New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Arijon</author>
</authors>
<title>Grammar of the Film Language.</title>
<date>1976</date>
<publisher>Silman-James Press,</publisher>
<location>Los Angeles, CA.</location>
<contexts>
<context position="7986" citStr="Arijon (1976)" startWordPosition="1298" endWordPosition="1300">tions. From this original repository we borrowed a set of relations (elaboration, background, sequence and circumstance), which are commonly used in descriptive text, like those we have analyzed (see Section 6.1). 5 Heuristics and constraints of cinematography Directors and film critics have identified several heuristics for making good movies. In designing a shot, it is important to consider the message that it has to convey and the (semantic) relations with the previous and following messages. Camera movements can be used to signal some of these semantic relations. For example, according to Arijon (1976), panning and tilting can be used to reveal spatial relations among objects and to move the watcher&apos;s attention from one center of interest to another; dollying can be employed to focus the attention on a particular zone or object previously displayed. For example, if an object is currently displayed and the following message deepens one aspect of it, a zoom on that aspect can be chosen. Besides rules for movement selection, cinematographers have also identified a set of constraints on possible camera movement combinations, in order to ensure a pleasant presentation. In particular, each camera</context>
<context position="9779" citStr="Arijon 1976" startWordPosition="1601" endWordPosition="1602"> a movie is cohesion. A video sequence has to be a continuum, an uninterrupted stream in which each piece is connected to the others and is part of a whole. To achieve cohesion in designing the visual part of a presentation it is worth considering the relations among the new information to be delivered and those already given (discourse history) and to provide rhetorical strategies to build the presentation. The combination of rules and constraints encode some of the basic &amp;quot;principles of cinematography&amp;quot;, which have been identified with the help of an expert director of documentaries (see also Arijon 1976). Rules and constraints are the core on which the system relies. They encode the rhetorical strategies that are the basic resource for: (i) selecting appropriate images, (ii) designing the presentation structure, (iii) completing each shot, (iv) synchronizing the visual part with the audio commentary and avoiding the &amp;quot;seasickness&amp;quot; effect. Rules are formalized in a context sensitive &amp;quot;presentation grammar&amp;quot;, are fired by a forward chaining mechanism and are relative to: (i) rhetorical relations among the text spans; (ii) the geometric properties of images selected from the information repository </context>
<context position="20989" citStr="Arijon, 1976" startWordPosition="3349" endWordPosition="3351"> the shot to be presented and plans the transition according to the following rules: ‚Ä¢ If two segments are linked by a relation of type elaboration a short cross fade applies; ‚Ä¢ If two segments are linked by a relation of type background or circumstance a long cross fade applies. ‚Ä¢ If two segments are linked by a relation of type sequence a cut applies. ‚Ä¢ If a relation of type enumeration holds among two or more segments a rapid sequence of cut applies. These rules have been selected according to the observations about the usual employment of transition effects in the field of cinematography (Arijon, 1976). Fade effects are fit for smooth transition, when there is a topic shift or when the center 100 Figure 9: The &amp;quot;Tournament&amp;quot; example. of interest changes but the new topic is related to the old one, as in the case of elaboration or background. Cut is more appropriate for abrupt and rapid changes, to emphasize the introduction of a new concept, as in the case of sequence. A special case holds when the verbal commentary enumerates a set of subjects or different aspects of the same object; in those cases a rapid sequence of cuts can be used to visually enumerate the elements described. 7 Examples </context>
</contexts>
<marker>Arijon, 1976</marker>
<rawString>Daniel Arijon. 1976. Grammar of the Film Language. Silman-James Press, Los Angeles, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Bares</author>
<author>James C Lester</author>
</authors>
<title>Realtime Generation of Customized 3d Animated Explanations for Knowledge-based Learning Environments.</title>
<date>1997</date>
<booktitle>In AAA/97 Proceedings of the Fourteenth National Conference on Artificial Intelligence,</booktitle>
<pages>347--354</pages>
<location>Rhode Island,</location>
<contexts>
<context position="4068" citStr="Bares and Lester, 1997" startWordPosition="631" endWordPosition="634">successful attempt to encode several of the principles of cinematography in the Declarative camera control language. Similar systems are BETTY (Butz, 1994) and CATHI (Butz, 1997). BETTY is an animation planner, which generates scripts for animated presentations. The CATHY system generates on-line descriptions of 3D animated clips for the illustration of technical devices, in the context of a coordinated multimedia document. Animated presentations have been successfully employed also in multimodal frameworks for the generation of explanations (Daniel et al., 1999) and in learning environments (Bares and Lester, 1997). The novelty of our approach lies in the use of rhetorical structure of the accompanying audio commentary in planning the video. In particular, knowledge of rhetorical structure is extremely useful in taking decisions related to the punctuation of the video, in order to reflect the rhythm of the audio commentary and its communicative goals. In our view, the verbal part of the documentary always drives the generation of the visual part. 3 Relevant concepts and terminology According to Metz (1974), cinematic representation is not like a human language, which is defined by a set of grammatical r</context>
</contexts>
<marker>Bares, Lester, 1997</marker>
<rawString>William. H. Bares and James. C. Lester. 1997. Realtime Generation of Customized 3d Animated Explanations for Knowledge-based Learning Environments. In AAA/97 Proceedings of the Fourteenth National Conference on Artificial Intelligence, pages 347-354, Rhode Island, July 27-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anreas Butz</author>
</authors>
<title>BETTY: Planning and Generating Animations for the Visualization of Movements and Spatial Relations.</title>
<date>1994</date>
<booktitle>In Proceedings of Advanced Visual Interfaces,</booktitle>
<location>Bary</location>
<contexts>
<context position="3600" citStr="Butz, 1994" startWordPosition="564" endWordPosition="565"> section 6 we illustrate the architecture of the engine and its parts. In section 7 we give some examples of how the engine works. Finally, in section 8, we outline conclusions and future work. 2 Related Work One of the first case studies of the generation of &amp;quot;motion presentations&amp;quot; is the work of (Karp and Feiner, 1993). Their system generates scripts for animation using top-down hierarchical planning techniques. (Christianson et al., 1996) presents a successful attempt to encode several of the principles of cinematography in the Declarative camera control language. Similar systems are BETTY (Butz, 1994) and CATHI (Butz, 1997). BETTY is an animation planner, which generates scripts for animated presentations. The CATHY system generates on-line descriptions of 3D animated clips for the illustration of technical devices, in the context of a coordinated multimedia document. Animated presentations have been successfully employed also in multimodal frameworks for the generation of explanations (Daniel et al., 1999) and in learning environments (Bares and Lester, 1997). The novelty of our approach lies in the use of rhetorical structure of the accompanying audio commentary in planning the video. In</context>
</contexts>
<marker>Butz, 1994</marker>
<rawString>Anreas Butz. 1994. BETTY: Planning and Generating Animations for the Visualization of Movements and Spatial Relations. In Proceedings of Advanced Visual Interfaces, Bary Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anreas Butz</author>
</authors>
<title>Anymation with CATHY.</title>
<date>1997</date>
<booktitle>In Fourteenth National Conference on Artificial Intelligence and Ninth Innovative Applications of Artificial Intelligence Conference,</booktitle>
<volume>1</volume>
<pages>957--962</pages>
<location>Providence, Rhode Island,</location>
<contexts>
<context position="3623" citStr="Butz, 1997" startWordPosition="568" endWordPosition="569">e the architecture of the engine and its parts. In section 7 we give some examples of how the engine works. Finally, in section 8, we outline conclusions and future work. 2 Related Work One of the first case studies of the generation of &amp;quot;motion presentations&amp;quot; is the work of (Karp and Feiner, 1993). Their system generates scripts for animation using top-down hierarchical planning techniques. (Christianson et al., 1996) presents a successful attempt to encode several of the principles of cinematography in the Declarative camera control language. Similar systems are BETTY (Butz, 1994) and CATHI (Butz, 1997). BETTY is an animation planner, which generates scripts for animated presentations. The CATHY system generates on-line descriptions of 3D animated clips for the illustration of technical devices, in the context of a coordinated multimedia document. Animated presentations have been successfully employed also in multimodal frameworks for the generation of explanations (Daniel et al., 1999) and in learning environments (Bares and Lester, 1997). The novelty of our approach lies in the use of rhetorical structure of the accompanying audio commentary in planning the video. In particular, knowledge </context>
</contexts>
<marker>Butz, 1997</marker>
<rawString>Anreas Butz. 1997. Anymation with CATHY. In Fourteenth National Conference on Artificial Intelligence and Ninth Innovative Applications of Artificial Intelligence Conference, volume 1, pages 957-962, Providence, Rhode Island, July 27-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David B Christianson</author>
<author>Sean E Anderson</author>
<author>Li We He</author>
<author>David Salesin</author>
<author>Daniel S Weld</author>
<author>Michael F Cohen</author>
</authors>
<title>Declarative Camera Control for Automatic Cinematography.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence and Eighth Innovative Applications of Artificial Intelligence Conference,</booktitle>
<volume>1</volume>
<pages>148--155</pages>
<location>Portland, Oregon,</location>
<contexts>
<context position="3433" citStr="Christianson et al., 1996" startWordPosition="537" endWordPosition="540">hetorical Structure Theory (RST) for the analysis of discourse structure. In section 5 we 95 present some of the heuristics that we have borrowed from the field of cinematography. In section 6 we illustrate the architecture of the engine and its parts. In section 7 we give some examples of how the engine works. Finally, in section 8, we outline conclusions and future work. 2 Related Work One of the first case studies of the generation of &amp;quot;motion presentations&amp;quot; is the work of (Karp and Feiner, 1993). Their system generates scripts for animation using top-down hierarchical planning techniques. (Christianson et al., 1996) presents a successful attempt to encode several of the principles of cinematography in the Declarative camera control language. Similar systems are BETTY (Butz, 1994) and CATHI (Butz, 1997). BETTY is an animation planner, which generates scripts for animated presentations. The CATHY system generates on-line descriptions of 3D animated clips for the illustration of technical devices, in the context of a coordinated multimedia document. Animated presentations have been successfully employed also in multimodal frameworks for the generation of explanations (Daniel et al., 1999) and in learning en</context>
</contexts>
<marker>Christianson, Anderson, He, Salesin, Weld, Cohen, 1996</marker>
<rawString>David B. Christianson, Sean E. Anderson, Li We He, David Salesin, Daniel S. Weld, and Michael F. Cohen. 1996. Declarative Camera Control for Automatic Cinematography. In Proceedings of the Thirteenth National Conference on Artificial Intelligence and Eighth Innovative Applications of Artificial Intelligence Conference, volume 1, pages 148-155, Portland, Oregon, August 4-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brent H Daniel</author>
<author>Charles B Callaway</author>
<author>William H Bares</author>
<author>James C Lester</author>
</authors>
<title>Student-sensitive Multimodal Explanation Generation for 3d Learning Environments.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence,</booktitle>
<volume>1</volume>
<pages>114--120</pages>
<location>Orlando, Florida,</location>
<contexts>
<context position="4014" citStr="Daniel et al., 1999" startWordPosition="623" endWordPosition="626">techniques. (Christianson et al., 1996) presents a successful attempt to encode several of the principles of cinematography in the Declarative camera control language. Similar systems are BETTY (Butz, 1994) and CATHI (Butz, 1997). BETTY is an animation planner, which generates scripts for animated presentations. The CATHY system generates on-line descriptions of 3D animated clips for the illustration of technical devices, in the context of a coordinated multimedia document. Animated presentations have been successfully employed also in multimodal frameworks for the generation of explanations (Daniel et al., 1999) and in learning environments (Bares and Lester, 1997). The novelty of our approach lies in the use of rhetorical structure of the accompanying audio commentary in planning the video. In particular, knowledge of rhetorical structure is extremely useful in taking decisions related to the punctuation of the video, in order to reflect the rhythm of the audio commentary and its communicative goals. In our view, the verbal part of the documentary always drives the generation of the visual part. 3 Relevant concepts and terminology According to Metz (1974), cinematic representation is not like a huma</context>
</contexts>
<marker>Daniel, Callaway, Bares, Lester, 1999</marker>
<rawString>Brent H. Daniel, Charles B. Callaway, William H. Bares, and James C. Lester. 1999. Student-sensitive Multimodal Explanation Generation for 3d Learning Environments. In Proceedings of the Sixteenth National Conference on Artificial Intelligence, volume 1, pages 114-120, Orlando, Florida, July 18-22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Karp</author>
<author>Steve Feiner</author>
</authors>
<title>Automated Presentation Planning of Animation using Task Decomposition with Heuristic Reasoning.</title>
<date>1993</date>
<booktitle>In Proceedings of Graphics Interface,</booktitle>
<pages>118--127</pages>
<contexts>
<context position="3310" citStr="Karp and Feiner, 1993" startWordPosition="522" endWordPosition="525">ks, relevant concepts and terminology of cinematography are introduced in section 3. Section 4 briefly summarizes the Rhetorical Structure Theory (RST) for the analysis of discourse structure. In section 5 we 95 present some of the heuristics that we have borrowed from the field of cinematography. In section 6 we illustrate the architecture of the engine and its parts. In section 7 we give some examples of how the engine works. Finally, in section 8, we outline conclusions and future work. 2 Related Work One of the first case studies of the generation of &amp;quot;motion presentations&amp;quot; is the work of (Karp and Feiner, 1993). Their system generates scripts for animation using top-down hierarchical planning techniques. (Christianson et al., 1996) presents a successful attempt to encode several of the principles of cinematography in the Declarative camera control language. Similar systems are BETTY (Butz, 1994) and CATHI (Butz, 1997). BETTY is an animation planner, which generates scripts for animated presentations. The CATHY system generates on-line descriptions of 3D animated clips for the illustration of technical devices, in the context of a coordinated multimedia document. Animated presentations have been succ</context>
</contexts>
<marker>Karp, Feiner, 1993</marker>
<rawString>Peter Karp and Steve Feiner. 1993. Automated Presentation Planning of Animation using Task Decomposition with Heuristic Reasoning. In Proceedings of Graphics Interface, pages 118-127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra Thompson</author>
</authors>
<title>Rhetorical Structure Theory: a Theory of Text Organization.</title>
<date>1987</date>
<booktitle>In The Structure of Discourse.</booktitle>
<publisher>Ablex Publishing Corporation.</publisher>
<contexts>
<context position="6062" citStr="Mann and Thompson, 1987" startWordPosition="973" endWordPosition="977">ong shots are considered as the punctuation symbols of cinematography; they affect the rhythm of the discourse and the message conveyed by the video. The main effects are cut - the first frame of the shot to be displayed immediately replaces the last frame of the shot currently on display; fade - a shot is gradually replaced by (fade out) or gradually replaces (fade in) a black screen or another shot and cross fade (or dissolve) which is the composition of a fade out on the displayed shot and a fade in applied to the shot to be shown. 4 Rhetorical Structure Theory Rhetorical Structure Theory (Mann and Thompson, 1987) allows the analysis of discourse structure in terms of dependency trees, with each node of the tree being a text span. Each branch of the tree represents a relationship between two nodes. One node is called the nucleus and the other is called the satellite. The information in the satellite relates to that found in the nucleus in that it expresses an idea related to what is said in the nucleus. For example, a background relation holds when a satellite provides a context to the information expressed in the nucleus. Figure 1 shows an example of a portion of a rhetorical tree. The second paragrap</context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>William C. Mann and Sandra Thompson. 1987. Rhetorical Structure Theory: a Theory of Text Organization. In The Structure of Discourse. Ablex Publishing Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark T Maybury</author>
</authors>
<title>Intelligent Multimedia Interfaces.</title>
<date>1993</date>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="1252" citStr="Maybury, 1993" startWordPosition="193" endWordPosition="194">e output is a complete script of a &amp;quot;video presentation&amp;quot;, with instructions for synchronizing images and movements with the playing of the audio commentary. The language of cinematography and a set of strategies similar to those used in documentaries are the basic resources to plan the animation. Strategies encompass constraints and conventions normally used in building video documentaries. 1 Introduction In the last decade there has been an increasing interest in the generation of multimedia presentations and a growing tendency towards the use of multi-modal interfaces (Wahlster et al., 1993; Maybury, 1993). These interests have raised the need for automatic generation not only of natural language, but also graphics and especially temporal media (Andr√©, 2000). In this paper, an engine to build video sequences of images starting from an audio commentary is described. The input for the engine is a representation of (possibly automatically) generated verbal commentary. The engine, taking into consideration the discourse structure of the commentary, retrieves the most appropriate set of images from an annotated database, plans the segmentation in shots as well as the camera movements and finally dec</context>
</contexts>
<marker>Maybury, 1993</marker>
<rawString>Mark T. Maybury. 1993. Intelligent Multimedia Interfaces. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Metz</author>
</authors>
<title>Film Language: a Semiotics of the Cinema.</title>
<date>1974</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="2108" citStr="Metz, 1974" startWordPosition="329" endWordPosition="330">ntary is described. The input for the engine is a representation of (possibly automatically) generated verbal commentary. The engine, taking into consideration the discourse structure of the commentary, retrieves the most appropriate set of images from an annotated database, plans the segmentation in shots as well as the camera movements and finally decides the transition effects among shots. The output of the engine is a complete script of a &amp;quot;video presentation&amp;quot;, with instructions for synchronizing images and movements with the playing of the audio commentary. The language of cinematography (Metz, 1974), including shot segmentation, camera movements and transition effects, is the basic resource to plan the animation and to synchronize the visual and the verbal parts of the presentation. In generating animations, a set of strategies similar to those used in documentaries are employed. Two broad classes of strategies have been identified. The first class encompasses constraints imposed by the grammar of cinematography, while the second deals with conventions normally used in guiding camera movements in the production of documentaries. After a short discussion on related works, relevant concept</context>
<context position="4569" citStr="Metz (1974)" startWordPosition="716" endWordPosition="717">for the generation of explanations (Daniel et al., 1999) and in learning environments (Bares and Lester, 1997). The novelty of our approach lies in the use of rhetorical structure of the accompanying audio commentary in planning the video. In particular, knowledge of rhetorical structure is extremely useful in taking decisions related to the punctuation of the video, in order to reflect the rhythm of the audio commentary and its communicative goals. In our view, the verbal part of the documentary always drives the generation of the visual part. 3 Relevant concepts and terminology According to Metz (1974), cinematic representation is not like a human language, which is defined by a set of grammatical rules. It is nevertheless guided by a set of generally accepted conventions. These guidelines may be used for developing multimedia presentations that can be best perceived by the viewer. Following, we briefly summarize the basic terminology of cinematography. 3.1 Shot and camera movements The shot is the basic unit of a video sequence. In the field of cinematography a shot is defined as a continuous view from single camera without interruption. Since we only deal with still images, we define a sh</context>
</contexts>
<marker>Metz, 1974</marker>
<rawString>Christian Metz. 1974. Film Language: a Semiotics of the Cinema. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
<author>Elisabeth Andr√©</author>
<author>Wolfgang Finkler</author>
<author>Hans-Jrgen Profitilich</author>
<author>Thomas Rist</author>
</authors>
<date>1993</date>
<journal>Plan-based Integration of Natural Language and Graphics Generation. Artificial Intelligence,</journal>
<pages>63--387</pages>
<contexts>
<context position="1236" citStr="Wahlster et al., 1993" startWordPosition="189" endWordPosition="192">effects among shots. The output is a complete script of a &amp;quot;video presentation&amp;quot;, with instructions for synchronizing images and movements with the playing of the audio commentary. The language of cinematography and a set of strategies similar to those used in documentaries are the basic resources to plan the animation. Strategies encompass constraints and conventions normally used in building video documentaries. 1 Introduction In the last decade there has been an increasing interest in the generation of multimedia presentations and a growing tendency towards the use of multi-modal interfaces (Wahlster et al., 1993; Maybury, 1993). These interests have raised the need for automatic generation not only of natural language, but also graphics and especially temporal media (Andr√©, 2000). In this paper, an engine to build video sequences of images starting from an audio commentary is described. The input for the engine is a representation of (possibly automatically) generated verbal commentary. The engine, taking into consideration the discourse structure of the commentary, retrieves the most appropriate set of images from an annotated database, plans the segmentation in shots as well as the camera movements</context>
</contexts>
<marker>Wahlster, Andr√©, Finkler, Profitilich, Rist, 1993</marker>
<rawString>Wolfgang Wahlster, Elisabeth Andr√©, Wolfgang Finkler, Hans-Jrgen Profitilich, and Thomas Rist. 1993. Plan-based Integration of Natural Language and Graphics Generation. Artificial Intelligence, 63:387-427.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>