<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.771237">
A FLEXIBLE EXAMPLE-BASED PARSER BASED ON THE SSTC •
</title>
<author confidence="0.633191">
Mosleh Hmoud Al-Adhaileh &amp; Tang Enya Kong
</author>
<affiliation confidence="0.757150333333333">
Computer Aided Translation Unit
School of computer sciences
University Sains Malaysia
</affiliation>
<address confidence="0.939881">
11800 PENANG, MALAYSIA
</address>
<email confidence="0.998085">
mosleh@cs.usm.my, enyakong@cs.usm.my
</email>
<sectionHeader confidence="0.957179" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.985923888888889">
In this paper we sketch an approach for Natural Language parsing. Our approach is an example-based
approach, which relies mainly on examples that already parsed to their representation structure, and on the
knowledge that we can get from these examples the required information to parse a new input sentence. In our
approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema
where each SSTC describes a sentence, a representation tree as well as the correspondence between substrings in
the sentence and subtrees in the representation tree. In the process of parsing, we first try to build subtrees for
phrases in the input sentence which have been successfully found in the example-base - a bottom up approach.
These subtrees will then be combined together to form a single rooted representation tree based on an example with
similar representation structure - a top down approach.
</bodyText>
<keyword confidence="0.597377">
Keywords: Example-based parsing, SSTC.
</keyword>
<sectionHeader confidence="0.999656" genericHeader="keywords">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999987679245283">
In natural language processing (NLP), one key
problem is how to design an effective parsing system.
Natural language parsing is the process of analyzing
or parsing that takes sentences in a natural language
and converts them to some representation form
suitable for further interpretation towards some
applications might be required, for example,
translation, text abstraction, question-answering, etc.
The generated representation tree structure can be a
phrase structure tree, a dependency tree or a logical
structure tree, as required by the application involved.
Here we design an approach for parsing natural
language to its representation structure, which
depends on related examples already parsed in the
example-base. This approach is called example-based
parsing, as oppose to the traditional approaches of
natural language parsing which normally are based on
rewriting rules. Here linguistic knowledge extracted
directly from the example-base will be used to parse a
natural language sentence (i.e. using past language
experiences instead of rules). For a new sentence, to
build its analysis (i.e. representation structure tree),
ideally if the sentence is already in the example-base,
its analysis is found there too, but in general, the
input sentence will not be found in the example-base.
In such case, a method is used to retrieve close related
examples and use the knowledge from these
examples to build the analysis for the input sentence.
In general, this approach relies on the assumption that
if two strings (phrase or sentence) are &amp;quot;close&amp;quot;, their
analysis should be &amp;quot;close&amp;quot; too. If the analysis of the
first one is known, the analysis of the other can be
obtained by making some modifications in the
analysis of the first one.
The example-based approach has become a
common technique for NLP applications, especially
in MT as reported in [I], [2] or [3]. However, a main
problem normally arises in the current approaches
which indirectly limits their applications in the
development of a large scale and practical example-
based system. Namely the lack of flexibility in
creating the representation tree due to the restriction
that correspondences between nodes (terminal or non
terminal) of the representation tree and words of the
sentence must be one-to-one and some even restrict it
to only in projective manner according to certain
traversal order. This restriction normally results to
the inefficient usage of the example-base. In this
paper, we shall first discuss on certain cases where
projective representation trees are inadequate for
characterizing representation structures of some
natural linguistic phenomena, i.e. featurisation,
lexicalisation and crossed dependencies. Next, we
</bodyText>
<note confidence="0.632443">
• The work reported in this paper is supported by the IRPA research programs, under project number 04-02-05-6001 funded by the Ministry of
Science. Technology and Environment, Malaysia.
</note>
<page confidence="0.997183">
687
</page>
<bodyText confidence="0.999910875">
propose to overcome the problem by introducing a
flexible annotation schema called Structured String-
Tree Correspondence(SSTC) which describes a
sentence, a representation tree, and the
correspondence between substrings in the sentence
and subtrees in the representation tree. Finally, we
present a algorithm to parse natural language
sentences based on the SSTC annotation schema.
</bodyText>
<sectionHeader confidence="0.999845" genericHeader="introduction">
2. NON-PROJECTIVE CORRESPONDE
-NCES IN NATURAL LANGUAGE
SENTENCES
</sectionHeader>
<bodyText confidence="0.9999525">
In this section, we shall present some cases
where projective representation tree is found to be
inadequate for characterizing representation tree of
some natural language sentences. The cases
illustrated here are featurisation, lexicalisation and
crossed dependencies. An example containing
mixture of these non-projective correspondences also
will be presented.
</bodyText>
<subsectionHeader confidence="0.934251">
2.1 Featurisation
</subsectionHeader>
<bodyText confidence="0.9999882">
Featurisation occurs when a linguist decides that a
particular substring in the sentence, should not be
represented as a subtree in the representation tree but
perhaps as a collection of features. For example, as
illustrated in figure 1, this would be the case for
prepositions in arguments which can be interpreted as
part of the predicate and not the argument, and should
be featurised into the predicate (e.g. &amp;quot;up&amp;quot; in &amp;quot;picks-
up&amp;quot;), the particle &amp;quot;up&amp;quot; is featurised as a part of the
feature properties of the verb &amp;quot;pick&amp;quot;.
</bodyText>
<figure confidence="0.9734502">
picks up
and
eats eats
pear
A
</figure>
<figureCaption confidence="0.983417">
John eats the apple and Mary the pear
Figure 2: Lexicalisation
</figureCaption>
<subsectionHeader confidence="0.999457">
2.3 Crossed dependencies
</subsectionHeader>
<bodyText confidence="0.975801466666667">
The most complicated case of string-tree
correspondence is when dependencies are intertwined
with each other. It is a very common phenomenon in
natural language. In crossed dependencies, subtree in
the tree corresponds to single substring in the
sentence, but the words in a substring are distributed
over the whole sentence in a discontinuous manner,
in relation to the subtree they correspond to. An
example of crossed dependencies is occurred in the
sentences of the form (an v bn n&gt;0), figure 3
illustrates the representation tree for the string &amp;quot;aa v
bb cc &amp;quot;(also written a.la.2 v b.lb.2 c.lc.2 to show
the positions), this akin to the &apos;respectively&apos; problem
in English sentence like &amp;quot;John and Mary give Paul
and Ann trousers and dresses respectively&amp;quot; [4].
</bodyText>
<equation confidence="0.791797111111111">
a.1 b.1
A-P,4
a.1 a.2 vm&apos;o.1 b.
AN
a.2 b.2 c.2
C.1 c.2
John apple
the
He picks up the ball
</equation>
<figureCaption confidence="0.977354">
Figure I: Featurisation
</figureCaption>
<subsectionHeader confidence="0.999165">
2.2 Lexicalisation
</subsectionHeader>
<bodyText confidence="0.999557909090909">
Lexicalisation is the case when a particular
subtree in the representation tree presents the
meaning of some part of the string, which is not
orally realized in phonological form. Lexicalisation
may result from the correspondence of a subtree in
the tree to an empty substring in the sentence, or
substring in the sentence to more than one subtree in
the tree. Figure 2 illustrates the sentence &amp;quot;John eats
the apple and Mary the pear&amp;quot; where &amp;quot;eats&amp;quot; in the
sentence corresponds to more than one node in the
tree.
</bodyText>
<figureCaption confidence="0.999129">
Figure 3: Crossed dependencies
</figureCaption>
<bodyText confidence="0.975445875">
Sometimes the sentence contains mixture of these
non-projective correspondences, figure 4 illustrates
the sentence &amp;quot;He picks the ball up&amp;quot;, which contains
both featurisation and crossed dependencies. Here,
the particle &amp;quot;up&amp;quot; is separated from its verb &amp;quot;picks&amp;quot; by
a noun phrase &amp;quot;the ball&amp;quot; in the string. And &amp;quot;up&amp;quot; is
featurised into the verb &amp;quot;picks&amp;quot; (e.g. &amp;quot;up&amp;quot; in &amp;quot;picks-
up&amp;quot;).
</bodyText>
<figure confidence="0.799118333333333">
picks up
He ball
picks the ba I p
</figure>
<figureCaption confidence="0.987289">
Figure 4: Mixture of featurisation
and crossed dependencies
</figureCaption>
<bodyText confidence="0.435131">
He A bal
</bodyText>
<page confidence="0.997595">
688
</page>
<sectionHeader confidence="0.9962475" genericHeader="method">
3. STRUCTURED STRING-TREE
CORRESPONDENCE (SSTC)
</sectionHeader>
<bodyText confidence="0.999854322580645">
The correspondence between the string on one
hand, and its representation of meaning on the other
hand, is defined in terms of finer subcorrespondences
between substrings of the sentence and subtrees of the
tree. Such correspondence is made of two interrelated
correspondences, one between nodes and substrings,
and the other between subtrees and substrings, (the
substrings being possibly discontinuous in both
cases).
The notation used in SSTC to denote a
correspondence consists of a pair of intervals X/Y
attached to each node in the tree, where X(SNODE)
denotes the interval containing the substring that
corresponds to the node, and Y(STREE) denotes the
interval containing the substring that corresponds to
the subtree having the node as root [4].
Figure 5 illustrates the sentence &amp;quot;all cats eat
mice&amp;quot; with its corresponding SSTC. It is a simple
projective correspondence. An interval is assigned to
each word in the sentence, i.e. (0-1) for &amp;quot;all&amp;quot;, (1-2)
for &amp;quot;cats&amp;quot;, (2-3) for &amp;quot;eat&amp;quot; and (3-4) for &amp;quot;mice&amp;quot;. A
substring in the sentence that corresponds to a node in
the representation tree is denoted by assigning the
interval of the substring to SNODE of the node, e.g.
the node &amp;quot;cats&amp;quot; with SNODE interval (1-2)
corresponds to the word &amp;quot;cats&amp;quot; in the string with the
similar interval. The correspondence between
subtrees and substrings are denoted by the interval
assigned to the STREE of each node e.g. the subtree
rooted at node &amp;quot;eat&amp;quot; with STREE interval (0-4)
corresponds to the whole sentence &amp;quot;all cats eat mice&amp;quot;.
</bodyText>
<figure confidence="0.36846675">
Tree eat(2-3/0-4)
\
cats mice
( 1 -2/0-2 1 (3-4/3-4)
all
(0-1/0-1)
String all cats eat mice
(0-1) (1-2) (2-3) (3-4)
</figure>
<figureCaption confidence="0.995863">
Figure 5: An SSTC recording the sentence &amp;quot;all cats
eat mice&amp;quot; and its Dependency tree together with the
correspondences between substrings of the sentence
and subtrees of the tree.
</figureCaption>
<sectionHeader confidence="0.9997935" genericHeader="method">
4. USES OF SSTC ANNOTATION IN
EXAMPLE-BASED PARSING
</sectionHeader>
<bodyText confidence="0.999992727272727">
In order to enhance the quality of example-
based systems, sentences in the example-base are
normally annotated with theirs constituency or
dependency structures which in turn allow example-
based parsing to be established at the structural
level. To facilitate such structural annotation, here
we annotate the examples based on the Structured
String-Tree Correspondence (SSTC). The SSTC is a
general structure that can associate, to string in a
language, arbitrary tree structure as desired by the
annotator to be the interpretation structure of the
string, and more importantly is the facility to specify
the correspondence between the string and the
associated tree which can be interpreted for both
analysis and synthesis in NLP. These features are
very much desired in the design of an annotation
scheme, in particular for the treatment of linguistic
phenomena which are not-standard e.g. crossed
dependencies [5].
Since the example in the example-base are
described in terms of SSTC, which consists of a
sentence (the text), a dependency tree&apos; (the linguistic
representation) and the mapping between the two
(correspondence); example-based parsing is
performed by giving a new input sentence, followed
by getting the related examples(i.e. examples that
contains same words in the input sentence) from the
example-base, and used them to compute the
representation tree for the input sentence guided by
the correspondence between the string and the tree
as discussed in the following sections. Figure 6
illustrates the general schema for example-based NL
parsing based on the SSTC schema.
</bodyText>
<sectionHeader confidence="0.91186" genericHeader="method">
4. 1 The parsing algorithm
</sectionHeader>
<bodyText confidence="0.972110636363636">
The example-based approach in MT [1], [2] or
[3], relies on the assumption that if two sentences
are &amp;quot;close&amp;quot;, their analysis should be &amp;quot;close&amp;quot; too. If
the analysis of the first one is known, the analysis of
the other can be obtained by making some
modifications in the analysis of the first one (i.e.
I Each node is tagged with syntactic category to enable
substitution at category level.
Representation
tree structure
in the output SSTC
</bodyText>
<figureCaption confidence="0.9904095">
Figure 6: Example-based natural language parsing based on
the SSTC schema.
</figureCaption>
<page confidence="0.990696">
689
</page>
<bodyText confidence="0.998329117647059">
close: distance not too large, modification: edit
operations (insert, delete, replace) [6].
In most of the cases, similar sentence might not
occurred in the example-base, so the system utilized
some close related examples to the given input
sentence (i.e. similar structure to the input sentence or
contain some words in the input sentence). For that it
is necessary to construct several subSSTCs (called
substitutions hereafter) for phrases in the input
sentence according to their occurrence in the
examples from the example-base. These substitutions
are then combined together to form a complete SSTC
as the output.
Suppose the system intends to parse the sentence
&amp;quot;the old man picks the green lamp up&amp;quot;, depending
on the following set of examples representing the
example-base.
</bodyText>
<table confidence="0.960046823529412">
picks[v] up[p] tums[v](3-4/0-5)
(1-2+4-5/0-5) signal[n] on[adv]
He[n] ball[n] (2-3/0-3) (4-5/4-5)
(0-1/0-1) (3-4/2-4)
the[det] the[det] green[adj]
(2-3/2-3) (0-1/0-1) (1-2/1-2)
He picks the ball up The green signal turns on
0-1 1-2 2-3 3-4 4-5 0-I 1-2 2-3 3-4 4-5
(1) (2)
is[v](2-3/0-4) died[v](3-4/0-4)
lamp[n] off[adv] man[n]
(1-2/0-2) (3-4/3-4) (2-3/0-3)
the[det] the[det] old[adj]
(0-1/0-1) (0-1/0-1) (1-2/1-2)
The lamp is off The old man died
0-1 1-2 2-33-4 0-1 1-2 2-3 3-4
(3) (4)
</table>
<bodyText confidence="0.998455333333333">
The example-base is first processed to retrieve
some knowledge related to each word in the example-
base to form a knowledge index. Figure 7 shows the
knowledge index constructed based on the example-
base given above. The knowledge retrieved for each
word consists of:
</bodyText>
<listItem confidence="0.989727482758621">
1. Example number: The example number of one of
the examples which containing this word with this
knowledge. Note that each example in the example-
base is assigned with a number as its identifier.
2. Frequency: The frequency of occurrence in the
example-base for this word with the similar
knowledge.
3. Category: Syntactic category of this word.
4. Type: Type of this word in the dependency tree (0:
terminal, 1: non-terminal).
- Terminal word: The word which is at the
bottom level of the tree structure, namely the
word without any son/s under it (i.e.
STREE=SNODE in SSTC annotation).
- Non terminal word: The word which is
linked to other word/s at the lower level,
namely the word that has son/s (i.e.
STREE*SNODE in SSTC annotation).
5. Status: Status of this word in the dependency tree
(0: root word, 1: non-root word, 2: friend word)
- Friend word: In case of featurisation, if a
word is featurised into other word, this
word is called friend for that word, e.g. the
word &amp;quot;up&amp;quot; is a friend for the word &amp;quot;picks&amp;quot;
in figure 1.
6. Parent category: Syntactic category of the parent
node of this word in the dependency tree.
7. Position: The position of the parent node in the
sentence (0: after this word, 1: before this word).
</listItem>
<bodyText confidence="0.797338125">
8. Next knowledge: A pointer pointing to the next
possible knowledge of this word. Note that a word
might have more than one knowledge, e.g. &amp;quot;man&amp;quot;
could be a verb or a noun.
Based on the constructed knowledge index in figure
7, the system built the following table of knowledge
for the input sentence:
The input sentence: the old man picks the green lamp up
</bodyText>
<table confidence="0.966991222222222">
0-1 1-2 2-3 3-4 4-5 5-6 6-7 7-8
1 4 det 0 1 n 0 nil
4 1 adj 0 1 n 0 nil
4 1 n 1 1 v 0 nil
1 1 v 1 0 - - nil
1 4 det 0 1 n 0 nil
2 1 adj 0 1 v 0 nil
3 1 n 1 1 v 0 nil
1 1 p 1 2 v 1 nil
</table>
<bodyText confidence="0.924135">
Note that to each word in the input sentence, the
system built a record which contain the word,
SNODE interval, and a linked list of possible
knowledge related to the word as recorded in the
knowledge index. The following figure describes an
example record for the word &lt;the&gt;:
This mean:
the word &lt;the&gt;, snode(0-1), one of the examples
that contain the word with this knowledge is
example 1, this knowledge repeated 4 time in the
example-base, the category of the word is &lt;det&gt;,
it is a terminal node, non-root node, the parent
category is &lt;n&gt;, and the parent appear after it in
the sentence.
the 0 1 1 4 det 0 1 0 nil
</bodyText>
<figureCaption confidence="0.99836575">
the 0 1
old 1 2
man 2 3
picks 3 4
the 4 5
green 5 6
lamp 6 7
up 7 8
</figureCaption>
<page confidence="0.811832">
690
</page>
<figure confidence="0.981218409090909">
Example No. &apos;frequency &amp;quot;category &apos;type &apos;status &apos;Parent eategorylPosition &amp;quot;Next Kn.
I 4 det 0 1 _._
n 0 nil.
4 1 adj 0 1 n 0 nil.
1 1 n 0 1 v 0 nil.
—
2 1 v 1 0 - - nil.
I I n 1 1 v
1 nil.
..
2 1 0 1
adj n 0 nil.
2 1 n I I v o nil.
2 I adv 0 I v 1 nil.
1 1 v I 0
- - nil.
3 I adv 0 1 v I nil.
4 I n 1 1 v 0 nil.
4 1 v 1 0 - - nil.
3 1 n 1 1 v 0
nil.
I I P I 2 v 1 nil.
</figure>
<figureCaption confidence="0.999998">
Figure 7: The knowledge index for the words in the example-base.
</figureCaption>
<bodyText confidence="0.999783">
This knowledge will be used to build the
substitutionsfor the input sentence, as we will discuss
in the next section.
</bodyText>
<subsectionHeader confidence="0.672174">
4.1.1 Substitutions generation
</subsectionHeader>
<bodyText confidence="0.999960190476191">
In order to build substitutions, the system first
classifies the words in the input sentence into
terminal words and non-terminal words. For each
terminal word, the system tries to identify the non-
terminal word it may be connected to based on the
syntactic category and the position of the non-
terminal word in the input sentence (i.e. before or
after the terminal word) guided by SNODE interval.
In the input sentence given above, the terminal
words are &amp;quot;the&amp;quot;, &amp;quot;old&amp;quot; and &amp;quot;green&amp;quot; and based on the
knowledge table for the words in the input sentence,
they may be connected as son node to the first non-
terminal with category [n] which appear after them in
the input sentence.
For ( &amp;quot;the&amp;quot; 0-1, and &amp;quot;old&amp;quot; 1-2 ) they are connected as
sons to the word (&amp;quot;man&amp;quot; 2-3).
The remainder non-terminal words, which are not
connected to any terminal word, will be treated as
separate substitutions.
From the input sentence the system builds the
following substitutions respectively :
</bodyText>
<equation confidence="0.9751986">
lamp[n] up[p]
(6-7/4-7) (7-8/ -)
the[det] green[adj]
(4-5/4-5) (5-6/5-6)
(1) (2) (3) (4)
</equation>
<figure confidence="0.953686476190476">
nowledge
table
Non-terminal
words
- Linked as son
- The non-terminal with
syntactic category [n].
- Appear after the words
Substitution
generator
lamp[n]
/\
the[det] green[adj]
the,
green
man[n]
(2-3/0-3)
picks[v]
(3-4/0-8)
the[det] old[adj]
(0-1/0-I) (1-2/1-2)
</figure>
<bodyText confidence="0.998526">
For (&amp;quot;the&amp;quot; 4-5, and &amp;quot;green&amp;quot; 5-6 they are connected
as sons to the word (&amp;quot;lamp&amp;quot; 6-7).
Note that this approach is quite similar to the
generation of constituents in bottom-up chart parsing
except that the problem of handling multiple
overlapping constituents is not addressed here.
</bodyText>
<subsectionHeader confidence="0.675672">
4.1.2 Substitutions combination
</subsectionHeader>
<bodyText confidence="0.9870478">
In order to combine the substitutions to form a
complete SSTC, the system first finds non-terminal
words of input sentence, which appear as root word
of some dependency trees in the example SSTCs. If
more than one example are found (in most cases), the
system will calculate the distance between the input
sentence and the examples, and the closest example
- Linked as son
- The non-terminal with
syntactic category [n].
</bodyText>
<figure confidence="0.823923727272727">
- Appear after the words.
nowledge
table
Non-terminal
words
Substitution
generator
man[n]
the[det] old[adj]
the,
old
</figure>
<page confidence="0.995281">
691
</page>
<bodyText confidence="0.995283666666667">
(namely one with minimum distance) will be chosen
to proceed further.
In our example, the word &amp;quot;picks&amp;quot; is the only
word in the sentence which can be the root word, so
example (1) which containing &amp;quot;pick&amp;quot; as root will be
used as the base to construct the output SSTC. The
system first generates the substitutions for example
(1) based on the same assumptions mentioned earlier
in substitutions generation, which are:
</bodyText>
<equation confidence="0.804942833333333">
he[n] Picks [v] ballEn] up[p]
(0-1 /0-1 ) (1-2/0-5) (3-4/2-4) (4-5/ -)
the [det]
(2-3/2-3)
(1) (2) (3) (4)
Distance calculation:
</equation>
<bodyText confidence="0.911672780821918">
Here the system utilizes distance calculation to
determine the plausible example, which SSTC
structure will be used as a base to combine the
substitutions at the input sentence. We define a
heuristic to calculate the distance, in terms of editing
operations. Editing operations are insert (E 4 p),
deletion (p4E ) and replacing (a 4 s). Edition
distances, which have been proposed in many works
[7], [8] and [9], reflect a sensible notion, and it can be
represented as metrics under some hypotheses. They
defined the edition distances as number of editing
operations to transfer one word to another form, i.e.
how many characters needed to be edited based on
insertion, deletion or replacement. Since words are
strings of characters, sentences are strings of words,
editing distances hence are not confined to words,
they may be used on sentences [6].
With the similar idea, we define the edition
distance as: (i) The distance is calculated at level of
substitutions (i.e. only the root nodes of the
substitutions will be considered, not all the words in
the sentences). (ii) The edit operations are done based
on the syntactic category of the root nodes, (i.e. the
comparison between the input sentence and an
example is based on the syntactic category of the root
nodes of their substitutions, not based on the words).
The distance is calculated based on the number of
editing operations (deletions and insertion) needed to
transfer the input sentence substitutions to the
example substitutions, by assigning weight to each of
these operations: 1 to insertion and 1 to deletion.
e.g.:
a) SI: The old man eats an apple.
S2: He eats a sweet cake.
an [det]
eats [v] cake [n]
a [det] sweet [acti]
In (a), the distance between Si and S2 is 0.
b) Si: He eats an apple in the garden.
S2: The boy who drinks tea eats the cake.
eats [v] pple[n]
an Fdet1
who [rel] drinks [v] tea [n] eats [v] cake [n]
the [det]
In (b), the distance between S1 and S2 is
(3+2)=5.
Note that when a substitution is decided to be
deleted from the example, all the words of the related
substitutions (i.e. the root of the substitutions and all
other words that may link to it as brothers, or son/s),
are deleted too. This series is determined by referring
to an example containing this substitution in the
example-base. For example in (b) above, the
substitution rooted with &amp;quot;who&amp;quot; must be deleted, hence
substitutions &amp;quot;drinks&amp;quot; and &amp;quot;tea&amp;quot; must be deleted too,
similarly &amp;quot;in&amp;quot; must be deleted hence &amp;quot;garden&amp;quot; must be
deleted too.
Before making the replacement, the system must
first check that the root nodes categories for
substitutions in both the example and the input
sentence are the same, and that these substitutions are
occurred in the same order (i.e. the distance is 0). If
there exist additional substitutions in the input
sentence (i.e. the distance 0), the system will either
combine more than one substitution into a single
substitution based on the knowledge index before
replacement is carried out or treat it as optional
substitution which will be added as additional subtree
under the root. On the other hand, additional
substitutions appear in the example will be treated as
optional substitutions and hence can be removed.
Additional substitutions are determined during
distance calculation.
</bodyText>
<sectionHeader confidence="0.949539" genericHeader="method">
Replacement:
</sectionHeader>
<bodyText confidence="0.999237666666667">
Next the substitutions in example (1) will be replaced
by the corresponding substitutions generated from the
input sentence to form a final SSTC. The replacement
</bodyText>
<figure confidence="0.99152375">
man [n] apple [n]
eats [v]
the det] old [ad]]
He [n]
He [n]
boy[n]
The fdet1
in[p] garden [n]
the [det]
692
Example-base
the[det] green[adj]
(4-5/4-5) (5-6/5-6)
the green lamp up
4-5 5-6 6-7 7-8
the[det] old[adj]
(0-1/0-1) (1-2/1-2)
The old man picks
0-1 1-2 2-3 3-4
Knowledge
index
Input sentence
substitutions
picks&apos; v)
IX]
the[det] green[adj]
(I
Output SSTC
structure
SSTC example
substitutions
The old man picks the green lamp up
man[n]
/\
the[det] 0IdIddil
uPEP1
picks[v] up [p]
(1-2+4-5/0-5)
He [n] ballln)
(0-1/0-1) (3-4/2-4)
the[det]
(2-3/2-3)
He picks the ball up
0-1 1-2 2-3 3-4 4-5
SSTC base ; ;
structure
he[n]
(0-1/0-1)
picks[v]
(I-210-5)
ball[n]
(3-4/2-4)
the[det]
(2-3/2-3)
uP[P]
(4-5/-)
(1
(4)
Replacement
Picksh/1 up1P1
(1-2+4-5/0-5)
marlin]
(2-3/0-3)
the[det[ old[adjj
(0-1/0-1) (1-2/1-2)
lamp( n
/6-7/4-7
grecuiadj1
(4-5/4-5) (5-6/5-6)
picks[v] up[p]
(3-4+2.B/0-8)
man[n](2-3/0-3) laman1(6-7/4-7)
</figure>
<bodyText confidence="0.986748714285714">
process is done by traversing the SSTC tree structure
for the example in preorder traversal, and each
substitution in the tree structure replaced with its
corresponding substitution in the input sentence. This
approach is analogous to top down parsing technique.
Figure 8, illustrates the parsing schema for the input
sentence&amp;quot; The old man picks the green lamp up&amp;quot;.
</bodyText>
<figure confidence="0.660965">
Input sentence
</figure>
<figureCaption confidence="0.995535333333333">
Figure 8: The parsing schema based on the SSTC for the
sentence &amp;quot;the old man picks the green lamp up&amp;quot; using
example (1).
</figureCaption>
<sectionHeader confidence="0.998422" genericHeader="conclusions">
5. CONCLUSION
</sectionHeader>
<bodyText confidence="0.998886857142857">
In this paper, we sketch an approach for parsing
NL string, which is an example-based approach
relies on the examples that already parsed to their
representation structures, and on the knowledge that
we can get from these examples information needed
to parse the input sentence.
A flexible annotation schema called Structured
String-Tree Correspondence (SSTC) is introduced to
express linguistic phenomena such as featurisation,
lexicalisation and crossed dependencies. We also
present an overview of the algorithm to parse natural
language sentences based on the SSTC annotation
schema. However, to obtain a full version of the
parsing algorithm, there are several other problems
which needed to be considered further, i.e. the
handling of multiple substitutions, an efficient
method to calculate the distance between the input
sentence and the examples, and lastly a detailed
formula to compute the resultant SSTC obtained from
the combination process especially when deletion of
optional substitutions are involved.
</bodyText>
<sectionHeader confidence="0.99808" genericHeader="references">
References:
</sectionHeader>
<reference confidence="0.99986359375">
[1] M.Nagao, &amp;quot;A Framework of a mechanical
translation between Japanese and English by analogy
principle&amp;quot;, in; A. Elithorn, R. Benerji, (Eds.),
Artificial and Human Intelligence, Elsevier:
Amsterdam.
[2] V.Sadler &amp; Vendelmans, &amp;quot;Pilot implementation of
a bilingual knowledge bank&amp;quot;, Proc. of Coling-90,
Helsinki, 3, 1990, 449-451.
[3] S. Sato &amp; M.Nagao, &amp;quot;Example-based Translation
of technical Terms&amp;quot;, Proc. of TMI-93, Koyoto, 1993,
58-68.
[4] Y. Zaharin &amp; C. Boitet, &amp;quot;Representation trees and
string-tree correspondences&amp;quot;, Proc. of Coling-88,
Budapest, 1988, 59-64.
[5] E. K. Tang &amp; Y. Zaharin, &amp;quot;Handling Crossed
Dependencies with the STCG&amp;quot;, Proc. of NLPRS&apos;95,
Seoul, 1995.
[6] Y.Lepage &amp; A.Shin-ichi, &amp;quot;Saussurian analogy: a
theoritical account and its application&amp;quot;, Proc. of
Coling-96, Copenhagen, 2, 1996, 717-722.
[7] V. I. Levenshtein, &amp;quot;Binary codes capable of
correcting deletions, insertions and reversals&amp;quot;, Dokl.
Akad. Nauk SSSR, 163, No. 4, 1965, 845-848.
English translation in Soviet Physics-doklady, 10,
No. 8, 1966, 707-710.
[8] Robert A. Wagner &amp; Michael J. Fischer, &amp;quot; The
String-to String Correction Problem&amp;quot;, Journal for the
Association of Computing Machinery, 21, No. 1,
1974, 168-173.
[9] Stanley M. Selkow, &amp;quot;The Tree-to-Tree Editing
Problem&amp;quot;, Information Processing Letters, 6, No. 6,
1977, 184-186.
</reference>
<page confidence="0.999134">
693
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.903362">
<title confidence="0.998678">A FLEXIBLE EXAMPLE-BASED PARSER BASED ON THE SSTC •</title>
<author confidence="0.974277">Mosleh Hmoud Al-Adhaileh</author>
<author confidence="0.974277">Tang Enya Kong</author>
<affiliation confidence="0.986393">Computer Aided Translation Unit School of computer sciences University Sains Malaysia</affiliation>
<address confidence="0.999243">11800 PENANG, MALAYSIA</address>
<email confidence="0.978291">mosleh@cs.usm.my,enyakong@cs.usm.my</email>
<abstract confidence="0.9999459">In this paper we sketch an approach for Natural Language parsing. Our approach is an example-based approach, which relies mainly on examples that already parsed to their representation structure, and on the knowledge that we can get from these examples the required information to parse a new input sentence. In our approach, examples are annotated with the Structured String Tree Correspondence (SSTC) annotation schema where each SSTC describes a sentence, a representation tree as well as the correspondence between substrings in sentence and subtrees in the representation tree. process of parsing, we first try to build subtrees for phrases in the input sentence which have been successfully found in the example-base a bottom up approach. These subtrees will then be combined together to form a single rooted representation tree based on an example with similar representation structure a top down approach.</abstract>
<keyword confidence="0.981681">Keywords: Example-based parsing, SSTC.</keyword>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>M Nagao</author>
</authors>
<title>A Framework of a mechanical translation between Japanese and English by analogy principle&amp;quot;, in;</title>
<journal>A. Elithorn, R. Benerji, (Eds.), Artificial and Human Intelligence,</journal>
<location>Elsevier: Amsterdam.</location>
<contexts>
<context position="11092" citStr="[1]" startWordPosition="1729" endWordPosition="1729">e linguistic representation) and the mapping between the two (correspondence); example-based parsing is performed by giving a new input sentence, followed by getting the related examples(i.e. examples that contains same words in the input sentence) from the example-base, and used them to compute the representation tree for the input sentence guided by the correspondence between the string and the tree as discussed in the following sections. Figure 6 illustrates the general schema for example-based NL parsing based on the SSTC schema. 4. 1 The parsing algorithm The example-based approach in MT [1], [2] or [3], relies on the assumption that if two sentences are &amp;quot;close&amp;quot;, their analysis should be &amp;quot;close&amp;quot; too. If the analysis of the first one is known, the analysis of the other can be obtained by making some modifications in the analysis of the first one (i.e. I Each node is tagged with syntactic category to enable substitution at category level. Representation tree structure in the output SSTC Figure 6: Example-based natural language parsing based on the SSTC schema. 689 close: distance not too large, modification: edit operations (insert, delete, replace) [6]. In most of the cases, simil</context>
</contexts>
<marker>[1]</marker>
<rawString>M.Nagao, &amp;quot;A Framework of a mechanical translation between Japanese and English by analogy principle&amp;quot;, in; A. Elithorn, R. Benerji, (Eds.), Artificial and Human Intelligence, Elsevier: Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Sadler</author>
<author>Vendelmans</author>
</authors>
<title>Pilot implementation of a bilingual knowledge bank&amp;quot;,</title>
<date>1990</date>
<booktitle>Proc. of Coling-90,</booktitle>
<pages>449--451</pages>
<location>Helsinki, 3,</location>
<contexts>
<context position="3100" citStr="[2]" startWordPosition="474" endWordPosition="474">will not be found in the example-base. In such case, a method is used to retrieve close related examples and use the knowledge from these examples to build the analysis for the input sentence. In general, this approach relies on the assumption that if two strings (phrase or sentence) are &amp;quot;close&amp;quot;, their analysis should be &amp;quot;close&amp;quot; too. If the analysis of the first one is known, the analysis of the other can be obtained by making some modifications in the analysis of the first one. The example-based approach has become a common technique for NLP applications, especially in MT as reported in [I], [2] or [3]. However, a main problem normally arises in the current approaches which indirectly limits their applications in the development of a large scale and practical examplebased system. Namely the lack of flexibility in creating the representation tree due to the restriction that correspondences between nodes (terminal or non terminal) of the representation tree and words of the sentence must be one-to-one and some even restrict it to only in projective manner according to certain traversal order. This restriction normally results to the inefficient usage of the example-base. In this paper,</context>
<context position="11097" citStr="[2]" startWordPosition="1730" endWordPosition="1730">guistic representation) and the mapping between the two (correspondence); example-based parsing is performed by giving a new input sentence, followed by getting the related examples(i.e. examples that contains same words in the input sentence) from the example-base, and used them to compute the representation tree for the input sentence guided by the correspondence between the string and the tree as discussed in the following sections. Figure 6 illustrates the general schema for example-based NL parsing based on the SSTC schema. 4. 1 The parsing algorithm The example-based approach in MT [1], [2] or [3], relies on the assumption that if two sentences are &amp;quot;close&amp;quot;, their analysis should be &amp;quot;close&amp;quot; too. If the analysis of the first one is known, the analysis of the other can be obtained by making some modifications in the analysis of the first one (i.e. I Each node is tagged with syntactic category to enable substitution at category level. Representation tree structure in the output SSTC Figure 6: Example-based natural language parsing based on the SSTC schema. 689 close: distance not too large, modification: edit operations (insert, delete, replace) [6]. In most of the cases, similar se</context>
</contexts>
<marker>[2]</marker>
<rawString>V.Sadler &amp; Vendelmans, &amp;quot;Pilot implementation of a bilingual knowledge bank&amp;quot;, Proc. of Coling-90, Helsinki, 3, 1990, 449-451.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sato</author>
<author>M Nagao</author>
</authors>
<title>Example-based Translation of technical Terms&amp;quot;,</title>
<date>1993</date>
<booktitle>Proc. of TMI-93,</booktitle>
<pages>58--68</pages>
<location>Koyoto,</location>
<contexts>
<context position="3107" citStr="[3]" startWordPosition="476" endWordPosition="476">t be found in the example-base. In such case, a method is used to retrieve close related examples and use the knowledge from these examples to build the analysis for the input sentence. In general, this approach relies on the assumption that if two strings (phrase or sentence) are &amp;quot;close&amp;quot;, their analysis should be &amp;quot;close&amp;quot; too. If the analysis of the first one is known, the analysis of the other can be obtained by making some modifications in the analysis of the first one. The example-based approach has become a common technique for NLP applications, especially in MT as reported in [I], [2] or [3]. However, a main problem normally arises in the current approaches which indirectly limits their applications in the development of a large scale and practical examplebased system. Namely the lack of flexibility in creating the representation tree due to the restriction that correspondences between nodes (terminal or non terminal) of the representation tree and words of the sentence must be one-to-one and some even restrict it to only in projective manner according to certain traversal order. This restriction normally results to the inefficient usage of the example-base. In this paper, we sha</context>
<context position="11104" citStr="[3]" startWordPosition="1732" endWordPosition="1732"> representation) and the mapping between the two (correspondence); example-based parsing is performed by giving a new input sentence, followed by getting the related examples(i.e. examples that contains same words in the input sentence) from the example-base, and used them to compute the representation tree for the input sentence guided by the correspondence between the string and the tree as discussed in the following sections. Figure 6 illustrates the general schema for example-based NL parsing based on the SSTC schema. 4. 1 The parsing algorithm The example-based approach in MT [1], [2] or [3], relies on the assumption that if two sentences are &amp;quot;close&amp;quot;, their analysis should be &amp;quot;close&amp;quot; too. If the analysis of the first one is known, the analysis of the other can be obtained by making some modifications in the analysis of the first one (i.e. I Each node is tagged with syntactic category to enable substitution at category level. Representation tree structure in the output SSTC Figure 6: Example-based natural language parsing based on the SSTC schema. 689 close: distance not too large, modification: edit operations (insert, delete, replace) [6]. In most of the cases, similar sentence </context>
</contexts>
<marker>[3]</marker>
<rawString>S. Sato &amp; M.Nagao, &amp;quot;Example-based Translation of technical Terms&amp;quot;, Proc. of TMI-93, Koyoto, 1993, 58-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zaharin</author>
<author>C Boitet</author>
</authors>
<title>Representation trees and string-tree correspondences&amp;quot;,</title>
<date>1988</date>
<booktitle>Proc. of Coling-88,</booktitle>
<pages>59--64</pages>
<location>Budapest,</location>
<contexts>
<context position="6365" citStr="[4]" startWordPosition="969" endWordPosition="969">age. In crossed dependencies, subtree in the tree corresponds to single substring in the sentence, but the words in a substring are distributed over the whole sentence in a discontinuous manner, in relation to the subtree they correspond to. An example of crossed dependencies is occurred in the sentences of the form (an v bn n&gt;0), figure 3 illustrates the representation tree for the string &amp;quot;aa v bb cc &amp;quot;(also written a.la.2 v b.lb.2 c.lc.2 to show the positions), this akin to the &apos;respectively&apos; problem in English sentence like &amp;quot;John and Mary give Paul and Ann trousers and dresses respectively&amp;quot; [4]. a.1 b.1 A-P,4 a.1 a.2 vm&apos;o.1 b. AN a.2 b.2 c.2 C.1 c.2 John apple the He picks up the ball Figure I: Featurisation 2.2 Lexicalisation Lexicalisation is the case when a particular subtree in the representation tree presents the meaning of some part of the string, which is not orally realized in phonological form. Lexicalisation may result from the correspondence of a subtree in the tree to an empty substring in the sentence, or substring in the sentence to more than one subtree in the tree. Figure 2 illustrates the sentence &amp;quot;John eats the apple and Mary the pear&amp;quot; where &amp;quot;eats&amp;quot; in the sentence </context>
<context position="8323" citStr="[4]" startWordPosition="1288" endWordPosition="1288">s between substrings of the sentence and subtrees of the tree. Such correspondence is made of two interrelated correspondences, one between nodes and substrings, and the other between subtrees and substrings, (the substrings being possibly discontinuous in both cases). The notation used in SSTC to denote a correspondence consists of a pair of intervals X/Y attached to each node in the tree, where X(SNODE) denotes the interval containing the substring that corresponds to the node, and Y(STREE) denotes the interval containing the substring that corresponds to the subtree having the node as root [4]. Figure 5 illustrates the sentence &amp;quot;all cats eat mice&amp;quot; with its corresponding SSTC. It is a simple projective correspondence. An interval is assigned to each word in the sentence, i.e. (0-1) for &amp;quot;all&amp;quot;, (1-2) for &amp;quot;cats&amp;quot;, (2-3) for &amp;quot;eat&amp;quot; and (3-4) for &amp;quot;mice&amp;quot;. A substring in the sentence that corresponds to a node in the representation tree is denoted by assigning the interval of the substring to SNODE of the node, e.g. the node &amp;quot;cats&amp;quot; with SNODE interval (1-2) corresponds to the word &amp;quot;cats&amp;quot; in the string with the similar interval. The correspondence between subtrees and substrings are denoted b</context>
</contexts>
<marker>[4]</marker>
<rawString>Y. Zaharin &amp; C. Boitet, &amp;quot;Representation trees and string-tree correspondences&amp;quot;, Proc. of Coling-88, Budapest, 1988, 59-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E K Tang</author>
<author>Y Zaharin</author>
</authors>
<title>Handling Crossed Dependencies with the STCG&amp;quot;,</title>
<date>1995</date>
<booktitle>Proc. of NLPRS&apos;95, Seoul,</booktitle>
<contexts>
<context position="10354" citStr="[5]" startWordPosition="1615" endWordPosition="1615">ples based on the Structured String-Tree Correspondence (SSTC). The SSTC is a general structure that can associate, to string in a language, arbitrary tree structure as desired by the annotator to be the interpretation structure of the string, and more importantly is the facility to specify the correspondence between the string and the associated tree which can be interpreted for both analysis and synthesis in NLP. These features are very much desired in the design of an annotation scheme, in particular for the treatment of linguistic phenomena which are not-standard e.g. crossed dependencies [5]. Since the example in the example-base are described in terms of SSTC, which consists of a sentence (the text), a dependency tree&apos; (the linguistic representation) and the mapping between the two (correspondence); example-based parsing is performed by giving a new input sentence, followed by getting the related examples(i.e. examples that contains same words in the input sentence) from the example-base, and used them to compute the representation tree for the input sentence guided by the correspondence between the string and the tree as discussed in the following sections. Figure 6 illustrates</context>
</contexts>
<marker>[5]</marker>
<rawString>E. K. Tang &amp; Y. Zaharin, &amp;quot;Handling Crossed Dependencies with the STCG&amp;quot;, Proc. of NLPRS&apos;95, Seoul, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Lepage</author>
<author>A Shin-ichi</author>
</authors>
<title>Saussurian analogy: a theoritical account and its application&amp;quot;,</title>
<date>1996</date>
<booktitle>Proc. of Coling-96,</booktitle>
<pages>717--722</pages>
<location>Copenhagen,</location>
<contexts>
<context position="11663" citStr="[6]" startWordPosition="1822" endWordPosition="1822">example-based approach in MT [1], [2] or [3], relies on the assumption that if two sentences are &amp;quot;close&amp;quot;, their analysis should be &amp;quot;close&amp;quot; too. If the analysis of the first one is known, the analysis of the other can be obtained by making some modifications in the analysis of the first one (i.e. I Each node is tagged with syntactic category to enable substitution at category level. Representation tree structure in the output SSTC Figure 6: Example-based natural language parsing based on the SSTC schema. 689 close: distance not too large, modification: edit operations (insert, delete, replace) [6]. In most of the cases, similar sentence might not occurred in the example-base, so the system utilized some close related examples to the given input sentence (i.e. similar structure to the input sentence or contain some words in the input sentence). For that it is necessary to construct several subSSTCs (called substitutions hereafter) for phrases in the input sentence according to their occurrence in the examples from the example-base. These substitutions are then combined together to form a complete SSTC as the output. Suppose the system intends to parse the sentence &amp;quot;the old man picks the</context>
<context position="19718" citStr="[6]" startWordPosition="3279" endWordPosition="3279">operations. Editing operations are insert (E 4 p), deletion (p4E ) and replacing (a 4 s). Edition distances, which have been proposed in many works [7], [8] and [9], reflect a sensible notion, and it can be represented as metrics under some hypotheses. They defined the edition distances as number of editing operations to transfer one word to another form, i.e. how many characters needed to be edited based on insertion, deletion or replacement. Since words are strings of characters, sentences are strings of words, editing distances hence are not confined to words, they may be used on sentences [6]. With the similar idea, we define the edition distance as: (i) The distance is calculated at level of substitutions (i.e. only the root nodes of the substitutions will be considered, not all the words in the sentences). (ii) The edit operations are done based on the syntactic category of the root nodes, (i.e. the comparison between the input sentence and an example is based on the syntactic category of the root nodes of their substitutions, not based on the words). The distance is calculated based on the number of editing operations (deletions and insertion) needed to transfer the input sente</context>
</contexts>
<marker>[6]</marker>
<rawString>Y.Lepage &amp; A.Shin-ichi, &amp;quot;Saussurian analogy: a theoritical account and its application&amp;quot;, Proc. of Coling-96, Copenhagen, 2, 1996, 717-722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals&amp;quot;,</title>
<date>1965</date>
<journal>Dokl. Akad. Nauk SSSR,</journal>
<volume>163</volume>
<pages>845--848</pages>
<contexts>
<context position="19266" citStr="[7]" startWordPosition="3205" endWordPosition="3205"> same assumptions mentioned earlier in substitutions generation, which are: he[n] Picks [v] ballEn] up[p] (0-1 /0-1 ) (1-2/0-5) (3-4/2-4) (4-5/ -) the [det] (2-3/2-3) (1) (2) (3) (4) Distance calculation: Here the system utilizes distance calculation to determine the plausible example, which SSTC structure will be used as a base to combine the substitutions at the input sentence. We define a heuristic to calculate the distance, in terms of editing operations. Editing operations are insert (E 4 p), deletion (p4E ) and replacing (a 4 s). Edition distances, which have been proposed in many works [7], [8] and [9], reflect a sensible notion, and it can be represented as metrics under some hypotheses. They defined the edition distances as number of editing operations to transfer one word to another form, i.e. how many characters needed to be edited based on insertion, deletion or replacement. Since words are strings of characters, sentences are strings of words, editing distances hence are not confined to words, they may be used on sentences [6]. With the similar idea, we define the edition distance as: (i) The distance is calculated at level of substitutions (i.e. only the root nodes of th</context>
</contexts>
<marker>[7]</marker>
<rawString>V. I. Levenshtein, &amp;quot;Binary codes capable of correcting deletions, insertions and reversals&amp;quot;, Dokl. Akad. Nauk SSSR, 163, No. 4, 1965, 845-848. English translation in Soviet Physics-doklady, 10, No. 8, 1966, 707-710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert A Wagner</author>
<author>Michael J Fischer</author>
</authors>
<title>The String-to String Correction Problem&amp;quot;,</title>
<date>1974</date>
<journal>Journal for the Association of Computing Machinery,</journal>
<volume>21</volume>
<pages>168--173</pages>
<contexts>
<context position="19271" citStr="[8]" startWordPosition="3206" endWordPosition="3206"> assumptions mentioned earlier in substitutions generation, which are: he[n] Picks [v] ballEn] up[p] (0-1 /0-1 ) (1-2/0-5) (3-4/2-4) (4-5/ -) the [det] (2-3/2-3) (1) (2) (3) (4) Distance calculation: Here the system utilizes distance calculation to determine the plausible example, which SSTC structure will be used as a base to combine the substitutions at the input sentence. We define a heuristic to calculate the distance, in terms of editing operations. Editing operations are insert (E 4 p), deletion (p4E ) and replacing (a 4 s). Edition distances, which have been proposed in many works [7], [8] and [9], reflect a sensible notion, and it can be represented as metrics under some hypotheses. They defined the edition distances as number of editing operations to transfer one word to another form, i.e. how many characters needed to be edited based on insertion, deletion or replacement. Since words are strings of characters, sentences are strings of words, editing distances hence are not confined to words, they may be used on sentences [6]. With the similar idea, we define the edition distance as: (i) The distance is calculated at level of substitutions (i.e. only the root nodes of the sub</context>
</contexts>
<marker>[8]</marker>
<rawString>Robert A. Wagner &amp; Michael J. Fischer, &amp;quot; The String-to String Correction Problem&amp;quot;, Journal for the Association of Computing Machinery, 21, No. 1, 1974, 168-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley M Selkow</author>
</authors>
<title>The Tree-to-Tree Editing Problem&amp;quot;,</title>
<date>1977</date>
<journal>Information Processing Letters,</journal>
<volume>6</volume>
<pages>184--186</pages>
<contexts>
<context position="19279" citStr="[9]" startWordPosition="3208" endWordPosition="3208">ions mentioned earlier in substitutions generation, which are: he[n] Picks [v] ballEn] up[p] (0-1 /0-1 ) (1-2/0-5) (3-4/2-4) (4-5/ -) the [det] (2-3/2-3) (1) (2) (3) (4) Distance calculation: Here the system utilizes distance calculation to determine the plausible example, which SSTC structure will be used as a base to combine the substitutions at the input sentence. We define a heuristic to calculate the distance, in terms of editing operations. Editing operations are insert (E 4 p), deletion (p4E ) and replacing (a 4 s). Edition distances, which have been proposed in many works [7], [8] and [9], reflect a sensible notion, and it can be represented as metrics under some hypotheses. They defined the edition distances as number of editing operations to transfer one word to another form, i.e. how many characters needed to be edited based on insertion, deletion or replacement. Since words are strings of characters, sentences are strings of words, editing distances hence are not confined to words, they may be used on sentences [6]. With the similar idea, we define the edition distance as: (i) The distance is calculated at level of substitutions (i.e. only the root nodes of the substitutio</context>
</contexts>
<marker>[9]</marker>
<rawString>Stanley M. Selkow, &amp;quot;The Tree-to-Tree Editing Problem&amp;quot;, Information Processing Letters, 6, No. 6, 1977, 184-186.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>