<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000330">
<title confidence="0.978977">
Co-regularizing character-based and word-based models for
semi-supervised Chinese word segmentation
</title>
<author confidence="0.765624">
Xiaodong Zeng† Derek F. Wong† Lidia S. Chao† Isabel Trancoso‡†Department of Computer and Information Science, University of Macau
</author>
<affiliation confidence="0.586041">
‡INESC-ID / Instituto Superior T´ecnico, Lisboa, Portugal
</affiliation>
<email confidence="0.6411175">
nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo,
isabel.trancoso@inesc-id.pt
</email>
<sectionHeader confidence="0.992721" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995524">
This paper presents a semi-supervised
Chinese word segmentation (CWS) ap-
proach that co-regularizes character-based
and word-based models. Similarly to
multi-view learning, the “segmentation
agreements” between the two differen-
t types of view are used to overcome the
scarcity of the label information on unla-
beled data. The proposed approach train-
s a character-based and word-based mod-
el on labeled data, respectively, as the ini-
tial models. Then, the two models are con-
stantly updated using unlabeled examples,
where the learning objective is maximiz-
ing their segmentation agreements. The a-
greements are regarded as a set of valuable
constraints for regularizing the learning of
both models on unlabeled data. The seg-
mentation for an input sentence is decod-
ed by using a joint scoring function com-
bining the two induced models. The e-
valuation on the Chinese tree bank reveals
that our model results in better gains over
the state-of-the-art semi-supervised mod-
els reported in the literature.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992241509434">
Chinese word segmentation (CWS) is a critical
and a necessary initial procedure with respect to
the majority of high-level Chinese language pro-
cessing tasks such as syntax parsing, informa-
tion extraction and machine translation, since Chi-
nese scripts are written in continuous characters
without explicit word boundaries. Although su-
pervised CWS models (Xue, 2003; Zhao et al.,
2006; Zhang and Clark, 2007; Sun, 2011) pro-
posed in the past years showed some reasonably
accurate results, the outstanding problem is that
they rely heavily on a large amount of labeled da-
ta. However, the production of segmented Chi-
nese texts is time-consuming and expensive, since
hand-labeling individual words and word bound-
aries is very hard (Jiao et al., 2006). So, one can-
not rely only on the manually segmented data to
build an everlasting model. This naturally pro-
vides motivation for using easily accessible raw
texts to enhance supervised CWS models, in semi-
supervised approaches. In the past years, however,
few semi-supervised CWS models have been pro-
posed. Xu et al. (2008) described a Bayesian semi-
supervised model by considering the segmentation
as the hidden variable in machine translation. Sun
and Xu (2011) enhanced the segmentation result-
s by interpolating the statistics-based features de-
rived from unlabeled data to a CRFs model. An-
other similar trial via “feature engineering” was
conducted by Wang et al. (2011).
The crux of solving semi-supervised learning
problem is the learning on unlabeled data. In-
spired by multi-view learning that exploits redun-
dant views of the same input data (Ganchev et
al., 2008), this paper proposes a semi-supervised
CWS model of co-regularizing from two dif-
ferent views (intrinsically two different models),
character-based and word-based, on unlabeled da-
ta. The motivation comes from that the two types
of model exhibit different strengths and they are
mutually complementary (Sun, 2010; Wang et al.,
2010). The proposed approach begins by train-
ing a character-based and word-based model on
labeled data respectively, and then both models
are regularized from each view by their segmen-
tation agreements, i.e., the identical outputs, of
unlabeled data. This paper introduces segmenta-
tion agreements as gainful knowledge for guiding
the learning on the texts without label information.
Moreover, in order to better combine the strengths
of the two models, the proposed approach uses a
joint scoring function in a log-linear combination
form for the decoding in the segmentation phase.
</bodyText>
<page confidence="0.975583">
171
</page>
<note confidence="0.5303575">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 171–176,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.977935" genericHeader="method">
2 Segmentation Models
</sectionHeader>
<bodyText confidence="0.999932333333333">
There are two classes of CWS models: character-
based and word-based. This section briefly re-
views two supervised models in these categories,
a character-based CRFs model, and a word-based
Perceptrons model, which are used in our ap-
proach.
</bodyText>
<subsectionHeader confidence="0.98249">
2.1 Character-based CRFs Model
</subsectionHeader>
<bodyText confidence="0.995594083333333">
Character-based models treat word segmentation
as a sequence labeling problem, assigning label-
s to the characters in a sentence indicating their
positions in a word. A 4 tag-set is used in this
paper: B (beginning), M (middle), E (end) and
S (single character). Xue (2003) first proposed
the use of CRFs model (Lafferty et al., 2001) in
character-based CWS. Let x = (x1x2...x|x|) ∈ X
denote a sentence, where each character and y =
(y1y2... y|y|) ∈ Y denote a tag sequence, yi ∈ T
being the tag assigned to xi. The goal is to achieve
a label sequence with the best score in the form,
</bodyText>
<equation confidence="0.9983945">
1
pθc(y|x) = Z(x; θc) exp{f(x, y) · θc} (1)
</equation>
<bodyText confidence="0.999917333333333">
where Z(x; θc) is a partition function that normal-
izes the exponential form to be a probability distri-
bution, and f(x, y) are arbitrary feature functions.
The aim of CRFs is to estimate the weight param-
eters θc that maximizes the conditional likelihood
of the training data:
</bodyText>
<equation confidence="0.945856">
log pθc(yi|xi) − γkθck22 (2)
</equation>
<bodyText confidence="0.9984985">
where γkθck22is a regularizer on parameters to
limit overfitting on rare features and avoid degen-
eracy in the case of correlated features. In this
paper, this objective function is optimized by s-
tochastic gradient method. For the decoding, the
Viterbi algorithm is employed.
</bodyText>
<subsectionHeader confidence="0.997767">
2.2 Word-based Perceptrons Model
</subsectionHeader>
<bodyText confidence="0.999987833333333">
Word-based models read a input sentence from left
to right and predict whether the current piece of
continuous characters is a word. After one word
is identified, the method moves on and searches
for a next possible word. Zhang and Clark (2007)
first proposed a word-based segmentation mod-
el using a discriminative Perceptrons algorithm.
Given a sentence x, let us denote a possible seg-
mented sentence as w ∈ w, and the function that
enumerates a set of segmentation candidates as
GEN:w = GEN(x) for x. The objective is to
maximize the following problem for all sentences:
</bodyText>
<equation confidence="0.99602">
ˆθw = argmax
w=GEN(x)
</equation>
<bodyText confidence="0.998047875">
where it maps the segmented sentence w to a glob-
al feature vector φ and denotes θw as its cor-
responding weight parameters. The parameter-
s θw can be estimated by using the Perceptron-
s method (Collins, 2002) or other online learning
algorithms, e.g., Passive Aggressive (Crammer et
al., 2006). For the decoding, a beam search decod-
ing method (Zhang and Clark, 2007) is used.
</bodyText>
<subsectionHeader confidence="0.99903">
2.3 Comparison Between Both Models
</subsectionHeader>
<bodyText confidence="0.9998385">
Character-based and word-based models present
different behaviors and each one has its own
strengths and weakness. Sun (2010) carried out a
thorough survey that includes theoretical and em-
pirical comparisons from four aspects. Here, two
critical properties of the two models supporting
the co-regularization in this study are highlight-
ed. Character-based models present better predic-
tion ability for new words, since they lay more
emphasis on the internal structure of a word and
thereby express more nonlinearity. On the oth-
er side, it is easier to define the word-level fea-
tures in word-based models. Hence, these models
have a greater representational power and conse-
quently better recognition performance for in-of-
vocabulary (IV) words.
</bodyText>
<sectionHeader confidence="0.7012855" genericHeader="method">
3 Semi-supervised Learning via
Co-regularizing Both Models
</sectionHeader>
<bodyText confidence="0.9998405">
As mentioned earlier, the primary challenge of
semi-supervised CWS concentrates on the unla-
beled data. Obviously, the learning on unlabeled
data does not come for “free”. Very often, it is
necessary to discover certain gainful information,
e.g., label constraints of unlabeled data, that is in-
corporated to guide the learner toward a desired
solution. In our approach, we believe that the seg-
mentation agreements (§ 3.1) from two differen-
t views, character-based and word-based models,
can be such gainful information. Since each of the
models has its own merits, their consensuses signi-
fy high confidence segmentations. This naturally
leads to a new learning objective that maximizes
segmentation agreements between two models on
unlabeled data.
</bodyText>
<equation confidence="0.997544833333333">
ˆθc = argmax
θc
�l
i=1
� |w |φ(x, wi) · θw (3)
i=1
</equation>
<page confidence="0.981856">
172
</page>
<bodyText confidence="0.994922">
This study proposes a co-regularized CWS
model based on character-based and word-based
models, built on a small amount of segmented sen-
tences (labeled data) and a large amount of raw
sentences (unlabeled data). The model induction
process is described in Algorithm 1: given labeled
dataset Dl and unlabeled dataset Du, the first t-
wo steps are training a CRFs (character-based) and
Perceptrons (word-based) model on the labeled
data Dl , respectively. Then, the parameters of
both models are continually updated using unla-
beled examples in a learning cycle. At each iter-
ation, the raw sentences in Du are segmented by
current character-based model Bc and word-based
model Bw. Meanwhile, all the segmentation agree-
ments A are collected (§ 3.1). Afterwards, the
agreements A are used as a set of constraints to
bias the learning of CRFs (§ 3.2) and Perceptron
(§ 3.3) on the unlabeled data. The convergence
criterion is the occurrence of a reduction of seg-
mentation agreements or reaching the maximum
number of learning iterations. In the final segmen-
tation phase, given a raw sentence, the decoding
requires both induced models (§ 3.4) in measuring
a segmentation score.
</bodyText>
<figure confidence="0.463678333333333">
Algorithm 1 Co-regularized CWS model induction
Require: n labeled sentences Dl; m unlabeled sentences Du
Ensure: Bc and Bw
</figure>
<listItem confidence="0.964288">
1: B0c crf train(Dl)
2: B0w perceptron train(Dl)
3: fort = 1...Tmax do
4: At agree(Du, Bt−1
c , Bt−1
w )
5: Btc crf train constraints(Du, At, Bt−1
c )
6: Bt w perceptron train constraints(Du, At, Bt−1
w )
7: end for
</listItem>
<subsectionHeader confidence="0.999202">
3.1 Agreements Between Two Models
</subsectionHeader>
<bodyText confidence="0.9999642">
Given a raw sentence, e.g., “R 正 在 北 京 看 奥
会开4式.(I am watching the opening ceremony
of the Olympics in Beijing.)”, the two segmenta-
tions shown in Figure 1 are the predictions from
a character-based and word-based model. The
segmentation agreements between the two mod-
els correspond to the identical words. In this ex-
ample, the five words, i.e. “R (I)”, “北京 (Bei-
jing)”, “看 (watch)”, “开4式 (opening ceremony)”
and “.(.)”, are the agreements.
</bodyText>
<subsectionHeader confidence="0.999765">
3.2 CRFs with Constraints
</subsectionHeader>
<bodyText confidence="0.999563176470588">
For the character-based model, this paper fol-
lows (T¨ackstr¨om et al., 2013) to incorporate the
segmentation agreements into CRFs. The main
idea is to constrain the size of the tag sequence
lattice according to the agreements for achieving
simplified learning. Figure 2 demonstrates an ex-
ample of the constrained lattice, where the bold
node represents that a definitive tag derived from
the agreements is assigned to the current charac-
ter, e.g., “R (I)” has only one possible tag “S”
because both models segmented it to a word with
a single character. Here, if the lattice of all admis-
sible tag sequences for the sentence x is denoted
as Y(x), the constrained lattice can be defined by
ˆY(x, ˜y), where y˜ refers to tags inferred from the
agreements. Thus, the objective function on unla-
beled data is modeled as:
</bodyText>
<equation confidence="0.570667">
ˆY(xi, ˜yi)|xi) − γkBck22
(4)
</equation>
<figureCaption confidence="0.79070725">
It is a marginal conditional probability given by
the total probability of all tag sequences consistent
with the constrained lattice ˆY(x, ˜y). This objec-
tive can be optimized by using LBFGS-B (Zhu et
al., 1997), a generic quasi-Newton gradient-based
optimizer.
Figure 1: The segmentations given by character-
based and word-based model, where the words in
“✷” refer to the segmentation agreements.
Figure 2: The constrained lattice representation
for a given sentence, “R正在北京看奥运会开4
式.”.
</figureCaption>
<subsectionHeader confidence="0.999699">
3.3 Perceptrons with Constraints
</subsectionHeader>
<bodyText confidence="0.999988">
For the word-based model, this study incorporates
segmentation agreements by a modified parame-
ter update criterion in Perceptrons online training,
as shown in Algorithm 2. Because there are no
“gold segmentations” for unlabeled sentences, the
output sentence predicted by the current model is
compared with the agreements instead of the “an-
swers” in the supervised case. At each parameter
</bodyText>
<equation confidence="0.977541">
ˆB� = argmax �m log pθc(
θc i=1
</equation>
<page confidence="0.994162">
173
</page>
<bodyText confidence="0.9892965">
update iteration k, each raw sentence x,, is decod-
ed with the current model into a segmentation z,,.
If the words in output z,, do not match the agree-
ments A(x,,) of the current sentence x,,, the pa-
rameters are updated by adding the global feature
vector of the current training example with the a-
greements and subtracting the global feature vec-
tor of the decoder output, as described in lines 3
and 4 of Algorithm 2.
Algorithm 2 Parameter update in word-based model
our experiments is from the XIN CMN portion of
Chinese Gigaword 2.0. The articles published in
1991-1993 and 1999-2004 are used as unlabeled
data, with 204 million words.
The feature templates in (Zhao et al., 2006)
and (Zhang and Clark, 2007) are used in train-ing
the CRFs model and Perceptrons model, respec-
tively. The experimental platform is implement-
ed based on two popular toolkits: CRF++ (Kudo,
2005) and Zpar (Zhang and Clark, 2011).
</bodyText>
<figure confidence="0.903360333333333">
1: fork= 1...K, u = 1...m do
2: calculate zu = argmax
w=GEN(x)
3: if zu =� A(xu)
4: θkw = θk−1
w + φ(A(xu)) − φ(zu)
5: end for
Data #Sent- #Sent- #Sent- OOV- OOV-
train dev test dev test
CTB-5 18,089 350 348 0.0811 0.0347
CTB-6 23,420 2,079 2,796 0.0545 0.0557
CTB-7 31,131 10,136 10,180 0.0549 0.0521
�|w|
i=1 φ(xu, wi) · θk−1
w
</figure>
<subsectionHeader confidence="0.949791">
3.4 The Joint Score Function for Decoding
</subsectionHeader>
<bodyText confidence="0.999828375">
There are two co-regularized models as results of
the previous induction steps. An intuitive idea is
that both induced models are combined to conduct
the segmentation, for the sake of integrating their
strengths. This paper employs a log-linear inter-
polation combination (Bishop, 2006) to formulate
a joint scoring function based on character-based
and word-based models in the decoding:
</bodyText>
<equation confidence="0.992119666666667">
Score(w) = α · log(pec(y|x))
+(1 − α) · log(φ(x, w) · θw)
(5)
</equation>
<bodyText confidence="0.999982833333333">
where the two terms of the logarithm are the s-
cores of character-based and word-based model-
s, respectively, for a given segmentation w. This
composite function uses a parameter α to weight
the contributions of the two models. The α value
is tuned using the development data.
</bodyText>
<sectionHeader confidence="0.995282" genericHeader="evaluation">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.998215">
4.1 Setting
</subsectionHeader>
<bodyText confidence="0.997125307692308">
The experimental data is taken from the Chinese
tree bank (CTB). In order to make a fair compar-
ison with the state-of-the-art results, the versions
of CTB-5, CTB-6, and CTB-7 are used for the e-
valuation. The training, development and testing
sets are defined according to the previous works.
For CTB-5, the data split from (Jiang et al., 2008)
is employed. For CTB-6, the same data split as
recommended in the CTB-6 official document is
used. For CTB-7, the datasets are formed accord-
ing to the way in (Wang et al., 2011). The cor-
responding statistic information on these data s-
plits is reported in Table 1. The unlabeled data in
</bodyText>
<tableCaption confidence="0.7226495">
Table 1: Statistics of CTB-5, CTB-6 and CTB-7
data.
</tableCaption>
<subsectionHeader confidence="0.997175">
4.2 Main Results
</subsectionHeader>
<bodyText confidence="0.999983111111111">
The development sets are mainly used to tune the
values of the weight factor α in Equation 5. We
evaluated the performance (F-score) of our model
on the three development sets by using differen-
t α values, where α is progressively increased in
steps of 0.1 (0 &lt; α &lt; 1.0). The best performed
settings of α for CTB-5, CTB-6 and CTB-7 on de-
velopment data are 0.7, 0.6 and 0.6, respectively.
With the chosen parameters, the test data is used
to measure the final performance.
Table 2 shows the F-score results of word seg-
mentation on CTB-5, CTB-6 and CTB-7 testing
sets. The line of “ours” reports the performance
of our semi-supervised model with the tuned pa-
rameters. We first compare it with the supervised
“baseline” method which joints character-based
and word-based model trained only on the training
set1. It can be observed that our semi-supervised
model is able to benefit from unlabeled data and
greatly improves the results over the supervised
baseline. We also compare our model with two
state-of-the-art semi-supervised methods of Wang
’11 (Wang et al., 2011) and Sun ’11 (Sun and X-
u, 2011). The performance scores of Wang ’11 are
directly taken from their paper, while the results of
Sun ’11 are obtained, using the program provided
by the author, on the same experimental data. The
</bodyText>
<footnote confidence="0.99034775">
1The “baseline” uses a different training configuration so
that the α values in the decoding are also need to be tuned on
the development sets. The tuned α values are {0.6, 0.6, 0.51
for CTB-5, CTB-6 and CTB-7.
</footnote>
<page confidence="0.99714">
174
</page>
<bodyText confidence="0.999737705882353">
bold scores indicate that our model does achieve
significant gains over these two semi-supervised
models. This outcome can further reveal that us-
ing the agreements from these two views to regu-
larize the learning can effectively guide the mod-
el toward a better solution. The third compari-
son candidate is Hatori ’12 (Hatori et al., 2012)
which reported the best performance in the litera-
ture on these three testing sets. It is a supervised
joint model of word segmentation, POS tagging
and dependency parsing. Impressively, our model
still outperforms Hatori ’12 on all three datasets.
Although there is only a 0.01 increase on CTB-5,
it can be seen as a significant improvement when
considering Hatori ’12 employs much richer train-
ing resources, i.e., sentences tagged with syntactic
information.
</bodyText>
<table confidence="0.999816833333333">
Method CTB-5 CTB-6 CTB-7
Ours 98.27 96.33 96.72
Baseline 97.58 94.71 94.87
Wang ’11 98.11 95.79 95.65
Sun ’11 98.04 95.44 95.34
Hatori ’12 98.26 96.18 96.07
</table>
<tableCaption confidence="0.9178175">
Table 2: F-score (%) results of five CWS models
on CTB-5, CTB-6 and CTB-7.
</tableCaption>
<sectionHeader confidence="0.998807" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999966875">
This paper proposed an alternative semi-
supervised CWS model that co-regularizes a
character- and word-based model by using their
segmentation agreements on unlabeled data. We
perform the agreements as valuable knowledge
for the regularization. The experiment results
reveal that this learning mechanism results in a
positive effect to the segmentation performance.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999953857142857">
The authors are grateful to the Science and Tech-
nology Development Fund of Macau and the Re-
search Committee of the University of Macau for
the funding support for our research, under the ref-
erence No. 017/2009/A and MYRG076(Y1-L2)-
FST13-WF. The authors also wish to thank the
anonymous reviewers for many helpful comments.
</bodyText>
<sectionHeader confidence="0.999108" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997740649122807">
Christopher M. Bishop. 2006. Pattern recognition and
machine learning.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of EMNLP, pages 1-8, Philadelphia, USA.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of ma-chine
learning research, 7:551-585.
Kuzman Ganchev, Joao Graca, John Blitzer, and Ben
Taskar. 2008. Multi-View Learning over Struc-
tured and Non-Identical Outputs. In Proceedings of
CUAI, pages 204-211, Helsinki, Finland.
Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and
Jun’ichi Tsujii. 2012. Incremental Joint Approach
to Word Segmentation, POS Tagging, and Depen-
dency Parsing in Chinese. In Proceedings of ACL,
pages 1045-1053, Jeju, Republic of Korea.
Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Liu.
2008. A Cascaded Linear Model for Joint Chinese
Word Segmentation and Part-of-Speech Tagging. In
Proceedings of ACL, pages 897-904, Columbus, O-
hio.
Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Au-
tomatic Adaptation of Annotation Standards: Chi-
nese Word Segmentation and POS Tagging - A Case
Study. In Proceedings of ACL and the 4th IJCNLP
of the AFNLP, pages 522-530, Suntec, Singapore.
Feng Jiao, Shaojun Wang and Chi-Hoon Lee. 2006.
Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In Pro-
ceedings ofACL and the 4th IJCNLP of the AFNLP,
pages 209-216, Strouds-burg, PA, USA.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
Software available at http://crfpp.sourceforge. net.
John Lafferty, Andrew McCallum, and Fernando Pe-
reira. 2001. Conditional Random Field: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proceedings of ICML, pages 282-
289, Williams College, USA.
Weiwei Sun. 2001. Word-based and character-based
word segmentation models: comparison and com-
bination. In Proceedings of COLING, pages 1211-
1219, Bejing, China.
Weiwei Sun. 2011. A stacked sub-word model for
joint Chinese word segmentation and part-of-speech
tagging. In Proceedings of ACL, pages 1385-1394,
Portland, Oregon.
Weiwei Sun and Jia Xu. 2011. Enhancing Chinese
word segmentation using unlabeled data. In Pro-
ceedings of EMNLP, pages 970-979, Scotland, UK.
Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan M-
cDonald, and Joakim Nivre. 2013. Token and Type
Constraints for Cross-Lingual Part-of-Speech Tag-
ging. In Transactions of the Association for Compu-
tational Linguistics, 1:1-12.
</reference>
<page confidence="0.984465">
175
</page>
<reference confidence="0.999521307692307">
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010.
A Character-Based Joint Model for Chinese Word
Segmentation. In Proceedings of COLING, pages
1173-1181, Bejing, China.
Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka,
Wenliang Chen, Yujie Zhang, and Kentaro Torisawa.
2011. Improving Chinese word segmentation and
POS tagging with semi-supervised methods using
large auto-analyzed data. In Proceedings of IJC-
NLP, pages 309-317, Hyderabad, India.
Jia Xu, Jianfeng Gao, Kristina Toutanova and Her-
mann Ney. 2008. Bayesian semi-supervised chinese
word segmentation for statistical machine transla-
tion. In Proceedings of COLING, pages 1017-1024,
Manchester, UK.
Nianwen Xue. 2003. Chinese word segmentation as
character tagging. Computational Linguistics and
Chinese Language Processing, 8(1):29-48.
Yue Zhang and Stephen Clark. 2007. Chinese segmen-
tation using a word-based perceptron algorithm. In
Proceedings of ACL, pages 840-847, Prague, Czech
Republic.
Yue Zhang and Stephen Clark. 2010. A fast decoder
for joint word segmentation and POS-tagging using
a single discriminative model. In Proceedings of
EMNLP, pages 843-852, Massachusetts, USA.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105-151.
Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang
Lu. 2006. Effective tag set selection in Chinese
word segmentation via conditional random field
modeling. In Proceedings of PACLIC, pages 87-94,
Wuhan, China.
Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge
Nocedal. 2006. L-BFGS-B: Fortran subroutines for
large scale bound constrained optimization. ACM
Transactions on Mathematical Software, 23:550-
560.
</reference>
<page confidence="0.998735">
176
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.713304">
<title confidence="0.979586">Co-regularizing character-based and word-based models semi-supervised Chinese word segmentation</title>
<author confidence="0.865616">Derek F Lidia S Isabel of Computer</author>
<author confidence="0.865616">Information Science</author>
<author confidence="0.865616">University of</author>
<affiliation confidence="0.818961">Instituto Superior T´ecnico, Lisboa,</affiliation>
<email confidence="0.991755">isabel.trancoso@inesc-id.pt</email>
<abstract confidence="0.997673384615384">This paper presents a semi-supervised Chinese word segmentation (CWS) approach that co-regularizes character-based and word-based models. Similarly to multi-view learning, the “segmentation agreements” between the two different types of view are used to overcome the scarcity of the label information on unlabeled data. The proposed approach trains a character-based and word-based model on labeled data, respectively, as the initial models. Then, the two models are constantly updated using unlabeled examples, where the learning objective is maximizing their segmentation agreements. The agreements are regarded as a set of valuable constraints for regularizing the learning of both models on unlabeled data. The segmentation for an input sentence is decoded by using a joint scoring function combining the two induced models. The evaluation on the Chinese tree bank reveals that our model results in better gains over the state-of-the-art semi-supervised models reported in the literature.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christopher M Bishop</author>
</authors>
<title>Pattern recognition and machine learning.</title>
<date>2006</date>
<contexts>
<context position="13592" citStr="Bishop, 2006" startWordPosition="2194" endWordPosition="2195">zu = argmax w=GEN(x) 3: if zu =� A(xu) 4: θkw = θk−1 w + φ(A(xu)) − φ(zu) 5: end for Data #Sent- #Sent- #Sent- OOV- OOVtrain dev test dev test CTB-5 18,089 350 348 0.0811 0.0347 CTB-6 23,420 2,079 2,796 0.0545 0.0557 CTB-7 31,131 10,136 10,180 0.0549 0.0521 �|w| i=1 φ(xu, wi) · θk−1 w 3.4 The Joint Score Function for Decoding There are two co-regularized models as results of the previous induction steps. An intuitive idea is that both induced models are combined to conduct the segmentation, for the sake of integrating their strengths. This paper employs a log-linear interpolation combination (Bishop, 2006) to formulate a joint scoring function based on character-based and word-based models in the decoding: Score(w) = α · log(pec(y|x)) +(1 − α) · log(φ(x, w) · θw) (5) where the two terms of the logarithm are the scores of character-based and word-based models, respectively, for a given segmentation w. This composite function uses a parameter α to weight the contributions of the two models. The α value is tuned using the development data. 4 Experiment 4.1 Setting The experimental data is taken from the Chinese tree bank (CTB). In order to make a fair comparison with the state-of-the-art results, </context>
</contexts>
<marker>Bishop, 2006</marker>
<rawString>Christopher M. Bishop. 2006. Pattern recognition and machine learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<location>Philadelphia, USA.</location>
<contexts>
<context position="6427" citStr="Collins, 2002" startWordPosition="1014" endWordPosition="1015"> searches for a next possible word. Zhang and Clark (2007) first proposed a word-based segmentation model using a discriminative Perceptrons algorithm. Given a sentence x, let us denote a possible segmented sentence as w ∈ w, and the function that enumerates a set of segmentation candidates as GEN:w = GEN(x) for x. The objective is to maximize the following problem for all sentences: ˆθw = argmax w=GEN(x) where it maps the segmented sentence w to a global feature vector φ and denotes θw as its corresponding weight parameters. The parameters θw can be estimated by using the Perceptrons method (Collins, 2002) or other online learning algorithms, e.g., Passive Aggressive (Crammer et al., 2006). For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. 2.3 Comparison Between Both Models Character-based and word-based models present different behaviors and each one has its own strengths and weakness. Sun (2010) carried out a thorough survey that includes theoretical and empirical comparisons from four aspects. Here, two critical properties of the two models supporting the co-regularization in this study are highlighted. Character-based models present better prediction ability f</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP, pages 1-8, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of ma-chine learning research,</journal>
<pages>7--551</pages>
<contexts>
<context position="6512" citStr="Crammer et al., 2006" startWordPosition="1024" endWordPosition="1027">rd-based segmentation model using a discriminative Perceptrons algorithm. Given a sentence x, let us denote a possible segmented sentence as w ∈ w, and the function that enumerates a set of segmentation candidates as GEN:w = GEN(x) for x. The objective is to maximize the following problem for all sentences: ˆθw = argmax w=GEN(x) where it maps the segmented sentence w to a global feature vector φ and denotes θw as its corresponding weight parameters. The parameters θw can be estimated by using the Perceptrons method (Collins, 2002) or other online learning algorithms, e.g., Passive Aggressive (Crammer et al., 2006). For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. 2.3 Comparison Between Both Models Character-based and word-based models present different behaviors and each one has its own strengths and weakness. Sun (2010) carried out a thorough survey that includes theoretical and empirical comparisons from four aspects. Here, two critical properties of the two models supporting the co-regularization in this study are highlighted. Character-based models present better prediction ability for new words, since they lay more emphasis on the internal structure of a word and th</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of ma-chine learning research, 7:551-585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Joao Graca</author>
<author>John Blitzer</author>
<author>Ben Taskar</author>
</authors>
<title>Multi-View Learning over Structured and Non-Identical Outputs.</title>
<date>2008</date>
<booktitle>In Proceedings of CUAI,</booktitle>
<pages>204--211</pages>
<location>Helsinki, Finland.</location>
<contexts>
<context position="2997" citStr="Ganchev et al., 2008" startWordPosition="451" endWordPosition="454">owever, few semi-supervised CWS models have been proposed. Xu et al. (2008) described a Bayesian semisupervised model by considering the segmentation as the hidden variable in machine translation. Sun and Xu (2011) enhanced the segmentation results by interpolating the statistics-based features derived from unlabeled data to a CRFs model. Another similar trial via “feature engineering” was conducted by Wang et al. (2011). The crux of solving semi-supervised learning problem is the learning on unlabeled data. Inspired by multi-view learning that exploits redundant views of the same input data (Ganchev et al., 2008), this paper proposes a semi-supervised CWS model of co-regularizing from two different views (intrinsically two different models), character-based and word-based, on unlabeled data. The motivation comes from that the two types of model exhibit different strengths and they are mutually complementary (Sun, 2010; Wang et al., 2010). The proposed approach begins by training a character-based and word-based model on labeled data respectively, and then both models are regularized from each view by their segmentation agreements, i.e., the identical outputs, of unlabeled data. This paper introduces s</context>
</contexts>
<marker>Ganchev, Graca, Blitzer, Taskar, 2008</marker>
<rawString>Kuzman Ganchev, Joao Graca, John Blitzer, and Ben Taskar. 2008. Multi-View Learning over Structured and Non-Identical Outputs. In Proceedings of CUAI, pages 204-211, Helsinki, Finland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Hatori</author>
<author>Takuya Matsuzaki</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1045--1053</pages>
<location>Jeju, Republic of</location>
<contexts>
<context position="16599" citStr="Hatori et al., 2012" startWordPosition="2712" endWordPosition="2715">e program provided by the author, on the same experimental data. The 1The “baseline” uses a different training configuration so that the α values in the decoding are also need to be tuned on the development sets. The tuned α values are {0.6, 0.6, 0.51 for CTB-5, CTB-6 and CTB-7. 174 bold scores indicate that our model does achieve significant gains over these two semi-supervised models. This outcome can further reveal that using the agreements from these two views to regularize the learning can effectively guide the model toward a better solution. The third comparison candidate is Hatori ’12 (Hatori et al., 2012) which reported the best performance in the literature on these three testing sets. It is a supervised joint model of word segmentation, POS tagging and dependency parsing. Impressively, our model still outperforms Hatori ’12 on all three datasets. Although there is only a 0.01 increase on CTB-5, it can be seen as a significant improvement when considering Hatori ’12 employs much richer training resources, i.e., sentences tagged with syntactic information. Method CTB-5 CTB-6 CTB-7 Ours 98.27 96.33 96.72 Baseline 97.58 94.71 94.87 Wang ’11 98.11 95.79 95.65 Sun ’11 98.04 95.44 95.34 Hatori ’12 </context>
</contexts>
<marker>Hatori, Matsuzaki, Miyao, Tsujii, 2012</marker>
<rawString>Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. 2012. Incremental Joint Approach to Word Segmentation, POS Tagging, and Dependency Parsing in Chinese. In Proceedings of ACL, pages 1045-1053, Jeju, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
<author>Yajuan Liu</author>
</authors>
<title>A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>897--904</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="14400" citStr="Jiang et al., 2008" startWordPosition="2331" endWordPosition="2334">he logarithm are the scores of character-based and word-based models, respectively, for a given segmentation w. This composite function uses a parameter α to weight the contributions of the two models. The α value is tuned using the development data. 4 Experiment 4.1 Setting The experimental data is taken from the Chinese tree bank (CTB). In order to make a fair comparison with the state-of-the-art results, the versions of CTB-5, CTB-6, and CTB-7 are used for the evaluation. The training, development and testing sets are defined according to the previous works. For CTB-5, the data split from (Jiang et al., 2008) is employed. For CTB-6, the same data split as recommended in the CTB-6 official document is used. For CTB-7, the datasets are formed according to the way in (Wang et al., 2011). The corresponding statistic information on these data splits is reported in Table 1. The unlabeled data in Table 1: Statistics of CTB-5, CTB-6 and CTB-7 data. 4.2 Main Results The development sets are mainly used to tune the values of the weight factor α in Equation 5. We evaluated the performance (F-score) of our model on the three development sets by using different α values, where α is progressively increased in s</context>
</contexts>
<marker>Jiang, Huang, Liu, Liu, 2008</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu, and Yajuan Liu. 2008. A Cascaded Linear Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging. In Proceedings of ACL, pages 897-904, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging - A Case Study.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>522--530</pages>
<location>Suntec, Singapore.</location>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, and Qun Liu. 2009. Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging - A Case Study. In Proceedings of ACL and the 4th IJCNLP of the AFNLP, pages 522-530, Suntec, Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Feng Jiao</author>
<author>Shaojun Wang</author>
<author>Chi-Hoon Lee</author>
</authors>
<title>Semi-supervised conditional random fields for improved sequence segmentation and labeling.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL and the 4th IJCNLP of the AFNLP,</booktitle>
<pages>209--216</pages>
<location>Strouds-burg, PA, USA.</location>
<contexts>
<context position="2130" citStr="Jiao et al., 2006" startWordPosition="312" endWordPosition="315">nese language processing tasks such as syntax parsing, information extraction and machine translation, since Chinese scripts are written in continuous characters without explicit word boundaries. Although supervised CWS models (Xue, 2003; Zhao et al., 2006; Zhang and Clark, 2007; Sun, 2011) proposed in the past years showed some reasonably accurate results, the outstanding problem is that they rely heavily on a large amount of labeled data. However, the production of segmented Chinese texts is time-consuming and expensive, since hand-labeling individual words and word boundaries is very hard (Jiao et al., 2006). So, one cannot rely only on the manually segmented data to build an everlasting model. This naturally provides motivation for using easily accessible raw texts to enhance supervised CWS models, in semisupervised approaches. In the past years, however, few semi-supervised CWS models have been proposed. Xu et al. (2008) described a Bayesian semisupervised model by considering the segmentation as the hidden variable in machine translation. Sun and Xu (2011) enhanced the segmentation results by interpolating the statistics-based features derived from unlabeled data to a CRFs model. Another simil</context>
</contexts>
<marker>Jiao, Wang, Lee, 2006</marker>
<rawString>Feng Jiao, Shaojun Wang and Chi-Hoon Lee. 2006. Semi-supervised conditional random fields for improved sequence segmentation and labeling. In Proceedings ofACL and the 4th IJCNLP of the AFNLP, pages 209-216, Strouds-burg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku Kudo</author>
</authors>
<title>CRF++: Yet another CRF toolkit. Software available at http://crfpp.sourceforge.</title>
<date>2005</date>
<pages>net.</pages>
<contexts>
<context position="12902" citStr="Kudo, 2005" startWordPosition="2072" endWordPosition="2073">rrent training example with the agreements and subtracting the global feature vector of the decoder output, as described in lines 3 and 4 of Algorithm 2. Algorithm 2 Parameter update in word-based model our experiments is from the XIN CMN portion of Chinese Gigaword 2.0. The articles published in 1991-1993 and 1999-2004 are used as unlabeled data, with 204 million words. The feature templates in (Zhao et al., 2006) and (Zhang and Clark, 2007) are used in train-ing the CRFs model and Perceptrons model, respectively. The experimental platform is implemented based on two popular toolkits: CRF++ (Kudo, 2005) and Zpar (Zhang and Clark, 2011). 1: fork= 1...K, u = 1...m do 2: calculate zu = argmax w=GEN(x) 3: if zu =� A(xu) 4: θkw = θk−1 w + φ(A(xu)) − φ(zu) 5: end for Data #Sent- #Sent- #Sent- OOV- OOVtrain dev test dev test CTB-5 18,089 350 348 0.0811 0.0347 CTB-6 23,420 2,079 2,796 0.0545 0.0557 CTB-7 31,131 10,136 10,180 0.0549 0.0521 �|w| i=1 φ(xu, wi) · θk−1 w 3.4 The Joint Score Function for Decoding There are two co-regularized models as results of the previous induction steps. An intuitive idea is that both induced models are combined to conduct the segmentation, for the sake of integrating</context>
</contexts>
<marker>Kudo, 2005</marker>
<rawString>Taku Kudo. 2005. CRF++: Yet another CRF toolkit. Software available at http://crfpp.sourceforge. net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Field: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proceedings of ICML,</booktitle>
<pages>282--289</pages>
<location>Williams College, USA.</location>
<contexts>
<context position="4720" citStr="Lafferty et al., 2001" startWordPosition="714" endWordPosition="717">Segmentation Models There are two classes of CWS models: characterbased and word-based. This section briefly reviews two supervised models in these categories, a character-based CRFs model, and a word-based Perceptrons model, which are used in our approach. 2.1 Character-based CRFs Model Character-based models treat word segmentation as a sequence labeling problem, assigning labels to the characters in a sentence indicating their positions in a word. A 4 tag-set is used in this paper: B (beginning), M (middle), E (end) and S (single character). Xue (2003) first proposed the use of CRFs model (Lafferty et al., 2001) in character-based CWS. Let x = (x1x2...x|x|) ∈ X denote a sentence, where each character and y = (y1y2... y|y|) ∈ Y denote a tag sequence, yi ∈ T being the tag assigned to xi. The goal is to achieve a label sequence with the best score in the form, 1 pθc(y|x) = Z(x; θc) exp{f(x, y) · θc} (1) where Z(x; θc) is a partition function that normalizes the exponential form to be a probability distribution, and f(x, y) are arbitrary feature functions. The aim of CRFs is to estimate the weight parameters θc that maximizes the conditional likelihood of the training data: log pθc(yi|xi) − γkθck22 (2) w</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Field: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proceedings of ICML, pages 282-289, Williams College, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>Word-based and character-based word segmentation models: comparison and combination.</title>
<date>2001</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1211--1219</pages>
<location>Bejing, China.</location>
<marker>Sun, 2001</marker>
<rawString>Weiwei Sun. 2001. Word-based and character-based word segmentation models: comparison and combination. In Proceedings of COLING, pages 1211-1219, Bejing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
</authors>
<title>A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>1385--1394</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="1803" citStr="Sun, 2011" startWordPosition="260" endWordPosition="261">duced models. The evaluation on the Chinese tree bank reveals that our model results in better gains over the state-of-the-art semi-supervised models reported in the literature. 1 Introduction Chinese word segmentation (CWS) is a critical and a necessary initial procedure with respect to the majority of high-level Chinese language processing tasks such as syntax parsing, information extraction and machine translation, since Chinese scripts are written in continuous characters without explicit word boundaries. Although supervised CWS models (Xue, 2003; Zhao et al., 2006; Zhang and Clark, 2007; Sun, 2011) proposed in the past years showed some reasonably accurate results, the outstanding problem is that they rely heavily on a large amount of labeled data. However, the production of segmented Chinese texts is time-consuming and expensive, since hand-labeling individual words and word boundaries is very hard (Jiao et al., 2006). So, one cannot rely only on the manually segmented data to build an everlasting model. This naturally provides motivation for using easily accessible raw texts to enhance supervised CWS models, in semisupervised approaches. In the past years, however, few semi-supervised</context>
</contexts>
<marker>Sun, 2011</marker>
<rawString>Weiwei Sun. 2011. A stacked sub-word model for joint Chinese word segmentation and part-of-speech tagging. In Proceedings of ACL, pages 1385-1394, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Jia Xu</author>
</authors>
<title>Enhancing Chinese word segmentation using unlabeled data.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>970--979</pages>
<location>Scotland, UK.</location>
<contexts>
<context position="2590" citStr="Sun and Xu (2011)" startWordPosition="386" endWordPosition="389">duction of segmented Chinese texts is time-consuming and expensive, since hand-labeling individual words and word boundaries is very hard (Jiao et al., 2006). So, one cannot rely only on the manually segmented data to build an everlasting model. This naturally provides motivation for using easily accessible raw texts to enhance supervised CWS models, in semisupervised approaches. In the past years, however, few semi-supervised CWS models have been proposed. Xu et al. (2008) described a Bayesian semisupervised model by considering the segmentation as the hidden variable in machine translation. Sun and Xu (2011) enhanced the segmentation results by interpolating the statistics-based features derived from unlabeled data to a CRFs model. Another similar trial via “feature engineering” was conducted by Wang et al. (2011). The crux of solving semi-supervised learning problem is the learning on unlabeled data. Inspired by multi-view learning that exploits redundant views of the same input data (Ganchev et al., 2008), this paper proposes a semi-supervised CWS model of co-regularizing from two different views (intrinsically two different models), character-based and word-based, on unlabeled data. The motiva</context>
<context position="15854" citStr="Sun and Xu, 2011" startWordPosition="2583" endWordPosition="2587">ows the F-score results of word segmentation on CTB-5, CTB-6 and CTB-7 testing sets. The line of “ours” reports the performance of our semi-supervised model with the tuned parameters. We first compare it with the supervised “baseline” method which joints character-based and word-based model trained only on the training set1. It can be observed that our semi-supervised model is able to benefit from unlabeled data and greatly improves the results over the supervised baseline. We also compare our model with two state-of-the-art semi-supervised methods of Wang ’11 (Wang et al., 2011) and Sun ’11 (Sun and Xu, 2011). The performance scores of Wang ’11 are directly taken from their paper, while the results of Sun ’11 are obtained, using the program provided by the author, on the same experimental data. The 1The “baseline” uses a different training configuration so that the α values in the decoding are also need to be tuned on the development sets. The tuned α values are {0.6, 0.6, 0.51 for CTB-5, CTB-6 and CTB-7. 174 bold scores indicate that our model does achieve significant gains over these two semi-supervised models. This outcome can further reveal that using the agreements from these two views to reg</context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Weiwei Sun and Jia Xu. 2011. Enhancing Chinese word segmentation using unlabeled data. In Proceedings of EMNLP, pages 970-979, Scotland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging.</title>
<date>2013</date>
<booktitle>In Transactions of the Association for Computational Linguistics,</booktitle>
<pages>1--1</pages>
<marker>T¨ackstr¨om, Das, Petrov, McDonald, Nivre, 2013</marker>
<rawString>Oscar T¨ackstr¨om, Dipanjan Das, Slav Petrov, Ryan McDonald, and Joakim Nivre. 2013. Token and Type Constraints for Cross-Lingual Part-of-Speech Tagging. In Transactions of the Association for Computational Linguistics, 1:1-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kun Wang</author>
<author>Chengqing Zong</author>
<author>Keh-Yih Su</author>
</authors>
<title>A Character-Based Joint Model for Chinese Word Segmentation.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1173--1181</pages>
<location>Bejing, China.</location>
<contexts>
<context position="3328" citStr="Wang et al., 2010" startWordPosition="500" endWordPosition="503">model. Another similar trial via “feature engineering” was conducted by Wang et al. (2011). The crux of solving semi-supervised learning problem is the learning on unlabeled data. Inspired by multi-view learning that exploits redundant views of the same input data (Ganchev et al., 2008), this paper proposes a semi-supervised CWS model of co-regularizing from two different views (intrinsically two different models), character-based and word-based, on unlabeled data. The motivation comes from that the two types of model exhibit different strengths and they are mutually complementary (Sun, 2010; Wang et al., 2010). The proposed approach begins by training a character-based and word-based model on labeled data respectively, and then both models are regularized from each view by their segmentation agreements, i.e., the identical outputs, of unlabeled data. This paper introduces segmentation agreements as gainful knowledge for guiding the learning on the texts without label information. Moreover, in order to better combine the strengths of the two models, the proposed approach uses a joint scoring function in a log-linear combination form for the decoding in the segmentation phase. 171 Proceedings of the </context>
</contexts>
<marker>Wang, Zong, Su, 2010</marker>
<rawString>Kun Wang, Chengqing Zong, and Keh-Yih Su. 2010. A Character-Based Joint Model for Chinese Word Segmentation. In Proceedings of COLING, pages 1173-1181, Bejing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiou Wang</author>
<author>Jun’ichi Kazama</author>
<author>Yoshimasa Tsuruoka</author>
<author>Wenliang Chen</author>
<author>Yujie Zhang</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Improving Chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data.</title>
<date>2011</date>
<booktitle>In Proceedings of IJCNLP,</booktitle>
<pages>309--317</pages>
<location>Hyderabad, India.</location>
<contexts>
<context position="2800" citStr="Wang et al. (2011)" startWordPosition="419" endWordPosition="422"> data to build an everlasting model. This naturally provides motivation for using easily accessible raw texts to enhance supervised CWS models, in semisupervised approaches. In the past years, however, few semi-supervised CWS models have been proposed. Xu et al. (2008) described a Bayesian semisupervised model by considering the segmentation as the hidden variable in machine translation. Sun and Xu (2011) enhanced the segmentation results by interpolating the statistics-based features derived from unlabeled data to a CRFs model. Another similar trial via “feature engineering” was conducted by Wang et al. (2011). The crux of solving semi-supervised learning problem is the learning on unlabeled data. Inspired by multi-view learning that exploits redundant views of the same input data (Ganchev et al., 2008), this paper proposes a semi-supervised CWS model of co-regularizing from two different views (intrinsically two different models), character-based and word-based, on unlabeled data. The motivation comes from that the two types of model exhibit different strengths and they are mutually complementary (Sun, 2010; Wang et al., 2010). The proposed approach begins by training a character-based and word-ba</context>
<context position="14578" citStr="Wang et al., 2011" startWordPosition="2364" endWordPosition="2367">ons of the two models. The α value is tuned using the development data. 4 Experiment 4.1 Setting The experimental data is taken from the Chinese tree bank (CTB). In order to make a fair comparison with the state-of-the-art results, the versions of CTB-5, CTB-6, and CTB-7 are used for the evaluation. The training, development and testing sets are defined according to the previous works. For CTB-5, the data split from (Jiang et al., 2008) is employed. For CTB-6, the same data split as recommended in the CTB-6 official document is used. For CTB-7, the datasets are formed according to the way in (Wang et al., 2011). The corresponding statistic information on these data splits is reported in Table 1. The unlabeled data in Table 1: Statistics of CTB-5, CTB-6 and CTB-7 data. 4.2 Main Results The development sets are mainly used to tune the values of the weight factor α in Equation 5. We evaluated the performance (F-score) of our model on the three development sets by using different α values, where α is progressively increased in steps of 0.1 (0 &lt; α &lt; 1.0). The best performed settings of α for CTB-5, CTB-6 and CTB-7 on development data are 0.7, 0.6 and 0.6, respectively. With the chosen parameters, the tes</context>
<context position="15823" citStr="Wang et al., 2011" startWordPosition="2576" endWordPosition="2579">he final performance. Table 2 shows the F-score results of word segmentation on CTB-5, CTB-6 and CTB-7 testing sets. The line of “ours” reports the performance of our semi-supervised model with the tuned parameters. We first compare it with the supervised “baseline” method which joints character-based and word-based model trained only on the training set1. It can be observed that our semi-supervised model is able to benefit from unlabeled data and greatly improves the results over the supervised baseline. We also compare our model with two state-of-the-art semi-supervised methods of Wang ’11 (Wang et al., 2011) and Sun ’11 (Sun and Xu, 2011). The performance scores of Wang ’11 are directly taken from their paper, while the results of Sun ’11 are obtained, using the program provided by the author, on the same experimental data. The 1The “baseline” uses a different training configuration so that the α values in the decoding are also need to be tuned on the development sets. The tuned α values are {0.6, 0.6, 0.51 for CTB-5, CTB-6 and CTB-7. 174 bold scores indicate that our model does achieve significant gains over these two semi-supervised models. This outcome can further reveal that using the agreeme</context>
</contexts>
<marker>Wang, Kazama, Tsuruoka, Chen, Zhang, Torisawa, 2011</marker>
<rawString>Yiou Wang, Jun’ichi Kazama, Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang, and Kentaro Torisawa. 2011. Improving Chinese word segmentation and POS tagging with semi-supervised methods using large auto-analyzed data. In Proceedings of IJCNLP, pages 309-317, Hyderabad, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jia Xu</author>
<author>Jianfeng Gao</author>
<author>Kristina Toutanova</author>
<author>Hermann Ney</author>
</authors>
<title>Bayesian semi-supervised chinese word segmentation for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1017--1024</pages>
<location>Manchester, UK.</location>
<contexts>
<context position="2451" citStr="Xu et al. (2008)" startWordPosition="365" endWordPosition="368">ed some reasonably accurate results, the outstanding problem is that they rely heavily on a large amount of labeled data. However, the production of segmented Chinese texts is time-consuming and expensive, since hand-labeling individual words and word boundaries is very hard (Jiao et al., 2006). So, one cannot rely only on the manually segmented data to build an everlasting model. This naturally provides motivation for using easily accessible raw texts to enhance supervised CWS models, in semisupervised approaches. In the past years, however, few semi-supervised CWS models have been proposed. Xu et al. (2008) described a Bayesian semisupervised model by considering the segmentation as the hidden variable in machine translation. Sun and Xu (2011) enhanced the segmentation results by interpolating the statistics-based features derived from unlabeled data to a CRFs model. Another similar trial via “feature engineering” was conducted by Wang et al. (2011). The crux of solving semi-supervised learning problem is the learning on unlabeled data. Inspired by multi-view learning that exploits redundant views of the same input data (Ganchev et al., 2008), this paper proposes a semi-supervised CWS model of c</context>
</contexts>
<marker>Xu, Gao, Toutanova, Ney, 2008</marker>
<rawString>Jia Xu, Jianfeng Gao, Kristina Toutanova and Hermann Ney. 2008. Bayesian semi-supervised chinese word segmentation for statistical machine translation. In Proceedings of COLING, pages 1017-1024, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<pages>8--1</pages>
<contexts>
<context position="1749" citStr="Xue, 2003" startWordPosition="250" endWordPosition="251">y using a joint scoring function combining the two induced models. The evaluation on the Chinese tree bank reveals that our model results in better gains over the state-of-the-art semi-supervised models reported in the literature. 1 Introduction Chinese word segmentation (CWS) is a critical and a necessary initial procedure with respect to the majority of high-level Chinese language processing tasks such as syntax parsing, information extraction and machine translation, since Chinese scripts are written in continuous characters without explicit word boundaries. Although supervised CWS models (Xue, 2003; Zhao et al., 2006; Zhang and Clark, 2007; Sun, 2011) proposed in the past years showed some reasonably accurate results, the outstanding problem is that they rely heavily on a large amount of labeled data. However, the production of segmented Chinese texts is time-consuming and expensive, since hand-labeling individual words and word boundaries is very hard (Jiao et al., 2006). So, one cannot rely only on the manually segmented data to build an everlasting model. This naturally provides motivation for using easily accessible raw texts to enhance supervised CWS models, in semisupervised appro</context>
<context position="4659" citStr="Xue (2003)" startWordPosition="705" endWordPosition="706">2013 Association for Computational Linguistics 2 Segmentation Models There are two classes of CWS models: characterbased and word-based. This section briefly reviews two supervised models in these categories, a character-based CRFs model, and a word-based Perceptrons model, which are used in our approach. 2.1 Character-based CRFs Model Character-based models treat word segmentation as a sequence labeling problem, assigning labels to the characters in a sentence indicating their positions in a word. A 4 tag-set is used in this paper: B (beginning), M (middle), E (end) and S (single character). Xue (2003) first proposed the use of CRFs model (Lafferty et al., 2001) in character-based CWS. Let x = (x1x2...x|x|) ∈ X denote a sentence, where each character and y = (y1y2... y|y|) ∈ Y denote a tag sequence, yi ∈ T being the tag assigned to xi. The goal is to achieve a label sequence with the best score in the form, 1 pθc(y|x) = Z(x; θc) exp{f(x, y) · θc} (1) where Z(x; θc) is a partition function that normalizes the exponential form to be a probability distribution, and f(x, y) are arbitrary feature functions. The aim of CRFs is to estimate the weight parameters θc that maximizes the conditional li</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Nianwen Xue. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation using a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>840--847</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1791" citStr="Zhang and Clark, 2007" startWordPosition="256" endWordPosition="259">on combining the two induced models. The evaluation on the Chinese tree bank reveals that our model results in better gains over the state-of-the-art semi-supervised models reported in the literature. 1 Introduction Chinese word segmentation (CWS) is a critical and a necessary initial procedure with respect to the majority of high-level Chinese language processing tasks such as syntax parsing, information extraction and machine translation, since Chinese scripts are written in continuous characters without explicit word boundaries. Although supervised CWS models (Xue, 2003; Zhao et al., 2006; Zhang and Clark, 2007; Sun, 2011) proposed in the past years showed some reasonably accurate results, the outstanding problem is that they rely heavily on a large amount of labeled data. However, the production of segmented Chinese texts is time-consuming and expensive, since hand-labeling individual words and word boundaries is very hard (Jiao et al., 2006). So, one cannot rely only on the manually segmented data to build an everlasting model. This naturally provides motivation for using easily accessible raw texts to enhance supervised CWS models, in semisupervised approaches. In the past years, however, few sem</context>
<context position="5871" citStr="Zhang and Clark (2007)" startWordPosition="915" endWordPosition="918">itional likelihood of the training data: log pθc(yi|xi) − γkθck22 (2) where γkθck22is a regularizer on parameters to limit overfitting on rare features and avoid degeneracy in the case of correlated features. In this paper, this objective function is optimized by stochastic gradient method. For the decoding, the Viterbi algorithm is employed. 2.2 Word-based Perceptrons Model Word-based models read a input sentence from left to right and predict whether the current piece of continuous characters is a word. After one word is identified, the method moves on and searches for a next possible word. Zhang and Clark (2007) first proposed a word-based segmentation model using a discriminative Perceptrons algorithm. Given a sentence x, let us denote a possible segmented sentence as w ∈ w, and the function that enumerates a set of segmentation candidates as GEN:w = GEN(x) for x. The objective is to maximize the following problem for all sentences: ˆθw = argmax w=GEN(x) where it maps the segmented sentence w to a global feature vector φ and denotes θw as its corresponding weight parameters. The parameters θw can be estimated by using the Perceptrons method (Collins, 2002) or other online learning algorithms, e.g., </context>
<context position="12737" citStr="Zhang and Clark, 2007" startWordPosition="2044" endWordPosition="2047">ntation z,,. If the words in output z,, do not match the agreements A(x,,) of the current sentence x,,, the parameters are updated by adding the global feature vector of the current training example with the agreements and subtracting the global feature vector of the decoder output, as described in lines 3 and 4 of Algorithm 2. Algorithm 2 Parameter update in word-based model our experiments is from the XIN CMN portion of Chinese Gigaword 2.0. The articles published in 1991-1993 and 1999-2004 are used as unlabeled data, with 204 million words. The feature templates in (Zhao et al., 2006) and (Zhang and Clark, 2007) are used in train-ing the CRFs model and Perceptrons model, respectively. The experimental platform is implemented based on two popular toolkits: CRF++ (Kudo, 2005) and Zpar (Zhang and Clark, 2011). 1: fork= 1...K, u = 1...m do 2: calculate zu = argmax w=GEN(x) 3: if zu =� A(xu) 4: θkw = θk−1 w + φ(A(xu)) − φ(zu) 5: end for Data #Sent- #Sent- #Sent- OOV- OOVtrain dev test dev test CTB-5 18,089 350 348 0.0811 0.0347 CTB-6 23,420 2,079 2,796 0.0545 0.0557 CTB-7 31,131 10,136 10,180 0.0549 0.0521 �|w| i=1 φ(xu, wi) · θk−1 w 3.4 The Joint Score Function for Decoding There are two co-regularized m</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang and Stephen Clark. 2007. Chinese segmentation using a word-based perceptron algorithm. In Proceedings of ACL, pages 840-847, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and POS-tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>843--852</pages>
<location>Massachusetts, USA.</location>
<marker>Zhang, Clark, 2010</marker>
<rawString>Yue Zhang and Stephen Clark. 2010. A fast decoder for joint word segmentation and POS-tagging using a single discriminative model. In Proceedings of EMNLP, pages 843-852, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<pages>37--1</pages>
<contexts>
<context position="12935" citStr="Zhang and Clark, 2011" startWordPosition="2076" endWordPosition="2079"> with the agreements and subtracting the global feature vector of the decoder output, as described in lines 3 and 4 of Algorithm 2. Algorithm 2 Parameter update in word-based model our experiments is from the XIN CMN portion of Chinese Gigaword 2.0. The articles published in 1991-1993 and 1999-2004 are used as unlabeled data, with 204 million words. The feature templates in (Zhao et al., 2006) and (Zhang and Clark, 2007) are used in train-ing the CRFs model and Perceptrons model, respectively. The experimental platform is implemented based on two popular toolkits: CRF++ (Kudo, 2005) and Zpar (Zhang and Clark, 2011). 1: fork= 1...K, u = 1...m do 2: calculate zu = argmax w=GEN(x) 3: if zu =� A(xu) 4: θkw = θk−1 w + φ(A(xu)) − φ(zu) 5: end for Data #Sent- #Sent- #Sent- OOV- OOVtrain dev test dev test CTB-5 18,089 350 348 0.0811 0.0347 CTB-6 23,420 2,079 2,796 0.0545 0.0557 CTB-7 31,131 10,136 10,180 0.0549 0.0521 �|w| i=1 φ(xu, wi) · θk−1 w 3.4 The Joint Score Function for Decoding There are two co-regularized models as results of the previous induction steps. An intuitive idea is that both induced models are combined to conduct the segmentation, for the sake of integrating their strengths. This paper empl</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105-151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
<author>Bao-Liang Lu</author>
</authors>
<title>Effective tag set selection in Chinese word segmentation via conditional random field modeling.</title>
<date>2006</date>
<booktitle>In Proceedings of PACLIC,</booktitle>
<pages>87--94</pages>
<location>Wuhan, China.</location>
<contexts>
<context position="1768" citStr="Zhao et al., 2006" startWordPosition="252" endWordPosition="255">oint scoring function combining the two induced models. The evaluation on the Chinese tree bank reveals that our model results in better gains over the state-of-the-art semi-supervised models reported in the literature. 1 Introduction Chinese word segmentation (CWS) is a critical and a necessary initial procedure with respect to the majority of high-level Chinese language processing tasks such as syntax parsing, information extraction and machine translation, since Chinese scripts are written in continuous characters without explicit word boundaries. Although supervised CWS models (Xue, 2003; Zhao et al., 2006; Zhang and Clark, 2007; Sun, 2011) proposed in the past years showed some reasonably accurate results, the outstanding problem is that they rely heavily on a large amount of labeled data. However, the production of segmented Chinese texts is time-consuming and expensive, since hand-labeling individual words and word boundaries is very hard (Jiao et al., 2006). So, one cannot rely only on the manually segmented data to build an everlasting model. This naturally provides motivation for using easily accessible raw texts to enhance supervised CWS models, in semisupervised approaches. In the past </context>
<context position="12709" citStr="Zhao et al., 2006" startWordPosition="2039" endWordPosition="2042">rrent model into a segmentation z,,. If the words in output z,, do not match the agreements A(x,,) of the current sentence x,,, the parameters are updated by adding the global feature vector of the current training example with the agreements and subtracting the global feature vector of the decoder output, as described in lines 3 and 4 of Algorithm 2. Algorithm 2 Parameter update in word-based model our experiments is from the XIN CMN portion of Chinese Gigaword 2.0. The articles published in 1991-1993 and 1999-2004 are used as unlabeled data, with 204 million words. The feature templates in (Zhao et al., 2006) and (Zhang and Clark, 2007) are used in train-ing the CRFs model and Perceptrons model, respectively. The experimental platform is implemented based on two popular toolkits: CRF++ (Kudo, 2005) and Zpar (Zhang and Clark, 2011). 1: fork= 1...K, u = 1...m do 2: calculate zu = argmax w=GEN(x) 3: if zu =� A(xu) 4: θkw = θk−1 w + φ(A(xu)) − φ(zu) 5: end for Data #Sent- #Sent- #Sent- OOV- OOVtrain dev test dev test CTB-5 18,089 350 348 0.0811 0.0347 CTB-6 23,420 2,079 2,796 0.0545 0.0557 CTB-7 31,131 10,136 10,180 0.0549 0.0521 �|w| i=1 φ(xu, wi) · θk−1 w 3.4 The Joint Score Function for Decoding Th</context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2006</marker>
<rawString>Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-Liang Lu. 2006. Effective tag set selection in Chinese word segmentation via conditional random field modeling. In Proceedings of PACLIC, pages 87-94, Wuhan, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ciyou Zhu</author>
<author>Richard H Byrd</author>
<author>Peihuang Lu</author>
<author>Jorge Nocedal</author>
</authors>
<title>L-BFGS-B: Fortran subroutines for large scale bound constrained optimization.</title>
<date>2006</date>
<journal>ACM Transactions on Mathematical Software,</journal>
<pages>23--550</pages>
<marker>Zhu, Byrd, Lu, Nocedal, 2006</marker>
<rawString>Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge Nocedal. 2006. L-BFGS-B: Fortran subroutines for large scale bound constrained optimization. ACM Transactions on Mathematical Software, 23:550-560.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>