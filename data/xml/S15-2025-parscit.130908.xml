<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001227">
<title confidence="0.903846">
MathLingBudapest: Concept Networks for Semantic Similarity
</title>
<note confidence="0.7849412">
G´abor Recski
Research Institute for Linguistics
Hungarian Academy of Sciences
Bencz´ur u. 33
1068 Budapest, Hungary
</note>
<email confidence="0.979805">
recski@mokk.bme.hu
</email>
<author confidence="0.964961">
Judit ´Acs
</author>
<affiliation confidence="0.959683">
HAS Research Institute for Linguistics and
Dept. of Automation and Applied Informatics
Budapest U of Technology and Economics
</affiliation>
<address confidence="0.942547">
Magyar tud´osok krt. 2 (Bldg. Q.)
1117 Budapest, Hungary
</address>
<email confidence="0.997548">
judit@mokk.bme.hu
</email>
<sectionHeader confidence="0.995607" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933466666667">
We present our approach to measuring seman-
tic similarity of sentence pairs used in Se-
meval 2015 tasks 1 and 2. We adopt the
sentence alignment framework of (Han et al.,
2013) and experiment with several measures
of word similarity. We hybridize the common
vector-based models with definition graphs
from the 4lang concept dictionary and de-
vise a measure of graph similarity that yields
good results on training data. We did not ad-
dress the specific challenges posed by Twitter
data, and this is reflected in placing 11th from
30 in Task 1, but our systems perform fairly
well on the generic datasets of Task 2, with the
hybrid approach placing 11th among 78 runs.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999757269230769">
This paper describes the systems participating in
Semeval-2015 Task 1 (Xu et al., 2015) and Task 2
(Agirre et al., 2015). To compute the semantic sim-
ilarity of two sentences we use the architecture pre-
sented in (Han et al., 2013) to find, for each word, its
counterpart in the other sentence that is semantically
most similar to it. We implemented several meth-
ods for measuring word similarity, among them (i)
a word embedding created by the method presented
in (Mikolov et al., 2013) and (ii) a metric based on
networks of concepts derived from the 4lang con-
cept lexicon (Kornai and Makrai, 2013; Kornai et
al., 2015) and definitions from the Longman Dic-
tionary of Contemporary English (Bullon, 2003). A
hybrid system exploiting both of these metrics yields
the best results and placed 11th among 73 systems
on Semeval Task 2a (Semantic Textual Similarity
for English). All components of our system are
available for download under an MIT license from
GitHub12. Section 2 describes the system architec-
ture and points out the main differences between our
system and that in (Han et al., 2013), section 3 out-
lines our word similarity metric derived from the
4lang concept lexicon. We present the evaluation
of our systems on both tasks in section 4, and section
5 provides a brief conclusion.
</bodyText>
<sectionHeader confidence="0.963491" genericHeader="introduction">
2 Architecture
</sectionHeader>
<bodyText confidence="0.999973210526316">
Our framework for determining the semantic simi-
larity of two sentences is based on the system pre-
sented in (Han et al., 2013). Their architecture,
Align and Penalize, involves computing an align-
ment score between two sentences based on some
measure of word similarity. We’ve chosen to reim-
plement this system so that we can experiment with
various notions of word similarity, including the
one based on 4lang and presented in section 3.
Although we reimplemented virtually all rules and
components described by (Han et al., 2013) for
experimentation, we shall only describe those that
ended up in at least one of the five configurations
submitted to Semeval.
The core idea behind the Align and Penalize archi-
tecture is, given two senteces S1 and S2 and some
measure of word similarity, to align each word of
one sentence with some word of the other sentence
so that the similarity of word pairs is maximized.
</bodyText>
<footnote confidence="0.999976">
1http://github.com/juditacs/semeval
2http://github.com/kornai/pymachine
</footnote>
<page confidence="0.886266">
138
</page>
<bodyText confidence="0.974489551020408">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 138–142,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
The mapping need not be one-to-one and is calcu-
lated independently for words of S1 (aligning them
with words from S2) and words of S2 (aligning them
with words from S1). The score of an alignment is
the sum of the similarities of each word pair in the
alignment, normalized by sentence length. The fi-
nal score assigned to a pair of sentences is the aver-
age of the alignment scores for each sentence. For
out-of-vocabulary (OOV) words, i.e. those that are
not covered by the component used for measuring
word similarity, we use the Dice-similarity over the
sets of character 4-grams in each word. Addition-
ally, we use simple rules to detect acronyms and
compounds: if a word of one sentence that is a se-
quence of 2-5 characters (e.g. ABC) has a matching
sequence of words in the other sentence (e.g. Ameri-
can Broadcasting Company), all words of the phrase
are aligned with this word and recieve an alignment
score of 1. If a sentence contains a sequence of two
words (e.g. long term or can not) that appear in the
other sentence without a space and with or without
a hyphen (e.g. long-term or cannot), these are also
aligned with a score of 1. The score returned by the
word similarity component can be boosted based on
WordNet (Miller, 1995), e.g. if one is a hypernym
of the other, if one appears frequently in glosses of
the other, or if they are derivationally related. For
the exact cases covered and a description of how
the boost is calculated, the reader is referred to (Han
et al., 2013). In our submissions we only used this
boost on word similarity scores obtained from word
embeddings.
The similarity score may be reduced by a vari-
ety of penalties, which we only enabled for Task
1 runs – they haven’t brought any improvement on
Task 2 datasets in any of our early experiments. Of
the penalties described in (Han et al., 2013) we only
used the one which decreases alignment scores if
the word similarity score for some word pair is very
small (&lt; 0.05). We also introduced two new types
of penalties based on our observations of error types
in Twitter data: if one sentence starts with a question
word and the other one does not or if one sentence
contains a past-tense verb and the other does not,
we reduce the overall score by 1/(L(S1) + L(S2)),
where L(S1) and L(S2) are the numbers of words in
each sentence.
</bodyText>
<sectionHeader confidence="0.964941" genericHeader="method">
3 Similarity from Concept Networks
</sectionHeader>
<bodyText confidence="0.999309238095238">
This section will present the word similarity mea-
sure based on principles of lexical semantics pre-
sented in (Kornai, 2010). The 4lang concept dic-
tionary (Kornai and Makrai, 2013) contains 3500
definitions created manually. Because the Longman
Defining Vocabulary (LDV) (Boguraev and Briscoe,
1989) is a subset of 4lang, we could automat-
ically extend this manually created seed to every
headword of the Longman Dictionary of Contem-
porary English (LDOCE) by processing their defi-
nitions with the Stanford Dependency Parser (Klein
and Manning, 2003), and mapping dependency re-
lations to sets of edges in the 4lang-style concept
graph. Details of the mapping will be described else-
where (Recski, 2015).
Since these definitions are essentially graphs of
concepts, we have experimented with similarity
functions over pairs of such graphs that capture se-
mantic similarity of the concepts defined by each of
them. There are two fundamentally different config-
urations present in 4lang graphs:
</bodyText>
<listItem confidence="0.7185025">
1. two nodes may be connected via a 0-edge,
which is a generalization over unary predi-
</listItem>
<equation confidence="0.6996285">
cation (dog →− bark), attribution (dog
0 →−0
</equation>
<bodyText confidence="0.858024384615385">
faithful), and hypernymy, or the IS A re-
lation (dog →− mammal).
0
2. two nodes can be connected, via a 1-edge and a
2-edge respectively, to a third one representing
a binary relation. Binaries include all transitive
verbs (e.g. cat ←−1 CATCH →−2 branch). and
a handful of binary primitives (e.g. tree ←−1
HAS →−2 branch).
We start by the intuition that similar con-
cepts will overlap in the elementary configura-
tions they take part in: they might share a 0-
neighbor, e.g. train →− vehicle
</bodyText>
<equation confidence="0.9606">
0 ←− car,
0
</equation>
<bodyText confidence="0.96377">
or they might be on the same path of 1- and
2-edges, e.g. park ←−1 IN →−2 town and
</bodyText>
<page confidence="0.427242">
2
</page>
<bodyText confidence="0.968396">
→− town.
We’ll define the predicates of a node as
the set of such configurations it takes part
in. For example, based on the definition
graph in Figure 1, the predicates of the concept
street ←− 1 IN
</bodyText>
<page confidence="0.99413">
139
</page>
<figureCaption confidence="0.999887">
Figure 1: 4lang definition of bird.
</figureCaption>
<bodyText confidence="0.94622">
bird are {vertebrate; (HAS, feather);
(HAS, wing); (MAKE, egg)j.
Our initial version of graph similarity is the Jac-
card similarity of the sets of predicates of each con-
cept, i.e.
</bodyText>
<equation confidence="0.998989">
S(w1, w2) = |P(w1) n P(w2)|
|P(w1) U P(w2)|
</equation>
<bodyText confidence="0.99959824137931">
For all words that are not among the 3500 de-
fined in 4lang we obtain definition graphs by au-
tomated parsing of Longman definitions and the ap-
plication of a simple mapping from dependency re-
lations to graph edges (Recski, 2015). By far the
largest source of noise in these graphs is that cur-
rently there is no postprocessor component that rec-
ognizes common structures of dictionary definitions
like appositive relative clauses. For example the
word casualty is defined by LDOCE as someone
who is hurt or killed in an accident or war and we
currently build the graph in Figure 2 instead of that
in Figure 3. To mitigate the effects of these anomal-
ities, we updated our definition of predicates: we let
them be “inherited” via paths of 0-edges encoding
the IS A-relationship.
We’ve also experimented with similarity mea-
sures that take into account the sets of all nodes ac-
cessible from each concept in their respective def-
inition graphs. This proved useful in establishing
that two concepts which would otherwise be treated
as entirely dissimilar are in fact somewhat related.
For example, given the definitions of the concepts
casualty and army in Figures 2 and 4, the node
war will allow us to assign nonzero similarity to the
pair (army, casualty). We found it most ef-
fective to use the maximum of these two types of
similarity.
Testing several versions of graph similarity on
</bodyText>
<figureCaption confidence="0.999985">
Figure 2: Definition of casualty built from LDOCE.
Figure 3: Expected definition of casualty.
Figure 4: Definition of army in 4lang.
</figureCaption>
<page confidence="0.990667">
140
</page>
<bodyText confidence="0.999768">
past years’ STS data, we found that if two words
w1 and w2 are connected by a path of 0-edges, it is
best to treat them as synonymous, i.e. assign to them
a similarity of 1. This proved very efficient for de-
termining semantic similarity of the most common
types of sentence pairs in the Semeval datasets. Two
descriptions of the same event (common in the head-
lines dataset) or the same picture (in images) will of-
ten only differ in their choice of words or choice of
concreteness. In a dataset from 2014, for example,
two descriptions, likely of the same picture, are A
bird holding on to a metal gate and A multi-colored
bird clings to a wire fence. Similarly, a pair of news
headlines are Piers Morgan questioned by police
and Piers Morgan Interviewed by Police. Although
wire is by no means a synonym for metal, nor does
being questioned mean exactly the same as being in-
terviewed, treating them as perfect synonyms proved
to be an efficient strategy when trying to assign sen-
tence similarity scores that correlate highly with hu-
man annotators’ judgements.
</bodyText>
<sectionHeader confidence="0.99026" genericHeader="method">
4 Submissions
</sectionHeader>
<bodyText confidence="0.998776636363636">
We shall now describe the particular configurations
used for each submission in Semeval. For Task 1
(Paraphrase and Semantic Similarity in Twitter) we
ran two systems: twitter-embed uses a single
source of word similarity, a word embedding built
from a corpus of word 6-grams from the Rovereto
Twitter N-Gram Corpus3 using the gensim4 pack-
age’s implementation of the method presented in
(Mikolov et al., 2013). Our second submission,
twitter-mash combines several sources of word
similarity by averaging the output of various systems
using weights that have been learned using plain
least squares regression on the training data avail-
able. The systems participating in the vote differ
in the word similarity measure they use: one sub-
set uses the character ngram baseline described in
section 2 with various parameters (n = 2, 3, 4, each
with Jaccard- and Dice-similarity), two systems use
word embeddings (built from 5-grams and 6-grams
of the Rovereto corpus, respectively) and one uses
the 4lang-based word similarity described in sec-
tion 3.
</bodyText>
<table confidence="0.998989571428571">
embedding hybrid
Task 1a: Paraphrase Identification
Precision 0.454 0.364
Recall 0.594 0.880
F-score 0.515 0.515
Task 1b: Semantic Similarity
Pearson 0.229 0.511
</table>
<tableCaption confidence="0.997774">
Table 1: Performance of submitted systems on Task 1.
</tableCaption>
<table confidence="0.999390375">
embedding machine hybrid
Task 2a: Semantic Similarity
answers-forums 0.704 0.698 0.723
answers-students 0.700 0.746 0.751
belief 0.733 0.736 0.747
headlines 0.769 0.805 0.804
images 0.804 0.841 0.844
mean Pearson 0.748 0.777 0.784
</table>
<tableCaption confidence="0.999658">
Table 2: Performance of submitted systems on Task 2.
</tableCaption>
<bodyText confidence="0.999551407407407">
For Task 2 (Semantic Textual Similarity) we were
allowed three submissions. The embedding sys-
tem uses a word embedding built from the first
1 billion words of the English Wikipedia using
the word2vec5 tool (Mikolov et al., 2013). The
machine system uses the word similarity measure
described in section 3 (both systems use the charac-
ter ngram baseline as a fallback for OOVs). Finally,
for the hybrid submission we used a weighted sum
of these two systems and the character ngram base-
line (weights were once again obtained using sim-
ple least square regression on the available training
data). In both hybrid submissions we trained on a
single dataset consisting of all training data avail-
able, we haven’t experimented with genre-specific
models.
Our results on each task are presented in Tables 1
and 2. In case of Task 1a (Paraphrase Identification)
our two systems performed equally in terms of F-
score and ranked 30th among 38 systems. On Task
1b the hybrid system performed considerably better
than the purely vector-based run, placing 11th out of
28 runs. On Task 2 our hybrid system ranked 11th
among 78 systems, the systems using the word em-
bedding and the 4lang-based similarity alone (with
string similarity as a fallback for OOVs in each case)
ranked 22nd and 15th, respectively.
</bodyText>
<footnote confidence="0.9999125">
3http://clic.cimec.unitn.it/amac/twitter ngram/
4http://radimrehurek.com/gensim 5https://code.google.com/p/word2vec/
</footnote>
<page confidence="0.996863">
141
</page>
<sectionHeader confidence="0.998572" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999936357142857">
In a framework like (Han et al., 2013) which ap-
proximates sentence similarity by word similarity,
the first order of business is to get the word simi-
larity right. Character ngrams are quite useful for
this, and remain an invaluable fallback even when
more complex measures of word similarity, such as
embeddings, are used. Dictionary-based methods,
such as the 4lang-based system presented here, are
slightly better, and require only a one-time invest-
ment of manual labor to generate the seed. Criti-
cally, the error characteristics of the context-based
(embedding) and the dictionary-based systems are
quite different, so hybridizing the two provides a real
boost over both.
</bodyText>
<sectionHeader confidence="0.99856" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999954285714286">
Recski designed and implemented the machine-
based similarity, ´Acs reimplemented the align-and-
penalize architecture and created the word embed-
ding. We thank Andr´as Kornai (HAS Institute for
Computer Science) and M´arton Sipos (Budapest U
of Technology) for useful comments and discus-
sions.
</bodyText>
<sectionHeader confidence="0.998573" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99958218">
Eneko Agirre, Carmen Banea, Claire Cardie, Daniel
Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo,
Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihal-
cea, German Rigau, Larraitz Uria, and Janyce Wiebe.
2015. SemEval-2015 Task 2: Semantic Textual Sim-
ilarity, English, Spanish and Pilot on Interpretability.
In Proceedings of the 9th International Workshop on
Semantic Evaluation (SemEval 2015), Denver, CO,
U.S.A.
Branimir K. Boguraev and Edward J. Briscoe. 1989.
Computational Lexicography for Natural Language
Processing. Longman.
Stephen Bullon. 2003. Longman Dictionary of Contem-
porary English 4. Longman.
Lushan Han, Abhay Kashyap, Tim Finin,
James Mayfield, and Jonathan Weese. 2013.
UMBC EBIQUITY-CORE: Semantic textual similar-
ity systems. In Proceedings of the 2nd Joint Conf. on
Lexical and Computational Semantics, pages 44–52.
Dan Klein and Christopher D Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics-Volume 1, pages 423–430.
Andr´as Kornai and M´arton Makrai. 2013. A 4lang fo-
galmi sz´ot´ar. In Attila Tan´acs and Veronika Vincze,
editors, IX. Magyar Sz´amit´og´epes Nyelv´eszeti Konfer-
encia, pages 62–70.
Andr´as Kornai, Judit ´Acs, M´arton Makrai, D´avid
Nemeskey, Katalin Pajkossy, and G´abor Recski. 2015.
Competence in lexical semantics. To appear in Proc.
*SEM-2015.
Andr´as Kornai. 2010. The algebra of lexical seman-
tics. In Christian Ebert, Gerhard J¨ager, and Jens
Michaelis, editors, Proceedings of the 11th Mathemat-
ics of Language Workshop, LNAI 6149, pages 174–
199. Springer.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Y. Bengio and Y. LeCun,
editors, Proc. ICLR 2013.
George A. Miller. 1995. Wordnet: a lexical database for
English. Communications of the ACM, 38(11):39–41.
G´abor Recski. 2015. Building concept graphs
from monolingual dictionary entries. Unpublished
manuscript.
Wei Xu, Chris Callison-Burch, and William B. Dolan.
2015. SemEval-2015 Task 1: Paraphrase and semantic
similarity in Twitter (PIT). In Proceedings of the 9th
International Workshop on Semantic Evaluation (Se-
mEval).
</reference>
<page confidence="0.99764">
142
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.068924">
<title confidence="0.7552985">MathLingBudapest: Concept Networks for Semantic Similarity G´abor Research Institute for Hungarian Academy of</title>
<abstract confidence="0.40284">Bencz´ur u.</abstract>
<address confidence="0.95498">1068 Budapest,</address>
<email confidence="0.998185">recski@mokk.bme.hu</email>
<author confidence="0.468356">Judit</author>
<affiliation confidence="0.958862666666667">HAS Research Institute for Linguistics Dept. of Automation and Applied Budapest U of Technology and</affiliation>
<address confidence="0.810364">Magyar tud´osok krt. 2 (Bldg. 1117 Budapest,</address>
<email confidence="0.998935">judit@mokk.bme.hu</email>
<abstract confidence="0.98784025">We present our approach to measuring semantic similarity of sentence pairs used in Semeval 2015 tasks 1 and 2. We adopt the sentence alignment framework of (Han et al., 2013) and experiment with several measures of word similarity. We hybridize the common vector-based models with definition graphs the dictionary and devise a measure of graph similarity that yields good results on training data. We did not address the specific challenges posed by Twitter data, and this is reflected in placing 11th from 30 in Task 1, but our systems perform fairly well on the generic datasets of Task 2, with the hybrid approach placing 11th among 78 runs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Eneko Agirre</author>
<author>Carmen Banea</author>
<author>Claire Cardie</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
</authors>
<title>Aitor Gonzalez-Agirre, Weiwei Guo, Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe.</title>
<date>2015</date>
<booktitle>SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015),</booktitle>
<location>Denver, CO, U.S.A.</location>
<contexts>
<context position="1215" citStr="Agirre et al., 2015" startWordPosition="188" endWordPosition="191">experiment with several measures of word similarity. We hybridize the common vector-based models with definition graphs from the 4lang concept dictionary and devise a measure of graph similarity that yields good results on training data. We did not address the specific challenges posed by Twitter data, and this is reflected in placing 11th from 30 in Task 1, but our systems perform fairly well on the generic datasets of Task 2, with the hybrid approach placing 11th among 78 runs. 1 Introduction This paper describes the systems participating in Semeval-2015 Task 1 (Xu et al., 2015) and Task 2 (Agirre et al., 2015). To compute the semantic similarity of two sentences we use the architecture presented in (Han et al., 2013) to find, for each word, its counterpart in the other sentence that is semantically most similar to it. We implemented several methods for measuring word similarity, among them (i) a word embedding created by the method presented in (Mikolov et al., 2013) and (ii) a metric based on networks of concepts derived from the 4lang concept lexicon (Kornai and Makrai, 2013; Kornai et al., 2015) and definitions from the Longman Dictionary of Contemporary English (Bullon, 2003). A hybrid system e</context>
</contexts>
<marker>Agirre, Banea, Cardie, Cer, Diab, 2015</marker>
<rawString>Eneko Agirre, Carmen Banea, Claire Cardie, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei Guo, Iigo Lopez-Gazpio, Montse Maritxalar, Rada Mihalcea, German Rigau, Larraitz Uria, and Janyce Wiebe. 2015. SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), Denver, CO, U.S.A.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Branimir K Boguraev</author>
<author>Edward J Briscoe</author>
</authors>
<date>1989</date>
<booktitle>Computational Lexicography for Natural Language Processing.</booktitle>
<publisher>Longman.</publisher>
<contexts>
<context position="6226" citStr="Boguraev and Briscoe, 1989" startWordPosition="1037" endWordPosition="1040">ations of error types in Twitter data: if one sentence starts with a question word and the other one does not or if one sentence contains a past-tense verb and the other does not, we reduce the overall score by 1/(L(S1) + L(S2)), where L(S1) and L(S2) are the numbers of words in each sentence. 3 Similarity from Concept Networks This section will present the word similarity measure based on principles of lexical semantics presented in (Kornai, 2010). The 4lang concept dictionary (Kornai and Makrai, 2013) contains 3500 definitions created manually. Because the Longman Defining Vocabulary (LDV) (Boguraev and Briscoe, 1989) is a subset of 4lang, we could automatically extend this manually created seed to every headword of the Longman Dictionary of Contemporary English (LDOCE) by processing their definitions with the Stanford Dependency Parser (Klein and Manning, 2003), and mapping dependency relations to sets of edges in the 4lang-style concept graph. Details of the mapping will be described elsewhere (Recski, 2015). Since these definitions are essentially graphs of concepts, we have experimented with similarity functions over pairs of such graphs that capture semantic similarity of the concepts defined by each </context>
</contexts>
<marker>Boguraev, Briscoe, 1989</marker>
<rawString>Branimir K. Boguraev and Edward J. Briscoe. 1989. Computational Lexicography for Natural Language Processing. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Bullon</author>
</authors>
<date>2003</date>
<journal>Longman Dictionary of Contemporary English</journal>
<volume>4</volume>
<publisher>Longman.</publisher>
<contexts>
<context position="1796" citStr="Bullon, 2003" startWordPosition="289" endWordPosition="290">and Task 2 (Agirre et al., 2015). To compute the semantic similarity of two sentences we use the architecture presented in (Han et al., 2013) to find, for each word, its counterpart in the other sentence that is semantically most similar to it. We implemented several methods for measuring word similarity, among them (i) a word embedding created by the method presented in (Mikolov et al., 2013) and (ii) a metric based on networks of concepts derived from the 4lang concept lexicon (Kornai and Makrai, 2013; Kornai et al., 2015) and definitions from the Longman Dictionary of Contemporary English (Bullon, 2003). A hybrid system exploiting both of these metrics yields the best results and placed 11th among 73 systems on Semeval Task 2a (Semantic Textual Similarity for English). All components of our system are available for download under an MIT license from GitHub12. Section 2 describes the system architecture and points out the main differences between our system and that in (Han et al., 2013), section 3 outlines our word similarity metric derived from the 4lang concept lexicon. We present the evaluation of our systems on both tasks in section 4, and section 5 provides a brief conclusion. 2 Archite</context>
</contexts>
<marker>Bullon, 2003</marker>
<rawString>Stephen Bullon. 2003. Longman Dictionary of Contemporary English 4. Longman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lushan Han</author>
<author>Abhay Kashyap</author>
<author>Tim Finin</author>
<author>James Mayfield</author>
<author>Jonathan Weese</author>
</authors>
<title>UMBC EBIQUITY-CORE: Semantic textual similarity systems.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2nd Joint Conf. on Lexical and Computational Semantics,</booktitle>
<pages>44--52</pages>
<contexts>
<context position="1324" citStr="Han et al., 2013" startWordPosition="208" endWordPosition="211"> graphs from the 4lang concept dictionary and devise a measure of graph similarity that yields good results on training data. We did not address the specific challenges posed by Twitter data, and this is reflected in placing 11th from 30 in Task 1, but our systems perform fairly well on the generic datasets of Task 2, with the hybrid approach placing 11th among 78 runs. 1 Introduction This paper describes the systems participating in Semeval-2015 Task 1 (Xu et al., 2015) and Task 2 (Agirre et al., 2015). To compute the semantic similarity of two sentences we use the architecture presented in (Han et al., 2013) to find, for each word, its counterpart in the other sentence that is semantically most similar to it. We implemented several methods for measuring word similarity, among them (i) a word embedding created by the method presented in (Mikolov et al., 2013) and (ii) a metric based on networks of concepts derived from the 4lang concept lexicon (Kornai and Makrai, 2013; Kornai et al., 2015) and definitions from the Longman Dictionary of Contemporary English (Bullon, 2003). A hybrid system exploiting both of these metrics yields the best results and placed 11th among 73 systems on Semeval Task 2a (</context>
<context position="2931" citStr="Han et al., 2013" startWordPosition="475" endWordPosition="478">s on both tasks in section 4, and section 5 provides a brief conclusion. 2 Architecture Our framework for determining the semantic similarity of two sentences is based on the system presented in (Han et al., 2013). Their architecture, Align and Penalize, involves computing an alignment score between two sentences based on some measure of word similarity. We’ve chosen to reimplement this system so that we can experiment with various notions of word similarity, including the one based on 4lang and presented in section 3. Although we reimplemented virtually all rules and components described by (Han et al., 2013) for experimentation, we shall only describe those that ended up in at least one of the five configurations submitted to Semeval. The core idea behind the Align and Penalize architecture is, given two senteces S1 and S2 and some measure of word similarity, to align each word of one sentence with some word of the other sentence so that the similarity of word pairs is maximized. 1http://github.com/juditacs/semeval 2http://github.com/kornai/pymachine 138 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 138–142, Denver, Colorado, June 4-5, 2015. c�2015 Ass</context>
<context position="5065" citStr="Han et al., 2013" startWordPosition="838" endWordPosition="841">ord and recieve an alignment score of 1. If a sentence contains a sequence of two words (e.g. long term or can not) that appear in the other sentence without a space and with or without a hyphen (e.g. long-term or cannot), these are also aligned with a score of 1. The score returned by the word similarity component can be boosted based on WordNet (Miller, 1995), e.g. if one is a hypernym of the other, if one appears frequently in glosses of the other, or if they are derivationally related. For the exact cases covered and a description of how the boost is calculated, the reader is referred to (Han et al., 2013). In our submissions we only used this boost on word similarity scores obtained from word embeddings. The similarity score may be reduced by a variety of penalties, which we only enabled for Task 1 runs – they haven’t brought any improvement on Task 2 datasets in any of our early experiments. Of the penalties described in (Han et al., 2013) we only used the one which decreases alignment scores if the word similarity score for some word pair is very small (&lt; 0.05). We also introduced two new types of penalties based on our observations of error types in Twitter data: if one sentence starts with</context>
<context position="13685" citStr="Han et al., 2013" startWordPosition="2281" endWordPosition="2284">ation) our two systems performed equally in terms of Fscore and ranked 30th among 38 systems. On Task 1b the hybrid system performed considerably better than the purely vector-based run, placing 11th out of 28 runs. On Task 2 our hybrid system ranked 11th among 78 systems, the systems using the word embedding and the 4lang-based similarity alone (with string similarity as a fallback for OOVs in each case) ranked 22nd and 15th, respectively. 3http://clic.cimec.unitn.it/amac/twitter ngram/ 4http://radimrehurek.com/gensim 5https://code.google.com/p/word2vec/ 141 5 Conclusion In a framework like (Han et al., 2013) which approximates sentence similarity by word similarity, the first order of business is to get the word similarity right. Character ngrams are quite useful for this, and remain an invaluable fallback even when more complex measures of word similarity, such as embeddings, are used. Dictionary-based methods, such as the 4lang-based system presented here, are slightly better, and require only a one-time investment of manual labor to generate the seed. Critically, the error characteristics of the context-based (embedding) and the dictionary-based systems are quite different, so hybridizing the </context>
</contexts>
<marker>Han, Kashyap, Finin, Mayfield, Weese, 2013</marker>
<rawString>Lushan Han, Abhay Kashyap, Tim Finin, James Mayfield, and Jonathan Weese. 2013. UMBC EBIQUITY-CORE: Semantic textual similarity systems. In Proceedings of the 2nd Joint Conf. on Lexical and Computational Semantics, pages 44–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="6475" citStr="Klein and Manning, 2003" startWordPosition="1077" endWordPosition="1080">are the numbers of words in each sentence. 3 Similarity from Concept Networks This section will present the word similarity measure based on principles of lexical semantics presented in (Kornai, 2010). The 4lang concept dictionary (Kornai and Makrai, 2013) contains 3500 definitions created manually. Because the Longman Defining Vocabulary (LDV) (Boguraev and Briscoe, 1989) is a subset of 4lang, we could automatically extend this manually created seed to every headword of the Longman Dictionary of Contemporary English (LDOCE) by processing their definitions with the Stanford Dependency Parser (Klein and Manning, 2003), and mapping dependency relations to sets of edges in the 4lang-style concept graph. Details of the mapping will be described elsewhere (Recski, 2015). Since these definitions are essentially graphs of concepts, we have experimented with similarity functions over pairs of such graphs that capture semantic similarity of the concepts defined by each of them. There are two fundamentally different configurations present in 4lang graphs: 1. two nodes may be connected via a 0-edge, which is a generalization over unary predication (dog →− bark), attribution (dog 0 →−0 faithful), and hypernymy, or th</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´as Kornai</author>
<author>M´arton Makrai</author>
</authors>
<title>A 4lang fogalmi sz´ot´ar.</title>
<date>2013</date>
<booktitle>In Attila Tan´acs and Veronika Vincze,</booktitle>
<pages>62--70</pages>
<editor>editors, IX. Magyar</editor>
<contexts>
<context position="1691" citStr="Kornai and Makrai, 2013" startWordPosition="271" endWordPosition="274">ng 78 runs. 1 Introduction This paper describes the systems participating in Semeval-2015 Task 1 (Xu et al., 2015) and Task 2 (Agirre et al., 2015). To compute the semantic similarity of two sentences we use the architecture presented in (Han et al., 2013) to find, for each word, its counterpart in the other sentence that is semantically most similar to it. We implemented several methods for measuring word similarity, among them (i) a word embedding created by the method presented in (Mikolov et al., 2013) and (ii) a metric based on networks of concepts derived from the 4lang concept lexicon (Kornai and Makrai, 2013; Kornai et al., 2015) and definitions from the Longman Dictionary of Contemporary English (Bullon, 2003). A hybrid system exploiting both of these metrics yields the best results and placed 11th among 73 systems on Semeval Task 2a (Semantic Textual Similarity for English). All components of our system are available for download under an MIT license from GitHub12. Section 2 describes the system architecture and points out the main differences between our system and that in (Han et al., 2013), section 3 outlines our word similarity metric derived from the 4lang concept lexicon. We present the e</context>
<context position="6107" citStr="Kornai and Makrai, 2013" startWordPosition="1022" endWordPosition="1025">y score for some word pair is very small (&lt; 0.05). We also introduced two new types of penalties based on our observations of error types in Twitter data: if one sentence starts with a question word and the other one does not or if one sentence contains a past-tense verb and the other does not, we reduce the overall score by 1/(L(S1) + L(S2)), where L(S1) and L(S2) are the numbers of words in each sentence. 3 Similarity from Concept Networks This section will present the word similarity measure based on principles of lexical semantics presented in (Kornai, 2010). The 4lang concept dictionary (Kornai and Makrai, 2013) contains 3500 definitions created manually. Because the Longman Defining Vocabulary (LDV) (Boguraev and Briscoe, 1989) is a subset of 4lang, we could automatically extend this manually created seed to every headword of the Longman Dictionary of Contemporary English (LDOCE) by processing their definitions with the Stanford Dependency Parser (Klein and Manning, 2003), and mapping dependency relations to sets of edges in the 4lang-style concept graph. Details of the mapping will be described elsewhere (Recski, 2015). Since these definitions are essentially graphs of concepts, we have experimente</context>
</contexts>
<marker>Kornai, Makrai, 2013</marker>
<rawString>Andr´as Kornai and M´arton Makrai. 2013. A 4lang fogalmi sz´ot´ar. In Attila Tan´acs and Veronika Vincze, editors, IX. Magyar Sz´amit´og´epes Nyelv´eszeti Konferencia, pages 62–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´as Kornai</author>
<author>Judit ´Acs</author>
<author>M´arton Makrai</author>
<author>D´avid Nemeskey</author>
<author>Katalin Pajkossy</author>
<author>G´abor Recski</author>
</authors>
<date>2015</date>
<note>Competence in lexical semantics. To appear in Proc. *SEM-2015.</note>
<marker>Kornai, ´Acs, Makrai, Nemeskey, Pajkossy, Recski, 2015</marker>
<rawString>Andr´as Kornai, Judit ´Acs, M´arton Makrai, D´avid Nemeskey, Katalin Pajkossy, and G´abor Recski. 2015. Competence in lexical semantics. To appear in Proc. *SEM-2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´as Kornai</author>
</authors>
<title>The algebra of lexical semantics.</title>
<date>2010</date>
<booktitle>Proceedings of the 11th Mathematics of Language Workshop, LNAI 6149,</booktitle>
<pages>174--199</pages>
<editor>In Christian Ebert, Gerhard J¨ager, and Jens Michaelis, editors,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="6051" citStr="Kornai, 2010" startWordPosition="1015" endWordPosition="1016">reases alignment scores if the word similarity score for some word pair is very small (&lt; 0.05). We also introduced two new types of penalties based on our observations of error types in Twitter data: if one sentence starts with a question word and the other one does not or if one sentence contains a past-tense verb and the other does not, we reduce the overall score by 1/(L(S1) + L(S2)), where L(S1) and L(S2) are the numbers of words in each sentence. 3 Similarity from Concept Networks This section will present the word similarity measure based on principles of lexical semantics presented in (Kornai, 2010). The 4lang concept dictionary (Kornai and Makrai, 2013) contains 3500 definitions created manually. Because the Longman Defining Vocabulary (LDV) (Boguraev and Briscoe, 1989) is a subset of 4lang, we could automatically extend this manually created seed to every headword of the Longman Dictionary of Contemporary English (LDOCE) by processing their definitions with the Stanford Dependency Parser (Klein and Manning, 2003), and mapping dependency relations to sets of edges in the 4lang-style concept graph. Details of the mapping will be described elsewhere (Recski, 2015). Since these definitions</context>
</contexts>
<marker>Kornai, 2010</marker>
<rawString>Andr´as Kornai. 2010. The algebra of lexical semantics. In Christian Ebert, Gerhard J¨ager, and Jens Michaelis, editors, Proceedings of the 11th Mathematics of Language Workshop, LNAI 6149, pages 174– 199. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>Proc. ICLR</booktitle>
<editor>In Y. Bengio and Y. LeCun, editors,</editor>
<contexts>
<context position="1579" citStr="Mikolov et al., 2013" startWordPosition="251" endWordPosition="254">t our systems perform fairly well on the generic datasets of Task 2, with the hybrid approach placing 11th among 78 runs. 1 Introduction This paper describes the systems participating in Semeval-2015 Task 1 (Xu et al., 2015) and Task 2 (Agirre et al., 2015). To compute the semantic similarity of two sentences we use the architecture presented in (Han et al., 2013) to find, for each word, its counterpart in the other sentence that is semantically most similar to it. We implemented several methods for measuring word similarity, among them (i) a word embedding created by the method presented in (Mikolov et al., 2013) and (ii) a metric based on networks of concepts derived from the 4lang concept lexicon (Kornai and Makrai, 2013; Kornai et al., 2015) and definitions from the Longman Dictionary of Contemporary English (Bullon, 2003). A hybrid system exploiting both of these metrics yields the best results and placed 11th among 73 systems on Semeval Task 2a (Semantic Textual Similarity for English). All components of our system are available for download under an MIT license from GitHub12. Section 2 describes the system architecture and points out the main differences between our system and that in (Han et al</context>
<context position="11090" citStr="Mikolov et al., 2013" startWordPosition="1878" endWordPosition="1881"> as being interviewed, treating them as perfect synonyms proved to be an efficient strategy when trying to assign sentence similarity scores that correlate highly with human annotators’ judgements. 4 Submissions We shall now describe the particular configurations used for each submission in Semeval. For Task 1 (Paraphrase and Semantic Similarity in Twitter) we ran two systems: twitter-embed uses a single source of word similarity, a word embedding built from a corpus of word 6-grams from the Rovereto Twitter N-Gram Corpus3 using the gensim4 package’s implementation of the method presented in (Mikolov et al., 2013). Our second submission, twitter-mash combines several sources of word similarity by averaging the output of various systems using weights that have been learned using plain least squares regression on the training data available. The systems participating in the vote differ in the word similarity measure they use: one subset uses the character ngram baseline described in section 2 with various parameters (n = 2, 3, 4, each with Jaccard- and Dice-similarity), two systems use word embeddings (built from 5-grams and 6-grams of the Rovereto corpus, respectively) and one uses the 4lang-based word </context>
<context position="12452" citStr="Mikolov et al., 2013" startWordPosition="2088" endWordPosition="2091"> 0.515 Task 1b: Semantic Similarity Pearson 0.229 0.511 Table 1: Performance of submitted systems on Task 1. embedding machine hybrid Task 2a: Semantic Similarity answers-forums 0.704 0.698 0.723 answers-students 0.700 0.746 0.751 belief 0.733 0.736 0.747 headlines 0.769 0.805 0.804 images 0.804 0.841 0.844 mean Pearson 0.748 0.777 0.784 Table 2: Performance of submitted systems on Task 2. For Task 2 (Semantic Textual Similarity) we were allowed three submissions. The embedding system uses a word embedding built from the first 1 billion words of the English Wikipedia using the word2vec5 tool (Mikolov et al., 2013). The machine system uses the word similarity measure described in section 3 (both systems use the character ngram baseline as a fallback for OOVs). Finally, for the hybrid submission we used a weighted sum of these two systems and the character ngram baseline (weights were once again obtained using simple least square regression on the available training data). In both hybrid submissions we trained on a single dataset consisting of all training data available, we haven’t experimented with genre-specific models. Our results on each task are presented in Tables 1 and 2. In case of Task 1a (Para</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Y. Bengio and Y. LeCun, editors, Proc. ICLR 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="4811" citStr="Miller, 1995" startWordPosition="793" endWordPosition="794">s to detect acronyms and compounds: if a word of one sentence that is a sequence of 2-5 characters (e.g. ABC) has a matching sequence of words in the other sentence (e.g. American Broadcasting Company), all words of the phrase are aligned with this word and recieve an alignment score of 1. If a sentence contains a sequence of two words (e.g. long term or can not) that appear in the other sentence without a space and with or without a hyphen (e.g. long-term or cannot), these are also aligned with a score of 1. The score returned by the word similarity component can be boosted based on WordNet (Miller, 1995), e.g. if one is a hypernym of the other, if one appears frequently in glosses of the other, or if they are derivationally related. For the exact cases covered and a description of how the boost is calculated, the reader is referred to (Han et al., 2013). In our submissions we only used this boost on word similarity scores obtained from word embeddings. The similarity score may be reduced by a variety of penalties, which we only enabled for Task 1 runs – they haven’t brought any improvement on Task 2 datasets in any of our early experiments. Of the penalties described in (Han et al., 2013) we </context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: a lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G´abor Recski</author>
</authors>
<title>Building concept graphs from monolingual dictionary entries.</title>
<date>2015</date>
<note>Unpublished manuscript.</note>
<contexts>
<context position="6626" citStr="Recski, 2015" startWordPosition="1104" endWordPosition="1105"> semantics presented in (Kornai, 2010). The 4lang concept dictionary (Kornai and Makrai, 2013) contains 3500 definitions created manually. Because the Longman Defining Vocabulary (LDV) (Boguraev and Briscoe, 1989) is a subset of 4lang, we could automatically extend this manually created seed to every headword of the Longman Dictionary of Contemporary English (LDOCE) by processing their definitions with the Stanford Dependency Parser (Klein and Manning, 2003), and mapping dependency relations to sets of edges in the 4lang-style concept graph. Details of the mapping will be described elsewhere (Recski, 2015). Since these definitions are essentially graphs of concepts, we have experimented with similarity functions over pairs of such graphs that capture semantic similarity of the concepts defined by each of them. There are two fundamentally different configurations present in 4lang graphs: 1. two nodes may be connected via a 0-edge, which is a generalization over unary predication (dog →− bark), attribution (dog 0 →−0 faithful), and hypernymy, or the IS A relation (dog →− mammal). 0 2. two nodes can be connected, via a 1-edge and a 2-edge respectively, to a third one representing a binary relation</context>
<context position="8319" citStr="Recski, 2015" startWordPosition="1407" endWordPosition="1408">ions it takes part in. For example, based on the definition graph in Figure 1, the predicates of the concept street ←− 1 IN 139 Figure 1: 4lang definition of bird. bird are {vertebrate; (HAS, feather); (HAS, wing); (MAKE, egg)j. Our initial version of graph similarity is the Jaccard similarity of the sets of predicates of each concept, i.e. S(w1, w2) = |P(w1) n P(w2)| |P(w1) U P(w2)| For all words that are not among the 3500 defined in 4lang we obtain definition graphs by automated parsing of Longman definitions and the application of a simple mapping from dependency relations to graph edges (Recski, 2015). By far the largest source of noise in these graphs is that currently there is no postprocessor component that recognizes common structures of dictionary definitions like appositive relative clauses. For example the word casualty is defined by LDOCE as someone who is hurt or killed in an accident or war and we currently build the graph in Figure 2 instead of that in Figure 3. To mitigate the effects of these anomalities, we updated our definition of predicates: we let them be “inherited” via paths of 0-edges encoding the IS A-relationship. We’ve also experimented with similarity measures that</context>
</contexts>
<marker>Recski, 2015</marker>
<rawString>G´abor Recski. 2015. Building concept graphs from monolingual dictionary entries. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
</authors>
<title>SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT).</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).</booktitle>
<contexts>
<context position="1182" citStr="Xu et al., 2015" startWordPosition="181" endWordPosition="184">rk of (Han et al., 2013) and experiment with several measures of word similarity. We hybridize the common vector-based models with definition graphs from the 4lang concept dictionary and devise a measure of graph similarity that yields good results on training data. We did not address the specific challenges posed by Twitter data, and this is reflected in placing 11th from 30 in Task 1, but our systems perform fairly well on the generic datasets of Task 2, with the hybrid approach placing 11th among 78 runs. 1 Introduction This paper describes the systems participating in Semeval-2015 Task 1 (Xu et al., 2015) and Task 2 (Agirre et al., 2015). To compute the semantic similarity of two sentences we use the architecture presented in (Han et al., 2013) to find, for each word, its counterpart in the other sentence that is semantically most similar to it. We implemented several methods for measuring word similarity, among them (i) a word embedding created by the method presented in (Mikolov et al., 2013) and (ii) a metric based on networks of concepts derived from the 4lang concept lexicon (Kornai and Makrai, 2013; Kornai et al., 2015) and definitions from the Longman Dictionary of Contemporary English </context>
</contexts>
<marker>Xu, Callison-Burch, Dolan, 2015</marker>
<rawString>Wei Xu, Chris Callison-Burch, and William B. Dolan. 2015. SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>