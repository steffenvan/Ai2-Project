<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001673">
<title confidence="0.998089">
Sentence Level Dialect Identification
for Machine Translation System Selection
</title>
<author confidence="0.9948">
Wael Salloum, Heba Elfardy, Linda Alamir-Salloum, Nizar Habash and Mona Diab†
</author>
<affiliation confidence="0.993267">
Center for Computational Learning Systems, Columbia University, New York, USA
</affiliation>
<email confidence="0.549628">
{wael,heba,habash}@ccls.columbia.edu
</email>
<affiliation confidence="0.987854">
†Department of Computer Science, The George Washington University, Washington DC, USA
</affiliation>
<email confidence="0.999109">
†mtdiab@email.gwu.edu
</email>
<sectionHeader confidence="0.997416" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9990235">
In this paper we study the use of sentence-
level dialect identification in optimizing
machine translation system selection when
translating mixed dialect input. We test
our approach on Arabic, a prototypical
diglossic language; and we optimize the
combination of four different machine
translation systems. Our best result im-
proves over the best single MT system
baseline by 1.0% BLEU and over a strong
system selection baseline by 0.6% BLEU
on a blind test set.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999967333333334">
A language can be described as a set of dialects,
among which one &amp;quot;standard variety&amp;quot; has a spe-
cial representative status.1 Despite being increas-
ingly ubiquitous in informal written genres such
as social media, most non-standard dialects are
resource-poor compared to their standard variety.
For statistical machine translation (MT), which re-
lies on the existence of parallel data, translating
from non-standard dialects is a challenge. In this
paper we study the use of sentence-level dialect
identification together with various linguistic fea-
tures in optimizing the selection of outputs of four
different MT systems on input text that includes a
mix of dialects.
We test our approach on Arabic, a prototypi-
cal diglossic language (Ferguson, 1959) where the
standard form of the language, Modern Standard
Arabic (MSA) and the regional dialects (DA) live
side-by-side and are closely related. MSA is the
language used in education, scripted speech and
official settings while DA is the primarily spoken
</bodyText>
<footnote confidence="0.975197">
1This paper presents work supported by the Defense Ad-
vanced Research Projects Agency (DARPA) contract No.
HR0011-12-C-0014. Any opinions, findings and conclusions
or recommendations expressed in this paper are those of the
authors and do not necessarily reflect the views of DARPA.
</footnote>
<bodyText confidence="0.9766274">
native vernacular. We consider two DAs: Egyp-
tian and Levantine Arabic in addition to MSA. Our
best system selection approach improves over our
best baseline single MT system by 1.0% absolute
BLEU point on a blind test set.
</bodyText>
<sectionHeader confidence="0.99993" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999500176470588">
Arabic Dialect Machine Translation. Two ap-
proaches have emerged to alleviate the problem
of DA-English parallel data scarcity: using MSA
as a bridge language (Sawaf, 2010; Salloum and
Habash, 2011; Salloum and Habash, 2013; Sajjad
et al., 2013), and using crowd sourcing to acquire
parallel data (Zbib et al., 2012). Sawaf (2010)
and Salloum and Habash (2013) used hybrid so-
lutions that combine rule-based algorithms and re-
sources such as lexicons and morphological ana-
lyzers with statistical models to map DA to MSA
before using MSA-to-English MT systems. Zbib
et al. (2012) obtained a 1.5M word parallel corpus
of DA-English using crowd sourcing. Applied on
a DA test set, a system trained on their 1.5M word
corpus outperformed a system that added 150M
words of MSA-English data, as well as outper-
forming a system with oracle DA-to-MSA pivot.
In this paper we use four MT systems that trans-
late from DA to English in different ways. Similar
to Zbib et al. (2012), we use DA-English, MSA-
English and DA+MSA-English systems. Our DA-
English data includes the 1.5M words created by
Zbib et al. (2012). Our fourth MT system uses
ELISSA, the DA-to-MSA MT tool by Salloum and
Habash (2013), to produce an MSA pivot.
Dialect Identification. There has been a num-
ber of efforts on dialect identification (Biadsy et
al., 2009; Zaidan and Callison-Burch, 2011; Ak-
bacak et al., 2011; Elfardy et al., 2013; Elfardy
and Diab, 2013). Elfardy et al. (2013) performed
token-level dialect ID by casting the problem as
a code-switching problem and treating MSA and
Egyptian as two different languages. They later
</bodyText>
<page confidence="0.962501">
772
</page>
<bodyText confidence="0.960251">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 772–778,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
used features from their token-level system to train
a classifier that performs sentence-level dialect ID
(Elfardy and Diab, 2013). In this paper, we use
AIDA, the system of Elfardy and Diab (2013), to
provide a variety of dialect ID features to train
classifiers that select, for a given sentence, the MT
system that produces the best translation.
</bodyText>
<subsectionHeader confidence="0.720845">
System Selection and Combination in Machine
</subsectionHeader>
<bodyText confidence="0.999976115384615">
Translation. The most popular approach to MT
system combination involves building confusion
networks from the outputs of different MT sys-
tems and decoding them to generate new transla-
tions (Rosti et al., 2007; Karakos et al., 2008; He
et al., 2008; Xu et al., 2011). Other researchers
explored the idea of re-ranking the n-best output
of MT systems using different types of syntactic
models (Och et al., 2004; Hasan et al., 2006; Ma
and McKeown, 2013). While most researchers
use target language features in training their re-
rankers, others considered source language fea-
tures (Ma and McKeown, 2013).
Most MT system combination work uses MT
systems employing different techniques to train on
the same data. However, in this paper, we use the
same MT algorithms for training, tuning, and test-
ing, but vary the training data, specifically in terms
of the degree of source language dialectness. Our
approach runs a classifier trained only on source
language features to decide which system should
translate each sentence in the test set, which means
that each sentence goes through one MT system
only. Since we do not combine the output of the
MT systems on the phrase level, we call our ap-
proach &amp;quot;system selection&amp;quot; to avoid confusion.
</bodyText>
<sectionHeader confidence="0.996064" genericHeader="method">
3 Machine Translation Experiments
</sectionHeader>
<bodyText confidence="0.975738538461539">
In this section, we present our MT experimental
setup and the four baseline systems we built, and
we evaluate their performance and the potential of
their combination. In the next section we present
and evaluate the system selection approach.
MT Tools and Settings. We use the open-source
Moses toolkit (Koehn et al., 2007) to build four
Arabic-English phrase-based statistical machine
translation systems (SMT). Our systems use a
standard phrase-based architecture. The parallel
corpora are word-aligned using GIZA++ (Och and
Ney, 2003). The language model for our systems
is trained on English Gigaword (Graff and Cieri,
2003). We use SRILM Toolkit (Stolcke, 2002)
to build a 5-gram language model with modified
Kneser-Ney smoothing. Feature weights are tuned
to maximize BLEU on tuning sets using Mini-
mum Error Rate Training (Och, 2003). Results
are presented in terms of BLEU (Papineni et al.,
2002). All evaluation results are case insensi-
tive. The English data is tokenized using simple
punctuation-based rules. The MSA portion of the
Arabic side is segmented according to the Arabic
Treebank (ATB) tokenization scheme (Maamouri
et al., 2004; Sadat and Habash, 2006) using the
MADA+TOKAN morphological analyzer and tok-
enizer v3.1 (Roth et al., 2008), while the DA por-
tion is ATB-tokenized with MADA-ARZ (Habash
et al., 2013). The Arabic text is also Alif/Ya nor-
malized. For more details on processing Arabic,
see (Habash, 2010).
MT Train/Tune/Test Data. We have two par-
allel corpora. The first is a DA-English corpus
of 5M tokenized words of Egyptian (-3.5M)
and Levantine (-1.5M). This corpus is part of
BOLT data. The second is an MSA-English cor-
pus of 57M tokenized words obtained from sev-
eral LDC corpora (10 times the size of the DA-
English data). We work with eight standard MT
test sets: three MSA sets from NIST MTEval with
four references (MT06, MT08, and MT09), four
Egyptian sets from LDC BOLT data with two ref-
erences (EgyDevV1, EgyDevV2, EgyDevV3, and
EgyTestV2), and one Levantine set from BBN
(Zbib et al., 2012) with one reference which we
split into LevDev and LevTest. We used MT08
and EgyDevV3 to tune SMT systems while we di-
vided the remaining sets among classifier training
data (5,562 sentences), dev (1,802 sentences) and
blind test (1,804 sentences) sets to ensure each of
these new sets has a variety of dialects and genres
(weblog and newswire).
</bodyText>
<listItem confidence="0.966085375">
MT Systems. We build four MT systems.
(1) DA-Only. This system is trained on the DA-
English data and tuned on EgyDevV3.
(2) MSA-Only. This system is trained on the
MSA-English data and tuned on MT08.
(3) DA+MSA. This system is trained on the
combination of both corpora (resulting in 62M to-
kenized2 words on the Arabic side) and tuned on
</listItem>
<bodyText confidence="0.813702125">
2Since the DA+MSA system is intended for DA data and
DA morphology, as far as tokenization is concerned, is more
complex, we tokenized the training data with dialect aware-
ness (DA with MADA-ARZ and MSA with MADA) since
MADA-ARZ does a lot better than MADA on DA (Habash
et al., 2013). Tuning and Test data, however, are tokenized
by MADA-ARZ since we do not assume any knowledge of
the dialect of a test sentence.
</bodyText>
<page confidence="0.991568">
773
</page>
<bodyText confidence="0.973010047619048">
EgyDevV3.
(4) MSA-Pivot. This MSA-pivoting system
uses Salloum and Habash (2013)’s DA-MSA MT
system followed by an Arabic-English SMT sys-
tem which is trained on both corpora augmented
with the DA-English where the DA side is prepro-
cessed with the same DA-MSA MT system then
tokenized with MADA-ARZ. The result is 67M
tokenized words on the Arabic side. EgyDevV3
was similarly preprocessed with the DA-MSA MT
system and MADA-ARZ and used for tuning the
system parameters. Test sets are similarly prepro-
cessed before decoding with the SMT system.
Baseline MT System Results. We report the re-
sults of our dev set on the four MT systems we
built in Table 1. The MSA-Pivot system produces
the best singleton result among all systems. All
differences in BLEU scores between the four sys-
tems are statistically significant above the 95%
level. Statistical significance is computed using
paired bootstrap re-sampling (Koehn, 2004).
</bodyText>
<table confidence="0.943036">
System Training Data (TD) BLEU
Name
DA-En MSA-En DAT-En TD Size
1. DA-Only 5M 5M 26.6
2. MSA-Only 57M 57M 32.7
3. DA+MSA 5M 57M 62M 33.6
4. MSA-Pivot 5M 57M 5M 67M 33.9
Oracle System Selection 39.3
</table>
<tableCaption confidence="0.998919">
Table 1: Results from the baseline MT systems and their or-
</tableCaption>
<bodyText confidence="0.943922076923077">
acle system selection. The training data west used in different
MT systems are also indicated. DAT (in the fourth column)
is the DA part of the 5M word DA-En parallel data processed
with the DA-MSA MT system.
Oracle System Selection. We also report in Ta-
ble 1 an oracle system selection where we pick, for
each sentence, the English translation that yields
the best BLEU score. This oracle indicates that
the upper bound for improvement achievable from
system selection is 5.4% BLEU. Excluding dif-
ferent systems from the combination lowered the
overall score between 0.9% and 1.8%, suggesting
the systems are indeed complementary.
</bodyText>
<sectionHeader confidence="0.983545" genericHeader="method">
4 MT System Selection
</sectionHeader>
<bodyText confidence="0.999981923076923">
The approach we take in this paper benefits from
the techniques and conclusions of previous papers
in that we build different MT systems similar to
those discussed above but instead of trying to find
which one is the best, we try to leverage the use
of all of them by automatically deciding what sen-
tences should go to which system. Our hypothesis
is that these systems complement each other in in-
teresting ways where the combination of their se-
lections could lead to better overall performance
stipulating that our approach could benefit from
the strengths while avoiding the weaknesses of
each individual system.
</bodyText>
<subsectionHeader confidence="0.987444">
4.1 Dialect ID Binary Classification
</subsectionHeader>
<bodyText confidence="0.9999868">
For baseline system selection, we use the clas-
sification decision of Elfardy and Diab (2013)’s
sentence-level dialect identification system to de-
cide on the target MT system. Since the deci-
sion is binary (DA or MSA) and we have four MT
systems, we considered all possible configurations
and determined empirically that the best configu-
ration is to select MSA-Only for the MSA tag and
MSA-Pivot for the DA tag. We do not report other
configuration results due to space restrictions.
</bodyText>
<subsectionHeader confidence="0.797323">
4.2 Feature-based Four-Class Classification
</subsectionHeader>
<bodyText confidence="0.997320666666667">
For our main approach, we train a four-class clas-
sifier to predict the target MT system to select
for each sentence using only source-language fea-
tures. We experimented with different classifiers
in the Weka Data Mining Tool (Hall et al., 2009)
for training and testing our system selection ap-
proach. The best performing classifier was Naive
Bayes (with Weka’s default settings).
Training Data Class Labels. We run the
5,562 sentences of the classification training
data through our four MT systems and produce
sentence-level BLEU scores (with length penalty).
We pick the name of the MT system with the high-
est BLEU score as the class label for that sen-
tence. When there is a tie in BLEU scores, we pick
the system label that yields better overall BLEU
scores from the systems tied.
Training Data Source-Language Features.
We use two sources of features extracted from
untokenized sentences to train our four-class
classifiers: basic and extended features.
</bodyText>
<subsectionHeader confidence="0.407826">
A. Basic Features
</subsectionHeader>
<bodyText confidence="0.990751666666667">
These are the same set of features that were used
by the dialect ID tool together with the class label
generated by this tool.
</bodyText>
<listItem confidence="0.994541142857143">
i. Token-Level Features. These features rely on
language models, MSA and Egyptian morphologi-
cal analyzers and a Highly Dialectal Egyptian lex-
icon to decide whether each word is MSA, Egyp-
tian, Both, or Out of Vocabulary.
ii. Perplexity Features. These are two features
that measure the perplexity of a sentence against
</listItem>
<page confidence="0.992827">
774
</page>
<bodyText confidence="0.996222952380952">
two language models: MSA and Egyptian.
iii. Meta Features. Features that do not di-
rectly relate to the dialectalness of words in the
given sentence but rather estimate how informal
the sentence is and include: percentage of to-
kens, punctuation, and Latin words, number of to-
kens, average word length, whether the sentence
has any words that have word-lengthening effects
or not, whether the sentence has any diacritized
words or not, whether the sentence has emoticons
or not, whether the sentence has consecutive re-
peated punctuation or not, whether the sentence
has a question mark or not, and whether the sen-
tence has an exclamation mark or not.
iv. The Dialect-Class Feature. We run the sen-
tence through the Dialect ID binary classifier and
we use the predicted class label (DA or MSA) as a
feature in our system. Since the Dialect ID system
was trained on a different data set, we think its de-
cision may provide additional information to our
classifiers.
</bodyText>
<subsubsectionHeader confidence="0.601138">
B. Extended Features
</subsubsectionHeader>
<bodyText confidence="0.960905">
We add features extracted from two sources.
</bodyText>
<listItem confidence="0.994435413793104">
i. MSA-Pivoting Features. Salloum and Habash
(2013) DA-MSA MT system produces interme-
diate files used for diagnosis or debugging pur-
poses. We exploit one file in which the sys-
tem identifies (or, &amp;quot;selects&amp;quot;) dialectal words and
phrases that need to be translated to MSA. We ex-
tract confidence indicating features. These fea-
tures are: sentence length (in words), percent-
age of selected words and phrases, number of se-
lected words, number of selected phrases, num-
ber of words morphologically selected as dialec-
tal by a mainly Levantine morphological analyzer,
number of words selected as dialectal by the tool’s
DA-MSA lexicons, number of OOV words against
the MSA-Pivot system training data, number of
words in the sentences that appeared less than 5
times in the training data, number of words in the
sentences that appeared between 5 and 10 times
in the training data, number of words in the sen-
tences that appeared between 10 and 15 times
in the training data, number of words that have
spelling errors and corrected by this tool (e.g.,
word-lengthening), number of punctuation marks,
and number of words that are written in Latin
script.
ii. MT Training Data Source-Side LM Perplex-
ity Features. The second set of features uses per-
plexity against language models built from the
source-side of the training data of each of the four
</listItem>
<bodyText confidence="0.996066">
baseline systems. These four features may tell the
classifier which system is more suitable to trans-
late a given sentence.
</bodyText>
<subsectionHeader confidence="0.997113">
4.3 System Selection Evaluation
</subsectionHeader>
<bodyText confidence="0.9999763">
Development Set. The first part of Table 2 re-
peats the best baseline system and the four-system
oracle combination from Table 1 for convenience.
The third row shows the result of running our sys-
tem selection baseline that uses the Dialect ID bi-
nary decision on the Dev set sentences to decide
on the target MT system. It improves over the best
single system baseline (MSA-Pivot) by a statisti-
cally significant 0.5% BLEU. Crucially, we should
note that this is a deterministic process.
</bodyText>
<table confidence="0.9995135">
System BLEU Diff.
Best Single MT System Baseline 33.9 0.0
Oracle 39.3 5.4
Dialect ID Binary Selection Baseline 34.4 0.5
Four-Class Classification
Basic Features 35.1 1.2
Extended Features 34.8 0.9
Basic + Extended Features 35.2 1.3
</table>
<tableCaption confidence="0.852556666666667">
Table 2: Results of baselines and system selection systems
on the Dev set in terms of BLEU. The best single MT system
baseline is MSA-Pivot.
</tableCaption>
<bodyText confidence="0.999409384615384">
The second part of Table 2 shows the results of
our four-class Naive Bayes classifiers trained on
the classification training data we created. The
first column shows the source of sentence level
features employed. As mentioned earlier, we use
the Basic features alone, the Extended features
alone, and then their combination. The classifier
that uses both feature sources simultaneously as
feature vectors is our best performer. It improves
over our best baseline single MT system by 1.3%
BLEU and over the Dialect ID Binary Classifica-
tion system selection baseline by 0.8% BLEU. Im-
provements are statistically significant.
</bodyText>
<table confidence="0.999417625">
System BLEU Diff.
DA-Only 26.6
MSA-Only 30.7
DA+MSA 32.4
MSA-Pivot 32.5
Four-System Oracle Combination 38.0 5.5
Best Dialect ID Binary Classifier 32.9 0.4
Best Classifier: Basic + Extended Features 33.5 1.0
</table>
<tableCaption confidence="0.9974215">
Table 3: Results of baselines and system selection systems
on the Blind test set in terms of BLEU.
</tableCaption>
<bodyText confidence="0.98938575">
Blind Test Set. Table 3 shows the results on our
Blind Test set. The first part of the table shows
the results of our four baseline MT systems. The
systems have the same rank as on the Dev set and
</bodyText>
<page confidence="0.997608">
775
</page>
<table confidence="0.999892714285714">
System All Dialect MSA
DA-Only 26.6 19.3 33.2
MSA-Only 32.7 14.7 50.0
DA+MSA 33.6 19.4 46.3
MSA-Pivot 33.9 19.6 46.4
Four-System Oracle Combination 39.3 24.4 52.1
Best Performing Classifier 35.2 19.8 50.0
</table>
<tableCaption confidence="0.7939898">
Table 4: Dialect breakdown of performance on the Dev set
for our best performing classifier against our four baselines
and their oracle combination. Our classifier does not know
of these subsets, it runs on the set as a whole; therefore, we
repeat its results in the second column for convenience.
</tableCaption>
<bodyText confidence="0.999622416666667">
MSA-Pivot is also the best performer. The differ-
ences in BLEU are statistically significant. The
second part shows the four-system oracle combi-
nation which shows a 5.5% BLEU upper bound
on improvements. The third part shows the re-
sults of the Dialect ID Binary Classification which
improves by 0.4% BLEU. The last row shows
the four-class classifier results which improves by
1.0% BLEU over the best single MT system base-
line and by 0.6% BLEU over the Dialect ID Bi-
nary Classification. Results on the Blind Test set
are consistent with the Dev set results.
</bodyText>
<sectionHeader confidence="0.991812" genericHeader="evaluation">
5 Discussion and Error Analysis
</sectionHeader>
<bodyText confidence="0.999347569230769">
DA versus MSA Performance. In Table 4, col-
umn All illustrates the results over the entire Dev
set, while columns DA and MSA show system
performance on the DA and MSA subsets of the
Dev set, respectively. The best single baseline MT
system for DA is MSA-Pivot has a large room for
improvement given the oracle upper bound (4.8%
BLEU absolute). However, our best system selec-
tion approach improves over MSA-Pivot by a small
margin of 0.2% BLEU absolute only, albeit a sta-
tistically significant improvement. The MSA col-
umn oracle shows a smaller improvement of 2.1%
BLEU absolute over the best single MSA-Only MT
system. Furthermore, when translating MSA with
our best system selection performer we get the
same results as the best baseline MT system for
MSA even though our system does not know the
dialect of the sentences a priori. If we consider the
breakdown of the performance in our best overall
(33.9% BLEU) single baseline MT system (MSA-
Pivot), we observe that the performance on MSA
is about 3.6% absolute BLEU points below our
best results; this suggests that most of the system
selection gain over the best single baseline is on
MSA selection.
Manual Error Analysis. We performed manual
error analysis on a Dev set sample of 250 sen-
tences distributed among the different dialects and
genres. Our best performing classifier selected the
best system in 48% of the DA cases and 52% of
the MSA cases. We did a detailed manual error
analysis for the cases where the classifier failed to
predict the best MT system. The sources of errors
we found cover 89% of the cases. In 21% of the
error cases, our classifier predicted a better trans-
lation than the one considered gold by BLEU due
to BLEU bias, e.g., severe sentence-level length
penalty due to an extra punctuation in a short sen-
tence. Also, 3% of errors are due to bad refer-
ences, e.g., a dialectal sentence in an MSA set that
the human translators did not understand.
A group of error sources resulted from MSA
sentences classified correctly as MSA-Only; how-
ever, one of the other three systems produced bet-
ter translations for two reasons. First, since the
MSA training data is from an older time span than
the DA data, 10% of errors are due to MSA sen-
tences that use recent terminology (e.g., Egyp-
tian revolution 2011: places, politicians, etc.)
that appear in the DA training data. Also, web
writing styles in MSA sentences such as blog
style (e.g., rhetorical questions), blog punctuation
marks (e.g., &amp;quot;..&amp;quot;, &amp;quot;???!!&amp;quot;), and formal MSA forum
greetings resulted in 23%, 16%, and 6% of the
cases, respectively.
Finally, in 10% of the cases our classifier is con-
fused by a code-switched sentence, e.g., a dialec-
tal proverb in an MSA sentence or a weak MSA
literal translation of dialectal words and phrases.
Some of these cases may be solved by adding
more features to our classifier, e.g., blog style writ-
ing features, while others need a radical change to
our technique such as word and phrase level di-
alect identification for MT system combination of
code-switched sentences.
</bodyText>
<sectionHeader confidence="0.989072" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999948083333333">
We presented a sentence-level classification ap-
proach for MT system selection for diglossic lan-
guages. We got a 1.0% BLEU improvement over
the best baseline single MT system. In the future
we plan to add more training data to see the effect
on the accuracy of system selection. We plan to
give different weights to different training exam-
ples based on the drop in BLEU score the exam-
ple can cause if classified incorrectly. We also plan
to explore confusion-network combination and re-
ranking techniques based on target language fea-
tures.
</bodyText>
<page confidence="0.997911">
776
</page>
<sectionHeader confidence="0.983488" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99923877118644">
Murat Akbacak, Dimitra Vergyri, Andreas Stolcke,
Nicolas Scheffer, and Arindam Mandal. 2011.
Effective arabic dialect classification using diverse
phonotactic models. In INTERSPEECH, volume 11,
pages 737–740.
Fadi Biadsy, Julia Hirschberg, and Nizar Habash.
2009. Spoken arabic dialect identification using
phonotactic modeling. In Proceedings of the Work-
shop on Computational Approaches to Semitic Lan-
guages at the meeting of the European Associa-
tion for Computational Linguistics (EACL), Athens,
Greece.
Heba Elfardy and Mona Diab. 2013. Sentence Level
Dialect Identification in Arabic. In Proceedings of
the 51th Annual Meeting of the Association for Com-
putational Linguistics (ACL-13), Sofia, Bulgaria.
Heba Elfardy, Mohamed Al-Badrashiny, and Mona
Diab. 2013. Code switch point detection in arabic.
In Proceedings of the 18th International Conference
on Application of Natural Language to Information
Systems (NLDB2013), MediaCity, UK.
Charles F Ferguson. 1959. Diglossia. Word,
15(2):325–340.
David Graff and Christopher Cieri. 2003. English Gi-
gaword, LDC Catalog No.: LDC2003T05. Linguis-
tic Data Consortium, University of Pennsylvania.
Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-
kander, and Nadi Tomeh. 2013. Morphological
Analysis and Disambiguation for Dialectal Arabic.
In Proceedings of the 2013 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies
(NAACL-HLT), Atlanta, GA.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan &amp; Claypool Publish-
ers.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explorations, 11(1):10–18.
S. Hasan, O. Bender, and H. Ney. 2006. Rerank-
ing translation hypotheses using structural proper-
ties. In EACL’06 Workshop on Learning Structured
Information in Natural Language Applications.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 98–107. Associa-
tion for Computational Linguistics.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Markus Dreyer. 2008. Machine translation
system combination using itg-based alignments. In
Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics on Human
Language Technologies: Short Papers, pages 81–84.
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177–180, Prague, Czech Re-
public.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP 2004, pages 388–395, Barcelona, Spain,
July. Association for Computational Linguistics.
Wei-Yun Ma and Kathleen McKeown. 2013. Using
a supertagged dependency language model to select
a good translation in system combination. In Pro-
ceedings of NAACL-HLT, pages 433–438.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
In NEMLAR Conference on Arabic Language Re-
sources and Tools, pages 102–109, Cairo, Egypt.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Franz Josef Och. 2004.
A smorgasbord of features for statistical machine
translation. In Meeting of the North American chap-
ter of the Association for Computational Linguistics.
Franz Josef Och. 2003. Minimum Error Rate Train-
ing for Statistical Machine Translation. In Proceed-
ings of the 41st Annual Conference of the Associa-
tion for Computational Linguistics, pages 160–167,
Sapporo, Japan.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311–318,
Philadelphia, PA.
Antti-Veikko Rosti, Spyros Matsoukas, and Richard
Schwartz. 2007. Improved word-level system com-
bination for machine translation. In Proceedings
of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 312–319, Prague,
Czech Republic, June. Association for Computa-
tional Linguistics.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic Morphological
Tagging, Diacritization, and Lemmatization Using
Lexeme Models and Feature Ranking. In Proceed-
ings ofACL-08: HLT, Short Papers, pages 117–120,
Columbus, Ohio.
Fatiha Sadat and Nizar Habash. 2006. Combination
of Arabic preprocessing schemes for statistical ma-
</reference>
<page confidence="0.969903">
777
</page>
<reference confidence="0.999800673076923">
chine translation. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 1–8, Sydney, Aus-
tralia, July. Association for Computational Linguis-
tics.
Hassan Sajjad, Kareem Darwish, and Yonatan Be-
linkov. 2013. Translating dialectal arabic to en-
glish. In The 51st Annual Meeting of the Association
for Computational Linguistics - Short Papers (ACL
Short Papers 2013), Sofia, Bulgaria.
Wael Salloum and Nizar Habash. 2011. Dialectal to
Standard Arabic Paraphrasing to Improve Arabic-
English Statistical Machine Translation. In Pro-
ceedings of the First Workshop on Algorithms and
Resources for Modelling of Dialects and Language
Varieties, pages 10–21, Edinburgh, Scotland.
Wael Salloum and Nizar Habash. 2013. Dialectal
Arabic to English Machine Translation: Pivoting
through Modern Standard Arabic. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies (NAACL-HLT), At-
lanta, GA.
Hassan Sawaf. 2010. Arabic dialect handling in hybrid
machine translation. In Proceedings of the Confer-
ence of the Association for Machine Translation in
the Americas (AMTA), Denver, Colorado.
Andreas Stolcke. 2002. SRILM an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing.
Daguang Xu, Yuan Cao, and Damianos Karakos. 2011.
Description of the jhu system combination scheme
for wmt 2011. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 171–176.
Association for Computational Linguistics.
Omar F Zaidan and Chris Callison-Burch. 2011. The
arabic online commentary dataset: an annotated
dataset of informal arabic with high dialectal con-
tent. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-11), pages 37–41.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-
Burch. 2012. Machine Translation of Arabic Di-
alects. In Proceedings of the 2012 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 49–59, Montréal, Canada, June. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.997035">
778
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.765791">
<title confidence="0.999209">Sentence Level Dialect for Machine Translation System Selection</title>
<author confidence="0.991226">Heba Elfardy Salloum</author>
<author confidence="0.991226">Linda Alamir-Salloum</author>
<author confidence="0.991226">Nizar Habash</author>
<affiliation confidence="0.8878235">Center for Computational Learning Systems, Columbia University, New York, of Computer Science, The George Washington University, Washington DC,</affiliation>
<abstract confidence="0.999540461538462">In this paper we study the use of sentencelevel dialect identification in optimizing machine translation system selection when translating mixed dialect input. We test our approach on Arabic, a prototypical diglossic language; and we optimize the combination of four different machine translation systems. Our best result improves over the best single MT system baseline by 1.0% BLEU and over a strong system selection baseline by 0.6% BLEU on a blind test set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Murat Akbacak</author>
<author>Dimitra Vergyri</author>
<author>Andreas Stolcke</author>
<author>Nicolas Scheffer</author>
<author>Arindam Mandal</author>
</authors>
<title>Effective arabic dialect classification using diverse phonotactic models.</title>
<date>2011</date>
<booktitle>In INTERSPEECH,</booktitle>
<volume>11</volume>
<pages>737--740</pages>
<contexts>
<context position="3758" citStr="Akbacak et al., 2011" startWordPosition="583" endWordPosition="587"> 150M words of MSA-English data, as well as outperforming a system with oracle DA-to-MSA pivot. In this paper we use four MT systems that translate from DA to English in different ways. Similar to Zbib et al. (2012), we use DA-English, MSAEnglish and DA+MSA-English systems. Our DAEnglish data includes the 1.5M words created by Zbib et al. (2012). Our fourth MT system uses ELISSA, the DA-to-MSA MT tool by Salloum and Habash (2013), to produce an MSA pivot. Dialect Identification. There has been a number of efforts on dialect identification (Biadsy et al., 2009; Zaidan and Callison-Burch, 2011; Akbacak et al., 2011; Elfardy et al., 2013; Elfardy and Diab, 2013). Elfardy et al. (2013) performed token-level dialect ID by casting the problem as a code-switching problem and treating MSA and Egyptian as two different languages. They later 772 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 772–778, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics used features from their token-level system to train a classifier that performs sentence-level dialect ID (Elfardy and Diab, 2013). In this paper, we use AIDA, t</context>
</contexts>
<marker>Akbacak, Vergyri, Stolcke, Scheffer, Mandal, 2011</marker>
<rawString>Murat Akbacak, Dimitra Vergyri, Andreas Stolcke, Nicolas Scheffer, and Arindam Mandal. 2011. Effective arabic dialect classification using diverse phonotactic models. In INTERSPEECH, volume 11, pages 737–740.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fadi Biadsy</author>
<author>Julia Hirschberg</author>
<author>Nizar Habash</author>
</authors>
<title>Spoken arabic dialect identification using phonotactic modeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the Workshop on Computational Approaches to Semitic Languages at the meeting of the European Association for Computational Linguistics (EACL),</booktitle>
<location>Athens, Greece.</location>
<contexts>
<context position="3703" citStr="Biadsy et al., 2009" startWordPosition="575" endWordPosition="578">heir 1.5M word corpus outperformed a system that added 150M words of MSA-English data, as well as outperforming a system with oracle DA-to-MSA pivot. In this paper we use four MT systems that translate from DA to English in different ways. Similar to Zbib et al. (2012), we use DA-English, MSAEnglish and DA+MSA-English systems. Our DAEnglish data includes the 1.5M words created by Zbib et al. (2012). Our fourth MT system uses ELISSA, the DA-to-MSA MT tool by Salloum and Habash (2013), to produce an MSA pivot. Dialect Identification. There has been a number of efforts on dialect identification (Biadsy et al., 2009; Zaidan and Callison-Burch, 2011; Akbacak et al., 2011; Elfardy et al., 2013; Elfardy and Diab, 2013). Elfardy et al. (2013) performed token-level dialect ID by casting the problem as a code-switching problem and treating MSA and Egyptian as two different languages. They later 772 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 772–778, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics used features from their token-level system to train a classifier that performs sentence-level dialect ID </context>
</contexts>
<marker>Biadsy, Hirschberg, Habash, 2009</marker>
<rawString>Fadi Biadsy, Julia Hirschberg, and Nizar Habash. 2009. Spoken arabic dialect identification using phonotactic modeling. In Proceedings of the Workshop on Computational Approaches to Semitic Languages at the meeting of the European Association for Computational Linguistics (EACL), Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heba Elfardy</author>
<author>Mona Diab</author>
</authors>
<title>Sentence Level Dialect Identification in Arabic.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics (ACL-13),</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="3805" citStr="Elfardy and Diab, 2013" startWordPosition="592" endWordPosition="595">outperforming a system with oracle DA-to-MSA pivot. In this paper we use four MT systems that translate from DA to English in different ways. Similar to Zbib et al. (2012), we use DA-English, MSAEnglish and DA+MSA-English systems. Our DAEnglish data includes the 1.5M words created by Zbib et al. (2012). Our fourth MT system uses ELISSA, the DA-to-MSA MT tool by Salloum and Habash (2013), to produce an MSA pivot. Dialect Identification. There has been a number of efforts on dialect identification (Biadsy et al., 2009; Zaidan and Callison-Burch, 2011; Akbacak et al., 2011; Elfardy et al., 2013; Elfardy and Diab, 2013). Elfardy et al. (2013) performed token-level dialect ID by casting the problem as a code-switching problem and treating MSA and Egyptian as two different languages. They later 772 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 772–778, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics used features from their token-level system to train a classifier that performs sentence-level dialect ID (Elfardy and Diab, 2013). In this paper, we use AIDA, the system of Elfardy and Diab (2013), to provid</context>
<context position="11565" citStr="Elfardy and Diab (2013)" startWordPosition="1871" endWordPosition="1874">rent MT systems similar to those discussed above but instead of trying to find which one is the best, we try to leverage the use of all of them by automatically deciding what sentences should go to which system. Our hypothesis is that these systems complement each other in interesting ways where the combination of their selections could lead to better overall performance stipulating that our approach could benefit from the strengths while avoiding the weaknesses of each individual system. 4.1 Dialect ID Binary Classification For baseline system selection, we use the classification decision of Elfardy and Diab (2013)’s sentence-level dialect identification system to decide on the target MT system. Since the decision is binary (DA or MSA) and we have four MT systems, we considered all possible configurations and determined empirically that the best configuration is to select MSA-Only for the MSA tag and MSA-Pivot for the DA tag. We do not report other configuration results due to space restrictions. 4.2 Feature-based Four-Class Classification For our main approach, we train a four-class classifier to predict the target MT system to select for each sentence using only source-language features. We experiment</context>
</contexts>
<marker>Elfardy, Diab, 2013</marker>
<rawString>Heba Elfardy and Mona Diab. 2013. Sentence Level Dialect Identification in Arabic. In Proceedings of the 51th Annual Meeting of the Association for Computational Linguistics (ACL-13), Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heba Elfardy</author>
<author>Mohamed Al-Badrashiny</author>
<author>Mona Diab</author>
</authors>
<title>Code switch point detection in arabic.</title>
<date>2013</date>
<booktitle>In Proceedings of the 18th International Conference on Application of Natural Language to Information Systems (NLDB2013),</booktitle>
<location>MediaCity, UK.</location>
<contexts>
<context position="3780" citStr="Elfardy et al., 2013" startWordPosition="588" endWordPosition="591">lish data, as well as outperforming a system with oracle DA-to-MSA pivot. In this paper we use four MT systems that translate from DA to English in different ways. Similar to Zbib et al. (2012), we use DA-English, MSAEnglish and DA+MSA-English systems. Our DAEnglish data includes the 1.5M words created by Zbib et al. (2012). Our fourth MT system uses ELISSA, the DA-to-MSA MT tool by Salloum and Habash (2013), to produce an MSA pivot. Dialect Identification. There has been a number of efforts on dialect identification (Biadsy et al., 2009; Zaidan and Callison-Burch, 2011; Akbacak et al., 2011; Elfardy et al., 2013; Elfardy and Diab, 2013). Elfardy et al. (2013) performed token-level dialect ID by casting the problem as a code-switching problem and treating MSA and Egyptian as two different languages. They later 772 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 772–778, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics used features from their token-level system to train a classifier that performs sentence-level dialect ID (Elfardy and Diab, 2013). In this paper, we use AIDA, the system of Elfardy a</context>
</contexts>
<marker>Elfardy, Al-Badrashiny, Diab, 2013</marker>
<rawString>Heba Elfardy, Mohamed Al-Badrashiny, and Mona Diab. 2013. Code switch point detection in arabic. In Proceedings of the 18th International Conference on Application of Natural Language to Information Systems (NLDB2013), MediaCity, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles F Ferguson</author>
</authors>
<date>1959</date>
<journal>Diglossia. Word,</journal>
<volume>15</volume>
<issue>2</issue>
<contexts>
<context position="1613" citStr="Ferguson, 1959" startWordPosition="233" endWordPosition="234">eing increasingly ubiquitous in informal written genres such as social media, most non-standard dialects are resource-poor compared to their standard variety. For statistical machine translation (MT), which relies on the existence of parallel data, translating from non-standard dialects is a challenge. In this paper we study the use of sentence-level dialect identification together with various linguistic features in optimizing the selection of outputs of four different MT systems on input text that includes a mix of dialects. We test our approach on Arabic, a prototypical diglossic language (Ferguson, 1959) where the standard form of the language, Modern Standard Arabic (MSA) and the regional dialects (DA) live side-by-side and are closely related. MSA is the language used in education, scripted speech and official settings while DA is the primarily spoken 1This paper presents work supported by the Defense Advanced Research Projects Agency (DARPA) contract No. HR0011-12-C-0014. Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA. native vernacular. We consider two DAs: Egyptian and Levantine </context>
</contexts>
<marker>Ferguson, 1959</marker>
<rawString>Charles F Ferguson. 1959. Diglossia. Word, 15(2):325–340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
<author>Christopher Cieri</author>
</authors>
<date>2003</date>
<booktitle>English Gigaword, LDC Catalog No.: LDC2003T05. Linguistic Data</booktitle>
<institution>Consortium, University of Pennsylvania.</institution>
<contexts>
<context position="6485" citStr="Graff and Cieri, 2003" startWordPosition="1015" endWordPosition="1018">s section, we present our MT experimental setup and the four baseline systems we built, and we evaluate their performance and the potential of their combination. In the next section we present and evaluate the system selection approach. MT Tools and Settings. We use the open-source Moses toolkit (Koehn et al., 2007) to build four Arabic-English phrase-based statistical machine translation systems (SMT). Our systems use a standard phrase-based architecture. The parallel corpora are word-aligned using GIZA++ (Och and Ney, 2003). The language model for our systems is trained on English Gigaword (Graff and Cieri, 2003). We use SRILM Toolkit (Stolcke, 2002) to build a 5-gram language model with modified Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the MADA+TOKAN morphological analyzer and tokeniz</context>
</contexts>
<marker>Graff, Cieri, 2003</marker>
<rawString>David Graff and Christopher Cieri. 2003. English Gigaword, LDC Catalog No.: LDC2003T05. Linguistic Data Consortium, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
<author>Ryan Roth</author>
<author>Owen Rambow</author>
<author>Ramy Eskander</author>
<author>Nadi Tomeh</author>
</authors>
<title>Morphological Analysis and Disambiguation for Dialectal Arabic.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<location>Atlanta, GA.</location>
<contexts>
<context position="7187" citStr="Habash et al., 2013" startWordPosition="1127" endWordPosition="1130">ied Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with MADA-ARZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). MT Train/Tune/Test Data. We have two parallel corpora. The first is a DA-English corpus of 5M tokenized words of Egyptian (-3.5M) and Levantine (-1.5M). This corpus is part of BOLT data. The second is an MSA-English corpus of 57M tokenized words obtained from several LDC corpora (10 times the size of the DAEnglish data). We work with eight standard MT test sets: three MSA sets from NIST MTEval with four references (MT06, MT08, and MT09), four Egyptian sets from LDC BOLT data with two refere</context>
<context position="8854" citStr="Habash et al., 2013" startWordPosition="1419" endWordPosition="1422">MT Systems. We build four MT systems. (1) DA-Only. This system is trained on the DAEnglish data and tuned on EgyDevV3. (2) MSA-Only. This system is trained on the MSA-English data and tuned on MT08. (3) DA+MSA. This system is trained on the combination of both corpora (resulting in 62M tokenized2 words on the Arabic side) and tuned on 2Since the DA+MSA system is intended for DA data and DA morphology, as far as tokenization is concerned, is more complex, we tokenized the training data with dialect awareness (DA with MADA-ARZ and MSA with MADA) since MADA-ARZ does a lot better than MADA on DA (Habash et al., 2013). Tuning and Test data, however, are tokenized by MADA-ARZ since we do not assume any knowledge of the dialect of a test sentence. 773 EgyDevV3. (4) MSA-Pivot. This MSA-pivoting system uses Salloum and Habash (2013)’s DA-MSA MT system followed by an Arabic-English SMT system which is trained on both corpora augmented with the DA-English where the DA side is preprocessed with the same DA-MSA MT system then tokenized with MADA-ARZ. The result is 67M tokenized words on the Arabic side. EgyDevV3 was similarly preprocessed with the DA-MSA MT system and MADA-ARZ and used for tuning the system parame</context>
</contexts>
<marker>Habash, Roth, Rambow, Eskander, Tomeh, 2013</marker>
<rawString>Nizar Habash, Ryan Roth, Owen Rambow, Ramy Eskander, and Nadi Tomeh. 2013. Morphological Analysis and Disambiguation for Dialectal Arabic. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nizar Habash</author>
</authors>
<title>Introduction to Arabic Natural Language Processing.</title>
<date>2010</date>
<publisher>Morgan &amp; Claypool Publishers.</publisher>
<contexts>
<context position="7290" citStr="Habash, 2010" startWordPosition="1146" endWordPosition="1147">raining (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with MADA-ARZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). MT Train/Tune/Test Data. We have two parallel corpora. The first is a DA-English corpus of 5M tokenized words of Egyptian (-3.5M) and Levantine (-1.5M). This corpus is part of BOLT data. The second is an MSA-English corpus of 57M tokenized words obtained from several LDC corpora (10 times the size of the DAEnglish data). We work with eight standard MT test sets: three MSA sets from NIST MTEval with four references (MT06, MT08, and MT09), four Egyptian sets from LDC BOLT data with two references (EgyDevV1, EgyDevV2, EgyDevV3, and EgyTestV2), and one Levantine set from BBN (Zbib et al., 2012) </context>
</contexts>
<marker>Habash, 2010</marker>
<rawString>Nizar Habash. 2010. Introduction to Arabic Natural Language Processing. Morgan &amp; Claypool Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA data mining software: an update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="12243" citStr="Hall et al., 2009" startWordPosition="1981" endWordPosition="1984"> the target MT system. Since the decision is binary (DA or MSA) and we have four MT systems, we considered all possible configurations and determined empirically that the best configuration is to select MSA-Only for the MSA tag and MSA-Pivot for the DA tag. We do not report other configuration results due to space restrictions. 4.2 Feature-based Four-Class Classification For our main approach, we train a four-class classifier to predict the target MT system to select for each sentence using only source-language features. We experimented with different classifiers in the Weka Data Mining Tool (Hall et al., 2009) for training and testing our system selection approach. The best performing classifier was Naive Bayes (with Weka’s default settings). Training Data Class Labels. We run the 5,562 sentences of the classification training data through our four MT systems and produce sentence-level BLEU scores (with length penalty). We pick the name of the MT system with the highest BLEU score as the class label for that sentence. When there is a tie in BLEU scores, we pick the system label that yields better overall BLEU scores from the systems tied. Training Data Source-Language Features. We use two sources o</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA data mining software: an update. SIGKDD Explorations, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hasan</author>
<author>O Bender</author>
<author>H Ney</author>
</authors>
<title>Reranking translation hypotheses using structural properties.</title>
<date>2006</date>
<booktitle>In EACL’06 Workshop on Learning Structured Information in Natural Language Applications.</booktitle>
<contexts>
<context position="5018" citStr="Hasan et al., 2006" startWordPosition="782" endWordPosition="785">rovide a variety of dialect ID features to train classifiers that select, for a given sentence, the MT system that produces the best translation. System Selection and Combination in Machine Translation. The most popular approach to MT system combination involves building confusion networks from the outputs of different MT systems and decoding them to generate new translations (Rosti et al., 2007; Karakos et al., 2008; He et al., 2008; Xu et al., 2011). Other researchers explored the idea of re-ranking the n-best output of MT systems using different types of syntactic models (Och et al., 2004; Hasan et al., 2006; Ma and McKeown, 2013). While most researchers use target language features in training their rerankers, others considered source language features (Ma and McKeown, 2013). Most MT system combination work uses MT systems employing different techniques to train on the same data. However, in this paper, we use the same MT algorithms for training, tuning, and testing, but vary the training data, specifically in terms of the degree of source language dialectness. Our approach runs a classifier trained only on source language features to decide which system should translate each sentence in the tes</context>
</contexts>
<marker>Hasan, Bender, Ney, 2006</marker>
<rawString>S. Hasan, O. Bender, and H. Ney. 2006. Reranking translation hypotheses using structural properties. In EACL’06 Workshop on Learning Structured Information in Natural Language Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodong He</author>
<author>Mei Yang</author>
<author>Jianfeng Gao</author>
<author>Patrick Nguyen</author>
<author>Robert Moore</author>
</authors>
<title>Indirect-hmmbased hypothesis alignment for combining outputs from machine translation systems.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>98--107</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4837" citStr="He et al., 2008" startWordPosition="751" endWordPosition="754">token-level system to train a classifier that performs sentence-level dialect ID (Elfardy and Diab, 2013). In this paper, we use AIDA, the system of Elfardy and Diab (2013), to provide a variety of dialect ID features to train classifiers that select, for a given sentence, the MT system that produces the best translation. System Selection and Combination in Machine Translation. The most popular approach to MT system combination involves building confusion networks from the outputs of different MT systems and decoding them to generate new translations (Rosti et al., 2007; Karakos et al., 2008; He et al., 2008; Xu et al., 2011). Other researchers explored the idea of re-ranking the n-best output of MT systems using different types of syntactic models (Och et al., 2004; Hasan et al., 2006; Ma and McKeown, 2013). While most researchers use target language features in training their rerankers, others considered source language features (Ma and McKeown, 2013). Most MT system combination work uses MT systems employing different techniques to train on the same data. However, in this paper, we use the same MT algorithms for training, tuning, and testing, but vary the training data, specifically in terms o</context>
</contexts>
<marker>He, Yang, Gao, Nguyen, Moore, 2008</marker>
<rawString>Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen, and Robert Moore. 2008. Indirect-hmmbased hypothesis alignment for combining outputs from machine translation systems. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 98–107. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damianos Karakos</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
<author>Markus Dreyer</author>
</authors>
<title>Machine translation system combination using itg-based alignments.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers,</booktitle>
<pages>81--84</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4820" citStr="Karakos et al., 2008" startWordPosition="747" endWordPosition="750">d features from their token-level system to train a classifier that performs sentence-level dialect ID (Elfardy and Diab, 2013). In this paper, we use AIDA, the system of Elfardy and Diab (2013), to provide a variety of dialect ID features to train classifiers that select, for a given sentence, the MT system that produces the best translation. System Selection and Combination in Machine Translation. The most popular approach to MT system combination involves building confusion networks from the outputs of different MT systems and decoding them to generate new translations (Rosti et al., 2007; Karakos et al., 2008; He et al., 2008; Xu et al., 2011). Other researchers explored the idea of re-ranking the n-best output of MT systems using different types of syntactic models (Och et al., 2004; Hasan et al., 2006; Ma and McKeown, 2013). While most researchers use target language features in training their rerankers, others considered source language features (Ma and McKeown, 2013). Most MT system combination work uses MT systems employing different techniques to train on the same data. However, in this paper, we use the same MT algorithms for training, tuning, and testing, but vary the training data, specif</context>
</contexts>
<marker>Karakos, Eisner, Khudanpur, Dreyer, 2008</marker>
<rawString>Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, and Markus Dreyer. 2008. Machine translation system combination using itg-based alignments. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, pages 81–84. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Christopher Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Christopher Dyer</author>
<author>Ondrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="6180" citStr="Koehn et al., 2007" startWordPosition="972" endWordPosition="975">de which system should translate each sentence in the test set, which means that each sentence goes through one MT system only. Since we do not combine the output of the MT systems on the phrase level, we call our approach &amp;quot;system selection&amp;quot; to avoid confusion. 3 Machine Translation Experiments In this section, we present our MT experimental setup and the four baseline systems we built, and we evaluate their performance and the potential of their combination. In the next section we present and evaluate the system selection approach. MT Tools and Settings. We use the open-source Moses toolkit (Koehn et al., 2007) to build four Arabic-English phrase-based statistical machine translation systems (SMT). Our systems use a standard phrase-based architecture. The parallel corpora are word-aligned using GIZA++ (Och and Ney, 2003). The language model for our systems is trained on English Gigaword (Graff and Cieri, 2003). We use SRILM Toolkit (Stolcke, 2002) to build a 5-gram language model with modified Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation res</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Christopher Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Christopher Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP 2004,</booktitle>
<pages>388--395</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="9910" citStr="Koehn, 2004" startWordPosition="1594" endWordPosition="1595"> is 67M tokenized words on the Arabic side. EgyDevV3 was similarly preprocessed with the DA-MSA MT system and MADA-ARZ and used for tuning the system parameters. Test sets are similarly preprocessed before decoding with the SMT system. Baseline MT System Results. We report the results of our dev set on the four MT systems we built in Table 1. The MSA-Pivot system produces the best singleton result among all systems. All differences in BLEU scores between the four systems are statistically significant above the 95% level. Statistical significance is computed using paired bootstrap re-sampling (Koehn, 2004). System Training Data (TD) BLEU Name DA-En MSA-En DAT-En TD Size 1. DA-Only 5M 5M 26.6 2. MSA-Only 57M 57M 32.7 3. DA+MSA 5M 57M 62M 33.6 4. MSA-Pivot 5M 57M 5M 67M 33.9 Oracle System Selection 39.3 Table 1: Results from the baseline MT systems and their oracle system selection. The training data west used in different MT systems are also indicated. DAT (in the fourth column) is the DA part of the 5M word DA-En parallel data processed with the DA-MSA MT system. Oracle System Selection. We also report in Table 1 an oracle system selection where we pick, for each sentence, the English translati</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP 2004, pages 388–395, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei-Yun Ma</author>
<author>Kathleen McKeown</author>
</authors>
<title>Using a supertagged dependency language model to select a good translation in system combination.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL-HLT,</booktitle>
<pages>433--438</pages>
<contexts>
<context position="5041" citStr="Ma and McKeown, 2013" startWordPosition="786" endWordPosition="789">dialect ID features to train classifiers that select, for a given sentence, the MT system that produces the best translation. System Selection and Combination in Machine Translation. The most popular approach to MT system combination involves building confusion networks from the outputs of different MT systems and decoding them to generate new translations (Rosti et al., 2007; Karakos et al., 2008; He et al., 2008; Xu et al., 2011). Other researchers explored the idea of re-ranking the n-best output of MT systems using different types of syntactic models (Och et al., 2004; Hasan et al., 2006; Ma and McKeown, 2013). While most researchers use target language features in training their rerankers, others considered source language features (Ma and McKeown, 2013). Most MT system combination work uses MT systems employing different techniques to train on the same data. However, in this paper, we use the same MT algorithms for training, tuning, and testing, but vary the training data, specifically in terms of the degree of source language dialectness. Our approach runs a classifier trained only on source language features to decide which system should translate each sentence in the test set, which means that</context>
</contexts>
<marker>Ma, McKeown, 2013</marker>
<rawString>Wei-Yun Ma and Kathleen McKeown. 2013. Using a supertagged dependency language model to select a good translation in system combination. In Proceedings of NAACL-HLT, pages 433–438.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mohamed Maamouri</author>
<author>Ann Bies</author>
<author>Tim Buckwalter</author>
<author>Wigdan Mekki</author>
</authors>
<title>The Penn Arabic Treebank: Building a Large-Scale Annotated Arabic Corpus.</title>
<date>2004</date>
<booktitle>In NEMLAR Conference on Arabic Language Resources and Tools,</booktitle>
<pages>102--109</pages>
<location>Cairo, Egypt.</location>
<contexts>
<context position="7004" citStr="Maamouri et al., 2004" startWordPosition="1097" endWordPosition="1100"> Ney, 2003). The language model for our systems is trained on English Gigaword (Graff and Cieri, 2003). We use SRILM Toolkit (Stolcke, 2002) to build a 5-gram language model with modified Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with MADA-ARZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). MT Train/Tune/Test Data. We have two parallel corpora. The first is a DA-English corpus of 5M tokenized words of Egyptian (-3.5M) and Levantine (-1.5M). This corpus is part of BOLT data. The second is an MSA-English corpus of 57M tokenized words obtained from several LDC corpora (10 times the size of the DAEngl</context>
</contexts>
<marker>Maamouri, Bies, Buckwalter, Mekki, 2004</marker>
<rawString>Mohamed Maamouri, Ann Bies, Tim Buckwalter, and Wigdan Mekki. 2004. The Penn Arabic Treebank: Building a Large-Scale Annotated Arabic Corpus. In NEMLAR Conference on Arabic Language Resources and Tools, pages 102–109, Cairo, Egypt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="6394" citStr="Och and Ney, 2003" startWordPosition="1000" endWordPosition="1003">pproach &amp;quot;system selection&amp;quot; to avoid confusion. 3 Machine Translation Experiments In this section, we present our MT experimental setup and the four baseline systems we built, and we evaluate their performance and the potential of their combination. In the next section we present and evaluate the system selection approach. MT Tools and Settings. We use the open-source Moses toolkit (Koehn et al., 2007) to build four Arabic-English phrase-based statistical machine translation systems (SMT). Our systems use a standard phrase-based architecture. The parallel corpora are word-aligned using GIZA++ (Och and Ney, 2003). The language model for our systems is trained on English Gigaword (Graff and Cieri, 2003). We use SRILM Toolkit (Stolcke, 2002) to build a 5-gram language model with modified Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>In Meeting of the North American chapter of the Association for Computational Linguistics.</booktitle>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Franz Josef Och. 2004. A smorgasbord of features for statistical machine translation. In Meeting of the North American chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training for Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="6696" citStr="Och, 2003" startWordPosition="1051" endWordPosition="1052">ion approach. MT Tools and Settings. We use the open-source Moses toolkit (Koehn et al., 2007) to build four Arabic-English phrase-based statistical machine translation systems (SMT). Our systems use a standard phrase-based architecture. The parallel corpora are word-aligned using GIZA++ (Och and Ney, 2003). The language model for our systems is trained on English Gigaword (Graff and Cieri, 2003). We use SRILM Toolkit (Stolcke, 2002) to build a 5-gram language model with modified Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with MADA-ARZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). MT T</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training for Statistical Machine Translation. In Proceedings of the 41st Annual Conference of the Association for Computational Linguistics, pages 160–167, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA.</location>
<contexts>
<context position="6760" citStr="Papineni et al., 2002" startWordPosition="1060" endWordPosition="1063">-source Moses toolkit (Koehn et al., 2007) to build four Arabic-English phrase-based statistical machine translation systems (SMT). Our systems use a standard phrase-based architecture. The parallel corpora are word-aligned using GIZA++ (Och and Ney, 2003). The language model for our systems is trained on English Gigaword (Graff and Cieri, 2003). We use SRILM Toolkit (Stolcke, 2002) to build a 5-gram language model with modified Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with MADA-ARZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). MT Train/Tune/Test Data. We have two parallel corpora. The first is </context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antti-Veikko Rosti</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>312--319</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="4798" citStr="Rosti et al., 2007" startWordPosition="743" endWordPosition="746">onal Linguistics used features from their token-level system to train a classifier that performs sentence-level dialect ID (Elfardy and Diab, 2013). In this paper, we use AIDA, the system of Elfardy and Diab (2013), to provide a variety of dialect ID features to train classifiers that select, for a given sentence, the MT system that produces the best translation. System Selection and Combination in Machine Translation. The most popular approach to MT system combination involves building confusion networks from the outputs of different MT systems and decoding them to generate new translations (Rosti et al., 2007; Karakos et al., 2008; He et al., 2008; Xu et al., 2011). Other researchers explored the idea of re-ranking the n-best output of MT systems using different types of syntactic models (Och et al., 2004; Hasan et al., 2006; Ma and McKeown, 2013). While most researchers use target language features in training their rerankers, others considered source language features (Ma and McKeown, 2013). Most MT system combination work uses MT systems employing different techniques to train on the same data. However, in this paper, we use the same MT algorithms for training, tuning, and testing, but vary the</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>Antti-Veikko Rosti, Spyros Matsoukas, and Richard Schwartz. 2007. Improved word-level system combination for machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 312–319, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Roth</author>
<author>Owen Rambow</author>
<author>Nizar Habash</author>
<author>Mona Diab</author>
<author>Cynthia Rudin</author>
</authors>
<title>Arabic Morphological Tagging, Diacritization, and Lemmatization Using Lexeme Models and Feature Ranking.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT, Short Papers,</booktitle>
<pages>117--120</pages>
<location>Columbus, Ohio.</location>
<contexts>
<context position="7112" citStr="Roth et al., 2008" startWordPosition="1114" endWordPosition="1117">SRILM Toolkit (Stolcke, 2002) to build a 5-gram language model with modified Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with MADA-ARZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). MT Train/Tune/Test Data. We have two parallel corpora. The first is a DA-English corpus of 5M tokenized words of Egyptian (-3.5M) and Levantine (-1.5M). This corpus is part of BOLT data. The second is an MSA-English corpus of 57M tokenized words obtained from several LDC corpora (10 times the size of the DAEnglish data). We work with eight standard MT test sets: three MSA sets from NIST MTEval with four references (M</context>
</contexts>
<marker>Roth, Rambow, Habash, Diab, Rudin, 2008</marker>
<rawString>Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab, and Cynthia Rudin. 2008. Arabic Morphological Tagging, Diacritization, and Lemmatization Using Lexeme Models and Feature Ranking. In Proceedings ofACL-08: HLT, Short Papers, pages 117–120, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fatiha Sadat</author>
<author>Nizar Habash</author>
</authors>
<title>Combination of Arabic preprocessing schemes for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sydney, Australia,</location>
<contexts>
<context position="7029" citStr="Sadat and Habash, 2006" startWordPosition="1101" endWordPosition="1104">ge model for our systems is trained on English Gigaword (Graff and Cieri, 2003). We use SRILM Toolkit (Stolcke, 2002) to build a 5-gram language model with modified Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with MADA-ARZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). MT Train/Tune/Test Data. We have two parallel corpora. The first is a DA-English corpus of 5M tokenized words of Egyptian (-3.5M) and Levantine (-1.5M). This corpus is part of BOLT data. The second is an MSA-English corpus of 57M tokenized words obtained from several LDC corpora (10 times the size of the DAEnglish data). We work with e</context>
</contexts>
<marker>Sadat, Habash, 2006</marker>
<rawString>Fatiha Sadat and Nizar Habash. 2006. Combination of Arabic preprocessing schemes for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 1–8, Sydney, Australia, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Sajjad</author>
<author>Kareem Darwish</author>
<author>Yonatan Belinkov</author>
</authors>
<title>Translating dialectal arabic to english.</title>
<date>2013</date>
<booktitle>In The 51st Annual Meeting of the Association for Computational Linguistics - Short Papers (ACL Short Papers</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="2632" citStr="Sajjad et al., 2013" startWordPosition="392" endWordPosition="395">gs and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA. native vernacular. We consider two DAs: Egyptian and Levantine Arabic in addition to MSA. Our best system selection approach improves over our best baseline single MT system by 1.0% absolute BLEU point on a blind test set. 2 Related Work Arabic Dialect Machine Translation. Two approaches have emerged to alleviate the problem of DA-English parallel data scarcity: using MSA as a bridge language (Sawaf, 2010; Salloum and Habash, 2011; Salloum and Habash, 2013; Sajjad et al., 2013), and using crowd sourcing to acquire parallel data (Zbib et al., 2012). Sawaf (2010) and Salloum and Habash (2013) used hybrid solutions that combine rule-based algorithms and resources such as lexicons and morphological analyzers with statistical models to map DA to MSA before using MSA-to-English MT systems. Zbib et al. (2012) obtained a 1.5M word parallel corpus of DA-English using crowd sourcing. Applied on a DA test set, a system trained on their 1.5M word corpus outperformed a system that added 150M words of MSA-English data, as well as outperforming a system with oracle DA-to-MSA pivot</context>
</contexts>
<marker>Sajjad, Darwish, Belinkov, 2013</marker>
<rawString>Hassan Sajjad, Kareem Darwish, and Yonatan Belinkov. 2013. Translating dialectal arabic to english. In The 51st Annual Meeting of the Association for Computational Linguistics - Short Papers (ACL Short Papers 2013), Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wael Salloum</author>
<author>Nizar Habash</author>
</authors>
<title>Dialectal to Standard Arabic Paraphrasing to Improve ArabicEnglish Statistical Machine Translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties,</booktitle>
<pages>10--21</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="2584" citStr="Salloum and Habash, 2011" startWordPosition="384" endWordPosition="387"> contract No. HR0011-12-C-0014. Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA. native vernacular. We consider two DAs: Egyptian and Levantine Arabic in addition to MSA. Our best system selection approach improves over our best baseline single MT system by 1.0% absolute BLEU point on a blind test set. 2 Related Work Arabic Dialect Machine Translation. Two approaches have emerged to alleviate the problem of DA-English parallel data scarcity: using MSA as a bridge language (Sawaf, 2010; Salloum and Habash, 2011; Salloum and Habash, 2013; Sajjad et al., 2013), and using crowd sourcing to acquire parallel data (Zbib et al., 2012). Sawaf (2010) and Salloum and Habash (2013) used hybrid solutions that combine rule-based algorithms and resources such as lexicons and morphological analyzers with statistical models to map DA to MSA before using MSA-to-English MT systems. Zbib et al. (2012) obtained a 1.5M word parallel corpus of DA-English using crowd sourcing. Applied on a DA test set, a system trained on their 1.5M word corpus outperformed a system that added 150M words of MSA-English data, as well as ou</context>
</contexts>
<marker>Salloum, Habash, 2011</marker>
<rawString>Wael Salloum and Nizar Habash. 2011. Dialectal to Standard Arabic Paraphrasing to Improve ArabicEnglish Statistical Machine Translation. In Proceedings of the First Workshop on Algorithms and Resources for Modelling of Dialects and Language Varieties, pages 10–21, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wael Salloum</author>
<author>Nizar Habash</author>
</authors>
<title>Dialectal Arabic to English Machine Translation: Pivoting through Modern Standard Arabic.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),</booktitle>
<location>Atlanta, GA.</location>
<contexts>
<context position="2610" citStr="Salloum and Habash, 2013" startWordPosition="388" endWordPosition="391">0014. Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA. native vernacular. We consider two DAs: Egyptian and Levantine Arabic in addition to MSA. Our best system selection approach improves over our best baseline single MT system by 1.0% absolute BLEU point on a blind test set. 2 Related Work Arabic Dialect Machine Translation. Two approaches have emerged to alleviate the problem of DA-English parallel data scarcity: using MSA as a bridge language (Sawaf, 2010; Salloum and Habash, 2011; Salloum and Habash, 2013; Sajjad et al., 2013), and using crowd sourcing to acquire parallel data (Zbib et al., 2012). Sawaf (2010) and Salloum and Habash (2013) used hybrid solutions that combine rule-based algorithms and resources such as lexicons and morphological analyzers with statistical models to map DA to MSA before using MSA-to-English MT systems. Zbib et al. (2012) obtained a 1.5M word parallel corpus of DA-English using crowd sourcing. Applied on a DA test set, a system trained on their 1.5M word corpus outperformed a system that added 150M words of MSA-English data, as well as outperforming a system with </context>
<context position="9069" citStr="Salloum and Habash (2013)" startWordPosition="1454" endWordPosition="1457"> This system is trained on the combination of both corpora (resulting in 62M tokenized2 words on the Arabic side) and tuned on 2Since the DA+MSA system is intended for DA data and DA morphology, as far as tokenization is concerned, is more complex, we tokenized the training data with dialect awareness (DA with MADA-ARZ and MSA with MADA) since MADA-ARZ does a lot better than MADA on DA (Habash et al., 2013). Tuning and Test data, however, are tokenized by MADA-ARZ since we do not assume any knowledge of the dialect of a test sentence. 773 EgyDevV3. (4) MSA-Pivot. This MSA-pivoting system uses Salloum and Habash (2013)’s DA-MSA MT system followed by an Arabic-English SMT system which is trained on both corpora augmented with the DA-English where the DA side is preprocessed with the same DA-MSA MT system then tokenized with MADA-ARZ. The result is 67M tokenized words on the Arabic side. EgyDevV3 was similarly preprocessed with the DA-MSA MT system and MADA-ARZ and used for tuning the system parameters. Test sets are similarly preprocessed before decoding with the SMT system. Baseline MT System Results. We report the results of our dev set on the four MT systems we built in Table 1. The MSA-Pivot system produ</context>
<context position="14500" citStr="Salloum and Habash (2013)" startWordPosition="2355" endWordPosition="2358">nce has emoticons or not, whether the sentence has consecutive repeated punctuation or not, whether the sentence has a question mark or not, and whether the sentence has an exclamation mark or not. iv. The Dialect-Class Feature. We run the sentence through the Dialect ID binary classifier and we use the predicted class label (DA or MSA) as a feature in our system. Since the Dialect ID system was trained on a different data set, we think its decision may provide additional information to our classifiers. B. Extended Features We add features extracted from two sources. i. MSA-Pivoting Features. Salloum and Habash (2013) DA-MSA MT system produces intermediate files used for diagnosis or debugging purposes. We exploit one file in which the system identifies (or, &amp;quot;selects&amp;quot;) dialectal words and phrases that need to be translated to MSA. We extract confidence indicating features. These features are: sentence length (in words), percentage of selected words and phrases, number of selected words, number of selected phrases, number of words morphologically selected as dialectal by a mainly Levantine morphological analyzer, number of words selected as dialectal by the tool’s DA-MSA lexicons, number of OOV words agains</context>
</contexts>
<marker>Salloum, Habash, 2013</marker>
<rawString>Wael Salloum and Nizar Habash. 2013. Dialectal Arabic to English Machine Translation: Pivoting through Modern Standard Arabic. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), Atlanta, GA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hassan Sawaf</author>
</authors>
<title>Arabic dialect handling in hybrid machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<location>Denver, Colorado.</location>
<contexts>
<context position="2558" citStr="Sawaf, 2010" startWordPosition="382" endWordPosition="383">gency (DARPA) contract No. HR0011-12-C-0014. Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA. native vernacular. We consider two DAs: Egyptian and Levantine Arabic in addition to MSA. Our best system selection approach improves over our best baseline single MT system by 1.0% absolute BLEU point on a blind test set. 2 Related Work Arabic Dialect Machine Translation. Two approaches have emerged to alleviate the problem of DA-English parallel data scarcity: using MSA as a bridge language (Sawaf, 2010; Salloum and Habash, 2011; Salloum and Habash, 2013; Sajjad et al., 2013), and using crowd sourcing to acquire parallel data (Zbib et al., 2012). Sawaf (2010) and Salloum and Habash (2013) used hybrid solutions that combine rule-based algorithms and resources such as lexicons and morphological analyzers with statistical models to map DA to MSA before using MSA-to-English MT systems. Zbib et al. (2012) obtained a 1.5M word parallel corpus of DA-English using crowd sourcing. Applied on a DA test set, a system trained on their 1.5M word corpus outperformed a system that added 150M words of MSA-E</context>
</contexts>
<marker>Sawaf, 2010</marker>
<rawString>Hassan Sawaf. 2010. Arabic dialect handling in hybrid machine translation. In Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA), Denver, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM an Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing.</booktitle>
<contexts>
<context position="6523" citStr="Stolcke, 2002" startWordPosition="1023" endWordPosition="1024">p and the four baseline systems we built, and we evaluate their performance and the potential of their combination. In the next section we present and evaluate the system selection approach. MT Tools and Settings. We use the open-source Moses toolkit (Koehn et al., 2007) to build four Arabic-English phrase-based statistical machine translation systems (SMT). Our systems use a standard phrase-based architecture. The parallel corpora are word-aligned using GIZA++ (Och and Ney, 2003). The language model for our systems is trained on English Gigaword (Graff and Cieri, 2003). We use SRILM Toolkit (Stolcke, 2002) to build a 5-gram language model with modified Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the MADA+TOKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM an Extensible Language Modeling Toolkit. In Proceedings of the International Conference on Spoken Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daguang Xu</author>
<author>Yuan Cao</author>
<author>Damianos Karakos</author>
</authors>
<title>Description of the jhu system combination scheme for wmt 2011.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>171--176</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4855" citStr="Xu et al., 2011" startWordPosition="755" endWordPosition="758">m to train a classifier that performs sentence-level dialect ID (Elfardy and Diab, 2013). In this paper, we use AIDA, the system of Elfardy and Diab (2013), to provide a variety of dialect ID features to train classifiers that select, for a given sentence, the MT system that produces the best translation. System Selection and Combination in Machine Translation. The most popular approach to MT system combination involves building confusion networks from the outputs of different MT systems and decoding them to generate new translations (Rosti et al., 2007; Karakos et al., 2008; He et al., 2008; Xu et al., 2011). Other researchers explored the idea of re-ranking the n-best output of MT systems using different types of syntactic models (Och et al., 2004; Hasan et al., 2006; Ma and McKeown, 2013). While most researchers use target language features in training their rerankers, others considered source language features (Ma and McKeown, 2013). Most MT system combination work uses MT systems employing different techniques to train on the same data. However, in this paper, we use the same MT algorithms for training, tuning, and testing, but vary the training data, specifically in terms of the degree of so</context>
</contexts>
<marker>Xu, Cao, Karakos, 2011</marker>
<rawString>Daguang Xu, Yuan Cao, and Damianos Karakos. 2011. Description of the jhu system combination scheme for wmt 2011. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 171–176. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Omar F Zaidan</author>
<author>Chris Callison-Burch</author>
</authors>
<title>The arabic online commentary dataset: an annotated dataset of informal arabic with high dialectal content.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL-11),</booktitle>
<pages>37--41</pages>
<contexts>
<context position="3736" citStr="Zaidan and Callison-Burch, 2011" startWordPosition="579" endWordPosition="582"> outperformed a system that added 150M words of MSA-English data, as well as outperforming a system with oracle DA-to-MSA pivot. In this paper we use four MT systems that translate from DA to English in different ways. Similar to Zbib et al. (2012), we use DA-English, MSAEnglish and DA+MSA-English systems. Our DAEnglish data includes the 1.5M words created by Zbib et al. (2012). Our fourth MT system uses ELISSA, the DA-to-MSA MT tool by Salloum and Habash (2013), to produce an MSA pivot. Dialect Identification. There has been a number of efforts on dialect identification (Biadsy et al., 2009; Zaidan and Callison-Burch, 2011; Akbacak et al., 2011; Elfardy et al., 2013; Elfardy and Diab, 2013). Elfardy et al. (2013) performed token-level dialect ID by casting the problem as a code-switching problem and treating MSA and Egyptian as two different languages. They later 772 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 772–778, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics used features from their token-level system to train a classifier that performs sentence-level dialect ID (Elfardy and Diab, 2013). In this</context>
</contexts>
<marker>Zaidan, Callison-Burch, 2011</marker>
<rawString>Omar F Zaidan and Chris Callison-Burch. 2011. The arabic online commentary dataset: an annotated dataset of informal arabic with high dialectal content. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL-11), pages 37–41.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Rabih Zbib</author>
<author>Erika Malchiodi</author>
<author>Jacob Devlin</author>
<author>David Stallard</author>
<author>Spyros Matsoukas</author>
<author>Richard Schwartz</author>
<author>John Makhoul</author>
<author>Omar F Zaidan</author>
<author>Chris CallisonBurch</author>
</authors>
<title>Machine Translation of Arabic Dialects.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>49--59</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montréal, Canada,</location>
<contexts>
<context position="2703" citStr="Zbib et al., 2012" startWordPosition="404" endWordPosition="407">f the authors and do not necessarily reflect the views of DARPA. native vernacular. We consider two DAs: Egyptian and Levantine Arabic in addition to MSA. Our best system selection approach improves over our best baseline single MT system by 1.0% absolute BLEU point on a blind test set. 2 Related Work Arabic Dialect Machine Translation. Two approaches have emerged to alleviate the problem of DA-English parallel data scarcity: using MSA as a bridge language (Sawaf, 2010; Salloum and Habash, 2011; Salloum and Habash, 2013; Sajjad et al., 2013), and using crowd sourcing to acquire parallel data (Zbib et al., 2012). Sawaf (2010) and Salloum and Habash (2013) used hybrid solutions that combine rule-based algorithms and resources such as lexicons and morphological analyzers with statistical models to map DA to MSA before using MSA-to-English MT systems. Zbib et al. (2012) obtained a 1.5M word parallel corpus of DA-English using crowd sourcing. Applied on a DA test set, a system trained on their 1.5M word corpus outperformed a system that added 150M words of MSA-English data, as well as outperforming a system with oracle DA-to-MSA pivot. In this paper we use four MT systems that translate from DA to Englis</context>
<context position="7889" citStr="Zbib et al., 2012" startWordPosition="1249" endWordPosition="1252"> see (Habash, 2010). MT Train/Tune/Test Data. We have two parallel corpora. The first is a DA-English corpus of 5M tokenized words of Egyptian (-3.5M) and Levantine (-1.5M). This corpus is part of BOLT data. The second is an MSA-English corpus of 57M tokenized words obtained from several LDC corpora (10 times the size of the DAEnglish data). We work with eight standard MT test sets: three MSA sets from NIST MTEval with four references (MT06, MT08, and MT09), four Egyptian sets from LDC BOLT data with two references (EgyDevV1, EgyDevV2, EgyDevV3, and EgyTestV2), and one Levantine set from BBN (Zbib et al., 2012) with one reference which we split into LevDev and LevTest. We used MT08 and EgyDevV3 to tune SMT systems while we divided the remaining sets among classifier training data (5,562 sentences), dev (1,802 sentences) and blind test (1,804 sentences) sets to ensure each of these new sets has a variety of dialects and genres (weblog and newswire). MT Systems. We build four MT systems. (1) DA-Only. This system is trained on the DAEnglish data and tuned on EgyDevV3. (2) MSA-Only. This system is trained on the MSA-English data and tuned on MT08. (3) DA+MSA. This system is trained on the combination of</context>
</contexts>
<marker>Zbib, Malchiodi, Devlin, Stallard, Matsoukas, Schwartz, Makhoul, Zaidan, CallisonBurch, 2012</marker>
<rawString>Rabih Zbib, Erika Malchiodi, Jacob Devlin, David Stallard, Spyros Matsoukas, Richard Schwartz, John Makhoul, Omar F. Zaidan, and Chris CallisonBurch. 2012. Machine Translation of Arabic Dialects. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 49–59, Montréal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>