<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009840">
<title confidence="0.976176">
Using Discourse Structure Improves Machine Translation Evaluation
</title>
<author confidence="0.954327">
Francisco Guzm´an Shafiq Joty Lluis M`arquez and Preslav Nakov
</author>
<affiliation confidence="0.9514965">
ALT Research Group
Qatar Computing Research Institute — Qatar Foundation
</affiliation>
<email confidence="0.992835">
{fguzman,sjoty,lmarquez,pnakov}@qf.org.qa
</email>
<sectionHeader confidence="0.997311" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999699117647059">
We present experiments in using dis-
course structure for improving machine
translation evaluation. We first design
two discourse-aware similarity measures,
which use all-subtree kernels to compare
discourse parse trees in accordance with
the Rhetorical Structure Theory. Then,
we show that these measures can help
improve a number of existing machine
translation evaluation metrics both at the
segment- and at the system-level. Rather
than proposing a single new metric, we
show that discourse information is com-
plementary to the state-of-the-art evalu-
ation metrics, and thus should be taken
into account in the development of future
richer evaluation metrics.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99867464516129">
From its foundations, Statistical Machine Transla-
tion (SMT) had two defining characteristics: first,
translation was modeled as a generative process at
the sentence-level. Second, it was purely statisti-
cal over words or word sequences and made lit-
tle to no use of linguistic information. Although
modern SMT systems have switched to a discrim-
inative log-linear framework, which allows for ad-
ditional sources as features, it is generally hard to
incorporate dependencies beyond a small window
of adjacent words, thus making it difficult to use
linguistically-rich models.
Recently, there have been two promising re-
search directions for improving SMT and its eval-
uation: (a) by using more structured linguistic
information, such as syntax (Galley et al., 2004;
Quirk et al., 2005), hierarchical structures (Chi-
ang, 2005), and semantic roles (Wu and Fung,
2009; Lo et al., 2012), and (b) by going beyond
the sentence-level, e.g., translating at the docu-
ment level (Hardmeier et al., 2012).
Going beyond the sentence-level is important
since sentences rarely stand on their own in a
well-written text. Rather, each sentence follows
smoothly from the ones before it, and leads into
the ones that come afterwards. The logical rela-
tionship between sentences carries important in-
formation that allows the text to express a meaning
as a whole beyond the sum of its separate parts.
Note that sentences can be made of several
clauses, which in turn can be interrelated through
the same logical relations. Thus, in a coherent text,
discourse units (sentences or clauses) are logically
connected: the meaning of a unit relates to that of
the previous and the following units.
Discourse analysis seeks to uncover this coher-
ence structure underneath the text. Several formal
theories of discourse have been proposed to de-
scribe the coherence structure (Mann and Thomp-
son, 1988; Asher and Lascarides, 2003; Webber,
2004). For example, the Rhetorical Structure The-
ory (Mann and Thompson, 1988), or RST, repre-
sents text by labeled hierarchical structures called
Discourse Trees (DTs), which can incorporate sev-
eral layers of other linguistic information, e.g.,
syntax, predicate-argument structure, etc.
Modeling discourse brings together the above
research directions (a) and (b), which makes it an
attractive goal for MT. This is demonstrated by the
establishment of a recent workshop dedicated to
Discourse in Machine Translation (Webber et al.,
2013), collocated with the 2013 annual meeting of
the Association of Computational Linguistics.
The area of discourse analysis for SMT is still
nascent and, to the best of our knowledge, no
previous research has attempted to use rhetorical
structure for SMT or machine translation evalua-
tion. One possible reason could be the unavailabil-
ity of accurate discourse parsers. However, this
situation is likely to change given the most recent
advances in automatic discourse analysis (Joty et
al., 2012; Joty et al., 2013).
</bodyText>
<page confidence="0.96892">
687
</page>
<note confidence="0.8311895">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999964552631579">
We believe that the semantic and pragmatic in-
formation captured in the form of DTs (i) can help
develop discourse-aware SMT systems that pro-
duce coherent translations, and (ii) can yield bet-
ter MT evaluation metrics. While in this work we
focus on the latter, we think that the former is also
within reach, and that SMT systems would bene-
fit from preserving the coherence relations in the
source language when generating target-language
translations.
In this paper, rather than proposing yet another
MT evaluation metric, we show that discourse
information is complementary to many existing
evaluation metrics, and thus should not be ignored.
We first design two discourse-aware similarity
measures, which use DTs generated by a publicly-
available discourse parser (Joty et al., 2012); then,
we show that they can help improve a number of
MT evaluation metrics at the segment- and at the
system-level in the context of the WMT11 and the
WMT12 metrics shared tasks (Callison-Burch et
al., 2011; Callison-Burch et al., 2012).
These metrics tasks are based on sentence-level
evaluation, which arguably can limit the benefits
of using global discourse properties. Fortunately,
several sentences are long and complex enough to
present rich discourse structures connecting their
basic clauses. Thus, although limited, this setting
is able to demonstrate the potential of discourse-
level information for MT evaluation. Furthermore,
sentence-level scoring (i) is compatible with most
translation systems, which work on a sentence-by-
sentence basis, (ii) could be beneficial to mod-
ern MT tuning mechanisms such as PRO (Hop-
kins and May, 2011) and MIRA (Watanabe et al.,
2007; Chiang et al., 2008), which also work at
the sentence-level, and (iii) could be used for re-
ranking n-best lists of translation hypotheses.
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.991742403225806">
Addressing discourse-level phenomena in ma-
chine translation is relatively new as a research di-
rection. Some recent work has looked at anaphora
resolution (Hardmeier and Federico, 2010) and
discourse connectives (Cartoni et al., 2011; Meyer,
2011), to mention two examples.1 However, so
far the attempts to incorporate discourse-related
knowledge in MT have been only moderately suc-
cessful, at best.
1We refer the reader to (Hardmeier, 2012) for an in-depth
overview of discourse-related research for MT.
A common argument, is that current automatic
evaluation metrics such as BLEU are inadequate
to capture discourse-related aspects of translation
quality (Hardmeier and Federico, 2010; Meyer et
al., 2012). Thus, there is consensus that discourse-
informed MT evaluation metrics are needed in or-
der to advance research in this direction. Here we
suggest some simple ways to create such metrics,
and we also show that they yield better correlation
with human judgments.
The field of automatic evaluation metrics for
MT is very active, and new metrics are contin-
uously being proposed, especially in the context
of the evaluation campaigns that run as part of
the Workshops on Statistical Machine Transla-
tion (WMT 2008-2012), and NIST Metrics for
Machine Translation Challenge (MetricsMATR),
among others. For example, at WMT12, 12 met-
rics were compared (Callison-Burch et al., 2012),
most of them new.
There have been several attempts to incorpo-
rate syntactic and semantic linguistic knowledge
into MT evaluation. For instance, at the syn-
tactic level, we find metrics that measure the
structural similarity between shallow syntactic se-
quences (Gim´enez and M`arquez, 2007; Popovic
and Ney, 2007) or between constituency trees (Liu
and Gildea, 2005). In the semantic case, there are
metrics that exploit the similarity over named en-
tities and predicate-argument structures (Gim´enez
and M`arquez, 2007; Lo et al., 2012).
In this work, instead of proposing a new metric,
we focus on enriching current MT evaluation met-
rics with discourse information. Our experiments
show that many existing metrics can benefit from
additional knowledge about discourse structure.
In comparison to the syntactic and semantic ex-
tensions of MT metrics, there have been very few
attempts to incorporate discourse information so
far. One example are the semantics-aware metrics
of Gim´enez and M`arquez (2009) and Comelles et
al. (2010), which use the Discourse Representa-
tion Theory (Kamp and Reyle, 1993) and tree-
based discourse representation structures (DRS)
produced by a semantic parser. They calculate the
similarity between the MT output and references
based on DRS subtree matching, as defined in (Liu
and Gildea, 2005), DRS lexical overlap, and DRS
morpho-syntactic overlap. However, they could
not improve correlation with human judgments, as
evaluated on the MetricsMATR dataset.
</bodyText>
<page confidence="0.997801">
688
</page>
<bodyText confidence="0.999951285714286">
Compared to the previous work, (i) we use a
different discourse representation (RST), (ii) we
compare discourse parses using all-subtree ker-
nels (Collins and Duffy, 2001), (iii) we evaluate
on much larger datasets, for several language pairs
and for multiple metrics, and (iv) we do demon-
strate better correlation with human judgments.
Wong and Kit (2012) recently proposed an
extension of MT metrics with a measure of
document-level lexical cohesion (Halliday and
Hasan, 1976). Lexical cohesion is achieved using
word repetitions and semantically similar words
such as synonyms, hypernyms, and hyponyms.
For BLEU and TER, they observed improved
correlation with human judgments on the MTC4
dataset when linearly interpolating these metrics
with their lexical cohesion score. Unlike their
work, which measures lexical cohesion at the
document-level, here we are concerned with co-
herence (rhetorical) structure, primarily at the
sentence-level.
</bodyText>
<sectionHeader confidence="0.958473" genericHeader="method">
3 Our Discourse-Based Measures
</sectionHeader>
<bodyText confidence="0.999981794117647">
Our working hypothesis is that the similarity be-
tween the discourse structures of an automatic and
of a reference translation provides additional in-
formation that can be valuable for evaluating MT
systems. In particular, we believe that good trans-
lations should tend to preserve discourse relations.
As an example, consider the three discourse
trees (DTs) shown in Figure 1: (a) for a reference
(human) translation, and (b) and (c) for transla-
tions of two different systems on the WMT12 test
dataset. The leaves of a DT correspond to con-
tiguous atomic text spans, called Elementary Dis-
course Units or EDUs (three in Figure 1a). Ad-
jacent spans are connected by certain coherence
relations (e.g., Elaboration, Attribution), forming
larger discourse units, which in turn are also sub-
ject to this relation linking. Discourse units linked
by a relation are further distinguished based on
their relative importance in the text: nuclei are
the core parts of the relation while satellites are
supportive ones. Note that the nuclearity and re-
lation labels in the reference translation are also
realized in the system translation in (b), but not
in (c), which makes (b) a better translation com-
pared to (c), according to our hypothesis. We ar-
gue that existing metrics that only use lexical and
syntactic information cannot distinguish well be-
tween (b) and (c).
In order to develop a discourse-aware evalua-
tion metric, we first generate discourse trees for
the reference and the system-translated sentences
using a discourse parser, and then we measure the
similarity between the two discourse trees. We de-
scribe these two steps below.
</bodyText>
<subsectionHeader confidence="0.999472">
3.1 Generating Discourse Trees
</subsectionHeader>
<bodyText confidence="0.999904541666667">
In Rhetorical Structure Theory, discourse analysis
involves two subtasks: (i) discourse segmentation,
or breaking the text into a sequence of EDUs, and
(ii) discourse parsing, or the task of linking the
units (EDUs and larger discourse units) into la-
beled discourse trees. Recently, Joty et al. (2012)
proposed discriminative models for both discourse
segmentation and discourse parsing at the sen-
tence level. The segmenter uses a maximum en-
tropy model that achieves state-of-the-art accuracy
on this task, having an F1-score of 90.5%, while
human agreement is 98.3%.
The discourse parser uses a dynamic Condi-
tional Random Field (Sutton et al., 2007) as a pars-
ing model in order to infer the probability of all
possible discourse tree constituents. The inferred
(posterior) probabilities are then used in a proba-
bilistic CKY-like bottom-up parsing algorithm to
find the most likely DT. Using the standard set
of 18 coarse-grained relations defined in (Carlson
and Marcu, 2001), the parser achieved an F1-score
of 79.8%, which is very close to the human agree-
ment of 83%. These high scores allowed us to de-
velop successful discourse similarity metrics.2
</bodyText>
<subsectionHeader confidence="0.999933">
3.2 Measuring Similarity
</subsectionHeader>
<bodyText confidence="0.9999899375">
A number of metrics have been proposed to mea-
sure the similarity between two labeled trees, e.g.,
Tree Edit Distance (Tai, 1979) and Tree Kernels
(Collins and Duffy, 2001; Moschitti and Basili,
2006). Tree kernels (TKs) provide an effective
way to integrate arbitrary tree structures in kernel-
based machine learning algorithms like SVMs.
In the present work, we use the convolution TK
defined in (Collins and Duffy, 2001), which effi-
ciently calculates the number of common subtrees
in two trees. Note that this kernel was originally
designed for syntactic parsing, where the subtrees
are subject to the constraint that their nodes are
taken with either all or none of the children. This
constraint of the TK imposes some limitations on
the type of substructures that can be compared.
</bodyText>
<footnote confidence="0.983649">
2The discourse parser is freely available from
http://alt.qcri.org/tools/
</footnote>
<page confidence="0.995147">
689
</page>
<figure confidence="0.998897666666667">
ElaborationROOT
SPAN Satellite
Attribution Nucleus
Voices are coming from Germany , SPANSatellite SPANNucleus
suggesting that ECB be the last resort creditor .
(b) A higher quality system translation.
</figure>
<figureCaption confidence="0.980857">
Figure 1: Example of three different discourse trees for the translations of a source sentence. (a) The
reference, (b) A higher quality translation, (c) A lower quality translation.
</figureCaption>
<figure confidence="0.9943493">
SP`NSatellite
(a) A reference (human) translation.
ElaborationROOT
(c) A lower quality system translation.
In Germany the ECB should be for the creditors of last resort .
In Germany voices,
SP`NSatellite SP`NNucleus
suggest the ECB should be lender of the last resort.
`ttribution Nucleus
SPANROOT
</figure>
<bodyText confidence="0.9998583125">
One way to cope with the limitations of the TK
is to change the representation of the trees to a
form that is suitable to capture the relevant infor-
mation for our task. We experiment with TKs ap-
plied to two different representations of the dis-
course tree: non-lexicalized (DR), and lexicalized
(DR-LEX). In Figure 2 we show the two represen-
tations for the subtree that spans the text: “sug-
gest the ECB should be the lender of last resort”,
which is highlighted in Figure 1b.
As shown in Figure 2a, DR does not include any
lexical item, and therefore measures the similar-
ity between two translations in terms of their dis-
course structures only. On the contrary, DR-LEX
includes the lexical items to account for lexical
matching; moreover, it separates the structure (the
skeleton) of the tree from its labels, i.e. the nucle-
arity and the relations, in order to allow the tree
kernel to give partial credit to subtrees that differ
in labels but match in their skeletons. More specif-
ically, it uses the tags SPAN and EDU to build the
skeleton of the tree, and considers the nuclearity
and/or the relation labels as properties, added as
children, of these tags.
For example, a SPAN has two properties (its
nuclearity and its relation), and an EDU has one
property (its nuclearity). The words of an EDU
are placed under the predefined children NGRAM.
In order to allow the tree kernel to find subtree
matches at the word level, we include an additional
layer of dummy leaves as was done in (Moschitti
et al., 2007); not shown in Figure 2, for simplicity.
</bodyText>
<sectionHeader confidence="0.998868" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999996428571429">
In our experiments, we used the data available for
the WMT12 and the WMT11 metrics shared tasks
for translations into English.3 This included the
output from the systems that participated in the
WMT12 and the WMT11 MT evaluation cam-
paigns, both consisting of 3,003 sentences, for
four different language pairs: Czech-English (CS-
EN), French-English (FR-EN), German-English
(DE-EN), and Spanish-English (ES-EN); as well as
a dataset with the English references.
We measured the correlation of the metrics with
the human judgments provided by the organizers.
The judgments represent rankings of the output
of five systems chosen at random, for a particu-
lar sentence, also chosen at random. Note that
each judgment effectively constitutes 10 pairwise
system rankings. The overall coverage, i.e. the
number of unique sentences that were evaluated,
was only a fraction of the total; the total number
of judgments, along with other information of the
datasets are shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.994015">
4.1 MT Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999291833333333">
In this study, we evaluate to what extent existing
evaluation metrics can benefit from additional dis-
course information. To do so, we contrast different
MT evaluation metrics with and without discourse
information. The evaluation metrics we used are
described below.
</bodyText>
<footnote confidence="0.917268">
3http://www.statmt.org/wmt{11,121/results.html
</footnote>
<page confidence="0.978173">
690
</page>
<figure confidence="0.998294384615384">
SPAN
(a) DT for DR
NUC
NUC NGRAM NUC NGRAM
Satellite suggest Nucleus the ECB should be lender of the last resort .
(b) DT for DR-LEX
Attribution
Nucleus
SPANSatellite SPAN
Nucleus
REL
EDU EDU
Nucleus Attribution
</figure>
<figureCaption confidence="0.989986">
Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b.
</figureCaption>
<table confidence="0.9214485">
WMT12 WMT11
systs ranks sents judges systs ranks sents judges
CS-EN 6 1,294 951 45 8 498 171 20
DE-EN 16 1,427 975 47 20 924 303 31
ES-EN 12 1,141 923 45 15 570 207 18
FR-EN 15 1,395 949 44 18 708 249 32
</table>
<tableCaption confidence="0.995625">
Table 1: Number of systems (systs), judgments
</tableCaption>
<bodyText confidence="0.998021258064516">
(ranks), unique sentences (sents), and different
judges (judges) for the different language pairs, for
the human evaluation of the WMT12 and WMT11
shared tasks.
Metrics from WMT12. We used the publicly
available scores for all metrics that participated
in the WMT12 metrics task (Callison-Burch et
al., 2012): SPEDE07PP, AMBER, METEOR,
TERRORCAT, SIMPBLEU, XENERRCATS,
WORDBLOCKEC, BLOCKERRCATS, and POSF.
Metrics from ASIYA. We used the freely avail-
able version of the ASIYA toolkit4 in order to ex-
tend the set of evaluation measures contrasted in
this study beyond those from the WMT12 metrics
task. ASIYA (Gim´enez and M`arquez, 2010a) is a
suite for MT evaluation that provides a large set of
metrics that use different levels of linguistic infor-
mation. For reproducibility, below we explain the
individual metrics with the exact names required
by the toolkit to calculate them.
First, we used ASIYA’s ULC (Gim´enez and
M`arquez, 2010b), which was the best performing
metric at the system and the segment levels at the
WMT08 and WMT09 metrics tasks. This is a uni-
form linear combination of 12 individual metrics.
From the original ULC, we only replaced TER and
Meteor individual metrics by newer versions tak-
ing into account synonymy lookup and paraphras-
ing: TERp-A and METEOR-pa in ASIYA’s termi-
nology. We will call this combined metric Asiya-
0809 in our experiments.
</bodyText>
<footnote confidence="0.8636">
4http://nlp.lsi.upc.edu/asiya/
</footnote>
<bodyText confidence="0.9999015">
To complement the set of individual metrics
that participated at the WMT12 metrics task, we
also computed the scores of other commonly-
used evaluation metrics: BLEU (Papineni et al.,
2002), NIST (Doddington, 2002), TER (Snover
et al., 2006), ROUGE-W (Lin, 2004), and three
METEOR variants (Denkowski and Lavie, 2011):
METEOR-ex (exact match), METEOR-st (+stem-
ming) and METEOR-sy (+synonyms). The uni-
form linear combination of the previous 7 indi-
vidual metrics plus the 12 from Asiya-0809 is re-
ported as Asiya-ALL in the experimental section.
The individual metrics combined in Asiya-ALL
can be naturally categorized according to the type
of linguistic information they use to compute the
quality scores. We grouped them in the follow-
ing four families and calculated the uniform linear
combination of the metrics in each group:5
</bodyText>
<listItem confidence="0.9979312">
1. Asiya-LEX. Combination of five metrics
based on lexical similarity: BLEU, NIST,
METEOR-ex, ROUGE-W, and TERp-A.
2. Asiya-SYN. Combination of four met-
rics ba-sed on syntactic information from
constituency and dependency parse trees:
‘CP-STM-4’, ‘DP-HWCM c-4’, ‘DP-
HWCM r-4’, and ‘DP-Or(*)’.
3. Asiya-SRL. Combination of three metric
variants based on predicate argument struc-
tures (semantic role labeling): ‘SR-Mr(*)’,
‘SR-Or(*)’, and ‘SR-Or’.
4. Asiya-SEM. Combination of two metrics
variants based on semantic parsing:6 ‘DR-
Or(*)’ and ‘DR-Orp(*)’.
</listItem>
<footnote confidence="0.993827">
5A detailed description of every individual metric can be
found at (Gim´enez and M`arquez, 2010b). For a more up-to-
date description, see the User Manual from ASIYA’s website.
6In ASIYA the metrics from this family are referred to as
“Discourse Representation” metrics. However, the structures
they consider are actually very different from the discourse
structures exploited in this paper. See the discussion in Sec-
tion 2. For clarity, we will refer to them as semantic parsing
metrics.
</footnote>
<page confidence="0.995557">
691
</page>
<bodyText confidence="0.9994202">
All uniform linear combinations are calculated
outside ASIYA. In order to make the scores of
the different metrics comparable, we performed a
min–max normalization, for each metric, and for
each language pair combination.
</bodyText>
<subsectionHeader confidence="0.997966">
4.2 Human Judgements and Learning
</subsectionHeader>
<bodyText confidence="0.999182705882353">
The human-annotated data from the WMT cam-
paigns encompasses series of rankings on the out-
put of different MT systems for every source sen-
tence. Annotators rank the output of five systems
according to perceived translation quality. The or-
ganizers relied on a random selection of systems,
and a large number of comparisons between pairs
of them, to make comparisons across systems fea-
sible (Callison-Burch et al., 2012). As a result,
for each source sentence, only relative rankings
were available. As in the WMT12 experimen-
tal setup, we use these rankings to calculate cor-
relation with human judgments at the sentence-
level, i.e. Kendall’s Tau; see (Callison-Burch et
al., 2012) for details.
For the experiments reported in Section 5.4, we
used pairwise rankings to discriminatively learn
the weights of the linear combinations of indi-
vidual metrics. In order to use the WMT12 data
for training a learning-to-rank model, we trans-
formed the five-way relative rankings into ten
pairwise comparisons. For instance, if a judge
ranked the output of systems A, B, C, D, E
as A &gt; B &gt; C &gt; D &gt; E, this would entail that
A &gt; B, A &gt; C, A &gt; D and A &gt; E, etc.
To determine the relative weights for the tuned
combinations, we followed a similar approach to
the one used by PRO to tune the relative weights of
the components of a log-linear SMT model (Hop-
kins and May, 2011), also using Maximum En-
tropy as the base learning algorithm. Unlike
PRO, (i) we use human judgments, not automatic
scores, and (ii) we train on all pairs, not on a sub-
sample.
</bodyText>
<sectionHeader confidence="0.998572" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999961833333333">
In this section, we explore how discourse informa-
tion can be used to improve machine translation
evaluation metrics. Below we present the evalua-
tion results at the system- and segment-level, using
our two basic metrics on discourse trees (Section
3.1), which are referred to as DR and DR-LEX.
</bodyText>
<subsectionHeader confidence="0.923292">
5.1 Evaluation
</subsectionHeader>
<bodyText confidence="0.999978555555556">
In our experiments, we only consider translation
into English, and use the data described in Table 1.
For evaluation, we follow the setup of the metrics
task of WMT12 (Callison-Burch et al., 2012): at
the system-level, we use the official script from
WMT12 to calculate the Spearman’s correlation,
where higher absolute values indicate better met-
rics performance; at the segment-level, we use
Kendall’s Tau for measuring correlation, where
negative values are worse than positive ones.7
In our experiments, we combine DR and
DR-LEX to other metrics in two different ways:
using uniform linear interpolation (at system- and
segment-level), and using a tuned linear interpo-
lation for the segment-level. We only present the
average results over all four language pairs. For
simplicity, in our tables we show results divided
into evaluation groups:
</bodyText>
<listItem confidence="0.979550571428571">
1. Group I: contains our evaluation metrics, DR
and DR-LEX.
2. Group II: includes the metrics that partici-
pated in the WMT12 metrics task, excluding
metrics which did not have results for all lan-
guage pairs.
3. Group III: contains other important evalu-
ation metrics, which were not considered
in the WMT12 metrics task: NIST and
ROUGE for both system- and segment-level,
and BLEU and TER at segment-level.
4. Group IV: includes the metric combinations
calculated with ASIYA and described in Sec-
tion 4.
</listItem>
<bodyText confidence="0.934894928571429">
For each metric in groups II, III and IV, we
present the results for the original metric as well
for the linear interpolation of that metric with DR
and with DR-LEX. The combinations with DR
and DR-LEX that improve over the original met-
rics are shown in bold, and those that degrade are
in italic. Furthermore, we also present overall re-
sults for: (i) the average score over all metrics, ex-
cluding DR and DR-LEX, and (ii) the differences
in the correlations for the DR/DR-LEX-combined
and the original metrics.
7We have fixed a bug in the scoring tool from WMT12,
which was making all scores positive. This made
TERRORCAT’s score negative, as we present it in Table 3.
</bodyText>
<page confidence="0.994701">
692
</page>
<table confidence="0.999837461538462">
Metrics +DR +DR-LEX
DR .807 – –
I DR-LEX .876 – –
SEMPOS .902 .853 .903
AMBER .857 .829 .869
METEOR .834 .861 .888
TERRORCAT .831 .854 .889
SIMPBLEU .823 .826 .859
II TER .812 .836 .848
BLEU .810 .830 .846
POSF .754 .841 .857
BLOCKERRCATS .751 .859 .855
WORDBLOCKEC .738 .822 .843
XENERRCATS .735 .819 .843
NIST .817 .842 .875
III .884 .899 .869
ROUGE
Asiya-LEX .879 .881 .882
Asiya-SYN .891 .913 .883
Asiya-SRL .917 .911 .909
IV .891 .889 .886
Asiya-SEM
Asiya-0809 .905 .914 .905
Asiya-ALL .899 .907 .896
average .839 .862 .874
diff. +.024 +.035
</table>
<tableCaption confidence="0.899616">
Table 2: Results on WMT12 at the system-level.
Spearman’s correlation with human judgments.
</tableCaption>
<subsectionHeader confidence="0.998183">
5.2 System-level Results
</subsectionHeader>
<bodyText confidence="0.999981296296296">
Table 2 shows the system-level experimental re-
sults for WMT12. We can see that DR is already
competitive by itself: on average, it has a cor-
relation of .807, very close to BLEU and TER
scores (.810 and .812, respectively). Moreover,
DR yields improvements when combined with 15
of the 19 metrics; worsening only four of the met-
rics. Overall, we observe an average improvement
of +.024, in the correlation with the human judg-
ments. This suggests that DR contains information
that is complementary to that used by the other
metrics. Note that this is true both for the indi-
vidual metrics from groups II and III, as well as
for the metric combinations in group IV. Combi-
nations in the last group involve several metrics
that already use linguistic information at different
levels and are hard to improve over; yet, adding
DR does improve, which shows that it has some
complementary information to offer.
As expected, DR-LEX performs better than DR
since it is lexicalized (at the unigram level), and
also gives partial credit to correct structures. Indi-
vidually, DR-LEX outperforms most of the metrics
from group II, and ranks as the second best metric
in that group. Furthermore, when combined with
individual metrics in group II, DR-LEX is able to
improve consistently over each one of them.
</bodyText>
<table confidence="0.9992135">
Metrics +DR +DR-LEX
DR -.433 – –
I DR-LEX .133 – –
SPEDE07PP .254 .190 .223
METEOR .247 .178 .217
AMBER .229 .180 .216
SIMPBLEU .172 .141 .191
II XENERRCATS .165 .132 .185
POSF .154 .125 .201
WORDBLOCKEC .153 .122 .181
BLOCKERRCATS .074 .068 .151
TERRORCAT -.186 -.111 -.104
NIST .214 .172 .206
ROUGE .185 .144 .201
III .217 .179 .229
TER
BLEU .185 .154 .190
Asiya-LEX .254 .237 .253
Asiya-SYN .177 .169 .191
Asiya-SRL -.023 .015 .161
IV .134 .152 .197
Asiya-SEM
Asiya-0809 .254 .250 .258
Asiya-ALL .268 .265 .270
average .165 .145 .190
diff. -.019 +.026
</table>
<tableCaption confidence="0.8950355">
Table 3: Results on WMT12 at the segment-level.
Kendall’s Tau with human judgments.
</tableCaption>
<bodyText confidence="0.99990375">
Note that, even though DR-LEX has better indi-
vidual performance than DR, it does not yield im-
provements when combined with most of the met-
rics in group IV.8 However, over all metrics and all
language pairs, DR-LEX is able to obtain an aver-
age improvement in correlation of +.035, which is
remarkably higher than that of DR. Thus, we can
conclude that at the system-level, adding discourse
information to a metric, even using the simplest of
the combination schemes, is a good idea for most
of the metrics, and can help to significantly im-
prove the correlation with human judgments.
</bodyText>
<subsectionHeader confidence="0.998107">
5.3 Segment-level Results: Non-tuned
</subsectionHeader>
<bodyText confidence="0.9999235">
Table 3 shows the results for WMT12 at the
segment-level. We can see that DR performs
badly, with a high negative Kendall’s Tau of -.433.
This should not be surprising: (a) the discourse
tree structure alone does not contain enough infor-
mation for a good evaluation at the segment-level,
and (b) this metric is more sensitive to the quality
of the DT, which can be wrong or void.
</bodyText>
<footnote confidence="0.844203125">
8In this work, we have not investigated the reasons behind
this phenomenon. We speculate that this might be caused by
the fact that the lexical information in DR-LEX is incorpo-
rated only in the form of unigram matching at the sentence-
level, while the metrics in group IV are already complex com-
bined metrics, which take into account stronger lexical mod-
els. Note, however, that the variations are very small and
might not be significant.
</footnote>
<page confidence="0.996826">
693
</page>
<table confidence="0.999891518518519">
Metrics Orig. Tuned
+DR +DR-LEX
DR -.433 – – –
I DR-LEX .133 – – –
SPEDE07PP .254 – .253 .254
METEOR .247 – .250 .251
AMBER .229 – .230 .232
SIMPBLEU .172 – .181 .199
II TERRORCAT -.186 – .181 .196
XENERRCATS .165 – .175 .194
POSF .154 – .160 .201
WORDBLOCKEC .153 – .161 .189
BLOCKERRCATS .074 – .087 .150
NIST .214 – .222 .224
ROUGE .185 – .196 .218
III .217 – .229 .246
TER
BLEU .185 – .189 .194
Asiya-LEX .254 .266 .269 .270
Asiya-SYN .177 .229 .228 .232
Asiya-SRL -.023 -.004 .039 .181
IV .134 .146 .179 .202
Asiya-SEM
Asiya-0809 .254 .295 .295 .295
Asiya-ALL .268 .296 .295 .295
average .165 .201 .222
diff. +.036 +.057
</table>
<tableCaption confidence="0.993135">
Table 4: Results on WMT12 at the segment-
</tableCaption>
<bodyText confidence="0.98398925">
level: tuning with cross-validation on WMT12.
Kendall’s Tau with human judgments.
Additionally, DR is more likely to produce a
high number of ties, which is harshly penalized
by WMT12’s definition of Kendall’s Tau. Con-
versely, ties and incomplete discourse analysis
were not a problem at the system-level, where ev-
idence from all 3,003 test sentences is aggregated,
and allows to rank systems more precisely. Due to
the low score of DR as an individual metric, it fails
to yield improvements when uniformly combined
with other metrics.
Again, DR-LEX is better than DR; with a pos-
itive Tau of +.133, yet as an individual metric, it
ranks poorly compared to other metrics in group
II. However, when linearly combined with other
metrics, DR-LEX outperforms 14 of the 19 met-
rics in Table 3. Across all metrics, DR-LEX yields
an average Tau improvement of +.026, i.e. from
.165 to .190. This is a large improvement, taking
into account that the combinations are just uniform
linear combinations. In subsection 5.4, we present
the results of tuning the linear combination in a
discriminative way.
</bodyText>
<subsectionHeader confidence="0.999076">
5.4 Segment-level Results: Tuned
</subsectionHeader>
<bodyText confidence="0.999982066666667">
We experimented with tuning the weights of the
individual metrics in the metric combinations, us-
ing the learning method described in Section 4.2.
First, we did this using cross-validation to tune
and test on WMT12. Later we tuned on WMT12
and evaluated on WMT11. For cross-validation
in WMT12, we used ten folds of approximately
equal sizes, each containing about 300 sentences:
we constructed the folds by putting together en-
tire documents, thus not allowing sentences from
a document to be split over two different folds.
During each cross-validation run, we trained our
pairwise ranker using the human judgments cor-
responding to nine of the ten folds. We aggre-
gated the data for different language pairs, and
produced a single set of tuning weights for all lan-
guage pairs.9 We then used the remaining fold for
evaluation
The results are shown in Table 4. As in previ-
ous sections we present the average results over
all four language pairs. We can see that the tuned
combinations with DR-LEX improve over most of
the individual metrics in groups II and III. Inter-
estingly, the tuned combinations that include the
much weaker metric DR now improve over 12 out
of 13 of the individual metrics in groups II and III,
and only slightly degrades the score of the 13th
one (SPEDE07PP).
Note that the ASIYA metrics are combinations
of several metrics, and these combinations (which
exclude DR and DR-LEX) can be also tuned; this
yields sizable improvements over the untuned ver-
sions as column three in the table shows. Com-
pared to this baseline, DR improves for three of
the six ASIYA metrics, while DR-LEX improves
for four of them. Note that improving over the
last two ASIYA metrics is very hard: they have
very high scores of .296 and .295; for compar-
ison, the best segment-level system at WMT12
(SPEDE07PP) achieved a Tau of .254.
On average, DR improves Tau from .165 to
.201, which is +.036, while DR-LEX improves to
.222, or +.057. These much larger improvements
highlight the importance of tuning the linear com-
bination when working at the segment-level.
</bodyText>
<subsectionHeader confidence="0.927663">
5.4.1 Testing on WMT11
</subsectionHeader>
<bodyText confidence="0.9838856">
In order to rule out the possibility that the im-
provement of the tuned metrics on WMT12 comes
from over-fitting, and to verify that the tuned met-
rics do generalize when applied to other sentences,
we also tested on a new test set: WMT11.
</bodyText>
<footnote confidence="0.994894">
9Tuning separately for each language pair yielded slightly
lower results.
</footnote>
<page confidence="0.997434">
694
</page>
<bodyText confidence="0.999983743589744">
Therefore, we tuned the weights on all WMT12
pairwise judgments (no cross-validation), and we
evaluated on WMT11. Since the metrics that par-
ticipated in WMT11 and WMT12 are different
(and even when they have the same name, there
is no guarantee that they have not changed from
2011 to 2012), we only report results for the ver-
sions of NIST, ROUGE, TER, and BLEU available
in ASIYA, as well as for the ASIYA metrics, thus
ensuring that the metrics in the experiments are
consistent for 2011 and 2012.
The results are shown in Table 5. Once again,
tuning yields sizable improvements over the sim-
ple combination for the ASIYA metrics (third col-
umn in Table 5). Adding DR and DR-LEX to the
combinations manages to improve over five and
four of the six tuned ASIYA metrics, respectively.
However, some of the differences are very small.
On the contrary, DR and DR-LEX significantly im-
prove over NIST, ROUGE, TER, and BLEU. Over-
all, DR improves the average Tau from .207 to
.244, which is +.037, while DR-LEX improves to
.267 or +.061. These improvements are very close
to those for the WMT12 cross-validation. This
shows that the weights learned on WMT12 gen-
eralize well, as they are also good for WMT11.
What is also interesting to note is that when tun-
ing is used, DR helps achieve sizeable improve-
ments, even if not as strong as for DR-LEX. This
is remarkable given that DR has a strong negative
Tau as an individual metric at the sentence-level.
This suggests that both DR and DR-LEX contain
information that is complementary to that of the
individual metrics that we experimented with.
Overall, from the experimental results in this
section, we can conclude that discourse structure
is an important information source to be taken into
account in the automatic evaluation of machine
translation output.
</bodyText>
<sectionHeader confidence="0.988024" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9975007">
In this paper we have shown that discourse struc-
ture can be used to improve automatic MT evalua-
tion. First, we defined two simple discourse-aware
similarity metrics (lexicalized and un-lexicalized),
which use the all-subtree kernel to compute sim-
ilarity between discourse parse trees in accor-
dance with the Rhetorical Structure Theory. Then,
after extensive experimentation on WMT12 and
WMT11 data, we showed that a variety of ex-
isting evaluation metrics can benefit from our
</bodyText>
<table confidence="0.999110277777778">
Metrics Orig. Tuned
+DR +DR-LEX
DR -.447 – – –
I DR-LEX .146 – – –
NIST .219 – .226 .232
ROUGE .205 – .218 .242
III .262 – .274 .296
TER
BLEU .186 – .192 .207
Asiya-LEX .282 .301 .302 .303
Asiya-SYN .216 .259 .260 .260
Asiya-SRL -.004 .017 .051 .200
IV .189 .194 .220 .239
Asiya-SEM
Asiya-0809 .300 .348 .349 .348
Asiya-ALL .313 .347 .347 .347
average .207 .244 .267
diff. +.037 +.061
</table>
<tableCaption confidence="0.678033666666667">
Table 5: Results on WMT11 at the segment-level:
tuning on the entire WMT12. Kendall’s Tau with
human judgments.
</tableCaption>
<bodyText confidence="0.999886">
discourse-based metrics, both at the segment- and
the system-level, especially when the discourse in-
formation is incorporated in an informed way (i.e.
using supervised tuning). Our results show that
discourse-based metrics can improve the state-of-
the-art MT metrics, by increasing correlation with
human judgments, even when only sentence-level
discourse information is used.
Addressing discourse-level phenomena in MT
is a relatively new research direction. Yet, many
of the ongoing efforts have been moderately suc-
cessful according to traditional evaluation met-
rics. There is a consensus in the MT community
that more discourse-aware metrics need to be pro-
posed for this area to move forward. We believe
this work is a valuable contribution towards this
longer-term goal.
The tuned combined metrics tested in this pa-
per are just an initial proposal, i.e. a simple ad-
justment of the relative weights for the individ-
ual metrics in a linear combination. In the fu-
ture, we plan to work on integrated representations
of syntactic, semantic and discourse-based struc-
tures, which would allow us to train evaluation
metrics based on more fine-grained features. Ad-
ditionally, we propose to use the discourse infor-
mation for MT in two different ways. First, at the
sentence-level, we can use discourse information
to re-rank alternative MT hypotheses; this could
be applied either for MT parameter tuning, or as a
post-processing step for the MT output. Second,
we propose to move in the direction of using dis-
course information beyond the sentence-level.
</bodyText>
<page confidence="0.998669">
695
</page>
<sectionHeader confidence="0.996147" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999707320754717">
Nicholas Asher and Alex Lascarides, 2003. Logics of
Conversation. Cambridge University Press.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
workshop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 22–64, Edinburgh, Scot-
land, July. ACL.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 workshop on statistical ma-
chine translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10–51, Montr´eal, Canada, June. ACL.
Lynn Carlson and Daniel Marcu. 2001. Discourse Tag-
ging Reference Manual. Technical Report ISI-TR-
545, University of Southern California Information
Sciences Institute.
Bruno Cartoni, Sandrine Zufferey, Thomas Meyer, and
Andrei Popescu-Belis. 2011. How comparable are
parallel corpora? measuring the distribution of gen-
eral vocabulary and connectives. In Proceedings of
the 4th Workshop on Building and Using Compa-
rable Corpora: Comparable Corpora and the Web,
pages 78–86, Portland, Oregon, June. ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP’08), Honolulu, Hawaii,
USA.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, ACL ’05, pages 263–
270, Ann Arbor, Michigan.
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Neural Informa-
tion Processing Systems, NIPS’01, pages 625–632,
Vancouver, Canada.
Elisabet Comelles, Jes´us Gim´enez, Llu´ıs M`arquez,
Irene Castell´on, and Victoria Arranz. 2010.
Document-level automatic mt evaluation based on
discourse representations. In Proceedings of the
Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, pages 333–338, Uppsala,
Sweden, July. ACL.
Michael Denkowski and Alon Lavie. 2011. Meteor
1.3: Automatic metric for reliable optimization and
evaluation of machine translation systems. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 85–91, Edinburgh, Scot-
land, July. ACL.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the Sec-
ond International Conference on Human Language
Technology Research, HLT ’02, pages 138–145, San
Francisco, CA, USA. Morgan Kaufmann Publishers
Inc.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What’s in a translation rule?
In Proceedings of the 2004 Conference of the North
American Chapter of the Association for Compu-
tational Linguistics: Human Language Technology,
HLT-NAACL, pages 273–280.
Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Linguis-
tic features for automatic evaluation of heterogenous
MT systems. In Proceedings of the Second Work-
shop on Statistical Machine Translation, pages 256–
264, Prague, Czech Republic, June. ACL.
Jes´us Gim´enez and Llu´ıs M`arquez. 2009. On the ro-
bustness of syntactic and semantic features for auto-
matic MT evaluation. In Proceedings of the Fourth
Workshop on Statistical Machine Translation, pages
250–258, Athens, Greece, March. ACL.
Jes´us Gim´enez and Llu´ıs M`arquez. 2010a. Asiya:
an Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, 94:77–86.
Jes´us Gim´enez and Llu´ıs M`arquez. 2010b. Linguistic
Measures for Automatic Machine Translation Eval-
uation. Machine Translation, 24(3–4):77–86.
Michael Halliday and Ruqaiya Hasan, 1976. Cohesion
in English. Longman, London.
Christian Hardmeier and Marcello Federico. 2010.
Modelling pronominal anaphora in statistical ma-
chine translation. In Proceedings of the Interna-
tional Workshop on Spoken Language Translation,
pages 283–289.
Christian Hardmeier, Joakim Nivre, and J¨org Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL ’12, pages 1179–1190, Jeju Island, Korea.
ACL.
Christian Hardmeier. 2012. Discourse in statistical
machine translation. a survey and a case study. Dis-
cours. Revue de linguistique, psycholinguistique et
informatique, 11(8726).
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’11.
</reference>
<page confidence="0.985958">
696
</page>
<reference confidence="0.999854660550459">
Shafiq Joty, Giuseppe Carenini, and Raymond T. Ng.
2012. A Novel Discriminative Framework for
Sentence-Level Discourse Analysis. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’12,
pages 904–915, Jeju Island, Korea. ACL.
Shafiq Joty, Giuseppe Carenini, Raymond T. Ng, and
Yashar Mehdad. 2013. Combining Intra- and
Multi-sentential Rhetorical Parsing for Document-
level Discourse Analysis. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics, ACL ’13, pages 486–496, Sofia,
Bulgaria. ACL.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic: Introduction to Model theoretic Semantics
of Natural Language, Formal Logic and Discourse
Representation Theory. Number 42 in Studies in
Linguistics and Philosophy. Kluwer Academic Pub-
lishers.
Chin-Yew Lin. 2004. ROUGE: A Package for Au-
tomatic Evaluation of Summaries. In Proceedings
of Workshop on Text Summarization Branches Out,
pages 74–81, Barcelona.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization, pages 25–32, Ann Ar-
bor, Michigan, June. ACL.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
2012. Fully automatic semantic mt evaluation. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 243–252, Montr´eal,
Canada, June. ACL.
William Mann and Sandra Thompson. 1988. Rhetor-
ical Structure Theory: Toward a Functional Theory
of Text Organization. Text, 8(3):243–281.
Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui,
and Andrea Gesmundo. 2012. Machine translation
of labeled discourse connectives. In Proceedings of
the Tenth Biennial Conference of the Association for
Machine Translation in the Americas (AMTA).
Thomas Meyer. 2011. Disambiguating temporal-
contrastive connectives for machine translation. In
Proceedings of the ACL 2011 Student Session, pages
46–51, Portland, OR, USA, June. ACL.
Alessandro Moschitti and Roberto Basili. 2006. A
Tree Kernel approach to Question and Answer Clas-
sification in Question Answering Systems. In Pro-
ceedings of the 5th international conference on Lan-
guage Resources and Evaluation, Genoa, Italy.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploiting
Syntactic and Shallow Semantic Kernels for Ques-
tion/Answer Classification. In Proceedings of the
ACL-2007, pages 776–783, Prague, Czech Repub-
lic.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the Association for Computational Linguistics
(ACL’02), Philadelphia, PA, USA.
Maja Popovic and Hermann Ney. 2007. Word error
rates: Decomposition over POS classes and applica-
tions for error analysis. In Proceedings of the Sec-
ond Workshop on Statistical Machine Translation,
pages 48–55, Prague, Czech Republic, June. ACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal smt. In Proceedings of the 43rd
Annual Meeting on Association for Computational
Linguistics, ACL ’05, pages 271–279, Ann Arbor,
Michigan.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference of the
Association for Machine Translation in the Ameri-
cas, AMTA ’06, Cambridge, MA, USA.
Charles Sutton, Andrew McCallum, and Khashayar
Rohanimanesh. 2007. Dynamic Conditional Ran-
dom Fields: Factorized Probabilistic Models for La-
beling and Segmenting Sequence Data. Journal of
Machine Learning Research (JMLR), 8:693–723.
Kuo-Chung Tai. 1979. The tree-to-tree correction
problem. Journal of the ACM, 26(3):422–433, July.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proceedings
of the 2007 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ’07,
Prague, Czech Republic.
Bonnie Webber, Andrei Popescu-Belis, Katja Markert,
and J¨org Tiedemann, editors. 2013. Proceedings of
the Workshop on Discourse in Machine Translation.
ACL, Sofia, Bulgaria, August.
Bonnie Webber. 2004. D-LTAG: Extending Lex-
icalized TAG to Discourse. Cognitive Science,
28(5):751–779.
Billy T. M. Wong and Chunyu Kit. 2012. Ex-
tending machine translation evaluation metrics with
lexical cohesion to document level. In Proceed-
ings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, EMNLP-
CoNLL, pages 1060–1068, Jeju Island, Korea, July.
ACL.
</reference>
<page confidence="0.976256">
697
</page>
<reference confidence="0.998738285714286">
Dekai Wu and Pascale Fung. 2009. Semantic roles
for smt: A hybrid two-pass model. In Proceed-
ings of Human Language Technologies: The 2009
Annual Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics, Companion Volume: Short Papers, pages 13–
16, Boulder, Colorado, June.
</reference>
<page confidence="0.997291">
698
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.816966">
<title confidence="0.999977">Using Discourse Structure Improves Machine Translation Evaluation</title>
<author confidence="0.994729">Guzm´an Shafiq Joty Lluis M`arquez</author>
<affiliation confidence="0.9085945">ALT Research Qatar Computing Research Institute — Qatar</affiliation>
<abstract confidence="0.999738722222222">We present experiments in using discourse structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segmentand at the system-level. Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should be taken into account in the development of future richer evaluation metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nicholas Asher</author>
<author>Alex Lascarides</author>
</authors>
<title>Logics of Conversation.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2817" citStr="Asher and Lascarides, 2003" startWordPosition="423" endWordPosition="426">rtant information that allows the text to express a meaning as a whole beyond the sum of its separate parts. Note that sentences can be made of several clauses, which in turn can be interrelated through the same logical relations. Thus, in a coherent text, discourse units (sentences or clauses) are logically connected: the meaning of a unit relates to that of the previous and the following units. Discourse analysis seeks to uncover this coherence structure underneath the text. Several formal theories of discourse have been proposed to describe the coherence structure (Mann and Thompson, 1988; Asher and Lascarides, 2003; Webber, 2004). For example, the Rhetorical Structure Theory (Mann and Thompson, 1988), or RST, represents text by labeled hierarchical structures called Discourse Trees (DTs), which can incorporate several layers of other linguistic information, e.g., syntax, predicate-argument structure, etc. Modeling discourse brings together the above research directions (a) and (b), which makes it an attractive goal for MT. This is demonstrated by the establishment of a recent workshop dedicated to Discourse in Machine Translation (Webber et al., 2013), collocated with the 2013 annual meeting of the Asso</context>
</contexts>
<marker>Asher, Lascarides, 2003</marker>
<rawString>Nicholas Asher and Alex Lascarides, 2003. Logics of Conversation. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Omar Zaidan</author>
</authors>
<title>Findings of the 2011 workshop on statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>22--64</pages>
<publisher>ACL.</publisher>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="5069" citStr="Callison-Burch et al., 2011" startWordPosition="771" endWordPosition="774">herence relations in the source language when generating target-language translations. In this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored. We first design two discourse-aware similarity measures, which use DTs generated by a publiclyavailable discourse parser (Joty et al., 2012); then, we show that they can help improve a number of MT evaluation metrics at the segment- and at the system-level in the context of the WMT11 and the WMT12 metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012). These metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanis</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Zaidan, 2011</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan. 2011. Findings of the 2011 workshop on statistical machine translation. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 22–64, Edinburgh, Scotland, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<title>Findings of the 2012 workshop on statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>10--51</pages>
<publisher>ACL.</publisher>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="5099" citStr="Callison-Burch et al., 2012" startWordPosition="775" endWordPosition="778">ce language when generating target-language translations. In this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored. We first design two discourse-aware similarity measures, which use DTs generated by a publiclyavailable discourse parser (Joty et al., 2012); then, we show that they can help improve a number of MT evaluation metrics at the segment- and at the system-level in the context of the WMT11 and the WMT12 metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012). These metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and Ma</context>
<context position="7274" citStr="Callison-Burch et al., 2012" startWordPosition="1107" endWordPosition="1110">rmed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judgments. The field of automatic evaluation metrics for MT is very active, and new metrics are continuously being proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For example, at WMT12, 12 metrics were compared (Callison-Burch et al., 2012), most of them new. There have been several attempts to incorporate syntactic and semantic linguistic knowledge into MT evaluation. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007) or between constituency trees (Liu and Gildea, 2005). In the semantic case, there are metrics that exploit the similarity over named entities and predicate-argument structures (Gim´enez and M`arquez, 2007; Lo et al., 2012). In this work, instead of proposing a new metric, we focus on e</context>
<context position="17868" citStr="Callison-Burch et al., 2012" startWordPosition="2792" endWordPosition="2795">nt DT representations for the highlighted subtree shown in Figure 1b. WMT12 WMT11 systs ranks sents judges systs ranks sents judges CS-EN 6 1,294 951 45 8 498 171 20 DE-EN 16 1,427 975 47 20 924 303 31 ES-EN 12 1,141 923 45 15 570 207 18 FR-EN 15 1,395 949 44 18 708 249 32 Table 1: Number of systems (systs), judgments (ranks), unique sentences (sents), and different judges (judges) for the different language pairs, for the human evaluation of the WMT12 and WMT11 shared tasks. Metrics from WMT12. We used the publicly available scores for all metrics that participated in the WMT12 metrics task (Callison-Burch et al., 2012): SPEDE07PP, AMBER, METEOR, TERRORCAT, SIMPBLEU, XENERRCATS, WORDBLOCKEC, BLOCKERRCATS, and POSF. Metrics from ASIYA. We used the freely available version of the ASIYA toolkit4 in order to extend the set of evaluation measures contrasted in this study beyond those from the WMT12 metrics task. ASIYA (Gim´enez and M`arquez, 2010a) is a suite for MT evaluation that provides a large set of metrics that use different levels of linguistic information. For reproducibility, below we explain the individual metrics with the exact names required by the toolkit to calculate them. First, we used ASIYA’s UL</context>
<context position="21504" citStr="Callison-Burch et al., 2012" startWordPosition="3354" endWordPosition="3357">d outside ASIYA. In order to make the scores of the different metrics comparable, we performed a min–max normalization, for each metric, and for each language pair combination. 4.2 Human Judgements and Learning The human-annotated data from the WMT campaigns encompasses series of rankings on the output of different MT systems for every source sentence. Annotators rank the output of five systems according to perceived translation quality. The organizers relied on a random selection of systems, and a large number of comparisons between pairs of them, to make comparisons across systems feasible (Callison-Burch et al., 2012). As a result, for each source sentence, only relative rankings were available. As in the WMT12 experimental setup, we use these rankings to calculate correlation with human judgments at the sentencelevel, i.e. Kendall’s Tau; see (Callison-Burch et al., 2012) for details. For the experiments reported in Section 5.4, we used pairwise rankings to discriminatively learn the weights of the linear combinations of individual metrics. In order to use the WMT12 data for training a learning-to-rank model, we transformed the five-way relative rankings into ten pairwise comparisons. For instance, if a ju</context>
<context position="23146" citStr="Callison-Burch et al., 2012" startWordPosition="3644" endWordPosition="3647">i) we use human judgments, not automatic scores, and (ii) we train on all pairs, not on a subsample. 5 Experimental Results In this section, we explore how discourse information can be used to improve machine translation evaluation metrics. Below we present the evaluation results at the system- and segment-level, using our two basic metrics on discourse trees (Section 3.1), which are referred to as DR and DR-LEX. 5.1 Evaluation In our experiments, we only consider translation into English, and use the data described in Table 1. For evaluation, we follow the setup of the metrics task of WMT12 (Callison-Burch et al., 2012): at the system-level, we use the official script from WMT12 to calculate the Spearman’s correlation, where higher absolute values indicate better metrics performance; at the segment-level, we use Kendall’s Tau for measuring correlation, where negative values are worse than positive ones.7 In our experiments, we combine DR and DR-LEX to other metrics in two different ways: using uniform linear interpolation (at system- and segment-level), and using a tuned linear interpolation for the segment-level. We only present the average results over all four language pairs. For simplicity, in our tables</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 workshop on statistical machine translation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 10–51, Montr´eal, Canada, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynn Carlson</author>
<author>Daniel Marcu</author>
</authors>
<title>Discourse Tagging Reference Manual.</title>
<date>2001</date>
<tech>Technical Report ISI-TR545,</tech>
<institution>University of Southern California Information Sciences Institute.</institution>
<contexts>
<context position="12372" citStr="Carlson and Marcu, 2001" startWordPosition="1897" endWordPosition="1900">segmentation and discourse parsing at the sentence level. The segmenter uses a maximum entropy model that achieves state-of-the-art accuracy on this task, having an F1-score of 90.5%, while human agreement is 98.3%. The discourse parser uses a dynamic Conditional Random Field (Sutton et al., 2007) as a parsing model in order to infer the probability of all possible discourse tree constituents. The inferred (posterior) probabilities are then used in a probabilistic CKY-like bottom-up parsing algorithm to find the most likely DT. Using the standard set of 18 coarse-grained relations defined in (Carlson and Marcu, 2001), the parser achieved an F1-score of 79.8%, which is very close to the human agreement of 83%. These high scores allowed us to develop successful discourse similarity metrics.2 3.2 Measuring Similarity A number of metrics have been proposed to measure the similarity between two labeled trees, e.g., Tree Edit Distance (Tai, 1979) and Tree Kernels (Collins and Duffy, 2001; Moschitti and Basili, 2006). Tree kernels (TKs) provide an effective way to integrate arbitrary tree structures in kernelbased machine learning algorithms like SVMs. In the present work, we use the convolution TK defined in (C</context>
</contexts>
<marker>Carlson, Marcu, 2001</marker>
<rawString>Lynn Carlson and Daniel Marcu. 2001. Discourse Tagging Reference Manual. Technical Report ISI-TR545, University of Southern California Information Sciences Institute.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bruno Cartoni</author>
<author>Sandrine Zufferey</author>
<author>Thomas Meyer</author>
<author>Andrei Popescu-Belis</author>
</authors>
<title>How comparable are parallel corpora? measuring the distribution of general vocabulary and connectives.</title>
<date>2011</date>
<booktitle>In Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web,</booktitle>
<pages>78--86</pages>
<publisher>ACL.</publisher>
<location>Portland, Oregon,</location>
<contexts>
<context position="6127" citStr="Cartoni et al., 2011" startWordPosition="930" endWordPosition="933">ce-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1We refer the reader to (Hardmeier, 2012) for an in-depth overview of discourse-related research for MT. A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direct</context>
</contexts>
<marker>Cartoni, Zufferey, Meyer, Popescu-Belis, 2011</marker>
<rawString>Bruno Cartoni, Sandrine Zufferey, Thomas Meyer, and Andrei Popescu-Belis. 2011. How comparable are parallel corpora? measuring the distribution of general vocabulary and connectives. In Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web, pages 78–86, Portland, Oregon, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
<author>Yuval Marton</author>
<author>Philip Resnik</author>
</authors>
<title>Online large-margin training of syntactic and structural translation features.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP’08),</booktitle>
<location>Honolulu, Hawaii, USA.</location>
<contexts>
<context position="5761" citStr="Chiang et al., 2008" startWordPosition="874" endWordPosition="877">e-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1We refer the reader to (Hardmeier, 2012) for an in-depth overview </context>
</contexts>
<marker>Chiang, Marton, Resnik, 2008</marker>
<rawString>David Chiang, Yuval Marton, and Philip Resnik. 2008. Online large-margin training of syntactic and structural translation features. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP’08), Honolulu, Hawaii, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1746" citStr="Chiang, 2005" startWordPosition="249" endWordPosition="251"> over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). Going beyond the sentence-level is important since sentences rarely stand on their own in a well-written text. Rather, each sentence follows smoothly from the ones before it, and leads into the ones that come afterwards. The logical relationship between sentences carries important information that allows the text to express a meaning as a whole beyond the sum of its separate parts. Note that sentences can be made of several cla</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 263– 270, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language.</title>
<date>2001</date>
<booktitle>In Neural Information Processing Systems, NIPS’01,</booktitle>
<pages>625--632</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="8935" citStr="Collins and Duffy, 2001" startWordPosition="1359" endWordPosition="1362">h use the Discourse Representation Theory (Kamp and Reyle, 1993) and treebased discourse representation structures (DRS) produced by a semantic parser. They calculate the similarity between the MT output and references based on DRS subtree matching, as defined in (Liu and Gildea, 2005), DRS lexical overlap, and DRS morpho-syntactic overlap. However, they could not improve correlation with human judgments, as evaluated on the MetricsMATR dataset. 688 Compared to the previous work, (i) we use a different discourse representation (RST), (ii) we compare discourse parses using all-subtree kernels (Collins and Duffy, 2001), (iii) we evaluate on much larger datasets, for several language pairs and for multiple metrics, and (iv) we do demonstrate better correlation with human judgments. Wong and Kit (2012) recently proposed an extension of MT metrics with a measure of document-level lexical cohesion (Halliday and Hasan, 1976). Lexical cohesion is achieved using word repetitions and semantically similar words such as synonyms, hypernyms, and hyponyms. For BLEU and TER, they observed improved correlation with human judgments on the MTC4 dataset when linearly interpolating these metrics with their lexical cohesion s</context>
<context position="12744" citStr="Collins and Duffy, 2001" startWordPosition="1959" endWordPosition="1962">urse tree constituents. The inferred (posterior) probabilities are then used in a probabilistic CKY-like bottom-up parsing algorithm to find the most likely DT. Using the standard set of 18 coarse-grained relations defined in (Carlson and Marcu, 2001), the parser achieved an F1-score of 79.8%, which is very close to the human agreement of 83%. These high scores allowed us to develop successful discourse similarity metrics.2 3.2 Measuring Similarity A number of metrics have been proposed to measure the similarity between two labeled trees, e.g., Tree Edit Distance (Tai, 1979) and Tree Kernels (Collins and Duffy, 2001; Moschitti and Basili, 2006). Tree kernels (TKs) provide an effective way to integrate arbitrary tree structures in kernelbased machine learning algorithms like SVMs. In the present work, we use the convolution TK defined in (Collins and Duffy, 2001), which efficiently calculates the number of common subtrees in two trees. Note that this kernel was originally designed for syntactic parsing, where the subtrees are subject to the constraint that their nodes are taken with either all or none of the children. This constraint of the TK imposes some limitations on the type of substructures that can</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution Kernels for Natural Language. In Neural Information Processing Systems, NIPS’01, pages 625–632, Vancouver, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Elisabet Comelles</author>
<author>Jes´us Gim´enez</author>
</authors>
<location>Llu´ıs M`arquez,</location>
<marker>Comelles, Gim´enez, </marker>
<rawString>Elisabet Comelles, Jes´us Gim´enez, Llu´ıs M`arquez,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Castell´on</author>
<author>Victoria Arranz</author>
</authors>
<title>Document-level automatic mt evaluation based on discourse representations.</title>
<date>2010</date>
<booktitle>In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR,</booktitle>
<pages>333--338</pages>
<publisher>ACL.</publisher>
<location>Uppsala, Sweden,</location>
<marker>Castell´on, Arranz, 2010</marker>
<rawString>Irene Castell´on, and Victoria Arranz. 2010. Document-level automatic mt evaluation based on discourse representations. In Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR, pages 333–338, Uppsala, Sweden, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems.</title>
<date>2011</date>
<booktitle>In Proceedings of the Sixth Workshop on Statistical Machine Translation,</booktitle>
<pages>85--91</pages>
<publisher>ACL.</publisher>
<location>Edinburgh, Scotland,</location>
<contexts>
<context position="19278" citStr="Denkowski and Lavie, 2011" startWordPosition="3014" endWordPosition="3017">f 12 individual metrics. From the original ULC, we only replaced TER and Meteor individual metrics by newer versions taking into account synonymy lookup and paraphrasing: TERp-A and METEOR-pa in ASIYA’s terminology. We will call this combined metric Asiya0809 in our experiments. 4http://nlp.lsi.upc.edu/asiya/ To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three METEOR variants (Denkowski and Lavie, 2011): METEOR-ex (exact match), METEOR-st (+stemming) and METEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 1. Asiya-LEX. Combination of five metrics based on lexical similarity: BLEU, NIST, METEO</context>
</contexts>
<marker>Denkowski, Lavie, 2011</marker>
<rawString>Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 85–91, Edinburgh, Scotland, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Human Language Technology Research, HLT ’02,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="19175" citStr="Doddington, 2002" startWordPosition="3000" endWordPosition="3001">he segment levels at the WMT08 and WMT09 metrics tasks. This is a uniform linear combination of 12 individual metrics. From the original ULC, we only replaced TER and Meteor individual metrics by newer versions taking into account synonymy lookup and paraphrasing: TERp-A and METEOR-pa in ASIYA’s terminology. We will call this combined metric Asiya0809 in our experiments. 4http://nlp.lsi.upc.edu/asiya/ To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three METEOR variants (Denkowski and Lavie, 2011): METEOR-ex (exact match), METEOR-st (+stemming) and METEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics i</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proceedings of the Second International Conference on Human Language Technology Research, HLT ’02, pages 138–145, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technology, HLT-NAACL,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="1685" citStr="Galley et al., 2004" startWordPosition="239" endWordPosition="242">ve process at the sentence-level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). Going beyond the sentence-level is important since sentences rarely stand on their own in a well-written text. Rather, each sentence follows smoothly from the ones before it, and leads into the ones that come afterwards. The logical relationship between sentences carries important information that allows the text to express a meaning as a whole beyond the sum of its s</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In Proceedings of the 2004 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technology, HLT-NAACL, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Linguistic features for automatic evaluation of heterogenous MT systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>256--264</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic,</location>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2007. Linguistic features for automatic evaluation of heterogenous MT systems. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 256– 264, Prague, Czech Republic, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>On the robustness of syntactic and semantic features for automatic MT evaluation.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>250--258</pages>
<publisher>ACL.</publisher>
<location>Athens, Greece,</location>
<marker>Gim´enez, M`arquez, 2009</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2009. On the robustness of syntactic and semantic features for automatic MT evaluation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 250–258, Athens, Greece, March. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Asiya: an Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2010</date>
<pages>94--77</pages>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2010a. Asiya: an Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague Bulletin of Mathematical Linguistics, 94:77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Linguistic Measures for Automatic Machine Translation Evaluation. Machine Translation,</title>
<date>2010</date>
<pages>24--3</pages>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2010b. Linguistic Measures for Automatic Machine Translation Evaluation. Machine Translation, 24(3–4):77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Halliday</author>
<author>Ruqaiya Hasan</author>
</authors>
<date>1976</date>
<booktitle>Cohesion in English. Longman,</booktitle>
<location>London.</location>
<contexts>
<context position="9242" citStr="Halliday and Hasan, 1976" startWordPosition="1407" endWordPosition="1410">p, and DRS morpho-syntactic overlap. However, they could not improve correlation with human judgments, as evaluated on the MetricsMATR dataset. 688 Compared to the previous work, (i) we use a different discourse representation (RST), (ii) we compare discourse parses using all-subtree kernels (Collins and Duffy, 2001), (iii) we evaluate on much larger datasets, for several language pairs and for multiple metrics, and (iv) we do demonstrate better correlation with human judgments. Wong and Kit (2012) recently proposed an extension of MT metrics with a measure of document-level lexical cohesion (Halliday and Hasan, 1976). Lexical cohesion is achieved using word repetitions and semantically similar words such as synonyms, hypernyms, and hyponyms. For BLEU and TER, they observed improved correlation with human judgments on the MTC4 dataset when linearly interpolating these metrics with their lexical cohesion score. Unlike their work, which measures lexical cohesion at the document-level, here we are concerned with coherence (rhetorical) structure, primarily at the sentence-level. 3 Our Discourse-Based Measures Our working hypothesis is that the similarity between the discourse structures of an automatic and of </context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Michael Halliday and Ruqaiya Hasan, 1976. Cohesion in English. Longman, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Marcello Federico</author>
</authors>
<title>Modelling pronominal anaphora in statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation,</booktitle>
<pages>283--289</pages>
<contexts>
<context position="6079" citStr="Hardmeier and Federico, 2010" startWordPosition="923" endWordPosition="926">elevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1We refer the reader to (Hardmeier, 2012) for an in-depth overview of discourse-related research for MT. A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are ne</context>
</contexts>
<marker>Hardmeier, Federico, 2010</marker>
<rawString>Christian Hardmeier and Marcello Federico. 2010. Modelling pronominal anaphora in statistical machine translation. In Proceedings of the International Workshop on Spoken Language Translation, pages 283–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Joakim Nivre</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Document-wide decoding for phrasebased statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12,</booktitle>
<pages>1179--1190</pages>
<publisher>ACL.</publisher>
<location>Jeju Island,</location>
<contexts>
<context position="1913" citStr="Hardmeier et al., 2012" startWordPosition="277" endWordPosition="280">ramework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). Going beyond the sentence-level is important since sentences rarely stand on their own in a well-written text. Rather, each sentence follows smoothly from the ones before it, and leads into the ones that come afterwards. The logical relationship between sentences carries important information that allows the text to express a meaning as a whole beyond the sum of its separate parts. Note that sentences can be made of several clauses, which in turn can be interrelated through the same logical relations. Thus, in a coherent text, discourse units (sentences or clauses) are logically connected: t</context>
</contexts>
<marker>Hardmeier, Nivre, Tiedemann, 2012</marker>
<rawString>Christian Hardmeier, Joakim Nivre, and J¨org Tiedemann. 2012. Document-wide decoding for phrasebased statistical machine translation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL ’12, pages 1179–1190, Jeju Island, Korea. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
</authors>
<title>Discourse in statistical machine translation. a survey and a case study. Discours. Revue de linguistique, psycholinguistique et informatique,</title>
<date>2012</date>
<pages>11--8726</pages>
<contexts>
<context position="6335" citStr="Hardmeier, 2012" startWordPosition="964" endWordPosition="965">atanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1We refer the reader to (Hardmeier, 2012) for an in-depth overview of discourse-related research for MT. A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judgments. The field of automatic evaluation metrics for MT is very active, and new</context>
</contexts>
<marker>Hardmeier, 2012</marker>
<rawString>Christian Hardmeier. 2012. Discourse in statistical machine translation. a survey and a case study. Discours. Revue de linguistique, psycholinguistique et informatique, 11(8726).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hopkins</author>
<author>Jonathan May</author>
</authors>
<title>Tuning as ranking.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP ’11.</booktitle>
<contexts>
<context position="5707" citStr="Hopkins and May, 2011" startWordPosition="863" endWordPosition="867"> et al., 2012). These metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1We refer the</context>
<context position="22444" citStr="Hopkins and May, 2011" startWordPosition="3525" endWordPosition="3529">we used pairwise rankings to discriminatively learn the weights of the linear combinations of individual metrics. In order to use the WMT12 data for training a learning-to-rank model, we transformed the five-way relative rankings into ten pairwise comparisons. For instance, if a judge ranked the output of systems A, B, C, D, E as A &gt; B &gt; C &gt; D &gt; E, this would entail that A &gt; B, A &gt; C, A &gt; D and A &gt; E, etc. To determine the relative weights for the tuned combinations, we followed a similar approach to the one used by PRO to tune the relative weights of the components of a log-linear SMT model (Hopkins and May, 2011), also using Maximum Entropy as the base learning algorithm. Unlike PRO, (i) we use human judgments, not automatic scores, and (ii) we train on all pairs, not on a subsample. 5 Experimental Results In this section, we explore how discourse information can be used to improve machine translation evaluation metrics. Below we present the evaluation results at the system- and segment-level, using our two basic metrics on discourse trees (Section 3.1), which are referred to as DR and DR-LEX. 5.1 Evaluation In our experiments, we only consider translation into English, and use the data described in T</context>
</contexts>
<marker>Hopkins, May, 2011</marker>
<rawString>Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, EMNLP ’11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
</authors>
<title>A Novel Discriminative Framework for Sentence-Level Discourse Analysis.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12,</booktitle>
<pages>904--915</pages>
<publisher>ACL.</publisher>
<location>Jeju Island,</location>
<contexts>
<context position="3856" citStr="Joty et al., 2012" startWordPosition="580" endWordPosition="583">is demonstrated by the establishment of a recent workshop dedicated to Discourse in Machine Translation (Webber et al., 2013), collocated with the 2013 annual meeting of the Association of Computational Linguistics. The area of discourse analysis for SMT is still nascent and, to the best of our knowledge, no previous research has attempted to use rhetorical structure for SMT or machine translation evaluation. One possible reason could be the unavailability of accurate discourse parsers. However, this situation is likely to change given the most recent advances in automatic discourse analysis (Joty et al., 2012; Joty et al., 2013). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics. While in this work we focus on the latter, we think that the former is also within reach, and that SMT systems would benefit from preserving the coherence relati</context>
<context position="11697" citStr="Joty et al. (2012)" startWordPosition="1791" endWordPosition="1794">b) and (c). In order to develop a discourse-aware evaluation metric, we first generate discourse trees for the reference and the system-translated sentences using a discourse parser, and then we measure the similarity between the two discourse trees. We describe these two steps below. 3.1 Generating Discourse Trees In Rhetorical Structure Theory, discourse analysis involves two subtasks: (i) discourse segmentation, or breaking the text into a sequence of EDUs, and (ii) discourse parsing, or the task of linking the units (EDUs and larger discourse units) into labeled discourse trees. Recently, Joty et al. (2012) proposed discriminative models for both discourse segmentation and discourse parsing at the sentence level. The segmenter uses a maximum entropy model that achieves state-of-the-art accuracy on this task, having an F1-score of 90.5%, while human agreement is 98.3%. The discourse parser uses a dynamic Conditional Random Field (Sutton et al., 2007) as a parsing model in order to infer the probability of all possible discourse tree constituents. The inferred (posterior) probabilities are then used in a probabilistic CKY-like bottom-up parsing algorithm to find the most likely DT. Using the stand</context>
</contexts>
<marker>Joty, Carenini, Ng, 2012</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, and Raymond T. Ng. 2012. A Novel Discriminative Framework for Sentence-Level Discourse Analysis. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’12, pages 904–915, Jeju Island, Korea. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shafiq Joty</author>
<author>Giuseppe Carenini</author>
<author>Raymond T Ng</author>
<author>Yashar Mehdad</author>
</authors>
<title>Combining Intra- and Multi-sentential Rhetorical Parsing for Documentlevel Discourse Analysis.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL ’13,</booktitle>
<pages>486--496</pages>
<publisher>ACL.</publisher>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="3876" citStr="Joty et al., 2013" startWordPosition="584" endWordPosition="587">the establishment of a recent workshop dedicated to Discourse in Machine Translation (Webber et al., 2013), collocated with the 2013 annual meeting of the Association of Computational Linguistics. The area of discourse analysis for SMT is still nascent and, to the best of our knowledge, no previous research has attempted to use rhetorical structure for SMT or machine translation evaluation. One possible reason could be the unavailability of accurate discourse parsers. However, this situation is likely to change given the most recent advances in automatic discourse analysis (Joty et al., 2012; Joty et al., 2013). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics. While in this work we focus on the latter, we think that the former is also within reach, and that SMT systems would benefit from preserving the coherence relations in the source la</context>
</contexts>
<marker>Joty, Carenini, Ng, Mehdad, 2013</marker>
<rawString>Shafiq Joty, Giuseppe Carenini, Raymond T. Ng, and Yashar Mehdad. 2013. Combining Intra- and Multi-sentential Rhetorical Parsing for Documentlevel Discourse Analysis. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL ’13, pages 486–496, Sofia, Bulgaria. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<title>From Discourse to Logic: Introduction to Model theoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory.</title>
<date>1993</date>
<journal>Number</journal>
<booktitle>in Studies in Linguistics and Philosophy.</booktitle>
<volume>42</volume>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="8375" citStr="Kamp and Reyle, 1993" startWordPosition="1276" endWordPosition="1279">tructures (Gim´enez and M`arquez, 2007; Lo et al., 2012). In this work, instead of proposing a new metric, we focus on enriching current MT evaluation metrics with discourse information. Our experiments show that many existing metrics can benefit from additional knowledge about discourse structure. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few attempts to incorporate discourse information so far. One example are the semantics-aware metrics of Gim´enez and M`arquez (2009) and Comelles et al. (2010), which use the Discourse Representation Theory (Kamp and Reyle, 1993) and treebased discourse representation structures (DRS) produced by a semantic parser. They calculate the similarity between the MT output and references based on DRS subtree matching, as defined in (Liu and Gildea, 2005), DRS lexical overlap, and DRS morpho-syntactic overlap. However, they could not improve correlation with human judgments, as evaluated on the MetricsMATR dataset. 688 Compared to the previous work, (i) we use a different discourse representation (RST), (ii) we compare discourse parses using all-subtree kernels (Collins and Duffy, 2001), (iii) we evaluate on much larger datas</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Hans Kamp and Uwe Reyle. 1993. From Discourse to Logic: Introduction to Model theoretic Semantics of Natural Language, Formal Logic and Discourse Representation Theory. Number 42 in Studies in Linguistics and Philosophy. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Proceedings of Workshop on Text Summarization Branches Out,</booktitle>
<pages>74--81</pages>
<location>Barcelona.</location>
<contexts>
<context position="19223" citStr="Lin, 2004" startWordPosition="3008" endWordPosition="3009"> This is a uniform linear combination of 12 individual metrics. From the original ULC, we only replaced TER and Meteor individual metrics by newer versions taking into account synonymy lookup and paraphrasing: TERp-A and METEOR-pa in ASIYA’s terminology. We will call this combined metric Asiya0809 in our experiments. 4http://nlp.lsi.upc.edu/asiya/ To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three METEOR variants (Denkowski and Lavie, 2011): METEOR-ex (exact match), METEOR-st (+stemming) and METEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 1. Asiya-LEX. Combination of five</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Proceedings of Workshop on Text Summarization Branches Out, pages 74–81, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>25--32</pages>
<publisher>ACL.</publisher>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="7640" citStr="Liu and Gildea, 2005" startWordPosition="1163" endWordPosition="1166">n campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For example, at WMT12, 12 metrics were compared (Callison-Burch et al., 2012), most of them new. There have been several attempts to incorporate syntactic and semantic linguistic knowledge into MT evaluation. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007) or between constituency trees (Liu and Gildea, 2005). In the semantic case, there are metrics that exploit the similarity over named entities and predicate-argument structures (Gim´enez and M`arquez, 2007; Lo et al., 2012). In this work, instead of proposing a new metric, we focus on enriching current MT evaluation metrics with discourse information. Our experiments show that many existing metrics can benefit from additional knowledge about discourse structure. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few attempts to incorporate discourse information so far. One example are the semantics-aware m</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 25–32, Ann Arbor, Michigan, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Anand Karthik Tumuluru</author>
<author>Dekai Wu</author>
</authors>
<title>Fully automatic semantic mt evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>243--252</pages>
<publisher>ACL.</publisher>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="1803" citStr="Lo et al., 2012" startWordPosition="259" endWordPosition="262">e of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). Going beyond the sentence-level is important since sentences rarely stand on their own in a well-written text. Rather, each sentence follows smoothly from the ones before it, and leads into the ones that come afterwards. The logical relationship between sentences carries important information that allows the text to express a meaning as a whole beyond the sum of its separate parts. Note that sentences can be made of several clauses, which in turn can be interrelated through the same </context>
<context position="7810" citStr="Lo et al., 2012" startWordPosition="1189" endWordPosition="1192">s. For example, at WMT12, 12 metrics were compared (Callison-Burch et al., 2012), most of them new. There have been several attempts to incorporate syntactic and semantic linguistic knowledge into MT evaluation. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007) or between constituency trees (Liu and Gildea, 2005). In the semantic case, there are metrics that exploit the similarity over named entities and predicate-argument structures (Gim´enez and M`arquez, 2007; Lo et al., 2012). In this work, instead of proposing a new metric, we focus on enriching current MT evaluation metrics with discourse information. Our experiments show that many existing metrics can benefit from additional knowledge about discourse structure. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few attempts to incorporate discourse information so far. One example are the semantics-aware metrics of Gim´enez and M`arquez (2009) and Comelles et al. (2010), which use the Discourse Representation Theory (Kamp and Reyle, 1993) and treebased discourse representa</context>
</contexts>
<marker>Lo, Tumuluru, Wu, 2012</marker>
<rawString>Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. 2012. Fully automatic semantic mt evaluation. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 243–252, Montr´eal, Canada, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Mann</author>
<author>Sandra Thompson</author>
</authors>
<title>Rhetorical Structure Theory: Toward a Functional Theory of Text Organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="2789" citStr="Mann and Thompson, 1988" startWordPosition="418" endWordPosition="422">en sentences carries important information that allows the text to express a meaning as a whole beyond the sum of its separate parts. Note that sentences can be made of several clauses, which in turn can be interrelated through the same logical relations. Thus, in a coherent text, discourse units (sentences or clauses) are logically connected: the meaning of a unit relates to that of the previous and the following units. Discourse analysis seeks to uncover this coherence structure underneath the text. Several formal theories of discourse have been proposed to describe the coherence structure (Mann and Thompson, 1988; Asher and Lascarides, 2003; Webber, 2004). For example, the Rhetorical Structure Theory (Mann and Thompson, 1988), or RST, represents text by labeled hierarchical structures called Discourse Trees (DTs), which can incorporate several layers of other linguistic information, e.g., syntax, predicate-argument structure, etc. Modeling discourse brings together the above research directions (a) and (b), which makes it an attractive goal for MT. This is demonstrated by the establishment of a recent workshop dedicated to Discourse in Machine Translation (Webber et al., 2013), collocated with the 201</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William Mann and Sandra Thompson. 1988. Rhetorical Structure Theory: Toward a Functional Theory of Text Organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Meyer</author>
<author>Andrei Popescu-Belis</author>
<author>Najeh Hajlaoui</author>
<author>Andrea Gesmundo</author>
</authors>
<title>Machine translation of labeled discourse connectives.</title>
<date>2012</date>
<booktitle>In Proceedings of the Tenth Biennial Conference of the Association for Machine Translation in the Americas (AMTA).</booktitle>
<contexts>
<context position="6601" citStr="Meyer et al., 2012" startWordPosition="1000" endWordPosition="1003">arch direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1We refer the reader to (Hardmeier, 2012) for an in-depth overview of discourse-related research for MT. A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judgments. The field of automatic evaluation metrics for MT is very active, and new metrics are continuously being proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For </context>
</contexts>
<marker>Meyer, Popescu-Belis, Hajlaoui, Gesmundo, 2012</marker>
<rawString>Thomas Meyer, Andrei Popescu-Belis, Najeh Hajlaoui, and Andrea Gesmundo. 2012. Machine translation of labeled discourse connectives. In Proceedings of the Tenth Biennial Conference of the Association for Machine Translation in the Americas (AMTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Meyer</author>
</authors>
<title>Disambiguating temporalcontrastive connectives for machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the ACL 2011 Student Session,</booktitle>
<pages>46--51</pages>
<publisher>ACL.</publisher>
<location>Portland, OR, USA,</location>
<contexts>
<context position="6141" citStr="Meyer, 2011" startWordPosition="934" endWordPosition="935">s compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1We refer the reader to (Hardmeier, 2012) for an in-depth overview of discourse-related research for MT. A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we s</context>
</contexts>
<marker>Meyer, 2011</marker>
<rawString>Thomas Meyer. 2011. Disambiguating temporalcontrastive connectives for machine translation. In Proceedings of the ACL 2011 Student Session, pages 46–51, Portland, OR, USA, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Roberto Basili</author>
</authors>
<title>A Tree Kernel approach to Question and Answer Classification in Question Answering Systems.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th international conference on Language Resources and Evaluation,</booktitle>
<location>Genoa, Italy.</location>
<contexts>
<context position="12773" citStr="Moschitti and Basili, 2006" startWordPosition="1963" endWordPosition="1966">he inferred (posterior) probabilities are then used in a probabilistic CKY-like bottom-up parsing algorithm to find the most likely DT. Using the standard set of 18 coarse-grained relations defined in (Carlson and Marcu, 2001), the parser achieved an F1-score of 79.8%, which is very close to the human agreement of 83%. These high scores allowed us to develop successful discourse similarity metrics.2 3.2 Measuring Similarity A number of metrics have been proposed to measure the similarity between two labeled trees, e.g., Tree Edit Distance (Tai, 1979) and Tree Kernels (Collins and Duffy, 2001; Moschitti and Basili, 2006). Tree kernels (TKs) provide an effective way to integrate arbitrary tree structures in kernelbased machine learning algorithms like SVMs. In the present work, we use the convolution TK defined in (Collins and Duffy, 2001), which efficiently calculates the number of common subtrees in two trees. Note that this kernel was originally designed for syntactic parsing, where the subtrees are subject to the constraint that their nodes are taken with either all or none of the children. This constraint of the TK imposes some limitations on the type of substructures that can be compared. 2The discourse </context>
</contexts>
<marker>Moschitti, Basili, 2006</marker>
<rawString>Alessandro Moschitti and Roberto Basili. 2006. A Tree Kernel approach to Question and Answer Classification in Question Answering Systems. In Proceedings of the 5th international conference on Language Resources and Evaluation, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-2007,</booktitle>
<pages>776--783</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="15626" citStr="Moschitti et al., 2007" startWordPosition="2433" endWordPosition="2436">ive partial credit to subtrees that differ in labels but match in their skeletons. More specifically, it uses the tags SPAN and EDU to build the skeleton of the tree, and considers the nuclearity and/or the relation labels as properties, added as children, of these tags. For example, a SPAN has two properties (its nuclearity and its relation), and an EDU has one property (its nuclearity). The words of an EDU are placed under the predefined children NGRAM. In order to allow the tree kernel to find subtree matches at the word level, we include an additional layer of dummy leaves as was done in (Moschitti et al., 2007); not shown in Figure 2, for simplicity. 4 Experimental Setup In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English.3 This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation campaigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English (CSEN), French-English (FR-EN), German-English (DE-EN), and Spanish-English (ES-EN); as well as a dataset with the English references. We measured the correlation of the metrics with the human judgments provided b</context>
</contexts>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting Syntactic and Shallow Semantic Kernels for Question/Answer Classification. In Proceedings of the ACL-2007, pages 776–783, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the Association for Computational Linguistics (ACL’02),</booktitle>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="19150" citStr="Papineni et al., 2002" startWordPosition="2995" endWordPosition="2998">ing metric at the system and the segment levels at the WMT08 and WMT09 metrics tasks. This is a uniform linear combination of 12 individual metrics. From the original ULC, we only replaced TER and Meteor individual metrics by newer versions taking into account synonymy lookup and paraphrasing: TERp-A and METEOR-pa in ASIYA’s terminology. We will call this combined metric Asiya0809 in our experiments. 4http://nlp.lsi.upc.edu/asiya/ To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three METEOR variants (Denkowski and Lavie, 2011): METEOR-ex (exact match), METEOR-st (+stemming) and METEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear com</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the Association for Computational Linguistics (ACL’02), Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maja Popovic</author>
<author>Hermann Ney</author>
</authors>
<title>Word error rates: Decomposition over POS classes and applications for error analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>48--55</pages>
<publisher>ACL.</publisher>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7587" citStr="Popovic and Ney, 2007" startWordPosition="1155" endWordPosition="1158">g proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For example, at WMT12, 12 metrics were compared (Callison-Burch et al., 2012), most of them new. There have been several attempts to incorporate syntactic and semantic linguistic knowledge into MT evaluation. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007) or between constituency trees (Liu and Gildea, 2005). In the semantic case, there are metrics that exploit the similarity over named entities and predicate-argument structures (Gim´enez and M`arquez, 2007; Lo et al., 2012). In this work, instead of proposing a new metric, we focus on enriching current MT evaluation metrics with discourse information. Our experiments show that many existing metrics can benefit from additional knowledge about discourse structure. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few attempts to incorporate discourse info</context>
</contexts>
<marker>Popovic, Ney, 2007</marker>
<rawString>Maja Popovic and Hermann Ney. 2007. Word error rates: Decomposition over POS classes and applications for error analysis. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 48–55, Prague, Czech Republic, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>271--279</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="1706" citStr="Quirk et al., 2005" startWordPosition="243" endWordPosition="246">tence-level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). Going beyond the sentence-level is important since sentences rarely stand on their own in a well-written text. Rather, each sentence follows smoothly from the ones before it, and leads into the ones that come afterwards. The logical relationship between sentences carries important information that allows the text to express a meaning as a whole beyond the sum of its separate parts. Note t</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 271–279, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas, AMTA ’06,</booktitle>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="19202" citStr="Snover et al., 2006" startWordPosition="3003" endWordPosition="3006"> WMT08 and WMT09 metrics tasks. This is a uniform linear combination of 12 individual metrics. From the original ULC, we only replaced TER and Meteor individual metrics by newer versions taking into account synonymy lookup and paraphrasing: TERp-A and METEOR-pa in ASIYA’s terminology. We will call this combined metric Asiya0809 in our experiments. 4http://nlp.lsi.upc.edu/asiya/ To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three METEOR variants (Denkowski and Lavie, 2011): METEOR-ex (exact match), METEOR-st (+stemming) and METEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 1. Asiya-LEX</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Biennial Conference of the Association for Machine Translation in the Americas, AMTA ’06, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles Sutton</author>
<author>Andrew McCallum</author>
<author>Khashayar Rohanimanesh</author>
</authors>
<title>Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data.</title>
<date>2007</date>
<journal>Journal of Machine Learning Research (JMLR),</journal>
<pages>8--693</pages>
<contexts>
<context position="12046" citStr="Sutton et al., 2007" startWordPosition="1845" endWordPosition="1848">, discourse analysis involves two subtasks: (i) discourse segmentation, or breaking the text into a sequence of EDUs, and (ii) discourse parsing, or the task of linking the units (EDUs and larger discourse units) into labeled discourse trees. Recently, Joty et al. (2012) proposed discriminative models for both discourse segmentation and discourse parsing at the sentence level. The segmenter uses a maximum entropy model that achieves state-of-the-art accuracy on this task, having an F1-score of 90.5%, while human agreement is 98.3%. The discourse parser uses a dynamic Conditional Random Field (Sutton et al., 2007) as a parsing model in order to infer the probability of all possible discourse tree constituents. The inferred (posterior) probabilities are then used in a probabilistic CKY-like bottom-up parsing algorithm to find the most likely DT. Using the standard set of 18 coarse-grained relations defined in (Carlson and Marcu, 2001), the parser achieved an F1-score of 79.8%, which is very close to the human agreement of 83%. These high scores allowed us to develop successful discourse similarity metrics.2 3.2 Measuring Similarity A number of metrics have been proposed to measure the similarity between</context>
</contexts>
<marker>Sutton, McCallum, Rohanimanesh, 2007</marker>
<rawString>Charles Sutton, Andrew McCallum, and Khashayar Rohanimanesh. 2007. Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data. Journal of Machine Learning Research (JMLR), 8:693–723.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuo-Chung Tai</author>
</authors>
<title>The tree-to-tree correction problem.</title>
<date>1979</date>
<journal>Journal of the ACM,</journal>
<volume>26</volume>
<issue>3</issue>
<contexts>
<context position="12702" citStr="Tai, 1979" startWordPosition="1954" endWordPosition="1955">ability of all possible discourse tree constituents. The inferred (posterior) probabilities are then used in a probabilistic CKY-like bottom-up parsing algorithm to find the most likely DT. Using the standard set of 18 coarse-grained relations defined in (Carlson and Marcu, 2001), the parser achieved an F1-score of 79.8%, which is very close to the human agreement of 83%. These high scores allowed us to develop successful discourse similarity metrics.2 3.2 Measuring Similarity A number of metrics have been proposed to measure the similarity between two labeled trees, e.g., Tree Edit Distance (Tai, 1979) and Tree Kernels (Collins and Duffy, 2001; Moschitti and Basili, 2006). Tree kernels (TKs) provide an effective way to integrate arbitrary tree structures in kernelbased machine learning algorithms like SVMs. In the present work, we use the convolution TK defined in (Collins and Duffy, 2001), which efficiently calculates the number of common subtrees in two trees. Note that this kernel was originally designed for syntactic parsing, where the subtrees are subject to the constraint that their nodes are taken with either all or none of the children. This constraint of the TK imposes some limitat</context>
</contexts>
<marker>Tai, 1979</marker>
<rawString>Kuo-Chung Tai. 1979. The tree-to-tree correction problem. Journal of the ACM, 26(3):422–433, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taro Watanabe</author>
<author>Jun Suzuki</author>
<author>Hajime Tsukada</author>
<author>Hideki Isozaki</author>
</authors>
<title>Online large-margin training for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’07,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5739" citStr="Watanabe et al., 2007" startWordPosition="870" endWordPosition="873">ks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1We refer the reader to (Hardmeier, 2012) for</context>
</contexts>
<marker>Watanabe, Suzuki, Tsukada, Isozaki, 2007</marker>
<rawString>Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki Isozaki. 2007. Online large-margin training for statistical machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ’07, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<date>2013</date>
<booktitle>Proceedings of the Workshop on Discourse in Machine Translation. ACL,</booktitle>
<editor>Bonnie Webber, Andrei Popescu-Belis, Katja Markert, and J¨org Tiedemann, editors.</editor>
<location>Sofia, Bulgaria,</location>
<marker>2013</marker>
<rawString>Bonnie Webber, Andrei Popescu-Belis, Katja Markert, and J¨org Tiedemann, editors. 2013. Proceedings of the Workshop on Discourse in Machine Translation. ACL, Sofia, Bulgaria, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Webber</author>
</authors>
<title>D-LTAG: Extending Lexicalized TAG to Discourse.</title>
<date>2004</date>
<journal>Cognitive Science,</journal>
<volume>28</volume>
<issue>5</issue>
<contexts>
<context position="2832" citStr="Webber, 2004" startWordPosition="427" endWordPosition="428">s the text to express a meaning as a whole beyond the sum of its separate parts. Note that sentences can be made of several clauses, which in turn can be interrelated through the same logical relations. Thus, in a coherent text, discourse units (sentences or clauses) are logically connected: the meaning of a unit relates to that of the previous and the following units. Discourse analysis seeks to uncover this coherence structure underneath the text. Several formal theories of discourse have been proposed to describe the coherence structure (Mann and Thompson, 1988; Asher and Lascarides, 2003; Webber, 2004). For example, the Rhetorical Structure Theory (Mann and Thompson, 1988), or RST, represents text by labeled hierarchical structures called Discourse Trees (DTs), which can incorporate several layers of other linguistic information, e.g., syntax, predicate-argument structure, etc. Modeling discourse brings together the above research directions (a) and (b), which makes it an attractive goal for MT. This is demonstrated by the establishment of a recent workshop dedicated to Discourse in Machine Translation (Webber et al., 2013), collocated with the 2013 annual meeting of the Association of Comp</context>
</contexts>
<marker>Webber, 2004</marker>
<rawString>Bonnie Webber. 2004. D-LTAG: Extending Lexicalized TAG to Discourse. Cognitive Science, 28(5):751–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy T M Wong</author>
<author>Chunyu Kit</author>
</authors>
<title>Extending machine translation evaluation metrics with lexical cohesion to document level.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL,</booktitle>
<pages>1060--1068</pages>
<publisher>ACL.</publisher>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="9120" citStr="Wong and Kit (2012)" startWordPosition="1389" endWordPosition="1392">the MT output and references based on DRS subtree matching, as defined in (Liu and Gildea, 2005), DRS lexical overlap, and DRS morpho-syntactic overlap. However, they could not improve correlation with human judgments, as evaluated on the MetricsMATR dataset. 688 Compared to the previous work, (i) we use a different discourse representation (RST), (ii) we compare discourse parses using all-subtree kernels (Collins and Duffy, 2001), (iii) we evaluate on much larger datasets, for several language pairs and for multiple metrics, and (iv) we do demonstrate better correlation with human judgments. Wong and Kit (2012) recently proposed an extension of MT metrics with a measure of document-level lexical cohesion (Halliday and Hasan, 1976). Lexical cohesion is achieved using word repetitions and semantically similar words such as synonyms, hypernyms, and hyponyms. For BLEU and TER, they observed improved correlation with human judgments on the MTC4 dataset when linearly interpolating these metrics with their lexical cohesion score. Unlike their work, which measures lexical cohesion at the document-level, here we are concerned with coherence (rhetorical) structure, primarily at the sentence-level. 3 Our Disco</context>
</contexts>
<marker>Wong, Kit, 2012</marker>
<rawString>Billy T. M. Wong and Chunyu Kit. 2012. Extending machine translation evaluation metrics with lexical cohesion to document level. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLPCoNLL, pages 1060–1068, Jeju Island, Korea, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Pascale Fung</author>
</authors>
<title>Semantic roles for smt: A hybrid two-pass model.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,</booktitle>
<pages>13--16</pages>
<location>Boulder, Colorado,</location>
<contexts>
<context position="1785" citStr="Wu and Fung, 2009" startWordPosition="255" endWordPosition="258">ade little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). Going beyond the sentence-level is important since sentences rarely stand on their own in a well-written text. Rather, each sentence follows smoothly from the ones before it, and leads into the ones that come afterwards. The logical relationship between sentences carries important information that allows the text to express a meaning as a whole beyond the sum of its separate parts. Note that sentences can be made of several clauses, which in turn can be interrelated</context>
</contexts>
<marker>Wu, Fung, 2009</marker>
<rawString>Dekai Wu and Pascale Fung. 2009. Semantic roles for smt: A hybrid two-pass model. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers, pages 13– 16, Boulder, Colorado, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>