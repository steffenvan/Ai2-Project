<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005914">
<title confidence="0.997766">
Multi-Pass Decoding With Complex Feature Guidance for Statistical
Machine Translation
</title>
<author confidence="0.924784">
Benjamin Marie
</author>
<note confidence="0.5871255">
LIMSI-CNRS, Orsay, France
Lingua et Machina, Le Chesnay, France
</note>
<email confidence="0.996659">
benjamin.marie@limsi.fr
</email>
<sectionHeader confidence="0.99377" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99944835">
In Statistical Machine Translation, some
complex features are still difficult to in-
tegrate during decoding and usually used
through the reranking of the k-best hy-
potheses produced by the decoder. We
propose a translation table partitioning
method that exploits the result of this
reranking to iteratively guide the decoder
in order to produce a new k-best list
more relevant to some complex features.
We report experiments on two transla-
tion domains and two translations direc-
tions which yield improvements of up to
1.4 BLEU over the reranking baseline us-
ing the same set of complex features. On
a practical viewpoint, our approach al-
lows SMT system developers to easily
integrate complex features into decoding
rather than being limited to their use in
one-time k-best list reranking.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999203882352941">
State-of-the-art Phrase-Based Statistical Machine
Translation (PBSMT) systems can use a large
number of feature functions decomposable into lo-
cal scores to efficiently evaluate the partial hy-
potheses built during decoding. However, some
feature functions are difficult to integrate into the
decoder mainly because they are not easily decom-
posable, very costly to compute and/or only avail-
able after complete hypotheses have been posited.
Usually such complex features are used through
the rescoring and reranking of the k-best transla-
tion hypotheses produced by the decoder (Och et
al., 2004). Although this reranking pass is per-
formed over the best part of the decoder search
space, it is limited by the actual diversity ex-
pressed in the k-best list. Additionally, reranking
being performed on a list generated by a simpler
</bodyText>
<note confidence="0.956755">
Aur´elien Max
LIMSI-CNRS, Orsay, France
Univ. Paris Sud, Orsay, France
</note>
<email confidence="0.983774">
aurelien.max@limsi.fr
</email>
<bodyText confidence="0.999926323529412">
set of features, it may not have access to hypothe-
ses that can best exploit the potential of the com-
plex features used. We describe a translation ta-
ble partitioning approach that exploits the result
of such a reranking to iteratively guide the de-
coder to produce new hypotheses that are more
relevant to the complex features used. To this end,
we focus in this work on the simple exploitation
of the disagreement between hypotheses ranked
best according to the decoder and to our feature-
rich decoder. In particular, we seek to provide
the next-pass decoder with separate translation ta-
bles that either contain bi-phrases that are unique
to the decoder’s one-best or to the reranker’s one-
best, in the hope that it will tend, in a soft man-
ner, to exploit the preferences expressed by the
complex features, and to otherwise explore alter-
native translation choices. Such a comparison is
then iteratively repeated, until convergence on a
development set between the new pass of the de-
coder and a reranker trained on the full set of
hypotheses generated thus far. On the test data,
this procedure thus produces after each iteration a
new decoder n-best, as well as an iteration-specific
new reranker best hypothesis. We report consis-
tent improvements of translation quality over a
strong reranking baseline using the same features
on 2 different domains and 2 translation directions.
The remainder of this article is organized as fol-
lows: we first briefly review related work (Sec-
tion 2), then introduce our approach (Section 3),
describe our experiments (Section 4), and finally
discuss our results and present our future work
(Section 5).
</bodyText>
<sectionHeader confidence="0.999802" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9996244">
Chen et al. (2008a; 2008b) expand the k-best list
of the decoder using three methods. One of them
involves re-decodings using models trained on the
decoder k-best list to integrate posterior knowl-
edge during the next re-decoding. The new k-best
</bodyText>
<page confidence="0.928685">
554
</page>
<bodyText confidence="0.960396696969697">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 554–559,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
list produced by the decoder is concatenated to the
original one and then reranked with complex fea-
tures, which yields improvements over a rerank-
ing performed on the original k-best list. The
reranking pass is done out of the loop and the re-
decodings do not exploit the reranking result that
used the complex features.
Recently, we proposed a rewriting system that
explores in a greedy fashion the neighborhood of
the one-best hypothesis found by the reranking
pass using complex features, assuming that a bet-
ter hypothesis can be very close to this seed hy-
pothesis (Marie and Max, 2014). Nevertheless,
this rewriting only explores a small search space,
limited by the greedy search algorithm that con-
centrates on individual, local rewritings.
Other works proposed methods to produce more
diverse lists of hypotheses by iteratively encourag-
ing the decoder to produce translations that are dif-
ferent from the previous one (Gimpel et al., 2013)
or by making small changes to the scoring func-
tion to extract k-best lists from other parts of the
search space (Devlin and Matsoukas, 2012). Some
useful diversity can be obtained as these hypothe-
ses can be combined using SMT system combina-
tion or help to better train reranking systems. But
in spite of the introduction of more diversity, these
methods do not guarantee that eventually lists con-
taining hypotheses that are more relevant to com-
plex features will be obtained.
</bodyText>
<sectionHeader confidence="0.995626" genericHeader="method">
3 Translation Table Partitioning
</sectionHeader>
<subsectionHeader confidence="0.999885">
3.1 Exploiting the Reranking Pass Result
</subsectionHeader>
<bodyText confidence="0.999981729166667">
Because all bi-phrases initially belong to the same
translation table, they share their feature weights
after tuning. Our main idea is to partition the set
of bi-phrases by putting aside, in new translation
tables, possibly misused bi-phrases according to
the reranking with complex features of the decoder
k-best list (Rerank). This partitioning gives to
subsequent tunings the opportunity to assign more
adapted weights to the features of these specific
groups of bi-phrases. Intuitively, if the Rerank
one-best hypothesis is different from that of the
initial decoder, the bi-phrases that account for the
differences should have received different weights
to encourage the decoder to either choose them or
instead avoid them.
To achieve the partitioning of the translation ta-
ble we compare the Rerank one-best hypothe-
sis to the decoder one-best and compute their dif-
ferences. On the one hand, there are n-grams
from the decoder one-best hypothesis that are not
found any more in the Rerank one-best; on
the other hand, there are n-grams that only exist
in the Rerank one-best hypothesis. Since the
decoder produces word alignments between the
source sentence to translate and its hypotheses,
we can extract all the bi-phrases from the transla-
tion table that are compatible with these n-grams
and their alignments. Each set of bi-phrases ex-
tracted from n-grams1 either appearing (IN) or
disappearing (OUT) in the Rerank one-best hy-
pothesis compared to the decoder’s, is moved to
a specific translation table. Then a new tuning is
performed for each relevant partitioning configu-
ration.
The described translation table partioning pro-
cedure can be performed iteratively as each new
decoding can be followed by Rerank on the new
k-best list generated. The differences between
Rerank and the decoder one-bests are extracted
anew and put in new translation tables at each it-
eration.2 Iterations are performed until no more
improvements of the BLEU score are obtained by
Rerank on a development set. The decoder is re-
tuned and Rerank is re-trained after each itera-
tion3 to obtain more specific and updated weights
for each old or new translation table. Finally, at
test time, the learned weights corresponding to the
current iteration are applied.
</bodyText>
<subsectionHeader confidence="0.998919">
3.2 Located Tokens
</subsectionHeader>
<bodyText confidence="0.992723444444444">
As a token can appear more than once in an input
text and in a sentence, and because complex fea-
tures are computed locally, the source tokens are
located: an identifier is concatenated to each token
to make them unique in the source text to translate.
Tokens of source phrases in the translation table
are also located, meaning that each bi-phrases is
duplicated to cover all located tokens. This proce-
dure allows our approach to differentiate changes
between Moses and Rerank one-best hypothe-
ses at the token level by taking context into ac-
1In decoders phrases typically have a fixed maximum
length, which corresponds to our maximum value for n.
2So, if both types of translation tables are extracted at
each iteration, 3 iterations would produce 6 translation tables
in addition to the remainder of the initial one. Note that a
bi-phrase can in fact be present in more than one translation
table after several iterations.
</bodyText>
<footnote confidence="0.52373275">
3Rerank re-training uses only the k-best list of the cur-
rent iteration. k-bests from different iteration cannot be con-
catenated as they use a different number of features corre-
sponding to a different number of translation tables.
</footnote>
<page confidence="0.992541">
555
</page>
<figure confidence="0.986752454545455">
a means of transport safer is the subway .
le@0 moyen@1 de@2 transport@3 le@4 plus@5 sˆur@6 c’@7 est@8 le@9 m´etro@10 .@11
the safest means of transport is the subway .
Moses
source
Rerank
source OUT IN
le@0 a the
le@0 moyen@1 a means
moyen@1 de@2 transport@3 a means of transport
le@4 plus@5 sˆur@6 safer safest
</figure>
<figureCaption confidence="0.9999346">
Figure 1: Example of IN and OUT translation tables extraction from the n-grams that differ between the
Rerank and Moses one-best hypotheses.
count. An example of IN and OUT translation ta-
bles extraction with located tokens is presented in
Figure 1.
</figureCaption>
<sectionHeader confidence="0.997244" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.947585">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999382375">
We ran experiments on two translation tasks for
different domains: the WMT’14 Medical trans-
lation task (medical) and the WMT’11 news
translation task (news) for the language pair Fr-
En on both directions. For both tasks we trained
two strong baseline systems using data provided
by WMT4. Statistics about the training, develop-
ment and testing data are presented in Table 1.
</bodyText>
<table confidence="0.997328111111111">
Tasks Corpus Sentences Tokens (Fr-En)
train 12M 383M - 318M
news dev 2,525 73k - 65k
test 3,003 85k - 74k
train 4.9M 91M - 78M
medical dev 500 12k - 10k
test 1,000 26k - 21k
in-domain LM 146M - 78M
for both tasks LM 2.5B - 6B
</table>
<tableCaption confidence="0.999815">
Table 1: Data used in our experiments.
</tableCaption>
<subsectionHeader confidence="0.982544">
4.2 MT system
</subsectionHeader>
<bodyText confidence="0.998643285714286">
For our experiments we used the Moses phrase-
based SMT toolkit (Koehn et al., 2007) with de-
fault settings and features, including the five fea-
tures from the translation table, and kb-mira
tuning (Cherry and Foster, 2012). Rerank is
trained using kb-mira on the 1,000-best list gen-
erated by Moses on the development set with the
</bodyText>
<footnote confidence="0.536047">
4http://www.statmt.org/wmt14
</footnote>
<bodyText confidence="0.9970435">
distinct-nbest parameter to have no dupli-
cates. Testing is also performed on distinct 1,000-
best lists. Rerank uses all the decoder features
along with the following complex features:
</bodyText>
<listItem confidence="0.8518709">
• MosesNorm: all decoder features and the
Moses score normalized by the hypothesis
length
• NNM: bilingual and monolingual neural net-
work models with a structured output layer
(SOUL) (Le et al., 2012)
• POSLM: 6-gram POS language model
• WPP: count-based word posterior probabil-
ity (Ueffing and Ney, 2007)
• TagRatio: ratio of translation hypothesis by
number of source tokens tagged as: verb,
noun or adjective
• Syntax: depth, number of nodes and num-
ber of unary rules of the syntactic parse nor-
malized by the hypothesis length (Carter and
Monz, 2011)
• IBM1: IBM1 features (Och et al., 2004;
Hildebrand and Vogel, 2008)
Part-of-speech tagging and syntactic parsing
were respectively performed with the Stanford
Part-of-speech Tagger (Toutanova and Manning,
2000) and the Shift-Reduce parser of Zhu et
al. (2013). We report the individual performance
of each feature set in Table 2 and the Rerank
performance when using all feature sets. As ex-
pected, the NNM feature set brings most of the im-
provements and attain by itself nearly the BLEU
score of Rerank when using all feature sets for
the news task with a gain of 1.4 and 1.1 BLEU re-
spectively for En→Fr and Fr→En over the Moses
</listItem>
<page confidence="0.99607">
556
</page>
<figureCaption confidence="0.99742">
Figure 2: BLEU score evolution over iterations
for the IN configuration on the test set of the
medical En→Fr translation task.
</figureCaption>
<bodyText confidence="0.9992724">
baseline. Among the other feature sets, POSLM
performs well, especially for the medical task
with an improvement of 0.3 and 0.5 BLEU for
En→Fr and Fr→En, respectively.
Some types of our complex features have already
been used during decoding, although sometimes
for a very important cost (Schwartz et al., 2011).
Our feature sets are to be considered only as ex-
perimental parameters, as any other feature types
usually used during reranking could also be used.
</bodyText>
<table confidence="0.942145916666667">
medical news
Features
En→Fr Fr→En En→Fr Fr→En
Moses 38.8 37.1 31.1 28.6
+ MosesNorm 38.9 37.2 31.1 28.7
+ NNM 41.9 38.9 32.5 29.8
+ POSLM 39.2 37.7 31.1 28.9
+ WPP 39.1 37.1 31.2 28.6
+ TagRatio 38.9 37.3 31.1 28.8
+ Syntax 38.8 37.2 31.2 28.9
+ IBM1 39.1 37.2 30.9 28.8
Rerank 42.8 40.1 32.5 29.9
</table>
<tableCaption confidence="0.992436">
Table 2: Reranking results for each set of features
added individually; Rerank uses the full set.
</tableCaption>
<subsectionHeader confidence="0.839216">
4.3 Results
</subsectionHeader>
<bodyText confidence="0.987486983606558">
Table 3 presents our results for different transla-
tion table partitioning configurations. For each
configuration, results are presented for the last
iteration of the multi-pass decoding performed
by Moses and the reranking of its k-best list
by the Rerank system using complex features.
First, we observe for the baseline systems that
Rerank outperforms Moses for all translation
tasks and directions, especially on medical with
improvements of 3.0 and 4.0 BLEU respectively
for Fr→En and En→Fr. These improvements il-
lustrate the strong potential of our set of complex
features to provide more accurate scores for trans-
lation hypotheses than the set of features used dur-
ing the initial decoding.
All studied configurations yield improvements
with multi-pass Moses over the Moses baseline,
showing the advantage of extracting from the main
translation table misused bi-phrases according to
a reranking pass done with complex features. As
illustrated by Figure 2, the multi-pass decoding
quickly reduces the gap in BLEU score between
our multi-pass Moses and Rerank one-best hy-
potheses. Although the 1,000-best oracle remains
at the same level over the iterations, the 1,000-
best average score5 increases by 2 BLEU at the
last iteration over the first 1,000-best hypotheses
produced by Moses, pointing out a strong im-
provement of the average quality of the 1,000-
best hypotheses. However, except for the IN
configuration on medical En→Fr, multi-pass
Moses does not bring improvements by itself
over the Rerank baseline. Nevertheless, multi-
pass Moses coupled with Rerank does improve
over Rerank baseline for all configurations on all
translation tasks. These consistent improvements
over the Rerank baseline demonstrate the abil-
ity of our procedure to help the Moses decoder
to produce k-best lists of better quality which are
more suitable to our complex features.
The IN configuration, which puts in a trans-
lation table all bi-phrases in the one-best hy-
pothesis of Rerank that do not belong to the
Moses one-best hypothesis, performs the best for
all translation tasks: multi-pass Rerank yields a
1.4 BLEU improvement over the Rerank base-
line on medical En→Fr, and 0.7 BLEU on
news En→Fr. Improvements are lower, but
nonetheless consistent, for the Fr→En direction,
with +0.9 and +0.5 BLEU respectively on the
medical and news tasks. The OUT configu-
ration yields smaller improvements in compari-
son, meaning that putting aside (a few) first-ranked
bi-phrases downgraded by Rerank is less use-
ful in order to produce better k-best lists with
Moses. Using in the same system both IN and
5To obtain this average we compute the arithmetic mean
of the 1,000-best hypotheses sentence-BLEU scores and se-
lect the hypothesis with the closest score to the mean. Once
we have selected an hypothesis for each sentence, the BLEU
score is computed.
</bodyText>
<figure confidence="0.993534333333333">
BLEU
40
45
60
55
50
350 1 2 3 4
iteration
Moses 1-best
Rerank 1-best
1,000-best average
1,000-best oracle
</figure>
<page confidence="0.986906">
557
</page>
<tableCaption confidence="0.941766">
Table 3: Results for different translation table partitioning configurations. OUT: configuration with a
translation table containing bi-phrases of the Moses 1-best not in the Rerank 1-best. IN: configuration
with a translation table containing bi-phrases of the Rerank 1-best not in the Moses 1-best. For all
configuration the main translation table is still used but does not contain the extracted bi-phrases.
</tableCaption>
<figure confidence="0.987685214285714">
news En→Fr
news Fr→En
Configuration
medical En→Fr
dev test # iter.
dev test # iter.
medical Fr→En
dev test # iter.
dev test # iter.
-
-
-
-
41.3 37.1
44.2 40.1
27.1 31.1
28.5 32.5
40.9 38.8
43.9 42.8
28.0 28.6
29.1 29.9
Moses
baseline
Rerank
Moses
OUT
Rerank
43.3 41.8 4
45.3 43.8
43.0 38.7 3
44.5 40.5
27.9 31.8 1
28.5 32.9
28.5 29.2 1
29.2 30.3
Moses
IN
Rerank
45.1 43.2 4
45.7 44.2
43.6 39.9 3
45.0 41.0
28.4 32.4 2
28.8 33.2
28.6 29.3 2
29.3 30.4
IN and OUT Moses
Rerank
44.8 42.4 4
45.3 43.5
42.8 38.7 3
44.5 40.6
28.3 32.1 2
28.7 32.9
28.8 29.2 2
29.3 30.4
</figure>
<bodyText confidence="0.79205875">
OUT iteration-specific translation tables (“IN and
OUT”) yields a performance situated between us-
ing IN and OUT separately, but which still consis-
tently improves over the baseline Rerank.
</bodyText>
<sectionHeader confidence="0.984831" genericHeader="conclusions">
5 Discussion and future work
</sectionHeader>
<bodyText confidence="0.999723613636364">
We have presented a method for guiding a phrase-
based decoder with translation tables partitioned
on the basis of k-best list reranking making use
of complex features. Our results showed consis-
tent improvements in BLEU score over a strong
Rerank baseline using the same features. We ex-
perimented with a simple criterion for iteratively
partitioning the original phrase table of the sys-
tem, and found that focusing on providing the next
iteration decoder with the bi-phrases that were
prefered at first rank by Rerank (IN) performed
best.6
We now intend to study how to better take ad-
vantage of the expected characteristics of our IN
and OUT tables, possibly by adding more features
to our iteration-specific tables, or by exploiting
information on bi-phrases computed on the full
reranked lists. For our future work, we also plan to
study approaches that can enhance the diversity in
the k-best lists (Chatterjee and Cancedda, 2010;
Gimpel et al., 2013) between each iteration of
the multi-pass decoding to train a better Rerank
after each decoding pass. Another area for im-
provement lies in the addition of yet more com-
plex features, for instance to allow a better dis-
6Interestingly, a control experiment showed that using
iteration-specific tables yields slightly better performance
than fusioning all bi-phrases of a given type in a non iteration-
specific table, possibly allowing later tunings to prefer the
contents of the most recent, and possibly more reliable tables.
course coherence modelling over iterations (Ture
et al., 2012; Hardmeier et al., 2012). Going fur-
ther, we could study the effect of using other hy-
potheses instead of the Rerank one-best to per-
form the comparison with the Moses one-best hy-
pothesis. For instance, we can reasonably expect
that making this comparison with the output of a
rewriting system, such as the one proposed in our
previous work (Marie and Max, 2014), could ex-
tract more misused and useful bi-phrases on which
to base our translation table partitioning since this
rewriting system’s output is usually better than the
Rerank one-best and not in the k-best list of the
decoder.
</bodyText>
<sectionHeader confidence="0.997639" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999839285714286">
The authors would like to thank the anonymous re-
viewers for their helpful comments, Quoc Khanh
Do for his assistance on using SOUL, and Li Gong
and Nicolas P´echeux for providing the authors
with data used in the experiments. The work of
the first author is supported by a CIFRE grant from
French ANRT.
</bodyText>
<sectionHeader confidence="0.998608" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9994755">
Simon Carter and Christof Monz. 2011. Syntactic
Discriminative Language Model Rerankers for Sta-
tistical Machine Translation. Machine Translation,
25:317–339.
Samidh Chatterjee and Nicola Cancedda. 2010. Min-
imum error rate training by sampling the transla-
tion lattice. In Proceedings of EMNLP, Cambridge,
USA.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008a. Exploiting N-best Hypotheses for SMT Self-
</reference>
<page confidence="0.975977">
558
</page>
<reference confidence="0.9999428125">
Enhancement. In Proceedings ofACL, short papers,
Columbus, USA.
Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li.
2008b. Regenerating Hypotheses for Statistical Ma-
chine Translation. In Proceedings of COLING,
Manchester, UK.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of NAACL, Montr´eal, Canada.
Jacob Devlin and Spyros Matsoukas. 2012. Trait-
based Hypothesis Selection for Machine Transla-
tion. In Proceedings of NAACL, Montr´eal, Canada.
Kevin Gimpel, Dhruv Batra, Chris Dyer, Gregory
Shakhnarovich, and Virginia Tech. 2013. A Sys-
tematic Exploration of Diversity in Machine Trans-
lation. In Proceedings of EMNLP, Seatlle, USA.
Christian Hardmeier, Joakim Nivre, and J¨org Tiede-
mann. 2012. Document-wide decoding for phrase-
based statistical machine translation. In Proceed-
ings of EMNLP-CoNLL, Jeju Island, Korea.
Almut Silja Hildebrand and Stephan Vogel. 2008.
Combination of machine translation systems via hy-
pothesis selection from combined n-best lists. In
Proceedings of AMTA, Honolulu, USA.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings ofACL, demo session, Prague, Czech
Republic.
Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Proceedings of NAACL,
Montr´eal, Canada.
Benjamin Marie and Aur´elien Max. 2014.
Confidence-based Rewriting of Machine Translation
Output. In Proceedings of EMNLP, Doha, Qatar.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004.
A Smorgasbord of Features for Statistical Machine
Translation. In Proceedings of NAACL, Boston,
USA.
Lane Schwartz, Chris Callison-Burch, William
Schuler, and Stephen Wu. 2011. Incremental syn-
tactic language models for phrase-based translation.
In Proceedings of ACL, Portland, USA.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the Knowledge Sources Used in
a Maximum Entropy Part-of-speech Tagger. In Pro-
ceedings of EMNLP, Hong Kong.
Ferhan Ture, Douglas W. Oard, and Philip Resnik.
2012. Encouraging consistent translation choices.
In Proceedings of NAACL, Montr´eal, Canada.
Nicola Ueffing and Hermann Ney. 2007. Word-
Level Confidence Estimation for Machine Transla-
tion. Computational Linguistics, 33(1):9–40.
Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,
and Jingbo Zhu. 2013. Fast and Accurate Shift-
Reduce Constituent Parsing. In Proceedings ofACL,
Sofia, Bulgaria.
</reference>
<page confidence="0.99863">
559
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.445734">
<title confidence="0.999668">Multi-Pass Decoding With Complex Feature Guidance for Machine Translation</title>
<author confidence="0.985601">Benjamin</author>
<affiliation confidence="0.879593">LIMSI-CNRS, Orsay,</affiliation>
<address confidence="0.497208">Lingua et Machina, Le Chesnay,</address>
<email confidence="0.994762">benjamin.marie@limsi.fr</email>
<abstract confidence="0.998763619047619">In Statistical Machine Translation, some complex features are still difficult to integrate during decoding and usually used the reranking of the hypotheses produced by the decoder. We propose a translation table partitioning method that exploits the result of this reranking to iteratively guide the decoder order to produce a new list more relevant to some complex features. We report experiments on two translation domains and two translations directions which yield improvements of up to 1.4 BLEU over the reranking baseline using the same set of complex features. On a practical viewpoint, our approach allows SMT system developers to easily integrate complex features into decoding rather than being limited to their use in list reranking.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Simon Carter</author>
<author>Christof Monz</author>
</authors>
<title>Syntactic Discriminative Language Model Rerankers for Statistical Machine Translation. Machine Translation,</title>
<date>2011</date>
<pages>25--317</pages>
<contexts>
<context position="11337" citStr="Carter and Monz, 2011" startWordPosition="1835" endWordPosition="1838"> all the decoder features along with the following complex features: • MosesNorm: all decoder features and the Moses score normalized by the hypothesis length • NNM: bilingual and monolingual neural network models with a structured output layer (SOUL) (Le et al., 2012) • POSLM: 6-gram POS language model • WPP: count-based word posterior probability (Ueffing and Ney, 2007) • TagRatio: ratio of translation hypothesis by number of source tokens tagged as: verb, noun or adjective • Syntax: depth, number of nodes and number of unary rules of the syntactic parse normalized by the hypothesis length (Carter and Monz, 2011) • IBM1: IBM1 features (Och et al., 2004; Hildebrand and Vogel, 2008) Part-of-speech tagging and syntactic parsing were respectively performed with the Stanford Part-of-speech Tagger (Toutanova and Manning, 2000) and the Shift-Reduce parser of Zhu et al. (2013). We report the individual performance of each feature set in Table 2 and the Rerank performance when using all feature sets. As expected, the NNM feature set brings most of the improvements and attain by itself nearly the BLEU score of Rerank when using all feature sets for the news task with a gain of 1.4 and 1.1 BLEU respectively for </context>
</contexts>
<marker>Carter, Monz, 2011</marker>
<rawString>Simon Carter and Christof Monz. 2011. Syntactic Discriminative Language Model Rerankers for Statistical Machine Translation. Machine Translation, 25:317–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samidh Chatterjee</author>
<author>Nicola Cancedda</author>
</authors>
<title>Minimum error rate training by sampling the translation lattice.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Cambridge, USA.</location>
<contexts>
<context position="18034" citStr="Chatterjee and Cancedda, 2010" startWordPosition="2950" endWordPosition="2953"> simple criterion for iteratively partitioning the original phrase table of the system, and found that focusing on providing the next iteration decoder with the bi-phrases that were prefered at first rank by Rerank (IN) performed best.6 We now intend to study how to better take advantage of the expected characteristics of our IN and OUT tables, possibly by adding more features to our iteration-specific tables, or by exploiting information on bi-phrases computed on the full reranked lists. For our future work, we also plan to study approaches that can enhance the diversity in the k-best lists (Chatterjee and Cancedda, 2010; Gimpel et al., 2013) between each iteration of the multi-pass decoding to train a better Rerank after each decoding pass. Another area for improvement lies in the addition of yet more complex features, for instance to allow a better dis6Interestingly, a control experiment showed that using iteration-specific tables yields slightly better performance than fusioning all bi-phrases of a given type in a non iterationspecific table, possibly allowing later tunings to prefer the contents of the most recent, and possibly more reliable tables. course coherence modelling over iterations (Ture et al.,</context>
</contexts>
<marker>Chatterjee, Cancedda, 2010</marker>
<rawString>Samidh Chatterjee and Nicola Cancedda. 2010. Minimum error rate training by sampling the translation lattice. In Proceedings of EMNLP, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>Exploiting N-best Hypotheses for SMT SelfEnhancement.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL, short papers,</booktitle>
<location>Columbus, USA.</location>
<contexts>
<context position="3584" citStr="Chen et al. (2008" startWordPosition="565" endWordPosition="568">us far. On the test data, this procedure thus produces after each iteration a new decoder n-best, as well as an iteration-specific new reranker best hypothesis. We report consistent improvements of translation quality over a strong reranking baseline using the same features on 2 different domains and 2 translation directions. The remainder of this article is organized as follows: we first briefly review related work (Section 2), then introduce our approach (Section 3), describe our experiments (Section 4), and finally discuss our results and present our future work (Section 5). 2 Related Work Chen et al. (2008a; 2008b) expand the k-best list of the decoder using three methods. One of them involves re-decodings using models trained on the decoder k-best list to integrate posterior knowledge during the next re-decoding. The new k-best 554 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 554–559, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics list produced by the decoder is concatenated to the original one and then reranked with comp</context>
</contexts>
<marker>Chen, Zhang, Aw, Li, 2008</marker>
<rawString>Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li. 2008a. Exploiting N-best Hypotheses for SMT SelfEnhancement. In Proceedings ofACL, short papers, Columbus, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Boxing Chen</author>
<author>Min Zhang</author>
<author>Aiti Aw</author>
<author>Haizhou Li</author>
</authors>
<title>Regenerating Hypotheses for Statistical Machine Translation.</title>
<date>2008</date>
<booktitle>In Proceedings of COLING,</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="3584" citStr="Chen et al. (2008" startWordPosition="565" endWordPosition="568">us far. On the test data, this procedure thus produces after each iteration a new decoder n-best, as well as an iteration-specific new reranker best hypothesis. We report consistent improvements of translation quality over a strong reranking baseline using the same features on 2 different domains and 2 translation directions. The remainder of this article is organized as follows: we first briefly review related work (Section 2), then introduce our approach (Section 3), describe our experiments (Section 4), and finally discuss our results and present our future work (Section 5). 2 Related Work Chen et al. (2008a; 2008b) expand the k-best list of the decoder using three methods. One of them involves re-decodings using models trained on the decoder k-best list to integrate posterior knowledge during the next re-decoding. The new k-best 554 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 554–559, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics list produced by the decoder is concatenated to the original one and then reranked with comp</context>
</contexts>
<marker>Chen, Zhang, Aw, Li, 2008</marker>
<rawString>Boxing Chen, Min Zhang, Aiti Aw, and Haizhou Li. 2008b. Regenerating Hypotheses for Statistical Machine Translation. In Proceedings of COLING, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>George Foster</author>
</authors>
<title>Batch Tuning Strategies for Statistical Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="10462" citStr="Cherry and Foster, 2012" startWordPosition="1694" endWordPosition="1697">ne systems using data provided by WMT4. Statistics about the training, development and testing data are presented in Table 1. Tasks Corpus Sentences Tokens (Fr-En) train 12M 383M - 318M news dev 2,525 73k - 65k test 3,003 85k - 74k train 4.9M 91M - 78M medical dev 500 12k - 10k test 1,000 26k - 21k in-domain LM 146M - 78M for both tasks LM 2.5B - 6B Table 1: Data used in our experiments. 4.2 MT system For our experiments we used the Moses phrasebased SMT toolkit (Koehn et al., 2007) with default settings and features, including the five features from the translation table, and kb-mira tuning (Cherry and Foster, 2012). Rerank is trained using kb-mira on the 1,000-best list generated by Moses on the development set with the 4http://www.statmt.org/wmt14 distinct-nbest parameter to have no duplicates. Testing is also performed on distinct 1,000- best lists. Rerank uses all the decoder features along with the following complex features: • MosesNorm: all decoder features and the Moses score normalized by the hypothesis length • NNM: bilingual and monolingual neural network models with a structured output layer (SOUL) (Le et al., 2012) • POSLM: 6-gram POS language model • WPP: count-based word posterior probabil</context>
</contexts>
<marker>Cherry, Foster, 2012</marker>
<rawString>Colin Cherry and George Foster. 2012. Batch Tuning Strategies for Statistical Machine Translation. In Proceedings of NAACL, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Devlin</author>
<author>Spyros Matsoukas</author>
</authors>
<title>Traitbased Hypothesis Selection for Machine Translation.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="5175" citStr="Devlin and Matsoukas, 2012" startWordPosition="817" endWordPosition="820">king pass using complex features, assuming that a better hypothesis can be very close to this seed hypothesis (Marie and Max, 2014). Nevertheless, this rewriting only explores a small search space, limited by the greedy search algorithm that concentrates on individual, local rewritings. Other works proposed methods to produce more diverse lists of hypotheses by iteratively encouraging the decoder to produce translations that are different from the previous one (Gimpel et al., 2013) or by making small changes to the scoring function to extract k-best lists from other parts of the search space (Devlin and Matsoukas, 2012). Some useful diversity can be obtained as these hypotheses can be combined using SMT system combination or help to better train reranking systems. But in spite of the introduction of more diversity, these methods do not guarantee that eventually lists containing hypotheses that are more relevant to complex features will be obtained. 3 Translation Table Partitioning 3.1 Exploiting the Reranking Pass Result Because all bi-phrases initially belong to the same translation table, they share their feature weights after tuning. Our main idea is to partition the set of bi-phrases by putting aside, in</context>
</contexts>
<marker>Devlin, Matsoukas, 2012</marker>
<rawString>Jacob Devlin and Spyros Matsoukas. 2012. Traitbased Hypothesis Selection for Machine Translation. In Proceedings of NAACL, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Dhruv Batra</author>
<author>Chris Dyer</author>
<author>Gregory Shakhnarovich</author>
<author>Virginia Tech</author>
</authors>
<title>A Systematic Exploration of Diversity in Machine Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Seatlle, USA.</location>
<contexts>
<context position="5034" citStr="Gimpel et al., 2013" startWordPosition="792" endWordPosition="795">ently, we proposed a rewriting system that explores in a greedy fashion the neighborhood of the one-best hypothesis found by the reranking pass using complex features, assuming that a better hypothesis can be very close to this seed hypothesis (Marie and Max, 2014). Nevertheless, this rewriting only explores a small search space, limited by the greedy search algorithm that concentrates on individual, local rewritings. Other works proposed methods to produce more diverse lists of hypotheses by iteratively encouraging the decoder to produce translations that are different from the previous one (Gimpel et al., 2013) or by making small changes to the scoring function to extract k-best lists from other parts of the search space (Devlin and Matsoukas, 2012). Some useful diversity can be obtained as these hypotheses can be combined using SMT system combination or help to better train reranking systems. But in spite of the introduction of more diversity, these methods do not guarantee that eventually lists containing hypotheses that are more relevant to complex features will be obtained. 3 Translation Table Partitioning 3.1 Exploiting the Reranking Pass Result Because all bi-phrases initially belong to the sa</context>
<context position="18056" citStr="Gimpel et al., 2013" startWordPosition="2954" endWordPosition="2957">ly partitioning the original phrase table of the system, and found that focusing on providing the next iteration decoder with the bi-phrases that were prefered at first rank by Rerank (IN) performed best.6 We now intend to study how to better take advantage of the expected characteristics of our IN and OUT tables, possibly by adding more features to our iteration-specific tables, or by exploiting information on bi-phrases computed on the full reranked lists. For our future work, we also plan to study approaches that can enhance the diversity in the k-best lists (Chatterjee and Cancedda, 2010; Gimpel et al., 2013) between each iteration of the multi-pass decoding to train a better Rerank after each decoding pass. Another area for improvement lies in the addition of yet more complex features, for instance to allow a better dis6Interestingly, a control experiment showed that using iteration-specific tables yields slightly better performance than fusioning all bi-phrases of a given type in a non iterationspecific table, possibly allowing later tunings to prefer the contents of the most recent, and possibly more reliable tables. course coherence modelling over iterations (Ture et al., 2012; Hardmeier et al</context>
</contexts>
<marker>Gimpel, Batra, Dyer, Shakhnarovich, Tech, 2013</marker>
<rawString>Kevin Gimpel, Dhruv Batra, Chris Dyer, Gregory Shakhnarovich, and Virginia Tech. 2013. A Systematic Exploration of Diversity in Machine Translation. In Proceedings of EMNLP, Seatlle, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Hardmeier</author>
<author>Joakim Nivre</author>
<author>J¨org Tiedemann</author>
</authors>
<title>Document-wide decoding for phrasebased statistical machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL, Jeju Island,</booktitle>
<contexts>
<context position="18664" citStr="Hardmeier et al., 2012" startWordPosition="3049" endWordPosition="3052">l et al., 2013) between each iteration of the multi-pass decoding to train a better Rerank after each decoding pass. Another area for improvement lies in the addition of yet more complex features, for instance to allow a better dis6Interestingly, a control experiment showed that using iteration-specific tables yields slightly better performance than fusioning all bi-phrases of a given type in a non iterationspecific table, possibly allowing later tunings to prefer the contents of the most recent, and possibly more reliable tables. course coherence modelling over iterations (Ture et al., 2012; Hardmeier et al., 2012). Going further, we could study the effect of using other hypotheses instead of the Rerank one-best to perform the comparison with the Moses one-best hypothesis. For instance, we can reasonably expect that making this comparison with the output of a rewriting system, such as the one proposed in our previous work (Marie and Max, 2014), could extract more misused and useful bi-phrases on which to base our translation table partitioning since this rewriting system’s output is usually better than the Rerank one-best and not in the k-best list of the decoder. Acknowledgments The authors would like </context>
</contexts>
<marker>Hardmeier, Nivre, Tiedemann, 2012</marker>
<rawString>Christian Hardmeier, Joakim Nivre, and J¨org Tiedemann. 2012. Document-wide decoding for phrasebased statistical machine translation. In Proceedings of EMNLP-CoNLL, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Almut Silja Hildebrand</author>
<author>Stephan Vogel</author>
</authors>
<title>Combination of machine translation systems via hypothesis selection from combined n-best lists.</title>
<date>2008</date>
<booktitle>In Proceedings of AMTA,</booktitle>
<location>Honolulu, USA.</location>
<contexts>
<context position="11406" citStr="Hildebrand and Vogel, 2008" startWordPosition="1847" endWordPosition="1850">res: • MosesNorm: all decoder features and the Moses score normalized by the hypothesis length • NNM: bilingual and monolingual neural network models with a structured output layer (SOUL) (Le et al., 2012) • POSLM: 6-gram POS language model • WPP: count-based word posterior probability (Ueffing and Ney, 2007) • TagRatio: ratio of translation hypothesis by number of source tokens tagged as: verb, noun or adjective • Syntax: depth, number of nodes and number of unary rules of the syntactic parse normalized by the hypothesis length (Carter and Monz, 2011) • IBM1: IBM1 features (Och et al., 2004; Hildebrand and Vogel, 2008) Part-of-speech tagging and syntactic parsing were respectively performed with the Stanford Part-of-speech Tagger (Toutanova and Manning, 2000) and the Shift-Reduce parser of Zhu et al. (2013). We report the individual performance of each feature set in Table 2 and the Rerank performance when using all feature sets. As expected, the NNM feature set brings most of the improvements and attain by itself nearly the BLEU score of Rerank when using all feature sets for the news task with a gain of 1.4 and 1.1 BLEU respectively for En→Fr and Fr→En over the Moses 556 Figure 2: BLEU score evolution ove</context>
</contexts>
<marker>Hildebrand, Vogel, 2008</marker>
<rawString>Almut Silja Hildebrand and Stephan Vogel. 2008. Combination of machine translation systems via hypothesis selection from combined n-best lists. In Proceedings of AMTA, Honolulu, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings ofACL, demo session,</booktitle>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra</location>
<contexts>
<context position="10325" citStr="Koehn et al., 2007" startWordPosition="1672" endWordPosition="1675">d the WMT’11 news translation task (news) for the language pair FrEn on both directions. For both tasks we trained two strong baseline systems using data provided by WMT4. Statistics about the training, development and testing data are presented in Table 1. Tasks Corpus Sentences Tokens (Fr-En) train 12M 383M - 318M news dev 2,525 73k - 65k test 3,003 85k - 74k train 4.9M 91M - 78M medical dev 500 12k - 10k test 1,000 26k - 21k in-domain LM 146M - 78M for both tasks LM 2.5B - 6B Table 1: Data used in our experiments. 4.2 MT system For our experiments we used the Moses phrasebased SMT toolkit (Koehn et al., 2007) with default settings and features, including the five features from the translation table, and kb-mira tuning (Cherry and Foster, 2012). Rerank is trained using kb-mira on the 1,000-best list generated by Moses on the development set with the 4http://www.statmt.org/wmt14 distinct-nbest parameter to have no duplicates. Testing is also performed on distinct 1,000- best lists. Rerank uses all the decoder features along with the following complex features: • MosesNorm: all decoder features and the Moses score normalized by the hypothesis length • NNM: bilingual and monolingual neural network mod</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings ofACL, demo session, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous Space Translation Models with Neural Networks.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="10984" citStr="Le et al., 2012" startWordPosition="1775" endWordPosition="1778">ing the five features from the translation table, and kb-mira tuning (Cherry and Foster, 2012). Rerank is trained using kb-mira on the 1,000-best list generated by Moses on the development set with the 4http://www.statmt.org/wmt14 distinct-nbest parameter to have no duplicates. Testing is also performed on distinct 1,000- best lists. Rerank uses all the decoder features along with the following complex features: • MosesNorm: all decoder features and the Moses score normalized by the hypothesis length • NNM: bilingual and monolingual neural network models with a structured output layer (SOUL) (Le et al., 2012) • POSLM: 6-gram POS language model • WPP: count-based word posterior probability (Ueffing and Ney, 2007) • TagRatio: ratio of translation hypothesis by number of source tokens tagged as: verb, noun or adjective • Syntax: depth, number of nodes and number of unary rules of the syntactic parse normalized by the hypothesis length (Carter and Monz, 2011) • IBM1: IBM1 features (Och et al., 2004; Hildebrand and Vogel, 2008) Part-of-speech tagging and syntactic parsing were respectively performed with the Stanford Part-of-speech Tagger (Toutanova and Manning, 2000) and the Shift-Reduce parser of Zhu</context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous Space Translation Models with Neural Networks. In Proceedings of NAACL, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Marie</author>
<author>Aur´elien Max</author>
</authors>
<title>Confidence-based Rewriting of Machine Translation Output.</title>
<date>2014</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<location>Doha, Qatar.</location>
<contexts>
<context position="4679" citStr="Marie and Max, 2014" startWordPosition="738" endWordPosition="741">on for Computational Linguistics list produced by the decoder is concatenated to the original one and then reranked with complex features, which yields improvements over a reranking performed on the original k-best list. The reranking pass is done out of the loop and the redecodings do not exploit the reranking result that used the complex features. Recently, we proposed a rewriting system that explores in a greedy fashion the neighborhood of the one-best hypothesis found by the reranking pass using complex features, assuming that a better hypothesis can be very close to this seed hypothesis (Marie and Max, 2014). Nevertheless, this rewriting only explores a small search space, limited by the greedy search algorithm that concentrates on individual, local rewritings. Other works proposed methods to produce more diverse lists of hypotheses by iteratively encouraging the decoder to produce translations that are different from the previous one (Gimpel et al., 2013) or by making small changes to the scoring function to extract k-best lists from other parts of the search space (Devlin and Matsoukas, 2012). Some useful diversity can be obtained as these hypotheses can be combined using SMT system combination</context>
</contexts>
<marker>Marie, Max, 2014</marker>
<rawString>Benjamin Marie and Aur´elien Max. 2014. Confidence-based Rewriting of Machine Translation Output. In Proceedings of EMNLP, Doha, Qatar.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Daniel Gildea</author>
</authors>
<title>Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar,</title>
<date>2004</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen</location>
<marker>Och, Gildea, 2004</marker>
<rawString>Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A Smorgasbord of Features for Statistical Machine Translation. In Proceedings of NAACL, Boston, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lane Schwartz</author>
<author>Chris Callison-Burch</author>
<author>William Schuler</author>
<author>Stephen Wu</author>
</authors>
<title>Incremental syntactic language models for phrase-based translation.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<location>Portland, USA.</location>
<contexts>
<context position="12410" citStr="Schwartz et al., 2011" startWordPosition="2015" endWordPosition="2018"> and attain by itself nearly the BLEU score of Rerank when using all feature sets for the news task with a gain of 1.4 and 1.1 BLEU respectively for En→Fr and Fr→En over the Moses 556 Figure 2: BLEU score evolution over iterations for the IN configuration on the test set of the medical En→Fr translation task. baseline. Among the other feature sets, POSLM performs well, especially for the medical task with an improvement of 0.3 and 0.5 BLEU for En→Fr and Fr→En, respectively. Some types of our complex features have already been used during decoding, although sometimes for a very important cost (Schwartz et al., 2011). Our feature sets are to be considered only as experimental parameters, as any other feature types usually used during reranking could also be used. medical news Features En→Fr Fr→En En→Fr Fr→En Moses 38.8 37.1 31.1 28.6 + MosesNorm 38.9 37.2 31.1 28.7 + NNM 41.9 38.9 32.5 29.8 + POSLM 39.2 37.7 31.1 28.9 + WPP 39.1 37.1 31.2 28.6 + TagRatio 38.9 37.3 31.1 28.8 + Syntax 38.8 37.2 31.2 28.9 + IBM1 39.1 37.2 30.9 28.8 Rerank 42.8 40.1 32.5 29.9 Table 2: Reranking results for each set of features added individually; Rerank uses the full set. 4.3 Results Table 3 presents our results for different</context>
</contexts>
<marker>Schwartz, Callison-Burch, Schuler, Wu, 2011</marker>
<rawString>Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu. 2011. Incremental syntactic language models for phrase-based translation. In Proceedings of ACL, Portland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-speech Tagger.</title>
<date>2000</date>
<booktitle>In Proceedings of EMNLP, Hong Kong.</booktitle>
<contexts>
<context position="11549" citStr="Toutanova and Manning, 2000" startWordPosition="1864" endWordPosition="1867"> models with a structured output layer (SOUL) (Le et al., 2012) • POSLM: 6-gram POS language model • WPP: count-based word posterior probability (Ueffing and Ney, 2007) • TagRatio: ratio of translation hypothesis by number of source tokens tagged as: verb, noun or adjective • Syntax: depth, number of nodes and number of unary rules of the syntactic parse normalized by the hypothesis length (Carter and Monz, 2011) • IBM1: IBM1 features (Och et al., 2004; Hildebrand and Vogel, 2008) Part-of-speech tagging and syntactic parsing were respectively performed with the Stanford Part-of-speech Tagger (Toutanova and Manning, 2000) and the Shift-Reduce parser of Zhu et al. (2013). We report the individual performance of each feature set in Table 2 and the Rerank performance when using all feature sets. As expected, the NNM feature set brings most of the improvements and attain by itself nearly the BLEU score of Rerank when using all feature sets for the news task with a gain of 1.4 and 1.1 BLEU respectively for En→Fr and Fr→En over the Moses 556 Figure 2: BLEU score evolution over iterations for the IN configuration on the test set of the medical En→Fr translation task. baseline. Among the other feature sets, POSLM perf</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the Knowledge Sources Used in a Maximum Entropy Part-of-speech Tagger. In Proceedings of EMNLP, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ferhan Ture</author>
<author>Douglas W Oard</author>
<author>Philip Resnik</author>
</authors>
<title>Encouraging consistent translation choices.</title>
<date>2012</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="18639" citStr="Ture et al., 2012" startWordPosition="3045" endWordPosition="3048">ncedda, 2010; Gimpel et al., 2013) between each iteration of the multi-pass decoding to train a better Rerank after each decoding pass. Another area for improvement lies in the addition of yet more complex features, for instance to allow a better dis6Interestingly, a control experiment showed that using iteration-specific tables yields slightly better performance than fusioning all bi-phrases of a given type in a non iterationspecific table, possibly allowing later tunings to prefer the contents of the most recent, and possibly more reliable tables. course coherence modelling over iterations (Ture et al., 2012; Hardmeier et al., 2012). Going further, we could study the effect of using other hypotheses instead of the Rerank one-best to perform the comparison with the Moses one-best hypothesis. For instance, we can reasonably expect that making this comparison with the output of a rewriting system, such as the one proposed in our previous work (Marie and Max, 2014), could extract more misused and useful bi-phrases on which to base our translation table partitioning since this rewriting system’s output is usually better than the Rerank one-best and not in the k-best list of the decoder. Acknowledgment</context>
</contexts>
<marker>Ture, Oard, Resnik, 2012</marker>
<rawString>Ferhan Ture, Douglas W. Oard, and Philip Resnik. 2012. Encouraging consistent translation choices. In Proceedings of NAACL, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>WordLevel Confidence Estimation for Machine Translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="11089" citStr="Ueffing and Ney, 2007" startWordPosition="1792" endWordPosition="1795">ank is trained using kb-mira on the 1,000-best list generated by Moses on the development set with the 4http://www.statmt.org/wmt14 distinct-nbest parameter to have no duplicates. Testing is also performed on distinct 1,000- best lists. Rerank uses all the decoder features along with the following complex features: • MosesNorm: all decoder features and the Moses score normalized by the hypothesis length • NNM: bilingual and monolingual neural network models with a structured output layer (SOUL) (Le et al., 2012) • POSLM: 6-gram POS language model • WPP: count-based word posterior probability (Ueffing and Ney, 2007) • TagRatio: ratio of translation hypothesis by number of source tokens tagged as: verb, noun or adjective • Syntax: depth, number of nodes and number of unary rules of the syntactic parse normalized by the hypothesis length (Carter and Monz, 2011) • IBM1: IBM1 features (Och et al., 2004; Hildebrand and Vogel, 2008) Part-of-speech tagging and syntactic parsing were respectively performed with the Stanford Part-of-speech Tagger (Toutanova and Manning, 2000) and the Shift-Reduce parser of Zhu et al. (2013). We report the individual performance of each feature set in Table 2 and the Rerank perfor</context>
</contexts>
<marker>Ueffing, Ney, 2007</marker>
<rawString>Nicola Ueffing and Hermann Ney. 2007. WordLevel Confidence Estimation for Machine Translation. Computational Linguistics, 33(1):9–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Muhua Zhu</author>
<author>Yue Zhang</author>
<author>Wenliang Chen</author>
<author>Min Zhang</author>
<author>Jingbo Zhu</author>
</authors>
<title>Fast and Accurate ShiftReduce Constituent Parsing.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL,</booktitle>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="11598" citStr="Zhu et al. (2013)" startWordPosition="1873" endWordPosition="1876">12) • POSLM: 6-gram POS language model • WPP: count-based word posterior probability (Ueffing and Ney, 2007) • TagRatio: ratio of translation hypothesis by number of source tokens tagged as: verb, noun or adjective • Syntax: depth, number of nodes and number of unary rules of the syntactic parse normalized by the hypothesis length (Carter and Monz, 2011) • IBM1: IBM1 features (Och et al., 2004; Hildebrand and Vogel, 2008) Part-of-speech tagging and syntactic parsing were respectively performed with the Stanford Part-of-speech Tagger (Toutanova and Manning, 2000) and the Shift-Reduce parser of Zhu et al. (2013). We report the individual performance of each feature set in Table 2 and the Rerank performance when using all feature sets. As expected, the NNM feature set brings most of the improvements and attain by itself nearly the BLEU score of Rerank when using all feature sets for the news task with a gain of 1.4 and 1.1 BLEU respectively for En→Fr and Fr→En over the Moses 556 Figure 2: BLEU score evolution over iterations for the IN configuration on the test set of the medical En→Fr translation task. baseline. Among the other feature sets, POSLM performs well, especially for the medical task with a</context>
</contexts>
<marker>Zhu, Zhang, Chen, Zhang, Zhu, 2013</marker>
<rawString>Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. 2013. Fast and Accurate ShiftReduce Constituent Parsing. In Proceedings ofACL, Sofia, Bulgaria.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>