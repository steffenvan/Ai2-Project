<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.377657">
Whitepaper of NEWS 2009 Machine Transliteration Shared Task*
</title>
<author confidence="0.616184">
Haizhou Li†, A Kumaran$, Min Zhang† and Vladimir Pervouchine†
</author>
<affiliation confidence="0.607561">
†Institute for Infocomm Research, A*STAR, Singapore 138632
</affiliation>
<email confidence="0.694442">
{hli,mzhang,vpervouchine}@i2r.a-star.edu.sg
</email>
<note confidence="0.647169">
$Multilingual Systems Research, Microsoft Research India
</note>
<email confidence="0.992584">
A.Kumaran@microsoft.com
</email>
<sectionHeader confidence="0.994659" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994">
Transliteration is defined as phonetic
translation of names across languages.
Transliteration of Named Entities (NEs)
is necessary in many applications, such
as machine translation, corpus alignment,
cross-language IR, information extraction
and automatic lexicon acquisition. All
such systems call for high-performance
transliteration, which is the focus of the
shared task in the NEWS 2009 workshop.
The objective of the shared task is to pro-
mote machine transliteration research by
providing a common benchmarking plat-
form for the community to evaluate the
state-of-the-art technologies.
</bodyText>
<sectionHeader confidence="0.96792" genericHeader="keywords">
1 Task Description
</sectionHeader>
<bodyText confidence="0.999205190476191">
The task is to develop machine transliteration sys-
tem in one or more of the specified language pairs
being considered for the task. Each language pair
consists of a source and a target language. The
training and development data sets released for
each language pair are to be used for developing
a transliteration system in whatever way that the
participants find appropriate. At the evaluation
time, a test set of source names only would be re-
leased, on which the participants are expected to
produce a ranked list of transliteration candidates
in another language (i.e. n-best transliterations),
and this will be evaluated using common metrics.
For every language pair the participants must sub-
mit one run that uses only the data provided by the
NEWS workshop organisers in a given language
pair (designated as “standard” runs). Users may
submit more runs (“non-standard”) for each lan-
guage pair that uses other data than those provided
by the NEWS 2009 workshop; such runs would be
evaluated and reported separately.
</bodyText>
<footnote confidence="0.390412">
*http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/
</footnote>
<sectionHeader confidence="0.877188" genericHeader="introduction">
2 Important Dates
</sectionHeader>
<table confidence="0.633967461538462">
Research paper submission deadline 1 May 2009
Shared task
Registration opens 16 Feb 2009
Registration closes 9 Apr 2009
Release Training/Development Data 16 Feb 2009
Release Test Data 10 Apr 2009
Results Submission Due 14 Apr 2009
Results Announcement 29 Apr 2009
Task (short) Papers Due 3 May 2009
For all submissions
Acceptance Notification 1 Jun 2009
Camera-Ready Copy Deadline 7 Jun 2009
Workshop Date 7 Aug 2009
</table>
<sectionHeader confidence="0.843342" genericHeader="method">
3 Participation
</sectionHeader>
<listItem confidence="0.896056421052632">
1. Registration (16 Feb 2009)
(a) NEWS Shared Task opens for registra-
tion.
(b) Prospective participants are to register to
the NEWS Workshop homepage.
2. Training &amp; Development Data (16 Feb 2009)
(a) Registered participants are to obtain
training and development data from the
Shared Task organiser and/or the desig-
nated copyright owners of databases.
3. Evaluation Script (16 Mar 2009)
(a) A sample test set and expected user out-
put format are to be released.
(b) An evaluation script, which runs on the
above two, is to be released.
(c) The participants must make sure that
their output is produced in a way that
the evaluation script may run and pro-
duce the expected output.
</listItem>
<page confidence="0.989424">
19
</page>
<note confidence="0.982765">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 19–26,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<listItem confidence="0.9797618125">
(d) The same script (with held out test data
and the user outputs) would be used for
final evaluation.
4. Test data (10 April 2009)
(a) The test data would be released on 10
Apr 2009, and the participants have a
maximum of 4 days to submit their re-
sults in the expected format.
(b) Only 1 “standard” run must be submit-
ted from every group on a given lan-
guage pair; more “non-standard” runs (0
to 4) may be submitted. In total, maxi-
mum 5 runs (1 “standard” run plus up to
4 “non-standard” runs) can be submit-
ted from each group on a registered lan-
guage pair.
(c) Any runs that are “non-standard” must
be tagged as such.
(d) The test set is a list of names in source
language only. Every group will pro-
duce and submit a ranked list of translit-
eration candidates in another language
for each given name in the test set.
Please note that this shared task is a
“transliteration generation” task, i.e.,
given a name in a source language one
is supposed to generate one or more
transliterations in a target language. It
is not the task of “transliteration discov-
ery”, i.e., given a name in the source lan-
guage and a set of names in the target
language evaluate how to find the ap-
propriate names from the target set that
are transliterations of the given source
name.
5. Results (29 April 2009)
(a) On 29 April 2009, the evaluation results
would be announced and will be made
available on the Workshop website.
(b) Note that only the scores (in respective
metrics) of the participating systems on
each language pairs would be published,
and no explicit ranking of the participat-
ing systems would be published.
(c) Note that this is a shared evaluation task
and not a competition; the results are
meant to be used to evaluate systems on
common data set with common metrics,
</listItem>
<bodyText confidence="0.918899545454546">
and not to rank the participating sys-
tems. While the participants can cite the
performance of their systems (scores on
metrics) from the workshop report, they
should not use any ranking information
in their publications.
(d) Further, all participants should agree not
to reveal identities of other participants
in any of their publications unless you
get permission from the other respective
participants. If the participants want
to remain anonymous in published
results, they should inform the or-
ganisers (mzhang@i2r.a-star.edu.sg,
a.kumaran@microsoft.com), at the time
of registration. Note that the results of
their systems would still be published,
but with the participant identities
masked. As a result, in this case, your
organisation name will still appear in
the web site as one of participants, but it
is not linked explicitly with your results.
</bodyText>
<listItem confidence="0.996732173913044">
6. Short Papers on Task (3 May 2009)
(a) Each submitting site is required to sub-
mit a 4-page system paper (short paper)
for its submissions, including their ap-
proach, data used and the results on ei-
ther test set or development set or by n-
fold cross validation on training set.
(b) All system short papers will be included
in the proceedings. Selected short pa-
pers will be presented orally in the
NEWS 2009 workshop. Reviewers’
comments for all system short papers
and the acceptance notification for the
system short papers for oral presentation
would be announced on 1 June 2009 to-
gether with that of other papers.
(c) All registered participants are required
to register and attend the workshop to
introduce your work.
(d) All paper submission and review
will be managed electronically
through https://www.softconf.com/acl-
ijcnlp09/NEWS/.
</listItem>
<sectionHeader confidence="0.964967" genericHeader="method">
4 Languages Involved
</sectionHeader>
<bodyText confidence="0.999909333333333">
The tasks are to transliterate personal names or
place names from a source to a target language as
summarised in Table 1.
</bodyText>
<page confidence="0.974282">
20
</page>
<table confidence="0.999251888888889">
Source language Target language Data Owner Approx. Data Size Task ID
English Chinese Institute for Infocomm Research 30K EnCh
English Japanese Katakana CJK Institute 25K EnJa
English Korean Hangul CJK Institute 7K EnKo
Japanese name (in English) Japanese Kanji CJK Institute 20K JnJk
English Hindi Microsoft Research India 15K EnHi
English Tamil Microsoft Research India 15K EnTa
English Kannada Microsoft Research India 15K EnKa
English Russian Microsoft Research India 10K EnRu
</table>
<tableCaption confidence="0.999834">
Table 1: Source and target languages for the shared task on transliteration.
</tableCaption>
<bodyText confidence="0.997180285714286">
The names given in the training sets for Chi-
nese, Japanese and Korean languages are Western
names and their CJK transliterations; the Japanese
Name (in English) → Japanese Kanji data set con-
sists only of native Japanese names. The Indic data
set (Hindi, Tamil, Kannada) consists of a mix of
Indian and Western names.
</bodyText>
<figure confidence="0.9710663125">
English → Chinese
Timothy →�#,�ff
English → Japanese Katakana
Harrington → �9✓r�
English → Korean Hangul
Bennett → 베넷
Japanese name in English → Japanese Kanji
Akihiro → tk9A=
English → Hindi
San Francisco → सैन फ्रान्सिस्को
English → Tamil
London → 6u6o* —bar
English → Kannada
Tokyo → dtraeeo-ttrae
English → Russian
Moscow → Moc&amp;quot;a
</figure>
<sectionHeader confidence="0.769073" genericHeader="method">
5 Standard Databases
</sectionHeader>
<subsectionHeader confidence="0.935813">
Training Data (Parallel)
</subsectionHeader>
<bodyText confidence="0.99461175">
Paired names between source and target lan-
guages; size 5K – 40K.
Training Data is used for training a basic
transliteration system.
</bodyText>
<subsectionHeader confidence="0.81791">
Development Data (Parallel)
</subsectionHeader>
<bodyText confidence="0.9982115">
Paired names between source and target lan-
guages; size 1K – 2K.
Development Data is in addition to the Train-
ing data, which is used for system fine-tuning
of parameters in case of need. Participants
are allowed to use it as part of training data.
</bodyText>
<subsectionHeader confidence="0.84942">
Testing Data
</subsectionHeader>
<bodyText confidence="0.9534635">
Source names only; size 1K – 3K.
This is a held-out set, which would be used
for evaluating the quality of the translitera-
tions.
</bodyText>
<listItem confidence="0.884272375">
1. Participants will need to obtain licenses from
the respective copyright owners and/or agree
to the terms and conditions of use that are
given on the downloading website (Li et al.,
2004; Kumaran and Kellner, 2007; MSRI,
2009; CJKI, 2009). NEWS 2009 will pro-
vide the contact details of each individual
database. The data would be provided in Uni-
code UTF-8 encoding, in XML format; the
results are expected to be submitted in XML
format. The XML formats will be announced
at the workshop website.
2. The data are provided in 3 sets as described
above.
3. Name pairs are distributed as-is, as provided
by the respective creators.
(a) While the databases are mostly man-
ually checked, there may be still in-
consistency (that is, non-standard usage,
region-specific usage, errors, etc.) or in-
completeness (that is, not all right varia-
tions may be covered).
(b) The participants may use any method to
further clean up the data provided.
</listItem>
<bodyText confidence="0.967678">
i. If they are cleaned up manually, we
appeal that such data be provided
back to the organisers for redistri-
bution to all the participating groups
in that language pair; such sharing
benefits all participants, and further
</bodyText>
<page confidence="0.995871">
21
</page>
<bodyText confidence="0.9876486875">
ensures that the evaluation provides
normalisation with respect to data
quality.
ii. If automatic cleanup were used,
such cleanup would be considered a
part of the system fielded, and hence
not required to be shared with all
participants.
4. We expect that the participants to use only the
data (parallel names) provided by the Shared
Task for transliteration task for a “standard”
run to ensure a fair evaluation. One such run
(using only the data provided by the shared
task) is mandatory for all participants for a
given language pair that they participate in.
5. If more data (either parallel names data or
monolingual data) were used, then all such
runs using extra data must be marked as
“non-standard”. For such “non-standard”
runs, it is required to disclose the size and
characteristics of the data used in the system
paper.
of these alternatives are considered as a correct
transliteration, and the first correct transliteration
in the ranked list is accepted as a correct hit.
The following notation is further assumed:
N : Total number of names (source
words) in the test set
ni : Number of reference transliterations
for i-th name in the test set (ni ≥ 1)
ri,j : j-th reference transliteration for i-th
name in the test set
</bodyText>
<listItem confidence="0.6252475">
ci,k : k-th candidate transliteration (system
output) for i-th name in the test set
(1 ≤ k ≤ 10)
Ki : Number of candidate transliterations
produced by a transliteration system
1. Word Accuracy in Top-1 (ACC) Also
</listItem>
<bodyText confidence="0.539886">
known as Word Error Rate, it measures correct-
ness of the first transliteration candidate in the can-
didate list produced by a transliteration system.
ACC = 1 means that all top candidates are cor-
rect transliterations i.e. they match one of the ref-
erences, and ACC = 0 means that none of the top
candidates are correct.
6. A participant may submit a maximum of 5 1 �N 1 if ∃ri,j : ri,j = ci,1;
runs for a given language pair (including the ACC = N { }
mandatory 1 “standard” run). 0 otherwise
i=1
</bodyText>
<sectionHeader confidence="0.988372" genericHeader="method">
6 Paper Format
</sectionHeader>
<bodyText confidence="0.9996645">
Paper submissions to NEWS 2009 should follow
the ACL-IJCNLP-2009 paper submission policy,
including paper format, blind review policy and ti-
tle and author format convention. Full papers (re-
search paper) are in two-column format without
exceeding eight (8) pages of content plus one extra
page for references and short papers (task paper)
are also in two-column format without exceeding
four (4) pages, including references. Submission
must conform to the official ACL-IJCNLP-2009
style guidelines. For details, please refer to the
website2.
</bodyText>
<sectionHeader confidence="0.995326" genericHeader="method">
7 Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.999856142857143">
We plan to measure the quality of the translitera-
tion task using the following 6 metrics. We accept
up to 10 output candidates in a ranked list for each
input entry.
Since a given source name may have multiple
correct target transliterations, all these alternatives
are treated equally in the evaluation. That is, any
</bodyText>
<equation confidence="0.792427">
(1)
</equation>
<sectionHeader confidence="0.377868" genericHeader="method">
2. Fuzziness in Top-1 (Mean F-score) The
</sectionHeader>
<bodyText confidence="0.9462574">
mean F-score measures how different, on average,
the top transliteration candidate is from its closest
reference. F-score for each source word is a func-
tion of Precision and Recall and equals 1 when the
top candidate matches one of the references, and
0 when there are no common characters between
the candidate and any of the references.
Precision and Recall are calculated based on the
length of the Longest Common Subsequence be-
tween a candidate and a reference:
</bodyText>
<equation confidence="0.975361">
1
LC�(c, r) = 2 (|c |+ |r |− ED(c, r)) (2)
</equation>
<bodyText confidence="0.993087571428571">
where ED is the edit distance and |x |is the length
of x. For example, the longest common subse-
quence between “abcd” and “afcde” is “acd” and
its length is 3. The best matching reference, that
is, the reference for which the edit distance has
the minimum, is taken for calculation. If the best
matching reference is given by
</bodyText>
<footnote confidence="0.905610333333333">
ri,m = arg min (ED(ci,1, ri,j)) (3)
j
2http://www.acl-ijcnlp-2009.org/main/authors/stylefiles/index.html
</footnote>
<page confidence="0.99035">
22
</page>
<table confidence="0.848144909090909">
then Recall, Precision and F-score for i-th word
are calculated as
Ri = =
Pi LC5(Ci,1, ri,m)
Fi
=
|ri,m|
LC�(�i,1, ri,m)
|�i,1|
2Ri x Pi
Ri + Pi
</table>
<listItem confidence="0.883905625">
• The length is computed in distinct Unicode
characters.
• No distinction is made on different character
types of a language (e.g., vowel vs. conso-
nants vs. combining diereses’ etc.)
3. Mean Reciprocal Rank (MRR) Measures
traditional MRR for any right answer produced by
the system, from among the candidates. 1/MRR
</listItem>
<bodyText confidence="0.943971">
tells approximately the average rank of the correct
transliteration. MRR closer to 1 implies that the
correct answer is mostly produced close to the top
of the n-best lists.
</bodyText>
<equation confidence="0.987339571428571">
� RRi
minj j if Iri,Ci,k : ri,j = Ci,ki
=
0 otherwise
(7)
1
MRR = �
</equation>
<bodyText confidence="0.933050857142857">
4. MAPref Measures tightly the precision in the
n-best candidates for i-th source name, for which
reference transliterations are available. If all of
the references are produced, then the MAP is 1.
Let’s denote the number of correct candidates for
the i-th source word in k-best list as num(i, k).
MAPref is then given by
</bodyText>
<equation confidence="0.952188">
!num(i, k) (9)
</equation>
<bodyText confidence="0.982103653846154">
5. MAP10 measures the precision in the 10-best
candidates for i-th source name provided by the
candidate system. In general, the higher MAP10
is, the better is the quality of the transliteration
system in capturing the multiple references. Note
that the number of reference transliterations may
be more or less than 10. If the number of refer-
ence transliterations is below 10, then MAP10 can
never be equal to 1. Only if the number of ref-
erence transliterations for every source word is at
least 10, then MAP10 could possibly be equal to 1.
Note that in general MAPm measures the “good-
ness in m-best” candidate list. We use m = 10
because we have asked the systems to produce up
to 10 candidates for every source name in the test
set.
6. MAPsys Measures the precision in the top
Ki-best candidates produced by the system for i-
th source name, for which ni reference translit-
erations are available. This measure allows the
systems to produce variable number of translitera-
tions, based on their confidence in identifying and
producing correct transliterations. If all of the ni
references are produced in the top-ni candidates
(that is, Ki = ni, and all of them are correct), then
the MAPsys is 1.
</bodyText>
<equation confidence="0.642069">
!num(i, k) (11)
</equation>
<sectionHeader confidence="0.590592" genericHeader="method">
8 Contact Us
</sectionHeader>
<bodyText confidence="0.63724">
If you have any questions about this share task and
the database, please email to
</bodyText>
<figure confidence="0.731917571428572">
Dr. Haizhou Li
Institute for Infocomm Research (I2R),
A*STAR
1 Fusionopolis Way
#08-05 South Tower, Connexis
Singapore 138632
hli@i2r.a-star.edu.sg
</figure>
<sectionHeader confidence="0.224144" genericHeader="method">
Dr. A. Kumaran
</sectionHeader>
<subsectionHeader confidence="0.332004">
Microsoft Research India
</subsectionHeader>
<bodyText confidence="0.7306875">
Scientia, 196/36, Sadashivnagar 2nd Main
Road
Bangalore 560080 INDIA
a.kumaran@microsoft.com
</bodyText>
<subsectionHeader confidence="0.493167">
Mr. Kurt Easterwood
</subsectionHeader>
<bodyText confidence="0.8860786">
The CJK Dictionary Institute (CJK Data)
Komine Building (3rd &amp; 4th floors)
34-14, 2-chome, Tohoku, Niiza-shi
Saitama 352-0001 JAPAN
akurt@cjki.org
</bodyText>
<equation confidence="0.948279615384616">
XN RRi (8)
i=1
XN
1
MAPref = �
i
1
ni
Xni
k=1
XN
1
MAPsys = �
i=1
1
Ki
Ki
X
k=1
XN
1
MAP10 = �
i=1
!num(i, k) (10)
1
1 0
</equation>
<page confidence="0.913267">
10
X
k=1
23
</page>
<sectionHeader confidence="0.982073" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.9571453">
CJKI. 2009. CJK Institute. http://www.cjk.org/.
A Kumaran and T. Kellner. 2007. A generic frame-
work for machine transliteration. In Proc. SIGIR,
pages 721–722.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proc. 42nd ACL Annual Meeting, pages 159–166,
Barcelona, Spain.
MSRI. 2009. Microsoft Research India.
http://research.microsoft.com/india.
</reference>
<table confidence="0.831400888888889">
Appendix A: Training/Development Data
• File Naming Conventions:
NEWS09 train XXYY nnnn.xml
NEWS09 dev XXYY nnnn.xml
NEWS09 test XXYY nnnn.xml
– XX: Source Language
– YY: Target Language
– nnnn: size of parallel/monolingual
names (“25K”, “10000”, etc)
</table>
<listItem confidence="0.886027">
• File formats:
</listItem>
<bodyText confidence="0.7588335">
All data will be made available in XML for-
mats (Figure 1).
</bodyText>
<listItem confidence="0.971514">
• Data Encoding Formats:
</listItem>
<bodyText confidence="0.997928666666667">
The data will be in Unicode UTF-8 encod-
ing files without byte-order mark, and in the
XML format specified.
</bodyText>
<sectionHeader confidence="0.940859" genericHeader="method">
Appendix B: Submission of Results
</sectionHeader>
<listItem confidence="0.3957148">
• File Naming Conventions:
NEWS09 result XXYY gggg nn descr.xml
– XX: Source Language
– YY: Target Language
– gggg: Group ID
</listItem>
<figureCaption confidence="0.42424775">
– nn: run ID. Note that run ID “1” stands for “stan-
dard” run where only the provided data are al-
lowed to be used. Run ID “2–5” means “non-
standard” run where additional data can be used.
</figureCaption>
<listItem confidence="0.970135">
– descr: Description of the run.
• File formats:
</listItem>
<bodyText confidence="0.949328">
All data will be made available in XML formats (Fig-
ure 2).
</bodyText>
<listItem confidence="0.947483">
• Data Encoding Formats:
</listItem>
<bodyText confidence="0.979014">
The results are expected to be submitted in UTF-8 en-
coded files without byte-order mark only, and in the
XML format specified.
</bodyText>
<page confidence="0.992108">
24
</page>
<figure confidence="0.996222307692307">
&lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&gt;
&lt;TransliterationCorpus
CorpusID = &amp;quot;NEWS2009-Train-EnHi-25K&amp;quot;
SourceLang = &amp;quot;English&amp;quot;
TargetLang = &amp;quot;Hindi&amp;quot;
CorpusType = &amp;quot;Train|Dev&amp;quot;
CorpusSize = &amp;quot;25000&amp;quot;
CorpusFormat = &amp;quot;UTF8&amp;quot;&gt;
&lt;Name ID=”1”&gt;
&lt;SourceName&gt;eeeeee1&lt;/SourceName&gt;
&lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh1_1&lt;/TargetName&gt;
&lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh1_2&lt;/TargetName&gt;
...
&lt;TargetName ID=&amp;quot;n&amp;quot;&gt;hhhhhh1_n&lt;/TargetName&gt;
&lt;/Name&gt;
&lt;Name ID=”2”&gt;
&lt;SourceName&gt;eeeeee2&lt;/SourceName&gt;
&lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh2_1&lt;/TargetName&gt;
&lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh2_2&lt;/TargetName&gt;
...
&lt;TargetName ID=&amp;quot;m&amp;quot;&gt;hhhhhh2_m&lt;/TargetName&gt;
&lt;/Name&gt;
...
&lt;!-- rest of the names to follow --&gt;
...
&lt;/TransliterationCorpus&gt;
</figure>
<figureCaption confidence="0.99865">
Figure 1: File: NEWS2009 Train EnHi 25K.xml
</figureCaption>
<page confidence="0.855112">
25
</page>
<figure confidence="0.99651734375">
&lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&gt;
&lt;TransliterationTaskResults
SourceLang = &amp;quot;English&amp;quot;
TargetLang = &amp;quot;Hindi&amp;quot;
GroupID = &amp;quot;Trans University&amp;quot;
RunID = &amp;quot;1&amp;quot;
RunType = &amp;quot;Standard&amp;quot;
Comments = &amp;quot;HMM Run with params: alpha=0.8 beta=1.25&amp;quot;&gt;
&lt;Name ID=&amp;quot;1&amp;quot;&gt;
&lt;SourceName&gt;eeeeee1&lt;/SourceName&gt;
&lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh11&lt;/TargetName&gt;
&lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh12&lt;/TargetName&gt;
&lt;TargetName ID=&amp;quot;3&amp;quot;&gt;hhhhhh13&lt;/TargetName&gt;
...
&lt;TargetName ID=&amp;quot;10&amp;quot;&gt;hhhhhh110&lt;/TargetName&gt;
&lt;!-- Participants to provide their
top 10 candidate transliterations --&gt;
&lt;/Name&gt;
&lt;Name ID=&amp;quot;2&amp;quot;&gt;
&lt;SourceName&gt;eeeeee2&lt;/SourceName&gt;
&lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh21&lt;/TargetName&gt;
&lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh22&lt;/TargetName&gt;
&lt;TargetName ID=&amp;quot;3&amp;quot;&gt;hhhhhh23&lt;/TargetName&gt;
...
&lt;TargetName ID=&amp;quot;10&amp;quot;&gt;hhhhhh110&lt;/TargetName&gt;
&lt;!-- Participants to provide their
top 10 candidate transliterations --&gt;
&lt;/Name&gt;
...
&lt;!-- All names in test corpus to follow --&gt;
...
&lt;/TransliterationTaskResults&gt;
</figure>
<figureCaption confidence="0.998883">
Figure 2: Example file: NEWS2009 EnHi TUniv 01 StdRunHMMBased.xml
</figureCaption>
<page confidence="0.991357">
26
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.7713665">of NEWS 2009 Machine Transliteration Shared A Min Vladimir</title>
<author confidence="0.294924">for Infocomm Research</author>
<author confidence="0.294924">Singapore ASTAR</author>
<affiliation confidence="0.633739">Systems Research, Microsoft Research</affiliation>
<email confidence="0.996868">A.Kumaran@microsoft.com</email>
<abstract confidence="0.998706657894737">Transliteration is defined as phonetic translation of names across languages. Transliteration of Named Entities (NEs) is necessary in many applications, such as machine translation, corpus alignment, cross-language IR, information extraction and automatic lexicon acquisition. All such systems call for high-performance transliteration, which is the focus of the shared task in the NEWS 2009 workshop. The objective of the shared task is to promote machine transliteration research by providing a common benchmarking platform for the community to evaluate the state-of-the-art technologies. 1 Task Description The task is to develop machine transliteration system in one or more of the specified language pairs being considered for the task. Each language pair consists of a source and a target language. The training and development data sets released for each language pair are to be used for developing a transliteration system in whatever way that the participants find appropriate. At the evaluation time, a test set of source names only would be released, on which the participants are expected to produce a ranked list of transliteration candidates another language (i.e. transliterations), and this will be evaluated using common metrics. For every language pair the participants must submit one run that uses only the data provided by the NEWS workshop organisers in a given language pair (designated as “standard” runs). Users may submit more runs (“non-standard”) for each language pair that uses other data than those provided by the NEWS 2009 workshop; such runs would be evaluated and reported separately.</abstract>
<note confidence="0.968138642857143">2 Important Dates paper submission deadline May 2009 Shared task Registration opens 16 Feb 2009 Registration closes 9 Apr 2009 Release Training/Development Data 16 Feb 2009 Release Test Data 10 Apr 2009 Results Submission Due 14 Apr 2009 Results Announcement 29 Apr 2009 Task (short) Papers Due 3 May 2009 For all submissions Acceptance Notification 1 Jun 2009 Camera-Ready Copy Deadline 7 Jun 2009 Workshop Date 7 Aug 2009</note>
<abstract confidence="0.981100401639344">3 Participation 1. Registration (16 Feb 2009) (a) NEWS Shared Task opens for registration. (b) Prospective participants are to register to the NEWS Workshop homepage. 2. Training &amp; Development Data (16 Feb 2009) (a) Registered participants are to obtain training and development data from the Shared Task organiser and/or the designated copyright owners of databases. 3. Evaluation Script (16 Mar 2009) (a) A sample test set and expected user output format are to be released. (b) An evaluation script, which runs on the above two, is to be released. (c) The participants must make sure that their output is produced in a way that the evaluation script may run and produce the expected output. 19 of the 2009 Named Entities Workshop, ACL-IJCNLP pages Singapore, 7 August 2009. ACL and AFNLP (d) The same script (with held out test data and the user outputs) would be used for final evaluation. 4. Test data (10 April 2009) (a) The test data would be released on 10 Apr 2009, and the participants have a maximum of 4 days to submit their results in the expected format. (b) Only 1 “standard” run must be submitted from every group on a given language pair; more “non-standard” runs (0 to 4) may be submitted. In total, maximum 5 runs (1 “standard” run plus up to 4 “non-standard” runs) can be submitted from each group on a registered language pair. (c) Any runs that are “non-standard” must be tagged as such. (d) The test set is a list of names in source language only. Every group will produce and submit a ranked list of transliteration candidates in another language for each given name in the test set. Please note that this shared task is a “transliteration generation” task, i.e., given a name in a source language one is supposed to generate one or more transliterations in a target language. It is not the task of “transliteration discovery”, i.e., given a name in the source language and a set of names in the target language evaluate how to find the appropriate names from the target set that are transliterations of the given source name. 5. Results (29 April 2009) (a) On 29 April 2009, the evaluation results would be announced and will be made available on the Workshop website. (b) Note that only the scores (in respective metrics) of the participating systems on each language pairs would be published, and no explicit ranking of the participating systems would be published. (c) Note that this is a shared evaluation task and not a competition; the results are meant to be used to evaluate systems on common data set with common metrics, and not to rank the participating systems. While the participants can cite the performance of their systems (scores on metrics) from the workshop report, they should not use any ranking information in their publications. (d) Further, all participants should agree not to reveal identities of other participants in any of their publications unless you get permission from the other respective participants. If the participants want to remain anonymous in published they should inform the organisers a.kumaran@microsoft.com), at the time of registration. Note that the results of their systems would still be published, but with the participant identities masked. As a result, in this case, your organisation name will still appear in the web site as one of participants, but it is not linked explicitly with your results. 6. Short Papers on Task (3 May 2009) (a) Each submitting site is required to submit a 4-page system paper (short paper) for its submissions, including their approach, data used and the results on eitest set or development set or by fold cross validation on training set. (b) All system short papers will be included in the proceedings. Selected short papers will be presented orally in the 2009 workshop. comments for all system short papers and the acceptance notification for the system short papers for oral presentation would be announced on 1 June 2009 together with that of other papers. (c) All registered participants are required to register and attend the workshop to introduce your work. (d) All paper submission and review will be managed electronically through https://www.softconf.com/aclijcnlp09/NEWS/. 4 Languages Involved The tasks are to transliterate personal names or place names from a source to a target language as summarised in Table 1. 20 Source language Target language Data Owner Approx. Data Size Task ID</abstract>
<note confidence="0.7995836">English Chinese Institute for Infocomm Research 30K EnCh English Japanese Katakana CJK Institute 25K EnJa English Korean Hangul CJK Institute 7K EnKo Japanese name (in English) Japanese Kanji CJK Institute 20K JnJk English Hindi Microsoft Research India 15K EnHi English Tamil Microsoft Research India 15K EnTa English Kannada Microsoft Research India 15K EnKa English Russian Microsoft Research India 10K EnRu Table 1: Source and target languages for the shared task on transliteration. The names given in the training sets for Chi-</note>
<abstract confidence="0.994116980392157">nese, Japanese and Korean languages are Western names and their CJK transliterations; the Japanese (in English) Kanji data set consists only of native Japanese names. The Indic data set (Hindi, Tamil, Kannada) consists of a mix of Indian and Western names. Katakana Hangul name in English Kanji Francisco फ्रान्सिस्को —bar 5 Standard Databases Training Data (Parallel) Paired names between source and target languages; size 5K – 40K. Training Data is used for training a basic transliteration system. Development Data (Parallel) Paired names between source and target languages; size 1K – 2K. Development Data is in addition to the Training data, which is used for system fine-tuning of parameters in case of need. Participants are allowed to use it as part of training data. Testing Data Source names only; size 1K – 3K. This is a held-out set, which would be used for evaluating the quality of the transliterations. 1. Participants will need to obtain licenses from the respective copyright owners and/or agree to the terms and conditions of use that are given on the downloading website (Li et al., 2004; Kumaran and Kellner, 2007; MSRI, 2009; CJKI, 2009). NEWS 2009 will provide the contact details of each individual database. The data would be provided in Unicode UTF-8 encoding, in XML format; the results are expected to be submitted in XML format. The XML formats will be announced at the workshop website. 2. The data are provided in 3 sets as described above. 3. Name pairs are distributed as-is, as provided by the respective creators. (a) While the databases are mostly manually checked, there may be still inconsistency (that is, non-standard usage, region-specific usage, errors, etc.) or incompleteness (that is, not all right variations may be covered). (b) The participants may use any method to further clean up the data provided. i. If they are cleaned up manually, we appeal that such data be provided back to the organisers for redistribution to all the participating groups in that language pair; such sharing benefits all participants, and further 21 ensures that the evaluation provides normalisation with respect to data quality. ii. If automatic cleanup were used, such cleanup would be considered a part of the system fielded, and hence not required to be shared with all participants. 4. We expect that the participants to use only the data (parallel names) provided by the Shared Task for transliteration task for a “standard” run to ensure a fair evaluation. One such run (using only the data provided by the shared task) is mandatory for all participants for a given language pair that they participate in. 5. If more data (either parallel names data or monolingual data) were used, then all such runs using extra data must be marked as “non-standard”. For such “non-standard” runs, it is required to disclose the size and characteristics of the data used in the system paper. of these alternatives are considered as a correct transliteration, and the first correct transliteration in the ranked list is accepted as a correct hit. The following notation is further assumed: Total number of names (source words) in the test set Number of reference transliterations name in the test set reference transliteration for name in the test set candidate transliteration (system for name in the test set Number of candidate transliterations produced by a transliteration system Word Accuracy in Top-1 (ACC) known as Word Error Rate, it measures correctness of the first transliteration candidate in the candidate list produced by a transliteration system. 1 means that all top candidates are correct transliterations i.e. they match one of the refand 0 means that none of the top candidates are correct. 6. A participant may submit a maximum of 5 runs for a given language pair (including the mandatory 1 “standard” run). 1 if 0 otherwise i=1 6 Paper Format Paper submissions to NEWS 2009 should follow the ACL-IJCNLP-2009 paper submission policy, including paper format, blind review policy and title and author format convention. Full papers (research paper) are in two-column format without exceeding eight (8) pages of content plus one extra page for references and short papers (task paper) are also in two-column format without exceeding four (4) pages, including references. Submission must conform to the official ACL-IJCNLP-2009 style guidelines. For details, please refer to the 7 Evaluation Metrics We plan to measure the quality of the transliteration task using the following 6 metrics. We accept up to 10 output candidates in a ranked list for each input entry. Since a given source name may have multiple correct target transliterations, all these alternatives are treated equally in the evaluation. That is, any (1) Fuzziness in Top-1 (Mean F-score) mean F-score measures how different, on average, the top transliteration candidate is from its closest reference. F-score for each source word is a function of Precision and Recall and equals 1 when the top candidate matches one of the references, and 0 when there are no common characters between the candidate and any of the references. Precision and Recall are calculated based on the length of the Longest Common Subsequence between a candidate and a reference: 1 = 2 − (2) the edit distance and the length For example, the longest common subsequence between “abcd” and “afcde” is “acd” and its length is 3. The best matching reference, that is, the reference for which the edit distance has the minimum, is taken for calculation. If the best matching reference is given by arg min (3) j 22 then Recall, Precision and F-score for i-th word are calculated as = = = • The length is computed in distinct Unicode characters. • No distinction is made on different character types of a language (e.g., vowel vs. consovs. combining Mean Reciprocal Rank (MRR) MRR any right answer by system, from among the candidates. tells approximately the average rank of the correct transliteration. MRR closer to 1 implies that the correct answer is mostly produced close to the top of the n-best lists. j = (7) 1 4. Measures tightly the precision in the candidates for source name, for which reference transliterations are available. If all of the references are produced, then the MAP is 1. Let’s denote the number of correct candidates for source word in list as then given by 5. measures the precision in the 10-best for source name provided by the system. In general, the higher is, the better is the quality of the transliteration system in capturing the multiple references. Note that the number of reference transliterations may be more or less than 10. If the number of refertransliterations is below 10, then can never be equal to 1. Only if the number of reference transliterations for every source word is at 10, then could possibly be equal to 1. that in general measures the “goodin candidate list. We use 10 because we have asked the systems to produce up to 10 candidates for every source name in the test set. Measures the precision in the top candidates produced by the system for source name, for which transliterations are available. This measure allows the systems to produce variable number of transliterations, based on their confidence in identifying and correct transliterations. If all of the are produced in the is, and all of them are correct), then is 1.</abstract>
<note confidence="0.770854">8 Contact Us If you have any questions about this share task and the database, please email to Dr. Haizhou Li Institute for Infocomm Research (I2R), A*STAR 1 Fusionopolis Way Singapore 138632</note>
<email confidence="0.634958">hli@i2r.a-star.edu.sg</email>
<author confidence="0.96187">A Kumaran</author>
<affiliation confidence="0.997083">Microsoft Research India</affiliation>
<address confidence="0.929038">Scientia, 196/36, Sadashivnagar 2nd Main Road Bangalore 560080 INDIA</address>
<email confidence="0.999484">a.kumaran@microsoft.com</email>
<author confidence="0.999191">Kurt Easterwood</author>
<affiliation confidence="0.958091">The CJK Dictionary Institute (CJK Data)</affiliation>
<address confidence="0.788245333333333">Komine Building (3rd &amp; 4th floors) 34-14, 2-chome, Tohoku, Niiza-shi Saitama 352-0001 JAPAN</address>
<email confidence="0.450118">akurt@cjki.org</email>
<note confidence="0.646257157894737">1 i 1 1 1 X 1 1 0 10 X 23 References CJKI. 2009. CJK Institute. http://www.cjk.org/. A Kumaran and T. Kellner. 2007. A generic framefor machine transliteration. In pages 721–722. Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine transliteration. 42nd ACL Annual pages 159–166,</note>
<address confidence="0.5392775">Barcelona, Spain. MSRI. 2009. Microsoft Research India.</address>
<web confidence="0.747185">http://research.microsoft.com/india.</web>
<note confidence="0.858928">Appendix A: Training/Development Data • File Naming Conventions: NEWS09 train XXYY nnnn.xml NEWS09 dev XXYY nnnn.xml NEWS09 test XXYY nnnn.xml</note>
<title confidence="0.757068">Source Language Target Language</title>
<abstract confidence="0.747620777777778">size of parallel/monolingual names (“25K”, “10000”, etc) • File formats: All data will be made available in XML formats (Figure 1). • Data Encoding Formats: The data will be in Unicode UTF-8 encoding files without byte-order mark, and in the XML format specified.</abstract>
<title confidence="0.723029166666667">Appendix B: Submission of Results • File Naming Conventions: result XXYY gggg nn Source Language Target Language Group ID</title>
<abstract confidence="0.767361642857143">run ID. Note that run ID “1” stands for “standard” run where only the provided data are allowed to be used. Run ID “2–5” means “nonstandard” run where additional data can be used. Description of the run. • File formats: All data will be made available in XML formats (Figure 2). • Data Encoding Formats: The results are expected to be submitted in UTF-8 encoded files without byte-order mark only, and in the XML format specified. 24 &lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&gt;</abstract>
<title confidence="0.5425652">lt;TransliterationCorpus CorpusID = &amp;quot;NEWS2009-Train-EnHi-25K&amp;quot; SourceLang = &amp;quot;English&amp;quot; TargetLang = &amp;quot;Hindi&amp;quot; CorpusType = &amp;quot;Train|Dev&amp;quot;</title>
<note confidence="0.8633686">CorpusSize = &amp;quot;25000&amp;quot; CorpusFormat = &amp;quot;UTF8&amp;quot;&gt; &lt;SourceName&gt;eeeeee1&lt;/SourceName&gt; &lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh1_1&lt;/TargetName&gt; &lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh1_2&lt;/TargetName&gt; ... &lt;TargetName ID=&amp;quot;n&amp;quot;&gt;hhhhhh1_n&lt;/TargetName&gt; &lt;/Name&gt; &lt;SourceName&gt;eeeeee2&lt;/SourceName&gt; &lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh2_1&lt;/TargetName&gt; &lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh2_2&lt;/TargetName&gt; ... &lt;TargetName ID=&amp;quot;m&amp;quot;&gt;hhhhhh2_m&lt;/TargetName&gt; &lt;/Name&gt; ... &lt;!-rest of the names to follow --&gt; ... &lt;/TransliterationCorpus&gt; Figure 1: File: NEWS2009 Train EnHi 25K.xml 25 &lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&gt; &lt;TransliterationTaskResults SourceLang = &amp;quot;English&amp;quot; TargetLang = &amp;quot;Hindi&amp;quot; GroupID = &amp;quot;Trans University&amp;quot; RunID = &amp;quot;1&amp;quot; RunType = &amp;quot;Standard&amp;quot; Comments = &amp;quot;HMM Run with params: alpha=0.8 beta=1.25&amp;quot;&gt; &lt;Name ID=&amp;quot;1&amp;quot;&gt; &lt;SourceName&gt;eeeeee1&lt;/SourceName&gt; &lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh11&lt;/TargetName&gt; &lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh12&lt;/TargetName&gt; &lt;TargetName ID=&amp;quot;3&amp;quot;&gt;hhhhhh13&lt;/TargetName&gt; ... &lt;TargetName ID=&amp;quot;10&amp;quot;&gt;hhhhhh110&lt;/TargetName&gt; &lt;!-- Participants to provide their top 10 candidate transliterations --&gt; &lt;/Name&gt; &lt;Name ID=&amp;quot;2&amp;quot;&gt; &lt;SourceName&gt;eeeeee2&lt;/SourceName&gt; &lt;TargetName ID=&amp;quot;1&amp;quot;&gt;hhhhhh21&lt;/TargetName&gt; &lt;TargetName ID=&amp;quot;2&amp;quot;&gt;hhhhhh22&lt;/TargetName&gt; &lt;TargetName ID=&amp;quot;3&amp;quot;&gt;hhhhhh23&lt;/TargetName&gt; ... &lt;TargetName ID=&amp;quot;10&amp;quot;&gt;hhhhhh110&lt;/TargetName&gt;</note>
<abstract confidence="0.792416833333333">lt;!-- Participants to provide their top 10 candidate transliterations --&gt; &lt;/Name&gt; ... &lt;!-- All names in test corpus to follow --&gt; ...</abstract>
<note confidence="0.706191333333333">lt;/TransliterationTaskResults&gt; Figure 2: Example file: NEWS2009 EnHi TUniv 01 StdRunHMMBased.xml 26</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>CJKI</author>
</authors>
<date>2009</date>
<note>CJK Institute. http://www.cjk.org/.</note>
<contexts>
<context position="8915" citStr="CJKI, 2009" startWordPosition="1448" endWordPosition="1449">en source and target languages; size 1K – 2K. Development Data is in addition to the Training data, which is used for system fine-tuning of parameters in case of need. Participants are allowed to use it as part of training data. Testing Data Source names only; size 1K – 3K. This is a held-out set, which would be used for evaluating the quality of the transliterations. 1. Participants will need to obtain licenses from the respective copyright owners and/or agree to the terms and conditions of use that are given on the downloading website (Li et al., 2004; Kumaran and Kellner, 2007; MSRI, 2009; CJKI, 2009). NEWS 2009 will provide the contact details of each individual database. The data would be provided in Unicode UTF-8 encoding, in XML format; the results are expected to be submitted in XML format. The XML formats will be announced at the workshop website. 2. The data are provided in 3 sets as described above. 3. Name pairs are distributed as-is, as provided by the respective creators. (a) While the databases are mostly manually checked, there may be still inconsistency (that is, non-standard usage, region-specific usage, errors, etc.) or incompleteness (that is, not all right variations may </context>
</contexts>
<marker>CJKI, 2009</marker>
<rawString>CJKI. 2009. CJK Institute. http://www.cjk.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>T Kellner</author>
</authors>
<title>A generic framework for machine transliteration.</title>
<date>2007</date>
<booktitle>In Proc. SIGIR,</booktitle>
<pages>721--722</pages>
<contexts>
<context position="8890" citStr="Kumaran and Kellner, 2007" startWordPosition="1442" endWordPosition="1445">ment Data (Parallel) Paired names between source and target languages; size 1K – 2K. Development Data is in addition to the Training data, which is used for system fine-tuning of parameters in case of need. Participants are allowed to use it as part of training data. Testing Data Source names only; size 1K – 3K. This is a held-out set, which would be used for evaluating the quality of the transliterations. 1. Participants will need to obtain licenses from the respective copyright owners and/or agree to the terms and conditions of use that are given on the downloading website (Li et al., 2004; Kumaran and Kellner, 2007; MSRI, 2009; CJKI, 2009). NEWS 2009 will provide the contact details of each individual database. The data would be provided in Unicode UTF-8 encoding, in XML format; the results are expected to be submitted in XML format. The XML formats will be announced at the workshop website. 2. The data are provided in 3 sets as described above. 3. Name pairs are distributed as-is, as provided by the respective creators. (a) While the databases are mostly manually checked, there may be still inconsistency (that is, non-standard usage, region-specific usage, errors, etc.) or incompleteness (that is, not </context>
</contexts>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>A Kumaran and T. Kellner. 2007. A generic framework for machine transliteration. In Proc. SIGIR, pages 721–722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source-channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In Proc. 42nd ACL Annual Meeting,</booktitle>
<pages>159--166</pages>
<location>Barcelona,</location>
<contexts>
<context position="8863" citStr="Li et al., 2004" startWordPosition="1438" endWordPosition="1441">n system. Development Data (Parallel) Paired names between source and target languages; size 1K – 2K. Development Data is in addition to the Training data, which is used for system fine-tuning of parameters in case of need. Participants are allowed to use it as part of training data. Testing Data Source names only; size 1K – 3K. This is a held-out set, which would be used for evaluating the quality of the transliterations. 1. Participants will need to obtain licenses from the respective copyright owners and/or agree to the terms and conditions of use that are given on the downloading website (Li et al., 2004; Kumaran and Kellner, 2007; MSRI, 2009; CJKI, 2009). NEWS 2009 will provide the contact details of each individual database. The data would be provided in Unicode UTF-8 encoding, in XML format; the results are expected to be submitted in XML format. The XML formats will be announced at the workshop website. 2. The data are provided in 3 sets as described above. 3. Name pairs are distributed as-is, as provided by the respective creators. (a) While the databases are mostly manually checked, there may be still inconsistency (that is, non-standard usage, region-specific usage, errors, etc.) or in</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine transliteration. In Proc. 42nd ACL Annual Meeting, pages 159–166, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MSRI</author>
</authors>
<date>2009</date>
<institution>Microsoft Research India.</institution>
<note>http://research.microsoft.com/india.</note>
<contexts>
<context position="8902" citStr="MSRI, 2009" startWordPosition="1446" endWordPosition="1447"> names between source and target languages; size 1K – 2K. Development Data is in addition to the Training data, which is used for system fine-tuning of parameters in case of need. Participants are allowed to use it as part of training data. Testing Data Source names only; size 1K – 3K. This is a held-out set, which would be used for evaluating the quality of the transliterations. 1. Participants will need to obtain licenses from the respective copyright owners and/or agree to the terms and conditions of use that are given on the downloading website (Li et al., 2004; Kumaran and Kellner, 2007; MSRI, 2009; CJKI, 2009). NEWS 2009 will provide the contact details of each individual database. The data would be provided in Unicode UTF-8 encoding, in XML format; the results are expected to be submitted in XML format. The XML formats will be announced at the workshop website. 2. The data are provided in 3 sets as described above. 3. Name pairs are distributed as-is, as provided by the respective creators. (a) While the databases are mostly manually checked, there may be still inconsistency (that is, non-standard usage, region-specific usage, errors, etc.) or incompleteness (that is, not all right va</context>
</contexts>
<marker>MSRI, 2009</marker>
<rawString>MSRI. 2009. Microsoft Research India. http://research.microsoft.com/india.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>