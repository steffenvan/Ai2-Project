<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007852">
<title confidence="0.984243">
Semantic Parsing as Machine Translation
</title>
<author confidence="0.991739">
Jacob Andreas
</author>
<affiliation confidence="0.99209">
Computer Laboratory
University of Cambridge
</affiliation>
<email confidence="0.974254">
jda33@cam.ac.uk
</email>
<author confidence="0.98549">
Andreas Vlachos
</author>
<affiliation confidence="0.990488">
Computer Laboratory
University of Cambridge
</affiliation>
<email confidence="0.97026">
av308@cam.ac.uk
</email>
<author confidence="0.991117">
Stephen Clark
</author>
<affiliation confidence="0.992528">
Computer Laboratory
University of Cambridge
</affiliation>
<email confidence="0.9887">
sc609@cam.ac.uk
</email>
<sectionHeader confidence="0.993698" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999876444444445">
Semantic parsing is the problem of de-
riving a structured meaning representation
from a natural language utterance. Here
we approach it as a straightforward ma-
chine translation task, and demonstrate
that standard machine translation com-
ponents can be adapted into a semantic
parser. In experiments on the multilingual
GeoQuery corpus we find that our parser
is competitive with the state of the art,
and in some cases achieves higher accu-
racy than recently proposed purpose-built
systems. These results support the use of
machine translation methods as an infor-
mative baseline in semantic parsing evalu-
ations, and suggest that research in seman-
tic parsing could benefit from advances in
machine translation.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997308">
Semantic parsing (SP) is the problem of trans-
forming a natural language (NL) utterance into
a machine-interpretable meaning representation
(MR). It is well-studied in NLP, and a wide va-
riety of methods have been proposed to tackle
it, e.g. rule-based (Popescu et al., 2003), super-
vised (Zelle, 1995), unsupervised (Goldwasser et
al., 2011), and response-based (Liang et al., 2011).
At least superficially, SP is simply a machine
translation (MT) task: we transform an NL ut-
terance in one language into a statement of an-
other (un-natural) meaning representation lan-
guage (MRL). Indeed, successful semantic parsers
often resemble MT systems in several impor-
tant respects, including the use of word align-
ment models as a starting point for rule extrac-
tion (Wong and Mooney, 2006; Kwiatkowski et
al., 2010) and the use of automata such as tree
transducers (Jones et al., 2012) to encode the re-
lationship between NL and MRL.
The key difference between the two tasks is that
in SP, the target language (the MRL) has very dif-
ferent properties to an NL. In particular, MRs must
conform strictly to a particular structure so that
they are machine-interpretable. Contrast this with
ordinary MT, where varying degrees of wrongness
are tolerated by human readers (and evaluation
metrics). To avoid producing malformed MRs, al-
most all of the existing research on SP has focused
on developing models with richer structure than
those commonly used for MT.
In this work we attempt to determine how ac-
curate a semantic parser we can build by treating
SP as a pure MT task, and describe pre- and post-
processing steps which allow structure to be pre-
served in the MT process.
Our contributions are as follows: We develop
a semantic parser using off-the-shelf MT compo-
nents, exploring phrase-based as well as hierarchi-
cal models. Experiments with four languages on
the popular GeoQuery corpus (Zelle, 1995) show
that our parser is competitve with the state-of-
the-art, in some cases achieving higher accuracy
than recently introduced purpose-built semantic
parsers. Our approach also appears to require
substantially less time to train than the two best-
performing semantic parsers. These results sup-
port the use of MT methods as an informative
baseline in SP evaluations and show that research
in SP could benefit from research advances in MT.
</bodyText>
<sectionHeader confidence="0.508862" genericHeader="method">
2 MT-based semantic parsing
</sectionHeader>
<bodyText confidence="0.999828875">
The input is a corpus of NL utterances paired with
MRs. In order to learn a semantic parser using
MT we linearize the MRs, learn alignments be-
tween the MRL and the NL, extract translation
rules, and learn a language model for the MRL.
We also specify a decoding procedure that will re-
turn structured MRs for an utterance during pre-
diction.
</bodyText>
<page confidence="0.993667">
47
</page>
<note confidence="0.4649095">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 47–52,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.953719">
states bordering Texas
state(next to(state(stateid(texas))))
4 STEM &amp; LINEARIZE
state border texa
state, next to, state, stateid, texas0
4 ALIGN
4 EXTRACT (PHRASE)
( state , state, )
( state border, state, border, )
( texa , state, stateid, texas0 )
...
4 EXTRACT (HIER)
[X] (state , state,)
[X] (state [X] texa ,
state, [X] state, stateid, texas0)
...
</figure>
<figureCaption confidence="0.9926675">
Figure 1: Illustration of preprocessing and rule ex-
traction.
</figureCaption>
<bodyText confidence="0.993880982456141">
Linearization We assume that the MRL is
variable-free (that is, the meaning representation
for each utterance is tree-shaped), noting that for-
malisms with variables, like the A-calculus, can
be mapped onto variable-free logical forms with
combinatory logics (Curry et al., 1980).
In order to learn a semantic parser using MT
we begin by converting these MRs to a form more
similar to NL. To do so, we simply take a preorder
traversal of every functional form, and label every
function with the number of arguments it takes.
After translation, recovery of the function is easy:
if the arity of every function in the MRL is known,
then every traversal uniquely specifies its corre-
sponding tree. Using an example from GeoQuery,
given an input function of the form
answer(population(city(cityid(‘seattle’, ‘wa’))))
we produce a “decorated” translation input of the
form
answer, population, city, cityid2 seattle0 wa0
where each subscript indicates the symbol’s arity
(constants, including strings, are treated as zero-
argument functions). Explicit argument number
labeling serves two functions. Most importantly,
it eliminates any possible ambiguity from the tree
reconstruction which takes place during decod-
ing: given any sequence of decorated MRL to-
kens, we can always reconstruct the correspond-
ing tree structure (if one exists). Arity labeling ad-
ditionally allows functions with variable numbers
of arguments (e.g. cityid, which in some training
examples is unary) to align with different natural
language strings depending on context.
Alignment Following the linearization of the
MRs, we find alignments between the MR tokens
and the NL tokens using the IBM Model 4 (Brown
et al., 1993). Once the alignment algorithm is
run in both directions (NL to MRL, MRL to NL),
we symmetrize the resulting alignments to obtain
a consensus many-to-many alignment (Och and
Ney, 2000; Koehn et al., 2005).
Rule extraction From the many-to-many align-
ment we need to extract a translation rule ta-
ble, consisting of corresponding phrases in NL
and MRL. We consider a phrase-based transla-
tion model (Koehn et al., 2003) and a hierarchi-
cal translation model (Chiang, 2005). Rules for
the phrase-based model consist of pairs of aligned
source and target sequences, while hierarchical
rules are SCFG productions containing at most
two instances of a single nonterminal symbol.
Note that both extraction algorithms can learn
rules which a traditional tree-transducer-based ap-
proach cannot—for example the right hand side
[X] river, all0 traverse, [X]
corresponding to the pair of disconnected tree
fragments:
</bodyText>
<equation confidence="0.9658285">
river
V
all
(where each X indicates a gap in the rule).
</equation>
<bodyText confidence="0.963454454545455">
Language modeling In addition to translation
rules learned from a parallel corpus, MT systems
also rely on an n-gram language model for the tar-
get language, estimated from a (typically larger)
monolingual corpus. In the case of SP, such a
monolingual corpus is rarely available, and we in-
stead use the MRs available in the training data to
learn a language model of the MRL. This informa-
tion helps guide the decoder towards well-formed
state border texa
state, next to, state, stateid, texas0
</bodyText>
<equation confidence="0.7265016">
[X]
V
traverse
V
[X]
</equation>
<page confidence="0.97627">
48
</page>
<bodyText confidence="0.999931782608696">
structures; it encodes, for example, the preferences
of predicates of the MRL for certain arguments.
Prediction Given a new NL utterance, we need
to find the n best translations (i.e. sequences
of decorated MRL tokens) that maximize the
weighted sum of the translation score (the prob-
abilities of the translations according to the rule
translation table) and the language model score, a
process usually referred to as decoding. Standard
decoding procedures for MT produce an n-best list
of all possible translations, but here we need to
restrict ourselves to translations corresponding to
well-formed MRs. In principle this could be done
by re-writing the beam search algorithm used in
decoding to immediately discard malformed MRs;
for the experiments in this paper we simply filter
the regular n-best list until we find a well-formed
MR. This filtering can be done with time linear in
the length of the example by exploiting the argu-
ment label numbers introduced during lineariza-
tion. Finally, we insert the brackets according to
the tree structure specified by the argument num-
ber labels.
</bodyText>
<sectionHeader confidence="0.998621" genericHeader="method">
3 Experimental setup
</sectionHeader>
<bodyText confidence="0.999946">
Dataset We conduct experiments on the Geo-
Query data set. The corpus consists of a set of
880 natural-language questions about U.S. geog-
raphy in four languages (English, German, Greek
and Thai), and their representations in a variable-
free MRL that can be executed against a Prolog
database interface. Initial experimentation was
done using 10 fold cross-validation on the 600-
sentence development set and the final evaluation
on a held-out test set of 280 sentences. All seman-
tic parsers for GeoQuery we compare against also
makes use of NP lists (Jones et al., 2012), which
contain MRs for every noun phrase that appears in
the NL utterances of each language. In our exper-
iments, the NP list was included by appending all
entries as extra training sentences to the end of the
training corpus of each language with 50 times the
weight of regular training examples, to ensure that
they are learned as translation rules.
Evaluation for each utterance is performed by
executing both the predicted and the gold standard
MRs against the database and obtaining their re-
spective answers. An MR is correct if it obtains
the same answer as the gold standard MR, allow-
ing for a fair comparison between systems using
different learning paradigms. Following Jones et
al. (2012) we report accuracy, i.e. the percent-
age of NL questions with correct answers, and F1,
i.e. the harmonic mean of precision (percentage of
correct answers obtained).
Implementation In all experiments, we use the
IBM Model 4 implementation from the GIZA++
toolkit (Och and Ney, 2000) for alignment, and
the phrase-based and hierarchical models imple-
mented in the Moses toolkit (Koehn et al., 2007)
for rule extraction. The best symmetrization algo-
rithm, translation and language model weights for
each language are selected using cross-validation
on the development set. In the case of English and
German, we also found that stemming (Bird et al.,
2009; Porter, 1980) was hepful in reducing data
sparsity.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.999894318181818">
We first compare the results for the two translation
rule extraction models, phrase-based and hierar-
chical (“MT-phrase” and “MT-hier” respectively
in Table 1). We find that the hierarchical model
performs better in all languages apart from Greek,
indicating that the long-range reorderings learned
by a hierarchical translation system are useful for
this task. These benefits are most pronounced in
the case of Thai, likely due to the the language’s
comparatively different word order.
We also present results for both models with-
out using the NP lists for training in Table 2. As
expected, the performances are almost uniformly
lower, but the parser still produces correct output
for the majority of examples.
As discussed above, one important modifica-
tion of the MT paradigm which allows us to pro-
duce structured output is the addition of structure-
checking to the beam search. It is not evident,
a priori, that this search procedure is guaran-
teed to find any well-formed outputs in reasonable
time; to test the effect of this extra requirement on
</bodyText>
<table confidence="0.9833784">
en de el th
MT-phrase 75.3 68.8 70.4 53.0
MT-phrase (-NP) 63.4 65.8 64.0 39.8
MT-hier 80.5 68.9 69.1 70.4
MT-hier (-NP) 62.5 69.9 62.9 62.1
</table>
<tableCaption confidence="0.9772785">
Table 2: GeoQuery accuracies with and without
NPs. Rows with (-NP) did not use the NP list.
</tableCaption>
<page confidence="0.996001">
49
</page>
<table confidence="0.952794">
English [en] German [de] Greek [el] Thai [th]
cc. cc. cc. cc.
WASP 71.1 77.7 65.7 74.9 70.7 78.6 71.4 75.0
UBL 82.1 82.1 75.0 75.0 73.6 73.7 66.4 66.4
tsVB 79.3 79.3 74.6 74.6 75.4 75.4 78.2 78.2
hybrid-tree 76.8 81.0 62.1 68.5 69.3 74.6 73.6 76.7
MT-phrase 75.3 75.8 68.8 70.8 70.4 73.0 53.0 54.4
MT-hier 80.5 81.8 68.9 71.8 69.1 72.3 70.4 70.7
</table>
<tableCaption confidence="0.8728825">
Table 1: Accuracy and Fi scores for the multilingual GeoQuery test set. Results for other systems as
reported by Jones et al. (2012).
</tableCaption>
<bodyText confidence="0.999682578947368">
the speed of SP, we investigate how many MRs
the decoder needs to generate before producing
one which is well-formed. In practice, increasing
search depth in the n-best list from 1 to 50 results
in a gain of no more than a percentage point or
two, and we conclude that our filtering method is
appropriate for the task.
We also compare the MT-based semantic
parsers to several recently published ones: WASP
(Wong and Mooney, 2006), which like the hier-
archical model described here learns a SCFG to
translate between NL and MRL; tsVB (Jones et
al., 2012), which uses variational Bayesian infer-
ence to learn weights for a tree transducer; UBL
(Kwiatkowski et al., 2010), which learns a CCG
lexicon with semantic annotations; and hybrid-
tree (Lu et al., 2008), which learns a synchronous
generative model over variable-free MRs and NL
strings.
In the results shown in Table 1 we observe that
on English GeoQuery data, the hierarchical trans-
lation model achieves scores competitive with the
state of the art, and in every language one of the
MT systems achieves accuracy at least as good as
a purpose-built semantic parser.
We conclude with an informal test of training
speeds. While differences in implementation and
factors like programming language choice make
a direct comparison of times necessarily impre-
cise, we note that the MT system takes less than
three minutes to train on the GeoQuery corpus,
while the publicly-available implementations of
tsVB and UBL require roughly twenty minutes and
five hours respectively on a 2.1 GHz CPU. So
in addition to competitive performance, the MT-
based parser also appears to be considerably more
efficient at training time than other parsers in the
literature.
</bodyText>
<sectionHeader confidence="0.999851" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.99991645945946">
WASP, an early automatically-learned SP system,
was strongly influenced by MT techniques. Like
the present work, it uses GIZA++ alignments as
a starting point for the rule extraction procedure,
and algorithms reminiscent of those used in syn-
tactic MT to extract rules.
tsVB also uses a piece of standard MT ma-
chinery, specifically tree transducers, which have
been profitably employed for syntax-based ma-
chine translation (Maletti, 2010). In that work,
however, the usual MT parameter-estimation tech-
nique of simply counting the number of rule oc-
currences does not improve scores, and the au-
thors instead resort to a variational inference pro-
cedure to acquire rule weights. The present work
is also the first we are aware of which uses phrase-
based rather than tree-based machine translation
techniques to learn a semantic parser. hybrid-tree
(Lu et al., 2008) similarly describes a generative
model over derivations of MRL trees.
The remaining system discussed in this paper,
UBL (Kwiatkowski et al., 2010), leverages the fact
that the MRL does not simply encode trees, but
rather A-calculus expressions. It employs resolu-
tion procedures specific to the A-calculus such as
splitting and unification in order to generate rule
templates. Like other systems described, it uses
GIZA alignments for initialization. Other work
which generalizes from variable-free meaning rep-
resentations to A-calculus expressions includes the
natural language generation procedure described
by Lu and Ng (2011).
UBL, like an MT system (and unlike most of the
other systems discussed in this section), extracts
rules at multiple levels of granularity by means of
this splitting and unification procedure. hybrid-
tree similarly benefits from the introduction of
</bodyText>
<page confidence="0.99008">
50
</page>
<bodyText confidence="0.788286">
multi-level rules composed from smaller rules, a
process similar to the one used for creating phrase
tables in a phrase-based MT system.
Peter F. Brown, Vincent J. Della Pietra, Stephen
A. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
parameter estimation. Computational Linguistics,
19(2):263–311.
</bodyText>
<sectionHeader confidence="0.92381" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9999676">
Our results validate the hypothesis that it is possi-
ble to adapt an ordinary MT system into a work-
ing semantic parser. In spite of the compara-
tive simplicity of the approach, it achieves scores
comparable to (and sometimes better than) many
state-of-the-art systems. For this reason, we argue
for the use of a machine translation baseline as a
point of comparison for new methods. The results
also demonstrate the usefulness of two techniques
which are crucial for successful MT, but which are
not widely used in semantic parsing. The first is
the incorporation of a language model (or com-
parable long-distance structure-scoring model) to
assign scores to predicted parses independent of
the transformation model. The second is the
use of large, composed rules (rather than rules
which trigger on only one lexical item, or on tree
portions of limited depth (Lu et al., 2008)) in
order to “memorize” frequently-occurring large-
scale structures.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999990818181818">
We have presented a semantic parser which uses
techniques from machine translation to learn map-
pings from natural language to variable-free mean-
ing representations. The parser performs com-
parably to several recent purpose-built semantic
parsers on the GeoQuery dataset, while training
considerably faster than state-of-the-art systems.
Our experiments demonstrate the usefulness of
several techniques which might be broadly applied
to other semantic parsers, and provides an infor-
mative basis for future work.
</bodyText>
<sectionHeader confidence="0.997654" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.952174333333333">
Jacob Andreas is supported by a Churchill Schol-
arship. Andreas Vlachos is funded by the Eu-
ropean Community’s Seventh Framework Pro-
gramme (FP7/2007-2013) under grant agree-
ment no. 270019 (SPACEBOOK project www.
spacebook-project.eu).
</bodyText>
<sectionHeader confidence="0.99832" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999606740740741">
Steven Bird, Edward Loper, and Edward Klein.
2009. Natural Language Processing with Python.
O’Reilly Media, Inc.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting on Association
for Computational Linguistics, pages 263–270, Ann
Arbor, Michigan.
H.B. Curry, J.R. Hindley, and J.P. Seldin. 1980. To
H.B. Curry: Essays on Combinatory Logic, Lambda
Calculus, and Formalism. Academic Press.
Dan Goldwasser, Roi Reichart, James Clarke, and Dan
Roth. 2011. Confidence driven unsupervised se-
mantic parsing. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
1486–1495, Portland, Oregon.
Bevan K. Jones, Mark Johnson, and Sharon Goldwater.
2012. Semantic parsing with bayesian tree transduc-
ers. In Proceedings of the 50th Annual Meeting of
the Association of Computational Linguistics, pages
488–496, Jeju, Korea.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
48–54, Edmonton, Canada.
Philipp Koehn, Amittai Axelrod, Alexandra Birch-
Mayne, Chris Callison-Burch, Miles Osborne, and
David Talbot. 2005. Edinburgh System Descrip-
tion for the 2005 IWSLT Speech Translation Evalu-
ation. In Proceedings of the International Workshop
on Spoken Language Translation.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics Companion
Volume Proceedings of the Demo and Poster Ses-
sions, pages 177–180, Prague, Czech Republic.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1223–1233, Cambridge, Mas-
sachusetts.
Percy Liang, Michael Jordan, and Dan Klein. 2011.
Learning dependency-based compositional seman-
tics. In Proceedings of the 49th Annual Meeting of
</reference>
<page confidence="0.980109">
51
</page>
<reference confidence="0.998534487804878">
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 590–599, Port-
land, Oregon.
Wei Lu and Hwee Tou Ng. 2011. A probabilistic
forest-to-string model for language generation from
typed lambda calculus expressions. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, EMNLP ’11, pages 1611–
1622. Association for Computational Linguistics.
Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke Zettle-
moyer. 2008. A generative model for parsing nat-
ural language to meaning representations. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 783–
792, Edinburgh, UK.
Andreas Maletti. 2010. Survey: Tree transducers
in machine translation. In Proceedings of the 2nd
Workshop on Non-Classical Models for Automata
and Applications, Jena, Germany.
Franz Josef Och and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Com-
putational Linguistics, pages 440–447, Hong Kong,
China.
Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.
2003. Towards a theory of natural language inter-
faces to databases. In Proceedings of the 8th Inter-
national Conference on Intelligent User Interfaces,
pages 149–157, Santa Monica, CA.
M. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130–137.
Yuk Wah Wong and Raymond Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the 2006 Human Lan-
guage Technology Conference of the North Amer-
ican Chapter of the Association of Computational
Linguistics, pages 439–446, New York.
John M. Zelle. 1995. Using Inductive Logic Program-
ming to Automate the Construction of Natural Lan-
guage Parsers. Ph.D. thesis, Department of Com-
puter Sciences, The University of Texas at Austin.
</reference>
<page confidence="0.998857">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.780910">
<title confidence="0.999917">Semantic Parsing as Machine Translation</title>
<author confidence="0.998121">Jacob</author>
<affiliation confidence="0.996959">Computer University of</affiliation>
<email confidence="0.972107">jda33@cam.ac.uk</email>
<author confidence="0.940383">Andreas</author>
<affiliation confidence="0.994755">Computer University of</affiliation>
<email confidence="0.955586">av308@cam.ac.uk</email>
<author confidence="0.981129">Stephen</author>
<affiliation confidence="0.996003">Computer University of</affiliation>
<email confidence="0.965781">sc609@cam.ac.uk</email>
<abstract confidence="0.998008578947369">Semantic parsing is the problem of deriving a structured meaning representation from a natural language utterance. Here we approach it as a straightforward machine translation task, and demonstrate that standard machine translation components can be adapted into a semantic parser. In experiments on the multilingual GeoQuery corpus we find that our parser is competitive with the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems. These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Edward Loper</author>
<author>Edward Klein</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python.</booktitle>
<publisher>O’Reilly Media, Inc.</publisher>
<contexts>
<context position="10476" citStr="Bird et al., 2009" startWordPosition="1673" endWordPosition="1676">centage of NL questions with correct answers, and F1, i.e. the harmonic mean of precision (percentage of correct answers obtained). Implementation In all experiments, we use the IBM Model 4 implementation from the GIZA++ toolkit (Och and Ney, 2000) for alignment, and the phrase-based and hierarchical models implemented in the Moses toolkit (Koehn et al., 2007) for rule extraction. The best symmetrization algorithm, translation and language model weights for each language are selected using cross-validation on the development set. In the case of English and German, we also found that stemming (Bird et al., 2009; Porter, 1980) was hepful in reducing data sparsity. 4 Results We first compare the results for the two translation rule extraction models, phrase-based and hierarchical (“MT-phrase” and “MT-hier” respectively in Table 1). We find that the hierarchical model performs better in all languages apart from Greek, indicating that the long-range reorderings learned by a hierarchical translation system are useful for this task. These benefits are most pronounced in the case of Thai, likely due to the the language’s comparatively different word order. We also present results for both models without us</context>
</contexts>
<marker>Bird, Loper, Klein, 2009</marker>
<rawString>Steven Bird, Edward Loper, and Edward Klein. 2009. Natural Language Processing with Python. O’Reilly Media, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>263--270</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="6441" citStr="Chiang, 2005" startWordPosition="1018" endWordPosition="1019">ment Following the linearization of the MRs, we find alignments between the MR tokens and the NL tokens using the IBM Model 4 (Brown et al., 1993). Once the alignment algorithm is run in both directions (NL to MRL, MRL to NL), we symmetrize the resulting alignments to obtain a consensus many-to-many alignment (Och and Ney, 2000; Koehn et al., 2005). Rule extraction From the many-to-many alignment we need to extract a translation rule table, consisting of corresponding phrases in NL and MRL. We consider a phrase-based translation model (Koehn et al., 2003) and a hierarchical translation model (Chiang, 2005). Rules for the phrase-based model consist of pairs of aligned source and target sequences, while hierarchical rules are SCFG productions containing at most two instances of a single nonterminal symbol. Note that both extraction algorithms can learn rules which a traditional tree-transducer-based approach cannot—for example the right hand side [X] river, all0 traverse, [X] corresponding to the pair of disconnected tree fragments: river V all (where each X indicates a gap in the rule). Language modeling In addition to translation rules learned from a parallel corpus, MT systems also rely on an </context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H B Curry</author>
<author>J R Hindley</author>
<author>J P Seldin</author>
</authors>
<title>To H.B. Curry: Essays on Combinatory Logic, Lambda Calculus, and Formalism.</title>
<date>1980</date>
<publisher>Academic Press.</publisher>
<contexts>
<context position="4565" citStr="Curry et al., 1980" startWordPosition="719" endWordPosition="722">EARIZE state border texa state, next to, state, stateid, texas0 4 ALIGN 4 EXTRACT (PHRASE) ( state , state, ) ( state border, state, border, ) ( texa , state, stateid, texas0 ) ... 4 EXTRACT (HIER) [X] (state , state,) [X] (state [X] texa , state, [X] state, stateid, texas0) ... Figure 1: Illustration of preprocessing and rule extraction. Linearization We assume that the MRL is variable-free (that is, the meaning representation for each utterance is tree-shaped), noting that formalisms with variables, like the A-calculus, can be mapped onto variable-free logical forms with combinatory logics (Curry et al., 1980). In order to learn a semantic parser using MT we begin by converting these MRs to a form more similar to NL. To do so, we simply take a preorder traversal of every functional form, and label every function with the number of arguments it takes. After translation, recovery of the function is easy: if the arity of every function in the MRL is known, then every traversal uniquely specifies its corresponding tree. Using an example from GeoQuery, given an input function of the form answer(population(city(cityid(‘seattle’, ‘wa’)))) we produce a “decorated” translation input of the form answer, popu</context>
</contexts>
<marker>Curry, Hindley, Seldin, 1980</marker>
<rawString>H.B. Curry, J.R. Hindley, and J.P. Seldin. 1980. To H.B. Curry: Essays on Combinatory Logic, Lambda Calculus, and Formalism. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Goldwasser</author>
<author>Roi Reichart</author>
<author>James Clarke</author>
<author>Dan Roth</author>
</authors>
<title>Confidence driven unsupervised semantic parsing.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1486--1495</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="1334" citStr="Goldwasser et al., 2011" startWordPosition="191" endWordPosition="194">racy than recently proposed purpose-built systems. These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation. 1 Introduction Semantic parsing (SP) is the problem of transforming a natural language (NL) utterance into a machine-interpretable meaning representation (MR). It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. The key difference </context>
</contexts>
<marker>Goldwasser, Reichart, Clarke, Roth, 2011</marker>
<rawString>Dan Goldwasser, Roi Reichart, James Clarke, and Dan Roth. 2011. Confidence driven unsupervised semantic parsing. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1486–1495, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bevan K Jones</author>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Semantic parsing with bayesian tree transducers.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>488--496</pages>
<location>Jeju,</location>
<contexts>
<context position="1867" citStr="Jones et al., 2012" startWordPosition="279" endWordPosition="282">opescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. The key difference between the two tasks is that in SP, the target language (the MRL) has very different properties to an NL. In particular, MRs must conform strictly to a particular structure so that they are machine-interpretable. Contrast this with ordinary MT, where varying degrees of wrongness are tolerated by human readers (and evaluation metrics). To avoid producing malformed MRs, almost all of the existing research on SP has focused on developing models with richer structure than those commonly used for MT. In this work we attempt to dete</context>
<context position="9128" citStr="Jones et al., 2012" startWordPosition="1452" endWordPosition="1455">by the argument number labels. 3 Experimental setup Dataset We conduct experiments on the GeoQuery data set. The corpus consists of a set of 880 natural-language questions about U.S. geography in four languages (English, German, Greek and Thai), and their representations in a variablefree MRL that can be executed against a Prolog database interface. Initial experimentation was done using 10 fold cross-validation on the 600- sentence development set and the final evaluation on a held-out test set of 280 sentences. All semantic parsers for GeoQuery we compare against also makes use of NP lists (Jones et al., 2012), which contain MRs for every noun phrase that appears in the NL utterances of each language. In our experiments, the NP list was included by appending all entries as extra training sentences to the end of the training corpus of each language with 50 times the weight of regular training examples, to ensure that they are learned as translation rules. Evaluation for each utterance is performed by executing both the predicted and the gold standard MRs against the database and obtaining their respective answers. An MR is correct if it obtains the same answer as the gold standard MR, allowing for a</context>
<context position="12302" citStr="Jones et al. (2012)" startWordPosition="1986" endWordPosition="1989">9 69.1 70.4 MT-hier (-NP) 62.5 69.9 62.9 62.1 Table 2: GeoQuery accuracies with and without NPs. Rows with (-NP) did not use the NP list. 49 English [en] German [de] Greek [el] Thai [th] cc. cc. cc. cc. WASP 71.1 77.7 65.7 74.9 70.7 78.6 71.4 75.0 UBL 82.1 82.1 75.0 75.0 73.6 73.7 66.4 66.4 tsVB 79.3 79.3 74.6 74.6 75.4 75.4 78.2 78.2 hybrid-tree 76.8 81.0 62.1 68.5 69.3 74.6 73.6 76.7 MT-phrase 75.3 75.8 68.8 70.8 70.4 73.0 53.0 54.4 MT-hier 80.5 81.8 68.9 71.8 69.1 72.3 70.4 70.7 Table 1: Accuracy and Fi scores for the multilingual GeoQuery test set. Results for other systems as reported by Jones et al. (2012). the speed of SP, we investigate how many MRs the decoder needs to generate before producing one which is well-formed. In practice, increasing search depth in the n-best list from 1 to 50 results in a gain of no more than a percentage point or two, and we conclude that our filtering method is appropriate for the task. We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; tsVB (Jones et al., 2012), which uses variational Bayesian inference to</context>
</contexts>
<marker>Jones, Johnson, Goldwater, 2012</marker>
<rawString>Bevan K. Jones, Mark Johnson, and Sharon Goldwater. 2012. Semantic parsing with bayesian tree transducers. In Proceedings of the 50th Annual Meeting of the Association of Computational Linguistics, pages 488–496, Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>48--54</pages>
<location>Edmonton, Canada.</location>
<contexts>
<context position="6389" citStr="Koehn et al., 2003" startWordPosition="1008" endWordPosition="1011">erent natural language strings depending on context. Alignment Following the linearization of the MRs, we find alignments between the MR tokens and the NL tokens using the IBM Model 4 (Brown et al., 1993). Once the alignment algorithm is run in both directions (NL to MRL, MRL to NL), we symmetrize the resulting alignments to obtain a consensus many-to-many alignment (Och and Ney, 2000; Koehn et al., 2005). Rule extraction From the many-to-many alignment we need to extract a translation rule table, consisting of corresponding phrases in NL and MRL. We consider a phrase-based translation model (Koehn et al., 2003) and a hierarchical translation model (Chiang, 2005). Rules for the phrase-based model consist of pairs of aligned source and target sequences, while hierarchical rules are SCFG productions containing at most two instances of a single nonterminal symbol. Note that both extraction algorithms can learn rules which a traditional tree-transducer-based approach cannot—for example the right hand side [X] river, all0 traverse, [X] corresponding to the pair of disconnected tree fragments: river V all (where each X indicates a gap in the rule). Language modeling In addition to translation rules learned</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pages 48–54, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Amittai Axelrod</author>
<author>Alexandra BirchMayne</author>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>David Talbot</author>
</authors>
<title>Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation.</title>
<date>2005</date>
<booktitle>In Proceedings of the International Workshop on Spoken Language Translation.</booktitle>
<contexts>
<context position="6178" citStr="Koehn et al., 2005" startWordPosition="973" endWordPosition="976">struct the corresponding tree structure (if one exists). Arity labeling additionally allows functions with variable numbers of arguments (e.g. cityid, which in some training examples is unary) to align with different natural language strings depending on context. Alignment Following the linearization of the MRs, we find alignments between the MR tokens and the NL tokens using the IBM Model 4 (Brown et al., 1993). Once the alignment algorithm is run in both directions (NL to MRL, MRL to NL), we symmetrize the resulting alignments to obtain a consensus many-to-many alignment (Och and Ney, 2000; Koehn et al., 2005). Rule extraction From the many-to-many alignment we need to extract a translation rule table, consisting of corresponding phrases in NL and MRL. We consider a phrase-based translation model (Koehn et al., 2003) and a hierarchical translation model (Chiang, 2005). Rules for the phrase-based model consist of pairs of aligned source and target sequences, while hierarchical rules are SCFG productions containing at most two instances of a single nonterminal symbol. Note that both extraction algorithms can learn rules which a traditional tree-transducer-based approach cannot—for example the right h</context>
</contexts>
<marker>Koehn, Axelrod, BirchMayne, Callison-Burch, Osborne, Talbot, 2005</marker>
<rawString>Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, and David Talbot. 2005. Edinburgh System Description for the 2005 IWSLT Speech Translation Evaluation. In Proceedings of the International Workshop on Spoken Language Translation.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
<author>Chris Dyer</author>
<author>Ondˇrej Bojar</author>
<author>Alexandra Constantin</author>
<author>Evan Herbst</author>
</authors>
<title>Moses: open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>177--180</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="10221" citStr="Koehn et al., 2007" startWordPosition="1633" endWordPosition="1636">and obtaining their respective answers. An MR is correct if it obtains the same answer as the gold standard MR, allowing for a fair comparison between systems using different learning paradigms. Following Jones et al. (2012) we report accuracy, i.e. the percentage of NL questions with correct answers, and F1, i.e. the harmonic mean of precision (percentage of correct answers obtained). Implementation In all experiments, we use the IBM Model 4 implementation from the GIZA++ toolkit (Och and Ney, 2000) for alignment, and the phrase-based and hierarchical models implemented in the Moses toolkit (Koehn et al., 2007) for rule extraction. The best symmetrization algorithm, translation and language model weights for each language are selected using cross-validation on the development set. In the case of English and German, we also found that stemming (Bird et al., 2009; Porter, 1980) was hepful in reducing data sparsity. 4 Results We first compare the results for the two translation rule extraction models, phrase-based and hierarchical (“MT-phrase” and “MT-hier” respectively in Table 1). We find that the hierarchical model performs better in all languages apart from Greek, indicating that the long-range reo</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, Herbst, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177–180, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing probabilistic ccg grammars from logical form with higherorder unification.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1223--1233</pages>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="1797" citStr="Kwiatkowski et al., 2010" startWordPosition="266" endWordPosition="269"> wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. The key difference between the two tasks is that in SP, the target language (the MRL) has very different properties to an NL. In particular, MRs must conform strictly to a particular structure so that they are machine-interpretable. Contrast this with ordinary MT, where varying degrees of wrongness are tolerated by human readers (and evaluation metrics). To avoid producing malformed MRs, almost all of the existing research on SP has focused on developing models with richer stru</context>
<context position="12970" citStr="Kwiatkowski et al., 2010" startWordPosition="2101" endWordPosition="2104">MRs the decoder needs to generate before producing one which is well-formed. In practice, increasing search depth in the n-best list from 1 to 50 results in a gain of no more than a percentage point or two, and we conclude that our filtering method is appropriate for the task. We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; tsVB (Jones et al., 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al., 2010), which learns a CCG lexicon with semantic annotations; and hybridtree (Lu et al., 2008), which learns a synchronous generative model over variable-free MRs and NL strings. In the results shown in Table 1 we observe that on English GeoQuery data, the hierarchical translation model achieves scores competitive with the state of the art, and in every language one of the MT systems achieves accuracy at least as good as a purpose-built semantic parser. We conclude with an informal test of training speeds. While differences in implementation and factors like programming language choice make a direct</context>
<context position="15027" citStr="Kwiatkowski et al., 2010" startWordPosition="2430" endWordPosition="2433">yed for syntax-based machine translation (Maletti, 2010). In that work, however, the usual MT parameter-estimation technique of simply counting the number of rule occurrences does not improve scores, and the authors instead resort to a variational inference procedure to acquire rule weights. The present work is also the first we are aware of which uses phrasebased rather than tree-based machine translation techniques to learn a semantic parser. hybrid-tree (Lu et al., 2008) similarly describes a generative model over derivations of MRL trees. The remaining system discussed in this paper, UBL (Kwiatkowski et al., 2010), leverages the fact that the MRL does not simply encode trees, but rather A-calculus expressions. It employs resolution procedures specific to the A-calculus such as splitting and unification in order to generate rule templates. Like other systems described, it uses GIZA alignments for initialization. Other work which generalizes from variable-free meaning representations to A-calculus expressions includes the natural language generation procedure described by Lu and Ng (2011). UBL, like an MT system (and unlike most of the other systems discussed in this section), extracts rules at multiple </context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic ccg grammars from logical form with higherorder unification. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1223–1233, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Michael Jordan</author>
<author>Dan Klein</author>
</authors>
<title>Learning dependency-based compositional semantics.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>590--599</pages>
<location>Portland, Oregon.</location>
<contexts>
<context position="1375" citStr="Liang et al., 2011" startWordPosition="197" endWordPosition="200">ems. These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation. 1 Introduction Semantic parsing (SP) is the problem of transforming a natural language (NL) utterance into a machine-interpretable meaning representation (MR). It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. The key difference between the two tasks is that in SP, the </context>
</contexts>
<marker>Liang, Jordan, Klein, 2011</marker>
<rawString>Percy Liang, Michael Jordan, and Dan Klein. 2011. Learning dependency-based compositional semantics. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 590–599, Portland, Oregon.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>A probabilistic forest-to-string model for language generation from typed lambda calculus expressions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1611--1622</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="15509" citStr="Lu and Ng (2011)" startWordPosition="2499" endWordPosition="2502">scribes a generative model over derivations of MRL trees. The remaining system discussed in this paper, UBL (Kwiatkowski et al., 2010), leverages the fact that the MRL does not simply encode trees, but rather A-calculus expressions. It employs resolution procedures specific to the A-calculus such as splitting and unification in order to generate rule templates. Like other systems described, it uses GIZA alignments for initialization. Other work which generalizes from variable-free meaning representations to A-calculus expressions includes the natural language generation procedure described by Lu and Ng (2011). UBL, like an MT system (and unlike most of the other systems discussed in this section), extracts rules at multiple levels of granularity by means of this splitting and unification procedure. hybridtree similarly benefits from the introduction of 50 multi-level rules composed from smaller rules, a process similar to the one used for creating phrase tables in a phrase-based MT system. Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311. 6</context>
</contexts>
<marker>Lu, Ng, 2011</marker>
<rawString>Wei Lu and Hwee Tou Ng. 2011. A probabilistic forest-to-string model for language generation from typed lambda calculus expressions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1611– 1622. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Lu</author>
<author>Hwee Tou Ng</author>
<author>Wee Sun Lee</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>A generative model for parsing natural language to meaning representations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>783--792</pages>
<location>Edinburgh, UK.</location>
<contexts>
<context position="13058" citStr="Lu et al., 2008" startWordPosition="2116" endWordPosition="2119">ing search depth in the n-best list from 1 to 50 results in a gain of no more than a percentage point or two, and we conclude that our filtering method is appropriate for the task. We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; tsVB (Jones et al., 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al., 2010), which learns a CCG lexicon with semantic annotations; and hybridtree (Lu et al., 2008), which learns a synchronous generative model over variable-free MRs and NL strings. In the results shown in Table 1 we observe that on English GeoQuery data, the hierarchical translation model achieves scores competitive with the state of the art, and in every language one of the MT systems achieves accuracy at least as good as a purpose-built semantic parser. We conclude with an informal test of training speeds. While differences in implementation and factors like programming language choice make a direct comparison of times necessarily imprecise, we note that the MT system takes less than t</context>
<context position="14880" citStr="Lu et al., 2008" startWordPosition="2408" endWordPosition="2411">ctic MT to extract rules. tsVB also uses a piece of standard MT machinery, specifically tree transducers, which have been profitably employed for syntax-based machine translation (Maletti, 2010). In that work, however, the usual MT parameter-estimation technique of simply counting the number of rule occurrences does not improve scores, and the authors instead resort to a variational inference procedure to acquire rule weights. The present work is also the first we are aware of which uses phrasebased rather than tree-based machine translation techniques to learn a semantic parser. hybrid-tree (Lu et al., 2008) similarly describes a generative model over derivations of MRL trees. The remaining system discussed in this paper, UBL (Kwiatkowski et al., 2010), leverages the fact that the MRL does not simply encode trees, but rather A-calculus expressions. It employs resolution procedures specific to the A-calculus such as splitting and unification in order to generate rule templates. Like other systems described, it uses GIZA alignments for initialization. Other work which generalizes from variable-free meaning representations to A-calculus expressions includes the natural language generation procedure </context>
<context position="16995" citStr="Lu et al., 2008" startWordPosition="2735" endWordPosition="2738">systems. For this reason, we argue for the use of a machine translation baseline as a point of comparison for new methods. The results also demonstrate the usefulness of two techniques which are crucial for successful MT, but which are not widely used in semantic parsing. The first is the incorporation of a language model (or comparable long-distance structure-scoring model) to assign scores to predicted parses independent of the transformation model. The second is the use of large, composed rules (rather than rules which trigger on only one lexical item, or on tree portions of limited depth (Lu et al., 2008)) in order to “memorize” frequently-occurring largescale structures. 7 Conclusions We have presented a semantic parser which uses techniques from machine translation to learn mappings from natural language to variable-free meaning representations. The parser performs comparably to several recent purpose-built semantic parsers on the GeoQuery dataset, while training considerably faster than state-of-the-art systems. Our experiments demonstrate the usefulness of several techniques which might be broadly applied to other semantic parsers, and provides an informative basis for future work. Acknowl</context>
</contexts>
<marker>Lu, Ng, Lee, Zettlemoyer, 2008</marker>
<rawString>Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke Zettlemoyer. 2008. A generative model for parsing natural language to meaning representations. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 783– 792, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Maletti</author>
</authors>
<title>Survey: Tree transducers in machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2nd Workshop on Non-Classical Models for Automata and Applications,</booktitle>
<location>Jena, Germany.</location>
<contexts>
<context position="14458" citStr="Maletti, 2010" startWordPosition="2340" endWordPosition="2341"> in addition to competitive performance, the MTbased parser also appears to be considerably more efficient at training time than other parsers in the literature. 5 Related Work WASP, an early automatically-learned SP system, was strongly influenced by MT techniques. Like the present work, it uses GIZA++ alignments as a starting point for the rule extraction procedure, and algorithms reminiscent of those used in syntactic MT to extract rules. tsVB also uses a piece of standard MT machinery, specifically tree transducers, which have been profitably employed for syntax-based machine translation (Maletti, 2010). In that work, however, the usual MT parameter-estimation technique of simply counting the number of rule occurrences does not improve scores, and the authors instead resort to a variational inference procedure to acquire rule weights. The present work is also the first we are aware of which uses phrasebased rather than tree-based machine translation techniques to learn a semantic parser. hybrid-tree (Lu et al., 2008) similarly describes a generative model over derivations of MRL trees. The remaining system discussed in this paper, UBL (Kwiatkowski et al., 2010), leverages the fact that the M</context>
</contexts>
<marker>Maletti, 2010</marker>
<rawString>Andreas Maletti. 2010. Survey: Tree transducers in machine translation. In Proceedings of the 2nd Workshop on Non-Classical Models for Automata and Applications, Jena, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Improved statistical alignment models.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>440--447</pages>
<location>Hong Kong, China.</location>
<contexts>
<context position="6157" citStr="Och and Ney, 2000" startWordPosition="969" endWordPosition="972">we can always reconstruct the corresponding tree structure (if one exists). Arity labeling additionally allows functions with variable numbers of arguments (e.g. cityid, which in some training examples is unary) to align with different natural language strings depending on context. Alignment Following the linearization of the MRs, we find alignments between the MR tokens and the NL tokens using the IBM Model 4 (Brown et al., 1993). Once the alignment algorithm is run in both directions (NL to MRL, MRL to NL), we symmetrize the resulting alignments to obtain a consensus many-to-many alignment (Och and Ney, 2000; Koehn et al., 2005). Rule extraction From the many-to-many alignment we need to extract a translation rule table, consisting of corresponding phrases in NL and MRL. We consider a phrase-based translation model (Koehn et al., 2003) and a hierarchical translation model (Chiang, 2005). Rules for the phrase-based model consist of pairs of aligned source and target sequences, while hierarchical rules are SCFG productions containing at most two instances of a single nonterminal symbol. Note that both extraction algorithms can learn rules which a traditional tree-transducer-based approach cannot—fo</context>
<context position="10107" citStr="Och and Ney, 2000" startWordPosition="1615" endWordPosition="1618">n for each utterance is performed by executing both the predicted and the gold standard MRs against the database and obtaining their respective answers. An MR is correct if it obtains the same answer as the gold standard MR, allowing for a fair comparison between systems using different learning paradigms. Following Jones et al. (2012) we report accuracy, i.e. the percentage of NL questions with correct answers, and F1, i.e. the harmonic mean of precision (percentage of correct answers obtained). Implementation In all experiments, we use the IBM Model 4 implementation from the GIZA++ toolkit (Och and Ney, 2000) for alignment, and the phrase-based and hierarchical models implemented in the Moses toolkit (Koehn et al., 2007) for rule extraction. The best symmetrization algorithm, translation and language model weights for each language are selected using cross-validation on the development set. In the case of English and German, we also found that stemming (Bird et al., 2009; Porter, 1980) was hepful in reducing data sparsity. 4 Results We first compare the results for the two translation rule extraction models, phrase-based and hierarchical (“MT-phrase” and “MT-hier” respectively in Table 1). We find</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz Josef Och and Hermann Ney. 2000. Improved statistical alignment models. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics, pages 440–447, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-Maria Popescu</author>
<author>Oren Etzioni</author>
<author>Henry Kautz</author>
</authors>
<title>Towards a theory of natural language interfaces to databases.</title>
<date>2003</date>
<booktitle>In Proceedings of the 8th International Conference on Intelligent User Interfaces,</booktitle>
<pages>149--157</pages>
<location>Santa Monica, CA.</location>
<contexts>
<context position="1268" citStr="Popescu et al., 2003" startWordPosition="182" endWordPosition="185">th the state of the art, and in some cases achieves higher accuracy than recently proposed purpose-built systems. These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation. 1 Introduction Semantic parsing (SP) is the problem of transforming a natural language (NL) utterance into a machine-interpretable meaning representation (MR). It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) </context>
</contexts>
<marker>Popescu, Etzioni, Kautz, 2003</marker>
<rawString>Ana-Maria Popescu, Oren Etzioni, and Henry Kautz. 2003. Towards a theory of natural language interfaces to databases. In Proceedings of the 8th International Conference on Intelligent User Interfaces, pages 149–157, Santa Monica, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<volume>14</volume>
<issue>3</issue>
<contexts>
<context position="10491" citStr="Porter, 1980" startWordPosition="1677" endWordPosition="1678">ions with correct answers, and F1, i.e. the harmonic mean of precision (percentage of correct answers obtained). Implementation In all experiments, we use the IBM Model 4 implementation from the GIZA++ toolkit (Och and Ney, 2000) for alignment, and the phrase-based and hierarchical models implemented in the Moses toolkit (Koehn et al., 2007) for rule extraction. The best symmetrization algorithm, translation and language model weights for each language are selected using cross-validation on the development set. In the case of English and German, we also found that stemming (Bird et al., 2009; Porter, 1980) was hepful in reducing data sparsity. 4 Results We first compare the results for the two translation rule extraction models, phrase-based and hierarchical (“MT-phrase” and “MT-hier” respectively in Table 1). We find that the hierarchical model performs better in all languages apart from Greek, indicating that the long-range reorderings learned by a hierarchical translation system are useful for this task. These benefits are most pronounced in the case of Thai, likely due to the the language’s comparatively different word order. We also present results for both models without using the NP list</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>439--446</pages>
<location>New York.</location>
<contexts>
<context position="1770" citStr="Wong and Mooney, 2006" startWordPosition="262" endWordPosition="265">l-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. The key difference between the two tasks is that in SP, the target language (the MRL) has very different properties to an NL. In particular, MRs must conform strictly to a particular structure so that they are machine-interpretable. Contrast this with ordinary MT, where varying degrees of wrongness are tolerated by human readers (and evaluation metrics). To avoid producing malformed MRs, almost all of the existing research on SP has focused on develop</context>
<context position="12733" citStr="Wong and Mooney, 2006" startWordPosition="2061" endWordPosition="2064"> 53.0 54.4 MT-hier 80.5 81.8 68.9 71.8 69.1 72.3 70.4 70.7 Table 1: Accuracy and Fi scores for the multilingual GeoQuery test set. Results for other systems as reported by Jones et al. (2012). the speed of SP, we investigate how many MRs the decoder needs to generate before producing one which is well-formed. In practice, increasing search depth in the n-best list from 1 to 50 results in a gain of no more than a percentage point or two, and we conclude that our filtering method is appropriate for the task. We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; tsVB (Jones et al., 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al., 2010), which learns a CCG lexicon with semantic annotations; and hybridtree (Lu et al., 2008), which learns a synchronous generative model over variable-free MRs and NL strings. In the results shown in Table 1 we observe that on English GeoQuery data, the hierarchical translation model achieves scores competitive with the state of the art, and in every language one </context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 439–446, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Zelle</author>
</authors>
<title>Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Sciences, The University of Texas at Austin.</institution>
<contexts>
<context position="1294" citStr="Zelle, 1995" startWordPosition="188" endWordPosition="189">e cases achieves higher accuracy than recently proposed purpose-built systems. These results support the use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation. 1 Introduction Semantic parsing (SP) is the problem of transforming a natural language (NL) utterance into a machine-interpretable meaning representation (MR). It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship</context>
<context position="2880" citStr="Zelle, 1995" startWordPosition="451" endWordPosition="452">s). To avoid producing malformed MRs, almost all of the existing research on SP has focused on developing models with richer structure than those commonly used for MT. In this work we attempt to determine how accurate a semantic parser we can build by treating SP as a pure MT task, and describe pre- and postprocessing steps which allow structure to be preserved in the MT process. Our contributions are as follows: We develop a semantic parser using off-the-shelf MT components, exploring phrase-based as well as hierarchical models. Experiments with four languages on the popular GeoQuery corpus (Zelle, 1995) show that our parser is competitve with the state-ofthe-art, in some cases achieving higher accuracy than recently introduced purpose-built semantic parsers. Our approach also appears to require substantially less time to train than the two bestperforming semantic parsers. These results support the use of MT methods as an informative baseline in SP evaluations and show that research in SP could benefit from research advances in MT. 2 MT-based semantic parsing The input is a corpus of NL utterances paired with MRs. In order to learn a semantic parser using MT we linearize the MRs, learn alignm</context>
</contexts>
<marker>Zelle, 1995</marker>
<rawString>John M. Zelle. 1995. Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers. Ph.D. thesis, Department of Computer Sciences, The University of Texas at Austin.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>