<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.9945015">
Word Sense Disambiguation in a Korean-to-Japanese
MT System Using Neural Networks
</title>
<author confidence="0.974721">
You-Jin Chung, Sin-Jae Kang, Kyong-Hi Moon, and Jong-Hyeok Lee
</author>
<affiliation confidence="0.9435975">
Div. of Electrical and Computer Engineering, Pohang University of Science and Technology (POSTECH)
and Advanced Information Technology Research Center(AlTrc)
</affiliation>
<address confidence="0.94372">
San 31, Hyoja-dong, Nam-gu, Pohang, R. of KOREA, 790-784
</address>
<email confidence="0.98739">
{prizer,sjkang,khmoon,jhlee}@postech.ac.kr
</email>
<sectionHeader confidence="0.986103" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999972384615385">
This paper presents a method to resolve
word sense ambiguity in a
Korean-to-Japanese machine translation
system using neural networks. The
execution of our neural network model is
based on the concept codes of a thesaurus.
Most previous word sense disambiguation
approaches based on neural networks have
limitations due to their huge feature set size.
By contrast, we reduce the number of
features of the network to a practical size by
using concept codes as features rather than
the lexical words themselves.
</bodyText>
<sectionHeader confidence="0.834202" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999780324324324">
Korean-to-Japanese machine translation (MT)
employs a direct MT strategy, where a Korean
homograph may be translated into a different
Japanese equivalent depending on which sense
is used in a given context. Thus, word sense
disambiguation (WSD) is essential to the
selection of an appropriate Japanese target word.
Much research on word sense disambiguation
has revealed that several different types of
information can contribute to the resolution of
lexical ambiguity. These include surrounding
words (an unordered set of words surrounding a
target word), local collocations (a short sequence
of words near a target word, taking word order
into account), syntactic relations (selectional
restrictions), parts of speech, morphological
forms, etc (McRoy, 1992, Ng and Zelle, 1997).
Some researchers use neural networks in
their word sense disambiguation systems
Because of its strong capability in classification
(Waltz et al., 1985, Gallant, 1991, Leacock et al.,
1993, and Mooney, 1996). Since, however, most
such methods require a few thousands of
features or large amounts of hand-written data
for training, it is not clear that the same neural
network models will be applicable to real world
applications.
We propose a word sense disambiguation
method that combines both the neural net-based
approach and the work of Li et al (2000),
especially focusing on the practicality of the
method for application to real world MT
systems. To reduce the number of input features
of neural networks to a practical size, we use
concept codes of a thesaurus as features.
In this paper, Yale Romanization is used to
represent Korean expressions.
</bodyText>
<sectionHeader confidence="0.712759" genericHeader="method">
1 System Architecture
</sectionHeader>
<bodyText confidence="0.999941619047619">
Our neural network method consists of two
phases. The first phase is the construction of the
feature set for the neural network; the second
phase is the construction and training of the
neural network. (see Figure 1.)
For practical reasons, a reasonably small
number of features is essential to the design of a
neural network. To construct a feature set of a
reasonable size, we adopt Li’s method (2000),
based on concept co-occurrence information
(CCI). CCI are concept codes of words which
co-occur with the target word for a specific
syntactic relation.
In accordance with Li’s method, we
automatically extract CCI from a corpus by
constructing a Korean sense-tagged corpus. To
accomplish this, we apply a Japanese-to-Korean
MT system. Next, we extract CCI from the
constructed corpus through partial parsing and
scanning. To eliminate noise and to reduce the
number of CCI, refinement proceesing is applied
</bodyText>
<note confidence="0.509328">
Feature Set Construction Neural Net Construction
</note>
<figureCaption confidence="0.99903">
Figure 1. System Architecture
</figureCaption>
<bodyText confidence="0.999285166666667">
to the extracted raw CCI. After completing
refinement processing, we use the remaining
CCI as features for the neural network. The
trained network parameters are stored in a
Korean-to-Japanese MT dictionary for WSD in
translation.
</bodyText>
<sectionHeader confidence="0.64927" genericHeader="method">
2 Construction of Refined Feature Set
</sectionHeader>
<subsectionHeader confidence="0.989515">
2.1 Automatic Construction of Sense-tagged
Corpus
</subsectionHeader>
<bodyText confidence="0.997401947368421">
For automatic construction of the sense-tagged
corpus, we used a Japanese-to-Korean MT
system called COBALT-J/K1. In the transfer
dictionary of COBALT-J/K, nominal and verbal
words are annotated with concept codes of the
Kadokawa thesaurus (Ohno and Hamanishi,
1981), which has a 4-level hierarchy of about
1,100 semantic classes, as shown in Figure 2.
Concept nodes in level L1, L2 and L3 are further
divided into 10 subclasses.
We made a slight modification of
COBALT-J/K to enable it to produce Korean
translations from a Japanese text, with all
nominal words tagged with specific concept
codes at level L4 of the Kadokawa thesaurus. As
a result, a Korean sense-tagged corpus of
1,060,000 sentences can be obtained from the
Japanese corpus (Asahi Shinbun, Japanese
Newspaper of Economics, etc.).
</bodyText>
<footnote confidence="0.988963">
1 COBALT-J/K (Collocation-Based Language
Translator from Japanese to Korean) is a high-quality
practical MT system developed by POSTECH.
</footnote>
<figureCaption confidence="0.983376">
Figure 2. Concept hierarchy of the Kadokawa
thesaurus
</figureCaption>
<bodyText confidence="0.999920681818182">
The quality of the constructed sense-tagged
corpus is a critical issue. To evaluate the quality,
we collected 1,658 sample sentences (29,420
eojeols2) from the corpus and checked their
precision. The total number of errors was 789,
and included such errors as morphological
analysis, sense ambiguity resolution and
unknown words. It corresponds to the accuracy
of 97.3% (28,631 / 29,420 eojeols).
Because almost all Japanese common nouns
represented by Chinese characters are
monosemous little transfer ambiguity is
exhibited in Japanese-to-Korean translation. In
our test, the number of ambiguity resolution
errors was 202 and it took only 0.69% of the
overall corpus (202 / 29,420 eojeols).
Considering the fact that the overall accuracy of
the constructed corpus exceeds 97% and only a
few sense ambiguity resolution errors were
found in the Japanese-to-Korean translation of
nouns, we regard the generated sense-tagged
corpus as highly reliable.
</bodyText>
<subsectionHeader confidence="0.999992">
2.2 Extraction of Raw CCI
</subsectionHeader>
<bodyText confidence="0.999233">
Unlike English, Korean has almost no syntactic
constraints on word order as long as the verb
appears in the final position. The variable word
order often results in discontinuous constituents.
Instead of using local collocations by word order,
Li et al. (2000) defined 13 patterns of CCI for
homographs using syntactically related words in
a sentence. Because we are concerned only with
</bodyText>
<footnote confidence="0.548454">
2 An Eojeol is a Korean syntactic unit consisting of a
content word and one or more function words.
</footnote>
<figure confidence="0.999406595744681">
Stored in
MT Dictionary
Japanese Corpus
COBALT-J/K
(Japanese-to-Korean
MT system)
Sense Tagged
Korean Corpus
Partial Parsing
&amp; Pattern Scanning
Raw CCI
Feature Set
Network
Construction
Neural Network
Network
Learning
Network
Parameters
CCI Refinement
Processing
Refined CCI
L1 noun
•••••
nature character society institute things
0 1 7 8 9
•••••
L2
astro- calen- animal pheno-
nomy dar mena
00 01 06 09
goods drugs food stationary machine
90 91 92 96 99
L3
•••••
•••••
•••••
orga- ani- sin- intes- egg sex
nism mal ews tine
•••••
060 061 066 067 068 069
supp-writing- count- bell
lies tool book •••••
960 961 962 969
L4
••••• •••••
•••••
</figure>
<tableCaption confidence="0.997536">
Table 1. Structure of CCI Patterns
</tableCaption>
<table confidence="0.999292666666667">
CCI type Structure of pattern
type0 unordered co-occurrence words
type1 noun + noun or noun + noun
type2 noun + uy + noun
type3 noun + other particles + noun
type4 noun + lo/ulo + verb
type5 noun + ey + verb
type6 noun + eygey + verb
type7 noun + eyse + verb
type8 noun + ul/lul + verb
type9 noun + i/ka + verb
type10 verb + relativizer + noun
</table>
<bodyText confidence="0.999818578947369">
noun homographs, we adopt 11 patterns from
them excluding verb patterns, as shown in Table
1. The words in bold indicate the target
homograph and the words in italic indicate
Korean particles.
For a homograph W, concept frequency
patterns (CFPs), i.e., ({&lt;C1,f1&gt;,&lt;C2,f2&gt;, ... ,
&lt;Ck,fk&gt;}, typei, W(Si)), are extracted from the
sense-tagged training corpus for each CCI type i
by partial parsing and pattern scanning, where k
is the number of concept codes in typei, fi is the
frequency of concept code Ci appearing in the
corpus, typei is an CCI type i, and W(Si) is a
homograph W with a sense Si. All concepts in
CFPs are three-digit concept codes at level L4 in
the Kadokawa thesaurus. Table 2 demonstrates
an example of CFP that can co-occur with the
homograph ‘nwun(eye)’ in the form of the CCI
type2 and their frequencies.
</bodyText>
<subsectionHeader confidence="0.999841">
2.3 CCI Refinement Processing
</subsectionHeader>
<bodyText confidence="0.999853">
The extracted CCI are too numerous and too
noisy to be used in a practical system, and must
to be further selected. To eliminate noise and to
reduce the number of CCI to a practical size, we
apply the refinement processing to the extracted
CCI. CCI refinement processing is composed of
2 processes: concept code discrimination and
concept code generalization.
</bodyText>
<subsubsectionHeader confidence="0.930095">
2.3.1 Concept Code Discrimination
</subsubsectionHeader>
<bodyText confidence="0.999873">
In the extracted CCI, the same concept code may
appear for determining the different meanings of
a homograph. To select the most probable
concept codes, which frequently co-occur with
the target sense of a homograph, Li defined the
discrimination value of a concept code using
</bodyText>
<tableCaption confidence="0.648051">
Table 2. Concept codes and frequencies in CFP
({&lt;Ci,fi&gt;}, type2, nwun(eye))
</tableCaption>
<bodyText confidence="0.976209421052631">
Code Freq. Code Freq. Code Freq. Code Freq.
103 4 107 8 121 7 126 4
143 8 160 5 179 7 277 4
320 8 331 6 416 7 419 12
433 4 501 13 503 10 504 11
505 6 507 12 508 27 513 5
530 6 538 16 552 4 557 7
573 5 709 5 718 5 719 4
733 5 819 4 834 4 966 4
987 9 other* 210
Ú ‘other’ in the table means the set of concept codes
with the frequencies less than 4.
Shannon’s entropy (Shannon, 1951). A concept
code with a small entropy has a large
discrimination value. If the discrimination value
of the concept code is larger than a threshold,
the concept code is selected as useful
information for deciding the word sense.
Otherwise, the concept code is discarded.
</bodyText>
<subsubsectionHeader confidence="0.863238">
2.3.2 Concept Code Generalization
</subsubsectionHeader>
<bodyText confidence="0.999992222222222">
After concept discrimination, co-occurring
concept codes in each CCI type must be further
selected and the code generalized. To perform
code generalization, Li adopted to Smadja’s
work (Smadja, 1993) and defined the code
strength using a code frequency and a standard
deviation in each level of the concept hierarchy.
The generalization filter selects the concept
codes with a strength larger than a threshold. We
perform this generalizaion processing on the
Kadokawa thesaurus level L4 and L3.
After processing, the system stores the
refined conceptual patterns ({C1, C2, C3, ...},
typei, W(Si)) as a knowledge source for WSD of
real texts. These refined CCI are used as input
features for the neural network. The more
specific description of the CCI extraction is
explained in Li (2000).
</bodyText>
<sectionHeader confidence="0.670563" genericHeader="method">
3 Construction of Neural Network
</sectionHeader>
<subsectionHeader confidence="0.994023">
3.1 Neural Network Architecture
</subsectionHeader>
<bodyText confidence="0.9999612">
Because of its strong capability for classification,
the multilayer feedforward neural network is
used in our sense classification system. As
shown in Figure 3, each node in the input layer
represents a concept code in CCI of a target
</bodyText>
<figure confidence="0.998719325">
CCI type i1
input
CCI type i2
input
...
CCI type ik
input
Output
(senses of the
target word)
...
...
...
Hidden
Layers
Inputs Outputs
CCI type 0
input
CCI type 1
input
CCI type 2
input
CCI type 8
input
419
239
323
022
078
080
696
028
26
23
38
74
50
...
nwun1 (snow)
nwun2 (eye)
</figure>
<figureCaption confidence="0.999978">
Figure 3. Topology of Neural Network
</figureCaption>
<bodyText confidence="0.999860222222222">
word and each node in the output layer
represents the sense of a target word. The
number of hidden layers and the number of
nodes in a hidden layer are another crucial issue.
To determine a good topology for the network,
we implemented a 2-layer (no hidden layer) and
a 3-layer (with a single hidden layer of 5 nodes)
network and compared their performance. The
comparison result is given in Section 5.
Each homograph has a network of its own.
Figure 43 demonstrates a construction example
of the input layer for the homograph ‘nwun’
with the sense ‘snow’ and ‘eye’. The left side is
the extracted CCI for each sense after refinement
processing. We construct the input layer for
‘nwun’ by merely integrating the concept codes
in both senses. The resulting input layer is
partitioned into several subgroups depending on
</bodyText>
<figure confidence="0.362441">
Refined CCI
Total 13 concept codes
</figure>
<figureCaption confidence="0.994645">
Figure 4. Construction of Input layer for ‘nwun’
</figureCaption>
<footnote confidence="0.917716333333333">
3 The concept codes in Figure 4 are simplified ones
for the ease of illustration. In reality there are 87
concept codes for ‘nwun’.
</footnote>
<figureCaption confidence="0.99964">
Figure 5. The Resulting Network for ‘nwun’
</figureCaption>
<bodyText confidence="0.998736666666667">
their CCI types, i.e., type 0, type 1, type 2 and
type 8. Figure 5 shows the overall network
architecture for ‘nwun’.
</bodyText>
<subsectionHeader confidence="0.999049">
3.2 Network Learning
</subsectionHeader>
<bodyText confidence="0.999983227272727">
We selected 875 Korean homographs requring
the WSD processing in a Korean-to-Japanese
translation. Among the selected nouns, 736
nouns (about 84%) had two senses and the other
139 nouns had more than 3 senses. Using the
extracted CCI, we constructed neural networks
and trained network parameters for each
homograph. The training patterns were also
extracted from the previously constructed
sense-tagged corpus.
The average number of input features (i.e.
input nodes) of the constructed networks was
approximately 54.1 and the average number of
senses (i.e. output nodes) was about 2.19. In the
case of a 2-layer network, the total number of
parameters (synaptic weights) needed to be
trained is about 118 (54.1×2.19) for each
homograph. This means that we merely need
storage for 118 floating point numbers (for
synaptic weights) and 54 integers (for input
features) for each homograph, which is a
reasonable size to be used in real applications.
</bodyText>
<sectionHeader confidence="0.951225" genericHeader="method">
4 Word Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.99804925">
Our WSD approach is a hybrid method, which
combines the advantage of corpus-based and
knowledge-based methods. Figure 6 shows our
overall WSD algorithm. For a given homograph,
sense disambiguation is performed as follows.
First, we search a collocation dictionary. The
Korean-to-Japanese translation system
COBALT-K/J has an MWTU (Multi-Word
</bodyText>
<figure confidence="0.97261378125">
nwun1 (snow)
• CCI type 0 : {26, 022}
• CCI type 1 : {080, 696}
• CCI type 8 : {38, 239}
integrate
nwun2 (eye)
• CCI type 0 : {74, 078}
• CCI type 2 : {50, 028, 419}
• CCI type 8 : {23, 323}
CCI type 0
input
CCI type 1
input
CCI type 2
input
CCI type 8
input
26
74
022
078
080
696
50
028
419
23
38
239
323
13 nodes
Collocation Dictionary
</figure>
<figureCaption confidence="0.99987">
Figure 6. The Proposed WSD Algorithm
</figureCaption>
<bodyText confidence="0.996311235294118">
Translation Units) dictionary, which contains
idioms, compound words, collocations, etc. If a
collocation of the target word exists in the
MWTU dictionary, we simply determine the
sense of the target word to the sense found in the
dictionary. This method is based on the idea of
‘one sense per collocation’. Next, we verify the
selectional restriction of the verb described in
the dictionary. If we cannot find any matched
patterns for selectional restrictions, we apply the
neural network approach. WSD in the neural
network stage is performed in the following 3
steps.
Step 1. Extract CCI from the context of the
target word. The window size of the context is a
single sentence. Consider, for example, the
sentence in Figure 7 which has the meaning of
“Seeing her eyes filled with tears, ...”. The
target word is the homograph ‘nwun’. We
extract its CCI from the sentence by partial
parsing and pattern scanning. In Figure 7, the
words ‘nwun’ and ‘kunye(her)’ with the concept
code 503 have the relation of &lt;noun + uy +
noun&gt;, which corresponds to ‘CCI type 2’ in
Table 1. There is no syntactic relation between
the words ‘nwun’ and ‘nwunmul(tears)’ with the
concept code 078, so we assign ‘CCI type 0’ to
the concept code 078.
Similarly, we can obtain all pairs of CCI types
and their concept codes appearing in the context.
All the extracted &lt;CCI-type: concept codes&gt;
pairs are as follows: {&lt;type 0: 078,274&gt;, &lt;type
2: 503&gt;, &lt;type 8: 331&gt;}.
Step 2. Obtain the input pattern for the
</bodyText>
<figure confidence="0.960039833333333">
nwunmwul-i katuk-han kunye-uy nwun-ul po-mye
input : *—�01 DLA°—I *—z 5-0] ...
target
concept code : [078] [274] [503] [331]
word
CCI type : (type 0) (type 0) (type 2) (type 8)
</figure>
<figureCaption confidence="0.9962195">
Figure 7. Construction of Input Pattern by Using
Concept Similarity Calculation
</figureCaption>
<bodyText confidence="0.9048569375">
network by calculating concept similarities
between the features of the input nodes and the
concept code in the extracted &lt;CCI-type:
concept codes&gt;. Concept similarity calculation
is performed only between the concept codes
with the same CCI-type. The calculated concept
similarity score is assigned to each input node as
the input value to the network.
Csim(Ci, Pj) in Equation 1 is used to calculate
the concept similarity between Ci and Pj, where
MSCA(Ci, Pj) is the most specific common
ancestor of concept codes Ci and Pj, and weight
is a weighting factor reflecting that Ci as a
descendant of Pj is preferable to other cases.
That is, if Ci is a descendant of Pj, we set weight
to 1. Otherwise we set weight to 0.5.
</bodyText>
<equation confidence="0.932983">
2 x level(MSCA(Ci , Pj x
))
(C
Csim (Ci, Pj) = level(Ci) + level(Pj)
</equation>
<bodyText confidence="0.771837">
The similarity values between the target
</bodyText>
<figureCaption confidence="0.9525995">
Figure 8. Concept Similarity on the Kadokawa
Thesaurus Hierarchy
</figureCaption>
<figure confidence="0.998664446153846">
YES
Answer
Select the most frequent sense
YES
Success
NO
Selectional Restrictions of the Verb
YES
Success
NO
Neural Networks
Success
NO
Input Layer
26
74
022
078
080
696
50
028
419
23
38
239
323
Similarity
Calculation
similarity values
(0.285)
(0.000)
(0.250)
(1.000)
(0.000)
(0.000)
(0.857)
(0.000)
(0.000)
(0.000)
(0.285)
(0.000)
(0.250)
{078}
CCI type 0
{274}
{none}
CCI type 1
{503}
CCI type 2
{331}
CCI type 8
CCI type 0
input
CCI type 1
input
CCI type 2
input
CCI type 8
input
Ci
(0.250) (0.250)
TOP
P3
(0.667)
</figure>
<page confidence="0.848689">
P2
(0.857)
P1
(0.375)
P4
(0.285)
</page>
<figure confidence="0.8541636">
P5 P5
(all 0.000)
...
L1
L2
</figure>
<page confidence="0.831377">
L3
L4
</page>
<equation confidence="0.840536">
weight (1)
</equation>
<bodyText confidence="0.9762439375">
concept Ci and each Pj on the Kadokawa
thesaurus hierarchy are shown in Figure 8.
These similarity values are computed using
Equation 1. For example, in ‘CCI-type 0’ part
calculation, the relation between the concept
codes 274 and 26 corresponds to the relation
between Ci and P4 in Figure 8. So we assign the
similarity 0.285 to the input node labeled by 26.
As another example, the concept codes 503 and
50 have a relation between Ci and P2 and we
obtain the similarity 0.857. If more than two
concept codes exist in one CCI-type, such as
&lt;CCI-type 0: 078, 274&gt;, the maximum
similarity value among them is assigned to the
input node, as in Equation 2.
( ) max ( ( i , j ))
</bodyText>
<equation confidence="0.738175">
Ci = Csim C P
Pi
</equation>
<bodyText confidence="0.998707736842105">
In Equation 2, Ci is the concept code of the
input node, and Pj is the concept codes in the
&lt;CCI-type: concept codes&gt; pair which has the
same CCI-type as Ci.
By adopting this concept similarity calculation,
we can achieve a broad coverage of the method.
If we use the exact matching scheme instead of
concept similarity, we may obtain only a few
concept codes matched with the features.
Consequently, sense disambiguation would fail
because of the absence of clues.
Step 3. Feed the obtained input pattern to the
neural network and compute activation strengths
for each output node. Next, select the sense of
the node that has a larger activation value than
all other output node. If the activation strength is
lower than the threshold, it will be discarded and
the network will not make any decisions. This
process is represented in Figure 9.
</bodyText>
<figureCaption confidence="0.987843">
Figure 9. Sense Disambiguation for ‘nwun’
</figureCaption>
<sectionHeader confidence="0.979215" genericHeader="method">
5 Experimental Evaluation
</sectionHeader>
<bodyText confidence="0.9999913">
For an experimental evaluation, 10 ambiguous
Korean nouns were selected, along with a total
of 500 test sentences in which one homograph
appears. In order to follow the ambiguity
distribution described in Section 3.2, we set the
number of test nouns with two senses to 8 (80%).
The test sentences were randomly selected from
the KIBS (Korean Information Base System)
corpus.
The experimental results are shown in Table 3,
where result A is the case when the most
frequent sense was taken as the answer. To
compare it with our approach (result C), we also
performed the experiment using Li’s method
(result B). For sense disambiguation, Li’s
method features which are similar to our method.
However, unlike our method, which combines
all features by using neural networks, Li
considers only one clue at each decision step. As
shown in the table, our approach exceeded Li’s
</bodyText>
<tableCaption confidence="0.999016">
Table 3. Comparison of WSD Results
</tableCaption>
<table confidence="0.97337775">
Word Sense No Precision (%)
(A) (B) (C)
pwuca 33 66 64 72
father &amp; child
rich man 17
kancang liver 37 74 84 74
soy source 13
kasa 39 78 68 82
housework
words of song 11
kwutwu 45 90 70 92
shoes
word of mouth 5
nwun 42 84 80 86
eye
snow 8
yongki 41 82 72 88
container
courage 9
uysa 27 54 80 84
doctor
intention 23
cikwu 27 54 84 92
district
the earth 23
whole body 39
censin one’s past 6 78 84 80
telegraph 5
one’s best 27
cenlyek 13 54 50 72
military strength
electric power 7
past record 3
Average Precision 71.4 73.6 82.2
Ú (A) : Baseline (B) : Li’s method
(C) : Proposed method (using a 2-layer NN)
</table>
<figure confidence="0.992923052631579">
(0.285)
(0.000)
threshold
nwun1 (snow)
nwun2 (eye)
(0.250)
(1.000)
(0.000)
(0.000)
(0.857)
...
(0.000)
(0.000)
(0.000)
(0.285)
(0.000)
(0.250)
InputVal
(2)
</figure>
<tableCaption confidence="0.805377">
Table 4. Average Precision and Coverage
for Each Stage of thePproposed Method
</tableCaption>
<table confidence="0.9895325">
&lt;Case 1 : 2-layer NN&gt;
COL VSR NN MFS
Avg. Prec 100.0% 91.2% 86.3% 56.1%
Avg. Cov 3.6% 6.8% 73.2% 16.4%
&lt;Case 2 : 3-layer NN&gt;
COL VSR NN MFS
Avg. Prec 100.0% 91.2% 87.1% 56.0%
Avg. Cov 3.6% 6.8% 72.5% 17.1%
</table>
<bodyText confidence="0.999947625">
in most of the results except ‘kancang’ and
‘censin’. This result shows that word sense
disambiguation can be improved by combining
several clues together (e.g. neural networks)
rather than using them independently (e.g. Li’s
method).
The performance for each stage of the
proposed method is shown in Table 4. Symbols
COL, VSR, NN and MFS in the table indicate 4
stages of our method in Figure 6, respectively.
In the NN stage, the 3-layer model did not show
a performance superior to the 2-layer model
because of the lack of training samples. Since
the 2-layer model has fewer parameters to be
trained, it is more efficient to generalize for
limited training corpora than the 3-layer model.
</bodyText>
<sectionHeader confidence="0.782072" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.99960075">
To resolve sense ambiguities in
Korean-to-Japanese MT, this paper has proposed
a practical word sense disambiguation method
using neural networks. Unlike most previous
approaches based on neural networks, we reduce
the number of features for the network to a
practical size by using concept codes rather than
lexical words. In an experimental evaluation, the
proposed WSD model using a 2-layer network
achieved an average precision of 82.2% with an
improvement over Li’s method by 8.6%. This
result is very promising for real world MT
systems.
We plan further research to improve precision
and to expand our method for verb homograph
disambiguation.
</bodyText>
<sectionHeader confidence="0.991507" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9983845">
This work was supported by the Korea Science
and Engineering Foundation (KOSEF) through
the Advanced Information Technology Research
Center(AITrc).
</bodyText>
<sectionHeader confidence="0.984009" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999154405405405">
Gallant S. (1991) A Practical Approach for
Representing Context and for Performing Word
Sense Disambiguation Using Neural Networks.
Neural Computation, 3/3, pp. 293-309
Leacock C., Twell G. and Voorhees E. (1993)
Corpus-based Statistical Sense Resolution. In
Proceedings of the ARPA Human Language
Technology Workshop, San Francisco, Morgan
Kaufman, pp. 260-265
Li H. F., Heo N. W., Moon K. H., Lee J. H. and Lee
G. B. (2000) Lexical Transfer Ambiguity
Resolution Using Automatically-Extracted Concept
Co-occurrence Information. International Journal
of Computer Processing of Oriental Languages,
13/1, pp. 53-68
McRoy S. (1992) Using Multiple Knowledge Sources
for Word Sense Discrimination. Computational
Linguistics, 18/1, pp. 1-30
Mooney R. (1996) Comparative Experiments on
Disambiguating Word Senses: An Illustration of
the Role of Bias in Machine Learning. In
Proceedings of the Conference on Empirical
Methods in Natural Language Processing,
Philadelphia, PA, pp. 82-91
Ng, H. T. and Zelle J. (1997) Corpus-Based
Approaches to Semantic Interpretation in Natural
Language Processing. AI Magazine, 18/4, pp.
45-64
Ohno S. and Hamanishi M. (1981) New Synonym
Dictionary. Kadokawa Shoten, Tokyo
Smadja F. (1993) Retrieving Collocations from Text:
Xtract. Computational Linguistics, 19/1, pp.
143-177
Waltz D. L. and Pollack J. (1985) Massively Parallel
Parsing: A Strongly Interactive Model of Natural
Language Interpretation. Cognitive Science, 9, pp.
51-74
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.448765">
<title confidence="0.9977645">Word Sense Disambiguation in a MT System Using Neural Networks</title>
<author confidence="0.929998">You-Jin Chung</author>
<author confidence="0.929998">Sin-Jae Kang</author>
<author confidence="0.929998">Kyong-Hi Moon</author>
<author confidence="0.929998">Jong-Hyeok</author>
<affiliation confidence="0.827678333333333">Div. of Electrical and Computer Engineering, Pohang University of Science and Technology and Advanced Information Technology Research San 31, Hyoja-dong, Nam-gu, Pohang, R. of KOREA,</affiliation>
<email confidence="0.967881">prizer@postech.ac.kr</email>
<email confidence="0.967881">sjkang@postech.ac.kr</email>
<email confidence="0.967881">khmoon@postech.ac.kr</email>
<email confidence="0.967881">jhlee@postech.ac.kr</email>
<abstract confidence="0.998910857142857">This paper presents a method to resolve word sense ambiguity in a Korean-to-Japanese machine translation system using neural networks. The execution of our neural network model is based on the concept codes of a thesaurus. Most previous word sense disambiguation approaches based on neural networks have limitations due to their huge feature set size. By contrast, we reduce the number of features of the network to a practical size by using concept codes as features rather than the lexical words themselves.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Gallant</author>
</authors>
<title>A Practical Approach for Representing Context and for Performing Word Sense Disambiguation Using Neural Networks.</title>
<date>1991</date>
<journal>Neural Computation,</journal>
<volume>3</volume>
<pages>293--309</pages>
<contexts>
<context position="1881" citStr="Gallant, 1991" startWordPosition="268" endWordPosition="269">ord sense disambiguation has revealed that several different types of information can contribute to the resolution of lexical ambiguity. These include surrounding words (an unordered set of words surrounding a target word), local collocations (a short sequence of words near a target word, taking word order into account), syntactic relations (selectional restrictions), parts of speech, morphological forms, etc (McRoy, 1992, Ng and Zelle, 1997). Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al., 1985, Gallant, 1991, Leacock et al., 1993, and Mooney, 1996). Since, however, most such methods require a few thousands of features or large amounts of hand-written data for training, it is not clear that the same neural network models will be applicable to real world applications. We propose a word sense disambiguation method that combines both the neural net-based approach and the work of Li et al (2000), especially focusing on the practicality of the method for application to real world MT systems. To reduce the number of input features of neural networks to a practical size, we use concept codes of a thesaur</context>
</contexts>
<marker>Gallant, 1991</marker>
<rawString>Gallant S. (1991) A Practical Approach for Representing Context and for Performing Word Sense Disambiguation Using Neural Networks. Neural Computation, 3/3, pp. 293-309</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>G Twell</author>
<author>E Voorhees</author>
</authors>
<title>Corpus-based Statistical Sense Resolution.</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop,</booktitle>
<pages>260--265</pages>
<location>San Francisco, Morgan Kaufman,</location>
<contexts>
<context position="1903" citStr="Leacock et al., 1993" startWordPosition="270" endWordPosition="273">biguation has revealed that several different types of information can contribute to the resolution of lexical ambiguity. These include surrounding words (an unordered set of words surrounding a target word), local collocations (a short sequence of words near a target word, taking word order into account), syntactic relations (selectional restrictions), parts of speech, morphological forms, etc (McRoy, 1992, Ng and Zelle, 1997). Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al., 1985, Gallant, 1991, Leacock et al., 1993, and Mooney, 1996). Since, however, most such methods require a few thousands of features or large amounts of hand-written data for training, it is not clear that the same neural network models will be applicable to real world applications. We propose a word sense disambiguation method that combines both the neural net-based approach and the work of Li et al (2000), especially focusing on the practicality of the method for application to real world MT systems. To reduce the number of input features of neural networks to a practical size, we use concept codes of a thesaurus as features. In thi</context>
</contexts>
<marker>Leacock, Twell, Voorhees, 1993</marker>
<rawString>Leacock C., Twell G. and Voorhees E. (1993) Corpus-based Statistical Sense Resolution. In Proceedings of the ARPA Human Language Technology Workshop, San Francisco, Morgan Kaufman, pp. 260-265</rawString>
</citation>
<citation valid="true">
<authors>
<author>H F Li</author>
<author>N W Heo</author>
<author>K H Moon</author>
<author>J H Lee</author>
<author>G B Lee</author>
</authors>
<title>Lexical Transfer Ambiguity Resolution Using Automatically-Extracted Concept Co-occurrence Information.</title>
<date>2000</date>
<journal>International Journal of Computer Processing of Oriental Languages,</journal>
<volume>13</volume>
<pages>53--68</pages>
<contexts>
<context position="2271" citStr="Li et al (2000)" startWordPosition="331" endWordPosition="334">morphological forms, etc (McRoy, 1992, Ng and Zelle, 1997). Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al., 1985, Gallant, 1991, Leacock et al., 1993, and Mooney, 1996). Since, however, most such methods require a few thousands of features or large amounts of hand-written data for training, it is not clear that the same neural network models will be applicable to real world applications. We propose a word sense disambiguation method that combines both the neural net-based approach and the work of Li et al (2000), especially focusing on the practicality of the method for application to real world MT systems. To reduce the number of input features of neural networks to a practical size, we use concept codes of a thesaurus as features. In this paper, Yale Romanization is used to represent Korean expressions. 1 System Architecture Our neural network method consists of two phases. The first phase is the construction of the feature set for the neural network; the second phase is the construction and training of the neural network. (see Figure 1.) For practical reasons, a reasonably small number of features</context>
<context position="6128" citStr="Li et al. (2000)" startWordPosition="926" endWordPosition="929">202 and it took only 0.69% of the overall corpus (202 / 29,420 eojeols). Considering the fact that the overall accuracy of the constructed corpus exceeds 97% and only a few sense ambiguity resolution errors were found in the Japanese-to-Korean translation of nouns, we regard the generated sense-tagged corpus as highly reliable. 2.2 Extraction of Raw CCI Unlike English, Korean has almost no syntactic constraints on word order as long as the verb appears in the final position. The variable word order often results in discontinuous constituents. Instead of using local collocations by word order, Li et al. (2000) defined 13 patterns of CCI for homographs using syntactically related words in a sentence. Because we are concerned only with 2 An Eojeol is a Korean syntactic unit consisting of a content word and one or more function words. Stored in MT Dictionary Japanese Corpus COBALT-J/K (Japanese-to-Korean MT system) Sense Tagged Korean Corpus Partial Parsing &amp; Pattern Scanning Raw CCI Feature Set Network Construction Neural Network Network Learning Network Parameters CCI Refinement Processing Refined CCI L1 noun ••••• nature character society institute things 0 1 7 8 9 ••••• L2 astro- calen- animal phe</context>
</contexts>
<marker>Li, Heo, Moon, Lee, Lee, 2000</marker>
<rawString>Li H. F., Heo N. W., Moon K. H., Lee J. H. and Lee G. B. (2000) Lexical Transfer Ambiguity Resolution Using Automatically-Extracted Concept Co-occurrence Information. International Journal of Computer Processing of Oriental Languages, 13/1, pp. 53-68</rawString>
</citation>
<citation valid="true">
<authors>
<author>S McRoy</author>
</authors>
<title>Using Multiple Knowledge Sources for Word Sense Discrimination.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<pages>1--30</pages>
<contexts>
<context position="1693" citStr="McRoy, 1992" startWordPosition="240" endWordPosition="241">valent depending on which sense is used in a given context. Thus, word sense disambiguation (WSD) is essential to the selection of an appropriate Japanese target word. Much research on word sense disambiguation has revealed that several different types of information can contribute to the resolution of lexical ambiguity. These include surrounding words (an unordered set of words surrounding a target word), local collocations (a short sequence of words near a target word, taking word order into account), syntactic relations (selectional restrictions), parts of speech, morphological forms, etc (McRoy, 1992, Ng and Zelle, 1997). Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al., 1985, Gallant, 1991, Leacock et al., 1993, and Mooney, 1996). Since, however, most such methods require a few thousands of features or large amounts of hand-written data for training, it is not clear that the same neural network models will be applicable to real world applications. We propose a word sense disambiguation method that combines both the neural net-based approach and the work of Li et al (2000), especially focusing </context>
</contexts>
<marker>McRoy, 1992</marker>
<rawString>McRoy S. (1992) Using Multiple Knowledge Sources for Word Sense Discrimination. Computational Linguistics, 18/1, pp. 1-30</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mooney</author>
</authors>
<title>Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>82--91</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="1922" citStr="Mooney, 1996" startWordPosition="275" endWordPosition="276">t several different types of information can contribute to the resolution of lexical ambiguity. These include surrounding words (an unordered set of words surrounding a target word), local collocations (a short sequence of words near a target word, taking word order into account), syntactic relations (selectional restrictions), parts of speech, morphological forms, etc (McRoy, 1992, Ng and Zelle, 1997). Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al., 1985, Gallant, 1991, Leacock et al., 1993, and Mooney, 1996). Since, however, most such methods require a few thousands of features or large amounts of hand-written data for training, it is not clear that the same neural network models will be applicable to real world applications. We propose a word sense disambiguation method that combines both the neural net-based approach and the work of Li et al (2000), especially focusing on the practicality of the method for application to real world MT systems. To reduce the number of input features of neural networks to a practical size, we use concept codes of a thesaurus as features. In this paper, Yale Roman</context>
</contexts>
<marker>Mooney, 1996</marker>
<rawString>Mooney R. (1996) Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Philadelphia, PA, pp. 82-91</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>J Zelle</author>
</authors>
<title>Corpus-Based Approaches to Semantic Interpretation in Natural Language Processing.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<pages>45--64</pages>
<contexts>
<context position="1714" citStr="Ng and Zelle, 1997" startWordPosition="242" endWordPosition="245">ing on which sense is used in a given context. Thus, word sense disambiguation (WSD) is essential to the selection of an appropriate Japanese target word. Much research on word sense disambiguation has revealed that several different types of information can contribute to the resolution of lexical ambiguity. These include surrounding words (an unordered set of words surrounding a target word), local collocations (a short sequence of words near a target word, taking word order into account), syntactic relations (selectional restrictions), parts of speech, morphological forms, etc (McRoy, 1992, Ng and Zelle, 1997). Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al., 1985, Gallant, 1991, Leacock et al., 1993, and Mooney, 1996). Since, however, most such methods require a few thousands of features or large amounts of hand-written data for training, it is not clear that the same neural network models will be applicable to real world applications. We propose a word sense disambiguation method that combines both the neural net-based approach and the work of Li et al (2000), especially focusing on the practicality o</context>
</contexts>
<marker>Ng, Zelle, 1997</marker>
<rawString>Ng, H. T. and Zelle J. (1997) Corpus-Based Approaches to Semantic Interpretation in Natural Language Processing. AI Magazine, 18/4, pp. 45-64</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ohno</author>
<author>M Hamanishi</author>
</authors>
<title>New Synonym Dictionary. Kadokawa Shoten,</title>
<date>1981</date>
<location>Tokyo</location>
<contexts>
<context position="4168" citStr="Ohno and Hamanishi, 1981" startWordPosition="624" endWordPosition="627">struction Figure 1. System Architecture to the extracted raw CCI. After completing refinement processing, we use the remaining CCI as features for the neural network. The trained network parameters are stored in a Korean-to-Japanese MT dictionary for WSD in translation. 2 Construction of Refined Feature Set 2.1 Automatic Construction of Sense-tagged Corpus For automatic construction of the sense-tagged corpus, we used a Japanese-to-Korean MT system called COBALT-J/K1. In the transfer dictionary of COBALT-J/K, nominal and verbal words are annotated with concept codes of the Kadokawa thesaurus (Ohno and Hamanishi, 1981), which has a 4-level hierarchy of about 1,100 semantic classes, as shown in Figure 2. Concept nodes in level L1, L2 and L3 are further divided into 10 subclasses. We made a slight modification of COBALT-J/K to enable it to produce Korean translations from a Japanese text, with all nominal words tagged with specific concept codes at level L4 of the Kadokawa thesaurus. As a result, a Korean sense-tagged corpus of 1,060,000 sentences can be obtained from the Japanese corpus (Asahi Shinbun, Japanese Newspaper of Economics, etc.). 1 COBALT-J/K (Collocation-Based Language Translator from Japanese t</context>
</contexts>
<marker>Ohno, Hamanishi, 1981</marker>
<rawString>Ohno S. and Hamanishi M. (1981) New Synonym Dictionary. Kadokawa Shoten, Tokyo</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
</authors>
<title>Retrieving Collocations from Text: Xtract.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<pages>143--177</pages>
<contexts>
<context position="9858" citStr="Smadja, 1993" startWordPosition="1593" endWordPosition="1594">n the table means the set of concept codes with the frequencies less than 4. Shannon’s entropy (Shannon, 1951). A concept code with a small entropy has a large discrimination value. If the discrimination value of the concept code is larger than a threshold, the concept code is selected as useful information for deciding the word sense. Otherwise, the concept code is discarded. 2.3.2 Concept Code Generalization After concept discrimination, co-occurring concept codes in each CCI type must be further selected and the code generalized. To perform code generalization, Li adopted to Smadja’s work (Smadja, 1993) and defined the code strength using a code frequency and a standard deviation in each level of the concept hierarchy. The generalization filter selects the concept codes with a strength larger than a threshold. We perform this generalizaion processing on the Kadokawa thesaurus level L4 and L3. After processing, the system stores the refined conceptual patterns ({C1, C2, C3, ...}, typei, W(Si)) as a knowledge source for WSD of real texts. These refined CCI are used as input features for the neural network. The more specific description of the CCI extraction is explained in Li (2000). 3 Constru</context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>Smadja F. (1993) Retrieving Collocations from Text: Xtract. Computational Linguistics, 19/1, pp. 143-177</rawString>
</citation>
<citation valid="true">
<authors>
<author>D L Waltz</author>
<author>J Pollack</author>
</authors>
<title>Massively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation.</title>
<date>1985</date>
<journal>Cognitive Science,</journal>
<volume>9</volume>
<pages>51--74</pages>
<marker>Waltz, Pollack, 1985</marker>
<rawString>Waltz D. L. and Pollack J. (1985) Massively Parallel Parsing: A Strongly Interactive Model of Natural Language Interpretation. Cognitive Science, 9, pp. 51-74</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>