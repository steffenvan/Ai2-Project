<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002137">
<note confidence="0.5906265">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
</note>
<figure confidence="0.981881672413793">
Association for Computational Linguistics
Feature vector
Charniak’s
parser
Argument Boundary Detection (ABD) module
FrameNet test data
FrameNet
training
data
Rule−based
Argument
Filtering
Argument Boundary
Detection
(6 target classes)
sem.names
sem.data
SVM
Train
SVM
Predict
ABD − class
mapping
SVM
Predict
Charniak’s
parser
Feature vector
Semantic Roles
Classifier
SVM
Train
FrameNet
training
data
Argument Classification module
Feature vector
Results
FrameNet
training
data
SVM
Predict
Parse
Trees
Correction
Charniak’s
parser
sem.data
Semantic Roles
Classifier
sem.names
SVM
Train
Argument Classification module
FrameNet test data
No. Feature Definition
BASELINE FEATURES
</figure>
<figureCaption confidence="0.9227666">
Fl Phrase type (G&amp;J) indicates the syntactic category of the argument
to be tagged with the semantic role
F2 Governing category (G&amp;J) is provided only for noun phrases and indicates
the syntactic category of the phrase that governs
the grammatical realization of the NP as the subject
(eg, S), or the object (eg, VP) of the predicate
F3 Parse tree path (G&amp;J) the path in the parse tree from the constituent
argument to the predicate
F4 Position (G&amp;J) indicates whether the argument appears before
or after the predicate
</figureCaption>
<table confidence="0.989289375">
F5 Voice (G&amp;J) indicates if the verb is in active or passive voice in
the sentence.
F6 Head word (G&amp;J), indicates the syntactic head of the phrase;
(Surdeanu et al.) in our implementation, it coincides with the content word
of (Surdeanu et al.) when applied to prepositional phrases
and subordinate clauses;
F7 Lemma (G&amp;J), represents the lemma of the target word
F8 Predicate (Surdeanu et al.) represents the surface form of the target word
MODIFIED FEATURES
F9 (F2&apos;) Governing category we extended this feature to prepositional phrases
(PP-prep.) and subordinate clauses (WH-, SBAR)
F10 Grammatical rule the grammatical rule expanding the common
ancestor of argument and verb; includes as special case
the subcategorization feature of (Pradhan et al. 2003)
NEW FEATURES
Fll Predicate we added a third component: (C3) verb&apos;s POS
F12 Argument structure the grammatical rules expanding all the nodes
along the path to the head word
F13 Frame FrameNet frame of the verb
F14 Distance number of tokens between the argument and target
F15 PropBank semantic argument the PropBank semantic type of the argument
F16 Diathesis alternation same structure as grammatical rule except it uses
the Propbank semantic argument feature
instead of phrase type
</table>
<tableCaption confidence="0.99961">
Table 1: The three sets of features used for the automatic semantic role classification.
</tableCaption>
<table confidence="0.99841">
Runs Results
Nr. correct Nr. attempted Nr. total P 0 R A
Non-restricted 13,660 15,208 16,279 0.898 0.897 0.839 93.4%
Restricted 12,697 15,735 16,279 0.807 0.777 0.780 96.7%
</table>
<tableCaption confidence="0.9785655">
Table 2: Table of results for both runs. &amp;quot;P&amp;quot; is precision, &amp;quot;0&amp;quot; is overlap, &amp;quot;R&amp;quot; is recall, and &amp;quot;A&amp;quot; is
attempted.
</tableCaption>
<bodyText confidence="0.883863666666667">
package LIBSVM, http://www.csie.ntu.edu.tw/
cjlin/libsvm,/ which implements the SVM algo-
rithm described above.
</bodyText>
<sectionHeader confidence="0.897776" genericHeader="abstract">
3 Experimental Setting and Results
</sectionHeader>
<subsectionHeader confidence="0.619759">
3.1 Feature vectors
</subsectionHeader>
<bodyText confidence="0.999458384615385">
The data for SVM had to be transformed into a
sparse vector format. The method used was to
give each feature a number of slots in the vec-
tor equal to the number of its values, and put
value 1 on the position that corresponds to the
actual value of the feature for a data example
and 0 for the rest. The range of values for each
feature are listed in parentheses: phrase type
(20), voice (2), headWord (16712) governing-
Category (786) path (2091), position (2), lemma
(1908), predicate (5235), posVerb (8), argu-
mentStructure (1510), alternation (243), gram-
maticalFunction (8), and frame (266).
</bodyText>
<figure confidence="0.994054666666667">
CC
and
NP-SB]
NNS 4j
bidders
Head
Word
ARGO
PP
for parts of
B.A.T&apos;s U.S. retail -
VP
NP
May Department
Stores Co.
ARG2,
NP
Limited Inc
NP
33
potential
VBP NP
include
NP
Dillard Department
Stores Inc.
NP-SB] —&gt;{NP } PP —&gt; JJ JJ NNS
</figure>
<figureCaption confidence="0.999688">
Figure 3: Example of Argument structure feature.
</figureCaption>
<subsectionHeader confidence="0.4422">
3.2 Model selection, training and
</subsectionHeader>
<bodyText confidence="0.9460636">
testing
We used the RBF kernel e_&amp;quot;HXi_XiH for our ex-
periments. There is need to find the best values
for -y and C, the cost coefficient of the opti-
mization problem(min .sNr7-&apos;w+C E!_i . This
model selection is done by a grid search proce-
dure. The estimation of -y and C takes from 10
to 20 hours on a single 3Gz Pentium 4 machine,
while the final training on all data takes only a
few hours.
</bodyText>
<sectionHeader confidence="0.567677" genericHeader="acknowledgments">
3.3 Results
</sectionHeader>
<bodyText confidence="0.8972656">
A summary of the results is shown in Table 2.
4 Acknowledgment
We acknowledge the contribution of A.M. Giu-
glea who worked on the feature extraction soft-
ware.
</bodyText>
<sectionHeader confidence="0.998974" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999261444444444">
C. Baker, C. Fillmore, and J. Lowe. The
Berkeley FrameNet Project. In Proceedings
of COLLING/ACL, Canada, 1998.
Daniel Gildea and Daniel Jurafsky. 2002. Au-
tomatic Labeling of Semantic Roles. In Com-
putational Linguistics, 28(3), 2002.
P. Kingsbury, M. Palmer, and M. Marcus.
Adding Semantic Annotation to the Penn
TreeBank. In Proceedings of the Human Lan-
guage Technology Conference (HLT 2002),
California, 2002.
S. Pradhan, K. Hacioglu, V. Krugler, W. Ward,
J. Martin, and D. Jurafsky. 2003. Semantic
role parsing: Adding semantic structure to
unstructured text. In ICDM, Florida.
M. Surdeanu et. al. 2003. Proceedings of
the Association for Computational Linguis-
tics (ACL-2003), Sapporo, Japan, 2003.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000005">
<note confidence="0.784937">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004</note>
<title confidence="0.579524333333333">Association for Computational Linguistics Feature vector Charniak’s</title>
<email confidence="0.367083">parser</email>
<abstract confidence="0.86354143902439">Argument Boundary Detection (ABD) module FrameNet test data FrameNet training data Rule−based Argument Filtering Argument (6 target classes) sem.names sem.data SVM Train SVM Predict ABD − class mapping SVM Predict Charniak’s parser Feature vector Semantic Roles Classifier SVM Train FrameNet training data Argument Classification module Feature vector Results FrameNet training data SVM Predict Correction Charniak’s parser sem.data</abstract>
<title confidence="0.815086222222222">Semantic Roles Classifier sem.names SVM Train Argument Classification module FrameNet test data No. Feature Definition BASELINE FEATURES</title>
<abstract confidence="0.971311047619048">Fl Phrase type (G&amp;J) indicates the syntactic category of the argument to be tagged with the semantic role F2 Governing category (G&amp;J) is provided only for noun phrases and indicates the syntactic category of the phrase that governs the grammatical realization of the NP as the subject (eg, S), or the object (eg, VP) of the predicate F3 Parse tree path (G&amp;J) the path in the parse tree from the constituent argument to the predicate F4 Position (G&amp;J) indicates whether the argument appears before or after the predicate F5 Voice (G&amp;J) indicates if the verb is in active or passive voice in the sentence. F6 Head word (G&amp;J), (Surdeanu et al.) indicates the syntactic head of the phrase; in our implementation, it coincides with the content word of (Surdeanu et al.) when applied to prepositional phrases and subordinate clauses; F7 Lemma (G&amp;J), represents the lemma of the target word F8 Predicate (Surdeanu et al.) represents the surface form of the target word MODIFIED FEATURES F9 (F2&apos;) Governing category extended this feature to phrases and clauses SBAR) F10 Grammatical rule the grammatical rule expanding the common ancestor of argument and verb; includes as special case of (Pradhan et al. 2003) NEW FEATURES Fll Predicate added a third component: (C3) POS F12 Argument structure the grammatical rules expanding all the nodes along the path to the head word F13 Frame FrameNet frame of the verb F14 Distance number of tokens between the argument and target F15 PropBank semantic argument the PropBank semantic type of the argument F16 Diathesis alternation same structure as grammatical rule except it uses the Propbank semantic argument feature instead of phrase type Table 1: The three sets of features used for the automatic semantic role classification.</abstract>
<note confidence="0.9684116">Runs Results Nr. correct Nr. attempted Nr. total P 0 R A Non-restricted 13,660 15,208 16,279 0.898 0.897 0.839 93.4% Restricted 12,697 15,735 16,279 0.807 0.777 0.780 96.7% Table 2: Table of results for both runs. &amp;quot;P&amp;quot; is precision, &amp;quot;0&amp;quot; is overlap, &amp;quot;R&amp;quot; is recall, and &amp;quot;A&amp;quot; is</note>
<abstract confidence="0.992747344827586">attempted. LIBSVM, implements the SVM algorithm described above. 3 Experimental Setting and Results 3.1 Feature vectors The data for SVM had to be transformed into a sparse vector format. The method used was to give each feature a number of slots in the vector equal to the number of its values, and put value 1 on the position that corresponds to the actual value of the feature for a data example and 0 for the rest. The range of values for each feature are listed in parentheses: phrase type (20), voice (2), headWord (16712) governing- Category (786) path (2091), position (2), lemma (1908), predicate (5235), posVerb (8), argumentStructure (1510), alternation (243), grammaticalFunction (8), and frame (266). CC and NP-SB] NNS 4j bidders Word ARGO PP for parts of B.A.T&apos;s U.S. retail -</abstract>
<title confidence="0.4867315">VP NP</title>
<author confidence="0.792174">May Department</author>
<note confidence="0.483636166666667">Stores Co. ARG2, NP Limited Inc NP potential</note>
<title confidence="0.8133545">include NP NP</title>
<author confidence="0.661673">Dillard</author>
<affiliation confidence="0.7858685">Stores Inc. NP-SB] —&gt;{NP } PP —&gt; JJ JJ NNS</affiliation>
<abstract confidence="0.994468">Figure 3: Example of Argument structure feature. 3.2 Model selection, training and testing used the RBF kernel e_&amp;quot;HXi_XiHfor our experiments. There is need to find the best values -y and cost coefficient of the optiproblem(min . This model selection is done by a grid search proce- The estimation of -y and from 10 to 20 hours on a single 3Gz Pentium 4 machine, while the final training on all data takes only a few hours. 3.3 Results A summary of the results is shown in Table 2. 4 Acknowledgment We acknowledge the contribution of A.M. Giuglea who worked on the feature extraction software.</abstract>
<title confidence="0.650339">References</title>
<author confidence="0.6370475">The FrameNet Project In</author>
<note confidence="0.61053775">COLLING/ACL, 1998. Gildea and Daniel Jurafsky. 2002. Au- Labeling of Semantic Roles. Com- Linguistics, 2002. P. Kingsbury, M. Palmer, and M. Marcus. Adding Semantic Annotation to the Penn In of the Human Lan- Technology Conference 2002), California, 2002. S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. Martin, and D. Jurafsky. 2003. Semantic role parsing: Adding semantic structure to text. In M. Surdeanu et. al. 2003. Proceedings of for Computational Linguis- (ACL-2003), Japan, 2003.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Baker</author>
<author>C Fillmore</author>
<author>J Lowe</author>
</authors>
<title>The Berkeley FrameNet Project.</title>
<date>1998</date>
<booktitle>In Proceedings of COLLING/ACL, Canada,</booktitle>
<marker>Baker, Fillmore, Lowe, 1998</marker>
<rawString>C. Baker, C. Fillmore, and J. Lowe. The Berkeley FrameNet Project. In Proceedings of COLLING/ACL, Canada, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic Labeling of Semantic Roles.</title>
<date>2002</date>
<journal>In Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling of Semantic Roles. In Computational Linguistics, 28(3), 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kingsbury</author>
<author>M Palmer</author>
<author>M Marcus</author>
</authors>
<title>Adding Semantic Annotation to the Penn TreeBank.</title>
<date>2002</date>
<booktitle>In Proceedings of the Human Language Technology Conference (HLT</booktitle>
<location>California,</location>
<marker>Kingsbury, Palmer, Marcus, 2002</marker>
<rawString>P. Kingsbury, M. Palmer, and M. Marcus. Adding Semantic Annotation to the Penn TreeBank. In Proceedings of the Human Language Technology Conference (HLT 2002), California, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pradhan</author>
<author>K Hacioglu</author>
<author>V Krugler</author>
<author>W Ward</author>
<author>J Martin</author>
<author>D Jurafsky</author>
</authors>
<title>Semantic role parsing: Adding semantic structure to unstructured text.</title>
<date>2003</date>
<booktitle>In ICDM,</booktitle>
<location>Florida.</location>
<contexts>
<context position="2087" citStr="Pradhan et al. 2003" startWordPosition="308" endWordPosition="311">e phrase; (Surdeanu et al.) in our implementation, it coincides with the content word of (Surdeanu et al.) when applied to prepositional phrases and subordinate clauses; F7 Lemma (G&amp;J), represents the lemma of the target word F8 Predicate (Surdeanu et al.) represents the surface form of the target word MODIFIED FEATURES F9 (F2&apos;) Governing category we extended this feature to prepositional phrases (PP-prep.) and subordinate clauses (WH-, SBAR) F10 Grammatical rule the grammatical rule expanding the common ancestor of argument and verb; includes as special case the subcategorization feature of (Pradhan et al. 2003) NEW FEATURES Fll Predicate we added a third component: (C3) verb&apos;s POS F12 Argument structure the grammatical rules expanding all the nodes along the path to the head word F13 Frame FrameNet frame of the verb F14 Distance number of tokens between the argument and target F15 PropBank semantic argument the PropBank semantic type of the argument F16 Diathesis alternation same structure as grammatical rule except it uses the Propbank semantic argument feature instead of phrase type Table 1: The three sets of features used for the automatic semantic role classification. Runs Results Nr. correct Nr</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2003</marker>
<rawString>S. Pradhan, K. Hacioglu, V. Krugler, W. Ward, J. Martin, and D. Jurafsky. 2003. Semantic role parsing: Adding semantic structure to unstructured text. In ICDM, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Surdeanu</author>
</authors>
<date>2003</date>
<booktitle>Proceedings of the Association for Computational Linguistics (ACL-2003),</booktitle>
<location>Sapporo, Japan,</location>
<marker>Surdeanu, 2003</marker>
<rawString>M. Surdeanu et. al. 2003. Proceedings of the Association for Computational Linguistics (ACL-2003), Sapporo, Japan, 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>