<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008210">
<title confidence="0.990257">
Multi-level Translation Quality Prediction with QUEST++
</title>
<author confidence="0.996837">
Lucia Specia, Gustavo Henrique Paetzold and Carolina Scarton
</author>
<affiliation confidence="0.984836">
Department of Computer Science
University of Sheffield, UK
</affiliation>
<email confidence="0.993151">
{l.specia,ghpaetzold1,c.scarton}@sheffield.ac.uk
</email>
<sectionHeader confidence="0.994676" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999918428571429">
This paper presents QUEST++ , an open
source tool for quality estimation which
can predict quality for texts at word, sen-
tence and document level. It also provides
pipelined processing, whereby predictions
made at a lower level (e.g. for words) can
be used as input to build models for pre-
dictions at a higher level (e.g. sentences).
QUEST++ allows the extraction of a va-
riety of features, and provides machine
learning algorithms to build and test qual-
ity estimation models. Results on recent
datasets show that QUEST++ achieves
state-of-the-art performance.
</bodyText>
<sectionHeader confidence="0.998779" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999676754098361">
Quality Estimation (QE) of Machine Translation
(MT) have become increasingly popular over the
last decade. With the goal of providing a predic-
tion on the quality of a machine translated text, QE
systems have the potential to make MT more use-
ful in a number of scenarios, for example, improv-
ing post-editing efficiency (Specia, 2011), select-
ing high quality segments (Soricut and Echihabi,
2010), selecting the best translation (Shah and
Specia, 2014), and highlighting words or phrases
that need revision (Bach et al., 2011).
Most recent work focuses on sentence-level QE.
This variant is addressed as a supervised machine
learning task using a variety of algorithms to in-
duce models from examples of sentence transla-
tions annotated with quality labels (e.g. 1-5 likert
scores). Sentence-level QE has been covered in
shared tasks organised by the Workshop on Statis-
tical Machine Translation (WMT) annually since
2012. While standard algorithms can be used to
build prediction models, key to this task is work
of feature engineering. Two open source feature
extraction toolkits are available for that: ASIYA1
and QUEST2 (Specia et al., 2013). The latter has
been used as the official baseline for the WMT
shared tasks and extended by a number of partic-
ipants, leading to improved results over the years
(Callison-Burch et al., 2012; Bojar et al., 2013;
Bojar et al., 2014).
QE at other textual levels have received much
less attention. Word-level QE (Blatz et al., 2004;
Luong et al., 2014) is seemingly a more challeng-
ing task where a quality label is to be produced
for each target word. An additional challenge is
the acquisition of sizable training sets. Although
significant efforts have been made, there is con-
siderable room for improvement. In fact, most
WMT13-14 QE shared task submissions were un-
able to beat a trivial baseline.
Document-level QE consists in predicting a sin-
gle label for entire documents, be it an absolute
score (Scarton and Specia, 2014) or a relative
ranking of translations by one or more MT sys-
tems (Soricut and Echihabi, 2010). While certain
sentences are perfect in isolation, their combina-
tion in context may lead to an incoherent docu-
ment. Conversely, while a sentence can be poor in
isolation, when put in context, it may benefit from
information in surrounding sentences, leading to
a good quality document. Feature engineering is
a challenge given the little availability of tools to
extract discourse-wide information. In addition,
no datasets with human-created labels are avail-
able and thus scores produced by automatic met-
rics have to be used as approximation (Scarton et
al., 2015).
Some applications require fine-grained, word-
level information on quality. For example, one
may want to highlight words that need fixing.
Document-level QE is needed particularly for gist-
ing purposes where post-editing is not an option.
</bodyText>
<footnote confidence="0.9999765">
1http://nlp.lsi.upc.edu/asiya/
2http://www.quest.dcs.shef.ac.uk/
</footnote>
<page confidence="0.949408">
115
</page>
<note confidence="0.7028555">
Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 115–120,
Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP
</note>
<bodyText confidence="0.999870318181818">
For example, for predictions on translations of
product reviews in order to decide whether or not
they are understandable by readers. We believe
that the limited progress in word and document-
level QE research is partially due to lack of a basic
framework that one can be build upon and extend.
QUEST++ is a significantly refactored and
expanded version of an existing open source
sentence-level toolkit, QUEST. Feature extrac-
tion modules for both word and document-level
QE were added and the three levels of prediction
were unified into a single pipeline, allowing for in-
teractions between word, sentence and document-
level QE. For example, word-level predictions can
be used as features for sentence-level QE. Finally,
sequence-labelling learning algorithms for word-
level QE were added. QUEST++ can be easily ex-
tended with new features at any textual level. The
architecture of the system is described in Section
2. Its main component, the feature extractor, is
presented in Section 3. Section 4 presents experi-
ments using the framework with various datasets.
</bodyText>
<sectionHeader confidence="0.954585" genericHeader="introduction">
2 Architecture
</sectionHeader>
<bodyText confidence="0.999953317073171">
QUEST++ has two main modules: a feature ex-
traction module and a machine learning module.
The first module is implemented in Java and pro-
vides a number of feature extractors, as well as
abstract classes for features, resources and pre-
processing steps so that extractors for new fea-
tures can be easily added. The basic functioning
of the feature extraction module requires raw text
files with the source and translation texts, and a
few resources (where available) such as the MT
source training corpus and source and target lan-
guage models (LMs). Configuration files are used
to indicate paths for resources and the features that
should be extracted. For its main resources (e.g.
LMs), if a resource is missing, QUEST++ can gen-
erate it automatically.
Figure 1 depicts the architecture of QUEST++ .
Document and Paragraph classes are used for
document-level feature extraction. A Document is
a group of Paragraphs, which in turn is a group
of Sentences. Sentence is used for both word- and
sentence-level feature extraction. A Feature Pro-
cessing Module was created for each level. Each
processing level is independent and can deal with
the peculiarities of its type of feature.
Machine learning QUEST++ provides scripts
to interface the Python toolkit scikit-learn3
(Pedregosa et al., ). This module is indepen-
dent from the feature extraction code and uses
the extracted feature sets to build and test QE
models. The module can be configured to run
different regression and classification algorithms,
feature selection methods and grid search for
hyper-parameter optimisation. Algorithms from
scikit-learn can be easily integrated by
modifying existing scripts.
For word-level prediction, QUEST++ provides
an interface for CRFSuite (Okazaki, 2007), a se-
quence labelling C++ library for Conditional Ran-
dom Fields (CRF). One can configure CRFSuite
training settings, produce models and test them.
</bodyText>
<sectionHeader confidence="0.999513" genericHeader="method">
3 Features
</sectionHeader>
<bodyText confidence="0.99974875">
Features in QUEST++ can be extracted from either
source or target (or both) sides of the corpus at a
given textual level. In order describe the features
supported, we denote:
</bodyText>
<listItem confidence="0.999956666666666">
• S and T the source and target documents,
• s and t for source and target sentences, and
• s and t for source and target words.
</listItem>
<bodyText confidence="0.997270733333333">
We concentrate on MT system-independent
(black-box) features, which are extracted based on
the output of the MT system rather than any of
its internal representations. These allow for more
flexible experiments and comparisons across MT
systems. System-dependent features can be ex-
tracted as long they are represented using a pre-
defined XML scheme. Most of the existing fea-
tures are either language-independent or depend
on linguistic resources such as POS taggers. The
latter can be extracted for any language, as long
as the resource is available. For a pipelined ap-
proach, predictions at a given level can become
features for higher level model, e.g. features based
on word-level predictions for sentence-level QE.
</bodyText>
<subsectionHeader confidence="0.999403">
3.1 Word level
</subsectionHeader>
<bodyText confidence="0.999931571428571">
We explore a range of features from recent work
(Bicici and Way, 2014; Camargo de Souza et al.,
2014; Luong et al., 2014; Wisniewski et al., 2014),
totalling 40 features of seven types:
Target context These are features that explore
the context of the target word. Given a word ti
in position i of a target sentence, we extract: ti,
</bodyText>
<footnote confidence="0.955562">
3http://scikit-learn.org/
</footnote>
<page confidence="0.996439">
116
</page>
<figureCaption confidence="0.999978">
Figure 1: Architecture of QUEST++
</figureCaption>
<bodyText confidence="0.99354270967742">
i.e., the word itself, bigrams ti−1ti and titi+1, and
trigrams ti−2ti−1ti, ti−1titi+1 and titi+1ti+2.
Alignment context These features explore the
word alignment between source and target sen-
tences. They require the 1-to-N alignment be-
tween source and target sentences to be provided.
Given a word ti in position i of a target sentence
and a word sj aligned to it in position j of a source
sentence, the features are: the aligned word sj it-
self, target-source bigrams sj−1ti and tisj+1, and
source-target bigrams ti−2sj, ti−1sj, sjti+1 and
sjti+2.
Lexical These features explore POS informa-
tion on the source and target words. Given
the POS tag Pti of word ti in position i of a
target sentence and the POS tag Psj of word
sj aligned to it in position j of a source sen-
tence, we extract: the POS tags Pti and Psj
themselves, the bigrams Pti−1Pti and PtiPti+1
and trigrams Pti−2Pti−1Pti, Pti−1PtiPti+1 and
PtiPti+1Pti+2. Four binary features are also ex-
tracted with value 1 if ti is a stop word, punctua-
tion symbol, proper noun or numeral.
LM These features are related to the n-gram fre-
quencies of a word’s context with respect to an LM
(Raybaud et al., 2011). Six features are extracted:
lexical and syntactic backoff behavior, as well as
lexical and syntactic longest preceding n-gram for
both a target word and an aligned source word.
Given a word ti in position i of a target sentence,
the lexical backoff behavior is calculated as:
</bodyText>
<equation confidence="0.967309">
{ 7 if ti−2, ti−1, ti exists
6 if ti−2, ti−1 and ti−1, ti exist
5 if only ti−1, ti exists
4 if ti−2, ti−1 and ti exist
3 if ti−1 and ti exist
2 if ti exists
1 if ti is out of the vocabulary
</equation>
<bodyText confidence="0.996652111111111">
The syntactic backoff behavior is calculated in
an analogous fashion: it verifies for the existence
of n-grams of POS tags in a POS-tagged LM. The
POS tags of target sentence are produced by the
Stanford Parser4 (integrated in QUEST++ ).
Syntactic QUEST++ provides one syntactic fea-
ture that proved very promising in previous work:
the Null Link (Xiong et al., 2010). It is a binary
feature that receives value 1 if a given word ti in
a target sentence has at least one dependency link
with another word tj, and 0 otherwise. The Stan-
ford Parser is used for dependency parsing.
Semantic These features explore the polysemy
of target and source words, i.e. the number of
senses existing as entries in a WordNet for a given
target word ti or a source word si. We employ
the Universal WordNet,5 which provides access to
WordNets of various languages.
</bodyText>
<footnote confidence="0.994282">
4http://nlp.stanford.edu/software/
lex-parser.shtml
5http://www.lexvo.org/uwn/
</footnote>
<equation confidence="0.709766">
f (ti) =
</equation>
<page confidence="0.977503">
117
</page>
<bodyText confidence="0.998231875">
Pseudo-reference This binary feature explores
the similarity between the target sentence and a
translation for the source sentence produced by an-
other MT system. The feature is 1 if the given
word ti in position i of a target sentence 5 is also
present in a pseudo-reference translation R. In our
experiments, the pseudo-reference is produced by
Moses systems trained over parallel corpora.
</bodyText>
<subsectionHeader confidence="0.999664">
3.2 Sentence level
</subsectionHeader>
<bodyText confidence="0.999674166666667">
Sentence-level QE features have been extensively
explored and described in previous work. The
number of QUEST++ features varies from 80 to
123 depending on the language pair. The complete
list is given as part of QUEST++ ’s documentation.
Some examples are:
</bodyText>
<listItem confidence="0.9997855">
• number of tokens in s &amp; t and their ratio,
• LM probability of s &amp; t,
• ratio of punctuation symbols in s &amp; t,
• ratio of percentage of numbers, content-/non-
content words, nouns/verbs/etc in s &amp; t,
• proportion of dependency relations between
(aligned) constituents in s &amp; t,
• difference in depth of syntactic trees of s &amp; t.
</listItem>
<bodyText confidence="0.999755666666667">
In our experiments, we use the set of 80 fea-
tures, as these can be extracted for all language
pairs of our datasets.
</bodyText>
<subsectionHeader confidence="0.999057">
3.3 Document level
</subsectionHeader>
<bodyText confidence="0.9999305">
Our document-level features follow from those in
the work of (Wong and Kit, 2012) on MT evalua-
tion and (Scarton and Specia, 2014) for document-
level QE. Nine features are extracted, in addition
to aggregated values of sentence-level features for
the entire document:
</bodyText>
<listItem confidence="0.998479333333333">
• content words/lemmas/nouns repetition in
5/T,
• ratio of content words/lemmas/nouns in 5/T,
</listItem>
<sectionHeader confidence="0.998011" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999912666666667">
In what follows, we evaluate QUEST++’s perfor-
mance for the three prediction levels and various
datasets.
</bodyText>
<subsectionHeader confidence="0.976455">
4.1 Word-level QE
</subsectionHeader>
<bodyText confidence="0.999593">
Datasets We use five word-level QE datasets:
the WMT14 English-Spanish, Spanish-English,
English-German and German-English datasets,
and the WMT15 English-Spanish dataset.
Metrics For the WMT14 data, we evaluate per-
formance in the three official classification tasks:
</bodyText>
<listItem confidence="0.998722428571429">
• Binary: A Good/Bad label, where Bad indi-
cates the need for editing the token.
• Level 1: A Good/Accuracy/Fluency label,
specifying the coarser level categories of er-
rors for each token, or Good for tokens with
no error.
• Multi-Class: One of 16 labels specifying the
</listItem>
<bodyText confidence="0.967194392857143">
error type for the token (mistranslation, miss-
ing word, etc.).
The evaluation metric is the average F-1 of all
but the Good class. For the WMT15 dataset, we
consider only the Binary classification task, since
the dataset does not provide other annotations.
Settings For all datasets, the models were
trained with the CRF module in QUEST++ . While
for the WMT14 German-English dataset we use
the Passive Aggressive learning algorithm, for the
remaining datasets, we use the Adaptive Reg-
ularization of Weight Vector (AROW) learning.
Through experimentation, we found that this setup
to be the most effective. The hyper-parameters for
each model were optimised through 10-fold cross
validation. The baseline is the majority class in
the training data, i.e. a system that always pre-
dicts “Unintelligible” for Multi-Class, “Fluency”
for Level 1, and “Bad” for the Binary setup.
Results The F-1 scores for the WMT14 datasets
are given in Tables 1–4, for QUEST++ and sys-
tems that oficially participated in the task. The re-
sults show that QUEST++ was able to outperform
all participating systems in WMT14 except for the
English-Spanish baseline in the Binary and Level
1 tasks. The results in Table 5 also highlight the
importance of selecting an adequate learning al-
gorithm in CRF models.
</bodyText>
<table confidence="0.9998259">
System Binary Level 1 Multiclass
QUEST++ 0.502 0.392 0.227
Baseline 0.525 0.404 0.222
LIG/BL 0.441 0.317 0.204
LIG/FS 0.444 0.317 0.204
FBK-1 0.487 0.372 0.170
FBK-2 0.426 0.385 0.230
LIMSI 0.473 − −
RTM-1 0.350 0.299 0.268
RTM-2 0.328 0.266 0.032
</table>
<tableCaption confidence="0.992947">
Table 1: F-1 for the WMT14 English-Spanish task
</tableCaption>
<subsectionHeader confidence="0.639501">
4.2 Pipeline for sentence-level QE
</subsectionHeader>
<bodyText confidence="0.924205">
Here we evaluate the pipeline of using word-level
predictions as features for sentence-level QE.
</bodyText>
<page confidence="0.993288">
118
</page>
<table confidence="0.983444454545455">
System Binary Level 1 Multiclass
QUEST++ 0.386 0.267 0.161
Baseline 0.299 0.151 0.019
RTM-1 0.269 0.219 0.087
RTM-2 0.291 0.239 0.081
Table 2: F-1 for the WMT14 Spanish-English task
System Binary Level 1 Multiclass
QUEST++ 0.507 0.287 0.161
Baseline 0.445 0.117 0.086
RTM-1 0.452 0.211 0.150
RTM-2 0.369 0.219 0.124
</table>
<tableCaption confidence="0.993585">
Table 3: F-1 for the WMT14 English-German task
</tableCaption>
<bodyText confidence="0.999860769230769">
Dataset We use the WMT15 dataset for word-
level QE. The split between training and test sets
was modified to allow for more sentences for train-
ing the sentence-level QE model. The 2000 last
sentences of the original training set were used
as test along with the original 1000 dev set sen-
tences. Therefore, word predictions were gener-
ated for 3000 sentences, which were later split in
2000 sentences for training and 1000 sentences for
testing the sentence-level model.
Features The 17 QUEST++ baseline features
are used alone (Baseline) and in combination with
four word-level prediction features:
</bodyText>
<listItem confidence="0.999923">
• count &amp; proportion of Good words,
• count &amp; proportion of Bad words.
</listItem>
<bodyText confidence="0.9997975625">
Oracle word level labels, as given in the original
dataset, are also used in a separate experiment to
study the potential of this pipelined approach.
Settings For learning sentence-level models, the
SVR algorithm with RBF kernel and hyperparam-
eters optimised via grid search in QUEST++ is
used. Evaluation is done using MAE (Mean Ab-
solute Error) as metric.
Results As shown in Table 6, the use of word-
level predictions as features led to no improve-
ment. However, the use of the oracle word-level
labels as features substantially improved the re-
sults, lowering the baseline error by half. We note
that the method used in this experiments is the
same as that in Section 4.1, but with fewer in-
stances for training the word-level models. Im-
</bodyText>
<table confidence="0.9993878">
System Binary Level 1 Multiclass
QUEST++ 0.401 0.230 0.079
Baseline 0.365 0.149 0.069
RTM-1 0.261 0.082 0.023
RTM-2 0.229 0.085 0.030
</table>
<tableCaption confidence="0.786581">
Table 4: F-1 for the WMT14 German-English task
</tableCaption>
<table confidence="0.952204615384616">
Algorithm Binary
AROW 0.379
PA 0.352
LBFGS 0.001
L2SGD 0.000
AP 0.000
Table 5: F-1 for the WMT15 English-Spanish task
proving word-level prediction could thus lead to
better results in the pipeline for sentence-level QE.
MAE
Baseline 0.159
Baseline+Predicted 0.158
Baseline+Oracle 0.07
</table>
<tableCaption confidence="0.997595">
Table 6: MAE values for sentence-level QE
</tableCaption>
<sectionHeader confidence="0.446498" genericHeader="method">
4.3 Pipeline for document-level QE
</sectionHeader>
<bodyText confidence="0.999845523809524">
Here we evaluate the pipeline of using sentence-
level predictions as features for QE of documents.
Dataset For training the sentence-level model,
we use the English-Spanish WMT13 training set
for sentence-level QE. For the document-level
model, we use English-Spanish WMT13 data
from the translation shared task. We mixed the
outputs of all MT systems, leading to 934 trans-
lated documents. 560 randomly selected docu-
ments were used for training and 374 for test-
ing. As quality labels, for sentence-level training
we consider both the HTER and the Likert labels
available. For document-level prediction, BLEU,
TER and METEOR are used as quality labels (not
as features), given the lack of human-target quality
labels for document-level prediction.
Features The 17 QUEST++ baseline features
are aggregated to produce document-level fea-
tures (Baseline). These are then combined with
document-level features (Section 3.3) and finally
with features from sentence-level predictions:
</bodyText>
<listItem confidence="0.9213534">
• maximum/minimum predicted HTER or Lik-
ert score,
• average predicted HTER or Likert score,
• Median, first quartile and third quartile pre-
dicted HTER or Likert score.
</listItem>
<bodyText confidence="0.998475666666667">
Oracle sentence labels are not possible as they
do not exist for the test set documents.
Settings For training and evaluation, we use the
same settings as for sentence-level.
Results Table 7 shows the results in terms of
MAE. The best result was achieved with the
</bodyText>
<page confidence="0.996974">
119
</page>
<bodyText confidence="0.99899575">
baseline plus HTER features, but no significant
improvements over the baseline were observed.
Document-level prediction is a very challenging
task: automatic metric scores used as labels do
not seem to reliably distinguish translations of dif-
ferent source documents, since they were primar-
ily designed to compare alternative translations for
the same source document.
</bodyText>
<table confidence="0.994043428571429">
BLEU TER METEOR
Baseline 0.049 0.050 0.055
Baseline+Doc-level 0.053 0.057 0.055
Baseline+HTER 0.053 0.048 0.054
Baseline+Likert 0.054 0.056 0.054
Baseline+Doc-level+HTER 0.053 0.054 0.054
Baseline+Doc-level+Likert 0.053 0.056 0.054
</table>
<tableCaption confidence="0.998845">
Table 7: MAE values for document-level QE
</tableCaption>
<sectionHeader confidence="0.997249" genericHeader="conclusions">
5 Remarks
</sectionHeader>
<bodyText confidence="0.9999289">
The source code for the framework, the datasets
and extra resources can be downloaded from
https://github.com/ghpaetzold/
questplusplus.
The license for the Java code, Python and
shell scripts is BSD, a permissive license with
no restrictions on the use or extensions of the
software for any purposes, including commer-
cial. For pre-existing code and resources, e.g.,
scikit-learn, their licenses apply.
</bodyText>
<sectionHeader confidence="0.997561" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9992365">
This work was supported by the European Associ-
ation for Machine Translation, the QT21 project
(H2020 No. 645452) and the EXPERT project
(EU Marie Curie ITN No. 317471).
</bodyText>
<sectionHeader confidence="0.99796" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999856121212122">
N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Good-
ness: a method for measuring MT confidence. In
ACL11.
E. Bicici and A. Way. 2014. Referential Transla-
tion Machines for Predicting Translation Quality. In
WMT14.
J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur,
C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2004. Confidence Estimation for Machine Transla-
tion. In COLING04.
O. Bojar, C. Buck, C. Callison-Burch, C. Federmann,
B. Haddow, P. Koehn, C. Monz, M. Post, R. Soricut,
and L. Specia. 2013. Findings of the 2013 Work-
shop on SMT. In WMT13.
O. Bojar, C. Buck, C. Federmann, B. Haddow,
P. Koehn, J. Leveling, C. Monz, P. Pecina, M. Post,
H. Saint-Amand, R. Soricut, L. Specia, and A. Tam-
chyna. 2014. Findings of the 2014 Workshop on
SMT. In WMT14.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Soricut, and L. Specia. 2012. Findings of the
2012 Workshop on SMT. In WMT12.
J. G. Camargo de Souza, J. Gonz´alez-Rubio, C. Buck,
M. Turchi, and M. Negri. 2014. FBK-UPV-
UEdin participation in the WMT14 Quality Estima-
tion shared-task. In WMT14.
N. Q. Luong, L. Besacier, and B. Lecouteux. 2014.
LIG System for Word Level QE task. In WMT14.
N. Okazaki. 2007. CRFsuite: a fast implementation
of Conditional Random Fields. http://www.
chokkan.org/software/crfsuite/.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research, 12.
S. Raybaud, D. Langlois, and K. Smali. 2011. This
sentence is wrong. Detecting errors in machine-
translated sentences. Machine Translation, 25(1).
C. Scarton and L. Specia. 2014. Document-level trans-
lation quality estimation: exploring discourse and
pseudo-references. In EAMT14.
C. Scarton, M. Zampieri, M. Vela, J. van Genabith, and
L. Specia. 2015. Searching for Context: a Study
on Document-Level Labels for Translation Quality
Estimation. In EAMT15.
K. Shah and L. Specia. 2014. Quality estimation for
translation selection. In EAMT14.
R. Soricut and A. Echihabi. 2010. Trustrank: Induc-
ing trust in automatic translations via ranking. In
ACL10.
L. Specia, K. Shah, J. G. C. de Souza, and T. Cohn.
2013. Quest - a translation quality estimation frame-
work. In ACL13.
L. Specia. 2011. Exploiting objective annotations
for measuring translation post-editing effort. In
EAMT11.
G. Wisniewski, N. Pcheux, A. Allauzen, and F. Yvon.
2014. LIMSI Submission for WMT’14 QE Task. In
WMT14.
B. T. M. Wong and C. Kit. 2012. Extending machine
translation evaluation metrics with lexical cohesion
to document level. In EMNLP/CONLL.
D. Xiong, M. Zhang, and H. Li. 2010. Error detection
for SMT using linguistic features. In ACL10.
</reference>
<page confidence="0.996236">
120
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.960044">
<title confidence="0.996636">Translation Quality Prediction with</title>
<author confidence="0.976728">Gustavo Henrique Paetzold Specia</author>
<affiliation confidence="0.9998025">Department of Computer University of Sheffield,</affiliation>
<abstract confidence="0.9990802">paper presents , an open source tool for quality estimation which can predict quality for texts at word, sentence and document level. It also provides pipelined processing, whereby predictions made at a lower level (e.g. for words) can be used as input to build models for predictions at a higher level (e.g. sentences). allows the extraction of a variety of features, and provides machine learning algorithms to build and test quality estimation models. Results on recent show that achieves state-of-the-art performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Bach</author>
<author>F Huang</author>
<author>Y Al-Onaizan</author>
</authors>
<title>Goodness: a method for measuring MT confidence.</title>
<date>2011</date>
<booktitle>In ACL11.</booktitle>
<contexts>
<context position="1333" citStr="Bach et al., 2011" startWordPosition="196" endWordPosition="199">ts on recent datasets show that QUEST++ achieves state-of-the-art performance. 1 Introduction Quality Estimation (QE) of Machine Translation (MT) have become increasingly popular over the last decade. With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting the best translation (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of sentence translations annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: ASIYA1 and QUEST2 (Spec</context>
</contexts>
<marker>Bach, Huang, Al-Onaizan, 2011</marker>
<rawString>N. Bach, F. Huang, and Y. Al-Onaizan. 2011. Goodness: a method for measuring MT confidence. In ACL11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Bicici</author>
<author>A Way</author>
</authors>
<title>Referential Translation Machines for Predicting Translation Quality.</title>
<date>2014</date>
<booktitle>In WMT14.</booktitle>
<contexts>
<context position="7949" citStr="Bicici and Way, 2014" startWordPosition="1250" endWordPosition="1253">or more flexible experiments and comparisons across MT systems. System-dependent features can be extracted as long they are represented using a predefined XML scheme. Most of the existing features are either language-independent or depend on linguistic resources such as POS taggers. The latter can be extracted for any language, as long as the resource is available. For a pipelined approach, predictions at a given level can become features for higher level model, e.g. features based on word-level predictions for sentence-level QE. 3.1 Word level We explore a range of features from recent work (Bicici and Way, 2014; Camargo de Souza et al., 2014; Luong et al., 2014; Wisniewski et al., 2014), totalling 40 features of seven types: Target context These are features that explore the context of the target word. Given a word ti in position i of a target sentence, we extract: ti, 3http://scikit-learn.org/ 116 Figure 1: Architecture of QUEST++ i.e., the word itself, bigrams ti−1ti and titi+1, and trigrams ti−2ti−1ti, ti−1titi+1 and titi+1ti+2. Alignment context These features explore the word alignment between source and target sentences. They require the 1-to-N alignment between source and target sentences to </context>
</contexts>
<marker>Bicici, Way, 2014</marker>
<rawString>E. Bicici and A. Way. 2014. Referential Translation Machines for Predicting Translation Quality. In WMT14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blatz</author>
<author>E Fitzgerald</author>
<author>G Foster</author>
<author>S Gandrabur</author>
<author>C Goutte</author>
<author>A Kulesza</author>
<author>A Sanchis</author>
<author>N Ueffing</author>
</authors>
<title>Confidence Estimation for Machine Translation.</title>
<date>2004</date>
<booktitle>In COLING04.</booktitle>
<contexts>
<context position="2277" citStr="Blatz et al., 2004" startWordPosition="350" endWordPosition="353">kshop on Statistical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: ASIYA1 and QUEST2 (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of participants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014). QE at other textual levels have received much less attention. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). While ce</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>J. Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C. Goutte, A. Kulesza, A. Sanchis, and N. Ueffing. 2004. Confidence Estimation for Machine Translation. In COLING04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>C Buck</author>
<author>C Callison-Burch</author>
<author>C Federmann</author>
<author>B Haddow</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>M Post</author>
<author>R Soricut</author>
<author>L Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on SMT. In WMT13.</booktitle>
<contexts>
<context position="2159" citStr="Bojar et al., 2013" startWordPosition="330" endWordPosition="333"> with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: ASIYA1 and QUEST2 (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of participants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014). QE at other textual levels have received much less attention. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton an</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>O. Bojar, C. Buck, C. Callison-Burch, C. Federmann, B. Haddow, P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. 2013. Findings of the 2013 Workshop on SMT. In WMT13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Bojar</author>
<author>C Buck</author>
<author>C Federmann</author>
<author>B Haddow</author>
<author>P Koehn</author>
<author>J Leveling</author>
<author>C Monz</author>
<author>P Pecina</author>
<author>M Post</author>
<author>H Saint-Amand</author>
<author>R Soricut</author>
<author>L Specia</author>
<author>A Tamchyna</author>
</authors>
<date>2014</date>
<booktitle>Findings of the 2014 Workshop on SMT. In WMT14.</booktitle>
<contexts>
<context position="2180" citStr="Bojar et al., 2014" startWordPosition="334" endWordPosition="337"> (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: ASIYA1 and QUEST2 (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of participants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014). QE at other textual levels have received much less attention. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a </context>
</contexts>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, Saint-Amand, Soricut, Specia, Tamchyna, 2014</marker>
<rawString>O. Bojar, C. Buck, C. Federmann, B. Haddow, P. Koehn, J. Leveling, C. Monz, P. Pecina, M. Post, H. Saint-Amand, R. Soricut, L. Specia, and A. Tamchyna. 2014. Findings of the 2014 Workshop on SMT. In WMT14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Callison-Burch</author>
<author>P Koehn</author>
<author>C Monz</author>
<author>M Post</author>
<author>R Soricut</author>
<author>L Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on SMT. In WMT12.</booktitle>
<contexts>
<context position="2139" citStr="Callison-Burch et al., 2012" startWordPosition="326" endWordPosition="329">ntence translations annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: ASIYA1 and QUEST2 (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of participants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014). QE at other textual levels have received much less attention. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolu</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Soricut, and L. Specia. 2012. Findings of the 2012 Workshop on SMT. In WMT12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Camargo de Souza</author>
<author>J Gonz´alez-Rubio</author>
<author>C Buck</author>
<author>M Turchi</author>
<author>M Negri</author>
</authors>
<title>FBK-UPVUEdin participation in the WMT14 Quality Estimation shared-task.</title>
<date>2014</date>
<booktitle>In WMT14.</booktitle>
<marker>de Souza, Gonz´alez-Rubio, Buck, Turchi, Negri, 2014</marker>
<rawString>J. G. Camargo de Souza, J. Gonz´alez-Rubio, C. Buck, M. Turchi, and M. Negri. 2014. FBK-UPVUEdin participation in the WMT14 Quality Estimation shared-task. In WMT14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Q Luong</author>
<author>L Besacier</author>
<author>B Lecouteux</author>
</authors>
<date>2014</date>
<booktitle>LIG System for Word Level QE task. In WMT14.</booktitle>
<contexts>
<context position="2298" citStr="Luong et al., 2014" startWordPosition="354" endWordPosition="357"> Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source feature extraction toolkits are available for that: ASIYA1 and QUEST2 (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of participants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014). QE at other textual levels have received much less attention. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). While certain sentences are p</context>
<context position="8000" citStr="Luong et al., 2014" startWordPosition="1260" endWordPosition="1263">T systems. System-dependent features can be extracted as long they are represented using a predefined XML scheme. Most of the existing features are either language-independent or depend on linguistic resources such as POS taggers. The latter can be extracted for any language, as long as the resource is available. For a pipelined approach, predictions at a given level can become features for higher level model, e.g. features based on word-level predictions for sentence-level QE. 3.1 Word level We explore a range of features from recent work (Bicici and Way, 2014; Camargo de Souza et al., 2014; Luong et al., 2014; Wisniewski et al., 2014), totalling 40 features of seven types: Target context These are features that explore the context of the target word. Given a word ti in position i of a target sentence, we extract: ti, 3http://scikit-learn.org/ 116 Figure 1: Architecture of QUEST++ i.e., the word itself, bigrams ti−1ti and titi+1, and trigrams ti−2ti−1ti, ti−1titi+1 and titi+1ti+2. Alignment context These features explore the word alignment between source and target sentences. They require the 1-to-N alignment between source and target sentences to be provided. Given a word ti in position i of a tar</context>
</contexts>
<marker>Luong, Besacier, Lecouteux, 2014</marker>
<rawString>N. Q. Luong, L. Besacier, and B. Lecouteux. 2014. LIG System for Word Level QE task. In WMT14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Okazaki</author>
</authors>
<title>CRFsuite: a fast implementation of Conditional Random Fields.</title>
<date>2007</date>
<note>http://www. chokkan.org/software/crfsuite/.</note>
<contexts>
<context position="6685" citStr="Okazaki, 2007" startWordPosition="1041" endWordPosition="1042">th the peculiarities of its type of feature. Machine learning QUEST++ provides scripts to interface the Python toolkit scikit-learn3 (Pedregosa et al., ). This module is independent from the feature extraction code and uses the extracted feature sets to build and test QE models. The module can be configured to run different regression and classification algorithms, feature selection methods and grid search for hyper-parameter optimisation. Algorithms from scikit-learn can be easily integrated by modifying existing scripts. For word-level prediction, QUEST++ provides an interface for CRFSuite (Okazaki, 2007), a sequence labelling C++ library for Conditional Random Fields (CRF). One can configure CRFSuite training settings, produce models and test them. 3 Features Features in QUEST++ can be extracted from either source or target (or both) sides of the corpus at a given textual level. In order describe the features supported, we denote: • S and T the source and target documents, • s and t for source and target sentences, and • s and t for source and target words. We concentrate on MT system-independent (black-box) features, which are extracted based on the output of the MT system rather than any of</context>
</contexts>
<marker>Okazaki, 2007</marker>
<rawString>N. Okazaki. 2007. CRFsuite: a fast implementation of Conditional Random Fields. http://www. chokkan.org/software/crfsuite/.</rawString>
</citation>
<citation valid="false">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<journal>Journal of Machine Learning Research,</journal>
<volume>12</volume>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, </marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Raybaud</author>
<author>D Langlois</author>
<author>K Smali</author>
</authors>
<title>This sentence is wrong. Detecting errors in machinetranslated sentences.</title>
<date>2011</date>
<journal>Machine Translation,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="9436" citStr="Raybaud et al., 2011" startWordPosition="1506" endWordPosition="1509">d sjti+2. Lexical These features explore POS information on the source and target words. Given the POS tag Pti of word ti in position i of a target sentence and the POS tag Psj of word sj aligned to it in position j of a source sentence, we extract: the POS tags Pti and Psj themselves, the bigrams Pti−1Pti and PtiPti+1 and trigrams Pti−2Pti−1Pti, Pti−1PtiPti+1 and PtiPti+1Pti+2. Four binary features are also extracted with value 1 if ti is a stop word, punctuation symbol, proper noun or numeral. LM These features are related to the n-gram frequencies of a word’s context with respect to an LM (Raybaud et al., 2011). Six features are extracted: lexical and syntactic backoff behavior, as well as lexical and syntactic longest preceding n-gram for both a target word and an aligned source word. Given a word ti in position i of a target sentence, the lexical backoff behavior is calculated as: { 7 if ti−2, ti−1, ti exists 6 if ti−2, ti−1 and ti−1, ti exist 5 if only ti−1, ti exists 4 if ti−2, ti−1 and ti exist 3 if ti−1 and ti exist 2 if ti exists 1 if ti is out of the vocabulary The syntactic backoff behavior is calculated in an analogous fashion: it verifies for the existence of n-grams of POS tags in a POS-</context>
</contexts>
<marker>Raybaud, Langlois, Smali, 2011</marker>
<rawString>S. Raybaud, D. Langlois, and K. Smali. 2011. This sentence is wrong. Detecting errors in machinetranslated sentences. Machine Translation, 25(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Scarton</author>
<author>L Specia</author>
</authors>
<title>Document-level translation quality estimation: exploring discourse and pseudo-references.</title>
<date>2014</date>
<booktitle>In EAMT14.</booktitle>
<contexts>
<context position="2774" citStr="Scarton and Specia, 2014" startWordPosition="433" endWordPosition="436"> al., 2013; Bojar et al., 2014). QE at other textual levels have received much less attention. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). While certain sentences are perfect in isolation, their combination in context may lead to an incoherent document. Conversely, while a sentence can be poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a good quality document. Feature engineering is a challenge given the little availability of tools to extract discourse-wide information. In addition, no datasets with human-created labels are available and thus scores produced by automatic metr</context>
<context position="12105" citStr="Scarton and Specia, 2014" startWordPosition="1972" endWordPosition="1975">umentation. Some examples are: • number of tokens in s &amp; t and their ratio, • LM probability of s &amp; t, • ratio of punctuation symbols in s &amp; t, • ratio of percentage of numbers, content-/noncontent words, nouns/verbs/etc in s &amp; t, • proportion of dependency relations between (aligned) constituents in s &amp; t, • difference in depth of syntactic trees of s &amp; t. In our experiments, we use the set of 80 features, as these can be extracted for all language pairs of our datasets. 3.3 Document level Our document-level features follow from those in the work of (Wong and Kit, 2012) on MT evaluation and (Scarton and Specia, 2014) for documentlevel QE. Nine features are extracted, in addition to aggregated values of sentence-level features for the entire document: • content words/lemmas/nouns repetition in 5/T, • ratio of content words/lemmas/nouns in 5/T, 4 Experiments In what follows, we evaluate QUEST++’s performance for the three prediction levels and various datasets. 4.1 Word-level QE Datasets We use five word-level QE datasets: the WMT14 English-Spanish, Spanish-English, English-German and German-English datasets, and the WMT15 English-Spanish dataset. Metrics For the WMT14 data, we evaluate performance in the t</context>
</contexts>
<marker>Scarton, Specia, 2014</marker>
<rawString>C. Scarton and L. Specia. 2014. Document-level translation quality estimation: exploring discourse and pseudo-references. In EAMT14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Scarton</author>
<author>M Zampieri</author>
<author>M Vela</author>
<author>J van Genabith</author>
<author>L Specia</author>
</authors>
<title>Searching for Context: a Study on Document-Level Labels for Translation Quality Estimation.</title>
<date>2015</date>
<booktitle>In EAMT15.</booktitle>
<marker>Scarton, Zampieri, Vela, van Genabith, Specia, 2015</marker>
<rawString>C. Scarton, M. Zampieri, M. Vela, J. van Genabith, and L. Specia. 2015. Searching for Context: a Study on Document-Level Labels for Translation Quality Estimation. In EAMT15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Shah</author>
<author>L Specia</author>
</authors>
<title>Quality estimation for translation selection.</title>
<date>2014</date>
<booktitle>In EAMT14.</booktitle>
<contexts>
<context position="1259" citStr="Shah and Specia, 2014" startWordPosition="184" endWordPosition="187">machine learning algorithms to build and test quality estimation models. Results on recent datasets show that QUEST++ achieves state-of-the-art performance. 1 Introduction Quality Estimation (QE) of Machine Translation (MT) have become increasingly popular over the last decade. With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting the best translation (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of sentence translations annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to this task is work of feature engineering. Two open source f</context>
</contexts>
<marker>Shah, Specia, 2014</marker>
<rawString>K. Shah and L. Specia. 2014. Quality estimation for translation selection. In EAMT14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>A Echihabi</author>
</authors>
<title>Trustrank: Inducing trust in automatic translations via ranking.</title>
<date>2010</date>
<booktitle>In ACL10.</booktitle>
<contexts>
<context position="1203" citStr="Soricut and Echihabi, 2010" startWordPosition="176" endWordPosition="179">allows the extraction of a variety of features, and provides machine learning algorithms to build and test quality estimation models. Results on recent datasets show that QUEST++ achieves state-of-the-art performance. 1 Introduction Quality Estimation (QE) of Machine Translation (MT) have become increasingly popular over the last decade. With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting the best translation (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of sentence translations annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012. While standard algorithms can be used to build prediction models, key to thi</context>
<context position="2867" citStr="Soricut and Echihabi, 2010" startWordPosition="450" endWordPosition="453">on. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline. Document-level QE consists in predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). While certain sentences are perfect in isolation, their combination in context may lead to an incoherent document. Conversely, while a sentence can be poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a good quality document. Feature engineering is a challenge given the little availability of tools to extract discourse-wide information. In addition, no datasets with human-created labels are available and thus scores produced by automatic metrics have to be used as approximation (Scarton et al., 2015). Some applications require fine-g</context>
</contexts>
<marker>Soricut, Echihabi, 2010</marker>
<rawString>R. Soricut and A. Echihabi. 2010. Trustrank: Inducing trust in automatic translations via ranking. In ACL10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
<author>K Shah</author>
<author>J G C de Souza</author>
<author>T Cohn</author>
</authors>
<title>Quest - a translation quality estimation framework.</title>
<date>2013</date>
<booktitle>In ACL13.</booktitle>
<marker>Specia, Shah, de Souza, Cohn, 2013</marker>
<rawString>L. Specia, K. Shah, J. G. C. de Souza, and T. Cohn. 2013. Quest - a translation quality estimation framework. In ACL13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Specia</author>
</authors>
<title>Exploiting objective annotations for measuring translation post-editing effort.</title>
<date>2011</date>
<booktitle>In EAMT11.</booktitle>
<contexts>
<context position="1141" citStr="Specia, 2011" startWordPosition="169" endWordPosition="170">ons at a higher level (e.g. sentences). QUEST++ allows the extraction of a variety of features, and provides machine learning algorithms to build and test quality estimation models. Results on recent datasets show that QUEST++ achieves state-of-the-art performance. 1 Introduction Quality Estimation (QE) of Machine Translation (MT) have become increasingly popular over the last decade. With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency (Specia, 2011), selecting high quality segments (Soricut and Echihabi, 2010), selecting the best translation (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011). Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of sentence translations annotated with quality labels (e.g. 1-5 likert scores). Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012. While standard</context>
</contexts>
<marker>Specia, 2011</marker>
<rawString>L. Specia. 2011. Exploiting objective annotations for measuring translation post-editing effort. In EAMT11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Wisniewski</author>
<author>N Pcheux</author>
<author>A Allauzen</author>
<author>F Yvon</author>
</authors>
<date>2014</date>
<booktitle>LIMSI Submission for WMT’14 QE Task. In WMT14.</booktitle>
<contexts>
<context position="8026" citStr="Wisniewski et al., 2014" startWordPosition="1264" endWordPosition="1267">pendent features can be extracted as long they are represented using a predefined XML scheme. Most of the existing features are either language-independent or depend on linguistic resources such as POS taggers. The latter can be extracted for any language, as long as the resource is available. For a pipelined approach, predictions at a given level can become features for higher level model, e.g. features based on word-level predictions for sentence-level QE. 3.1 Word level We explore a range of features from recent work (Bicici and Way, 2014; Camargo de Souza et al., 2014; Luong et al., 2014; Wisniewski et al., 2014), totalling 40 features of seven types: Target context These are features that explore the context of the target word. Given a word ti in position i of a target sentence, we extract: ti, 3http://scikit-learn.org/ 116 Figure 1: Architecture of QUEST++ i.e., the word itself, bigrams ti−1ti and titi+1, and trigrams ti−2ti−1ti, ti−1titi+1 and titi+1ti+2. Alignment context These features explore the word alignment between source and target sentences. They require the 1-to-N alignment between source and target sentences to be provided. Given a word ti in position i of a target sentence and a word sj</context>
</contexts>
<marker>Wisniewski, Pcheux, Allauzen, Yvon, 2014</marker>
<rawString>G. Wisniewski, N. Pcheux, A. Allauzen, and F. Yvon. 2014. LIMSI Submission for WMT’14 QE Task. In WMT14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B T M Wong</author>
<author>C Kit</author>
</authors>
<title>Extending machine translation evaluation metrics with lexical cohesion to document level.</title>
<date>2012</date>
<booktitle>In EMNLP/CONLL.</booktitle>
<contexts>
<context position="12057" citStr="Wong and Kit, 2012" startWordPosition="1963" endWordPosition="1966">te list is given as part of QUEST++ ’s documentation. Some examples are: • number of tokens in s &amp; t and their ratio, • LM probability of s &amp; t, • ratio of punctuation symbols in s &amp; t, • ratio of percentage of numbers, content-/noncontent words, nouns/verbs/etc in s &amp; t, • proportion of dependency relations between (aligned) constituents in s &amp; t, • difference in depth of syntactic trees of s &amp; t. In our experiments, we use the set of 80 features, as these can be extracted for all language pairs of our datasets. 3.3 Document level Our document-level features follow from those in the work of (Wong and Kit, 2012) on MT evaluation and (Scarton and Specia, 2014) for documentlevel QE. Nine features are extracted, in addition to aggregated values of sentence-level features for the entire document: • content words/lemmas/nouns repetition in 5/T, • ratio of content words/lemmas/nouns in 5/T, 4 Experiments In what follows, we evaluate QUEST++’s performance for the three prediction levels and various datasets. 4.1 Word-level QE Datasets We use five word-level QE datasets: the WMT14 English-Spanish, Spanish-English, English-German and German-English datasets, and the WMT15 English-Spanish dataset. Metrics For </context>
</contexts>
<marker>Wong, Kit, 2012</marker>
<rawString>B. T. M. Wong and C. Kit. 2012. Extending machine translation evaluation metrics with lexical cohesion to document level. In EMNLP/CONLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Xiong</author>
<author>M Zhang</author>
<author>H Li</author>
</authors>
<title>Error detection for SMT using linguistic features.</title>
<date>2010</date>
<booktitle>In ACL10.</booktitle>
<contexts>
<context position="10270" citStr="Xiong et al., 2010" startWordPosition="1659" endWordPosition="1662">arget sentence, the lexical backoff behavior is calculated as: { 7 if ti−2, ti−1, ti exists 6 if ti−2, ti−1 and ti−1, ti exist 5 if only ti−1, ti exists 4 if ti−2, ti−1 and ti exist 3 if ti−1 and ti exist 2 if ti exists 1 if ti is out of the vocabulary The syntactic backoff behavior is calculated in an analogous fashion: it verifies for the existence of n-grams of POS tags in a POS-tagged LM. The POS tags of target sentence are produced by the Stanford Parser4 (integrated in QUEST++ ). Syntactic QUEST++ provides one syntactic feature that proved very promising in previous work: the Null Link (Xiong et al., 2010). It is a binary feature that receives value 1 if a given word ti in a target sentence has at least one dependency link with another word tj, and 0 otherwise. The Stanford Parser is used for dependency parsing. Semantic These features explore the polysemy of target and source words, i.e. the number of senses existing as entries in a WordNet for a given target word ti or a source word si. We employ the Universal WordNet,5 which provides access to WordNets of various languages. 4http://nlp.stanford.edu/software/ lex-parser.shtml 5http://www.lexvo.org/uwn/ f (ti) = 117 Pseudo-reference This binar</context>
</contexts>
<marker>Xiong, Zhang, Li, 2010</marker>
<rawString>D. Xiong, M. Zhang, and H. Li. 2010. Error detection for SMT using linguistic features. In ACL10.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>