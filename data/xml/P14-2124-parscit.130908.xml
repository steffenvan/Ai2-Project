<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021505">
<title confidence="0.871395">
XMEANT: Better semantic MT evaluation without reference translations
</title>
<author confidence="0.88585">
Lo, Chi-kiu Beloucif, Meriem Saers, Markus Wu, Dekai
</author>
<affiliation confidence="0.96006025">
HKUST
Human Language Technology Center
Department of Computer Science and Engineering
Hong Kong University of Science and Technology
</affiliation>
<email confidence="0.994276">
{jackielo|mbeloucif|masaers|dekai}@cs.ust.hk
</email>
<sectionHeader confidence="0.997339" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999716142857143">
We introduce XMEANT—a new cross-lingual
version of the semantic frame based MT
evaluation metric MEANT—which can cor-
relate even more closely with human ade-
quacy judgments than monolingual MEANT
and eliminates the need for expensive hu-
man references. Previous work established
that MEANT reflects translation adequacy
with state-of-the-art accuracy, and optimiz-
ing MT systems against MEANT robustly im-
proves translation quality. However, to go
beyond tuning weights in the loglinear SMT
model, a cross-lingual objective function that
can deeply integrate semantic frame crite-
ria into the MT training pipeline is needed.
We show that cross-lingual XMEANT out-
performs monolingual MEANT by (1) replac-
ing the monolingual context vector model in
MEANT with simple translation probabilities,
and (2) incorporating bracketing ITG con-
straints.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965754385965">
We show that XMEANT, a new cross-lingual ver-
sion of MEANT (Lo et al., 2012), correlates with
human judgment even more closely than MEANT
for evaluating MT adequacy via semantic frames,
despite discarding the need for expensive human
reference translations. XMEANT is obtained by
(1) using simple lexical translation probabilities,
instead of the monolingual context vector model
used in MEANT for computing the semantic role
fillers similarities, and (2) incorporating bracket-
ing ITG constrains for word alignment within the
semantic role fillers. We conjecture that the rea-
son that XMEANT correlates more closely with
human adequacy judgement than MEANT is that
on the one hand, the semantic structure of the
MT output is closer to that of the input sentence
than that of the reference translation, and on the
other hand, the BITG constraints the word align-
ment more accurately than the heuristic bag-of-
word aggregation used in MEANT. Our results
suggest that MT translation adequacy is more ac-
curately evaluated via the cross-lingual semantic
frame similarities of the input and the MT output
which may obviate the need for expensive human
reference translations.
The MEANT family of metrics (Lo and Wu,
2011a, 2012; Lo et al., 2012) adopt the princi-
ple that a good translation is one where a human
can successfully understand the central meaning
of the foreign sentence as captured by the basic
event structure: “who did what to whom, when,
where and why” (Pradhan et al., 2004). MEANT
measures similarity between the MT output and
the reference translations by comparing the simi-
larities between the semantic frame structures of
output and reference translations. It is well estab-
lished that the MEANT family of metrics corre-
lates better with human adequacy judgments than
commonly used MT evaluation metrics (Lo and
Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu,
2013b; Mach´aˇcek and Bojar, 2013). In addition,
the translation adequacy across different genres
(ranging from formal news to informal web fo-
rum and public speech) and different languages
(English and Chinese) is improved by replacing
BLEU or TER with MEANT during parameter
tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo
et al., 2013b).
In order to continue driving MT towards better
translation adequacy by deeply integrating seman-
tic frame criteria into the MT training pipeline, it is
necessary to have a cross-lingual semantic objec-
tive function that assesses the semantic frame sim-
ilarities of input and output sentences. We there-
fore propose XMEANT, a cross-lingual MT evalu-
ation metric, that modifies MEANT using (1) sim-
ple translation probabilities (in our experiments,
</bodyText>
<page confidence="0.970207">
765
</page>
<bodyText confidence="0.9641899">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 765–771,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
from quick IBM-1 training), to replace the mono-
lingual context vector model in MEANT, and (2)
constraints from BITGs (bracketing ITGs). We
show that XMEANT assesses MT adequacy more
accurately than MEANT (as measured by correla-
tion with human adequacy judgement) without the
need for expensive human reference translations in
the output language.
</bodyText>
<sectionHeader confidence="0.999969" genericHeader="related work">
2 Related Work
</sectionHeader>
<subsectionHeader confidence="0.998214">
2.1 MT evaluation metrics
</subsectionHeader>
<bodyText confidence="0.999739825">
Surface-form oriented metrics such as BLEU (Pa-
pineni et al., 2002), NIST (Doddington, 2002),
METEOR (Banerjee and Lavie, 2005), CDER
(Leusch et al., 2006), WER (Nießen et al., 2000),
and TER (Snover et al., 2006) do not correctly re-
flect the meaning similarities of the input sentence.
In fact, a number of large scale meta-evaluations
(Callison-Burch et al., 2006; Koehn and Monz,
2006) report cases where BLEU strongly dis-
agrees with human judgments of translation ade-
quacy.
This has caused a recent surge of work to de-
velop better ways to automatically measure MT
adequacy. Owczarzak et al. (2007a,b) improved
correlation with human fluency judgments by us-
ing LFG to extend the approach of evaluating syn-
tactic dependency structure similarity proposed by
Liu and Gildea (2005), but did not achieve higher
correlation with human adequacy judgments than
metrics like METEOR. TINE (Rios et al., 2011) is
a recall-oriented metric which aims to preserve the
basic event structure but it performs comparably
to BLEU and worse than METEOR on correlation
with human adequacy judgments. ULC (Gim´enez
and M`arquez, 2007, 2008) incorporates several
semantic features and shows improved correla-
tion with human judgement on translation quality
(Callison-Burch et al., 2007, 2008) but no work
has been done towards tuning an SMT system us-
ing a pure form of ULC perhaps due to its expen-
sive run time. Similarly, SPEDE (Wang and Man-
ning, 2012) predicts the edit sequence for match-
ing the MT output to the reference via an inte-
grated probabilistic FSM and PDA model. Sagan
(Castillo and Estrella, 2012) is a semantic textual
similarity metric based on a complex textual en-
tailment pipeline. These aggregated metrics re-
quire sophisticated feature extraction steps, con-
tain several dozens of parameters to tune, and em-
ploy expensive linguistic resources like WordNet
</bodyText>
<figureCaption confidence="0.999017">
Figure 1: Monolingual MEANT algorithm.
</figureCaption>
<bodyText confidence="0.993198666666667">
or paraphrase tables; the expensive training, tun-
ing, and/or running time makes them hard to in-
corporate into the MT development cycle.
</bodyText>
<subsectionHeader confidence="0.991827">
2.2 The MEANT family of metrics
</subsectionHeader>
<bodyText confidence="0.999939136363636">
MEANT (Lo et al., 2012), which is the weighted f-
score over the matched semantic role labels of the
automatically aligned semantic frames and role
fillers, that outperforms BLEU, NIST, METEOR,
WER, CDER and TER in correlation with human
adequacy judgments. MEANT is easily portable
to other languages, requiring only an automatic se-
mantic parser and a large monolingual corpus in
the output language for identifying the semantic
structures and the lexical similarity between the
semantic role fillers of the reference and transla-
tion.
Figure 1 shows the algorithm and equations for
computing MEANT. qPi,j and q�i,jare the argument
of type j in frame i in MT and REF respectively.
wPi and wz are the weights for frame i in MT/REF
respectively. These weights estimate the degree of
contribution of each frame to the overall meaning
of the sentence. wpred and wj are the weights of
the lexical similarities of the predicates and role
fillers of the arguments of type j of all frame be-
tween the reference translations and the MT out-
</bodyText>
<page confidence="0.996286">
766
</page>
<figureCaption confidence="0.989405666666667">
Figure 2: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic
shallow semantic parser. The reference and MT output are parsed by an English automatic shallow
semantic parser. There are no semantic frames for MT3 since the system decided to drop the predicate.
</figureCaption>
<bodyText confidence="0.999944918918919">
put. There is a total of 12 weights for the set
of semantic role labels in MEANT as defined in
Lo and Wu (2011b). For MEANT, they are de-
termined using supervised estimation via a sim-
ple grid search to optimize the correlation with
human adequacy judgments (Lo and Wu, 2011a).
For UMEANT (Lo and Wu, 2012), they are es-
timated in an unsupervised manner using relative
frequency of each semantic role label in the refer-
ences and thus UMEANT is useful when human
judgments on adequacy of the development set are
unavailable.
si,pred and si,� are the lexical similarities based
on a context vector model of the predicates and
role fillers of the arguments of type j between the
reference translations and the MT output. Lo et al.
(2012) and Tumuluru et al. (2012) described how
the lexical and phrasal similarities of the semantic
role fillers are computed. A subsequent variant of
the aggregation function inspired by Mihalcea et
al. (2006) that normalizes phrasal similarities ac-
cording to the phrase length more accurately was
used in more recent work (Lo et al., 2013a; Lo
and Wu, 2013a; Lo et al., 2013b). In this paper,
we employ a newer version of MEANT that uses
f-score to aggregate individual token similarities
into the composite phrasal similarities of seman-
tic role fillers, as our experiments indicate this is
more accurate than the previously used aggrega-
tion functions.
Recent studies (Lo et al., 2013a; Lo and Wu,
2013a; Lo et al., 2013b) show that tuning MT sys-
tems against MEANT produces more robustly ad-
equate translations than the common practice of
tuning against BLEU or TER across different data
genres, such as formal newswire text, informal
web forum text and informal public speech.
</bodyText>
<subsectionHeader confidence="0.997338">
2.3 MT quality estimation
</subsectionHeader>
<bodyText confidence="0.999984346153846">
Evaluating cross-lingual MT quality is similar to
the work of MT quality estimation (QE). Broadly
speaking, there are two different approaches to
QE: surface-based and feature-based.
Token-based QE models, such as those in Gan-
drabur et al. (2006) and Ueffing and Ney (2005)
fail to assess the overall MT quality because trans-
lation goodness is not a compositional property. In
contrast, Blatz et al. (2004) introduced a sentence-
level QE system where an arbitrary threshold is
used to classify the MT output as good or bad.
The fundamental problem of this approach is that
it defines QE as a binary classification task rather
than attempting to measure the degree of goodness
of the MT output. To address this problem, Quirk
(2004) related the sentence-level correctness of the
QE model to human judgment and achieved a high
correlation with human judgement for a small an-
notated corpus; however, the proposed model does
not scale well to larger data sets.
Feature-based QE models (Xiong et al., 2010;
He et al., 2011; Ma et al., 2011; Specia, 2011;
Avramidis, 2012; Mehdad et al., 2012; Almaghout
and Specia, 2013; Avramidis and Popovi´c, 2013;
Shah et al., 2013) throw a wide range of linguis-
tic and non-linguistic features into machine learn-
</bodyText>
<page confidence="0.991365">
767
</page>
<figureCaption confidence="0.999674">
Figure 3: Cross-lingual XMEANT algorithm.
</figureCaption>
<bodyText confidence="0.999908125">
ing algorithms for predicting MT quality. Al-
though the feature-based QE system of Avramidis
and Popovi´c (2013) slightly outperformed ME-
TEOR on correlation with human adequacy judg-
ment, these “black box” approaches typically lack
representational transparency, require expensive
running time, and/or must be discriminatively re-
trained for each language and text type.
</bodyText>
<sectionHeader confidence="0.972926" genericHeader="method">
3 XMEANT: a cross-lingual MEANT
</sectionHeader>
<bodyText confidence="0.999945">
Like MEANT, XMEANT aims to evaluate how
well MT preserves the core semantics, while
maintaining full representational transparency.
But whereas MEANT measures lexical similar-
ity using a monolingual context vector model,
XMEANT instead substitutes simple cross-lingual
lexical translation probabilities.
XMEANT differs only minimally from
MEANT, as underlined in figure 3. The same
weights obtained by optimizing MEANT against
human adequacy judgement were used for
XMEANT. The weights can also be estimated in
unsupervised fashion using the relative frequency
of each semantic role label in the foreign input, as
in UMEANT.
To aggregate individual lexical translation prob-
abilities into phrasal similarities between cross-
lingual semantic role fillers, we compared two nat-
ural approaches to generalizing MEANT’s method
of comparing semantic parses, as described below.
</bodyText>
<subsectionHeader confidence="0.9264085">
3.1 Applying MEANT’s f-score within
semantic role fillers
</subsectionHeader>
<bodyText confidence="0.99793265">
The first natural approach is to extend MEANT’s
f-score based method of aggregating semantic
parse accuracy, so as to also apply to aggregat-
ing lexical translation probabilities within seman-
tic role filler phrases. However, since we are miss-
ing structure information within the flat role filler
phrases, we can no longer assume an injective
mapping for aligning the tokens of the role fillers
between the foreign input and the MT output. We
therefore relax the assumption and thus for cross-
lingual phrasal precision/recall, we align each to-
ken of the role fillers in the output/input string
to the token of the role fillers in the input/output
string that has the maximum lexical translation
probability. The precise definition of the cross-
lingual phrasal similarities is as follows:
ei,pred ≡ the output side of the pred of aligned frame i
fi,pred ≡ the input side of the pred of aligned frame i
ei,j ≡ the output side of the ARG j of aligned frame i
fi,j ≡ the input side of the ARG j of aligned frame i
</bodyText>
<equation confidence="0.977402166666667">
√
p(e, f) = t (e|f) t (f|e)
∑
eEe max
fEf p(e, f)
|e|
∑fEf ma- p(e, f)
rece,f = |f|
2 · precei,pred,fi,pred · recei,pred,fi,pred
precei,pred,fi,pred + recei,pred,fi,pred
2 · precei,j,fi,j · recei,j,fi,j
precei,j ,fi,j + recei,j ,fi,j
</equation>
<bodyText confidence="0.999986375">
where the joint probability p is defined as the har-
monized the two directions of the translation table
t trained using IBM model 1 (Brown et al., 1993).
prece,f is the precision and rece,f is the recall of
the phrasal similarities of the role fillers. si,pred
and si,j are the f-scores of the phrasal similarities
of the predicates and role fillers of the arguments
of type j between the input and the MT output.
</bodyText>
<subsectionHeader confidence="0.9478355">
3.2 Applying MEANT’s ITG bias within
semantic role fillers
</subsectionHeader>
<bodyText confidence="0.999992166666667">
The second natural approach is to extend
MEANT’s ITG bias on compositional reorder-
ing, so as to also apply to aggregating lexical
translation probabilities within semantic role filler
phrases. Addanki et al. (2012) showed empiri-
cally that cross-lingual semantic role reordering of
the kind that MEANT is based upon is fully cov-
ered within ITG constraints. In Wu et al. (2014),
we extend ITG constraints into aligning the tokens
within the semantic role fillers within monolingual
MEANT, thus replacing its previous monolingual
phrasal aggregation heuristic. Here we borrow the
</bodyText>
<equation confidence="0.996410666666667">
prece,f =
si,pred =
si,j =
</equation>
<page confidence="0.969979">
768
</page>
<bodyText confidence="0.99794025">
idea for the cross-lingual case, using the length-
normalized inside probability at the root of a BITG
biparse (Wu, 1997; Zens and Ney, 2003; Saers and
Wu, 2009) as follows:
</bodyText>
<equation confidence="0.4798044">
G ({A} , W°, W1, R, A)
R {A --+ [AA] , A --+ (AA), A --+ e/f}
p ([AA] JA) = p ((AA)JA) = 0.25
1 p (e/f IA) = 2 √t (elf) t (f le)
1
</equation>
<tableCaption confidence="0.873186">
Table 1: Sentence-level correlation with HAJ
</tableCaption>
<table confidence="0.9669099375">
(GALE phase 2.5 evaluation data)
Metric Kendall
HMEANT 0.53
XMEANT (BITG) 0.51
MEANT (f-score) 0.48
XMEANT (f-score) 0.46
MEANT (2013) 0.46
NIST 0.29
BLEU/METEOR/TER/PER 0.20
CDER 0.12
WER 0.10
si,pred =
1 ( (A � ))
ln P ⇒ei,pred/fi,pred|G
max(|ei,pred|,|fi,pred|) 5 Conclusion
1
</table>
<equation confidence="0.99730825">
si,7 =
( (A � ))
ln P ⇒ei,j/fi,j|G
max(|ei,j |,|fi,j |)
</equation>
<bodyText confidence="0.999964642857143">
where G is a bracketing ITG, whose only nonter-
minal is A, and where R is a set of transduction
rules where e E W0 U {E} is an output token
(or the null token), and f E W1 U {E} is an in-
put token (or the null token). The rule probabil-
ity function p is defined using fixed probabilities
for the structural rules, and a translation table t
trained using IBM model 1 in both directions. To
calculate the inside probability of a pair of seg-
ments, P (A =*=&gt;. e/f |G) , we use the algorithm de-
scribed in Saers et al. (2009). si,pred and si,� are
the length normalized BITG parsing probabilities
of the predicates and role fillers of the arguments
of type j between the input and the MT output.
</bodyText>
<sectionHeader confidence="0.999971" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.99964469047619">
Table 1 shows that for human adequacy judgments
at the sentence level, the f-score based XMEANT
(1) correlates significantly more closely than other
commonly used monolingual automatic MT eval-
uation metrics, and (2) even correlates nearly as
well as monolingual MEANT. This suggests that
the semantic structure of the MT output is indeed
closer to that of the input sentence than that of the
reference translation.
Furthermore, the ITG-based XMEANT (1) sig-
nificantly outperforms MEANT, and (2) is an au-
tomatic metric that is nearly as accurate as the
HMEANT human subjective version. This indi-
cates that BITG constraints indeed provide a more
robust token alignment compared to the heuris-
tics previously employed in MEANT. It is also
consistent with results observed while estimating
word alignment probabilities, where BITG con-
straints outperformed alignments from GIZA++
(Saers and Wu, 2009).
We have presented XMEANT, a new cross-lingual
variant of MEANT, that correlates even more
closely with human translation adequacy judg-
ments than MEANT, without the expensive human
references. This is (1) accomplished by replacing
monolingual MEANT’s context vector model with
simple translation probabilities when computing
similarities of semantic role fillers, and (2) fur-
ther improved by incorporating BITG constraints
for aligning the tokens in semantic role fillers.
While monolingual MEANT alone accurately re-
flects adequacy via semantic frames and optimiz-
ing SMT against MEANT improves translation,
the new cross-lingual XMEANT semantic objec-
tive function moves closer toward deep integration
of semantics into the MT training pipeline.
The phrasal similarity scoring has only been
minimally adapted to cross-lingual semantic role
fillers in this first study of XMEANT. We expect
further improvements to XMEANT, but these first
results already demonstrate XMEANT’s potential
to drive research progress toward semantic SMT.
</bodyText>
<sectionHeader confidence="0.998697" genericHeader="conclusions">
6 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999371214285714">
This material is based upon work supported
in part by the Defense Advanced Research
Projects Agency (DARPA) under BOLT con-
tract nos. HR0011-12-C-0014 and HR0011-12-
C-0016, and GALE contract nos. HR0011-06-C-
0022 and HR0011-06-C-0023; by the European
Union under the FP7 grant agreement no. 287658;
and by the Hong Kong Research Grants Council
(RGC) research grants GRF620811, GRF621008,
and GRF612806. Any opinions, findings and
conclusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of DARPA, the EU, or
RGC.
</bodyText>
<page confidence="0.789792">
1
769
</page>
<sectionHeader confidence="0.980945" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.952513341880342">
Karteek Addanki, Chi-Kiu Lo, Markus Saers, and
Dekai Wu. LTG vs. ITG coverage of cross-lingual
verb frame alternations. In 16th Annual Conference
of the European Association for Machine Transla-
tion (EAMT-2012), Trento, Italy, May 2012.
Hala Almaghout and Lucia Specia. A CCG-based qual-
ity estimation metric for statistical machine transla-
tion. In Machine Translation Summit XIV (MT Sum-
mit 2013), Nice, France, 2013.
Eleftherios Avramidis and Maja Popovi´c. Machine
learning methods for comparative and time-oriented
quality estimation of machine translation output. In
8th Workshop on Statistical Machine Translation
(WMT 2013), 2013.
Eleftherios Avramidis. Quality estimation for machine
translation output using linguistic analysis and de-
coding features. In 7th Workshop on Statistical Ma-
chine Translation (WMT 2012), 2012.
Satanjeev Banerjee and Alon Lavie. METEOR: An
automatic metric for MT evaluation with improved
correlation with human judgments. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, Ann Ar-
bor, Michigan, June 2005.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. Confidence estimation
for machine translation. In 20th international con-
ference on Computational Linguistics, 2004.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. The mathemat-
ics of machine translation: Parameter estimation.
Computational Linguistics, 19(2):263–311, 1993.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. Re-evaluating the role of BLEU in machine
translation research. In 11th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL-2006), 2006.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. (meta-)
evaluation of machine translation. In Second Work-
shop on Statistical Machine Translation (WMT-07),
2007.
Chris Callison-Burch, Cameron Fordyce, Philipp
Koehn, Christof Monz, and Josh Schroeder. Further
meta-evaluation of machine translation. In Third
Workshop on Statistical Machine Translation (WMT-
08), 2008.
Julio Castillo and Paula Estrella. Semantic textual sim-
ilarity for MT evaluation. In 7th Workshop on Sta-
tistical Machine Translation (WMT 2012), 2012.
George Doddington. Automatic evaluation of machine
translation quality using n-gram co-occurrence
statistics. In The second international conference on
Human Language Technology Research (HLT ’02),
San Diego, California, 2002.
Simona Gandrabur, George Foster, and Guy Lapalme.
Confidence estimation for nlp applications. ACM
Transactions on Speech and Language Processing,
2006.
Jes´us Gim´enez and Llu´ıs M`arquez. Linguistic features
for automatic evaluation of heterogenous MT sys-
tems. In Second Workshop on Statistical Machine
Translation (WMT-07), pages 256–264, Prague,
Czech Republic, June 2007.
Jes´us Gim´enez and Llu´ıs M`arquez. A smorgasbord
of features for automatic MT evaluation. In Third
Workshop on Statistical Machine Translation (WMT-
08), Columbus, Ohio, June 2008.
Yifan He, Yanjun Ma, Andy Way, and Josef van
Genabith. Rich linguistic features for translation
memory-inspired consistent translation. In 13th Ma-
chine Translation Summit (MT Summit XIII), 2011.
Philipp Koehn and Christof Monz. Manual and auto-
matic evaluation of machine translation between eu-
ropean languages. In Workshop on Statistical Ma-
chine Translation (WMT-06), 2006.
Gregor Leusch, Nicola Ueffing, and Hermann Ney.
CDer: Efficient MT evaluation using block move-
ments. In 11th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL-2006), 2006.
Ding Liu and Daniel Gildea. Syntactic features for
evaluation of machine translation. In Workshop on
Intrinsic and Extrinsic Evaluation Measures for Ma-
chine Translation and/or Summarization, Ann Ar-
bor, Michigan, June 2005.
Chi-kiu Lo and Dekai Wu. MEANT: An inexpensive,
high-accuracy, semi-automatic metric for evaluating
translation utility based on semantic roles. In 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies
(ACL HLT 2011), 2011.
Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How
semantic frames evaluate MT more accurately. In
Twenty-second International Joint Conference on
Artificial Intelligence (IJCAI-11), 2011.
Chi-kiu Lo and Dekai Wu. Unsupervised vs. super-
vised weight estimation for semantic MT evalua-
tion metrics. In Sixth Workshop on Syntax, Seman-
tics and Structure in Statistical Translation (SSST-
6), 2012.
Chi-kiu Lo and Dekai Wu. Can informal genres be
better translated by tuning on automatic semantic
metrics? In 14th Machine Translation Summit (MT
Summit XIV), 2013.
Chi-kiu Lo and Dekai Wu. MEANT at WMT 2013:
A tunable, accurate yet inexpensive semantic frame
based mt evaluation metric. In 8th Workshop on Sta-
tistical Machine Translation (WMT 2013), 2013.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.
Fully automatic semantic MT evaluation. In 7th
Workshop on Statistical Machine Translation (WMT
2012), 2012.
Chi-kiu Lo, Karteek Addanki, Markus Saers, and
Dekai Wu. Improving machine translation by train-
ing against an automatic semantic frame based eval-
</reference>
<page confidence="0.984308">
770
</page>
<reference confidence="0.999246051282052">
uation metric. In 51st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2013),
2013.
Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Im-
proving machine translation into Chinese by tun-
ing against Chinese MEANT. In International
Workshop on Spoken Language Translation (IWSLT
2013), 2013.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. Consistent translation using discriminative
learning: a translation memory-inspired approach.
In 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies (ACL HLT 2011). Association for Computa-
tional Linguistics, 2011.
Matous&amp;quot; Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. Results of the
WMT13 metrics shared task. In Eighth Workshop on
Statistical Machine Translation (WMT 2013), Sofia,
Bulgaria, August 2013.
Yashar Mehdad, Matteo Negri, and Marcello Federico.
Match without a referee: evaluating mt adequacy
without reference translations. In 7th Workshop on
Statistical Machine Translation (WMT 2012), 2012.
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. Corpus-based and knowledge-based measures
of text semantic similarity. In The Twenty-first Na-
tional Conference on Artificial Intelligence (AAAI-
06), volume 21. Menlo Park, CA; Cambridge, MA;
London; AAAI Press; MIT Press; 1999, 2006.
Sonja Nießen, Franz Josef Och, Gregor Leusch, and
Hermann Ney. A evaluation tool for machine trans-
lation: Fast evaluation for MT research. In The
Second International Conference on Language Re-
sources and Evaluation (LREC 2000), 2000.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. Dependency-based automatic evaluation for
machine translation. In Syntax and Structure in Sta-
tistical Translation (SSST), 2007.
Karolina Owczarzak, Josef van Genabith, and Andy
Way. Evaluating machine translation with LFG de-
pendencies. Machine Translation, 21:95–119, 2007.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. BLEU: a method for automatic evalua-
tion of machine translation. In 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-02), pages 311–318, Philadelphia, Pennsylva-
nia, July 2002.
Sameer Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Dan Jurafsky. Shallow seman-
tic parsing using support vector machines. In Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (HLT-NAACL 2004), 2004.
Christopher Quirk. Training a sentence-level machine
translation confidence measure. In Fourth Interna-
tional Conference on Language Resources and Eval-
uation (LREC 2004), Lisbon, Portugal, May 2004.
Miguel Rios, Wilker Aziz, and Lucia Specia. Tine: A
metric to assess MT adequacy. In Sixth Workshop on
Statistical Machine Translation (WMT 2011), 2011.
Markus Saers and Dekai Wu. Improving phrase-based
translation via word alignments from stochastic in-
version transduction grammars. In Third Workshop
on Syntax and Structure in Statistical Translation
(SSST-3), Boulder, Colorado, June 2009.
Markus Saers, Joakim Nivre, and Dekai Wu. Learning
stochastic bracketing inversion transduction gram-
mars with a cubic time biparsing algorithm. In 11th
International Conference on Parsing Technologies
(IWPT’09), Paris, France, October 2009.
Kashif Shah, Trevor Cohn, and Lucia Specia. An in-
vestigation on the effectiveness of features for trans-
lation quality estimation. In Machine Translation
Summit XIV (MT Summit 2013), Nice, France, 2013.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. A study of trans-
lation edit rate with targeted human annotation. In
7th Biennial Conference Association for Machine
Translation in the Americas (AMTA 2006), pages
223–231, Cambridge, Massachusetts, August 2006.
Lucia Specia. Exploiting objective annotations for
measuring translation post-editing effort. In 15th
Conference of the European Association for Ma-
chine Translation, pages 73–80, 2011.
Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu.
Accuracy and robustness in measuring the lexical
similarity of semantic role fillers for automatic se-
mantic MT evaluation. In 26th Pacific Asia Confer-
ence on Language, Information, and Computation
(PACLIC 26), 2012.
Nicola Ueffing and Hermann Ney. Word-level con-
fidence estimation for machine translation using
phrase-based translation models. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 763–770, 2005.
Mengqiu Wang and Christopher D. Manning. SPEDE:
Probabilistic edit distance metrics for MT evalua-
tion. In 7th Workshop on Statistical Machine Trans-
lation (WMT 2012), 2012.
Dekai Wu, Chi-kiu Lo, Meriem Beloucif, and Markus
Saers. IMEANT: Improving semantic frame based
MT evaluation via inversion transduction grammars.
Forthcoming, 2014.
Dekai Wu. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23(3):377–403, 1997.
Deyi Xiong, Min Zhang, and Haizhou Li. Error detec-
tion for statistical machine translation using linguis-
tic features. In 48th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2010),
2010.
Richard Zens and Hermann Ney. A comparative
study on reordering constraints in statistical machine
translation. In 41st Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL-2003),
pages 144–151, Stroudsburg, Pennsylvania, 2003.
</reference>
<page confidence="0.997939">
771
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.345245">
<title confidence="0.993863">XMEANT: Better semantic MT evaluation without reference translations</title>
<author confidence="0.98777">Chi-kiu Beloucif Lo</author>
<author confidence="0.98777">Meriem Saers</author>
<author confidence="0.98777">Markus Wu</author>
<affiliation confidence="0.779313">Human Language Technology Department of Computer Science and</affiliation>
<author confidence="0.804896">Hong Kong University of Science</author>
<author confidence="0.804896">Technology</author>
<abstract confidence="0.988441227272727">introduce XMEANT—a new version of the semantic frame based MT evaluation metric MEANT—which can correlate even more closely with human adequacy judgments than monolingual MEANT and eliminates the need for expensive human references. Previous work established that MEANT reflects translation adequacy with state-of-the-art accuracy, and optimizing MT systems against MEANT robustly improves translation quality. However, to go beyond tuning weights in the loglinear SMT model, a cross-lingual objective function that can deeply integrate semantic frame criteria into the MT training pipeline is needed. We show that cross-lingual XMEANT outperforms monolingual MEANT by (1) replacing the monolingual context vector model in MEANT with simple translation probabilities, and (2) incorporating bracketing ITG constraints.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Karteek Addanki</author>
<author>Chi-Kiu Lo</author>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>LTG vs. ITG coverage of cross-lingual verb frame alternations.</title>
<date>2012</date>
<booktitle>In 16th Annual Conference of the European Association for Machine Translation (EAMT-2012),</booktitle>
<location>Trento, Italy,</location>
<contexts>
<context position="14068" citStr="Addanki et al. (2012)" startWordPosition="2249" endWordPosition="2252">o directions of the translation table t trained using IBM model 1 (Brown et al., 1993). prece,f is the precision and rece,f is the recall of the phrasal similarities of the role fillers. si,pred and si,j are the f-scores of the phrasal similarities of the predicates and role fillers of the arguments of type j between the input and the MT output. 3.2 Applying MEANT’s ITG bias within semantic role fillers The second natural approach is to extend MEANT’s ITG bias on compositional reordering, so as to also apply to aggregating lexical translation probabilities within semantic role filler phrases. Addanki et al. (2012) showed empirically that cross-lingual semantic role reordering of the kind that MEANT is based upon is fully covered within ITG constraints. In Wu et al. (2014), we extend ITG constraints into aligning the tokens within the semantic role fillers within monolingual MEANT, thus replacing its previous monolingual phrasal aggregation heuristic. Here we borrow the prece,f = si,pred = si,j = 768 idea for the cross-lingual case, using the lengthnormalized inside probability at the root of a BITG biparse (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009) as follows: G ({A} , W°, W1, R, A) R {A --+ [A</context>
</contexts>
<marker>Addanki, Lo, Saers, Wu, 2012</marker>
<rawString>Karteek Addanki, Chi-Kiu Lo, Markus Saers, and Dekai Wu. LTG vs. ITG coverage of cross-lingual verb frame alternations. In 16th Annual Conference of the European Association for Machine Translation (EAMT-2012), Trento, Italy, May 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hala Almaghout</author>
<author>Lucia Specia</author>
</authors>
<title>A CCG-based quality estimation metric for statistical machine translation.</title>
<date>2013</date>
<booktitle>In Machine Translation Summit XIV (MT Summit 2013),</booktitle>
<location>Nice, France,</location>
<contexts>
<context position="10640" citStr="Almaghout and Specia, 2013" startWordPosition="1702" endWordPosition="1705">ssify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learn767 Figure 3: Cross-lingual XMEANT algorithm. ing algorithms for predicting MT quality. Although the feature-based QE system of Avramidis and Popovi´c (2013) slightly outperformed METEOR on correlation with human adequacy judgment, these “black box” approaches typically lack representational transparency, require expensive running time, and/or must be discriminatively retrained for each language and text type. 3 XMEANT: a cross-lingual MEANT Like MEANT, XMEANT aims </context>
</contexts>
<marker>Almaghout, Specia, 2013</marker>
<rawString>Hala Almaghout and Lucia Specia. A CCG-based quality estimation metric for statistical machine translation. In Machine Translation Summit XIV (MT Summit 2013), Nice, France, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleftherios Avramidis</author>
<author>Maja Popovi´c</author>
</authors>
<title>Machine learning methods for comparative and time-oriented quality estimation of machine translation output.</title>
<date>2013</date>
<booktitle>In 8th Workshop on Statistical Machine Translation (WMT</booktitle>
<marker>Avramidis, Popovi´c, 2013</marker>
<rawString>Eleftherios Avramidis and Maja Popovi´c. Machine learning methods for comparative and time-oriented quality estimation of machine translation output. In 8th Workshop on Statistical Machine Translation (WMT 2013), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleftherios Avramidis</author>
</authors>
<title>Quality estimation for machine translation output using linguistic analysis and decoding features.</title>
<date>2012</date>
<booktitle>In 7th Workshop on Statistical Machine Translation (WMT</booktitle>
<contexts>
<context position="10591" citStr="Avramidis, 2012" startWordPosition="1696" endWordPosition="1697"> an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learn767 Figure 3: Cross-lingual XMEANT algorithm. ing algorithms for predicting MT quality. Although the feature-based QE system of Avramidis and Popovi´c (2013) slightly outperformed METEOR on correlation with human adequacy judgment, these “black box” approaches typically lack representational transparency, require expensive running time, and/or must be discriminatively retrained for each language and text type. 3 XMEAN</context>
</contexts>
<marker>Avramidis, 2012</marker>
<rawString>Eleftherios Avramidis. Quality estimation for machine translation output using linguistic analysis and decoding features. In 7th Workshop on Statistical Machine Translation (WMT 2012), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="4534" citStr="Banerjee and Lavie, 2005" startWordPosition="689" endWordPosition="692">s), pages 765–771, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency s</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Ann Arbor, Michigan, June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Blatz</author>
<author>Erin Fitzgerald</author>
<author>George Foster</author>
<author>Simona Gandrabur</author>
<author>Cyril Goutte</author>
<author>Alex Kulesza</author>
<author>Alberto Sanchis</author>
<author>Nicola Ueffing</author>
</authors>
<title>Confidence estimation for machine translation.</title>
<date>2004</date>
<booktitle>In 20th international conference on Computational Linguistics,</booktitle>
<contexts>
<context position="9933" citStr="Blatz et al. (2004)" startWordPosition="1582" endWordPosition="1585">equate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et</context>
</contexts>
<marker>Blatz, Fitzgerald, Foster, Gandrabur, Goutte, Kulesza, Sanchis, Ueffing, 2004</marker>
<rawString>John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, and Nicola Ueffing. Confidence estimation for machine translation. In 20th international conference on Computational Linguistics, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="13533" citStr="Brown et al., 1993" startWordPosition="2161" endWordPosition="2164">as follows: ei,pred ≡ the output side of the pred of aligned frame i fi,pred ≡ the input side of the pred of aligned frame i ei,j ≡ the output side of the ARG j of aligned frame i fi,j ≡ the input side of the ARG j of aligned frame i √ p(e, f) = t (e|f) t (f|e) ∑ eEe max fEf p(e, f) |e| ∑fEf ma- p(e, f) rece,f = |f| 2 · precei,pred,fi,pred · recei,pred,fi,pred precei,pred,fi,pred + recei,pred,fi,pred 2 · precei,j,fi,j · recei,j,fi,j precei,j ,fi,j + recei,j ,fi,j where the joint probability p is defined as the harmonized the two directions of the translation table t trained using IBM model 1 (Brown et al., 1993). prece,f is the precision and rece,f is the recall of the phrasal similarities of the role fillers. si,pred and si,j are the f-scores of the phrasal similarities of the predicates and role fillers of the arguments of type j between the input and the MT output. 3.2 Applying MEANT’s ITG bias within semantic role fillers The second natural approach is to extend MEANT’s ITG bias on compositional reordering, so as to also apply to aggregating lexical translation probabilities within semantic role filler phrases. Addanki et al. (2012) showed empirically that cross-lingual semantic role reordering o</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. The mathematics of machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006),</booktitle>
<contexts>
<context position="4772" citStr="Callison-Burch et al., 2006" startWordPosition="729" endWordPosition="732">ing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic ev</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of BLEU in machine translation research. In 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>(meta-) evaluation of machine translation.</title>
<date>2007</date>
<booktitle>In Second Workshop on Statistical Machine Translation (WMT-07),</booktitle>
<contexts>
<context position="5671" citStr="Callison-Burch et al., 2007" startWordPosition="868" endWordPosition="871">fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the reference via an integrated probabilistic FSM and PDA model. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain several dozens of parameters to tune, and employ expensive linguistic resources like WordNet Figure 1: M</context>
</contexts>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2007</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. (meta-) evaluation of machine translation. In Second Workshop on Statistical Machine Translation (WMT-07), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Cameron Fordyce</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<title>Further meta-evaluation of machine translation.</title>
<date>2008</date>
<booktitle>In Third Workshop on Statistical Machine Translation (WMT08),</booktitle>
<marker>Callison-Burch, Fordyce, Koehn, Monz, Schroeder, 2008</marker>
<rawString>Chris Callison-Burch, Cameron Fordyce, Philipp Koehn, Christof Monz, and Josh Schroeder. Further meta-evaluation of machine translation. In Third Workshop on Statistical Machine Translation (WMT08), 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julio Castillo</author>
<author>Paula Estrella</author>
</authors>
<title>Semantic textual similarity for MT evaluation.</title>
<date>2012</date>
<booktitle>In 7th Workshop on Statistical Machine Translation (WMT</booktitle>
<contexts>
<context position="5997" citStr="Castillo and Estrella, 2012" startWordPosition="929" endWordPosition="932"> event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the reference via an integrated probabilistic FSM and PDA model. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain several dozens of parameters to tune, and employ expensive linguistic resources like WordNet Figure 1: Monolingual MEANT algorithm. or paraphrase tables; the expensive training, tuning, and/or running time makes them hard to incorporate into the MT development cycle. 2.2 The MEANT family of metrics MEANT (Lo et al., 2012), which is the weighted fscore over the matched semantic role labels of the automatically aligned semantic </context>
</contexts>
<marker>Castillo, Estrella, 2012</marker>
<rawString>Julio Castillo and Paula Estrella. Semantic textual similarity for MT evaluation. In 7th Workshop on Statistical Machine Translation (WMT 2012), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In The second international conference on Human Language Technology Research (HLT ’02),</booktitle>
<location>San Diego, California,</location>
<contexts>
<context position="4499" citStr="Doddington, 2002" startWordPosition="686" endWordPosition="687">al Linguistics (Short Papers), pages 765–771, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach o</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In The second international conference on Human Language Technology Research (HLT ’02), San Diego, California, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simona Gandrabur</author>
<author>George Foster</author>
<author>Guy Lapalme</author>
</authors>
<title>Confidence estimation for nlp applications.</title>
<date>2006</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<contexts>
<context position="9773" citStr="Gandrabur et al. (2006)" startWordPosition="1554" endWordPosition="1558">sed aggregation functions. Recent studies (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) show that tuning MT systems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human</context>
</contexts>
<marker>Gandrabur, Foster, Lapalme, 2006</marker>
<rawString>Simona Gandrabur, George Foster, and Guy Lapalme. Confidence estimation for nlp applications. ACM Transactions on Speech and Language Processing, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Linguistic features for automatic evaluation of heterogenous MT systems.</title>
<date>2007</date>
<booktitle>In Second Workshop on Statistical Machine Translation (WMT-07),</booktitle>
<pages>256--264</pages>
<location>Prague, Czech Republic,</location>
<marker>Gim´enez, M`arquez, 2007</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. Linguistic features for automatic evaluation of heterogenous MT systems. In Second Workshop on Statistical Machine Translation (WMT-07), pages 256–264, Prague, Czech Republic, June 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>A smorgasbord of features for automatic MT evaluation.</title>
<date>2008</date>
<booktitle>In Third Workshop on Statistical Machine Translation (WMT08),</booktitle>
<location>Columbus, Ohio,</location>
<marker>Gim´enez, M`arquez, 2008</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. A smorgasbord of features for automatic MT evaluation. In Third Workshop on Statistical Machine Translation (WMT08), Columbus, Ohio, June 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yifan He</author>
<author>Yanjun Ma</author>
<author>Andy Way</author>
<author>Josef van Genabith</author>
</authors>
<title>Rich linguistic features for translation memory-inspired consistent translation.</title>
<date>2011</date>
<booktitle>In 13th Machine Translation Summit (MT Summit XIII),</booktitle>
<marker>He, Ma, Way, van Genabith, 2011</marker>
<rawString>Yifan He, Yanjun Ma, Andy Way, and Josef van Genabith. Rich linguistic features for translation memory-inspired consistent translation. In 13th Machine Translation Summit (MT Summit XIII), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between european languages.</title>
<date>2006</date>
<booktitle>In Workshop on Statistical Machine Translation (WMT-06),</booktitle>
<contexts>
<context position="4795" citStr="Koehn and Monz, 2006" startWordPosition="733" endWordPosition="736">T assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it pe</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Philipp Koehn and Christof Monz. Manual and automatic evaluation of machine translation between european languages. In Workshop on Statistical Machine Translation (WMT-06), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregor Leusch</author>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>CDer: Efficient MT evaluation using block movements.</title>
<date>2006</date>
<booktitle>In 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006),</booktitle>
<contexts>
<context position="4562" citStr="Leusch et al., 2006" startWordPosition="694" endWordPosition="697">yland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed</context>
</contexts>
<marker>Leusch, Ueffing, Ney, 2006</marker>
<rawString>Gregor Leusch, Nicola Ueffing, and Hermann Ney. CDer: Efficient MT evaluation using block movements. In 11th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2006), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="5187" citStr="Liu and Gildea (2005)" startWordPosition="796" endWordPosition="799">R (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. Syntactic features for evaluation of machine translation. In Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, Ann Arbor, Michigan, June 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT</booktitle>
<contexts>
<context position="2369" citStr="Lo and Wu, 2011" startWordPosition="349" endWordPosition="352">lates more closely with human adequacy judgement than MEANT is that on the one hand, the semantic structure of the MT output is closer to that of the input sentence than that of the reference translation, and on the other hand, the BITG constraints the word alignment more accurately than the heuristic bag-ofword aggregation used in MEANT. Our results suggest that MT translation adequacy is more accurately evaluated via the cross-lingual semantic frame similarities of the input and the MT output which may obviate the need for expensive human reference translations. The MEANT family of metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012) adopt the principle that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metri</context>
<context position="7906" citStr="Lo and Wu (2011" startWordPosition="1247" endWordPosition="1250">ning of the sentence. wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT out766 Figure 2: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic shallow semantic parser. The reference and MT output are parsed by an English automatic shallow semantic parser. There are no semantic frames for MT3 since the system decided to drop the predicate. put. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,� are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT out</context>
</contexts>
<marker>Lo, Wu, 2011</marker>
<rawString>Chi-kiu Lo and Dekai Wu. MEANT: An inexpensive, high-accuracy, semi-automatic metric for evaluating translation utility based on semantic roles. In 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT 2011), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>SMT vs. AI redux: How semantic frames evaluate MT more accurately.</title>
<date>2011</date>
<booktitle>In Twenty-second International Joint Conference on Artificial Intelligence (IJCAI-11),</booktitle>
<contexts>
<context position="2369" citStr="Lo and Wu, 2011" startWordPosition="349" endWordPosition="352">lates more closely with human adequacy judgement than MEANT is that on the one hand, the semantic structure of the MT output is closer to that of the input sentence than that of the reference translation, and on the other hand, the BITG constraints the word alignment more accurately than the heuristic bag-ofword aggregation used in MEANT. Our results suggest that MT translation adequacy is more accurately evaluated via the cross-lingual semantic frame similarities of the input and the MT output which may obviate the need for expensive human reference translations. The MEANT family of metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012) adopt the principle that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metri</context>
<context position="7906" citStr="Lo and Wu (2011" startWordPosition="1247" endWordPosition="1250">ning of the sentence. wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT out766 Figure 2: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic shallow semantic parser. The reference and MT output are parsed by an English automatic shallow semantic parser. There are no semantic frames for MT3 since the system decided to drop the predicate. put. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,� are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT out</context>
</contexts>
<marker>Lo, Wu, 2011</marker>
<rawString>Chi-kiu Lo and Dekai Wu. SMT vs. AI redux: How semantic frames evaluate MT more accurately. In Twenty-second International Joint Conference on Artificial Intelligence (IJCAI-11), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>Unsupervised vs. supervised weight estimation for semantic MT evaluation metrics.</title>
<date>2012</date>
<booktitle>In Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST6),</booktitle>
<contexts>
<context position="8100" citStr="Lo and Wu, 2012" startWordPosition="1280" endWordPosition="1283">the MT out766 Figure 2: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic shallow semantic parser. The reference and MT output are parsed by an English automatic shallow semantic parser. There are no semantic frames for MT3 since the system decided to drop the predicate. put. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,� are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function in</context>
</contexts>
<marker>Lo, Wu, 2012</marker>
<rawString>Chi-kiu Lo and Dekai Wu. Unsupervised vs. supervised weight estimation for semantic MT evaluation metrics. In Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST6), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>Can informal genres be better translated by tuning on automatic semantic metrics?</title>
<date>2013</date>
<booktitle>In 14th Machine Translation Summit (MT Summit XIV),</booktitle>
<contexts>
<context position="3029" citStr="Lo and Wu, 2013" startWordPosition="460" endWordPosition="463"> that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´aˇcek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of inp</context>
<context position="8880" citStr="Lo and Wu, 2013" startWordPosition="1411" endWordPosition="1414">on adequacy of the development set are unavailable. si,pred and si,� are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we employ a newer version of MEANT that uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers, as our experiments indicate this is more accurate than the previously used aggregation functions. Recent studies (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) show that tuning MT systems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and inf</context>
</contexts>
<marker>Lo, Wu, 2013</marker>
<rawString>Chi-kiu Lo and Dekai Wu. Can informal genres be better translated by tuning on automatic semantic metrics? In 14th Machine Translation Summit (MT Summit XIV), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame based mt evaluation metric.</title>
<date>2013</date>
<booktitle>In 8th Workshop on Statistical Machine Translation (WMT</booktitle>
<contexts>
<context position="3029" citStr="Lo and Wu, 2013" startWordPosition="460" endWordPosition="463"> that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´aˇcek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of inp</context>
<context position="8880" citStr="Lo and Wu, 2013" startWordPosition="1411" endWordPosition="1414">on adequacy of the development set are unavailable. si,pred and si,� are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we employ a newer version of MEANT that uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers, as our experiments indicate this is more accurate than the previously used aggregation functions. Recent studies (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) show that tuning MT systems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and inf</context>
</contexts>
<marker>Lo, Wu, 2013</marker>
<rawString>Chi-kiu Lo and Dekai Wu. MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame based mt evaluation metric. In 8th Workshop on Statistical Machine Translation (WMT 2013), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
</authors>
<title>Anand Karthik Tumuluru, and Dekai Wu. Fully automatic semantic MT evaluation.</title>
<date>2012</date>
<booktitle>In 7th Workshop on Statistical Machine Translation (WMT</booktitle>
<marker>Lo, 2012</marker>
<rawString>Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. Fully automatic semantic MT evaluation. In 7th Workshop on Statistical Machine Translation (WMT 2012), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Karteek Addanki</author>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>Improving machine translation by training against an automatic semantic frame based evaluation metric.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="3324" citStr="Lo et al., 2013" startWordPosition="506" endWordPosition="509">ranslations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´aˇcek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of input and output sentences. We therefore propose XMEANT, a cross-lingual MT evaluation metric, that modifies MEANT using (1) simple translation probabilities (in our experiments, 765 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 765–7</context>
<context position="8862" citStr="Lo et al., 2013" startWordPosition="1407" endWordPosition="1410">n human judgments on adequacy of the development set are unavailable. si,pred and si,� are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we employ a newer version of MEANT that uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers, as our experiments indicate this is more accurate than the previously used aggregation functions. Recent studies (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) show that tuning MT systems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web </context>
</contexts>
<marker>Lo, Addanki, Saers, Wu, 2013</marker>
<rawString>Chi-kiu Lo, Karteek Addanki, Markus Saers, and Dekai Wu. Improving machine translation by training against an automatic semantic frame based evaluation metric. In 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chi-kiu Lo</author>
<author>Meriem Beloucif</author>
<author>Dekai Wu</author>
</authors>
<title>Improving machine translation into Chinese by tuning against Chinese MEANT.</title>
<date>2013</date>
<booktitle>In International Workshop on Spoken Language Translation (IWSLT</booktitle>
<contexts>
<context position="3324" citStr="Lo et al., 2013" startWordPosition="506" endWordPosition="509">ranslations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´aˇcek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of input and output sentences. We therefore propose XMEANT, a cross-lingual MT evaluation metric, that modifies MEANT using (1) simple translation probabilities (in our experiments, 765 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 765–7</context>
<context position="8862" citStr="Lo et al., 2013" startWordPosition="1407" endWordPosition="1410">n human judgments on adequacy of the development set are unavailable. si,pred and si,� are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we employ a newer version of MEANT that uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers, as our experiments indicate this is more accurate than the previously used aggregation functions. Recent studies (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) show that tuning MT systems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web </context>
</contexts>
<marker>Lo, Beloucif, Wu, 2013</marker>
<rawString>Chi-kiu Lo, Meriem Beloucif, and Dekai Wu. Improving machine translation into Chinese by tuning against Chinese MEANT. In International Workshop on Spoken Language Translation (IWSLT 2013), 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanjun Ma</author>
<author>Yifan He</author>
<author>Andy Way</author>
<author>Josef van Genabith</author>
</authors>
<title>Consistent translation using discriminative learning: a translation memory-inspired approach.</title>
<date>2011</date>
<booktitle>In 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT 2011). Association for Computational Linguistics,</booktitle>
<marker>Ma, He, Way, van Genabith, 2011</marker>
<rawString>Yanjun Ma, Yifan He, Andy Way, and Josef van Genabith. Consistent translation using discriminative learning: a translation memory-inspired approach. In 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL HLT 2011). Association for Computational Linguistics, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matous Mach´acek</author>
<author>Ondrej Bojar</author>
</authors>
<title>Results of the WMT13 metrics shared task.</title>
<date>2013</date>
<booktitle>In Eighth Workshop on Statistical Machine Translation (WMT 2013),</booktitle>
<location>Sofia, Bulgaria,</location>
<marker>Mach´acek, Bojar, 2013</marker>
<rawString>Matous&amp;quot; Mach´a&amp;quot;cek and Ond&amp;quot;rej Bojar. Results of the WMT13 metrics shared task. In Eighth Workshop on Statistical Machine Translation (WMT 2013), Sofia, Bulgaria, August 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Match without a referee: evaluating mt adequacy without reference translations.</title>
<date>2012</date>
<booktitle>In 7th Workshop on Statistical Machine Translation (WMT</booktitle>
<contexts>
<context position="10612" citStr="Mehdad et al., 2012" startWordPosition="1698" endWordPosition="1701">eshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learn767 Figure 3: Cross-lingual XMEANT algorithm. ing algorithms for predicting MT quality. Although the feature-based QE system of Avramidis and Popovi´c (2013) slightly outperformed METEOR on correlation with human adequacy judgment, these “black box” approaches typically lack representational transparency, require expensive running time, and/or must be discriminatively retrained for each language and text type. 3 XMEANT: a cross-lingual ME</context>
</contexts>
<marker>Mehdad, Negri, Federico, 2012</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. Match without a referee: evaluating mt adequacy without reference translations. In 7th Workshop on Statistical Machine Translation (WMT 2012), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>1999</date>
<booktitle>In The Twenty-first National Conference on Artificial Intelligence (AAAI06),</booktitle>
<volume>21</volume>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<marker>Mihalcea, Corley, Strapparava, 1999</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. Corpus-based and knowledge-based measures of text semantic similarity. In The Twenty-first National Conference on Artificial Intelligence (AAAI06), volume 21. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nießen</author>
<author>Franz Josef Och</author>
<author>Gregor Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>A evaluation tool for machine translation: Fast evaluation for MT research.</title>
<date>2000</date>
<booktitle>In The Second International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="4589" citStr="Nießen et al., 2000" startWordPosition="699" endWordPosition="702">. c�2014 Association for Computational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), </context>
</contexts>
<marker>Nießen, Och, Leusch, Ney, 2000</marker>
<rawString>Sonja Nießen, Franz Josef Och, Gregor Leusch, and Hermann Ney. A evaluation tool for machine translation: Fast evaluation for MT research. In The Second International Conference on Language Resources and Evaluation (LREC 2000), 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Dependency-based automatic evaluation for machine translation.</title>
<date>2007</date>
<booktitle>In Syntax and Structure in Statistical Translation (SSST),</booktitle>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. Dependency-based automatic evaluation for machine translation. In Syntax and Structure in Statistical Translation (SSST), 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Evaluating machine translation with LFG dependencies.</title>
<date>2007</date>
<journal>Machine Translation,</journal>
<volume>21</volume>
<marker>Owczarzak, van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef van Genabith, and Andy Way. Evaluating machine translation with LFG dependencies. Machine Translation, 21:95–119, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In 40th Annual Meeting of the Association for Computational Linguistics (ACL-02),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, Pennsylvania,</location>
<contexts>
<context position="4474" citStr="Papineni et al., 2002" startWordPosition="680" endWordPosition="684">he Association for Computational Linguistics (Short Papers), pages 765–771, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. BLEU: a method for automatic evaluation of machine translation. In 40th Annual Meeting of the Association for Computational Linguistics (ACL-02), pages 311–318, Philadelphia, Pennsylvania, July 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Dan Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL</booktitle>
<contexts>
<context position="2640" citStr="Pradhan et al., 2004" startWordPosition="397" endWordPosition="400">nt more accurately than the heuristic bag-ofword aggregation used in MEANT. Our results suggest that MT translation adequacy is more accurately evaluated via the cross-lingual semantic frame similarities of the input and the MT output which may obviate the need for expensive human reference translations. The MEANT family of metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012) adopt the principle that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´aˇcek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is i</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Dan Jurafsky. Shallow semantic parsing using support vector machines. In Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics (HLT-NAACL 2004), 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Quirk</author>
</authors>
<title>Training a sentence-level machine translation confidence measure.</title>
<date>2004</date>
<booktitle>In Fourth International Conference on Language Resources and Evaluation (LREC 2004),</booktitle>
<location>Lisbon, Portugal,</location>
<contexts>
<context position="10257" citStr="Quirk (2004)" startWordPosition="1640" endWordPosition="1641">two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learn767 Figure 3: Cross-lingual XMEANT algorithm. ing algorithms for predicting MT quality.</context>
</contexts>
<marker>Quirk, 2004</marker>
<rawString>Christopher Quirk. Training a sentence-level machine translation confidence measure. In Fourth International Conference on Language Resources and Evaluation (LREC 2004), Lisbon, Portugal, May 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miguel Rios</author>
<author>Wilker Aziz</author>
<author>Lucia Specia</author>
</authors>
<title>Tine: A metric to assess MT adequacy.</title>
<date>2011</date>
<booktitle>In Sixth Workshop on Statistical Machine Translation (WMT</booktitle>
<contexts>
<context position="5308" citStr="Rios et al., 2011" startWordPosition="815" endWordPosition="818">ce. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the referen</context>
</contexts>
<marker>Rios, Aziz, Specia, 2011</marker>
<rawString>Miguel Rios, Wilker Aziz, and Lucia Specia. Tine: A metric to assess MT adequacy. In Sixth Workshop on Statistical Machine Translation (WMT 2011), 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Dekai Wu</author>
</authors>
<title>Improving phrase-based translation via word alignments from stochastic inversion transduction grammars.</title>
<date>2009</date>
<booktitle>In Third Workshop on Syntax and Structure in Statistical Translation (SSST-3),</booktitle>
<location>Boulder, Colorado,</location>
<contexts>
<context position="14621" citStr="Saers and Wu, 2009" startWordPosition="2340" endWordPosition="2343">ilities within semantic role filler phrases. Addanki et al. (2012) showed empirically that cross-lingual semantic role reordering of the kind that MEANT is based upon is fully covered within ITG constraints. In Wu et al. (2014), we extend ITG constraints into aligning the tokens within the semantic role fillers within monolingual MEANT, thus replacing its previous monolingual phrasal aggregation heuristic. Here we borrow the prece,f = si,pred = si,j = 768 idea for the cross-lingual case, using the lengthnormalized inside probability at the root of a BITG biparse (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009) as follows: G ({A} , W°, W1, R, A) R {A --+ [AA] , A --+ (AA), A --+ e/f} p ([AA] JA) = p ((AA)JA) = 0.25 1 p (e/f IA) = 2 √t (elf) t (f le) 1 Table 1: Sentence-level correlation with HAJ (GALE phase 2.5 evaluation data) Metric Kendall HMEANT 0.53 XMEANT (BITG) 0.51 MEANT (f-score) 0.48 XMEANT (f-score) 0.46 MEANT (2013) 0.46 NIST 0.29 BLEU/METEOR/TER/PER 0.20 CDER 0.12 WER 0.10 si,pred = 1 ( (A � )) ln P ⇒ei,pred/fi,pred|G max(|ei,pred|,|fi,pred|) 5 Conclusion 1 si,7 = ( (A � )) ln P ⇒ei,j/fi,j|G max(|ei,j |,|fi,j |) where G is a bracketing ITG, whose only nonterminal is A, and where R is a </context>
<context position="16736" citStr="Saers and Wu, 2009" startWordPosition="2717" endWordPosition="2720">suggests that the semantic structure of the MT output is indeed closer to that of the input sentence than that of the reference translation. Furthermore, the ITG-based XMEANT (1) significantly outperforms MEANT, and (2) is an automatic metric that is nearly as accurate as the HMEANT human subjective version. This indicates that BITG constraints indeed provide a more robust token alignment compared to the heuristics previously employed in MEANT. It is also consistent with results observed while estimating word alignment probabilities, where BITG constraints outperformed alignments from GIZA++ (Saers and Wu, 2009). We have presented XMEANT, a new cross-lingual variant of MEANT, that correlates even more closely with human translation adequacy judgments than MEANT, without the expensive human references. This is (1) accomplished by replacing monolingual MEANT’s context vector model with simple translation probabilities when computing similarities of semantic role fillers, and (2) further improved by incorporating BITG constraints for aligning the tokens in semantic role fillers. While monolingual MEANT alone accurately reflects adequacy via semantic frames and optimizing SMT against MEANT improves trans</context>
</contexts>
<marker>Saers, Wu, 2009</marker>
<rawString>Markus Saers and Dekai Wu. Improving phrase-based translation via word alignments from stochastic inversion transduction grammars. In Third Workshop on Syntax and Structure in Statistical Translation (SSST-3), Boulder, Colorado, June 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Saers</author>
<author>Joakim Nivre</author>
<author>Dekai Wu</author>
</authors>
<title>Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm.</title>
<date>2009</date>
<booktitle>In 11th International Conference on Parsing Technologies (IWPT’09),</booktitle>
<location>Paris, France,</location>
<contexts>
<context position="15662" citStr="Saers et al. (2009)" startWordPosition="2547" endWordPosition="2550">pred|G max(|ei,pred|,|fi,pred|) 5 Conclusion 1 si,7 = ( (A � )) ln P ⇒ei,j/fi,j|G max(|ei,j |,|fi,j |) where G is a bracketing ITG, whose only nonterminal is A, and where R is a set of transduction rules where e E W0 U {E} is an output token (or the null token), and f E W1 U {E} is an input token (or the null token). The rule probability function p is defined using fixed probabilities for the structural rules, and a translation table t trained using IBM model 1 in both directions. To calculate the inside probability of a pair of segments, P (A =*=&gt;. e/f |G) , we use the algorithm described in Saers et al. (2009). si,pred and si,� are the length normalized BITG parsing probabilities of the predicates and role fillers of the arguments of type j between the input and the MT output. 4 Results Table 1 shows that for human adequacy judgments at the sentence level, the f-score based XMEANT (1) correlates significantly more closely than other commonly used monolingual automatic MT evaluation metrics, and (2) even correlates nearly as well as monolingual MEANT. This suggests that the semantic structure of the MT output is indeed closer to that of the input sentence than that of the reference translation. Furt</context>
</contexts>
<marker>Saers, Nivre, Wu, 2009</marker>
<rawString>Markus Saers, Joakim Nivre, and Dekai Wu. Learning stochastic bracketing inversion transduction grammars with a cubic time biparsing algorithm. In 11th International Conference on Parsing Technologies (IWPT’09), Paris, France, October 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashif Shah</author>
<author>Trevor Cohn</author>
<author>Lucia Specia</author>
</authors>
<title>An investigation on the effectiveness of features for translation quality estimation.</title>
<date>2013</date>
<booktitle>In Machine Translation Summit XIV (MT Summit 2013),</booktitle>
<location>Nice, France,</location>
<contexts>
<context position="10690" citStr="Shah et al., 2013" startWordPosition="1710" endWordPosition="1713">m of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learn767 Figure 3: Cross-lingual XMEANT algorithm. ing algorithms for predicting MT quality. Although the feature-based QE system of Avramidis and Popovi´c (2013) slightly outperformed METEOR on correlation with human adequacy judgment, these “black box” approaches typically lack representational transparency, require expensive running time, and/or must be discriminatively retrained for each language and text type. 3 XMEANT: a cross-lingual MEANT Like MEANT, XMEANT aims to evaluate how well MT preserves the core semanti</context>
</contexts>
<marker>Shah, Cohn, Specia, 2013</marker>
<rawString>Kashif Shah, Trevor Cohn, and Lucia Specia. An investigation on the effectiveness of features for translation quality estimation. In Machine Translation Summit XIV (MT Summit 2013), Nice, France, 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A study of translation edit rate with targeted human annotation.</title>
<date>2006</date>
<booktitle>In 7th Biennial Conference Association for Machine Translation in the Americas (AMTA</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts,</location>
<contexts>
<context position="4620" citStr="Snover et al., 2006" startWordPosition="705" endWordPosition="708">ational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher corr</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. A study of translation edit rate with targeted human annotation. In 7th Biennial Conference Association for Machine Translation in the Americas (AMTA 2006), pages 223–231, Cambridge, Massachusetts, August 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
</authors>
<title>Exploiting objective annotations for measuring translation post-editing effort.</title>
<date>2011</date>
<booktitle>In 15th Conference of the European Association for Machine Translation,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="10574" citStr="Specia, 2011" startWordPosition="1694" endWordPosition="1695">E system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learn767 Figure 3: Cross-lingual XMEANT algorithm. ing algorithms for predicting MT quality. Although the feature-based QE system of Avramidis and Popovi´c (2013) slightly outperformed METEOR on correlation with human adequacy judgment, these “black box” approaches typically lack representational transparency, require expensive running time, and/or must be discriminatively retrained for each language and t</context>
</contexts>
<marker>Specia, 2011</marker>
<rawString>Lucia Specia. Exploiting objective annotations for measuring translation post-editing effort. In 15th Conference of the European Association for Machine Translation, pages 73–80, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anand Karthik Tumuluru</author>
<author>Chi-kiu Lo</author>
<author>Dekai Wu</author>
</authors>
<title>Accuracy and robustness in measuring the lexical similarity of semantic role fillers for automatic semantic MT evaluation.</title>
<date>2012</date>
<booktitle>In 26th Pacific Asia Conference on Language, Information, and Computation (PACLIC 26),</booktitle>
<contexts>
<context position="8554" citStr="Tumuluru et al. (2012)" startWordPosition="1358" endWordPosition="1361">termined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,� are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we employ a newer version of MEANT that uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers, as our experiments indicate this is more accurate than the previously used </context>
</contexts>
<marker>Tumuluru, Lo, Wu, 2012</marker>
<rawString>Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu. Accuracy and robustness in measuring the lexical similarity of semantic role fillers for automatic semantic MT evaluation. In 26th Pacific Asia Conference on Language, Information, and Computation (PACLIC 26), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nicola Ueffing</author>
<author>Hermann Ney</author>
</authors>
<title>Word-level confidence estimation for machine translation using phrase-based translation models.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>763--770</pages>
<contexts>
<context position="9800" citStr="Ueffing and Ney (2005)" startWordPosition="1560" endWordPosition="1563">ecent studies (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b) show that tuning MT systems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small anno</context>
</contexts>
<marker>Ueffing, Ney, 2005</marker>
<rawString>Nicola Ueffing and Hermann Ney. Word-level confidence estimation for machine translation using phrase-based translation models. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 763–770, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>SPEDE: Probabilistic edit distance metrics for MT evaluation.</title>
<date>2012</date>
<booktitle>In 7th Workshop on Statistical Machine Translation (WMT</booktitle>
<contexts>
<context position="5839" citStr="Wang and Manning, 2012" startWordPosition="901" endWordPosition="905">correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the reference via an integrated probabilistic FSM and PDA model. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain several dozens of parameters to tune, and employ expensive linguistic resources like WordNet Figure 1: Monolingual MEANT algorithm. or paraphrase tables; the expensive training, tuning, and/or running time makes them hard to incorporate into the MT development cycle. 2.2 </context>
</contexts>
<marker>Wang, Manning, 2012</marker>
<rawString>Mengqiu Wang and Christopher D. Manning. SPEDE: Probabilistic edit distance metrics for MT evaluation. In 7th Workshop on Statistical Machine Translation (WMT 2012), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
<author>Chi-kiu Lo</author>
<author>Meriem Beloucif</author>
<author>Markus Saers</author>
</authors>
<title>IMEANT: Improving semantic frame based MT evaluation via inversion transduction grammars. Forthcoming,</title>
<date>2014</date>
<contexts>
<context position="14229" citStr="Wu et al. (2014)" startWordPosition="2277" endWordPosition="2280">of the role fillers. si,pred and si,j are the f-scores of the phrasal similarities of the predicates and role fillers of the arguments of type j between the input and the MT output. 3.2 Applying MEANT’s ITG bias within semantic role fillers The second natural approach is to extend MEANT’s ITG bias on compositional reordering, so as to also apply to aggregating lexical translation probabilities within semantic role filler phrases. Addanki et al. (2012) showed empirically that cross-lingual semantic role reordering of the kind that MEANT is based upon is fully covered within ITG constraints. In Wu et al. (2014), we extend ITG constraints into aligning the tokens within the semantic role fillers within monolingual MEANT, thus replacing its previous monolingual phrasal aggregation heuristic. Here we borrow the prece,f = si,pred = si,j = 768 idea for the cross-lingual case, using the lengthnormalized inside probability at the root of a BITG biparse (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009) as follows: G ({A} , W°, W1, R, A) R {A --+ [AA] , A --+ (AA), A --+ e/f} p ([AA] JA) = p ((AA)JA) = 0.25 1 p (e/f IA) = 2 √t (elf) t (f le) 1 Table 1: Sentence-level correlation with HAJ (GALE phase 2.5 eva</context>
</contexts>
<marker>Wu, Lo, Beloucif, Saers, 2014</marker>
<rawString>Dekai Wu, Chi-kiu Lo, Meriem Beloucif, and Markus Saers. IMEANT: Improving semantic frame based MT evaluation via inversion transduction grammars. Forthcoming, 2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="14580" citStr="Wu, 1997" startWordPosition="2334" endWordPosition="2335">ing lexical translation probabilities within semantic role filler phrases. Addanki et al. (2012) showed empirically that cross-lingual semantic role reordering of the kind that MEANT is based upon is fully covered within ITG constraints. In Wu et al. (2014), we extend ITG constraints into aligning the tokens within the semantic role fillers within monolingual MEANT, thus replacing its previous monolingual phrasal aggregation heuristic. Here we borrow the prece,f = si,pred = si,j = 768 idea for the cross-lingual case, using the lengthnormalized inside probability at the root of a BITG biparse (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009) as follows: G ({A} , W°, W1, R, A) R {A --+ [AA] , A --+ (AA), A --+ e/f} p ([AA] JA) = p ((AA)JA) = 0.25 1 p (e/f IA) = 2 √t (elf) t (f le) 1 Table 1: Sentence-level correlation with HAJ (GALE phase 2.5 evaluation data) Metric Kendall HMEANT 0.53 XMEANT (BITG) 0.51 MEANT (f-score) 0.48 XMEANT (f-score) 0.46 MEANT (2013) 0.46 NIST 0.29 BLEU/METEOR/TER/PER 0.20 CDER 0.12 WER 0.10 si,pred = 1 ( (A � )) ln P ⇒ei,pred/fi,pred|G max(|ei,pred|,|fi,pred|) 5 Conclusion 1 si,7 = ( (A � )) ln P ⇒ei,j/fi,j|G max(|ei,j |,|fi,j |) where G is a bracketing ITG, whose</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–403, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Min Zhang</author>
<author>Haizhou Li</author>
</authors>
<title>Error detection for statistical machine translation using linguistic features.</title>
<date>2010</date>
<booktitle>In 48th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="10526" citStr="Xiong et al., 2010" startWordPosition="1682" endWordPosition="1685">rast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learn767 Figure 3: Cross-lingual XMEANT algorithm. ing algorithms for predicting MT quality. Although the feature-based QE system of Avramidis and Popovi´c (2013) slightly outperformed METEOR on correlation with human adequacy judgment, these “black box” approaches typically lack representational transparency, require expensive running time, and/or must be di</context>
</contexts>
<marker>Xiong, Zhang, Li, 2010</marker>
<rawString>Deyi Xiong, Min Zhang, and Haizhou Li. Error detection for statistical machine translation using linguistic features. In 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>A comparative study on reordering constraints in statistical machine translation.</title>
<date>2003</date>
<booktitle>In 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003),</booktitle>
<pages>144--151</pages>
<location>Stroudsburg, Pennsylvania,</location>
<contexts>
<context position="14600" citStr="Zens and Ney, 2003" startWordPosition="2336" endWordPosition="2339">l translation probabilities within semantic role filler phrases. Addanki et al. (2012) showed empirically that cross-lingual semantic role reordering of the kind that MEANT is based upon is fully covered within ITG constraints. In Wu et al. (2014), we extend ITG constraints into aligning the tokens within the semantic role fillers within monolingual MEANT, thus replacing its previous monolingual phrasal aggregation heuristic. Here we borrow the prece,f = si,pred = si,j = 768 idea for the cross-lingual case, using the lengthnormalized inside probability at the root of a BITG biparse (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009) as follows: G ({A} , W°, W1, R, A) R {A --+ [AA] , A --+ (AA), A --+ e/f} p ([AA] JA) = p ((AA)JA) = 0.25 1 p (e/f IA) = 2 √t (elf) t (f le) 1 Table 1: Sentence-level correlation with HAJ (GALE phase 2.5 evaluation data) Metric Kendall HMEANT 0.53 XMEANT (BITG) 0.51 MEANT (f-score) 0.48 XMEANT (f-score) 0.46 MEANT (2013) 0.46 NIST 0.29 BLEU/METEOR/TER/PER 0.20 CDER 0.12 WER 0.10 si,pred = 1 ( (A � )) ln P ⇒ei,pred/fi,pred|G max(|ei,pred|,|fi,pred|) 5 Conclusion 1 si,7 = ( (A � )) ln P ⇒ei,j/fi,j|G max(|ei,j |,|fi,j |) where G is a bracketing ITG, whose only nonterminal is</context>
</contexts>
<marker>Zens, Ney, 2003</marker>
<rawString>Richard Zens and Hermann Ney. A comparative study on reordering constraints in statistical machine translation. In 41st Annual Meeting of the Association for Computational Linguistics (ACL-2003), pages 144–151, Stroudsburg, Pennsylvania, 2003.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>