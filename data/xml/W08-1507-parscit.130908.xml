<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000831">
<title confidence="0.994665">
Language Understanding in Maryland Virtual Patient
</title>
<author confidence="0.969430666666667">
Sergei Nirenburg
Stephen Beale
Marjorie McShane
</author>
<affiliation confidence="0.999302">
University of Maryland Baltimore County
</affiliation>
<email confidence="0.7308605">
{sergei, sbeale,
marge}@umbc.edu
</email>
<sectionHeader confidence="0.993852" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999920375">
This paper discusses language under-
standing in the Maryland Virtual Patient
environment. Language understanding is
just one of many cognitive functions of
the virtual patients in MVP, others in-
cluding decision making about healthcare
and lifestyle, and the experiencing and
remembering of interoceptive events.
</bodyText>
<sectionHeader confidence="0.99875" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999433304347826">
Maryland Virtual Patient2 (MVP) is an agent-
oriented environment for automating certain fac-
ets of medical training. The environment con-
tains a network of human and software agents, at
whose core is a virtual patient – a knowledge-
based model of a person with a disease. This
model is implemented in a computer simulation.
The virtual patient is a “double agent” that dis-
plays both physiological and cognitive function.
Physiologically, it undergoes both normal and
pathological processes in response to internal and
external stimuli. Cognitively, it experiences
symptoms, has lifestyle preferences, has memory
(many of whose details fade with time), and
communicates with the human user about its per-
sonal history and symptoms. Other software
agents in the MVP environment include consult-
ing physicians, lab technicians and a virtual men-
tor (tutor).
What makes virtual patient modeling feasible
– considering that comprehensively modeling
human physiology would be a boundless en-
deavor – is our task-oriented approach: we are
</bodyText>
<footnote confidence="0.9428262">
© 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved.
2 Patent pending.
</footnote>
<author confidence="0.867113">
Bruce Jarrell
George Fantry
</author>
<affiliation confidence="0.98531">
University of Maryland School of Medicine
</affiliation>
<email confidence="0.997103">
BJarrell@som.umaryland.edu
GFantry@medicine.umaryland.edu
</email>
<bodyText confidence="0.995938575757576">
not trying to recreate the human organism in all
its details, we are modeling it to the extent neces-
sary to support its realistic autonomous function-
ing in applications aimed at training the diagnos-
tic and treatment skills of medical personnel.
Trainees can use MVP to interview a virtual
patient; order lab tests; receive the results of lab
tests from technician agents; receive interpreta-
tions of lab tests from consulting physician
agents; posit hypotheses, clinical diagnoses and
definitive diagnoses; prescribe treatments; fol-
low-up after those treatments to judge their effi-
cacy; follow a patient’s condition over an ex-
tended period of time, with the trainee having
control over the speed of simulation (i.e., the
clock); and, if desired, receive mentoring from
the automatic mentor.
The virtual patient (VP) simulation is
grounded in an ontologically-defined model of
human anatomy and physiology. Instances of
virtual patients with particular diseases and par-
ticular physiological peculiarities are generated
from core ontological knowledge about human
physiology and anatomy by grafting a disease
process onto a generic instance of a human. Dis-
ease processes themselves are described as com-
plex events in the underlying ontology.
2 Reasoning by the Cognitive Agent
The cognitive side of the VP carries out reason-
ing in response to two types of input: interocep-
tion (the experiencing of physical stimuli, like
symptoms) and language input. Specifically its
functioning includes:
</bodyText>
<listItem confidence="0.998159666666667">
1. experiencing, interpreting and remember-
ing symptoms
2. deciding to go see a doctor, initially and
during treatment
3. understanding the doctor’s language input
as well as its intent
</listItem>
<page confidence="0.990303">
36
</page>
<note confidence="0.604211">
Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 36–39
</note>
<bodyText confidence="0.7390006">
Manchester, August 2008
4. deciding whether to ask knowledge-
seeking questions about a test or interven-
tion suggested by the doctor
5. deciding whether to agree to a test or inter-
vention suggested by the doctor.
6. deciding on what specifically to say in re-
sponse to the doctor’s questions,
recommendations, etc.
In this paper we concentrate on point 3. We point
readers to other works about MVP (e.g.,
McShane et al. 2007) for a discussion of other
aspects of MVP.
Five types of subdialogs are supported in
MVP.
</bodyText>
<listItem confidence="0.968584055555555">
1. Requests for information and responses.
These include (a) the physician asking the
patient questions about symptoms and life-
style, and (b) the patient asking questions
about features of suggested interventions
as well as other options.
2. Requests for action and responses – pri-
marily the physician suggesting that the
patient agree to have an intervention.
3. Domain descriptions provided by the user,
the key points of which must be under-
stood and remembered (“learned”) by the
VP.
4. Scheduling follow-up appointments.
5. General dialog topics, like greetings, ex-
pressions of gratitude and other means for
making the dialog more realistic in the
user’s eyes.
</listItem>
<bodyText confidence="0.999521955555556">
Our approach to treating dialog is unlike most
other approaches in that all language-oriented
reasoning is carried out on the basis of formal
interpretations of text meaning. We call these
interpretations text meaning representations or
TMRs. Note that TMRs are written using the
same ontologically grounded metalanguage as is
used to represent interoception. In short, all
knowledge and reasoning in our environment
employs the same metalanguage, so whether a
patient experiences new symptoms or learns in-
formation about its disease from the user, the
new information will be stored the same way in
the patient’s memory.
There are several advantages to orienting an
agent’s language processing around TMRs rather
than text strings. First, TMRs are unambiguous,
since linguistic ambiguity is resolved as the
TMRs are being produced. Second, TMRs re-
duce to a single representation many types of
linguistic paraphrase, be it lexical (esophagus ~
food pipe), syntactic (I will administer it to you ~
It will be administered to you by me) or even se-
mantic (Does the food get stuck when you swal-
low? ~ Do you have difficulty swallowing?).
Third, TMRs facilitate the detection of which
aspects of meaning are central and which are of
secondary importance. For example, the analyzer
can determine which portions of input utterances
merely convey politeness. To take an extreme
example for illustration, the question “Do you
have difficultly swallowing?” could be rendered
by an overly polite physician as: “If you don’t
mind, I would really appreciate it if you would
tell me whether you have any difficulty swallow-
ing.”
When the VP receives language input, it uses
its lexicon, ontology and a reasoning-enabled
analyzer to create a TMR corresponding to the
input. Next, it determines the intent of that input
– e.g., through the recognition of indirect speech
acts. After that it plans its response then gener-
ates its response. Here we talk about the first two
stages of text processing: understanding the dia-
log turn and understanding its intent.
</bodyText>
<sectionHeader confidence="0.923142" genericHeader="method">
3 Understanding a Dialog Turn
</sectionHeader>
<bodyText confidence="0.999671">
The input to understanding a dialog turn is text
input by the user. Background knowledge that
must be leveraged is the knowledge stored in the
lexicon, ontology and the patient’s long-term
memory of assertions, also called its fact reposi-
tory. The output is a TMR. TMR production ac-
tually comprises two stages: the first stage, pro-
duction of the basic TMR, involves disambigua-
tion and the determination of semantic depend-
encies; the second stage, production of the ex-
tended TMR, adds the results of procedural se-
mantic routines, like the resolution of reference.
For example, the following questions are all
synonyms at the level of extended TMR, at least
at the grain-size of description needed for our
current application: Have you been coughing?
Do you find yourself coughing? Do you experi-
ence any coughing? Do you ever experience
coughing? Do you have a cough? Any coughing?
Coughing? etc. All of these questions ask
whether or not the patient has the symptom onto-
logically described as the event called COUGH.
The extended TMR for this set of questions is:
</bodyText>
<equation confidence="0.957673214285714">
(REQUEST-INFO-1
(THEME MODALITY-1.VALUE))
(MODALITY-1
(TYPE EPISTEMIC)
(SCOPE ASPECT-1))
37
(ASPECT-1
(ITERATION MULTIPLE)
(SCOPE COUGH-1))
(COUGH-1
(EXPERIENCER HUMAN-1)
(TIME
(FIND-INTERVAL (FIND-ANCHOR-TIME)
(FIND-INTERVAL-LENGTH) BEFORE)))
</equation>
<bodyText confidence="0.999853470588236">
This TMR is read as follows. The input creates
an instance of REQUEST-INFO. The instance is
numbered, like all TMR instances, to distinguish
it from other instances of that concept. The
THEME of REQUEST-INFO-1 – i.e., what is being
asked about – is whether or not COUGH-1 has
occurred repetitively; this is shown in the AS-
PECT-1 frame. The COUGH event itself has the
VP, HUMAN-1, as the EXPERIENCER. The time of
the COUGH event is calculated using a procedural
semantic routine that seeks a certain time interval
in the past (we leave out details of which period
of time in order to avoid a lengthy tangent). Al-
though this example is a bit complex – involving
both aspect and modality – it provides some in-
sight into the format and content of TMRs in our
environment.
The text analyzer can automatically create this
same TMR for all of the different inputs in large
part thanks to the lexicon. Syntactic knowledge
in lexicon entries in OntoSem is formulated us-
ing an extended form of Lexical Functional
Grammar, with variables used to link entities in
the syntactic structure (syn-struc) zone of an en-
try with those in the semantic structure (sem-
struc) zone. Lexicon entries can also contain
calls to procedural semantic routines (meaning-
procedures). The caret means “the meaning of” a
given variable. $var0 is the head entry.
Have you been coughing? is a syntactic trans-
formation of Do you cough?, which is under-
stood directly by the analyzer as a question about
cough (v.), which is mapped to the concept
COUGH in the respective lexicon entry.
</bodyText>
<equation confidence="0.944724166666667">
(cough-v1
(syn-struc
((subject ((root $var1) (cat n)))
(root $var0) (cat v)))
(sem-struc
(COUGH (EXPERIENCER (value ^$var1)))))
</equation>
<bodyText confidence="0.999450862068965">
For the other paraphrases, “superfluous” words
must be attributed null semantics. For example,
to find oneself verb-ing is semantically same as
to verb, the only real difference being stylistic.
There is a lexical sense of find that attributes null
semantics to find oneself in the collocation find
oneself doing X.
Examples in which question processing is
folded into the lexicon entry are Any + EVENT ?
(Any coughing?) and EVENT? (Coughing?). The
lexicon entry that covers these is keyed on the
question mark, since it is the only element that is
always available in these turns of phrase (since
“any” is optional). The sem-struc is headed by
the concept REQUEST-INFO, whose THEME is the
value of epistemic modality scoping over the
event in question.
This brief overview is intended only to give a
taste of the process of language understanding by
virtual patients in MVP. This process is exactly
the same as language understanding in other ap-
plications of our text processor, called OntoSem
(see Nirenburg and Raskin 2004).
The eventualities of text understanding by the
cognitive agent of the VP are: (a) successful un-
derstanding, (b) the VP’s belief that it under-
stood, only to be corrected by the user, or (c) the
failure of understanding, in which case the VP
asks for clarification by the user.
</bodyText>
<sectionHeader confidence="0.886281" genericHeader="method">
4 Understanding the Intent of a Dialog
Turn
</sectionHeader>
<bodyText confidence="0.999952846153846">
The extended TMR is our most complete model
of the meaning of an utterance, but it does not
include what is called indirect speech act proc-
essing – i.e., understanding intentions of the
speaker when they are not overtly mentioned in
the utterance. Well-known examples of the di-
chotomy between expressed meaning and in-
tended meaning include It’s cold in here (which
might be a statement/complaint or might be an
indirect request for the interlocutor to do some-
thing about it, like close the window) and Can
you pass the salt? (which might be a question
about physical ability or an indirect request).
Our work on indirect speech acts includes
long-term, fundamental theory building as well
as short-term, immediately implementable solu-
tions. At a fundamental level, speech act process-
ing requires the speaker and the interlocutor to
keep a full inventory of their beliefs about the
other’s knowledge, their understanding of their
own and the other’s plans and goals, both long-
term and immediate, their understanding of what
is and what is not within each person’s or agent’s
power to do, and so on. More immediately, we
have implemented a means of detecting indirect
speech acts in the dialogs between VPs and us-
</bodyText>
<page confidence="0.997969">
38
</page>
<bodyText confidence="0.99956225">
ers. Our approach, like all of our approaches to
automatic reasoning, is grounded in TMRs.
There are three utterance types that the VP ex-
pects of the user, which correspond to three user
plans: asking questions to learn information that
will aid in diagnosis and treatment, explaining
things to educate the VP, and giving advice to
the VP about what it should do. At any point in
the dialog when the user stops typing and expects
a response from the VP, the VP must decide
which of the plans the user is pursuing. Surface-
level heuristics are not always definitive: e.g.,
Would you agree to have a Heller myotomy? is
both a question and advice, and I think that hav-
ing a Heller myotomy is the best option is both
information and advice.
We prepare the VP to interpret indirect speech
acts by creating TMR correspondences between
the direct and the indirect meaning of certain
types of utterances. Let us take as an example the
doctor’s offering advice on what to do. There are
many ways the doctor can present advice, includ-
ing the following, provided below with their re-
spective TMRs. In all of these TMRs, HUMAN-1
is the doctor and HUMAN-2 is the patient (these
TMRs are simplified for purposes of exposition;
also note that all reference resolution has been
carried out). INTERVENTION stands for any event
that is ontologically an intervention – that is, a
test or a medical procedure. Note that the lexicon
directly supports the automatic generation of
these TMRs.
</bodyText>
<sectionHeader confidence="0.3347065" genericHeader="method">
1. I (would) advise/suggest/recommend
(having) INTERVENTION
</sectionHeader>
<equation confidence="0.433610888888889">
(ADVISE-1
(THEME INTERVENTION-1)
(AGENT HUMAN-1)
(INTERVENTION-1
(EXPERIENCER HUMAN-2))
2. I think you should have INTERVENTION
(MODALITY-1
(TYPE BELIEF)
(VALUE (&gt; .7))
(SCOPE MODALITY-2)
(ATTRIBUTED-TO HUMAN-1))
(MODALITY-2
(TYPE OBLIGATIVE)
(VALUE .8)
(SCOPE INTERVENTION-1)
(ATTRIBUTED-TO HUMAN-1))
(INTERVENTION-1
(EXPERIENCER HUMAN-2)))
</equation>
<bodyText confidence="0.790569">
3. I&apos;d like to schedule you for &lt;set you up
for, set you up to have&gt; INTERVENTION
</bodyText>
<equation confidence="0.917739">
(MODALITY-1
(TYPE VOLITIVE)
(SCOPE EVENT-1)
(VALUE .8)
(ATTRIBUTED-TO HUMAN-1))
(SCHEDULE-EVENT-1
(AGENT HUMAN-1)
(THEME INTERVENTION-1)
(BENEFICIARY HUMAN-2))
(INTERVENTION-1
(EXPERIENCER HUMAN-2))
</equation>
<bodyText confidence="0.999786076923077">
The “core” meaning that the VP must glean
from any of these TMRs is the meaning shown in
(1): that the doctor is advising that the patient
have the intervention. The correlations between
the TMRs in (2) and (3) and this core TMR are
established using a TMR-to-TMR translation
function. The efficacy of this translation process
depends on (a) preparing for the full inventory of
possible types of input TMRs that correspond to
the given meaning, and (b) being able to extract
from more complex TMRs these basic kernels of
meaning. We have already implemented part (a)
in our current system. Part (b) requires more
long-term effort, the problem essentially being
that one needs to teach the system to zero in on
what is important and ignore what is unimpor-
tant. For example, negation is very important: I
advise you to have INTERVENTION is very differ-
ent from I do not advise you to have INTERVEN-
TION. However, I think I would choose to advise
you to have INTERVENTION includes aspects of
meaning (‘think’, ‘would choose’) that are really
not important and should be simplified to the
main meaning of the proposition. We consider
research on this aspect of agent reasoning to be a
long-term endeavor
</bodyText>
<sectionHeader confidence="0.99927" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999434333333333">
McShane, Marjorie, Sergei Nirenburg, Stephen Beale,
Bruce Jarrell and George Fantry. 2007. Knowl-
edge-based modeling and simulation of diseases
with highly differentiated clinical manifestations.
11th Conference on Artificial Intelligence in Medi-
cine (AIME 07), Amsterdam, The Netherlands, July
7-11, 2007.
Nirenburg, Sergei and Victor Raskin. 2004. Ontologi-
cal Semantics. MIT Press.
</reference>
<page confidence="0.999532">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.238580">
<title confidence="0.932783">Language Understanding in Maryland Virtual Patient Sergei</title>
<author confidence="0.7421075">Stephen Marjorie</author>
<affiliation confidence="0.783287">University of Maryland Baltimore County {sergei,</affiliation>
<email confidence="0.998469">marge}@umbc.edu</email>
<abstract confidence="0.999273666666667">This paper discusses language understanding in the Maryland Virtual Patient environment. Language understanding is just one of many cognitive functions of the virtual patients in MVP, others including decision making about healthcare and lifestyle, and the experiencing and remembering of interoceptive events.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marjorie McShane</author>
<author>Sergei Nirenburg</author>
<author>Stephen Beale</author>
<author>Bruce Jarrell</author>
<author>George Fantry</author>
</authors>
<title>Knowledge-based modeling and simulation of diseases with highly differentiated clinical manifestations.</title>
<date>2007</date>
<booktitle>11th Conference on Artificial Intelligence in Medicine (AIME 07),</booktitle>
<location>Amsterdam, The Netherlands,</location>
<contexts>
<context position="4085" citStr="McShane et al. 2007" startWordPosition="603" endWordPosition="606">erstanding the doctor’s language input as well as its intent 36 Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 36–39 Manchester, August 2008 4. deciding whether to ask knowledgeseeking questions about a test or intervention suggested by the doctor 5. deciding whether to agree to a test or intervention suggested by the doctor. 6. deciding on what specifically to say in response to the doctor’s questions, recommendations, etc. In this paper we concentrate on point 3. We point readers to other works about MVP (e.g., McShane et al. 2007) for a discussion of other aspects of MVP. Five types of subdialogs are supported in MVP. 1. Requests for information and responses. These include (a) the physician asking the patient questions about symptoms and lifestyle, and (b) the patient asking questions about features of suggested interventions as well as other options. 2. Requests for action and responses – primarily the physician suggesting that the patient agree to have an intervention. 3. Domain descriptions provided by the user, the key points of which must be understood and remembered (“learned”) by the VP. 4. Scheduling follow-up</context>
</contexts>
<marker>McShane, Nirenburg, Beale, Jarrell, Fantry, 2007</marker>
<rawString>McShane, Marjorie, Sergei Nirenburg, Stephen Beale, Bruce Jarrell and George Fantry. 2007. Knowledge-based modeling and simulation of diseases with highly differentiated clinical manifestations. 11th Conference on Artificial Intelligence in Medicine (AIME 07), Amsterdam, The Netherlands, July 7-11, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergei Nirenburg</author>
<author>Victor Raskin</author>
</authors>
<title>Ontological Semantics.</title>
<date>2004</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10919" citStr="Nirenburg and Raskin 2004" startWordPosition="1710" endWordPosition="1713">NT ? (Any coughing?) and EVENT? (Coughing?). The lexicon entry that covers these is keyed on the question mark, since it is the only element that is always available in these turns of phrase (since “any” is optional). The sem-struc is headed by the concept REQUEST-INFO, whose THEME is the value of epistemic modality scoping over the event in question. This brief overview is intended only to give a taste of the process of language understanding by virtual patients in MVP. This process is exactly the same as language understanding in other applications of our text processor, called OntoSem (see Nirenburg and Raskin 2004). The eventualities of text understanding by the cognitive agent of the VP are: (a) successful understanding, (b) the VP’s belief that it understood, only to be corrected by the user, or (c) the failure of understanding, in which case the VP asks for clarification by the user. 4 Understanding the Intent of a Dialog Turn The extended TMR is our most complete model of the meaning of an utterance, but it does not include what is called indirect speech act processing – i.e., understanding intentions of the speaker when they are not overtly mentioned in the utterance. Well-known examples of the dic</context>
</contexts>
<marker>Nirenburg, Raskin, 2004</marker>
<rawString>Nirenburg, Sergei and Victor Raskin. 2004. Ontological Semantics. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>