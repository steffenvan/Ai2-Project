<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006370">
<title confidence="0.993722">
Text Classification in Asian Languages without Word Segmentation
</title>
<author confidence="0.978616">
Fuchun Peng Xiangji Huang Dale Schuurmans Shaojun Wang
</author>
<affiliation confidence="0.998766666666667">
School of Computer Science, University of Waterloo, Ontario, Canada
Department of Computer Science, University of Massachusetts, Amherst, MA, USA
Department of Statistics, University of Toronto, Ontario, Canada
</affiliation>
<email confidence="0.972077">
f3peng, jhuang, dale, sjwang@ai.uwaterloo.ca
</email>
<sectionHeader confidence="0.998403" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999931347826087">
We present a simple approach for Asian
language text classification without word
segmentation, based on statistical-gram
language modeling. In particular, we ex-
amine Chinese and Japanese text classi-
fication. With character-gram models,
our approach avoids word segmentation.
However, unlike traditional ad hoc-gram
models, the statistical language model-
ing based approach has strong informa-
tion theoretic basis and avoids explicit fea-
ture selection procedure which potentially
loses significantly amount of useful infor-
mation. We systematically study the key
factors in language modeling and their in-
fluence on classification. Experiments on
Chinese TREC and Japanese NTCIR topic
detection show that the simple approach
can achieve better performance compared
to traditional approaches while avoiding
word segmentation, which demonstrates
its superiority in Asian language text clas-
sification.
</bodyText>
<sectionHeader confidence="0.999629" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974342465754">
Text classification addresses the problem of assign-
ing a given passage of text (or a document) to one or
more predefined classes. This is an important area
of information retrieval research that has been heav-
ily investigated, although most of the research activ-
ity has concentrated on English text (Dumais, 1998;
Yang, 1999). Text classification in Asian languages
such as Chinese and Japanese, however, is also an
important (and relatively more recent) area of re-
search that introduces a number of additional diffi-
culties. One difficulty with Chinese and Japanese
text classification is that, unlike English, Chinese
and Japanese texts do not have explicit whitespace
between words. This means that some form of
word segmentation is normally required before fur-
ther processing. However, word segmentation itself
is a difficult problem in these languages. A second
difficulty is that there is a lack of standard bench-
mark data sets for these languages. Nevertheless,
recently, there has been significant notable progress
on Chinese and Japanese text classification (Aizawa,
2001; He et al., 2001).
Many standard machine learning techniques have
been applied to text categorization problems, such
as naive Bayes classifiers, support vector machines,
linear least squares models, neural networks, and k-
nearest neighbor classifiers (Sebastiani, 2002; Yang,
1999). Unfortunately, most current text classi-
fiers work with word level features. However, word
identification in Asian languages, such as Chinese
and Japanese, is itself a hard problem. To avoid
the word segmentation problems, character level-
gram models have been proposed (Cavnar and Tren-
kle, 1994; Damashek, 1995). There, they used-
grams as features for a traditional feature selection
process and then deployed classifiers based on cal-
culating feature-vector similarities. This approach
has many shortcomings. First, there are an enor-
mous number of possible features to consider in text
categorization, and standard feature selection ap-
proaches do not always cope well in such circum-
stances. For example, given a sufficiently large num-
ber of features, the cumulative effect of uncommon
features can still have an important effect on clas-
sification accuracy, even though infrequent features
contribute less information than common features
individually. Therefore, throwing away uncommon
features is usually not an appropriate strategy in this
domain (Aizawa, 2001). Another problem is that
feature selection normally uses indirect tests, such as
or mutual information, which involve setting ar-
bitrary thresholds and conducting a heuristic greedy
search to find a good subset of features. Moreover,
by treating text categorization as a classical classifi-
cation problem, standard approaches can ignore the
fact that texts are written in natural language, which
means that they have many implicit regularities that
can be well modeled by specific tools from natural
language processing.
In this paper, we present a simple text categoriza-
tion approach based on statistical-gram language
modeling to overcome the above shortcomings in a
principled fashion. An advantage we exploit is that
the language modeling approach does not discard
low frequency features during classification, as is
commonly done in traditional classification learning
approaches. Also, the language modeling approach
uses-gram models to capture more contextual in-
formation than standard bag-of-words approaches,
and employs better smoothing techniques than stan-
dard classification learning. These advantages are
supported by our empirical results on Chinese and
Japanese data.
</bodyText>
<sectionHeader confidence="0.986487" genericHeader="method">
2 Language Model Text Classifiers
</sectionHeader>
<bodyText confidence="0.999690625">
The goal of language modeling is to predict the
probability of natural word sequences; or more sim-
ply, to put high probability on word sequences that
actually occur (and low probability on word se-
quences that never occur). Given a word sequence
to be used as a test corpus, the quality of
a language model can be measured by the empirical
perplexity (or entropy) on this corpus
</bodyText>
<subsubsectionHeader confidence="0.850126">
Perplexity
</subsubsectionHeader>
<bodyText confidence="0.998825">
The goal of language modeling is to obtain a small
perplexity.
</bodyText>
<subsectionHeader confidence="0.983723">
2.1 -gram language modeling
</subsectionHeader>
<bodyText confidence="0.999313821428571">
The simplest and most successful basis for language
modeling is the-gram model. Note that by the
chain rule of probability we can write the probability
of any sequence as
An-gram model approximates this probability by
assuming that the only words relevant to predicting
are the previous words; that
is, it assumes the Markov-gram independence as-
sumption
A straightforward maximum likelihood estimate of
-gram probabilities from a corpus is given by the
observed frequency
where #(.) is the number of occurrences of a speci-
fied gram in the training corpus. Unfortunately, us-
ing grams of length up to entails estimating the
probability of events, where is the size of the
word vocabulary. This quickly overwhelms modern
computational and data resources for even modest
choices of (beyond 3 to 6). Also, because of the
heavy tailed nature of language (i.e. Zipf’s law) one
is likely to encounter novel-grams that were never
witnessed during training. Therefore, some mecha-
nism for assigning non-zero probability to novel-
grams is a central and unavoidable issue. One stan-
dard approach to smoothing probability estimates to
cope with sparse data problems (and to cope with
potentially missing-grams) is to use some sort of
back-off estimator
</bodyText>
<page confidence="0.524823">
if
otherwise
(1)
</page>
<bodyText confidence="0.999470285714286">
where
Our approach to applying language models to text
categorization is to use Bayesian decision theory.
Assume we wish to classify a text
into a category . A natural
choice is to pick the category that has the largest
posterior probability given the text. That is,
</bodyText>
<figure confidence="0.877729571428571">
(7)
(5)
is the discounted probability, and
is a normalization constant calculated to be
Using Bayes rule, this can be rewritten as
(8)
(6)
</figure>
<bodyText confidence="0.999586695652174">
The discounted probability (5) can be com-
puted using different smoothing approaches includ-
ing Laplace smoothing, linear smoothing, absolute
smoothing, Good-Turing smoothing and Witten-
Bell smoothing (Chen and Goodman, 1998).
The language models described above use indi-
vidual words as the basic unit, although one could
instead consider models that use individual char-
acters as the basic unit. The remaining details re-
main the same in this case. The only difference is
that the character vocabulary is always much smaller
than the word vocabulary, which means that one can
normally use a much higher order,, in a charac-
ter level-gram model (although the text spanned
by a character model is still usually less than that
spanned by a word model). The benefits of the char-
acter level model in the context of text classification
are multi-fold: it avoids the need for explicit word
segmentation in the case of Asian languages, and it
greatly reduces the sparse data problems associated
with large vocabulary models. In this paper, we ex-
periment with character level models to avoid word
segmentation in Chinese and Japanese.
</bodyText>
<subsectionHeader confidence="0.990466">
2.2 Language models as text classifiers
</subsectionHeader>
<bodyText confidence="0.979002888888889">
Text classifiers attempt to identify attributes which
distinguish documents in different categories. Such
attributes may include vocabulary terms, word av-
erage length, local-grams, or global syntactic and
semantic properties. Language models also attempt
capture such regularities, and hence provide another
natural avenue to constructing text classifiers.
(9)
Here, is the likelihood of under category
, which can be computed by-gram language mod-
eling. The likelihood is related to perplexity by
Equ. (1). The prior can be computed from
training data or can be used to incorporate more as-
sumptions, such as a uniform or Dirichelet distribu-
tion.
Therefore, our approach is to learn a separate
back-off language model for each category, by train-
ing on a data set from that category. Then, to cate-
gorize a new text, we supply to each language
model, evaluate the likelihood (or entropy) of un-
der the model, and pick the winning category ac-
cording to Equ. (9).
The inference of an-gram based text classifier
is very similar to a naive Bayes classifier (to be
dicussed below). In fact,-gram classifiers are a
straightforward generalization of naive Bayes (Peng
and Schuurmans, 2003).
</bodyText>
<sectionHeader confidence="0.987901" genericHeader="method">
3 Traditional Text Classifiers
</sectionHeader>
<bodyText confidence="0.9998865">
We introduce the three standard text classifiers that
we will compare against below.
</bodyText>
<subsectionHeader confidence="0.99875">
3.1 Naive Bayes classifiers
</subsectionHeader>
<bodyText confidence="0.9920516875">
A simple yet effective learning algorithm for text
classification is the naive Bayes classifier. In this
model, a document is normally represented by a
vector of attributes . The
naive Bayes model assumes that all of the attribute
values , are independent given the category label
. Thus, a maximum a posteriori (MAP) classifier
can be constructed as follows.
To cope with features that remain unobserved dur-
ing training, the estimate of is usually ad-
justed by Laplace smoothing
where is the frequency of attribute in ,
, and . A special case of
Laplace smoothing is add one smoothing, obtained
by setting . We use add one smoothing in our
experiments below.
</bodyText>
<subsectionHeader confidence="0.998568">
3.2 Ad hoc-gram text classifiers
</subsectionHeader>
<bodyText confidence="0.999530217391304">
In this method a test document and a class label
are both represented by vectors of-gram features,
and a distance measure between the representations
of and is defined. The features to be used dur-
ing classification are usually selected by employing
heuristic methods, such as or mutual information
scoring, that involve setting cutoff thresholds and
conducting a greedy search for a good feature sub-
set. We refer this method as ad hoc-gram based
text classifier. The final classification decision is
made according to
Different distance metrics can be used in this ap-
proach. We implemented a simple re-ranking dis-
tance, which is sometimes referred to as the out-out-
place (OOP) measure (Cavnar and Trenkle, 1994).
In this method, a document is represented by an-
gram profile that contains selected-grams sorted
by decreasing frequency. For each-gram in a test
document profile, we find its counterpart in the class
profile and compute the number of places its loca-
tion differs. The distance between a test document
and a class is computed by summing the individual
out-of-place values.
</bodyText>
<subsectionHeader confidence="0.975626">
3.3 Support vector machine classifiers
</subsectionHeader>
<bodyText confidence="0.984409">
Given a set of linearly separable training exam-
ples , where each
sample belongs to one of the two classes,
, the SVM approach seeks the optimal hy-
perplane that separates the positive
and negative examples with the largest margin. The
problem can be formulated as solving the following
quadratic programming problem (Vapnik, 1995).
minimize (13)
subject to
In our experiments below, we use the
(Joachims, 1998) toolkit with default
settings.
</bodyText>
<sectionHeader confidence="0.996883" genericHeader="method">
4 Empirical evaluation
</sectionHeader>
<bodyText confidence="0.999734208333333">
We now present our experimental results on Chinese
and Japanese text classification problems. The Chi-
nese data set we used has been previously investi-
gated in (He et al., 2001). The corpus is a subset of
the TREC-5 People’s Daily news corpus published
by the Linguistic Data Consortium (LDC) in 1995.
The entire TREC-5 data set contains 164,789 docu-
ments on a variety of topics, including international
and domestic news, sports, and culture. The corpus
was originally intended for research on information
retrieval. To make the data set suitable for text cat-
egorization, documents were first clustered into 101
groups that shared the same headline (as indicated
by an SGML tag). The six most frequent groups
were selected to make a Chinese text categorization
data set.
For Japanese text classification, we consider
the Japanese text classification data investigated
by (Aizawa, 2001). This data set was converted
from the NTCIR-J1 data set originally created for
Japanese text retrieval research. The conversion pro-
cess is similar to Chinese data. The final text classi-
fication dataset has 24 categories which are unevenly
distributed.
</bodyText>
<subsectionHeader confidence="0.986241">
4.1 Experimental paradigm
</subsectionHeader>
<bodyText confidence="0.999981">
Both of the Chinese and Japanese data sets involve
classifying into a large number of categories, where
each document is assigned a single category. Many
classification techniques, such as SVMs, are intrin-
sically defined for two class problems, and have to
be extended to handle these multiple category data
</bodyText>
<equation confidence="0.77258">
distance (12)
</equation>
<bodyText confidence="0.99912975">
sets. For SVMs, we employ a standard technique of
first converting the category classification prob-
lem to binary classification problems.
For the experiments on Chinese data, we fol-
low (He et al., 2001) and convert the problem into
6 binary classification problems. In each case, we
randomly select 500 positive examples and then se-
lect 500 negative examples evenly from among the
remaining negative categories to form the training
data. The testing set contains 100 positive docu-
ments and 100 negative documents generated in the
same way. The training set and testing set do no
overlap and do not contain repeated documents.
For the experiments on Japanese data, we fol-
low (Aizawa, 2001) and directly experiment with
a 24-class classification problem. The NTCIR data
sets are unevenly distributed across categories. The
training data consists of 310,355 documents dis-
tributed unevenly among the categories (with a min-
imum of 1,747 and maximum of 83,668 documents
per category), and the testing set contains 10,000
documents unevenly distributed among categories
(with a minimum of 56 and maximum of 2,696 doc-
uments per category).
</bodyText>
<subsectionHeader confidence="0.997314">
4.2 Measuring classification performance
</subsectionHeader>
<bodyText confidence="0.999988894736842">
In the Chinese experiments, where 6 binary classifi-
cation problems are formulated, we measured classi-
fication performance by micro-averaged F-measure
scores. To calculate the micro-averaged score, we
formed an aggregate confusion matrix by adding up
the individual confusion matrices from each cate-
gory. The micro-averaged precision, recall, and F-
measure can then be computed based on the aggre-
gated confusion matrix.
For the Japanese experiments, we measured over-
all accuracy and the macro-averaged F-measure.
Here the precision, recall, and F-measures of each
individual category can be computed based on a
confusion matrix. Macro-averaged scores
can be computed by averaging the individual scores.
The overall accuracy is computed by dividing the
number of correctly identified documents (summing
the numbers across the diagonal) by the total number
of test documents.
</bodyText>
<subsectionHeader confidence="0.984169">
4.3 Results on Chinese data
</subsectionHeader>
<bodyText confidence="0.999838272727273">
Table 1 gives the results of the character level lan-
guage modeling approach, where rows correspond
to different smoothing techniques. Columns corre-
spond to different-gram order . The
entries are the micro-average F-measure. (Note that
the naive Bayes result corresponds to-gram order
1 with add one smoothing, which is italicized in the
table.) The results the ad hoc OOP classifier, and for
the SVM classifier are shown in Table 2 and Table 3
respectively, where the columns labeled ”Feature #”
are the number of features selected.
</bodyText>
<table confidence="0.999296166666667">
1 2 3 4
Add-one 0.856 0.802 0.797 0.805
Absolute 0.856 0.868 0.867 0.868
Good-Turing 0.856 0.863 0.861 0.862
Linear 0.857 0.861 0.861 0.865
Witten-Bell 0.857 0.860 0.865 0.864
</table>
<tableCaption confidence="0.994633">
Table 1: Results of character level language model-
ing classifier on Chinese data.
</tableCaption>
<table confidence="0.9999254">
Feature # Micro-F1 Feature # Micro-F1
100 0.7808 500 0.7848
200 0.8012 1000 0.7883
300 0.8087 1500 0.7664
400 0.7889 2000 0.7290
</table>
<tableCaption confidence="0.955815">
Table 2: Results of the character level OOP classifier
on Chinese data.
</tableCaption>
<table confidence="0.999898">
Feature # Micro-F1 Feature # Micro-F1
100 0.811 500 0.817
200 0.813 1000 0.817
300 0.817 1500 0.815
400 0.816 2000 0.816
</table>
<tableCaption confidence="0.993017">
Table 3: Results of the character level SVM classi-
fier on Chinese data.
</tableCaption>
<subsectionHeader confidence="0.89964">
4.4 Results on Japanese data
</subsectionHeader>
<bodyText confidence="0.99981015">
For the Japanese data, we experimented with byte
level models (where in fact each Japanese charac-
ter is represented by two bytes). We used byte level
models to avoid possible character level segmen-
tation errors that might be introduced, because we
lacked the knowledge to detect misalignment errors
in Japanese characters. The results of byte level lan-
guage modeling classifiers on the Japanese data are
shown in Table 4. (Note that the naive Bayes re-
sult corresponds to-gram order 2 with add one
smoothing, which is italicized in the table.) The re-
sults for the OOP classifier are shown in Table 5.
Note that SVM is not applied in this situation since
we are conducting multiple category classification
directly while SVM is designed for binary classifi-
cation. However, Aizawa (Aizawa, 2001) reported
a performance of abut 85% with SVMs by convert-
ing the problem into a 24 binary classification prob-
lem and by performing word segmentation as pre-
processing.
</bodyText>
<table confidence="0.999697454545455">
Feature # Accuracy Macro-F
100 0.2044 0.1692
200 0.2830 0.2308
300 0.3100 0.2677
400 0.3616 0.3118
500 0.3682 0.3295
1000 0.4416 0.4073
2000 0.4990 0.4510
3000 0.4770 0.4315
4000 0.4462 0.3820
5000 0.3706 0.3139
</table>
<tableCaption confidence="0.989563">
Table 5: Results of byte level OOP classifier on
Japanese data.
</tableCaption>
<sectionHeader confidence="0.944344" genericHeader="evaluation">
5 Discussion and analysis
</sectionHeader>
<bodyText confidence="0.999783857142857">
We now give a detailed analysis and discussion
based on the above results. We first compare the
language model based classifiers with other classi-
fiers, and then analyze the influence of the order
of the-gram model, the influence of the smooth-
ing method, and the influence of feature selection in
tradition approaches.
</bodyText>
<subsectionHeader confidence="0.999319">
5.1 Comparing classifier performance
</subsectionHeader>
<bodyText confidence="0.99994076">
Table 6 summarizes the best results obtained by each
classifier. The results for the language model (LM)
classifiers are better than (or at least comparable to )
other approaches for both the Chinese and Japanese
data, while avoiding word segmentation. The SVM
result on Japanese data is obtained from (Aizawa,
2001) where word segmentation was performed as
a preprocessing. Note that SVM classifiers do not
perform as well in our Chinese text classification
as they did in English text classification (Dumais,
1998), neither did they in Japanese text classifica-
tion (Aizawa, 2001). The reason worths further in-
vestigations.
Overall, the language modeling approach appears
to demonstrate state of the art performance for Chi-
nese and Japanese text classification. The reasons
for the improvement appear to be three-fold: First,
the language modeling approach always considers
every feature during classification, and can thereby
avoid an error-prone feature selection process. Sec-
ond, the use of-grams in the model relaxes the re-
strictive independence assumption of naive Bayes.
Third, the techniques of statistical language model-
ing offer better smoothing methods for coping with
features that are unobserved during training.
</bodyText>
<table confidence="0.9995346">
LM NB OOP SVM
Chinese Character Level
0.868 0.856 0.8087 0.817
Japanese Byte Level
0.84 0.66 0.4990 85% (Aizawa, 2001)
</table>
<tableCaption confidence="0.999833">
Table 6: Comparison of best classifier results
</tableCaption>
<subsectionHeader confidence="0.999288">
5.2 Influence of the-gram order
</subsectionHeader>
<bodyText confidence="0.9999756">
The order is a key factor in-gram language mod-
eling. An order that is too small will not capture
sufficient information to accurately model character
dependencies. On the other hand, a context that
is too large will create sparse data problems in train-
ing. In our Chinese experiments, we did not observe
significant improvement when using higher order-
gram models. The reason is due to the early onset
of sparse data problems. At the moment, we only
have limited training data for Chinese data set (1M
in size, 500 documents per class for training). If
more training data were available, the higher order
models may begin to show an advantage. For ex-
ample, in the larger Japanese data set (average 7M
size, 12,931 documents per class for training) we
</bodyText>
<table confidence="0.9993429">
Add-one Absolute Good-Turing Linear Witten-Bell
Accu. F-Mac Accu. F-Mac Accu. F-Mac Accu. F-Mac Accu. F-Mac
1 0.33 0.29 0.33 0.29 0.34 0.29 0.34 0.29 0.34 0.29
2 0.66 0.63 0.66 0.62 0.66 0.61 0.66 0.63 0.66 0.62
3 0.77 0.68 0.75 0.72 0.75 0.72 0.76 0.73 0.75 0.72
4 0.74 0.51 0.81 0.77 0.81 0.76 0.82 0.76 0.81 0.77
5 0.69 0.42 0.83 0.77 0.83 0.76 0.83 0.76 0.83 0.77
6 0.66 0.42 0.84 0.76 0.83 0.75 0.83 0.75 0.84 0.77
7 0.64 0.38 0.84 0.75 0.83 0.74 0.83 0.74 0.84 0.76
8 0.62 0.31 0.83 0.74 0.83 0.73 0.83 0.73 0.84 0.76
</table>
<tableCaption confidence="0.999836">
Table 4: Results of byte level language model classifier on Japanese data.
</tableCaption>
<bodyText confidence="0.99943675">
observe an obvious increase in classification perfor-
mance with higher order models (Table 4). How-
ever, here too, when becomes too large, overfitting
will begin to occur, as better illustrated in Figure 1.
values, a superior smoothing method in the sense of
perplexity reduction does not necessarily lead to a
better decision from the perspective of categoriza-
tion accuracy.
</bodyText>
<figure confidence="0.99447709375">
Chinese Topic Detection
0.8
0.7
0.6
0.5
0.4
0.3
0.2
1 2 3 4 5 6 7 8
0.85
0.8
Accuracy
0.75
0.7
0.65
1 1.5 2 2.5 3 3.5 4
Japanese Topic Detection
1
0.8
0.6
0.4
0.2
1 2 3 4 5 6 7 8
Accuracy
Absolute
Good−Turing
Linear
Witten−Bell
Adding One
Overall accuracy
Add one smoothing on Japanese
order n or n−gram model
</figure>
<figureCaption confidence="0.834139">
Figure 1: Effects of order of-gram language mod-
els
</figureCaption>
<subsectionHeader confidence="0.996208">
5.3 Influence of smoothing techniques
</subsectionHeader>
<bodyText confidence="0.961294928571429">
Smoothing plays an key role in language model-
ing. Its effect on classification is illustrated in Fig-
ure 2. In both cases we have examined, add one
smoothing is obviously the worst smoothing tech-
nique, since it systematically overfits much earlier
than the more sophisticated smoothing techniques.
The other smoothing techniques do not demonstrate
a significant difference in classification accuracy on
our Chinese and Japanese data, although they do
show a difference in the perplexity of the language
models themselves (not shown here to save space).
Since our goal is to make a final decision based on
the ranking of perplexities, not just their absolute
order n of n−gram models
</bodyText>
<figureCaption confidence="0.985928">
Figure 2: Effects of the smoothing techniques
</figureCaption>
<subsectionHeader confidence="0.778855">
5.4 Influence of feature selection
</subsectionHeader>
<bodyText confidence="0.995002222222222">
The number of features selected is a key factor in de-
termining the classification performance of the OOP
and SVM classifiers, as shown in Figure 3. Obvi-
ously the OOP classifier is adversely affected by in-
creasing the number of selected features. By con-
trast, the SVM classifier is very robust with respect
to the number of features, which is expected because
the complexity of the SVM classifier is determined
by the number of support vectors, not the dimension-
ality of the feature space. In practice, some heuristic
search methods are normally used to obtain an op-
timal subset of features. However, in our language
modeling based approach, we avoid explicit feature
selection by considering all possible features and
the importance of each individual feature is mea-
sured by its contribution to the perplexity (or en-
tropy) value.
number of selected features
</bodyText>
<figureCaption confidence="0.990932">
Figure 3: Effects of the number of selected features
</figureCaption>
<sectionHeader confidence="0.945789" genericHeader="related work">
5.5 Related Work
</sectionHeader>
<bodyText confidence="0.99997095">
The use of-gram models has also been extensively
investigated in information retrieval. However, un-
like previous research (Cavnar and Trenkle, 1994;
Damashek, 1995), where researchers have used-
grams as features for a traditional feature selection
process and then deployed classifiers based on cal-
culating feature-vector similarities, we consider all
-grams as features and determine their importance
implicitly by assessing their contribution to perplex-
ity. In this way, we avoid an error prone feature se-
lection step.
Language modeling for text classification is a rel-
atively new area. In principle, any language model
can be used to perform text categorization. However,
-gram models are extremely simple and have been
found to be effective in many applications. Teahan
and Harper (Teahan and Harper, 2001) used a PPM
(prediction by partial matching) model for text cate-
gorization where they seek a model that obtains the
best compression on a new document.
</bodyText>
<sectionHeader confidence="0.999681" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999962">
We have presented a simple language model based
approach without word segmentation for Chinese
and Japanese text classification. By comparison to
three standard text classifiers, the language model-
ing approach consistently demonstrates better clas-
sification accuracies while avoiding word segmen-
tation and feature selection. Although straightfor-
ward, the language modeling approach appears to
give state of the art results for Chinese and Japanese
text classification.
It has been found that word segmentation in Chi-
nese text retrieval is tricky and the relationship be-
tween word segmentation and retrieval performance
is not monotonic (Peng et al., 2002). However, since
text classification and text retrieval are two different
tasks, it is not clear whether the same relationship
exists in text classification context. We are currently
investigating this issue and interesting findings have
already been observed.
</bodyText>
<sectionHeader confidence="0.999423" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999452789473684">
A. Aizawa. 2001. Linguistic Techniques to Improve the
Performance of Automatic Text Categorization. Pro-
ceedings NLPRS2001.
W. Cavnar and J. Trenkle. 1994. N-Gram-Based Text
Categorization. Proceedings of SDAIR-94
S. Chen and J. Goodman. 1998. An Empirical Study of
Smoothing Techniques for Language Modeling. TR-
10-98, Harvard University
M. Damashek. 1995. Gauging Similarity with N-Grams:
Language-Independent Categorization of Text? Sci-
ence, 267(10), pages 843-848.
S. Dumais, J. Platt, D. Heckerman, and M. Sahami 1998.
Inductive Learning Algorithms and Representations
for Text Categorization. Proceedings of CIKM98
J. He, A. Tan, and C. Tan. 2001. On Machine Learning
Methods for Chinese Documents Classification. Ap-
plied Intelligence’s Special Issue on Text and Web Min-
ing
T. Joachims. 1998. Text Categorization with Support
Vector Machines: Learning with Many Relevant Fea-
tures. Proceedings of the ECML-1998
F. Peng, X. Huang, D. Schuurmans, and N. Cercone.
2002. Investigating the Relationship of Word Segmen-
tation Performance and Retrieval Performance in Chi-
nese IR. Proceedings of COLING2002
F. Peng and D. Schuurmans. 2003. Combining Naive
Bayes and N-Gram Language Models for Text Classi-
fication. Proceedings of ECIR2003
F. Sebastiani. 2002. Machine Learning in Automated
Text Categorization. ACM Computing Surveys, 34(1).
W. Teahan and D. Harper. 2001. Using Compression-
Based Language Models for Text Categorization. Pro-
ceedings of LMIR2001
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer-Verlag.
Y. Yang. 1999. An Evaluation of Statistical Approaches
to Text Categorization. Information Retrieval Journal,
1/2.
</reference>
<figure confidence="0.998099375">
0.85
0.7
0 200 400 600 800 1000 1200 1400 1600 1800 2000
OOP
SVM
M����−F
0.75
0.8
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.792960">
<title confidence="0.999915">Text Classification in Asian Languages without Word Segmentation</title>
<author confidence="0.998872">Fuchun Peng Xiangji Huang Dale Schuurmans Shaojun Wang</author>
<affiliation confidence="0.978488666666667">School of Computer Science, University of Waterloo, Ontario, Canada Department of Computer Science, University of Massachusetts, Amherst, MA, USA Department of Statistics, University of Toronto, Ontario, Canada</affiliation>
<email confidence="0.98149">f3peng,jhuang,dale,sjwang@ai.uwaterloo.ca</email>
<abstract confidence="0.994253958333333">We present a simple approach for Asian language text classification without word segmentation, based on statistical-gram language modeling. In particular, we examine Chinese and Japanese text classification. With character-gram models, our approach avoids word segmentation. However, unlike traditional ad hoc-gram models, the statistical language modeling based approach has strong information theoretic basis and avoids explicit feature selection procedure which potentially loses significantly amount of useful information. We systematically study the key factors in language modeling and their influence on classification. Experiments on Chinese TREC and Japanese NTCIR topic detection show that the simple approach can achieve better performance compared to traditional approaches while avoiding word segmentation, which demonstrates its superiority in Asian language text classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Aizawa</author>
</authors>
<title>Linguistic Techniques to Improve the Performance of Automatic Text Categorization.</title>
<date>2001</date>
<booktitle>Proceedings NLPRS2001.</booktitle>
<contexts>
<context position="2372" citStr="Aizawa, 2001" startWordPosition="338" endWordPosition="339">arch that introduces a number of additional difficulties. One difficulty with Chinese and Japanese text classification is that, unlike English, Chinese and Japanese texts do not have explicit whitespace between words. This means that some form of word segmentation is normally required before further processing. However, word segmentation itself is a difficult problem in these languages. A second difficulty is that there is a lack of standard benchmark data sets for these languages. Nevertheless, recently, there has been significant notable progress on Chinese and Japanese text classification (Aizawa, 2001; He et al., 2001). Many standard machine learning techniques have been applied to text categorization problems, such as naive Bayes classifiers, support vector machines, linear least squares models, neural networks, and knearest neighbor classifiers (Sebastiani, 2002; Yang, 1999). Unfortunately, most current text classifiers work with word level features. However, word identification in Asian languages, such as Chinese and Japanese, is itself a hard problem. To avoid the word segmentation problems, character levelgram models have been proposed (Cavnar and Trenkle, 1994; Damashek, 1995). There</context>
<context position="3716" citStr="Aizawa, 2001" startWordPosition="534" endWordPosition="535">-vector similarities. This approach has many shortcomings. First, there are an enormous number of possible features to consider in text categorization, and standard feature selection approaches do not always cope well in such circumstances. For example, given a sufficiently large number of features, the cumulative effect of uncommon features can still have an important effect on classification accuracy, even though infrequent features contribute less information than common features individually. Therefore, throwing away uncommon features is usually not an appropriate strategy in this domain (Aizawa, 2001). Another problem is that feature selection normally uses indirect tests, such as or mutual information, which involve setting arbitrary thresholds and conducting a heuristic greedy search to find a good subset of features. Moreover, by treating text categorization as a classical classification problem, standard approaches can ignore the fact that texts are written in natural language, which means that they have many implicit regularities that can be well modeled by specific tools from natural language processing. In this paper, we present a simple text categorization approach based on statist</context>
<context position="12721" citStr="Aizawa, 2001" startWordPosition="1970" endWordPosition="1971">ta Consortium (LDC) in 1995. The entire TREC-5 data set contains 164,789 documents on a variety of topics, including international and domestic news, sports, and culture. The corpus was originally intended for research on information retrieval. To make the data set suitable for text categorization, documents were first clustered into 101 groups that shared the same headline (as indicated by an SGML tag). The six most frequent groups were selected to make a Chinese text categorization data set. For Japanese text classification, we consider the Japanese text classification data investigated by (Aizawa, 2001). This data set was converted from the NTCIR-J1 data set originally created for Japanese text retrieval research. The conversion process is similar to Chinese data. The final text classification dataset has 24 categories which are unevenly distributed. 4.1 Experimental paradigm Both of the Chinese and Japanese data sets involve classifying into a large number of categories, where each document is assigned a single category. Many classification techniques, such as SVMs, are intrinsically defined for two class problems, and have to be extended to handle these multiple category data distance (12)</context>
<context position="14010" citStr="Aizawa, 2001" startWordPosition="2176" endWordPosition="2177">gory classification problem to binary classification problems. For the experiments on Chinese data, we follow (He et al., 2001) and convert the problem into 6 binary classification problems. In each case, we randomly select 500 positive examples and then select 500 negative examples evenly from among the remaining negative categories to form the training data. The testing set contains 100 positive documents and 100 negative documents generated in the same way. The training set and testing set do no overlap and do not contain repeated documents. For the experiments on Japanese data, we follow (Aizawa, 2001) and directly experiment with a 24-class classification problem. The NTCIR data sets are unevenly distributed across categories. The training data consists of 310,355 documents distributed unevenly among the categories (with a minimum of 1,747 and maximum of 83,668 documents per category), and the testing set contains 10,000 documents unevenly distributed among categories (with a minimum of 56 and maximum of 2,696 documents per category). 4.2 Measuring classification performance In the Chinese experiments, where 6 binary classification problems are formulated, we measured classification perfor</context>
<context position="17400" citStr="Aizawa, 2001" startWordPosition="2713" endWordPosition="2714">acter level segmentation errors that might be introduced, because we lacked the knowledge to detect misalignment errors in Japanese characters. The results of byte level language modeling classifiers on the Japanese data are shown in Table 4. (Note that the naive Bayes result corresponds to-gram order 2 with add one smoothing, which is italicized in the table.) The results for the OOP classifier are shown in Table 5. Note that SVM is not applied in this situation since we are conducting multiple category classification directly while SVM is designed for binary classification. However, Aizawa (Aizawa, 2001) reported a performance of abut 85% with SVMs by converting the problem into a 24 binary classification problem and by performing word segmentation as preprocessing. Feature # Accuracy Macro-F 100 0.2044 0.1692 200 0.2830 0.2308 300 0.3100 0.2677 400 0.3616 0.3118 500 0.3682 0.3295 1000 0.4416 0.4073 2000 0.4990 0.4510 3000 0.4770 0.4315 4000 0.4462 0.3820 5000 0.3706 0.3139 Table 5: Results of byte level OOP classifier on Japanese data. 5 Discussion and analysis We now give a detailed analysis and discussion based on the above results. We first compare the language model based classifiers wit</context>
<context position="18805" citStr="Aizawa, 2001" startWordPosition="2938" endWordPosition="2939">omparing classifier performance Table 6 summarizes the best results obtained by each classifier. The results for the language model (LM) classifiers are better than (or at least comparable to ) other approaches for both the Chinese and Japanese data, while avoiding word segmentation. The SVM result on Japanese data is obtained from (Aizawa, 2001) where word segmentation was performed as a preprocessing. Note that SVM classifiers do not perform as well in our Chinese text classification as they did in English text classification (Dumais, 1998), neither did they in Japanese text classification (Aizawa, 2001). The reason worths further investigations. Overall, the language modeling approach appears to demonstrate state of the art performance for Chinese and Japanese text classification. The reasons for the improvement appear to be three-fold: First, the language modeling approach always considers every feature during classification, and can thereby avoid an error-prone feature selection process. Second, the use of-grams in the model relaxes the restrictive independence assumption of naive Bayes. Third, the techniques of statistical language modeling offer better smoothing methods for coping with f</context>
</contexts>
<marker>Aizawa, 2001</marker>
<rawString>A. Aizawa. 2001. Linguistic Techniques to Improve the Performance of Automatic Text Categorization. Proceedings NLPRS2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Cavnar</author>
<author>J Trenkle</author>
</authors>
<title>N-Gram-Based Text Categorization.</title>
<date>1994</date>
<booktitle>Proceedings of SDAIR-94</booktitle>
<contexts>
<context position="2948" citStr="Cavnar and Trenkle, 1994" startWordPosition="419" endWordPosition="423">e and Japanese text classification (Aizawa, 2001; He et al., 2001). Many standard machine learning techniques have been applied to text categorization problems, such as naive Bayes classifiers, support vector machines, linear least squares models, neural networks, and knearest neighbor classifiers (Sebastiani, 2002; Yang, 1999). Unfortunately, most current text classifiers work with word level features. However, word identification in Asian languages, such as Chinese and Japanese, is itself a hard problem. To avoid the word segmentation problems, character levelgram models have been proposed (Cavnar and Trenkle, 1994; Damashek, 1995). There, they usedgrams as features for a traditional feature selection process and then deployed classifiers based on calculating feature-vector similarities. This approach has many shortcomings. First, there are an enormous number of possible features to consider in text categorization, and standard feature selection approaches do not always cope well in such circumstances. For example, given a sufficiently large number of features, the cumulative effect of uncommon features can still have an important effect on classification accuracy, even though infrequent features contri</context>
<context position="10960" citStr="Cavnar and Trenkle, 1994" startWordPosition="1688" endWordPosition="1691">am features, and a distance measure between the representations of and is defined. The features to be used during classification are usually selected by employing heuristic methods, such as or mutual information scoring, that involve setting cutoff thresholds and conducting a greedy search for a good feature subset. We refer this method as ad hoc-gram based text classifier. The final classification decision is made according to Different distance metrics can be used in this approach. We implemented a simple re-ranking distance, which is sometimes referred to as the out-outplace (OOP) measure (Cavnar and Trenkle, 1994). In this method, a document is represented by angram profile that contains selected-grams sorted by decreasing frequency. For each-gram in a test document profile, we find its counterpart in the class profile and compute the number of places its location differs. The distance between a test document and a class is computed by summing the individual out-of-place values. 3.3 Support vector machine classifiers Given a set of linearly separable training examples , where each sample belongs to one of the two classes, , the SVM approach seeks the optimal hyperplane that separates the positive and n</context>
<context position="23608" citStr="Cavnar and Trenkle, 1994" startWordPosition="3741" endWordPosition="3744"> dimensionality of the feature space. In practice, some heuristic search methods are normally used to obtain an optimal subset of features. However, in our language modeling based approach, we avoid explicit feature selection by considering all possible features and the importance of each individual feature is measured by its contribution to the perplexity (or entropy) value. number of selected features Figure 3: Effects of the number of selected features 5.5 Related Work The use of-gram models has also been extensively investigated in information retrieval. However, unlike previous research (Cavnar and Trenkle, 1994; Damashek, 1995), where researchers have usedgrams as features for a traditional feature selection process and then deployed classifiers based on calculating feature-vector similarities, we consider all -grams as features and determine their importance implicitly by assessing their contribution to perplexity. In this way, we avoid an error prone feature selection step. Language modeling for text classification is a relatively new area. In principle, any language model can be used to perform text categorization. However, -gram models are extremely simple and have been found to be effective in </context>
</contexts>
<marker>Cavnar, Trenkle, 1994</marker>
<rawString>W. Cavnar and J. Trenkle. 1994. N-Gram-Based Text Categorization. Proceedings of SDAIR-94</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
<author>J Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modeling. TR10-98,</title>
<date>1998</date>
<location>Harvard University</location>
<contexts>
<context position="7294" citStr="Chen and Goodman, 1998" startWordPosition="1090" endWordPosition="1093"> Our approach to applying language models to text categorization is to use Bayesian decision theory. Assume we wish to classify a text into a category . A natural choice is to pick the category that has the largest posterior probability given the text. That is, (7) (5) is the discounted probability, and is a normalization constant calculated to be Using Bayes rule, this can be rewritten as (8) (6) The discounted probability (5) can be computed using different smoothing approaches including Laplace smoothing, linear smoothing, absolute smoothing, Good-Turing smoothing and WittenBell smoothing (Chen and Goodman, 1998). The language models described above use individual words as the basic unit, although one could instead consider models that use individual characters as the basic unit. The remaining details remain the same in this case. The only difference is that the character vocabulary is always much smaller than the word vocabulary, which means that one can normally use a much higher order,, in a character level-gram model (although the text spanned by a character model is still usually less than that spanned by a word model). The benefits of the character level model in the context of text classificati</context>
</contexts>
<marker>Chen, Goodman, 1998</marker>
<rawString>S. Chen and J. Goodman. 1998. An Empirical Study of Smoothing Techniques for Language Modeling. TR10-98, Harvard University</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Damashek</author>
</authors>
<title>Gauging Similarity with N-Grams:</title>
<date>1995</date>
<journal>Language-Independent Categorization of Text? Science,</journal>
<volume>267</volume>
<issue>10</issue>
<pages>843--848</pages>
<contexts>
<context position="2965" citStr="Damashek, 1995" startWordPosition="424" endWordPosition="425">fication (Aizawa, 2001; He et al., 2001). Many standard machine learning techniques have been applied to text categorization problems, such as naive Bayes classifiers, support vector machines, linear least squares models, neural networks, and knearest neighbor classifiers (Sebastiani, 2002; Yang, 1999). Unfortunately, most current text classifiers work with word level features. However, word identification in Asian languages, such as Chinese and Japanese, is itself a hard problem. To avoid the word segmentation problems, character levelgram models have been proposed (Cavnar and Trenkle, 1994; Damashek, 1995). There, they usedgrams as features for a traditional feature selection process and then deployed classifiers based on calculating feature-vector similarities. This approach has many shortcomings. First, there are an enormous number of possible features to consider in text categorization, and standard feature selection approaches do not always cope well in such circumstances. For example, given a sufficiently large number of features, the cumulative effect of uncommon features can still have an important effect on classification accuracy, even though infrequent features contribute less informa</context>
<context position="23625" citStr="Damashek, 1995" startWordPosition="3745" endWordPosition="3746">ture space. In practice, some heuristic search methods are normally used to obtain an optimal subset of features. However, in our language modeling based approach, we avoid explicit feature selection by considering all possible features and the importance of each individual feature is measured by its contribution to the perplexity (or entropy) value. number of selected features Figure 3: Effects of the number of selected features 5.5 Related Work The use of-gram models has also been extensively investigated in information retrieval. However, unlike previous research (Cavnar and Trenkle, 1994; Damashek, 1995), where researchers have usedgrams as features for a traditional feature selection process and then deployed classifiers based on calculating feature-vector similarities, we consider all -grams as features and determine their importance implicitly by assessing their contribution to perplexity. In this way, we avoid an error prone feature selection step. Language modeling for text classification is a relatively new area. In principle, any language model can be used to perform text categorization. However, -gram models are extremely simple and have been found to be effective in many applications</context>
</contexts>
<marker>Damashek, 1995</marker>
<rawString>M. Damashek. 1995. Gauging Similarity with N-Grams: Language-Independent Categorization of Text? Science, 267(10), pages 843-848.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>J Platt</author>
<author>D Heckerman</author>
<author>M Sahami</author>
</authors>
<title>Inductive Learning Algorithms and Representations for Text Categorization.</title>
<date>1998</date>
<booktitle>Proceedings of CIKM98</booktitle>
<marker>Dumais, Platt, Heckerman, Sahami, 1998</marker>
<rawString>S. Dumais, J. Platt, D. Heckerman, and M. Sahami 1998. Inductive Learning Algorithms and Representations for Text Categorization. Proceedings of CIKM98</rawString>
</citation>
<citation valid="true">
<authors>
<author>J He</author>
<author>A Tan</author>
<author>C Tan</author>
</authors>
<title>On Machine Learning Methods for Chinese Documents Classification. Applied Intelligence’s Special Issue on Text and Web Mining</title>
<date>2001</date>
<contexts>
<context position="2390" citStr="He et al., 2001" startWordPosition="340" endWordPosition="343">oduces a number of additional difficulties. One difficulty with Chinese and Japanese text classification is that, unlike English, Chinese and Japanese texts do not have explicit whitespace between words. This means that some form of word segmentation is normally required before further processing. However, word segmentation itself is a difficult problem in these languages. A second difficulty is that there is a lack of standard benchmark data sets for these languages. Nevertheless, recently, there has been significant notable progress on Chinese and Japanese text classification (Aizawa, 2001; He et al., 2001). Many standard machine learning techniques have been applied to text categorization problems, such as naive Bayes classifiers, support vector machines, linear least squares models, neural networks, and knearest neighbor classifiers (Sebastiani, 2002; Yang, 1999). Unfortunately, most current text classifiers work with word level features. However, word identification in Asian languages, such as Chinese and Japanese, is itself a hard problem. To avoid the word segmentation problems, character levelgram models have been proposed (Cavnar and Trenkle, 1994; Damashek, 1995). There, they usedgrams a</context>
<context position="12012" citStr="He et al., 2001" startWordPosition="1857" endWordPosition="1860">separable training examples , where each sample belongs to one of the two classes, , the SVM approach seeks the optimal hyperplane that separates the positive and negative examples with the largest margin. The problem can be formulated as solving the following quadratic programming problem (Vapnik, 1995). minimize (13) subject to In our experiments below, we use the (Joachims, 1998) toolkit with default settings. 4 Empirical evaluation We now present our experimental results on Chinese and Japanese text classification problems. The Chinese data set we used has been previously investigated in (He et al., 2001). The corpus is a subset of the TREC-5 People’s Daily news corpus published by the Linguistic Data Consortium (LDC) in 1995. The entire TREC-5 data set contains 164,789 documents on a variety of topics, including international and domestic news, sports, and culture. The corpus was originally intended for research on information retrieval. To make the data set suitable for text categorization, documents were first clustered into 101 groups that shared the same headline (as indicated by an SGML tag). The six most frequent groups were selected to make a Chinese text categorization data set. For J</context>
<context position="13524" citStr="He et al., 2001" startWordPosition="2094" endWordPosition="2097">ication dataset has 24 categories which are unevenly distributed. 4.1 Experimental paradigm Both of the Chinese and Japanese data sets involve classifying into a large number of categories, where each document is assigned a single category. Many classification techniques, such as SVMs, are intrinsically defined for two class problems, and have to be extended to handle these multiple category data distance (12) sets. For SVMs, we employ a standard technique of first converting the category classification problem to binary classification problems. For the experiments on Chinese data, we follow (He et al., 2001) and convert the problem into 6 binary classification problems. In each case, we randomly select 500 positive examples and then select 500 negative examples evenly from among the remaining negative categories to form the training data. The testing set contains 100 positive documents and 100 negative documents generated in the same way. The training set and testing set do no overlap and do not contain repeated documents. For the experiments on Japanese data, we follow (Aizawa, 2001) and directly experiment with a 24-class classification problem. The NTCIR data sets are unevenly distributed acro</context>
</contexts>
<marker>He, Tan, Tan, 2001</marker>
<rawString>J. He, A. Tan, and C. Tan. 2001. On Machine Learning Methods for Chinese Documents Classification. Applied Intelligence’s Special Issue on Text and Web Mining</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with Support Vector Machines: Learning with Many Relevant Features.</title>
<date>1998</date>
<booktitle>Proceedings of the ECML-1998</booktitle>
<contexts>
<context position="11781" citStr="Joachims, 1998" startWordPosition="1822" endWordPosition="1823">ile and compute the number of places its location differs. The distance between a test document and a class is computed by summing the individual out-of-place values. 3.3 Support vector machine classifiers Given a set of linearly separable training examples , where each sample belongs to one of the two classes, , the SVM approach seeks the optimal hyperplane that separates the positive and negative examples with the largest margin. The problem can be formulated as solving the following quadratic programming problem (Vapnik, 1995). minimize (13) subject to In our experiments below, we use the (Joachims, 1998) toolkit with default settings. 4 Empirical evaluation We now present our experimental results on Chinese and Japanese text classification problems. The Chinese data set we used has been previously investigated in (He et al., 2001). The corpus is a subset of the TREC-5 People’s Daily news corpus published by the Linguistic Data Consortium (LDC) in 1995. The entire TREC-5 data set contains 164,789 documents on a variety of topics, including international and domestic news, sports, and culture. The corpus was originally intended for research on information retrieval. To make the data set suitabl</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Text Categorization with Support Vector Machines: Learning with Many Relevant Features. Proceedings of the ECML-1998</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>X Huang</author>
<author>D Schuurmans</author>
<author>N Cercone</author>
</authors>
<title>Investigating the Relationship of Word Segmentation Performance and Retrieval Performance in Chinese IR.</title>
<date>2002</date>
<booktitle>Proceedings of COLING2002</booktitle>
<marker>Peng, Huang, Schuurmans, Cercone, 2002</marker>
<rawString>F. Peng, X. Huang, D. Schuurmans, and N. Cercone. 2002. Investigating the Relationship of Word Segmentation Performance and Retrieval Performance in Chinese IR. Proceedings of COLING2002</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>D Schuurmans</author>
</authors>
<title>Combining Naive Bayes and N-Gram Language Models for Text Classification.</title>
<date>2003</date>
<booktitle>Proceedings of ECIR2003</booktitle>
<contexts>
<context position="9411" citStr="Peng and Schuurmans, 2003" startWordPosition="1434" endWordPosition="1437">or can be used to incorporate more assumptions, such as a uniform or Dirichelet distribution. Therefore, our approach is to learn a separate back-off language model for each category, by training on a data set from that category. Then, to categorize a new text, we supply to each language model, evaluate the likelihood (or entropy) of under the model, and pick the winning category according to Equ. (9). The inference of an-gram based text classifier is very similar to a naive Bayes classifier (to be dicussed below). In fact,-gram classifiers are a straightforward generalization of naive Bayes (Peng and Schuurmans, 2003). 3 Traditional Text Classifiers We introduce the three standard text classifiers that we will compare against below. 3.1 Naive Bayes classifiers A simple yet effective learning algorithm for text classification is the naive Bayes classifier. In this model, a document is normally represented by a vector of attributes . The naive Bayes model assumes that all of the attribute values , are independent given the category label . Thus, a maximum a posteriori (MAP) classifier can be constructed as follows. To cope with features that remain unobserved during training, the estimate of is usually adjus</context>
</contexts>
<marker>Peng, Schuurmans, 2003</marker>
<rawString>F. Peng and D. Schuurmans. 2003. Combining Naive Bayes and N-Gram Language Models for Text Classification. Proceedings of ECIR2003</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Sebastiani</author>
</authors>
<title>Machine Learning in Automated Text Categorization.</title>
<date>2002</date>
<journal>ACM Computing Surveys,</journal>
<volume>34</volume>
<issue>1</issue>
<contexts>
<context position="2640" citStr="Sebastiani, 2002" startWordPosition="375" endWordPosition="376"> is normally required before further processing. However, word segmentation itself is a difficult problem in these languages. A second difficulty is that there is a lack of standard benchmark data sets for these languages. Nevertheless, recently, there has been significant notable progress on Chinese and Japanese text classification (Aizawa, 2001; He et al., 2001). Many standard machine learning techniques have been applied to text categorization problems, such as naive Bayes classifiers, support vector machines, linear least squares models, neural networks, and knearest neighbor classifiers (Sebastiani, 2002; Yang, 1999). Unfortunately, most current text classifiers work with word level features. However, word identification in Asian languages, such as Chinese and Japanese, is itself a hard problem. To avoid the word segmentation problems, character levelgram models have been proposed (Cavnar and Trenkle, 1994; Damashek, 1995). There, they usedgrams as features for a traditional feature selection process and then deployed classifiers based on calculating feature-vector similarities. This approach has many shortcomings. First, there are an enormous number of possible features to consider in text c</context>
</contexts>
<marker>Sebastiani, 2002</marker>
<rawString>F. Sebastiani. 2002. Machine Learning in Automated Text Categorization. ACM Computing Surveys, 34(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Teahan</author>
<author>D Harper</author>
</authors>
<title>Using CompressionBased Language Models for Text Categorization.</title>
<date>2001</date>
<booktitle>Proceedings of LMIR2001</booktitle>
<contexts>
<context position="24270" citStr="Teahan and Harper, 2001" startWordPosition="3842" endWordPosition="3845">ave usedgrams as features for a traditional feature selection process and then deployed classifiers based on calculating feature-vector similarities, we consider all -grams as features and determine their importance implicitly by assessing their contribution to perplexity. In this way, we avoid an error prone feature selection step. Language modeling for text classification is a relatively new area. In principle, any language model can be used to perform text categorization. However, -gram models are extremely simple and have been found to be effective in many applications. Teahan and Harper (Teahan and Harper, 2001) used a PPM (prediction by partial matching) model for text categorization where they seek a model that obtains the best compression on a new document. 6 Conclusion We have presented a simple language model based approach without word segmentation for Chinese and Japanese text classification. By comparison to three standard text classifiers, the language modeling approach consistently demonstrates better classification accuracies while avoiding word segmentation and feature selection. Although straightforward, the language modeling approach appears to give state of the art results for Chinese </context>
</contexts>
<marker>Teahan, Harper, 2001</marker>
<rawString>W. Teahan and D. Harper. 2001. Using CompressionBased Language Models for Text Categorization. Proceedings of LMIR2001</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Vapnik</author>
</authors>
<title>The Nature of Statistical Learning Theory.</title>
<date>1995</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="11701" citStr="Vapnik, 1995" startWordPosition="1809" endWordPosition="1810">ach-gram in a test document profile, we find its counterpart in the class profile and compute the number of places its location differs. The distance between a test document and a class is computed by summing the individual out-of-place values. 3.3 Support vector machine classifiers Given a set of linearly separable training examples , where each sample belongs to one of the two classes, , the SVM approach seeks the optimal hyperplane that separates the positive and negative examples with the largest margin. The problem can be formulated as solving the following quadratic programming problem (Vapnik, 1995). minimize (13) subject to In our experiments below, we use the (Joachims, 1998) toolkit with default settings. 4 Empirical evaluation We now present our experimental results on Chinese and Japanese text classification problems. The Chinese data set we used has been previously investigated in (He et al., 2001). The corpus is a subset of the TREC-5 People’s Daily news corpus published by the Linguistic Data Consortium (LDC) in 1995. The entire TREC-5 data set contains 164,789 documents on a variety of topics, including international and domestic news, sports, and culture. The corpus was origina</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>V. Vapnik. 1995. The Nature of Statistical Learning Theory. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
</authors>
<title>An Evaluation of Statistical Approaches to Text Categorization.</title>
<date>1999</date>
<journal>Information Retrieval Journal,</journal>
<volume>1</volume>
<contexts>
<context position="1618" citStr="Yang, 1999" startWordPosition="224" endWordPosition="225">ion. Experiments on Chinese TREC and Japanese NTCIR topic detection show that the simple approach can achieve better performance compared to traditional approaches while avoiding word segmentation, which demonstrates its superiority in Asian language text classification. 1 Introduction Text classification addresses the problem of assigning a given passage of text (or a document) to one or more predefined classes. This is an important area of information retrieval research that has been heavily investigated, although most of the research activity has concentrated on English text (Dumais, 1998; Yang, 1999). Text classification in Asian languages such as Chinese and Japanese, however, is also an important (and relatively more recent) area of research that introduces a number of additional difficulties. One difficulty with Chinese and Japanese text classification is that, unlike English, Chinese and Japanese texts do not have explicit whitespace between words. This means that some form of word segmentation is normally required before further processing. However, word segmentation itself is a difficult problem in these languages. A second difficulty is that there is a lack of standard benchmark da</context>
</contexts>
<marker>Yang, 1999</marker>
<rawString>Y. Yang. 1999. An Evaluation of Statistical Approaches to Text Categorization. Information Retrieval Journal, 1/2.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>