<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003560">
<title confidence="0.9983925">
Timed Annotations — Enhancing MUC7 Metadata
by the Time It Takes to Annotate Named Entities
</title>
<author confidence="0.992886">
Katrin Tomanek and Udo Hahn
</author>
<affiliation confidence="0.828956">
Jena University Language &amp; Information Engineering (JULIE) Lab
Friedrich-Schiller-Universit¨at Jena, Germany
</affiliation>
<email confidence="0.700432">
{katrin.tomanek|udo.hahn}@uni-jena.de
</email>
<sectionHeader confidence="0.986388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999977363636363">
We report on the re-annotation of selected
types of named entities from the MUC7
corpus where our focus lies on record-
ing the time it takes to annotate these
entities given two basic annotation units
– sentences vs. complex noun phrases.
Such information may be helpful to lay
the empirical foundations for the develop-
ment of cost measures for annotation pro-
cesses based on the investment in time for
decision-making per entity mention.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999890666666667">
Manually supplied annotation metadata is at the
heart of (semi)supervised machine learning tech-
niques which have become very popular in NLP
research. At their flipside, they create an enor-
mous bottleneck because major shifts in the do-
main of discourse, the basic entities of interest, or
the text genre often require new annotation efforts.
But annotations are costly in terms of getting well-
trained and intelligible human resources involved.
Surprisingly, cost awareness has not been a pri-
mary concern in most of the past linguistic anno-
tation initiatives. Only recently, annotation strate-
gies (such as Active Learning (Cohn et al., 1996))
which strive for minimizing the annotation load
have gained increasing attention. Still, when it
comes to the empirically plausible assessment of
annotation costs even proponents of Active Learn-
ing make overly simplistic and empirically ques-
tionable assumptions, e.g., the uniformity of an-
notation costs over the number of linguistic units
(e.g., tokens) to be annotated.
We here consider the time it takes to annotate
a particular entity mention as a natural indicator
of effort for named entity annotations. In order to
lay the empirical foundations for experimentally
grounded annotation cost models we couple com-
mon named entity annotation metadata with a time
stamp reflecting the time measured for decision
making.1
Previously, two studies – one dealing with POS
annotation (Haertel et al., 2008), the other with
named entity and relation annotation (Settles et al.,
2008) – have measured the time needed to anno-
tate sentences on small data sets and attempted to
learn predictive models of annotation cost. How-
ever, these data sets do not meet our requirements
as we envisage a large, coherent, and also well-
known newspaper entity corpus extended by an-
notation costs on a fine-grained level. Especially
size and coherence of such a corpus are not only
essential for building accurate cost models but also
as a reference baseline for cost-sensitive annota-
tion strategies. Moreover, the annotation level for
which cost information is available is crucial be-
cause document- or sentence-level data might be
too coarse for several applications. Accordingly,
this paper introduces MUC7T, our extension to the
entity annotations of the MUC7 corpus (Linguis-
tic Data Consortium, 2001) where time stamps are
added to two levels of annotation granularity, viz.
sentences and complex noun phrases.
</bodyText>
<sectionHeader confidence="0.910803" genericHeader="method">
2 Corpus Annotation
</sectionHeader>
<subsectionHeader confidence="0.99617">
2.1 Annotation Task
</subsectionHeader>
<bodyText confidence="0.9998946">
Our annotation initiative constitutes an extension
to the named entity annotations (ENAMEX) of the
English part of the MUC7 corpus covering New
York Times articles from 1996. ENAMEX annota-
tions cover three types of named entities, viz. PER-
SONS, LOCATIONS, and ORGANIZATIONS. We in-
structed two human annotators, both advanced stu-
dents of linguistics with good English language
skills, to re-annotate the MUC7 corpus for the
ENAMEX subtask. To be as consistent as possi-
</bodyText>
<footnote confidence="0.9506565">
1These time stamps should not be confounded with the an-
notation of temporal expressions (TIMEX in MUC7, or even
more advanced metadata using TIMEML for the creation of
the TIMEBANK (Pustejovsky et al., 2003)).
</footnote>
<page confidence="0.911607">
112
</page>
<note confidence="0.958402">
Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 112–115,
Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.999907444444445">
ble with the existing MUC7 annotations, the an-
notators had to follow the original guidelines of
the MUC7 named entity task. For ease of re-
annotation, we intentionally ignored temporal and
number expressions (TIMEX and NUMEX).
MUC7 covers three distinct document sets for
the named entity task. We used one of these sets
to train the annotators and develop the annotation
design, and another one for our actual annotation
initiative which consists of 100 articles reporting
on airplane crashes. We split lengthy documents
(27 out of 100) into halves to fit on the annota-
tion screen without the need for scrolling. Further-
more, we excluded two documents due to over-
length which would have required overly many
splits. Our final corpus contains 3,113 sentences
(76,900 tokens) (see Section 3.1 for more details).
Time-stamped ENAMEX annotation of this cor-
pus constitutes MUC7T, our extension of MUC7.
Annotation time measurements were taken on two
syntactically different annotation units of single
documents: (a) complete sentences and (b) com-
plex noun phrases. The annotation task was de-
fined such as to assign an entity type label to each
token of an annotation unit. Sentence-level anno-
tation units where derived by the OPENNLP2 sen-
tence splitter. The use of complex noun phrases
(CNPs) as an alternative annotation unit is mo-
tivated by the fact that in MUC7 the syntactic
encoding of named entity mentions basically oc-
curs through nominal phrases. CNPs were derived
from the sentences’ constituency structure using
the OPENNLP parser (trained on PENNTREE-
BANK data) to determine top-level noun phrases.
To avoid overly long phrases, CNPs dominating
special syntactic structures, such as coordinations,
appositions, or relative clauses, were split up at
discriminative functional elements (e.g., a relative
pronoun) and these elements were eliminated. An
evaluation of our CNP extractor on ENAMEX an-
notations in MUC7 showed that 98.95% of all en-
tities where completely covered by automatically
identified CNPs. For the remaining 1.05% of the
entity mentions, parsing errors were the most com-
mon source of incomplete coverage.
</bodyText>
<subsectionHeader confidence="0.999079">
2.2 Annotation and Time Measurement
</subsectionHeader>
<bodyText confidence="0.997867">
While the annotation task itself was “officially”
declared to yield only annotations of named en-
tity mentions within the different annotation units,
</bodyText>
<footnote confidence="0.4904">
2http://opennlp.sourceforge.net
</footnote>
<bodyText confidence="0.99941331372549">
we were primarily interested in the time needed
for these annotations. For precise time measure-
ments, single annotation examples were shown to
the annotators, one at a time. An annotation exam-
ple consists of the chosen MUC7 document with
one annotation unit (sentence or CNP) selected
and highlighted. Only the highlighted part of the
document could be annotated and the annotators
were asked to read only as much of the context sur-
rounding the annotation unit as necessary to make
a proper annotation decision. To present the an-
notation examples to annotators and allow for an-
notation without extra time overhead for the “me-
chanical” assignment of entity types, our annota-
tion GUI is controlled by keyboard shortcuts. This
minimizes annotation time compared to mouse-
controlled annotation such that the measured time
reflects only the amount of time needed for taking
an annotation decision.
In order to avoid learning effects at the annota-
tors’ side on originally consecutive syntactic sub-
units, we randomly shuffled all annotation exam-
ples so that subsequent annotation examples were
not drawn from the same document. Hence, an-
notation times were not biased by the order of ap-
pearance of the annotation examples.
Annotators were given blocks of either 500
CNP- or 100 sentence-level annotation examples.
They were asked to annotate each block in a single
run under noise-free conditions, without breaks
and disruptions. They were also instructed not to
annotate for too long stretches of time to avoid tir-
ing effects making time measurements unreliable.
All documents were first annotated with respect
to CNP-level examples within 2-3 weeks, with
only very few hours per day of concrete annota-
tion work. After completion of the CNP-level an-
notation, the same documents had to be annotated
on the sentence level as well. Due to randomiza-
tion and rare access to surrounding context during
the CNP-level annotation, annotators credibly re-
ported that they had indeed not remembered the
sentences from the CNP-level round. Thus, the
time measurements taken on the sentence level do
not seem to exhibit any human memory bias.
Both annotators went through all annotation ex-
amples so that we have double annotations of the
complete data set. Prior to coding, they indepen-
dently got used to the annotation guidelines and
were trained on several hundred examples. For the
annotators’ performance see Section 3.2.
</bodyText>
<page confidence="0.998635">
113
</page>
<sectionHeader confidence="0.998115" genericHeader="method">
3 Analysis
</sectionHeader>
<subsectionHeader confidence="0.999946">
3.1 Corpus Statistics
</subsectionHeader>
<bodyText confidence="0.998776555555556">
Table 1 summarizes statistics on the time-stamped
MUC7 corpus. About 60% of all tokens are cov-
ered by CNPs (45,097 out of 76,900 tokens) show-
ing that sentences are made up from CNPs to a
large extent. Still, removing the non-CNP to-
kens markedly reduces the amount of tokens to
be considered for entity annotation. CNPs cover
slightly less entities (3,937) than complete sen-
tences (3,971), a marginal loss only.
</bodyText>
<table confidence="0.728943625">
sentences 3,113
sentence tokens 76,900
chunks 15,203
chunk tokens 45,097
entity mentions in sentences 3,971
entity mentions in CNPs 3,937
sentences with entity mentions 63%
CNPs with entity mentions 23%
</table>
<tableCaption confidence="0.999293">
Table 1: Descriptive statistics of time-stamped MUC7 corpus
</tableCaption>
<bodyText confidence="0.999199555555556">
On the average, sentences have a length of
24.7 tokens, while CNPs are rather short with
3.0 tokens, on the average. However, CNPs vary
tremendously in their length, with the shortest
ones having only one token and the longest ones
(mostly due to parsing errors) spanning over 30
(and more) tokens. Figure 1 depicts the length
distribution of sentences and CNPs showing that
a reasonable portion of CNPs have less than five
tokens, while the distribution of sentence lengths
almost follows a normal distribution in the interval
[0, 50]. While 63% of all sentences contain at least
one entity mention, only 23% of CNPs contain en-
tity mentions. These statistics show that CNPs are
generally rather short and a large fraction of CNPs
does not contain entity mentions at all. We may
hypothesize that this observation will be reflected
by annotation times.
</bodyText>
<figure confidence="0.9772482">
sentence length
frequency
0 100 300 500
0 20 40 60 80
tokens tokens
</figure>
<figureCaption confidence="0.999886">
Figure 1: Length distribution of sentences and CNPs
</figureCaption>
<subsectionHeader confidence="0.999837">
3.2 Annotation Performance
</subsectionHeader>
<bodyText confidence="0.99917516">
To test the validity of the guidelines and the gen-
eral performance of our annotators A and B, we
compared their annotation results on 5 blocks of
sentence-level annotation examples created dur-
ing training. Annotation performance was mea-
sured in terms of Cohen’s kappa coefficient K on
the token level and entity-segment F-score against
MUC7 annotations. The annotators achieved
KA = 0.95 and KB = 0.96, and FA = 0.92
and FB = 0.94, respectively.3 Moreover, they ex-
hibit an inter-annotator agreement of KA,B = 0.94
and an averaged mutual F-score of FA,B = 0.90.
These numbers reveal that the task is well-defined
and the annotators have sufficiently internalized
the annotation guidelines to produce valid results.
Figure 2 shows the annotators’ scores against
the original MUC7 annotations for the 31 blocks
of sentence-level annotations (3,113 sentences)
which range from K = 0.89 to K = 0.98. Largely,
annotation performance is similar for both anno-
tators and shows that they consistently found a
block either rather hard or easy to annotate. More-
over, annotation performance seems stationary –
no general trend in annotation performance over
time can be observed.
</bodyText>
<figure confidence="0.6796715">
sentence−level annotation
blocks
</figure>
<figureCaption confidence="0.998391">
Figure 2: Average kappa coefficient per block
</figureCaption>
<subsectionHeader confidence="0.998697">
3.3 Time Measurements
</subsectionHeader>
<bodyText confidence="0.989499076923077">
Figure 3 shows the average annotation time per
block (CNPs and sentences). Considering the
CNP-level annotations, there is a learning effect
for annotator B during the first 9 blocks. Af-
ter that, both annotators are approximately on a
par regarding the annotation time. For sentence-
level annotations, both annotators again yield sim-
ilar annotation times per block, without any learn-
ing effects. Similar to annotation performance,
3Entity-specific F-scores against MUC7 annotations for
A and B are 0.90 and 0.92 for LOCATION, 0.92 and 0.93
for ORGANIZATION, and 0.96 and 0.98 for PERSON, respec-
tively.
</bodyText>
<figure confidence="0.99176048275862">
0 5 10 15 20 25 30
kappa
0.80 0.85 0.90 0.95 1.00
● ●
● ●
● ● ●
● ●
●
●
●annotator A
● annotator B
●
●
●
●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●●●
● ●
●
●
●
●
●
●
●
●
●
● ●
●
●
●
●
CNP length
frequency
0 2000 6000
0 5 10 1 5 20
114
CNP−level annotation
blocks
sentence−level annotation
blocks
</figure>
<figureCaption confidence="0.999945">
Figure 3: Average annotation times per block
</figureCaption>
<bodyText confidence="0.999477375">
analysis of annotation time shows that the annota-
tion behavior is largely stationary (excluding first
rounds of CNP-level annotation) which allows sin-
gle time measurements to be interpreted indepen-
dently of previous time measurements. Both, time
and performance plots exhibit that there are blocks
which were generally harder or easier than other
ones because both annotators operated in tandem.
</bodyText>
<subsectionHeader confidence="0.989966">
3.4 Easy and Hard Annotation Examples
</subsectionHeader>
<bodyText confidence="0.999955769230769">
As we have shown, inter-annotator variation of
annotation performance is moderate. Intra-block
performance, in contrast, is subject to high vari-
ance. Figure 4 shows the distribution of annota-
tor A’s CNP-level annotation times for block 20.
A’s average annotation time on this block amounts
to 1.37 seconds per CNP, the shortest time be-
ing 0.54, the longest one amounting 10.2 seconds.
The figure provides ample evidence for an ex-
tremely skewed time investment for coding CNPs.
A preliminary manual analysis revealed CNPs
with very low annotation times are mostly short
and consist of stop words and pronouns only, or
</bodyText>
<subsectionHeader confidence="0.468276">
CNP−level annotation
</subsectionHeader>
<bodyText confidence="0.52005">
frequency
0 50 150 250
</bodyText>
<footnote confidence="0.6996885">
2 4 6 8 10
annotation time
</footnote>
<figureCaption confidence="0.99986">
Figure 4: Distribution of annotation times in one block
</figureCaption>
<bodyText confidence="0.999971888888889">
are otherwise simple noun phrases with a sur-
face structure incompatible with entity mentions
(e.g., all tokens are lower-cased). Here, humans
can quickly exclude the occurrence of entity men-
tions which results in low annotation times. CNPs
which took desparately long (more than 6 seconds)
were outliers indicating distraction or loss of con-
centration. Times between 3 and 5 seconds were
basically caused by semantically complex CNPs.
</bodyText>
<sectionHeader confidence="0.999558" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999990916666667">
We have created a time-stamped version of MUC7
entity annotations, MUC7T, on two levels of anno-
tation granularity – sentences and complex noun
phrases. Especially the phrase-level annotations
allow for fine-grained time measurement. We will
use this corpus for studies on (time) cost-sensitive
Active Learning. MUC7T can also be used to de-
rive or learn accurate annotation cost models al-
lowing to predict annotation time on new data. We
are currently investigating causal factors of anno-
tation complexity for named entity annotation on
the basis of MUC7T.
</bodyText>
<sectionHeader confidence="0.994726" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999634">
This work was funded by the EC within the
BOOTStrep (FP6-028099) and CALBC (FP7-
231727) projects. We want to thank Oleg Lichten-
wald (JULIE Lab) for implementing the noun
phrase extractor for our experiments.
</bodyText>
<sectionHeader confidence="0.998579" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997255619047619">
David A. Cohn, Zoubin Ghahramani, and Michael I.
Jordan. 1996. Active learning with statistical mod-
els. Journal of Artificial Intelligence Research,
4:129–145.
Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-
roll, and Peter McClanahan. 2008. Assessing the
costs of sampling methods in active learning for an-
notation. In Proceedings of the ACL-08: HLT, Short
Papers, pages 65–68.
Linguistic Data Consortium. 2001. Message Under-
standing Conference 7. LDC2001T02. FTP file.
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, and Marcia Lazo. 2003. The TIMEBANK
corpus. In Proceedings of the Corpus Linguistics
2003 Conference, pages 647–656.
Burr Settles, Mark Craven, and Lewis Friedland. 2008.
Active learning with real annotation costs. In Pro-
ceedings of the NIPS’08 Workshop on Cost Sensitive
Learning, pages 1–10.
</reference>
<figure confidence="0.99993281081081">
0 5 10 15 20 25 30
seconds
1.0 1.2 1.4 1.6 1.8 2.0
●
●
●
●
●●● ●●●● ●
● annotator A
● annotator B
●
●
●
●
●
●
● ● ●
● ● ● ●
● ● ● ● ●
● ● ● ●
● ● ● ● ● ●
● ● ● ●●
●
0 5 10 15 20 25 30
seconds
4.0 4.5 5.0 5.5 6.0
●
●
●
●annotator A
● annotator B
● ● ●
●
● ● ●
● ●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
● ● ●
●
● ●
●●●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
</figure>
<page confidence="0.971926">
115
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.866134">
<title confidence="0.9891615">Annotations — Enhancing by the Time It Takes to Annotate Named Entities</title>
<author confidence="0.999872">Tomanek Hahn</author>
<affiliation confidence="0.998256">University Language &amp; Information Engineering</affiliation>
<address confidence="0.896897">Friedrich-Schiller-Universit¨at Jena, Germany</address>
<abstract confidence="0.998917833333333">We report on the re-annotation of selected of named entities from the corpus where our focus lies on recording the time it takes to annotate these entities given two basic annotation units sentences noun phrases. Such information may be helpful to lay the empirical foundations for the development of cost measures for annotation processes based on the investment in time for decision-making per entity mention.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David A Cohn</author>
<author>Zoubin Ghahramani</author>
<author>Michael I Jordan</author>
</authors>
<title>Active learning with statistical models.</title>
<date>1996</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>4--129</pages>
<contexts>
<context position="1367" citStr="Cohn et al., 1996" startWordPosition="205" endWordPosition="208">otation metadata is at the heart of (semi)supervised machine learning techniques which have become very popular in NLP research. At their flipside, they create an enormous bottleneck because major shifts in the domain of discourse, the basic entities of interest, or the text genre often require new annotation efforts. But annotations are costly in terms of getting welltrained and intelligible human resources involved. Surprisingly, cost awareness has not been a primary concern in most of the past linguistic annotation initiatives. Only recently, annotation strategies (such as Active Learning (Cohn et al., 1996)) which strive for minimizing the annotation load have gained increasing attention. Still, when it comes to the empirically plausible assessment of annotation costs even proponents of Active Learning make overly simplistic and empirically questionable assumptions, e.g., the uniformity of annotation costs over the number of linguistic units (e.g., tokens) to be annotated. We here consider the time it takes to annotate a particular entity mention as a natural indicator of effort for named entity annotations. In order to lay the empirical foundations for experimentally grounded annotation cost mo</context>
</contexts>
<marker>Cohn, Ghahramani, Jordan, 1996</marker>
<rawString>David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan. 1996. Active learning with statistical models. Journal of Artificial Intelligence Research, 4:129–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robbie Haertel</author>
<author>Eric Ringger</author>
<author>Kevin Seppi</author>
<author>James Carroll</author>
<author>Peter McClanahan</author>
</authors>
<title>Assessing the costs of sampling methods in active learning for annotation.</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-08: HLT, Short Papers,</booktitle>
<pages>65--68</pages>
<contexts>
<context position="2171" citStr="Haertel et al., 2008" startWordPosition="328" endWordPosition="331"> Active Learning make overly simplistic and empirically questionable assumptions, e.g., the uniformity of annotation costs over the number of linguistic units (e.g., tokens) to be annotated. We here consider the time it takes to annotate a particular entity mention as a natural indicator of effort for named entity annotations. In order to lay the empirical foundations for experimentally grounded annotation cost models we couple common named entity annotation metadata with a time stamp reflecting the time measured for decision making.1 Previously, two studies – one dealing with POS annotation (Haertel et al., 2008), the other with named entity and relation annotation (Settles et al., 2008) – have measured the time needed to annotate sentences on small data sets and attempted to learn predictive models of annotation cost. However, these data sets do not meet our requirements as we envisage a large, coherent, and also wellknown newspaper entity corpus extended by annotation costs on a fine-grained level. Especially size and coherence of such a corpus are not only essential for building accurate cost models but also as a reference baseline for cost-sensitive annotation strategies. Moreover, the annotation </context>
</contexts>
<marker>Haertel, Ringger, Seppi, Carroll, McClanahan, 2008</marker>
<rawString>Robbie Haertel, Eric Ringger, Kevin Seppi, James Carroll, and Peter McClanahan. 2008. Assessing the costs of sampling methods in active learning for annotation. In Proceedings of the ACL-08: HLT, Short Papers, pages 65–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linguistic Data Consortium</author>
</authors>
<date>2001</date>
<booktitle>Message Understanding Conference 7. LDC2001T02. FTP file.</booktitle>
<contexts>
<context position="3050" citStr="Consortium, 2001" startWordPosition="470" endWordPosition="471">ts as we envisage a large, coherent, and also wellknown newspaper entity corpus extended by annotation costs on a fine-grained level. Especially size and coherence of such a corpus are not only essential for building accurate cost models but also as a reference baseline for cost-sensitive annotation strategies. Moreover, the annotation level for which cost information is available is crucial because document- or sentence-level data might be too coarse for several applications. Accordingly, this paper introduces MUC7T, our extension to the entity annotations of the MUC7 corpus (Linguistic Data Consortium, 2001) where time stamps are added to two levels of annotation granularity, viz. sentences and complex noun phrases. 2 Corpus Annotation 2.1 Annotation Task Our annotation initiative constitutes an extension to the named entity annotations (ENAMEX) of the English part of the MUC7 corpus covering New York Times articles from 1996. ENAMEX annotations cover three types of named entities, viz. PERSONS, LOCATIONS, and ORGANIZATIONS. We instructed two human annotators, both advanced students of linguistics with good English language skills, to re-annotate the MUC7 corpus for the ENAMEX subtask. To be as c</context>
</contexts>
<marker>Consortium, 2001</marker>
<rawString>Linguistic Data Consortium. 2001. Message Understanding Conference 7. LDC2001T02. FTP file.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Patrick Hanks</author>
<author>Roser Sauri</author>
<author>Andrew See</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Dragomir Radev</author>
<author>Beth Sundheim</author>
<author>David Day</author>
<author>Lisa Ferro</author>
<author>Marcia Lazo</author>
</authors>
<title>The TIMEBANK corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of the Corpus Linguistics 2003 Conference,</booktitle>
<pages>647--656</pages>
<contexts>
<context position="3875" citStr="Pustejovsky et al., 2003" startWordPosition="599" endWordPosition="602">n to the named entity annotations (ENAMEX) of the English part of the MUC7 corpus covering New York Times articles from 1996. ENAMEX annotations cover three types of named entities, viz. PERSONS, LOCATIONS, and ORGANIZATIONS. We instructed two human annotators, both advanced students of linguistics with good English language skills, to re-annotate the MUC7 corpus for the ENAMEX subtask. To be as consistent as possi1These time stamps should not be confounded with the annotation of temporal expressions (TIMEX in MUC7, or even more advanced metadata using TIMEML for the creation of the TIMEBANK (Pustejovsky et al., 2003)). 112 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 112–115, Suntec, Singapore, 6-7 August 2009. c�2009 ACL and AFNLP ble with the existing MUC7 annotations, the annotators had to follow the original guidelines of the MUC7 named entity task. For ease of reannotation, we intentionally ignored temporal and number expressions (TIMEX and NUMEX). MUC7 covers three distinct document sets for the named entity task. We used one of these sets to train the annotators and develop the annotation design, and another one for our actual annotation initiative which consists </context>
</contexts>
<marker>Pustejovsky, Hanks, Sauri, See, Gaizauskas, Setzer, Radev, Sundheim, Day, Ferro, Lazo, 2003</marker>
<rawString>James Pustejovsky, Patrick Hanks, Roser Sauri, Andrew See, Robert Gaizauskas, Andrea Setzer, Dragomir Radev, Beth Sundheim, David Day, Lisa Ferro, and Marcia Lazo. 2003. The TIMEBANK corpus. In Proceedings of the Corpus Linguistics 2003 Conference, pages 647–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Burr Settles</author>
<author>Mark Craven</author>
<author>Lewis Friedland</author>
</authors>
<title>Active learning with real annotation costs.</title>
<date>2008</date>
<booktitle>In Proceedings of the NIPS’08 Workshop on Cost Sensitive Learning,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="2247" citStr="Settles et al., 2008" startWordPosition="340" endWordPosition="343">ions, e.g., the uniformity of annotation costs over the number of linguistic units (e.g., tokens) to be annotated. We here consider the time it takes to annotate a particular entity mention as a natural indicator of effort for named entity annotations. In order to lay the empirical foundations for experimentally grounded annotation cost models we couple common named entity annotation metadata with a time stamp reflecting the time measured for decision making.1 Previously, two studies – one dealing with POS annotation (Haertel et al., 2008), the other with named entity and relation annotation (Settles et al., 2008) – have measured the time needed to annotate sentences on small data sets and attempted to learn predictive models of annotation cost. However, these data sets do not meet our requirements as we envisage a large, coherent, and also wellknown newspaper entity corpus extended by annotation costs on a fine-grained level. Especially size and coherence of such a corpus are not only essential for building accurate cost models but also as a reference baseline for cost-sensitive annotation strategies. Moreover, the annotation level for which cost information is available is crucial because document- o</context>
</contexts>
<marker>Settles, Craven, Friedland, 2008</marker>
<rawString>Burr Settles, Mark Craven, and Lewis Friedland. 2008. Active learning with real annotation costs. In Proceedings of the NIPS’08 Workshop on Cost Sensitive Learning, pages 1–10.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>