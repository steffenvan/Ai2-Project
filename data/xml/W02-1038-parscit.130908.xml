<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009470">
<affiliation confidence="0.703483">
Proceedings of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), Philadelphia, July 2002, pp. 296-303.
Association for Computational Linguistics.
</affiliation>
<figure confidence="0.998326387096774">
P(W1)
W2
Wn
&lt;loc&gt;
&lt;oov&gt;
W1
P(W1/W1)
P(W1/&lt;loc&gt;)
&lt;tmpl−1&gt;
&lt;loc&gt; :
&lt;tmpl−2&gt;
&lt;loc list&gt;
&lt;tmpl&gt;
&lt;tmpl&gt; :
&lt;tmpl−3&gt;
&lt;tmpl−4&gt;
Roma
&lt;Word&gt;
Stati Uniti d’America
&lt;loc lst&gt; :
&lt;prep&gt; &lt;Word&gt;
Il Cairo
&lt;tmpl−1&gt; :
&lt;tmpl−2&gt; :
&lt;tmpl−3&gt; :
&lt;Word&gt;
&lt;prep&gt; &lt;Word&gt;
&lt;tmpl−4&gt; :
&lt;Word&gt;
&lt;prep&gt;
&lt;prep&gt; &lt;Word&gt;
</figure>
<bodyText confidence="0.971129529411765">
(Huang et al., 2001) or of weighted finite state
acceptors (Pereira et al., 1994) and that the
most probable path for an input text can be effi-
ciently computed through a Viterbi-like decod-
ing algorithm (Brugnara and Federico, 1997).
Here, in particular, the decoding software used
for speech recognition is used by converting the
input front-end to a stream of ASCII charac-
ters, and by replacing acoustic models to single
state HMMs with delta distributions over sin-
gle ASCII characters. The NE LM is compiled
into a set of distinct PFSNs, corresponding to
the main trigram LM, the class related models,
etc. Significant memory savings are achieved by
exploiting a tree-based topology for the trigram
and bag-of-word models (Bert°ldi et al., 2001;
Huang et al., 2001).
</bodyText>
<sectionHeader confidence="0.989284" genericHeader="abstract">
4 NE LM Training
</sectionHeader>
<bodyText confidence="0.9998493">
Given a manually parsed corpus, Maximum
Likelihood (ML) estimation of the NE LM just
requires collecting sufficient statistics for its pa-
rameter sets. Otherwise, if just an untagged
text is available, training of the LM can be per-
formed by the Expectation-Maximization algo-
rithm (Dempster et al., 1977) or, more easily, by
the Viterbi training method, also known as seg-
mental K-means algorithm (Juang and Rabiner,
1990).
</bodyText>
<subsectionHeader confidence="0.993721">
4.1 Viterbi training
</subsectionHeader>
<bodyText confidence="0.9999265">
Let M be an available estimate of the NE LM
and W an untagged text. A new estimate M
can be obtained by searching for the best parse
tree T, under M, and by computing, then, the
ML estimate M, under T . This corresponds to
performing the two following steps:
</bodyText>
<listItem confidence="0.9976305">
1. i&amp;quot; = arg mr log Pr(T, W; 11-4- )
2. M = arg max log Pr(T, W; M)
</listItem>
<bodyText confidence="0.999847">
The following inequalities show that the above
procedure can be iteratively used to improve the
likelihood of the best parse tree:
</bodyText>
<equation confidence="0.774595">
max log Pr(T, W; &gt; log Prq, W;
&gt;log Prq, W;
= max log Pr(T, W; M) (3)
</equation>
<bodyText confidence="0.999400619047619">
However, the above property does not tell if
the parameter transformation M M indeed
converges to a fixed point. A tricky convergence
proof of the segmental K-means algorithm ap-
plied to HMMs can be found in (Juang and Ra-
biner, 1990), while bounds on the distance be-
tween HMM parameters estimated by EM and
Viterbi training are discussed in (Merhav and
Ephraim, 1991).
In this work, few iterations of Viterbi train-
ing were applied, as relative likelihood improve-
ments of the best interpretation drastically re-
duced after the first iteration. Figure 4 shows
how training is applied to the NE LM. Start-
ing from some model estimate M, the corpus
is tagged according to the most probable parse
tree T. Hence, sufficient statistics are extracted
from the tagged corpus in order to estimate M.
In some cases, little supervision in terms of man-
ually checked NE lists is used to filter out unre-
liably tagged data.
</bodyText>
<subsectionHeader confidence="0.956908">
4.2 Incremental training
</subsectionHeader>
<bodyText confidence="0.9999887">
Training of the NE LM goes through the defi-
nition and estimation of two simpler intermedi-
ate models, which are obtained by fixing some
of the parameters in (II, A, T, E). In particu-
lar, the notation (II, A = 0, E) indicates a list
model, as the switch parameters in A inhibit the
template part, while (II, A = 1, T) indicates a
template model, as the lists of known-entries are
inhibited 4. Estimates of intermediate models
will be used to initialize the complete NE LM.
</bodyText>
<subsectionHeader confidence="0.820133">
4.3 Intermediate model Mo
</subsectionHeader>
<bodyText confidence="0.999929">
A list model (II, A = 0, E) is estimated start-
ing from a supervised list of NE entries which
may also include ambiguous cases5. Initializa-
tion and estimation of distributions in II and E
are performed on a sub corpus made of sentences
</bodyText>
<footnote confidence="0.930074">
4Not active parameters are just omitted from the no-
tation.
5See Section 5.2 for details.
</footnote>
<figure confidence="0.971649689655172">
Untagged
Corpus
~
NE Recognition
M
Tagged
Corpus
Supervised
NE Lists
Training
M
0.92
May ’99 - Y-r
0.9
0.88
0.86
Correlation Coefficient
0.84
0.82
0.8
0.78
0.76
0.74
1999
1992
1994 1993
1996 1995
1998 1997
Y-r
</figure>
<table confidence="0.969998833333333">
E0 E2
# wrd coy. # wrd cov.
loc 662 75.42 25,752 96.61
org 608 62.66 19,605 84.00
per 1,132 44.19 48,109 84.50
all 2,122 59.94 70,261 89.75
</table>
<tableCaption confidence="0.887824666666667">
Table 2: Size of the NE lists of model Mo and
M2 and coverage statistics with respect to test
data.
</tableCaption>
<table confidence="0.50258475">
Initial Final
M3 71.10 71.78
M1 77.76 79.94
m2 85.92 87.17
</table>
<tableCaption confidence="0.7363975">
Table 3: F-scores of models M0-M2 on the test
set, before and after training.
</tableCaption>
<bodyText confidence="0.999856357142857">
after removing punctuation and capitalization
information, and for model M3. Three differ-
ent settings of the BN transcription system were
considered in order to evaluate NE recognition
with different word error rates (namely red,
rec2, and rec3). Moreover, as a reference, man-
ual transcripts without punctuation and capital-
ization (txt-i) were also used.
Results on the reference transcripts (txt-i)
show that the lack of punctuation and capi-
talization causes a 8.6% relative loss in perfor-
mance, i.e. F-score drops from 87.17 to 81.12.
This is mainly due to the increase in ambigu-
ity caused by common words which may also
occur in proper names. However, the incremen-
tal training procedure allowed for a significantly
improvement over the initial model Mo, i.e. a
20% F-score relative improvement, from 67.42
to 81.12.
Experiments on automatic transcripts with dif-
ferent WERs show relative decreases in perfor-
mance, with respect to txt-i, ranging between
10.4% and 13.0%, for WERs between 19.8% and
23.0%. The relative improvement between the
initial and final models, Mo and M3, is around
15-16% for all automatic transcripts. The rea-
son for the lower performance improvement may
be that M3 basically augments Mo with less fre-
</bodyText>
<table confidence="0.9965225">
txt-i red 1 rec2 rec3
Mo 67.42 62.95 61.96 61.34
M3 81.12 72.69 72.14 70.57
wer% 0.0 19.8 21.3 23.0
</table>
<tableCaption confidence="0.935248">
Table 4: F-score by models M3 and Mo on BN
transcripts with different WERs.
</tableCaption>
<bodyText confidence="0.923202">
quent proper names which are probably more
difficult to recognize, given the statistical nature
of the speech recognizer.
</bodyText>
<sectionHeader confidence="0.996471" genericHeader="keywords">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999253833333333">
This section compares the here proposed NE LM
with the NE tagged LM, presented in (Gotoh et
al., 1999; Renals et al., 1999). The NE tagged
LM uses a different decomposition of the prob-
ability Pr(W,T), which can be related to an or-
dinary class based trigram model, i.e.:
</bodyText>
<equation confidence="0.991207">
TI
Pr(W,T) = HPr(tVi,ti I Wi-24-21-0i-lti-1)
i=1
</equation>
<bodyText confidence="0.999986689655172">
where T now corresponds to a word-by-word
tagging of W with classes in E U { eo}, with eo
denoting the not-NE class. NE recognition with
this model can also be performed by Viterbi de-
coding. However, this requires estimating prob-
abilities in the space (V x (EU{ e0}))3, in contrast
to the probability space (V UE)3 used by the NE
LM.
Moreover, the cascade structure of the NE LM
can span longer dependencies, i.e. across words
and NE classes, than the NE tagged LM can
do. On the other side, the latter model is prob-
ably more flexible in the composition of NEs.
In other words, new NEs can be recognized by
concatenating known entries. The capability of
finding new NEs is, for what concerns the NE
LM, limited to the template model. Hence, a
possible improvement could be to replace the
current bag-of-word model in M3 with an n-
gram model, estimated on the entries currently
used in E.
Interestingly, similar levels of performance are
reported in (Renals et al., 1999) when NE recog-
nition is carried out on clean, case-insensitive
texts and automatic transcripts (with 21%
WER), i.e. F-scores are 85.0 and 75.0, respec-
tively. Of course, a true comparison between the
two approaches should be perfomed on the same
language and data.
</bodyText>
<sectionHeader confidence="0.98857" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9996255">
This paper presented a statistical language
model for NE recognition which was developed
for the Italian broadcast news domain. The
model integrates trigram statistics on words
and NE classes, with probabilistic finite state
language models. A bootstrap training tech-
nique is presented which permits to estimate the
model by means of very inexpensive language
resources: a large newspaper corpus and a few
thousand manually classified NEs.
Experimental results were provided for two au-
tomatically transcribed broadcast news shows.
Presented results are comparable with those ob-
tained, on similar conditions, by NE taggers for
American English broadcast news, which were
trained on much more supervised data.
</bodyText>
<sectionHeader confidence="0.993308" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.998795">
This work was carried out under the projects
CORETEX (IST-1999-11876) and WebFAQ
partially funded, respectively, by the European
Commission and the Fondo Unico della Provin-
cia di Trento.
</bodyText>
<sectionHeader confidence="0.997788" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999461392156863">
N. Bertoldi, F. Brugnara, M. Cettolo, M. Federico,
and D. Giuliani. 2001. From broadcast news to
spontaneous dialogue transcription: portability is-
sues. In Proceedings of the IEEE International
Conference on Acoustics, Speech and Signal Pro-
cessing, Salt Lake City, UT.
F. Brugnara and M. Federico. 1997. Dynamic lan-
guage models for interactive speech applications.
In Proceedings of the 5th European Conference
on Speech Communication and Technology, pages
2751-2754, Rhodes, Greece.
N. Chinchor, E. Brown, L. Ferro, and P. Robin-
son. 1999. 1999 Named entity Recog-
nition Task definition. Technical Re-
port Version 1.4, MITRE, Corp., August.
http://www.nist.gov/speech/tests/ie—er/er_99/
doc/ne99_taskdef_v1_4.ps.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum-likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical
Society, B, 39:1-38.
Y. Gotoh, S. Renals, and G. Williams. 1999. Named
entity tagged language models. In Proceedings
of ICASSP, volume I, pages 513-516, Phoenix,
March.
X. Huang, A. Acero, H.-W. Hon, and R. Reddy.
2001. Spoken Language Processing: A Guide
to Theory, Algorithm and System Development.
Prentice Hall.
B.-H. Juang and L. R. Rabiner. 1990. The segmen-
tal k-means algorithm for estimating parameters
of hidden Markov models. IEEE Trans. Acoust.,
Speech and Signal Proc., ASSP-38(9):1639-1641.
N. Merhav and Y. Ephraim. 1991. Hidden Markov
modeling using a dominant state sequence with
application to speech recognitionof states. Com-
puter Speech and Language, 5:327-339.
F. Pereira, M. Riley, and R. Sproat. 1994. Weighted
rational transductions and their application to hu-
man language processing. In Proceedings of the
ARPA Human Language Technology Workshop,
pages 262-267, Plainsboro, NJ.
M. A. Przybocki, J. G. Fiscus, J. S. Garofolo, and
D. S. Pallet. 1999. 1998 HUB-4 Information Ex-
traction Evaluation. In Proceedings of the DARPA
Broadcast News Workshop, Herndon, VA.
S. Renals, Y. Gotoh, R. Gaizauskas, and M. Steven-
son. 1999. Baseline IE-NE experiments using
the Sprach/Lassie system. In Proceedings of the
DARPA Broadcast News Workshop, Herndon, VA,
February.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<note confidence="0.9128834">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Philadelphia, July 2002, pp. 296-303. Association for Computational Linguistics. P(W1) W2</note>
<abstract confidence="0.958229026548673">Wn &lt;loc&gt; &lt;oov&gt; W1 P(W1/W1) P(W1/&lt;loc&gt;) &lt;tmpl−1&gt; &lt;loc&gt; : &lt;tmpl−2&gt; &lt;loc list&gt; &lt;tmpl&gt; &lt;tmpl&gt; : &lt;tmpl−3&gt; &lt;tmpl−4&gt; Roma &lt;Word&gt; Stati Uniti d’America &lt;loc lst&gt; : &lt;prep&gt; &lt;Word&gt; Il Cairo &lt;tmpl−1&gt; : &lt;tmpl−2&gt; : &lt;tmpl−3&gt; : &lt;Word&gt; &lt;prep&gt; &lt;Word&gt; &lt;tmpl−4&gt; : &lt;Word&gt; &lt;prep&gt; &lt;prep&gt; &lt;Word&gt; (Huang et al., 2001) or of weighted finite state acceptors (Pereira et al., 1994) and that the most probable path for an input text can be efficiently computed through a Viterbi-like decoding algorithm (Brugnara and Federico, 1997). Here, in particular, the decoding software used for speech recognition is used by converting the input front-end to a stream of ASCII characters, and by replacing acoustic models to single state HMMs with delta distributions over single ASCII characters. The NE LM is compiled into a set of distinct PFSNs, corresponding to the main trigram LM, the class related models, etc. Significant memory savings are achieved by exploiting a tree-based topology for the trigram and bag-of-word models (Bert°ldi et al., 2001; Huang et al., 2001). 4 NE LM Training Given a manually parsed corpus, Maximum Likelihood (ML) estimation of the NE LM just requires collecting sufficient statistics for its parameter sets. Otherwise, if just an untagged text is available, training of the LM can be performed by the Expectation-Maximization algorithm (Dempster et al., 1977) or, more easily, by the Viterbi training method, also known as segmental K-means algorithm (Juang and Rabiner, 1990). 4.1 Viterbi training Let M be an available estimate of the NE LM and W an untagged text. A new estimate M can be obtained by searching for the best parse by computing, then, the estimate M, under . corresponds to performing the two following steps: i&amp;quot; = arg mr log Pr(T, W; ) 2. M = arg max log Pr(T, W; M) The following inequalities show that the above procedure can be iteratively used to improve the likelihood of the best parse tree: max log Pr(T, W; &gt; log Prq, W; &gt;log Prq, W; = max log Pr(T, W; M) (3) However, the above property does not tell if the parameter transformation M M converges to a fixed point. A tricky convergence proof of the segmental K-means algorithm applied to HMMs can be found in (Juang and Rabiner, 1990), while bounds on the distance between HMM parameters estimated by EM and Viterbi training are discussed in (Merhav and Ephraim, 1991). In this work, few iterations of Viterbi training were applied, as relative likelihood improvements of the best interpretation drastically reduced after the first iteration. Figure 4 shows how training is applied to the NE LM. Startfrom some model estimate corpus is tagged according to the most probable parse sufficient statistics are extracted from the tagged corpus in order to estimate M. In some cases, little supervision in terms of manually checked NE lists is used to filter out unreliably tagged data. 4.2 Incremental training Training of the NE LM goes through the definition and estimation of two simpler intermediate models, which are obtained by fixing some the parameters in (II, A, E). particuthe notation (II, A = 0, a the switch parameters in A inhibit the part, while (II, A = 1, a model, the lists of known-entries are of intermediate models will be used to initialize the complete NE LM. 4.3 Intermediate model Mo list model (II, A = 0, estimated starting from a supervised list of NE entries which also include ambiguous Initializaand estimation of distributions in II and are performed on a sub corpus made of sentences active parameters are just omitted from the notation. Section 5.2 for details.</abstract>
<note confidence="0.781202333333333">Untagged Corpus ~</note>
<title confidence="0.893743375">NE Recognition M Tagged Corpus Supervised NE Lists Training M</title>
<pubnum confidence="0.566256">0.92</pubnum>
<date confidence="0.757033">May ’99 - Y-r</date>
<note confidence="0.821171">0.9 0.88 0.86 Correlation Coefficient 0.84 0.82 0.8 0.78 0.76 0.74</note>
<date confidence="0.9936934">1999 1992 1994 1993 1996 1995 1998 1997</date>
<abstract confidence="0.972970564814814">Y-r E0 E2 loc 662 75.42 25,752 96.61 org 608 62.66 19,605 84.00 per 1,132 44.19 48,109 84.50 all 2,122 59.94 70,261 89.75 Table 2: Size of the NE lists of model Mo and coverage statistics with respect to test data. Initial Final 71.78 M1 79.94 87.17 Table 3: F-scores of models M0-M2 on the test set, before and after training. after removing punctuation and capitalization and for model different settings of the BN transcription system were considered in order to evaluate NE recognition different word error rates (namely as a reference, manual transcripts without punctuation and capitalization (txt-i) were also used. Results on the reference transcripts (txt-i) show that the lack of punctuation and capitalization causes a 8.6% relative loss in performance, i.e. F-score drops from 87.17 to 81.12. This is mainly due to the increase in ambiguity caused by common words which may also occur in proper names. However, the incremental training procedure allowed for a significantly improvement over the initial model Mo, i.e. a 20% F-score relative improvement, from 67.42 to 81.12. Experiments on automatic transcripts with different WERs show relative decreases in performance, with respect to txt-i, ranging between 10.4% and 13.0%, for WERs between 19.8% and 23.0%. The relative improvement between the and final models, and around 15-16% for all automatic transcripts. The reason for the lower performance improvement may that augments Mo with less fretxt-i red 1 rec2 rec3 Mo 67.42 62.95 61.96 61.34 M3 81.12 72.69 72.14 70.57 wer% 0.0 19.8 21.3 23.0 4: F-score by models on BN transcripts with different WERs. quent proper names which are probably more difficult to recognize, given the statistical nature of the speech recognizer. 6 Discussion This section compares the here proposed NE LM the tagged LM, in (Gotoh et al., 1999; Renals et al., 1999). The NE tagged LM uses a different decomposition of the probability Pr(W,T), which can be related to an ordinary class based trigram model, i.e.: TI = HPr(tVi,ti I i=1 corresponds to a word-by-word tagging of W with classes in E U { eo}, with eo denoting the not-NE class. NE recognition with this model can also be performed by Viterbi decoding. However, this requires estimating probin the space (V x in contrast the probability space (V used by the NE LM. Moreover, the cascade structure of the NE LM can span longer dependencies, i.e. across words and NE classes, than the NE tagged LM can do. On the other side, the latter model is probably more flexible in the composition of NEs. In other words, new NEs can be recognized by concatenating known entries. The capability of finding new NEs is, for what concerns the NE LM, limited to the template model. Hence, a possible improvement could be to replace the bag-of-word model in an ngram model, estimated on the entries currently in Interestingly, similar levels of performance are reported in (Renals et al., 1999) when NE recognition is carried out on clean, case-insensitive texts and automatic transcripts (with 21% WER), i.e. F-scores are 85.0 and 75.0, respectively. Of course, a true comparison between the two approaches should be perfomed on the same language and data. 7 Conclusion This paper presented a statistical language model for NE recognition which was developed for the Italian broadcast news domain. The model integrates trigram statistics on words and NE classes, with probabilistic finite state language models. A bootstrap training technique is presented which permits to estimate the model by means of very inexpensive language resources: a large newspaper corpus and a few thousand manually classified NEs. Experimental results were provided for two automatically transcribed broadcast news shows. Presented results are comparable with those obtained, on similar conditions, by NE taggers for American English broadcast news, which were trained on much more supervised data.</abstract>
<note confidence="0.909759">8 Acknowledgements This work was carried out under the projects CORETEX (IST-1999-11876) and WebFAQ partially funded, respectively, by the European and the Unico della Provincia di Trento.</note>
<title confidence="0.928656">References</title>
<author confidence="0.682465">From broadcast news to</author>
<affiliation confidence="0.701073666666667">spontaneous dialogue transcription: portability is- In of the IEEE International Conference on Acoustics, Speech and Signal Pro-</affiliation>
<address confidence="0.653678">City, UT.</address>
<abstract confidence="0.495664666666667">F. Brugnara and M. Federico. 1997. Dynamic language models for interactive speech applications. of the 5th European Conference</abstract>
<affiliation confidence="0.932808">Speech Communication and Technology,</affiliation>
<address confidence="0.8048525">2751-2754, Rhodes, Greece. Chinchor, E. Brown, L. Ferro, and P. Robin-</address>
<note confidence="0.897893375">1999. 1999 Named entity Recog- Task definition. Technical port Version 1.4, MITRE, Corp., August. doc/ne99_taskdef_v1_4.ps. A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum-likelihood from incomplete data via the algorithm. of the Royal Statistical B, Y. Gotoh, S. Renals, and G. Williams. 1999. Named tagged language models. In ICASSP, I, pages 513-516, Phoenix, March. X. Huang, A. Acero, H.-W. Hon, and R. Reddy. Language Processing: A Guide to Theory, Algorithm and System Development. Prentice Hall.</note>
<abstract confidence="0.696005772727273">B.-H. Juang and L. R. Rabiner. 1990. The segmental k-means algorithm for estimating parameters hidden Markov models. Trans. Acoust., and Signal Proc., N. Merhav and Y. Ephraim. 1991. Hidden Markov modeling using a dominant state sequence with to speech recognitionof states. Com- Speech and Language, F. Pereira, M. Riley, and R. Sproat. 1994. Weighted rational transductions and their application to hulanguage processing. In of the ARPA Human Language Technology Workshop, pages 262-267, Plainsboro, NJ. M. A. Przybocki, J. G. Fiscus, J. S. Garofolo, and D. S. Pallet. 1999. 1998 HUB-4 Information Ex- Evaluation. In of the DARPA News Workshop, VA. S. Renals, Y. Gotoh, R. Gaizauskas, and M. Stevenson. 1999. Baseline IE-NE experiments using Sprach/Lassie system. In of the Broadcast News Workshop, VA, February.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Bertoldi</author>
<author>F Brugnara</author>
<author>M Cettolo</author>
<author>M Federico</author>
<author>D Giuliani</author>
</authors>
<title>From broadcast news to spontaneous dialogue transcription: portability issues.</title>
<date>2001</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<location>Salt Lake City, UT.</location>
<marker>Bertoldi, Brugnara, Cettolo, Federico, Giuliani, 2001</marker>
<rawString>N. Bertoldi, F. Brugnara, M. Cettolo, M. Federico, and D. Giuliani. 2001. From broadcast news to spontaneous dialogue transcription: portability issues. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, Salt Lake City, UT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Brugnara</author>
<author>M Federico</author>
</authors>
<title>Dynamic language models for interactive speech applications.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th European Conference on Speech Communication and Technology,</booktitle>
<pages>2751--2754</pages>
<location>Rhodes, Greece.</location>
<contexts>
<context position="684" citStr="Brugnara and Federico, 1997" startWordPosition="102" endWordPosition="105">tural Language Processing (EMNLP), Philadelphia, July 2002, pp. 296-303. Association for Computational Linguistics. P(W1) W2 Wn &lt;loc&gt; &lt;oov&gt; W1 P(W1/W1) P(W1/&lt;loc&gt;) &lt;tmpl−1&gt; &lt;loc&gt; : &lt;tmpl−2&gt; &lt;loc list&gt; &lt;tmpl&gt; &lt;tmpl&gt; : &lt;tmpl−3&gt; &lt;tmpl−4&gt; Roma &lt;Word&gt; Stati Uniti d’America &lt;loc lst&gt; : &lt;prep&gt; &lt;Word&gt; Il Cairo &lt;tmpl−1&gt; : &lt;tmpl−2&gt; : &lt;tmpl−3&gt; : &lt;Word&gt; &lt;prep&gt; &lt;Word&gt; &lt;tmpl−4&gt; : &lt;Word&gt; &lt;prep&gt; &lt;prep&gt; &lt;Word&gt; (Huang et al., 2001) or of weighted finite state acceptors (Pereira et al., 1994) and that the most probable path for an input text can be efficiently computed through a Viterbi-like decoding algorithm (Brugnara and Federico, 1997). Here, in particular, the decoding software used for speech recognition is used by converting the input front-end to a stream of ASCII characters, and by replacing acoustic models to single state HMMs with delta distributions over single ASCII characters. The NE LM is compiled into a set of distinct PFSNs, corresponding to the main trigram LM, the class related models, etc. Significant memory savings are achieved by exploiting a tree-based topology for the trigram and bag-of-word models (Bert°ldi et al., 2001; Huang et al., 2001). 4 NE LM Training Given a manually parsed corpus, Maximum Likel</context>
</contexts>
<marker>Brugnara, Federico, 1997</marker>
<rawString>F. Brugnara and M. Federico. 1997. Dynamic language models for interactive speech applications. In Proceedings of the 5th European Conference on Speech Communication and Technology, pages 2751-2754, Rhodes, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Chinchor</author>
<author>E Brown</author>
<author>L Ferro</author>
<author>P Robinson</author>
</authors>
<title>Named entity Recognition Task definition.</title>
<date>1999</date>
<tech>Technical Report Version 1.4,</tech>
<location>MITRE, Corp.,</location>
<note>http://www.nist.gov/speech/tests/ie—er/er_99/ doc/ne99_taskdef_v1_4.ps.</note>
<marker>Chinchor, Brown, Ferro, Robinson, 1999</marker>
<rawString>N. Chinchor, E. Brown, L. Ferro, and P. Robinson. 1999. 1999 Named entity Recognition Task definition. Technical Report Version 1.4, MITRE, Corp., August. http://www.nist.gov/speech/tests/ie—er/er_99/ doc/ne99_taskdef_v1_4.ps.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum-likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, B,</journal>
<pages>39--1</pages>
<contexts>
<context position="1541" citStr="Dempster et al., 1977" startWordPosition="241" endWordPosition="244">gle ASCII characters. The NE LM is compiled into a set of distinct PFSNs, corresponding to the main trigram LM, the class related models, etc. Significant memory savings are achieved by exploiting a tree-based topology for the trigram and bag-of-word models (Bert°ldi et al., 2001; Huang et al., 2001). 4 NE LM Training Given a manually parsed corpus, Maximum Likelihood (ML) estimation of the NE LM just requires collecting sufficient statistics for its parameter sets. Otherwise, if just an untagged text is available, training of the LM can be performed by the Expectation-Maximization algorithm (Dempster et al., 1977) or, more easily, by the Viterbi training method, also known as segmental K-means algorithm (Juang and Rabiner, 1990). 4.1 Viterbi training Let M be an available estimate of the NE LM and W an untagged text. A new estimate M can be obtained by searching for the best parse tree T, under M, and by computing, then, the ML estimate M, under T . This corresponds to performing the two following steps: 1. i&amp;quot; = arg mr log Pr(T, W; 11-4- ) 2. M = arg max log Pr(T, W; M) The following inequalities show that the above procedure can be iteratively used to improve the likelihood of the best parse tree: max</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum-likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, B, 39:1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Gotoh</author>
<author>S Renals</author>
<author>G Williams</author>
</authors>
<title>Named entity tagged language models.</title>
<date>1999</date>
<booktitle>In Proceedings of ICASSP, volume I,</booktitle>
<pages>513--516</pages>
<location>Phoenix,</location>
<contexts>
<context position="6185" citStr="Gotoh et al., 1999" startWordPosition="1061" endWordPosition="1064">provement between the initial and final models, Mo and M3, is around 15-16% for all automatic transcripts. The reason for the lower performance improvement may be that M3 basically augments Mo with less fretxt-i red 1 rec2 rec3 Mo 67.42 62.95 61.96 61.34 M3 81.12 72.69 72.14 70.57 wer% 0.0 19.8 21.3 23.0 Table 4: F-score by models M3 and Mo on BN transcripts with different WERs. quent proper names which are probably more difficult to recognize, given the statistical nature of the speech recognizer. 6 Discussion This section compares the here proposed NE LM with the NE tagged LM, presented in (Gotoh et al., 1999; Renals et al., 1999). The NE tagged LM uses a different decomposition of the probability Pr(W,T), which can be related to an ordinary class based trigram model, i.e.: TI Pr(W,T) = HPr(tVi,ti I Wi-24-21-0i-lti-1) i=1 where T now corresponds to a word-by-word tagging of W with classes in E U { eo}, with eo denoting the not-NE class. NE recognition with this model can also be performed by Viterbi decoding. However, this requires estimating probabilities in the space (V x (EU{ e0}))3, in contrast to the probability space (V UE)3 used by the NE LM. Moreover, the cascade structure of the NE LM can</context>
</contexts>
<marker>Gotoh, Renals, Williams, 1999</marker>
<rawString>Y. Gotoh, S. Renals, and G. Williams. 1999. Named entity tagged language models. In Proceedings of ICASSP, volume I, pages 513-516, Phoenix, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Huang</author>
<author>A Acero</author>
<author>H-W Hon</author>
<author>R Reddy</author>
</authors>
<title>Spoken Language Processing: A Guide to Theory, Algorithm and System Development.</title>
<date>2001</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="1220" citStr="Huang et al., 2001" startWordPosition="189" endWordPosition="192">ly computed through a Viterbi-like decoding algorithm (Brugnara and Federico, 1997). Here, in particular, the decoding software used for speech recognition is used by converting the input front-end to a stream of ASCII characters, and by replacing acoustic models to single state HMMs with delta distributions over single ASCII characters. The NE LM is compiled into a set of distinct PFSNs, corresponding to the main trigram LM, the class related models, etc. Significant memory savings are achieved by exploiting a tree-based topology for the trigram and bag-of-word models (Bert°ldi et al., 2001; Huang et al., 2001). 4 NE LM Training Given a manually parsed corpus, Maximum Likelihood (ML) estimation of the NE LM just requires collecting sufficient statistics for its parameter sets. Otherwise, if just an untagged text is available, training of the LM can be performed by the Expectation-Maximization algorithm (Dempster et al., 1977) or, more easily, by the Viterbi training method, also known as segmental K-means algorithm (Juang and Rabiner, 1990). 4.1 Viterbi training Let M be an available estimate of the NE LM and W an untagged text. A new estimate M can be obtained by searching for the best parse tree T</context>
</contexts>
<marker>Huang, Acero, Hon, Reddy, 2001</marker>
<rawString>X. Huang, A. Acero, H.-W. Hon, and R. Reddy. 2001. Spoken Language Processing: A Guide to Theory, Algorithm and System Development. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B-H Juang</author>
<author>L R Rabiner</author>
</authors>
<title>The segmental k-means algorithm for estimating parameters of hidden Markov models.</title>
<date>1990</date>
<journal>IEEE Trans. Acoust., Speech and Signal Proc.,</journal>
<pages>38--9</pages>
<contexts>
<context position="1658" citStr="Juang and Rabiner, 1990" startWordPosition="260" endWordPosition="263"> class related models, etc. Significant memory savings are achieved by exploiting a tree-based topology for the trigram and bag-of-word models (Bert°ldi et al., 2001; Huang et al., 2001). 4 NE LM Training Given a manually parsed corpus, Maximum Likelihood (ML) estimation of the NE LM just requires collecting sufficient statistics for its parameter sets. Otherwise, if just an untagged text is available, training of the LM can be performed by the Expectation-Maximization algorithm (Dempster et al., 1977) or, more easily, by the Viterbi training method, also known as segmental K-means algorithm (Juang and Rabiner, 1990). 4.1 Viterbi training Let M be an available estimate of the NE LM and W an untagged text. A new estimate M can be obtained by searching for the best parse tree T, under M, and by computing, then, the ML estimate M, under T . This corresponds to performing the two following steps: 1. i&amp;quot; = arg mr log Pr(T, W; 11-4- ) 2. M = arg max log Pr(T, W; M) The following inequalities show that the above procedure can be iteratively used to improve the likelihood of the best parse tree: max log Pr(T, W; &gt; log Prq, W; &gt;log Prq, W; = max log Pr(T, W; M) (3) However, the above property does not tell if the p</context>
</contexts>
<marker>Juang, Rabiner, 1990</marker>
<rawString>B.-H. Juang and L. R. Rabiner. 1990. The segmental k-means algorithm for estimating parameters of hidden Markov models. IEEE Trans. Acoust., Speech and Signal Proc., ASSP-38(9):1639-1641.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Merhav</author>
<author>Y Ephraim</author>
</authors>
<title>Hidden Markov modeling using a dominant state sequence with application to speech recognitionof states. Computer Speech and Language,</title>
<date>1991</date>
<pages>5--327</pages>
<contexts>
<context position="2574" citStr="Merhav and Ephraim, 1991" startWordPosition="435" endWordPosition="438">rg mr log Pr(T, W; 11-4- ) 2. M = arg max log Pr(T, W; M) The following inequalities show that the above procedure can be iteratively used to improve the likelihood of the best parse tree: max log Pr(T, W; &gt; log Prq, W; &gt;log Prq, W; = max log Pr(T, W; M) (3) However, the above property does not tell if the parameter transformation M M indeed converges to a fixed point. A tricky convergence proof of the segmental K-means algorithm applied to HMMs can be found in (Juang and Rabiner, 1990), while bounds on the distance between HMM parameters estimated by EM and Viterbi training are discussed in (Merhav and Ephraim, 1991). In this work, few iterations of Viterbi training were applied, as relative likelihood improvements of the best interpretation drastically reduced after the first iteration. Figure 4 shows how training is applied to the NE LM. Starting from some model estimate M, the corpus is tagged according to the most probable parse tree T. Hence, sufficient statistics are extracted from the tagged corpus in order to estimate M. In some cases, little supervision in terms of manually checked NE lists is used to filter out unreliably tagged data. 4.2 Incremental training Training of the NE LM goes through t</context>
</contexts>
<marker>Merhav, Ephraim, 1991</marker>
<rawString>N. Merhav and Y. Ephraim. 1991. Hidden Markov modeling using a dominant state sequence with application to speech recognitionof states. Computer Speech and Language, 5:327-339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pereira</author>
<author>M Riley</author>
<author>R Sproat</author>
</authors>
<title>Weighted rational transductions and their application to human language processing.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Human Language Technology Workshop,</booktitle>
<pages>262--267</pages>
<location>Plainsboro, NJ.</location>
<marker>Pereira, Riley, Sproat, 1994</marker>
<rawString>F. Pereira, M. Riley, and R. Sproat. 1994. Weighted rational transductions and their application to human language processing. In Proceedings of the ARPA Human Language Technology Workshop, pages 262-267, Plainsboro, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Przybocki</author>
<author>J G Fiscus</author>
<author>J S Garofolo</author>
<author>D S Pallet</author>
</authors>
<title>HUB-4 Information Extraction Evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of the DARPA Broadcast News Workshop,</booktitle>
<location>Herndon, VA.</location>
<marker>Przybocki, Fiscus, Garofolo, Pallet, 1999</marker>
<rawString>M. A. Przybocki, J. G. Fiscus, J. S. Garofolo, and D. S. Pallet. 1999. 1998 HUB-4 Information Extraction Evaluation. In Proceedings of the DARPA Broadcast News Workshop, Herndon, VA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Renals</author>
<author>Y Gotoh</author>
<author>R Gaizauskas</author>
<author>M Stevenson</author>
</authors>
<title>Baseline IE-NE experiments using the Sprach/Lassie system.</title>
<date>1999</date>
<booktitle>In Proceedings of the DARPA Broadcast News Workshop,</booktitle>
<location>Herndon, VA,</location>
<contexts>
<context position="6207" citStr="Renals et al., 1999" startWordPosition="1065" endWordPosition="1068">e initial and final models, Mo and M3, is around 15-16% for all automatic transcripts. The reason for the lower performance improvement may be that M3 basically augments Mo with less fretxt-i red 1 rec2 rec3 Mo 67.42 62.95 61.96 61.34 M3 81.12 72.69 72.14 70.57 wer% 0.0 19.8 21.3 23.0 Table 4: F-score by models M3 and Mo on BN transcripts with different WERs. quent proper names which are probably more difficult to recognize, given the statistical nature of the speech recognizer. 6 Discussion This section compares the here proposed NE LM with the NE tagged LM, presented in (Gotoh et al., 1999; Renals et al., 1999). The NE tagged LM uses a different decomposition of the probability Pr(W,T), which can be related to an ordinary class based trigram model, i.e.: TI Pr(W,T) = HPr(tVi,ti I Wi-24-21-0i-lti-1) i=1 where T now corresponds to a word-by-word tagging of W with classes in E U { eo}, with eo denoting the not-NE class. NE recognition with this model can also be performed by Viterbi decoding. However, this requires estimating probabilities in the space (V x (EU{ e0}))3, in contrast to the probability space (V UE)3 used by the NE LM. Moreover, the cascade structure of the NE LM can span longer dependenc</context>
</contexts>
<marker>Renals, Gotoh, Gaizauskas, Stevenson, 1999</marker>
<rawString>S. Renals, Y. Gotoh, R. Gaizauskas, and M. Stevenson. 1999. Baseline IE-NE experiments using the Sprach/Lassie system. In Proceedings of the DARPA Broadcast News Workshop, Herndon, VA, February.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>