<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.051686">
<title confidence="0.9958375">
UoY: Graphs of Unambiguous Vertices
for Word Sense Induction and Disambiguation
</title>
<author confidence="0.986229">
Ioannis Korkontzelos, Suresh Manandhar
</author>
<affiliation confidence="0.994246">
Department of Computer Science
The University of York
</affiliation>
<address confidence="0.885896">
Heslington, York, YO10 5NG, UK
</address>
<email confidence="0.994843">
{johnkork, suresh}@cs.york.ac.uk
</email>
<sectionHeader confidence="0.993845" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999903384615385">
This paper presents an unsupervised
graph-based method for automatic word
sense induction and disambiguation. The
innovative part of our method is the as-
signment of either a word or a word pair
to each vertex of the constructed graph.
Word senses are induced by clustering the
constructed graph. In the disambiguation
stage, each induced cluster is scored ac-
cording to the number of its vertices found
in the context of the target word. Our sys-
tem participated in SemEval-2010 word
sense induction and disambiguation task.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961754098361">
There exists significant evidence that word sense
disambiguation is important for a variety of nat-
ural language processing tasks: machine transla-
tion, information retrieval, grammatical analysis,
speech and text processing (Veronis, 2004). How-
ever, the “fixed-list” of senses paradigm, where the
senses of a target word is a closed list of defini-
tions coming from a standard dictionary (Agirre
et al., 2006), was long ago abandoned. The rea-
son is that sense lists, such as WordNet (Miller,
1995), miss many senses, especially domain-
specific ones (Pantel and Lin, 2002). The miss-
ing concepts are not recognised. Moreover, senses
cannot be easily related to their use in context.
Word sense induction methods can be divided
into vector-space models and graph based ones.
In a vector-space model, each context of a target
word is represented as a feature vector, e.g. fre-
quency of cooccurring words (Katz and Gies-
brecht, 2006). Context vectors are clustered and
the resulting clusters represent the induced senses.
Recently, graph-based methods have been em-
ployed for word sense induction (Agirre and
Soroa, 2007). Typically, graph-based methods
represent each context word of the target word as
a vertex. Two vertices are connected via an edge
if they cooccur in one or more instances. Once
the cooccurrence graph has been constructed, dif-
ferent graph clustering algorithms are applied to
partition the graph. Each cluster (partition) con-
sists of a set of words that are semantically related
to the particular sense (Veronis, 2004). The poten-
tial advantage of graph-based methods is that they
can combine both local and global cooccurrence
information (Agirre et al., 2006).
Klapaftis and Manandhar (2008) presented a
graph-based approach that represents pairs of
words as vertices instead of single words. They
claimed that single words might appear with more
than one senses of the target word, while they hy-
pothesize that a pair of words is unambiguous.
Hard-clustering the graph will potentially identify
less conflating senses of the target word.
In this paper, we relax the above hypothesis be-
cause in some cases a single word is unambiguous.
We present a method that generates two-word ver-
tices only when a single word vertex is unambigu-
ous. If the word is judged as unambiguous, then it
is represented as a single-word vertex. Otherwise,
it is represented as a pair-of-words vertex.
The approach of Klapaftis and Manandhar
(2008) achieved good results in both evaluation
settings of the SemEval-2007 task. A test in-
stance is disambiguated towards one of the in-
duced senses if one or more pairs of words rep-
resenting that sense cooccur in the test instance.
This creates a sparsity problem, because a cooc-
currence of two words is generally less likely than
the occurrence of a single word. We expect our ap-
proach to address the data sparsity problem with-
out conflating the induced senses.
</bodyText>
<sectionHeader confidence="0.908052" genericHeader="method">
2 Word Sense Induction
</sectionHeader>
<bodyText confidence="0.993674">
In this section we present our word sense in-
duction and disambiguation algorithms. Figure
</bodyText>
<page confidence="0.985996">
355
</page>
<bodyText confidence="0.986118948717949">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 355–358,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
1 shows an example showing how the sense in-
duction algorithm works: The left side of part
I shows the context nouns of four snippets con-
taining the target noun “chip”. The most rele-
vant of these nouns are represented as single word
vertices (part II). Note that “customer” was not
judged to be significantly relevant. In addition,
the system introduced several vertices represent-
ing pairs of nouns. For example, note the vertex
“company potato”. The set of sentences contain-
ing the context word “company” was judged as
very different from the set of sentences contain-
ing “company” and “potato”. Thus, our system
hypothesizes that probably “company” and “com-
pany potato” are relevant to different senses of
“chip”, and allows them to be clustered accord-
ingly. Vertices whose content nouns or pairs of
nouns cooccur in some snippet are connected with
an edge (part III and right side of part I). Edge
weights depend upon the conditional probabilities
of the occurrence frequencies of the vertex con-
tents in a large corpus, e.g. w2,6 in part III. Hard-
clustering the graph produces the induced senses
of “chip”: (a) potato crisp, and (b) microchip.
In the following subsections, the system is de-
scribed in detail. Figure 2 shows a block diagram
overview of the sense induction system. It consists
of three main components: (a) corpus preprocess-
ing, (b) graph construction, and (c) clustering.
In a number of different stages, the system uses
a reference corpus to count occurrences of word
or word pairs. It is chosen to be large because fre-
quencies of words in a large corpus are more sig-
nificant statistically. Ideally we would use the web
or another large repository, but for the purposes of
the SemEval-2010 task we used the union of all
snippets of all target words.
</bodyText>
<subsectionHeader confidence="0.998068">
2.1 Corpus Preprocessing
</subsectionHeader>
<bodyText confidence="0.999963666666667">
Corpus preprocessing aims to capture words that
are contextually related to the target word. Ini-
tially, all snippets1 that contain the target word are
lemmatised and PoS tagged using the GENIA tag-
ger2. Words that occur in a stoplist are filtered out.
Instead of using all words as context, only nouns
are kept, since they are more discriminative than
verbs, adverbs and adjectives, that appear in a va-
riety of different contexts.
</bodyText>
<footnote confidence="0.999952666666667">
1We refer to instances of the target word as snippets, since
they can be either sentences or paragraphs.
2www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger
</footnote>
<figureCaption confidence="0.999492">
Figure 1: An example showing how the proposed
word sense induction system works.
</figureCaption>
<bodyText confidence="0.998877642857143">
Nouns that occur infrequently in the reference
corpus are removed (parameter P1). Then, log-
likelihood ratio (LL) (Dunning, 1993) is em-
ployed to compare the distribution of each noun
to its distribution in reference corpus. The null
hypothesis is that the two distributions are simi-
lar. If this is true, LL is small value and the cor-
responding noun is removed (parameter P2). We
also filter out nouns that are more indicative in the
reference corpus than in the target word corpus;
i.e. the nouns whose relative frequency in the for-
mer is larger than in the latter. At the end of this
stage, each snippet is a list of lemmatised nouns
contextually related to the target word.
</bodyText>
<subsectionHeader confidence="0.999418">
2.2 Constructing the Graph
</subsectionHeader>
<bodyText confidence="0.975844772727273">
All nouns appearing in the list of the previous
stage output are represented as graph vertices.
Moreover, some vertices representing pairs of
nouns are added. Each noun within a snippet is
combined with every other, generating (n ) pairs.
2
Log-likelihood filtering with respect to the refer-
ence corpus is used to filter out unimportant pairs.
Thereafter, we aim to keep only pairs that might
refer to a different sense of the target word than
their component nouns. For each pair we construct
a vector containing the snippet IDs in which they
occur. Similarly we construct a vector for each
component noun. We discard a pair if its vector is
very similar to both the vectors of its component
nouns, otherwise we represent it as a vertex pair.
Dice coefficient was used as a similarity measure
and parameter P4 as threshold value.
Edges are drawn based on cooccurrence of the
corresponding vertices contents in one or more
snippets. Edges whose respective vertices con-
tents are infrequent are rejected. The weight ap-
</bodyText>
<page confidence="0.998536">
356
</page>
<figureCaption confidence="0.995012">
Figure 2: A: Block diagram presenting the system overview. B, C, D: Block diagrams further analysing
the structure of complex components of A. Parameter names appear within square brackets.
</figureCaption>
<bodyText confidence="0.98153775">
plied to each edge is the maximum of the condi-
tional probabilities of the corresponding vertices
contents (e.g. w2,6, part III, figure 1). Low weight
edges are filtered out (parameter P3).
</bodyText>
<subsectionHeader confidence="0.999638">
2.3 Clustering the Graph
</subsectionHeader>
<bodyText confidence="0.9999593125">
Chinese Whispers (CW) (Biemann, 2006) was
used to cluster the graph. CW is a randomised
graph-clustering algorithm, time-linear to the
number of edges. The number of clusters it pro-
duces is automatically inferred. Evaluation has
shown that CW suits well in sense induction appli-
cations, where class distributions are often highly
skewed. In our experiments, CW produced less
clusters using a constant mutation rate (5%).
To further reduce the number of induced clus-
ters, we applied a post-processing stage, which
exploits the one sense per collocation property
(Yarowsky, 1995). For each cluster li, we gener-
ated the set Si of all snippets that contain at least
one vertex content of li. Then, any clusters la and
lb were merged if Sa C Sb or Sa Q Sb.
</bodyText>
<sectionHeader confidence="0.988795" genericHeader="method">
3 Word Sense Disambiguation
</sectionHeader>
<bodyText confidence="0.999977285714286">
The induced senses are used to sense-tag each test
instance of the target word (snippet). Given a snip-
pet, each induced cluster is assigned a score equal
to the number of its vertex contents (single or pairs
of words) occurring in the snippet. The instance is
assigned to the sense with the highest score or with
equal weights to all highest scoring senses.
</bodyText>
<subsectionHeader confidence="0.446743">
4 Tuning parameter and inducing senses
</subsectionHeader>
<bodyText confidence="0.999995157894737">
The algorithm depends upon 4 parameters: P1
thresholds frequencies and P3 collocation weights.
P2 is the LL threshold and P4 the similarity thresh-
old for discarding pair-of-nouns vertices.
We chose P1 E 15,10,15}, P2 E
12, 3, 4, 5,10,15, 25, 35}, P3 E 10.2, 0.3, 0.4}
and P4 E 10.2, 0.4, 0.6, 0.8}. The parameter tun-
ing was done using the trial data of the SemEval-
2010 task and on the noun data of correspond-
ing SemEval-2007 task. Parameters were tuned
by choosing the maximum supervised recall. For
both data sets, the chosen parameter values were
P1 — 10, P3 — 0.4 and P4 — 0.8. Due to the
size difference of the datasets, for the Semeval-
2010 trial data P2 — 3, while for the SemEval-
2007 noun data P2 — 10. The latter was adopted
because the size of training data was announced to
be large. We induced senses on the training data
and then disambiguated the test data instances.
</bodyText>
<sectionHeader confidence="0.967871" genericHeader="evaluation">
5 Evaluation results
</sectionHeader>
<bodyText confidence="0.9998737">
Three different measures, V-Measure, F-Score,
and supervised recall on word sense disambigua-
tion task, were used for evaluation. V-Measure
and F-Score are unsupervised. Supervised recall
was measured on two different data splits. Table 1
shows the performance of our system, UoY, for all
measures and in comparison with the best, worst
and average performing system and the random
and most frequent sense (MFS) baselines. Results
are shown for all words, and nouns and verbs only.
</bodyText>
<page confidence="0.992822">
357
</page>
<table confidence="0.999948842105263">
System V-Msr F-Sc S-R80 S-R60
All UoY 15.70 49.76 62.44 61.96
Best 16.20 63.31 62.44 61.96
Worst 0.00 16.10 18.72 18.91
Average 6.36 48.72 54.95 54.27
MFS 0.00 63.40 58.67 58.25
Random 4.40 31.92 57.25 56.52
Nouns UoY 20.60 38.23 59.43 58.62
Best 20.60 57.10 59.43 58.62
Average 7.08 44.42 47.85 46.90
Worst 0.00 15.80 1.55 1.52
MFS 0.00 57.00 53.22 52.45
Random 4.20 30.40 51.45 50.21
Verbs UoY 8.50 66.55 66.82 66.82
Best 15.60 72.40 69.06 68.59
Average 5.95 54.23 65.25 65.00
Worst 0.10 16.40 43.76 44.23
MFS 0.00 72.70 66.63 66.70
Random 4.64 34.10 65.69 65.73
</table>
<tableCaption confidence="0.940899666666667">
Table 1: Summary of results (%). V-Msr: V-
Measure, F-Sc: F-Score, S-RX: Supervised recall
under data split: X% training, (100-X)% test
</tableCaption>
<bodyText confidence="0.97956524">
Table 2 shows the ranks of UoY for all evalu-
ation categories. Our system was generally very
highly ranked. It outperformed the random base-
line in all cases and the MFS baseline in measures
but F-Score. No participant system managed to
achive higher F-Score than the MFS baseline.
The main disadvantage of the system seems to
be the large number of induced senses. The rea-
sons are data sparcity and tuning on nouns, that
might have led to parameters that induce more
senses. However, the system performs best among
systems that produce comparable numbers of clus-
ters. Table 3 shows the number of senses of UoY
and the gold-standard. UoY produces significantly
more senses than the gold-standard, especially for
nouns, while for verbs figures are similar.
The system achieves low F-Scores, because this
measure favours fewer induced senses. Moreover,
we observe that most scores are lower for verbs
than nouns. This is probably because parameters
are tuned on nouns and because in general nouns
appear with more senses than verbs, allowing our
system to adapt better. As an overall conclusion,
each evaluation measure is more or less biased to-
wards small or large numbers of induced senses.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999803833333333">
We presented a graph-based approach for word
sense induction and disambiguation. Our ap-
proach represents as a graph vertex an unambigu-
ous unit: (a) a single word, if it is judged as unam-
biguous, or (b) a pair of words, otherwise. Graph
edges model the cooccurrences of the content of
</bodyText>
<table confidence="0.998778">
V-Msr F-Sc S-R80 S-R60
All 2 15 1 1
NounslVerbs 113 186 116 115
</table>
<tableCaption confidence="0.928112">
Table 2: Ranks of UoY (out of 26 systems)
</tableCaption>
<table confidence="0.999518333333333">
All Nouns Verbs
Gold-standard 3.79 4.46 3.12
UoY 11.54 17.32 5.76
</table>
<tableCaption confidence="0.999396">
Table 3: Number of senses
</tableCaption>
<bodyText confidence="0.99953975">
the vertices that they join. Hard-clustering the
graph induces a set of senses. To disambiguate
a test instance, we assign it to the induced sense
whose vertices contents occur mostly in the in-
stance. Results show that our system achieves very
high recall and V-measure performance, higher
than both baselines. It achieves low F-Scores due
to the large number of induced senses.
</bodyText>
<sectionHeader confidence="0.999411" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999906617647059">
E. Agirre and A. Soroa. 2007. Semeval-2007 task 02:
Evaluating word sense induction and discrimination
systems. In proceedings of SemEval-2007, Czech
Republic. ACL.
E. Agirre, D. Martinez, O. Lopez de Lacalle, and
A. Soroa. 2006. Two graph-based algorithms for
state-of-the-art wsd. In proceedings of EMNLP,
Sydney, Australia. ACL.
C. Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In proceedings
of TextGraphs, New York City. ACL.
T. Dunning. 1993. Accurate methods for the statistics
of surprise and coincidence. Computational Lin-
guistics, 19(1):61–74.
G. Katz and E. Giesbrecht. 2006. Automatic identifi-
cation of non-compositional multi-word expressions
using latent semantic analysis. In proceedings of the
ACL workshop on Multi-Word Expressions, Sydney,
Australia. ACL.
I. Klapaftis and S. Manandhar. 2008. Word sense in-
duction using graphs of collocations. In proceedings
of ECAI-2008, Patras, Greece.
G. Miller. 1995. Wordnet: a lexical database for en-
glish. Communications of the ACM, 38(11):39–41.
P. Pantel and D. Lin. 2002. Discovering word senses
from text. In proceedings of KDD-2002, New York,
NY, USA. ACM Press.
J. Veronis. 2004. Hyperlex: lexical cartography for in-
formation retrieval. Computer Speech &amp; Language,
18(3):223–252, July.
D. Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In proceed-
ings of ACL, Cambridge, MA, USA. ACL.
</reference>
<page confidence="0.997932">
358
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974816">
<title confidence="0.9996525">UoY: Graphs of Unambiguous Vertices for Word Sense Induction and Disambiguation</title>
<author confidence="0.989454">Ioannis Korkontzelos</author>
<author confidence="0.989454">Suresh Manandhar</author>
<affiliation confidence="0.999503">Department of Computer Science The University of York</affiliation>
<address confidence="0.999596">Heslington, York, YO10 5NG, UK</address>
<abstract confidence="0.9988065">This paper presents an unsupervised graph-based method for automatic word sense induction and disambiguation. The innovative part of our method is the assignment of either a word or a word pair to each vertex of the constructed graph. Word senses are induced by clustering the constructed graph. In the disambiguation stage, each induced cluster is scored according to the number of its vertices found in the context of the target word. Our system participated in SemEval-2010 word sense induction and disambiguation task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>A Soroa</author>
</authors>
<title>Semeval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In proceedings of SemEval-2007,</booktitle>
<publisher>ACL.</publisher>
<location>Czech Republic.</location>
<contexts>
<context position="1894" citStr="Agirre and Soroa, 2007" startWordPosition="289" endWordPosition="292">r, 1995), miss many senses, especially domainspecific ones (Pantel and Lin, 2002). The missing concepts are not recognised. Moreover, senses cannot be easily related to their use in context. Word sense induction methods can be divided into vector-space models and graph based ones. In a vector-space model, each context of a target word is represented as a feature vector, e.g. frequency of cooccurring words (Katz and Giesbrecht, 2006). Context vectors are clustered and the resulting clusters represent the induced senses. Recently, graph-based methods have been employed for word sense induction (Agirre and Soroa, 2007). Typically, graph-based methods represent each context word of the target word as a vertex. Two vertices are connected via an edge if they cooccur in one or more instances. Once the cooccurrence graph has been constructed, different graph clustering algorithms are applied to partition the graph. Each cluster (partition) consists of a set of words that are semantically related to the particular sense (Veronis, 2004). The potential advantage of graph-based methods is that they can combine both local and global cooccurrence information (Agirre et al., 2006). Klapaftis and Manandhar (2008) presen</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>E. Agirre and A. Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination systems. In proceedings of SemEval-2007, Czech Republic. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>D Martinez</author>
<author>O Lopez de Lacalle</author>
<author>A Soroa</author>
</authors>
<title>Two graph-based algorithms for state-of-the-art wsd.</title>
<date>2006</date>
<booktitle>In proceedings of EMNLP,</booktitle>
<publisher>ACL.</publisher>
<location>Sydney, Australia.</location>
<marker>Agirre, Martinez, de Lacalle, Soroa, 2006</marker>
<rawString>E. Agirre, D. Martinez, O. Lopez de Lacalle, and A. Soroa. 2006. Two graph-based algorithms for state-of-the-art wsd. In proceedings of EMNLP, Sydney, Australia. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Biemann</author>
</authors>
<title>Chinese whispers - an efficient graph clustering algorithm and its application to natural language processing problems.</title>
<date>2006</date>
<booktitle>In proceedings of TextGraphs,</booktitle>
<publisher>ACL.</publisher>
<location>New York City.</location>
<contexts>
<context position="8589" citStr="Biemann, 2006" startWordPosition="1397" endWordPosition="1398">based on cooccurrence of the corresponding vertices contents in one or more snippets. Edges whose respective vertices contents are infrequent are rejected. The weight ap356 Figure 2: A: Block diagram presenting the system overview. B, C, D: Block diagrams further analysing the structure of complex components of A. Parameter names appear within square brackets. plied to each edge is the maximum of the conditional probabilities of the corresponding vertices contents (e.g. w2,6, part III, figure 1). Low weight edges are filtered out (parameter P3). 2.3 Clustering the Graph Chinese Whispers (CW) (Biemann, 2006) was used to cluster the graph. CW is a randomised graph-clustering algorithm, time-linear to the number of edges. The number of clusters it produces is automatically inferred. Evaluation has shown that CW suits well in sense induction applications, where class distributions are often highly skewed. In our experiments, CW produced less clusters using a constant mutation rate (5%). To further reduce the number of induced clusters, we applied a post-processing stage, which exploits the one sense per collocation property (Yarowsky, 1995). For each cluster li, we generated the set Si of all snippe</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>C. Biemann. 2006. Chinese whispers - an efficient graph clustering algorithm and its application to natural language processing problems. In proceedings of TextGraphs, New York City. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="6554" citStr="Dunning, 1993" startWordPosition="1053" endWordPosition="1054">ed using the GENIA tagger2. Words that occur in a stoplist are filtered out. Instead of using all words as context, only nouns are kept, since they are more discriminative than verbs, adverbs and adjectives, that appear in a variety of different contexts. 1We refer to instances of the target word as snippets, since they can be either sentences or paragraphs. 2www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger Figure 1: An example showing how the proposed word sense induction system works. Nouns that occur infrequently in the reference corpus are removed (parameter P1). Then, loglikelihood ratio (LL) (Dunning, 1993) is employed to compare the distribution of each noun to its distribution in reference corpus. The null hypothesis is that the two distributions are similar. If this is true, LL is small value and the corresponding noun is removed (parameter P2). We also filter out nouns that are more indicative in the reference corpus than in the target word corpus; i.e. the nouns whose relative frequency in the former is larger than in the latter. At the end of this stage, each snippet is a list of lemmatised nouns contextually related to the target word. 2.2 Constructing the Graph All nouns appearing in the</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>T. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Katz</author>
<author>E Giesbrecht</author>
</authors>
<title>Automatic identification of non-compositional multi-word expressions using latent semantic analysis.</title>
<date>2006</date>
<booktitle>In proceedings of the ACL workshop on Multi-Word Expressions,</booktitle>
<publisher>ACL.</publisher>
<location>Sydney, Australia.</location>
<contexts>
<context position="1707" citStr="Katz and Giesbrecht, 2006" startWordPosition="261" endWordPosition="265">enses of a target word is a closed list of definitions coming from a standard dictionary (Agirre et al., 2006), was long ago abandoned. The reason is that sense lists, such as WordNet (Miller, 1995), miss many senses, especially domainspecific ones (Pantel and Lin, 2002). The missing concepts are not recognised. Moreover, senses cannot be easily related to their use in context. Word sense induction methods can be divided into vector-space models and graph based ones. In a vector-space model, each context of a target word is represented as a feature vector, e.g. frequency of cooccurring words (Katz and Giesbrecht, 2006). Context vectors are clustered and the resulting clusters represent the induced senses. Recently, graph-based methods have been employed for word sense induction (Agirre and Soroa, 2007). Typically, graph-based methods represent each context word of the target word as a vertex. Two vertices are connected via an edge if they cooccur in one or more instances. Once the cooccurrence graph has been constructed, different graph clustering algorithms are applied to partition the graph. Each cluster (partition) consists of a set of words that are semantically related to the particular sense (Veronis,</context>
</contexts>
<marker>Katz, Giesbrecht, 2006</marker>
<rawString>G. Katz and E. Giesbrecht. 2006. Automatic identification of non-compositional multi-word expressions using latent semantic analysis. In proceedings of the ACL workshop on Multi-Word Expressions, Sydney, Australia. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Klapaftis</author>
<author>S Manandhar</author>
</authors>
<title>Word sense induction using graphs of collocations.</title>
<date>2008</date>
<booktitle>In proceedings of ECAI-2008,</booktitle>
<location>Patras, Greece.</location>
<contexts>
<context position="2487" citStr="Klapaftis and Manandhar (2008)" startWordPosition="382" endWordPosition="385">se induction (Agirre and Soroa, 2007). Typically, graph-based methods represent each context word of the target word as a vertex. Two vertices are connected via an edge if they cooccur in one or more instances. Once the cooccurrence graph has been constructed, different graph clustering algorithms are applied to partition the graph. Each cluster (partition) consists of a set of words that are semantically related to the particular sense (Veronis, 2004). The potential advantage of graph-based methods is that they can combine both local and global cooccurrence information (Agirre et al., 2006). Klapaftis and Manandhar (2008) presented a graph-based approach that represents pairs of words as vertices instead of single words. They claimed that single words might appear with more than one senses of the target word, while they hypothesize that a pair of words is unambiguous. Hard-clustering the graph will potentially identify less conflating senses of the target word. In this paper, we relax the above hypothesis because in some cases a single word is unambiguous. We present a method that generates two-word vertices only when a single word vertex is unambiguous. If the word is judged as unambiguous, then it is represe</context>
</contexts>
<marker>Klapaftis, Manandhar, 2008</marker>
<rawString>I. Klapaftis and S. Manandhar. 2008. Word sense induction using graphs of collocations. In proceedings of ECAI-2008, Patras, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="1279" citStr="Miller, 1995" startWordPosition="194" endWordPosition="195">the target word. Our system participated in SemEval-2010 word sense induction and disambiguation task. 1 Introduction There exists significant evidence that word sense disambiguation is important for a variety of natural language processing tasks: machine translation, information retrieval, grammatical analysis, speech and text processing (Veronis, 2004). However, the “fixed-list” of senses paradigm, where the senses of a target word is a closed list of definitions coming from a standard dictionary (Agirre et al., 2006), was long ago abandoned. The reason is that sense lists, such as WordNet (Miller, 1995), miss many senses, especially domainspecific ones (Pantel and Lin, 2002). The missing concepts are not recognised. Moreover, senses cannot be easily related to their use in context. Word sense induction methods can be divided into vector-space models and graph based ones. In a vector-space model, each context of a target word is represented as a feature vector, e.g. frequency of cooccurring words (Katz and Giesbrecht, 2006). Context vectors are clustered and the resulting clusters represent the induced senses. Recently, graph-based methods have been employed for word sense induction (Agirre a</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>G. Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In proceedings of KDD-2002,</booktitle>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1352" citStr="Pantel and Lin, 2002" startWordPosition="203" endWordPosition="206">se induction and disambiguation task. 1 Introduction There exists significant evidence that word sense disambiguation is important for a variety of natural language processing tasks: machine translation, information retrieval, grammatical analysis, speech and text processing (Veronis, 2004). However, the “fixed-list” of senses paradigm, where the senses of a target word is a closed list of definitions coming from a standard dictionary (Agirre et al., 2006), was long ago abandoned. The reason is that sense lists, such as WordNet (Miller, 1995), miss many senses, especially domainspecific ones (Pantel and Lin, 2002). The missing concepts are not recognised. Moreover, senses cannot be easily related to their use in context. Word sense induction methods can be divided into vector-space models and graph based ones. In a vector-space model, each context of a target word is represented as a feature vector, e.g. frequency of cooccurring words (Katz and Giesbrecht, 2006). Context vectors are clustered and the resulting clusters represent the induced senses. Recently, graph-based methods have been employed for word sense induction (Agirre and Soroa, 2007). Typically, graph-based methods represent each context wo</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>P. Pantel and D. Lin. 2002. Discovering word senses from text. In proceedings of KDD-2002, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veronis</author>
</authors>
<title>Hyperlex: lexical cartography for information retrieval.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="1022" citStr="Veronis, 2004" startWordPosition="148" endWordPosition="149">of either a word or a word pair to each vertex of the constructed graph. Word senses are induced by clustering the constructed graph. In the disambiguation stage, each induced cluster is scored according to the number of its vertices found in the context of the target word. Our system participated in SemEval-2010 word sense induction and disambiguation task. 1 Introduction There exists significant evidence that word sense disambiguation is important for a variety of natural language processing tasks: machine translation, information retrieval, grammatical analysis, speech and text processing (Veronis, 2004). However, the “fixed-list” of senses paradigm, where the senses of a target word is a closed list of definitions coming from a standard dictionary (Agirre et al., 2006), was long ago abandoned. The reason is that sense lists, such as WordNet (Miller, 1995), miss many senses, especially domainspecific ones (Pantel and Lin, 2002). The missing concepts are not recognised. Moreover, senses cannot be easily related to their use in context. Word sense induction methods can be divided into vector-space models and graph based ones. In a vector-space model, each context of a target word is represented</context>
<context position="2313" citStr="Veronis, 2004" startWordPosition="358" endWordPosition="359">t, 2006). Context vectors are clustered and the resulting clusters represent the induced senses. Recently, graph-based methods have been employed for word sense induction (Agirre and Soroa, 2007). Typically, graph-based methods represent each context word of the target word as a vertex. Two vertices are connected via an edge if they cooccur in one or more instances. Once the cooccurrence graph has been constructed, different graph clustering algorithms are applied to partition the graph. Each cluster (partition) consists of a set of words that are semantically related to the particular sense (Veronis, 2004). The potential advantage of graph-based methods is that they can combine both local and global cooccurrence information (Agirre et al., 2006). Klapaftis and Manandhar (2008) presented a graph-based approach that represents pairs of words as vertices instead of single words. They claimed that single words might appear with more than one senses of the target word, while they hypothesize that a pair of words is unambiguous. Hard-clustering the graph will potentially identify less conflating senses of the target word. In this paper, we relax the above hypothesis because in some cases a single wor</context>
</contexts>
<marker>Veronis, 2004</marker>
<rawString>J. Veronis. 2004. Hyperlex: lexical cartography for information retrieval. Computer Speech &amp; Language, 18(3):223–252, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In proceedings of ACL,</booktitle>
<publisher>ACL.</publisher>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="9129" citStr="Yarowsky, 1995" startWordPosition="1481" endWordPosition="1482">rameter P3). 2.3 Clustering the Graph Chinese Whispers (CW) (Biemann, 2006) was used to cluster the graph. CW is a randomised graph-clustering algorithm, time-linear to the number of edges. The number of clusters it produces is automatically inferred. Evaluation has shown that CW suits well in sense induction applications, where class distributions are often highly skewed. In our experiments, CW produced less clusters using a constant mutation rate (5%). To further reduce the number of induced clusters, we applied a post-processing stage, which exploits the one sense per collocation property (Yarowsky, 1995). For each cluster li, we generated the set Si of all snippets that contain at least one vertex content of li. Then, any clusters la and lb were merged if Sa C Sb or Sa Q Sb. 3 Word Sense Disambiguation The induced senses are used to sense-tag each test instance of the target word (snippet). Given a snippet, each induced cluster is assigned a score equal to the number of its vertex contents (single or pairs of words) occurring in the snippet. The instance is assigned to the sense with the highest score or with equal weights to all highest scoring senses. 4 Tuning parameter and inducing senses </context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In proceedings of ACL, Cambridge, MA, USA. ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>