<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.067537">
<title confidence="0.9842475">
Building a semantically transparent corpus
for the generation of referring expressions
</title>
<author confidence="0.64101">
Kees van Deemter and Ielka van der Sluis and Albert Gatt
</author>
<affiliation confidence="0.9944">
Department of Computing Science
University of Aberdeen
</affiliation>
<email confidence="0.998893">
{kvdeemte,ivdsluis,agatt}@csd.abdn.ac.uk
</email>
<sectionHeader confidence="0.997391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999947818181818">
This paper discusses the construction of
a corpus for the evaluation of algorithms
that generate referring expressions. It is
argued that such an evaluation task re-
quires a semantically transparent corpus,
and controlled experiments are the best
way to create such a resource. We address
a number of issues that have arisen in an
ongoing evaluation study, among which is
the problem of judging the output of GRE
algorithms against a human gold standard.
</bodyText>
<sectionHeader confidence="0.830199" genericHeader="categories and subject descriptors">
1 Creating and using a corpus for GRE
</sectionHeader>
<bodyText confidence="0.99994965">
A decade ago, Dale and Reiter (1995) published
a seminal paper in which they compared a num-
ber of GRE algorithms. These algorithms included
a Full Brevity (FB) algorithm which generates de-
scriptions of minimal length, a greedy algorithm
(GA), and an Incremental Algorithm (IA). The
authors argued that the latter was the best model
of human referential behaviour, and versions of
the IA have since come to represent the state
of the art in GRE. Dale and Reiter’s hypothe-
sis was motivated by psycholinguistic findings,
notably that speakers tend to initiate references
before they have completely scanned a domain.
However, this finding affords different algorithmic
interpretations. Similarly, the finding that basic-
level terms in referring expressions allow hearers
to form a psychological gestalt could be incorpo-
rated into practically any GRE algorithm.1
We decided to put Dale and Reiter’s hypothesis
to the test by an evaluation of the output of dif-
</bodyText>
<footnote confidence="0.9923385">
1A separate argument for IA involves tractability, but al-
though some alternatives (such as FB) are intractable, others
(such as GA) are only polynomial, and can therefore not eas-
ily be dismissed on purely computational grounds.
</footnote>
<bodyText confidence="0.983296921052631">
ferent GRE algorithms against human production.
However, it is notoriously difficult to obtain suit-
able corpora for a task that is as semantically in-
tensive as Content Determination (for GRE). Al-
though existing corpora are valuable resources,
NLG often requires information that is not avail-
able in text. Suppose, for example, that a corpus
contained articles about politics, how would the
output of a GRE algorithm be evaluated against the
corpus? It would be difficult to infer from an ar-
ticle exactly which representatives in the British
House of Commons are Liberal Democrats, or
Scottish. Combining multiple texts is hazardous,
since facts could alter across sources and time.
Moreover, the conditions under which such texts
were produced (e.g. fault-critical or not, as ex-
plained below) are hard to determine.
A recent GRE evaluation by Gupta and Stent
(2005) focused on dialogue corpora, using MAP-
TASK and COCONUT, both of which have an as-
sociated domain. Their results show that referent
identification in MAPTASK often requires no more
than a TYPE attribute, so that none of the algo-
rithms performed better than a baseline. In con-
trast to MAPTASK, COCONUT has a more elabo-
rate domain, but it is characterised by a collabora-
tive task, and references frequently go beyond the
identification criterion that is typically invoked in
GRE2. Mindful of the limitations of existing cor-
pora, and of the extent to which evaluation de-
pends on the corpus under study, we are using
controlled experiments to create a corpus whose
construction will ensure that existing algorithms
can be adequately differentiated on an identifica-
tion task.
2Jordan and Walker (2000) have demonstrated a signifi-
cantly better match to the human data when task-related con-
straints are taken into account.
</bodyText>
<page confidence="0.881411">
130
</page>
<bodyText confidence="0.7933475">
Proceedings of the Fourth International Natural Language Generation Conference, pages 130–132,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.378162" genericHeader="method">
2 Setup of the experiment
</sectionHeader>
<bodyText confidence="0.99986944">
Like Dale and Reiter (1995), we focused on first-
mention descriptions. However, we decided to in-
clude simple ‘disjunctive’ references to sets (as
in ‘the red chair and the black table’), in addi-
tion to conjunctions of atomic properties, since
these can be handled by essentially the same al-
gorithms (van Deemter, 2002). For generality, we
looked at two very different domains. One of these
involved artificially constructed pictures of furni-
ture, where the available attributes and values are
relatively easy to determine. The other involved
real photographs of individuals, which provide a
richer range of options to subjects. To date, data
has been collected from 19 participants, and anal-
ysis is in progress.
Our first challenge was to make the experiment
naturalistic. Subjects were shown 38 randomised
trials, each depicting a set of objects, one or two
of which were the targets, surrounded by 6 dis-
tractors (Figure 1). In each case, a minimal distin-
guishing description of the targets was available.
Subjects were led to believe that they would be
describing the targets for an interlocutor. Once a
description was typed, the system removed from
the screen what it took to be the referents.
</bodyText>
<figureCaption confidence="0.999475">
Figure 1: A stimulus example from the furniture domain.
</figureCaption>
<bodyText confidence="0.999900793103448">
Three groups performed the task in different
conditions, namely: (+FaultCritical), where
half the subjects in the (+FaultCritical) case
could use location (‘in the top left corner’). The
(+FaultCritical) group was told: ‘Our program
will eventually be used in situations where it is
crucial that it understands descriptions accurately.
In these situations, there will often be no option to
correct mistakes. Therefore, (...) you will not get
the chance to revise (your description)’. By con-
trast, the (−FaultCritical) subjects were given
the opportunity to revise their description should
the system have got it wrong. Subjects in the
(−Location) condition were told that their inter-
locutor could see exactly the same pictures as they
could, but these had been jumbled up; by con-
trast, (+Location) subjects were led to believe
that their addressee could see the pictures in ex-
actly the same position.
The second main challenge was to create tri-
als that would distinguish between all the algo-
rithms. For instance, if trials involved only one at-
tribute, say an object’s TYPE (e.g., chair or table),
they would not allow us to distinguish IA from
FB, as both would always generate the shortest de-
scription. Subtler issues arise with local brevity
(Reiter, 1990), an optimisation strategy which re-
quires sufficiently complex trials to make a differ-
ence.
</bodyText>
<sectionHeader confidence="0.915735" genericHeader="method">
3 How to analyse the data?
</sectionHeader>
<bodyText confidence="0.998263387096774">
Our semantically transparent corpus can be
used for testing various hypotheses, for in-
stance about when an algorithm should
overspecify descriptions (e.g. more in
(+FaultCritical, +Location) (Arts, 2004),
and/or when the target is a set). Here, we focus on
the issue raised in Section 1, namely, which of the
algorithms discussed in Dale and Reiter (1995)
matches human behaviour best.
The first problem is determining the relevant al-
gorithms. The IA comes in different flavours, be-
cause its output depends on the order in which
the different properties are attempted (commonly
called the preference order). It is possible to
consider all different IAs (trying every conceiv-
able preference order), but this would increase the
number of statistical hypotheses to be tested, im-
pacting the validity of the results and requiring a
Bonferroni correction. Instead, we are using a pre-
test to find the optimal version of IA, comparing
only that version to the other algorithms.
The second question is how to assess algorithm
performance. Since our production experiment
does not yield a single gold standard (GS), an al-
gorithm might match subjects better in one con-
dition (e.g. (+FaultCritical), or perform bet-
ter in one domain (e.g. furniture). Moreover, it
might match subjects poorly overall due to sam-
ple variation, while evincing a perfect match with
a single individual. Using both a by-subjects and a
by-items analysis will partially control for sample
</bodyText>
<page confidence="0.99657">
131
</page>
<bodyText confidence="0.997360230769231">
dispersion.
How should we calculate the match between an
algorithm and a GS? Once again, there are two
facets to this problem. Since we are focusing on
Content Determination, each human description
could be viewed as associating, with the relevant
trial, a set of properties. Our approach will be to
annotate each human description with the set of at-
tributes it contains. However, the real data is often
messy. For example, when one subject called an
object ‘the non-coloured table’, and another called
it ‘the grey desk’, both may be expressing the same
attributes (i.e. TYPE and COLOUR). Also, while it
is often assumed that the output of GRE is a def-
inite noun phrase, this is not always the case in
our corpus, which contains indefinite distinguish-
ing descriptions such as ‘a red chair, facing to
the right’, and telegraphic messages such as ‘red,
right-facing’.
The second aspect to the problem concerns the
actual human-algorithm comparison. Suppose the
GS equals the output of one subject, and we are
comparing two algorithms, x and y. Suppose our
subject produced ‘the two huge red sofas’, which
the GS associates with the set {sofa, red, large}.
Suppose our algorithms describe the target as:
Output from x : {sofa, red, top}
Output from y : {sofa, red, large, top}
Which of these algorithms matches the GS best?
Algorithm y adds a property (perhaps overspecify-
ing even more than the GS). Algorithm x has the
same length as the GS, but replaces one property
by another. Several reasonable ways of assess-
ing the differences can be devised, one of which is
Levenshtein distance (which suggests preferring y
over x, since the latter involves a deletion and an
addition) (Levenshtein, 1966). We also intend to
examine how often the GS over- or underspecifies
where the algorithm does not.
</bodyText>
<sectionHeader confidence="0.999494" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999758541666667">
Corpora can be an invaluable resource for NLG
as long as the necessary contextual information
and the conditions under which the texts in a cor-
pus were produced are known. We believe that
controlled and balanced experiments are needed
for building semantically transparent resources,
whose construction we have discussed. As shown
in this paper, evaluation of algorithms against the
number of gold standards obtained with such a
corpus needs careful consideration.
Evaluation of GRE – and NLG systems more
generally – would benefit from more investiga-
tion of the differences between readers and pro-
ducers. In future work, we intend to follow up
with a reader-oriented experiment in which we test
the speed and/or accuracy with which the output
of different GRE algorithms is understood by sub-
jects. The dependent variables here will be non-
linguistic (perhaps involving subjects clicking on
pictures of presumed target referents). This illus-
trates a more general issue in this area, namely
that corpora should, in our view, only be a start-
ing point, with which data of different kinds can
be associated.
</bodyText>
<sectionHeader confidence="0.999679" genericHeader="acknowledgments">
5 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999794">
Thanks to Ehud Reiter, Richard Power
and Emiel Krahmer for useful comments.
This work is part of the TUNA project
(http://www.csd.abdn.ac.uk/
research/tuna/), funded by the EPSRC
in the UK (GR/S13330/01).
</bodyText>
<sectionHeader confidence="0.99946" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999940961538462">
[Arts2004] A. Arts. 2004. Overspeci�cation in Instruc-
tive Texts. Ph.D. thesis, Tilburg University.
[Dale and Reiter1995] R. Dale and E. Reiter. 1995.
Computational interpretations of the Gricean max-
ims in the generation of referring expressions. Cog-
nitive Science, 18:233–263.
[van Deemter2002] K. van Deemter. 2002. Generat-
ing referring expressions: Boolean extensions of the
incremental algorithm. Computational Linguistics,
28(1):37–52.
[Gupta and Stent2005] S. Gupta and A. J. Stent. 2005.
Automatic evaluation of referring expression gener-
ation using corpora. In Proceedings of the 1st Work-
shop on Using Corpora in NLG, Birmingham, UK.
[Jordan and Walker2000] P. Jordan and M. Walker.
2000. Learning attribute selections for non-
pronominal expressions. In Proceedings of the 38th
Annual Meeting of the Association for Computa-
tional Linguistics.
[Levenshtein1966] V. Levenshtein. 1966. Binary codes
capable of correcting deletions, insertions and rever-
sals. Soviet Physics Doklady, 10(8):707–710.
[Reiter1990] E. Reiter. 1990. The computational com-
plexity of avoiding conversational implicatures. In
Proceedings of the 28th ACL Meeting, pages 97–
104. MIT Press.
</reference>
<page confidence="0.99773">
132
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.046677">
<title confidence="0.9994865">Building a semantically transparent for the generation of referring expressions</title>
<author confidence="0.999868">van_Deemter van_der_Sluis</author>
<affiliation confidence="0.9969665">Department of Computing University of</affiliation>
<abstract confidence="0.998652055793992">This paper discusses the construction of a corpus for the evaluation of algorithms that generate referring expressions. It is argued that such an evaluation task requires a semantically transparent corpus, and controlled experiments are the best way to create such a resource. We address a number of issues that have arisen in an ongoing evaluation study, among which is problem of judging the output of algorithms against a human gold standard. Creating and using a corpus for A decade ago, Dale and Reiter (1995) published a seminal paper in which they compared a numof These algorithms included Full Brevity algorithm which generates descriptions of minimal length, a greedy algorithm and an Incremental Algorithm The authors argued that the latter was the best model of human referential behaviour, and versions of the IA have since come to represent the state the art in Dale and Reiter’s hypothesis was motivated by psycholinguistic findings, notably that speakers tend to initiate references before they have completely scanned a domain. However, this finding affords different algorithmic interpretations. Similarly, the finding that basiclevel terms in referring expressions allow hearers to form a psychological gestalt could be incorpointo practically any We decided to put Dale and Reiter’s hypothesis the test by an evaluation of the output of difseparate argument for tractability, but alsome alternatives (such as are intractable, others as are only polynomial, and can therefore not easily be dismissed on purely computational grounds. against human production. However, it is notoriously difficult to obtain suitable corpora for a task that is as semantically inas Content Determination (for Although existing corpora are valuable resources, NLG often requires information that is not available in text. Suppose, for example, that a corpus contained articles about politics, how would the of a be evaluated against the corpus? It would be difficult to infer from an article exactly which representatives in the British House of Commons are Liberal Democrats, or Scottish. Combining multiple texts is hazardous, since facts could alter across sources and time. Moreover, the conditions under which such texts produced (e.g. not, as explained below) are hard to determine. recent by Gupta and Stent focused on dialogue corpora, using both of which have an associated domain. Their results show that referent in requires no more a so that none of the algorithms performed better than a baseline. In conto a more elaborate domain, but it is characterised by a collaborative task, and references frequently go beyond the identification criterion that is typically invoked in Mindful of the limitations of existing corpora, and of the extent to which evaluation depends on the corpus under study, we are using controlled experiments to create a corpus whose construction will ensure that existing algorithms can be adequately differentiated on an identification task. and Walker (2000) have demonstrated a significantly better match to the human data when task-related constraints are taken into account. 130 of the Fourth International Natural Language Generation pages July 2006. Association for Computational Linguistics 2 Setup of the experiment Like Dale and Reiter (1995), we focused on firstmention descriptions. However, we decided to include simple ‘disjunctive’ references to sets (as in ‘the red chair and the black table’), in addition to conjunctions of atomic properties, since these can be handled by essentially the same algorithms (van Deemter, 2002). For generality, we looked at two very different domains. One of these involved artificially constructed pictures of furniture, where the available attributes and values are relatively easy to determine. The other involved real photographs of individuals, which provide a richer range of options to subjects. To date, data has been collected from 19 participants, and analysis is in progress. Our first challenge was to make the experiment naturalistic. Subjects were shown 38 randomised trials, each depicting a set of objects, one or two of which were the targets, surrounded by 6 distractors (Figure 1). In each case, a minimal distinguishing description of the targets was available. Subjects were led to believe that they would be describing the targets for an interlocutor. Once a description was typed, the system removed from the screen what it took to be the referents. 1: stimulus example from the furniture domain. Three groups performed the task in different namely: where the subjects in the could use location (‘in the top left corner’). The was told: ‘Our program will eventually be used in situations where it is crucial that it understands descriptions accurately. In these situations, there will often be no option to correct mistakes. Therefore, (...) you will not get the chance to revise (your description)’. By conthe were given the opportunity to revise their description should the system have got it wrong. Subjects in the were told that their interlocutor could see exactly the same pictures as they could, but these had been jumbled up; by conwere led to believe that their addressee could see the pictures in exactly the same position. The second main challenge was to create trials that would distinguish between all the algorithms. For instance, if trials involved only one atsay an object’s would not allow us to distinguish as both would always generate the shortest de- Subtler issues arise with brevity (Reiter, 1990), an optimisation strategy which requires sufficiently complex trials to make a difference. 3 How to analyse the data? Our semantically transparent corpus can be used for testing various hypotheses, for instance about when an algorithm should overspecify descriptions (e.g. more 2004), and/or when the target is a set). Here, we focus on the issue raised in Section 1, namely, which of the algorithms discussed in Dale and Reiter (1995) matches human behaviour best. The first problem is determining the relevant al- The in different flavours, because its output depends on the order in which the different properties are attempted (commonly called the preference order). It is possible to (trying every conceivable preference order), but this would increase the number of statistical hypotheses to be tested, impacting the validity of the results and requiring a Bonferroni correction. Instead, we are using a preto find the optimal version of comparing only that version to the other algorithms. The second question is how to assess algorithm performance. Since our production experiment not yield a standard an algorithm might match subjects better in one con- (e.g. or perform better in one domain (e.g. furniture). Moreover, it might match subjects poorly overall due to sample variation, while evincing a perfect match with single individual. Using both a a will partially control for sample 131 dispersion. should we calculate the an and a Once again, there are two facets to this problem. Since we are focusing on Content Determination, each human description could be viewed as associating, with the relevant trial, a set of properties. Our approach will be to annotate each human description with the set of attributes it contains. However, the real data is often messy. For example, when one subject called an object ‘the non-coloured table’, and another called it ‘the grey desk’, both may be expressing the same (i.e. Also, while it often assumed that the output of a definite noun phrase, this is not always the case in our corpus, which contains indefinite distinguishdescriptions such as red chair, facing to and telegraphic messages such as The second aspect to the problem concerns the actual human-algorithm comparison. Suppose the the output of one subject, and we are two algorithms, Suppose our subject produced ‘the two huge red sofas’, which with the set red, Suppose our algorithms describe the target as: from red, from red, large, of these algorithms matches the a property (perhaps overspecifyeven more than the Algorithm the length as the but replaces one property by another. Several reasonable ways of assessing the differences can be devised, one of which is distance (which suggests preferring since the latter involves a deletion addition) (Levenshtein, 1966). We also intend to how often the or underspecifies where the algorithm does not. 4 Conclusion can be an invaluable resource for as long as the necessary contextual information and the conditions under which the texts in a corpus were produced are known. We believe that controlled and balanced experiments are needed for building semantically transparent resources, whose construction we have discussed. As shown in this paper, evaluation of algorithms against the number of gold standards obtained with such a corpus needs careful consideration. of and more generally – would benefit from more investigation of the differences between readers and producers. In future work, we intend to follow up with a reader-oriented experiment in which we test the speed and/or accuracy with which the output different is understood by subjects. The dependent variables here will be nonlinguistic (perhaps involving subjects clicking on pictures of presumed target referents). This illustrates a more general issue in this area, namely that corpora should, in our view, only be a starting point, with which data of different kinds can be associated. 5 Acknowledgments Thanks to Ehud Reiter, Richard Power and Emiel Krahmer for useful comments.</abstract>
<note confidence="0.6770752">This work is part of the TUNA project funded by the EPSRC in the UK (GR/S13330/01). References A. Arts. 2004. in Instruc- Ph.D. thesis, Tilburg University. [Dale and Reiter1995] R. Dale and E. Reiter. 1995. Computational interpretations of the Gricean maxin the generation of referring expressions. Cog- 18:233–263. [van Deemter2002] K. van Deemter. 2002. Generating referring expressions: Boolean extensions of the algorithm. 28(1):37–52. [Gupta and Stent2005] S. Gupta and A. J. Stent. 2005. Automatic evaluation of referring expression generusing corpora. In of the 1st Workshop on Using Corpora in NLG, Birmingham, UK. [Jordan and Walker2000] P. Jordan and M. Walker. Learning attribute selections for expressions. In of the 38th Annual Meeting of the Association for Computa- [Levenshtein1966] V. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and rever- Physics 10(8):707–710. [Reiter1990] E. Reiter. 1990. The computational complexity of avoiding conversational implicatures. In of the 28th ACL pages 97– 104. MIT Press. 132</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Arts</author>
</authors>
<title>Overspeci�cation in Instructive Texts.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>Tilburg University.</institution>
<marker>[Arts2004]</marker>
<rawString>A. Arts. 2004. Overspeci�cation in Instructive Texts. Ph.D. thesis, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Dale</author>
<author>E Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<pages>18--233</pages>
<marker>[Dale and Reiter1995]</marker>
<rawString>R. Dale and E. Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 18:233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K van Deemter</author>
</authors>
<title>Generating referring expressions: Boolean extensions of the incremental algorithm.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>1</issue>
<marker>[van Deemter2002]</marker>
<rawString>K. van Deemter. 2002. Generating referring expressions: Boolean extensions of the incremental algorithm. Computational Linguistics, 28(1):37–52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gupta</author>
<author>A J Stent</author>
</authors>
<title>Automatic evaluation of referring expression generation using corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 1st Workshop on Using Corpora in NLG,</booktitle>
<location>Birmingham, UK.</location>
<marker>[Gupta and Stent2005]</marker>
<rawString>S. Gupta and A. J. Stent. 2005. Automatic evaluation of referring expression generation using corpora. In Proceedings of the 1st Workshop on Using Corpora in NLG, Birmingham, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jordan</author>
<author>M Walker</author>
</authors>
<title>Learning attribute selections for nonpronominal expressions.</title>
<date>2000</date>
<booktitle>In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<marker>[Jordan and Walker2000]</marker>
<rawString>P. Jordan and M. Walker. 2000. Learning attribute selections for nonpronominal expressions. In Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<marker>[Levenshtein1966]</marker>
<rawString>V. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Reiter</author>
</authors>
<title>The computational complexity of avoiding conversational implicatures.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th ACL Meeting,</booktitle>
<pages>97--104</pages>
<publisher>MIT Press.</publisher>
<marker>[Reiter1990]</marker>
<rawString>E. Reiter. 1990. The computational complexity of avoiding conversational implicatures. In Proceedings of the 28th ACL Meeting, pages 97– 104. MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>