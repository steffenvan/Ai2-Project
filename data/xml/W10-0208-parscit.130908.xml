<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.998732">
Evaluation of Unsupervised Emotion Models
to Textual Affect Recognition
</title>
<author confidence="0.998241">
Sunghwan Mac Kim
</author>
<affiliation confidence="0.995565333333333">
School of Electrical
and Information Engineering
University of Sydney
</affiliation>
<address confidence="0.528195">
Sydney, Australia
</address>
<email confidence="0.995612">
skim1871@uni.sydney.edu.au
</email>
<author confidence="0.99136">
Alessandro Valitutti
</author>
<affiliation confidence="0.997971">
Department of Cognitive Science
and Education
University of Trento
</affiliation>
<address confidence="0.601377">
Trento, Italy
</address>
<email confidence="0.996202">
a.valitutti@email.unitn.it
</email>
<author confidence="0.987371">
Rafael A. Calvo
</author>
<affiliation confidence="0.91859425">
School of Electrical
and Information Engineering
University of Sydney
Sydney, Australia
</affiliation>
<email confidence="0.991861">
rafa@ee.usyd.edu.au
</email>
<sectionHeader confidence="0.998655" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.995636909090909">
In this paper we present an evaluation of new
techniques for automatically detecting emo-
tions in text. The study estimates categorical
model and dimensional model for the recogni-
tion of four affective states: Anger, Fear, Joy,
and Sadness that are common emotions in
three datasets: SemEval-2007 “Affective
Text”, ISEAR (International Survey on Emo-
tion Antecedents and Reactions), and child-
ren’s fairy tales. In the first model, WordNet-
Affect is used as a linguistic lexical resource
and three dimensionality reduction techniques
are evaluated: Latent Semantic Analysis
(LSA), Probabilistic Latent Semantic Analysis
(PLSA), and Non-negative Matrix Factoriza-
tion (NMF). In the second model, ANEW (Af-
fective Norm for English Words), a normative
database with affective terms, is employed.
Experiments show that a categorical model us-
ing NMF results in better performances for
SemEval and fairy tales, whereas a dimension-
al model performs better with ISEAR.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999947382978723">
Supervised and unsupervised approaches have
been used to automatically recognize expressions
of emotion in text such as happiness, sadness,
anger, etc... Supervised learning techniques
have the disadvantage that large annotated data-
sets are required for training. Since the emotional
interpretations of a text can be highly subjective,
more than one annotator is needed, and this
makes the process of the annotation very time
consuming and expensive. For this reason, unsu-
pervised methods are normally preferred in the
realm of Natural Language Processing (NLP)
and emotions.
Supervised and unsupervised techniques have
been compared before. (Strapparava and Mihal-
cea 2008) describe the comparison between a
supervised (Naïve Bayes) and an unsupervised
(Latent Semantic Analysis - LSA) method for
recognizing six basic emotions.
These techniques have been applied to many
areas, particularly in improving Intelligent Tutor-
ing Systems. For example, (D’Mello, Craig et al.
2008) used LSA but for detecting utterance types
and affect in students’ dialogue within Autotutor.
(D&apos;Mello, Graesser et al. 2007) proposed five
categories for describing the affect states in stu-
dent-system dialogue.
Significant differences arise not only between
these two types of techniques but also between
different emotion models, and these differences
have significant implications in all these areas.
While considering emotions and learning, (Kort,
Reilly et al. 2001) proposed (but provided no
empirical evidence) a model that combines two
emotion models, placing categories in a valence-
arousal plane. This mixed approach has also been
used in other domains such as blog posts where
(Aman and Szpakowicz 2007) studied how to
identify emotion categories as well as emotion
intensity. To date, many researchers have, how-
ever, utilized and evaluated supervised methods,
mainly based on the categorical emotion model.
In this study, the goal is to evaluate the merits
of two conceptualizations of emotions (a cate-
gorical model and a dimensional model) in
which an unsupervised approach is used. The
evaluation incorporates three dimensionality re-
</bodyText>
<page confidence="0.984704">
62
</page>
<note confidence="0.9905375">
Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 62–70,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999872733333333">
duction methods and two linguistic lexical re-
sources.
The rest of the paper is organized as follows:
In Section 2 we present representative research
of the emotion models used to capture the affec-
tive states of a text. Section 3 describes the tech-
niques of affect classification utilizing lexical
resources. More specifically, it describes the role
of emotion models and lexical resources in the
affect classification. In addition, we give an
overview of the dimension reduction methods
used in the study. In Section 4 we go over the
affective datasets used. Section 5 provides the
results of the evaluation, before coming to our
discussion in Section 6.
</bodyText>
<sectionHeader confidence="0.995734" genericHeader="method">
2 Emotion Models
</sectionHeader>
<bodyText confidence="0.999975488372093">
There are two significantly different models for
representing emotions: the categorical model and
dimensional model (Russell 2003).
The categorical model assumes that there are
discrete emotional categories such as Ekman’s
six basic emotions - anger, disgust, fear, joy,
sadness, and surprise - (Ekman 1992). There are
a number of primary and unrelated emotions in
the model. Each emotion is characterized by a
specific set of features, expressing eliciting con-
ditions or responses. Some researchers have ar-
gued that a different set of emotions is required
for different domains. For instance, the following
emotion classes are used in the field of teaching
and education: boredom, delight, flow, confusion,
frustration, and surprise. The advantage of such
a representation is that it represents human emo-
tions intuitively with easy to understand emotion
labels.
A second approach is the dimensional model,
which represents affects in a dimensional form
(Russell 2003). Emotional states are related each
other by a common set of dimensions (e.g. va-
lence or arousal) and are generally defined in a
two or three dimensional space. Each emotion
occupies some location in this space. A valence
dimension indicates positive and negative emo-
tions on different ends of the scale. The arousal
dimension differentiates excited vs. calm states.
Sometimes a third, dominance dimension is used
to differentiate if the subject feels in control of
the situation or not.
The categorical model and the dimensional
model have two different methods for estimating
the actual emotional states of a person. In the
former, a person is usually required to choose
one emotion out of an emotion set that represents
the best feeling. On the other hand, the latter ex-
ploits rating scales for each dimension like the
Self Assessment Manikin (SAM) (Lang 1980),
which consists of pictures of manikins, to esti-
mate the degree of valence, arousal, and domi-
nance.
</bodyText>
<sectionHeader confidence="0.998637" genericHeader="method">
3 Automatic Affect Classification
</sectionHeader>
<subsectionHeader confidence="0.9939705">
3.1 Categorical classification with features
derived from WordNet-Affect
</subsectionHeader>
<bodyText confidence="0.999793735294117">
WordNet-Affect (Strapparava and Valitutti 2004)
is an affective lexical repository of words refer-
ring to emotional states. WordNet-Affect extends
WordNet by assigning a variety of affect labels
to a subset of synsets representing affective con-
cepts in WordNet (emotional synsets). In addi-
tion, WordNet-Affect has an additional hierarchy
of affective domain labels. There are publicly
available lists relevant to the six basic emotion
categories extracted from WordNet-Affect and
we used four of the six lists of emotional words
among them for our experiment.
In addition to WordNet-Affect, we exploited a
Vector Space Model (VSM) in which terms and
textual documents can be represented through a
term-by-document matrix. More specifically,
terms are encoded as vectors, whose components
are co-occurrence frequencies of words in corpo-
ra documents. Frequencies are weighted accord-
ing to the log-entropy with respect to a tf-idf
weighting schema (Yates and Neto 1999). Final-
ly, the number of dimensions is reduced through
the dimension reduction methods.
The vector-based representation enables words,
sentences, and sets of synonyms (i.e. WordNet
synsets) to be represented in a unifying way with
vectors. VSM provides a variety of definitions of
distance between vectors, corresponding to dif-
ferent measures of semantic similarity. In par-
ticular, we take advantage of cosine angle be-
tween an input vector (input sentence) and an
emotional vector (i.e. the vector representing an
emotional synset) as similarity measures to iden-
tify which emotion the sentence connotes.
</bodyText>
<subsectionHeader confidence="0.998633">
3.2 Dimension Reduction Methods
</subsectionHeader>
<bodyText confidence="0.99996">
The VSM representation can be reduced with
techniques well known in Information Retrieval:
LSA, Probabilistic LSA (PLSA), or the Non-
negative Matrix Factorization (NMF) representa-
tions.
Cosine similarities can be defined in these re-
presentations, and here, as other authors have
done, we use a rule that if the cosine similarity
</bodyText>
<page confidence="0.99703">
63
</page>
<bodyText confidence="0.96289145945946">
does not exceed a threshold, the input sentence is
labeled as “neutral”, the absence of emotion.
Otherwise, it is labeled with one emotion asso-
ciated with the closest emotional vector having
the highest similarity value. We use a predeter-
mined threshold (t = 0.65) for the purpose of va-
lidating a strong emotional analogy between two
vectors (Penumatsa, Ventura et al. 2006).
If we define the similarity between a given in-
put text, I, and an emotional class, E , as
sim(I, E ), the categorical classification result,
CCR, is more formally represented as follows:
CCR(I)
Jarg max (sim(I, E )) if sim(I, E) &gt;_ t
= j
&amp;quot;neutral&amp;quot; if sim(I, E) &lt; t
One class with the maximum score is selected as
the final emotion class.
Dimensionality reduction in VSM reduces the
computation time and reduces the noise in the
data. This enables the unimportant data to dissi-
pate and underlying semantic text to become
more patent. We will review three statistical di-
mensionality reduction methods (LSA, PLSA,
and NMF) that are utilized in a category-based
emotion model.
Latent Semantic Analysis (LSA) is the earliest
approach successfully applied to various text
manipulation areas (Landauer, Foltz et al. 1998).
The main idea of LSA is to map terms or docu-
ments into a vector space of reduced dimensio-
nality that is the latent semantic space. The map-
ping of the given terms/document vectors to this
space is based on singular vector decomposition
(SVD). It is known that SVD is a reliable tech-
nique for matrix decomposition. It can decom-
pose a matrix as the product of three matrices.
</bodyText>
<equation confidence="0.861202">
A = UEVT ^ UkEkVkT = Ak (1)
</equation>
<bodyText confidence="0.99950525">
where Ak is the closest matrix of rank k to the
original matrix. The columns of Vk represent the
coordinates for documents in the latent space.
Probabilistic Latent Semantic Anlaysis (PLSA)
(Hofmann 2001) has two characteristics distin-
guishing it from LSA. PLSA defines proper
probability distributions and the reduced matrix
does not contain negative values. Based on the
combination of LSA and some probabilistic theo-
ries such as Bayes rules, the PLSA allows us to
find the latent topics, the association of docu-
ments and topics, and the association of terms
and topics. In the equation (2), z is a latent class
variable (i.e. discrete emotion category), while w
and d denote the elements of term vectors and
document vectors, respectively.
</bodyText>
<equation confidence="0.9374485">
P(d,w) = I P(z)P(w|z)P(d|z) (2)
z
</equation>
<bodyText confidence="0.999959263157895">
where P(w|z) and P(d|z) are topic-specific word
distribution and document distribution, indivi-
dually. The decomposition of PLSA, unlike that
of LSA, is performed by means of the likelihood
function. In other words, P(z), P(w|z), and P(d|z)
are determined by the maximum likelihood esti-
mation (MLE) and this maximization is per-
formed through adopting the Expectation Max-
imization (EM) algorithm. For document similar-
ities, each row of the P(d|z) matrix is considered
with the low-dimensional representation in the
semantic topic space.
Non-negative Matrix Factorization (NMF)
(Lee and Seung 1999) has been successfully ap-
plied to semantic analysis. Given a non-negative
matrix A, NMF finds non-negative factors W and
H that are reduced-dimensional matrices. The
product WH can be regarded as a compressed
form of the data in A.
</bodyText>
<equation confidence="0.924542">
A � WH = I WH (3)
</equation>
<bodyText confidence="0.9954525">
W is a basis vector matrix and H is an encoded
matrix of the basis vectors in the equation (3).
NMF solves the following minimization problem
(4) in order to obtain an approximation A by
computing W and H in terms of minimizing the
Frobenius norm of the error.
</bodyText>
<equation confidence="0.697566">
��� IIA = WHII� 2 , s. t. W, H &gt;_ 0 (4)
W,H
</equation>
<bodyText confidence="0.999773142857143">
where W, H &gt; 0 means that all elements of Wand
H are non-negative. This non-negative peculiari-
ty is desirable for handling text data that always
require non-negativity constraints. The classifi-
cation of documents is performed based on the
columns of matrix H that represent the docu-
ments.
</bodyText>
<subsectionHeader confidence="0.755395">
3.3 Three-dimensional estimation with fea-
tures derived from ANEW
</subsectionHeader>
<bodyText confidence="0.999893428571429">
Dimensional models have been studied by psy-
chologists often by providing a stimulus (e.g. a
photo or a text), and then asking subjects to re-
port on the affective experience. ANEW (Brad-
ley and Lang 1999) is a set of normative emo-
tional ratings for a collection of English words
(N=1,035), where after reading the words, sub-
jects reported their emotions in a three dimen-
sional representation. This collection provides
the rated values for valence, arousal, and domin-
ance for each word rated using the Self Assess-
ment Manikin (SAM). For each word w, the
normative database provides coordinates w� in an
affective space as:
</bodyText>
<page confidence="0.920986">
64
</page>
<equation confidence="0.974234">
w� = (valence, arousal, dominance) (5)
= AN W(w)
</equation>
<bodyText confidence="0.9999395">
The occurrences of these words in a text can
be used, in a naïve way, to weight the sentence in
this emotional plane. This is a naïve approach
since words often change their meaning or emo-
tional value when they are used in different con-
texts.
As a counterpart to the categorical classifica-
tion above, this approach assumes that an input
sentence pertains to an emotion based on the
least distance between each other on the Va-
lence-Arousal-Dominance (VAD) space. The
input sentence consists of a number of words and
the VAD value of this sentence is computed by
averaging the VAD values of the words:
</bodyText>
<equation confidence="0.967057666666667">
Z7
sentence = L-1w (6)
n
</equation>
<bodyText confidence="0.999860222222222">
where n is the total number of words in the input
sentence.
Since not many words are available in this
normative database, a series of synonyms from
WordNet-Affect are used in order to calculate
the position of each emotion. These emotional
synsets are converted to the 3-dimensional VAD
space and averaged for the purpose of producing
a single point for the target emotion as follows:
</bodyText>
<equation confidence="0.649429">
emotion = �1
</equation>
<bodyText confidence="0.99130135">
ik w (7)
where k denotes the total number of synonyms in
an emotion. Anger, fear, joy, and sadness emo-
tions are mapped on the VAD space. Let Ac, Fc,
Jc, and Sc be the centroids of four emotions. Then
the centroids, which are calculated by the equa-
tion (7), are as follows: Ac = (2.55, 6.60, 5.05), Fc
= (3.20, 5.92, 3.60), Jc = (7.40, 5.73, 6.20), and
Sc = (3.15, 4.56, 4.00). Apart from the four emo-
tions, we manually define neutral to be (5, 5, 5).
If the centroid of an input sentence is the most
approximate to that of an emotion, the sentence
is tagged as the emotion (with the nearest neigh-
bor algorithm). The centroid sentence ������������might be
close to an �����������
emotion on the VAD space, even if
they do not share any terms in common. We de-
fine the distance threshold (empirically set to 4)
to validate the appropriate proximity like the ca-
tegorical classification.
</bodyText>
<sectionHeader confidence="0.999633" genericHeader="method">
4 Emotion-Labeled Data
</sectionHeader>
<bodyText confidence="0.999916621621621">
Three emotional datasets, with sentence-level
emotion annotations, were employed for the
evaluation described in the next section. The first
dataset is “Affective Text” from the SemEval
2007 task (Strapparava and Mihalcea 2007). 1
This dataset consists of news headlines excerpted
from newspapers and news web sites. Headlines
are suitable for our experiments because head-
lines are typically intended to express emotions
in order to draw the readers’ attention. This data-
set has six emotion classes: anger, disgust, fear,
joy, sadness and surprise, and is composed of
1,250 annotated headlines. The notable characte-
ristics are that SemEval dataset does not only
allow one sentence to be tagged with multiple
emotions, but the dataset also contains a neutral
category in contrast to other datasets.
We also use the ISEAR (International Survey
on Emotion Antecedents and Reactions) dataset,
which consists of 7,666 sentences (Scherer and
Wallbott 1994), with regard to our experiments. 2
For building the ISEAR, 1,096 participants who
have different cultural backgrounds completed
questionnaires about experiences and reactions
for seven emotions including anger, disgust, fear,
joy, sadness, shame and guilt.
The annotated sentences of the third dataset
are culled from fairy tales (Alm 2009). Emotions
are particularly significant elements in the lite-
rary genre of fairy tales. The label set with five
emotion classes is as follows: angry-disgusted,
fearful, happy, sad and surprised. There are 176
stories by three authors: B. Potter, H.C. Ander-
sen, and Grimm’s. The dataset is composed of
only sentences with affective high agreements,
which means that annotators highly agreed upon
the sentences (four identical emotion labels).
</bodyText>
<table confidence="0.9992355">
Emotion SemEval ISEAR Fairy Total
tales
Anger 62 2,168 218 2,448
Fear 124 1,090 166 1,380
Joy 148 1,090 445 1,683
Sadness 145 1,082 264 1,491
</table>
<tableCaption confidence="0.999864">
Table 1: Number of sentences for each emotion
</tableCaption>
<bodyText confidence="0.9999128">
In our study, we have taken into account four
emotion classes (Anger, Fear, Joy and Sadness)
which are in the intersection among three data-
sets (SemEval, ISEAR and Fairy tales). The
number of sentences for each emotion and each
</bodyText>
<footnote confidence="0.844764666666667">
1 The dataset is publicly available at
http://www.cse.unt.edu/~rada/affecti
vetext.
2 Available at
http://www.unige.ch/fapse/emotion/da
tabanks/isear.html
</footnote>
<page confidence="0.998712">
65
</page>
<table confidence="0.999949272727273">
Data set SemEval ISEAR Fairy tales
Emotion Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
Anger MCB 0.000 0.000 - 0.399 1.000 0.571 0.000 0.000 -
CLSA 0.089 0.151 0.112 0.468 0.970 0.631 0.386 0.749 0.510
CPLSA 0.169 0.440 0.244 0.536 0.397 0.456 0.239 0.455 0.313
CNMF 0.294 0.263 0.278 0.410 0.987 0.579 0.773 0.560 0.650
DIM 0.161 0.192 0.175 0.708 0.179 0.286 0.604 0.290 0.392
Fear MCB 0.000 0.000 - 0.000 0.000 - 0.000 0.000 -
CLSA 0.434 0.622 0.511 0.633 0.038 0.071 0.710 0.583 0.640
CPLSA 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000
CNMF 0.525 0.750 0.618 0.689 0.029 0.056 0.704 0.784 0.741
DIM 0.404 0.404 0.404 0.531 0.263 0.351 0.444 0.179 0.255
Joy MCB 0.309 1.000 0.472 0.000 0.000 - 0.407 1.000 0.579
CLSA 0.455 0.359 0.402 0.333 0.061 0.103 0.847 0.637 0.727
CPLSA 0.250 0.258 0.254 0.307 0.381 0.340 0.555 0.358 0.436
CNMF 0.773 0.557 0.648 0.385 0.005 0.010 0.802 0.761 0.781
DIM 0.573 0.934 0.710 0.349 0.980 0.515 0.661 0.979 0.789
Sadness MCB 0.000 0.000 - 0.000 0.000 - 0.000 0.000 -
CLSA 0.472 0.262 0.337 0.500 0.059 0.106 0.704 0.589 0.642
CPLSA 0.337 0.431 0.378 0.198 0.491 0.282 0.333 0.414 0.370
CNMF 0.500 0.453 0.475 0.360 0.009 0.017 0.708 0.821 0.760
DIM 0.647 0.157 0.253 0.522 0.249 0.337 0.408 0.169 0.240
</table>
<tableCaption confidence="0.998867">
Table 3: Emotion identification results
</tableCaption>
<listItem confidence="0.68204">
dataset used in our experiment is shown in Table
1. In addition, sample sentences from the anno-
tated corpus appear in Table 2.
</listItem>
<bodyText confidence="0.910661857142857">
Dataset Sentences tagged with Sadness/Sad
SemEval Bangladesh ferry sink, 15 dead.
When I left a man in whom I really
believed.
The flower could not, as on the pre-
vious evening, fold up its petals and
sleep; it dropped sorrowfully.
</bodyText>
<tableCaption confidence="0.9798055">
Table 2: Sample sentences labeled with sadness/sad
from the datasets
</tableCaption>
<sectionHeader confidence="0.976282" genericHeader="evaluation">
5 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999352666666667">
The goal of the affect classification is to predict a
single emotional label given an input sentence.
Four different approaches were implemented in
Matlab. A categorical model based on a VSM
with dimensionality reduction variants, (LSA,
PLSA, and NMF), and a dimensional model,
each with evaluated with two similarity measures
(cosine angle and nearest neighbor). Stopwords
were removed in all approaches. A Matlab tool-
kit (Zeimpekis and Gallopoulos 2005), was used
to generate the term-by-sentence matrix from the
text.
The evaluation in Table 3 shows Majority
Class Baseline (MCB) as the baseline algorithm.
The MCB is the performance of a classifier that
always predicts the majority class. In SemEval
and Fairy tales the majority class is joy, while
anger is the majority emotion in case of ISEAR.
The five approaches were evaluated on the data-
set of 479 news headlines (SemEval), 5,430 res-
ponses to questions (ISEAR), and 1,093 fairy
tales’ sentences. We define the following acro-
nyms to identify the approaches:
CLSA: LSA-based categorical classification
CPLSA: PLSA-based categorical classifica-
tion
CNMF: NMF-based categorical classification
DIM: Dimension-based estimation
The measure of accuracies used here were:
Cohen’s Kappa (Cohen 1960), average precision,
recall, and F-measure. While the kappa scores
are useful in obtaining an overview of the relia-
bility of the various classification approaches,
they do not provide any insight on the accuracy
at the category level for which precision, recall,
and F-measure are necessary.
</bodyText>
<figure confidence="0.687287666666667">
ISEAR
Fairy
tales
</figure>
<page confidence="0.582005">
66
</page>
<table confidence="0.83392825">
5.1 Precision, Recall, and F-measure datasets, while DIM surpasses the others in
Classification accuracy is usually measured in ISEAR. In particular, CPLSA outperforms
terms of precision, recall, and F-measure. Table CLSA and CNMF in ISEAR because their per-
3 shows these values obtained by five approach- formances are relatively poor. The result implies
</table>
<bodyText confidence="0.948287214285714">
es for the automatic classification of four emo- that statistical models which consider a proba-
tions. The highest results for a given type of bility distribution over the latent space do not
scoring and datasets are marked in bold for each always achieve sound performances. In addition,
individual class. We do not include the accuracy we can infer that models (CNMF and DIM) with
values in our results due to the imbalanced pro- non-negative factors are appropriate for dealing
portions of categories (see Table 1). The accura- with these text collections.
cy metric does not provide adequate information, Another notable result is that the precision, re-
whereas precision, recall, and F-measure can ef- call, and F-measure are generally higher in fairy
fectively evaluate the classification performance tales than in the other datasets. These sentences
with respect to imbalanced datasets (He and Gar- in the fairy tales tend to have more emotional
cia 2009). terms and the length of sentences is longer. The
As can be seen from the table, the perfor- nature of fairy tales makes unsupervised models
mances of each approach hinge on each dataset yield better performance (see Table 2). In addi-
and emotion category, respectively. In the case tion, affective high agreement sentence is anoth-
of the SemEval dataset, precision, recall and F- er plausible contributing reason for the encourag-
measure for CNMF and DIM are comparable. ing experimental results.
DIM approach gives the best result for joy, In summary, categorical NMF model and di-
which has a relatively large number of sentences. mensional model show the better emotion identi-
In ISEAR, DIM generally outperforms other ap- fication performance as a whole.
proaches except for some cases, whereas CNMF 5.2 Cohen’s Kappa
has the best recall score after the baseline for the The kappa statistic measures the proportion of
anger category. Figure 1 indicates the results of agreement between two raters with correction for
3-dimensional and 2-dimensional attribute evalu- chance. The kappa score is used as the metric to
ations for ISEAR. When it comes to fairy tales, compare the performance of each approach. Fig-
CNMF generally performs better than the other ure 3 graphically depicts the mean kappa scores
techniques. Joy also has the largest number of and its standard errors obtained from the emotion
data instances in fairy tales and the best recall classification. Comparisons between four ap-
ignoring the baseline and F-measure are obtained proaches are shown across all three datasets.
with the approach based on DIM for this affect MCB is excluded in the comparison because the
category. CNMF gets the best emotion detection mean kappa score of MCB is 0.
performance for anger, fear, and sadness in Let MKCLSA, MKCPLSA, MKCNMF, and MKDIM be
terms of the F-measure. the mean kappa scores of four methods. The
Figure 2 and Table 4 display results among highest score (MKCNMF = 0.382) is achieved by
different approaches obtained on the three differ- the CNMF when the dataset is SemEval. In fairy
ent datasets. We compute the classification per- tales, the CNMF method (MKCNMF = 0.652) also
formance by macro-average, which gives equal displays better result than the others (MKCLSA =
weight to every category regardless of how many 0.506, MKDIM = 0.304). On the contrary, the
sentences are assigned to it.3 This measurement achieved results are significantly different in the
prevents the results from being biased given the case of the ISEAR dataset in comparison with
imbalanced data distribution. From this summa- the aforementioned datasets. The DIM (MKDIM =
rized information, we can see that CPLSA per- 0.210) clearly outperforms all methods. The kap-
forms less effectively with several low perfor- pa score of the CPLSA approach (MKCPLSA =
mance results across all datasets. CNMF is supe- 0.099) is quantitatively and significantly higher
rior to other methods in SemEval and Fairy tales than the CLSA (MKCLSA = 0.031) and CNMF
(MKCNMF = 0.011). Kappa score for the NMF-
based methods is remarkably lower than the oth-
er three approaches.
According to (Fleiss and Cohen 1973), a kap-
pa value higher than 0.4 means a fair to good
level of agreement beyond chance alone and it is
3 Macro-averaging scores are defined as:
Pm = 11
C C1Pi,Rm = C 1ri,Fm = C C1 f
where C is total number of categories, and pi, ri, and fi
stand for precision, recall, and F-measure, respective-
ly, for each category i.
</bodyText>
<page confidence="0.675879">
67
</page>
<figureCaption confidence="0.867451">
Figure 1: Distribution of the ISEAR dataset in the 3-dimensional and 2-dimensional sentment space. Th
sentences is longer in comparison with those
</figureCaption>
<bodyText confidence="0.725217">
blue ‘x’ denotes the location of one sentence corresponding to vlence, arousal, and dminance.
other datasets. The nature of fairy t
</bodyText>
<figure confidence="0.784347333333333">
fi en
(a) (b) (c)
i
</figure>
<figureCaption confidence="0.999763">
Figure 2: Comparisons of Precision, Recall, and F-measur: (a) SemEval; (b) ISEAR; (c) Firy tales.
</figureCaption>
<bodyText confidence="0.599955">
differences in the kappa scores across a
</bodyText>
<table confidence="0.999390875">
Data set SemEval et MKLS nd MKDIM b
ISEAR Fairy tales
Prec. Rec. F1 Prec. kpp es ur . T est
Rec. F1 Prec. Rec. F1
03 hi MF
MCB 0.077 0.250 0.118 0.100 swhat 0.143 is SEval by t 0.145
CLSA 0.363 0.348 0.340 w (MKN 0.228 0.102 0.250 the
CPLSA 0.189 0.282 0.219 0.484 0.250 MKNM 0.662 airy ta 0.630
CNMF 0.523 0.506 0.505 N the det 0.270 0652) 0.640 bet
DIM 0.446 0.422 0.386 0.260 0.282 he oth 0.282 displ 0.280
t meth 0.166 (MKLS 0.307 KDIM
0.461 0.317 e that 0.747 0506 0.733
= sult th 0.372 e achd 0.731 are
0.528 0.258 rent i 0.530 resu 0.419
304) se of 0.404 in
0.417 AR dat
</table>
<tableCaption confidence="0.719817">
Table 4: Overall average results
(b) ISEAR; (c) Fairy tales.
The most frequent word
</tableCaption>
<figure confidence="0.992176">
(a) (b) (c)
</figure>
<figureCaption confidence="0.999836">
Figure 3: Comparisons of Mean Kappa: (a) SemEval;
</figureCaption>
<page confidence="0.99581">
68
</page>
<bodyText confidence="0.9999916875">
an acceptable level of agreement. On the basis of
this definition, the kappa score obtained by our
best classifier (MKCNMF = 0.652) would be rea-
sonable. Most of the values are too low to say
that two raters (human judges and computer ap-
proaches) agreed upon the affective states. How-
ever, we have another reason with respect to this
metric in the experiment. We make use of the
kappa score as an unbiased metric of the relia-
bility for comparing four methods. In other
words, these measures are of importance in terms
of the relative magnitude. Hence, the kappa re-
sults are meaningful and interpretable in spite of
low values. We can observe that the NMF-based
categorical model and the dimensional model
both experienced higher performance.
</bodyText>
<subsectionHeader confidence="0.992756">
5.3 Frequently occurring words
</subsectionHeader>
<bodyText confidence="0.999956127659575">
The most frequent words used in fairy tales for
each emotion are listed in Table 5. We choose
this dataset since there are varying lexical items
and affective high agreement sentences, as men-
tioned in Section 5.1. Stemming is not used be-
cause it might hide important differences as be-
tween ‘loving’ and ‘loved’. CNMF and DIM
were selected for the comparison with the Gold
Standard because they were the two methods
with the better performance than the others. Gold
Standard is the annotated dataset by human raters
for the evaluation of algorithm performance. The
words most frequently used to describe anger
across all methods include: cried, great, tears,
king, thought, and eyes. Those used to describe
fear include: heart, cried, mother, thought, man,
and good. Joy contains happy, good, and cried
whereas sadness has only cried for three methods.
There is something unexpected for the word
frequencies. We can observe that the association
between frequently used words and emotion cat-
egories is unusual and even opposite. For in-
stance, a ‘joy’ is one of the most frequent words
referred to for sadness in the Gold Standard. In
CNMF and DIM, a ‘good’ is employed frequent-
ly with regard to fear. Moreover, some words
occur with the same frequency in more catego-
ries. For example, the word ‘cried’ is utilized to
express anger, fear, and joy in the Gold Standard,
CNMF, and DIM. In order to find a possible ex-
planation in the complexity of language used in
the emotional expression, some sentences ex-
tracted from fairy tales are listed below:
“The cook was frightened when he heard the or-
der, and said to Cat-skin, You must have let a
hair fall into the soup; if it be so, you will have a
good beating.” – which expresses fear
“When therefore she came to the castle gate she
saw him, and cried aloud for joy.” – which is the
expression for joy
“Gretel was not idle; she ran screaming to her
master, and cried: You have invited a fine guest!”
– which is the expression for angry-disgusted
From these examples, we can observe that in
these cases the affective meaning is not simply
propagated form the lexicon, but is the effect of
the linguistic structure at a higher level.
</bodyText>
<sectionHeader confidence="0.999541" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.747822277777778">
We compared the performances of three tech-
niques, based on the categorical representation of
emotions, and one based on the dimensional rep-
resentation. This paper has highlighted that the
NMF-based categorical classification performs
Model Emotion Top 10 words
Gold Standard Anger king, thought, eyes, great, cried, looked, joy, mother, wife, tears
Fear great, cried, good, happy, thought, man, heart, poor, child, mother
Joy thought, mother, good, cried, man, day, wept, beautiful, back, happy
Sadness cried, fell, father, mother, back, joy, dead, danced, wife, tears
CNMF Anger great, cried, eyes, mother, poor, joy, king, heart, thought, tears
Fear cried, king, happy, good, man, heart, thought, father, boy, mother
Joy mother, thought, cried, king, day, great, home, joy, good, child
Sadness thought, cried, good, great, looked, mother, man, time, king, heart
DIM Anger eyes, fell, heart, tears, cried, good, stood, great, king, thought
Fear king, cried, heart, mother, good, thought, looked, man, child, time
Joy eyes, man, children, danced, cried, good, time, happy, great, wedding
Sadness cried, thought, great, king, good, happy, sat, home, joy, found
</bodyText>
<tableCaption confidence="0.982938">
Table 5: Most frequent 10 words from fairy tales
</tableCaption>
<page confidence="0.998874">
69
</page>
<bodyText confidence="0.999946714285714">
the best among categorical approaches to classi-
fication. When comparing categorical against
dimensional classification, the categorical NMF
model and the dimensional model have better
performances. Nevertheless, we cannot general-
ize inferences on which of these techniques is the
best performer because results vary among data-
sets. As a future work, we aim at performing a
further investigation on this connection in order
to identify more effective strategies applicable to
a generic dataset. Furthermore, we aim at explor-
ing improvements in the methodology, employed
in this work, and based on the combination of
emotional modeling and empirical methods.
</bodyText>
<sectionHeader confidence="0.999575" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.81203">
This research is partially sponsored by a Norman
</bodyText>
<listItem confidence="0.6038435">
I. Price Scholarship from the University of Syd-
ney.
</listItem>
<sectionHeader confidence="0.991625" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9998783875">
C. O. Alm (2009). Affect in Text and Speech, VDM
Verlag Dr. Müller.
S. Aman and S. Szpakowicz (2007). Identifying ex-
pressions of emotion in text. Text, Speech and Di-
alogue.
M. M. Bradley and P. J. Lang (1999). Affective
norms for English words (ANEW): Instruction
manual and affective ratings. University of Flori-
da: The Center for Research in Psychophysiology.
J. Cohen (1960). A coefficient of agreement for no-
minal scales. Educational and psychological mea-
surement 20(1): 37-46.
S. D&apos;Mello, A. Graesser, and R. W. Picard (2007).
Toward an affect-sensitive AutoTutor. IEEE Intel-
ligent Systems 22(4): 53-61.
S. D’Mello, S. Craig, A. Witherspoon, B. Mcdaniel,
and A. Graesser (2008). Automatic detection of
learner’s affect from conversational cues. User
Modeling and User-Adapted Interaction 18(1): 45-
80.
P. Ekman (1992). An argument for basic emotions.
Cognition &amp; Emotion 6(3): 169-200.
J. L. Fleiss and J. Cohen (1973). The equivalence of
weighted kappa and the intraclass correlation.
Educational and psychological measurement 33:
613-619.
H. He and E. A. Garcia (2009). Learning from Imba-
lanced Data. IEEE Transactions on Knowledge
and Data Engineering 21(9): 1263.
T. Hofmann (2001). Unsupervised learning by proba-
bilistic latent semantic analysis. Machine Learning
42(1): 177-196.
B. Kort, R. Reilly, and R. W. Picard (2001). An affec-
tive model of interplay between emotions and
learning: Reengineering educational pedagogy-
building a learning companion. IEEE International
Conference on Advanced Learning Technologies,
2001. Proceedings.
T. K. Landauer, P. W. Foltz, and D. Laham (1998).
An introduction to latent semantic analysis. Dis-
course processes, Citeseer. 25: 259-284.
P. J. Lang (1980). Behavioral treatment and bio-
behavioral assessment: Computer applications.
Technology in mental health care delivery sys-
tems: 119-137.
D. D. Lee and H. S. Seung (1999). Learning the parts
of objects by non-negative matrix factorization.
Nature 401(6755): 788-791.
P. Penumatsa, M. Ventura, A.C. Graesser, M. Lou-
werse, X. Hu, Z. Cai, and D.R. Franceschetti
(2006). The Right Threshold Value: What Is the
Right Threshold of Cosine Measure When Using
Latent Semantic Analysis for Evaluating Student
Answers? International Journal on Artificial Intel-
ligence Tools, World Scientific Publishing.
J. A. Russell (2003). Core affect and the psychologi-
cal construction of emotion. Psychological review
110(1): 145-172.
K. R. Scherer and H. G. Wallbott (1994). Evidence
for universality and cultural variation of differen-
tial emotion response patterning. Journal of Perso-
nality and Social Psychology 66: 310-328.
C. Strapparava and R. Mihalcea (2007). Semeval-
2007 task 14: Affective text. Proceedings of the
4th International Workshop on Semantic Evalua-
tions, Association for Computational Linguistics.
C. Strapparava and R. Mihalcea (2008). Learning to
identify emotions in text. SAC &apos;08: Proceedings of
the 2008 ACM symposium on Applied computing,
Fortaleza, Ceara, Brazil, ACM.
C. Strapparava and A. Valitutti (2004). WordNet-
Affect: an affective extension of WordNet. Pro-
ceedings of LREC.
R. B. Yates and B. R. Neto (1999). Modern informa-
tion retrieval. ACM P.
Zeimpekis D. and E. Gallopoulos (2005). TMG: A
MATLAB toolbox for generating term-document
matrices from text collections. Grouping multidi-
mensional data: Recent advances in clustering:
187-210.
</reference>
<page confidence="0.998457">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.630306">
<title confidence="0.9981555">Evaluation of Unsupervised Emotion to Textual Affect Recognition</title>
<author confidence="0.998445">Sunghwan Mac</author>
<affiliation confidence="0.999055">School of and Information University of</affiliation>
<address confidence="0.863649">Sydney, Australia</address>
<email confidence="0.992991">skim1871@uni.sydney.edu.au</email>
<author confidence="0.980857">Alessandro</author>
<affiliation confidence="0.998456333333333">Department of Cognitive and University of</affiliation>
<address confidence="0.962321">Trento, Italy</address>
<email confidence="0.997151">a.valitutti@email.unitn.it</email>
<author confidence="0.980847">A Rafael</author>
<affiliation confidence="0.998953333333333">School of and Information University of</affiliation>
<address confidence="0.954987">Sydney, Australia</address>
<email confidence="0.99881">rafa@ee.usyd.edu.au</email>
<abstract confidence="0.990868565217391">In this paper we present an evaluation of new techniques for automatically detecting emotions in text. The study estimates categorical model and dimensional model for the recogniof four affective states: are common emotions in three datasets: SemEval-2007 “Affective Text”, ISEAR (International Survey on Emotion Antecedents and Reactions), and children’s fairy tales. In the first model, WordNet- Affect is used as a linguistic lexical resource and three dimensionality reduction techniques are evaluated: Latent Semantic Analysis (LSA), Probabilistic Latent Semantic Analysis (PLSA), and Non-negative Matrix Factorization (NMF). In the second model, ANEW (Affective Norm for English Words), a normative database with affective terms, is employed. Experiments show that a categorical model using NMF results in better performances for SemEval and fairy tales, whereas a dimensional model performs better with ISEAR.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C O</author>
</authors>
<title>Alm</title>
<date>2009</date>
<booktitle>Affect in Text and Speech, VDM Verlag Dr.</booktitle>
<publisher>Müller.</publisher>
<marker>O, 2009</marker>
<rawString>C. O. Alm (2009). Affect in Text and Speech, VDM Verlag Dr. Müller.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Aman</author>
<author>S Szpakowicz</author>
</authors>
<title>Identifying expressions of emotion in text. Text, Speech and Dialogue.</title>
<date>2007</date>
<contexts>
<context position="3126" citStr="Aman and Szpakowicz 2007" startWordPosition="444" endWordPosition="447">utor. (D&apos;Mello, Graesser et al. 2007) proposed five categories for describing the affect states in student-system dialogue. Significant differences arise not only between these two types of techniques but also between different emotion models, and these differences have significant implications in all these areas. While considering emotions and learning, (Kort, Reilly et al. 2001) proposed (but provided no empirical evidence) a model that combines two emotion models, placing categories in a valencearousal plane. This mixed approach has also been used in other domains such as blog posts where (Aman and Szpakowicz 2007) studied how to identify emotion categories as well as emotion intensity. To date, many researchers have, however, utilized and evaluated supervised methods, mainly based on the categorical emotion model. In this study, the goal is to evaluate the merits of two conceptualizations of emotions (a categorical model and a dimensional model) in which an unsupervised approach is used. The evaluation incorporates three dimensionality re62 Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 62–70, Los Angeles, California, June 201</context>
</contexts>
<marker>Aman, Szpakowicz, 2007</marker>
<rawString>S. Aman and S. Szpakowicz (2007). Identifying expressions of emotion in text. Text, Speech and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M M Bradley</author>
<author>P J Lang</author>
</authors>
<title>Affective norms for English words (ANEW): Instruction manual and affective ratings.</title>
<date>1999</date>
<institution>University of Florida: The Center for</institution>
<note>Research in Psychophysiology.</note>
<contexts>
<context position="12497" citStr="Bradley and Lang 1999" startWordPosition="1951" endWordPosition="1955">us norm of the error. ��� IIA = WHII� 2 , s. t. W, H &gt;_ 0 (4) W,H where W, H &gt; 0 means that all elements of Wand H are non-negative. This non-negative peculiarity is desirable for handling text data that always require non-negativity constraints. The classification of documents is performed based on the columns of matrix H that represent the documents. 3.3 Three-dimensional estimation with features derived from ANEW Dimensional models have been studied by psychologists often by providing a stimulus (e.g. a photo or a text), and then asking subjects to report on the affective experience. ANEW (Bradley and Lang 1999) is a set of normative emotional ratings for a collection of English words (N=1,035), where after reading the words, subjects reported their emotions in a three dimensional representation. This collection provides the rated values for valence, arousal, and dominance for each word rated using the Self Assessment Manikin (SAM). For each word w, the normative database provides coordinates w� in an affective space as: 64 w� = (valence, arousal, dominance) (5) = AN W(w) The occurrences of these words in a text can be used, in a naïve way, to weight the sentence in this emotional plane. This is a na</context>
</contexts>
<marker>Bradley, Lang, 1999</marker>
<rawString>M. M. Bradley and P. J. Lang (1999). Affective norms for English words (ANEW): Instruction manual and affective ratings. University of Florida: The Center for Research in Psychophysiology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales. Educational and psychological measurement</title>
<date>1960</date>
<volume>20</volume>
<issue>1</issue>
<pages>37--46</pages>
<contexts>
<context position="20193" citStr="Cohen 1960" startWordPosition="3217" endWordPosition="3218">f a classifier that always predicts the majority class. In SemEval and Fairy tales the majority class is joy, while anger is the majority emotion in case of ISEAR. The five approaches were evaluated on the dataset of 479 news headlines (SemEval), 5,430 responses to questions (ISEAR), and 1,093 fairy tales’ sentences. We define the following acronyms to identify the approaches: CLSA: LSA-based categorical classification CPLSA: PLSA-based categorical classification CNMF: NMF-based categorical classification DIM: Dimension-based estimation The measure of accuracies used here were: Cohen’s Kappa (Cohen 1960), average precision, recall, and F-measure. While the kappa scores are useful in obtaining an overview of the reliability of the various classification approaches, they do not provide any insight on the accuracy at the category level for which precision, recall, and F-measure are necessary. ISEAR Fairy tales 66 5.1 Precision, Recall, and F-measure datasets, while DIM surpasses the others in Classification accuracy is usually measured in ISEAR. In particular, CPLSA outperforms terms of precision, recall, and F-measure. Table CLSA and CNMF in ISEAR because their per3 shows these values obtained </context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen (1960). A coefficient of agreement for nominal scales. Educational and psychological measurement 20(1): 37-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S D&apos;Mello</author>
<author>A Graesser</author>
<author>R W Picard</author>
</authors>
<title>Toward an affect-sensitive AutoTutor.</title>
<date>2007</date>
<journal>IEEE Intelligent Systems</journal>
<volume>22</volume>
<issue>4</issue>
<pages>53--61</pages>
<marker>D&apos;Mello, Graesser, Picard, 2007</marker>
<rawString>S. D&apos;Mello, A. Graesser, and R. W. Picard (2007). Toward an affect-sensitive AutoTutor. IEEE Intelligent Systems 22(4): 53-61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S D’Mello</author>
<author>S Craig</author>
<author>A Witherspoon</author>
<author>B Mcdaniel</author>
<author>A Graesser</author>
</authors>
<title>Automatic detection of learner’s affect from conversational cues. User Modeling and User-Adapted Interaction</title>
<date>2008</date>
<volume>18</volume>
<issue>1</issue>
<pages>45--80</pages>
<marker>D’Mello, Craig, Witherspoon, Mcdaniel, Graesser, 2008</marker>
<rawString>S. D’Mello, S. Craig, A. Witherspoon, B. Mcdaniel, and A. Graesser (2008). Automatic detection of learner’s affect from conversational cues. User Modeling and User-Adapted Interaction 18(1): 45-80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Ekman</author>
</authors>
<title>An argument for basic emotions.</title>
<date>1992</date>
<journal>Cognition &amp; Emotion</journal>
<volume>6</volume>
<issue>3</issue>
<pages>169--200</pages>
<contexts>
<context position="4757" citStr="Ekman 1992" startWordPosition="697" endWordPosition="698">nd lexical resources in the affect classification. In addition, we give an overview of the dimension reduction methods used in the study. In Section 4 we go over the affective datasets used. Section 5 provides the results of the evaluation, before coming to our discussion in Section 6. 2 Emotion Models There are two significantly different models for representing emotions: the categorical model and dimensional model (Russell 2003). The categorical model assumes that there are discrete emotional categories such as Ekman’s six basic emotions - anger, disgust, fear, joy, sadness, and surprise - (Ekman 1992). There are a number of primary and unrelated emotions in the model. Each emotion is characterized by a specific set of features, expressing eliciting conditions or responses. Some researchers have argued that a different set of emotions is required for different domains. For instance, the following emotion classes are used in the field of teaching and education: boredom, delight, flow, confusion, frustration, and surprise. The advantage of such a representation is that it represents human emotions intuitively with easy to understand emotion labels. A second approach is the dimensional model, </context>
</contexts>
<marker>Ekman, 1992</marker>
<rawString>P. Ekman (1992). An argument for basic emotions. Cognition &amp; Emotion 6(3): 169-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Fleiss</author>
<author>J Cohen</author>
</authors>
<title>The equivalence of weighted kappa and the intraclass correlation.</title>
<date>1973</date>
<booktitle>Educational and psychological measurement 33:</booktitle>
<pages>613--619</pages>
<contexts>
<context position="25000" citStr="Fleiss and Cohen 1973" startWordPosition="3987" endWordPosition="3990">mparison with imbalanced data distribution. From this summa- the aforementioned datasets. The DIM (MKDIM = rized information, we can see that CPLSA per- 0.210) clearly outperforms all methods. The kapforms less effectively with several low perfor- pa score of the CPLSA approach (MKCPLSA = mance results across all datasets. CNMF is supe- 0.099) is quantitatively and significantly higher rior to other methods in SemEval and Fairy tales than the CLSA (MKCLSA = 0.031) and CNMF (MKCNMF = 0.011). Kappa score for the NMFbased methods is remarkably lower than the other three approaches. According to (Fleiss and Cohen 1973), a kappa value higher than 0.4 means a fair to good level of agreement beyond chance alone and it is 3 Macro-averaging scores are defined as: Pm = 11 C C1Pi,Rm = C 1ri,Fm = C C1 f where C is total number of categories, and pi, ri, and fi stand for precision, recall, and F-measure, respectively, for each category i. 67 Figure 1: Distribution of the ISEAR dataset in the 3-dimensional and 2-dimensional sentment space. Th sentences is longer in comparison with those blue ‘x’ denotes the location of one sentence corresponding to vlence, arousal, and dminance. other datasets. The nature of fairy t </context>
</contexts>
<marker>Fleiss, Cohen, 1973</marker>
<rawString>J. L. Fleiss and J. Cohen (1973). The equivalence of weighted kappa and the intraclass correlation. Educational and psychological measurement 33: 613-619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H He</author>
<author>E A Garcia</author>
</authors>
<title>Learning from Imbalanced Data.</title>
<date>2009</date>
<journal>IEEE Transactions on Knowledge and Data Engineering</journal>
<volume>21</volume>
<issue>9</issue>
<pages>1263</pages>
<marker>He, Garcia, 2009</marker>
<rawString>H. He and E. A. Garcia (2009). Learning from Imbalanced Data. IEEE Transactions on Knowledge and Data Engineering 21(9): 1263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Hofmann</author>
</authors>
<title>Unsupervised learning by probabilistic latent semantic analysis.</title>
<date>2001</date>
<journal>Machine Learning</journal>
<volume>42</volume>
<issue>1</issue>
<pages>177--196</pages>
<contexts>
<context position="10218" citStr="Hofmann 2001" startWordPosition="1570" endWordPosition="1571"> 1998). The main idea of LSA is to map terms or documents into a vector space of reduced dimensionality that is the latent semantic space. The mapping of the given terms/document vectors to this space is based on singular vector decomposition (SVD). It is known that SVD is a reliable technique for matrix decomposition. It can decompose a matrix as the product of three matrices. A = UEVT ^ UkEkVkT = Ak (1) where Ak is the closest matrix of rank k to the original matrix. The columns of Vk represent the coordinates for documents in the latent space. Probabilistic Latent Semantic Anlaysis (PLSA) (Hofmann 2001) has two characteristics distinguishing it from LSA. PLSA defines proper probability distributions and the reduced matrix does not contain negative values. Based on the combination of LSA and some probabilistic theories such as Bayes rules, the PLSA allows us to find the latent topics, the association of documents and topics, and the association of terms and topics. In the equation (2), z is a latent class variable (i.e. discrete emotion category), while w and d denote the elements of term vectors and document vectors, respectively. P(d,w) = I P(z)P(w|z)P(d|z) (2) z where P(w|z) and P(d|z) are</context>
</contexts>
<marker>Hofmann, 2001</marker>
<rawString>T. Hofmann (2001). Unsupervised learning by probabilistic latent semantic analysis. Machine Learning 42(1): 177-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kort</author>
<author>R Reilly</author>
<author>R W Picard</author>
</authors>
<title>An affective model of interplay between emotions and learning: Reengineering educational pedagogybuilding a learning companion.</title>
<date>2001</date>
<booktitle>IEEE International Conference on Advanced Learning Technologies,</booktitle>
<publisher>Proceedings.</publisher>
<marker>Kort, Reilly, Picard, 2001</marker>
<rawString>B. Kort, R. Reilly, and R. W. Picard (2001). An affective model of interplay between emotions and learning: Reengineering educational pedagogybuilding a learning companion. IEEE International Conference on Advanced Learning Technologies, 2001. Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>An introduction to latent semantic analysis. Discourse processes,</title>
<date>1998</date>
<journal>Citeseer.</journal>
<volume>25</volume>
<pages>259--284</pages>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>T. K. Landauer, P. W. Foltz, and D. Laham (1998). An introduction to latent semantic analysis. Discourse processes, Citeseer. 25: 259-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Lang</author>
</authors>
<title>Behavioral treatment and biobehavioral assessment: Computer applications. Technology in mental health care delivery systems:</title>
<date>1980</date>
<pages>119--137</pages>
<contexts>
<context position="6281" citStr="Lang 1980" startWordPosition="941" endWordPosition="942">d negative emotions on different ends of the scale. The arousal dimension differentiates excited vs. calm states. Sometimes a third, dominance dimension is used to differentiate if the subject feels in control of the situation or not. The categorical model and the dimensional model have two different methods for estimating the actual emotional states of a person. In the former, a person is usually required to choose one emotion out of an emotion set that represents the best feeling. On the other hand, the latter exploits rating scales for each dimension like the Self Assessment Manikin (SAM) (Lang 1980), which consists of pictures of manikins, to estimate the degree of valence, arousal, and dominance. 3 Automatic Affect Classification 3.1 Categorical classification with features derived from WordNet-Affect WordNet-Affect (Strapparava and Valitutti 2004) is an affective lexical repository of words referring to emotional states. WordNet-Affect extends WordNet by assigning a variety of affect labels to a subset of synsets representing affective concepts in WordNet (emotional synsets). In addition, WordNet-Affect has an additional hierarchy of affective domain labels. There are publicly availabl</context>
</contexts>
<marker>Lang, 1980</marker>
<rawString>P. J. Lang (1980). Behavioral treatment and biobehavioral assessment: Computer applications. Technology in mental health care delivery systems: 119-137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lee</author>
<author>H S Seung</author>
</authors>
<title>Learning the parts of objects by non-negative matrix factorization.</title>
<date>1999</date>
<journal>Nature</journal>
<volume>401</volume>
<issue>6755</issue>
<pages>788--791</pages>
<contexts>
<context position="11386" citStr="Lee and Seung 1999" startWordPosition="1750" endWordPosition="1753"> P(z)P(w|z)P(d|z) (2) z where P(w|z) and P(d|z) are topic-specific word distribution and document distribution, individually. The decomposition of PLSA, unlike that of LSA, is performed by means of the likelihood function. In other words, P(z), P(w|z), and P(d|z) are determined by the maximum likelihood estimation (MLE) and this maximization is performed through adopting the Expectation Maximization (EM) algorithm. For document similarities, each row of the P(d|z) matrix is considered with the low-dimensional representation in the semantic topic space. Non-negative Matrix Factorization (NMF) (Lee and Seung 1999) has been successfully applied to semantic analysis. Given a non-negative matrix A, NMF finds non-negative factors W and H that are reduced-dimensional matrices. The product WH can be regarded as a compressed form of the data in A. A � WH = I WH (3) W is a basis vector matrix and H is an encoded matrix of the basis vectors in the equation (3). NMF solves the following minimization problem (4) in order to obtain an approximation A by computing W and H in terms of minimizing the Frobenius norm of the error. ��� IIA = WHII� 2 , s. t. W, H &gt;_ 0 (4) W,H where W, H &gt; 0 means that all elements of Wan</context>
</contexts>
<marker>Lee, Seung, 1999</marker>
<rawString>D. D. Lee and H. S. Seung (1999). Learning the parts of objects by non-negative matrix factorization. Nature 401(6755): 788-791.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Penumatsa</author>
<author>M Ventura</author>
<author>A C Graesser</author>
<author>M Louwerse</author>
<author>X Hu</author>
<author>Z Cai</author>
<author>D R Franceschetti</author>
</authors>
<title>The Right Threshold Value: What Is the Right Threshold of Cosine Measure When Using Latent Semantic Analysis for Evaluating Student Answers?</title>
<date>2006</date>
<journal>International Journal on Artificial Intelligence Tools, World Scientific Publishing.</journal>
<marker>Penumatsa, Ventura, Graesser, Louwerse, Hu, Cai, Franceschetti, 2006</marker>
<rawString>P. Penumatsa, M. Ventura, A.C. Graesser, M. Louwerse, X. Hu, Z. Cai, and D.R. Franceschetti (2006). The Right Threshold Value: What Is the Right Threshold of Cosine Measure When Using Latent Semantic Analysis for Evaluating Student Answers? International Journal on Artificial Intelligence Tools, World Scientific Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Russell</author>
</authors>
<title>Core affect and the psychological construction of emotion.</title>
<date>2003</date>
<journal>Psychological review</journal>
<volume>110</volume>
<issue>1</issue>
<pages>145--172</pages>
<contexts>
<context position="4580" citStr="Russell 2003" startWordPosition="670" endWordPosition="671">e affective states of a text. Section 3 describes the techniques of affect classification utilizing lexical resources. More specifically, it describes the role of emotion models and lexical resources in the affect classification. In addition, we give an overview of the dimension reduction methods used in the study. In Section 4 we go over the affective datasets used. Section 5 provides the results of the evaluation, before coming to our discussion in Section 6. 2 Emotion Models There are two significantly different models for representing emotions: the categorical model and dimensional model (Russell 2003). The categorical model assumes that there are discrete emotional categories such as Ekman’s six basic emotions - anger, disgust, fear, joy, sadness, and surprise - (Ekman 1992). There are a number of primary and unrelated emotions in the model. Each emotion is characterized by a specific set of features, expressing eliciting conditions or responses. Some researchers have argued that a different set of emotions is required for different domains. For instance, the following emotion classes are used in the field of teaching and education: boredom, delight, flow, confusion, frustration, and surpr</context>
</contexts>
<marker>Russell, 2003</marker>
<rawString>J. A. Russell (2003). Core affect and the psychological construction of emotion. Psychological review 110(1): 145-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R Scherer</author>
<author>H G Wallbott</author>
</authors>
<title>Evidence for universality and cultural variation of differential emotion response patterning.</title>
<date>1994</date>
<journal>Journal of Personality and Social Psychology</journal>
<volume>66</volume>
<pages>310--328</pages>
<contexts>
<context position="15847" citStr="Scherer and Wallbott 1994" startWordPosition="2520" endWordPosition="2523">es are suitable for our experiments because headlines are typically intended to express emotions in order to draw the readers’ attention. This dataset has six emotion classes: anger, disgust, fear, joy, sadness and surprise, and is composed of 1,250 annotated headlines. The notable characteristics are that SemEval dataset does not only allow one sentence to be tagged with multiple emotions, but the dataset also contains a neutral category in contrast to other datasets. We also use the ISEAR (International Survey on Emotion Antecedents and Reactions) dataset, which consists of 7,666 sentences (Scherer and Wallbott 1994), with regard to our experiments. 2 For building the ISEAR, 1,096 participants who have different cultural backgrounds completed questionnaires about experiences and reactions for seven emotions including anger, disgust, fear, joy, sadness, shame and guilt. The annotated sentences of the third dataset are culled from fairy tales (Alm 2009). Emotions are particularly significant elements in the literary genre of fairy tales. The label set with five emotion classes is as follows: angry-disgusted, fearful, happy, sad and surprised. There are 176 stories by three authors: B. Potter, H.C. Andersen,</context>
</contexts>
<marker>Scherer, Wallbott, 1994</marker>
<rawString>K. R. Scherer and H. G. Wallbott (1994). Evidence for universality and cultural variation of differential emotion response patterning. Journal of Personality and Social Psychology 66: 310-328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>R Mihalcea</author>
</authors>
<title>Semeval2007 task 14: Affective text.</title>
<date>2007</date>
<booktitle>Proceedings of the 4th International Workshop on Semantic Evaluations, Association for Computational Linguistics.</booktitle>
<contexts>
<context position="15124" citStr="Strapparava and Mihalcea 2007" startWordPosition="2408" endWordPosition="2411">roximate to that of an emotion, the sentence is tagged as the emotion (with the nearest neighbor algorithm). The centroid sentence ������������might be close to an ����������� emotion on the VAD space, even if they do not share any terms in common. We define the distance threshold (empirically set to 4) to validate the appropriate proximity like the categorical classification. 4 Emotion-Labeled Data Three emotional datasets, with sentence-level emotion annotations, were employed for the evaluation described in the next section. The first dataset is “Affective Text” from the SemEval 2007 task (Strapparava and Mihalcea 2007). 1 This dataset consists of news headlines excerpted from newspapers and news web sites. Headlines are suitable for our experiments because headlines are typically intended to express emotions in order to draw the readers’ attention. This dataset has six emotion classes: anger, disgust, fear, joy, sadness and surprise, and is composed of 1,250 annotated headlines. The notable characteristics are that SemEval dataset does not only allow one sentence to be tagged with multiple emotions, but the dataset also contains a neutral category in contrast to other datasets. We also use the ISEAR (Intern</context>
</contexts>
<marker>Strapparava, Mihalcea, 2007</marker>
<rawString>C. Strapparava and R. Mihalcea (2007). Semeval2007 task 14: Affective text. Proceedings of the 4th International Workshop on Semantic Evaluations, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>R Mihalcea</author>
</authors>
<title>Learning to identify emotions in text.</title>
<date>2008</date>
<booktitle>SAC &apos;08: Proceedings of the 2008 ACM symposium on Applied computing,</booktitle>
<publisher>ACM.</publisher>
<location>Fortaleza, Ceara, Brazil,</location>
<contexts>
<context position="2109" citStr="Strapparava and Mihalcea 2008" startWordPosition="292" endWordPosition="296"> been used to automatically recognize expressions of emotion in text such as happiness, sadness, anger, etc... Supervised learning techniques have the disadvantage that large annotated datasets are required for training. Since the emotional interpretations of a text can be highly subjective, more than one annotator is needed, and this makes the process of the annotation very time consuming and expensive. For this reason, unsupervised methods are normally preferred in the realm of Natural Language Processing (NLP) and emotions. Supervised and unsupervised techniques have been compared before. (Strapparava and Mihalcea 2008) describe the comparison between a supervised (Naïve Bayes) and an unsupervised (Latent Semantic Analysis - LSA) method for recognizing six basic emotions. These techniques have been applied to many areas, particularly in improving Intelligent Tutoring Systems. For example, (D’Mello, Craig et al. 2008) used LSA but for detecting utterance types and affect in students’ dialogue within Autotutor. (D&apos;Mello, Graesser et al. 2007) proposed five categories for describing the affect states in student-system dialogue. Significant differences arise not only between these two types of techniques but als</context>
</contexts>
<marker>Strapparava, Mihalcea, 2008</marker>
<rawString>C. Strapparava and R. Mihalcea (2008). Learning to identify emotions in text. SAC &apos;08: Proceedings of the 2008 ACM symposium on Applied computing, Fortaleza, Ceara, Brazil, ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Strapparava</author>
<author>A Valitutti</author>
</authors>
<title>WordNetAffect: an affective extension of WordNet.</title>
<date>2004</date>
<booktitle>Proceedings of LREC.</booktitle>
<contexts>
<context position="6536" citStr="Strapparava and Valitutti 2004" startWordPosition="973" endWordPosition="976"> The categorical model and the dimensional model have two different methods for estimating the actual emotional states of a person. In the former, a person is usually required to choose one emotion out of an emotion set that represents the best feeling. On the other hand, the latter exploits rating scales for each dimension like the Self Assessment Manikin (SAM) (Lang 1980), which consists of pictures of manikins, to estimate the degree of valence, arousal, and dominance. 3 Automatic Affect Classification 3.1 Categorical classification with features derived from WordNet-Affect WordNet-Affect (Strapparava and Valitutti 2004) is an affective lexical repository of words referring to emotional states. WordNet-Affect extends WordNet by assigning a variety of affect labels to a subset of synsets representing affective concepts in WordNet (emotional synsets). In addition, WordNet-Affect has an additional hierarchy of affective domain labels. There are publicly available lists relevant to the six basic emotion categories extracted from WordNet-Affect and we used four of the six lists of emotional words among them for our experiment. In addition to WordNet-Affect, we exploited a Vector Space Model (VSM) in which terms an</context>
</contexts>
<marker>Strapparava, Valitutti, 2004</marker>
<rawString>C. Strapparava and A. Valitutti (2004). WordNetAffect: an affective extension of WordNet. Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R B Yates</author>
<author>B R Neto</author>
</authors>
<title>Modern information retrieval.</title>
<date>1999</date>
<journal>ACM P.</journal>
<contexts>
<context position="7454" citStr="Yates and Neto 1999" startWordPosition="1112" endWordPosition="1115">tive domain labels. There are publicly available lists relevant to the six basic emotion categories extracted from WordNet-Affect and we used four of the six lists of emotional words among them for our experiment. In addition to WordNet-Affect, we exploited a Vector Space Model (VSM) in which terms and textual documents can be represented through a term-by-document matrix. More specifically, terms are encoded as vectors, whose components are co-occurrence frequencies of words in corpora documents. Frequencies are weighted according to the log-entropy with respect to a tf-idf weighting schema (Yates and Neto 1999). Finally, the number of dimensions is reduced through the dimension reduction methods. The vector-based representation enables words, sentences, and sets of synonyms (i.e. WordNet synsets) to be represented in a unifying way with vectors. VSM provides a variety of definitions of distance between vectors, corresponding to different measures of semantic similarity. In particular, we take advantage of cosine angle between an input vector (input sentence) and an emotional vector (i.e. the vector representing an emotional synset) as similarity measures to identify which emotion the sentence connot</context>
</contexts>
<marker>Yates, Neto, 1999</marker>
<rawString>R. B. Yates and B. R. Neto (1999). Modern information retrieval. ACM P.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zeimpekis</author>
<author>E Gallopoulos</author>
</authors>
<title>TMG: A MATLAB toolbox for generating term-document matrices from text collections. Grouping multidimensional data: Recent advances in clustering:</title>
<date>2005</date>
<pages>187--210</pages>
<contexts>
<context position="19399" citStr="Zeimpekis and Gallopoulos 2005" startWordPosition="3095" endWordPosition="3098">s on the previous evening, fold up its petals and sleep; it dropped sorrowfully. Table 2: Sample sentences labeled with sadness/sad from the datasets 5 Experiments and Results The goal of the affect classification is to predict a single emotional label given an input sentence. Four different approaches were implemented in Matlab. A categorical model based on a VSM with dimensionality reduction variants, (LSA, PLSA, and NMF), and a dimensional model, each with evaluated with two similarity measures (cosine angle and nearest neighbor). Stopwords were removed in all approaches. A Matlab toolkit (Zeimpekis and Gallopoulos 2005), was used to generate the term-by-sentence matrix from the text. The evaluation in Table 3 shows Majority Class Baseline (MCB) as the baseline algorithm. The MCB is the performance of a classifier that always predicts the majority class. In SemEval and Fairy tales the majority class is joy, while anger is the majority emotion in case of ISEAR. The five approaches were evaluated on the dataset of 479 news headlines (SemEval), 5,430 responses to questions (ISEAR), and 1,093 fairy tales’ sentences. We define the following acronyms to identify the approaches: CLSA: LSA-based categorical classific</context>
</contexts>
<marker>Zeimpekis, Gallopoulos, 2005</marker>
<rawString>Zeimpekis D. and E. Gallopoulos (2005). TMG: A MATLAB toolbox for generating term-document matrices from text collections. Grouping multidimensional data: Recent advances in clustering: 187-210.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>