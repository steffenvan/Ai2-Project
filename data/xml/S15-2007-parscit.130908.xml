<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.125009">
<title confidence="0.980895">
ROB: Using Semantic Meaning to Recognize Paraphrases
</title>
<author confidence="0.987856">
Rob van der Goot Gertjan van Noord
</author>
<affiliation confidence="0.999272">
University of Groningen University of Groningen
</affiliation>
<email confidence="0.997325">
r.van.der.goot@rug.nl g.j.m.van.noord@rug.nl
</email>
<sectionHeader confidence="0.9956" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999896772727273">
Paraphrase recognition is the task of iden-
tifying whether two pieces of natural lan-
guage represent similar meanings. This pa-
per describes a system participating in the
shared task 1 of SemEval 2015, which is about
paraphrase detection and semantic similarity
in twitter. Our approach is to exploit se-
mantically meaningful features to detect para-
phrases. An existing state-of-the-art model
for predicting semantic similarity is adapted
to this task.
A wide variety of features is used, ranging
from different types of models, to lexical over-
lap and synset overlap. A maximum entropy
classifier is then trained on these features. In
addition to the detection of paraphrases, a sim-
ilarity score is also predicted, using the proba-
bilities of the classifier. To improve the results,
normalization is used as preprocessing step.
Our final system achieves a F1 score of 0.620
(10th out of 18 teams), and a Pearson correla-
tion of 0.515 (6th out of 13 teams).
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999988071428572">
A good paraphrase detection system can be useful in
many natural language processing tasks, like search-
ing, translating or summarization. For clean texts,
F1 scores as high as 0.84 have been reported on para-
phrase detection (Madnani et al., 2012).
However, previous research focused almost solely
on clean text. Thanks to the Twitter Paraphrase Cor-
pus (Xu et al., 2014), this has now changed. Car-
rying out this task on noisy texts is a new chal-
lenge. The abundant availability of social media data
and the high redundancy that naturally exists in this
data makes this task highly relevant (Zanzotto et al.,
2011).
Our approach is based on the model described
by Bjerva et al. (2014). This model has proved
to achieve state-of-the-art results at predicting se-
mantic similarity (Marelli et al., 2014). It is based
on overlaps of semantically meaningful properties
of sentences. A random forest regression model
(Breiman, 2001) combines these features to predict a
semantic similarity score. We rely heavily on the as-
sumption that semantically meaningful features can
also be used to identify paraphrases.
The features of the existing system are also used
in the new system. However, the old system used a
regression model, while the new task demands class-
based output. Hence, the machine learning model
model is changed to a maximum entropy model.
</bodyText>
<sectionHeader confidence="0.989441" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999870571428571">
The Twitter Paraphrase Corpus consists of two dis-
tinct parts, the training data differs significantly from
the test data.
The 17,790 tweet pairs for training are collected
between April 24th and May 3rd, 2014. These
tweets are selected based on the trending topics
of that period. Annotation of the training data is
done by human annotators from Amazon Mechan-
ical Turk. Every sentence pair is annotated by 5 dif-
ferent annotators, resulting in a score of 0-5. Based
on this score we create a binary paraphrase judge-
ment. If 0, 1 or 2 annotators judged positively, we
treat the sentence pair as not being a paraphrase, for
3, 4 or 5 positive judgements we treat the sentence
</bodyText>
<page confidence="0.980169">
40
</page>
<bodyText confidence="0.958014571428571">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 40–44,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
pair as a paraphrase.
The test data is collected between May 13th and
June 10th, and is thus based on different trending
topics. This assures the integrity of the evaluation.
In contrast to the training data, this data is anno-
tated by an expert similarity rating on a 5-point Lik-
ert scale (Likert, 1932), to mimic the training data.
Sentence pairs with a similarity score of 0, 1 and 2
are considered non-paraphrases, and sentence pairs
with scores of 4 and 5 are considered paraphrases.
The one uncertain category (similarity score of 3) is
discarded in the evaluation.
Using this data, we end up with two different
types of gold data per sentence pair. Firstly, we have
the binary gold data that indicates if a sentence pair
is a paraphrase. Secondly, we have the raw annota-
tions that can be used as a similarity score. These an-
notations are normalized by dividing them by their
maximum score (5), so we end up with (0.0, 0.2, 0.4,
0.6, 0.8, 1.0) as possible similarity scores.
The tweets in the corpus are already tokenized us-
ing TweetMotif (O’Connor et al., 2010). Addition-
ally, Part Of Speech (POS) tags are provided by a
tagger that is adapted to twitter (Derczynski et al.,
2013). Named entity tags are also obtained from an
adapted tagger (Ritter et al., 2011).
</bodyText>
<sectionHeader confidence="0.984755" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.999677333333333">
The model is based on a state-of-the-art semantic
similarity prediction model (Bjerva et al., 2014). It
is mainly based on overlap features extracted from
different parsers, but also includes synset overlap,
and a Compositional Distributional Semantic Model
(CDSM). The parsers used in this model are a con-
stituency parser (Steedman, 2001), logical parser
Paradox (Claessen and S¨orensson, 2003) and the
DRS parser Boxer (Bos, 2008).
</bodyText>
<subsectionHeader confidence="0.983239">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.999957333333333">
Our model uses 25 features in total. Due to space
constraints we cannot describe them all in detail
here. Instead we group the features as follows:
</bodyText>
<listItem confidence="0.996227529411765">
• Lexical features: word overlap, proportional
sentence length difference.
• POS: noun overlap, verb overlap.
• Logical model: instance overlap, relation over-
lap.
• DRS: agent overlap, patient overlap, DRS com-
plexity.
• Entailments: binary features for: neutral, en-
tailment and contradiction predictions.
• CDSM: The cosine distance between the ele-
ment wise addition of the vectors in each sen-
tence is used.
• Synsets (WordNet): The distance of the closest
synsets of each word in both sentences, and the
distance between the noun synsets.
• Named entity: overlap between named enti-
ties1.
</listItem>
<bodyText confidence="0.99904375">
For a complete detailed overview we refer to
the paper describing the semantic similarity system
(Bjerva et al., 2014), or for even more detail (van der
Goot, 2014).
</bodyText>
<subsectionHeader confidence="0.999458">
3.2 Maximum Entropy Models
</subsectionHeader>
<bodyText confidence="0.9999661">
We will compare two different maximum entropy
models. The maximum entropy implementation of
Scikit-Learn (Pedregosa et al., 2011) is used.
The first maximum entropy model is a binary
model that also outputs a probability. From this
model, the normal binary output is not used, instead
we use the estimated probability that something is
a paraphrase. Using this value, we can set our own
threshold to have more control on the final output.
The second maximum entropy model is a multi-
class model. This classifier is based on the 6 dif-
ferent classes in our data, and thus outputs 6 proba-
bilities. We use the similarity score of each class as
weight to convert all probabilities to one probabil-
ity. For each class we multiply the similarity score
with the probability that our model predicts for this
class. The results of the 6 classes are then summed
to get a single probability. This classification model
uses more specific training data, thus it should have
a more precise output.
</bodyText>
<subsectionHeader confidence="0.986729">
3.3 Normalization
</subsectionHeader>
<bodyText confidence="0.99943725">
A normalization approach very similar to that de-
scribed by Han et al. (2013) is used to try to im-
prove the parses. This normalization consists of
three steps.
</bodyText>
<footnote confidence="0.998083">
1This is the only feature not present in the original semantic
similarity system
</footnote>
<page confidence="0.998054">
41
</page>
<figureCaption confidence="0.9999">
Figure 1: Precision and recall for the different classifiers.
</figureCaption>
<bodyText confidence="0.999974703703704">
The first step is to decide which tokens might need
a correction, this is decided by a dictionary lookup
in the Aspell dictionary2.
The second step is the generation of possible cor-
rections for every misspelled word. For this, the As-
pell source code is adapted to lower the costs of dele-
tion in its algorithm, because we assume words are
often typed in an abbreviated form in this domain.
The last step is the ranking of the candidates. Here
we use a different approach than the traditional ap-
proach. Instead of using a static formula to predict
the probability of each candidate, we want to use a
more flexible approach. Google N-gram probabili-
ties (Brants and Franz, 2006), Aspell scores and dic-
tionary lookups are combined using logistic regres-
sion. To adjusts the weights of the regression model,
200 sentences are normalized manually. The result-
ing model is then applied to all the other sentences.
This normalization approach does not reach a per-
fect accuracy, and normalizing a sentence might re-
move meaningful information. So instead of using
the normalization as straightforward pre processing
of the data, we use the raw and the normalized sen-
tence in the model. For each feature, scores are cal-
culated for both versions of the sentence. The high-
est of these scores be used as input for our maximum
entropy model.
</bodyText>
<sectionHeader confidence="0.999408" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.9968485">
This chapter is divided in the two sub tasks of para-
phrase detection and similarity prediction. A strong
</bodyText>
<footnote confidence="0.938622">
2www.aspell.net
</footnote>
<figureCaption confidence="0.997229">
Figure 2: F-Score for the different classifiers. P is the
threshold that decides if a sentence pair is a paraphrase.
</figureCaption>
<bodyText confidence="0.999243333333333">
baseline is used, namely a state-of-the art model for
clean text: a logistic regression model that uses sim-
ple lexical overlap features (Das and Smith, 2009).
</bodyText>
<subsectionHeader confidence="0.997003">
4.1 Paraphrase Detection
</subsectionHeader>
<bodyText confidence="0.999986">
The evaluation is done on expert annotations, which
are only available for the test set. The binary and
multi-class classifiers are evaluated separately. Ad-
ditionally, we also tried to improve the system by
using normalization.
The precision and recall of both classifiers is plot-
ted in Figure 1. In this graph the differences are
barely visible, therefore it looks like both models are
approximately equal.
If we look at the F-scores of Figure 2, the dif-
ferences are bigger. The highest F-scores of both
classifiers are 0.604 and 0.610 for respectively the
binary and the multi-class classifier. Both classifiers
outperform the baseline F-score of 0.583.
These graphs also show that the default output of
the binary deos not perform well, so it is really nec-
essary to use the probabilities.
</bodyText>
<subsectionHeader confidence="0.814902">
4.1.1 Feature Comparisons
</subsectionHeader>
<bodyText confidence="0.99950375">
We use the same grouping for features as in 3.1.
The absolute weights of all features within each
group are summed. For the multi-class classifier the
weights are averaged over all 6 classes. Also an ab-
lation experiment is done. An overview this evalua-
tion is shown in Table 1.
In the ablation experiments we see that it is not
always better to use more features. Especially the
</bodyText>
<page confidence="0.997438">
42
</page>
<table confidence="0.999611636363636">
Weights Ablation
Feat. group Binary Multi Binary Multi
Lexical 2.43 1.65 0.601 0.598
POS 0.79 0.71 0.600 0.600
Log. model 0.74 1.61 0.573 0.606
DRS 3.57 1.88 0.551 0.553
Entailments 0.51 2.79 0.584 0.589
CDSM 5.29 3.63 0.538 0.523
Synsets 0.49 0.63 0.588 0.584
NE 0.06 0.09 0.597 0.599
All - - 0.600 0.604
</table>
<tableCaption confidence="0.9866335">
Table 1: Absolute weights of the feature groups and fea-
ture group ablation F1-Scores.
</tableCaption>
<bodyText confidence="0.999341">
logical model should be left out in the multi-class
entropy model. The models differ in some aspects,
whereas some features are important for both. More
specifically, we can see that the parsers outputs and
lexical features are more important for the multi-
class model, while the other features are more im-
portant for the binary model.
</bodyText>
<subsectionHeader confidence="0.712353">
4.1.2 Normalization
</subsectionHeader>
<bodyText confidence="0.999991">
After the normalization of the sentences, we run
the systems again. These runs are not plotted in the
graphs, because the differences are small. Despite
the small differences, there is one little performance
boost on the top-runs of the multi-class classifiers,
resulting in the highest F-score of 0.62.
</bodyText>
<subsectionHeader confidence="0.998867">
4.2 Semantic Similarity Prediction
</subsectionHeader>
<bodyText confidence="0.99775825">
Even though we do not have real semantic similarity
training data, we simulate semantic similarity using
the amount of the positive judgements per sentence
pair. Our system is evolved from a semantic simi-
larity prediction system, so this model should work
well for this task. The Pearson correlation between
the different annotations of experts (test) and crowd-
sourcing (training) is 0.735.
For this sub task we will also try different heuris-
tics using both our classifiers. We start with the
multi-class classifier, because it is trained to give
back a similarity score. The model produces prob-
abilities for each class, the class with the high-
est probability is used as output. We call this the
Highest P method.
Another model can be built using the predicted
</bodyText>
<table confidence="0.958163">
Baseline Highest P Binary P Weighted
R 0.511 0.416 0.508 0.515
</table>
<tableCaption confidence="0.9902525">
Table 2: Pearson correlation (R) for the different similar-
ity prediction approaches.
</tableCaption>
<bodyText confidence="0.994318">
weights, similar to section 3.2. We refer to this as
the Weighted method.
Besides the multi-class classifier, we also trained
a binary classifier. The only way for this classifier to
output a degree score, is using the probability. This
is called Binary P.
Only the weighted method beats the baseline. Re-
sults of all three approaches and the baseline can be
found in Table 2.
</bodyText>
<sectionHeader confidence="0.996543" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999958555555555">
The main conclusion to draw from these experi-
ments is that by using deep semantic features, we
can achieve a maximum F-score of 0.61 on the para-
phrase detection task. By using normalization we
can improve this F-score to 0.62.
Following from this, it is safe to conclude that a
semantic similarity prediction system can be used in
paraphrase detection reasonably well. Our system
had an average result on this shared task (10th out of
18 teams)3. The advantage of this system is that it
can be created easily from existing tools.
Unsurprisingly, the results on the semantic simi-
larity task were better (6th out of 13 teams). Even
though the gold data does not represent a real se-
mantic similarity, but a scale of positive annotations
of the paraphrase detection task.
The source code of our system has been made
publicly available4.
</bodyText>
<sectionHeader confidence="0.994954" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996357166666667">
This paper is part of the ’Parsing Algorithms for Uncer-
tain Input’ project, supported by the Nuance Foundation.
We would like to thank the organizers of the shared
task (Xu et al., 2015). Additionally, we would also like to
thank the anonymous reviewers and Johannes Bjerva for
the valuable feedback on this paper.
</bodyText>
<footnote confidence="0.9973625">
3http://alt.qcri.org/semeval2015/task1
4https://bitbucket.org/robvanderg/sem15
</footnote>
<page confidence="0.999737">
43
</page>
<sectionHeader confidence="0.989967" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999658594936709">
Johannes Bjerva, Johan Bos, Rob van der Goot, and
Malvina Nissim. 2014. The Meaning Factory: For-
mal semantics for recognizing textual entailment and
determining semantic similarity. SemEval 2014: In-
ternational Workshop on Semantic Evaluation, pages
642–646.
Johan Bos. 2008. Wide-coverage semantic analysis with
Boxer. In Proceedings of the 2008 Conference on Se-
mantics in Text Processing, pages 277–286.
Thorsten Brants and Alex Franz. 2006. Web 1T5-gram
corpus version 1.1.
Leo Breiman. 2001. Random forests. Machine learning,
45(1):5–32.
Koen Claessen and Niklas S¨orensson. 2003. New tech-
niques that improve MACE-style finite model find-
ing. In Proceedings of the CADE-19 Workshop:
Model Computation-Principles, Algorithms, Applica-
tions, pages 11–27.
Dipanjan Das and Noah A Smith. 2009. Paraphrase iden-
tification as probabilistic quasi-synchronous recogni-
tion. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing of theAFNLP: Volume 1-Volume 1, pages 468–
476.
Leon Derczynski, Alan Ritter, Sam Clark, and Kalina
Bontcheva. 2013. Twitter part-of-speech tagging for
all: Overcoming sparse and noisy data. In RANLP,
pages 198–206.
Bo Han, Paul Cook, and Timothy Baldwin. 2013. Lex-
ical normalization for social media text. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
4(1):5.
Rensis Likert. 1932. A technique for the measurement
of attitudes. Archives ofpsychology.
Nitin Madnani, Joel Tetreault, and Martin Chodorow.
2012. Re-examining machine translation metrics for
paraphrase identification. In Proceedings of the 2012
Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Lan-
guage Technologies, pages 182–190.
Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-
faella Bernardi, Stefano Menini, and Roberto Zampar-
elli. 2014. Semeval-2014 task 1: Evaluation of com-
positional distributional semantic models on full sen-
tences through semantic relatedness and textual entail-
ment. SemEval-2014.
Brendan O’Connor, Michel Krieger, and David Ahn.
2010. Tweetmotif: Exploratory search and topic sum-
marization for Twitter.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, et al. 2011. Scikit-learn: Machine
learning in Python. The Journal of Machine Learning
Research, 12:2825–2830.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named
entity recognition in tweets: an experimental study. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 1524–1534.
Mark Steedman. 2001. The Syntactic Process.
Rob van der Goot. 2014. Automatic estimation of se-
mantic relatedness for sentences using machine learn-
ing. Master’s thesis, University of Groningen.
Wei Xu, Alan Ritter, Chris Callison-Burch, William B
Dolan, and Yangfeng Ji. 2014. Extracting lexically
divergent paraphrases from Twitter. Transactions of
the Association for Computational Linguistics, 2:435–
448.
Wei Xu, Chris Callison-Burch, and William B. Dolan.
2015. SemEval-2015 Task 1: Paraphrase and semantic
similarity in Twitter (PIT). In Proceedings of the 9th
International Workshop on Semantic Evaluation (Se-
mEval).
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Kostas Tsioutsiouliklis. 2011. Linguistic redun-
dancy in Twitter. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 659–669.
</reference>
<page confidence="0.999286">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.640245">
<title confidence="0.999662">ROB: Using Semantic Meaning to Recognize Paraphrases</title>
<author confidence="0.999845">Rob van_der_Goot Gertjan van_Noord</author>
<affiliation confidence="0.999045">University of Groningen University of Groningen</affiliation>
<email confidence="0.953703">r.van.der.goot@rug.nlg.j.m.van.noord@rug.nl</email>
<abstract confidence="0.96398852173913">Paraphrase recognition is the task of identifying whether two pieces of natural language represent similar meanings. This paper describes a system participating in the shared task 1 of SemEval 2015, which is about paraphrase detection and semantic similarity in twitter. Our approach is to exploit semantically meaningful features to detect paraphrases. An existing state-of-the-art model for predicting semantic similarity is adapted to this task. A wide variety of features is used, ranging from different types of models, to lexical overlap and synset overlap. A maximum entropy classifier is then trained on these features. In addition to the detection of paraphrases, a similarity score is also predicted, using the probabilities of the classifier. To improve the results, normalization is used as preprocessing step. Our final system achieves a F1 score of 0.620 (10th out of 18 teams), and a Pearson correlation of 0.515 (6th out of 13 teams).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Johannes Bjerva</author>
<author>Johan Bos</author>
<author>Rob van der Goot</author>
<author>Malvina Nissim</author>
</authors>
<title>The Meaning Factory: Formal semantics for recognizing textual entailment and determining semantic similarity.</title>
<date>2014</date>
<booktitle>SemEval 2014: International Workshop on Semantic Evaluation,</booktitle>
<pages>642--646</pages>
<marker>Bjerva, Bos, van der Goot, Nissim, 2014</marker>
<rawString>Johannes Bjerva, Johan Bos, Rob van der Goot, and Malvina Nissim. 2014. The Meaning Factory: Formal semantics for recognizing textual entailment and determining semantic similarity. SemEval 2014: International Workshop on Semantic Evaluation, pages 642–646.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-coverage semantic analysis with Boxer.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Semantics in Text Processing,</booktitle>
<pages>277--286</pages>
<contexts>
<context position="5077" citStr="Bos, 2008" startWordPosition="832" endWordPosition="833">S) tags are provided by a tagger that is adapted to twitter (Derczynski et al., 2013). Named entity tags are also obtained from an adapted tagger (Ritter et al., 2011). 3 Method The model is based on a state-of-the-art semantic similarity prediction model (Bjerva et al., 2014). It is mainly based on overlap features extracted from different parsers, but also includes synset overlap, and a Compositional Distributional Semantic Model (CDSM). The parsers used in this model are a constituency parser (Steedman, 2001), logical parser Paradox (Claessen and S¨orensson, 2003) and the DRS parser Boxer (Bos, 2008). 3.1 Features Our model uses 25 features in total. Due to space constraints we cannot describe them all in detail here. Instead we group the features as follows: • Lexical features: word overlap, proportional sentence length difference. • POS: noun overlap, verb overlap. • Logical model: instance overlap, relation overlap. • DRS: agent overlap, patient overlap, DRS complexity. • Entailments: binary features for: neutral, entailment and contradiction predictions. • CDSM: The cosine distance between the element wise addition of the vectors in each sentence is used. • Synsets (WordNet): The dist</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-coverage semantic analysis with Boxer. In Proceedings of the 2008 Conference on Semantics in Text Processing, pages 277–286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1T5-gram corpus version 1.1.</title>
<date>2006</date>
<contexts>
<context position="8000" citStr="Brants and Franz, 2006" startWordPosition="1323" endWordPosition="1326"> a correction, this is decided by a dictionary lookup in the Aspell dictionary2. The second step is the generation of possible corrections for every misspelled word. For this, the Aspell source code is adapted to lower the costs of deletion in its algorithm, because we assume words are often typed in an abbreviated form in this domain. The last step is the ranking of the candidates. Here we use a different approach than the traditional approach. Instead of using a static formula to predict the probability of each candidate, we want to use a more flexible approach. Google N-gram probabilities (Brants and Franz, 2006), Aspell scores and dictionary lookups are combined using logistic regression. To adjusts the weights of the regression model, 200 sentences are normalized manually. The resulting model is then applied to all the other sentences. This normalization approach does not reach a perfect accuracy, and normalizing a sentence might remove meaningful information. So instead of using the normalization as straightforward pre processing of the data, we use the raw and the normalized sentence in the model. For each feature, scores are calculated for both versions of the sentence. The highest of these score</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1T5-gram corpus version 1.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<booktitle>Machine learning,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="2080" citStr="Breiman, 2001" startWordPosition="330" endWordPosition="331">. Thanks to the Twitter Paraphrase Corpus (Xu et al., 2014), this has now changed. Carrying out this task on noisy texts is a new challenge. The abundant availability of social media data and the high redundancy that naturally exists in this data makes this task highly relevant (Zanzotto et al., 2011). Our approach is based on the model described by Bjerva et al. (2014). This model has proved to achieve state-of-the-art results at predicting semantic similarity (Marelli et al., 2014). It is based on overlaps of semantically meaningful properties of sentences. A random forest regression model (Breiman, 2001) combines these features to predict a semantic similarity score. We rely heavily on the assumption that semantically meaningful features can also be used to identify paraphrases. The features of the existing system are also used in the new system. However, the old system used a regression model, while the new task demands classbased output. Hence, the machine learning model model is changed to a maximum entropy model. 2 Data The Twitter Paraphrase Corpus consists of two distinct parts, the training data differs significantly from the test data. The 17,790 tweet pairs for training are collected</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random forests. Machine learning, 45(1):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen Claessen</author>
<author>Niklas S¨orensson</author>
</authors>
<title>New techniques that improve MACE-style finite model finding.</title>
<date>2003</date>
<booktitle>In Proceedings of the CADE-19 Workshop: Model Computation-Principles, Algorithms, Applications,</booktitle>
<pages>11--27</pages>
<marker>Claessen, S¨orensson, 2003</marker>
<rawString>Koen Claessen and Niklas S¨orensson. 2003. New techniques that improve MACE-style finite model finding. In Proceedings of the CADE-19 Workshop: Model Computation-Principles, Algorithms, Applications, pages 11–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Noah A Smith</author>
</authors>
<title>Paraphrase identification as probabilistic quasi-synchronous recognition.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of theAFNLP: Volume</booktitle>
<volume>1</volume>
<pages>468--476</pages>
<contexts>
<context position="9058" citStr="Das and Smith, 2009" startWordPosition="1499" endWordPosition="1502">ta, we use the raw and the normalized sentence in the model. For each feature, scores are calculated for both versions of the sentence. The highest of these scores be used as input for our maximum entropy model. 4 Evaluation This chapter is divided in the two sub tasks of paraphrase detection and similarity prediction. A strong 2www.aspell.net Figure 2: F-Score for the different classifiers. P is the threshold that decides if a sentence pair is a paraphrase. baseline is used, namely a state-of-the art model for clean text: a logistic regression model that uses simple lexical overlap features (Das and Smith, 2009). 4.1 Paraphrase Detection The evaluation is done on expert annotations, which are only available for the test set. The binary and multi-class classifiers are evaluated separately. Additionally, we also tried to improve the system by using normalization. The precision and recall of both classifiers is plotted in Figure 1. In this graph the differences are barely visible, therefore it looks like both models are approximately equal. If we look at the F-scores of Figure 2, the differences are bigger. The highest F-scores of both classifiers are 0.604 and 0.610 for respectively the binary and the </context>
</contexts>
<marker>Das, Smith, 2009</marker>
<rawString>Dipanjan Das and Noah A Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of theAFNLP: Volume 1-Volume 1, pages 468– 476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leon Derczynski</author>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Kalina Bontcheva</author>
</authors>
<title>Twitter part-of-speech tagging for all: Overcoming sparse and noisy data. In</title>
<date>2013</date>
<booktitle>RANLP,</booktitle>
<pages>198--206</pages>
<contexts>
<context position="4552" citStr="Derczynski et al., 2013" startWordPosition="749" endWordPosition="752">luation. Using this data, we end up with two different types of gold data per sentence pair. Firstly, we have the binary gold data that indicates if a sentence pair is a paraphrase. Secondly, we have the raw annotations that can be used as a similarity score. These annotations are normalized by dividing them by their maximum score (5), so we end up with (0.0, 0.2, 0.4, 0.6, 0.8, 1.0) as possible similarity scores. The tweets in the corpus are already tokenized using TweetMotif (O’Connor et al., 2010). Additionally, Part Of Speech (POS) tags are provided by a tagger that is adapted to twitter (Derczynski et al., 2013). Named entity tags are also obtained from an adapted tagger (Ritter et al., 2011). 3 Method The model is based on a state-of-the-art semantic similarity prediction model (Bjerva et al., 2014). It is mainly based on overlap features extracted from different parsers, but also includes synset overlap, and a Compositional Distributional Semantic Model (CDSM). The parsers used in this model are a constituency parser (Steedman, 2001), logical parser Paradox (Claessen and S¨orensson, 2003) and the DRS parser Boxer (Bos, 2008). 3.1 Features Our model uses 25 features in total. Due to space constraint</context>
</contexts>
<marker>Derczynski, Ritter, Clark, Bontcheva, 2013</marker>
<rawString>Leon Derczynski, Alan Ritter, Sam Clark, and Kalina Bontcheva. 2013. Twitter part-of-speech tagging for all: Overcoming sparse and noisy data. In RANLP, pages 198–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Han</author>
<author>Paul Cook</author>
<author>Timothy Baldwin</author>
</authors>
<title>Lexical normalization for social media text.</title>
<date>2013</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>4--1</pages>
<contexts>
<context position="7097" citStr="Han et al. (2013)" startWordPosition="1166" endWordPosition="1169">entropy model is a multiclass model. This classifier is based on the 6 different classes in our data, and thus outputs 6 probabilities. We use the similarity score of each class as weight to convert all probabilities to one probability. For each class we multiply the similarity score with the probability that our model predicts for this class. The results of the 6 classes are then summed to get a single probability. This classification model uses more specific training data, thus it should have a more precise output. 3.3 Normalization A normalization approach very similar to that described by Han et al. (2013) is used to try to improve the parses. This normalization consists of three steps. 1This is the only feature not present in the original semantic similarity system 41 Figure 1: Precision and recall for the different classifiers. The first step is to decide which tokens might need a correction, this is decided by a dictionary lookup in the Aspell dictionary2. The second step is the generation of possible corrections for every misspelled word. For this, the Aspell source code is adapted to lower the costs of deletion in its algorithm, because we assume words are often typed in an abbreviated for</context>
</contexts>
<marker>Han, Cook, Baldwin, 2013</marker>
<rawString>Bo Han, Paul Cook, and Timothy Baldwin. 2013. Lexical normalization for social media text. ACM Transactions on Intelligent Systems and Technology (TIST), 4(1):5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rensis Likert</author>
</authors>
<title>A technique for the measurement of attitudes. Archives ofpsychology.</title>
<date>1932</date>
<contexts>
<context position="3669" citStr="Likert, 1932" startWordPosition="596" endWordPosition="597">ged positively, we treat the sentence pair as not being a paraphrase, for 3, 4 or 5 positive judgements we treat the sentence 40 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 40–44, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics pair as a paraphrase. The test data is collected between May 13th and June 10th, and is thus based on different trending topics. This assures the integrity of the evaluation. In contrast to the training data, this data is annotated by an expert similarity rating on a 5-point Likert scale (Likert, 1932), to mimic the training data. Sentence pairs with a similarity score of 0, 1 and 2 are considered non-paraphrases, and sentence pairs with scores of 4 and 5 are considered paraphrases. The one uncertain category (similarity score of 3) is discarded in the evaluation. Using this data, we end up with two different types of gold data per sentence pair. Firstly, we have the binary gold data that indicates if a sentence pair is a paraphrase. Secondly, we have the raw annotations that can be used as a similarity score. These annotations are normalized by dividing them by their maximum score (5), so </context>
</contexts>
<marker>Likert, 1932</marker>
<rawString>Rensis Likert. 1932. A technique for the measurement of attitudes. Archives ofpsychology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Joel Tetreault</author>
<author>Martin Chodorow</author>
</authors>
<title>Re-examining machine translation metrics for paraphrase identification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>182--190</pages>
<contexts>
<context position="1402" citStr="Madnani et al., 2012" startWordPosition="217" endWordPosition="220">r is then trained on these features. In addition to the detection of paraphrases, a similarity score is also predicted, using the probabilities of the classifier. To improve the results, normalization is used as preprocessing step. Our final system achieves a F1 score of 0.620 (10th out of 18 teams), and a Pearson correlation of 0.515 (6th out of 13 teams). 1 Introduction A good paraphrase detection system can be useful in many natural language processing tasks, like searching, translating or summarization. For clean texts, F1 scores as high as 0.84 have been reported on paraphrase detection (Madnani et al., 2012). However, previous research focused almost solely on clean text. Thanks to the Twitter Paraphrase Corpus (Xu et al., 2014), this has now changed. Carrying out this task on noisy texts is a new challenge. The abundant availability of social media data and the high redundancy that naturally exists in this data makes this task highly relevant (Zanzotto et al., 2011). Our approach is based on the model described by Bjerva et al. (2014). This model has proved to achieve state-of-the-art results at predicting semantic similarity (Marelli et al., 2014). It is based on overlaps of semantically meanin</context>
</contexts>
<marker>Madnani, Tetreault, Chodorow, 2012</marker>
<rawString>Nitin Madnani, Joel Tetreault, and Martin Chodorow. 2012. Re-examining machine translation metrics for paraphrase identification. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 182–190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Marelli</author>
<author>Luisa Bentivogli</author>
<author>Marco Baroni</author>
<author>Raffaella Bernardi</author>
<author>Stefano Menini</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<tech>SemEval-2014.</tech>
<contexts>
<context position="1954" citStr="Marelli et al., 2014" startWordPosition="310" endWordPosition="313">.84 have been reported on paraphrase detection (Madnani et al., 2012). However, previous research focused almost solely on clean text. Thanks to the Twitter Paraphrase Corpus (Xu et al., 2014), this has now changed. Carrying out this task on noisy texts is a new challenge. The abundant availability of social media data and the high redundancy that naturally exists in this data makes this task highly relevant (Zanzotto et al., 2011). Our approach is based on the model described by Bjerva et al. (2014). This model has proved to achieve state-of-the-art results at predicting semantic similarity (Marelli et al., 2014). It is based on overlaps of semantically meaningful properties of sentences. A random forest regression model (Breiman, 2001) combines these features to predict a semantic similarity score. We rely heavily on the assumption that semantically meaningful features can also be used to identify paraphrases. The features of the existing system are also used in the new system. However, the old system used a regression model, while the new task demands classbased output. Hence, the machine learning model model is changed to a maximum entropy model. 2 Data The Twitter Paraphrase Corpus consists of two</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>Marco Marelli, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. SemEval-2014.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brendan O’Connor</author>
<author>Michel Krieger</author>
<author>David Ahn</author>
</authors>
<title>Tweetmotif: Exploratory search and topic summarization for Twitter.</title>
<date>2010</date>
<marker>O’Connor, Krieger, Ahn, 2010</marker>
<rawString>Brendan O’Connor, Michel Krieger, and David Ahn. 2010. Tweetmotif: Exploratory search and topic summarization for Twitter.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand</author>
</authors>
<title>Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer,</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, et</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Bertrand, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in Python. The Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: an experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1524--1534</pages>
<contexts>
<context position="4634" citStr="Ritter et al., 2011" startWordPosition="763" endWordPosition="766"> pair. Firstly, we have the binary gold data that indicates if a sentence pair is a paraphrase. Secondly, we have the raw annotations that can be used as a similarity score. These annotations are normalized by dividing them by their maximum score (5), so we end up with (0.0, 0.2, 0.4, 0.6, 0.8, 1.0) as possible similarity scores. The tweets in the corpus are already tokenized using TweetMotif (O’Connor et al., 2010). Additionally, Part Of Speech (POS) tags are provided by a tagger that is adapted to twitter (Derczynski et al., 2013). Named entity tags are also obtained from an adapted tagger (Ritter et al., 2011). 3 Method The model is based on a state-of-the-art semantic similarity prediction model (Bjerva et al., 2014). It is mainly based on overlap features extracted from different parsers, but also includes synset overlap, and a Compositional Distributional Semantic Model (CDSM). The parsers used in this model are a constituency parser (Steedman, 2001), logical parser Paradox (Claessen and S¨orensson, 2003) and the DRS parser Boxer (Bos, 2008). 3.1 Features Our model uses 25 features in total. Due to space constraints we cannot describe them all in detail here. Instead we group the features as fol</context>
</contexts>
<marker>Ritter, Clark, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an experimental study. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1524–1534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2001</date>
<contexts>
<context position="4984" citStr="Steedman, 2001" startWordPosition="818" endWordPosition="819">s are already tokenized using TweetMotif (O’Connor et al., 2010). Additionally, Part Of Speech (POS) tags are provided by a tagger that is adapted to twitter (Derczynski et al., 2013). Named entity tags are also obtained from an adapted tagger (Ritter et al., 2011). 3 Method The model is based on a state-of-the-art semantic similarity prediction model (Bjerva et al., 2014). It is mainly based on overlap features extracted from different parsers, but also includes synset overlap, and a Compositional Distributional Semantic Model (CDSM). The parsers used in this model are a constituency parser (Steedman, 2001), logical parser Paradox (Claessen and S¨orensson, 2003) and the DRS parser Boxer (Bos, 2008). 3.1 Features Our model uses 25 features in total. Due to space constraints we cannot describe them all in detail here. Instead we group the features as follows: • Lexical features: word overlap, proportional sentence length difference. • POS: noun overlap, verb overlap. • Logical model: instance overlap, relation overlap. • DRS: agent overlap, patient overlap, DRS complexity. • Entailments: binary features for: neutral, entailment and contradiction predictions. • CDSM: The cosine distance between the</context>
</contexts>
<marker>Steedman, 2001</marker>
<rawString>Mark Steedman. 2001. The Syntactic Process.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rob van der Goot</author>
</authors>
<title>Automatic estimation of semantic relatedness for sentences using machine learning. Master’s thesis,</title>
<date>2014</date>
<institution>University of Groningen.</institution>
<marker>van der Goot, 2014</marker>
<rawString>Rob van der Goot. 2014. Automatic estimation of semantic relatedness for sentences using machine learning. Master’s thesis, University of Groningen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alan Ritter</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
<author>Yangfeng Ji</author>
</authors>
<title>Extracting lexically divergent paraphrases from Twitter. Transactions of the Association for Computational Linguistics,</title>
<date>2014</date>
<pages>2--435</pages>
<contexts>
<context position="1525" citStr="Xu et al., 2014" startWordPosition="237" endWordPosition="240">e probabilities of the classifier. To improve the results, normalization is used as preprocessing step. Our final system achieves a F1 score of 0.620 (10th out of 18 teams), and a Pearson correlation of 0.515 (6th out of 13 teams). 1 Introduction A good paraphrase detection system can be useful in many natural language processing tasks, like searching, translating or summarization. For clean texts, F1 scores as high as 0.84 have been reported on paraphrase detection (Madnani et al., 2012). However, previous research focused almost solely on clean text. Thanks to the Twitter Paraphrase Corpus (Xu et al., 2014), this has now changed. Carrying out this task on noisy texts is a new challenge. The abundant availability of social media data and the high redundancy that naturally exists in this data makes this task highly relevant (Zanzotto et al., 2011). Our approach is based on the model described by Bjerva et al. (2014). This model has proved to achieve state-of-the-art results at predicting semantic similarity (Marelli et al., 2014). It is based on overlaps of semantically meaningful properties of sentences. A random forest regression model (Breiman, 2001) combines these features to predict a semanti</context>
</contexts>
<marker>Xu, Ritter, Callison-Burch, Dolan, Ji, 2014</marker>
<rawString>Wei Xu, Alan Ritter, Chris Callison-Burch, William B Dolan, and Yangfeng Ji. 2014. Extracting lexically divergent paraphrases from Twitter. Transactions of the Association for Computational Linguistics, 2:435– 448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Chris Callison-Burch</author>
<author>William B Dolan</author>
</authors>
<title>SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT).</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).</booktitle>
<marker>Xu, Callison-Burch, Dolan, 2015</marker>
<rawString>Wei Xu, Chris Callison-Burch, and William B. Dolan. 2015. SemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Kostas Tsioutsiouliklis</author>
</authors>
<title>Linguistic redundancy in Twitter.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>659--669</pages>
<contexts>
<context position="1768" citStr="Zanzotto et al., 2011" startWordPosition="280" endWordPosition="283">roduction A good paraphrase detection system can be useful in many natural language processing tasks, like searching, translating or summarization. For clean texts, F1 scores as high as 0.84 have been reported on paraphrase detection (Madnani et al., 2012). However, previous research focused almost solely on clean text. Thanks to the Twitter Paraphrase Corpus (Xu et al., 2014), this has now changed. Carrying out this task on noisy texts is a new challenge. The abundant availability of social media data and the high redundancy that naturally exists in this data makes this task highly relevant (Zanzotto et al., 2011). Our approach is based on the model described by Bjerva et al. (2014). This model has proved to achieve state-of-the-art results at predicting semantic similarity (Marelli et al., 2014). It is based on overlaps of semantically meaningful properties of sentences. A random forest regression model (Breiman, 2001) combines these features to predict a semantic similarity score. We rely heavily on the assumption that semantically meaningful features can also be used to identify paraphrases. The features of the existing system are also used in the new system. However, the old system used a regressio</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Tsioutsiouliklis, 2011</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Kostas Tsioutsiouliklis. 2011. Linguistic redundancy in Twitter. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 659–669.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>