<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.207842">
<title confidence="0.973899">
NEWS 2009 Machine Transliteration Shared Task System Description:
Transliteration with Letter-to-Phoneme Technology
</title>
<author confidence="0.978227">
Colin Cherry and Hisami Suzuki
</author>
<affiliation confidence="0.948308">
Microsoft Research
</affiliation>
<address confidence="0.9484075">
One Microsoft Way
Redmond, WA, 98052
</address>
<email confidence="0.999408">
{colinc,hisamis}@microsoft.com
</email>
<sectionHeader confidence="0.99391" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999888105263158">
We interpret the problem of transliterat-
ing English named entities into Hindi or
Japanese Katakana as a variant of the
letter-to-phoneme (L2P) subtask of text-
to-speech processing. Therefore, we apply
a re-implementation of a state-of-the-art,
discriminative L2P system (Jiampojamarn
et al., 2008) to the problem, without fur-
ther modification. In doing so, we hope
to provide a baseline for the NEWS 2009
Machine Transliteration Shared Task (Li
et al., 2009), indicating how much can be
achieved without transliteration-specific
technology. This paper briefly sum-
marizes the original work and our re-
implementation. We also describe a bug
in our submitted implementation, and pro-
vide updated results on the development
and test sets.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999829551724138">
Transliteration occurs when a word is borrowed
into a language with a different character set from
its language of origin. The word is transcribed into
the new character set in a manner that maintains
phonetic correspondence.
When attempting to automate machine translit-
eration, modeling the channel that transforms
source language characters into transliterated tar-
get language characters is a key component to
good performance. Since the primary signal fol-
lowed by human transliterators is phonetic corre-
spondence, it makes sense that a letter-to-phoneme
(L2P) transcription engine would perform well at
this task. Of course, transliteration is often framed
within the larger problems of translation and bilin-
gual named entity co-reference, making available
a number of other interesting features, such as tar-
get lexicons (Knight and Graehl, 1998), distribu-
tional similarity (Bilac and Tanaka, 2005), or the
dates of an entity’s mentions in the news (Kle-
mentiev and Roth, 2006). However, this task’s fo-
cus on generation has isolated the character-level
component, which makes L2P technology a near-
ideal match. For our submission, we re-implement
the L2P approach described by Jiampojamarn et
al. (2008) as faithfully as possible, and apply it
unmodified to the transliteration shared task for
the English-to-Hindi (Kumaran and Kellner, 2007)
and English-to-Japanese Katakana1 tests.
</bodyText>
<sectionHeader confidence="0.993725" genericHeader="introduction">
2 Approach
</sectionHeader>
<subsectionHeader confidence="0.999709">
2.1 Summary of L2P approach
</subsectionHeader>
<bodyText confidence="0.999957153846154">
The core of the L2P transduction engine is the
dynamic programming algorithm for monotone
phrasal decoding (Zens and Ney, 2004). The main
feature of this algorithm is its capability to trans-
duce many consecutive characters with a single
operation. This algorithm is used to conduct a
search for a max-weight derivation according to
a linear model with indicator features. A sample
derivation is shown in Figure 1.
There are two main categories of features: con-
text and transition features, which follow the first
two feature templates described by Jiampojamarn
et al. (2008). Context features are centered around
a transduction operation. These features include
an indicator for the operation itself, which is then
conjoined with indicators for all n-grams of source
context within a fixed window of the operation.
Transition features are Markov or n-gram features.
They ensure that the produced target string makes
sense as a character sequence, and are represented
as indicators on the presence of target n-grams.
The feature templates have two main parameters,
the size S of the character window from which
source context features are drawn, and the max-
imum length T of target n-gram indicators. We
fit these parameters using grid search over 1-best
</bodyText>
<footnote confidence="0.998841">
1Provided by http://www.cjk.org
</footnote>
<page confidence="0.965328">
69
</page>
<note confidence="0.9957555">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 69–71,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.514317">
ame → TA , ri → J , can → tS
</bodyText>
<figureCaption confidence="0.966472">
Figure 1: Example derivation transforming
“American” into “1AJt✓S”.
</figureCaption>
<bodyText confidence="0.999790714285714">
accuracy on the provided development sets.
The engine’s features are trained using the
structured perceptron (Collins, 2002). Jiampo-
jamarn et al. (2008) show strong improvements
in the L2P domain using MIRA in place of the
perceptron update; unfortunately, we did not im-
plement a k-best MIRA update due to time con-
straints. In our implementation, no special con-
sideration was given to the availability of multi-
ple correct answers in the training data; we always
pick the first reference transliteration and treat it
as the only correct answer. Investigating the use
of all correct answers would be an obvious next
step to improve the system.
</bodyText>
<subsectionHeader confidence="0.997297">
2.2 Major differences in implementation
</subsectionHeader>
<bodyText confidence="0.973020928571429">
Our system made two alternate design decisions
(we do not claim improvements) over those made
by (Jiampojamarn et al., 2008), mostly based on
the availability of software. First, we employed a
beam of 40 candidates in our decoder, to enable ef-
ficient use of large language model contexts. This
is put to good use in the Hindi task, where we
found n-gram indicators of length up to n = 6
provided optimal development performance.
Second, we employed an alternate character
aligner to create our training derivations. This
aligner is similar to recent non-compositional
phrasal word-alignment models (Zhang et al.,
2008), limited so it can only produce monotone
character alignments. The aligner creates sub-
string alignments, without insertion or deletion
operators. As such, an aligned transliteration pair
also serves as a transliteration derivation. We em-
ployed a maximum substring length of 3.
The training data was heuristically cleaned af-
ter alignment. Any derivation found by the aligner
that uses an operation occurring fewer than 3 times
throughout the entire training set was eliminated.
This reduced training set sizes to 8,511 pairs
for English-Hindi and 20,306 pairs for English-
Katakana.
Table 1: Development and test 1-best accuracies,
as reported by the official evaluation tool
</bodyText>
<table confidence="0.9991596">
System / Test set With Bug Fixed
Hindi Dev 36.7 39.6
Hindi Test 41.8 46.6
Katakana Dev 46.0 47.1
Katakana Test 46.6 46.9
</table>
<sectionHeader confidence="0.983425" genericHeader="method">
3 The Bug
</sectionHeader>
<bodyText confidence="0.999984444444445">
The submitted version of our system had a bug
in its transition features: instead of generating an
indicator for every possible n-gram in the gener-
ated target sequence, it generated n-grams over
target substrings, defined by the operations used
during transduction. Consider, for example, the
derivation shown in Figure 1, which generates
“1AJtS”. With buggy trigram transition
features, the final operation would produce the
single indicator [AJ|tS], instead of the two
character-level trigrams [AJ|t] and [Jt✓|S].
This leads to problems with data sparsity, which
we had not noticed on unrelated experiments with
larger training data. We report results both with
the bug and with fixed transition features. We do
so to emphasize the importance of a fine-grained
language discriminative language model, as op-
posed to one which operates on a substring level.
</bodyText>
<sectionHeader confidence="0.996435" genericHeader="method">
4 Development
</sectionHeader>
<bodyText confidence="0.999670375">
Development consisted of performing a parameter
grid search over S and T for each language pair’s
development set. All combinations of S = 0 ... 4
and T = 0... 7 were tested for each language
pair. Based on these experiments, we selected (for
the fixed version), values of S = 2, T = 6 for
English-Hindi, and S = 4, T = 3 for English-
Katakana.
</bodyText>
<sectionHeader confidence="0.999874" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999977">
The results of our internal experiments with the
official evaluation tool are shown in Table 1. We
report 1-best accuracy on both development and
test sets, with both the buggy and fixed versions of
our system. As one can see, the bug makes less of
an impact in the English-Katakana setting, where
more training data is available.
</bodyText>
<page confidence="0.997088">
70
</page>
<sectionHeader confidence="0.998789" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999901444444444">
We have demonstrated that an automatic letter-
to-phoneme transducer performs fairly well on
this transliteration shared task, with no language-
specific or transliteration-specific modifications.
Instead, we simply considered Hindi or Katakana
to be an alternate encoding for English phonemes.
In the future, we would like to investigate proper
use of multiple reference answers during percep-
tron training.
</bodyText>
<sectionHeader confidence="0.997385" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998938">
We would like to thank the NEWS 2009 Machine
Transliteration Shared Task organizers for creating
this venue for comparing transliteration methods.
We would also like to thank Chris Quirk for pro-
viding us with his alignment software.
</bodyText>
<sectionHeader confidence="0.998731" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998372181818182">
Slaven Bilac and Hozumi Tanaka. 2005. Extracting
transliteration pairs from comparable corpora. In
Proceedings of the Annual Meeting of the Natural
Language Processing Society, Japan.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint processing and discriminative
training for letter-to-phoneme conversion. In ACL,
pages 905–913, Columbus, Ohio, June.
Alexandre Klementiev and Dan Roth. 2006. Named
entity transliteration and discovery from multilin-
gual comparable corpora. In HLT-NAACL, pages
82–88, New York City, USA, June.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599–612.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In Proc. of
the 30th SIGIR.
Haizhou Li, A. Kumaran, Vladimir Pervouchine, and
Min Zhang. 2009. Report on NEWS 2009 machine
transliteration shared task. In Proceedings of ACL-
IJCNLP 2009 Named Entities Workshop (NEWS
2009), Singapore.
Richard Zens and Hermann Ney. 2004. Improvements
in phrase-based statistical machine translation. In
HLT-NAACL, pages 257–264, Boston, USA, May.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL, pages 97–105, Columbus, Ohio, June.
</reference>
<page confidence="0.999144">
71
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.376980">
<title confidence="0.7947495">NEWS 2009 Machine Transliteration Shared Task System Transliteration with Letter-to-Phoneme Technology</title>
<author confidence="0.718443">Cherry</author>
<affiliation confidence="0.907663">Microsoft</affiliation>
<address confidence="0.858033">One Microsoft Redmond, WA,</address>
<abstract confidence="0.99908985">We interpret the problem of transliterating English named entities into Hindi or Japanese Katakana as a variant of the letter-to-phoneme (L2P) subtask of textto-speech processing. Therefore, we apply a re-implementation of a state-of-the-art, discriminative L2P system (Jiampojamarn et al., 2008) to the problem, without further modification. In doing so, we hope to provide a baseline for the NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009), indicating how much can be achieved without transliteration-specific This paper briefly marizes the original work and our reimplementation. We also describe a bug in our submitted implementation, and provide updated results on the development and test sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Slaven Bilac</author>
<author>Hozumi Tanaka</author>
</authors>
<title>Extracting transliteration pairs from comparable corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the Annual Meeting of the Natural Language Processing Society,</booktitle>
<contexts>
<context position="1891" citStr="Bilac and Tanaka, 2005" startWordPosition="270" endWordPosition="273"> modeling the channel that transforms source language characters into transliterated target language characters is a key component to good performance. Since the primary signal followed by human transliterators is phonetic correspondence, it makes sense that a letter-to-phoneme (L2P) transcription engine would perform well at this task. Of course, transliteration is often framed within the larger problems of translation and bilingual named entity co-reference, making available a number of other interesting features, such as target lexicons (Knight and Graehl, 1998), distributional similarity (Bilac and Tanaka, 2005), or the dates of an entity’s mentions in the news (Klementiev and Roth, 2006). However, this task’s focus on generation has isolated the character-level component, which makes L2P technology a nearideal match. For our submission, we re-implement the L2P approach described by Jiampojamarn et al. (2008) as faithfully as possible, and apply it unmodified to the transliteration shared task for the English-to-Hindi (Kumaran and Kellner, 2007) and English-to-Japanese Katakana1 tests. 2 Approach 2.1 Summary of L2P approach The core of the L2P transduction engine is the dynamic programming algorithm </context>
</contexts>
<marker>Bilac, Tanaka, 2005</marker>
<rawString>Slaven Bilac and Hozumi Tanaka. 2005. Extracting transliteration pairs from comparable corpora. In Proceedings of the Annual Meeting of the Natural Language Processing Society, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="4057" citStr="Collins, 2002" startWordPosition="611" endWordPosition="612">emplates have two main parameters, the size S of the character window from which source context features are drawn, and the maximum length T of target n-gram indicators. We fit these parameters using grid search over 1-best 1Provided by http://www.cjk.org 69 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 69–71, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP ame → TA , ri → J , can → tS Figure 1: Example derivation transforming “American” into “1AJt✓S”. accuracy on the provided development sets. The engine’s features are trained using the structured perceptron (Collins, 2002). Jiampojamarn et al. (2008) show strong improvements in the L2P domain using MIRA in place of the perceptron update; unfortunately, we did not implement a k-best MIRA update due to time constraints. In our implementation, no special consideration was given to the availability of multiple correct answers in the training data; we always pick the first reference transliteration and treat it as the only correct answer. Investigating the use of all correct answers would be an obvious next step to improve the system. 2.2 Major differences in implementation Our system made two alternate design decis</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sittichai Jiampojamarn</author>
<author>Colin Cherry</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Joint processing and discriminative training for letter-to-phoneme conversion. In</title>
<date>2008</date>
<booktitle>ACL,</booktitle>
<pages>905--913</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2194" citStr="Jiampojamarn et al. (2008)" startWordPosition="319" endWordPosition="322">ngine would perform well at this task. Of course, transliteration is often framed within the larger problems of translation and bilingual named entity co-reference, making available a number of other interesting features, such as target lexicons (Knight and Graehl, 1998), distributional similarity (Bilac and Tanaka, 2005), or the dates of an entity’s mentions in the news (Klementiev and Roth, 2006). However, this task’s focus on generation has isolated the character-level component, which makes L2P technology a nearideal match. For our submission, we re-implement the L2P approach described by Jiampojamarn et al. (2008) as faithfully as possible, and apply it unmodified to the transliteration shared task for the English-to-Hindi (Kumaran and Kellner, 2007) and English-to-Japanese Katakana1 tests. 2 Approach 2.1 Summary of L2P approach The core of the L2P transduction engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). The main feature of this algorithm is its capability to transduce many consecutive characters with a single operation. This algorithm is used to conduct a search for a max-weight derivation according to a linear model with indicator features. A sample</context>
<context position="4085" citStr="Jiampojamarn et al. (2008)" startWordPosition="613" endWordPosition="617">o main parameters, the size S of the character window from which source context features are drawn, and the maximum length T of target n-gram indicators. We fit these parameters using grid search over 1-best 1Provided by http://www.cjk.org 69 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 69–71, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP ame → TA , ri → J , can → tS Figure 1: Example derivation transforming “American” into “1AJt✓S”. accuracy on the provided development sets. The engine’s features are trained using the structured perceptron (Collins, 2002). Jiampojamarn et al. (2008) show strong improvements in the L2P domain using MIRA in place of the perceptron update; unfortunately, we did not implement a k-best MIRA update due to time constraints. In our implementation, no special consideration was given to the availability of multiple correct answers in the training data; we always pick the first reference transliteration and treat it as the only correct answer. Investigating the use of all correct answers would be an obvious next step to improve the system. 2.2 Major differences in implementation Our system made two alternate design decisions (we do not claim improv</context>
</contexts>
<marker>Jiampojamarn, Cherry, Kondrak, 2008</marker>
<rawString>Sittichai Jiampojamarn, Colin Cherry, and Grzegorz Kondrak. 2008. Joint processing and discriminative training for letter-to-phoneme conversion. In ACL, pages 905–913, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Dan Roth</author>
</authors>
<title>Named entity transliteration and discovery from multilingual comparable corpora.</title>
<date>2006</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>82--88</pages>
<location>New York City, USA,</location>
<contexts>
<context position="1969" citStr="Klementiev and Roth, 2006" startWordPosition="284" endWordPosition="288">literated target language characters is a key component to good performance. Since the primary signal followed by human transliterators is phonetic correspondence, it makes sense that a letter-to-phoneme (L2P) transcription engine would perform well at this task. Of course, transliteration is often framed within the larger problems of translation and bilingual named entity co-reference, making available a number of other interesting features, such as target lexicons (Knight and Graehl, 1998), distributional similarity (Bilac and Tanaka, 2005), or the dates of an entity’s mentions in the news (Klementiev and Roth, 2006). However, this task’s focus on generation has isolated the character-level component, which makes L2P technology a nearideal match. For our submission, we re-implement the L2P approach described by Jiampojamarn et al. (2008) as faithfully as possible, and apply it unmodified to the transliteration shared task for the English-to-Hindi (Kumaran and Kellner, 2007) and English-to-Japanese Katakana1 tests. 2 Approach 2.1 Summary of L2P approach The core of the L2P transduction engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). The main feature of this a</context>
</contexts>
<marker>Klementiev, Roth, 2006</marker>
<rawString>Alexandre Klementiev and Dan Roth. 2006. Named entity transliteration and discovery from multilingual comparable corpora. In HLT-NAACL, pages 82–88, New York City, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<pages>24--4</pages>
<contexts>
<context position="1839" citStr="Knight and Graehl, 1998" startWordPosition="263" endWordPosition="266"> When attempting to automate machine transliteration, modeling the channel that transforms source language characters into transliterated target language characters is a key component to good performance. Since the primary signal followed by human transliterators is phonetic correspondence, it makes sense that a letter-to-phoneme (L2P) transcription engine would perform well at this task. Of course, transliteration is often framed within the larger problems of translation and bilingual named entity co-reference, making available a number of other interesting features, such as target lexicons (Knight and Graehl, 1998), distributional similarity (Bilac and Tanaka, 2005), or the dates of an entity’s mentions in the news (Klementiev and Roth, 2006). However, this task’s focus on generation has isolated the character-level component, which makes L2P technology a nearideal match. For our submission, we re-implement the L2P approach described by Jiampojamarn et al. (2008) as faithfully as possible, and apply it unmodified to the transliteration shared task for the English-to-Hindi (Kumaran and Kellner, 2007) and English-to-Japanese Katakana1 tests. 2 Approach 2.1 Summary of L2P approach The core of the L2P trans</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>Tobias Kellner</author>
</authors>
<title>A generic framework for machine transliteration.</title>
<date>2007</date>
<booktitle>In Proc. of the 30th SIGIR.</booktitle>
<contexts>
<context position="2333" citStr="Kumaran and Kellner, 2007" startWordPosition="339" endWordPosition="342">amed entity co-reference, making available a number of other interesting features, such as target lexicons (Knight and Graehl, 1998), distributional similarity (Bilac and Tanaka, 2005), or the dates of an entity’s mentions in the news (Klementiev and Roth, 2006). However, this task’s focus on generation has isolated the character-level component, which makes L2P technology a nearideal match. For our submission, we re-implement the L2P approach described by Jiampojamarn et al. (2008) as faithfully as possible, and apply it unmodified to the transliteration shared task for the English-to-Hindi (Kumaran and Kellner, 2007) and English-to-Japanese Katakana1 tests. 2 Approach 2.1 Summary of L2P approach The core of the L2P transduction engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). The main feature of this algorithm is its capability to transduce many consecutive characters with a single operation. This algorithm is used to conduct a search for a max-weight derivation according to a linear model with indicator features. A sample derivation is shown in Figure 1. There are two main categories of features: context and transition features, which follow the first two fe</context>
</contexts>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>A. Kumaran and Tobias Kellner. 2007. A generic framework for machine transliteration. In Proc. of the 30th SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>A Kumaran</author>
<author>Vladimir Pervouchine</author>
<author>Min Zhang</author>
</authors>
<title>machine transliteration shared task.</title>
<date>2009</date>
<journal>Report on NEWS</journal>
<booktitle>In Proceedings of ACLIJCNLP 2009 Named Entities Workshop (NEWS</booktitle>
<contexts>
<context position="700" citStr="Li et al., 2009" startWordPosition="93" endWordPosition="96">th Letter-to-Phoneme Technology Colin Cherry and Hisami Suzuki Microsoft Research One Microsoft Way Redmond, WA, 98052 {colinc,hisamis}@microsoft.com Abstract We interpret the problem of transliterating English named entities into Hindi or Japanese Katakana as a variant of the letter-to-phoneme (L2P) subtask of textto-speech processing. Therefore, we apply a re-implementation of a state-of-the-art, discriminative L2P system (Jiampojamarn et al., 2008) to the problem, without further modification. In doing so, we hope to provide a baseline for the NEWS 2009 Machine Transliteration Shared Task (Li et al., 2009), indicating how much can be achieved without transliteration-specific technology. This paper briefly summarizes the original work and our reimplementation. We also describe a bug in our submitted implementation, and provide updated results on the development and test sets. 1 Introduction Transliteration occurs when a word is borrowed into a language with a different character set from its language of origin. The word is transcribed into the new character set in a manner that maintains phonetic correspondence. When attempting to automate machine transliteration, modeling the channel that trans</context>
</contexts>
<marker>Li, Kumaran, Pervouchine, Zhang, 2009</marker>
<rawString>Haizhou Li, A. Kumaran, Vladimir Pervouchine, and Min Zhang. 2009. Report on NEWS 2009 machine transliteration shared task. In Proceedings of ACLIJCNLP 2009 Named Entities Workshop (NEWS 2009), Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Improvements in phrase-based statistical machine translation.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>257--264</pages>
<location>Boston, USA,</location>
<contexts>
<context position="2541" citStr="Zens and Ney, 2004" startWordPosition="370" endWordPosition="373">entions in the news (Klementiev and Roth, 2006). However, this task’s focus on generation has isolated the character-level component, which makes L2P technology a nearideal match. For our submission, we re-implement the L2P approach described by Jiampojamarn et al. (2008) as faithfully as possible, and apply it unmodified to the transliteration shared task for the English-to-Hindi (Kumaran and Kellner, 2007) and English-to-Japanese Katakana1 tests. 2 Approach 2.1 Summary of L2P approach The core of the L2P transduction engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). The main feature of this algorithm is its capability to transduce many consecutive characters with a single operation. This algorithm is used to conduct a search for a max-weight derivation according to a linear model with indicator features. A sample derivation is shown in Figure 1. There are two main categories of features: context and transition features, which follow the first two feature templates described by Jiampojamarn et al. (2008). Context features are centered around a transduction operation. These features include an indicator for the operation itself, which is then conjoined wi</context>
</contexts>
<marker>Zens, Ney, 2004</marker>
<rawString>Richard Zens and Hermann Ney. 2004. Improvements in phrase-based statistical machine translation. In HLT-NAACL, pages 257–264, Boston, USA, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Zhang</author>
<author>Chris Quirk</author>
<author>Robert C Moore</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of noncompositional phrases with synchronous parsing.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>97--105</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="5233" citStr="Zhang et al., 2008" startWordPosition="802" endWordPosition="805"> Our system made two alternate design decisions (we do not claim improvements) over those made by (Jiampojamarn et al., 2008), mostly based on the availability of software. First, we employed a beam of 40 candidates in our decoder, to enable efficient use of large language model contexts. This is put to good use in the Hindi task, where we found n-gram indicators of length up to n = 6 provided optimal development performance. Second, we employed an alternate character aligner to create our training derivations. This aligner is similar to recent non-compositional phrasal word-alignment models (Zhang et al., 2008), limited so it can only produce monotone character alignments. The aligner creates substring alignments, without insertion or deletion operators. As such, an aligned transliteration pair also serves as a transliteration derivation. We employed a maximum substring length of 3. The training data was heuristically cleaned after alignment. Any derivation found by the aligner that uses an operation occurring fewer than 3 times throughout the entire training set was eliminated. This reduced training set sizes to 8,511 pairs for English-Hindi and 20,306 pairs for EnglishKatakana. Table 1: Developmen</context>
</contexts>
<marker>Zhang, Quirk, Moore, Gildea, 2008</marker>
<rawString>Hao Zhang, Chris Quirk, Robert C. Moore, and Daniel Gildea. 2008. Bayesian learning of noncompositional phrases with synchronous parsing. In ACL, pages 97–105, Columbus, Ohio, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>