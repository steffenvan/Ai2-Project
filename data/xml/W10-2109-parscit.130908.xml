<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003458">
<title confidence="0.940709">
No sentence is too confusing to ignore
</title>
<author confidence="0.997576">
Paul Cook
</author>
<affiliation confidence="0.902061">
Department of Computer Science
University of Toronto
Toronto, Canada
</affiliation>
<email confidence="0.996873">
pcook@cs.toronto.edu
</email>
<author confidence="0.993569">
Suzanne Stevenson
</author>
<affiliation confidence="0.901860666666667">
Department of Computer Science
University of Toronto
Toronto, Canada
</affiliation>
<email confidence="0.998529">
suzanne@cs.toronto.edu
</email>
<sectionHeader confidence="0.993893" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9806073">
We consider sentences of the form No
X is too Y to Z, in which X is a noun
phrase, Y is an adjective phrase, and Z
is a verb phrase. Such constructions are
ambiguous, with two possible (and oppo-
site!) interpretations, roughly meaning ei-
ther that “Every X Zs”, or that “No X Zs”.
The interpretations have been noted to de-
pend on semantic and pragmatic factors.
We show here that automatic disambigua-
tion of this pragmatically complex con-
struction can be largely achieved by us-
ing features of the lexical semantic prop-
erties of the verb (i.e., Z) participating in
the construction. We discuss our experi-
mental findings in the context of construc-
tion grammar, which suggests a possible
account of this phenomenon.
1 No noun is too adjective to verb
Consider the following two sentences:
</bodyText>
<listItem confidence="0.997771">
(1) No interest is too narrow to deserve its own
newsletter.
(2) No item is too minor to escape his attention.
</listItem>
<bodyText confidence="0.995722363636364">
Each of these sentences has the form of No Xis too
Y to Z, where X, Y, and Z are a noun phrase, ad-
jective phrase, and verb phrase, respectively. Sen-
tence (1) is generally taken to mean that every in-
terest deserves its own newsletter, regardless of
how narrow it is. On the other hand, (2) is typi-
cally interpreted as meaning that no item escapes
his attention, regardless of how minor it is. That
is, sentences with the identical form of No X is too
Y to Z either can mean that “every X Zs”, or can
mean the opposite—that “no X Zs”!1
</bodyText>
<footnote confidence="0.746535">
1Note that in examples (1) and (2), the nouns interest and
item are the subjects of the verbs deserve and escape, respec-
</footnote>
<bodyText confidence="0.999764894736842">
This “verbal illusion” (Wason and Reich, 1979),
so-called because there are two opposite inter-
pretations for the very same structure, is of in-
terest to us for two reasons. First, the con-
tradictory nature of the possible meanings has
been explained in terms of pragmatic factors con-
cerning the relevant presuppositions of the sen-
tences. According to Wason and Reich (1979)
(as explained in more detail below), sentences
such as (2) are actually nonsensical, but people
coerce them into a sensible reading by revers-
ing the interpretation. One of our goals in this
work is to explore whether computational lin-
guistic techniques—specifically automatic corpus
analysis drawing on lexical resources—can help
to elucidate the factors influencing interpretation
of such sentences across a collection of actual us-
ages.
The second reason for our interest in this con-
struction is that it illustrates a complex ambigu-
ity that can cause difficulty for natural language
processing applications that seek to semantically
interpret text. Faced with the above two sen-
tences, a parsing system (in the absence of spe-
cific knowledge of this construction) will presum-
ably find the exact same structure for each, giv-
ing no basis on which to determine the correct
meaning from the parse. (Unsurprisingly, when
we run the C&amp;C Parser (Curran et al., 2007) on (1)
and (2) it assigns the same structure to each sen-
tence.) Our second goal in this work is thus to ex-
plore whether increased linguistic understanding
of this phenomenon could be used to disambiguate
such examples automatically. Specifically, we use
this construction as an example of the kind of
difficulties faced in semantic interpretation when
meaning may be determined by pragmatic or other
extra-syntactic factors, in order to explore whether
</bodyText>
<footnote confidence="0.986823333333333">
tively. In this construction the noun can also be the object of
the verb, as in the title of this paper which claims no sentence
can/should be ignored.
</footnote>
<page confidence="0.978023">
61
</page>
<note confidence="0.979547">
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 61–69,
Uppsala, Sweden, 16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.996118385416667">
lexical semantic features can be used as cues to
resolving pragmatic ambiguity when a complex
semantico-pragmatic model is not feasible.
In the remainder of this paper, we present the
first computational study of the No X is too Y to
Z phenomenon, which attempts to automatically
determine the meaning of instances of this seman-
tically and pragmatically complex construction. In
Section 2 we present previous analyses of this
construction, and our hypothesis. In Section 3,
we describe the creation of a dataset of instances
that verifies that both interpretations (“every” and
“no”) indeed occur in corpora. We then analyze
the human annotations in this dataset in more de-
tail in Section 4. In Section 5, we present the fea-
ture model we use to describe the instances, which
taps into the lexical semantics and polarity of the
constituents. In Section 6, we describe machine
learning experiments and classification results that
support our hypothesis that the interpretation of
this construction largely depends on the semantics
of its component verb. In Section 7 we suggest that
our results support an analysis of this phenomenon
within construction grammar, and point to some
future directions in our research in Section 8.
2 Background and our proposal
The No X is too Y to Z construction was investi-
gated by Wason and Reich (1979), and discussed
more recently by Pullum (2004) and Liberman
(2009a,b). Here we highlight some of the most
important properties of this complex phenomenon.
Our presentation owes much to the lucid discus-
sion and clarification of this topic, and of the work
of Wason and Reich specifically, by Liberman.
Wason and Reich argue that the compositional
interpretation of sentences of the form of (1) and
(2) is “every X Zs”. Intuitively, this can be under-
stood by considering a sentence identical to sen-
tence (1), but without a negative subject: This in-
terest is too narrow to deserve its own newslet-
ter, which means that “this interest is so narrow
that it does not deserve a newsletter”. This ex-
ample indicates that the meaning of too narrow
to deserve its own newsletter is “so narrow that
it does not deserve a newsletter”. When this neg-
ative “too” assertion is compositionally combined
with the No interest subject of sentence (1), it re-
sults in a meaning with two negatives: “No inter-
est is so narrow that it does not deserve a newslet-
ter”, or simply, “Every interest deserves a newslet-
ter”. Wason and Reich note that in sentences such
as (1), the compositional “every” interpretation is
consistent with common beliefs about the world,
and thus refer to such sentences as “pragmatic”.
By contrast, the compositional interpretation of
sentences such as (2) does not correspond to our
common sense beliefs. Consider an analogous
(non-negative subject) sentence to sentence (2)—
i.e., This item is too minor to escape his attention.
It is nonsensical that “This item is so minor that
it does not escape his attention”, since being more
“minor” entails more likelihood of escaping atten-
tion, not less. The compositional interpretation of
(2) is similarly nonsensical—i.e., that “No item
is so minor that it does not escape his attention”;
Such sentences are thus termed “non-pragmatic”
by Wason and Reich, who argue that the com-
plexity of the non-pragmatic sentences—arising in
part due to the number of negations they contain—
causes the listener or reader to misconstrue them.
According to their reasoning, listeners choose an
interpretation that is consistent with their beliefs
about the world—namely that “no X Zs”, in this
case that “No item escapes his attention”—instead
of the compositional interpretation (“Every item
escapes his attention”).
While Wason and Reich focus on the compo-
sitional semantics and pragmatics of these sen-
tences, they also note that the non-pragmatic ex-
amples typically use a verb that itself has some
aspect of negation, such as ignore, miss, and over-
look. This property is also pointed out by Pullum
(2004), who notes that avoid in his example of
the construction means “manage to not do” some-
thing. Building on this observation, we hypothe-
size that lexical properties of the component con-
stituents of this construction, particularly the verb,
can be important cues to its semantico-pragmatic
interpretation. Specifically, we hypothesize that
the pragmatic (“every” interpretation) and non-
pragmatic (“no” interpretation) sentences will tend
to involve verbs with different semantics. Given
that verbs of different semantic classes have differ-
ent selectional preferences, we also expect to see
the “every” and “no” sentences associated with se-
mantically different nouns and adjectives.
</bodyText>
<sectionHeader confidence="0.998191" genericHeader="keywords">
3 Dataset
</sectionHeader>
<subsectionHeader confidence="0.99025">
3.1 Extraction
</subsectionHeader>
<bodyText confidence="0.9395275">
To create a dataset of usages of the construction
no NP is too AP to VP—referred to as the tar-
</bodyText>
<page confidence="0.998501">
62
</page>
<bodyText confidence="0.991079039215686">
get construction—we use two corpora: the British
National Corpus (Burnard, 2000), an approxi-
mately one hundred million word corpus of late-
twentieth century British English, and The New
York Times Annotated Corpus (Sandhaus, 2008),
approximately one billion words of non-newswire
text from the New York Times from the years
1987–2006. We extract all sentences in these cor-
pora containing the sequence of strings no, is too,
and to separated by one or more words. We then
manually filter all sentences that do not have no
NP as the subject of is too, or that do not have to
VP as an argument of is too. After removing dupli-
cates, this results in 170 sentences. We randomly
select 20 of these sentences for development data,
leaving 150 sentences for testing.
Although we find only 170 examples of the
target construction in 1.1 billion words of text,
note that our extraction process is quite strict and
misses some relevant usages. For example, we do
not extract sentences of the form Nothing is too Y
to Z in which the subject NP does not contain the
word no. Nor do we extract usages of the related
construction No X is too Yfor Z, where Z is an NP
related to a verb, as in No interest is too narrow
for attention. (We would only extract the latter if
there were an infinitive verb embedded in or fol-
lowing the NP.) In the present study we limit our
consideration to sentences of the form discussed
by Wason and Reich (1979), but intend to con-
sider related constructions such as these—which
appear to exhibit the same ambiguity as the target
construction—in the future.
We next manually identify the noun, adjective,
and verb that participate in the target construction
in each sentence. Although this could be done au-
tomatically using a parser (e.g., Collins, 2003) or
chunker (e.g., Abney, 1991), here we want to en-
sure error-free identification. We also note a num-
ber of sentences containing co-ordination, such as
in the following example.
(3) These days, no topic is too recent or
specialized to disqualify it from museum
apotheosis.
This sentence contains two instances of the tar-
get construction: one corresponding to the noun-
adjective-verb triple topic, recent, disqualify, and
the other to the triple topic, specialized, disqual-
ify. In general, we consider each unique noun-
adjective-verb triple participating in the target con-
struction as a separate instance.
</bodyText>
<subsectionHeader confidence="0.999373">
3.2 Annotation
</subsectionHeader>
<bodyText confidence="0.985829814814815">
We used Amazon Mechanical Turk (AMT,
https://www.mturk.com/)to obtain judge-
ments as to the correct interpretation of each in-
stance of the target construction in both the devel-
opment and testing datasets. For each instance, we
generated two paraphrases, one corresponding to
each of the interpretations discussed in Section 1.
We then presented the given instance of the target
construction along with its two paraphrases to an-
notators through AMT, as shown in Table 1. In
generating the paraphrases, one of the authors se-
lected the most appropriate paraphrase, in their
judgement, where can in the paraphrases in Ta-
ble 1 was selected from can, should, will, and ∅.
Note that the paraphrases do not contain the ad-
jective from the target construction. In the case of
multiple instances of the target construction with
differing adjectives but the same noun and verb,
we only solicited judgements for one instance, and
used these judgements for the other instances. In
our dataset we observe that all instances obtained
from the same sentence which differ only with re-
spect to their noun or verb have the same inter-
pretation. We therefore believe that instances with
the same noun and verb but a different adjective
are unlikely to differ in their interpretation.
Instructions:
</bodyText>
<listItem confidence="0.986423857142857">
• Read the sentence below.
• Based on your interpretation of that sen-
tence, select the answer that most closely
matches your interpretation.
• Select “I don’t know” if neither answer is
close to your interpretation, or if you are
really unsure.
</listItem>
<bodyText confidence="0.904736666666667">
That success was accomplished in large part to
tight control on costs , and no cost is too small
to be scrutinized .
</bodyText>
<listItem confidence="0.999783">
• Every cost can be scrutinized.
• No cost can be scrutinized.
• I don’t know.
</listItem>
<bodyText confidence="0.906253">
Enter any feedback you have about this HIT. We
greatly appreciate you taking the time to do so.
</bodyText>
<tableCaption confidence="0.9749875">
Table 1: A sample of the Amazon Mechanical
Turk annotation task.
</tableCaption>
<page confidence="0.998851">
63
</page>
<bodyText confidence="0.999983880952381">
We also allowed the judges to optionally enter
any feedback about the annotation task which in
some cases—discussed in the following section—
was useful in determining whether the judges
found a particular instance difficult to annotate.2
For each instance of the target construction we
obtained three judgements from unique workers
on AMT. For approximately 80% of the items,
the judgements were unanimous. In the remaining
cases we solicited four additional judgements, and
used the majority judgement. We paid $0.05 per
judgement; the average time spent on each annota-
tion was approximately twenty seconds, resulting
in an average hourly wage of about $10.
The development data was also annotated by
three native English speaking experts (compu-
tational linguists with extensive linguistic back-
ground, two of whom are also authors of this pa-
per). The inter-annotator agreement among these
judges is very high, with pairwise observed agree-
ments of 1.00, 0.90, and 0.90, and corresponding
unweighted Kappa scores of 1.00, 0.79, and 0.79.
The majority judgements of these annotators are
the same as those obtained from AMT on the de-
velopment data, giving us confidence in the reli-
ability of the AMT judgements. These findings
are consistent with those of Snow et al. (2008) in
showing that AMT judgements can be as reliable
as those of expert judges.
Finally, we remove a small number of items
from the testing dataset which were difficult to
paraphrase due to ellipsis of the verb participating
in the target construction, or an extra negation in
the verb phrase. We further remove one sentence
because we believe the paraphrases we provided
are in fact misleading. The number of sentences
and of instances (i.e., noun-verb-adjective triples)
of the target construction in the development and
testing datasets is given in Table 2. 160 of the 199
testing instances (80%) have the “every” interpre-
tation, with the remainder having the “no” inter-
pretation.
</bodyText>
<sectionHeader confidence="0.531649" genericHeader="introduction">
4 Analysis of annotation
</sectionHeader>
<bodyText confidence="0.999535">
We now more closely examine the annotations ob-
tained from AMT to better determine the extent to
</bodyText>
<footnote confidence="0.994264833333333">
2In other cases the comments were more humourous. In
response to the following sentence If you’ve ever yearned
to live on Sesame Street, where no problem is too big to be
solved by a not-too-big slice of strawberry-rhubarb pie, this
is the spot for you, one judge told us her preferred types of
pie.
</footnote>
<table confidence="0.944866333333333">
Dataset # sentences # instances
Development 20 33
Test 140 199
</table>
<tableCaption confidence="0.972512">
Table 2: The number of sentences containing the
</tableCaption>
<bodyText confidence="0.997067214285714">
target construction, and the number of resulting in-
stances.
which they are reliable. We also consider specific
instances of the target construction that are judged
inconsistently to establish some of the causes of
disagreement.
One of the three experts who annotated the de-
velopment items (discussed in Section 3.2) also
annotated twenty items selected at random from
the testing data. In this case two instances are
judged differently than the majority judgement ob-
tained from AMT. These instances are given below
with the noun, adjective and verb in the target con-
struction underlined.
</bodyText>
<listItem confidence="0.699193714285714">
(4) When it comes to the clash of candidates on
national television, no detail, it seems, is too
minor for negotiation, no risk too small to
eliminate.
(5) Lectures by big-name Wall Street felons will
show why no swindler is too big to beat the
rap by peaching on small-timers.
</listItem>
<bodyText confidence="0.99998065">
For sentence (4), the AMT judgements were unan-
imously for the “no” interpretation whereas the
expert annotator chose the “every” interpretation.
We are uncertain as to the reason for this disagree-
ment, but are convinced that the “every” interpre-
tation is the intended one.
In the case of sentence (5), the AMT judge-
ments were split four–three for the “every” and
“no” interpretations, respectively, while the ex-
pert annotator chose the “no” interpretation. For
this sentence the provided paraphrases were Ev-
ery swindler can beat the rap and No swindler
can beat the rap. If attention in the sentence
is restricted to the target construction—i.e., no
swindler is too big to beat the rap by peaching
on small-timers—either of the “no” and “every”
interpretations is possible. That is, this clause
alone can mean that “no swindler is ‘big’ enough
to be able to beat the rap” (the “no” interpreta-
tion), or that “no swindler is ‘big’ enough that they
</bodyText>
<page confidence="0.997186">
64
</page>
<bodyText confidence="0.998717115384615">
are above peaching on small-timers” (or in other
words, “every swindler is able to beat the rap by
peaching on small-timers”, the “every” interpreta-
tion). However, the intention of the sentence as the
“no” interpretation is clear from the referral in the
main clause to big-name Wall Street felons, which
implies that “big” swindlers have not beaten the
rap. Since the AMT annotators may not be devot-
ing a large amount of attention to the task, they
may focus only on the target construction and not
the preliminary disambiguating material. In this
event, they may be choosing between the “every”
and “no” interpretations based on how cynical they
are of the ability (or lack thereof) of the American
legal system to punish Wall Street criminals.
We also examine a small number of examples
in the testing set which do not receive a clear
majority judgement from AMT. For this analysis
we consider items for which the difference in the
number of judgements for each of the “every” and
the “no” interpretations is one or less This gives
four instances of the target construction, one of
which we have already discussed above, example
(5); the others are presented below, again with the
noun, adjective, and verb participating in the target
construction underlined:
</bodyText>
<listItem confidence="0.8177015">
(6) Where are our priorities when we so
carefully weigh costs and medical efficacy in
deciding to offer a medical lifeline to the
elderly, yet no amount of money is too great
to spend on the debatable paths we’ve taken
in our war against terror?
(7) No neighborhood is too remote to diminish
Mr. Levine’s determination to discover and
announce some previously unheralded treat.
(8) No one is too remote anymore to be
concerned about style, Ms. Hansen
suggested.
</listItem>
<bodyText confidence="0.999923384615385">
In example (6) the author is using the target con-
struction to express somebody else’s viewpoint
that “any amount should be spent on the war
against terror”. Therefore the literal reading of
the target construction appears to be the “every”
interpretation. However, this construction is be-
ing used rhetorically (as part of the overall sen-
tence) to express the author’s belief that “too much
money is being spent on the war against terror”,
which is close in meaning to the “no” interpreta-
tion. It appears that the annotators are split be-
tween these two readings. For sentence (7) the
atypicality of neighbourhood as the subject of di-
minish may make this instance particularly diffi-
cult for the judges. Sentence (8) appears to us to be
a clear example of the “every” interpretation. The
paraphrases for this usage are “Everyone should
be concerned about style” and “No one should be
concerned about style”. In this case it is possible
that the judges are biased by their beliefs about
whether one should be concerned about style, and
that this is giving rise to the lack of agreement.
These examples illustrate that some of these us-
ages are clearly complex for people to annotate.
Such complex examples may require more context
to be annotated with confidence.
</bodyText>
<sectionHeader confidence="0.997057" genericHeader="method">
5 Model
</sectionHeader>
<bodyText confidence="0.999970379310344">
To test our hypothesis that the interaction of the se-
mantics of the noun, adjective, and verb in the tar-
get construction contributes to its pragmatic inter-
pretation, we represent each instance in our dataset
as a vector of features that capture aspects of the
semantics of its component words.
WordNet To tap into general lexical semantic
properties of the words in the construction, we
use features that draw on the semantic classes of
words in WordNet (Fellbaum, 1998). These bi-
nary features each represent a synset in WordNet,
and are turned on or off for the component words
(the noun, adjective, and verb) in each instance
of the target construction. A synset feature is on
for a word if the synset occurs on the path from
all senses of the word to the root, and off other-
wise. We use WordNet version 3.0 accessed using
NLTK version 2.0 (Bird et al., 2009).
Polarity Because of the observation that the
verb in the target construction, in particular, has
some property of negativity in the “no” interpre-
tation, we also use features representing the se-
mantic polarity of the noun, adjective, and verb
in each instance. The features are tertiary, repre-
senting positive, neutral, or negative polarity. We
obtain polarity information from the subjectivity
lexicon provided by Wilson et al. (2005), and con-
sider words to be neutral if they have both positive
and negative polarity, or are not in the lexicon.
</bodyText>
<sectionHeader confidence="0.996602" genericHeader="method">
6 Experimental results
</sectionHeader>
<subsectionHeader confidence="0.947579">
6.1 Experimental setup
</subsectionHeader>
<bodyText confidence="0.999813">
To evaluate our model we conduct a 5-fold cross-
validation experiment using the items in the test-
</bodyText>
<page confidence="0.999169">
65
</page>
<bodyText confidence="0.999968125">
ing dataset. When partitioning the items in the
testing dataset into the five parts necessary for the
cross-validation experiment, we ensure that all the
instances of the target construction from a single
sentence are in the same part. This ensures that
no instance used for training is from the same sen-
tence as an instance used for testing. We further
ensure that the proportion of items in each class is
roughly the same in each split.
For each of the five runs, we linearly scale the
training data to be in the range [−1, 1], and ap-
ply the same transformation to the testing data.
We train a support vector machine (LIBSVM ver-
sion 2.9, Chang and Lin, 2001) with a radial ba-
sis function kernel on the training portion in each
run, setting the cost and gamma parameters using
cross-validation on just the training portion, and
then test the classifier on the testing portion for
that run using the same parameter settings. We
micro-average the accuracy obtained on each of
the five runs. Finally, we repeat each 5-fold cross-
validation experiment five times, with five random
splits, and report the average accuracy over these
trials.
</bodyText>
<subsectionHeader confidence="0.820645">
6.2 Results
</subsectionHeader>
<bodyText confidence="0.999929514285714">
Results for experiments using various subsets of
the features are presented in Table 3. We re-
strict the component word—the noun, adjective, or
verb—for which we extract features to those listed
in column “Word”, and extract only the features
given in column “Features” (WordNet, polarity, or
all). The majority baseline is 80%, corresponding
to always selecting the “every” interpretation. Ac-
curacies shown in boldface are significantly better
than the majority class baseline using a paired t-
test. (In all cases where the difference is signifi-
cant, we obtain p ≪ 0.01.)
We first consider the results using features ex-
tracted only for the noun, adjective, or verb indi-
vidually, using all features. The best accuracy in
this group of experiments, 87%, is achieved using
the verb features, and is significantly higher than
the majority baseline. On the other hand, the clas-
sifiers trained on the noun and adjective features
individually perform no better than the baseline.
These results support our hypothesis that lexical
semantic properties of the component verb in the
No X is too Y to Z construction do indeed play
an important role in determining its interpretation.
Although we proposed that selectional constraints
from the verb would also lead to differing seman-
tics of the nouns and adjectives in the two interpre-
tations, our WordNet features are likely too sim-
plistic to capture this effect, if it does hold. Before
ruling out the semantic contribution of these words
to the interpretation, we need to explore whether
a more sophisticated model of selectional prefer-
ences, as in Ciaramita and Johnson (2000) or Clark
and Weir (2002), yields more informative features
for the noun and adjective.
</bodyText>
<table confidence="0.999727888888889">
Experimental setup % accuracy
Word Features
Noun All 80
Adjective All 80
Verb All 87
All WordNet 88
All Polarity 80
All All 88
Majority baseline 80
</table>
<tableCaption confidence="0.996067">
Table 3: % accuracy on testing data for each exper-
</tableCaption>
<bodyText confidence="0.997539954545454">
imental condition and the majority baseline. Ac-
curacies in boldface are statistically significantly
different from the baseline.
We now consider the results using the WordNet
and polarity features individually, but extracted for
all three component words. The WordNet features
perform as well as the best results using all fea-
tures for all three words, which gives further sup-
port to our hypothesis that the semantics of the
components of the target construction are related
to its interpretation. The polarity features perform
poorly. This is perhaps unsurprising as polarity is
a poor approximation to the property of “negativ-
ity” that we are attempting to capture. Moreover,
many of the nouns, adjectives, and verbs in our
dataset either have neutral polarity or are not in
the polarity lexicon, and therefore the polarity fea-
tures are not very discriminative. In future work,
we plan to examine the WordNet classes of the
verbs that occur in the “no” interpretation to try to
more precisely characterize the property of nega-
tivity that these verbs tend to have.
</bodyText>
<subsectionHeader confidence="0.99866">
6.3 Error analysis
</subsectionHeader>
<bodyText confidence="0.9998762">
To better understand the errors our classifier is
making, we examine the specific instances which
are classified incorrectly. Here we focus on the
experiment using all features for all three com-
ponent words. There are 23 instances which are
</bodyText>
<page confidence="0.977917">
66
</page>
<bodyText confidence="0.999880857142857">
consistently mis-classified in all runs of the exper-
iment. According to the AMT judgements, each of
these instances corresponds to the “no” interpreta-
tion. These errors reflect the bias of the classifier
towards the more frequent class, the “every” inter-
pretation.
We further note that two of the instances dis-
cussed in Section 4—examples (4) and (6)—are
among those instances consistently classified in-
correctly. The majority judgement from AMT for
both of these instances is the “no” interpretation,
while in our assessment they are in fact the “ev-
ery” interpretation. We are therefore not surprised
to see these items “mis-classified” as “every”.
Example (8) was incorrectly classified in one
trial. In this case we agree with the gold-standard
label obtained from AMT in judging this instance
as the “every” interpretation; nevertheless, this
does appear to be a difficult instance given the low
agreement observed for the AMT judgements.
It is interesting that no items with an “every” in-
terpretation are consistently misclassified. In the
context of our overall results showing the impact
of the verb features on performance, we conclude
that the “no” interpretation arises due to particular
lexical semantic properties of certain verbs. We
suspect then that the consistent errors on the 21
truly misclassified expressions (23 minus the 2 in-
stances discussed above that we believe to be an-
notated incorrectly) are due to sparse data. That
is, if it is indeed the verb that plays a major role in
leading to a “no” interpretation, there may simply
be insufficient numbers of such verbs for training
a supervised model in a dataset with only 39 ex-
amples of those usages.
</bodyText>
<sectionHeader confidence="0.997162" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999981538461538">
We have presented the first computational study of
the semantically and pragmatically complex con-
struction No X is too Y to Z. We have developed
a computational model that automatically disam-
biguates the construction with an accuracy of 88%,
reducing the error-rate over the majority-baseline
by 40%. The model uses features that tap into the
lexical semantics of the component words partic-
ipating in the construction, particularly the verb.
These results demonstrate that lexical properties
can be successful in resolving an ambiguity pre-
viously thought to depend on complex pragmatic
inference over presuppositions (as in Wason and
Reich (1979)).
These results can be usefully situated within
the context of linguistic and psycholinguistic work
on semantic interpretation processing. Beginning
around 20 years ago, work in modeling of human
semantic preferences has focused on the extent to
which properties of lexical items influence the in-
terpretation of various linguistic ambiguities (e.g.,
Trueswell and Tanenhaus, 1994). While semantic
context and plausibility are also proposed to play
a role in human interpretation of ambiguous sen-
tences (e.g., Crain and Steedman, 1985; Altmann
and Steedman, 1988), it has been pointed out that
it would be difficult to “operationalize” the com-
plex interactions of presuppositional factors with
real-world knowledge in a precise algorithm for
disambiguation (Jurafsky, 1996). Although not in-
tended as proposing a cognitive model, the work
here can be seen as connected to these lines of re-
search, in investigating the extent to which lexical
factors can be used as proxies to more “hidden”
features that underlie the appropriate interpreta-
tion of a pragmatically complex construction.
Moreover, as in the approach of Jurafsky
(1996), the phenomenon we investigate here may
be best considered within a constructional analy-
sis (e.g., Langacker, 1987), in which both the syn-
tactic construction and the particular lexical items
contribute to the determination of the meaning of a
usage. We suggest that a clause of the form No Xis
too Y to Z might be the (identical) surface expres-
sion of two underlying constructions—one with
the “every” interpretation and one with the “no”
interpretation—which place differing constraints
on the semantics of the verb. (E.g., in the “no”
interpretation, the verb typically has some “neg-
ative” semantic property, as noted in Section 2.)
Looked at from the other perspective, the lexical
semantic properties of the verb might determine
which No X is too Y to Z construction (and associ-
ated interpretation) it is compatible with. Our re-
sults support this view, by showing that semantic
classes of verbs have predictive value in selecting
the correct interpretation.
Note that such a constructional analysis of
this phenomenon assumes that both interpretations
of these sentences are linguistically valid, given
the appropriate lexical instantiation. This stands
in contrast to the analysis of Wason and Reich
(1979), which presumes that people are apply-
ing some higher-level reasoning to “correct” an
ill-formed statement in the case of the “no” in-
</bodyText>
<page confidence="0.998205">
67
</page>
<bodyText confidence="0.9998185">
terpretation. While such extra-grammatical infer-
ence may play a role in support of language under-
standing when people are faced with noisy data, it
seems unlikely to us that a construction that is used
quite readily and with a predictable interpretation
is nonsensical according to rules of grammar. Our
results point to an alternative linguistic analysis,
one whose further development may also help to
improve automatic disambiguation of instances of
No X is too Y to Z. In the next section, we discuss
directions for future work that could elaborate on
these preliminary findings.
</bodyText>
<sectionHeader confidence="0.999291" genericHeader="discussions">
8 Future Work
</sectionHeader>
<bodyText confidence="0.999974">
One limitation of this study is that the dataset used
is rather small, consisting of just 199 instances
of the target construction. As discussed in Sec-
tion 3.1, the extraction process we use to obtain
our experimental items has low recall; in particular
it misses variants of the target construction such as
Nothing is too Y to Z and No X is too Yfor Z. In
the future we intend to expand our dataset by ex-
tracting such usages. Furthermore, the data used
in the present study is primarily taken from news
text. While we do not adopt the view of some that
usages of the target construction having the “no”
interpretation are errors, it could be the case that
such usages are more frequent in less formal text.
In the future we also intend to extract usages of
the target construction from datasets of less formal
text, such as blogs (e.g., Burton et al., 2009).
Constructions other than No X is too Y to Z ex-
hibit a similar ambiguity. For example, the con-
struction X didn’t wait to Y is ambiguous between
“X did Y right away” and “X didn’t do Y at all”
(Karttunen, 2007). In the future we would like to
extend our study to consider more such construc-
tions which are ambiguous due to the interpreta-
tion of negation.
In Section 4 we note that for some instances the
complexity of the sentences containing the target
construction may make it difficult for the anno-
tators to judge the meaning of the target. In the
future we intend to present simplified versions of
these sentences—which retain the noun, adjective,
and verb from the target construction in the orig-
inal sentence—to the judges to avoid this issue.
Such an approach will also help us to focus more
clearly on observable lexical semantic effects.
We are particularly interested in further explor-
ing the hypothesis that it is the semantics of the
component verb that gives rise to the meaning of
the target construction. Recall Pullum’s (2004)
observation that the verb in the “no” interpretation
involves explicitly not acting. Using this intuition,
we have informally observed that it is largely pos-
sible to (manually) predict the interpretation of the
target construction knowing only the component
verb. We are interested in establishing the extent to
which this observation holds, and precisely which
aspects of a verb’s meaning give rise to the inter-
pretation of the target construction.
Our current model of the semantics of the target
construction does not capture Wason and Reich’s
(1979) observation that the compositional mean-
ing of instances having the “no” interpretation is
non-pragmatic. While we do not adopt their view
that these usages are somehow “errors”, we do
think that their observation can indicate other pos-
sible lexical semantic properties that may help to
identify the correct interpretation. Taking the clas-
sic example from Wason and Reich, no head in-
jury is too trivial to ignore, one clue to the “no”
interpretation is that generally a head injury is not
something that is ignored. On the other hand, con-
sidering Wason and Reich’s example no missile is
too small to ban, it is widely believed that missiles
should be banned. We would like to add features
that capture this knowledge to our model.
In preliminary experiments we have used co-
occurrence information as an approximation to
this knowledge. (For example, we would expect
that head injury would tend to co-occur less with
ignore than with antonymous verbs such as treat
or address.) Although our early results using
co-occurrence features do not indicate that they
are an improvement over the other features con-
sidered (WordNet and polarity), it may also be
the case that our present formulation of these co-
occurrence features does not effectively capture
the intended knowledge. In the future we plan
to further consider such features, especially those
that model the selectional preferences of the verb
participating in the target construction.
These several strands of future work—
increasing the size of the dataset, improving the
quality of annotation, and exploring additional
features in our computational model—will en-
able us to extend our linguistic analysis of this
interesting phenomenon, as well as to improve
performance on automatic disambiguation of this
complex construction.
</bodyText>
<page confidence="0.999163">
68
</page>
<sectionHeader confidence="0.998819" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997606">
We thank Magali Boizot-Roche and Timothy
Fowler for their help in preparing the data for this
study. This research was financially supported by
the Natural Sciences and Engineering Research
Council of Canada and the University of Toronto.
</bodyText>
<sectionHeader confidence="0.999161" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998486936170213">
Steven Abney. 1991. Parsing by chunks. In Robert
Berwick, Steven Abney, and Carol Tenny, ed-
itors, Principle-Based Parsing: Computation
and Psycholinguistics, pages 257–278. Kluwer
Academic Publishers.
Gerry T. M. Altmann and Mark Steedman. 1988.
Interaction with context during human sentence
processing. Cognition, 30(3):191–238.
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with
Python. O’Reilly Media Inc.
Lou Burnard. 2000. The British National Cor-
pus Users Reference Guide. Oxford University
Computing Services.
Kevin Burton, Akshay Java, and Ian Soboroff.
2009. The ICWSM 2009 Spinn3r Dataset. In
Proc. of the Third International Conference on
Weblogs and Social Media. San Jose, CA.
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: a library for support vector machines.
Software available at http://www.csie.
ntu.edu.tw/˜cjlin/libsvm.
Massimiliano Ciaramita and Mark Johnson. 2000.
Explaining away ambiguity: Learning verb se-
lectional preference with Bayesian networks. In
Proceedings of the 18th International Confer-
ence on Computational Linguistics (COLING
2000), pages 187–193. Saarbr¨ucken, Germany.
Stephen Clark and David Weir. 2002. Class-based
probability estimation using a semantic hier-
archy. Computational Linguistics, 28(2):187–
206.
Michael Collins. 2003. Head-driven statistical
models for natural language parsing. Compu-
tational Linguistics, 29(4):589–637.
Stephen Crain and Mark Steedman. 1985. On
not being led up the garden path: The use
of context by the psychological syntax pro-
cessor. In David R. Dowty, Lauri Karttunen,
and Arnold M. Zwicky, editors, Natural lan-
guage parsing: Psychological, computational,
and theoretical perspectives, pages 320–358.
Cambridge University Press, Cambridge.
James Curran, Stephen Clark, and Johan Bos.
2007. Linguistically motivated large-scale NLP
with C&amp;C and Boxer. In Proceedings of the
45th Annual Meeting of the Association for
Computational Linguistics Companion Volume
Proceedings of the Demo and Poster Sessions,
pages 33–36. Prague, Czech Republic.
Christiane Fellbaum, editor. 1998. Wordnet: An
Electronic Lexical Database. Bradford Books.
Daniel Jurafsky. 1996. A probabilistic model of
lexical and syntactic access and disambigua-
tion. Cognitive Science, 20(2):137–194.
Lauri Karttunen. 2007. Wordplay. Computational
Linguistics, 33(4):443–467.
Ronald W. Langacker. 1987. Foundations of
Cognitive Grammar: Theoretical Prerequisites,
volume 1. Stanford University Press, Stanford.
Mark Liberman. 2009a. No detail too small.
Retrieved 9 February 2010 from http://
languagelog.ldc.upenn.edu/nll/.
Mark Liberman. 2009b. No wug is too
dax to be zonged. Retrieved 9 February
2010 from http://languagelog.ldc.
upenn.edu/nll/.
Geoffrey K. Pullum. 2004. Too complex to
avoid judgment? Retrieved 7 April 2010 from
http://itre.cis.upenn.edu/˜myl/
languagelog/.
Evan Sandhaus. 2008. The New York Times An-
notated Corpus. Linguistic Data Consortium,
Philadelphia, PA.
Rion Snow, Brendan O’Connor, Daniel Jurafsky,
and Andrew Y. Ng. 2008. Cheap and fast — But
is it good? Evaluating non-expert annotations
for natural language tasks. In Proceedings of
EMNLP-2008, pages 254–263. Honolulu, HI.
John Trueswell and Michael J. Tanenhaus. 1994.
Toward a lexicalist framework for constraint-
based syntactic ambiguity resolution. In
Charles Clifton, Lyn Frazier, and Keith Rayner,
editors, Perspectives on Sentence Processing,
pages 155–179. Lawrence Erlbaum, Hillsdale,
NJ.
Peter Wason and Shuli Reich. 1979. A verbal il-
lusion. The Quarterly Journal of Experimental
Psychology, 31(4):591–597.
Theresa Wilson, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing contextual polarity in
phrase-level sentiment analysis. In Proceedings
ofHLT/EMNLP-2005, pages 347–354. Vancou-
ver, Canada.
</reference>
<page confidence="0.999317">
69
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.990591">No sentence is too confusing to ignore</title>
<author confidence="0.998654">Paul</author>
<affiliation confidence="0.872673">Department of Computer University of Toronto,</affiliation>
<email confidence="0.999577">pcook@cs.toronto.edu</email>
<author confidence="0.940186">Suzanne</author>
<affiliation confidence="0.875508666666667">Department of Computer University of Toronto,</affiliation>
<email confidence="0.999902">suzanne@cs.toronto.edu</email>
<abstract confidence="0.997530475746269">consider sentences of the form is too Y to in which X is a noun phrase, Y is an adjective phrase, and Z is a verb phrase. Such constructions are ambiguous, with two possible (and opposite!) interpretations, roughly meaning either that “Every X Zs”, or that “No X Zs”. The interpretations have been noted to depend on semantic and pragmatic factors. We show here that automatic disambiguation of this pragmatically complex construction can be largely achieved by using features of the lexical semantic propof the verb (i.e., participating in the construction. We discuss our experimental findings in the context of construction grammar, which suggests a possible account of this phenomenon. 1 No noun is too adjective to verb Consider the following two sentences: (1) No interest is too narrow to deserve its own newsletter. (2) No item is too minor to escape his attention. of these sentences has the form of Xis too to where X, Y, and Z are a noun phrase, adjective phrase, and verb phrase, respectively. Sen- (1) is generally taken to mean that interest deserves its own newsletter, regardless of how narrow it is. On the other hand, (2) is typiinterpreted as meaning that escapes his attention, regardless of how minor it is. That sentences with the identical form of X is too to Z can mean that Zs”, or can the opposite—that that in examples (1) and (2), the nouns the subjects of the verbs respec- This “verbal illusion” (Wason and Reich, 1979), so-called because there are two opposite interpretations for the very same structure, is of interest to us for two reasons. First, the contradictory nature of the possible meanings has been explained in terms of pragmatic factors concerning the relevant presuppositions of the sentences. According to Wason and Reich (1979) (as explained in more detail below), sentences such as (2) are actually nonsensical, but people coerce them into a sensible reading by reversing the interpretation. One of our goals in this work is to explore whether computational linguistic techniques—specifically automatic corpus analysis drawing on lexical resources—can help to elucidate the factors influencing interpretation of such sentences across a collection of actual usages. The second reason for our interest in this construction is that it illustrates a complex ambiguity that can cause difficulty for natural language processing applications that seek to semantically interpret text. Faced with the above two sentences, a parsing system (in the absence of specific knowledge of this construction) will presumably find the exact same structure for each, giving no basis on which to determine the correct meaning from the parse. (Unsurprisingly, when we run the C&amp;C Parser (Curran et al., 2007) on (1) and (2) it assigns the same structure to each sentence.) Our second goal in this work is thus to explore whether increased linguistic understanding of this phenomenon could be used to disambiguate such examples automatically. Specifically, we use this construction as an example of the kind of difficulties faced in semantic interpretation when meaning may be determined by pragmatic or other extra-syntactic factors, in order to explore whether tively. In this construction the noun can also be the object of the verb, as in the title of this paper which claims no sentence can/should be ignored. 61 of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL pages Sweden, 16 July 2010. Association for Computational Linguistics lexical semantic features can be used as cues to resolving pragmatic ambiguity when a complex semantico-pragmatic model is not feasible. In the remainder of this paper, we present the computational study of the X is too Y to which attempts to automatically determine the meaning of instances of this semantically and pragmatically complex construction. In Section 2 we present previous analyses of this construction, and our hypothesis. In Section 3, we describe the creation of a dataset of instances that verifies that both interpretations (“every” and “no”) indeed occur in corpora. We then analyze the human annotations in this dataset in more detail in Section 4. In Section 5, we present the feature model we use to describe the instances, which taps into the lexical semantics and polarity of the constituents. In Section 6, we describe machine learning experiments and classification results that support our hypothesis that the interpretation of this construction largely depends on the semantics of its component verb. In Section 7 we suggest that our results support an analysis of this phenomenon within construction grammar, and point to some future directions in our research in Section 8. 2 Background and our proposal X is too Y to Z was investigated by Wason and Reich (1979), and discussed more recently by Pullum (2004) and Liberman (2009a,b). Here we highlight some of the most important properties of this complex phenomenon. Our presentation owes much to the lucid discussion and clarification of this topic, and of the work of Wason and Reich specifically, by Liberman. Wason and Reich argue that the compositional interpretation of sentences of the form of (1) and (2) is “every X Zs”. Intuitively, this can be understood by considering a sentence identical to sen- (1), but without a negative subject: interest is too narrow to deserve its own newsletwhich means that “this interest is so narrow that it does not deserve a newsletter”. This exindicates that the meaning of narrow deserve its own newsletter “so narrow that it does not deserve a newsletter”. When this negative “too” assertion is compositionally combined the interest of sentence (1), it results in a meaning with two negatives: “No interest is so narrow that it does not deserve a newsletter”, or simply, “Every interest deserves a newsletter”. Wason and Reich note that in sentences such as (1), the compositional “every” interpretation is consistent with common beliefs about the world, and thus refer to such sentences as “pragmatic”. By contrast, the compositional interpretation of sentences such as (2) does not correspond to our common sense beliefs. Consider an analogous (non-negative subject) sentence to sentence (2)— item is too minor to escape his It is nonsensical that “This item is so minor that it does not escape his attention”, since being more “minor” entails more likelihood of escaping attention, not less. The compositional interpretation of (2) is similarly nonsensical—i.e., that “No item is so minor that it does not escape his attention”; Such sentences are thus termed “non-pragmatic” by Wason and Reich, who argue that the complexity of the non-pragmatic sentences—arising in part due to the number of negations they contain— causes the listener or reader to misconstrue them. According to their reasoning, listeners choose an interpretation that is consistent with their beliefs about the world—namely that “no X Zs”, in this case that “No item escapes his attention”—instead of the compositional interpretation (“Every item escapes his attention”). While Wason and Reich focus on the compositional semantics and pragmatics of these sentences, they also note that the non-pragmatic examples typically use a verb that itself has some of negation, such as and over- This property is also pointed out by Pullum who notes that his example of construction means “manage to something. Building on this observation, we hypothesize that lexical properties of the component constituents of this construction, particularly the verb, can be important cues to its semantico-pragmatic interpretation. Specifically, we hypothesize that the pragmatic (“every” interpretation) and nonpragmatic (“no” interpretation) sentences will tend to involve verbs with different semantics. Given that verbs of different semantic classes have different selectional preferences, we also expect to see the “every” and “no” sentences associated with semantically different nouns and adjectives. 3 Dataset 3.1 Extraction To create a dataset of usages of the construction NP is too AP to to as the tar- 62 get construction—we use two corpora: the British National Corpus (Burnard, 2000), an approximately one hundred million word corpus of latetwentieth century British English, and The New York Times Annotated Corpus (Sandhaus, 2008), approximately one billion words of non-newswire text from the New York Times from the years 1987–2006. We extract all sentences in these corcontaining the sequence of strings by one or more words. We then filter all sentences that do not have the subject of or that do not have an argument of After removing duplicates, this results in 170 sentences. We randomly select 20 of these sentences for development data, leaving 150 sentences for testing. Although we find only 170 examples of the target construction in 1.1 billion words of text, note that our extraction process is quite strict and misses some relevant usages. For example, we do extract sentences of the form is too Y Z which the subject NP does not contain the Nor do we extract usages of the related X is too Yfor where Z is an NP to a verb, as in interest is too narrow (We would only extract the latter if there were an infinitive verb embedded in or following the NP.) In the present study we limit our consideration to sentences of the form discussed by Wason and Reich (1979), but intend to consider related constructions such as these—which appear to exhibit the same ambiguity as the target construction—in the future. We next manually identify the noun, adjective, and verb that participate in the target construction in each sentence. Although this could be done automatically using a parser (e.g., Collins, 2003) or chunker (e.g., Abney, 1991), here we want to ensure error-free identification. We also note a number of sentences containing co-ordination, such as in the following example. (3) These days, no topic is too recent or specialized to disqualify it from museum apotheosis. This sentence contains two instances of the target construction: one corresponding to the nountriple and other to the triple disqual- In general, we consider each unique nounadjective-verb triple participating in the target construction as a separate instance. 3.2 Annotation We used Amazon Mechanical Turk (AMT, obtain judgements as to the correct interpretation of each instance of the target construction in both the development and testing datasets. For each instance, we generated two paraphrases, one corresponding to each of the interpretations discussed in Section 1. We then presented the given instance of the target construction along with its two paraphrases to annotators through AMT, as shown in Table 1. In generating the paraphrases, one of the authors selected the most appropriate paraphrase, in their where the paraphrases in Ta- 1 was selected from and Note that the paraphrases do not contain the adjective from the target construction. In the case of multiple instances of the target construction with differing adjectives but the same noun and verb, we only solicited judgements for one instance, and used these judgements for the other instances. In our dataset we observe that all instances obtained from the same sentence which differ only with respect to their noun or verb have the same interpretation. We therefore believe that instances with the same noun and verb but a different adjective are unlikely to differ in their interpretation. Instructions: • Read the sentence below. • Based on your interpretation of that sentence, select the answer that most closely matches your interpretation. • Select “I don’t know” if neither answer is close to your interpretation, or if you are really unsure. That success was accomplished in large part to tight control on costs , and no cost is too small to be scrutinized . • Every cost can be scrutinized. • No cost can be scrutinized. • I don’t know. Enter any feedback you have about this HIT. We greatly appreciate you taking the time to do so. Table 1: A sample of the Amazon Mechanical Turk annotation task. 63 We also allowed the judges to optionally enter any feedback about the annotation task which in some cases—discussed in the following section— was useful in determining whether the judges a particular instance difficult to For each instance of the target construction we obtained three judgements from unique workers on AMT. For approximately 80% of the items, the judgements were unanimous. In the remaining cases we solicited four additional judgements, and used the majority judgement. We paid $0.05 per judgement; the average time spent on each annotation was approximately twenty seconds, resulting in an average hourly wage of about $10. The development data was also annotated by three native English speaking experts (computational linguists with extensive linguistic background, two of whom are also authors of this paper). The inter-annotator agreement among these judges is very high, with pairwise observed agreements of 1.00, 0.90, and 0.90, and corresponding unweighted Kappa scores of 1.00, 0.79, and 0.79. The majority judgements of these annotators are the same as those obtained from AMT on the development data, giving us confidence in the reliability of the AMT judgements. These findings are consistent with those of Snow et al. (2008) in showing that AMT judgements can be as reliable as those of expert judges. Finally, we remove a small number of items from the testing dataset which were difficult to paraphrase due to ellipsis of the verb participating in the target construction, or an extra negation in the verb phrase. We further remove one sentence because we believe the paraphrases we provided are in fact misleading. The number of sentences and of instances (i.e., noun-verb-adjective triples) of the target construction in the development and testing datasets is given in Table 2. 160 of the 199 testing instances (80%) have the “every” interpretation, with the remainder having the “no” interpretation. 4 Analysis of annotation We now more closely examine the annotations obtained from AMT to better determine the extent to other cases the comments were more humourous. In to the following sentence you’ve ever yearned to live on Sesame Street, where no problem is too big to be solved by a not-too-big slice of strawberry-rhubarb pie, this the spot for one judge told us her preferred types of pie. Dataset # sentences # instances Development 20 33 Test 140 199 Table 2: The number of sentences containing the target construction, and the number of resulting instances. which they are reliable. We also consider specific instances of the target construction that are judged inconsistently to establish some of the causes of disagreement. One of the three experts who annotated the development items (discussed in Section 3.2) also annotated twenty items selected at random from the testing data. In this case two instances are judged differently than the majority judgement obtained from AMT. These instances are given below with the noun, adjective and verb in the target construction underlined. (4) When it comes to the clash of candidates on national television, no detail, it seems, is too for negotiation, no risktoo smallto eliminate. (5) Lectures by big-name Wall Street felons will why no swindleris too bigto beatthe rap by peaching on small-timers. For sentence (4), the AMT judgements were unanimously for the “no” interpretation whereas the expert annotator chose the “every” interpretation. We are uncertain as to the reason for this disagreement, but are convinced that the “every” interpretation is the intended one. In the case of sentence (5), the AMT judgements were split four–three for the “every” and “no” interpretations, respectively, while the expert annotator chose the “no” interpretation. For sentence the provided paraphrases were Evswindler can beat the rap swindler beat the If attention in the sentence restricted to the target construction—i.e., swindler is too big to beat the rap by peaching of the “no” and “every” interpretations is possible. That is, this clause alone can mean that “no swindler is ‘big’ enough to be able to beat the rap” (the “no” interpretation), or that “no swindler is ‘big’ enough that they 64 are above peaching on small-timers” (or in other words, “every swindler is able to beat the rap by peaching on small-timers”, the “every” interpretation). However, the intention of the sentence as the “no” interpretation is clear from the referral in the clause to Wall Street which that “big” swindlers have the rap. Since the AMT annotators may not be devoting a large amount of attention to the task, they may focus only on the target construction and not the preliminary disambiguating material. In this event, they may be choosing between the “every” and “no” interpretations based on how cynical they are of the ability (or lack thereof) of the American legal system to punish Wall Street criminals. We also examine a small number of examples in the testing set which do not receive a clear majority judgement from AMT. For this analysis we consider items for which the difference in the number of judgements for each of the “every” and the “no” interpretations is one or less This gives four instances of the target construction, one of which we have already discussed above, example (5); the others are presented below, again with the noun, adjective, and verb participating in the target construction underlined: (6) Where are our priorities when we so carefully weigh costs and medical efficacy in deciding to offer a medical lifeline to the yet no amountof money is too great to spend on the debatable paths we’ve taken in our war against terror? No neighborhood is too remoteto Mr. Levine’s determination to discover and announce some previously unheralded treat. No oneis too remoteanymore to be concernedabout style, Ms. Hansen suggested. In example (6) the author is using the target construction to express somebody else’s viewpoint that “any amount should be spent on the war against terror”. Therefore the literal reading of the target construction appears to be the “every” interpretation. However, this construction is being used rhetorically (as part of the overall sentence) to express the author’s belief that “too much money is being spent on the war against terror”, which is close in meaning to the “no” interpretation. It appears that the annotators are split between these two readings. For sentence (7) the of the subject of dimake this instance particularly difficult for the judges. Sentence (8) appears to us to be a clear example of the “every” interpretation. The paraphrases for this usage are “Everyone should be concerned about style” and “No one should be concerned about style”. In this case it is possible that the judges are biased by their beliefs about whether one should be concerned about style, and that this is giving rise to the lack of agreement. These examples illustrate that some of these usages are clearly complex for people to annotate. Such complex examples may require more context to be annotated with confidence. 5 Model To test our hypothesis that the interaction of the semantics of the noun, adjective, and verb in the target construction contributes to its pragmatic interpretation, we represent each instance in our dataset as a vector of features that capture aspects of the semantics of its component words. tap into general lexical semantic properties of the words in the construction, we use features that draw on the semantic classes of words in WordNet (Fellbaum, 1998). These binary features each represent a synset in WordNet, and are turned on or off for the component words (the noun, adjective, and verb) in each instance of the target construction. A synset feature is on for a word if the synset occurs on the path from all senses of the word to the root, and off otherwise. We use WordNet version 3.0 accessed using NLTK version 2.0 (Bird et al., 2009). of the observation that the verb in the target construction, in particular, has some property of negativity in the “no” interpretation, we also use features representing the semantic polarity of the noun, adjective, and verb in each instance. The features are tertiary, representing positive, neutral, or negative polarity. We obtain polarity information from the subjectivity lexicon provided by Wilson et al. (2005), and consider words to be neutral if they have both positive and negative polarity, or are not in the lexicon. 6 Experimental results 6.1 Experimental setup To evaluate our model we conduct a 5-fold crossexperiment using the items in the test- 65 ing dataset. When partitioning the items in the testing dataset into the five parts necessary for the cross-validation experiment, we ensure that all the instances of the target construction from a single sentence are in the same part. This ensures that no instance used for training is from the same sentence as an instance used for testing. We further ensure that the proportion of items in each class is roughly the same in each split. For each of the five runs, we linearly scale the data to be in the range and apply the same transformation to the testing data. We train a support vector machine (LIBSVM version 2.9, Chang and Lin, 2001) with a radial basis function kernel on the training portion in each run, setting the cost and gamma parameters using cross-validation on just the training portion, and then test the classifier on the testing portion for that run using the same parameter settings. We micro-average the accuracy obtained on each of the five runs. Finally, we repeat each 5-fold crossvalidation experiment five times, with five random splits, and report the average accuracy over these trials. 6.2 Results Results for experiments using various subsets of the features are presented in Table 3. We restrict the component word—the noun, adjective, or verb—for which we extract features to those listed in column “Word”, and extract only the features given in column “Features” (WordNet, polarity, or all). The majority baseline is 80%, corresponding to always selecting the “every” interpretation. Accuracies shown in boldface are significantly better than the majority class baseline using a paired ttest. (In all cases where the difference is signifiwe obtain We first consider the results using features extracted only for the noun, adjective, or verb individually, using all features. The best accuracy in this group of experiments, 87%, is achieved using the verb features, and is significantly higher than the majority baseline. On the other hand, the classifiers trained on the noun and adjective features individually perform no better than the baseline. These results support our hypothesis that lexical semantic properties of the component verb in the X is too Y to Z do indeed play an important role in determining its interpretation. Although we proposed that selectional constraints from the verb would also lead to differing semantics of the nouns and adjectives in the two interpretations, our WordNet features are likely too simplistic to capture this effect, if it does hold. Before ruling out the semantic contribution of these words to the interpretation, we need to explore whether a more sophisticated model of selectional preferences, as in Ciaramita and Johnson (2000) or Clark and Weir (2002), yields more informative features for the noun and adjective. Experimental setup % accuracy</abstract>
<note confidence="0.887163142857143">Word Features Noun All 80 Adjective All 80 Verb All 87 All WordNet 88 All Polarity 80 All All 88</note>
<abstract confidence="0.996014320512821">Majority baseline 80 Table 3: % accuracy on testing data for each experimental condition and the majority baseline. Accuracies in boldface are statistically significantly different from the baseline. We now consider the results using the WordNet and polarity features individually, but extracted for all three component words. The WordNet features perform as well as the best results using all features for all three words, which gives further support to our hypothesis that the semantics of the components of the target construction are related to its interpretation. The polarity features perform poorly. This is perhaps unsurprising as polarity is a poor approximation to the property of “negativity” that we are attempting to capture. Moreover, many of the nouns, adjectives, and verbs in our dataset either have neutral polarity or are not in the polarity lexicon, and therefore the polarity features are not very discriminative. In future work, we plan to examine the WordNet classes of the verbs that occur in the “no” interpretation to try to more precisely characterize the property of negativity that these verbs tend to have. 6.3 Error analysis To better understand the errors our classifier is making, we examine the specific instances which are classified incorrectly. Here we focus on the experiment using all features for all three component words. There are 23 instances which are 66 consistently mis-classified in all runs of the experiment. According to the AMT judgements, each of these instances corresponds to the “no” interpretation. These errors reflect the bias of the classifier towards the more frequent class, the “every” interpretation. We further note that two of the instances discussed in Section 4—examples (4) and (6)—are among those instances consistently classified incorrectly. The majority judgement from AMT for both of these instances is the “no” interpretation, while in our assessment they are in fact the “every” interpretation. We are therefore not surprised to see these items “mis-classified” as “every”. Example (8) was incorrectly classified in one trial. In this case we agree with the gold-standard label obtained from AMT in judging this instance as the “every” interpretation; nevertheless, this does appear to be a difficult instance given the low agreement observed for the AMT judgements. It is interesting that no items with an “every” interpretation are consistently misclassified. In the context of our overall results showing the impact of the verb features on performance, we conclude that the “no” interpretation arises due to particular lexical semantic properties of certain verbs. We suspect then that the consistent errors on the 21 truly misclassified expressions (23 minus the 2 instances discussed above that we believe to be annotated incorrectly) are due to sparse data. That is, if it is indeed the verb that plays a major role in leading to a “no” interpretation, there may simply be insufficient numbers of such verbs for training a supervised model in a dataset with only 39 examples of those usages. 7 Discussion We have presented the first computational study of the semantically and pragmatically complex con- X is too Y to We have developed a computational model that automatically disambiguates the construction with an accuracy of 88%, reducing the error-rate over the majority-baseline by 40%. The model uses features that tap into the lexical semantics of the component words participating in the construction, particularly the verb. These results demonstrate that lexical properties can be successful in resolving an ambiguity previously thought to depend on complex pragmatic inference over presuppositions (as in Wason and Reich (1979)). These results can be usefully situated within the context of linguistic and psycholinguistic work on semantic interpretation processing. Beginning around 20 years ago, work in modeling of human semantic preferences has focused on the extent to which properties of lexical items influence the interpretation of various linguistic ambiguities (e.g., Trueswell and Tanenhaus, 1994). While semantic context and plausibility are also proposed to play a role in human interpretation of ambiguous sentences (e.g., Crain and Steedman, 1985; Altmann and Steedman, 1988), it has been pointed out that it would be difficult to “operationalize” the complex interactions of presuppositional factors with real-world knowledge in a precise algorithm for disambiguation (Jurafsky, 1996). Although not intended as proposing a cognitive model, the work here can be seen as connected to these lines of research, in investigating the extent to which lexical factors can be used as proxies to more “hidden” features that underlie the appropriate interpretation of a pragmatically complex construction. Moreover, as in the approach of Jurafsky (1996), the phenomenon we investigate here may be best considered within a constructional analysis (e.g., Langacker, 1987), in which both the syntactic construction and the particular lexical items contribute to the determination of the meaning of a We suggest that a clause of the form Xis Y to Z be the (identical) surface expresof constructions—one with the “every” interpretation and one with the “no” interpretation—which place differing constraints on the semantics of the verb. (E.g., in the “no” interpretation, the verb typically has some “negative” semantic property, as noted in Section 2.) Looked at from the other perspective, the lexical semantic properties of the verb might determine X is too Y to Z (and associated interpretation) it is compatible with. Our results support this view, by showing that semantic classes of verbs have predictive value in selecting the correct interpretation. Note that such a constructional analysis of this phenomenon assumes that both interpretations of these sentences are linguistically valid, given the appropriate lexical instantiation. This stands in contrast to the analysis of Wason and Reich (1979), which presumes that people are applying some higher-level reasoning to “correct” an statement in the case of the “no” in- 67 terpretation. While such extra-grammatical inference may play a role in support of language understanding when people are faced with noisy data, it seems unlikely to us that a construction that is used quite readily and with a predictable interpretation is nonsensical according to rules of grammar. Our results point to an alternative linguistic analysis, one whose further development may also help to improve automatic disambiguation of instances of X is too Y to In the next section, we discuss directions for future work that could elaborate on these preliminary findings. 8 Future Work One limitation of this study is that the dataset used is rather small, consisting of just 199 instances of the target construction. As discussed in Section 3.1, the extraction process we use to obtain our experimental items has low recall; in particular it misses variants of the target construction such as is too Y to Z X is too Yfor In the future we intend to expand our dataset by extracting such usages. Furthermore, the data used in the present study is primarily taken from news text. While we do not adopt the view of some that usages of the target construction having the “no” interpretation are errors, it could be the case that such usages are more frequent in less formal text. In the future we also intend to extract usages of the target construction from datasets of less formal text, such as blogs (e.g., Burton et al., 2009). other than X is too Y to Z exhibit a similar ambiguity. For example, the condidn’t wait to Y ambiguous between “X did Y right away” and “X didn’t do Y at all” (Karttunen, 2007). In the future we would like to extend our study to consider more such constructions which are ambiguous due to the interpretation of negation. In Section 4 we note that for some instances the complexity of the sentences containing the target construction may make it difficult for the annotators to judge the meaning of the target. In the future we intend to present simplified versions of these sentences—which retain the noun, adjective, and verb from the target construction in the original sentence—to the judges to avoid this issue. Such an approach will also help us to focus more clearly on observable lexical semantic effects. We are particularly interested in further exploring the hypothesis that it is the semantics of the component verb that gives rise to the meaning of the target construction. Recall Pullum’s (2004) observation that the verb in the “no” interpretation explicitly Using this intuition, we have informally observed that it is largely possible to (manually) predict the interpretation of the target construction knowing only the component verb. We are interested in establishing the extent to which this observation holds, and precisely which aspects of a verb’s meaning give rise to the interpretation of the target construction. Our current model of the semantics of the target construction does not capture Wason and Reich’s (1979) observation that the compositional meaning of instances having the “no” interpretation is non-pragmatic. While we do not adopt their view that these usages are somehow “errors”, we do think that their observation can indicate other possible lexical semantic properties that may help to identify the correct interpretation. Taking the clasexample from Wason and Reich, head inis too trivial to one clue to the “no” interpretation is that generally a head injury is not something that is ignored. On the other hand, con- Wason and Reich’s example missile is small to it is widely believed that missiles should be banned. We would like to add features that capture this knowledge to our model. In preliminary experiments we have used cooccurrence information as an approximation to this knowledge. (For example, we would expect injury tend to co-occur less with with antonymous verbs such as Although our early results using co-occurrence features do not indicate that they are an improvement over the other features considered (WordNet and polarity), it may also be the case that our present formulation of these cooccurrence features does not effectively capture the intended knowledge. In the future we plan to further consider such features, especially those that model the selectional preferences of the verb participating in the target construction. These several strands of future work— increasing the size of the dataset, improving the quality of annotation, and exploring additional features in our computational model—will enable us to extend our linguistic analysis of this interesting phenomenon, as well as to improve performance on automatic disambiguation of this complex construction.</abstract>
<note confidence="0.470318857142857">68 Acknowledgments We thank Magali Boizot-Roche and Timothy Fowler for their help in preparing the data for this study. This research was financially supported by the Natural Sciences and Engineering Research Council of Canada and the University of Toronto.</note>
<title confidence="0.906454">References</title>
<author confidence="0.877828">Parsing by chunks In Robert Steven Abney</author>
<author confidence="0.877828">Carol Tenny</author>
<author confidence="0.877828">ed-</author>
<keyword confidence="0.301481">Parsing: Computation</keyword>
<note confidence="0.946193">pages 257–278. Kluwer Academic Publishers. Gerry T. M. Altmann and Mark Steedman. 1988. Interaction with context during human sentence 30(3):191–238.</note>
<author confidence="0.928537">Steven Bird</author>
<author confidence="0.928537">Edward Loper</author>
<author confidence="0.928537">Ewan Klein</author>
<affiliation confidence="0.9216935">Language Processing with O’Reilly Media Inc.</affiliation>
<address confidence="0.839744">Burnard. 2000. British National Cor-</address>
<affiliation confidence="0.827483">Users Reference Oxford University Computing Services.</affiliation>
<note confidence="0.53688925">Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The ICWSM 2009 Spinn3r Dataset. In Proc. of the Third International Conference on and Social San Jose, CA. Chang and Chih-Jen Lin. 2001. LIBa library for support vector available at Massimiliano Ciaramita and Mark Johnson. 2000. away ambiguity: Learning verb selectional preference with Bayesian networks. In Proceedings of the 18th International Conference on Computational Linguistics (COLING pages 187–193. Saarbr¨ucken, Germany. Stephen Clark and David Weir. 2002. Class-based probability estimation using a semantic hier- 28(2):187– 206. Michael Collins. 2003. Head-driven statistical for natural language parsing. Compu- 29(4):589–637.</note>
<author confidence="0.52104">On</author>
<abstract confidence="0.6472554">not being led up the garden path: The use context by the psychological syntax processor. In David R. Dowty, Lauri Arnold M. Zwicky, editors, language parsing: Psychological, computational,</abstract>
<note confidence="0.904615384615385">theoretical pages 320–358. Cambridge University Press, Cambridge. James Curran, Stephen Clark, and Johan Bos. 2007. Linguistically motivated large-scale NLP C&amp;C and Boxer. In of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume of the Demo and Poster pages 33–36. Prague, Czech Republic. Fellbaum, editor. 1998. An Lexical Bradford Books. Daniel Jurafsky. 1996. A probabilistic model of lexical and syntactic access and disambigua- 20(2):137–194. Karttunen. 2007. Wordplay. 33(4):443–467. W. Langacker. 1987. of Grammar: Theoretical volume 1. Stanford University Press, Stanford. Mark Liberman. 2009a. No detail too small. 9 February 2010 from Mark Liberman. 2009b. No wug is dax to be zonged. Retrieved 9 February from Geoffrey K. Pullum. 2004. Too complex to avoid judgment? Retrieved 7 April 2010 from</note>
<web confidence="0.998805">http://itre.cis.upenn.edu/˜myl/</web>
<author confidence="0.987015">The New York Times An-</author>
<affiliation confidence="0.987779">notated Corpus. Linguistic Data Consortium,</affiliation>
<address confidence="0.988223">Philadelphia, PA.</address>
<author confidence="0.7304475">Cheap</author>
<author confidence="0.7304475">fast</author>
<abstract confidence="0.614994235294118">is it good? Evaluating non-expert annotations natural language tasks. In of pages 254–263. Honolulu, HI. John Trueswell and Michael J. Tanenhaus. 1994. a lexicalist framework for constraintbased syntactic ambiguity resolution. Charles Clifton, Lyn Frazier, and Keith Rayner, on Sentence pages 155–179. Lawrence Erlbaum, Hillsdale, NJ. Peter Wason and Shuli Reich. 1979. A verbal il- Quarterly Journal of Experimental 31(4):591–597. Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in sentiment analysis. In pages 347–354. Vancou-</abstract>
<address confidence="0.686779">ver, Canada.</address>
<intro confidence="0.545485">69</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Parsing by chunks.</title>
<date>1991</date>
<booktitle>Principle-Based Parsing: Computation and Psycholinguistics,</booktitle>
<pages>257--278</pages>
<editor>In Robert Berwick, Steven Abney, and Carol Tenny, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="10504" citStr="Abney, 1991" startWordPosition="1748" endWordPosition="1749"> is too narrow for attention. (We would only extract the latter if there were an infinitive verb embedded in or following the NP.) In the present study we limit our consideration to sentences of the form discussed by Wason and Reich (1979), but intend to consider related constructions such as these—which appear to exhibit the same ambiguity as the target construction—in the future. We next manually identify the noun, adjective, and verb that participate in the target construction in each sentence. Although this could be done automatically using a parser (e.g., Collins, 2003) or chunker (e.g., Abney, 1991), here we want to ensure error-free identification. We also note a number of sentences containing co-ordination, such as in the following example. (3) These days, no topic is too recent or specialized to disqualify it from museum apotheosis. This sentence contains two instances of the target construction: one corresponding to the nounadjective-verb triple topic, recent, disqualify, and the other to the triple topic, specialized, disqualify. In general, we consider each unique nounadjective-verb triple participating in the target construction as a separate instance. 3.2 Annotation We used Amazo</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Steven Abney. 1991. Parsing by chunks. In Robert Berwick, Steven Abney, and Carol Tenny, editors, Principle-Based Parsing: Computation and Psycholinguistics, pages 257–278. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerry T M Altmann</author>
<author>Mark Steedman</author>
</authors>
<title>Interaction with context during human sentence processing.</title>
<date>1988</date>
<journal>Cognition,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="29093" citStr="Altmann and Steedman, 1988" startWordPosition="4817" endWordPosition="4820">tic inference over presuppositions (as in Wason and Reich (1979)). These results can be usefully situated within the context of linguistic and psycholinguistic work on semantic interpretation processing. Beginning around 20 years ago, work in modeling of human semantic preferences has focused on the extent to which properties of lexical items influence the interpretation of various linguistic ambiguities (e.g., Trueswell and Tanenhaus, 1994). While semantic context and plausibility are also proposed to play a role in human interpretation of ambiguous sentences (e.g., Crain and Steedman, 1985; Altmann and Steedman, 1988), it has been pointed out that it would be difficult to “operationalize” the complex interactions of presuppositional factors with real-world knowledge in a precise algorithm for disambiguation (Jurafsky, 1996). Although not intended as proposing a cognitive model, the work here can be seen as connected to these lines of research, in investigating the extent to which lexical factors can be used as proxies to more “hidden” features that underlie the appropriate interpretation of a pragmatically complex construction. Moreover, as in the approach of Jurafsky (1996), the phenomenon we investigate </context>
</contexts>
<marker>Altmann, Steedman, 1988</marker>
<rawString>Gerry T. M. Altmann and Mark Steedman. 1988. Interaction with context during human sentence processing. Cognition, 30(3):191–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Edward Loper</author>
<author>Ewan Klein</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media Inc.</booktitle>
<contexts>
<context position="21125" citStr="Bird et al., 2009" startWordPosition="3524" endWordPosition="3527">e aspects of the semantics of its component words. WordNet To tap into general lexical semantic properties of the words in the construction, we use features that draw on the semantic classes of words in WordNet (Fellbaum, 1998). These binary features each represent a synset in WordNet, and are turned on or off for the component words (the noun, adjective, and verb) in each instance of the target construction. A synset feature is on for a word if the synset occurs on the path from all senses of the word to the root, and off otherwise. We use WordNet version 3.0 accessed using NLTK version 2.0 (Bird et al., 2009). Polarity Because of the observation that the verb in the target construction, in particular, has some property of negativity in the “no” interpretation, we also use features representing the semantic polarity of the noun, adjective, and verb in each instance. The features are tertiary, representing positive, neutral, or negative polarity. We obtain polarity information from the subjectivity lexicon provided by Wilson et al. (2005), and consider words to be neutral if they have both positive and negative polarity, or are not in the lexicon. 6 Experimental results 6.1 Experimental setup To eva</context>
</contexts>
<marker>Bird, Loper, Klein, 2009</marker>
<rawString>Steven Bird, Edward Loper, and Ewan Klein. 2009. Natural Language Processing with Python. O’Reilly Media Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lou Burnard</author>
</authors>
<title>The British National Corpus Users Reference Guide.</title>
<date>2000</date>
<institution>Oxford University Computing Services.</institution>
<contexts>
<context position="8786" citStr="Burnard, 2000" startWordPosition="1445" endWordPosition="1446">semantico-pragmatic interpretation. Specifically, we hypothesize that the pragmatic (“every” interpretation) and nonpragmatic (“no” interpretation) sentences will tend to involve verbs with different semantics. Given that verbs of different semantic classes have different selectional preferences, we also expect to see the “every” and “no” sentences associated with semantically different nouns and adjectives. 3 Dataset 3.1 Extraction To create a dataset of usages of the construction no NP is too AP to VP—referred to as the tar62 get construction—we use two corpora: the British National Corpus (Burnard, 2000), an approximately one hundred million word corpus of latetwentieth century British English, and The New York Times Annotated Corpus (Sandhaus, 2008), approximately one billion words of non-newswire text from the New York Times from the years 1987–2006. We extract all sentences in these corpora containing the sequence of strings no, is too, and to separated by one or more words. We then manually filter all sentences that do not have no NP as the subject of is too, or that do not have to VP as an argument of is too. After removing duplicates, this results in 170 sentences. We randomly select 20</context>
</contexts>
<marker>Burnard, 2000</marker>
<rawString>Lou Burnard. 2000. The British National Corpus Users Reference Guide. Oxford University Computing Services.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Burton</author>
<author>Akshay Java</author>
<author>Ian Soboroff</author>
</authors>
<date>2009</date>
<booktitle>The ICWSM 2009 Spinn3r Dataset. In Proc. of the Third International Conference on Weblogs and Social Media.</booktitle>
<location>San Jose, CA.</location>
<contexts>
<context position="32465" citStr="Burton et al., 2009" startWordPosition="5373" endWordPosition="5376">; in particular it misses variants of the target construction such as Nothing is too Y to Z and No X is too Yfor Z. In the future we intend to expand our dataset by extracting such usages. Furthermore, the data used in the present study is primarily taken from news text. While we do not adopt the view of some that usages of the target construction having the “no” interpretation are errors, it could be the case that such usages are more frequent in less formal text. In the future we also intend to extract usages of the target construction from datasets of less formal text, such as blogs (e.g., Burton et al., 2009). Constructions other than No X is too Y to Z exhibit a similar ambiguity. For example, the construction X didn’t wait to Y is ambiguous between “X did Y right away” and “X didn’t do Y at all” (Karttunen, 2007). In the future we would like to extend our study to consider more such constructions which are ambiguous due to the interpretation of negation. In Section 4 we note that for some instances the complexity of the sentences containing the target construction may make it difficult for the annotators to judge the meaning of the target. In the future we intend to present simplified versions o</context>
</contexts>
<marker>Burton, Java, Soboroff, 2009</marker>
<rawString>Kevin Burton, Akshay Java, and Ian Soboroff. 2009. The ICWSM 2009 Spinn3r Dataset. In Proc. of the Third International Conference on Weblogs and Social Media. San Jose, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines. Software available at http://www.csie.</title>
<date>2001</date>
<tech>ntu.edu.tw/˜cjlin/libsvm.</tech>
<contexts>
<context position="22478" citStr="Chang and Lin, 2001" startWordPosition="3755" endWordPosition="3758"> in the testing dataset into the five parts necessary for the cross-validation experiment, we ensure that all the instances of the target construction from a single sentence are in the same part. This ensures that no instance used for training is from the same sentence as an instance used for testing. We further ensure that the proportion of items in each class is roughly the same in each split. For each of the five runs, we linearly scale the training data to be in the range [−1, 1], and apply the same transformation to the testing data. We train a support vector machine (LIBSVM version 2.9, Chang and Lin, 2001) with a radial basis function kernel on the training portion in each run, setting the cost and gamma parameters using cross-validation on just the training portion, and then test the classifier on the testing portion for that run using the same parameter settings. We micro-average the accuracy obtained on each of the five runs. Finally, we repeat each 5-fold crossvalidation experiment five times, with five random splits, and report the average accuracy over these trials. 6.2 Results Results for experiments using various subsets of the features are presented in Table 3. We restrict the componen</context>
</contexts>
<marker>Chang, Lin, 2001</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2001. LIBSVM: a library for support vector machines. Software available at http://www.csie. ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Mark Johnson</author>
</authors>
<title>Explaining away ambiguity: Learning verb selectional preference with Bayesian networks.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th International Conference on Computational Linguistics (COLING</booktitle>
<pages>187--193</pages>
<location>Saarbr¨ucken, Germany.</location>
<contexts>
<context position="24582" citStr="Ciaramita and Johnson (2000)" startWordPosition="4096" endWordPosition="4099">pport our hypothesis that lexical semantic properties of the component verb in the No X is too Y to Z construction do indeed play an important role in determining its interpretation. Although we proposed that selectional constraints from the verb would also lead to differing semantics of the nouns and adjectives in the two interpretations, our WordNet features are likely too simplistic to capture this effect, if it does hold. Before ruling out the semantic contribution of these words to the interpretation, we need to explore whether a more sophisticated model of selectional preferences, as in Ciaramita and Johnson (2000) or Clark and Weir (2002), yields more informative features for the noun and adjective. Experimental setup % accuracy Word Features Noun All 80 Adjective All 80 Verb All 87 All WordNet 88 All Polarity 80 All All 88 Majority baseline 80 Table 3: % accuracy on testing data for each experimental condition and the majority baseline. Accuracies in boldface are statistically significantly different from the baseline. We now consider the results using the WordNet and polarity features individually, but extracted for all three component words. The WordNet features perform as well as the best results u</context>
</contexts>
<marker>Ciaramita, Johnson, 2000</marker>
<rawString>Massimiliano Ciaramita and Mark Johnson. 2000. Explaining away ambiguity: Learning verb selectional preference with Bayesian networks. In Proceedings of the 18th International Conference on Computational Linguistics (COLING 2000), pages 187–193. Saarbr¨ucken, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>David Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<pages>206</pages>
<contexts>
<context position="24607" citStr="Clark and Weir (2002)" startWordPosition="4101" endWordPosition="4104">l semantic properties of the component verb in the No X is too Y to Z construction do indeed play an important role in determining its interpretation. Although we proposed that selectional constraints from the verb would also lead to differing semantics of the nouns and adjectives in the two interpretations, our WordNet features are likely too simplistic to capture this effect, if it does hold. Before ruling out the semantic contribution of these words to the interpretation, we need to explore whether a more sophisticated model of selectional preferences, as in Ciaramita and Johnson (2000) or Clark and Weir (2002), yields more informative features for the noun and adjective. Experimental setup % accuracy Word Features Noun All 80 Adjective All 80 Verb All 87 All WordNet 88 All Polarity 80 All All 88 Majority baseline 80 Table 3: % accuracy on testing data for each experimental condition and the majority baseline. Accuracies in boldface are statistically significantly different from the baseline. We now consider the results using the WordNet and polarity features individually, but extracted for all three component words. The WordNet features perform as well as the best results using all features for all</context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>Stephen Clark and David Weir. 2002. Class-based probability estimation using a semantic hierarchy. Computational Linguistics, 28(2):187– 206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>4</issue>
<contexts>
<context position="10473" citStr="Collins, 2003" startWordPosition="1743" endWordPosition="1744">ated to a verb, as in No interest is too narrow for attention. (We would only extract the latter if there were an infinitive verb embedded in or following the NP.) In the present study we limit our consideration to sentences of the form discussed by Wason and Reich (1979), but intend to consider related constructions such as these—which appear to exhibit the same ambiguity as the target construction—in the future. We next manually identify the noun, adjective, and verb that participate in the target construction in each sentence. Although this could be done automatically using a parser (e.g., Collins, 2003) or chunker (e.g., Abney, 1991), here we want to ensure error-free identification. We also note a number of sentences containing co-ordination, such as in the following example. (3) These days, no topic is too recent or specialized to disqualify it from museum apotheosis. This sentence contains two instances of the target construction: one corresponding to the nounadjective-verb triple topic, recent, disqualify, and the other to the triple topic, specialized, disqualify. In general, we consider each unique nounadjective-verb triple participating in the target construction as a separate instanc</context>
</contexts>
<marker>Collins, 2003</marker>
<rawString>Michael Collins. 2003. Head-driven statistical models for natural language parsing. Computational Linguistics, 29(4):589–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Crain</author>
<author>Mark Steedman</author>
</authors>
<title>On not being led up the garden path: The use of context by the psychological syntax processor.</title>
<date>1985</date>
<booktitle>Natural language parsing: Psychological, computational, and theoretical perspectives,</booktitle>
<pages>320--358</pages>
<editor>In David R. Dowty, Lauri Karttunen, and Arnold M. Zwicky, editors,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="29064" citStr="Crain and Steedman, 1985" startWordPosition="4813" endWordPosition="4816">o depend on complex pragmatic inference over presuppositions (as in Wason and Reich (1979)). These results can be usefully situated within the context of linguistic and psycholinguistic work on semantic interpretation processing. Beginning around 20 years ago, work in modeling of human semantic preferences has focused on the extent to which properties of lexical items influence the interpretation of various linguistic ambiguities (e.g., Trueswell and Tanenhaus, 1994). While semantic context and plausibility are also proposed to play a role in human interpretation of ambiguous sentences (e.g., Crain and Steedman, 1985; Altmann and Steedman, 1988), it has been pointed out that it would be difficult to “operationalize” the complex interactions of presuppositional factors with real-world knowledge in a precise algorithm for disambiguation (Jurafsky, 1996). Although not intended as proposing a cognitive model, the work here can be seen as connected to these lines of research, in investigating the extent to which lexical factors can be used as proxies to more “hidden” features that underlie the appropriate interpretation of a pragmatically complex construction. Moreover, as in the approach of Jurafsky (1996), t</context>
</contexts>
<marker>Crain, Steedman, 1985</marker>
<rawString>Stephen Crain and Mark Steedman. 1985. On not being led up the garden path: The use of context by the psychological syntax processor. In David R. Dowty, Lauri Karttunen, and Arnold M. Zwicky, editors, Natural language parsing: Psychological, computational, and theoretical perspectives, pages 320–358. Cambridge University Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Stephen Clark</author>
<author>Johan Bos</author>
</authors>
<title>Linguistically motivated large-scale NLP with C&amp;C and Boxer.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>33--36</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3137" citStr="Curran et al., 2007" startWordPosition="528" endWordPosition="531">date the factors influencing interpretation of such sentences across a collection of actual usages. The second reason for our interest in this construction is that it illustrates a complex ambiguity that can cause difficulty for natural language processing applications that seek to semantically interpret text. Faced with the above two sentences, a parsing system (in the absence of specific knowledge of this construction) will presumably find the exact same structure for each, giving no basis on which to determine the correct meaning from the parse. (Unsurprisingly, when we run the C&amp;C Parser (Curran et al., 2007) on (1) and (2) it assigns the same structure to each sentence.) Our second goal in this work is thus to explore whether increased linguistic understanding of this phenomenon could be used to disambiguate such examples automatically. Specifically, we use this construction as an example of the kind of difficulties faced in semantic interpretation when meaning may be determined by pragmatic or other extra-syntactic factors, in order to explore whether tively. In this construction the noun can also be the object of the verb, as in the title of this paper which claims no sentence can/should be ign</context>
</contexts>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>James Curran, Stephen Clark, and Johan Bos. 2007. Linguistically motivated large-scale NLP with C&amp;C and Boxer. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 33–36. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<title>Wordnet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>Bradford Books.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. Wordnet: An Electronic Lexical Database. Bradford Books.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
</authors>
<title>A probabilistic model of lexical and syntactic access and disambiguation.</title>
<date>1996</date>
<journal>Cognitive Science,</journal>
<volume>20</volume>
<issue>2</issue>
<contexts>
<context position="29303" citStr="Jurafsky, 1996" startWordPosition="4849" endWordPosition="4850">0 years ago, work in modeling of human semantic preferences has focused on the extent to which properties of lexical items influence the interpretation of various linguistic ambiguities (e.g., Trueswell and Tanenhaus, 1994). While semantic context and plausibility are also proposed to play a role in human interpretation of ambiguous sentences (e.g., Crain and Steedman, 1985; Altmann and Steedman, 1988), it has been pointed out that it would be difficult to “operationalize” the complex interactions of presuppositional factors with real-world knowledge in a precise algorithm for disambiguation (Jurafsky, 1996). Although not intended as proposing a cognitive model, the work here can be seen as connected to these lines of research, in investigating the extent to which lexical factors can be used as proxies to more “hidden” features that underlie the appropriate interpretation of a pragmatically complex construction. Moreover, as in the approach of Jurafsky (1996), the phenomenon we investigate here may be best considered within a constructional analysis (e.g., Langacker, 1987), in which both the syntactic construction and the particular lexical items contribute to the determination of the meaning of </context>
</contexts>
<marker>Jurafsky, 1996</marker>
<rawString>Daniel Jurafsky. 1996. A probabilistic model of lexical and syntactic access and disambiguation. Cognitive Science, 20(2):137–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<date>2007</date>
<journal>Wordplay. Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="32675" citStr="Karttunen, 2007" startWordPosition="5417" endWordPosition="5418">d in the present study is primarily taken from news text. While we do not adopt the view of some that usages of the target construction having the “no” interpretation are errors, it could be the case that such usages are more frequent in less formal text. In the future we also intend to extract usages of the target construction from datasets of less formal text, such as blogs (e.g., Burton et al., 2009). Constructions other than No X is too Y to Z exhibit a similar ambiguity. For example, the construction X didn’t wait to Y is ambiguous between “X did Y right away” and “X didn’t do Y at all” (Karttunen, 2007). In the future we would like to extend our study to consider more such constructions which are ambiguous due to the interpretation of negation. In Section 4 we note that for some instances the complexity of the sentences containing the target construction may make it difficult for the annotators to judge the meaning of the target. In the future we intend to present simplified versions of these sentences—which retain the noun, adjective, and verb from the target construction in the original sentence—to the judges to avoid this issue. Such an approach will also help us to focus more clearly on </context>
</contexts>
<marker>Karttunen, 2007</marker>
<rawString>Lauri Karttunen. 2007. Wordplay. Computational Linguistics, 33(4):443–467.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald W Langacker</author>
</authors>
<title>Foundations of Cognitive Grammar: Theoretical Prerequisites,</title>
<date>1987</date>
<volume>1</volume>
<publisher>Stanford University Press, Stanford.</publisher>
<contexts>
<context position="29777" citStr="Langacker, 1987" startWordPosition="4925" endWordPosition="4926">ize” the complex interactions of presuppositional factors with real-world knowledge in a precise algorithm for disambiguation (Jurafsky, 1996). Although not intended as proposing a cognitive model, the work here can be seen as connected to these lines of research, in investigating the extent to which lexical factors can be used as proxies to more “hidden” features that underlie the appropriate interpretation of a pragmatically complex construction. Moreover, as in the approach of Jurafsky (1996), the phenomenon we investigate here may be best considered within a constructional analysis (e.g., Langacker, 1987), in which both the syntactic construction and the particular lexical items contribute to the determination of the meaning of a usage. We suggest that a clause of the form No Xis too Y to Z might be the (identical) surface expression of two underlying constructions—one with the “every” interpretation and one with the “no” interpretation—which place differing constraints on the semantics of the verb. (E.g., in the “no” interpretation, the verb typically has some “negative” semantic property, as noted in Section 2.) Looked at from the other perspective, the lexical semantic properties of the ver</context>
</contexts>
<marker>Langacker, 1987</marker>
<rawString>Ronald W. Langacker. 1987. Foundations of Cognitive Grammar: Theoretical Prerequisites, volume 1. Stanford University Press, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Liberman</author>
</authors>
<title>No detail too small.</title>
<date>2009</date>
<journal>Retrieved</journal>
<volume>9</volume>
<note>from http:// languagelog.ldc.upenn.edu/nll/.</note>
<contexts>
<context position="5333" citStr="Liberman (2009" startWordPosition="887" endWordPosition="888">he lexical semantics and polarity of the constituents. In Section 6, we describe machine learning experiments and classification results that support our hypothesis that the interpretation of this construction largely depends on the semantics of its component verb. In Section 7 we suggest that our results support an analysis of this phenomenon within construction grammar, and point to some future directions in our research in Section 8. 2 Background and our proposal The No X is too Y to Z construction was investigated by Wason and Reich (1979), and discussed more recently by Pullum (2004) and Liberman (2009a,b). Here we highlight some of the most important properties of this complex phenomenon. Our presentation owes much to the lucid discussion and clarification of this topic, and of the work of Wason and Reich specifically, by Liberman. Wason and Reich argue that the compositional interpretation of sentences of the form of (1) and (2) is “every X Zs”. Intuitively, this can be understood by considering a sentence identical to sentence (1), but without a negative subject: This interest is too narrow to deserve its own newsletter, which means that “this interest is so narrow that it does not deser</context>
</contexts>
<marker>Liberman, 2009</marker>
<rawString>Mark Liberman. 2009a. No detail too small. Retrieved 9 February 2010 from http:// languagelog.ldc.upenn.edu/nll/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Liberman</author>
</authors>
<title>No wug is too dax to be zonged.</title>
<date>2009</date>
<journal>Retrieved</journal>
<volume>9</volume>
<note>from http://languagelog.ldc. upenn.edu/nll/.</note>
<contexts>
<context position="5333" citStr="Liberman (2009" startWordPosition="887" endWordPosition="888">he lexical semantics and polarity of the constituents. In Section 6, we describe machine learning experiments and classification results that support our hypothesis that the interpretation of this construction largely depends on the semantics of its component verb. In Section 7 we suggest that our results support an analysis of this phenomenon within construction grammar, and point to some future directions in our research in Section 8. 2 Background and our proposal The No X is too Y to Z construction was investigated by Wason and Reich (1979), and discussed more recently by Pullum (2004) and Liberman (2009a,b). Here we highlight some of the most important properties of this complex phenomenon. Our presentation owes much to the lucid discussion and clarification of this topic, and of the work of Wason and Reich specifically, by Liberman. Wason and Reich argue that the compositional interpretation of sentences of the form of (1) and (2) is “every X Zs”. Intuitively, this can be understood by considering a sentence identical to sentence (1), but without a negative subject: This interest is too narrow to deserve its own newsletter, which means that “this interest is so narrow that it does not deser</context>
</contexts>
<marker>Liberman, 2009</marker>
<rawString>Mark Liberman. 2009b. No wug is too dax to be zonged. Retrieved 9 February 2010 from http://languagelog.ldc. upenn.edu/nll/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey K Pullum</author>
</authors>
<title>Too complex to avoid judgment?</title>
<date>2004</date>
<journal>Retrieved</journal>
<volume>7</volume>
<note>from http://itre.cis.upenn.edu/˜myl/ languagelog/.</note>
<contexts>
<context position="5314" citStr="Pullum (2004)" startWordPosition="884" endWordPosition="885"> which taps into the lexical semantics and polarity of the constituents. In Section 6, we describe machine learning experiments and classification results that support our hypothesis that the interpretation of this construction largely depends on the semantics of its component verb. In Section 7 we suggest that our results support an analysis of this phenomenon within construction grammar, and point to some future directions in our research in Section 8. 2 Background and our proposal The No X is too Y to Z construction was investigated by Wason and Reich (1979), and discussed more recently by Pullum (2004) and Liberman (2009a,b). Here we highlight some of the most important properties of this complex phenomenon. Our presentation owes much to the lucid discussion and clarification of this topic, and of the work of Wason and Reich specifically, by Liberman. Wason and Reich argue that the compositional interpretation of sentences of the form of (1) and (2) is “every X Zs”. Intuitively, this can be understood by considering a sentence identical to sentence (1), but without a negative subject: This interest is too narrow to deserve its own newsletter, which means that “this interest is so narrow tha</context>
<context position="7905" citStr="Pullum (2004)" startWordPosition="1311" endWordPosition="1312">auses the listener or reader to misconstrue them. According to their reasoning, listeners choose an interpretation that is consistent with their beliefs about the world—namely that “no X Zs”, in this case that “No item escapes his attention”—instead of the compositional interpretation (“Every item escapes his attention”). While Wason and Reich focus on the compositional semantics and pragmatics of these sentences, they also note that the non-pragmatic examples typically use a verb that itself has some aspect of negation, such as ignore, miss, and overlook. This property is also pointed out by Pullum (2004), who notes that avoid in his example of the construction means “manage to not do” something. Building on this observation, we hypothesize that lexical properties of the component constituents of this construction, particularly the verb, can be important cues to its semantico-pragmatic interpretation. Specifically, we hypothesize that the pragmatic (“every” interpretation) and nonpragmatic (“no” interpretation) sentences will tend to involve verbs with different semantics. Given that verbs of different semantic classes have different selectional preferences, we also expect to see the “every” a</context>
</contexts>
<marker>Pullum, 2004</marker>
<rawString>Geoffrey K. Pullum. 2004. Too complex to avoid judgment? Retrieved 7 April 2010 from http://itre.cis.upenn.edu/˜myl/ languagelog/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evan Sandhaus</author>
</authors>
<title>The New York Times Annotated Corpus. Linguistic Data Consortium,</title>
<date>2008</date>
<location>Philadelphia, PA.</location>
<contexts>
<context position="8935" citStr="Sandhaus, 2008" startWordPosition="1468" endWordPosition="1469">sentences will tend to involve verbs with different semantics. Given that verbs of different semantic classes have different selectional preferences, we also expect to see the “every” and “no” sentences associated with semantically different nouns and adjectives. 3 Dataset 3.1 Extraction To create a dataset of usages of the construction no NP is too AP to VP—referred to as the tar62 get construction—we use two corpora: the British National Corpus (Burnard, 2000), an approximately one hundred million word corpus of latetwentieth century British English, and The New York Times Annotated Corpus (Sandhaus, 2008), approximately one billion words of non-newswire text from the New York Times from the years 1987–2006. We extract all sentences in these corpora containing the sequence of strings no, is too, and to separated by one or more words. We then manually filter all sentences that do not have no NP as the subject of is too, or that do not have to VP as an argument of is too. After removing duplicates, this results in 170 sentences. We randomly select 20 of these sentences for development data, leaving 150 sentences for testing. Although we find only 170 examples of the target construction in 1.1 bil</context>
</contexts>
<marker>Sandhaus, 2008</marker>
<rawString>Evan Sandhaus. 2008. The New York Times Annotated Corpus. Linguistic Data Consortium, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast — But is it good? Evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of EMNLP-2008,</booktitle>
<pages>254--263</pages>
<location>Honolulu, HI.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast — But is it good? Evaluating non-expert annotations for natural language tasks. In Proceedings of EMNLP-2008, pages 254–263. Honolulu, HI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Trueswell</author>
<author>Michael J Tanenhaus</author>
</authors>
<title>Toward a lexicalist framework for constraintbased syntactic ambiguity resolution.</title>
<date>1994</date>
<booktitle>Perspectives on Sentence Processing,</booktitle>
<pages>155--179</pages>
<editor>In Charles Clifton, Lyn Frazier, and Keith Rayner, editors,</editor>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="28911" citStr="Trueswell and Tanenhaus, 1994" startWordPosition="4789" endWordPosition="4792"> in the construction, particularly the verb. These results demonstrate that lexical properties can be successful in resolving an ambiguity previously thought to depend on complex pragmatic inference over presuppositions (as in Wason and Reich (1979)). These results can be usefully situated within the context of linguistic and psycholinguistic work on semantic interpretation processing. Beginning around 20 years ago, work in modeling of human semantic preferences has focused on the extent to which properties of lexical items influence the interpretation of various linguistic ambiguities (e.g., Trueswell and Tanenhaus, 1994). While semantic context and plausibility are also proposed to play a role in human interpretation of ambiguous sentences (e.g., Crain and Steedman, 1985; Altmann and Steedman, 1988), it has been pointed out that it would be difficult to “operationalize” the complex interactions of presuppositional factors with real-world knowledge in a precise algorithm for disambiguation (Jurafsky, 1996). Although not intended as proposing a cognitive model, the work here can be seen as connected to these lines of research, in investigating the extent to which lexical factors can be used as proxies to more “</context>
</contexts>
<marker>Trueswell, Tanenhaus, 1994</marker>
<rawString>John Trueswell and Michael J. Tanenhaus. 1994. Toward a lexicalist framework for constraintbased syntactic ambiguity resolution. In Charles Clifton, Lyn Frazier, and Keith Rayner, editors, Perspectives on Sentence Processing, pages 155–179. Lawrence Erlbaum, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Wason</author>
<author>Shuli Reich</author>
</authors>
<title>A verbal illusion.</title>
<date>1979</date>
<journal>The Quarterly Journal of Experimental Psychology,</journal>
<volume>31</volume>
<issue>4</issue>
<contexts>
<context position="1853" citStr="Wason and Reich, 1979" startWordPosition="321" endWordPosition="324">re a noun phrase, adjective phrase, and verb phrase, respectively. Sentence (1) is generally taken to mean that every interest deserves its own newsletter, regardless of how narrow it is. On the other hand, (2) is typically interpreted as meaning that no item escapes his attention, regardless of how minor it is. That is, sentences with the identical form of No X is too Y to Z either can mean that “every X Zs”, or can mean the opposite—that “no X Zs”!1 1Note that in examples (1) and (2), the nouns interest and item are the subjects of the verbs deserve and escape, respecThis “verbal illusion” (Wason and Reich, 1979), so-called because there are two opposite interpretations for the very same structure, is of interest to us for two reasons. First, the contradictory nature of the possible meanings has been explained in terms of pragmatic factors concerning the relevant presuppositions of the sentences. According to Wason and Reich (1979) (as explained in more detail below), sentences such as (2) are actually nonsensical, but people coerce them into a sensible reading by reversing the interpretation. One of our goals in this work is to explore whether computational linguistic techniques—specifically automati</context>
<context position="5268" citStr="Wason and Reich (1979)" startWordPosition="875" endWordPosition="878">ent the feature model we use to describe the instances, which taps into the lexical semantics and polarity of the constituents. In Section 6, we describe machine learning experiments and classification results that support our hypothesis that the interpretation of this construction largely depends on the semantics of its component verb. In Section 7 we suggest that our results support an analysis of this phenomenon within construction grammar, and point to some future directions in our research in Section 8. 2 Background and our proposal The No X is too Y to Z construction was investigated by Wason and Reich (1979), and discussed more recently by Pullum (2004) and Liberman (2009a,b). Here we highlight some of the most important properties of this complex phenomenon. Our presentation owes much to the lucid discussion and clarification of this topic, and of the work of Wason and Reich specifically, by Liberman. Wason and Reich argue that the compositional interpretation of sentences of the form of (1) and (2) is “every X Zs”. Intuitively, this can be understood by considering a sentence identical to sentence (1), but without a negative subject: This interest is too narrow to deserve its own newsletter, wh</context>
<context position="10131" citStr="Wason and Reich (1979)" startWordPosition="1688" endWordPosition="1691">et construction in 1.1 billion words of text, note that our extraction process is quite strict and misses some relevant usages. For example, we do not extract sentences of the form Nothing is too Y to Z in which the subject NP does not contain the word no. Nor do we extract usages of the related construction No X is too Yfor Z, where Z is an NP related to a verb, as in No interest is too narrow for attention. (We would only extract the latter if there were an infinitive verb embedded in or following the NP.) In the present study we limit our consideration to sentences of the form discussed by Wason and Reich (1979), but intend to consider related constructions such as these—which appear to exhibit the same ambiguity as the target construction—in the future. We next manually identify the noun, adjective, and verb that participate in the target construction in each sentence. Although this could be done automatically using a parser (e.g., Collins, 2003) or chunker (e.g., Abney, 1991), here we want to ensure error-free identification. We also note a number of sentences containing co-ordination, such as in the following example. (3) These days, no topic is too recent or specialized to disqualify it from muse</context>
<context position="28530" citStr="Wason and Reich (1979)" startWordPosition="4735" endWordPosition="4738">rst computational study of the semantically and pragmatically complex construction No X is too Y to Z. We have developed a computational model that automatically disambiguates the construction with an accuracy of 88%, reducing the error-rate over the majority-baseline by 40%. The model uses features that tap into the lexical semantics of the component words participating in the construction, particularly the verb. These results demonstrate that lexical properties can be successful in resolving an ambiguity previously thought to depend on complex pragmatic inference over presuppositions (as in Wason and Reich (1979)). These results can be usefully situated within the context of linguistic and psycholinguistic work on semantic interpretation processing. Beginning around 20 years ago, work in modeling of human semantic preferences has focused on the extent to which properties of lexical items influence the interpretation of various linguistic ambiguities (e.g., Trueswell and Tanenhaus, 1994). While semantic context and plausibility are also proposed to play a role in human interpretation of ambiguous sentences (e.g., Crain and Steedman, 1985; Altmann and Steedman, 1988), it has been pointed out that it wou</context>
<context position="30873" citStr="Wason and Reich (1979)" startWordPosition="5097" endWordPosition="5100">e” semantic property, as noted in Section 2.) Looked at from the other perspective, the lexical semantic properties of the verb might determine which No X is too Y to Z construction (and associated interpretation) it is compatible with. Our results support this view, by showing that semantic classes of verbs have predictive value in selecting the correct interpretation. Note that such a constructional analysis of this phenomenon assumes that both interpretations of these sentences are linguistically valid, given the appropriate lexical instantiation. This stands in contrast to the analysis of Wason and Reich (1979), which presumes that people are applying some higher-level reasoning to “correct” an ill-formed statement in the case of the “no” in67 terpretation. While such extra-grammatical inference may play a role in support of language understanding when people are faced with noisy data, it seems unlikely to us that a construction that is used quite readily and with a predictable interpretation is nonsensical according to rules of grammar. Our results point to an alternative linguistic analysis, one whose further development may also help to improve automatic disambiguation of instances of No X is too</context>
</contexts>
<marker>Wason, Reich, 1979</marker>
<rawString>Peter Wason and Shuli Reich. 1979. A verbal illusion. The Quarterly Journal of Experimental Psychology, 31(4):591–597.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings ofHLT/EMNLP-2005,</booktitle>
<pages>347--354</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="21561" citStr="Wilson et al. (2005)" startWordPosition="3591" endWordPosition="3594">n for a word if the synset occurs on the path from all senses of the word to the root, and off otherwise. We use WordNet version 3.0 accessed using NLTK version 2.0 (Bird et al., 2009). Polarity Because of the observation that the verb in the target construction, in particular, has some property of negativity in the “no” interpretation, we also use features representing the semantic polarity of the noun, adjective, and verb in each instance. The features are tertiary, representing positive, neutral, or negative polarity. We obtain polarity information from the subjectivity lexicon provided by Wilson et al. (2005), and consider words to be neutral if they have both positive and negative polarity, or are not in the lexicon. 6 Experimental results 6.1 Experimental setup To evaluate our model we conduct a 5-fold crossvalidation experiment using the items in the test65 ing dataset. When partitioning the items in the testing dataset into the five parts necessary for the cross-validation experiment, we ensure that all the instances of the target construction from a single sentence are in the same part. This ensures that no instance used for training is from the same sentence as an instance used for testing. </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings ofHLT/EMNLP-2005, pages 347–354. Vancouver, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>