<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000909">
<title confidence="0.994363">
Feature Subsumption for Opinion Analysis
</title>
<author confidence="0.897495">
Ellen Riloff and Siddharth Patwardhan
</author>
<affiliation confidence="0.768237">
School of Computing
University of Utah
Salt Lake City, UT 84112
</affiliation>
<email confidence="0.998415">
{riloff,sidd}@cs.utah.edu
</email>
<author confidence="0.813674">
Janyce Wiebe
</author>
<affiliation confidence="0.979809">
Department of Computer Science
University of Pittsburgh
</affiliation>
<address confidence="0.755485">
Pittsburgh, PA 15260
</address>
<email confidence="0.999028">
wiebe@cs.pitt.edu
</email>
<sectionHeader confidence="0.994805" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999627">
Lexical features are key to many ap-
proaches to sentiment analysis and opin-
ion detection. A variety of representations
have been used, including single words,
multi-word Ngrams, phrases, and lexico-
syntactic patterns. In this paper, we use a
subsumption hierarchy to formally define
different types of lexical features and their
relationship to one another, both in terms
of representational coverage and perfor-
mance. We use the subsumption hierar-
chy in two ways: (1) as an analytic tool
to automatically identify complex features
that outperform simpler features, and (2)
to reduce a feature set by removing un-
necessary features. We show that reduc-
ing the feature set improves performance
on three opinion classification tasks, espe-
cially when combined with traditional fea-
ture selection.
</bodyText>
<sectionHeader confidence="0.998886" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999891153846154">
Sentiment analysis and opinion recognition are ac-
tive research areas that have many potential ap-
plications, including review mining, product rep-
utation analysis, multi-document summarization,
and multi-perspective question answering. Lexi-
cal features are key to many approaches, and a va-
riety of representations have been used, including
single words, multi-word Ngrams, phrases, and
lexico-syntactic patterns. It is common for dif-
ferent features to overlap representationally. For
example, the unigram “happy” will match all of
the texts that the bigram “very happy” matches.
Since both features represent a positive sentiment
and the bigram matches fewer contexts than the
unigram, it is probably sufficient just to have the
unigram. However, there are many cases where
a feature captures a subtlety or non-compositional
meaning that a simpler feature does not. For exam-
ple, “basket case” is a highly opinionated phrase,
but the words “basket” and “case” individually
are not. An open question in opinion analysis is
how often more complex feature representations
are needed, and which types of features are most
valuable. Our first goal is to devise a method to
automatically identify features that are represen-
tationally subsumed by a simpler feature but that
are better opinion indicators. These subjective ex-
pressions could then be added to a subjectivity lex-
icon (Esuli and Sebastiani, 2005), and used to gain
understanding about which types of complex fea-
tures capture meaningful expressions that are im-
portant for opinion recognition.
Many opinion classifiers are created by adopt-
ing a “kitchen sink” approach that throws together
a variety of features. But in many cases adding
new types of features does not improve perfor-
mance. For example, Pang et al. (2002) found that
unigrams outperformed bigrams, and unigrams
outperformed the combination of unigrams plus
bigrams. Our second goal is to automatically iden-
tify features that are unnecessary because similar
features provide equal or better coverage and dis-
criminatory value. Our hypothesis is that a re-
duced feature set, which selectively combines un-
igrams with only the most valuable complex fea-
tures, will perform better than a larger feature set
that includes the entire “kitchen sink” of features.
In this paper, we explore the use of a subsump-
tion hierarchy to formally define the subsump-
tion relationships between different types of tex-
tual features. We use the subsumption hierarchy
in two ways. First, we use subsumption as an an-
</bodyText>
<page confidence="0.975132">
440
</page>
<note confidence="0.855781">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 440–448,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.998315333333333">
alytic tool to compare features of different com-
plexities and automatically identify complex fea-
tures that substantially outperform their simpler
counterparts. Second, we use the subsumption hi-
erarchy to reduce a feature set based on represen-
tational overlap and on performance. We conduct
experiments with three opinion data sets and show
that the reduced feature sets can improve classifi-
cation performance.
</bodyText>
<sectionHeader confidence="0.994242" genericHeader="introduction">
2 The Subsumption Hierarchy
</sectionHeader>
<subsectionHeader confidence="0.950602">
2.1 Text Representations
</subsectionHeader>
<bodyText confidence="0.989810317073171">
We analyze two feature representations that have
been used for opinion analysis: Ngrams and Ex-
traction Patterns. Information extraction (IE)
patterns are lexico-syntactic patterns that rep-
resent expressions which identify role relation-
ships. For example, the pattern “&lt;subj&gt;
ActVP(recommended)” extracts the subject of
active-voice instances of the verb “recommended”
as the recommender. The pattern “&lt;subj&gt;
PassVP(recommended)” extracts the subject of
passive-voice instances of “recommended” as the
object being recommended.
(Riloff and Wiebe, 2003) explored the idea
of using extraction patterns to represent more
complex subjective expressions that have non-
compositional meanings. For example, the expres-
sion “drive (someone) up the wall” expresses the
feeling of being annoyed, but the meanings of the
words “drive”, “up”, and “wall” have no emotional
connotations individually. Furthermore, this ex-
pression is not a fixed word sequence that can be
adequately modeled by Ngrams. Any noun phrase
can appear between the words “drive’ and “up”, so
a flexible representation is needed to capture the
general pattern “drives &lt;NP&gt; up the wall”.
This example represents a general phenomenon:
many expressions allow intervening noun phrases
and/or modifying terms. For example:
“stepped on &lt;mods&gt; toes”
Ex: stepped on the boss’ toes
“dealt &lt;np&gt; &lt;mods&gt; blow”
Ex: dealt the company a decisive blow
“brought &lt;np&gt; to &lt;mods&gt; knees”
Ex: brought the man to his knees
(Riloff and Wiebe, 2003) also showed that syn-
tactic variations of the same verb phrase can be-
have very differently. For example, they found that
passive-voice constructions of the verb “ask” had
a 100% correlation with opinion sentences, but
active-voice constructions had only a 63% corre-
lation with opinions.
</bodyText>
<table confidence="0.995407555555556">
Pattern Type Example Pattern
&lt;subj&gt; PassVP &lt;subj&gt; is satisfied
&lt;subj&gt; ActVP &lt;subj&gt; complained
&lt;subj&gt; ActVP Dobj &lt;subj&gt; dealt blow
&lt;subj&gt; ActInfVP &lt;subj&gt; appear to be
&lt;subj&gt; PassInfVP &lt;subj&gt; is meant to be
&lt;subj&gt; AuxVP Dobj &lt;subj&gt; has position
&lt;subj&gt; AuxVP Adj &lt;subj&gt; is happy
ActVP &lt;dobj&gt; endorsed &lt;dobj&gt;
InfVP &lt;dobj&gt; to condemn &lt;dobj&gt;
ActInfVP &lt;dobj&gt; get to know &lt;dobj&gt;
PassInfVP &lt;dobj&gt; is meant to be &lt;dobj&gt;
Subj AuxVP &lt;dobj&gt; fact is &lt;dobj&gt;
NP Prep &lt;np&gt; opinion on &lt;np&gt;
ActVP Prep &lt;np&gt; agrees with &lt;np&gt;
PassVP Prep &lt;np&gt; is worried about &lt;np&gt;
InfVP Prep &lt;np&gt; to resort to &lt;np&gt;
&lt;possessive&gt; NP &lt;noun&gt;’s speech
</table>
<figureCaption confidence="0.998301">
Figure 1: Extraction Pattern Types
</figureCaption>
<bodyText confidence="0.999895227272727">
Our goal is to use the subsumption hierarchy
to identify Ngram and extraction pattern features
that are more strongly associated with opinions
than simpler features. We used three types of fea-
tures in our research: unigrams, bigrams, and IE
patterns. The Ngram features were generated us-
ing the Ngram Statistics Package (NSP) (Baner-
jee and Pedersen, 2003).1 The extraction pat-
terns (EPs) were automatically generated using
the Sundance/AutoSlog software package (Riloff
and Phillips, 2004). AutoSlog relies on the Sun-
dance shallow parser and can be applied exhaus-
tively to a text corpus to generate IE patterns that
can extract every noun phrase in the corpus. Au-
toSlog has been used to learn IE patterns for the
domains of terrorism, joint ventures, and micro-
electronics (Riloff, 1996), as well as for opinion
analysis (Riloff and Wiebe, 2003). Figure 1 shows
the 17 types of extraction patterns that AutoSlog
generates. PassVP refers to passive-voice verb
phrases (VPs), ActVP refers to active-voice VPs,
InfVP refers to infinitive VPs, and AuxVP refers
</bodyText>
<footnote confidence="0.96088825">
&apos;NSP is freely available for use under the GPL from
http://search.cpan.org/dist/Text-NSP. We discarded Ngrams
that consisted entirely of stopwords. We used a list of 281
stopwords.
</footnote>
<page confidence="0.998414">
441
</page>
<bodyText confidence="0.99892325">
to VPs where the main verb is a form of “to be”
or “to have”. Subjects (subj), direct objects (dobj),
PP objects (np), and possessives can be extracted
by the patterns.2
</bodyText>
<subsectionHeader confidence="0.997006">
2.2 The Subsumption Hierarchy
</subsectionHeader>
<bodyText confidence="0.99863275925926">
We created a subsumption hierarchy that defines
the representational scope of different types of fea-
tures. We will say that feature A representation-
ally subsumes feature B if the set of text spans
that match feature A is a superset of the set of text
spans that match feature B. For example, the uni-
gram “happy” subsumes the bigram “very happy”
because the set of text spans that match “happy”
includes the text spans that match “very happy”.
First, we define a hierarchy of valid subsump-
tion relationships, shown in Figure 2. The 2Gram
node, for example, is a child of the 1Gram node
because a 1Gram can subsume a 2Gram. Ngrams
may subsume extraction patterns as well. Ev-
ery extraction pattern has at least one correspond-
ing 1Gram that will subsume it.3. For example,
the 1Gram “recommended” subsumes the pattern
“&lt;subj&gt; ActVP(recommended)” because the pat-
tern only matches active-voice instances of “rec-
ommended”. An extraction pattern may also
subsume another extraction pattern. For exam-
ple, “&lt;subj&gt; ActVP(recommended)” subsumes
“&lt;subj&gt; ActVP(recommended) Dobj(movie)”.
To compare specific features we need to for-
mally define the representation of each type of
feature in the hierarchy. For example, the hierar-
chy dictates that a 2Gram can subsume the pattern
“ActInfVP &lt;dobj&gt;”, but this should hold only if
the words in the bigram correspond to adjacent
words in the pattern. For example, the 2Gram “to
fish” subsumes the pattern “ActInfVP(like to fish)
&lt;dobj&gt;”. But the 2Gram “like fish” should not
subsume it. Similarly, consider the pattern “In-
fVP(plan) &lt;dobj&gt;”, which represents the infini-
tive “to plan”. This pattern subsumes the pattern
“ActInfVP(want to plan) &lt;dobj&gt;”, but it should
not subsume the pattern “ActInfVP(plan to start)”.
To ensure that different features truly subsume
each other representationally, we formally define
each type of feature based on words, sequential
2However, the items extracted by the patterns are not ac-
tually used by our opinion classifiers; only the patterns them-
selves are matched against the text.
3Because every type of extraction pattern shown in Fig-
ure 1 contains at least one word (not including the extracted
phrases, which are not used as part of our feature representa-
tion).
dependencies, and syntactic dependencies. A se-
quential dependency between words wi and wi+1
means that wi and wi+1 must be adjacent, and that
wi must precede wi+1. Figure 3 shows the formal
definition of a bigram (2Gram) node. The bigram
is defined as two words with a sequential depen-
dency indicating that they must be adjacent.
</bodyText>
<figure confidence="0.66036525">
Name = 2Gram
Constituent[0] = WORD1
Constituent[1] = WORD2
Dependency = Sequential(0, 1)
</figure>
<figureCaption confidence="0.999477">
Figure 3: 2Gram Definition
</figureCaption>
<bodyText confidence="0.999819117647059">
A syntactic dependency between words wi and
wi+1 means that wi has a specific syntactic rela-
tionship to wi+1, and wi must precede wi+1. For
example, consider the extraction pattern “NP Prep
&lt;np&gt;”, in which the object of the preposition at-
taches to the NP. Figure 4 shows the definition of
this extraction pattern in the hierarchy. The pat-
tern itself contains three components: the NP, the
attaching preposition, and the object of the prepo-
sition (which is the NP that the pattern extracts).
The definition also includes two syntactic depen-
dencies: the first dependency is between the NP
and the preposition (meaning that the preposition
syntactically attaches to the NP), while the second
dependency is between the preposition and the ex-
traction (meaning that the extracted NP is the syn-
tactic object of the preposition).
</bodyText>
<equation confidence="0.9964685">
Name = NP Prep &lt;np&gt;
Constituent[0] = NP
Constituent[1] = PREP
Constituent[2] = NP EXTRACTION
Dependency = Syntactic(0, 1)
Dependency = Syntactic(1, 2)
</equation>
<figureCaption confidence="0.992841">
Figure 4: “NP Prep &lt;np&gt;” Pattern Definition
</figureCaption>
<bodyText confidence="0.999928">
Consequently, the bigram “affair with” will not
subsume the extraction pattern “affair with &lt;np&gt;”
because the bigram requires the noun and preposi-
tion to be adjacent but the pattern does not. For ex-
ample, the extraction pattern matches the text “an
affair in his mind with Countess Olenska” but the
bigram does not. Conversely, the extraction pat-
tern does not subsume the bigram either because
the pattern requires syntactic attachment but the
bigram does not. For example, the bigram matches
</bodyText>
<page confidence="0.988224">
442
</page>
<figure confidence="0.99989756">
1Gram
2Gram
&lt;possessive&gt; NP
&lt;subj&gt; AuxVP AdjP
&lt;subj&gt; AuxVP Dobj
ActVP &lt;dobj&gt;
ActVP Prep &lt;np&gt;
NP Prep &lt;np&gt;
PassVP Prep &lt;np&gt;
Subj AuxVP &lt;dobj&gt;
&lt;subj&gt; ActVP
InfVP Prep &lt;np&gt;
NP Prep:OF &lt;np&gt;
PassVP Prep:OF &lt;np&gt;
ActVP Prep:OF &lt;np&gt;
&lt;subj&gt; ActVP Dobj
&lt;subj&gt; PassVP
InfVP &lt;dobj&gt;
3Gram
InfVP Prep:OF &lt;np&gt;
PassInfVP &lt;dobj&gt;
&lt;subj&gt; PassInfVP
ActInfVP &lt;dobj&gt;
&lt;subj&gt; ActInfVP
4Gram
</figure>
<figureCaption confidence="0.999944">
Figure 2: The Subsumption Hierarchy
</figureCaption>
<bodyText confidence="0.999959722222222">
the sentence “He ended the affair with a sense of
relief”, but the extraction pattern does not.
Figure 5 shows the definition of another ex-
traction pattern, “InfVP &lt;dobj&gt;”, which includes
both syntactic and sequential dependencies. This
pattern would match the text “to protest high
taxes”. The pattern definition has three compo-
nents: the infinitive “to”, a verb, and the direct ob-
ject of the verb (which is the NP that the pattern
extracts). The definition also shows two syntac-
tic dependencies. The first dependency indicates
that the verb syntactically attaches to the infinitive
“to”. The second dependency indicates that the ex-
tracted NP syntactically attaches to the verb (i.e.,
it is the direct object of that particular verb).
The pattern definition also includes a sequen-
tial dependency, which specifies that “to” must be
adjacent to the verb. Strictly speaking, our parser
does not require them to be adjacent. For exam-
ple, the parser allows intervening adverbs to split
infinitives (e.g., “to strongly protest high taxes”),
and this does happen occasionally. But split in-
finitives are relatively rare, so in the vast major-
ity of cases the infinitive “to” will be adjacent to
the verb. Consequently, we decided that a bigram
(e.g., “to protest”) should representationally sub-
sume this extraction pattern because the syntac-
tic flexibility afforded by the pattern is negligi-
ble. The sequential dependency link represents
this judgment call that the infinitive “to” and the
verb are adjacent in most cases.
For all of the node definitions, we used our best
judgment to make decisions of this kind. We tried
to represent major distinctions between features,
without getting caught up in minor differences that
were likely to be negligible in practice.
</bodyText>
<equation confidence="0.992956">
Name = InfVP &lt;dobj&gt;
Constituent[0] = INFINITIVE TO
Constituent[1] = VERB
Constituent[2] = DOBJ EXTRACTION
Dependency = Syntactic(0, 1)
Dependency = Syntactic(1, 2)
Dependency = Sequential(0, 1)
</equation>
<figureCaption confidence="0.993704">
Figure 5: “InfVP &lt;dobj&gt;” Pattern Definition
</figureCaption>
<bodyText confidence="0.9998349">
To use the subsumption hierarchy, we assign
each feature to its appropriate node in the hierar-
chy based on its type. Then we perform a top-
down breadth-first traversal. Each feature is com-
pared with the features at its ancestor nodes. If
a feature’s words and dependencies are a superset
of an ancestor’s words and dependencies, then it
is subsumed by the (more general) ancestor and
discarded.4 When the subsumption process is fin-
ished, a feature remains in the hierarchy only if
</bodyText>
<footnote confidence="0.9883005">
4The words that they have in common must also be in the
same relative order.
</footnote>
<page confidence="0.998079">
443
</page>
<bodyText confidence="0.97213">
there are no features above it that subsume it.
</bodyText>
<subsectionHeader confidence="0.827187">
2.3 Performance-based Subsumption
</subsectionHeader>
<bodyText confidence="0.999941277777778">
Representational subsumption is concerned with
whether one feature is more general than another.
But the purpose of using the subsumption hier-
archy is to identify more complex features that
outperform simpler ones. Applying the subsump-
tion hierarchy to features without regard to per-
formance would simply eliminate all features that
have a more general counterpart in the feature set.
For example, all bigrams would be discarded if
their component unigrams were also present in the
hierarchy.
To estimate the quality of a feature, we use In-
formation Gain (IG) because that has been shown
to work well as a metric for feature selection (For-
man, 2003). We will say that feature A be-
haviorally subsumes feature B if two criteria are
met: (1) A representationally subsumes B, and (2)
IG(A) &gt; IG(B) - 6, where 6 is a parameter repre-
senting an acceptable margin of performance dif-
ference. For example, if 6=0 then condition (2)
means that feature A is just as valuable as fea-
ture B because its information gain is the same or
higher. If 6&gt;0 then feature A is allowed to be a lit-
tle worse than feature B, but within an acceptable
margin. For example, 6=.0001 means that A’s in-
formation gain may be up to .0001 lower than B’s
information gain, and that is considered to be an
acceptable performance difference (i.e., A is good
enough that we are comfortable discarding B in
favor of the more general feature A).
Note that based on the subsumption hierarchy
shown in Figure 2, all 1Grams will always sur-
vive the subsumption process because they cannot
be subsumed by any other types of features. Our
goal is to identify complex features that are worth
adding to a set of unigram features.
</bodyText>
<sectionHeader confidence="0.99448" genericHeader="method">
3 Data Sets
</sectionHeader>
<bodyText confidence="0.999979428571429">
We used three opinion-related data sets for our
analyses and experiments: the OP data set created
by (Wiebe et al., 2004), the Polarity data set5 cre-
ated by (Pang and Lee, 2004), and the MPQA data
set created by (Wiebe et al., 2005).6 The OP and
Polarity data sets involve document-level opinion
classification, while the MPQA data set involves
</bodyText>
<footnote confidence="0.997083">
5Version v2.0, which is available at:
http://www.cs.cornell.edu/people/pabo/movie-review-data/
6Available at http://www.cs.pitt.edu/mpqa/databaserelease/
</footnote>
<bodyText confidence="0.999464321428571">
sentence-level classification.
The OP data consists of 2,452 documents from
the Penn Treebank (Marcus et al., 1993). Metadata
tags assigned by the Wall Street Journal define the
opinion/non-opinion classes: the class of any doc-
ument labeled Editorial, Letter to the Editor, Arts
&amp; Leisure Review, or Viewpoint by the Wall Street
Journal is opinion, and the class of documents in
all other categories (such as Business and News)
is non-opinion. This data set is highly skewed,
with only 9% of the documents belonging to the
opinion class. Consequently, a trivial (but useless)
opinion classifier that labels all documents as non-
opinion articles would achieve 91% accuracy.
The Polarity data consists of 700 positive and
700 negative reviews from the Internet Movie
Database (IMDb) archive. The positive and neg-
ative classes were derived from author ratings ex-
pressed in stars or numerical values. The MPQA
data consists of English language versions of ar-
ticles from the world press. It contains 9,732
sentences that have been manually annotated for
subjective expressions. The opinion/non-opinion
classes are derived from the lower-level annota-
tions: a sentence is an opinion if it contains a sub-
jective expression of medium or higher intensity;
otherwise, it is a non-opinion sentence. 55% of the
sentences belong to the opinion class.
</bodyText>
<sectionHeader confidence="0.9266625" genericHeader="method">
4 Using the Subsumption Hierarchy for
Analysis
</sectionHeader>
<bodyText confidence="0.99993355">
In this section, we illustrate how the subsump-
tion hierarchy can be used as an analytic tool to
automatically identify features that substantially
outperform simpler counterparts. These features
represent specialized usages and expressions that
would be good candidates for addition to a sub-
jectivity lexicon. Figure 6 shows pairs of features,
where the first is more general and the second is
more specific. These feature pairs were identified
by the subsumption hierarchy as being representa-
tionally similar but behaviorally different (so the
more specific feature was retained). The IGain
column shows the information gain values pro-
duced from the training set of one cross-validation
fold. The Class column shows the class that the
more specific feature is correlated with (the more
general feature is usually not strongly correlated
with either class).
The top table in Figure 6 contains examples for
the opinion/non-opinion classification task from
</bodyText>
<page confidence="0.998553">
444
</page>
<table confidence="0.891666916666667">
Opinion/Non-Opinion Classification
ID Feature IGain Class Example
A1 line .0016 - ... issue consists of notes backed by credit line receivables
A2 the line .0075 opin ...lays it on the line; ...steps across the line
B1 nation .0046 - ... has 750,000 cable-tv subscribers around the nation
B2 a nation .0080 opin It’s not that we are spawning a nation of ascetics ...
C1 begin .0006 - Campeau buyers will begin writing orders...
C2 begin with .0036 opin To begin with, we should note that in contrast...
D1 benefits .0040 - ... earlier period included $235,000 in tax benefits.
DEP NP Prep(benefits to) .0090 opin ... boon to the rich with no proven benefits to the economy
E1 due .0001 - ... an estimated $ 1.23 billion in debt due next spring
EEP ActVP Prep(due to) .0038 opin It’s all due to the intense scrutiny...
Positive/Negative Sentiment Classification
ID Feature IGain Class Example
F1 short .0014 - to make a long story short...
F2 nothing short .0039 pos nothing short of spectacular
G1 ugly .0008 - ...an ugly monster on a cruise liner
G2 and ugly .0054 neg it’s a disappointment to see something this dumb and ugly
H1 disaster .0010 - ...rated pg-13 for disaster related elements
HEP AuxVP Dobj(be disaster) .0048 neg ... this is such a confused disaster of a film
I1 work .0002 - the next day during the drive to work...
IEP ActVP(work) .0062 pos the film will work just as well...
J1 manages .0003 - he still manages to find time for his wife
JEP ActInfVP(manages to keep) .0054 pos this film manages to keep up a rapid pace
</table>
<figureCaption confidence="0.578123">
Figure 6: Sample features that behave differently, as revealed by the subsumption hierarchy.
(1 ==�- unigram; 2 ==�- bigram; EP ==&gt;. extraction pattern)
</figureCaption>
<bodyText confidence="0.999905535714286">
the OP data. The more specific features are more
strongly correlated with opinion articles. Surpris-
ingly, simply adding a determiner can dramatically
change behavior. Consider A2. There are many
subjective idioms involving “the line” (two are
shown in the table; others include “toe the line”
and “draw the line”), while objective language
about credit lines, phone lines, etc. uses the deter-
miner less often. Similarly, consider B2. Adding
“a” to “nation” often corresponds to an abstract
reference used when making an argument (e.g.,
“a nation of ascetics”), whereas other instances
of “nation” are used more literally (e.g., “the 6th
largest in the nation”). 21% of feature B1’s in-
stances appear in opinion articles, while 70% of
feature B2’s instances are in opinion articles.
“Begin with” (C2) captures an adverbial phrase
used in argumentation (“To begin with...”) but
does not match objective usages such as “will
begin” an action. The word “benefits” alone
(D1) matches phrases like “tax benefits” and “em-
ployee benefits” that are not opinion expressions,
while DEP typically matches positive senses of
the word “benefits”. Interestingly, the bigram
“benefits to” is not highly correlated with opin-
ions because it matches infinitive phrases such
as “tax benefits to provide” and “health benefits
to cut”. In this case, the extraction pattern “NP
Prep(benefits to)” is more discriminating than the
bigram for opinion classification. The extraction
pattern EEP is also highly correlated with opin-
ions, while the unigram “due” and the bigram
“due to” are not.
The bottom table in Figure 6 shows feature
pairs identified for their behavioral differences on
the Polarity data set, where the task is to distin-
guish positive reviews from negative reviews. F2
and G2 are bigrams that behave differently from
their component unigrams. The expression “noth-
ing short (of)” is typically used to express posi-
tive sentiments, while “nothing” and “short” by
themselves are not. The word “ugly” is often used
as a descriptive modifier that is not expressing
a sentiment per se, while “and ugly” appears in
predicate adjective constructions that are express-
ing a negative sentiment. The extraction pattern
HEP is more discriminatory than H1 because it
distinguishes negative sentiments (“the film is a
disaster!”) from plot descriptions (“the disaster
movie...”). IEP shows that active-voice usages of
“work” are strong positive indicators, while the
unigram “work” appears in a variety of both pos-
itive and negative contexts. Finally, JEP shows
that the expression “manages to keep” is a strong
positive indicator, while “manages” by itelf is
much less discriminating.
</bodyText>
<page confidence="0.998253">
445
</page>
<bodyText confidence="0.9995348">
These examples illustrate that the subsumption
hierarchy can be a powerful tool to better under-
stand the behaviors of different kinds of features,
and to identify specific features that may be desir-
able for inclusion in specialized lexical resources.
</bodyText>
<sectionHeader confidence="0.823872" genericHeader="method">
5 Using the Subsumption Hierarchy to
Reduce Feature Sets
</sectionHeader>
<bodyText confidence="0.9998735">
When creating opinion classifiers, people often
throw in a variety of features and trust the ma-
chine learning algorithm to figure out how to make
the best use of them. However, we hypothesized
that classifiers may perform better if we can proac-
tively eliminate features that are not necesary be-
cause they are subsumed by other features. In this
section, we present a series of experiments to ex-
plore this hypothesis. First, we present the results
for an SVM classifier trained using different sets
of unigram, bigram, and extraction pattern fea-
tures, both before and after subsumption. Next, we
evaluate a standard feature selection approach as
an alternative to subsumption and then show that
combining subsumption with standard feature se-
lection produces the best results of all.
</bodyText>
<subsectionHeader confidence="0.996891">
5.1 Classification Experiments
</subsectionHeader>
<bodyText confidence="0.9982359375">
To see whether feature subsumption can improve
classification performance, we trained an SVM
classifier for each of the three opinion data sets.
We used the SVM&amp;quot;ght (Joachims, 1998) package
with a linear kernel. For the Polarity and OP data
we discarded all features that have frequency &lt; 5,
and for the MPQA data we discarded features that
have frequency &lt; 2 because this data set is sub-
stantially smaller. All of our experimental results
are averages over 3-fold cross-validation.
First, we created 4 baseline classifiers: a 1Gram
classifier that uses only the unigram features; a
1+2Gram classifier that uses unigram and bigram
features; a 1+EP classifier that uses unigram and
extraction pattern features, and a 1+2+EP classi-
fier that uses all three types of features. Next, we
created analogous 1+2Gram, 1+EP, and 1+2+EP
classifiers but applied the subsumption hierar-
chy first to eliminate unnecessary features be-
fore training the classifier. We experimented with
three delta values for the subsumption process:
5=.0005, .001, and .002.
Figures 7, 8, and 9 show the results. The sub-
sumption process produced small but consistent
improvements on all 3 data sets. For example, Fig-
ure 8 shows the results on the OP data, where all
of the accuracy values produced after subsumption
(the rightmost 3 columns) are higher than the ac-
curacy values produced without subsumption (the
Base[line] column). For all three data sets, the best
overall accuracy (shown in boldface) was always
achieved after subsumption.
</bodyText>
<table confidence="0.9986636">
Features Base 5=.0005 5=.001 5=.002
1Gram 79.8
1+2Gram 81.2 81.0 81.3 81.0
1+EP 81.7 81.4 81.4 82.0
1+2+EP 81.7 82.3 82.3 82.7
</table>
<figureCaption confidence="0.767552">
Figure 7: Accuracies on Polarity Data
</figureCaption>
<table confidence="0.9966596">
Features Base 5=.0005 5=.001 5=.002
1Gram 97.5 - - -
1+2Gram 98.0 98.7 98.6 98.7
1+EP 97.2 97.8 97.9 97.9
1+2+EP 97.8 98.6 98.7 98.7
</table>
<figureCaption confidence="0.701321">
Figure 8: Accuracies on OP Data
</figureCaption>
<table confidence="0.994268">
Features Base 5=.0005 5=.001 5=.002
1Gram 74.8
1+2Gram 74.3 74.9 74.6 74.8
1+EP 74.4 74.6 74.6 74.6
1+2+EP 74.4 74.9 74.7 74.6
</table>
<figureCaption confidence="0.997804">
Figure 9: Accuracies on MPQA Data
</figureCaption>
<bodyText confidence="0.99996455">
We also observed that subsumption had a dra-
matic effect on the F-measure scores on the OP
data, which are shown in Figure 10. The OP data
set is fundamentally different from the other data
sets because it is so highly skewed, with 91% of
the documents belonging to the non-opinion class.
Without subsumption, the classifier was conser-
vative about assigning documents to the opinion
class, achieving F-measure scores in the 82-88
range. After subsumption, the overall accuracy
improved but the F-measure scores increased more
dramatically. These numbers show that the sub-
sumption process produced not only a more ac-
curate classifier, but a more useful classifier that
identifies more documents as being opinion arti-
cles.
For the MPQA data, we get a very small im-
provement of 0.1% (74.8% -* 74.9%) using sub-
sumption. But note that without subsumption the
performance actually decreased when bigrams and
</bodyText>
<page confidence="0.996688">
446
</page>
<table confidence="0.9979442">
Features Base 5=.0005 5=.001 5=.002
1Gram 84.5
1+2Gram 88.0 92.5 92.0 92.3
1+EP 82.4 86.9 87.4 87.4
1+2+EP 86.7 91.8 92.5 92.3
</table>
<figureCaption confidence="0.737305">
Figure 10: F-measures on OP Data
</figureCaption>
<figure confidence="0.9523825">
1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
Top N
</figure>
<figureCaption confidence="0.998625">
Figure 11: Feature Selection on OP Data
</figureCaption>
<bodyText confidence="0.999416333333333">
extraction patterns were added! The subsumption
process counteracted the negative effect of adding
the more complex features.
</bodyText>
<subsectionHeader confidence="0.9933">
5.2 Feature Selection Experiments
</subsectionHeader>
<bodyText confidence="0.999967681818182">
We conducted a second series of experiments to
determine whether a traditional feature selection
approach would produce the same, or better, im-
provements as subsumption. For each feature, we
computed its information gain (IG) and then se-
lected the N features with the highest scores.7 We
experimented with values of N ranging from 1,000
to 10,000 in increments of 1,000.
We hypothesized that applying subsumption be-
fore traditional feature selection might also help to
identify a more diverse set of high-performing fea-
tures. In a parallel set of experiments, we explored
this hypothesis by first applying subsumption to
reduce the size of the feature set, and then select-
ing the best N features using information gain.
Figures 11, 12, and 13 show the results of these
experiments for the 1+2+EP classifiers. Each
graph shows four lines. One line corresponds to
the baseline classifier with no subsumption, and
another line corresponds to the baseline classifier
with subsumption using the best 6 value for that
data set. Each of these two lines corresponds to
</bodyText>
<footnote confidence="0.919688">
7In the case of ties, we included all features with the same
score as the Nth-best as well.
</footnote>
<figure confidence="0.920327833333333">
83.5
83
82.5
82
1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
Top N
</figure>
<figureCaption confidence="0.939573">
Figure 12: Feature Selection on Polarity Data
</figureCaption>
<figure confidence="0.9982388">
75.5
75
74.5
1000 2000 3000 4000 5000 6000 7000 8000 9000 10000
Top N
</figure>
<figureCaption confidence="0.99961">
Figure 13: Feature Selection on MPQA Data
</figureCaption>
<bodyText confidence="0.999934217391305">
just a single data point (accuracy value), but we
drew that value as a line across the graph for the
sake of comparison. The other two lines on the
graph correspond to (a) feature selection for dif-
ferent values of N (shown on the x-axis), and (b)
subsumption followed by feature selection for dif-
ferent values of N.
On all 3 data sets, traditional feature selection
performs worse than the baseline in some cases,
and it virtually never outperforms the best classi-
fier trained after subsumption (but without feature
selection). Furthermore, the combination of sub-
sumption plus feature selection generally performs
best of all, and nearly always outperforms feature
selection alone. For all 3 data sets, our best ac-
curacy results were achieved by performing sub-
sumption prior to feature selection. The best accu-
racy results are 99.0% on the OP data, 83.1% on
the Polarity data, and 75.4% on the MPQA data.
For the OP data, the improvement over baseline
for both accuracy and F-measure are statistically
significant at the p &lt; 0.05 level (paired t-test). For
the MPQA data, the improvement over baseline is
</bodyText>
<figure confidence="0.997553676470588">
Accuracy (%)
98.8
98.6
98.4
98.2
97.8
97.6
99
98
Baseline
Subsumption 8=0.002
Feature Selection
Subsumption 8=0.002 + Feature Selection
Baseline
Subsumption 8=0.002
Feature Selection
Subsumption 8=0.002 + Feature Selection
Baseline
Subsumption 8=0.0005
Feature Selection
Subsumption 8=0.0005 + Feature Selection
Accuracy (%) 81.5
81
80.5
80
79.5
79
78.5
78
Accuracy (%) 74
73.5
73
72.5
72
</figure>
<page confidence="0.959145">
447
</page>
<bodyText confidence="0.8797055">
statistically significant at the p &lt; 0.10 level. M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proc. KDD-04.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.747161823529412">
Many features and classification algorithms have
been explored in sentiment analysis and opinion
recognition. Lexical cues of differing complexi-
ties have been used, including single words and
Ngrams (e.g., (Mullen and Collier, 2004; Pang et
al., 2002; Turney, 2002; Yu and Hatzivassiloglou,
2003; Wiebe et al., 2004)), as well as phrases
and lexico-syntactic patterns (e.g, (Kim and Hovy,
2004; Hu and Liu, 2004; Popescu and Etzioni,
2005; Riloff and Wiebe, 2003; Whitelaw et al.,
2005)). While many of these studies investigate
combinations of features and feature selection,
this is the first work that uses the notion of sub-
sumption to compare Ngrams and lexico-syntactic
patterns to identify complex features that outper-
form simpler counterparts and to reduce a com-
bined feature set to improve opinion classification.
</bodyText>
<sectionHeader confidence="0.999633" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999970666666667">
This paper uses a subsumption hierarchy of
feature representations as (1) an analytic tool
to compare features of different complexities,
and (2) an automatic tool to remove unneces-
sary features to improve opinion classification
performance. Experiments with three opinion
data sets showed that subsumption can improve
classification accuracy, especially when combined
with feature selection.
</bodyText>
<sectionHeader confidence="0.997651" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999345">
This research was supported by NSF Grants IIS-
0208798 and IIS-0208985, the ARDA AQUAINT
Program, and the Institute for Scientific Comput-
ing Research and the Center for Applied Scientific
Computing within Lawrence Livermore National
Laboratory.
</bodyText>
<sectionHeader confidence="0.99946" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999944033333333">
S. Banerjee and T. Pedersen. 2003. The Design, Imple-
mentation, and Use of the Ngram Statistics Package.
In Proc. Fourth Int’l Conference on Intelligent Text
Processing and Computational Linguistics.
A. Esuli and F. Sebastiani. 2005. Determining the se-
mantic orientation of terms through gloss analysis.
In Proc. CIKM-05.
G. Forman. 2003. An Extensive Empirical Study of
Feature Selection Metrics for Text Classification. J.
Mach. Learn. Res., 3:1289–1305.
T. Joachims. 1998. Making Large-Scale Support
Vector Machine Learning Practical. In A. Smola
B. Sch¨olkopf, C. Burges, editor, Advances in Ker-
nel Methods: Support Vector Machines. MIT Press,
Cambridge, MA.
S-M. Kim and E. Hovy. 2004. Determining the senti-
ment of opinions. In Proc. COLING-04.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English:
The Penn Treebank. Computational Linguistics,
19(2):313–330.
T. Mullen and N. Collier. 2004. Sentiment Analysis
Using Support Vector Machines with Diverse Infor-
mation Sources. In Proc. EMNLP-04.
B. Pang and L. Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proc. ACL-04.
B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs
up? Sentiment Classification using Machine Learn-
ing Techniques. In Proc. EMNLP-02.
A-M. Popescu and O. Etzioni. 2005. Extracting prod-
uct features and opinions from reviews. In Proc.
HLT-EMNLP-05.
E. Riloff and W. Phillips. 2004. An Introduction to the
Sundance and AutoSlog Systems. Technical Report
UUCS-04-015, School of Computing, University of
Utah.
E. Riloff and J. Wiebe. 2003. Learning Extraction Pat-
terns for Subjective Expressions. In Proc. EMNLP-
03.
E. Riloff. 1996. An Empirical Study of Automated
Dictionary Construction for Information Extraction
in Three Domains. Artificial Intelligence, 85:101–
134.
P. Turney. 2002. Thumbs up or thumbs down? Seman-
tic orientation applied to unsupervised classification
of reviews. In Proc. ACL-02.
C. Whitelaw, N. Garg, and S. Argamon. 2005. Us-
ing appraisal groups for sentiment analysis. In Proc.
CIKM-05.
J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Mar-
tin. 2004. Learning subjective language. Computa-
tional Linguistics, 30(3):277–308.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating
expressions of opinions and emotions in language.
Language Resources and Evaluation, 39(2/3).
H. Yu and V. Hatzivassiloglou. 2003. Towards an-
swering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proc. EMNLP-03.
</reference>
<page confidence="0.997721">
448
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.204632">
<title confidence="0.999975">Feature Subsumption for Opinion Analysis</title>
<author confidence="0.995208">Ellen Riloff</author>
<author confidence="0.995208">Siddharth</author>
<affiliation confidence="0.9985385">School of University of</affiliation>
<address confidence="0.613022">Salt Lake City, UT</address>
<author confidence="0.479204">Janyce</author>
<affiliation confidence="0.99989">Department of Computer University of</affiliation>
<address confidence="0.776541">Pittsburgh, PA</address>
<email confidence="0.999615">wiebe@cs.pitt.edu</email>
<abstract confidence="0.996096904761905">Lexical features are key to many approaches to sentiment analysis and opinion detection. A variety of representations have been used, including single words, multi-word Ngrams, phrases, and lexicosyntactic patterns. In this paper, we use a hierarchy formally define different types of lexical features and their relationship to one another, both in terms of representational coverage and performance. We use the subsumption hierarchy in two ways: (1) as an analytic tool to automatically identify complex features that outperform simpler features, and (2) to reduce a feature set by removing unnecessary features. We show that reducing the feature set improves performance on three opinion classification tasks, especially when combined with traditional feature selection.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>The Design, Implementation, and Use of the Ngram Statistics Package. In</title>
<date>2003</date>
<booktitle>Proc. Fourth Int’l Conference on Intelligent Text Processing and Computational Linguistics.</booktitle>
<contexts>
<context position="7011" citStr="Banerjee and Pedersen, 2003" startWordPosition="1068" endWordPosition="1072">assInfVP &lt;dobj&gt; is meant to be &lt;dobj&gt; Subj AuxVP &lt;dobj&gt; fact is &lt;dobj&gt; NP Prep &lt;np&gt; opinion on &lt;np&gt; ActVP Prep &lt;np&gt; agrees with &lt;np&gt; PassVP Prep &lt;np&gt; is worried about &lt;np&gt; InfVP Prep &lt;np&gt; to resort to &lt;np&gt; &lt;possessive&gt; NP &lt;noun&gt;’s speech Figure 1: Extraction Pattern Types Our goal is to use the subsumption hierarchy to identify Ngram and extraction pattern features that are more strongly associated with opinions than simpler features. We used three types of features in our research: unigrams, bigrams, and IE patterns. The Ngram features were generated using the Ngram Statistics Package (NSP) (Banerjee and Pedersen, 2003).1 The extraction patterns (EPs) were automatically generated using the Sundance/AutoSlog software package (Riloff and Phillips, 2004). AutoSlog relies on the Sundance shallow parser and can be applied exhaustively to a text corpus to generate IE patterns that can extract every noun phrase in the corpus. AutoSlog has been used to learn IE patterns for the domains of terrorism, joint ventures, and microelectronics (Riloff, 1996), as well as for opinion analysis (Riloff and Wiebe, 2003). Figure 1 shows the 17 types of extraction patterns that AutoSlog generates. PassVP refers to passive-voice ve</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>S. Banerjee and T. Pedersen. 2003. The Design, Implementation, and Use of the Ngram Statistics Package. In Proc. Fourth Int’l Conference on Intelligent Text Processing and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Esuli</author>
<author>F Sebastiani</author>
</authors>
<title>Determining the semantic orientation of terms through gloss analysis.</title>
<date>2005</date>
<booktitle>In Proc. CIKM-05.</booktitle>
<contexts>
<context position="2487" citStr="Esuli and Sebastiani, 2005" startWordPosition="369" endWordPosition="372"> a feature captures a subtlety or non-compositional meaning that a simpler feature does not. For example, “basket case” is a highly opinionated phrase, but the words “basket” and “case” individually are not. An open question in opinion analysis is how often more complex feature representations are needed, and which types of features are most valuable. Our first goal is to devise a method to automatically identify features that are representationally subsumed by a simpler feature but that are better opinion indicators. These subjective expressions could then be added to a subjectivity lexicon (Esuli and Sebastiani, 2005), and used to gain understanding about which types of complex features capture meaningful expressions that are important for opinion recognition. Many opinion classifiers are created by adopting a “kitchen sink” approach that throws together a variety of features. But in many cases adding new types of features does not improve performance. For example, Pang et al. (2002) found that unigrams outperformed bigrams, and unigrams outperformed the combination of unigrams plus bigrams. Our second goal is to automatically identify features that are unnecessary because similar features provide equal or</context>
</contexts>
<marker>Esuli, Sebastiani, 2005</marker>
<rawString>A. Esuli and F. Sebastiani. 2005. Determining the semantic orientation of terms through gloss analysis. In Proc. CIKM-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Forman</author>
</authors>
<title>An Extensive Empirical Study of Feature Selection Metrics for Text Classification.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1289</pages>
<contexts>
<context position="15980" citStr="Forman, 2003" startWordPosition="2517" endWordPosition="2519">erned with whether one feature is more general than another. But the purpose of using the subsumption hierarchy is to identify more complex features that outperform simpler ones. Applying the subsumption hierarchy to features without regard to performance would simply eliminate all features that have a more general counterpart in the feature set. For example, all bigrams would be discarded if their component unigrams were also present in the hierarchy. To estimate the quality of a feature, we use Information Gain (IG) because that has been shown to work well as a metric for feature selection (Forman, 2003). We will say that feature A behaviorally subsumes feature B if two criteria are met: (1) A representationally subsumes B, and (2) IG(A) &gt; IG(B) - 6, where 6 is a parameter representing an acceptable margin of performance difference. For example, if 6=0 then condition (2) means that feature A is just as valuable as feature B because its information gain is the same or higher. If 6&gt;0 then feature A is allowed to be a little worse than feature B, but within an acceptable margin. For example, 6=.0001 means that A’s information gain may be up to .0001 lower than B’s information gain, and that is c</context>
</contexts>
<marker>Forman, 2003</marker>
<rawString>G. Forman. 2003. An Extensive Empirical Study of Feature Selection Metrics for Text Classification. J. Mach. Learn. Res., 3:1289–1305.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Making Large-Scale Support Vector Machine Learning Practical.</title>
<date>1998</date>
<booktitle>Advances in Kernel Methods: Support Vector Machines.</booktitle>
<editor>In A. Smola B. Sch¨olkopf, C. Burges, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="25524" citStr="Joachims, 1998" startWordPosition="4062" endWordPosition="4063">periments to explore this hypothesis. First, we present the results for an SVM classifier trained using different sets of unigram, bigram, and extraction pattern features, both before and after subsumption. Next, we evaluate a standard feature selection approach as an alternative to subsumption and then show that combining subsumption with standard feature selection produces the best results of all. 5.1 Classification Experiments To see whether feature subsumption can improve classification performance, we trained an SVM classifier for each of the three opinion data sets. We used the SVM&amp;quot;ght (Joachims, 1998) package with a linear kernel. For the Polarity and OP data we discarded all features that have frequency &lt; 5, and for the MPQA data we discarded features that have frequency &lt; 2 because this data set is substantially smaller. All of our experimental results are averages over 3-fold cross-validation. First, we created 4 baseline classifiers: a 1Gram classifier that uses only the unigram features; a 1+2Gram classifier that uses unigram and bigram features; a 1+EP classifier that uses unigram and extraction pattern features, and a 1+2+EP classifier that uses all three types of features. Next, we</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>T. Joachims. 1998. Making Large-Scale Support Vector Machine Learning Practical. In A. Smola B. Sch¨olkopf, C. Burges, editor, Advances in Kernel Methods: Support Vector Machines. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S-M Kim</author>
<author>E Hovy</author>
</authors>
<title>Determining the sentiment of opinions.</title>
<date>2004</date>
<booktitle>In Proc. COLING-04.</booktitle>
<contexts>
<context position="32086" citStr="Kim and Hovy, 2004" startWordPosition="5139" endWordPosition="5142">curacy (%) 81.5 81 80.5 80 79.5 79 78.5 78 Accuracy (%) 74 73.5 73 72.5 72 447 statistically significant at the p &lt; 0.10 level. M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proc. KDD-04. 6 Related Work Many features and classification algorithms have been explored in sentiment analysis and opinion recognition. Lexical cues of differing complexities have been used, including single words and Ngrams (e.g., (Mullen and Collier, 2004; Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004)), as well as phrases and lexico-syntactic patterns (e.g, (Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005; Riloff and Wiebe, 2003; Whitelaw et al., 2005)). While many of these studies investigate combinations of features and feature selection, this is the first work that uses the notion of subsumption to compare Ngrams and lexico-syntactic patterns to identify complex features that outperform simpler counterparts and to reduce a combined feature set to improve opinion classification. 7 Conclusions This paper uses a subsumption hierarchy of feature representations as (1) an analytic tool to compare features of different complexities, and (2) an automati</context>
</contexts>
<marker>Kim, Hovy, 2004</marker>
<rawString>S-M. Kim and E. Hovy. 2004. Determining the sentiment of opinions. In Proc. COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="17637" citStr="Marcus et al., 1993" startWordPosition="2792" endWordPosition="2795"> Sets We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al., 2004), the Polarity data set5 created by (Pang and Lee, 2004), and the MPQA data set created by (Wiebe et al., 2005).6 The OP and Polarity data sets involve document-level opinion classification, while the MPQA data set involves 5Version v2.0, which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/ 6Available at http://www.cs.pitt.edu/mpqa/databaserelease/ sentence-level classification. The OP data consists of 2,452 documents from the Penn Treebank (Marcus et al., 1993). Metadata tags assigned by the Wall Street Journal define the opinion/non-opinion classes: the class of any document labeled Editorial, Letter to the Editor, Arts &amp; Leisure Review, or Viewpoint by the Wall Street Journal is opinion, and the class of documents in all other categories (such as Business and News) is non-opinion. This data set is highly skewed, with only 9% of the documents belonging to the opinion class. Consequently, a trivial (but useless) opinion classifier that labels all documents as nonopinion articles would achieve 91% accuracy. The Polarity data consists of 700 positive </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mullen</author>
<author>N Collier</author>
</authors>
<title>Sentiment Analysis Using Support Vector Machines with Diverse Information Sources.</title>
<date>2004</date>
<booktitle>In Proc. EMNLP-04.</booktitle>
<contexts>
<context position="31924" citStr="Mullen and Collier, 2004" startWordPosition="5113" endWordPosition="5116">ubsumption 8=0.002 Feature Selection Subsumption 8=0.002 + Feature Selection Baseline Subsumption 8=0.0005 Feature Selection Subsumption 8=0.0005 + Feature Selection Accuracy (%) 81.5 81 80.5 80 79.5 79 78.5 78 Accuracy (%) 74 73.5 73 72.5 72 447 statistically significant at the p &lt; 0.10 level. M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proc. KDD-04. 6 Related Work Many features and classification algorithms have been explored in sentiment analysis and opinion recognition. Lexical cues of differing complexities have been used, including single words and Ngrams (e.g., (Mullen and Collier, 2004; Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004)), as well as phrases and lexico-syntactic patterns (e.g, (Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005; Riloff and Wiebe, 2003; Whitelaw et al., 2005)). While many of these studies investigate combinations of features and feature selection, this is the first work that uses the notion of subsumption to compare Ngrams and lexico-syntactic patterns to identify complex features that outperform simpler counterparts and to reduce a combined feature set to improve opinion classification. 7 Conclus</context>
</contexts>
<marker>Mullen, Collier, 2004</marker>
<rawString>T. Mullen and N. Collier. 2004. Sentiment Analysis Using Support Vector Machines with Diverse Information Sources. In Proc. EMNLP-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proc. ACL-04.</booktitle>
<contexts>
<context position="17200" citStr="Pang and Lee, 2004" startWordPosition="2738" endWordPosition="2741">nsidered to be an acceptable performance difference (i.e., A is good enough that we are comfortable discarding B in favor of the more general feature A). Note that based on the subsumption hierarchy shown in Figure 2, all 1Grams will always survive the subsumption process because they cannot be subsumed by any other types of features. Our goal is to identify complex features that are worth adding to a set of unigram features. 3 Data Sets We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al., 2004), the Polarity data set5 created by (Pang and Lee, 2004), and the MPQA data set created by (Wiebe et al., 2005).6 The OP and Polarity data sets involve document-level opinion classification, while the MPQA data set involves 5Version v2.0, which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/ 6Available at http://www.cs.pitt.edu/mpqa/databaserelease/ sentence-level classification. The OP data consists of 2,452 documents from the Penn Treebank (Marcus et al., 1993). Metadata tags assigned by the Wall Street Journal define the opinion/non-opinion classes: the class of any document labeled Editorial, Letter to the Editor, Arts</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>B. Pang and L. Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proc. ACL-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Pang</author>
<author>L Lee</author>
<author>S Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment Classification using Machine Learning Techniques. In</title>
<date>2002</date>
<booktitle>Proc. EMNLP-02.</booktitle>
<contexts>
<context position="2860" citStr="Pang et al. (2002)" startWordPosition="430" endWordPosition="433">a method to automatically identify features that are representationally subsumed by a simpler feature but that are better opinion indicators. These subjective expressions could then be added to a subjectivity lexicon (Esuli and Sebastiani, 2005), and used to gain understanding about which types of complex features capture meaningful expressions that are important for opinion recognition. Many opinion classifiers are created by adopting a “kitchen sink” approach that throws together a variety of features. But in many cases adding new types of features does not improve performance. For example, Pang et al. (2002) found that unigrams outperformed bigrams, and unigrams outperformed the combination of unigrams plus bigrams. Our second goal is to automatically identify features that are unnecessary because similar features provide equal or better coverage and discriminatory value. Our hypothesis is that a reduced feature set, which selectively combines unigrams with only the most valuable complex features, will perform better than a larger feature set that includes the entire “kitchen sink” of features. In this paper, we explore the use of a subsumption hierarchy to formally define the subsumption relatio</context>
<context position="31943" citStr="Pang et al., 2002" startWordPosition="5117" endWordPosition="5120"> Selection Subsumption 8=0.002 + Feature Selection Baseline Subsumption 8=0.0005 Feature Selection Subsumption 8=0.0005 + Feature Selection Accuracy (%) 81.5 81 80.5 80 79.5 79 78.5 78 Accuracy (%) 74 73.5 73 72.5 72 447 statistically significant at the p &lt; 0.10 level. M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proc. KDD-04. 6 Related Work Many features and classification algorithms have been explored in sentiment analysis and opinion recognition. Lexical cues of differing complexities have been used, including single words and Ngrams (e.g., (Mullen and Collier, 2004; Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004)), as well as phrases and lexico-syntactic patterns (e.g, (Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005; Riloff and Wiebe, 2003; Whitelaw et al., 2005)). While many of these studies investigate combinations of features and feature selection, this is the first work that uses the notion of subsumption to compare Ngrams and lexico-syntactic patterns to identify complex features that outperform simpler counterparts and to reduce a combined feature set to improve opinion classification. 7 Conclusions This paper use</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>B. Pang, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment Classification using Machine Learning Techniques. In Proc. EMNLP-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A-M Popescu</author>
<author>O Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proc. HLT-EMNLP-05.</booktitle>
<contexts>
<context position="32131" citStr="Popescu and Etzioni, 2005" startWordPosition="5147" endWordPosition="5150">5 78 Accuracy (%) 74 73.5 73 72.5 72 447 statistically significant at the p &lt; 0.10 level. M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proc. KDD-04. 6 Related Work Many features and classification algorithms have been explored in sentiment analysis and opinion recognition. Lexical cues of differing complexities have been used, including single words and Ngrams (e.g., (Mullen and Collier, 2004; Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004)), as well as phrases and lexico-syntactic patterns (e.g, (Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005; Riloff and Wiebe, 2003; Whitelaw et al., 2005)). While many of these studies investigate combinations of features and feature selection, this is the first work that uses the notion of subsumption to compare Ngrams and lexico-syntactic patterns to identify complex features that outperform simpler counterparts and to reduce a combined feature set to improve opinion classification. 7 Conclusions This paper uses a subsumption hierarchy of feature representations as (1) an analytic tool to compare features of different complexities, and (2) an automatic tool to remove unnecessary features to impr</context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>A-M. Popescu and O. Etzioni. 2005. Extracting product features and opinions from reviews. In Proc. HLT-EMNLP-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>W Phillips</author>
</authors>
<title>An Introduction to the Sundance and AutoSlog Systems.</title>
<date>2004</date>
<tech>Technical Report UUCS-04-015,</tech>
<institution>School of Computing, University of Utah.</institution>
<contexts>
<context position="7145" citStr="Riloff and Phillips, 2004" startWordPosition="1086" endWordPosition="1089">sVP Prep &lt;np&gt; is worried about &lt;np&gt; InfVP Prep &lt;np&gt; to resort to &lt;np&gt; &lt;possessive&gt; NP &lt;noun&gt;’s speech Figure 1: Extraction Pattern Types Our goal is to use the subsumption hierarchy to identify Ngram and extraction pattern features that are more strongly associated with opinions than simpler features. We used three types of features in our research: unigrams, bigrams, and IE patterns. The Ngram features were generated using the Ngram Statistics Package (NSP) (Banerjee and Pedersen, 2003).1 The extraction patterns (EPs) were automatically generated using the Sundance/AutoSlog software package (Riloff and Phillips, 2004). AutoSlog relies on the Sundance shallow parser and can be applied exhaustively to a text corpus to generate IE patterns that can extract every noun phrase in the corpus. AutoSlog has been used to learn IE patterns for the domains of terrorism, joint ventures, and microelectronics (Riloff, 1996), as well as for opinion analysis (Riloff and Wiebe, 2003). Figure 1 shows the 17 types of extraction patterns that AutoSlog generates. PassVP refers to passive-voice verb phrases (VPs), ActVP refers to active-voice VPs, InfVP refers to infinitive VPs, and AuxVP refers &apos;NSP is freely available for use </context>
</contexts>
<marker>Riloff, Phillips, 2004</marker>
<rawString>E. Riloff and W. Phillips. 2004. An Introduction to the Sundance and AutoSlog Systems. Technical Report UUCS-04-015, School of Computing, University of Utah.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Wiebe</author>
</authors>
<title>Learning Extraction Patterns for Subjective Expressions. In</title>
<date>2003</date>
<booktitle>Proc. EMNLP03.</booktitle>
<contexts>
<context position="4794" citStr="Riloff and Wiebe, 2003" startWordPosition="713" endWordPosition="716">ation performance. 2 The Subsumption Hierarchy 2.1 Text Representations We analyze two feature representations that have been used for opinion analysis: Ngrams and Extraction Patterns. Information extraction (IE) patterns are lexico-syntactic patterns that represent expressions which identify role relationships. For example, the pattern “&lt;subj&gt; ActVP(recommended)” extracts the subject of active-voice instances of the verb “recommended” as the recommender. The pattern “&lt;subj&gt; PassVP(recommended)” extracts the subject of passive-voice instances of “recommended” as the object being recommended. (Riloff and Wiebe, 2003) explored the idea of using extraction patterns to represent more complex subjective expressions that have noncompositional meanings. For example, the expression “drive (someone) up the wall” expresses the feeling of being annoyed, but the meanings of the words “drive”, “up”, and “wall” have no emotional connotations individually. Furthermore, this expression is not a fixed word sequence that can be adequately modeled by Ngrams. Any noun phrase can appear between the words “drive’ and “up”, so a flexible representation is needed to capture the general pattern “drives &lt;NP&gt; up the wall”. This ex</context>
<context position="7500" citStr="Riloff and Wiebe, 2003" startWordPosition="1148" endWordPosition="1151">ms, bigrams, and IE patterns. The Ngram features were generated using the Ngram Statistics Package (NSP) (Banerjee and Pedersen, 2003).1 The extraction patterns (EPs) were automatically generated using the Sundance/AutoSlog software package (Riloff and Phillips, 2004). AutoSlog relies on the Sundance shallow parser and can be applied exhaustively to a text corpus to generate IE patterns that can extract every noun phrase in the corpus. AutoSlog has been used to learn IE patterns for the domains of terrorism, joint ventures, and microelectronics (Riloff, 1996), as well as for opinion analysis (Riloff and Wiebe, 2003). Figure 1 shows the 17 types of extraction patterns that AutoSlog generates. PassVP refers to passive-voice verb phrases (VPs), ActVP refers to active-voice VPs, InfVP refers to infinitive VPs, and AuxVP refers &apos;NSP is freely available for use under the GPL from http://search.cpan.org/dist/Text-NSP. We discarded Ngrams that consisted entirely of stopwords. We used a list of 281 stopwords. 441 to VPs where the main verb is a form of “to be” or “to have”. Subjects (subj), direct objects (dobj), PP objects (np), and possessives can be extracted by the patterns.2 2.2 The Subsumption Hierarchy We </context>
<context position="32155" citStr="Riloff and Wiebe, 2003" startWordPosition="5151" endWordPosition="5154">3 72.5 72 447 statistically significant at the p &lt; 0.10 level. M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proc. KDD-04. 6 Related Work Many features and classification algorithms have been explored in sentiment analysis and opinion recognition. Lexical cues of differing complexities have been used, including single words and Ngrams (e.g., (Mullen and Collier, 2004; Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004)), as well as phrases and lexico-syntactic patterns (e.g, (Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005; Riloff and Wiebe, 2003; Whitelaw et al., 2005)). While many of these studies investigate combinations of features and feature selection, this is the first work that uses the notion of subsumption to compare Ngrams and lexico-syntactic patterns to identify complex features that outperform simpler counterparts and to reduce a combined feature set to improve opinion classification. 7 Conclusions This paper uses a subsumption hierarchy of feature representations as (1) an analytic tool to compare features of different complexities, and (2) an automatic tool to remove unnecessary features to improve opinion classificati</context>
</contexts>
<marker>Riloff, Wiebe, 2003</marker>
<rawString>E. Riloff and J. Wiebe. 2003. Learning Extraction Patterns for Subjective Expressions. In Proc. EMNLP03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>An Empirical Study of Automated Dictionary Construction for Information Extraction in Three Domains.</title>
<date>1996</date>
<journal>Artificial Intelligence,</journal>
<volume>85</volume>
<pages>134</pages>
<contexts>
<context position="7442" citStr="Riloff, 1996" startWordPosition="1140" endWordPosition="1141"> three types of features in our research: unigrams, bigrams, and IE patterns. The Ngram features were generated using the Ngram Statistics Package (NSP) (Banerjee and Pedersen, 2003).1 The extraction patterns (EPs) were automatically generated using the Sundance/AutoSlog software package (Riloff and Phillips, 2004). AutoSlog relies on the Sundance shallow parser and can be applied exhaustively to a text corpus to generate IE patterns that can extract every noun phrase in the corpus. AutoSlog has been used to learn IE patterns for the domains of terrorism, joint ventures, and microelectronics (Riloff, 1996), as well as for opinion analysis (Riloff and Wiebe, 2003). Figure 1 shows the 17 types of extraction patterns that AutoSlog generates. PassVP refers to passive-voice verb phrases (VPs), ActVP refers to active-voice VPs, InfVP refers to infinitive VPs, and AuxVP refers &apos;NSP is freely available for use under the GPL from http://search.cpan.org/dist/Text-NSP. We discarded Ngrams that consisted entirely of stopwords. We used a list of 281 stopwords. 441 to VPs where the main verb is a form of “to be” or “to have”. Subjects (subj), direct objects (dobj), PP objects (np), and possessives can be ext</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. An Empirical Study of Automated Dictionary Construction for Information Extraction in Three Domains. Artificial Intelligence, 85:101– 134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proc. ACL-02.</booktitle>
<contexts>
<context position="31957" citStr="Turney, 2002" startWordPosition="5121" endWordPosition="5122">ion 8=0.002 + Feature Selection Baseline Subsumption 8=0.0005 Feature Selection Subsumption 8=0.0005 + Feature Selection Accuracy (%) 81.5 81 80.5 80 79.5 79 78.5 78 Accuracy (%) 74 73.5 73 72.5 72 447 statistically significant at the p &lt; 0.10 level. M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proc. KDD-04. 6 Related Work Many features and classification algorithms have been explored in sentiment analysis and opinion recognition. Lexical cues of differing complexities have been used, including single words and Ngrams (e.g., (Mullen and Collier, 2004; Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004)), as well as phrases and lexico-syntactic patterns (e.g, (Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005; Riloff and Wiebe, 2003; Whitelaw et al., 2005)). While many of these studies investigate combinations of features and feature selection, this is the first work that uses the notion of subsumption to compare Ngrams and lexico-syntactic patterns to identify complex features that outperform simpler counterparts and to reduce a combined feature set to improve opinion classification. 7 Conclusions This paper uses a subsumptio</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. Turney. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. In Proc. ACL-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Whitelaw</author>
<author>N Garg</author>
<author>S Argamon</author>
</authors>
<title>Using appraisal groups for sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proc. CIKM-05.</booktitle>
<contexts>
<context position="32179" citStr="Whitelaw et al., 2005" startWordPosition="5155" endWordPosition="5158">lly significant at the p &lt; 0.10 level. M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proc. KDD-04. 6 Related Work Many features and classification algorithms have been explored in sentiment analysis and opinion recognition. Lexical cues of differing complexities have been used, including single words and Ngrams (e.g., (Mullen and Collier, 2004; Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004)), as well as phrases and lexico-syntactic patterns (e.g, (Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005; Riloff and Wiebe, 2003; Whitelaw et al., 2005)). While many of these studies investigate combinations of features and feature selection, this is the first work that uses the notion of subsumption to compare Ngrams and lexico-syntactic patterns to identify complex features that outperform simpler counterparts and to reduce a combined feature set to improve opinion classification. 7 Conclusions This paper uses a subsumption hierarchy of feature representations as (1) an analytic tool to compare features of different complexities, and (2) an automatic tool to remove unnecessary features to improve opinion classification performance. Experime</context>
</contexts>
<marker>Whitelaw, Garg, Argamon, 2005</marker>
<rawString>C. Whitelaw, N. Garg, and S. Argamon. 2005. Using appraisal groups for sentiment analysis. In Proc. CIKM-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>R Bruce</author>
<author>M Bell</author>
<author>M Martin</author>
</authors>
<title>Learning subjective language.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>3</issue>
<contexts>
<context position="17144" citStr="Wiebe et al., 2004" startWordPosition="2727" endWordPosition="2730">to .0001 lower than B’s information gain, and that is considered to be an acceptable performance difference (i.e., A is good enough that we are comfortable discarding B in favor of the more general feature A). Note that based on the subsumption hierarchy shown in Figure 2, all 1Grams will always survive the subsumption process because they cannot be subsumed by any other types of features. Our goal is to identify complex features that are worth adding to a set of unigram features. 3 Data Sets We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al., 2004), the Polarity data set5 created by (Pang and Lee, 2004), and the MPQA data set created by (Wiebe et al., 2005).6 The OP and Polarity data sets involve document-level opinion classification, while the MPQA data set involves 5Version v2.0, which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/ 6Available at http://www.cs.pitt.edu/mpqa/databaserelease/ sentence-level classification. The OP data consists of 2,452 documents from the Penn Treebank (Marcus et al., 1993). Metadata tags assigned by the Wall Street Journal define the opinion/non-opinion classes: the class of an</context>
<context position="32009" citStr="Wiebe et al., 2004" startWordPosition="5127" endWordPosition="5130">umption 8=0.0005 Feature Selection Subsumption 8=0.0005 + Feature Selection Accuracy (%) 81.5 81 80.5 80 79.5 79 78.5 78 Accuracy (%) 74 73.5 73 72.5 72 447 statistically significant at the p &lt; 0.10 level. M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proc. KDD-04. 6 Related Work Many features and classification algorithms have been explored in sentiment analysis and opinion recognition. Lexical cues of differing complexities have been used, including single words and Ngrams (e.g., (Mullen and Collier, 2004; Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004)), as well as phrases and lexico-syntactic patterns (e.g, (Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005; Riloff and Wiebe, 2003; Whitelaw et al., 2005)). While many of these studies investigate combinations of features and feature selection, this is the first work that uses the notion of subsumption to compare Ngrams and lexico-syntactic patterns to identify complex features that outperform simpler counterparts and to reduce a combined feature set to improve opinion classification. 7 Conclusions This paper uses a subsumption hierarchy of feature representations as (1) an ana</context>
</contexts>
<marker>Wiebe, Wilson, Bruce, Bell, Martin, 2004</marker>
<rawString>J. Wiebe, T. Wilson, R. Bruce, M. Bell, and M. Martin. 2004. Learning subjective language. Computational Linguistics, 30(3):277–308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Wiebe</author>
<author>T Wilson</author>
<author>C Cardie</author>
</authors>
<title>Annotating expressions of opinions and emotions in language. Language Resources and Evaluation,</title>
<date>2005</date>
<pages>39--2</pages>
<contexts>
<context position="17255" citStr="Wiebe et al., 2005" startWordPosition="2749" endWordPosition="2752">e., A is good enough that we are comfortable discarding B in favor of the more general feature A). Note that based on the subsumption hierarchy shown in Figure 2, all 1Grams will always survive the subsumption process because they cannot be subsumed by any other types of features. Our goal is to identify complex features that are worth adding to a set of unigram features. 3 Data Sets We used three opinion-related data sets for our analyses and experiments: the OP data set created by (Wiebe et al., 2004), the Polarity data set5 created by (Pang and Lee, 2004), and the MPQA data set created by (Wiebe et al., 2005).6 The OP and Polarity data sets involve document-level opinion classification, while the MPQA data set involves 5Version v2.0, which is available at: http://www.cs.cornell.edu/people/pabo/movie-review-data/ 6Available at http://www.cs.pitt.edu/mpqa/databaserelease/ sentence-level classification. The OP data consists of 2,452 documents from the Penn Treebank (Marcus et al., 1993). Metadata tags assigned by the Wall Street Journal define the opinion/non-opinion classes: the class of any document labeled Editorial, Letter to the Editor, Arts &amp; Leisure Review, or Viewpoint by the Wall Street Jour</context>
</contexts>
<marker>Wiebe, Wilson, Cardie, 2005</marker>
<rawString>J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating expressions of opinions and emotions in language. Language Resources and Evaluation, 39(2/3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yu</author>
<author>V Hatzivassiloglou</author>
</authors>
<title>Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences.</title>
<date>2003</date>
<booktitle>In Proc. EMNLP-03.</booktitle>
<contexts>
<context position="31988" citStr="Yu and Hatzivassiloglou, 2003" startWordPosition="5123" endWordPosition="5126">Feature Selection Baseline Subsumption 8=0.0005 Feature Selection Subsumption 8=0.0005 + Feature Selection Accuracy (%) 81.5 81 80.5 80 79.5 79 78.5 78 Accuracy (%) 74 73.5 73 72.5 72 447 statistically significant at the p &lt; 0.10 level. M. Hu and B. Liu. 2004. Mining and summarizing customer reviews. In Proc. KDD-04. 6 Related Work Many features and classification algorithms have been explored in sentiment analysis and opinion recognition. Lexical cues of differing complexities have been used, including single words and Ngrams (e.g., (Mullen and Collier, 2004; Pang et al., 2002; Turney, 2002; Yu and Hatzivassiloglou, 2003; Wiebe et al., 2004)), as well as phrases and lexico-syntactic patterns (e.g, (Kim and Hovy, 2004; Hu and Liu, 2004; Popescu and Etzioni, 2005; Riloff and Wiebe, 2003; Whitelaw et al., 2005)). While many of these studies investigate combinations of features and feature selection, this is the first work that uses the notion of subsumption to compare Ngrams and lexico-syntactic patterns to identify complex features that outperform simpler counterparts and to reduce a combined feature set to improve opinion classification. 7 Conclusions This paper uses a subsumption hierarchy of feature represen</context>
</contexts>
<marker>Yu, Hatzivassiloglou, 2003</marker>
<rawString>H. Yu and V. Hatzivassiloglou. 2003. Towards answering opinion questions: Separating facts from opinions and identifying the polarity of opinion sentences. In Proc. EMNLP-03.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>