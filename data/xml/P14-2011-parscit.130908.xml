<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000090">
<title confidence="0.998635">
A Feature-Enriched Tree Kernel for Relation Extraction
</title>
<author confidence="0.993802">
Le Sun and Xianpei Han
</author>
<affiliation confidence="0.880600666666667">
State Key Laboratory of Computer Science
Institute of Software, Chinese Academy of Sciences
HaiDian District, Beijing, China.
</affiliation>
<email confidence="0.995906">
{sunle, xianpei}@nfs.iscas.ac.cn
</email>
<sectionHeader confidence="0.997251" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999609235294117">
Tree kernel is an effective technique for rela-
tion extraction. However, the traditional syn-
tactic tree representation is often too coarse or
ambiguous to accurately capture the semantic
relation information between two entities. In
this paper, we propose a new tree kernel,
called feature-enriched tree kernel (FTK),
which can enhance the traditional tree kernel
by: 1) refining the syntactic tree representation
by annotating each tree node with a set of dis-
criminant features; and 2) proposing a new
tree kernel which can better measure the syn-
tactic tree similarity by taking all features into
consideration. Experimental results show that
our method can achieve a 5.4% F-measure im-
provement over the traditional convolution
tree kernel.
</bodyText>
<sectionHeader confidence="0.999512" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998713">
Relation Extraction (RE) aims to identify a set of
predefined relations between pairs of entities in
text. In recent years, relation extraction has re-
ceived considerable research attention. An effec-
tive technique is the tree kernel (Zelenko et al.,
2003; Zhou et al., 2007; Zhang et al., 2006; Qian
et al., 2008), which can exploit syntactic parse tree
information for relation extraction. Given a pair of
entities in a sentence, the tree kernel-based RE
method first represents the relation information
between them using a proper sub-tree (e.g., SPT –
the sub-tree enclosed by the shortest path linking
the two involved entities). For example, the three
syntactic tree representations in Figure 1. Then the
similarity between two trees are computed using a
tree kernel, e.g., the convolution tree kernel pro-
posed by Collins and Duffy (2001). Finally, new
relation instances are extracted using kernel based
classifiers, e.g., the SVM classifier.
Unfortunately, one main shortcoming of the
traditional tree kernel is that the syntactic tree rep-
resentation usually cannot accurately capture the
</bodyText>
<figureCaption confidence="0.99688">
Figure 1. The ambiguity of possessive structure
</figureCaption>
<bodyText confidence="0.988912588235294">
relation information between two entities. This is
mainly due to the following two reasons:
1) The syntactic tree focuses on representing
syntactic relation/structure, which is often too
coarse or ambiguous to capture the semantic re-
lation information. In a syntactic tree, each node
indicates a clause/phrase/word and is only labeled
with a Treebank tag (Marcus et al., 1993). The
Treebank tag, unfortunately, is usually too coarse
or too general to capture semantic information.
For example, all the three trees in Figure 1 share
the same possessive syntactic structure, but ex-
press quite different semantic relations: where
“Mary’s brothers” expresses PER-SOC Family
relation, “Mary’s toys” expresses Possession rela-
tion, and “New York’s airports” expresses PHYS-
Located relation.
2) Some critical information may lost during
sub-tree representation extraction. For example,
in Figure 2, when extracting SPT representation,
all nodes outside the shortest-path will be pruned,
such as the nodes [NN plants] and [POS ’s] in tree
T1. In this pruning process, the critical infor-
mation “word town is the possessor of the posses-
sive phrase the town’s plants” will be lost, which
in turn will lead to the misclassification of the
DISC relation between one and town.
This paper proposes a new tree kernel, referred
as feature-enriched tree kernel (FTK), which can
effectively resolve the above problems by enhanc-
ing the traditional tree kernel in following ways:
1) We refine the syntactic tree representa-
tion by annotating each tree node with a set of dis-
criminant features. These features are utilized to
</bodyText>
<figure confidence="0.9996516875">
(a) (b) (c)
Mary &apos;s
NN POS
NP NN
NP
brothers
Mary &apos;s
NN POS
NP NN
NP
toys
NN POS
NY &apos;s
NP NN
NP
airports
</figure>
<page confidence="0.97886">
61
</page>
<bodyText confidence="0.906361928571429">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 61–67,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
better capture the semantic relation information
between two entities. For example, in order to dif-
ferentiate the syntactic tree representations in Fig-
ure 1, FTK will annotate them with several fea-
tures indicating “brother is a male sibling”, “toy
is an artifact”, “Iew York is a city”, “airport is
facility”, etc.
2) Based on the refined syntactic tree repre-
sentation, we propose a new tree kernel – feature-
enriched tree kernel, which can better measure the
similarity between two trees by also taking all fea-
tures into consideration.
</bodyText>
<equation confidence="0.923737190476191">
(T2)
NP 2 PP 3
Possessor, Contain_Arg2_GPE,
RootPath:NP-PP-NP,
EndWithPOS, ...
EntType:GPE, MentionType:NOM,
RootPath:NP-PP-NP-NP, ...
WN:town, WN:district, WN:region,
WN:location, Match_Arg2_GPE ...
PPPhrase, RootPath:NP-PP,
Contain_Arg2_ORG, ...
PP_Head, RootPath:NP-PP-NP,
Contain_Arg2_ORG,...
EntType:ORG, MentionType:NOM,
RootPath:NP-PP-NP-NP, ...
WN:team, WN:social_unit,
WN:group, WN:organization,
Match_Arg2_ORG ...
NP
1
E1
</equation>
<figure confidence="0.989720794871795">
4
NP
5
CD
6 IN 7 NP 8
one
of
9 10
E2
11
DT
12
NN
13
the
14
town
15
(T4) NP 1
NP 2 PP 3
E1
4 5
CD
6 IN 7 NP8
one
9 10
E2
11
DT
12
NN
13
the
14
team
15
Feature Vector
PossessivePhrase, RootPath:NP-PP,
Contain_Arg2_GPE, ...
</figure>
<figureCaption confidence="0.964014">
Figure 2. SPT representation extraction
</figureCaption>
<figure confidence="0.9965903125">
NP
NP PP
NP PP
NP
NP
E1
E1
Prune
CD
IN
NP
CD
NP
IN
of
E2
one
of
one
E2
POS NN
&apos;s plants
DT
NN
DT
NN
the
town
the
town
NP
NP PP
NP PP
NP
E1
Prune
NP
E1
CD
PP
IN
IN
NP
CD
NP
IN
of
E2
NP
one
of
one
E2
DT NN
NN
DT
NN
in USA
the teams
the
teams
NP
(T1) NP (T2)
(T3) (T4)
</figure>
<bodyText confidence="0.999919363636364">
We have experimented our method on the ACE
2004 RDC corpus. Experimental results show that
our method can achieve a 5.4% F-measure im-
provement over the traditional convolution tree
kernel based method.
This paper is organized as follows. Section 2
describes the feature-enriched tree kernel. Section
3 presents the features we used. Section 4 dis-
cusses the experiments. Section 5 briefly reviews
the related work. Finally Section 6 concludes this
paper.
</bodyText>
<sectionHeader confidence="0.960818" genericHeader="method">
2 The Feature-Enriched Tree Kernel
</sectionHeader>
<bodyText confidence="0.9993235">
In this section, we describe the proposed feature-
enriched tree kernel (FTK) for relation extraction.
</bodyText>
<subsectionHeader confidence="0.996038">
2.1 Refining Syntactic Tree Representation
</subsectionHeader>
<bodyText confidence="0.999334333333333">
As described in above, syntactic tree is often too
coarse or too ambiguous to represent the semantic
relation information between two entities. To re-
solve this problem, we refine the syntactic tree
representation by annotating each tree node with
a set of discriminant features.
</bodyText>
<figureCaption confidence="0.972861">
Figure 3. Syntactic tree enriched with features
</figureCaption>
<bodyText confidence="0.998253619047619">
Specifically, for each node in a syntactic tree
, we represent it as a tuple:
where is its phrase label (i.e., its Treebank tag),
and is a feature vector which indicates the
characteristics of node , which is represented as:
NP
of
where fi is a feature and is associated with a weight
. The feature we used includes charac-
teristics of relation instance, phrase properties and
context information (See Section 3 for details).
For demonstration, Figure 3 shows the feature-
enriched version of tree T2 and tree T4 in Figure
2. We can see that, although T2 and T4 share the
same syntactic structure, the annotated features
can still differentiate them. For example, the IP5
node in tree T2 and the IP5 node in tree T4 are
differentiated using their features Possessive-
Phrase and PPPhrase, which indicate that IP5 in
T2 is a possessive phrase, meanwhile IP5 in T4 is
a preposition phrase.
</bodyText>
<subsectionHeader confidence="0.991555">
2.2 Feature-Enriched Tree Kernel
</subsectionHeader>
<bodyText confidence="0.999363875">
This section describes how to take into account
the annotated features for a better tree similarity.
In Collins and Duffy’s convolution tree kernel
(CTK), the similarity between two trees T1 and T2
is the number of their common sub-trees:
Using this formula, CTK only considers whether
two enumerated sub-trees have the identical syn-
tactic structure (the indicator is 1 if the
</bodyText>
<page confidence="0.996324">
62
</page>
<bodyText confidence="0.99817315625">
two sub-trees and have the identical syntac-
tic structure and 0 otherwise). Such an assumption
makes CTK can only capture the syntactic struc-
ture similarity between two trees, while ignoring
other useful information.
To resolve the above problem, the feature-en-
riched tree kernel (FTK) compute the similarity
between two trees as the sum of the similarities
between their common sub-trees:
where is the similarity between enumer-
ated sub-trees and , which is computed as:
where is the same indicator function as in
CTK; is a pair of aligned nodes between
and , where and are correspondingly in
the same position of tree and ; is the
set of all aligned node pairs; is the
feature vector similarity between node and ,
computed as the dot product between their feature
vectors and .
Notice that, if all nodes are not annotated with
features, will be equal to . In this
perspective, we can view as a similarity
adjusted version of , i.e., only
considers whether two nodes are equal, in contrast
further considers the feature similarity
between two nodes.
The Computation of FTK. As the same as
CTK, FTK can be efficiently computed as:
where is the set of nodes in tree , and
evaluates the sum of the similarities of
common sub-trees rooted at node and node ,
which is recursively computed as follows:
</bodyText>
<listItem confidence="0.906356666666667">
1) If the production rules of and are differ-
ent, = 0;
2) If both and is pre-terminal nodes,
;
Otherwise go to step 3;
3) Calculate recursively as:
</listItem>
<equation confidence="0.7182565">
¢(n1,n2) = ¸ £ (1 + sim(n1, n2))
#ch(n1)
X£ (1 + ¢(ch(n1, k), ch(n2, k))
k=1
</equation>
<sectionHeader confidence="0.999755" genericHeader="method">
3 Features for Relation Extraction
</sectionHeader>
<bodyText confidence="0.9994005">
This section presents the features we used to en-
rich the syntactic tree representation.
</bodyText>
<subsectionHeader confidence="0.991091">
3.1 Instance Feature
</subsectionHeader>
<bodyText confidence="0.8336535">
Relation instances of the same type often share
some common characteristics. In this paper, we
add the following instance features to the root
node of a sub-tree representation:
</bodyText>
<listItem confidence="0.999797416666667">
1) Syntactico-Semantic structure. A fea-
ture indicates whether a relation instance has the
following four syntactico-semantic structures in
(Chan &amp; Roth, 2011) – Premodifiers, Possessive,
Preposition, Formulaic and Verbal.
2) Entity-related information of argu-
ments. Features about the entity information of
arguments, including: a) #TP1-#TP2: the concat
of the major entity types of arguments; b) #ST1-
#ST2: the concat of the sub entity types of argu-
ments; c) #MT1-#MT2: the concat of the mention
types of arguments.
</listItem>
<bodyText confidence="0.7913188">
3) Base phrase chunking features. Fea-
tures about the phrase path between two argu-
ments and the phrases’ head before and after the
arguments, which are the same as the phrase
chunking features in (Zhou, et al., 2005).
</bodyText>
<subsectionHeader confidence="0.999503">
3.2 Phrase Feature
</subsectionHeader>
<bodyText confidence="0.9976356">
As discussed in above, the Treebank tag is too
coarse to capture the property of a phrase node.
Therefore, we enrich each phrase node with fea-
tures about its lexical pattern, its content infor-
mation, and its lexical semantics:
</bodyText>
<listItem confidence="0.893225041666667">
1) Lexical Pattern. We capture the lexical
pattern of a phrase node using the following fea-
tures: a) LP_Poss: A feature indicates the node is
a possessive phrase; b) LP_PP: A feature indi-
cates the node is a preposition phrase; c) LP_CC:
A feature indicates the node is a conjunction
phrase; d) LP_EndWithPUNC: A feature indicates
the node ends with a punctuation; e) LP_EndWith-
POSS: A feature indicates the node ends with a
possessive word.
2) Content Information. We capture the
property of a node’s content using the following
features: a) MB_#Num: The number of mentions
contained in the phrase; b) MB_C_#Type: A fea-
ture indicates that the phrase contains a mention
with major entity type #Type; c) MW_#Num: The
number of words within the phrase.
3) Lexical Semantics. If the node is a pre-
terminal node, we capture its lexical semantic by
adding features indicating its WordNet sense in-
formation. Specifically, the first WordNet sense
of the terminal word, and all this sense’s hyponym
senses will be added as features. For example,
WordNet senses {New York#1, city#1, district#1,
</listItem>
<page confidence="0.998922">
63
</page>
<bodyText confidence="0.9971215">
region#1, ...} will be added as features to the [II
Iew York] node in Figure 1.
</bodyText>
<subsectionHeader confidence="0.989524">
3.3 Context Information Feature
</subsectionHeader>
<bodyText confidence="0.814866225">
The context information of a phrase node is criti-
cal for identifying the role and the importance of
a sub-tree in the whole relation instance. This pa-
per captures the following context information:
1) Contextual path from sub-tree root to
the phrase node. As shown in Zhou et al. (2007),
the context path from root to the phrase node is an
effective context information feature. In this paper,
we use the same settings in (Zhou et al., 2007), i.e.,
each phrase node is enriched with its context paths
of length 1, 2, 3.
2) Relative position with arguments. We
observed that a phrase’s relative position with the
relation’s arguments is useful for identifying the
role of the phrase node in the whole relation in-
stance. To capture the relative position infor-
mation, we define five possible relative positions
between a phrase node and an argument, corre-
sponding match, cover, within, overlap and other.
Using these five relative positions, we capture the
context information using the following features:
a) #RP_Arg1Head_#Arg1Type: a feature in-
dicates the relative position of a phrase node with
argument 1’s head phrase, where #RP is the rela-
tive position (one of match, cover, within, overlap,
other), and #Arg1Type is the major entity type of
argument 1. One example feature may be
Match_Arg1Head_LOC.
b) #RP_Arg2Head_#Arg2Type: The relative
position with argument 2’s head phrase;
c) #RP_Arg1Extend_#Arg1Type: The rela-
tive position with argument 1’s extended phrase;
d) #PR_Arg2Extend_#Arg2Type: The rela-
tive position with argument 2’s extended phrase.
Feature weighting. Currently, we set all fea-
tures with an uniform weight , which is
used to control the relative importance of the fea-
ture in the final tree similarity: the larger the fea-
ture weight, the more important the feature in the
final tree similarity.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.997559">
4.1 Experimental Setting
</subsectionHeader>
<bodyText confidence="0.9999848125">
To assess the feature-enriched tree kernel, we
evaluate our method on the ACE RDC 2004 cor-
pus using the same experimental settings as (Qian
et al., 2008). That is, we parse all sentences using
the Charniak’s parser (Charniak, 2001), relation
instances are generated by iterating over all pairs
of entity mentions occurring in the same sentence.
In our experiments, we implement the feature-en-
riched tree kernel by extending the SVMlight (Joa-
chims, 1998) with the proposed tree kernel func-
tion (Moschitti, 2004). We apply the one vs. oth-
ers strategy for multiple classification using SVM.
For SVM training, the parameter C is set to 2.4 for
all experiments, and the tree kernel parameter λ is
tuned to 0.2 for FTK and 0.4 (the optimal param-
eter setting used in Qian et al.(2008)) for CTK.
</bodyText>
<subsectionHeader confidence="0.986193">
4.2 Experimental Results
4.2.1 Overall performance
</subsectionHeader>
<bodyText confidence="0.999566266666667">
We compare our method with the standard convo-
lution tree kernel (CTK) on the state-of-the-art
context sensitive shortest path-enclosed tree rep-
resentation (CSPT, Zhou et al., 2007). We exper-
iment our method with four different feature set-
tings, correspondingly: 1) FTK with only instance
features – FTK(instance); 2) FTK with only
phrase features – FTK(phrase); 3) FTK with only
context information features – FTK(context); and
4) FTK with all features – FTK. The overall per-
formance of CTK and FTK is shown in Table 1,
the F-measure improvements over CTK are also
shown inside the parentheses. The detailed perfor-
mance of FTK on the 7 major relation types of
ACE 2004 is shown in Table 2.
</bodyText>
<table confidence="0.998245333333333">
P(%) R(%) F
CTK 77.1 61.3 68.3 (-------)
FTK(instance) 78.5 64.6 70.9 (+2.6%)
FTK(phrase) 78.3 64.2 70.5 (+2.2%)
FTK(context) 80.1 67.5 73.2 (+4.9%)
FTK 81.2 67.4 73.7 (+5.4%)
</table>
<tableCaption confidence="0.993793">
Table 1. Overall Performance
</tableCaption>
<table confidence="0.99995325">
Relation Type P(%) R(%) F Impr
EMP-ORG 84.7 82.4 83.5 5.8%
PER-SOC 79.9 70.7 75.0 1.0%
PHYS 73.3 64.4 68.6 7.0%
ART 83.6 57.5 68.2 1.7%
GPE-AFF 74.7 56.6 64.4 4.3%
DISC 81.6 48.0 60.5 6.6%
OTHER-AFF 74.2 36.8 49.2 1.0%
</table>
<tableCaption confidence="0.9783685">
Table 2. FTK on the 7 major relation types and
their F-measure improvement over CTK
</tableCaption>
<bodyText confidence="0.9981665">
From Table 1 and 2, we can see that:
1) By refining the syntactic tree with discri-
minant features and incorporating these features
into the final tree similarity, FTK can significantly
improve the relation extraction performance:
compared with the convolution tree kernel base-
line CTK, our method can achieve a 5.4% F-meas-
ure improvement.
</bodyText>
<page confidence="0.998308">
64
</page>
<bodyText confidence="0.933076782608696">
2) All types of features can improve the per-
formance of relation extraction: FTK can corre-
spondingly get 2.6%, 2.2% and 4.9% F-measure
improvements using instance features, phrase fea-
tures and context information features.
3) Within the three types of features, context
information feature can achieve the highest F-
measure improvement. We believe this may be-
cause: ① The context information is useful in
providing clues for identifying the role and the im-
portance of a sub-tree; and ② The context-free as-
sumption of CTK is too strong, some critical in-
formation will lost in the CTK computation.
4) The performance improvement of FTK
varies significantly on different relation types: in
Table 2, most performance improvement gains
from the EMP-ORG, PHYS, GPE-AFF and DISC
relation types. We believe this may because the
discriminant features will better complement the
syntactic tree for capturing EMP-ORG, PHYS,
GPE-AFF and DISC relation. On contrast the fea-
tures may be redundant to the syntactic infor-
mation for other relation types.
</bodyText>
<table confidence="0.999360923076923">
System P(%) R(%) F
Qian et al., (2008): composite kernel 83.0 72.0 77.1
Zhou et al., (2007): composite kernel 82.2 70.2 75.8
Ours: FTK with CSPT 81.2 67.4 73.7
Zhou et al., (2007): context sensitive 81.1 66.7 73.2
CTK with CSPT
Ours: FTK with SPT 81.1 66.2 72.9
Jiang &amp; Zhai (2007): MaxEnt classi- 74.6 71.3 72.9
fier with features
Zhang et al., (2006): composite kernel 76.1 68.4 72.1
Zhao &amp; Grishman, (2005): Composite 69.2 70.5 70.4
kernel
Zhang et al., (2006): CTK with SPT 74.1 62.4 67.7
</table>
<tableCaption confidence="0.92936">
Table 3. Comparison of different systems on the
ACE RDC 2004 corpus
</tableCaption>
<subsectionHeader confidence="0.880831">
4.2.2 Comparison with other systems
</subsectionHeader>
<bodyText confidence="0.999988058823529">
Finally, Table 3 compares the performance of our
method with several other systems. From Table 3,
we can see that FTK can achieve competitive per-
formance: ① It achieves a 0.8% F-measure im-
provement over the feature-based system of Jiang
&amp; Zhai (2007); ② It achieves a 0.5% F-measure
improvement over a state-of-the-art tree kernel:
context sensitive CTK with CSPT of Zhou et al.,
(2007); ③ The F-measure of our system is slightly
lower than the current best performance on ACE
2004 (Qian et al., 2008) – 73.7 vs. 77.1, we believe
this is because the system of (Qian et al., 2008)
adopts two extra techniques: composing tree ker-
nel with a state-of-the-art feature-based kernel and
using a more proper sub-tree representation. We
believe these two techniques can also be used to
further improve the performance of our system.
</bodyText>
<sectionHeader confidence="0.999842" genericHeader="evaluation">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999982612903226">
This section briefly reviews the related work. A
classical technique for relation extraction is to
model the task as a feature-based classification
problem (Kambhatla, 2004; Zhou et al., 2005;
Jiang &amp; Zhai, 2007; Chan &amp; Roth, 2010; Chan &amp;
Roth, 2011), and feature engineering is obviously
the key for performance improvement. As an al-
ternative, tree kernel-based method implicitly de-
fines features by directly measuring the similarity
between two structures (Bunescu and Mooney,
2005; Bunescu and Mooney, 2006; Zelenko et al,
2003; Culotta and Sorensen, 2004; Zhang et al.,
2006). Composite kernels were also be used (Zhao
and Grishman, 2005; Zhang et al., 2006).
The main drawback of the current tree kernel is
that the syntactic tree representation often cannot
accurately capture the relation information. To re-
solve this problem, Zhou et al. (2007) took the an-
cestral information of sub-trees into consideration;
Reichartz and Korte (2010) incorporated depend-
ency type information into a tree kernel; Plank and
Moschitti (2013) and Liu et al. (2013) embedded
semantic information into tree kernel. Bloehdorn
and Moschitti (2007a, 2007b) proposed Syntactic
Semantic Tree Kernels (SSTK), which can cap-
ture the semantic similarity between leaf nodes.
Moschitti (2009) proposed a tree kernel which
specify a kernel function over any pair of nodes
between two trees, and it was further extended and
applied in other tasks in (Croce et al., 2011; Croce
et al., 2012; Mehdad et al., 2010).
</bodyText>
<sectionHeader confidence="0.998906" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999978571428571">
This paper proposes a feature-enriched tree kernel,
which can: 1) refine the syntactic tree representa-
tion; and 2) better measure the similarity between
two trees. For future work, we want to develop a
feature weighting algorithm which can accurately
measure the relevance of a feature to a relation in-
stance for better RE performance.
</bodyText>
<sectionHeader confidence="0.998827" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.980830166666667">
This work is supported by the National Natural
Science Foundation of China under Grants no.
61100152 and 61272324, and the Open Project of
Beijing Key Laboratory of Internet Culture and
Digital Dissemination Research under Grants no.
ICDD201204.
</bodyText>
<page confidence="0.999488">
65
</page>
<sectionHeader confidence="0.99475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998322895238095">
Agichtein, E. and Gravano, L. 2000. Snowball: Ex-
tracting relations from large plain-text collections.
In: Proceedings of the 5th ACM Conference on Dig-
ital Libraries, pp. 85–94.
Plank, B. and Moschitti, A. 2013. Embedding Semantic
Similarity in Tree Kernels for Domain Adaptation of
Relation Extraction”. In: Proceedings of ACL 2013.
Banko, M., Cafarella, M. J., Soderland, S., Broadhead,
M. and Etzioni, O. 2007. Open information extrac-
tion from the Web. In: Proceedings of the 20th Inter-
national Joint Conference on Artificial Intelligence,
pp. 2670–2676.
Bunescu, R. and Mooney, R. 2005. A shortest path de-
pendency kernel for relation extraction. In: Pro-
ceedings of the Human Language Technology Con-
ference and the Conference on Empirical Methods
in Natural Language Processing, pp.724–731.
Bloehdorn, S. and Moschitti, A. 2007a. Combined Syn-
tactic and Semantic Kernels for Text Classification.
In: Proceedings of the 29th European Conference on
Information Retrieval (ECIR).
Bloehdorn, S. and Moschitti, A. 2007b. Structure and
semantics for expressive text kernels. In: Proceeding
of ACM 16th Conference on Information and
Knowledge Management (CIKM).
Bunescu, R. and Mooney. R., 2006. Subsequence ker-
nels for relation extraction. In: Advances in Neural
Information Processing Systems 18, pp. 171–178.
Charniak, E., 2001. Immediate-head parsing for lan-
guage models. In: Proceedings of the 39th Annual
Meeting on Association for Computational Linguis-
tics, pp. 124-131.
Chan, Y. S. and Roth, D. 2010. Exploiting background
knowledge for relation extraction. In: Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pp. 152–160.
Chan, Y. S. and Roth, D. 2011. Exploiting syntactico-
semantic structures for relation extraction. In: Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics, pp. 551–560.
Croce, D., Moschitti, A. and Basili, R. 2011. Struc-
tured lexical similarity via convolution kernels on
dependency trees. In: Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pp. 1034–1046.
Croce, D., Moschitti, A., Basili, R. and Palmer, M.
2012. Verb Classification using Distributional Sim-
ilarity in Syntactic and Semantic Structures. In: Pro-
ceedings of ACL 2012, pp. 263-272.
Culotta, A. and Sorensen, J. 2004. Dependency tree
kernels for relation extraction. In: Proceedings of
the 42nd Annual Meeting of the Association for
Computational Linguistics, pp. 423–429.
Grishman, R. and Sundheim, B. 1996. Message under-
standing conference-6: A brief history. In: Proceed-
ings of the 16th International Conference on Com-
putational Linguistics, pp. 466–471.
Collins, M. and Duffy, N., 2001. Convolution Kernels
for Natural Language. In: Proceedings of NIPS
2001.
Liu, D., et al. 2013. Incorporating lexical semantic
similarity to tree kernel-based Chinese relation ex-
traction. In: Proceedings of Chinese Lexical Seman-
tics 2013.
Jiang, J. and Zhai, C. 2007. A systematic exploration of
the feature space for relation extraction. In: Pro-
ceedings of the Human Language Technology Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pp. 113–120.
Joachims, T. 1998. Text Categorization with Su
pport Vector Machine: learning with many rele-
vant features. ECML-1998: 137-142.
Kambhatla, N. 2004. Combining lexical, syntactic, and
semantic features with maximum entropy models for
extracting relations. In: the Proceedings of 42st An-
nual Meeting of the Association for Computational
Linguistics, pp. 178–181.
Krause, S., Li, H., Uszkoreit, H., &amp; Xu, F. 2012. Large-
scale learning of relation-extraction rules with dis-
tant supervision from the web. In: Proceedings of
ISWC 2012, pp. 263-278.
Marcus, M. P., Marcinkiewicz, M. A., &amp; Santorini, B.
1993. Building a large annotated corpus of English:
The Penn Treebank. Computational linguistics,
19(2), 313-330.
Moschitti, A. 2004. A study on Convolution Kernels for
Shallow Semantic Parsing. In: Proceedings of the
42-th Conference on Association for Computational
Linguistic (ACL-2004).
Moschitti, A. 2009. Syntactic and semantic kernels for
short text pair categorization. In: Proceedings of the
12th Conference of the European Chapter of the
ACL (EACL 2009), pp. 576–584.
Mehdad, Y., Moschitti, A. and Zanzotto, F. 2010. Syn-
tactic/Semantic Structures for Textual Entailment
Recognition. In: Proceedings of Human Language
Technology - North American chapter of the Asso-
ciation for Computational Linguistics.
Mintz, M., Bills, S., Snow, R. and Jurafsky D. 2009.
Distant supervision for relation extraction without
labeled data. In: Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the Association
for Computational Linguistics and the 4th Interna-
tional Joint Conference on Natural Language Pro-
cessing of the AFNLP, pp. 1003–1011.
</reference>
<page confidence="0.854756">
66
</page>
<reference confidence="0.999627913043479">
Qian L., Zhou G., Kong F., Zhu Q., and Qian P., 2008.
Exploiting constituent dependencies for tree kernel
based semantic relation extraction. In: Proceedings
of the 22nd International Conference on Computa-
tional Linguistics, pp. 697-704.
Reichartz, F. and H. Korte, et al. 2010. Semantic rela-
tion extraction with kernels over typed dependency
trees. In: Proceedings of the 16th ACM SIGKDD
international conference on Knowledge discovery
and data mining.
Zelenko, D., Aone, C., and Richardella, A. 2003. Ker-
nel methods for relation extraction. Journal of Ma-
chine Learning Research, 3:1083–1106.
Zhang, M., Zhang, J., and Su, J. 2006. Exploring syn-
tactic features for relation extraction using a convo-
lution tree kernel. In: Proceedings of the Human
Language Technology Conference and the North
American Chapter of the Association for Computa-
tional Linguistics, pages 288–295.
Zhang, M., Zhang, J., Su, J. and Zhou, G. 2006. A com-
posite kernel to extract relations between entities
with both flat and structured features. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th Annual Meeting
of the Association for Computational Linguistics,
pages 825–832.
Zhao, S. and Grishman, R. 2005. Extracting relations
with integrated information using kernel methods.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics, pages
419–426.
Zhou, G., Su, J., Zhang, J., and Zhang, M. 2005. Ex-
ploring various knowledge in relation extraction. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics, pages 427–
434.
Zhou, G. and Zhang M. 2007. Extracting relation in-
formation from text documents by exploring various
types of knowledge. Information Processing &amp; Man-
agement 43(4): 969--982.
Zhou, G., et al. 2007. Tree kernel-based relation ex-
traction with context-sensitive structured parse tree
information. In: Proceedings of the 2007 Joint Con-
ference on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pp. 728–736.
</reference>
<page confidence="0.999506">
67
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.692338">
<title confidence="0.99997">A Feature-Enriched Tree Kernel for Relation Extraction</title>
<author confidence="0.999589">Sun Han</author>
<affiliation confidence="0.9934125">State Key Laboratory of Computer Science Institute of Software, Chinese Academy of Sciences</affiliation>
<address confidence="0.736191">HaiDian District, Beijing, China.</address>
<email confidence="0.986551">sunle@nfs.iscas.ac.cn</email>
<email confidence="0.986551">xianpei@nfs.iscas.ac.cn</email>
<abstract confidence="0.997723111111111">Tree kernel is an effective technique for relation extraction. However, the traditional syntree representation is often coarse accurately capture the semantic relation information between two entities. In this paper, we propose a new tree kernel, tree kernel which can enhance the traditional tree kernel by: 1) refining the syntactic tree representation by annotating each tree node with a set of discriminant features; and 2) proposing a new tree kernel which can better measure the syntactic tree similarity by taking all features into consideration. Experimental results show that our method can achieve a 5.4% F-measure improvement over the traditional convolution tree kernel.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agichtein</author>
<author>L Gravano</author>
</authors>
<title>Snowball: Extracting relations from large plain-text collections. In:</title>
<date>2000</date>
<booktitle>Proceedings of the 5th ACM Conference on Digital Libraries,</booktitle>
<pages>85--94</pages>
<marker>Agichtein, Gravano, 2000</marker>
<rawString>Agichtein, E. and Gravano, L. 2000. Snowball: Extracting relations from large plain-text collections. In: Proceedings of the 5th ACM Conference on Digital Libraries, pp. 85–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Plank</author>
<author>A Moschitti</author>
</authors>
<title>Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction”. In:</title>
<date>2013</date>
<booktitle>Proceedings of ACL</booktitle>
<contexts>
<context position="19573" citStr="Plank and Moschitti (2013)" startWordPosition="3205" endWordPosition="3208">ctly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between leaf nodes. Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in (Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010). 6 Conclusions and Future Work This paper proposes a feature-enriched tree kernel, which can: 1) refine the syntactic tree representation; and 2)</context>
</contexts>
<marker>Plank, Moschitti, 2013</marker>
<rawString>Plank, B. and Moschitti, A. 2013. Embedding Semantic Similarity in Tree Kernels for Domain Adaptation of Relation Extraction”. In: Proceedings of ACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M J Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open information extraction from the Web. In:</title>
<date>2007</date>
<booktitle>Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2670--2676</pages>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>Banko, M., Cafarella, M. J., Soderland, S., Broadhead, M. and Etzioni, O. 2007. Open information extraction from the Web. In: Proceedings of the 20th International Joint Conference on Artificial Intelligence, pp. 2670–2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bunescu</author>
<author>R Mooney</author>
</authors>
<title>A shortest path dependency kernel for relation extraction. In:</title>
<date>2005</date>
<booktitle>Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>724--731</pages>
<contexts>
<context position="19025" citStr="Bunescu and Mooney, 2005" startWordPosition="3119" endWordPosition="3122">r sub-tree representation. We believe these two techniques can also be used to further improve the performance of our system. 5 Related Work This section briefly reviews the related work. A classical technique for relation extraction is to model the task as a feature-based classification problem (Kambhatla, 2004; Zhou et al., 2005; Jiang &amp; Zhai, 2007; Chan &amp; Roth, 2010; Chan &amp; Roth, 2011), and feature engineering is obviously the key for performance improvement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Bunescu, R. and Mooney, R. 2005. A shortest path dependency kernel for relation extraction. In: Proceedings of the Human Language Technology Conference and the Conference on Empirical Methods in Natural Language Processing, pp.724–731.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bloehdorn</author>
<author>A Moschitti</author>
</authors>
<title>Combined Syntactic and Semantic Kernels for Text Classification. In:</title>
<date>2007</date>
<booktitle>Proceedings of the 29th European Conference on Information Retrieval (ECIR).</booktitle>
<contexts>
<context position="19673" citStr="Bloehdorn and Moschitti (2007" startWordPosition="3220" endWordPosition="3223">y, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between leaf nodes. Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in (Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010). 6 Conclusions and Future Work This paper proposes a feature-enriched tree kernel, which can: 1) refine the syntactic tree representation; and 2) better measure the similarity between two trees. For future work, we want to develop a feature weig</context>
</contexts>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Bloehdorn, S. and Moschitti, A. 2007a. Combined Syntactic and Semantic Kernels for Text Classification. In: Proceedings of the 29th European Conference on Information Retrieval (ECIR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Bloehdorn</author>
<author>A Moschitti</author>
</authors>
<title>Structure and semantics for expressive text kernels. In:</title>
<date>2007</date>
<booktitle>Proceeding of ACM 16th Conference on Information and Knowledge Management (CIKM).</booktitle>
<contexts>
<context position="19673" citStr="Bloehdorn and Moschitti (2007" startWordPosition="3220" endWordPosition="3223">y, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between leaf nodes. Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in (Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010). 6 Conclusions and Future Work This paper proposes a feature-enriched tree kernel, which can: 1) refine the syntactic tree representation; and 2) better measure the similarity between two trees. For future work, we want to develop a feature weig</context>
</contexts>
<marker>Bloehdorn, Moschitti, 2007</marker>
<rawString>Bloehdorn, S. and Moschitti, A. 2007b. Structure and semantics for expressive text kernels. In: Proceeding of ACM 16th Conference on Information and Knowledge Management (CIKM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R</author>
</authors>
<title>Subsequence kernels for relation extraction. In:</title>
<date>2006</date>
<booktitle>Advances in Neural Information Processing Systems 18,</booktitle>
<pages>171--178</pages>
<marker>R, 2006</marker>
<rawString>Bunescu, R. and Mooney. R., 2006. Subsequence kernels for relation extraction. In: Advances in Neural Information Processing Systems 18, pp. 171–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Immediate-head parsing for language models. In:</title>
<date>2001</date>
<booktitle>Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>124--131</pages>
<contexts>
<context position="13922" citStr="Charniak, 2001" startWordPosition="2275" endWordPosition="2276"> #PR_Arg2Extend_#Arg2Type: The relative position with argument 2’s extended phrase. Feature weighting. Currently, we set all features with an uniform weight , which is used to control the relative importance of the feature in the final tree similarity: the larger the feature weight, the more important the feature in the final tree similarity. 4 Experiments 4.1 Experimental Setting To assess the feature-enriched tree kernel, we evaluate our method on the ACE RDC 2004 corpus using the same experimental settings as (Qian et al., 2008). That is, we parse all sentences using the Charniak’s parser (Charniak, 2001), relation instances are generated by iterating over all pairs of entity mentions occurring in the same sentence. In our experiments, we implement the feature-enriched tree kernel by extending the SVMlight (Joachims, 1998) with the proposed tree kernel function (Moschitti, 2004). We apply the one vs. others strategy for multiple classification using SVM. For SVM training, the parameter C is set to 2.4 for all experiments, and the tree kernel parameter λ is tuned to 0.2 for FTK and 0.4 (the optimal parameter setting used in Qian et al.(2008)) for CTK. 4.2 Experimental Results 4.2.1 Overall perf</context>
</contexts>
<marker>Charniak, 2001</marker>
<rawString>Charniak, E., 2001. Immediate-head parsing for language models. In: Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, pp. 124-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>D Roth</author>
</authors>
<title>Exploiting background knowledge for relation extraction. In:</title>
<date>2010</date>
<booktitle>Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>152--160</pages>
<contexts>
<context position="18772" citStr="Chan &amp; Roth, 2010" startWordPosition="3082" endWordPosition="3085"> best performance on ACE 2004 (Qian et al., 2008) – 73.7 vs. 77.1, we believe this is because the system of (Qian et al., 2008) adopts two extra techniques: composing tree kernel with a state-of-the-art feature-based kernel and using a more proper sub-tree representation. We believe these two techniques can also be used to further improve the performance of our system. 5 Related Work This section briefly reviews the related work. A classical technique for relation extraction is to model the task as a feature-based classification problem (Kambhatla, 2004; Zhou et al., 2005; Jiang &amp; Zhai, 2007; Chan &amp; Roth, 2010; Chan &amp; Roth, 2011), and feature engineering is obviously the key for performance improvement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this probl</context>
</contexts>
<marker>Chan, Roth, 2010</marker>
<rawString>Chan, Y. S. and Roth, D. 2010. Exploiting background knowledge for relation extraction. In: Proceedings of the 23rd International Conference on Computational Linguistics, pp. 152–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>D Roth</author>
</authors>
<title>Exploiting syntacticosemantic structures for relation extraction. In:</title>
<date>2011</date>
<booktitle>Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>551--560</pages>
<contexts>
<context position="9813" citStr="Chan &amp; Roth, 2011" startWordPosition="1601" endWordPosition="1604">odes, ; Otherwise go to step 3; 3) Calculate recursively as: ¢(n1,n2) = ¸ £ (1 + sim(n1, n2)) #ch(n1) X£ (1 + ¢(ch(n1, k), ch(n2, k)) k=1 3 Features for Relation Extraction This section presents the features we used to enrich the syntactic tree representation. 3.1 Instance Feature Relation instances of the same type often share some common characteristics. In this paper, we add the following instance features to the root node of a sub-tree representation: 1) Syntactico-Semantic structure. A feature indicates whether a relation instance has the following four syntactico-semantic structures in (Chan &amp; Roth, 2011) – Premodifiers, Possessive, Preposition, Formulaic and Verbal. 2) Entity-related information of arguments. Features about the entity information of arguments, including: a) #TP1-#TP2: the concat of the major entity types of arguments; b) #ST1- #ST2: the concat of the sub entity types of arguments; c) #MT1-#MT2: the concat of the mention types of arguments. 3) Base phrase chunking features. Features about the phrase path between two arguments and the phrases’ head before and after the arguments, which are the same as the phrase chunking features in (Zhou, et al., 2005). 3.2 Phrase Feature As d</context>
<context position="18792" citStr="Chan &amp; Roth, 2011" startWordPosition="3086" endWordPosition="3089">n ACE 2004 (Qian et al., 2008) – 73.7 vs. 77.1, we believe this is because the system of (Qian et al., 2008) adopts two extra techniques: composing tree kernel with a state-of-the-art feature-based kernel and using a more proper sub-tree representation. We believe these two techniques can also be used to further improve the performance of our system. 5 Related Work This section briefly reviews the related work. A classical technique for relation extraction is to model the task as a feature-based classification problem (Kambhatla, 2004; Zhou et al., 2005; Jiang &amp; Zhai, 2007; Chan &amp; Roth, 2010; Chan &amp; Roth, 2011), and feature engineering is obviously the key for performance improvement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (200</context>
</contexts>
<marker>Chan, Roth, 2011</marker>
<rawString>Chan, Y. S. and Roth, D. 2011. Exploiting syntacticosemantic structures for relation extraction. In: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pp. 551–560.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Croce</author>
<author>A Moschitti</author>
<author>R Basili</author>
</authors>
<title>Structured lexical similarity via convolution kernels on dependency trees. In:</title>
<date>2011</date>
<booktitle>Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1034--1046</pages>
<contexts>
<context position="19985" citStr="Croce et al., 2011" startWordPosition="3271" endWordPosition="3274">problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between leaf nodes. Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in (Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010). 6 Conclusions and Future Work This paper proposes a feature-enriched tree kernel, which can: 1) refine the syntactic tree representation; and 2) better measure the similarity between two trees. For future work, we want to develop a feature weighting algorithm which can accurately measure the relevance of a feature to a relation instance for better RE performance. Acknowledgments This work is supported by the National Natural Science Foundation of China under Grants no. 61100152 and 61272324, and the Open Project of Beijing Key Laboratory of Internet </context>
</contexts>
<marker>Croce, Moschitti, Basili, 2011</marker>
<rawString>Croce, D., Moschitti, A. and Basili, R. 2011. Structured lexical similarity via convolution kernels on dependency trees. In: Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pp. 1034–1046.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Croce</author>
<author>A Moschitti</author>
<author>R Basili</author>
<author>M Palmer</author>
</authors>
<title>Verb Classification using Distributional Similarity in Syntactic and Semantic Structures. In:</title>
<date>2012</date>
<booktitle>Proceedings of ACL 2012,</booktitle>
<pages>263--272</pages>
<contexts>
<context position="20005" citStr="Croce et al., 2012" startWordPosition="3275" endWordPosition="3278"> (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between leaf nodes. Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in (Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010). 6 Conclusions and Future Work This paper proposes a feature-enriched tree kernel, which can: 1) refine the syntactic tree representation; and 2) better measure the similarity between two trees. For future work, we want to develop a feature weighting algorithm which can accurately measure the relevance of a feature to a relation instance for better RE performance. Acknowledgments This work is supported by the National Natural Science Foundation of China under Grants no. 61100152 and 61272324, and the Open Project of Beijing Key Laboratory of Internet Culture and Digital </context>
</contexts>
<marker>Croce, Moschitti, Basili, Palmer, 2012</marker>
<rawString>Croce, D., Moschitti, A., Basili, R. and Palmer, M. 2012. Verb Classification using Distributional Similarity in Syntactic and Semantic Structures. In: Proceedings of ACL 2012, pp. 263-272.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Culotta</author>
<author>J Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction. In:</title>
<date>2004</date>
<booktitle>Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--429</pages>
<contexts>
<context position="19100" citStr="Culotta and Sorensen, 2004" startWordPosition="3131" endWordPosition="3134">ed to further improve the performance of our system. 5 Related Work This section briefly reviews the related work. A classical technique for relation extraction is to model the task as a feature-based classification problem (Kambhatla, 2004; Zhou et al., 2005; Jiang &amp; Zhai, 2007; Chan &amp; Roth, 2010; Chan &amp; Roth, 2011), and feature engineering is obviously the key for performance improvement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntacti</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Culotta, A. and Sorensen, J. 2004. Dependency tree kernels for relation extraction. In: Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pp. 423–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>B Sundheim</author>
</authors>
<title>Message understanding conference-6: A brief history. In:</title>
<date>1996</date>
<booktitle>Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>466--471</pages>
<marker>Grishman, Sundheim, 1996</marker>
<rawString>Grishman, R. and Sundheim, B. 1996. Message understanding conference-6: A brief history. In: Proceedings of the 16th International Conference on Computational Linguistics, pp. 466–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution Kernels for Natural Language. In:</title>
<date>2001</date>
<booktitle>Proceedings of NIPS</booktitle>
<contexts>
<context position="1844" citStr="Collins and Duffy (2001)" startWordPosition="279" endWordPosition="282">e tree kernel (Zelenko et al., 2003; Zhou et al., 2007; Zhang et al., 2006; Qian et al., 2008), which can exploit syntactic parse tree information for relation extraction. Given a pair of entities in a sentence, the tree kernel-based RE method first represents the relation information between them using a proper sub-tree (e.g., SPT – the sub-tree enclosed by the shortest path linking the two involved entities). For example, the three syntactic tree representations in Figure 1. Then the similarity between two trees are computed using a tree kernel, e.g., the convolution tree kernel proposed by Collins and Duffy (2001). Finally, new relation instances are extracted using kernel based classifiers, e.g., the SVM classifier. Unfortunately, one main shortcoming of the traditional tree kernel is that the syntactic tree representation usually cannot accurately capture the Figure 1. The ambiguity of possessive structure relation information between two entities. This is mainly due to the following two reasons: 1) The syntactic tree focuses on representing syntactic relation/structure, which is often too coarse or ambiguous to capture the semantic relation information. In a syntactic tree, each node indicates a cla</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Collins, M. and Duffy, N., 2001. Convolution Kernels for Natural Language. In: Proceedings of NIPS 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Liu</author>
</authors>
<title>Incorporating lexical semantic similarity to tree kernel-based Chinese relation extraction. In:</title>
<date>2013</date>
<booktitle>Proceedings of Chinese Lexical Semantics</booktitle>
<marker>Liu, 2013</marker>
<rawString>Liu, D., et al. 2013. Incorporating lexical semantic similarity to tree kernel-based Chinese relation extraction. In: Proceedings of Chinese Lexical Semantics 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>C Zhai</author>
</authors>
<title>A systematic exploration of the feature space for relation extraction. In:</title>
<date>2007</date>
<booktitle>Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>113--120</pages>
<contexts>
<context position="17387" citStr="Jiang &amp; Zhai (2007)" startWordPosition="2850" endWordPosition="2853">t performance improvement gains from the EMP-ORG, PHYS, GPE-AFF and DISC relation types. We believe this may because the discriminant features will better complement the syntactic tree for capturing EMP-ORG, PHYS, GPE-AFF and DISC relation. On contrast the features may be redundant to the syntactic information for other relation types. System P(%) R(%) F Qian et al., (2008): composite kernel 83.0 72.0 77.1 Zhou et al., (2007): composite kernel 82.2 70.2 75.8 Ours: FTK with CSPT 81.2 67.4 73.7 Zhou et al., (2007): context sensitive 81.1 66.7 73.2 CTK with CSPT Ours: FTK with SPT 81.1 66.2 72.9 Jiang &amp; Zhai (2007): MaxEnt classi- 74.6 71.3 72.9 fier with features Zhang et al., (2006): composite kernel 76.1 68.4 72.1 Zhao &amp; Grishman, (2005): Composite 69.2 70.5 70.4 kernel Zhang et al., (2006): CTK with SPT 74.1 62.4 67.7 Table 3. Comparison of different systems on the ACE RDC 2004 corpus 4.2.2 Comparison with other systems Finally, Table 3 compares the performance of our method with several other systems. From Table 3, we can see that FTK can achieve competitive performance:  It achieves a 0.8% F-measure improvement over the feature-based system of Jiang &amp; Zhai (2007);  It achieves a 0.5% F-measure i</context>
<context position="18753" citStr="Jiang &amp; Zhai, 2007" startWordPosition="3078" endWordPosition="3081">wer than the current best performance on ACE 2004 (Qian et al., 2008) – 73.7 vs. 77.1, we believe this is because the system of (Qian et al., 2008) adopts two extra techniques: composing tree kernel with a state-of-the-art feature-based kernel and using a more proper sub-tree representation. We believe these two techniques can also be used to further improve the performance of our system. 5 Related Work This section briefly reviews the related work. A classical technique for relation extraction is to model the task as a feature-based classification problem (Kambhatla, 2004; Zhou et al., 2005; Jiang &amp; Zhai, 2007; Chan &amp; Roth, 2010; Chan &amp; Roth, 2011), and feature engineering is obviously the key for performance improvement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To</context>
</contexts>
<marker>Jiang, Zhai, 2007</marker>
<rawString>Jiang, J. and Zhai, C. 2007. A systematic exploration of the feature space for relation extraction. In: Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics, pp. 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Joachims</author>
</authors>
<title>Text Categorization with Su pport Vector Machine: learning with many relevant features.</title>
<date>1998</date>
<volume>1998</volume>
<pages>137--142</pages>
<contexts>
<context position="14144" citStr="Joachims, 1998" startWordPosition="2308" endWordPosition="2310"> in the final tree similarity: the larger the feature weight, the more important the feature in the final tree similarity. 4 Experiments 4.1 Experimental Setting To assess the feature-enriched tree kernel, we evaluate our method on the ACE RDC 2004 corpus using the same experimental settings as (Qian et al., 2008). That is, we parse all sentences using the Charniak’s parser (Charniak, 2001), relation instances are generated by iterating over all pairs of entity mentions occurring in the same sentence. In our experiments, we implement the feature-enriched tree kernel by extending the SVMlight (Joachims, 1998) with the proposed tree kernel function (Moschitti, 2004). We apply the one vs. others strategy for multiple classification using SVM. For SVM training, the parameter C is set to 2.4 for all experiments, and the tree kernel parameter λ is tuned to 0.2 for FTK and 0.4 (the optimal parameter setting used in Qian et al.(2008)) for CTK. 4.2 Experimental Results 4.2.1 Overall performance We compare our method with the standard convolution tree kernel (CTK) on the state-of-the-art context sensitive shortest path-enclosed tree representation (CSPT, Zhou et al., 2007). We experiment our method with fo</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Joachims, T. 1998. Text Categorization with Su pport Vector Machine: learning with many relevant features. ECML-1998: 137-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Kambhatla</author>
</authors>
<title>Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations. In:</title>
<date>2004</date>
<booktitle>the Proceedings of 42st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>178--181</pages>
<contexts>
<context position="18714" citStr="Kambhatla, 2004" startWordPosition="3072" endWordPosition="3073">measure of our system is slightly lower than the current best performance on ACE 2004 (Qian et al., 2008) – 73.7 vs. 77.1, we believe this is because the system of (Qian et al., 2008) adopts two extra techniques: composing tree kernel with a state-of-the-art feature-based kernel and using a more proper sub-tree representation. We believe these two techniques can also be used to further improve the performance of our system. 5 Related Work This section briefly reviews the related work. A classical technique for relation extraction is to model the task as a feature-based classification problem (Kambhatla, 2004; Zhou et al., 2005; Jiang &amp; Zhai, 2007; Chan &amp; Roth, 2010; Chan &amp; Roth, 2011), and feature engineering is obviously the key for performance improvement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurate</context>
</contexts>
<marker>Kambhatla, 2004</marker>
<rawString>Kambhatla, N. 2004. Combining lexical, syntactic, and semantic features with maximum entropy models for extracting relations. In: the Proceedings of 42st Annual Meeting of the Association for Computational Linguistics, pp. 178–181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Krause</author>
<author>H Li</author>
<author>H Uszkoreit</author>
<author>F Xu</author>
</authors>
<title>Largescale learning of relation-extraction rules with distant supervision from the web. In:</title>
<date>2012</date>
<booktitle>Proceedings of ISWC 2012,</booktitle>
<pages>263--278</pages>
<marker>Krause, Li, Uszkoreit, Xu, 2012</marker>
<rawString>Krause, S., Li, H., Uszkoreit, H., &amp; Xu, F. 2012. Largescale learning of relation-extraction rules with distant supervision from the web. In: Proceedings of ISWC 2012, pp. 263-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<pages>313--330</pages>
<contexts>
<context position="2521" citStr="Marcus et al., 1993" startWordPosition="379" endWordPosition="382">nel based classifiers, e.g., the SVM classifier. Unfortunately, one main shortcoming of the traditional tree kernel is that the syntactic tree representation usually cannot accurately capture the Figure 1. The ambiguity of possessive structure relation information between two entities. This is mainly due to the following two reasons: 1) The syntactic tree focuses on representing syntactic relation/structure, which is often too coarse or ambiguous to capture the semantic relation information. In a syntactic tree, each node indicates a clause/phrase/word and is only labeled with a Treebank tag (Marcus et al., 1993). The Treebank tag, unfortunately, is usually too coarse or too general to capture semantic information. For example, all the three trees in Figure 1 share the same possessive syntactic structure, but express quite different semantic relations: where “Mary’s brothers” expresses PER-SOC Family relation, “Mary’s toys” expresses Possession relation, and “New York’s airports” expresses PHYSLocated relation. 2) Some critical information may lost during sub-tree representation extraction. For example, in Figure 2, when extracting SPT representation, all nodes outside the shortest-path will be pruned</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Marcus, M. P., Marcinkiewicz, M. A., &amp; Santorini, B. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational linguistics, 19(2), 313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>A study on Convolution Kernels for Shallow Semantic Parsing. In:</title>
<date>2004</date>
<booktitle>Proceedings of the 42-th Conference on Association for Computational Linguistic (ACL-2004).</booktitle>
<contexts>
<context position="14201" citStr="Moschitti, 2004" startWordPosition="2318" endWordPosition="2319">ight, the more important the feature in the final tree similarity. 4 Experiments 4.1 Experimental Setting To assess the feature-enriched tree kernel, we evaluate our method on the ACE RDC 2004 corpus using the same experimental settings as (Qian et al., 2008). That is, we parse all sentences using the Charniak’s parser (Charniak, 2001), relation instances are generated by iterating over all pairs of entity mentions occurring in the same sentence. In our experiments, we implement the feature-enriched tree kernel by extending the SVMlight (Joachims, 1998) with the proposed tree kernel function (Moschitti, 2004). We apply the one vs. others strategy for multiple classification using SVM. For SVM training, the parameter C is set to 2.4 for all experiments, and the tree kernel parameter λ is tuned to 0.2 for FTK and 0.4 (the optimal parameter setting used in Qian et al.(2008)) for CTK. 4.2 Experimental Results 4.2.1 Overall performance We compare our method with the standard convolution tree kernel (CTK) on the state-of-the-art context sensitive shortest path-enclosed tree representation (CSPT, Zhou et al., 2007). We experiment our method with four different feature settings, correspondingly: 1) FTK wi</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Moschitti, A. 2004. A study on Convolution Kernels for Shallow Semantic Parsing. In: Proceedings of the 42-th Conference on Association for Computational Linguistic (ACL-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>Syntactic and semantic kernels for short text pair categorization. In:</title>
<date>2009</date>
<booktitle>Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>576--584</pages>
<contexts>
<context position="19810" citStr="Moschitti (2009)" startWordPosition="3241" endWordPosition="3242">al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between leaf nodes. Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in (Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010). 6 Conclusions and Future Work This paper proposes a feature-enriched tree kernel, which can: 1) refine the syntactic tree representation; and 2) better measure the similarity between two trees. For future work, we want to develop a feature weighting algorithm which can accurately measure the relevance of a feature to a relation instance for better RE performance. Acknowledgments</context>
</contexts>
<marker>Moschitti, 2009</marker>
<rawString>Moschitti, A. 2009. Syntactic and semantic kernels for short text pair categorization. In: Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pp. 576–584.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Mehdad</author>
<author>A Moschitti</author>
<author>F Zanzotto</author>
</authors>
<title>Syntactic/Semantic Structures for Textual Entailment Recognition. In:</title>
<date>2010</date>
<booktitle>Proceedings of Human Language Technology - North American chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="20027" citStr="Mehdad et al., 2010" startWordPosition="3279" endWordPosition="3282">estral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between leaf nodes. Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in (Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010). 6 Conclusions and Future Work This paper proposes a feature-enriched tree kernel, which can: 1) refine the syntactic tree representation; and 2) better measure the similarity between two trees. For future work, we want to develop a feature weighting algorithm which can accurately measure the relevance of a feature to a relation instance for better RE performance. Acknowledgments This work is supported by the National Natural Science Foundation of China under Grants no. 61100152 and 61272324, and the Open Project of Beijing Key Laboratory of Internet Culture and Digital Dissemination Research</context>
</contexts>
<marker>Mehdad, Moschitti, Zanzotto, 2010</marker>
<rawString>Mehdad, Y., Moschitti, A. and Zanzotto, F. 2010. Syntactic/Semantic Structures for Textual Entailment Recognition. In: Proceedings of Human Language Technology - North American chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Mintz</author>
<author>S Bills</author>
<author>R Snow</author>
<author>D Jurafsky</author>
</authors>
<title>Distant supervision for relation extraction without labeled data. In:</title>
<date>2009</date>
<booktitle>Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>1003--1011</pages>
<marker>Mintz, Bills, Snow, Jurafsky, 2009</marker>
<rawString>Mintz, M., Bills, S., Snow, R. and Jurafsky D. 2009. Distant supervision for relation extraction without labeled data. In: Proceedings of the Joint Conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pp. 1003–1011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Qian</author>
<author>G Zhou</author>
<author>F Kong</author>
<author>Q Zhu</author>
<author>P Qian</author>
</authors>
<title>Exploiting constituent dependencies for tree kernel based semantic relation extraction. In:</title>
<date>2008</date>
<booktitle>Proceedings of the 22nd International Conference on Computational Linguistics,</booktitle>
<pages>697--704</pages>
<contexts>
<context position="1314" citStr="Qian et al., 2008" startWordPosition="196" endWordPosition="199">et of discriminant features; and 2) proposing a new tree kernel which can better measure the syntactic tree similarity by taking all features into consideration. Experimental results show that our method can achieve a 5.4% F-measure improvement over the traditional convolution tree kernel. 1 Introduction Relation Extraction (RE) aims to identify a set of predefined relations between pairs of entities in text. In recent years, relation extraction has received considerable research attention. An effective technique is the tree kernel (Zelenko et al., 2003; Zhou et al., 2007; Zhang et al., 2006; Qian et al., 2008), which can exploit syntactic parse tree information for relation extraction. Given a pair of entities in a sentence, the tree kernel-based RE method first represents the relation information between them using a proper sub-tree (e.g., SPT – the sub-tree enclosed by the shortest path linking the two involved entities). For example, the three syntactic tree representations in Figure 1. Then the similarity between two trees are computed using a tree kernel, e.g., the convolution tree kernel proposed by Collins and Duffy (2001). Finally, new relation instances are extracted using kernel based cla</context>
<context position="13844" citStr="Qian et al., 2008" startWordPosition="2261" endWordPosition="2264">Arg1Extend_#Arg1Type: The relative position with argument 1’s extended phrase; d) #PR_Arg2Extend_#Arg2Type: The relative position with argument 2’s extended phrase. Feature weighting. Currently, we set all features with an uniform weight , which is used to control the relative importance of the feature in the final tree similarity: the larger the feature weight, the more important the feature in the final tree similarity. 4 Experiments 4.1 Experimental Setting To assess the feature-enriched tree kernel, we evaluate our method on the ACE RDC 2004 corpus using the same experimental settings as (Qian et al., 2008). That is, we parse all sentences using the Charniak’s parser (Charniak, 2001), relation instances are generated by iterating over all pairs of entity mentions occurring in the same sentence. In our experiments, we implement the feature-enriched tree kernel by extending the SVMlight (Joachims, 1998) with the proposed tree kernel function (Moschitti, 2004). We apply the one vs. others strategy for multiple classification using SVM. For SVM training, the parameter C is set to 2.4 for all experiments, and the tree kernel parameter λ is tuned to 0.2 for FTK and 0.4 (the optimal parameter setting u</context>
<context position="17144" citStr="Qian et al., (2008)" startWordPosition="2806" endWordPosition="2809">portance of a sub-tree; and  The context-free assumption of CTK is too strong, some critical information will lost in the CTK computation. 4) The performance improvement of FTK varies significantly on different relation types: in Table 2, most performance improvement gains from the EMP-ORG, PHYS, GPE-AFF and DISC relation types. We believe this may because the discriminant features will better complement the syntactic tree for capturing EMP-ORG, PHYS, GPE-AFF and DISC relation. On contrast the features may be redundant to the syntactic information for other relation types. System P(%) R(%) F Qian et al., (2008): composite kernel 83.0 72.0 77.1 Zhou et al., (2007): composite kernel 82.2 70.2 75.8 Ours: FTK with CSPT 81.2 67.4 73.7 Zhou et al., (2007): context sensitive 81.1 66.7 73.2 CTK with CSPT Ours: FTK with SPT 81.1 66.2 72.9 Jiang &amp; Zhai (2007): MaxEnt classi- 74.6 71.3 72.9 fier with features Zhang et al., (2006): composite kernel 76.1 68.4 72.1 Zhao &amp; Grishman, (2005): Composite 69.2 70.5 70.4 kernel Zhang et al., (2006): CTK with SPT 74.1 62.4 67.7 Table 3. Comparison of different systems on the ACE RDC 2004 corpus 4.2.2 Comparison with other systems Finally, Table 3 compares the performance</context>
</contexts>
<marker>Qian, Zhou, Kong, Zhu, Qian, 2008</marker>
<rawString>Qian L., Zhou G., Kong F., Zhu Q., and Qian P., 2008. Exploiting constituent dependencies for tree kernel based semantic relation extraction. In: Proceedings of the 22nd International Conference on Computational Linguistics, pp. 697-704.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Reichartz</author>
<author>H Korte</author>
</authors>
<title>Semantic relation extraction with kernels over typed dependency trees. In:</title>
<date>2010</date>
<booktitle>Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining.</booktitle>
<contexts>
<context position="19485" citStr="Reichartz and Korte (2010)" startWordPosition="3192" endWordPosition="3195">ovement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between leaf nodes. Moschitti (2009) proposed a tree kernel which specify a kernel function over any pair of nodes between two trees, and it was further extended and applied in other tasks in (Croce et al., 2011; Croce et al., 2012; Mehdad et al., 2010). 6 Conclusions and Future Work This paper proposes a feat</context>
</contexts>
<marker>Reichartz, Korte, 2010</marker>
<rawString>Reichartz, F. and H. Korte, et al. 2010. Semantic relation extraction with kernels over typed dependency trees. In: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>A Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1083</pages>
<contexts>
<context position="1255" citStr="Zelenko et al., 2003" startWordPosition="184" endWordPosition="187">tic tree representation by annotating each tree node with a set of discriminant features; and 2) proposing a new tree kernel which can better measure the syntactic tree similarity by taking all features into consideration. Experimental results show that our method can achieve a 5.4% F-measure improvement over the traditional convolution tree kernel. 1 Introduction Relation Extraction (RE) aims to identify a set of predefined relations between pairs of entities in text. In recent years, relation extraction has received considerable research attention. An effective technique is the tree kernel (Zelenko et al., 2003; Zhou et al., 2007; Zhang et al., 2006; Qian et al., 2008), which can exploit syntactic parse tree information for relation extraction. Given a pair of entities in a sentence, the tree kernel-based RE method first represents the relation information between them using a proper sub-tree (e.g., SPT – the sub-tree enclosed by the shortest path linking the two involved entities). For example, the three syntactic tree representations in Figure 1. Then the similarity between two trees are computed using a tree kernel, e.g., the convolution tree kernel proposed by Collins and Duffy (2001). Finally, </context>
<context position="19072" citStr="Zelenko et al, 2003" startWordPosition="3127" endWordPosition="3130">niques can also be used to further improve the performance of our system. 5 Related Work This section briefly reviews the related work. A classical technique for relation extraction is to model the task as a feature-based classification problem (Kambhatla, 2004; Zhou et al., 2005; Jiang &amp; Zhai, 2007; Chan &amp; Roth, 2010; Chan &amp; Roth, 2011), and feature engineering is obviously the key for performance improvement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (200</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Zelenko, D., Aone, C., and Richardella, A. 2003. Kernel methods for relation extraction. Journal of Machine Learning Research, 3:1083–1106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhang</author>
<author>J Zhang</author>
<author>J Su</author>
</authors>
<title>Exploring syntactic features for relation extraction using a convolution tree kernel. In:</title>
<date>2006</date>
<booktitle>Proceedings of the Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>288--295</pages>
<contexts>
<context position="1294" citStr="Zhang et al., 2006" startWordPosition="192" endWordPosition="195">h tree node with a set of discriminant features; and 2) proposing a new tree kernel which can better measure the syntactic tree similarity by taking all features into consideration. Experimental results show that our method can achieve a 5.4% F-measure improvement over the traditional convolution tree kernel. 1 Introduction Relation Extraction (RE) aims to identify a set of predefined relations between pairs of entities in text. In recent years, relation extraction has received considerable research attention. An effective technique is the tree kernel (Zelenko et al., 2003; Zhou et al., 2007; Zhang et al., 2006; Qian et al., 2008), which can exploit syntactic parse tree information for relation extraction. Given a pair of entities in a sentence, the tree kernel-based RE method first represents the relation information between them using a proper sub-tree (e.g., SPT – the sub-tree enclosed by the shortest path linking the two involved entities). For example, the three syntactic tree representations in Figure 1. Then the similarity between two trees are computed using a tree kernel, e.g., the convolution tree kernel proposed by Collins and Duffy (2001). Finally, new relation instances are extracted us</context>
<context position="17458" citStr="Zhang et al., (2006)" startWordPosition="2862" endWordPosition="2865">SC relation types. We believe this may because the discriminant features will better complement the syntactic tree for capturing EMP-ORG, PHYS, GPE-AFF and DISC relation. On contrast the features may be redundant to the syntactic information for other relation types. System P(%) R(%) F Qian et al., (2008): composite kernel 83.0 72.0 77.1 Zhou et al., (2007): composite kernel 82.2 70.2 75.8 Ours: FTK with CSPT 81.2 67.4 73.7 Zhou et al., (2007): context sensitive 81.1 66.7 73.2 CTK with CSPT Ours: FTK with SPT 81.1 66.2 72.9 Jiang &amp; Zhai (2007): MaxEnt classi- 74.6 71.3 72.9 fier with features Zhang et al., (2006): composite kernel 76.1 68.4 72.1 Zhao &amp; Grishman, (2005): Composite 69.2 70.5 70.4 kernel Zhang et al., (2006): CTK with SPT 74.1 62.4 67.7 Table 3. Comparison of different systems on the ACE RDC 2004 corpus 4.2.2 Comparison with other systems Finally, Table 3 compares the performance of our method with several other systems. From Table 3, we can see that FTK can achieve competitive performance:  It achieves a 0.8% F-measure improvement over the feature-based system of Jiang &amp; Zhai (2007);  It achieves a 0.5% F-measure improvement over a state-of-the-art tree kernel: context sensitive CTK w</context>
<context position="19121" citStr="Zhang et al., 2006" startWordPosition="3135" endWordPosition="3138">rformance of our system. 5 Related Work This section briefly reviews the related work. A classical technique for relation extraction is to model the task as a feature-based classification problem (Kambhatla, 2004; Zhou et al., 2005; Jiang &amp; Zhai, 2007; Chan &amp; Roth, 2010; Chan &amp; Roth, 2011), and feature engineering is obviously the key for performance improvement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kerne</context>
</contexts>
<marker>Zhang, Zhang, Su, 2006</marker>
<rawString>Zhang, M., Zhang, J., and Su, J. 2006. Exploring syntactic features for relation extraction using a convolution tree kernel. In: Proceedings of the Human Language Technology Conference and the North American Chapter of the Association for Computational Linguistics, pages 288–295.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhang</author>
<author>J Zhang</author>
<author>J Su</author>
<author>G Zhou</author>
</authors>
<title>A composite kernel to extract relations between entities with both flat and structured features.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>825--832</pages>
<contexts>
<context position="1294" citStr="Zhang et al., 2006" startWordPosition="192" endWordPosition="195">h tree node with a set of discriminant features; and 2) proposing a new tree kernel which can better measure the syntactic tree similarity by taking all features into consideration. Experimental results show that our method can achieve a 5.4% F-measure improvement over the traditional convolution tree kernel. 1 Introduction Relation Extraction (RE) aims to identify a set of predefined relations between pairs of entities in text. In recent years, relation extraction has received considerable research attention. An effective technique is the tree kernel (Zelenko et al., 2003; Zhou et al., 2007; Zhang et al., 2006; Qian et al., 2008), which can exploit syntactic parse tree information for relation extraction. Given a pair of entities in a sentence, the tree kernel-based RE method first represents the relation information between them using a proper sub-tree (e.g., SPT – the sub-tree enclosed by the shortest path linking the two involved entities). For example, the three syntactic tree representations in Figure 1. Then the similarity between two trees are computed using a tree kernel, e.g., the convolution tree kernel proposed by Collins and Duffy (2001). Finally, new relation instances are extracted us</context>
<context position="17458" citStr="Zhang et al., (2006)" startWordPosition="2862" endWordPosition="2865">SC relation types. We believe this may because the discriminant features will better complement the syntactic tree for capturing EMP-ORG, PHYS, GPE-AFF and DISC relation. On contrast the features may be redundant to the syntactic information for other relation types. System P(%) R(%) F Qian et al., (2008): composite kernel 83.0 72.0 77.1 Zhou et al., (2007): composite kernel 82.2 70.2 75.8 Ours: FTK with CSPT 81.2 67.4 73.7 Zhou et al., (2007): context sensitive 81.1 66.7 73.2 CTK with CSPT Ours: FTK with SPT 81.1 66.2 72.9 Jiang &amp; Zhai (2007): MaxEnt classi- 74.6 71.3 72.9 fier with features Zhang et al., (2006): composite kernel 76.1 68.4 72.1 Zhao &amp; Grishman, (2005): Composite 69.2 70.5 70.4 kernel Zhang et al., (2006): CTK with SPT 74.1 62.4 67.7 Table 3. Comparison of different systems on the ACE RDC 2004 corpus 4.2.2 Comparison with other systems Finally, Table 3 compares the performance of our method with several other systems. From Table 3, we can see that FTK can achieve competitive performance:  It achieves a 0.8% F-measure improvement over the feature-based system of Jiang &amp; Zhai (2007);  It achieves a 0.5% F-measure improvement over a state-of-the-art tree kernel: context sensitive CTK w</context>
<context position="19121" citStr="Zhang et al., 2006" startWordPosition="3135" endWordPosition="3138">rformance of our system. 5 Related Work This section briefly reviews the related work. A classical technique for relation extraction is to model the task as a feature-based classification problem (Kambhatla, 2004; Zhou et al., 2005; Jiang &amp; Zhai, 2007; Chan &amp; Roth, 2010; Chan &amp; Roth, 2011), and feature engineering is obviously the key for performance improvement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kerne</context>
</contexts>
<marker>Zhang, Zhang, Su, Zhou, 2006</marker>
<rawString>Zhang, M., Zhang, J., Su, J. and Zhou, G. 2006. A composite kernel to extract relations between entities with both flat and structured features. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 825–832.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Zhao</author>
<author>R Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>419--426</pages>
<contexts>
<context position="19183" citStr="Zhao and Grishman, 2005" startWordPosition="3145" endWordPosition="3148">ly reviews the related work. A classical technique for relation extraction is to model the task as a feature-based classification problem (Kambhatla, 2004; Zhou et al., 2005; Jiang &amp; Zhai, 2007; Chan &amp; Roth, 2010; Chan &amp; Roth, 2011), and feature engineering is obviously the key for performance improvement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the relation information. To resolve this problem, Zhou et al. (2007) took the ancestral information of sub-trees into consideration; Reichartz and Korte (2010) incorporated dependency type information into a tree kernel; Plank and Moschitti (2013) and Liu et al. (2013) embedded semantic information into tree kernel. Bloehdorn and Moschitti (2007a, 2007b) proposed Syntactic Semantic Tree Kernels (SSTK), which can capture the semantic similarity between l</context>
<context position="17515" citStr="Zhao &amp; Grishman, (2005)" startWordPosition="2871" endWordPosition="2874">criminant features will better complement the syntactic tree for capturing EMP-ORG, PHYS, GPE-AFF and DISC relation. On contrast the features may be redundant to the syntactic information for other relation types. System P(%) R(%) F Qian et al., (2008): composite kernel 83.0 72.0 77.1 Zhou et al., (2007): composite kernel 82.2 70.2 75.8 Ours: FTK with CSPT 81.2 67.4 73.7 Zhou et al., (2007): context sensitive 81.1 66.7 73.2 CTK with CSPT Ours: FTK with SPT 81.1 66.2 72.9 Jiang &amp; Zhai (2007): MaxEnt classi- 74.6 71.3 72.9 fier with features Zhang et al., (2006): composite kernel 76.1 68.4 72.1 Zhao &amp; Grishman, (2005): Composite 69.2 70.5 70.4 kernel Zhang et al., (2006): CTK with SPT 74.1 62.4 67.7 Table 3. Comparison of different systems on the ACE RDC 2004 corpus 4.2.2 Comparison with other systems Finally, Table 3 compares the performance of our method with several other systems. From Table 3, we can see that FTK can achieve competitive performance:  It achieves a 0.8% F-measure improvement over the feature-based system of Jiang &amp; Zhai (2007);  It achieves a 0.5% F-measure improvement over a state-of-the-art tree kernel: context sensitive CTK with CSPT of Zhou et al., (2007);  The F-measure of our s</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Zhao, S. and Grishman, R. 2005. Extracting relations with integrated information using kernel methods. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 419–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>J Su</author>
<author>J Zhang</author>
<author>M Zhang</author>
</authors>
<title>Exploring various knowledge in relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>427--434</pages>
<contexts>
<context position="10388" citStr="Zhou, et al., 2005" startWordPosition="1694" endWordPosition="1697">o-semantic structures in (Chan &amp; Roth, 2011) – Premodifiers, Possessive, Preposition, Formulaic and Verbal. 2) Entity-related information of arguments. Features about the entity information of arguments, including: a) #TP1-#TP2: the concat of the major entity types of arguments; b) #ST1- #ST2: the concat of the sub entity types of arguments; c) #MT1-#MT2: the concat of the mention types of arguments. 3) Base phrase chunking features. Features about the phrase path between two arguments and the phrases’ head before and after the arguments, which are the same as the phrase chunking features in (Zhou, et al., 2005). 3.2 Phrase Feature As discussed in above, the Treebank tag is too coarse to capture the property of a phrase node. Therefore, we enrich each phrase node with features about its lexical pattern, its content information, and its lexical semantics: 1) Lexical Pattern. We capture the lexical pattern of a phrase node using the following features: a) LP_Poss: A feature indicates the node is a possessive phrase; b) LP_PP: A feature indicates the node is a preposition phrase; c) LP_CC: A feature indicates the node is a conjunction phrase; d) LP_EndWithPUNC: A feature indicates the node ends with a p</context>
<context position="18733" citStr="Zhou et al., 2005" startWordPosition="3074" endWordPosition="3077">stem is slightly lower than the current best performance on ACE 2004 (Qian et al., 2008) – 73.7 vs. 77.1, we believe this is because the system of (Qian et al., 2008) adopts two extra techniques: composing tree kernel with a state-of-the-art feature-based kernel and using a more proper sub-tree representation. We believe these two techniques can also be used to further improve the performance of our system. 5 Related Work This section briefly reviews the related work. A classical technique for relation extraction is to model the task as a feature-based classification problem (Kambhatla, 2004; Zhou et al., 2005; Jiang &amp; Zhai, 2007; Chan &amp; Roth, 2010; Chan &amp; Roth, 2011), and feature engineering is obviously the key for performance improvement. As an alternative, tree kernel-based method implicitly defines features by directly measuring the similarity between two structures (Bunescu and Mooney, 2005; Bunescu and Mooney, 2006; Zelenko et al, 2003; Culotta and Sorensen, 2004; Zhang et al., 2006). Composite kernels were also be used (Zhao and Grishman, 2005; Zhang et al., 2006). The main drawback of the current tree kernel is that the syntactic tree representation often cannot accurately capture the rela</context>
</contexts>
<marker>Zhou, Su, Zhang, Zhang, 2005</marker>
<rawString>Zhou, G., Su, J., Zhang, J., and Zhang, M. 2005. Exploring various knowledge in relation extraction. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 427– 434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
<author>M Zhang</author>
</authors>
<title>Extracting relation information from text documents by exploring various types of knowledge.</title>
<date>2007</date>
<journal>Information Processing &amp; Management</journal>
<volume>43</volume>
<issue>4</issue>
<pages>969--982</pages>
<marker>Zhou, Zhang, 2007</marker>
<rawString>Zhou, G. and Zhang M. 2007. Extracting relation information from text documents by exploring various types of knowledge. Information Processing &amp; Management 43(4): 969--982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Zhou</author>
</authors>
<title>Tree kernel-based relation extraction with context-sensitive structured parse tree information. In:</title>
<date>2007</date>
<booktitle>Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>728--736</pages>
<marker>Zhou, 2007</marker>
<rawString>Zhou, G., et al. 2007. Tree kernel-based relation extraction with context-sensitive structured parse tree information. In: Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 728–736.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>