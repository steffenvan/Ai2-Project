<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000237">
<title confidence="0.998226">
JU_CSE_NLP: Language Independent Cross-lingual
Textual Entailment System
</title>
<author confidence="0.9876355">
Snehasis Neogi1, Partha Pakray2, Sivaji Bandyopadhyay1,
Alexander Gelbukh3
</author>
<affiliation confidence="0.959023666666666">
1Computer Science &amp; Engineering Department
Jadavpur University, Kolkata, India
2Computer Science &amp; Engineering Department
Jadavpur University, Kolkata, India
Intern at Xerox Research Centre Europe
Grenoble, France
3Center for Computing Research
National Polytechnic Institute
Mexico City, Mexico
</affiliation>
<email confidence="0.913790333333333">
{snehasis1981,parthapakray}@gmail.com
sbandyopadhyay@cse.jdvu.ac.in
gelbukh@gelbukh.com
</email>
<sectionHeader confidence="0.996718" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964322580645">
This article presents the experiments car-
ried out at Jadavpur University as part of
the participation in Cross-lingual Textual
Entailment for Content Synchronization
(CLTE) of task 8 @ Semantic Evaluation
Exercises (SemEval-2012). The work ex-
plores cross-lingual textual entailment as a
relation between two texts in different lan-
guages and proposes different measures
for entailment decision in a four way clas-
sification tasks (forward, backward, bidi-
rectional and no-entailment). We set up
different heuristics and measures for eva-
luating the entailment between two texts
based on lexical relations. Experiments
have been carried out with both the text
and hypothesis converted to the same lan-
guage using the Microsoft Bing translation
system. The entailment system considers
Named Entity, Noun Chunks, Part of
speech, N-Gram and some text similarity
measures of the text pair to decide the en-
tailment judgments. Rules have been de-
veloped to encounter the multi way
entailment issue. Our system decides on
the entailment judgment after comparing
the entailment scores for the text pairs.
Four different rules have been developed
for the four different classes of entailment.
The best run is submitted for Italian –
English language with accuracy 0.326.
</bodyText>
<sectionHeader confidence="0.99939" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999183916666667">
Textual Entailment (TE) (Dagan and Glick-
man, 2004) is one of the recent challenges of
Natural Language Processing (NLP). The Task
8 of SemEval-20121 [1] defines a textual en-
tailment system that specifies two major as-
pects: the task is based on cross-lingual
corpora and the entailment decision must be
four ways. Given a pair of topically related text
fragments (T1 and T2) in different languages,
the CLTE task consists of automatically anno-
tating it with one of the following entailment
judgments:
</bodyText>
<listItem confidence="0.866702818181818">
i. Bidirectional (T1 -&gt;T2 &amp; T1 &lt;- T2): the two
fragments entail each other (semantic equiva-
lence)
ii. Forward (T1 -&gt; T2 &amp; T1!&lt;- T2): unidirec-
tional entailment from T1 to T2 .
iii. Backward (T1! -&gt; T2 &amp; T1 &lt;- T2): unidirec-
tional entailment from T2 to T1.
iv. No Entailment (T1! -&gt; T2 &amp; T1! &lt;- T2):
there is no entailment between T1 and T2.
CLTE (Cross Lingual Textual Entailment) task
consists of 1,000 CLTE dataset pairs (500 for
</listItem>
<footnote confidence="0.971069">
1http://www.cs.york.ac.uk/semeval2012/index.php?id=tasks
</footnote>
<page confidence="0.944509">
689
</page>
<note confidence="0.853081">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 689–695,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.948770135135135">
training and 500 for test) available for the fol-
lowing language combinations:
- Spanish/English (spa-eng)
- German/English (deu-eng).
- Italian/English (ita-eng)
- French/English (fra-eng)
Seven Recognizing Textual Entailment (RTE)
evaluation tracks have already been held: RTE-1
in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in
2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009,
RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE
task produces a generic framework for entail-
ment task across NLP applications. The RTE
challenges have moved from 2 – way entailment
task (YES, NO) to 3 – way task (YES, NO,
UNKNOWN). EVALITA/IRTE [9] task is simi-
lar to the RTE challenge for the Italian language.
So far, TE has been applied only in a monolin-
gual setting. Cross-lingual Textual Entailment
(CLTE) has been proposed ([10], [11], [12]) as
an extension of Textual Entailment. In 2010,
Parser Training and Evaluation using Textual
Entailment [13] was organized by SemEval-2.
Recognizing Inference in Text (RITE)2 orga-
nized by NTCIR-9 in 2011 is the first to expand
TE as a 5-way entailment task (forward, back-
ward, bi-directional, contradiction and indepen-
dent) in a monolingual scenario [14].
We have participated in RTE-5 [15], RTE-6
[16], RTE-7 [17], SemEval-2 Parser Training
and Evaluation using Textual Entailment Task
and RITE [18].
Section 2 describes our Cross-lingual Textual
Entailment system. The various experiments
carried out on the development and test data sets
are described in Section 3 along with the results.
The conclusions are drawn in Section 4.
</bodyText>
<sectionHeader confidence="0.930173" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.999959375">
Our system for CLTE task is based on a set of
heuristics that assigns entailment scores to a text
pair based on lexical relations. The text and the
hypothesis in a text pair are translated to the
same language using the Microsoft Bing ma-
chine translation system. The system separates
the text pairs (T1 and T2) available in different
languages and preprocesses them. After prepro-
</bodyText>
<footnote confidence="0.741818">
2 http://artigas.lti.cs.cmu.edu/rite/Main_Page
</footnote>
<bodyText confidence="0.99798825">
cessing we have used several techniques such as
Word Overlaps, Named Entity matching, Chunk
matching, POS matching to evaluate the sepa-
rated text pairs. These modules return a set of
score statistics, which helps the system to go for
multi-class entailment decision based on the
predefined rules. We have submitted 3 runs for
each language pair for the CLTE task and there
are some minor differences in the architectures
that constitute the 3 runs. The three system ar-
chitectures are described in section 2.1, section
2.2 and section 2.3.
</bodyText>
<subsectionHeader confidence="0.86245">
2.1 System Architecture 1: CLTE Task
with Translated English Text
</subsectionHeader>
<bodyText confidence="0.9990885625">
The system architecture of Cross-lingual textual
entailment consists of various components such
as Preprocessing Module, Lexical Similarity
Module, Text Similarity Module. Lexical Simi-
larity module again is divided into subsequent
modules like POS matching, Chunk matching
and Named Entity matching. Our system calcu-
lates these measures twice once considering T1
as text and T2 as hypothesis and once T2 as text
and T1 as hypothesis. The mapping is done in
both directions T1-to-T2 and T2-to-T1 to arrive
at the appropriate four way entailment decision
using a set of rules. Each of these modules is
now being described in subsequent subsections.
Figure 1 shows our system architecture where
the text sentence is translated to English.
</bodyText>
<figureCaption confidence="0.999751">
Figure 1: System Architecture
</figureCaption>
<figure confidence="0.9760861">
(fra, ita, deu,
spa language)
(English
language)
Preprocessing
(Stop word removal,
Co referencing)
Translated in
Eng. Using Bing
Translator
T1 – Hypothesis
T2 - Text
T1- Text
T2- Hypothesis
N-Gram Module
Chunking Module
Text Similarity Module Named Entity POS Module
Σ Lexical Score (S1)
Σ Lexical Score (S2)
If (S1&gt;S2) Then Entailment = “forward”
If (S1&lt;S2) Then Entailment = “backward”
If (S1=S2) or (abs (S1-S2) &lt;Threshold) Then Entailment = “bidirectional”
If (S1=S2 and (S1=S2) &lt;Threshold) Then Entailment = “no_entailment”
T1.txt
T2.txt
Preprocessing
(Stop word removal,
Co referencing)
CLTE Task Data
(T1, T2)
</figure>
<page confidence="0.740335">
690
</page>
<subsubsectionHeader confidence="0.678348">
2.1.1 Preprocessing Module the module we get two scores, one for T1-T2
</subsubsectionHeader>
<bodyText confidence="0.912317230769231">
pair and another for T2-T1 pair.
The system separates the T1 and T2 pair from
the CLTE task data. T1 sentences are in differ-
ent languages (In French, Italian, German and
Spanish) where as T2 sentences are in English.
Microsoft Bing translator3 API for Bing transla-
tor (microsoft-translator-java-api-0.4-jar-with-
dependencies.jar) is being used to translate the
T1 text sentences into English. The translated
T1 and T2 sentences are passed through the two
sub modules.
i. Stop word Removal: Stop words are removed
from the T1 and T2 sentences.
</bodyText>
<listItem confidence="0.994619388888889">
ii. Co-reference: Co–reference chains are eva-
luated for the datasets before passing them to the
TE module. The objective is to increase the en-
tailment score after substituting the anaphors
with their antecedents. A word or phrase in the
sentence is used to refer to an entity introduced
earlier or later in the discourse and both having
same things then they have the same referent or
co-reference. When the reader must look back to
the previous context, co-reference is called
&amp;quot;Anaphoric Reference&amp;quot;. When the reader must
look forward, it is termed &amp;quot;Cataphoric Refer-
ence&amp;quot;. To address this problem we used a tool
called JavaRAP4 (A java based implementation
of Anaphora Procedure (RAP) - an algorithm by
Lappin and Leass (1994)). It has been observed
that the presence of co – referential expressions
are very small in sentence based paradigm.
</listItem>
<subsectionHeader confidence="0.481218">
2.1.2 Lexical Based Textual Entailment
(TE) Module
</subsectionHeader>
<bodyText confidence="0.999837916666667">
T1 - T2 pairs are the inputs to the system. The
TE module is executed once by considering T1
as text and T2 as hypothesis and again by consi-
dering T2 as text and T1 as hypothesis. The
overall TE module is a collection of several lex-
ical based sub modules.
i. I-Gram Match module: The N-Gram match
basically measures the percentage match of the
unigram, bigram and trigram of hypothesis
present in the corresponding text. These scores
are simply combined to get an overall N – Gram
matching score for a particular pair. By running
</bodyText>
<footnote confidence="0.996891">
3 http://code.google.com/p/microsoft-translator-java-api/
4 http://aye.comp.nus.edu.sg/~qiu/NLPTools/JavaRAP.html
</footnote>
<bodyText confidence="0.997061095238095">
ii. Chunk Similarity module: In this sub mod-
ule our system evaluates the key NP-chunks of
both text and hypothesis identified using NP
Chunker v1.15. Then our system checks the
presence of NP-Chunks of hypothesis in the cor-
responding text. System calculates the overall
value for the chunk matching, i.e., number of
text NP-chunks that match with hypothesis NP-
chunks. If the chunks are not similar in their sur-
face form then our system goes for WordNet
matching for the words and if they match in
WordNet synsets information, the chunks are
considered as similar.
WordNet [19] is one of most important resource
for lexical analysis. The WordNet 2.0 has been
used for WordNet based chunk matching. The
API for WordNet Searching (JAWS)6 is an API
that provides Java applications with the ability
to retrieve data from the WordNet database. Let
us consider the following example taken from
training data:
</bodyText>
<figure confidence="0.624437">
T1: Due/JJ to/TO [an/DT error/NN of/IN com-
munication/NN] between/IN [the/DT police/NN]
...
T2: On/IN [Tuesday/NNP] [a/DT failed/VBN
communication/NN] between/IN...
</figure>
<bodyText confidence="0.9980596">
The chunk in T1 [error communication] matches
with T2 [failed communication] via WordNet
based synsets information. A weight is assigned
to the score depending upon the nature of chunk
matching.
</bodyText>
<figure confidence="0.6145662">
M[i] = Wm[i] * P / Wc[i]
Where N= Total number of chunk containing
hypothesis.
M[i] = Match Score of the ith Chunk.
Wm[i] = Number of words matched in the ith
chunk.
Wc[i] = Total words in the ith chunk.
1 if surface word matches.
and ρ =
1/2 if matche via WordNet
</figure>
<footnote confidence="0.9681395">
5 http://www.dcs.shef.ac.uk/~mark/phd/software/
6 http://lyle.smu.edu/~tspell/jaws/index.html
</footnote>
<page confidence="0.995392">
691
</page>
<bodyText confidence="0.956192333333333">
System takes into consideration several text si-
milarity measures calculated over the T1-T2
pair. These text similarity measures are summed
up to produce a total score for a particular text
pair. Similar to the Lexical module, text simi-
larity module is also executed for both T1-T2
and T2-T1 pairs.
iii. Text Distance Module: The following major
text similarity measures have been considered
by our system. The text similarity measure
scores are added to generate the final text dis-
tance score.
</bodyText>
<listItem confidence="0.984586882352941">
• Cosine Similarity
• Levenshtein Distance
• Euclidean Distance
• MongeElkan Distance
•
NeedlemanWunch Distance
• SmithWaterman Distance
• Block Distance
• Jaro Similarity
• MatchingCoefficient Similarity
• Dice Similarity
• OverlapCoefficient
• QGrams Distance
iv. Named Entity Matching: It is based on the
detection and matching of Named Entities in the
T1-T2 pair. Stanford Named Entity Recognizer7
(NER) is used to tag the Named Entities in both
T1 and T2. System simply matches the number
of hypothesis NEs present in the text. A score is
allocated for the matching.
IE_match = (Iumber of common IEs in Text
and Hypothesis)/(Iumber of IEs in Hypothe-
sis).
v. Part-of-Speech (POS) Matching: This mod-
ule basically deals with matching the common
POS tags between T1 and T2 pair. Stanford POS
tagger8 is used to tag the part of speech in both
T1 and T2. System matches the verb and noun
POS words in the hypothesis that match in the
text. A score is allocated based on the number of
POS matching.
POS_match = (Iumber of verb and noun
POS in Text and Hypothesis)/(Total number of
verb and noun POS in hypothesis).
</listItem>
<footnote confidence="0.987025">
7 http://nlp.stanford.edu/software/CRF-NER.shtml
8 http://nlp.stanford.edu/software/tagger.shtml
</footnote>
<bodyText confidence="0.965485333333333">
System adds all the lexical matching scores to
evaluate the total score for a particular T1- T2
pair, i.e.,
</bodyText>
<equation confidence="0.985245">
Pair]: (T] – Text and T2 – Hypothesis)
Pair2: (T] – Hypothesis and T2 - Text).
</equation>
<bodyText confidence="0.999050714285714">
Total lexical score for each pair can be mathe-
matically represented by:
where S1 represents the score for the pair with
T1 as text and T2 as hypothesis while S2
represents the score from T1 to T2. The figure 2
shows the sample output values of the TE mod-
ule.
</bodyText>
<figureCaption confidence="0.995166">
Figure 2: output values of this module
</figureCaption>
<bodyText confidence="0.999959944444444">
The system finally compares the above two val-
ues S1 and S2 as obtained from the lexical mod-
ule to go for four-class entailment decision. If
score S1, i.e., the mapping score with T1 as text
and T2 as hypothesis is greater than the score
S2, i.e., mapping score with T2 as text and T1 as
hypothesis, then the entailment class will be
“forward”. Similarly if S1 is less than S2, i.e.,
T2 now acts as the text and T1 acts as the hypo-
thesis then the entailment class will be “back-
ward”. Similarly if both the scores S1 and S2 are
equal the entailment class will be “bidirectional”
(entails in both directions). Measuring “bidirec-
tional” entailment is much more difficult than
any other entailment decision due to combina-
tions of different lexical scores. As our system
produces a final score (S1 and S2) that is basi-
cally the sum over different similarity measures,
</bodyText>
<page confidence="0.996879">
692
</page>
<bodyText confidence="0.999871307692308">
the tendency of identical S1 – S2 will be quite
small. As a result we establish another heuristic
for “bidirectional” class. If the absolute value
difference between S1 and S2 is below the thre-
shold value, our system recognizes the pair as
“bidirectional” (abs (S1 – S2) &lt; threshold). This
threshold has been set as 5 based on observation
from the training file. If the individual scores S1
and S2 are below a certain threshold, again set
based on the observation in the training file, then
system concludes the entailment class as
“no_entailment”. This threshold has been set as
20 based on observation from the training file.
</bodyText>
<subsectionHeader confidence="0.8385995">
2.2 System Architecture 2: CLTE Task
with translated hypothesis
</subsectionHeader>
<bodyText confidence="0.999983151515151">
System Architecture 2 is based on lexical match-
ing between the text pairs (T1, T2) and basically
measures the same attributes as in the architec-
ture 1. In this architecture, the English hypothe-
sis sentences are translated to the language of
the text sentence (French, Italian, Spanish and
German) using the Microsoft Bing Translator.
The CLTE dataset is preprocessed after separat-
ing the (T1, T2) pairs. Preprocessing module
includes stop word removal and co-referencing.
After preprocessing, the system executes the TE
module for lexical matching between the text
pairs. This module comprises N-Gram matching,
Text Similarity, Named Entity Matching, POS
matching and Chunking. The TE module is ex-
ecuted once with T1 as text and T2 as hypothe-
sis and again with T1 as hypothesis and T2 as
text. But in this architecture N-Gram matching
and text similarity modules differ from the pre-
vious architecture. In system architecture 1, the
N-Gram matching and text similarity values are
calculated on the English text translated from T1
(i.e., Text in Spanish, German, French and Ital-
ian languages). In system architecture 2, the Mi-
crosoft Bing translator is used to translate T2
texts (in English) to different languages (i.e. in
Spanish, German, French and Italian) and calcu-
late N – Gram matching and Text Similarity
values on these (T1 – newly translated T2) pairs.
Other lexical sub modules are executed as be-
fore. These lexical matching scores are stored
and compared according to the heuristic defined
in section 2.1.
</bodyText>
<subsectionHeader confidence="0.9987255">
2.3 System Architecture 3: CLTE task
using Voting
</subsectionHeader>
<bodyText confidence="0.9998564">
The system considers the output of the previous
two systems (Run 1 from System architecture 1
and Run 2 from System architecture 2) as input.
If the entailment decision of both the runs agrees
then this is output as the final entailment label.
Otherwise, if they do not agree, the final entail-
ment label will be “no_entailment”. The voting
rule can be defined as the ANDing rule where
logical AND operation of the two inputs are
considered to arrive at the final evaluation class.
</bodyText>
<sectionHeader confidence="0.980672" genericHeader="method">
3 Experiments on Datasets and Results
</sectionHeader>
<bodyText confidence="0.99472625">
Three runs (Run 1, Run 2 and Run 3) for each
language were submitted for the SemEval-3
Task 8. The descriptions of submissions for the
CLTE task are as follows:
</bodyText>
<listItem confidence="0.996340333333333">
• Run1: Lexical matching between text pairs
(Based on system Architecture – 1).
• Run2: Lexical matching between text pairs
(Based on System Architecture – 2).
• Run3: ANDing Module between Run1 and
Run2. (Based on System Architecture –3).
</listItem>
<tableCaption confidence="0.6795024">
The CLTE dataset consists of 500 training
CLTE pairs and 500 test CLTE pairs. The re-
sults for Run 1, Run 2 and Run 3 for each lan-
guage on CLTE Development set are shown in
Table 1.
</tableCaption>
<table confidence="0.999728846153846">
Run Name Accuracy
JU-CSE-NLP_deu-eng_run1 0.284
JU-CSE-NLP_deu-eng_run2 0.268
JU-CSE-NLP_deu-eng_run3 0.270
JU-CSE-NLP_fra-eng_run1 0.290
JU-CSE-NLP_fra-eng_run2 0.320
JU-CSE-NLP_fra-eng_run3 0.278
JU-CSE-NLP_ita-eng_run1 0.302
JU-CSE-NLP_ita-eng_run2 0.298
JU-CSE-NLP_ita-eng_run3 0.298
JU-CSE-NLP_spa-eng_run1 0.270
JU-CSE-NLP_spa-eng_run2 0.262
JU-CSE-NLP_spa-eng_run3 0.262
</table>
<tableCaption confidence="0.999946">
Table 1: Results on Development set
</tableCaption>
<page confidence="0.999071">
693
</page>
<bodyText confidence="0.9948685">
The comparison of the runs for different lan-
guages shows that in case of deu-eng language
pair system architecture – 1 is useful for devel-
opment data whereas system architecture – 2 is
more accurate for test data. For fra-eng language
pair, system architecture - 2 is more accurate for
development data whereas voting helps to get
more accurate results for test data. Similar to the
deu-eng language pair, ita-eng language pair
shows same trends, i.e., system architecture – 1
is more helpful for development data and system
architecture – 2 is more accurate for test data. In
case of spa-eng language pair system architec-
ture – 1 is helpful for both development and test
data.
The results for Run 1, Run 2 and Run 3 for each
language on CLTE Test set are shown in Table
2.
</bodyText>
<table confidence="0.898787538461538">
Run Name Accuracy
JU-CSE-NLP_deu-eng_run1 0.262
JU-CSE-NLP_deu-eng_run2 0.296
JU-CSE-NLP_deu-eng_run3 0.264
JU-CSE-NLP_fra-eng_run1 0.288
JU-CSE-NLP_fra-eng_run2 0.294
JU-CSE-NLP_fra-eng_run3 0.296
JU-CSE-NLP_ita-eng_run1 0.316
JU-CSE-NLP_ita-eng_run2 0.326
JU-CSE-NLP_ita-eng_run3 0.314
JU-CSE-NLP_spa-eng_run1 0.274
JU-CSE-NLP_spa-eng_run2 0.266
JU-CSE-NLP_spa-eng_run3 0.272
</table>
<tableCaption confidence="0.998551">
Table 2: Results on Test Set
</tableCaption>
<sectionHeader confidence="0.997616" genericHeader="conclusions">
4 Conclusions and Future Works
</sectionHeader>
<bodyText confidence="0.999990111111111">
We have participated in Task 8 of Semeval-2012
named Cross Lingual Textual Entailment mainly
based on lexical matching and translation of text
and hypothesis sentences in the cross lingual
corpora. Both lexical matching and translation
have their limitations. Lexical matching is useful
for simple sentences but fails to retain high ac-
curacy for complex sentences with number of
clauses. Semantic graph matching or conceptual
graph is a good substitution to overcome these
limitations. Machine learning technique is
another important tool for multi-class entailment
task. Features can be trained by some machine
learning tools (such as SVM, Naïve Bayes or
Decision tree etc.) with multi-way entailment
(forward, backward, bi-directional, no-
entailment) as its class. Works have been started
in these directions.
</bodyText>
<sectionHeader confidence="0.99809" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996251857142857">
The work was carried out under partial support
of the DST India-CONACYT Mexico project
“Answer Validation through Textual Entail-
ment” funded by DST, Government of India and
partial support of the project CLIA Phase II
(Cross Lingual Information Access) funded by
DIT, Government of India.
</bodyText>
<sectionHeader confidence="0.999362" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99953578125">
[1] Negri, M., Marchetti, A., Mehdad, Y., Bentivogli,
L., and Giampiccolo, D.: Semeval-2012 Task 8:
Cross-lingual Textual Entailment for Content Syn-
chronization. In Proceedings of the 6th International
Workshop on Semantic Evaluation (SemEval 2012).
[2] Dagan, I., Glickman, O., Magnini, B.: The
PASCAL Recognising Textual Entailment Chal-
lenge. Proceedings of the First PASCAL Recog-
nizing Textual Entailment Workshop. (2005).
[3] Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L.,
Giampiccolo, D., Magnini, B., Szpektor, I.: The-
Seond PASCAL Recognising Textual Entailment
Challenge. Proceedings of the Second PASCAL
Challenges Workshop on Recognising Textual En-
tailment, Venice, Italy (2006).
[4] Giampiccolo, D., Magnini, B., Dagan, I., Dolan,
B.: The Third PASCAL Recognizing Textual En-
tailment Challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and
Paraphrasing, Prague, Czech Republic. (2007).
[5] Giampiccolo, D., Dang, H. T., Magnini, B., Da-
gan, I., Cabrio, E.: The Fourth PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2008
Proceedings. (2008)
[6] Bentivogli, L., Dagan, I., Dang. H.T., Giampicco-
lo, D., Magnini, B.: The Fifth PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2009
Workshop, National Institute of Standards and
Technology Gaithersburg, Maryland USA. (2009).
[7] Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa
Trang Dang,Danilo Giampiccolo: The Sixth
PASCAL Recognizing Textual Entailment Chal-
</reference>
<page confidence="0.985992">
694
</page>
<reference confidence="0.999890140350877">
lenge. In TAC 2010 Notebook Proceedings.
(2010)
[8] Bentivogli, L., Clark, P., Dagan, I., Dang, H.,
Giampiccolo, D.: The Seventh PASCAL Recogniz-
ing Textual Entailment Challenge. In TAC 2011
Notebook Proceedings. (2011)
[9] Bos, Johan, Fabio Massimo Zanzotto, and Marco
Pennacchiotti. 2009. Textual Entailment at
EVALITA 2009: In Proceedings of EVALITA
2009.
[10] Mehdad, Yashar, Matteo Negri, and Marcello
Federico.2010. Towards Cross-Lingual Textual
entailment. In Proceedings of the 11th Annual
Conference of the North American Chapter of the
Association for Computational Linguistics,
NAACL-HLT 2010. LA, USA.
[11] Negri, Matteo, and Yashar Mehdad. 2010.
Creating a Bilingual Entailment Corpus through
Translations with Mechanical Turk: $100 for a
10-day Rush. In Proceedings of the NAACL-HLT
2010, Creating Speech and Text Language Data
With Amazon&apos;s Mechanical Turk Workshop. LA,
USA.
[12] Mehdad, Yashar, Matteo Negri, Marcello Fede-
rico. 2011. Using Bilingual Parallel Corpora for
Cross-Lingual Textual Entailment. In Proceedings
of ACL 2011.
[13] Yuret, D., Han, A., Turgut, Z.: SemEval-2010
Task 12: Parser Evaluation using Textual Entail-
ments. Proceedings of the SemEval-2010 Evalua-
tion Exercises on Semantic Evaluation. (2010).
[14] H. Shima, H. Kanayama, C.-W. Lee, C.-J. Lin,T.
Mitamura, S. S. Y. Miyao, and K. Takeda. Over-
view of ntcir-9 rite: Recognizing inference in text.
In NTCIR-9 Proceedings,2011.
[15] Pakray, P., Bandyopadhyay, S., Gelbukh, A.:
Lexical based two-way RTE System at RTE-5. Sys-
tem Report, TAC RTE Notebook. (2009)
[16] Pakray, P., Pal, S., Poria, S., Bandyopadhyay, S.,
, Gelbukh, A.: JU_CSE_TAC: Textual Entailment
Recognition System at TAC RTE-6. System Re-
port, Text Analysis Conference Recognizing Tex-
tual Entailment Track (TAC RTE) Notebook.
(2010)
[17] Pakray, P., Neogi, S., Bhaskar, P., Poria, S.,
Bandyopadhyay, S., Gelbukh, A.: A Textual En-
tailment System using Anaphora Resolution. Sys-
tem Report. Text Analysis Conference
Recognizing Textual Entailment Track Notebook,
November 14-15. (2011)
[18] Pakray, P., Neogi, S., Bandyopadhyay, S., Gel-
bukh, A.: A Textual Entailment System using Web
based Machine Translation System. NTCIR-9, Na-
tional Center of Sciences, Tokyo, Japan. Decem-
ber 6-9, 2011. (2011)
[19] Fellbaum, C.: WordNet: An Electronic Lexical
Database. MIT Press (1998).
</reference>
<page confidence="0.998787">
695
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.085616">
<title confidence="0.9985025">JU_CSE_NLP: Language Independent Cross-lingual Textual Entailment System</title>
<author confidence="0.997338">Partha Sivaji</author>
<affiliation confidence="0.9876604">Science &amp; Engineering Jadavpur University, Kolkata, Science &amp; Engineering Jadavpur University, Kolkata, Intern at Xerox Research Centre</affiliation>
<address confidence="0.580933">Grenoble,</address>
<title confidence="0.7400015">for Computing National Polytechnic</title>
<author confidence="0.702918">Mexico City</author>
<email confidence="0.999837">gelbukh@gelbukh.com</email>
<abstract confidence="0.952619290322581">This article presents the experiments carried out at Jadavpur University as part of the participation in Cross-lingual Textual Entailment for Content Synchronization (CLTE) of task 8 @ Semantic Evaluation Exercises (SemEval-2012). The work explores cross-lingual textual entailment as a relation between two texts in different languages and proposes different measures for entailment decision in a four way classification tasks (forward, backward, bidirectional and no-entailment). We set up different heuristics and measures for evaluating the entailment between two texts based on lexical relations. Experiments have been carried out with both the text and hypothesis converted to the same language using the Microsoft Bing translation system. The entailment system considers Named Entity, Noun Chunks, Part of speech, N-Gram and some text similarity measures of the text pair to decide the entailment judgments. Rules have been developed to encounter the multi way entailment issue. Our system decides on the entailment judgment after comparing the entailment scores for the text pairs. Four different rules have been developed for the four different classes of entailment. best run is submitted for Italian</abstract>
<note confidence="0.681693">English language with accuracy 0.326.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Negri</author>
<author>A Marchetti</author>
<author>Y Mehdad</author>
<author>L Bentivogli</author>
<author>D Giampiccolo</author>
</authors>
<title>Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="1963" citStr="[1]" startWordPosition="273" endWordPosition="273">, N-Gram and some text similarity measures of the text pair to decide the entailment judgments. Rules have been developed to encounter the multi way entailment issue. Our system decides on the entailment judgment after comparing the entailment scores for the text pairs. Four different rules have been developed for the four different classes of entailment. The best run is submitted for Italian – English language with accuracy 0.326. 1 Introduction Textual Entailment (TE) (Dagan and Glickman, 2004) is one of the recent challenges of Natural Language Processing (NLP). The Task 8 of SemEval-20121 [1] defines a textual entailment system that specifies two major aspects: the task is based on cross-lingual corpora and the entailment decision must be four ways. Given a pair of topically related text fragments (T1 and T2) in different languages, the CLTE task consists of automatically annotating it with one of the following entailment judgments: i. Bidirectional (T1 -&gt;T2 &amp; T1 &lt;- T2): the two fragments entail each other (semantic equivalence) ii. Forward (T1 -&gt; T2 &amp; T1!&lt;- T2): unidirectional entailment from T1 to T2 . iii. Backward (T1! -&gt; T2 &amp; T1 &lt;- T2): unidirectional entailment from T2 to T1</context>
</contexts>
<marker>[1]</marker>
<rawString>Negri, M., Marchetti, A., Mehdad, Y., Bentivogli, L., and Giampiccolo, D.: Semeval-2012 Task 8: Cross-lingual Textual Entailment for Content Synchronization. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2005</date>
<booktitle>Proceedings of the First PASCAL Recognizing Textual Entailment Workshop.</booktitle>
<contexts>
<context position="3262" citStr="[2]" startWordPosition="473" endWordPosition="473">ss Lingual Textual Entailment) task consists of 1,000 CLTE dataset pairs (500 for 1http://www.cs.york.ac.uk/semeval2012/index.php?id=tasks 689 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 689–695, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics training and 500 for test) available for the following language combinations: - Spanish/English (spa-eng) - German/English (deu-eng). - Italian/English (ita-eng) - French/English (fra-eng) Seven Recognizing Textual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluatio</context>
</contexts>
<marker>[2]</marker>
<rawString>Dagan, I., Glickman, O., Magnini, B.: The PASCAL Recognising Textual Entailment Challenge. Proceedings of the First PASCAL Recognizing Textual Entailment Workshop. (2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bar-Haim</author>
<author>I Dagan</author>
<author>B Dolan</author>
<author>L Ferro</author>
<author>D Giampiccolo</author>
<author>B Magnini</author>
<author>Szpektor</author>
</authors>
<title>I.: TheSeond PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment,</booktitle>
<location>Venice,</location>
<contexts>
<context position="3273" citStr="[3]" startWordPosition="475" endWordPosition="475">Textual Entailment) task consists of 1,000 CLTE dataset pairs (500 for 1http://www.cs.york.ac.uk/semeval2012/index.php?id=tasks 689 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 689–695, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics training and 500 for test) available for the following language combinations: - Spanish/English (spa-eng) - German/English (deu-eng). - Italian/English (ita-eng) - French/English (fra-eng) Seven Recognizing Textual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Tex</context>
</contexts>
<marker>[3]</marker>
<rawString>Bar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampiccolo, D., Magnini, B., Szpektor, I.: TheSeond PASCAL Recognising Textual Entailment Challenge. Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment, Venice, Italy (2006).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Giampiccolo</author>
<author>B Magnini</author>
<author>I Dagan</author>
<author>B Dolan</author>
</authors>
<title>The Third PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing,</booktitle>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3292" citStr="[4]" startWordPosition="479" endWordPosition="479"> task consists of 1,000 CLTE dataset pairs (500 for 1http://www.cs.york.ac.uk/semeval2012/index.php?id=tasks 689 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 689–695, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics training and 500 for test) available for the following language combinations: - Spanish/English (spa-eng) - German/English (deu-eng). - Italian/English (ita-eng) - French/English (fra-eng) Seven Recognizing Textual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13</context>
</contexts>
<marker>[4]</marker>
<rawString>Giampiccolo, D., Magnini, B., Dagan, I., Dolan, B.: The Third PASCAL Recognizing Textual Entailment Challenge. In Proceedings of the ACLPASCAL Workshop on Textual Entailment and Paraphrasing, Prague, Czech Republic. (2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Giampiccolo</author>
<author>H T Dang</author>
<author>B Magnini</author>
<author>I Dagan</author>
<author>Cabrio</author>
</authors>
<title>E.: The Fourth PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2008</date>
<booktitle>In TAC 2008 Proceedings.</booktitle>
<contexts>
<context position="3311" citStr="[5]" startWordPosition="483" endWordPosition="483">,000 CLTE dataset pairs (500 for 1http://www.cs.york.ac.uk/semeval2012/index.php?id=tasks 689 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 689–695, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics training and 500 for test) available for the following language combinations: - Spanish/English (spa-eng) - German/English (deu-eng). - Italian/English (ita-eng) - French/English (fra-eng) Seven Recognizing Textual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by </context>
</contexts>
<marker>[5]</marker>
<rawString>Giampiccolo, D., Dang, H. T., Magnini, B., Dagan, I., Cabrio, E.: The Fourth PASCAL Recognizing Textual Entailment Challenge. In TAC 2008 Proceedings. (2008)</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Giampiccolo</author>
<author>D Magnini</author>
<author>B</author>
</authors>
<title>The Fifth PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2009</date>
<booktitle>In TAC 2009 Workshop, National Institute of Standards and Technology</booktitle>
<location>Gaithersburg, Maryland USA.</location>
<contexts>
<context position="3330" citStr="[6]" startWordPosition="487" endWordPosition="487">airs (500 for 1http://www.cs.york.ac.uk/semeval2012/index.php?id=tasks 689 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 689–695, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics training and 500 for test) available for the following language combinations: - Spanish/English (spa-eng) - German/English (deu-eng). - Italian/English (ita-eng) - French/English (fra-eng) Seven Recognizing Textual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recogniz</context>
</contexts>
<marker>[6]</marker>
<rawString>Bentivogli, L., Dagan, I., Dang. H.T., Giampiccolo, D., Magnini, B.: The Fifth PASCAL Recognizing Textual Entailment Challenge. In TAC 2009 Workshop, National Institute of Standards and Technology Gaithersburg, Maryland USA. (2009).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luisa Bentivogli</author>
</authors>
<title>Peter Clark, Ido Dagan, Hoa Trang Dang,Danilo Giampiccolo: The Sixth PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2010</date>
<booktitle>In TAC 2010 Notebook Proceedings.</booktitle>
<contexts>
<context position="3349" citStr="[7]" startWordPosition="491" endWordPosition="491">://www.cs.york.ac.uk/semeval2012/index.php?id=tasks 689 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 689–695, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics training and 500 for test) available for the following language combinations: - Spanish/English (spa-eng) - German/English (deu-eng). - Italian/English (ita-eng) - French/English (fra-eng) Seven Recognizing Textual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Te</context>
</contexts>
<marker>[7]</marker>
<rawString>Luisa Bentivogli, Peter Clark, Ido Dagan, Hoa Trang Dang,Danilo Giampiccolo: The Sixth PASCAL Recognizing Textual Entailment Challenge. In TAC 2010 Notebook Proceedings. (2010)</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bentivogli</author>
<author>P Clark</author>
<author>I Dagan</author>
<author>H Dang</author>
<author>D Giampiccolo</author>
</authors>
<title>The Seventh PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2011</date>
<booktitle>In TAC 2011 Notebook Proceedings.</booktitle>
<contexts>
<context position="3371" citStr="[8]" startWordPosition="496" endWordPosition="496">emeval2012/index.php?id=tasks 689 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 689–695, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics training and 500 for test) available for the following language combinations: - Spanish/English (spa-eng) - German/English (deu-eng). - Italian/English (ita-eng) - French/English (fra-eng) Seven Recognizing Textual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized b</context>
</contexts>
<marker>[8]</marker>
<rawString>Bentivogli, L., Clark, P., Dagan, I., Dang, H., Giampiccolo, D.: The Seventh PASCAL Recognizing Textual Entailment Challenge. In TAC 2011 Notebook Proceedings. (2011)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
</authors>
<title>Textual Entailment at EVALITA</title>
<date>2009</date>
<booktitle>In Proceedings of EVALITA</booktitle>
<contexts>
<context position="3585" citStr="[9]" startWordPosition="534" endWordPosition="534">d 500 for test) available for the following language combinations: - Spanish/English (spa-eng) - German/English (deu-eng). - Italian/English (ita-eng) - French/English (fra-eng) Seven Recognizing Textual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario [14]. We have participated in RTE-5 [15], RTE-6 [1</context>
</contexts>
<marker>[9]</marker>
<rawString>Bos, Johan, Fabio Massimo Zanzotto, and Marco Pennacchiotti. 2009. Textual Entailment at EVALITA 2009: In Proceedings of EVALITA 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico 2010</author>
</authors>
<title>Towards Cross-Lingual Textual entailment.</title>
<date>2010</date>
<booktitle>In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT</booktitle>
<publisher>LA, USA.</publisher>
<contexts>
<context position="3771" citStr="[10]" startWordPosition="566" endWordPosition="566">ecognizing Textual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario [14]. We have participated in RTE-5 [15], RTE-6 [16], RTE-7 [17], SemEval-2 Parser Training and Evaluation using Textual Entailment Task and RITE [18]. Section 2 describes our Cross-lingual Textual Entailment system. The various experim</context>
</contexts>
<marker>[10]</marker>
<rawString>Mehdad, Yashar, Matteo Negri, and Marcello Federico.2010. Towards Cross-Lingual Textual entailment. In Proceedings of the 11th Annual Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT 2010. LA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matteo Negri</author>
<author>Yashar Mehdad</author>
</authors>
<title>Creating a Bilingual Entailment Corpus through Translations with Mechanical Turk: $100 for a 10-day Rush.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL-HLT</booktitle>
<location>LA, USA.</location>
<contexts>
<context position="3777" citStr="[11]" startWordPosition="567" endWordPosition="567">zing Textual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario [14]. We have participated in RTE-5 [15], RTE-6 [16], RTE-7 [17], SemEval-2 Parser Training and Evaluation using Textual Entailment Task and RITE [18]. Section 2 describes our Cross-lingual Textual Entailment system. The various experiments c</context>
</contexts>
<marker>[11]</marker>
<rawString>Negri, Matteo, and Yashar Mehdad. 2010. Creating a Bilingual Entailment Corpus through Translations with Mechanical Turk: $100 for a 10-day Rush. In Proceedings of the NAACL-HLT 2010, Creating Speech and Text Language Data With Amazon&apos;s Mechanical Turk Workshop. LA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL</booktitle>
<contexts>
<context position="3783" citStr="[12]" startWordPosition="568" endWordPosition="568">extual Entailment (RTE) evaluation tracks have already been held: RTE-1 in 2005 [2], RTE-2 [3] in 2006, RTE-3 [4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario [14]. We have participated in RTE-5 [15], RTE-6 [16], RTE-7 [17], SemEval-2 Parser Training and Evaluation using Textual Entailment Task and RITE [18]. Section 2 describes our Cross-lingual Textual Entailment system. The various experiments carried</context>
</contexts>
<marker>[12]</marker>
<rawString>Mehdad, Yashar, Matteo Negri, Marcello Federico. 2011. Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment. In Proceedings of ACL 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yuret</author>
<author>A Han</author>
<author>Z Turgut</author>
</authors>
<title>SemEval-2010 Task 12: Parser Evaluation using Textual Entailments.</title>
<date>2010</date>
<booktitle>Proceedings of the SemEval-2010 Evaluation Exercises on Semantic Evaluation.</booktitle>
<contexts>
<context position="3893" citStr="[13]" startWordPosition="584" endWordPosition="584">[4] in 2007, RTE-4 [5] in 2008, RTE-5 [6] in 2009, RTE-6 [7] in 2010 and RTE-7 [8] in 2011. RTE task produces a generic framework for entailment task across NLP applications. The RTE challenges have moved from 2 – way entailment task (YES, NO) to 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario [14]. We have participated in RTE-5 [15], RTE-6 [16], RTE-7 [17], SemEval-2 Parser Training and Evaluation using Textual Entailment Task and RITE [18]. Section 2 describes our Cross-lingual Textual Entailment system. The various experiments carried out on the development and test data sets are described in Section 3 along with the results. The conclusions </context>
</contexts>
<marker>[13]</marker>
<rawString>Yuret, D., Han, A., Turgut, Z.: SemEval-2010 Task 12: Parser Evaluation using Textual Entailments. Proceedings of the SemEval-2010 Evaluation Exercises on Semantic Evaluation. (2010).</rawString>
</citation>
<citation valid="false">
<authors>
<author>H Shima</author>
<author>H Kanayama</author>
<author>C-W Lee</author>
<author>C-J Lin</author>
<author>T Mitamura</author>
<author>S S Y Miyao</author>
<author>K Takeda</author>
</authors>
<title>Overview of ntcir-9 rite: Recognizing inference in text.</title>
<booktitle>In NTCIR-9 Proceedings,2011.</booktitle>
<contexts>
<context position="4139" citStr="[14]" startWordPosition="623" endWordPosition="623"> 3 – way task (YES, NO, UNKNOWN). EVALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario [14]. We have participated in RTE-5 [15], RTE-6 [16], RTE-7 [17], SemEval-2 Parser Training and Evaluation using Textual Entailment Task and RITE [18]. Section 2 describes our Cross-lingual Textual Entailment system. The various experiments carried out on the development and test data sets are described in Section 3 along with the results. The conclusions are drawn in Section 4. 2 System Architecture Our system for CLTE task is based on a set of heuristics that assigns entailment scores to a text pair based on lexical relations. The text and the hypothesis in a text pair are translated to the same</context>
</contexts>
<marker>[14]</marker>
<rawString>H. Shima, H. Kanayama, C.-W. Lee, C.-J. Lin,T. Mitamura, S. S. Y. Miyao, and K. Takeda. Overview of ntcir-9 rite: Recognizing inference in text. In NTCIR-9 Proceedings,2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pakray</author>
<author>S Bandyopadhyay</author>
<author>Gelbukh</author>
</authors>
<title>A.: Lexical based two-way</title>
<date>2009</date>
<booktitle>RTE System at RTE-5. System Report, TAC RTE Notebook.</booktitle>
<contexts>
<context position="4175" citStr="[15]" startWordPosition="629" endWordPosition="629">ALITA/IRTE [9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario [14]. We have participated in RTE-5 [15], RTE-6 [16], RTE-7 [17], SemEval-2 Parser Training and Evaluation using Textual Entailment Task and RITE [18]. Section 2 describes our Cross-lingual Textual Entailment system. The various experiments carried out on the development and test data sets are described in Section 3 along with the results. The conclusions are drawn in Section 4. 2 System Architecture Our system for CLTE task is based on a set of heuristics that assigns entailment scores to a text pair based on lexical relations. The text and the hypothesis in a text pair are translated to the same language using the Microsoft Bing m</context>
</contexts>
<marker>[15]</marker>
<rawString>Pakray, P., Bandyopadhyay, S., Gelbukh, A.: Lexical based two-way RTE System at RTE-5. System Report, TAC RTE Notebook. (2009)</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pakray</author>
<author>S Pal</author>
<author>S Poria</author>
<author>S Bandyopadhyay</author>
</authors>
<title>A.: JU_CSE_TAC: Textual Entailment Recognition System at TAC RTE-6. System Report, Text Analysis Conference Recognizing Textual Entailment Track (TAC RTE) Notebook.</title>
<date>2010</date>
<contexts>
<context position="4187" citStr="[16]" startWordPosition="631" endWordPosition="631">9] task is similar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario [14]. We have participated in RTE-5 [15], RTE-6 [16], RTE-7 [17], SemEval-2 Parser Training and Evaluation using Textual Entailment Task and RITE [18]. Section 2 describes our Cross-lingual Textual Entailment system. The various experiments carried out on the development and test data sets are described in Section 3 along with the results. The conclusions are drawn in Section 4. 2 System Architecture Our system for CLTE task is based on a set of heuristics that assigns entailment scores to a text pair based on lexical relations. The text and the hypothesis in a text pair are translated to the same language using the Microsoft Bing machine trans</context>
</contexts>
<marker>[16]</marker>
<rawString>Pakray, P., Pal, S., Poria, S., Bandyopadhyay, S., , Gelbukh, A.: JU_CSE_TAC: Textual Entailment Recognition System at TAC RTE-6. System Report, Text Analysis Conference Recognizing Textual Entailment Track (TAC RTE) Notebook. (2010)</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pakray</author>
<author>S Neogi</author>
<author>P Bhaskar</author>
<author>S Poria</author>
<author>S Bandyopadhyay</author>
<author>Gelbukh</author>
</authors>
<title>A.: A Textual Entailment System using Anaphora Resolution. System Report. Text Analysis Conference Recognizing Textual Entailment Track Notebook,</title>
<date>2011</date>
<contexts>
<context position="4199" citStr="[17]" startWordPosition="633" endWordPosition="633">imilar to the RTE challenge for the Italian language. So far, TE has been applied only in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario [14]. We have participated in RTE-5 [15], RTE-6 [16], RTE-7 [17], SemEval-2 Parser Training and Evaluation using Textual Entailment Task and RITE [18]. Section 2 describes our Cross-lingual Textual Entailment system. The various experiments carried out on the development and test data sets are described in Section 3 along with the results. The conclusions are drawn in Section 4. 2 System Architecture Our system for CLTE task is based on a set of heuristics that assigns entailment scores to a text pair based on lexical relations. The text and the hypothesis in a text pair are translated to the same language using the Microsoft Bing machine translation syste</context>
</contexts>
<marker>[17]</marker>
<rawString>Pakray, P., Neogi, S., Bhaskar, P., Poria, S., Bandyopadhyay, S., Gelbukh, A.: A Textual Entailment System using Anaphora Resolution. System Report. Text Analysis Conference Recognizing Textual Entailment Track Notebook, November 14-15. (2011)</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pakray</author>
<author>S Neogi</author>
<author>S Bandyopadhyay</author>
<author>Gelbukh</author>
</authors>
<title>A.: A Textual Entailment System using Web based</title>
<date>2011</date>
<booktitle>Machine Translation System. NTCIR-9, National Center of Sciences,</booktitle>
<location>Tokyo, Japan.</location>
<contexts>
<context position="4285" citStr="[18]" startWordPosition="645" endWordPosition="645"> in a monolingual setting. Cross-lingual Textual Entailment (CLTE) has been proposed ([10], [11], [12]) as an extension of Textual Entailment. In 2010, Parser Training and Evaluation using Textual Entailment [13] was organized by SemEval-2. Recognizing Inference in Text (RITE)2 organized by NTCIR-9 in 2011 is the first to expand TE as a 5-way entailment task (forward, backward, bi-directional, contradiction and independent) in a monolingual scenario [14]. We have participated in RTE-5 [15], RTE-6 [16], RTE-7 [17], SemEval-2 Parser Training and Evaluation using Textual Entailment Task and RITE [18]. Section 2 describes our Cross-lingual Textual Entailment system. The various experiments carried out on the development and test data sets are described in Section 3 along with the results. The conclusions are drawn in Section 4. 2 System Architecture Our system for CLTE task is based on a set of heuristics that assigns entailment scores to a text pair based on lexical relations. The text and the hypothesis in a text pair are translated to the same language using the Microsoft Bing machine translation system. The system separates the text pairs (T1 and T2) available in different languages an</context>
</contexts>
<marker>[18]</marker>
<rawString>Pakray, P., Neogi, S., Bandyopadhyay, S., Gelbukh, A.: A Textual Entailment System using Web based Machine Translation System. NTCIR-9, National Center of Sciences, Tokyo, Japan. December 6-9, 2011. (2011)</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press</publisher>
<contexts>
<context position="9684" citStr="[19]" startWordPosition="1503" endWordPosition="1503">/~qiu/NLPTools/JavaRAP.html ii. Chunk Similarity module: In this sub module our system evaluates the key NP-chunks of both text and hypothesis identified using NP Chunker v1.15. Then our system checks the presence of NP-Chunks of hypothesis in the corresponding text. System calculates the overall value for the chunk matching, i.e., number of text NP-chunks that match with hypothesis NPchunks. If the chunks are not similar in their surface form then our system goes for WordNet matching for the words and if they match in WordNet synsets information, the chunks are considered as similar. WordNet [19] is one of most important resource for lexical analysis. The WordNet 2.0 has been used for WordNet based chunk matching. The API for WordNet Searching (JAWS)6 is an API that provides Java applications with the ability to retrieve data from the WordNet database. Let us consider the following example taken from training data: T1: Due/JJ to/TO [an/DT error/NN of/IN communication/NN] between/IN [the/DT police/NN] ... T2: On/IN [Tuesday/NNP] [a/DT failed/VBN communication/NN] between/IN... The chunk in T1 [error communication] matches with T2 [failed communication] via WordNet based synsets informa</context>
</contexts>
<marker>[19]</marker>
<rawString>Fellbaum, C.: WordNet: An Electronic Lexical Database. MIT Press (1998).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>