<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005421">
<title confidence="0.93518">
CoMiC: Adapting a Short Answer Assessment System for Answer Selection
</title>
<author confidence="0.760998">
Bj¨orn Rudzewitz Ramon Ziai
</author>
<address confidence="0.74020125">
Sonderforschungsbereich 833
Eberhard Karls Universit¨at T¨ubingen
NauklerstraBe 35
72070 T¨ubingen, Germany
</address>
<email confidence="0.999121">
{brzdwtz,rziai}@sfs.uni-tuebingen.de
</email>
<sectionHeader confidence="0.9986" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999796666666667">
Open forum threads exhibit a great variabil-
ity in the quality and quantity of the answers
they attract, making it difficult to manually
moderate and separate relevant from irrelevant
content. The goal of SemEval 2015 Task 3
(Subtask A, English) is to build systems that
automatically distinguish between relevant and
irrelevant content in forum threads.
We extend a short answer assessment system to
build relations between forum questions and an-
swers with respect to similarity, question type,
and answer content. The features are used in
a sequence classifier to account for the conver-
sation character of threads. The performance
of this approach is modest in comparison to
the other task participants and also to the per-
formance the system usually reaches in short
answer assessment. However, the new features
implemented for this task are a first step in
developing more fine-grained question-answer
features and identifying relevant answers.
</bodyText>
<sectionHeader confidence="0.999472" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996920704545455">
In this paper, we discuss the adaptation of our Short
Answer Assessment (SAA) system CoMiC (Meurers
et al., 2011) to Task 3, Subtask A (English) of Sem-
Eval 2015, Answer Selection in Community Question
Answering. The aim in the task was to distinguish
helpful from unhelpful answers in a community fo-
rum given a question.
We enter the QA landscape from the perspective
of evaluating student answers to reading comprehen-
sion questions with respect to whether they contain
the targeted content. In such settings, one generally
has a reference answer to which a candidate answer
can be compared, making alignment-based systems a
natural solution. This is not the case for QA, where a
system has to select or rank candidate answers with
regard to a question posed. However, the present task
is still interesting to us because it shares a central
characteristic with SAA: one needs to identify the
relevant part of an answer, given a question. In theo-
retical linguistics, that relevant part is usually called
focus (cf., e.g., Krifka (2007)), and several research
groups have made efforts to annotate it in corpus data
(Hajiˇcov´a and Sgall, 2001; Ritz et al., 2008; Calhoun
et al., 2010; Ziai and Meurers, 2014).
Automatic approaches to identifying focus have
however yet to be proposed, so for the current task,
we adapted and used our SAA system to align can-
didate answers with the forum question, identifying
whether and how question material was picked up,
which in turn should indicate whether answers are
on-topic. We then used a number of features to char-
acterize the unaligned answer material, from POS
classes to temporal expressions. We also encoded
which question words were present in the question in
the hope that the resulting classifier would pick up
connections between individual question words and
the different answer features in an approximation to
identifying the focus of the answer.
The paper is organized as follows: Section 2 briefly
discusses the data of the task before section 3 presents
the details of our system architecture and the features
we used. Section 4 then shows the results of our
efforts and a short error analysis, and finally section 5
concludes and discusses directions for further efforts.
</bodyText>
<page confidence="0.969186">
247
</page>
<note confidence="0.6064085">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 247–251,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.990748" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999964">
The English dataset used in the task is a collection of
web-crawled forum1 texts where each item consists
of a question and responses to the question. Each
response has one of the six labels Good, Bad, Poten-
tial, Dialogue, non-English, or Other, describing its
potential for answering the corresponding question.
The correct label for every response had to be pre-
dicted by the systems at test time. The dataset is not
balanced since it contains more Good labelled an-
swers than answers with another label. The language
used in the questions and responses exhibits strong
deviations from standard English. For a detailed de-
scription, refer to (M`arquez et al., 2015).
</bodyText>
<sectionHeader confidence="0.994551" genericHeader="method">
3 System Details
</sectionHeader>
<bodyText confidence="0.999981888888889">
In this section, we describe the CoMiC system and its
extensions for Task 3 of SemEval 2015. We begin by
going briefly over the baseline system and its features
and continue by describing in detail the new features
introduced for this task.
The baseline CoMiC system is an alignment-based
short answer assessment system. Alignments be-
tween a student and a target answer are computed
on different linguistic levels. The quantities of align-
ments of a certain quality are used as features and
given to a classifier that predicts a binary correctness
label for the student answer. A detailed description
can be found in (Meurers et al., 2011).
For this task, we adapt the system by making it es-
tablish alignments between forum questions and the
corresponding answers. Thus it is used primarily
as a text similarity system extended by features to
differentiate between given and new material.
</bodyText>
<subsectionHeader confidence="0.976181">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.9999978">
The system uses the standard features from the
CoMiC system and a range of new features. Al-
though the new features described here were used in
the context of Question Answering, we are planning
to explore to what extent the usage of these features
will improve the CoMiC system in the context of
short answer assessment. The following sections will
start with an overview about the standard CoMiC
features and will continue with a detailed description
of the new features.
</bodyText>
<footnote confidence="0.959709">
1http://www.qatarliving.com/forum
</footnote>
<subsectionHeader confidence="0.55665">
3.1.1 CoMiC
</subsectionHeader>
<bodyText confidence="0.999354666666667">
As mentioned in the introduction, the CoMiC sys-
tem is designed to judge the contents of a short an-
swer to a reading comprehension question based on
alignment with a target answer (Meurers et al., 2011).
The features it uses express the linguistic unit and
nature of the successful alignments found between
candidate and target answer. In the present setting,
we used the standard CoMiC features to determine
the degree of similarity between the candidate answer
and the forum question, in order to find out whether
the answer does indeed pick up on question topic
material. These features are summarized in Table 1.
</bodyText>
<figure confidence="0.804446269230769">
Feature Description
1. Keyword Overlap Percent of dependency heads
aligned (relative to question)
2./3. Token Overlap Percent of aligned
question/candidate tokens
4./5. Chunk Overlap Percent of aligned
question/candidate chunks (as
identified by OpenNLP2)
6./7. Triple Overlap Percent of aligned
question/candidate
dependency triples
8. Token Match Percent of token alignments
that were token-identical
9. Similarity Match Percent of token alignments
resolved using PMI-IR
(Turney, 2001)
10. Type Match Percent of token alignments
resolved using WordNet
hierarchy (Fellbaum, 1998)
11. Lemma Match Percent of token alignments
that were lemma-resolved
12. Synonym Match Percent of token alignments
sharing same WordNet synset
13. Variety of Match Number of kinds of
(0-5) token-level alignments
(features 8–12)
</figure>
<tableCaption confidence="0.999172">
Table 1: Standard features in the CoMiC system
</tableCaption>
<subsubsectionHeader confidence="0.627568">
3.1.2 POS-Specific Weighting
</subsubsectionHeader>
<bodyText confidence="0.9999508">
The system uses four features that measure how
much of the material not given in the question be-
longs to a group of syntactically related categories.
The idea is to weight new material by estimating a
distribution of general syntactic classes over it. After
</bodyText>
<footnote confidence="0.97933">
2http://opennlp.apache.org/
</footnote>
<page confidence="0.99244">
248
</page>
<bodyText confidence="0.9949825">
the alignment process, the distribution of groups of
POS categories of non-aligned tokens is computed
with respect to all non-aligned tokens. As a basis,
the Penn Treebank POS tags from prior annotation
are used. Four groups are distinguished which are
composed in the following way:
</bodyText>
<listItem confidence="0.999971166666667">
• nouns: subsumes all nominal categories
• verbs: subsumes full verbs, auxiliaries, modals,
and participles
• adjfv: subsumes all adjectival and adverbial cat-
egories
• rest: subsumes all categories not listed above
</listItem>
<bodyText confidence="0.994905888888889">
For every of the four groups, the frequency of each
POS tag in this group in the non-aligned material is
computed, normalized against the frequency of all
POS tags in the non-aligned material, and summed up
to get the overall proportion of this group in the non-
aligned material. Previous experiments suggested
to prefer this approach with coarse groups over an
approach with more fine-grained POS classes due to
its overall robustness needed in this context.
</bodyText>
<subsectionHeader confidence="0.810677">
3.1.3 Question Words
</subsectionHeader>
<bodyText confidence="0.9999532">
In an approximation to identifying question types,
we encoded the presence or absence of the wh-words
who, how, why, when, where, which, whom, whose
and what with a binary feature for each. We also
encode the presence of modal and auxiliary verbs in
the first three tokens of a sentence in order to detect
questions such as “Can anyone help me?”.
The idea behind these features was to enable asso-
ciations between them and the features characterizing
the new material in the answer.
</bodyText>
<subsectionHeader confidence="0.599397">
3.1.4 Named Entity Recognition
</subsectionHeader>
<bodyText confidence="0.999949833333333">
We used the Stanford Named Entity Recognizer
(Finkel et al., 2005) to detect named entities in new
answer material. For each of the three standard
NE classes PERSON, ORGANIZATION and LO-
CATION, we encode its presence or absence in a
binary feature. Additionally, we encode the total
number of syntactic chunks found in the answer, of
which the named entities constitute a subset.
By detecting NEs, we wanted to enable the re-
sulting classifier to pick up connections between the
previously mentioned wh-features and the named en-
tities.
</bodyText>
<subsectionHeader confidence="0.837064">
3.1.5 Temporal Expressions
</subsectionHeader>
<bodyText confidence="0.99998025">
The system uses a binary feature indicating the
presence or absence of one or more temporal ex-
pressions in every answer. In combination with the
question word features, the system can build relations
between questions asking for temporal content and
the presence of temporal expressions in the answer.
The system therefore makes use of an adapted ver-
sion of the HeidelTime temporal tagger (Str¨otgen and
Gertz, 2013) due to its ability to parse web content
with a high accuracy. No distinction is made between
different kinds of temporal expressions recognized
by the HeidelTime module.
</bodyText>
<subsectionHeader confidence="0.999682">
3.2 Adaptation to Social Media Language
</subsectionHeader>
<bodyText confidence="0.999976035714286">
Since the CoMiC system is designed for the assess-
ment of short answers of language learners, several
adaptations were needed in order for the system to
be able to deal with the noisiness of social media
language. These adaptations consist of multiple steps
that will be described in this section.
The first step towards normalizing the language con-
sists of the removal of HTML markup present in
several answers. For this purpose, the CoMiC system
was extended by adding an additional module that
parses the raw input and recursively extracts the text
content while removing any HTML markup. The
jsoup module3 was used to accomplish this task.
The second step in the normalization process is driven
by the idea to exclude certain tokens from further
processing if they are recognized as being of a cate-
gory unlikely to contribute usefully in deeper analysis
by the system, such as emoticons, e-mail addresses,
hashtags, abbreviations, symbols, punctuation se-
quences, etc. Therefore we use an adapted version of
the ark-tweet-nlp module (Gimpel et al., 2011) in the
tokenization step which allows parallel tokenization
and POS tagging with a tagset tailored to cover the
specifics of social media language. The exclusion of
noisy material is done after sentence segmentation,
allowing to preserve sentences including all tokens
from the text, at the same time excluding unwanted
material from further analysis and alignment.
</bodyText>
<footnote confidence="0.955335">
3http://jsoup.org/
</footnote>
<page confidence="0.997397">
249
</page>
<subsectionHeader confidence="0.990579">
3.3 Model
</subsectionHeader>
<bodyText confidence="0.999993">
We trained two different models based on separate
classification methods. We first experimented with
memory-based learning using TiMBL (Daelemans
et al., 2007), using the cosine as distance metric and
k = 5 nearest neighbors that each instance was com-
pared to. In order to take advantage of the fact that a
forum thread is in fact a conversation and the useful-
ness of a given forum answer may depend on previ-
ous answers, we also employed a CRF tagger (MAL-
LET, McCallum (2002)) to classify a sequence of
forum posts instead of a single instance. We used one
Markov order for the CRF. To our knowledge, this is
the only model in the competition that attempted to
classify answer sequences.
The CRF performed slightly better than the
memory-based approach on the development set,
which we attribute to its ability to take an answer’s
context into account. We submitted it as our primary
run and the memory-based one as the contrastive run.
</bodyText>
<sectionHeader confidence="0.999902" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.99985724">
Evaluation was done using two scenarios: fine-
grained (Good, Potential, Dialogue, Bad) and coarse-
grained (Good, Potential, Bad), with missing classes
always collapsed into Bad. Table 2 shows the coarse-
grained accuracies and Macro F1 scores of our sys-
tem variants on development and test set for the En-
glish Subtask A. The CRF approach used in the pri-
mary system outperforms the contrastive memory-
based approach on both data sets in terms of accuracy.
In case of the primary system, the model seems to
transfer well since the accuracy on the test set is
even higher than on the development set. In case
of the contrastive system, the accuracy drops when
the model is applied to the test set. The table also
shows the accuracy for the best-performing system,
JAIST-contrastive, and the majority baseline.
These accuracies are rather modest, both in com-
parison to accuracy values of the CoMiC system
when used for the task of short answer assessment
for which the system is intended and designed, and
also in comparison to other task participants.
An error analysis showed several problems that
influenced the performance of the system. The nois-
iness of the input text on the syntactic and morpho-
logical level caused the POS tagger to assign incor-
</bodyText>
<table confidence="0.9801575">
System Dev. Set Test Set
Acc. F1 Acc. F1
Best system – – 73.76 57.29
CoMiC-prim. 54.89 28.41 54.20 30.63
CoMiC-contr. 53.37 24.36 50.56 23.36
Maj. baseline 53.19 23.15 50.46 22.36
</table>
<tableCaption confidence="0.9920355">
Table 2: Coarse-grained accuracy and Macro F1 of sys-
tems on development and test set for Subtask A, English
</tableCaption>
<bodyText confidence="0.9997155">
rect POS tags. This led to problems for modules
that make use of POS information. The noisiness is
reflected also in the fact that not all lemmas are iden-
tified correctly. Another problem is that the spelling
correction component struggled with certain forms
and did not always find the spelling-corrected form.
The main problem was that too few tokens and hardly
any chunks could be aligned to the question, severely
influencing the alignment-based features. The sys-
tem also got mislead in cases where the person who
posed the question reformulated the question for oth-
ers, since the classifier failed to use the high similar-
ity between the question and the answer as a clear
indicator for an unhelpful answer.
</bodyText>
<sectionHeader confidence="0.999349" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999995647058824">
We applied the short answer system CoMiC to the
task of question selection. The standard CoMiC sys-
tem was used to determine the similarity between a
question and an answer. We added new features to
the CoMiC system to enable the classifier to build re-
lations between the question type and certain answer
features. Extensions to the system were necessary
in order to deal with the noisiness of web texts. We
applied a CRF classifier that takes into account the
context of answers in the forum and found a posi-
tive effect on performance. The results of the task
show that our system performs rather moderately
when used for this task it is not designed or intended
for. However, the new features implemented for this
task are a first step in developing more fine-grained
question-answer features which eventually could be
useful for identifying the relevant part of an answer.
</bodyText>
<sectionHeader confidence="0.998318" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9995775">
We would like to thank two anonymous reviewers for
their detailed and helpful comments.
</bodyText>
<page confidence="0.992288">
250
</page>
<sectionHeader confidence="0.982398" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.962454054054054">
Sasha Calhoun, Jean Carletta, Jason Brenier, Neil Mayo,
Dan Jurafsky, Mark Steedman, and David Beaver. 2010.
The NXT-format switchboard corpus: A rich resource
for investigating the syntax, semantics, pragmatics and
prosody of dialogue. Language Resources and Evalua-
tion, 44:387–419.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch, 2007. TiMBL: Tilburg Memory-
Based Learner Reference Guide, ILK Technical Report
ILK 07-03. Induction of Linguistic Knowledge Re-
search Group Department of Communication and In-
formation Sciences, Tilburg University, Tilburg, The
Netherlands, July 11. Version 6.0.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, Mas-
sachusetts.
Jenny Rose Finkel, Trond Grenager, and Christopher Man-
ning. 2005. Incorporating non-local information into
information extraction systems by gibbs sampling. In
Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 363–370,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Di-
panjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A
Smith. 2011. Part-of-speech tagging for twitter: An-
notation, features, and experiments. In Proceedings of
the 49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies:
short papers-Volume 2, pages 42–47. Association for
Computational Linguistics.
Eva Hajiˇcov´a and Petr Sgall. 2001. Topic-focus and
salience. In Proceedings of the 39th Annual Meeting
on Association for Computational Linguistics, ACL
’01, pages 276–281, Toulouse, France. Association for
Computational Linguistics.
Manfred Krifka. 2007. Basic notions of information struc-
ture. In Caroline Fery, Gisbert Fanselow, and Manfred
Krifka, editors, The notions of information structure,
volume 6 of Interdisciplinary Studies on Information
Structure (ISIS), pages 13–55. Universit¨atsverlag Pots-
dam, Potsdam.
Andrew Kachites McCallum. 2002. MAL-
LET: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey Bai-
ley. 2011. Integrating parallel analysis modules to
evaluate the meaning of answers to reading comprehen-
sion questions. IJCEELL. Special Issue on Automatic
Free-text Evaluation, 21(4):355–369.
Llu´ıs M`arquez, James Glass, Walid Magdy, Alessandro
Moschitti, Preslav Nakov, and Bilal Randeree. 2015.
Semeval-2015 task 3: Answer selection in community
question answering. In Proceedings of the 9th Inter-
national Workshop on Semantic Evaluation (SemEval
2015). Association for Computational Linguistics.
Julia Ritz, Stefanie Dipper, and Michael G¨otze. 2008.
Annotation of information structure: An evaluation
across different types of texts. In Proceedings of the 6th
International Conference on Language Resources and
Evaluation, pages 2137–2142, Marrakech, Morocco.
Jannik Str¨otgen and Michael Gertz. 2013. Multilin-
gual and cross-domain temporal tagging. Language
Resources and Evaluation, 47(2):269–298.
Peter Turney. 2001. Mining the web for synonyms: PMI-
IR versus LSA on TOEFL. In Proceedings of the
Twelfth European Conference on Machine Learning
(ECML-2001), pages 491–502, Freiburg, Germany.
Ramon Ziai and Detmar Meurers. 2014. Focus annotation
in reading comprehension data. In Proceedings of the
8th Linguistic Annotation Workshop (LAW VIII, 2014),
pages 159–168, Dublin, Ireland. COLING, Association
for Computational Linguistics.
</reference>
<page confidence="0.997889">
251
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.304721">
<title confidence="0.999809">CoMiC: Adapting a Short Answer Assessment System for Answer Selection</title>
<author confidence="0.983926">Bj¨orn Rudzewitz Ramon Ziai</author>
<title confidence="0.478096">Sonderforschungsbereich</title>
<author confidence="0.814127">Eberhard Karls Universit¨at</author>
<affiliation confidence="0.633836">NauklerstraBe</affiliation>
<address confidence="0.702907">72070 T¨ubingen,</address>
<abstract confidence="0.999224454545455">Open forum threads exhibit a great variability in the quality and quantity of the answers they attract, making it difficult to manually moderate and separate relevant from irrelevant content. The goal of SemEval 2015 Task 3 (Subtask A, English) is to build systems that automatically distinguish between relevant and irrelevant content in forum threads. We extend a short answer assessment system to build relations between forum questions and answers with respect to similarity, question type, and answer content. The features are used in a sequence classifier to account for the conversation character of threads. The performance of this approach is modest in comparison to the other task participants and also to the performance the system usually reaches in short answer assessment. However, the new features implemented for this task are a first step in developing more fine-grained question-answer features and identifying relevant answers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sasha Calhoun</author>
<author>Jean Carletta</author>
<author>Jason Brenier</author>
<author>Neil Mayo</author>
<author>Dan Jurafsky</author>
<author>Mark Steedman</author>
<author>David Beaver</author>
</authors>
<title>The NXT-format switchboard corpus: A rich resource for investigating the syntax, semantics, pragmatics and prosody of dialogue. Language Resources and Evaluation,</title>
<date>2010</date>
<pages>44--387</pages>
<contexts>
<context position="2395" citStr="Calhoun et al., 2010" startWordPosition="369" endWordPosition="372">didate answer can be compared, making alignment-based systems a natural solution. This is not the case for QA, where a system has to select or rank candidate answers with regard to a question posed. However, the present task is still interesting to us because it shares a central characteristic with SAA: one needs to identify the relevant part of an answer, given a question. In theoretical linguistics, that relevant part is usually called focus (cf., e.g., Krifka (2007)), and several research groups have made efforts to annotate it in corpus data (Hajiˇcov´a and Sgall, 2001; Ritz et al., 2008; Calhoun et al., 2010; Ziai and Meurers, 2014). Automatic approaches to identifying focus have however yet to be proposed, so for the current task, we adapted and used our SAA system to align candidate answers with the forum question, identifying whether and how question material was picked up, which in turn should indicate whether answers are on-topic. We then used a number of features to characterize the unaligned answer material, from POS classes to temporal expressions. We also encoded which question words were present in the question in the hope that the resulting classifier would pick up connections between </context>
</contexts>
<marker>Calhoun, Carletta, Brenier, Mayo, Jurafsky, Steedman, Beaver, 2010</marker>
<rawString>Sasha Calhoun, Jean Carletta, Jason Brenier, Neil Mayo, Dan Jurafsky, Mark Steedman, and David Beaver. 2010. The NXT-format switchboard corpus: A rich resource for investigating the syntax, semantics, pragmatics and prosody of dialogue. Language Resources and Evaluation, 44:387–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko van der Sloot</author>
<author>Antal van den Bosch</author>
</authors>
<title>TiMBL: Tilburg MemoryBased Learner Reference Guide, ILK</title>
<date>2007</date>
<journal>Version</journal>
<tech>Technical Report ILK 07-03.</tech>
<volume>6</volume>
<institution>Induction of Linguistic Knowledge Research Group Department of Communication and Information Sciences, Tilburg University,</institution>
<location>Tilburg, The Netherlands,</location>
<marker>Daelemans, Zavrel, van der Sloot, van den Bosch, 2007</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and Antal van den Bosch, 2007. TiMBL: Tilburg MemoryBased Learner Reference Guide, ILK Technical Report ILK 07-03. Induction of Linguistic Knowledge Research Group Department of Communication and Information Sciences, Tilburg University, Tilburg, The Netherlands, July 11. Version 6.0.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jenny Rose Finkel</author>
<author>Trond Grenager</author>
<author>Christopher Manning</author>
</authors>
<title>Incorporating non-local information into information extraction systems by gibbs sampling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>363--370</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9090" citStr="Finkel et al., 2005" startWordPosition="1435" endWordPosition="1438">ed in this context. 3.1.3 Question Words In an approximation to identifying question types, we encoded the presence or absence of the wh-words who, how, why, when, where, which, whom, whose and what with a binary feature for each. We also encode the presence of modal and auxiliary verbs in the first three tokens of a sentence in order to detect questions such as “Can anyone help me?”. The idea behind these features was to enable associations between them and the features characterizing the new material in the answer. 3.1.4 Named Entity Recognition We used the Stanford Named Entity Recognizer (Finkel et al., 2005) to detect named entities in new answer material. For each of the three standard NE classes PERSON, ORGANIZATION and LOCATION, we encode its presence or absence in a binary feature. Additionally, we encode the total number of syntactic chunks found in the answer, of which the named entities constitute a subset. By detecting NEs, we wanted to enable the resulting classifier to pick up connections between the previously mentioned wh-features and the named entities. 3.1.5 Temporal Expressions The system uses a binary feature indicating the presence or absence of one or more temporal expressions i</context>
</contexts>
<marker>Finkel, Grenager, Manning, 2005</marker>
<rawString>Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 363–370, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<pages>42--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 42–47. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eva Hajiˇcov´a</author>
<author>Petr Sgall</author>
</authors>
<title>Topic-focus and salience.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL ’01,</booktitle>
<pages>276--281</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Toulouse, France.</location>
<marker>Hajiˇcov´a, Sgall, 2001</marker>
<rawString>Eva Hajiˇcov´a and Petr Sgall. 2001. Topic-focus and salience. In Proceedings of the 39th Annual Meeting on Association for Computational Linguistics, ACL ’01, pages 276–281, Toulouse, France. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Krifka</author>
</authors>
<title>Basic notions of information structure.</title>
<date>2007</date>
<booktitle>The notions of information structure,</booktitle>
<volume>6</volume>
<pages>13--55</pages>
<editor>In Caroline Fery, Gisbert Fanselow, and Manfred Krifka, editors,</editor>
<publisher>Universit¨atsverlag</publisher>
<location>Potsdam, Potsdam.</location>
<contexts>
<context position="2248" citStr="Krifka (2007)" startWordPosition="346" endWordPosition="347">n questions with respect to whether they contain the targeted content. In such settings, one generally has a reference answer to which a candidate answer can be compared, making alignment-based systems a natural solution. This is not the case for QA, where a system has to select or rank candidate answers with regard to a question posed. However, the present task is still interesting to us because it shares a central characteristic with SAA: one needs to identify the relevant part of an answer, given a question. In theoretical linguistics, that relevant part is usually called focus (cf., e.g., Krifka (2007)), and several research groups have made efforts to annotate it in corpus data (Hajiˇcov´a and Sgall, 2001; Ritz et al., 2008; Calhoun et al., 2010; Ziai and Meurers, 2014). Automatic approaches to identifying focus have however yet to be proposed, so for the current task, we adapted and used our SAA system to align candidate answers with the forum question, identifying whether and how question material was picked up, which in turn should indicate whether answers are on-topic. We then used a number of features to characterize the unaligned answer material, from POS classes to temporal expressi</context>
</contexts>
<marker>Krifka, 2007</marker>
<rawString>Manfred Krifka. 2007. Basic notions of information structure. In Caroline Fery, Gisbert Fanselow, and Manfred Krifka, editors, The notions of information structure, volume 6 of Interdisciplinary Studies on Information Structure (ISIS), pages 13–55. Universit¨atsverlag Potsdam, Potsdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>MALLET: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="12143" citStr="McCallum (2002)" startWordPosition="1931" endWordPosition="1932">ll tokens from the text, at the same time excluding unwanted material from further analysis and alignment. 3http://jsoup.org/ 249 3.3 Model We trained two different models based on separate classification methods. We first experimented with memory-based learning using TiMBL (Daelemans et al., 2007), using the cosine as distance metric and k = 5 nearest neighbors that each instance was compared to. In order to take advantage of the fact that a forum thread is in fact a conversation and the usefulness of a given forum answer may depend on previous answers, we also employed a CRF tagger (MALLET, McCallum (2002)) to classify a sequence of forum posts instead of a single instance. We used one Markov order for the CRF. To our knowledge, this is the only model in the competition that attempted to classify answer sequences. The CRF performed slightly better than the memory-based approach on the development set, which we attribute to its ability to take an answer’s context into account. We submitted it as our primary run and the memory-based one as the contrastive run. 4 Results Evaluation was done using two scenarios: finegrained (Good, Potential, Dialogue, Bad) and coarsegrained (Good, Potential, Bad), </context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. MALLET: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Detmar Meurers</author>
<author>Ramon Ziai</author>
<author>Niels Ott</author>
<author>Stacey Bailey</author>
</authors>
<title>Integrating parallel analysis modules to evaluate the meaning of answers to reading comprehension questions.</title>
<date>2011</date>
<journal>IJCEELL. Special Issue on Automatic Free-text Evaluation,</journal>
<volume>21</volume>
<issue>4</issue>
<contexts>
<context position="1327" citStr="Meurers et al., 2011" startWordPosition="191" endWordPosition="194">th respect to similarity, question type, and answer content. The features are used in a sequence classifier to account for the conversation character of threads. The performance of this approach is modest in comparison to the other task participants and also to the performance the system usually reaches in short answer assessment. However, the new features implemented for this task are a first step in developing more fine-grained question-answer features and identifying relevant answers. 1 Introduction In this paper, we discuss the adaptation of our Short Answer Assessment (SAA) system CoMiC (Meurers et al., 2011) to Task 3, Subtask A (English) of SemEval 2015, Answer Selection in Community Question Answering. The aim in the task was to distinguish helpful from unhelpful answers in a community forum given a question. We enter the QA landscape from the perspective of evaluating student answers to reading comprehension questions with respect to whether they contain the targeted content. In such settings, one generally has a reference answer to which a candidate answer can be compared, making alignment-based systems a natural solution. This is not the case for QA, where a system has to select or rank cand</context>
<context position="4959" citStr="Meurers et al., 2011" startWordPosition="785" endWordPosition="788">we describe the CoMiC system and its extensions for Task 3 of SemEval 2015. We begin by going briefly over the baseline system and its features and continue by describing in detail the new features introduced for this task. The baseline CoMiC system is an alignment-based short answer assessment system. Alignments between a student and a target answer are computed on different linguistic levels. The quantities of alignments of a certain quality are used as features and given to a classifier that predicts a binary correctness label for the student answer. A detailed description can be found in (Meurers et al., 2011). For this task, we adapt the system by making it establish alignments between forum questions and the corresponding answers. Thus it is used primarily as a text similarity system extended by features to differentiate between given and new material. 3.1 Features The system uses the standard features from the CoMiC system and a range of new features. Although the new features described here were used in the context of Question Answering, we are planning to explore to what extent the usage of these features will improve the CoMiC system in the context of short answer assessment. The following se</context>
</contexts>
<marker>Meurers, Ziai, Ott, Bailey, 2011</marker>
<rawString>Detmar Meurers, Ramon Ziai, Niels Ott, and Stacey Bailey. 2011. Integrating parallel analysis modules to evaluate the meaning of answers to reading comprehension questions. IJCEELL. Special Issue on Automatic Free-text Evaluation, 21(4):355–369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Llu´ıs M`arquez</author>
<author>James Glass</author>
<author>Walid Magdy</author>
<author>Alessandro Moschitti</author>
<author>Preslav Nakov</author>
<author>Bilal Randeree</author>
</authors>
<title>Semeval-2015 task 3: Answer selection in community question answering.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval</booktitle>
<marker>M`arquez, Glass, Magdy, Moschitti, Nakov, Randeree, 2015</marker>
<rawString>Llu´ıs M`arquez, James Glass, Walid Magdy, Alessandro Moschitti, Preslav Nakov, and Bilal Randeree. 2015. Semeval-2015 task 3: Answer selection in community question answering. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Ritz</author>
<author>Stefanie Dipper</author>
<author>Michael G¨otze</author>
</authors>
<title>Annotation of information structure: An evaluation across different types of texts.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation,</booktitle>
<pages>2137--2142</pages>
<location>Marrakech, Morocco.</location>
<marker>Ritz, Dipper, G¨otze, 2008</marker>
<rawString>Julia Ritz, Stefanie Dipper, and Michael G¨otze. 2008. Annotation of information structure: An evaluation across different types of texts. In Proceedings of the 6th International Conference on Language Resources and Evaluation, pages 2137–2142, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jannik Str¨otgen</author>
<author>Michael Gertz</author>
</authors>
<title>Multilingual and cross-domain temporal tagging.</title>
<date>2013</date>
<journal>Language Resources and Evaluation,</journal>
<volume>47</volume>
<issue>2</issue>
<marker>Str¨otgen, Gertz, 2013</marker>
<rawString>Jannik Str¨otgen and Michael Gertz. 2013. Multilingual and cross-domain temporal tagging. Language Resources and Evaluation, 47(2):269–298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Mining the web for synonyms: PMIIR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001),</booktitle>
<pages>491--502</pages>
<location>Freiburg, Germany.</location>
<contexts>
<context position="6844" citStr="Turney, 2001" startWordPosition="1079" endWordPosition="1080">rder to find out whether the answer does indeed pick up on question topic material. These features are summarized in Table 1. Feature Description 1. Keyword Overlap Percent of dependency heads aligned (relative to question) 2./3. Token Overlap Percent of aligned question/candidate tokens 4./5. Chunk Overlap Percent of aligned question/candidate chunks (as identified by OpenNLP2) 6./7. Triple Overlap Percent of aligned question/candidate dependency triples 8. Token Match Percent of token alignments that were token-identical 9. Similarity Match Percent of token alignments resolved using PMI-IR (Turney, 2001) 10. Type Match Percent of token alignments resolved using WordNet hierarchy (Fellbaum, 1998) 11. Lemma Match Percent of token alignments that were lemma-resolved 12. Synonym Match Percent of token alignments sharing same WordNet synset 13. Variety of Match Number of kinds of (0-5) token-level alignments (features 8–12) Table 1: Standard features in the CoMiC system 3.1.2 POS-Specific Weighting The system uses four features that measure how much of the material not given in the question belongs to a group of syntactically related categories. The idea is to weight new material by estimating a d</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter Turney. 2001. Mining the web for synonyms: PMIIR versus LSA on TOEFL. In Proceedings of the Twelfth European Conference on Machine Learning (ECML-2001), pages 491–502, Freiburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramon Ziai</author>
<author>Detmar Meurers</author>
</authors>
<title>Focus annotation in reading comprehension data.</title>
<date>2014</date>
<journal>COLING, Association for Computational Linguistics.</journal>
<booktitle>In Proceedings of the 8th Linguistic Annotation Workshop (LAW VIII,</booktitle>
<pages>159--168</pages>
<location>Dublin,</location>
<contexts>
<context position="2420" citStr="Ziai and Meurers, 2014" startWordPosition="373" endWordPosition="376">ompared, making alignment-based systems a natural solution. This is not the case for QA, where a system has to select or rank candidate answers with regard to a question posed. However, the present task is still interesting to us because it shares a central characteristic with SAA: one needs to identify the relevant part of an answer, given a question. In theoretical linguistics, that relevant part is usually called focus (cf., e.g., Krifka (2007)), and several research groups have made efforts to annotate it in corpus data (Hajiˇcov´a and Sgall, 2001; Ritz et al., 2008; Calhoun et al., 2010; Ziai and Meurers, 2014). Automatic approaches to identifying focus have however yet to be proposed, so for the current task, we adapted and used our SAA system to align candidate answers with the forum question, identifying whether and how question material was picked up, which in turn should indicate whether answers are on-topic. We then used a number of features to characterize the unaligned answer material, from POS classes to temporal expressions. We also encoded which question words were present in the question in the hope that the resulting classifier would pick up connections between individual question words</context>
</contexts>
<marker>Ziai, Meurers, 2014</marker>
<rawString>Ramon Ziai and Detmar Meurers. 2014. Focus annotation in reading comprehension data. In Proceedings of the 8th Linguistic Annotation Workshop (LAW VIII, 2014), pages 159–168, Dublin, Ireland. COLING, Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>