<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000310">
<title confidence="0.997477">
ΔBLEU: A Discriminative Metric for Generation Tasks
with Intrinsically Diverse Targets
</title>
<author confidence="0.96926">
Michel Galley&apos;† Chris Brockett&apos; Alessandro Sordoni2* Yangfeng Jia*
Michael Auli4* Chris Quirk&apos; Margaret Mitchell&apos; Jianfeng Gao&apos; Bill Dolan&apos;
</author>
<affiliation confidence="0.944931">
1Microsoft Research, Redmond, WA, USA
2DIRO, Universit´e de Montr´eal, Montr´eal, QC, Canada
3Georgia Institute of Technology, Atlanta, GA, USA
4Facebook AI Research, Menlo Park, CA, USA
</affiliation>
<sectionHeader confidence="0.985794" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999717">
We introduce Discriminative BLEU
(OBLEU), a novel metric for intrinsic
evaluation of generated text in tasks that
admit a diverse range of possible outputs.
Reference strings are scored for quality
by human raters on a scale of [−1, +1]
to weight multi-reference BLEU. In tasks
involving generation of conversational
responses, OBLEU correlates reasonably
with human judgments and outperforms
sentence-level and IBM BLEU in terms of
both Spearman’s ρ and Kendall’s τ.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99982745">
Many natural language processing tasks involve
the generation of texts where a variety of outputs
are acceptable or even desirable. Tasks with intrin-
sically diverse targets range from machine transla-
tion, summarization, sentence compression, para-
phrase generation, and image-to-text to generation
of conversational interactions. A major hurdle for
these tasks is automation of evaluation, since the
space of plausible outputs can be enormous, and
it is it impractical to run a new human evaluation
every time a new model is built or parameters are
modified.
In Statistical Machine Translation (SMT), the
automation problem has to a large extent been ame-
liorated by metrics such as BLEU (Papineni et al.,
2002) and METEOR (Banerjee and Lavie, 2005)
Although BLEU is not immune from criticism (e.g.,
Callison-Burch et al. (2006)), its properties are well
understood, BLEU scores have been shown to cor-
relate well with human judgments (Doddington,
</bodyText>
<footnote confidence="0.964143">
*The entirety of this work was conducted while at Mi-
crosoft Research.
</footnote>
<note confidence="0.612402">
†Corresponding author: mgalley@microsoft.com
</note>
<bodyText confidence="0.973039846153846">
2002; Coughlin, 2003; Graham and Baldwin, 2014;
Graham et al., 2015) in SMT, and it has allowed
the field to proceed.
BLEU has been less successfully applied to non-
SMT generation tasks owing to the larger space
of plausible outputs. As a result, attempts have
been made to adapt the metric. To foster diversity
in paraphrase generation, Sun and Zhou (2012)
propose a metric called iBLEU in which the BLEU
score is discounted by a BLEU score computed be-
tween the source and paraphrase. This solution,
in addition to being dependent on a tunable pa-
rameter, is specific only to paraphrase. In image
captioning tasks, Vendantam et al. (2015), employ
a variant of BLEU in which n-grams are weighted
by tf·idf. This assumes the availability of a corpus
with which to compute tf ·idf. Both the above can
be seen as attempting to capture a notion of target
goodness that is not being captured in BLEU.
In this paper, we introduce Discriminative BLEU
(OBLEU), a new metric that embeds human judg-
ments concerning the quality of reference sen-
tences directly into the computation of corpus-level
multiple-reference BLEU. In effect, we push part of
the burden of human evaluation into the automated
metric, where it can be repeatedly utilized.
Our testbed for this metric is data-driven con-
versation, a field that has begun to attract inter-
est (Ritter et al., 2011; Sordoni et al., 2015) as an
alternative to conventional rule-driven or scripted
dialog systems. Intrinsic evaluation in this field
is exceptionally challenging because the semantic
space of possible responses resists definition and is
only weakly constrained by conversational inputs.
Below, we describe OBLEU and investigate its
characteristics in comparison to standard BLEU in
the context of conversational response generation.
We demonstrate that OBLEU correlates well with
human evaluation scores in this task and thus can
</bodyText>
<page confidence="0.922111">
445
</page>
<note confidence="0.379116666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 445–450,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999844">
Figure 1: Example of consecutive utterances of a dialog.
</figureCaption>
<bodyText confidence="0.994877">
provide a basis for automated training and evalua-
tion of data-driven conversation systems—and, we
ultimately believe, other text generation tasks with
inherently diverse targets.
</bodyText>
<sectionHeader confidence="0.903709" genericHeader="method">
2 Evaluating Conversational Responses
</sectionHeader>
<bodyText confidence="0.998454666666667">
Given an input message m and a prior conversation
history c, the goal of a response generation
system is to produce a hypothesis h that is both
well-formed and a pertinent response to message
m (example in Fig. 1). We assume that a set of J
references {ri,j} is available for the context ci and
message mi, where i ∈ {1... I} is an index over
the test set. In the case of BLEU,1 the automatic
score of the system output h1 ... hI is defined as:
</bodyText>
<equation confidence="0.977706666666667">
�
1 if q &gt; ρ
BP = (1—PA)
</equation>
<bodyText confidence="0.9629605">
e otherwise
where ρ and q are respectively hypothesis and refer-
ence lengths.2 Then corpus-level n-gram precision
is defined as:
</bodyText>
<equation confidence="0.913133">
EEg E n-grams(hi) maxj {#g(hi,ri,j)}
i
Ei Eg E n-grams(hi) #g(hi)
</equation>
<bodyText confidence="0.998528571428571">
where #g(·) is the number of occurrences of
n-gram g in a given sentence, and #g(u, v) is a
shorthand for min {#g(u), #g(v)}.
It has been demonstrated that metrics such as
BLEU show increased correlation with human judg-
ment as the number of references increases (Przy-
bocki et al., 2008; Dreyer and Marcu, 2012). Unfor-
tunately, gathering multiple references is difficult
in the case of conversations. Data gathered from
naturally occurring conversations offer only one
response per message. One could search (c, m)
pairs that occur multiple times in conversational
data with the hope of finding distinct responses,
but this solution is not feasible. Indeed, the larger
</bodyText>
<footnote confidence="0.99291725">
1Unless mentioned otherwise, BLEU refers to the original
IBM BLEU as first described in (Papineni et al., 2002).
2In the case of multiple references, BLEU selects the refer-
ence whose length is closest to that of the hypothesis.
</footnote>
<bodyText confidence="0.999706090909091">
the context, the less likely we are to find pairs that
match exactly. Furthermore, while it is feasible to
have writers create additional references when the
downstream task is relatively unambiguous (e.g.,
MT), this approach is more questionable in the
case of more subjective tasks such as conversa-
tional response generation. Our solution is to mine
candidate responses from conversational data and
have judges rate the quality of these responses. Our
new metric thus naturally incorporates qualitative
weights associated with references.
</bodyText>
<sectionHeader confidence="0.998643" genericHeader="method">
3 Discriminative BLEU
</sectionHeader>
<bodyText confidence="0.990585266666667">
Discriminative BLEU, or ABLEU, extends BLEU
by exploiting human qualitative judgments wi,j ∈
[−1, +1] associated with references ri,j. It is dis-
criminative in that it both rewards matches with
“good” reference responses (w &gt; 0) and penalizes
matches with “bad” reference responses (w &lt; 0).
Formally, ABLEU is defined as in Equation 1 and 2,
except that pn is instead defined as: ��11
EEiEg E n-grams(hi) maxj:g E ri,j { wi,j · #g (hi, ri,j) }
Eg E n-grams(hi) maxj {wi,j ·#g(hi)}
i
In a nutshell, this is saying that each n-gram match
is weighted by the highest scoring reference in
which it occurs, and this weight can sometimes be
negative. To ensure that the denominator is never
zero, we assume that, for each i there exists at least
one reference ri,j whose weight wi,j is strictly pos-
itive. In addition to its discriminative nature, this
metric has two interesting properties. First, if all
weights wi,j are equal to 1, then the metric score is
identical to BLEU. As such, ABLEU admits BLEU
as a special case. Second, as with IBM BLEU, the
maximum theoretical score is also 1. If the hy-
pothesis happens to match the highest weighted
reference for each sentence, the numerator equals
the denominator and the metric score becomes 1.
While we find this metric particularly appropriate
for response generation, the metric makes no as-
sumption on the task and is applicable to other text
generation tasks such as MT and image captioning.
</bodyText>
<sectionHeader confidence="0.999616" genericHeader="method">
4 Data
</sectionHeader>
<subsectionHeader confidence="0.998888">
4.1 Multi-reference Datasets
</subsectionHeader>
<bodyText confidence="0.9999632">
To create the multi-reference BLEU dev and test
sets used in this study, we adapted and extended the
methodology of Sordoni et al. (2015). From a cor-
pus of 29M Twitter context-message-response con-
versational triples, we randomly extracted approxi-
</bodyText>
<equation confidence="0.963634166666667">
(1: )
BLEU = BP · exp log pn (1)
n
with:
(2)
pn =
</equation>
<page confidence="0.994482">
446
</page>
<tableCaption confidence="0.892671666666667">
Context c Message m Response r Score
Table 1: Sample reference sets created by our multi-reference extraction algorithm, along with the weights used in ΔBLEU.
Triples from which additional references are extracted are in italics. Boxed sentences are in our multi-reference dev set.
</tableCaption>
<bodyText confidence="0.9804376">
i was about to text you and my two cousins got
excited cause they thought you were “rihanna”
aww, i can imagine their disappointment they were very disappointed!!!
yes. luckily, the whole thing feels very much of
the past now.
na this is anything but a disappointment..
they were belly rolling, filarious.
the weather in russia is very cool.
yippee how many 711 are there like 5!
sweaarr i thought there were more
</bodyText>
<equation confidence="0.62140325">
i can imagine!
i can imagine, banks doesn’t disappoint
lmaoo i can imagine.
i can imagine it
</equation>
<bodyText confidence="0.893671627906977">
yes. my ex-boyfriend, killed my cat. like i say,
it was the start of a bad time...
its good.. for some reason i can’t name stand
out tracks but i’ve been playing it since it
dropped
at my lil cousins dancing to “dance for you”.
these kids are a mess.
what’s sick about it?? do you know how long it
is?? no so how is it sick?
your imagination is wrong, very wrong at that.
walking outside with lightning and thunder
sounds 10x scarier than when you ’re inside
play in the rain haha i love listening to thunder! i would def be
out there if i wasn’t so sleepy
0.9
also rain(s) no play, rain(s)
no beach or running today, stupid weather. lets play in the rain. “yolo” isnt that the
vibe??
then its raining... go outside and play in the rain lol1
i’m scared of the thunder
i would but its thundering to
my only official day off this week... now what
to do
play in the rain!
ohh i love that song
now playing silence nice music i ’m playing rain
yeah it is, i just dont see the point of the first 2
laps
me either it would be sick if there was like 5
laps
exactly and if there were more riders so they
drafted and crashed each other
i’ve got that muck off dry shower stuff to try out,
hope it’s muddy now.
igot you, wen iroll up ill pass that shit. iaint
stingy.
i think if we go for it we cud get 5 laps in an
hour!
me either!!
they more the marrier.
how much are they ? like $5
i dont eat gravy on biscuits. me either.
it’s saad oh yeah the snow is very beautiful yeah i can imagine
</bodyText>
<figure confidence="0.997493416666667">
0.6
0.8
0.6
0.4
-0.1
-0.7
if i had a buddy to go play in it with, i would.
it’s no fun alone
0.5
0.2
0.1
yeah right haha. i wan na go swimming
0.1
-0.2
0.3
0.8
i’ve heard its a 30 min lap. but that was from a
dh rider!
i thought they were more then that but ok
well then! why were the biscuits needed?
0.6
0.4
-0.3
-0.8
</figure>
<bodyText confidence="0.999585688888889">
mately 33K candidate triples that were then judged
for conversational quality on a 5-point Likert-type
scale by 3 crowdsourced annotators. Of these, 4232
triples scored an average 4 or higher; these were
randomly binned to create seed dev and test sets
of 2118 triples and 2114 triples respectively. Note
that the dev set is not used in the experiments of
this paper, since OBLEU and IBM BLEU are met-
rics that do not require training. However, the dev
set is released along with a test set in the dataset
release accompanying this paper.
We then sought to identify candidate triples in
the 29M corpus for which both message and re-
sponse are similar to the original messages and
responses in these seed sets. To this end, we em-
ployed an information retrieval algorithm with a
bag-of-words BM25 similarity function (Robertson
et al., 1995), as detailed in Sordoni et al. (2015),
to extract the top 15 responses for each message-
response pair. Unlike Sordoni et al. (2015), we
further appended the original messages (as if par-
roted back). The new triples were then scored for
quality of the response in light of both context and
message by 5 crowdsourced raters each on a 5-
point Likert-type scale.3 Crucially, and again in
contradistinction to Sordoni et al. (2015), we did
not impose a score cutoff on these synthetic multi-
reference sets. Instead, we retained all candidate
responses and scaled their scores into [−1, +1].
Table 1 presents representative multi-reference
examples (from the dev set) together with their con-
verted scores. The context and messages associated
with the supplementary mined responses are also
shown for illustrative purposes to demonstrate the
range of conversations from which they were taken.
In the table, negative-weighted mined responses are
semantically orthogonal to the intent of their newly
assigned context and message. Strongly negatively
weighted responses are completely out of the ball-
park (“the weather in Russia is very cool”, “well
then! Why were the biscuits needed?”); others are a
little more plausible, but irrelevant or possibly topic
changing (“ohh I love that song”). Higher-valued
positive-weighted mined responses are typically
reasonably appropriate and relevant (even though
</bodyText>
<footnote confidence="0.9599715">
3For this work, we sought 2 additional annotations of the
seed responses for consistency with the mined responses. As a
result, scores for some seed responses slipped below our initial
threshold of 4. Nonetheless, these responses were retained.
</footnote>
<page confidence="0.998393">
447
</page>
<bodyText confidence="0.9984935">
extracted from a completely unrelated conversa-
tion), and in some cases can outscore the original
response, as can be seen in the third set of exam-
ples.
</bodyText>
<subsectionHeader confidence="0.989464">
4.2 Human Evaluation of System Outputs
</subsectionHeader>
<bodyText confidence="0.9976131875">
Responses generated by the 7 systems used in this
study on the 2114-triple test set were hand evalu-
ated by 5 crowdsourced raters each on a 5-point
Likert-type scale. From these 7 systems, 12 system
pairs were evaluated, for a total of about pairwise
126K ratings (12 · 5 · 2114). Here too, raters were
asked to evaluate responses in terms of their rele-
vance to both context and message. Outputs from
different systems were randomly interleaved for
presentation to the raters. We obtained human rat-
ings on the following systems:
Phrase-based MT: A phrase-based MT system
similar to (Ritter et al., 2011), whose weights
have been manually tuned. We also included
four variants of that system, which we tuned with
MERT (Och, 2003). These variants differ in their
number of features, and augment (Ritter et al.,
2011) with the following phrase-level features: edit
distance between source and target, cosine similar-
ity, Jaccard index and distance, length ratio, and
DSSM score (Huang et al., 2013).
RNN-based MT: the log-probability according to
the RNN model of (Sordoni et al., 2015).
Baseline: a random baseline.
While OBLEU relies on human qualitative judg-
ments, it is important to note that human judgments
on multi-references (§ 4.1) and those on system out-
puts were collected completely independently. We
also note that the set of systems listed above specif-
ically does not include a retrieval-based model, as
this might have introduced spurious correlation be-
tween the two datasets (§ 4.1 and § 4.2).
</bodyText>
<sectionHeader confidence="0.997646" genericHeader="method">
5 Setup
</sectionHeader>
<bodyText confidence="0.999944088235294">
We use two rank correlation coefficients—
Kendall’s T and Spearman’s p—to assess the level
of correlation between human qualitative ratings
(§4.2) and automated metric scores. More formally,
we compute each correlation coefficient on a series
of paired observations (m1, q1), · · · , (mN, qN).
Here, mi and qi are respectively differences in auto-
matic metric scores and qualitative ratings for two
given systems A and B on a given subset of the
test set.4While much prior work assesses automatic
metrics for MT and other tasks (Lavie and Agarwal,
2007; Hodosh et al., 2013) by computing correla-
tions on observations consisting of single-sentence
system outputs, it has been shown (e.g., Przybocki
et al. (2008)) that correlation coefficients signifi-
cantly increase as observation units become larger.
For instance, corpus-level or system-level correla-
tions tend to be much higher than sentence-level
correlations; Graham and Baldwin (2014) show
that BLEU is competitive with more recent and ad-
vanced metrics when assessed at the system level.5
Therefore, we define our observation unit size to
be M = 100 sentences (responses),6 unless stated
otherwise. We evaluate qi by averaging human rat-
ings on the M sentences, and mi by computing
metric scores on the same set of sentences.7 We
compare three different metrics: BLEU, OBLEU,
and sentence-level BLEU (sBLEU). The last com-
putes sentence-level BLEU scores (Nakov et al.,
2012) and averages them on the M sentences (akin
to macro-averaging). Finally, unless otherwise
noted, all versions of BLEU use n-gram order up
to 2 (BLEU-2), as this achieves better correlation
for all metrics on this data.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9979064">
The main results of our study are shown in Table 2.
OBLEU achieves better correlation with human
than BLEU, when comparing the best configura-
tion of each metric.8 In the case of Spearman’s p,
the confidence intervals of BLEU (.265, .416) and
</bodyText>
<footnote confidence="0.76967716">
4For each given observation pair (mi, qi), we randomize
the order in which A and B are presented to the raters in order
to avoid any positional bias.
5We do not intend to minimize the benefit of a metric that
would be competitive at the sentence-level, which would be
particularly useful for detailed error analyses. However, our
main goal is to reliably evaluate generation systems on test
sets of thousands of sentences, in which case any metric with
good corpus-level correlation (such as BLEU, as shown in
(Graham and Baldwin, 2014)) would be sufficient.
6Enumerating all possible ways of assigning sentences to
observations would cause a combinatorial explosion. Instead,
for all our results we sample 1K assignments and average
correlations coefficients over them (using the same 1K assign-
ments across all metrics). These assignments are done in such
a way that all sentences within an observation belong to the
same system pair.
7We refrained from using larger units, as creating larger
observation units M reduces the total number of units N. This
would have caused confidence intervals to be so wide as to
make this study inconclusive.
8This is also the case on single reference. While ΔBLEU
and BLEU would have the same correlation if original refer-
ences all had the same score of 1, it is not unusual for original
references to get ratings below 1.
</footnote>
<page confidence="0.994843">
448
</page>
<figure confidence="0.9923895">
0.6
0.5
Spearman’s rho
0.4
0.3
0.2
0.1
0
</figure>
<table confidence="0.978342466666667">
Metric refs. Spearman’s p Kendall’s T
BLEU single .260 (.178, .337) .171 (.087, .252)
BLEU w &gt; 0.6 .343 (.265, .416) .232 (.150, .312)
BLEU all .318 (.239, .392) .212 (.129, .292)
sBLEU single .265 (.183, .342) .175 (.091, .256)
sBLEU w &gt; 0.6 .330 (.252, .404) .222 (.140, .302)
sBLEU all .258 (.177, .336) .167 (.083, .249)
ABLEU single .280 (.199,.357) .187 (.103, .268)
ABLEU w &gt; 0.6 .405 (.331,.474) .281 (.200, .357)
ABLEU all .484 (.415,.546) .342 (.265, .415)
discriminative BLEU
IBM BLEU
sentence-level BLEU
1 0.6 0.2 -0.2 -0.6 -1
(A) threshold on reference scores
</table>
<tableCaption confidence="0.99328575">
Table 2: Human correlations for IBM BLEU, sentence-level
BLEU, and ABLEU with 95% confidence intervals. This
compares 3 types of references: single only, high scoring
references (w &gt; 0.6), and all references.
</tableCaption>
<bodyText confidence="0.999743241379311">
ABLEU (.415, .546) barely overlap, while interval
overlap is more significant in the case of Kendall’s
T. Correlation coefficients degrade for BLEU as
we go from w ≥ 0.6 to using all references. This
is expected, since BLEU treats all references as
equal and has no way of discriminating between
them. On the other hand, correlation coefficients
increase for ABLEU after adding lower scoring ref-
erences. It is also worth noticing that BLEU and
sBLEU obtain roughly comparable correlation co-
efficients. This may come as a surprise, because it
has been suggested elsewhere that sBLEU has much
worse correlation than BLEU computed at the cor-
pus level (Przybocki et al., 2008). We surmise that
(at least for this task and data) the differences in
correlations between BLEU and sBLEU observed
in prior work may be less the result of a difference
between micro- and macro-averaging than they are
the effect of different observation unit sizes (as
discussed in §5).
Finally, Figure 2 shows how Spearman’s p is
affected along three dimensions of study. In par-
ticular, we see that ABLEU actually benefits from
the references with negative ratings. While the im-
provement is not pronounced, we note that most ref-
erences have positive ratings. Negatively-weighted
references could have a greater effect if, for exam-
ple, randomly extracted responses had also been
annotated.
</bodyText>
<sectionHeader confidence="0.999505" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9998498">
ABLEU correlates well with human quality judg-
ments of generated conversational responses, out-
performing both IBM BLEU and sentence-level
BLEU in this task and demonstrating that it can
serve as a plausible intrinsic metric for system de-
</bodyText>
<figure confidence="0.99591025">
1 10 20 50 100
(B) correlations with different observation unit sizes (M)
1 2 3 4
(C) n-gram order
</figure>
<figureCaption confidence="0.958462333333333">
Figure 2: A comparison of BLEU, sentence-level BLEU, and
ABLEU along three dimensions: (A) decreasing the threshold
on reference scores wi,j; (B) increasing the unit size for the
correlation study from a single sentence (M=1) to a size of
100; (C) going from BLEU-1 to BLEU-4 for the different
versions of BLEU.
</figureCaption>
<bodyText confidence="0.999661111111111">
velopment.9 An upfront cost is paid for human
evaluation of the reference set, but following that,
the need for further human evaluation can be min-
imized during system development. ABLEU may
help other tasks that use multiple references for
intrinsic evaluation, including image-to-text, sen-
tence compression, and paraphrase generation, and
even statistical machine translation. Evaluation of
ABLEU in these tasks awaits future work.
</bodyText>
<sectionHeader confidence="0.998811" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999765666666667">
We thank the anonymous reviewers, Jian-Yun Nie,
and Alan Ritter for their helpful comments and
suggestions.
</bodyText>
<footnote confidence="0.975346666666667">
9An implementation of ABLEU, multi-reference dev and
test sets, and human rated outputs are available at:
http://research.microsoft.com/convo
</footnote>
<figure confidence="0.997284454545455">
Spearman’s rho
0.6
0.5
0.4
0.3
0.2
0.1
0
discriminative BLEU
IBM BLEU
sentence-level BLEU
Spearman’s rho
0.6
0.5
0.4
0.3
0.2
0.1
0
discriminative BLEU
IBM BLEU
sentence-level BLEU
</figure>
<page confidence="0.997926">
449
</page>
<sectionHeader confidence="0.998233" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999841319444445">
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Proc.
of ACL Workshop on Intrinsic and Extrinsic Evalua-
tion Measures for Machine Translation and/or Sum-
marization, pages 65–72.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in
machine translation research. In EACL, pages 249–
256.
Deborah Coughlin. 2003. Correlating automated and
human assessments of machine translation quality.
In Proc. of MT Summit IX, pages 63–70.
George Doddington. 2002. Automatic evaluation
of machine translation quality using n-gram co-
occurrence statistics. In Proc. of HLT, pages 138–
145.
Markus Dreyer and Daniel Marcu. 2012. HyTER:
Meaning-equivalent semantics for translation evalu-
ation. In Proc. of HLT-NAACL, pages 162–171.
Yvette Graham and Timothy Baldwin. 2014. Testing
for significance of increased correlation with human
judgment. In Proc. of EMNLP, pages 172–176.
Yvette Graham, Timothy Baldwin, and Nitika Mathur.
2015. Accurate evaluation of segment-level ma-
chine translation metrics. In Proc. of NAACL-HLT,
pages 1183–1191.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
Data, models and evaluation metrics. J. Artif. Int.
Res., 47(1):853–899.
Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng,
Alex Acero, and Larry Heck. 2013. Learning deep
structured semantic models for web search using
clickthrough data. In Proc. of the 22nd ACM Inter-
national Conference on Information &amp; Knowledge
Management, pages 2333–2338.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for mt evaluation with high lev-
els of correlation with human judgments. In Proc.
of the Workshop on Statistical Machine Translation
(StatMT), pages 228–231.
Preslav Nakov, Francisco Guzman, and Stephan Vo-
gel. 2012. Optimizing for Sentence-Level BLEU+1
Yields Short Translations. In Proc. of COLING.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of ACL,
pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. of ACL,
pages 311–318.
M. Przybocki, K. Peterson, and S. Bronsart. 2008.
Official results of the NIST 2008 ”Metrics for MA-
chine TRanslation” challenge (MetricsMATR08).
http://nist.gov/speech/tests/metricsmatr/2008/results/.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proc. of EMNLP, pages 583–593.
Stephen E Robertson, Steve Walker, Susan Jones, et al.
1995. Okapi at TREC-3. In TREC.
Alessandro Sordoni, Michel Galley, Michael Auli,
Chris Brockett, Yangfeng Ji, Meg Mitchell, Jian-Yun
Nie, Jianfeng Gao, and Bill Dolan. 2015. A neural
network approach to context-sensitive generation of
conversational responses. In Proc. of NAACL-HLT.
Hong Sun and Ming Zhou. 2012. Joint learning of a
dual SMT system for paraphrase generation. In ACL,
pages 38–42.
Ramakrishna Vedantam, C. Lawrence Zitnick, and
Devi Parikh. 2015. CIDEr: Consensus-based image
description evaluation. In CVPR.
</reference>
<page confidence="0.997871">
450
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.677768">
<title confidence="0.989198333333333">A Discriminative Metric for Generation with Intrinsically Diverse Targets Yangfeng</title>
<author confidence="0.959845">Chris</author>
<affiliation confidence="0.987131">Research, Redmond, WA, Universit´e de Montr´eal, Montr´eal, QC, Institute of Technology, Atlanta, GA,</affiliation>
<address confidence="0.975486">AI Research, Menlo Park, CA, USA</address>
<abstract confidence="0.994432333333333">introduce Discriminative a novel metric for intrinsic evaluation of generated text in tasks that admit a diverse range of possible outputs. Reference strings are scored for quality human raters on a scale of weight multi-reference In tasks involving generation of conversational reasonably with human judgments and outperforms and IBM terms of</abstract>
<intro confidence="0.775418">Spearman’s Kendall’s</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="1653" citStr="Banerjee and Lavie, 2005" startWordPosition="241" endWordPosition="244">esirable. Tasks with intrinsically diverse targets range from machine translation, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) Although BLEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, BLEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. †Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. BLEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metr</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In Proc. of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluating the role of BLEU in machine translation research.</title>
<date>2006</date>
<booktitle>In EACL,</booktitle>
<pages>249--256</pages>
<contexts>
<context position="1732" citStr="Callison-Burch et al. (2006)" startWordPosition="253" endWordPosition="256">ation, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) Although BLEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, BLEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. †Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. BLEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the role of BLEU in machine translation research. In EACL, pages 249– 256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deborah Coughlin</author>
</authors>
<title>Correlating automated and human assessments of machine translation quality.</title>
<date>2003</date>
<booktitle>In Proc. of MT Summit IX,</booktitle>
<pages>63--70</pages>
<contexts>
<context position="1986" citStr="Coughlin, 2003" startWordPosition="291" endWordPosition="292">l to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) Although BLEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, BLEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. †Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. BLEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iBLEU in which the BLEU score is discounted by a BLEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendan</context>
</contexts>
<marker>Coughlin, 2003</marker>
<rawString>Deborah Coughlin. 2003. Correlating automated and human assessments of machine translation quality. In Proc. of MT Summit IX, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram cooccurrence statistics.</title>
<date>2002</date>
<booktitle>In Proc. of HLT,</booktitle>
<pages>138--145</pages>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram cooccurrence statistics. In Proc. of HLT, pages 138– 145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Markus Dreyer</author>
<author>Daniel Marcu</author>
</authors>
<title>HyTER: Meaning-equivalent semantics for translation evaluation.</title>
<date>2012</date>
<booktitle>In Proc. of HLT-NAACL,</booktitle>
<pages>162--171</pages>
<contexts>
<context position="5375" citStr="Dreyer and Marcu, 2012" startWordPosition="847" endWordPosition="850"> set. In the case of BLEU,1 the automatic score of the system output h1 ... hI is defined as: � 1 if q &gt; ρ BP = (1—PA) e otherwise where ρ and q are respectively hypothesis and reference lengths.2 Then corpus-level n-gram precision is defined as: EEg E n-grams(hi) maxj {#g(hi,ri,j)} i Ei Eg E n-grams(hi) #g(hi) where #g(·) is the number of occurrences of n-gram g in a given sentence, and #g(u, v) is a shorthand for min {#g(u), #g(v)}. It has been demonstrated that metrics such as BLEU show increased correlation with human judgment as the number of references increases (Przybocki et al., 2008; Dreyer and Marcu, 2012). Unfortunately, gathering multiple references is difficult in the case of conversations. Data gathered from naturally occurring conversations offer only one response per message. One could search (c, m) pairs that occur multiple times in conversational data with the hope of finding distinct responses, but this solution is not feasible. Indeed, the larger 1Unless mentioned otherwise, BLEU refers to the original IBM BLEU as first described in (Papineni et al., 2002). 2In the case of multiple references, BLEU selects the reference whose length is closest to that of the hypothesis. the context, t</context>
</contexts>
<marker>Dreyer, Marcu, 2012</marker>
<rawString>Markus Dreyer and Daniel Marcu. 2012. HyTER: Meaning-equivalent semantics for translation evaluation. In Proc. of HLT-NAACL, pages 162–171.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvette Graham</author>
<author>Timothy Baldwin</author>
</authors>
<title>Testing for significance of increased correlation with human judgment.</title>
<date>2014</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>172--176</pages>
<contexts>
<context position="2012" citStr="Graham and Baldwin, 2014" startWordPosition="293" endWordPosition="296">uman evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) Although BLEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, BLEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. †Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. BLEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iBLEU in which the BLEU score is discounted by a BLEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), employ </context>
<context position="16007" citStr="Graham and Baldwin (2014)" startWordPosition="2659" endWordPosition="2662">respectively differences in automatic metric scores and qualitative ratings for two given systems A and B on a given subset of the test set.4While much prior work assesses automatic metrics for MT and other tasks (Lavie and Agarwal, 2007; Hodosh et al., 2013) by computing correlations on observations consisting of single-sentence system outputs, it has been shown (e.g., Przybocki et al. (2008)) that correlation coefficients significantly increase as observation units become larger. For instance, corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that BLEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwise. We evaluate qi by averaging human ratings on the M sentences, and mi by computing metric scores on the same set of sentences.7 We compare three different metrics: BLEU, OBLEU, and sentence-level BLEU (sBLEU). The last computes sentence-level BLEU scores (Nakov et al., 2012) and averages them on the M sentences (akin to macro-averaging). Finally, unless otherwise noted, all versions </context>
<context position="17504" citStr="Graham and Baldwin, 2014" startWordPosition="2911" endWordPosition="2914"> metric.8 In the case of Spearman’s p, the confidence intervals of BLEU (.265, .416) and 4For each given observation pair (mi, qi), we randomize the order in which A and B are presented to the raters in order to avoid any positional bias. 5We do not intend to minimize the benefit of a metric that would be competitive at the sentence-level, which would be particularly useful for detailed error analyses. However, our main goal is to reliably evaluate generation systems on test sets of thousands of sentences, in which case any metric with good corpus-level correlation (such as BLEU, as shown in (Graham and Baldwin, 2014)) would be sufficient. 6Enumerating all possible ways of assigning sentences to observations would cause a combinatorial explosion. Instead, for all our results we sample 1K assignments and average correlations coefficients over them (using the same 1K assignments across all metrics). These assignments are done in such a way that all sentences within an observation belong to the same system pair. 7We refrained from using larger units, as creating larger observation units M reduces the total number of units N. This would have caused confidence intervals to be so wide as to make this study incon</context>
</contexts>
<marker>Graham, Baldwin, 2014</marker>
<rawString>Yvette Graham and Timothy Baldwin. 2014. Testing for significance of increased correlation with human judgment. In Proc. of EMNLP, pages 172–176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yvette Graham</author>
<author>Timothy Baldwin</author>
<author>Nitika Mathur</author>
</authors>
<title>Accurate evaluation of segment-level machine translation metrics.</title>
<date>2015</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>1183--1191</pages>
<contexts>
<context position="2034" citStr="Graham et al., 2015" startWordPosition="297" endWordPosition="300"> a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) Although BLEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, BLEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. †Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. BLEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iBLEU in which the BLEU score is discounted by a BLEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), employ a variant of BLEU in w</context>
</contexts>
<marker>Graham, Baldwin, Mathur, 2015</marker>
<rawString>Yvette Graham, Timothy Baldwin, and Nitika Mathur. 2015. Accurate evaluation of segment-level machine translation metrics. In Proc. of NAACL-HLT, pages 1183–1191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micah Hodosh</author>
<author>Peter Young</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Framing image description as a ranking task: Data, models and evaluation metrics.</title>
<date>2013</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>47</volume>
<issue>1</issue>
<contexts>
<context position="15641" citStr="Hodosh et al., 2013" startWordPosition="2609" endWordPosition="2612">en the two datasets (§ 4.1 and § 4.2). 5 Setup We use two rank correlation coefficients— Kendall’s T and Spearman’s p—to assess the level of correlation between human qualitative ratings (§4.2) and automated metric scores. More formally, we compute each correlation coefficient on a series of paired observations (m1, q1), · · · , (mN, qN). Here, mi and qi are respectively differences in automatic metric scores and qualitative ratings for two given systems A and B on a given subset of the test set.4While much prior work assesses automatic metrics for MT and other tasks (Lavie and Agarwal, 2007; Hodosh et al., 2013) by computing correlations on observations consisting of single-sentence system outputs, it has been shown (e.g., Przybocki et al. (2008)) that correlation coefficients significantly increase as observation units become larger. For instance, corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that BLEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwise. We evaluate qi by a</context>
</contexts>
<marker>Hodosh, Young, Hockenmaier, 2013</marker>
<rawString>Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing image description as a ranking task: Data, models and evaluation metrics. J. Artif. Int. Res., 47(1):853–899.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Po-Sen Huang</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
<author>Li Deng</author>
<author>Alex Acero</author>
<author>Larry Heck</author>
</authors>
<title>Learning deep structured semantic models for web search using clickthrough data.</title>
<date>2013</date>
<booktitle>In Proc. of the 22nd ACM International Conference on Information &amp; Knowledge Management,</booktitle>
<pages>2333--2338</pages>
<contexts>
<context position="14548" citStr="Huang et al., 2013" startWordPosition="2429" endWordPosition="2432">and message. Outputs from different systems were randomly interleaved for presentation to the raters. We obtained human ratings on the following systems: Phrase-based MT: A phrase-based MT system similar to (Ritter et al., 2011), whose weights have been manually tuned. We also included four variants of that system, which we tuned with MERT (Och, 2003). These variants differ in their number of features, and augment (Ritter et al., 2011) with the following phrase-level features: edit distance between source and target, cosine similarity, Jaccard index and distance, length ratio, and DSSM score (Huang et al., 2013). RNN-based MT: the log-probability according to the RNN model of (Sordoni et al., 2015). Baseline: a random baseline. While OBLEU relies on human qualitative judgments, it is important to note that human judgments on multi-references (§ 4.1) and those on system outputs were collected completely independently. We also note that the set of systems listed above specifically does not include a retrieval-based model, as this might have introduced spurious correlation between the two datasets (§ 4.1 and § 4.2). 5 Setup We use two rank correlation coefficients— Kendall’s T and Spearman’s p—to assess</context>
</contexts>
<marker>Huang, He, Gao, Deng, Acero, Heck, 2013</marker>
<rawString>Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, and Larry Heck. 2013. Learning deep structured semantic models for web search using clickthrough data. In Proc. of the 22nd ACM International Conference on Information &amp; Knowledge Management, pages 2333–2338.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>METEOR: An automatic metric for mt evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In Proc. of the Workshop on Statistical Machine Translation (StatMT),</booktitle>
<pages>228--231</pages>
<contexts>
<context position="15619" citStr="Lavie and Agarwal, 2007" startWordPosition="2605" endWordPosition="2608">purious correlation between the two datasets (§ 4.1 and § 4.2). 5 Setup We use two rank correlation coefficients— Kendall’s T and Spearman’s p—to assess the level of correlation between human qualitative ratings (§4.2) and automated metric scores. More formally, we compute each correlation coefficient on a series of paired observations (m1, q1), · · · , (mN, qN). Here, mi and qi are respectively differences in automatic metric scores and qualitative ratings for two given systems A and B on a given subset of the test set.4While much prior work assesses automatic metrics for MT and other tasks (Lavie and Agarwal, 2007; Hodosh et al., 2013) by computing correlations on observations consisting of single-sentence system outputs, it has been shown (e.g., Przybocki et al. (2008)) that correlation coefficients significantly increase as observation units become larger. For instance, corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that BLEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwis</context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for mt evaluation with high levels of correlation with human judgments. In Proc. of the Workshop on Statistical Machine Translation (StatMT), pages 228–231.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Francisco Guzman</author>
<author>Stephan Vogel</author>
</authors>
<title>Optimizing for Sentence-Level BLEU+1 Yields Short Translations.</title>
<date>2012</date>
<booktitle>In Proc. of COLING.</booktitle>
<contexts>
<context position="16496" citStr="Nakov et al., 2012" startWordPosition="2740" endWordPosition="2743">ance, corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that BLEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwise. We evaluate qi by averaging human ratings on the M sentences, and mi by computing metric scores on the same set of sentences.7 We compare three different metrics: BLEU, OBLEU, and sentence-level BLEU (sBLEU). The last computes sentence-level BLEU scores (Nakov et al., 2012) and averages them on the M sentences (akin to macro-averaging). Finally, unless otherwise noted, all versions of BLEU use n-gram order up to 2 (BLEU-2), as this achieves better correlation for all metrics on this data. 6 Results The main results of our study are shown in Table 2. OBLEU achieves better correlation with human than BLEU, when comparing the best configuration of each metric.8 In the case of Spearman’s p, the confidence intervals of BLEU (.265, .416) and 4For each given observation pair (mi, qi), we randomize the order in which A and B are presented to the raters in order to avoid</context>
</contexts>
<marker>Nakov, Guzman, Vogel, 2012</marker>
<rawString>Preslav Nakov, Francisco Guzman, and Stephan Vogel. 2012. Optimizing for Sentence-Level BLEU+1 Yields Short Translations. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="14282" citStr="Och, 2003" startWordPosition="2390" endWordPosition="2391">rced raters each on a 5-point Likert-type scale. From these 7 systems, 12 system pairs were evaluated, for a total of about pairwise 126K ratings (12 · 5 · 2114). Here too, raters were asked to evaluate responses in terms of their relevance to both context and message. Outputs from different systems were randomly interleaved for presentation to the raters. We obtained human ratings on the following systems: Phrase-based MT: A phrase-based MT system similar to (Ritter et al., 2011), whose weights have been manually tuned. We also included four variants of that system, which we tuned with MERT (Och, 2003). These variants differ in their number of features, and augment (Ritter et al., 2011) with the following phrase-level features: edit distance between source and target, cosine similarity, Jaccard index and distance, length ratio, and DSSM score (Huang et al., 2013). RNN-based MT: the log-probability according to the RNN model of (Sordoni et al., 2015). Baseline: a random baseline. While OBLEU relies on human qualitative judgments, it is important to note that human judgments on multi-references (§ 4.1) and those on system outputs were collected completely independently. We also note that the </context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proc. of ACL, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="1615" citStr="Papineni et al., 2002" startWordPosition="235" endWordPosition="238">of outputs are acceptable or even desirable. Tasks with intrinsically diverse targets range from machine translation, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) Although BLEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, BLEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. †Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. BLEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, att</context>
<context position="5844" citStr="Papineni et al., 2002" startWordPosition="918" endWordPosition="921">etrics such as BLEU show increased correlation with human judgment as the number of references increases (Przybocki et al., 2008; Dreyer and Marcu, 2012). Unfortunately, gathering multiple references is difficult in the case of conversations. Data gathered from naturally occurring conversations offer only one response per message. One could search (c, m) pairs that occur multiple times in conversational data with the hope of finding distinct responses, but this solution is not feasible. Indeed, the larger 1Unless mentioned otherwise, BLEU refers to the original IBM BLEU as first described in (Papineni et al., 2002). 2In the case of multiple references, BLEU selects the reference whose length is closest to that of the hypothesis. the context, the less likely we are to find pairs that match exactly. Furthermore, while it is feasible to have writers create additional references when the downstream task is relatively unambiguous (e.g., MT), this approach is more questionable in the case of more subjective tasks such as conversational response generation. Our solution is to mine candidate responses from conversational data and have judges rate the quality of these responses. Our new metric thus naturally inc</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. of ACL, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Przybocki</author>
<author>K Peterson</author>
<author>S Bronsart</author>
</authors>
<date>2008</date>
<booktitle>Official results of the NIST</booktitle>
<note>Metrics for MAchine TRanslation” challenge (MetricsMATR08). http://nist.gov/speech/tests/metricsmatr/2008/results/.</note>
<contexts>
<context position="5350" citStr="Przybocki et al., 2008" startWordPosition="842" endWordPosition="846">s an index over the test set. In the case of BLEU,1 the automatic score of the system output h1 ... hI is defined as: � 1 if q &gt; ρ BP = (1—PA) e otherwise where ρ and q are respectively hypothesis and reference lengths.2 Then corpus-level n-gram precision is defined as: EEg E n-grams(hi) maxj {#g(hi,ri,j)} i Ei Eg E n-grams(hi) #g(hi) where #g(·) is the number of occurrences of n-gram g in a given sentence, and #g(u, v) is a shorthand for min {#g(u), #g(v)}. It has been demonstrated that metrics such as BLEU show increased correlation with human judgment as the number of references increases (Przybocki et al., 2008; Dreyer and Marcu, 2012). Unfortunately, gathering multiple references is difficult in the case of conversations. Data gathered from naturally occurring conversations offer only one response per message. One could search (c, m) pairs that occur multiple times in conversational data with the hope of finding distinct responses, but this solution is not feasible. Indeed, the larger 1Unless mentioned otherwise, BLEU refers to the original IBM BLEU as first described in (Papineni et al., 2002). 2In the case of multiple references, BLEU selects the reference whose length is closest to that of the h</context>
<context position="15778" citStr="Przybocki et al. (2008)" startWordPosition="2629" endWordPosition="2632">vel of correlation between human qualitative ratings (§4.2) and automated metric scores. More formally, we compute each correlation coefficient on a series of paired observations (m1, q1), · · · , (mN, qN). Here, mi and qi are respectively differences in automatic metric scores and qualitative ratings for two given systems A and B on a given subset of the test set.4While much prior work assesses automatic metrics for MT and other tasks (Lavie and Agarwal, 2007; Hodosh et al., 2013) by computing correlations on observations consisting of single-sentence system outputs, it has been shown (e.g., Przybocki et al. (2008)) that correlation coefficients significantly increase as observation units become larger. For instance, corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that BLEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwise. We evaluate qi by averaging human ratings on the M sentences, and mi by computing metric scores on the same set of sentences.7 We compare three different me</context>
<context position="19827" citStr="Przybocki et al., 2008" startWordPosition="3298" endWordPosition="3301"> overlap is more significant in the case of Kendall’s T. Correlation coefficients degrade for BLEU as we go from w ≥ 0.6 to using all references. This is expected, since BLEU treats all references as equal and has no way of discriminating between them. On the other hand, correlation coefficients increase for ABLEU after adding lower scoring references. It is also worth noticing that BLEU and sBLEU obtain roughly comparable correlation coefficients. This may come as a surprise, because it has been suggested elsewhere that sBLEU has much worse correlation than BLEU computed at the corpus level (Przybocki et al., 2008). We surmise that (at least for this task and data) the differences in correlations between BLEU and sBLEU observed in prior work may be less the result of a difference between micro- and macro-averaging than they are the effect of different observation unit sizes (as discussed in §5). Finally, Figure 2 shows how Spearman’s p is affected along three dimensions of study. In particular, we see that ABLEU actually benefits from the references with negative ratings. While the improvement is not pronounced, we note that most references have positive ratings. Negatively-weighted references could hav</context>
</contexts>
<marker>Przybocki, Peterson, Bronsart, 2008</marker>
<rawString>M. Przybocki, K. Peterson, and S. Bronsart. 2008. Official results of the NIST 2008 ”Metrics for MAchine TRanslation” challenge (MetricsMATR08). http://nist.gov/speech/tests/metricsmatr/2008/results/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Colin Cherry</author>
<author>William B Dolan</author>
</authors>
<title>Data-driven response generation in social media.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>583--593</pages>
<contexts>
<context position="3317" citStr="Ritter et al., 2011" startWordPosition="516" endWordPosition="519">ty of a corpus with which to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in BLEU. In this paper, we introduce Discriminative BLEU (OBLEU), a new metric that embeds human judgments concerning the quality of reference sentences directly into the computation of corpus-level multiple-reference BLEU. In effect, we push part of the burden of human evaluation into the automated metric, where it can be repeatedly utilized. Our testbed for this metric is data-driven conversation, a field that has begun to attract interest (Ritter et al., 2011; Sordoni et al., 2015) as an alternative to conventional rule-driven or scripted dialog systems. Intrinsic evaluation in this field is exceptionally challenging because the semantic space of possible responses resists definition and is only weakly constrained by conversational inputs. Below, we describe OBLEU and investigate its characteristics in comparison to standard BLEU in the context of conversational response generation. We demonstrate that OBLEU correlates well with human evaluation scores in this task and thus can 445 Proceedings of the 53rd Annual Meeting of the Association for Comp</context>
<context position="14157" citStr="Ritter et al., 2011" startWordPosition="2367" endWordPosition="2370">of System Outputs Responses generated by the 7 systems used in this study on the 2114-triple test set were hand evaluated by 5 crowdsourced raters each on a 5-point Likert-type scale. From these 7 systems, 12 system pairs were evaluated, for a total of about pairwise 126K ratings (12 · 5 · 2114). Here too, raters were asked to evaluate responses in terms of their relevance to both context and message. Outputs from different systems were randomly interleaved for presentation to the raters. We obtained human ratings on the following systems: Phrase-based MT: A phrase-based MT system similar to (Ritter et al., 2011), whose weights have been manually tuned. We also included four variants of that system, which we tuned with MERT (Och, 2003). These variants differ in their number of features, and augment (Ritter et al., 2011) with the following phrase-level features: edit distance between source and target, cosine similarity, Jaccard index and distance, length ratio, and DSSM score (Huang et al., 2013). RNN-based MT: the log-probability according to the RNN model of (Sordoni et al., 2015). Baseline: a random baseline. While OBLEU relies on human qualitative judgments, it is important to note that human judg</context>
</contexts>
<marker>Ritter, Cherry, Dolan, 2011</marker>
<rawString>Alan Ritter, Colin Cherry, and William B. Dolan. 2011. Data-driven response generation in social media. In Proc. of EMNLP, pages 583–593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen E Robertson</author>
<author>Steve Walker</author>
<author>Susan Jones</author>
</authors>
<title>Okapi at TREC-3.</title>
<date>1995</date>
<booktitle>In TREC.</booktitle>
<contexts>
<context position="11722" citStr="Robertson et al., 1995" startWordPosition="1977" endWordPosition="1980">inned to create seed dev and test sets of 2118 triples and 2114 triples respectively. Note that the dev set is not used in the experiments of this paper, since OBLEU and IBM BLEU are metrics that do not require training. However, the dev set is released along with a test set in the dataset release accompanying this paper. We then sought to identify candidate triples in the 29M corpus for which both message and response are similar to the original messages and responses in these seed sets. To this end, we employed an information retrieval algorithm with a bag-of-words BM25 similarity function (Robertson et al., 1995), as detailed in Sordoni et al. (2015), to extract the top 15 responses for each messageresponse pair. Unlike Sordoni et al. (2015), we further appended the original messages (as if parroted back). The new triples were then scored for quality of the response in light of both context and message by 5 crowdsourced raters each on a 5- point Likert-type scale.3 Crucially, and again in contradistinction to Sordoni et al. (2015), we did not impose a score cutoff on these synthetic multireference sets. Instead, we retained all candidate responses and scaled their scores into [−1, +1]. Table 1 present</context>
</contexts>
<marker>Robertson, Walker, Jones, 1995</marker>
<rawString>Stephen E Robertson, Steve Walker, Susan Jones, et al. 1995. Okapi at TREC-3. In TREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Sordoni</author>
<author>Michel Galley</author>
<author>Michael Auli</author>
<author>Chris Brockett</author>
<author>Yangfeng Ji</author>
<author>Meg Mitchell</author>
<author>Jian-Yun Nie</author>
<author>Jianfeng Gao</author>
<author>Bill Dolan</author>
</authors>
<title>A neural network approach to context-sensitive generation of conversational responses.</title>
<date>2015</date>
<booktitle>In Proc. of NAACL-HLT.</booktitle>
<contexts>
<context position="3340" citStr="Sordoni et al., 2015" startWordPosition="520" endWordPosition="523">hich to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in BLEU. In this paper, we introduce Discriminative BLEU (OBLEU), a new metric that embeds human judgments concerning the quality of reference sentences directly into the computation of corpus-level multiple-reference BLEU. In effect, we push part of the burden of human evaluation into the automated metric, where it can be repeatedly utilized. Our testbed for this metric is data-driven conversation, a field that has begun to attract interest (Ritter et al., 2011; Sordoni et al., 2015) as an alternative to conventional rule-driven or scripted dialog systems. Intrinsic evaluation in this field is exceptionally challenging because the semantic space of possible responses resists definition and is only weakly constrained by conversational inputs. Below, we describe OBLEU and investigate its characteristics in comparison to standard BLEU in the context of conversational response generation. We demonstrate that OBLEU correlates well with human evaluation scores in this task and thus can 445 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics a</context>
<context position="8135" citStr="Sordoni et al. (2015)" startWordPosition="1299" endWordPosition="1302">s BLEU as a special case. Second, as with IBM BLEU, the maximum theoretical score is also 1. If the hypothesis happens to match the highest weighted reference for each sentence, the numerator equals the denominator and the metric score becomes 1. While we find this metric particularly appropriate for response generation, the metric makes no assumption on the task and is applicable to other text generation tasks such as MT and image captioning. 4 Data 4.1 Multi-reference Datasets To create the multi-reference BLEU dev and test sets used in this study, we adapted and extended the methodology of Sordoni et al. (2015). From a corpus of 29M Twitter context-message-response conversational triples, we randomly extracted approxi(1: ) BLEU = BP · exp log pn (1) n with: (2) pn = 446 Context c Message m Response r Score Table 1: Sample reference sets created by our multi-reference extraction algorithm, along with the weights used in ΔBLEU. Triples from which additional references are extracted are in italics. Boxed sentences are in our multi-reference dev set. i was about to text you and my two cousins got excited cause they thought you were “rihanna” aww, i can imagine their disappointment they were very disappo</context>
<context position="11760" citStr="Sordoni et al. (2015)" startWordPosition="1984" endWordPosition="1987">f 2118 triples and 2114 triples respectively. Note that the dev set is not used in the experiments of this paper, since OBLEU and IBM BLEU are metrics that do not require training. However, the dev set is released along with a test set in the dataset release accompanying this paper. We then sought to identify candidate triples in the 29M corpus for which both message and response are similar to the original messages and responses in these seed sets. To this end, we employed an information retrieval algorithm with a bag-of-words BM25 similarity function (Robertson et al., 1995), as detailed in Sordoni et al. (2015), to extract the top 15 responses for each messageresponse pair. Unlike Sordoni et al. (2015), we further appended the original messages (as if parroted back). The new triples were then scored for quality of the response in light of both context and message by 5 crowdsourced raters each on a 5- point Likert-type scale.3 Crucially, and again in contradistinction to Sordoni et al. (2015), we did not impose a score cutoff on these synthetic multireference sets. Instead, we retained all candidate responses and scaled their scores into [−1, +1]. Table 1 presents representative multi-reference examp</context>
<context position="14636" citStr="Sordoni et al., 2015" startWordPosition="2443" endWordPosition="2446"> to the raters. We obtained human ratings on the following systems: Phrase-based MT: A phrase-based MT system similar to (Ritter et al., 2011), whose weights have been manually tuned. We also included four variants of that system, which we tuned with MERT (Och, 2003). These variants differ in their number of features, and augment (Ritter et al., 2011) with the following phrase-level features: edit distance between source and target, cosine similarity, Jaccard index and distance, length ratio, and DSSM score (Huang et al., 2013). RNN-based MT: the log-probability according to the RNN model of (Sordoni et al., 2015). Baseline: a random baseline. While OBLEU relies on human qualitative judgments, it is important to note that human judgments on multi-references (§ 4.1) and those on system outputs were collected completely independently. We also note that the set of systems listed above specifically does not include a retrieval-based model, as this might have introduced spurious correlation between the two datasets (§ 4.1 and § 4.2). 5 Setup We use two rank correlation coefficients— Kendall’s T and Spearman’s p—to assess the level of correlation between human qualitative ratings (§4.2) and automated metric </context>
</contexts>
<marker>Sordoni, Galley, Auli, Brockett, Ji, Mitchell, Nie, Gao, Dolan, 2015</marker>
<rawString>Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Meg Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015. A neural network approach to context-sensitive generation of conversational responses. In Proc. of NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hong Sun</author>
<author>Ming Zhou</author>
</authors>
<title>Joint learning of a dual SMT system for paraphrase generation. In</title>
<date>2012</date>
<booktitle>ACL,</booktitle>
<pages>38--42</pages>
<contexts>
<context position="2322" citStr="Sun and Zhou (2012)" startWordPosition="347" endWordPosition="350"> Callison-Burch et al. (2006)), its properties are well understood, BLEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. †Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. BLEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iBLEU in which the BLEU score is discounted by a BLEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), employ a variant of BLEU in which n-grams are weighted by tf·idf. This assumes the availability of a corpus with which to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in BLEU. In this paper, we introduce Discriminative BLEU (OBLEU), a new</context>
</contexts>
<marker>Sun, Zhou, 2012</marker>
<rawString>Hong Sun and Ming Zhou. 2012. Joint learning of a dual SMT system for paraphrase generation. In ACL, pages 38–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramakrishna Vedantam</author>
<author>C Lawrence Zitnick</author>
<author>Devi Parikh</author>
</authors>
<title>CIDEr: Consensus-based image description evaluation.</title>
<date>2015</date>
<booktitle>In CVPR.</booktitle>
<marker>Vedantam, Zitnick, Parikh, 2015</marker>
<rawString>Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. CIDEr: Consensus-based image description evaluation. In CVPR.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>