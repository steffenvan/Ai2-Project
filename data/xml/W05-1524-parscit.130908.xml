<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.180973">
<title confidence="0.949441">
TFLEX: Speeding up Deep Parsing with Strategic Pruning
</title>
<author confidence="0.952348">
Myroslava O. Dzikovska
</author>
<affiliation confidence="0.978781">
Human Communication Research Centre
University of Edinburgh
</affiliation>
<address confidence="0.98193">
Edinburgh, EH8 9LW, UK
</address>
<email confidence="0.999568">
mdzikovs@inf.ed.ac.uk
</email>
<author confidence="0.965625">
Carolyn P. Rose
</author>
<affiliation confidence="0.890428">
Carnegie Mellon University
Language Technologies Institute
Pittsburgh PA 15213, USA
</affiliation>
<email confidence="0.999095">
cprose@cs.cmu.edu
</email>
<sectionHeader confidence="0.999638" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.986934787878788">
This paper presents a method for speeding up a
deep parser through backbone extraction and prun-
ing based on CFG ambiguity packing.&apos; The TRIPS
grammar is a wide-coverage grammar for deep nat-
ural language understanding in dialogue, utilized in
6 different application domains, and with high cov-
erage and sentence-level accuracy on human-human
task-oriented dialogue corpora (Dzikovska, 2004).
The TRIPS parser uses a best-first beam search al-
gorithm and a chart size limit, both of which are a
form of pruning focused on finding an n-best list of
interpretations. However, for longer sentences lim-
iting the chart size results in failed parses, while in-
creasing the chart size limits significantly impacts
the parsing speed.
It is possible to speed up parsing by implement-
ing faster unification algorithms, but this requires
considerable implementation effort. Instead, we de-
veloped a new parser, TFLEX, which uses a sim-
pler technique to address efficiency issues. TFLEX
combines the TRIPS grammar with the fast parsing
technologies implemented in the LCFLEX parser
(Ros´e and Lavie, 2001). LCFLEX is an all-paths
parser which uses left-corner prediction and ambi-
guity packing, and which was shown to be efficient
on other unification augmented context-free gram-
mars. We describe a way to transfer the TRIPS
grammar to LCFLEX, and a pruning method which
achieves significant improvements in both speed and
coverage compared to the original TRIPS parser.
&apos;This material is based on work supported by grants from
the Office of Naval Research under numbers N000140510048
and N000140510043.
</bodyText>
<sectionHeader confidence="0.98998" genericHeader="keywords">
2 TFLEX
</sectionHeader>
<bodyText confidence="0.99997853125">
To use the TRIPS grammar in LCFLEX we first ex-
tracted a CFG backbone from the TRIPS grammar,
with CFG non-terminals corresponding directly to
TRIPS constituent categories. To each CFG rule
we attach a corresponding TRIPS rule. Whenever
a CFG rule completes, a TRIPS unification function
is called to do all the unification operations associ-
ated with the TRIPS rule. If the unification fails, the
constituent built by the CFG is cancelled.
The TFLEX pruning algorithm uses ambiguity
packing to provide good pruning points. For exam-
ple, in the sentence “we have a heart attack victim
at marketplace mall” the phrase “a heart attack vic-
tim” has two interpretations depending on whether
“heart” modifies “attack” or “attack victim”. These
interpretations will be ambiguity packed in the CFG
structure, which offers an opportunity to make prun-
ing more strategic by focusing specifically on com-
peting interpretations for the same utterance span.
For any constituent where ambiguity-packed non-
head daughters differ only in local features, we
prune the interpretations coming from them to a
specified prune beam width based on their TRIPS
scores. In the example above, pruning will happen
at the point of making a VP “have a heart attack vic-
tim”. The NP will be ambiguity packed, and we will
prune alternative VP interpretations resulting from
combining the same sense of the verb “have” and
different interpretations of the NP.
This approach works better than the original
TRIPS best-first algorithm, because for long sen-
tence the TRIPS chart contains a large number
</bodyText>
<page confidence="0.988668">
194
</page>
<bodyText confidence="0.9382199">
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 194–195,
Vancouver, October 2005. c�2005 Association for Computational Linguistics
of similar constituents, and the parser frequently
reaches the chart size limit before finding the correct
constituent to use. Ambiguity packing in TFLEX
helps chose the best constituents to prune by prun-
ing competing interpretations which cover the same
span and have the same non-local features, thus
making it less likely that a constituent essential for
building a parse will be pruned.
</bodyText>
<sectionHeader confidence="0.998722" genericHeader="introduction">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.99985965625">
Our evaluation data is an excerpt from the Monroe
corpus that has been used in previous TRIPS re-
search on parsing speed and accuracy (Swift et al.,
2004). The test contained 1042 utterances, from 1
to 45 words in length (mean 5.38 words/utt, st. dev.
5.7 words/utt). Using a hold-out set, we determined
that a beam width of 3 was an optimal setting for
TFLEX. We then compared TFLEX at beam width
3 to the TRIPS parser with chart size limits of 1500,
5000, and 10000. As our evaluation metrics we re-
port are average parse time per sentence and proba-
bility of finding at least one parse, the latter being a
measure approximating parsing accuracy.
The results are presented in Figure 1. We grouped
sentences into equivalence classes based on length
with a 5-word increment. On sentences greater
than 10 words long, TFLEX is significantly more
likely to produce a parse than any of the TRIPS
parsers (evaluated using a binary logistic regression,
p &lt; .001). Moreover, for sentences greater than
20 words long, no form of TRIPS parser returned
a complete parse. TFLEX is significantly faster
than TRIPS-10000, statistically indistinguishable in
terms of parse time from TRIPS-5000, and signifi-
cantly slower than TRIPS-1500 (p &lt; .001).
Thus, TFLEX presents a superior balance of cov-
erage and efficiency especially for long sentences
(10 words or more) since for these sentences it is
significantly more likely to find a parse than any ver-
sion of TRIPS, even a version where the chart size is
expanded to an extent that it becomes significantly
slower (i.e., TRIPS-10000).
</bodyText>
<sectionHeader confidence="0.999725" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.998558666666667">
In this paper, we described a combination of effi-
cient parsing techniques to improve parsing speed
and coverage with the TRIPS deep parsing grammar.
</bodyText>
<figureCaption confidence="0.684573">
Figure 1: Parse times and probability of getting a
parse depending on (aggregated) sentence lengths.
</figureCaption>
<bodyText confidence="0.999230222222222">
5 denotes sentences with 5 or fewer words, 25 sen-
tences with more than 20 words.
The TFLEX system uses an all-paths left-corner
parsing from the LCFLEX parser, made tractable
by a pruning algorithm based on ambiguity packing
and local features, generalizable to other unification
grammars. Our pruning algorithm provides a bet-
ter efficiency-coverage balance than best-first pars-
ing with chart limits as utilised by the TRIPS parser.
</bodyText>
<sectionHeader confidence="0.999277" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999559">
M. O. Dzikovska. 2004. A Practical Semantic Represen-
tation For Natural Language Parsing. Ph.D. thesis,
University of Rochester.
C. P. Ros´e and A. Lavie. 2001. Balancing robustness
and efficiency in unification-augmented context-free
parsers for large practical applications. In J.C. Junqua
and G Van Noord, editors, Robustness in Language
and Speech Technology. Kluwer Academic Press.
M. Swift, J. Allen, and D. Gildea. 2004. Skeletons in
the parser: Using a shallow parser to improve deep
parsing. In Proceedings of COLING-04.
J. Tetreault, M. Swift, P. Prithviraj, M. Dzikovska, and J.
Allen. 2004. Discourse annotation in the monroe cor-
pus. In ACL-04 workshop on Discourse Annotation.
</reference>
<page confidence="0.998935">
195
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.455460">
<title confidence="0.999753">Speeding up Deep Parsing with Strategic Pruning</title>
<author confidence="0.983174">O Myroslava</author>
<affiliation confidence="0.842635">Human Communication Research University of Edinburgh, EH8 9LW,</affiliation>
<email confidence="0.990955">mdzikovs@inf.ed.ac.uk</email>
<author confidence="0.983412">P Carolyn</author>
<affiliation confidence="0.975476">Carnegie Mellon Language Technologies</affiliation>
<address confidence="0.965502">Pittsburgh PA 15213,</address>
<email confidence="0.956186">cprose@cs.cmu.edu</email>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M O Dzikovska</author>
</authors>
<title>A Practical Semantic Representation For Natural Language Parsing.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="705" citStr="Dzikovska, 2004" startWordPosition="95" endWordPosition="96">nication Research Centre University of Edinburgh Edinburgh, EH8 9LW, UK mdzikovs@inf.ed.ac.uk Carolyn P. Rose Carnegie Mellon University Language Technologies Institute Pittsburgh PA 15213, USA cprose@cs.cmu.edu 1 Introduction This paper presents a method for speeding up a deep parser through backbone extraction and pruning based on CFG ambiguity packing.&apos; The TRIPS grammar is a wide-coverage grammar for deep natural language understanding in dialogue, utilized in 6 different application domains, and with high coverage and sentence-level accuracy on human-human task-oriented dialogue corpora (Dzikovska, 2004). The TRIPS parser uses a best-first beam search algorithm and a chart size limit, both of which are a form of pruning focused on finding an n-best list of interpretations. However, for longer sentences limiting the chart size results in failed parses, while increasing the chart size limits significantly impacts the parsing speed. It is possible to speed up parsing by implementing faster unification algorithms, but this requires considerable implementation effort. Instead, we developed a new parser, TFLEX, which uses a simpler technique to address efficiency issues. TFLEX combines the TRIPS gr</context>
</contexts>
<marker>Dzikovska, 2004</marker>
<rawString>M. O. Dzikovska. 2004. A Practical Semantic Representation For Natural Language Parsing. Ph.D. thesis, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C P Ros´e</author>
<author>A Lavie</author>
</authors>
<title>Balancing robustness and efficiency in unification-augmented context-free parsers for large practical applications.</title>
<date>2001</date>
<booktitle>Robustness in Language and Speech Technology.</booktitle>
<editor>In J.C. Junqua and G Van Noord, editors,</editor>
<publisher>Kluwer Academic Press.</publisher>
<marker>Ros´e, Lavie, 2001</marker>
<rawString>C. P. Ros´e and A. Lavie. 2001. Balancing robustness and efficiency in unification-augmented context-free parsers for large practical applications. In J.C. Junqua and G Van Noord, editors, Robustness in Language and Speech Technology. Kluwer Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Swift</author>
<author>J Allen</author>
<author>D Gildea</author>
</authors>
<title>Skeletons in the parser: Using a shallow parser to improve deep parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of COLING-04.</booktitle>
<contexts>
<context position="4198" citStr="Swift et al., 2004" startWordPosition="649" endWordPosition="652">ctober 2005. c�2005 Association for Computational Linguistics of similar constituents, and the parser frequently reaches the chart size limit before finding the correct constituent to use. Ambiguity packing in TFLEX helps chose the best constituents to prune by pruning competing interpretations which cover the same span and have the same non-local features, thus making it less likely that a constituent essential for building a parse will be pruned. 3 Evaluation Our evaluation data is an excerpt from the Monroe corpus that has been used in previous TRIPS research on parsing speed and accuracy (Swift et al., 2004). The test contained 1042 utterances, from 1 to 45 words in length (mean 5.38 words/utt, st. dev. 5.7 words/utt). Using a hold-out set, we determined that a beam width of 3 was an optimal setting for TFLEX. We then compared TFLEX at beam width 3 to the TRIPS parser with chart size limits of 1500, 5000, and 10000. As our evaluation metrics we report are average parse time per sentence and probability of finding at least one parse, the latter being a measure approximating parsing accuracy. The results are presented in Figure 1. We grouped sentences into equivalence classes based on length with a</context>
</contexts>
<marker>Swift, Allen, Gildea, 2004</marker>
<rawString>M. Swift, J. Allen, and D. Gildea. 2004. Skeletons in the parser: Using a shallow parser to improve deep parsing. In Proceedings of COLING-04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Tetreault</author>
<author>M Swift</author>
<author>P Prithviraj</author>
<author>M Dzikovska</author>
<author>J Allen</author>
</authors>
<title>Discourse annotation in the monroe corpus.</title>
<date>2004</date>
<booktitle>In ACL-04 workshop on Discourse Annotation.</booktitle>
<marker>Tetreault, Swift, Prithviraj, Dzikovska, Allen, 2004</marker>
<rawString>J. Tetreault, M. Swift, P. Prithviraj, M. Dzikovska, and J. Allen. 2004. Discourse annotation in the monroe corpus. In ACL-04 workshop on Discourse Annotation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>