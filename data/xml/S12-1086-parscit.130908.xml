<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018894">
<title confidence="0.9945405">
Weiwei: A Simple Unsupervised Latent Semantics based Approach for
Sentence Similarity
</title>
<author confidence="0.9993">
Weiwei Guo Mona Diab
</author>
<affiliation confidence="0.9977575">
Department of Computer Science, Center for Computational Learning Systems,
Columbia University, Columbia University,
</affiliation>
<email confidence="0.998984">
weiwei@cs.columbia.edu mdiab@ccls.columbia.edu
</email>
<sectionHeader confidence="0.997359" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997594764705882">
The Semantic Textual Similarity (STS) shared
task (Agirre et al., 2012) computes the degree
of semantic equivalence between two sen-
tences.1 We show that a simple unsupervised
latent semantics based approach, Weighted
Textual Matrix Factorization that only exploits
bag-of-words features, can outperform most
systems for this task. The key to the approach
is to carefully handle missing words that are
not in the sentence, and thus rendering it su-
perior to Latent Semantic Analysis (LSA) and
Latent Dirichlet Allocation (LDA). Our sys-
tem ranks 20 out of 89 systems according to
the official evaluation metric for the task, Pear-
son correlation, and it ranks 10/89 and 19/89
in the other two evaluation metrics employed
by the organizers.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996658857142857">
Identifying the degree of semantic similarity [SS]
between two sentences is helpful for many NLP top-
ics. In Machine Translation (Kauchak and Barzi-
lay, 2006) and Text Summarization (Zhou et al.,
2006), results are automatically evaluated based on
sentence comparison. In Text Coherence Detection
(Lapata and Barzilay, 2005), sentences are linked to-
gether by similar or related words. For Word Sense
Disambiguation, researchers (Banerjee and Peder-
sen, 2003; Guo and Diab, 2012a) construct a sense
similarity measure from the sentence similarity of
the sense definitions.
Almost all SS approaches decompose the task into
word pairwise similarity problems. For example, Is-
</bodyText>
<footnote confidence="0.9253245">
1Mona Diab, co-author of this paper, is one of the task orga-
nizers
</footnote>
<bodyText confidence="0.889391861111111">
lam and Inkpen (2008) create a matrix for each sen-
tence pair, where columns are the words in the first
sentence and rows are the words in the second sen-
tence, and each cell stores the distributional similar-
ity of the two words. Then they create an alignment
between words in two sentences, and sentence simi-
larity is calculated based on the sum of the similarity
of aligned word pairs. There are two disadvantages
with word similarity based approaches: 1. lexical
ambiguity as the word pairwise similarity ignores
the semantic interaction between the word and sen-
tence/context. 2. word co-occurrence information
is not as sufficiently exploited as they are in latent
variable models such as Latent Semantic Analysis
(LSA) (Landauer et al., 1998) and Latent Dirichilet
Allocation (LDA) (Blei et al., 2003). On the other
hand, latent variable models can solve the two issues
naturally by modeling the semantics of words and
sentences simultaneously in the low-dimensional la-
tent space.
However, attempts at addressing SS using LSA
perform significantly below word similarity based
models (Mihalcea et al., 2006; O’Shea et al., 2008).
We believe the reason is that the observed words
in a sentence are too few for latent variable mod-
els to learn robust semantics. For example, given
the two sentences of WordNet sense definitions for
bank#n#1 and stock#n#1:
bank#n#1: a financial institution that accepts de-
posits and channels the money into lending activities
stock#n#1: the capital raised by a corporation
through the issue of shares entitling holders to an
ownership interest (equity)
LDA can only find the dominant topic (the
financial topic) based on the observed words with-
out further discernibility. In this case, many sen-
</bodyText>
<page confidence="0.957672">
586
</page>
<note confidence="0.532338">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 586–590,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999931416666667">
tences will share the same latent semantics profile,
as long as they are in the same topic/domain.
In our work (Guo and Diab, 2012b), we propose
to model the missing words (words that are not in
the sentence) to address the sparseness issue for the
SS task. Our intuition is since observed words in a
sentence are too few to tell us what the sentence is
about, missing words can be used to tell us what the
sentence is not about. We assume that the semantic
space of both the observed and missing words make
up the complete semantic profile of a sentence. We
implement our idea using a weighted matrix factor-
ization approach (Srebro and Jaakkola, 2003), which
allows us to treat observed words and missing words
differently.
It should be noted that our approach is very gen-
eral (similar to LSA/LDA) in that it can be applied to
any genre of short texts, in a manner different from
existing work that models short texts by using addi-
tional data, e.g., Ramage et al. (2010) model tweets
using their metadata (author, hashtag, etc). Also we
do not extract additional features such as multiwords
expression or syntax from sentences – all we use is
bag-of-words feature.
</bodyText>
<sectionHeader confidence="0.999879" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9998206875">
Almost all current SS methods work in the high-
dimensional word space, and rely heavily on
word/sense similarity measures. The word/sense
similarity measure is either knowledge based (Li et
al., 2006; Feng et al., 2008; Ho et al., 2010; Tsatsa-
ronis et al., 2010), corpus-based (Islam and Inkpen,
2008) or hybrid (Mihalcea et al., 2006). Almost all
of them are evaluated on a data set introduced in (Li
et al., 2006). The LI06 data set consists of 65 pairs
of noun definitions selected from the Collin Cobuild
Dictionary. A subset of 30 pairs is further selected
by LI06 to render the similarity scores evenly dis-
tributed. Our approach has outperformed most of the
previous methods on LI06 achieving the second best
Pearson’s correlation and the best Spearman corre-
lation (Guo and Diab, 2012b).
</bodyText>
<sectionHeader confidence="0.89845" genericHeader="method">
3 Learning Latent Semantics of Sentences
</sectionHeader>
<subsectionHeader confidence="0.964979">
3.1 Intuition
</subsectionHeader>
<bodyText confidence="0.999884666666667">
Given only a few observed words in a sentence, there
are many hypotheses of latent vectors that are highly
related to the observed words. Therefore, missing
</bodyText>
<figureCaption confidence="0.999139">
Figure 1: Matrix Factorization
</figureCaption>
<bodyText confidence="0.999423461538462">
words can be used to prune the hypotheses that are
also highly related to the missing words.
Consider the hypotheses of latent vectors in Ta-
ble 1 for the sentence of the WordNet definition
of bank#n#1. Assume there are 3 dimensions in
our latent model: financial, sport, institution. We
use Rvo to denote the sum of relatedness between
latent vector v and all observed words; similarly,
Rvm is the sum of relatedness between the vector
v and all missing words. Hypothesis v1 is given
by topic models, where only the financial sen-
tence is found, and it has the maximum relatedness
to observed words in bank#n#1 sentence Rv1
</bodyText>
<listItem confidence="0.952781">
o =20.
v2 is the ideal latent vector, since it also detects
that bank#n#1 is related to institution. It has a
slightly smaller Rve
o =18, but more importantly, re-
latedness to missing words Rvem=300 is substantially
smaller than Rv1m=600.
</listItem>
<bodyText confidence="0.9999795">
However, we cannot simply choose a hypothesis
with the maximum Ro − Rm value, since v3, which
is clearly not related to bank#n#1 but with a min-
imum Rm=100, will be our final answer. The so-
lution is straightforward: give a smaller weight to
missing words, e.g., so that the algorithm tries to
select a hypothesis with maximum value of Ro −
0.01 x Rm. To implement this idea, we model the
missing words in the weighted matrix factorization
framework [WMF] (Srebro and Jaakkola, 2003).
</bodyText>
<subsectionHeader confidence="0.9994215">
3.2 Modeling Missing Words by Weighted
Matrix Factorization
</subsectionHeader>
<bodyText confidence="0.999677416666667">
Given a corpus we represent the corpus as an
M x N matrix X. The row entries of the matrix
are the unique N words in the corpus, and the M
columns are the sentence ids of all the sentences.
The yielded N x M co-occurrence matrix X com-
prises the TF-IDF values in each Xij cell, namely
that TF-IDF value of word wi in sentence sj.
In WMF, the original matrix X is factorized into
two matrices such that X pz� PTQ, where P is a Kx
M matrix, and Q is a K x N matrix (Figure 1). In
this scenario, the latent semantics of each word wi or
sentence sj is represented as a K-dimension vector
</bodyText>
<page confidence="0.979797">
587
</page>
<table confidence="0.997804">
financial sport institution Ro Rm Ro − Rm Ro − 0.01Rm
v1 1 0 0 20 600 -580 14
v2 0.6 0 0.1 18 300 -282 15
v3 0.2 0.3 0.2 5 100 -95 4
</table>
<tableCaption confidence="0.999877">
Table 1: Three possible hypotheses of latent vectors for definition of bank#n#1
</tableCaption>
<bodyText confidence="0.9991125">
P·,i or Q·,j. Note that the inner product of P·,i and
Q·,j is used to approximate the semantic relatedness
of word wi and sentence sj: Xij Pz� P·,i · Q·,j, as the
shaded parts in Figure 1.
In WMF each cell is associated with a weight, so
missing words cells (Xij=0) can have a much less
contribution than observed words. Assume wm is
the weight for missing words cells. The latent vec-
tors of words P and sentences Q are estimated by
minimizing the objective function:
</bodyText>
<table confidence="0.8608654">
E E Wij (P·,i · Q·,j − Xij 2 + λ||P||22 + λ||Q||22 (1)
i j
J 1, if Xij # 0
where Wi,j = l
wm, if Xij = 0
</table>
<bodyText confidence="0.997302818181818">
Equation 1 explicitly requires the latent vector of
sentence Q·,j to be not related to missing words
(P·,i · Q·,j should be close to 0 for missing words
Xij = 0). Also weight wm for missing words is
very small to make sure latent vectors such as v3 in
Table 1 will not be chosen. In experiments we set
wm = 0.01. We refer to our approach as Weighted
Textual Matrix Factorization (WTMF).
After we run WTMF on the sentence corpus, the
similarity of the two sentences sj and sk can be com-
puted by the inner product of Q·,j and Q·,k.
</bodyText>
<subsectionHeader confidence="0.867556">
3.3 Inference
</subsectionHeader>
<bodyText confidence="0.9903964">
The latent vectors in P and Q are first randomly
initialized, then can be computed iteratively by the
following equations (derivation is omitted due to
limited space, but can be found in (Srebro and
Jaakkola, 2003)):
</bodyText>
<equation confidence="0.9925965">
( )−1
P·,i = Q W� (i)Q&gt; + λI Q W�(i)X&gt;i,·
( )−1 (2)
Q·,j = P W� (j)P&gt; + λI P W� (i)X·,j
</equation>
<bodyText confidence="0.999861857142857">
where W��i) = diag(W·,i) is an M x M diagonal
matrix containing ith row of weight matrix W. Sim-
ilarly, W0j) = diag(W·,j) is an N x N diagonal
matrix containing jth column of W.
Since most of the cells have the same value of 0,
the inference can be further optimized to save com-
putation, which has been described in (Steck, 2010).
</bodyText>
<sectionHeader confidence="0.993247" genericHeader="method">
4 Data Preprocessing
</sectionHeader>
<bodyText confidence="0.999955230769231">
The data sets for WTMF comprises two dictionar-
ies WordNet (Fellbaum, 1998), Wiktionary,2 and
the Brown corpus. We did not link the senses be-
tween WordNet and Wiktionary, therefore the defini-
tion sentences are simply treated as individual docu-
ments. We crawl Wiktionary and remove the entries
that are not tagged as noun, verb, adjective, or ad-
verb, resulting in 220,000 entries. For both WordNet
and Wiktionary, target words are added to the defini-
tion (e.g. the word bank is added into the definition
sentence of bank#n#1). Also usage examples are
appended to definition sentences (hence sentences
become short texts). For the Brown corpus, each
sentence is treated as a document in order to create
more co-occurrence. The importance of words in a
sentence is estimated by the TF-IDF schema.
All data is tokenized, pos-tagged3, and lemma-
tized4. To reduce word sparsity issue, we take
an additional preprocessing step: for each lemma-
tized word, we find all its possible lemmas, and
choose the most frequent lemma according to Word-
Net::QueryData. For example, the word thinkings is
first lemmatized as thinking, then we discover think-
ing has possible lemmas thinking and think, finally
we choose think as targeted lemma. The STS data is
also preprocessed using the same pipeline.
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.984268">
5.1 Setting
</subsectionHeader>
<bodyText confidence="0.99623">
STS data: The sentence pair data in the STS
task is collected from five sources: 1. MSR Para-
phrase corpus (Dolan et al., 2004), 2. MSR video
data (Chen and Dolan, 2011), 3. SMT europarl data,
</bodyText>
<footnote confidence="0.999966">
2http://en.wiktionary.org/wiki/Wiktionary:Main Page
3http://nlp.stanford.edu/software/tagger.shtml
4http://wn-similarity.sourceforge.net, WordNet::QueryData
</footnote>
<page confidence="0.99005">
588
</page>
<table confidence="0.997804">
models MSRpar MSRvid SMT-eur ON-WN SMT-news
LDA 0.274 0.7682 0.452 0.619 0.366
WTMF 0.411(67/89) 0.835(11/89) 0.513(10/89) 0.727(1/89) 0.438(28/89)
</table>
<tableCaption confidence="0.967884">
Table 2: Performance of LDA and WTMF on each individual test set of Task 6 STS data
</tableCaption>
<table confidence="0.965377">
ALL ALLnrm Mean
0.695(20/89) 0.830(10/89) 0.608(19/89)
</table>
<tableCaption confidence="0.999589">
Table 3: Performance of WTMF on all test sets
</tableCaption>
<bodyText confidence="0.94739052173913">
4. OntoNotes-WordNet data (Hovy et al., 2006), 5.
SMT news data.
Evaluation Metrics: Since the systems are re-
quired to assigned a similarity score to each sentence
pair, Pearson’s correlation is used to measure the
performance of systems on each of the 5 data sets.
However, measuring the overall performance on the
concatenation of 5 data sets is rarely discussed in
previous work. Accordingly the organizers of STS
task provide three evaluation metrics: 1. ALL: Pear-
son correlation with the gold standard for the com-
bined 5 data sets. 2. ALLnrm: Pearson correlation
after the system outputs for each data set are fitted
to the gold standard using least squares. 3. Mean:
Weighted mean across the 5 data sets, where the
weight depends on the number of pairs in the dataset.
WTMF Model: Our model is built on Word-
Net+Wiktionary+Brown+training data of STS. Each
sentence of STS test data is transformed into a latent
vector using Equation 2. Then sentence pair similar-
ity is computed by the cosine similarity of the two
latent vectors. We employ the parameters used in
(Guo and Diab, 2012b) (A = 20, wm = 0.01).
</bodyText>
<subsectionHeader confidence="0.893508">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.9999365">
Table 3 summarizes the overall performance of
WTMF on the concatenation of 5 data sets followed
by the corresponding rank among all participating
systems.5 There are 88 submitted results in total and
1 baseline which is simply the cosine similarity of
surface word vectors.
Table 2 compares the individual performance of
LDA (trained on the same corpus) and WTMF on
each data set. WTMF outperforms LDA by a large
margin. This is because LDA only uses 10 observed
words to infer a 100 dimension vector, while WTMF
takes advantage of much more missing words to
</bodyText>
<footnote confidence="0.9924475">
5http://www.cs.york.ac.uk/semeval-2012/
task6/index.php?id=results-update
</footnote>
<bodyText confidence="0.9990455">
learn more robust latent semantic vectors.
WTMF model achieves great overall perfor-
mance, with ranks 20, 10, 19 out of 89 reported re-
sults in three evaluation metrics respectively. It is
worth noting that WTMF is unsupervised in that it
does not use the training data similarity values, also
the only feature WTMF uses is bag-of-words fea-
tures without other information such as syntax, sen-
timent, etc. indicating that these additional features
could lead to even more improvement.
Observing the individual performance on each of
the 5 data set, we find WTMF ranks relatively high
in the four data sets: MSRvid (11/89), SMT-eur
(11/89), ON-WN (1/89), SMT-news (28/89). How-
ever, WTMF is outperformed by most of the systems
on MSRpar data set (67/89). We analyze the data set
and find that different from the other four data sets,
MSRpar is related to a lot of other NLP topics such
as textual entailment or sentiment coherence. There-
fore, our feature set (bag of words) is too shallow for
this data set indicating that using syntax and more
semantically oriented features could be helpful.
</bodyText>
<sectionHeader confidence="0.997754" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999988888888889">
We introduce a new latent variable model WTMF
that is competitive with high dimensional ap-
proaches to the STS task. In WTMF model, we ex-
plicitly model missing words to alleviate the sparsity
problem in modeling short texts. For future work,
we would like to combine our methods with existing
word similarity based approaches and add more nu-
anced features incorporating syntax and semantics
in the latent model.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997875">
This research was funded by the Office of the Di-
rector of National Intelligence (ODNI), Intelligence
Advanced Research Projects Activity (IARPA),
through the U.S. Army Research Lab. All state-
ments of fact, opinion or conclusions contained
herein are those of the authors and should not be
construed as representing the official views or poli-
cies of IARPA, the ODNI or the U.S. Government.
</bodyText>
<page confidence="0.998026">
589
</page>
<sectionHeader confidence="0.995444" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998309073684211">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot
on semantic textual similarity. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012).
Satanjeev Banerjee and Ted Pedersen. 2003. Extended
gloss overlaps as a measure of semantic relatedness.
In Proceedings of the 18th International Joint Confer-
ence on Artificial Intelligence, pages 805–810.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet allocation. Journal of Machine
Learning Research, 3.
David L. Chen and William B. Dolan. 2011. Collecting
highly parallel data for paraphrase evaluation. In Pro-
ceedings of the 49th Annual Meeting of the Association
for Computational Linguistics.
William Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources. In
Proceedings of the 20th International Conference on
Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Jin Feng, Yi-Ming Zhou, and Trevor Martin. 2008. Sen-
tence similarity based on relevance. In Proceedings of
IPMU.
Weiwei Guo and Mona Diab. 2012a. Learning the latent
semantics of a concept from its definition. In Proceed-
ings of the 50th Annual Meeting of the Association for
Computational Linguistics.
Weiwei Guo and Mona Diab. 2012b. Modeling sen-
tences in the latent space. In Proceedings of the 50th
Annual Meeting of the Association for Computational
Linguistics.
Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Ab-
dul Kadir, and Shyamala C. Doraisamy. 2010. Word
sense disambiguation-based sentence similarity. In
Proceedings of the 23rd International Conference on
Computational Linguistics.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the North American
Chapter of the ACL.
Aminul Islam and Diana Inkpen. 2008. Semantic
text similarity using corpus-based word similarity and
string similarity. ACM Transactions on Knowledge
Discovery from Data, 2.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In Proceedings of the
Human Language Technology Conference of the North
American Chapter of the ACL.
Thomas K Landauer, Peter W. Foltz, and Darrell Laham.
1998. An introduction to latent semantic analysis.
Discourse Processes, 25.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and representa-
tions. In Proceedings of the 19th International Joint
Conference on Artificial Intelligence.
Yuhua Li, Davi d McLean, Zuhair A. Bandar, James D. O
Shea, and Keeley Crockett. 2006. Sentence similar-
ity based on semantic nets and corpus statistics. IEEE
Transaction on Knowledge and Data Engineering, 18.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the 21st
National Conference on Articial Intelligence.
James O’Shea, Zuhair Bandar, Keeley Crockett, and
David McLean. 2008. A comparative study of two
short text semantic similarity measures. In Proceed-
ings of the Agent and Multi-Agent Systems: Technolo-
gies and Applications, Second KES International Sym-
posium (KES-AMSTA).
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing microblogs with topic models. In Pro-
ceedings of the Fourth International AAAI Conference
on Weblogs and Social Media.
Nathan Srebro and Tommi Jaakkola. 2003. Weighted
low-rank approximations. In Proceedings of the Twen-
tieth International Conference on Machine Learning.
Harald Steck. 2010. Training and testing of recom-
mender systems on data missing not at random. In
Proceedings of the 16th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing.
George Tsatsaronis, Iraklis Varlamis, and Michalis Vazir-
giannis. 2010. Text relatedness based on a word the-
saurus. Journal of Articial Intelligence Research, 37.
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,
and Eduard Hovy. 2006. Paraeval: Using paraphrases
to evaluate summaries automatically. In Proceedings
of Human Language Tech-nology Conference of the
North American Chapter of the ACL,.
</reference>
<page confidence="0.997322">
590
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.971303">
<title confidence="0.9922575">Weiwei: A Simple Unsupervised Latent Semantics based Approach Sentence Similarity</title>
<author confidence="0.999939">Weiwei Guo Mona Diab</author>
<affiliation confidence="0.9999855">Department of Computer Science, Center for Computational Learning Systems, Columbia University, Columbia University,</affiliation>
<email confidence="0.999878">weiwei@cs.columbia.edumdiab@ccls.columbia.edu</email>
<abstract confidence="0.998862333333333">The Semantic Textual Similarity (STS) shared task (Agirre et al., 2012) computes the degree of semantic equivalence between two sen- We show that a simple unsupervised latent semantics based approach, Weighted Textual Matrix Factorization that only exploits bag-of-words features, can outperform most systems for this task. The key to the approach is to carefully handle missing words that are not in the sentence, and thus rendering it superior to Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). Our system ranks 20 out of 89 systems according to the official evaluation metric for the task, Pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>Semeval-2012 task 6: A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. Semeval-2012 task 6: A pilot on semantic textual similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of the 18th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>805--810</pages>
<contexts>
<context position="1485" citStr="Banerjee and Pedersen, 2003" startWordPosition="213" endWordPosition="217">he official evaluation metric for the task, Pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is helpful for many NLP topics. In Machine Translation (Kauchak and Barzilay, 2006) and Text Summarization (Zhou et al., 2006), results are automatically evaluated based on sentence comparison. In Text Coherence Detection (Lapata and Barzilay, 2005), sentences are linked together by similar or related words. For Word Sense Disambiguation, researchers (Banerjee and Pedersen, 2003; Guo and Diab, 2012a) construct a sense similarity measure from the sentence similarity of the sense definitions. Almost all SS approaches decompose the task into word pairwise similarity problems. For example, Is1Mona Diab, co-author of this paper, is one of the task organizers lam and Inkpen (2008) create a matrix for each sentence pair, where columns are the words in the first sentence and rows are the words in the second sentence, and each cell stores the distributional similarity of the two words. Then they create an alignment between words in two sentences, and sentence similarity is ca</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, pages 805–810.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<title>Latent dirichlet allocation.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>3</volume>
<contexts>
<context position="2570" citStr="Blei et al., 2003" startWordPosition="391" endWordPosition="394">stributional similarity of the two words. Then they create an alignment between words in two sentences, and sentence similarity is calculated based on the sum of the similarity of aligned word pairs. There are two disadvantages with word similarity based approaches: 1. lexical ambiguity as the word pairwise similarity ignores the semantic interaction between the word and sentence/context. 2. word co-occurrence information is not as sufficiently exploited as they are in latent variable models such as Latent Semantic Analysis (LSA) (Landauer et al., 1998) and Latent Dirichilet Allocation (LDA) (Blei et al., 2003). On the other hand, latent variable models can solve the two issues naturally by modeling the semantics of words and sentences simultaneously in the low-dimensional latent space. However, attempts at addressing SS using LSA perform significantly below word similarity based models (Mihalcea et al., 2006; O’Shea et al., 2008). We believe the reason is that the observed words in a sentence are too few for latent variable models to learn robust semantics. For example, given the two sentences of WordNet sense definitions for bank#n#1 and stock#n#1: bank#n#1: a financial institution that accepts de</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003. Latent dirichlet allocation. Journal of Machine Learning Research, 3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David L Chen</author>
<author>William B Dolan</author>
</authors>
<title>Collecting highly parallel data for paraphrase evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11332" citStr="Chen and Dolan, 2011" startWordPosition="1938" endWordPosition="1941">uce word sparsity issue, we take an additional preprocessing step: for each lemmatized word, we find all its possible lemmas, and choose the most frequent lemma according to WordNet::QueryData. For example, the word thinkings is first lemmatized as thinking, then we discover thinking has possible lemmas thinking and think, finally we choose think as targeted lemma. The STS data is also preprocessed using the same pipeline. 5 Experiments 5.1 Setting STS data: The sentence pair data in the STS task is collected from five sources: 1. MSR Paraphrase corpus (Dolan et al., 2004), 2. MSR video data (Chen and Dolan, 2011), 3. SMT europarl data, 2http://en.wiktionary.org/wiki/Wiktionary:Main Page 3http://nlp.stanford.edu/software/tagger.shtml 4http://wn-similarity.sourceforge.net, WordNet::QueryData 588 models MSRpar MSRvid SMT-eur ON-WN SMT-news LDA 0.274 0.7682 0.452 0.619 0.366 WTMF 0.411(67/89) 0.835(11/89) 0.513(10/89) 0.727(1/89) 0.438(28/89) Table 2: Performance of LDA and WTMF on each individual test set of Task 6 STS data ALL ALLnrm Mean 0.695(20/89) 0.830(10/89) 0.608(19/89) Table 3: Performance of WTMF on all test sets 4. OntoNotes-WordNet data (Hovy et al., 2006), 5. SMT news data. Evaluation Metric</context>
</contexts>
<marker>Chen, Dolan, 2011</marker>
<rawString>David L. Chen and William B. Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="11290" citStr="Dolan et al., 2004" startWordPosition="1930" endWordPosition="1933">ed, pos-tagged3, and lemmatized4. To reduce word sparsity issue, we take an additional preprocessing step: for each lemmatized word, we find all its possible lemmas, and choose the most frequent lemma according to WordNet::QueryData. For example, the word thinkings is first lemmatized as thinking, then we discover thinking has possible lemmas thinking and think, finally we choose think as targeted lemma. The STS data is also preprocessed using the same pipeline. 5 Experiments 5.1 Setting STS data: The sentence pair data in the STS task is collected from five sources: 1. MSR Paraphrase corpus (Dolan et al., 2004), 2. MSR video data (Chen and Dolan, 2011), 3. SMT europarl data, 2http://en.wiktionary.org/wiki/Wiktionary:Main Page 3http://nlp.stanford.edu/software/tagger.shtml 4http://wn-similarity.sourceforge.net, WordNet::QueryData 588 models MSRpar MSRvid SMT-eur ON-WN SMT-news LDA 0.274 0.7682 0.452 0.619 0.366 WTMF 0.411(67/89) 0.835(11/89) 0.513(10/89) 0.727(1/89) 0.438(28/89) Table 2: Performance of LDA and WTMF on each individual test set of Task 6 STS data ALL ALLnrm Mean 0.695(20/89) 0.830(10/89) 0.608(19/89) Table 3: Performance of WTMF on all test sets 4. OntoNotes-WordNet data (Hovy et al., </context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>William Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of the 20th International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="9933" citStr="Fellbaum, 1998" startWordPosition="1708" endWordPosition="1709">ations (derivation is omitted due to limited space, but can be found in (Srebro and Jaakkola, 2003)): ( )−1 P·,i = Q W� (i)Q&gt; + λI Q W�(i)X&gt;i,· ( )−1 (2) Q·,j = P W� (j)P&gt; + λI P W� (i)X·,j where W��i) = diag(W·,i) is an M x M diagonal matrix containing ith row of weight matrix W. Similarly, W0j) = diag(W·,j) is an N x N diagonal matrix containing jth column of W. Since most of the cells have the same value of 0, the inference can be further optimized to save computation, which has been described in (Steck, 2010). 4 Data Preprocessing The data sets for WTMF comprises two dictionaries WordNet (Fellbaum, 1998), Wiktionary,2 and the Brown corpus. We did not link the senses between WordNet and Wiktionary, therefore the definition sentences are simply treated as individual documents. We crawl Wiktionary and remove the entries that are not tagged as noun, verb, adjective, or adverb, resulting in 220,000 entries. For both WordNet and Wiktionary, target words are added to the definition (e.g. the word bank is added into the definition sentence of bank#n#1). Also usage examples are appended to definition sentences (hence sentences become short texts). For the Brown corpus, each sentence is treated as a do</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin Feng</author>
<author>Yi-Ming Zhou</author>
<author>Trevor Martin</author>
</authors>
<title>Sentence similarity based on relevance.</title>
<date>2008</date>
<booktitle>In Proceedings of IPMU.</booktitle>
<contexts>
<context position="5063" citStr="Feng et al., 2008" startWordPosition="803" endWordPosition="806">milar to LSA/LDA) in that it can be applied to any genre of short texts, in a manner different from existing work that models short texts by using additional data, e.g., Ramage et al. (2010) model tweets using their metadata (author, hashtag, etc). Also we do not extract additional features such as multiwords expression or syntax from sentences – all we use is bag-of-words feature. 2 Related Work Almost all current SS methods work in the highdimensional word space, and rely heavily on word/sense similarity measures. The word/sense similarity measure is either knowledge based (Li et al., 2006; Feng et al., 2008; Ho et al., 2010; Tsatsaronis et al., 2010), corpus-based (Islam and Inkpen, 2008) or hybrid (Mihalcea et al., 2006). Almost all of them are evaluated on a data set introduced in (Li et al., 2006). The LI06 data set consists of 65 pairs of noun definitions selected from the Collin Cobuild Dictionary. A subset of 30 pairs is further selected by LI06 to render the similarity scores evenly distributed. Our approach has outperformed most of the previous methods on LI06 achieving the second best Pearson’s correlation and the best Spearman correlation (Guo and Diab, 2012b). 3 Learning Latent Semant</context>
</contexts>
<marker>Feng, Zhou, Martin, 2008</marker>
<rawString>Jin Feng, Yi-Ming Zhou, and Trevor Martin. 2008. Sentence similarity based on relevance. In Proceedings of IPMU.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Learning the latent semantics of a concept from its definition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1505" citStr="Guo and Diab, 2012" startWordPosition="218" endWordPosition="221"> for the task, Pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is helpful for many NLP topics. In Machine Translation (Kauchak and Barzilay, 2006) and Text Summarization (Zhou et al., 2006), results are automatically evaluated based on sentence comparison. In Text Coherence Detection (Lapata and Barzilay, 2005), sentences are linked together by similar or related words. For Word Sense Disambiguation, researchers (Banerjee and Pedersen, 2003; Guo and Diab, 2012a) construct a sense similarity measure from the sentence similarity of the sense definitions. Almost all SS approaches decompose the task into word pairwise similarity problems. For example, Is1Mona Diab, co-author of this paper, is one of the task organizers lam and Inkpen (2008) create a matrix for each sentence pair, where columns are the words in the first sentence and rows are the words in the second sentence, and each cell stores the distributional similarity of the two words. Then they create an alignment between words in two sentences, and sentence similarity is calculated based on th</context>
<context position="3795" citStr="Guo and Diab, 2012" startWordPosition="583" endWordPosition="586">and channels the money into lending activities stock#n#1: the capital raised by a corporation through the issue of shares entitling holders to an ownership interest (equity) LDA can only find the dominant topic (the financial topic) based on the observed words without further discernibility. In this case, many sen586 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 586–590, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics tences will share the same latent semantics profile, as long as they are in the same topic/domain. In our work (Guo and Diab, 2012b), we propose to model the missing words (words that are not in the sentence) to address the sparseness issue for the SS task. Our intuition is since observed words in a sentence are too few to tell us what the sentence is about, missing words can be used to tell us what the sentence is not about. We assume that the semantic space of both the observed and missing words make up the complete semantic profile of a sentence. We implement our idea using a weighted matrix factorization approach (Srebro and Jaakkola, 2003), which allows us to treat observed words and missing words differently. It sh</context>
<context position="5635" citStr="Guo and Diab, 2012" startWordPosition="901" endWordPosition="904">edge based (Li et al., 2006; Feng et al., 2008; Ho et al., 2010; Tsatsaronis et al., 2010), corpus-based (Islam and Inkpen, 2008) or hybrid (Mihalcea et al., 2006). Almost all of them are evaluated on a data set introduced in (Li et al., 2006). The LI06 data set consists of 65 pairs of noun definitions selected from the Collin Cobuild Dictionary. A subset of 30 pairs is further selected by LI06 to render the similarity scores evenly distributed. Our approach has outperformed most of the previous methods on LI06 achieving the second best Pearson’s correlation and the best Spearman correlation (Guo and Diab, 2012b). 3 Learning Latent Semantics of Sentences 3.1 Intuition Given only a few observed words in a sentence, there are many hypotheses of latent vectors that are highly related to the observed words. Therefore, missing Figure 1: Matrix Factorization words can be used to prune the hypotheses that are also highly related to the missing words. Consider the hypotheses of latent vectors in Table 1 for the sentence of the WordNet definition of bank#n#1. Assume there are 3 dimensions in our latent model: financial, sport, institution. We use Rvo to denote the sum of relatedness between latent vector v a</context>
<context position="12937" citStr="Guo and Diab, 2012" startWordPosition="2182" endWordPosition="2185">on correlation with the gold standard for the combined 5 data sets. 2. ALLnrm: Pearson correlation after the system outputs for each data set are fitted to the gold standard using least squares. 3. Mean: Weighted mean across the 5 data sets, where the weight depends on the number of pairs in the dataset. WTMF Model: Our model is built on WordNet+Wiktionary+Brown+training data of STS. Each sentence of STS test data is transformed into a latent vector using Equation 2. Then sentence pair similarity is computed by the cosine similarity of the two latent vectors. We employ the parameters used in (Guo and Diab, 2012b) (A = 20, wm = 0.01). 5.2 Results Table 3 summarizes the overall performance of WTMF on the concatenation of 5 data sets followed by the corresponding rank among all participating systems.5 There are 88 submitted results in total and 1 baseline which is simply the cosine similarity of surface word vectors. Table 2 compares the individual performance of LDA (trained on the same corpus) and WTMF on each data set. WTMF outperforms LDA by a large margin. This is because LDA only uses 10 observed words to infer a 100 dimension vector, while WTMF takes advantage of much more missing words to 5http</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012a. Learning the latent semantics of a concept from its definition. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1505" citStr="Guo and Diab, 2012" startWordPosition="218" endWordPosition="221"> for the task, Pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is helpful for many NLP topics. In Machine Translation (Kauchak and Barzilay, 2006) and Text Summarization (Zhou et al., 2006), results are automatically evaluated based on sentence comparison. In Text Coherence Detection (Lapata and Barzilay, 2005), sentences are linked together by similar or related words. For Word Sense Disambiguation, researchers (Banerjee and Pedersen, 2003; Guo and Diab, 2012a) construct a sense similarity measure from the sentence similarity of the sense definitions. Almost all SS approaches decompose the task into word pairwise similarity problems. For example, Is1Mona Diab, co-author of this paper, is one of the task organizers lam and Inkpen (2008) create a matrix for each sentence pair, where columns are the words in the first sentence and rows are the words in the second sentence, and each cell stores the distributional similarity of the two words. Then they create an alignment between words in two sentences, and sentence similarity is calculated based on th</context>
<context position="3795" citStr="Guo and Diab, 2012" startWordPosition="583" endWordPosition="586">and channels the money into lending activities stock#n#1: the capital raised by a corporation through the issue of shares entitling holders to an ownership interest (equity) LDA can only find the dominant topic (the financial topic) based on the observed words without further discernibility. In this case, many sen586 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 586–590, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics tences will share the same latent semantics profile, as long as they are in the same topic/domain. In our work (Guo and Diab, 2012b), we propose to model the missing words (words that are not in the sentence) to address the sparseness issue for the SS task. Our intuition is since observed words in a sentence are too few to tell us what the sentence is about, missing words can be used to tell us what the sentence is not about. We assume that the semantic space of both the observed and missing words make up the complete semantic profile of a sentence. We implement our idea using a weighted matrix factorization approach (Srebro and Jaakkola, 2003), which allows us to treat observed words and missing words differently. It sh</context>
<context position="5635" citStr="Guo and Diab, 2012" startWordPosition="901" endWordPosition="904">edge based (Li et al., 2006; Feng et al., 2008; Ho et al., 2010; Tsatsaronis et al., 2010), corpus-based (Islam and Inkpen, 2008) or hybrid (Mihalcea et al., 2006). Almost all of them are evaluated on a data set introduced in (Li et al., 2006). The LI06 data set consists of 65 pairs of noun definitions selected from the Collin Cobuild Dictionary. A subset of 30 pairs is further selected by LI06 to render the similarity scores evenly distributed. Our approach has outperformed most of the previous methods on LI06 achieving the second best Pearson’s correlation and the best Spearman correlation (Guo and Diab, 2012b). 3 Learning Latent Semantics of Sentences 3.1 Intuition Given only a few observed words in a sentence, there are many hypotheses of latent vectors that are highly related to the observed words. Therefore, missing Figure 1: Matrix Factorization words can be used to prune the hypotheses that are also highly related to the missing words. Consider the hypotheses of latent vectors in Table 1 for the sentence of the WordNet definition of bank#n#1. Assume there are 3 dimensions in our latent model: financial, sport, institution. We use Rvo to denote the sum of relatedness between latent vector v a</context>
<context position="12937" citStr="Guo and Diab, 2012" startWordPosition="2182" endWordPosition="2185">on correlation with the gold standard for the combined 5 data sets. 2. ALLnrm: Pearson correlation after the system outputs for each data set are fitted to the gold standard using least squares. 3. Mean: Weighted mean across the 5 data sets, where the weight depends on the number of pairs in the dataset. WTMF Model: Our model is built on WordNet+Wiktionary+Brown+training data of STS. Each sentence of STS test data is transformed into a latent vector using Equation 2. Then sentence pair similarity is computed by the cosine similarity of the two latent vectors. We employ the parameters used in (Guo and Diab, 2012b) (A = 20, wm = 0.01). 5.2 Results Table 3 summarizes the overall performance of WTMF on the concatenation of 5 data sets followed by the corresponding rank among all participating systems.5 There are 88 submitted results in total and 1 baseline which is simply the cosine similarity of surface word vectors. Table 2 compares the individual performance of LDA (trained on the same corpus) and WTMF on each data set. WTMF outperforms LDA by a large margin. This is because LDA only uses 10 observed words to infer a 100 dimension vector, while WTMF takes advantage of much more missing words to 5http</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Weiwei Guo and Mona Diab. 2012b. Modeling sentences in the latent space. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chukfong Ho</author>
</authors>
<title>Masrah Azrifah Azmi Murad, Rabiah Abdul Kadir, and Shyamala</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics.</booktitle>
<marker>Ho, 2010</marker>
<rawString>Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Abdul Kadir, and Shyamala C. Doraisamy. 2010. Word sense disambiguation-based sentence similarity. In Proceedings of the 23rd International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL.</booktitle>
<contexts>
<context position="11895" citStr="Hovy et al., 2006" startWordPosition="2005" endWordPosition="2008">et al., 2004), 2. MSR video data (Chen and Dolan, 2011), 3. SMT europarl data, 2http://en.wiktionary.org/wiki/Wiktionary:Main Page 3http://nlp.stanford.edu/software/tagger.shtml 4http://wn-similarity.sourceforge.net, WordNet::QueryData 588 models MSRpar MSRvid SMT-eur ON-WN SMT-news LDA 0.274 0.7682 0.452 0.619 0.366 WTMF 0.411(67/89) 0.835(11/89) 0.513(10/89) 0.727(1/89) 0.438(28/89) Table 2: Performance of LDA and WTMF on each individual test set of Task 6 STS data ALL ALLnrm Mean 0.695(20/89) 0.830(10/89) 0.608(19/89) Table 3: Performance of WTMF on all test sets 4. OntoNotes-WordNet data (Hovy et al., 2006), 5. SMT news data. Evaluation Metrics: Since the systems are required to assigned a similarity score to each sentence pair, Pearson’s correlation is used to measure the performance of systems on each of the 5 data sets. However, measuring the overall performance on the concatenation of 5 data sets is rarely discussed in previous work. Accordingly the organizers of STS task provide three evaluation metrics: 1. ALL: Pearson correlation with the gold standard for the combined 5 data sets. 2. ALLnrm: Pearson correlation after the system outputs for each data set are fitted to the gold standard us</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aminul Islam</author>
<author>Diana Inkpen</author>
</authors>
<title>Semantic text similarity using corpus-based word similarity and string similarity.</title>
<date>2008</date>
<journal>ACM Transactions on Knowledge Discovery from Data,</journal>
<volume>2</volume>
<contexts>
<context position="5146" citStr="Islam and Inkpen, 2008" startWordPosition="817" endWordPosition="820"> manner different from existing work that models short texts by using additional data, e.g., Ramage et al. (2010) model tweets using their metadata (author, hashtag, etc). Also we do not extract additional features such as multiwords expression or syntax from sentences – all we use is bag-of-words feature. 2 Related Work Almost all current SS methods work in the highdimensional word space, and rely heavily on word/sense similarity measures. The word/sense similarity measure is either knowledge based (Li et al., 2006; Feng et al., 2008; Ho et al., 2010; Tsatsaronis et al., 2010), corpus-based (Islam and Inkpen, 2008) or hybrid (Mihalcea et al., 2006). Almost all of them are evaluated on a data set introduced in (Li et al., 2006). The LI06 data set consists of 65 pairs of noun definitions selected from the Collin Cobuild Dictionary. A subset of 30 pairs is further selected by LI06 to render the similarity scores evenly distributed. Our approach has outperformed most of the previous methods on LI06 achieving the second best Pearson’s correlation and the best Spearman correlation (Guo and Diab, 2012b). 3 Learning Latent Semantics of Sentences 3.1 Intuition Given only a few observed words in a sentence, there</context>
</contexts>
<marker>Islam, Inkpen, 2008</marker>
<rawString>Aminul Islam and Diana Inkpen. 2008. Semantic text similarity using corpus-based word similarity and string similarity. ACM Transactions on Knowledge Discovery from Data, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL.</booktitle>
<contexts>
<context position="1187" citStr="Kauchak and Barzilay, 2006" startWordPosition="170" endWordPosition="174">atures, can outperform most systems for this task. The key to the approach is to carefully handle missing words that are not in the sentence, and thus rendering it superior to Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). Our system ranks 20 out of 89 systems according to the official evaluation metric for the task, Pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is helpful for many NLP topics. In Machine Translation (Kauchak and Barzilay, 2006) and Text Summarization (Zhou et al., 2006), results are automatically evaluated based on sentence comparison. In Text Coherence Detection (Lapata and Barzilay, 2005), sentences are linked together by similar or related words. For Word Sense Disambiguation, researchers (Banerjee and Pedersen, 2003; Guo and Diab, 2012a) construct a sense similarity measure from the sentence similarity of the sense definitions. Almost all SS approaches decompose the task into word pairwise similarity problems. For example, Is1Mona Diab, co-author of this paper, is one of the task organizers lam and Inkpen (2008)</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<volume>25</volume>
<contexts>
<context position="2511" citStr="Landauer et al., 1998" startWordPosition="382" endWordPosition="385">e the words in the second sentence, and each cell stores the distributional similarity of the two words. Then they create an alignment between words in two sentences, and sentence similarity is calculated based on the sum of the similarity of aligned word pairs. There are two disadvantages with word similarity based approaches: 1. lexical ambiguity as the word pairwise similarity ignores the semantic interaction between the word and sentence/context. 2. word co-occurrence information is not as sufficiently exploited as they are in latent variable models such as Latent Semantic Analysis (LSA) (Landauer et al., 1998) and Latent Dirichilet Allocation (LDA) (Blei et al., 2003). On the other hand, latent variable models can solve the two issues naturally by modeling the semantics of words and sentences simultaneously in the low-dimensional latent space. However, attempts at addressing SS using LSA perform significantly below word similarity based models (Mihalcea et al., 2006; O’Shea et al., 2008). We believe the reason is that the observed words in a sentence are too few for latent variable models to learn robust semantics. For example, given the two sentences of WordNet sense definitions for bank#n#1 and s</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K Landauer, Peter W. Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis. Discourse Processes, 25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatic evaluation of text coherence: Models and representations.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="1353" citStr="Lapata and Barzilay, 2005" startWordPosition="194" endWordPosition="197">erior to Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). Our system ranks 20 out of 89 systems according to the official evaluation metric for the task, Pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is helpful for many NLP topics. In Machine Translation (Kauchak and Barzilay, 2006) and Text Summarization (Zhou et al., 2006), results are automatically evaluated based on sentence comparison. In Text Coherence Detection (Lapata and Barzilay, 2005), sentences are linked together by similar or related words. For Word Sense Disambiguation, researchers (Banerjee and Pedersen, 2003; Guo and Diab, 2012a) construct a sense similarity measure from the sentence similarity of the sense definitions. Almost all SS approaches decompose the task into word pairwise similarity problems. For example, Is1Mona Diab, co-author of this paper, is one of the task organizers lam and Inkpen (2008) create a matrix for each sentence pair, where columns are the words in the first sentence and rows are the words in the second sentence, and each cell stores the dis</context>
</contexts>
<marker>Lapata, Barzilay, 2005</marker>
<rawString>Mirella Lapata and Regina Barzilay. 2005. Automatic evaluation of text coherence: Models and representations. In Proceedings of the 19th International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuhua Li</author>
<author>Davi d McLean</author>
<author>Zuhair A Bandar</author>
<author>James D O Shea</author>
<author>Keeley Crockett</author>
</authors>
<title>Sentence similarity based on semantic nets and corpus statistics.</title>
<date>2006</date>
<journal>IEEE Transaction on Knowledge and Data Engineering,</journal>
<volume>18</volume>
<contexts>
<context position="5044" citStr="Li et al., 2006" startWordPosition="799" endWordPosition="802"> very general (similar to LSA/LDA) in that it can be applied to any genre of short texts, in a manner different from existing work that models short texts by using additional data, e.g., Ramage et al. (2010) model tweets using their metadata (author, hashtag, etc). Also we do not extract additional features such as multiwords expression or syntax from sentences – all we use is bag-of-words feature. 2 Related Work Almost all current SS methods work in the highdimensional word space, and rely heavily on word/sense similarity measures. The word/sense similarity measure is either knowledge based (Li et al., 2006; Feng et al., 2008; Ho et al., 2010; Tsatsaronis et al., 2010), corpus-based (Islam and Inkpen, 2008) or hybrid (Mihalcea et al., 2006). Almost all of them are evaluated on a data set introduced in (Li et al., 2006). The LI06 data set consists of 65 pairs of noun definitions selected from the Collin Cobuild Dictionary. A subset of 30 pairs is further selected by LI06 to render the similarity scores evenly distributed. Our approach has outperformed most of the previous methods on LI06 achieving the second best Pearson’s correlation and the best Spearman correlation (Guo and Diab, 2012b). 3 Lea</context>
</contexts>
<marker>Li, McLean, Bandar, Shea, Crockett, 2006</marker>
<rawString>Yuhua Li, Davi d McLean, Zuhair A. Bandar, James D. O Shea, and Keeley Crockett. 2006. Sentence similarity based on semantic nets and corpus statistics. IEEE Transaction on Knowledge and Data Engineering, 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Articial Intelligence.</booktitle>
<contexts>
<context position="2874" citStr="Mihalcea et al., 2006" startWordPosition="437" endWordPosition="440">airwise similarity ignores the semantic interaction between the word and sentence/context. 2. word co-occurrence information is not as sufficiently exploited as they are in latent variable models such as Latent Semantic Analysis (LSA) (Landauer et al., 1998) and Latent Dirichilet Allocation (LDA) (Blei et al., 2003). On the other hand, latent variable models can solve the two issues naturally by modeling the semantics of words and sentences simultaneously in the low-dimensional latent space. However, attempts at addressing SS using LSA perform significantly below word similarity based models (Mihalcea et al., 2006; O’Shea et al., 2008). We believe the reason is that the observed words in a sentence are too few for latent variable models to learn robust semantics. For example, given the two sentences of WordNet sense definitions for bank#n#1 and stock#n#1: bank#n#1: a financial institution that accepts deposits and channels the money into lending activities stock#n#1: the capital raised by a corporation through the issue of shares entitling holders to an ownership interest (equity) LDA can only find the dominant topic (the financial topic) based on the observed words without further discernibility. In t</context>
<context position="5180" citStr="Mihalcea et al., 2006" startWordPosition="823" endWordPosition="826">k that models short texts by using additional data, e.g., Ramage et al. (2010) model tweets using their metadata (author, hashtag, etc). Also we do not extract additional features such as multiwords expression or syntax from sentences – all we use is bag-of-words feature. 2 Related Work Almost all current SS methods work in the highdimensional word space, and rely heavily on word/sense similarity measures. The word/sense similarity measure is either knowledge based (Li et al., 2006; Feng et al., 2008; Ho et al., 2010; Tsatsaronis et al., 2010), corpus-based (Islam and Inkpen, 2008) or hybrid (Mihalcea et al., 2006). Almost all of them are evaluated on a data set introduced in (Li et al., 2006). The LI06 data set consists of 65 pairs of noun definitions selected from the Collin Cobuild Dictionary. A subset of 30 pairs is further selected by LI06 to render the similarity scores evenly distributed. Our approach has outperformed most of the previous methods on LI06 achieving the second best Pearson’s correlation and the best Spearman correlation (Guo and Diab, 2012b). 3 Learning Latent Semantics of Sentences 3.1 Intuition Given only a few observed words in a sentence, there are many hypotheses of latent vec</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the 21st National Conference on Articial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James O’Shea</author>
<author>Zuhair Bandar</author>
<author>Keeley Crockett</author>
<author>David McLean</author>
</authors>
<title>A comparative study of two short text semantic similarity measures.</title>
<date>2008</date>
<booktitle>In Proceedings of the Agent and Multi-Agent Systems: Technologies and Applications, Second KES International Symposium (KES-AMSTA).</booktitle>
<marker>O’Shea, Bandar, Crockett, McLean, 2008</marker>
<rawString>James O’Shea, Zuhair Bandar, Keeley Crockett, and David McLean. 2008. A comparative study of two short text semantic similarity measures. In Proceedings of the Agent and Multi-Agent Systems: Technologies and Applications, Second KES International Symposium (KES-AMSTA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ramage</author>
<author>Susan Dumais</author>
<author>Dan Liebling</author>
</authors>
<title>Characterizing microblogs with topic models.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media.</booktitle>
<contexts>
<context position="4636" citStr="Ramage et al. (2010)" startWordPosition="735" endWordPosition="738">is about, missing words can be used to tell us what the sentence is not about. We assume that the semantic space of both the observed and missing words make up the complete semantic profile of a sentence. We implement our idea using a weighted matrix factorization approach (Srebro and Jaakkola, 2003), which allows us to treat observed words and missing words differently. It should be noted that our approach is very general (similar to LSA/LDA) in that it can be applied to any genre of short texts, in a manner different from existing work that models short texts by using additional data, e.g., Ramage et al. (2010) model tweets using their metadata (author, hashtag, etc). Also we do not extract additional features such as multiwords expression or syntax from sentences – all we use is bag-of-words feature. 2 Related Work Almost all current SS methods work in the highdimensional word space, and rely heavily on word/sense similarity measures. The word/sense similarity measure is either knowledge based (Li et al., 2006; Feng et al., 2008; Ho et al., 2010; Tsatsaronis et al., 2010), corpus-based (Islam and Inkpen, 2008) or hybrid (Mihalcea et al., 2006). Almost all of them are evaluated on a data set introdu</context>
</contexts>
<marker>Ramage, Dumais, Liebling, 2010</marker>
<rawString>Daniel Ramage, Susan Dumais, and Dan Liebling. 2010. Characterizing microblogs with topic models. In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathan Srebro</author>
<author>Tommi Jaakkola</author>
</authors>
<title>Weighted low-rank approximations.</title>
<date>2003</date>
<booktitle>In Proceedings of the Twentieth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="4317" citStr="Srebro and Jaakkola, 2003" startWordPosition="677" endWordPosition="680"> latent semantics profile, as long as they are in the same topic/domain. In our work (Guo and Diab, 2012b), we propose to model the missing words (words that are not in the sentence) to address the sparseness issue for the SS task. Our intuition is since observed words in a sentence are too few to tell us what the sentence is about, missing words can be used to tell us what the sentence is not about. We assume that the semantic space of both the observed and missing words make up the complete semantic profile of a sentence. We implement our idea using a weighted matrix factorization approach (Srebro and Jaakkola, 2003), which allows us to treat observed words and missing words differently. It should be noted that our approach is very general (similar to LSA/LDA) in that it can be applied to any genre of short texts, in a manner different from existing work that models short texts by using additional data, e.g., Ramage et al. (2010) model tweets using their metadata (author, hashtag, etc). Also we do not extract additional features such as multiwords expression or syntax from sentences – all we use is bag-of-words feature. 2 Related Work Almost all current SS methods work in the highdimensional word space, a</context>
<context position="7224" citStr="Srebro and Jaakkola, 2003" startWordPosition="1172" endWordPosition="1175">tution. It has a slightly smaller Rve o =18, but more importantly, relatedness to missing words Rvem=300 is substantially smaller than Rv1m=600. However, we cannot simply choose a hypothesis with the maximum Ro − Rm value, since v3, which is clearly not related to bank#n#1 but with a minimum Rm=100, will be our final answer. The solution is straightforward: give a smaller weight to missing words, e.g., so that the algorithm tries to select a hypothesis with maximum value of Ro − 0.01 x Rm. To implement this idea, we model the missing words in the weighted matrix factorization framework [WMF] (Srebro and Jaakkola, 2003). 3.2 Modeling Missing Words by Weighted Matrix Factorization Given a corpus we represent the corpus as an M x N matrix X. The row entries of the matrix are the unique N words in the corpus, and the M columns are the sentence ids of all the sentences. The yielded N x M co-occurrence matrix X comprises the TF-IDF values in each Xij cell, namely that TF-IDF value of word wi in sentence sj. In WMF, the original matrix X is factorized into two matrices such that X pz� PTQ, where P is a Kx M matrix, and Q is a K x N matrix (Figure 1). In this scenario, the latent semantics of each word wi or senten</context>
<context position="9417" citStr="Srebro and Jaakkola, 2003" startWordPosition="1604" endWordPosition="1607"> 0 for missing words Xij = 0). Also weight wm for missing words is very small to make sure latent vectors such as v3 in Table 1 will not be chosen. In experiments we set wm = 0.01. We refer to our approach as Weighted Textual Matrix Factorization (WTMF). After we run WTMF on the sentence corpus, the similarity of the two sentences sj and sk can be computed by the inner product of Q·,j and Q·,k. 3.3 Inference The latent vectors in P and Q are first randomly initialized, then can be computed iteratively by the following equations (derivation is omitted due to limited space, but can be found in (Srebro and Jaakkola, 2003)): ( )−1 P·,i = Q W� (i)Q&gt; + λI Q W�(i)X&gt;i,· ( )−1 (2) Q·,j = P W� (j)P&gt; + λI P W� (i)X·,j where W��i) = diag(W·,i) is an M x M diagonal matrix containing ith row of weight matrix W. Similarly, W0j) = diag(W·,j) is an N x N diagonal matrix containing jth column of W. Since most of the cells have the same value of 0, the inference can be further optimized to save computation, which has been described in (Steck, 2010). 4 Data Preprocessing The data sets for WTMF comprises two dictionaries WordNet (Fellbaum, 1998), Wiktionary,2 and the Brown corpus. We did not link the senses between WordNet and </context>
</contexts>
<marker>Srebro, Jaakkola, 2003</marker>
<rawString>Nathan Srebro and Tommi Jaakkola. 2003. Weighted low-rank approximations. In Proceedings of the Twentieth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Steck</author>
</authors>
<title>Training and testing of recommender systems on data missing not at random.</title>
<date>2010</date>
<booktitle>In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="9836" citStr="Steck, 2010" startWordPosition="1693" endWordPosition="1694"> P and Q are first randomly initialized, then can be computed iteratively by the following equations (derivation is omitted due to limited space, but can be found in (Srebro and Jaakkola, 2003)): ( )−1 P·,i = Q W� (i)Q&gt; + λI Q W�(i)X&gt;i,· ( )−1 (2) Q·,j = P W� (j)P&gt; + λI P W� (i)X·,j where W��i) = diag(W·,i) is an M x M diagonal matrix containing ith row of weight matrix W. Similarly, W0j) = diag(W·,j) is an N x N diagonal matrix containing jth column of W. Since most of the cells have the same value of 0, the inference can be further optimized to save computation, which has been described in (Steck, 2010). 4 Data Preprocessing The data sets for WTMF comprises two dictionaries WordNet (Fellbaum, 1998), Wiktionary,2 and the Brown corpus. We did not link the senses between WordNet and Wiktionary, therefore the definition sentences are simply treated as individual documents. We crawl Wiktionary and remove the entries that are not tagged as noun, verb, adjective, or adverb, resulting in 220,000 entries. For both WordNet and Wiktionary, target words are added to the definition (e.g. the word bank is added into the definition sentence of bank#n#1). Also usage examples are appended to definition sente</context>
</contexts>
<marker>Steck, 2010</marker>
<rawString>Harald Steck. 2010. Training and testing of recommender systems on data missing not at random. In Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Tsatsaronis</author>
</authors>
<title>Iraklis Varlamis, and Michalis Vazirgiannis.</title>
<date>2010</date>
<journal>Journal of Articial Intelligence Research,</journal>
<volume>37</volume>
<marker>Tsatsaronis, 2010</marker>
<rawString>George Tsatsaronis, Iraklis Varlamis, and Michalis Vazirgiannis. 2010. Text relatedness based on a word thesaurus. Journal of Articial Intelligence Research, 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Chin-Yew Lin</author>
<author>Dragos Stefan Munteanu</author>
<author>Eduard Hovy</author>
</authors>
<title>Paraeval: Using paraphrases to evaluate summaries automatically.</title>
<date>2006</date>
<booktitle>In Proceedings of Human Language Tech-nology Conference of the North American Chapter of the ACL,.</booktitle>
<contexts>
<context position="1230" citStr="Zhou et al., 2006" startWordPosition="178" endWordPosition="181">he key to the approach is to carefully handle missing words that are not in the sentence, and thus rendering it superior to Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). Our system ranks 20 out of 89 systems according to the official evaluation metric for the task, Pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is helpful for many NLP topics. In Machine Translation (Kauchak and Barzilay, 2006) and Text Summarization (Zhou et al., 2006), results are automatically evaluated based on sentence comparison. In Text Coherence Detection (Lapata and Barzilay, 2005), sentences are linked together by similar or related words. For Word Sense Disambiguation, researchers (Banerjee and Pedersen, 2003; Guo and Diab, 2012a) construct a sense similarity measure from the sentence similarity of the sense definitions. Almost all SS approaches decompose the task into word pairwise similarity problems. For example, Is1Mona Diab, co-author of this paper, is one of the task organizers lam and Inkpen (2008) create a matrix for each sentence pair, wh</context>
</contexts>
<marker>Zhou, Lin, Munteanu, Hovy, 2006</marker>
<rawString>Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy. 2006. Paraeval: Using paraphrases to evaluate summaries automatically. In Proceedings of Human Language Tech-nology Conference of the North American Chapter of the ACL,.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>