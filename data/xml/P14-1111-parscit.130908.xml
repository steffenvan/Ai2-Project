<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.985331">
Low-Resource Semantic Role Labeling
</title>
<author confidence="0.998854">
Matthew R. Gormley&apos; Margaret Mitchell&apos; Benjamin Van Durme&apos; Mark Dredze&apos;
</author>
<affiliation confidence="0.919673666666667">
&apos;Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, MD 21211
&apos;Microsoft Research
</affiliation>
<address confidence="0.985549">
Redmond, WA 98052
</address>
<email confidence="0.999557">
mrg@cs.jhu.eduImemitc@microsoft.comIvandurme@cs.jhu.eduImdredze@cs.jhu.edu
</email>
<sectionHeader confidence="0.9974" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999000333333333">
We explore the extent to which high-
resource manual annotations such as tree-
banks are necessary for the task of se-
mantic role labeling (SRL). We examine
how performance changes without syntac-
tic supervision, comparing both joint and
pipelined methods to induce latent syn-
tax. This work highlights a new applica-
tion of unsupervised grammar induction
and demonstrates several approaches to
SRL in the absence of supervised syntax.
Our best models obtain competitive results
in the high-resource setting and state-of-
the-art results in the low resource setting,
reaching 72.48% F1 averaged across lan-
guages. We release our code for this work
along with a larger toolkit for specifying
arbitrary graphical structure.1
</bodyText>
<sectionHeader confidence="0.99952" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999818722222222">
The goal of semantic role labeling (SRL) is to
identify predicates and arguments and label their
semantic contribution in a sentence. Such labeling
defines who did what to whom, when, where and
how. For example, in the sentence “The kids ran
the marathon”, ran assigns a role to kids to denote
that they are the runners; and a role to marathon to
denote that it is the race course.
Models for SRL have increasingly come to rely
on an array of NLP tools (e.g., parsers, lem-
matizers) in order to obtain state-of-the-art re-
sults (Bj¨orkelund et al., 2009; Zhao et al., 2009).
Each tool is typically trained on hand-annotated
data, thus placing SRL at the end of a very high-
resource NLP pipeline. However, richly annotated
data such as that provided in parsing treebanks is
expensive to produce, and may be tied to specific
domains (e.g., newswire). Many languages do
</bodyText>
<footnote confidence="0.930503">
1http://www.cs.jhu.edu/˜mrg/software/
</footnote>
<bodyText confidence="0.98427803125">
not have such supervised resources (low-resource
languages), which makes exploring SRL cross-
linguistically difficult.
The problem of SRL for low-resource lan-
guages is an important one to solve, as solutions
pave the way for a wide range of applications: Ac-
curate identification of the semantic roles of enti-
ties is a critical step for any application sensitive to
semantics, from information retrieval to machine
translation to question answering.
In this work, we explore models that minimize
the need for high-resource supervision. We ex-
amine approaches in a joint setting where we
marginalize over latent syntax to find the optimal
semantic role assignment; and a pipeline setting
where we first induce an unsupervised grammar.
We find that the joint approach is a viable alterna-
tive for making reasonable semantic role predic-
tions, outperforming the pipeline models. These
models can be effectively trained with access to
only SRL annotations, and mark a state-of-the-art
contribution for low-resource SRL.
To better understand the effect of the low-
resource grammars and features used in these
models, we further include comparisons with (1)
models that use higher-resource versions of the
same features; (2) state-of-the-art high resource
models; and (3) previous work on low-resource
grammar induction. In sum, this paper makes
several experimental and modeling contributions,
summarized below.
Experimental contributions:
</bodyText>
<listItem confidence="0.997530142857143">
• Comparison of pipeline and joint models for
SRL.
• Subtractive experiments that consider the re-
moval of supervised data.
• Analysis of the induced grammars in un-
supervised, distantly-supervised, and joint
training settings.
</listItem>
<page confidence="0.955364">
1177
</page>
<note confidence="0.668306">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1177–1187,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
Modeling contributions:
</note>
<listItem confidence="0.977387">
• Simpler joint CRF for syntactic and semantic
dependency parsing than previously reported.
• New application of unsupervised grammar
induction: low-resource SRL.
• Constrained grammar induction using SRL
for distant-supervision.
• Use of Brown clusters in place of POS tags
for low-resource SRL.
</listItem>
<bodyText confidence="0.999915461538462">
The pipeline models are introduced in § 3.1 and
jointly-trained models for syntactic and semantic
dependencies (similar in form to Naradowsky et
al. (2012)) are introduced in § 3.2. In the pipeline
models, we develop a novel approach to unsu-
pervised grammar induction and explore perfor-
mance using SRL as distant supervision. The joint
models use a non-loopy conditional random field
(CRF) with a global factor constraining latent syn-
tactic edge variables to form a tree. Efficient exact
marginal inference is possible by embedding a dy-
namic programming algorithm within belief prop-
agation as in Smith and Eisner (2008).
Even at the expense of no dependency path fea-
tures, the joint models best pipeline-trained mod-
els for state-of-the-art performance in the low-
resource setting (§ 4.4). When the models have ac-
cess to observed syntactic trees, they achieve near
state-of-the-art accuracy in the high-resource set-
ting on some languages (§ 4.3).
Examining the learning curve of the joint and
pipeline models in two languages demonstrates
that a small number of labeled SRL examples may
be essential for good end-task performance, but
that the choice of a good model for grammar in-
duction has an even greater impact.
</bodyText>
<sectionHeader confidence="0.999947" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999938584615385">
Our work builds upon research in both seman-
tic role labeling and unsupervised grammar in-
duction (Klein and Manning, 2004; Spitkovsky
et al., 2010a). Previous related approaches to se-
mantic role labeling include joint classification of
semantic arguments (Toutanova et al., 2005; Jo-
hansson and Nugues, 2008), latent syntax induc-
tion (Boxwell et al., 2011; Naradowsky et al.,
2012), and feature engineering for SRL (Zhao et
al., 2009; Bj¨orkelund et al., 2009).
Toutanova et al. (2005) introduced one of
the first joint approaches for SRL and demon-
strated that a model that scores the full predicate-
argument structure of a parse tree could lead to
significant error reduction over independent clas-
sifiers for each predicate-argument relation.
Johansson and Nugues (2008) and Lluis et al.
(2013) extend this idea by coupling predictions of
a dependency parser with predictions from a se-
mantic role labeler. In the model from Johans-
son and Nugues (2008), the outputs from an SRL
pipeline are reranked based on the full predicate-
argument structure that they form. The candidate
set of syntactic-semantic structures is reranked us-
ing the probability of the syntactic tree and seman-
tic structure. Lluis et al. (2013) use a joint arc-
factored model that predicts full syntactic paths
along with predicate-argument structures via dual
decomposition.
Boxwell et al. (2011) and Naradowsky et al.
(2012) observe that syntax may be treated as la-
tent when a treebank is not available. Boxwell
et al. (2011) describe a method for training a se-
mantic role labeler by extracting features from a
packed CCG parse chart, where the parse weights
are given by a simple ruleset. Naradowsky et
al. (2012) marginalize over latent syntactic depen-
dency parses.
Both Boxwell et al. (2011) and Naradowsky
et al. (2012) suggest methods for SRL without
supervised syntax, however, their features come
largely from supervised resources. Even in their
lowest resource setting, Boxwell et al. (2011) re-
quire an oracle CCG tag dictionary extracted from
a treebank. Naradowsky et al. (2012) limit their
exploration to a small set of basic features, and
included high-resource supervision in the form
of lemmas, POS tags, and morphology available
from the CoNLL 2009 data.
There has not yet been a comparison of tech-
niques for SRL that do not rely on a syntactic
treebank, and no exploration of probabilistic mod-
els for unsupervised grammar induction within an
SRL pipeline that we have been able to find.
Related work for the unsupervised learning of
dependency structures separately from semantic
roles primarily comes from Klein and Manning
(2004), who introduced the Dependency Model
with Valence (DMV). This is a robust generative
model that uses a head-outward process over word
classes, where heads generate arguments.
Spitkovsky et al. (2010a) show that Viterbi
(hard) EM training of the DMV with simple uni-
form initialization of the model parameters yields
higher accuracy models than standard soft-EM
</bodyText>
<page confidence="0.989287">
1178
</page>
<figure confidence="0.7205485">
Train Time, Constrained Grammar Induction:
Observed Constraints
</figure>
<figureCaption confidence="0.961121">
Figure 1: Pipeline approach to SRL. In this sim-
</figureCaption>
<bodyText confidence="0.999140875">
ple pipeline, the first stage syntactically parses the
corpus, and the second stage predicts semantic
predicate-argument structure for each sentence us-
ing the labels of the first stage as features. In our
low-resource pipelines, we assume that the syntac-
tic parser is given no labeled parses—however, it
may optionally utilize the semantic parses as dis-
tant supervision. Our experiments also consider
‘longer’ pipelines that include earlier stages: a
morphological analyzer, POS tagger, lemmatizer.
training. In Viterbi EM, the E-step finds the max-
imum likelihood corpus parse given the current
model parameters. The M-step then finds the
maximum likelihood parameters given the corpus
parse. We utilize this approach to produce unsu-
pervised syntactic features for the SRL task.
Grammar induction work has further demon-
strated that distant supervision in the form of
ACE-style relations (Naseem and Barzilay, 2011)
or HTML markup (Spitkovsky et al., 2010b)
can lead to considerable gains. Recent work in
fully unsupervised dependency parsing has sup-
planted these methods with even higher accuracies
(Spitkovsky et al., 2013) by arranging optimiz-
ers into networks that suggest informed restarts
based on previously identified local optima. We do
not reimplement these approaches within the SRL
pipeline here, but provide comparison of these
methods against our grammar induction approach
in isolation in § 4.5.
In both pipeline and joint models, we use fea-
tures adapted from state-of-the-art approaches to
SRL. This includes Zhao et al. (2009) features,
who use feature templates from combinations
of word properties, syntactic positions including
head and children, and semantic properties; and
features from Bj¨orkelund et al. (2009), who utilize
features on syntactic siblings and the dependency
path concatenated with the direction of each edge.
Features are described further in § 3.3.
</bodyText>
<sectionHeader confidence="0.998003" genericHeader="method">
3 Approaches
</sectionHeader>
<bodyText confidence="0.99408">
We consider an array of models, varying:
</bodyText>
<listItem confidence="0.998496333333333">
1. Pipeline vs. joint training (Figures 1 and 2)
2. Types of supervision
3. The objective function at the level of syntax
</listItem>
<subsectionHeader confidence="0.999139">
3.1 Unsupervised Syntax in the Pipeline
</subsectionHeader>
<bodyText confidence="0.999890022222222">
Typical SRL systems are trained following a
pipeline where the first component is trained on
supervised data, and each subsequent component
is trained using the 1-best output of the previous
components. A typical pipeline consists of a POS
tagger, dependency parser, and semantic role la-
beler. In this section, we introduce pipelines that
remove the need for a supervised tagger and parser
by training in an unsupervised and distantly super-
vised fashion.
Brown Clusters We use fully unsupervised
Brown clusters (Brown et al., 1992) in place of
POS tags. Brown clusters have been used to good
effect for various NLP tasks such as named entity
recognition (Miller et al., 2004) and dependency
parsing (Koo et al., 2008; Spitkovsky et al., 2011).
The clusters are formed by a greedy hierachi-
cal clustering algorithm that finds an assignment
of words to classes by maximizing the likelihood
of the training data under a latent-class bigram
model. Each word type is assigned to a fine-
grained cluster at a leaf of the hierarchy of clusters.
Each cluster can be uniquely identified by the path
from the root cluster to that leaf. Representing this
path as a bit-string (with 1 indicating a left and 0
indicating a right child) allows a simple coarsen-
ing of the clusters by truncating the bit-strings. We
train 1000 Brown clusters for each of the CoNLL-
2009 languages on Wikipedia text.2
Unsupervised Grammar Induction Our first
method for grammar induction is fully unsuper-
vised Viterbi EM training of the Dependency
Model with Valence (DMV) (Klein and Manning,
2004), with uniform initialization of the model pa-
rameters. We define the DMV such that it gener-
ates sequences of word classes: either POS tags
or Brown clusters as in Spitkovsky et al. (2011).
The DMV is a simple generative model for pro-
jective dependency trees. Children are generated
recursively for each node. Conditioned on the par-
ent class, the direction (right or left), and the cur-
rent valence (first child or not), a coin is flipped to
decide whether to generate another child; the dis-
tribution over child classes is conditioned on only
the parent class and direction.
</bodyText>
<footnote confidence="0.996711">
2The Wikipedia text was tokenized for Polyglot (Al-Rfou’
et al., 2013): http://bit.ly/embeddings
</footnote>
<figure confidence="0.9967339">
Parsing
Model
Semantic
Dependency
Model
Text Labeled
With Semantic
Roles
Corpus
Text
</figure>
<page confidence="0.990323">
1179
</page>
<bodyText confidence="0.999682549019608">
Constrained Grammar Induction Our second
method, which we will refer to as DMV+C, in-
duces grammar in a distantly supervised fashion
by using a constrained parser in the E-step of
Viterbi EM. Since the parser is part of a pipeline,
we constrain it to respect the downstream SRL an-
notations during training. At test time, the parser
is unconstrained.
Dependency-based semantic role labeling can
be described as a simple structured prediction
problem: the predicted structure is a labeled di-
rected graph, where nodes correspond to words
in the sentence. Each directed edge indicates that
there is a predicate-argument relationship between
the two words; the parent is the predicate and the
child the argument. The label on the edge indi-
cates the type of semantic relationship. Unlike
syntactic dependency parsing, the graph is not re-
quired to be a tree, nor even a connected graph.
Self-loops and crossing arcs are permitted.
The constrained syntactic DMV parser treats
the semantic graph as observed, and constrains the
syntactic parent to be chosen from one of the se-
mantic parents, if there are any. In some cases,
imposing this constraint would not permit any pro-
jective dependency parses—in this case, we ignore
the semantic constraint for that sentence. We parse
with the CKY algorithm (Younger, 1967; Aho and
Ullman, 1972) by utilizing a PCFG corresponding
to the DMV (Cohn et al., 2010). Each chart cell al-
lows only non-terminals compatible with the con-
strained sets. This can be viewed as a variation of
Pereira and Schabes (1992).
Semantic Dependency Model As described
above, semantic role labeling can be cast as a
structured prediction problem where the structure
is a labeled semantic dependency graph. We de-
fine a conditional random field (CRF) (Lafferty et
al., 2001) for this task. Because each word in a
sentence may be in a semantic relationship with
any other word (including itself), a sentence of
length n has n2 possible edges. We define a single
L+1-ary variable for each edge, whose value can
be any of L semantic labels or a special label indi-
cating there is no predicate-argument relationship
between the two words. In this way, we jointly
perform identification (determining whether a se-
mantic relationship exists) and classification (de-
termining the semantic label). This use of an L+1-
ary variable is in contrast to the model of Narad-
owsky et al. (2012), which used a more complex
</bodyText>
<figureCaption confidence="0.56215">
Figure 2: Factor graph for the joint syntac-
tic/semantic dependency parsing model.
</figureCaption>
<bodyText confidence="0.9974442">
set of binary variables and required a constraint
factor permitting AT-MOST-ONE. We include one
unary factor for each variable.
We optionally include additional variables that
perform word sense disambiguation for each pred-
icate. Each has a unary factor and is completely
disconnected from the semantic edge (similar to
Naradowsky et al. (2012)). These variables range
over all the predicate senses observed in the train-
ing data for the lemma of that predicate.
</bodyText>
<subsectionHeader confidence="0.9853575">
3.2 Joint Syntactic and Semantic Parsing
Model
</subsectionHeader>
<bodyText confidence="0.9999776">
In Section 3.1, we introduced pipeline-trained
models for SRL, which used grammar induction
to predict unlabeled syntactic parses. In this sec-
tion, we define a simple model for joint syntactic
and semantic dependency parsing.
This model extends the CRF model in Section
3.1 to include the projective syntactic dependency
parse for a sentence. This is done by includ-
ing an additional n2 binary variables that indicate
whether or not a directed syntactic dependency
edge exists between a pair of words in the sen-
tence. Unlike the semantic dependencies, these
syntactic variables must be coupled so that they
produce a projective dependency parse; this re-
quires an additional global constraint factor to en-
sure that this is the case (Smith and Eisner, 2008).
The constraint factor touches all n2 syntactic-edge
variables, and multiplies in 1.0 if they form a pro-
jective dependency parse, and 0.0 otherwise. We
couple each syntactic edge variable to its semantic
edge variable with a binary factor. Figure 2 shows
the factor graph for this joint model.
Note that our factor graph does not contain any
loops, thereby permitting efficient exact marginal
inference just as in Naradowsky et al. (2012). We
</bodyText>
<figure confidence="0.854781304347826">
DEPTREE
Role
Dep
1,1
1,1
Role
Dep
1,2
1,2
Role
Dep
1,3
1,3
...
...
Role
n,n
Dep
n,n
1180
Property Possible values Template Possible values
1 word form
before, after, on
2 lower case word form
3 5-char word form prefixes
4 capitalization
5 top-800 word form
6 brown cluster
7 brown cluster, length 5
8 lemma
9 POS tag
10 morphological features
all lower-case forms
all 5-char form prefixes
True, False
top-800 word forms
000, 1100, 010110001, ...
length 5 prefixes of brown clusters
all word lemmas
NNP, CD, JJ, DT, ...
Gender, Case, Number,...
distance, continuity
binned distance
geneological relationship
path-grams
Z+
</figure>
<table confidence="0.9241045">
&gt; 2, 5, 10, 20, 30, or 40
parent, child, ancestor, descendant
the NN went
Table 3: Additional standalone templates.
all word forms relative position
(different across languages)
11 dependencylabel
12 edge direction
</table>
<tableCaption confidence="0.999333">
Table 1: Word and edge properties in templates.
</tableCaption>
<equation confidence="0.996829285714286">
i, i-1, i+1 noFarChildren(wi)
parent(wi) rightNearSib(wi)
allChildren(wi) leftNearSib(wi)
rightNearChild(wi) firstVSupp(wi)
rightFarChild(wi) lastVSupp(wi)
leftNearChild(wi) firstNSupp(wi)
leftFarChild(wi) lastNSupp(wi)
</equation>
<bodyText confidence="0.981177466666667">
Table 2: Word positions used in templates. Based
on current word position (i), positions related to
current word wi, possible parent, child (wp, wc),
lowest common ancestor between parent/child
(wlca), and syntactic root (wroot).
train our CRF models by maximizing conditional
log-likelihood using stochastic gradient descent
with an adaptive learning rate (AdaGrad) (Duchi
et al., 2011) over mini-batches.
The unary and binary factors are defined with
exponential family potentials. In the next section,
we consider binary features of the observations
(the sentence and labels from previous pipeline
stages) which are conjoined with the state of the
variables in the factor.
</bodyText>
<subsectionHeader confidence="0.624908">
3.3 Features for CRF Models
</subsectionHeader>
<bodyText confidence="0.999572418604651">
Our feature design stems from two key ideas.
First, for SRL, it has been observed that fea-
ture bigrams (the concatenation of simple fea-
tures such as a predicate’s POS tag and an ar-
gument’s word) are important for state-of-the-art
(Zhao et al., 2009; Bj¨orkelund et al., 2009). Sec-
ond, for syntactic dependency parsing, combining
Brown cluster features with word forms or POS
tags yields high accuracy even with little training
data (Koo et al., 2008).
We create binary indicator features for each
model using feature templates. Our feature tem-
plate definitions build from those used by the top
performing systems in the CoNLL-2009 Shared
Task, Zhao et al. (2009) and Bj¨orkelund et al.
(2009) and from features in syntactic dependency
parsing (McDonald et al., 2005; Koo et al., 2008).
Template Creation Feature templates are de-
fined over triples of (property, positions, order).
Properties, listed in Table 1, are extracted from
word positions within the sentence, shown in Ta-
ble 2. Single positions for a word wi include
its syntactic parent, its leftmost farthest child
(leftFarChild), its rightmost nearest sibling (rightNearSib),
etc. Following Zhao et al. (2009), we include the
notion of verb and noun supports and sections of
the dependency path. Also following Zhao et al.
(2009), properties from a set of positions can be
put together in three possible orders: as the given
sequence, as a sorted list of unique strings, and re-
moving all duplicated neighbored strings. We con-
sider both template unigrams and bigrams, com-
bining two templates in sequence.
Additional templates we include are the relative
position (Bj¨orkelund et al., 2009), geneological re-
lationship, distance (Zhao et al., 2009), and binned
distance (Koo et al., 2008) between two words in
the path. From Llu´ıs et al. (2013), we use 1, 2,3-
gram
,3-
gram path features of words/POS tags (path-grams),
and the number of non-consecutive token pairs in
a predicate-argument path (continuity).
</bodyText>
<subsectionHeader confidence="0.950606">
3.4 Feature Selection
</subsectionHeader>
<bodyText confidence="0.9999456">
Constructing all feature template unigrams and bi-
grams would yield an unwieldy number of fea-
tures. We therefore determine the top N template
bigrams for a dataset and factor a according to an
information gain measure (Martins et al., 2011):
</bodyText>
<equation confidence="0.865831">
p(f, xa)
p(f, xa) log2
</equation>
<bodyText confidence="0.97307325">
p(f)p(xa)
where Tm is the mth feature template, f is a par-
ticular instantiation of that template, and xa is an
assignment to the variables in factor a. The proba-
bilities are empirical estimates computed from the
training data. This is simply the mutual informa-
tion of the feature template instantiation with the
variable assignment.
This filtering approach was treated as a sim-
ple baseline in Martins et al. (2011) to contrast
with increasingly popular gradient based regular-
ization approaches. Unlike the gradient based ap-
</bodyText>
<equation confidence="0.800147888888889">
SBJ, NMOD, LOC, ...
Up, Down
linePath(wp, wc)
depPath(wp, wc)
depPath(wp, wlca)
depPath(wc, wlca)
depPath(wlca, wroot)
�IGa,m = E
f∈Tm xa
</equation>
<page confidence="0.925464">
1181
</page>
<bodyText confidence="0.999900777777778">
proaches, this filtering approach easily scales to
many features since we can decompose the mem-
ory usage over feature templates.
As an additional speedup, we reduce the dimen-
sionality of our feature space to 1 million for each
clique using a common trick referred to as fea-
ture hashing (Weinberger et al., 2009): we map
each feature instantiation to an integer using a hash
function3 modulo the desired dimentionality.
</bodyText>
<sectionHeader confidence="0.999794" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999883214285714">
We are interested in the effects of varied super-
vision using pipeline and joint training for SRL.
To compare to prior work (i.e., submissions to the
CoNLL-2009 Shared Task), we also consider the
joint task of semantic role labeling and predicate
sense disambiguation. Our experiments are sub-
tractive, beginning with all supervision available
and then successively removing (a) dependency
syntax, (b) morphological features, (c) POS tags,
and (d) lemmas. Dependency syntax is the most
expensive and difficult to obtain of these various
forms of supervision. We explore the importance
of both the labels and structure, and what quantity
of supervision is useful.
</bodyText>
<subsectionHeader confidence="0.961653">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999887846153846">
The CoNLL-2009 Shared Task (Hajiˇc et al., 2009)
dataset contains POS tags, lemmas, morpholog-
ical features, syntactic dependencies, predicate
senses, and semantic roles annotations for 7 lan-
guages: Catalan, Chinese, Czech, English, Ger-
man, Japanese,4 Spanish. The CoNLL-2005 and
-2008 Shared Task datasets provide English SRL
annotation, and for cross dataset comparability we
consider only verbal predicates (more details in
§ 4.4). To compare with prior approaches that use
semantic supervision for grammar induction, we
utilize Section 23 of the WSJ portion of the Penn
Treebank (Marcus et al., 1993).
</bodyText>
<subsectionHeader confidence="0.995102">
4.2 Feature Template Sets
</subsectionHeader>
<bodyText confidence="0.99995725">
Our primary feature set IGC consists of 127 tem-
plate unigrams that emphasize coarse properties
(i.e., properties 7, 9, and 11 in Table 1). We also
explore the 31 template unigrams5 IGB described
</bodyText>
<footnote confidence="0.997847">
3To reduce hash collisions, We use MurmurHash v3
https://code.google.com/p/smhasher.
4We do not report results on Japanese as that data was
only made freely available to researchers that competed in
CoNLL 2009.
5Because we do not include a binary factor between pred-
icate sense and semantic role, we do not include sense as a
</footnote>
<bodyText confidence="0.995109181818182">
by Bj¨orkelund et al. (2009). Each of IGC and IGB
also include 32 template bigrams selected by in-
formation gain on 1000 sentences—we select a
different set of template bigrams for each dataset.
We compare against the language-specific fea-
ture sets detailed in the literature on high-resource
top-performing SRL systems: From Bj¨orkelund et
al. (2009), these are feature sets for German, En-
glish, Spanish and Chinese, obtained by weeks of
forward selection (Bde,en,es,zh); and from Zhao et
al. (2009), these are features for Catalan Zca.6
</bodyText>
<subsectionHeader confidence="0.992176">
4.3 High-resource SRL
</subsectionHeader>
<bodyText confidence="0.997938382352941">
We first compare our models trained as a pipeline,
using all available supervision (syntax, morphol-
ogy, POS tags, lemmas) from the CoNLL-2009
data. Table 4(a) shows the results of our model
with gold syntax and a richer feature set than
that of Naradowsky et al. (2012), which only
looked at whether a syntactic dependency edge
was present. This highlights an important advan-
tage of the pipeline trained model: the features can
consider any part of the syntax (e.g., arbitrary sub-
trees), whereas the joint model is limited to those
features over which it can efficiently marginalize
(e.g., short dependency paths). This holds true
even in the pipeline setting where no syntactic su-
pervision is available.
Table 4(b) contrasts our high-resource results
for the task of SRL and sense disambiguation
with the top systems in the CoNLL-2009 Shared
Task, giving further insight into the performance
of the simple information gain feature selection
technique. With supervised syntax, our sim-
ple information gain feature selection technique
(§ 3.4) performs admirably. However, the orig-
inal unigram Bj¨orkelund features (Bde,en,es,zh),
which were tuned for a high-resource model, ob-
tain higher F1 than our information gain set us-
ing the same features in unigram and bigram tem-
plates (IGB). This suggests that further work on
feature selection may improve the results. We
find that IGB obtain higher F1 than the original
Bj¨orkelund feature sets (Bde,en,es,zh) in the low-
resource pipeline setting with constrained gram-
mar induction (DMV+C).
feature for argument prediction.
</bodyText>
<footnote confidence="0.988576333333333">
6This covers all CoNLL languages but Czech, where fea-
ture sets were not made publicly available in either work. In
Czech, we disallowed template bigrams involving path-grams.
</footnote>
<page confidence="0.969849">
1182
</page>
<table confidence="0.999574894736842">
SRL Approach Feature Set Dep. Parser Avg. ca cs de en es zh
Pipeline IGC Gold 84.98 84.97 87.65 79.14 86.54 84.22 87.35
Pipeline IGB Gold 84.74 85.15 86.64 79.50 85.77 84.40 86.95
) Naradowsky et al. (2012) Gold 72.73 69.59 74.84 66.49 78.55 68.93 77.97
Bj¨orkelund et al. (2009) Supervised 81.55 80.01 85.41 79.71 85.63 79.91 78.60
Zhao et al. (2009) Supervised 80.85 80.32 85.19 75.99 85.44 80.46 77.72
Pipeline IGC Supervised 78.03 76.24 83.34 74.19 81.96 76.12 76.35
) Pipeline Z.a Supervised *77.62 77.62 — — — — —
Pipeline Bde,en,es,zh Supervised *76.49 — — 72.17 81.15 76.65 75.99
Pipeline IGB Supervised 75.68 74.59 81.61 69.08 78.86 74.51 75.44
Joint IGC Marginalized 72.48 71.35 81.03 65.15 76.16 71.03 70.14
Joint IGB Marginalized 72.40 71.55 80.04 64.80 75.57 71.21 71.21
Naradowsky et al. (2012) Marginalized 71.27 67.99 73.16 67.26 76.12 66.74 76.32
) Pipeline IGC DMV+C (bc) 70.08 68.21 79.63 62.25 73.81 68.73 67.86
Pipeline Z.a DMV+C (bc) *69.67 69.67 — — — — —
Pipeline IGC DMV (bc) 69.26 68.04 79.58 58.47 74.78 68.36 66.35
Pipeline IGB DMV (bc) 66.81 63.31 77.38 59.91 72.02 65.96 62.28
Pipeline IGB DMV+C (bc) 65.61 61.89 77.48 58.97 69.11 63.31 62.92
Pipeline Bde,en,es,zh DMV+C (bc) *63.06 — — 57.75 68.32 63.70 62.45
</table>
<tableCaption confidence="0.946234666666667">
Table 4: Test F1 for SRL and sense disambiguation on CoNLL’09 in high-resource and low-resource
settings: we study (a) gold syntax, (b) supervised syntax, and (c) unsupervised syntax. Results are
ranked by F1 with bold numbers indicating the best F1 for a language and level of supervision.
</tableCaption>
<figure confidence="0.792940636363636">
*Indicates partial averages for the language-specific feature sets (Z.a and Bde,en,es,zh), for which we show results only on the
languages for which the sets were publicly available.
(a
(b
(c
❑� PRY’08
❑ B’11 (tdc)
❑ B’11 (td)
❑� JN’08
❑ Joint, IGC
❑ Joint, IGB
</figure>
<bodyText confidence="0.7419545">
Table 5: F1 for SRL approaches (without sense
disambiguation) in matched and mismatched
train/test settings for CoNLL 2005 span and 2008
head supervision. We contrast low-resource (❑)
and high-resource settings (0), where latter uses a
treebank. See § 4.4 for caveats to this comparison.
</bodyText>
<subsectionHeader confidence="0.989358">
4.4 Low-Resource SRL
</subsectionHeader>
<bodyText confidence="0.999875951219512">
CoNLL-2009 Table 4(c) includes results for our
low-resource approaches and Naradowsky et al.
(2012) on predicting semantic roles as well as
sense. In the low-resource setting of the CoNLL-
2009 Shared task without syntactic supervision,
our joint model (Joint) with marginalized syntax
obtains state-of-the-art results with features IGC
described in § 4.2. This model outperforms prior
work (Naradowsky et al., 2012) and our pipeline
model (Pipeline) with contrained (DMV+C) and
unconstrained grammar induction (DMV) trained
on brown clusters (bc).
In the low-resource setting, training and decod-
ing times for the pipeline and joint methods are
similar as computation time tends to be dominated
by feature extraction.
These results begin to answer a key research
question in this work: The joint models outper-
form the pipeline models in the low-resource set-
ting. This holds even when using the same feature
selection process. Further, the best-performing
low-resource features found in this work are those
based on coarse feature templates and selected
by information gain. Templates for these fea-
tures generalize well to the high-resource setting.
However, analysis of the induced grammars in
the pipeline setting suggests that the book is not
closed on the issue. We return to this in § 4.5.
CoNLL-2008, -2005 To finish out comparisons
with state-of-the-art SRL, we contrast our ap-
proach with that of Boxwell et al. (2011), who
evaluate on SRL in isolation (without sense disam-
biguation, as in CoNLL-2009). They report results
on Prop-CCGbank (Boxwell and White, 2008),
which uses the same training/testing splits as the
CoNLL-2005 Shared Task. Their results are there-
fore loosely7 comparable to results on the CoNLL-
2005 dataset, which we can compare here.
There is an additional complication in com-
paring SRL approaches directly: The CoNLL-
2005 dataset defines arguments as spans instead of
</bodyText>
<footnote confidence="0.898995285714286">
7The comparison is imperfect for two reasons: first, the
CCGBank contains only 99.44% of the original PTB sen-
tences (Hockenmaier and Steedman, 2007); second, because
PropBank was annotated over CFGs, after converting to CCG
only 99.977% of the argument spans were exact matches
(Boxwell and White, 2008). However, this comparison was
adopted by Boxwell et al. (2011), so we use it here.
</footnote>
<figure confidence="0.997000142857143">
2005
spans
(oracle
tree)
72.0
67.1
2008
heads
84.32
—
—
2005
spans
79.44
71.5
65.0
85.93
72.9
67.3
79.90
35.0
37.8
2008
heads
test
train
2005
spans
</figure>
<page confidence="0.985479">
1183
</page>
<bodyText confidence="0.949091245283019">
heads, which runs counter to our head-based syn-
tactic representation. This creates a mismatched
train/test scenario: we must train our model to pre-
dict argument heads, but then test on our models
ability to predict argument spans.8 We therefore
train our models on the CoNLL-2008 argument
heads,9 and post-process and convert from heads
to spans using the conversion algorithm available
from Johansson and Nugues (2008).10 The heads
are either from an MBR tree or an oracle tree. This
gives Boxwell et al. (2011) the advantage, since
our syntactic dependency parses are optimized to
pick out semantic argument heads, not spans.
Table 5 presents our results. Boxwell et al.
(2011) (B’11) uses additional supervision in the
form of a CCG tag dictionary derived from su-
pervised data with (tdc) and without (tc) a cut-
off. Our model does very poorly on the ’05 span-
based evaluation because the constituent bracket-
ing of the marginalized trees are inaccurate. This
is elucidated by instead evaluating on the ora-
cle spans, where our F1 scores are higher than
Boxwell et al. (2011). We also contrast with rela-
vant high-resource methods with span/head con-
versions from Johansson and Nugues (2008): Pun-
yakanok et al. (2008) (PRY’08) and Johansson and
Nugues (2008) (JN’08).
Subtractive Study In our subsequent experi-
ments, we study the effectiveness of our models
as the available supervision is decreased. We in-
crementally remove dependency syntax, morpho-
logical features, POS tags, then lemmas. For these
experiments, we utilize the coarse-grained feature
set (IGC), which includes Brown clusters.
Across languages, we find the largest drop in
F1 when we remove POS tags; and we find a
gain in F1 when we remove lemmas. This indi-
cates that lemmas, which are a high-resource an-
notation, may not provide a significant benefit for
this task. The effect of removing morphological
features is different across languages, with little
change in performance for Catalan and Spanish,
8We were unable to obtain the system output of Boxwell
et al. (2011) in order to convert their spans to dependencies
and evaluate the other mismatched train/test setting.
9CoNLL-2005, -2008, and -2009 were derived from Prop-
Bank and share the same source text; -2008 and -2009 use
argument heads.
10Specifically, we use their Algorithm 2, which produces
the span dominated by each argument, with special handling
of the case when the argument head dominates that of the
predicate. Also following Johansson and Nugues (2008), we
recover the ’05 sentences missing from the ’08 evaluation set.
</bodyText>
<table confidence="0.997483833333333">
Rem #FT ca de es
– 127+32 74.46 72.62 74.23
Dep 40+32 67.43 64.24 67.18
Mor 30+32 67.84 59.78 66.94
POS 23+32 64.40 54.68 62.71
Lem 21+32 64.85 54.89 63.80
</table>
<tableCaption confidence="0.974664">
Table 6: Subtractive experiments. Each row con-
</tableCaption>
<bodyText confidence="0.915641">
tains the F1 for SRL only (without sense disam-
biguation) where the supervision type of that row
and all above it have been removed. Removed su-
pervision types (Rem) are: syntactic dependencies
(Dep), morphology (Mor), POS tags (POS), and
lemmas (Lem). #FT indicates the number of fea-
ture templates used (unigrams+bigrams).
</bodyText>
<figure confidence="0.817329714285714">
Language / Dependency Parser
Catalan / Marginalized
Catalan / DMV+C
German / Marginalized
German / DMV+C
0 20000 40000 60000
Number of Training Sentences
</figure>
<figureCaption confidence="0.625342">
Figure 3: Learning curve for semantic dependency
supervision in Catalan and German. F1 of SRL
only (without sense disambiguation) shown as the
number of training sentences is increased.
</figureCaption>
<bodyText confidence="0.999793357142857">
but a drop in performance for German. This may
reflect a difference between the languages, or may
reflect the difference between the annotation of the
languages: both the Catalan and Spanish data orig-
inated from the Ancora project,11 while the Ger-
man data came from another source.
Figure 3 contains the learning curve for SRL su-
pervision in our lowest resource setting for two
example languages, Catalan and German. This
shows how F1 of SRL changes as we adjust
the number of training examples. We find that
the joint training approach to grammar induction
yields consistently higher SRL performance than
its distantly supervised counterpart.
</bodyText>
<subsectionHeader confidence="0.99929">
4.5 Analysis of Grammar Induction
</subsectionHeader>
<bodyText confidence="0.923246">
Table 7 shows grammar induction accuracy in
low-resource settings. We find that the gap be-
tween the supervised parser and the unsupervised
methods is quite large, despite the reasonable ac-
curacy both methods achieve for the SRL end task.
</bodyText>
<figure confidence="0.904647375">
11http://clic.ub.edu/corpus/ancora
70
60
Labeled F1
50
40
30
20
</figure>
<page confidence="0.974174">
1184
</page>
<table confidence="0.999300555555555">
Dependency Avg. ca cs de en es zh
Parser
Supervised* 87.1 89.4 85.3 89.6 88.4 89.2 80.7
DMV (pos) 30.2 45.3 22.7 20.9 32.9 41.9 17.2
DMV (bc) 22.1 18.8 32.8 19.6 22.4 20.5 18.6
DMV+C (pos) 37.5 50.2 34.9 21.5 36.9 49.8 32.0
DMV+C (bc) 40.2 46.3 37.5 28.7 40.6 50.4 37.5
Marginal, IGC 43.8 50.3 45.8 27.2 44.2 46.3 48.5
Marginal, IGB 50.2 52.4 43.4 41.3 52.6 55.2 56.2
</table>
<tableCaption confidence="0.9499194">
Table 7: Unlabeled directed dependency accuracy
on CoNLL’09 test set in low-resource settings.
DMV models are trained on either POS tags (pos)
or Brown clusters (bc). *Indicates the supervised parser
outputs provided by the CoNLL’09 Shared Task.
</tableCaption>
<table confidence="0.9955514">
WSJ∞ Distant
Supervision
SAJM’10 44.8 none
SAJ’13 64.4 none
SJA’10 50.4 HTML
NB’11 59.4 ACE05
DMV (bc) 24.8 none
DMV+C (bc) 44.8 SRL
Marginalized, IGC 48.8 SRL
Marginalized, IGB 58.9 SRL
</table>
<tableCaption confidence="0.995922">
Table 8: Comparison of grammar induction ap-
</tableCaption>
<bodyText confidence="0.951809324324324">
proaches. We contrast the DMV trained with
Viterbi EM+uniform initialization (DMV), our
constrained DMV (DMV+C), and our model’s
MBR decoding of latent syntax (Marginalized)
with other recent work: Spitkovsky et al. (2010a)
(SAJM’10), Spitkovsky et al. (2010b) (SJA’10),
Naseem and Barzilay (2011) (NB’11), and the CS
model of Spitkovsky et al. (2013) (SAJ’13).
This suggests that refining the low-resource gram-
mar induction methods may lead to gains in SRL.
Interestingly, the marginalized grammars best
the DMV grammar induction method; however,
this difference is less pronounced when the DMV
is constrained using SRL labels as distant super-
vision. This could indicate that a better model for
grammar induction would result in better perfor-
mance for SRL. We therefore turn to an analysis of
other approaches to grammar induction in Table 8,
evaluated on the Penn Treebank. We contrast with
methods using distant supervision (Naseem and
Barzilay, 2011; Spitkovsky et al., 2010b) and fully
unsupervised dependency parsing (Spitkovsky et
al., 2013). Following prior work, we exclude
punctuation from evaluation and convert the con-
stituency trees to dependencies.12
The approach from Spitkovsky et al. (2013)
12Naseem and Barzilay (2011) and our results use the
Penn converter (Pierre and Heiki-Jaan, 2007). Spitkovsky et
al. (2010b; 2013) use Collins (1999) head percolation rules.
(SAJ’13) outperforms all other approaches, in-
cluding our marginalized settings. We therefore
may be able to achieve further gains in the pipeline
model by considering better models of latent syn-
tax, or better search techniques that break out
of local optima. Similarly, improving the non-
convex optimization of our latent-variable CRF
(Marginalized) may offer further gains.
</bodyText>
<sectionHeader confidence="0.999187" genericHeader="conclusions">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999954717948718">
We have compared various approaches for low-
resource semantic role labeling at the state-of-the-
art level. We find that we can outperform prior
work in the low-resource setting by coupling the
selection of feature templates based on informa-
tion gain with a joint model that marginalizes over
latent syntax.
We utilize unlabeled data in both generative and
discriminative models for dependency syntax and
in generative word clustering. Our discriminative
joint models treat latent syntax as a structured-
feature to be optimized for the end-task of SRL,
while our other grammar induction techniques op-
timize for unlabeled data likelihood—optionally
with distant supervision. We observe that careful
use of these unlabeled data resources can improve
performance on the end task.
Our subtractive experiments suggest that lemma
annotations, a high-resource annotation, may not
provide a large benefit for SRL. Our grammar in-
duction analysis indicates that relatively low accu-
racy can still result in reasonable SRL predictions;
still, the models do not outperform those that use
supervised syntax, and we aim to explore how well
the pipeline models in particular improve when we
apply higher accuracy unsupervised grammar in-
duction techniques.
We have utilized well studied datasets in order
to best understand the quality of our models rela-
tive to prior work. In future work, we hope to ex-
plore the effectiveness of our approaches on truly
low resource settings by using crowdsourcing to
develop semantic role datasets in other languages
and domains.
Acknowledgments We thank Richard Johans-
son, Dennis Mehay, and Stephen Boxwell for help
with data. We also thank Jason Naradowsky, Jason
Eisner, and anonymous reviewers for comments
on the paper.
</bodyText>
<page confidence="0.990777">
1185
</page>
<sectionHeader confidence="0.994174" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999501275229358">
Alfred V. Aho and Jeffrey D. Ullman. 1972. The
Theory of Parsing, Translation, and Compiling.
Prentice-Hall, Inc.
Rami Al-Rfou’, Bryan Perozzi, and Steven Skiena.
2013. Polyglot: Distributed word representations
for multilingual NLP. In Proceedings of the 17th
Conference on Computational Natural Language
Learning (CoNLL 2013). Association for Computa-
tional Linguistics.
Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues.
2009. Multilingual semantic role labeling. In Pro-
ceedings of the 13th Conference on Computational
Natural Language Learning (CoNLL 2009): Shared
Task. Association for Computational Linguistics.
Stephen Boxwell and Michael White. 2008. Project-
ing propbank roles onto the CCGbank. In Proceed-
ings of the International Conference on Language
Resources and Evaluation (LREC 2008). European
Language Resources Association.
Stephen Boxwell, Chris Brew, Jason Baldridge, Dennis
Mehay, and Sujith Ravi. 2011. Semantic role label-
ing without treebanks? In Proceedings of the 5th In-
ternational Joint Conference on Natural Language
Processing (IJCNLP). Asian Federation of Natural
Language Processing.
Peter F. Brown, Peter V. Desouza, Robert L. Mer-
cer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992. Class-based n-gram models of natural lan-
guage. Computational linguistics, 18(4).
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. The
Journal of Machine Learning Research, 11.
Michael Collins. 1999. Head-driven statistical models
for natural language parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL 2009): Shared Task. Association for
Computational Linguistics.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: a corpus of CCG derivations and dependency
structures extracted from the Penn Treebank. Com-
putational Linguistics, 33(3).
Richard Johansson and Pierre Nugues. 2008.
Dependency-based semantic role labeling of Prop-
Bank. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2008). Association for Computational Lin-
guistics.
Dan Klein and Christopher Manning. 2004. Corpus-
Based induction of syntactic structure: Models of
dependency and constituency. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics (ACL 2004). Association for Computa-
tional Linguistics.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In Proceedings of ACL-08: HLT. Association for
Computational Linguistics.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceed-
ings of the 18th International Conference on Ma-
chine Learning (ICML 2001). Morgan Kaufmann.
Xavier Llu´ıs, Xavier Carreras, and Llu´ıs M`arquez.
2013. Joint arc-factored parsing of syntactic and se-
mantic dependencies. Transactions of the Associa-
tion for Computational Linguistics (TACL).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The Penn Treebank. Com-
putational linguistics, 19(2).
Andre Martins, Noah Smith, Mario Figueiredo, and
Pedro Aguiar. 2011. Structured sparsity in struc-
tured prediction. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2011). Association for Compu-
tational Linguistics.
R. McDonald, K. Crammer, and F. Pereira. 2005. On-
line large-margin training of dependency parsers. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL 2005).
Association for Computational Linguistics.
Scott Miller, Jethran Guinness, and Alex Zamanian.
2004. Name tagging with word clusters and dis-
criminative training. In Susan Dumais, Daniel
Marcu, and Salim Roukos, editors, HLT-NAACL
2004: Main Proceedings. Association for Compu-
tational Linguistics.
Jason Naradowsky, Sebastian Riedel, and David Smith.
2012. Improving NLP through marginalization of
hidden syntactic structure. In Proceedings of the
2012 Conference on Empirical Methods in Natural
Language Processing (EMNLP 2012). Association
for Computational Linguistics.
Tahira Naseem and Regina Barzilay. 2011. Using
semantic cues to learn syntax. In Proceedings of
the 25th AAAI Conference on Artificial Intelligence
(AAAI 2011). AAAI Press.
</reference>
<page confidence="0.86276">
1186
</page>
<reference confidence="0.999793692307692">
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed cor-
pora. In Proceedings of the 30th Annual Meeting of
the Association for Computational Linguistics (ACL
1992).
Nugues Pierre and Kalep Heiki-Jaan. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. NODALIDA 2007 Proceedings.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Computational Linguistics,
34(2).
David A. Smith and Jason Eisner. 2008. Dependency
parsing by belief propagation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2008). Association for
Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Juraf-
sky, and Christopher D Manning. 2010a. Viterbi
training improves unsupervised dependency parsing.
In Proceedings of the 14th Conference on Computa-
tional Natural Language Learning (CoNLL 2010).
Association for Computational Linguistics.
Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-
shawi. 2010b. Profiting from mark-up: Hyper-text
annotations for guided parsing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2010). Association for
Computational Linguistics.
Valentin I. Spitkovsky, Hiyan Alshawi, Angel X.
Chang, and Daniel Jurafsky. 2011. Unsupervised
dependency parsing without gold part-of-speech
tags. In Proceedings of the 2011 Conference on
Empirical Methods in Natural Language Processing
(EMNLP 2011). Association for Computational Lin-
guistics.
Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-
rafsky. 2013. Breaking out of local optima with
count transforms and model recombination: A study
in grammar induction. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2013). Association for
Computational Linguistics.
Kristina Toutanova, Aria Haghighi, and Christopher
Manning. 2005. Joint learning improves semantic
role labeling. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics (ACL 2005). Association for Computational Lin-
guistics.
Kilian Weinberger, Anirban Dasgupta, John Langford,
Alex Smola, and Josh Attenberg. 2009. Feature
hashing for large scale multitask learning. In L´eon
Bottou and Michael Littman, editors, Proceedings
of the 26th Annual International Conference on Ma-
chine Learning (ICML 2009). Omnipress.
Daniel H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2).
Hai Zhao, Wenliang Chen, Chunyu Kity, and Guodong
Zhou. 2009. Multilingual dependency learning: A
huge feature engineering method to semantic depen-
dency parsing. In Proceedings of the 13th Confer-
ence on Computational Natural Language Learning
(CoNLL 2009): Shared Task. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.995168">
1187
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.847021">
<title confidence="0.999971">Low-Resource Semantic Role Labeling</title>
<author confidence="0.999823">R Van</author>
<affiliation confidence="0.974708">Language Technology Center of</affiliation>
<address confidence="0.995305">Johns Hopkins University, Baltimore, MD 21211 Redmond, WA 98052</address>
<abstract confidence="0.999681777777778">We explore the extent to which highresource manual annotations such as treebanks are necessary for the task of semantic role labeling (SRL). We examine how performance changes without syntacsupervision, comparing both to induce latent syntax. This work highlights a new application of unsupervised grammar induction and demonstrates several approaches to SRL in the absence of supervised syntax. Our best models obtain competitive results in the high-resource setting and state-ofthe-art results in the low resource setting, reaching 72.48% F1 averaged across languages. We release our code for this work along with a larger toolkit for specifying</abstract>
<intro confidence="0.880646">graphical</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling.</booktitle>
<publisher>Prentice-Hall, Inc.</publisher>
<contexts>
<context position="14203" citStr="Aho and Ullman, 1972" startWordPosition="2222" endWordPosition="2225">. The label on the edge indicates the type of semantic relationship. Unlike syntactic dependency parsing, the graph is not required to be a tree, nor even a connected graph. Self-loops and crossing arcs are permitted. The constrained syntactic DMV parser treats the semantic graph as observed, and constrains the syntactic parent to be chosen from one of the semantic parents, if there are any. In some cases, imposing this constraint would not permit any projective dependency parses—in this case, we ignore the semantic constraint for that sentence. We parse with the CKY algorithm (Younger, 1967; Aho and Ullman, 1972) by utilizing a PCFG corresponding to the DMV (Cohn et al., 2010). Each chart cell allows only non-terminals compatible with the constrained sets. This can be viewed as a variation of Pereira and Schabes (1992). Semantic Dependency Model As described above, semantic role labeling can be cast as a structured prediction problem where the structure is a labeled semantic dependency graph. We define a conditional random field (CRF) (Lafferty et al., 2001) for this task. Because each word in a sentence may be in a semantic relationship with any other word (including itself), a sentence of length n h</context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling. Prentice-Hall, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rami Al-Rfou’</author>
<author>Bryan Perozzi</author>
<author>Steven Skiena</author>
</authors>
<title>Polyglot: Distributed word representations for multilingual NLP.</title>
<date>2013</date>
<booktitle>In Proceedings of the 17th Conference on Computational Natural Language Learning (CoNLL 2013). Association for Computational Linguistics.</booktitle>
<marker>Al-Rfou’, Perozzi, Skiena, 2013</marker>
<rawString>Rami Al-Rfou’, Bryan Perozzi, and Steven Skiena. 2013. Polyglot: Distributed word representations for multilingual NLP. In Proceedings of the 17th Conference on Computational Natural Language Learning (CoNLL 2013). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>Love Hafdell</author>
<author>Pierre Nugues</author>
</authors>
<title>Multilingual semantic role labeling.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL</booktitle>
<marker>Bj¨orkelund, Hafdell, Nugues, 2009</marker>
<rawString>Anders Bj¨orkelund, Love Hafdell, and Pierre Nugues. 2009. Multilingual semantic role labeling. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Boxwell</author>
<author>Michael White</author>
</authors>
<title>Projecting propbank roles onto the CCGbank.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2008). European Language Resources Association.</booktitle>
<contexts>
<context position="30033" citStr="Boxwell and White, 2008" startWordPosition="4738" endWordPosition="4741">rforming low-resource features found in this work are those based on coarse feature templates and selected by information gain. Templates for these features generalize well to the high-resource setting. However, analysis of the induced grammars in the pipeline setting suggests that the book is not closed on the issue. We return to this in § 4.5. CoNLL-2008, -2005 To finish out comparisons with state-of-the-art SRL, we contrast our approach with that of Boxwell et al. (2011), who evaluate on SRL in isolation (without sense disambiguation, as in CoNLL-2009). They report results on Prop-CCGbank (Boxwell and White, 2008), which uses the same training/testing splits as the CoNLL-2005 Shared Task. Their results are therefore loosely7 comparable to results on the CoNLL2005 dataset, which we can compare here. There is an additional complication in comparing SRL approaches directly: The CoNLL2005 dataset defines arguments as spans instead of 7The comparison is imperfect for two reasons: first, the CCGBank contains only 99.44% of the original PTB sentences (Hockenmaier and Steedman, 2007); second, because PropBank was annotated over CFGs, after converting to CCG only 99.977% of the argument spans were exact matches</context>
</contexts>
<marker>Boxwell, White, 2008</marker>
<rawString>Stephen Boxwell and Michael White. 2008. Projecting propbank roles onto the CCGbank. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2008). European Language Resources Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Boxwell</author>
<author>Chris Brew</author>
<author>Jason Baldridge</author>
<author>Dennis Mehay</author>
<author>Sujith Ravi</author>
</authors>
<title>Semantic role labeling without treebanks?</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP). Asian Federation of Natural Language Processing.</booktitle>
<contexts>
<context position="5715" citStr="Boxwell et al., 2011" startWordPosition="866" endWordPosition="869">ning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Johansson and Nugues (2008) and Lluis et al. (2013) extend this idea by coupling predictions of a dependency parser with predictions from a semantic role labeler. In the model from Johansson and Nugues (2008), th</context>
<context position="7122" citStr="Boxwell et al. (2011)" startWordPosition="1096" endWordPosition="1099">lity of the syntactic tree and semantic structure. Lluis et al. (2013) use a joint arcfactored model that predicts full syntactic paths along with predicate-argument structures via dual decomposition. Boxwell et al. (2011) and Naradowsky et al. (2012) observe that syntax may be treated as latent when a treebank is not available. Boxwell et al. (2011) describe a method for training a semantic role labeler by extracting features from a packed CCG parse chart, where the parse weights are given by a simple ruleset. Naradowsky et al. (2012) marginalize over latent syntactic dependency parses. Both Boxwell et al. (2011) and Naradowsky et al. (2012) suggest methods for SRL without supervised syntax, however, their features come largely from supervised resources. Even in their lowest resource setting, Boxwell et al. (2011) require an oracle CCG tag dictionary extracted from a treebank. Naradowsky et al. (2012) limit their exploration to a small set of basic features, and included high-resource supervision in the form of lemmas, POS tags, and morphology available from the CoNLL 2009 data. There has not yet been a comparison of techniques for SRL that do not rely on a syntactic treebank, and no exploration of pr</context>
<context position="29887" citStr="Boxwell et al. (2011)" startWordPosition="4716" endWordPosition="4719">outperform the pipeline models in the low-resource setting. This holds even when using the same feature selection process. Further, the best-performing low-resource features found in this work are those based on coarse feature templates and selected by information gain. Templates for these features generalize well to the high-resource setting. However, analysis of the induced grammars in the pipeline setting suggests that the book is not closed on the issue. We return to this in § 4.5. CoNLL-2008, -2005 To finish out comparisons with state-of-the-art SRL, we contrast our approach with that of Boxwell et al. (2011), who evaluate on SRL in isolation (without sense disambiguation, as in CoNLL-2009). They report results on Prop-CCGbank (Boxwell and White, 2008), which uses the same training/testing splits as the CoNLL-2005 Shared Task. Their results are therefore loosely7 comparable to results on the CoNLL2005 dataset, which we can compare here. There is an additional complication in comparing SRL approaches directly: The CoNLL2005 dataset defines arguments as spans instead of 7The comparison is imperfect for two reasons: first, the CCGBank contains only 99.44% of the original PTB sentences (Hockenmaier an</context>
<context position="31408" citStr="Boxwell et al. (2011)" startWordPosition="4963" endWordPosition="4966">s 84.32 — — 2005 spans 79.44 71.5 65.0 85.93 72.9 67.3 79.90 35.0 37.8 2008 heads test train 2005 spans 1183 heads, which runs counter to our head-based syntactic representation. This creates a mismatched train/test scenario: we must train our model to predict argument heads, but then test on our models ability to predict argument spans.8 We therefore train our models on the CoNLL-2008 argument heads,9 and post-process and convert from heads to spans using the conversion algorithm available from Johansson and Nugues (2008).10 The heads are either from an MBR tree or an oracle tree. This gives Boxwell et al. (2011) the advantage, since our syntactic dependency parses are optimized to pick out semantic argument heads, not spans. Table 5 presents our results. Boxwell et al. (2011) (B’11) uses additional supervision in the form of a CCG tag dictionary derived from supervised data with (tdc) and without (tc) a cutoff. Our model does very poorly on the ’05 spanbased evaluation because the constituent bracketing of the marginalized trees are inaccurate. This is elucidated by instead evaluating on the oracle spans, where our F1 scores are higher than Boxwell et al. (2011). We also contrast with relavant high-r</context>
<context position="32931" citStr="Boxwell et al. (2011)" startWordPosition="5211" endWordPosition="5214">remove dependency syntax, morphological features, POS tags, then lemmas. For these experiments, we utilize the coarse-grained feature set (IGC), which includes Brown clusters. Across languages, we find the largest drop in F1 when we remove POS tags; and we find a gain in F1 when we remove lemmas. This indicates that lemmas, which are a high-resource annotation, may not provide a significant benefit for this task. The effect of removing morphological features is different across languages, with little change in performance for Catalan and Spanish, 8We were unable to obtain the system output of Boxwell et al. (2011) in order to convert their spans to dependencies and evaluate the other mismatched train/test setting. 9CoNLL-2005, -2008, and -2009 were derived from PropBank and share the same source text; -2008 and -2009 use argument heads. 10Specifically, we use their Algorithm 2, which produces the span dominated by each argument, with special handling of the case when the argument head dominates that of the predicate. Also following Johansson and Nugues (2008), we recover the ’05 sentences missing from the ’08 evaluation set. Rem #FT ca de es – 127+32 74.46 72.62 74.23 Dep 40+32 67.43 64.24 67.18 Mor 30</context>
</contexts>
<marker>Boxwell, Brew, Baldridge, Mehay, Ravi, 2011</marker>
<rawString>Stephen Boxwell, Chris Brew, Jason Baldridge, Dennis Mehay, and Sujith Ravi. 2011. Semantic role labeling without treebanks? In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP). Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="11089" citStr="Brown et al., 1992" startWordPosition="1709" endWordPosition="1712">. The objective function at the level of syntax 3.1 Unsupervised Syntax in the Pipeline Typical SRL systems are trained following a pipeline where the first component is trained on supervised data, and each subsequent component is trained using the 1-best output of the previous components. A typical pipeline consists of a POS tagger, dependency parser, and semantic role labeler. In this section, we introduce pipelines that remove the need for a supervised tagger and parser by training in an unsupervised and distantly supervised fashion. Brown Clusters We use fully unsupervised Brown clusters (Brown et al., 1992) in place of POS tags. Brown clusters have been used to good effect for various NLP tasks such as named entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008; Spitkovsky et al., 2011). The clusters are formed by a greedy hierachical clustering algorithm that finds an assignment of words to classes by maximizing the likelihood of the training data under a latent-class bigram model. Each word type is assigned to a finegrained cluster at a leaf of the hierarchy of clusters. Each cluster can be uniquely identified by the path from the root cluster to that leaf. Represen</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F. Brown, Peter V. Desouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trevor Cohn</author>
<author>Phil Blunsom</author>
<author>Sharon Goldwater</author>
</authors>
<title>Inducing tree-substitution grammars.</title>
<date>2010</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>11</volume>
<contexts>
<context position="14268" citStr="Cohn et al., 2010" startWordPosition="2234" endWordPosition="2237"> Unlike syntactic dependency parsing, the graph is not required to be a tree, nor even a connected graph. Self-loops and crossing arcs are permitted. The constrained syntactic DMV parser treats the semantic graph as observed, and constrains the syntactic parent to be chosen from one of the semantic parents, if there are any. In some cases, imposing this constraint would not permit any projective dependency parses—in this case, we ignore the semantic constraint for that sentence. We parse with the CKY algorithm (Younger, 1967; Aho and Ullman, 1972) by utilizing a PCFG corresponding to the DMV (Cohn et al., 2010). Each chart cell allows only non-terminals compatible with the constrained sets. This can be viewed as a variation of Pereira and Schabes (1992). Semantic Dependency Model As described above, semantic role labeling can be cast as a structured prediction problem where the structure is a labeled semantic dependency graph. We define a conditional random field (CRF) (Lafferty et al., 2001) for this task. Because each word in a sentence may be in a semantic relationship with any other word (including itself), a sentence of length n has n2 possible edges. We define a single L+1-ary variable for eac</context>
</contexts>
<marker>Cohn, Blunsom, Goldwater, 2010</marker>
<rawString>Trevor Cohn, Phil Blunsom, and Sharon Goldwater. 2010. Inducing tree-substitution grammars. The Journal of Machine Learning Research, 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-driven statistical models for natural language parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="37503" citStr="Collins (1999)" startWordPosition="5944" endWordPosition="5945">mance for SRL. We therefore turn to an analysis of other approaches to grammar induction in Table 8, evaluated on the Penn Treebank. We contrast with methods using distant supervision (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b) and fully unsupervised dependency parsing (Spitkovsky et al., 2013). Following prior work, we exclude punctuation from evaluation and convert the constituency trees to dependencies.12 The approach from Spitkovsky et al. (2013) 12Naseem and Barzilay (2011) and our results use the Penn converter (Pierre and Heiki-Jaan, 2007). Spitkovsky et al. (2010b; 2013) use Collins (1999) head percolation rules. (SAJ’13) outperforms all other approaches, including our marginalized settings. We therefore may be able to achieve further gains in the pipeline model by considering better models of latent syntax, or better search techniques that break out of local optima. Similarly, improving the nonconvex optimization of our latent-variable CRF (Marginalized) may offer further gains. 5 Discussion and Future Work We have compared various approaches for lowresource semantic role labeling at the state-of-theart level. We find that we can outperform prior work in the low-resource setti</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-driven statistical models for natural language parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<volume>12</volume>
<contexts>
<context position="18566" citStr="Duchi et al., 2011" startWordPosition="2908" endWordPosition="2911"> templates. i, i-1, i+1 noFarChildren(wi) parent(wi) rightNearSib(wi) allChildren(wi) leftNearSib(wi) rightNearChild(wi) firstVSupp(wi) rightFarChild(wi) lastVSupp(wi) leftNearChild(wi) firstNSupp(wi) leftFarChild(wi) lastNSupp(wi) Table 2: Word positions used in templates. Based on current word position (i), positions related to current word wi, possible parent, child (wp, wc), lowest common ancestor between parent/child (wlca), and syntactic root (wroot). train our CRF models by maximizing conditional log-likelihood using stochastic gradient descent with an adaptive learning rate (AdaGrad) (Duchi et al., 2011) over mini-batches. The unary and binary factors are defined with exponential family potentials. In the next section, we consider binary features of the observations (the sentence and labels from previous pipeline stages) which are conjoined with the state of the variables in the factor. 3.3 Features for CRF Models Our feature design stems from two key ideas. First, for SRL, it has been observed that feature bigrams (the concatenation of simple features such as a predicate’s POS tag and an argument’s word) are important for state-of-the-art (Zhao et al., 2009; Bj¨orkelund et al., 2009). Second</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research, 12.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL</booktitle>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="30504" citStr="Hockenmaier and Steedman, 2007" startWordPosition="4812" endWordPosition="4815"> et al. (2011), who evaluate on SRL in isolation (without sense disambiguation, as in CoNLL-2009). They report results on Prop-CCGbank (Boxwell and White, 2008), which uses the same training/testing splits as the CoNLL-2005 Shared Task. Their results are therefore loosely7 comparable to results on the CoNLL2005 dataset, which we can compare here. There is an additional complication in comparing SRL approaches directly: The CoNLL2005 dataset defines arguments as spans instead of 7The comparison is imperfect for two reasons: first, the CCGBank contains only 99.44% of the original PTB sentences (Hockenmaier and Steedman, 2007); second, because PropBank was annotated over CFGs, after converting to CCG only 99.977% of the argument spans were exact matches (Boxwell and White, 2008). However, this comparison was adopted by Boxwell et al. (2011), so we use it here. 2005 spans (oracle tree) 72.0 67.1 2008 heads 84.32 — — 2005 spans 79.44 71.5 65.0 85.93 72.9 67.3 79.90 35.0 37.8 2008 heads test train 2005 spans 1183 heads, which runs counter to our head-based syntactic representation. This creates a mismatched train/test scenario: we must train our model to predict argument heads, but then test on our models ability to p</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Dependency-based semantic role labeling of PropBank.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP 2008). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5668" citStr="Johansson and Nugues, 2008" startWordPosition="857" endWordPosition="861"> setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Johansson and Nugues (2008) and Lluis et al. (2013) extend this idea by coupling predictions of a dependency parser with predictions from a semantic role labeler. In</context>
<context position="31315" citStr="Johansson and Nugues (2008)" startWordPosition="4945" endWordPosition="4948">s adopted by Boxwell et al. (2011), so we use it here. 2005 spans (oracle tree) 72.0 67.1 2008 heads 84.32 — — 2005 spans 79.44 71.5 65.0 85.93 72.9 67.3 79.90 35.0 37.8 2008 heads test train 2005 spans 1183 heads, which runs counter to our head-based syntactic representation. This creates a mismatched train/test scenario: we must train our model to predict argument heads, but then test on our models ability to predict argument spans.8 We therefore train our models on the CoNLL-2008 argument heads,9 and post-process and convert from heads to spans using the conversion algorithm available from Johansson and Nugues (2008).10 The heads are either from an MBR tree or an oracle tree. This gives Boxwell et al. (2011) the advantage, since our syntactic dependency parses are optimized to pick out semantic argument heads, not spans. Table 5 presents our results. Boxwell et al. (2011) (B’11) uses additional supervision in the form of a CCG tag dictionary derived from supervised data with (tdc) and without (tc) a cutoff. Our model does very poorly on the ’05 spanbased evaluation because the constituent bracketing of the marginalized trees are inaccurate. This is elucidated by instead evaluating on the oracle spans, whe</context>
<context position="33385" citStr="Johansson and Nugues (2008)" startWordPosition="5282" endWordPosition="5285">gical features is different across languages, with little change in performance for Catalan and Spanish, 8We were unable to obtain the system output of Boxwell et al. (2011) in order to convert their spans to dependencies and evaluate the other mismatched train/test setting. 9CoNLL-2005, -2008, and -2009 were derived from PropBank and share the same source text; -2008 and -2009 use argument heads. 10Specifically, we use their Algorithm 2, which produces the span dominated by each argument, with special handling of the case when the argument head dominates that of the predicate. Also following Johansson and Nugues (2008), we recover the ’05 sentences missing from the ’08 evaluation set. Rem #FT ca de es – 127+32 74.46 72.62 74.23 Dep 40+32 67.43 64.24 67.18 Mor 30+32 67.84 59.78 66.94 POS 23+32 64.40 54.68 62.71 Lem 21+32 64.85 54.89 63.80 Table 6: Subtractive experiments. Each row contains the F1 for SRL only (without sense disambiguation) where the supervision type of that row and all above it have been removed. Removed supervision types (Rem) are: syntactic dependencies (Dep), morphology (Mor), POS tags (POS), and lemmas (Lem). #FT indicates the number of feature templates used (unigrams+bigrams). Language</context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based semantic role labeling of PropBank. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing (EMNLP 2008). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>CorpusBased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="5482" citStr="Klein and Manning, 2004" startWordPosition="831" endWordPosition="834">e-of-the-art performance in the lowresource setting (§ 4.4). When the models have access to observed syntactic trees, they achieve near state-of-the-art accuracy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicat</context>
<context position="7975" citStr="Klein and Manning (2004)" startWordPosition="1232" endWordPosition="1235">ag dictionary extracted from a treebank. Naradowsky et al. (2012) limit their exploration to a small set of basic features, and included high-resource supervision in the form of lemmas, POS tags, and morphology available from the CoNLL 2009 data. There has not yet been a comparison of techniques for SRL that do not rely on a syntactic treebank, and no exploration of probabilistic models for unsupervised grammar induction within an SRL pipeline that we have been able to find. Related work for the unsupervised learning of dependency structures separately from semantic roles primarily comes from Klein and Manning (2004), who introduced the Dependency Model with Valence (DMV). This is a robust generative model that uses a head-outward process over word classes, where heads generate arguments. Spitkovsky et al. (2010a) show that Viterbi (hard) EM training of the DMV with simple uniform initialization of the model parameters yields higher accuracy models than standard soft-EM 1178 Train Time, Constrained Grammar Induction: Observed Constraints Figure 1: Pipeline approach to SRL. In this simple pipeline, the first stage syntactically parses the corpus, and the second stage predicts semantic predicate-argument st</context>
<context position="12117" citStr="Klein and Manning, 2004" startWordPosition="1883" endWordPosition="1886"> model. Each word type is assigned to a finegrained cluster at a leaf of the hierarchy of clusters. Each cluster can be uniquely identified by the path from the root cluster to that leaf. Representing this path as a bit-string (with 1 indicating a left and 0 indicating a right child) allows a simple coarsening of the clusters by truncating the bit-strings. We train 1000 Brown clusters for each of the CoNLL2009 languages on Wikipedia text.2 Unsupervised Grammar Induction Our first method for grammar induction is fully unsupervised Viterbi EM training of the Dependency Model with Valence (DMV) (Klein and Manning, 2004), with uniform initialization of the model parameters. We define the DMV such that it generates sequences of word classes: either POS tags or Brown clusters as in Spitkovsky et al. (2011). The DMV is a simple generative model for projective dependency trees. Children are generated recursively for each node. Conditioned on the parent class, the direction (right or left), and the current valence (first child or not), a coin is flipped to decide whether to generate another child; the distribution over child classes is conditioned on only the parent class and direction. 2The Wikipedia text was tok</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher Manning. 2004. CorpusBased induction of syntactic structure: Models of dependency and constituency. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL 2004). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11274" citStr="Koo et al., 2008" startWordPosition="1742" endWordPosition="1745">sed data, and each subsequent component is trained using the 1-best output of the previous components. A typical pipeline consists of a POS tagger, dependency parser, and semantic role labeler. In this section, we introduce pipelines that remove the need for a supervised tagger and parser by training in an unsupervised and distantly supervised fashion. Brown Clusters We use fully unsupervised Brown clusters (Brown et al., 1992) in place of POS tags. Brown clusters have been used to good effect for various NLP tasks such as named entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008; Spitkovsky et al., 2011). The clusters are formed by a greedy hierachical clustering algorithm that finds an assignment of words to classes by maximizing the likelihood of the training data under a latent-class bigram model. Each word type is assigned to a finegrained cluster at a leaf of the hierarchy of clusters. Each cluster can be uniquely identified by the path from the root cluster to that leaf. Representing this path as a bit-string (with 1 indicating a left and 0 indicating a right child) allows a simple coarsening of the clusters by truncating the bit-strings. We train 1000 Brown cl</context>
<context position="19333" citStr="Koo et al., 2008" startWordPosition="3033" endWordPosition="3036">he observations (the sentence and labels from previous pipeline stages) which are conjoined with the state of the variables in the factor. 3.3 Features for CRF Models Our feature design stems from two key ideas. First, for SRL, it has been observed that feature bigrams (the concatenation of simple features such as a predicate’s POS tag and an argument’s word) are important for state-of-the-art (Zhao et al., 2009; Bj¨orkelund et al., 2009). Second, for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data (Koo et al., 2008). We create binary indicator features for each model using feature templates. Our feature template definitions build from those used by the top performing systems in the CoNLL-2009 Shared Task, Zhao et al. (2009) and Bj¨orkelund et al. (2009) and from features in syntactic dependency parsing (McDonald et al., 2005; Koo et al., 2008). Template Creation Feature templates are defined over triples of (property, positions, order). Properties, listed in Table 1, are extracted from word positions within the sentence, shown in Table 2. Single positions for a word wi include its syntactic parent, its l</context>
<context position="20627" citStr="Koo et al., 2008" startWordPosition="3238" endWordPosition="3241">arSib), etc. Following Zhao et al. (2009), we include the notion of verb and noun supports and sections of the dependency path. Also following Zhao et al. (2009), properties from a set of positions can be put together in three possible orders: as the given sequence, as a sorted list of unique strings, and removing all duplicated neighbored strings. We consider both template unigrams and bigrams, combining two templates in sequence. Additional templates we include are the relative position (Bj¨orkelund et al., 2009), geneological relationship, distance (Zhao et al., 2009), and binned distance (Koo et al., 2008) between two words in the path. From Llu´ıs et al. (2013), we use 1, 2,3- gram ,3- gram path features of words/POS tags (path-grams), and the number of non-consecutive token pairs in a predicate-argument path (continuity). 3.4 Feature Selection Constructing all feature template unigrams and bigrams would yield an unwieldy number of features. We therefore determine the top N template bigrams for a dataset and factor a according to an information gain measure (Martins et al., 2011): p(f, xa) p(f, xa) log2 p(f)p(xa) where Tm is the mth feature template, f is a particular instantiation of that tem</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In Proceedings of ACL-08: HLT. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings of the 18th International Conference on Machine Learning (ICML</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="14657" citStr="Lafferty et al., 2001" startWordPosition="2297" endWordPosition="2300">ny projective dependency parses—in this case, we ignore the semantic constraint for that sentence. We parse with the CKY algorithm (Younger, 1967; Aho and Ullman, 1972) by utilizing a PCFG corresponding to the DMV (Cohn et al., 2010). Each chart cell allows only non-terminals compatible with the constrained sets. This can be viewed as a variation of Pereira and Schabes (1992). Semantic Dependency Model As described above, semantic role labeling can be cast as a structured prediction problem where the structure is a labeled semantic dependency graph. We define a conditional random field (CRF) (Lafferty et al., 2001) for this task. Because each word in a sentence may be in a semantic relationship with any other word (including itself), a sentence of length n has n2 possible edges. We define a single L+1-ary variable for each edge, whose value can be any of L semantic labels or a special label indicating there is no predicate-argument relationship between the two words. In this way, we jointly perform identification (determining whether a semantic relationship exists) and classification (determining the semantic label). This use of an L+1- ary variable is in contrast to the model of Naradowsky et al. (2012</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning (ICML 2001). Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Llu´ıs</author>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Joint arc-factored parsing of syntactic and semantic dependencies.</title>
<date>2013</date>
<journal>Transactions of the Association for Computational Linguistics (TACL).</journal>
<marker>Llu´ıs, Carreras, M`arquez, 2013</marker>
<rawString>Xavier Llu´ıs, Xavier Carreras, and Llu´ıs M`arquez. 2013. Joint arc-factored parsing of syntactic and semantic dependencies. Transactions of the Association for Computational Linguistics (TACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The Penn Treebank. Computational linguistics,</title>
<date>1993</date>
<contexts>
<context position="23508" citStr="Marcus et al., 1993" startWordPosition="3694" endWordPosition="3697">useful. 4.1 Data The CoNLL-2009 Shared Task (Hajiˇc et al., 2009) dataset contains POS tags, lemmas, morphological features, syntactic dependencies, predicate senses, and semantic roles annotations for 7 languages: Catalan, Chinese, Czech, English, German, Japanese,4 Spanish. The CoNLL-2005 and -2008 Shared Task datasets provide English SRL annotation, and for cross dataset comparability we consider only verbal predicates (more details in § 4.4). To compare with prior approaches that use semantic supervision for grammar induction, we utilize Section 23 of the WSJ portion of the Penn Treebank (Marcus et al., 1993). 4.2 Feature Template Sets Our primary feature set IGC consists of 127 template unigrams that emphasize coarse properties (i.e., properties 7, 9, and 11 in Table 1). We also explore the 31 template unigrams5 IGB described 3To reduce hash collisions, We use MurmurHash v3 https://code.google.com/p/smhasher. 4We do not report results on Japanese as that data was only made freely available to researchers that competed in CoNLL 2009. 5Because we do not include a binary factor between predicate sense and semantic role, we do not include sense as a by Bj¨orkelund et al. (2009). Each of IGC and IGB a</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The Penn Treebank. Computational linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andre Martins</author>
<author>Noah Smith</author>
<author>Mario Figueiredo</author>
<author>Pedro Aguiar</author>
</authors>
<title>Structured sparsity in structured prediction.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="21111" citStr="Martins et al., 2011" startWordPosition="3317" endWordPosition="3320">tive position (Bj¨orkelund et al., 2009), geneological relationship, distance (Zhao et al., 2009), and binned distance (Koo et al., 2008) between two words in the path. From Llu´ıs et al. (2013), we use 1, 2,3- gram ,3- gram path features of words/POS tags (path-grams), and the number of non-consecutive token pairs in a predicate-argument path (continuity). 3.4 Feature Selection Constructing all feature template unigrams and bigrams would yield an unwieldy number of features. We therefore determine the top N template bigrams for a dataset and factor a according to an information gain measure (Martins et al., 2011): p(f, xa) p(f, xa) log2 p(f)p(xa) where Tm is the mth feature template, f is a particular instantiation of that template, and xa is an assignment to the variables in factor a. The probabilities are empirical estimates computed from the training data. This is simply the mutual information of the feature template instantiation with the variable assignment. This filtering approach was treated as a simple baseline in Martins et al. (2011) to contrast with increasingly popular gradient based regularization approaches. Unlike the gradient based apSBJ, NMOD, LOC, ... Up, Down linePath(wp, wc) depPat</context>
</contexts>
<marker>Martins, Smith, Figueiredo, Aguiar, 2011</marker>
<rawString>Andre Martins, Noah Smith, Mario Figueiredo, and Pedro Aguiar. 2011. Structured sparsity in structured prediction. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="19648" citStr="McDonald et al., 2005" startWordPosition="3083" endWordPosition="3086">h as a predicate’s POS tag and an argument’s word) are important for state-of-the-art (Zhao et al., 2009; Bj¨orkelund et al., 2009). Second, for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data (Koo et al., 2008). We create binary indicator features for each model using feature templates. Our feature template definitions build from those used by the top performing systems in the CoNLL-2009 Shared Task, Zhao et al. (2009) and Bj¨orkelund et al. (2009) and from features in syntactic dependency parsing (McDonald et al., 2005; Koo et al., 2008). Template Creation Feature templates are defined over triples of (property, positions, order). Properties, listed in Table 1, are extracted from word positions within the sentence, shown in Table 2. Single positions for a word wi include its syntactic parent, its leftmost farthest child (leftFarChild), its rightmost nearest sibling (rightNearSib), etc. Following Zhao et al. (2009), we include the notion of verb and noun supports and sections of the dependency path. Also following Zhao et al. (2009), properties from a set of positions can be put together in three possible or</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Jethran Guinness</author>
<author>Alex Zamanian</author>
</authors>
<title>Name tagging with word clusters and discriminative training.</title>
<date>2004</date>
<booktitle>HLT-NAACL 2004: Main Proceedings. Association for Computational Linguistics.</booktitle>
<editor>In Susan Dumais, Daniel Marcu, and Salim Roukos, editors,</editor>
<contexts>
<context position="11233" citStr="Miller et al., 2004" startWordPosition="1735" endWordPosition="1738">ere the first component is trained on supervised data, and each subsequent component is trained using the 1-best output of the previous components. A typical pipeline consists of a POS tagger, dependency parser, and semantic role labeler. In this section, we introduce pipelines that remove the need for a supervised tagger and parser by training in an unsupervised and distantly supervised fashion. Brown Clusters We use fully unsupervised Brown clusters (Brown et al., 1992) in place of POS tags. Brown clusters have been used to good effect for various NLP tasks such as named entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008; Spitkovsky et al., 2011). The clusters are formed by a greedy hierachical clustering algorithm that finds an assignment of words to classes by maximizing the likelihood of the training data under a latent-class bigram model. Each word type is assigned to a finegrained cluster at a leaf of the hierarchy of clusters. Each cluster can be uniquely identified by the path from the root cluster to that leaf. Representing this path as a bit-string (with 1 indicating a left and 0 indicating a right child) allows a simple coarsening of the clusters by truncatin</context>
</contexts>
<marker>Miller, Guinness, Zamanian, 2004</marker>
<rawString>Scott Miller, Jethran Guinness, and Alex Zamanian. 2004. Name tagging with word clusters and discriminative training. In Susan Dumais, Daniel Marcu, and Salim Roukos, editors, HLT-NAACL 2004: Main Proceedings. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Naradowsky</author>
<author>Sebastian Riedel</author>
<author>David Smith</author>
</authors>
<title>Improving NLP through marginalization of hidden syntactic structure.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP 2012). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4287" citStr="Naradowsky et al. (2012)" startWordPosition="635" endWordPosition="638">ciation for Computational Linguistics, pages 1177–1187, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics Modeling contributions: • Simpler joint CRF for syntactic and semantic dependency parsing than previously reported. • New application of unsupervised grammar induction: low-resource SRL. • Constrained grammar induction using SRL for distant-supervision. • Use of Brown clusters in place of POS tags for low-resource SRL. The pipeline models are introduced in § 3.1 and jointly-trained models for syntactic and semantic dependencies (similar in form to Naradowsky et al. (2012)) are introduced in § 3.2. In the pipeline models, we develop a novel approach to unsupervised grammar induction and explore performance using SRL as distant supervision. The joint models use a non-loopy conditional random field (CRF) with a global factor constraining latent syntactic edge variables to form a tree. Efficient exact marginal inference is possible by embedding a dynamic programming algorithm within belief propagation as in Smith and Eisner (2008). Even at the expense of no dependency path features, the joint models best pipeline-trained models for state-of-the-art performance in </context>
<context position="5741" citStr="Naradowsky et al., 2012" startWordPosition="870" endWordPosition="873">t and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Johansson and Nugues (2008) and Lluis et al. (2013) extend this idea by coupling predictions of a dependency parser with predictions from a semantic role labeler. In the model from Johansson and Nugues (2008), the outputs from an SRL pipe</context>
<context position="7042" citStr="Naradowsky et al. (2012)" startWordPosition="1084" endWordPosition="1087">m. The candidate set of syntactic-semantic structures is reranked using the probability of the syntactic tree and semantic structure. Lluis et al. (2013) use a joint arcfactored model that predicts full syntactic paths along with predicate-argument structures via dual decomposition. Boxwell et al. (2011) and Naradowsky et al. (2012) observe that syntax may be treated as latent when a treebank is not available. Boxwell et al. (2011) describe a method for training a semantic role labeler by extracting features from a packed CCG parse chart, where the parse weights are given by a simple ruleset. Naradowsky et al. (2012) marginalize over latent syntactic dependency parses. Both Boxwell et al. (2011) and Naradowsky et al. (2012) suggest methods for SRL without supervised syntax, however, their features come largely from supervised resources. Even in their lowest resource setting, Boxwell et al. (2011) require an oracle CCG tag dictionary extracted from a treebank. Naradowsky et al. (2012) limit their exploration to a small set of basic features, and included high-resource supervision in the form of lemmas, POS tags, and morphology available from the CoNLL 2009 data. There has not yet been a comparison of techn</context>
<context position="15258" citStr="Naradowsky et al. (2012)" startWordPosition="2399" endWordPosition="2403">(Lafferty et al., 2001) for this task. Because each word in a sentence may be in a semantic relationship with any other word (including itself), a sentence of length n has n2 possible edges. We define a single L+1-ary variable for each edge, whose value can be any of L semantic labels or a special label indicating there is no predicate-argument relationship between the two words. In this way, we jointly perform identification (determining whether a semantic relationship exists) and classification (determining the semantic label). This use of an L+1- ary variable is in contrast to the model of Naradowsky et al. (2012), which used a more complex Figure 2: Factor graph for the joint syntactic/semantic dependency parsing model. set of binary variables and required a constraint factor permitting AT-MOST-ONE. We include one unary factor for each variable. We optionally include additional variables that perform word sense disambiguation for each predicate. Each has a unary factor and is completely disconnected from the semantic edge (similar to Naradowsky et al. (2012)). These variables range over all the predicate senses observed in the training data for the lemma of that predicate. 3.2 Joint Syntactic and Sema</context>
<context position="17070" citStr="Naradowsky et al. (2012)" startWordPosition="2689" endWordPosition="2692">riables must be coupled so that they produce a projective dependency parse; this requires an additional global constraint factor to ensure that this is the case (Smith and Eisner, 2008). The constraint factor touches all n2 syntactic-edge variables, and multiplies in 1.0 if they form a projective dependency parse, and 0.0 otherwise. We couple each syntactic edge variable to its semantic edge variable with a binary factor. Figure 2 shows the factor graph for this joint model. Note that our factor graph does not contain any loops, thereby permitting efficient exact marginal inference just as in Naradowsky et al. (2012). We DEPTREE Role Dep 1,1 1,1 Role Dep 1,2 1,2 Role Dep 1,3 1,3 ... ... Role n,n Dep n,n 1180 Property Possible values Template Possible values 1 word form before, after, on 2 lower case word form 3 5-char word form prefixes 4 capitalization 5 top-800 word form 6 brown cluster 7 brown cluster, length 5 8 lemma 9 POS tag 10 morphological features all lower-case forms all 5-char form prefixes True, False top-800 word forms 000, 1100, 010110001, ... length 5 prefixes of brown clusters all word lemmas NNP, CD, JJ, DT, ... Gender, Case, Number,... distance, continuity binned distance geneological r</context>
<context position="24886" citStr="Naradowsky et al. (2012)" startWordPosition="3918" endWordPosition="3921">against the language-specific feature sets detailed in the literature on high-resource top-performing SRL systems: From Bj¨orkelund et al. (2009), these are feature sets for German, English, Spanish and Chinese, obtained by weeks of forward selection (Bde,en,es,zh); and from Zhao et al. (2009), these are features for Catalan Zca.6 4.3 High-resource SRL We first compare our models trained as a pipeline, using all available supervision (syntax, morphology, POS tags, lemmas) from the CoNLL-2009 data. Table 4(a) shows the results of our model with gold syntax and a richer feature set than that of Naradowsky et al. (2012), which only looked at whether a syntactic dependency edge was present. This highlights an important advantage of the pipeline trained model: the features can consider any part of the syntax (e.g., arbitrary subtrees), whereas the joint model is limited to those features over which it can efficiently marginalize (e.g., short dependency paths). This holds true even in the pipeline setting where no syntactic supervision is available. Table 4(b) contrasts our high-resource results for the task of SRL and sense disambiguation with the top systems in the CoNLL-2009 Shared Task, giving further insig</context>
<context position="26568" citStr="Naradowsky et al. (2012)" startWordPosition="4183" endWordPosition="4186">lection may improve the results. We find that IGB obtain higher F1 than the original Bj¨orkelund feature sets (Bde,en,es,zh) in the lowresource pipeline setting with constrained grammar induction (DMV+C). feature for argument prediction. 6This covers all CoNLL languages but Czech, where feature sets were not made publicly available in either work. In Czech, we disallowed template bigrams involving path-grams. 1182 SRL Approach Feature Set Dep. Parser Avg. ca cs de en es zh Pipeline IGC Gold 84.98 84.97 87.65 79.14 86.54 84.22 87.35 Pipeline IGB Gold 84.74 85.15 86.64 79.50 85.77 84.40 86.95 ) Naradowsky et al. (2012) Gold 72.73 69.59 74.84 66.49 78.55 68.93 77.97 Bj¨orkelund et al. (2009) Supervised 81.55 80.01 85.41 79.71 85.63 79.91 78.60 Zhao et al. (2009) Supervised 80.85 80.32 85.19 75.99 85.44 80.46 77.72 Pipeline IGC Supervised 78.03 76.24 83.34 74.19 81.96 76.12 76.35 ) Pipeline Z.a Supervised *77.62 77.62 — — — — — Pipeline Bde,en,es,zh Supervised *76.49 — — 72.17 81.15 76.65 75.99 Pipeline IGB Supervised 75.68 74.59 81.61 69.08 78.86 74.51 75.44 Joint IGC Marginalized 72.48 71.35 81.03 65.15 76.16 71.03 70.14 Joint IGB Marginalized 72.40 71.55 80.04 64.80 75.57 71.21 71.21 Naradowsky et al. (201</context>
<context position="28564" citStr="Naradowsky et al. (2012)" startWordPosition="4509" endWordPosition="4512">pecific feature sets (Z.a and Bde,en,es,zh), for which we show results only on the languages for which the sets were publicly available. (a (b (c ❑� PRY’08 ❑ B’11 (tdc) ❑ B’11 (td) ❑� JN’08 ❑ Joint, IGC ❑ Joint, IGB Table 5: F1 for SRL approaches (without sense disambiguation) in matched and mismatched train/test settings for CoNLL 2005 span and 2008 head supervision. We contrast low-resource (❑) and high-resource settings (0), where latter uses a treebank. See § 4.4 for caveats to this comparison. 4.4 Low-Resource SRL CoNLL-2009 Table 4(c) includes results for our low-resource approaches and Naradowsky et al. (2012) on predicting semantic roles as well as sense. In the low-resource setting of the CoNLL2009 Shared task without syntactic supervision, our joint model (Joint) with marginalized syntax obtains state-of-the-art results with features IGC described in § 4.2. This model outperforms prior work (Naradowsky et al., 2012) and our pipeline model (Pipeline) with contrained (DMV+C) and unconstrained grammar induction (DMV) trained on brown clusters (bc). In the low-resource setting, training and decoding times for the pipeline and joint methods are similar as computation time tends to be dominated by fea</context>
</contexts>
<marker>Naradowsky, Riedel, Smith, 2012</marker>
<rawString>Jason Naradowsky, Sebastian Riedel, and David Smith. 2012. Improving NLP through marginalization of hidden syntactic structure. In Proceedings of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP 2012). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
</authors>
<title>Using semantic cues to learn syntax.</title>
<date>2011</date>
<booktitle>In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI</booktitle>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="9366" citStr="Naseem and Barzilay, 2011" startWordPosition="1441" endWordPosition="1444">ses—however, it may optionally utilize the semantic parses as distant supervision. Our experiments also consider ‘longer’ pipelines that include earlier stages: a morphological analyzer, POS tagger, lemmatizer. training. In Viterbi EM, the E-step finds the maximum likelihood corpus parse given the current model parameters. The M-step then finds the maximum likelihood parameters given the corpus parse. We utilize this approach to produce unsupervised syntactic features for the SRL task. Grammar induction work has further demonstrated that distant supervision in the form of ACE-style relations (Naseem and Barzilay, 2011) or HTML markup (Spitkovsky et al., 2010b) can lead to considerable gains. Recent work in fully unsupervised dependency parsing has supplanted these methods with even higher accuracies (Spitkovsky et al., 2013) by arranging optimizers into networks that suggest informed restarts based on previously identified local optima. We do not reimplement these approaches within the SRL pipeline here, but provide comparison of these methods against our grammar induction approach in isolation in § 4.5. In both pipeline and joint models, we use features adapted from state-of-the-art approaches to SRL. This</context>
<context position="36443" citStr="Naseem and Barzilay (2011)" startWordPosition="5779" endWordPosition="5782">rown clusters (bc). *Indicates the supervised parser outputs provided by the CoNLL’09 Shared Task. WSJ∞ Distant Supervision SAJM’10 44.8 none SAJ’13 64.4 none SJA’10 50.4 HTML NB’11 59.4 ACE05 DMV (bc) 24.8 none DMV+C (bc) 44.8 SRL Marginalized, IGC 48.8 SRL Marginalized, IGB 58.9 SRL Table 8: Comparison of grammar induction approaches. We contrast the DMV trained with Viterbi EM+uniform initialization (DMV), our constrained DMV (DMV+C), and our model’s MBR decoding of latent syntax (Marginalized) with other recent work: Spitkovsky et al. (2010a) (SAJM’10), Spitkovsky et al. (2010b) (SJA’10), Naseem and Barzilay (2011) (NB’11), and the CS model of Spitkovsky et al. (2013) (SAJ’13). This suggests that refining the low-resource grammar induction methods may lead to gains in SRL. Interestingly, the marginalized grammars best the DMV grammar induction method; however, this difference is less pronounced when the DMV is constrained using SRL labels as distant supervision. This could indicate that a better model for grammar induction would result in better performance for SRL. We therefore turn to an analysis of other approaches to grammar induction in Table 8, evaluated on the Penn Treebank. We contrast with meth</context>
</contexts>
<marker>Naseem, Barzilay, 2011</marker>
<rawString>Tahira Naseem and Regina Barzilay. 2011. Using semantic cues to learn syntax. In Proceedings of the 25th AAAI Conference on Artificial Intelligence (AAAI 2011). AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Yves Schabes</author>
</authors>
<title>Insideoutside reestimation from partially bracketed corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="14413" citStr="Pereira and Schabes (1992)" startWordPosition="2259" endWordPosition="2262">e permitted. The constrained syntactic DMV parser treats the semantic graph as observed, and constrains the syntactic parent to be chosen from one of the semantic parents, if there are any. In some cases, imposing this constraint would not permit any projective dependency parses—in this case, we ignore the semantic constraint for that sentence. We parse with the CKY algorithm (Younger, 1967; Aho and Ullman, 1972) by utilizing a PCFG corresponding to the DMV (Cohn et al., 2010). Each chart cell allows only non-terminals compatible with the constrained sets. This can be viewed as a variation of Pereira and Schabes (1992). Semantic Dependency Model As described above, semantic role labeling can be cast as a structured prediction problem where the structure is a labeled semantic dependency graph. We define a conditional random field (CRF) (Lafferty et al., 2001) for this task. Because each word in a sentence may be in a semantic relationship with any other word (including itself), a sentence of length n has n2 possible edges. We define a single L+1-ary variable for each edge, whose value can be any of L semantic labels or a special label indicating there is no predicate-argument relationship between the two wor</context>
</contexts>
<marker>Pereira, Schabes, 1992</marker>
<rawString>Fernando Pereira and Yves Schabes. 1992. Insideoutside reestimation from partially bracketed corpora. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics (ACL 1992).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nugues Pierre</author>
<author>Kalep Heiki-Jaan</author>
</authors>
<title>Extended constituent-to-dependency conversion for english. NODALIDA</title>
<date>2007</date>
<note>Proceedings.</note>
<contexts>
<context position="37451" citStr="Pierre and Heiki-Jaan, 2007" startWordPosition="5934" endWordPosition="5937">a better model for grammar induction would result in better performance for SRL. We therefore turn to an analysis of other approaches to grammar induction in Table 8, evaluated on the Penn Treebank. We contrast with methods using distant supervision (Naseem and Barzilay, 2011; Spitkovsky et al., 2010b) and fully unsupervised dependency parsing (Spitkovsky et al., 2013). Following prior work, we exclude punctuation from evaluation and convert the constituency trees to dependencies.12 The approach from Spitkovsky et al. (2013) 12Naseem and Barzilay (2011) and our results use the Penn converter (Pierre and Heiki-Jaan, 2007). Spitkovsky et al. (2010b; 2013) use Collins (1999) head percolation rules. (SAJ’13) outperforms all other approaches, including our marginalized settings. We therefore may be able to achieve further gains in the pipeline model by considering better models of latent syntax, or better search techniques that break out of local optima. Similarly, improving the nonconvex optimization of our latent-variable CRF (Marginalized) may offer further gains. 5 Discussion and Future Work We have compared various approaches for lowresource semantic role labeling at the state-of-theart level. We find that we</context>
</contexts>
<marker>Pierre, Heiki-Jaan, 2007</marker>
<rawString>Nugues Pierre and Kalep Heiki-Jaan. 2007. Extended constituent-to-dependency conversion for english. NODALIDA 2007 Proceedings.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-tau Yih</author>
</authors>
<title>The importance of syntactic parsing and inference in semantic role labeling.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="32109" citStr="Punyakanok et al. (2008)" startWordPosition="5079" endWordPosition="5083">ut semantic argument heads, not spans. Table 5 presents our results. Boxwell et al. (2011) (B’11) uses additional supervision in the form of a CCG tag dictionary derived from supervised data with (tdc) and without (tc) a cutoff. Our model does very poorly on the ’05 spanbased evaluation because the constituent bracketing of the marginalized trees are inaccurate. This is elucidated by instead evaluating on the oracle spans, where our F1 scores are higher than Boxwell et al. (2011). We also contrast with relavant high-resource methods with span/head conversions from Johansson and Nugues (2008): Punyakanok et al. (2008) (PRY’08) and Johansson and Nugues (2008) (JN’08). Subtractive Study In our subsequent experiments, we study the effectiveness of our models as the available supervision is decreased. We incrementally remove dependency syntax, morphological features, POS tags, then lemmas. For these experiments, we utilize the coarse-grained feature set (IGC), which includes Brown clusters. Across languages, we find the largest drop in F1 when we remove POS tags; and we find a gain in F1 when we remove lemmas. This indicates that lemmas, which are a high-resource annotation, may not provide a significant benef</context>
</contexts>
<marker>Punyakanok, Roth, Yih, 2008</marker>
<rawString>Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008. The importance of syntactic parsing and inference in semantic role labeling. Computational Linguistics, 34(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Dependency parsing by belief propagation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2008). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4751" citStr="Smith and Eisner (2008)" startWordPosition="710" endWordPosition="713">RL. The pipeline models are introduced in § 3.1 and jointly-trained models for syntactic and semantic dependencies (similar in form to Naradowsky et al. (2012)) are introduced in § 3.2. In the pipeline models, we develop a novel approach to unsupervised grammar induction and explore performance using SRL as distant supervision. The joint models use a non-loopy conditional random field (CRF) with a global factor constraining latent syntactic edge variables to form a tree. Efficient exact marginal inference is possible by embedding a dynamic programming algorithm within belief propagation as in Smith and Eisner (2008). Even at the expense of no dependency path features, the joint models best pipeline-trained models for state-of-the-art performance in the lowresource setting (§ 4.4). When the models have access to observed syntactic trees, they achieve near state-of-the-art accuracy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Re</context>
<context position="16631" citStr="Smith and Eisner, 2008" startWordPosition="2618" endWordPosition="2621">In this section, we define a simple model for joint syntactic and semantic dependency parsing. This model extends the CRF model in Section 3.1 to include the projective syntactic dependency parse for a sentence. This is done by including an additional n2 binary variables that indicate whether or not a directed syntactic dependency edge exists between a pair of words in the sentence. Unlike the semantic dependencies, these syntactic variables must be coupled so that they produce a projective dependency parse; this requires an additional global constraint factor to ensure that this is the case (Smith and Eisner, 2008). The constraint factor touches all n2 syntactic-edge variables, and multiplies in 1.0 if they form a projective dependency parse, and 0.0 otherwise. We couple each syntactic edge variable to its semantic edge variable with a binary factor. Figure 2 shows the factor graph for this joint model. Note that our factor graph does not contain any loops, thereby permitting efficient exact marginal inference just as in Naradowsky et al. (2012). We DEPTREE Role Dep 1,1 1,1 Role Dep 1,2 1,2 Role Dep 1,3 1,3 ... ... Role n,n Dep n,n 1180 Property Possible values Template Possible values 1 word form befor</context>
</contexts>
<marker>Smith, Eisner, 2008</marker>
<rawString>David A. Smith and Jason Eisner. 2008. Dependency parsing by belief propagation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP 2008). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
<author>Christopher D Manning</author>
</authors>
<title>Viterbi training improves unsupervised dependency parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 14th Conference on Computational Natural Language Learning (CoNLL 2010). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5507" citStr="Spitkovsky et al., 2010" startWordPosition="835" endWordPosition="838">in the lowresource setting (§ 4.4). When the models have access to observed syntactic trees, they achieve near state-of-the-art accuracy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Joha</context>
<context position="8174" citStr="Spitkovsky et al. (2010" startWordPosition="1262" endWordPosition="1265">morphology available from the CoNLL 2009 data. There has not yet been a comparison of techniques for SRL that do not rely on a syntactic treebank, and no exploration of probabilistic models for unsupervised grammar induction within an SRL pipeline that we have been able to find. Related work for the unsupervised learning of dependency structures separately from semantic roles primarily comes from Klein and Manning (2004), who introduced the Dependency Model with Valence (DMV). This is a robust generative model that uses a head-outward process over word classes, where heads generate arguments. Spitkovsky et al. (2010a) show that Viterbi (hard) EM training of the DMV with simple uniform initialization of the model parameters yields higher accuracy models than standard soft-EM 1178 Train Time, Constrained Grammar Induction: Observed Constraints Figure 1: Pipeline approach to SRL. In this simple pipeline, the first stage syntactically parses the corpus, and the second stage predicts semantic predicate-argument structure for each sentence using the labels of the first stage as features. In our low-resource pipelines, we assume that the syntactic parser is given no labeled parses—however, it may optionally uti</context>
<context position="9406" citStr="Spitkovsky et al., 2010" startWordPosition="1448" endWordPosition="1451">semantic parses as distant supervision. Our experiments also consider ‘longer’ pipelines that include earlier stages: a morphological analyzer, POS tagger, lemmatizer. training. In Viterbi EM, the E-step finds the maximum likelihood corpus parse given the current model parameters. The M-step then finds the maximum likelihood parameters given the corpus parse. We utilize this approach to produce unsupervised syntactic features for the SRL task. Grammar induction work has further demonstrated that distant supervision in the form of ACE-style relations (Naseem and Barzilay, 2011) or HTML markup (Spitkovsky et al., 2010b) can lead to considerable gains. Recent work in fully unsupervised dependency parsing has supplanted these methods with even higher accuracies (Spitkovsky et al., 2013) by arranging optimizers into networks that suggest informed restarts based on previously identified local optima. We do not reimplement these approaches within the SRL pipeline here, but provide comparison of these methods against our grammar induction approach in isolation in § 4.5. In both pipeline and joint models, we use features adapted from state-of-the-art approaches to SRL. This includes Zhao et al. (2009) features, w</context>
<context position="36367" citStr="Spitkovsky et al. (2010" startWordPosition="5769" endWordPosition="5772">w-resource settings. DMV models are trained on either POS tags (pos) or Brown clusters (bc). *Indicates the supervised parser outputs provided by the CoNLL’09 Shared Task. WSJ∞ Distant Supervision SAJM’10 44.8 none SAJ’13 64.4 none SJA’10 50.4 HTML NB’11 59.4 ACE05 DMV (bc) 24.8 none DMV+C (bc) 44.8 SRL Marginalized, IGC 48.8 SRL Marginalized, IGB 58.9 SRL Table 8: Comparison of grammar induction approaches. We contrast the DMV trained with Viterbi EM+uniform initialization (DMV), our constrained DMV (DMV+C), and our model’s MBR decoding of latent syntax (Marginalized) with other recent work: Spitkovsky et al. (2010a) (SAJM’10), Spitkovsky et al. (2010b) (SJA’10), Naseem and Barzilay (2011) (NB’11), and the CS model of Spitkovsky et al. (2013) (SAJ’13). This suggests that refining the low-resource grammar induction methods may lead to gains in SRL. Interestingly, the marginalized grammars best the DMV grammar induction method; however, this difference is less pronounced when the DMV is constrained using SRL labels as distant supervision. This could indicate that a better model for grammar induction would result in better performance for SRL. We therefore turn to an analysis of other approaches to grammar</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, Manning, 2010</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, Daniel Jurafsky, and Christopher D Manning. 2010a. Viterbi training improves unsupervised dependency parsing. In Proceedings of the 14th Conference on Computational Natural Language Learning (CoNLL 2010). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Daniel Jurafsky</author>
<author>Hiyan Alshawi</author>
</authors>
<title>Profiting from mark-up: Hyper-text annotations for guided parsing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5507" citStr="Spitkovsky et al., 2010" startWordPosition="835" endWordPosition="838">in the lowresource setting (§ 4.4). When the models have access to observed syntactic trees, they achieve near state-of-the-art accuracy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Joha</context>
<context position="8174" citStr="Spitkovsky et al. (2010" startWordPosition="1262" endWordPosition="1265">morphology available from the CoNLL 2009 data. There has not yet been a comparison of techniques for SRL that do not rely on a syntactic treebank, and no exploration of probabilistic models for unsupervised grammar induction within an SRL pipeline that we have been able to find. Related work for the unsupervised learning of dependency structures separately from semantic roles primarily comes from Klein and Manning (2004), who introduced the Dependency Model with Valence (DMV). This is a robust generative model that uses a head-outward process over word classes, where heads generate arguments. Spitkovsky et al. (2010a) show that Viterbi (hard) EM training of the DMV with simple uniform initialization of the model parameters yields higher accuracy models than standard soft-EM 1178 Train Time, Constrained Grammar Induction: Observed Constraints Figure 1: Pipeline approach to SRL. In this simple pipeline, the first stage syntactically parses the corpus, and the second stage predicts semantic predicate-argument structure for each sentence using the labels of the first stage as features. In our low-resource pipelines, we assume that the syntactic parser is given no labeled parses—however, it may optionally uti</context>
<context position="9406" citStr="Spitkovsky et al., 2010" startWordPosition="1448" endWordPosition="1451">semantic parses as distant supervision. Our experiments also consider ‘longer’ pipelines that include earlier stages: a morphological analyzer, POS tagger, lemmatizer. training. In Viterbi EM, the E-step finds the maximum likelihood corpus parse given the current model parameters. The M-step then finds the maximum likelihood parameters given the corpus parse. We utilize this approach to produce unsupervised syntactic features for the SRL task. Grammar induction work has further demonstrated that distant supervision in the form of ACE-style relations (Naseem and Barzilay, 2011) or HTML markup (Spitkovsky et al., 2010b) can lead to considerable gains. Recent work in fully unsupervised dependency parsing has supplanted these methods with even higher accuracies (Spitkovsky et al., 2013) by arranging optimizers into networks that suggest informed restarts based on previously identified local optima. We do not reimplement these approaches within the SRL pipeline here, but provide comparison of these methods against our grammar induction approach in isolation in § 4.5. In both pipeline and joint models, we use features adapted from state-of-the-art approaches to SRL. This includes Zhao et al. (2009) features, w</context>
<context position="36367" citStr="Spitkovsky et al. (2010" startWordPosition="5769" endWordPosition="5772">w-resource settings. DMV models are trained on either POS tags (pos) or Brown clusters (bc). *Indicates the supervised parser outputs provided by the CoNLL’09 Shared Task. WSJ∞ Distant Supervision SAJM’10 44.8 none SAJ’13 64.4 none SJA’10 50.4 HTML NB’11 59.4 ACE05 DMV (bc) 24.8 none DMV+C (bc) 44.8 SRL Marginalized, IGC 48.8 SRL Marginalized, IGB 58.9 SRL Table 8: Comparison of grammar induction approaches. We contrast the DMV trained with Viterbi EM+uniform initialization (DMV), our constrained DMV (DMV+C), and our model’s MBR decoding of latent syntax (Marginalized) with other recent work: Spitkovsky et al. (2010a) (SAJM’10), Spitkovsky et al. (2010b) (SJA’10), Naseem and Barzilay (2011) (NB’11), and the CS model of Spitkovsky et al. (2013) (SAJ’13). This suggests that refining the low-resource grammar induction methods may lead to gains in SRL. Interestingly, the marginalized grammars best the DMV grammar induction method; however, this difference is less pronounced when the DMV is constrained using SRL labels as distant supervision. This could indicate that a better model for grammar induction would result in better performance for SRL. We therefore turn to an analysis of other approaches to grammar</context>
</contexts>
<marker>Spitkovsky, Jurafsky, Alshawi, 2010</marker>
<rawString>Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Alshawi. 2010b. Profiting from mark-up: Hyper-text annotations for guided parsing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Angel X Chang</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Unsupervised dependency parsing without gold part-of-speech tags.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="11300" citStr="Spitkovsky et al., 2011" startWordPosition="1746" endWordPosition="1749"> subsequent component is trained using the 1-best output of the previous components. A typical pipeline consists of a POS tagger, dependency parser, and semantic role labeler. In this section, we introduce pipelines that remove the need for a supervised tagger and parser by training in an unsupervised and distantly supervised fashion. Brown Clusters We use fully unsupervised Brown clusters (Brown et al., 1992) in place of POS tags. Brown clusters have been used to good effect for various NLP tasks such as named entity recognition (Miller et al., 2004) and dependency parsing (Koo et al., 2008; Spitkovsky et al., 2011). The clusters are formed by a greedy hierachical clustering algorithm that finds an assignment of words to classes by maximizing the likelihood of the training data under a latent-class bigram model. Each word type is assigned to a finegrained cluster at a leaf of the hierarchy of clusters. Each cluster can be uniquely identified by the path from the root cluster to that leaf. Representing this path as a bit-string (with 1 indicating a left and 0 indicating a right child) allows a simple coarsening of the clusters by truncating the bit-strings. We train 1000 Brown clusters for each of the CoN</context>
</contexts>
<marker>Spitkovsky, Alshawi, Chang, Jurafsky, 2011</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, Angel X. Chang, and Daniel Jurafsky. 2011. Unsupervised dependency parsing without gold part-of-speech tags. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing (EMNLP 2011). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valentin I Spitkovsky</author>
<author>Hiyan Alshawi</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Breaking out of local optima with count transforms and model recombination: A study in grammar induction.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013). Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9576" citStr="Spitkovsky et al., 2013" startWordPosition="1473" endWordPosition="1476">raining. In Viterbi EM, the E-step finds the maximum likelihood corpus parse given the current model parameters. The M-step then finds the maximum likelihood parameters given the corpus parse. We utilize this approach to produce unsupervised syntactic features for the SRL task. Grammar induction work has further demonstrated that distant supervision in the form of ACE-style relations (Naseem and Barzilay, 2011) or HTML markup (Spitkovsky et al., 2010b) can lead to considerable gains. Recent work in fully unsupervised dependency parsing has supplanted these methods with even higher accuracies (Spitkovsky et al., 2013) by arranging optimizers into networks that suggest informed restarts based on previously identified local optima. We do not reimplement these approaches within the SRL pipeline here, but provide comparison of these methods against our grammar induction approach in isolation in § 4.5. In both pipeline and joint models, we use features adapted from state-of-the-art approaches to SRL. This includes Zhao et al. (2009) features, who use feature templates from combinations of word properties, syntactic positions including head and children, and semantic properties; and features from Bj¨orkelund et </context>
<context position="36497" citStr="Spitkovsky et al. (2013)" startWordPosition="5789" endWordPosition="5792">puts provided by the CoNLL’09 Shared Task. WSJ∞ Distant Supervision SAJM’10 44.8 none SAJ’13 64.4 none SJA’10 50.4 HTML NB’11 59.4 ACE05 DMV (bc) 24.8 none DMV+C (bc) 44.8 SRL Marginalized, IGC 48.8 SRL Marginalized, IGB 58.9 SRL Table 8: Comparison of grammar induction approaches. We contrast the DMV trained with Viterbi EM+uniform initialization (DMV), our constrained DMV (DMV+C), and our model’s MBR decoding of latent syntax (Marginalized) with other recent work: Spitkovsky et al. (2010a) (SAJM’10), Spitkovsky et al. (2010b) (SJA’10), Naseem and Barzilay (2011) (NB’11), and the CS model of Spitkovsky et al. (2013) (SAJ’13). This suggests that refining the low-resource grammar induction methods may lead to gains in SRL. Interestingly, the marginalized grammars best the DMV grammar induction method; however, this difference is less pronounced when the DMV is constrained using SRL labels as distant supervision. This could indicate that a better model for grammar induction would result in better performance for SRL. We therefore turn to an analysis of other approaches to grammar induction in Table 8, evaluated on the Penn Treebank. We contrast with methods using distant supervision (Naseem and Barzilay, 20</context>
</contexts>
<marker>Spitkovsky, Alshawi, Jurafsky, 2013</marker>
<rawString>Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2013. Breaking out of local optima with count transforms and model recombination: A study in grammar induction. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing (EMNLP 2013). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="5639" citStr="Toutanova et al., 2005" startWordPosition="853" endWordPosition="856">acy in the high-resource setting on some languages (§ 4.3). Examining the learning curve of the joint and pipeline models in two languages demonstrates that a small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Johansson and Nugues (2008) and Lluis et al. (2013) extend this idea by coupling predictions of a dependency parser with predictions fro</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher Manning. 2005. Joint learning improves semantic role labeling. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics (ACL 2005). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilian Weinberger</author>
<author>Anirban Dasgupta</author>
<author>John Langford</author>
<author>Alex Smola</author>
<author>Josh Attenberg</author>
</authors>
<title>Feature hashing for large scale multitask learning.</title>
<date>2009</date>
<booktitle>In L´eon Bottou and Michael Littman, editors, Proceedings of the 26th Annual International Conference on Machine Learning (ICML 2009). Omnipress.</booktitle>
<contexts>
<context position="22113" citStr="Weinberger et al., 2009" startWordPosition="3482" endWordPosition="3485">ach was treated as a simple baseline in Martins et al. (2011) to contrast with increasingly popular gradient based regularization approaches. Unlike the gradient based apSBJ, NMOD, LOC, ... Up, Down linePath(wp, wc) depPath(wp, wc) depPath(wp, wlca) depPath(wc, wlca) depPath(wlca, wroot) �IGa,m = E f∈Tm xa 1181 proaches, this filtering approach easily scales to many features since we can decompose the memory usage over feature templates. As an additional speedup, we reduce the dimensionality of our feature space to 1 million for each clique using a common trick referred to as feature hashing (Weinberger et al., 2009): we map each feature instantiation to an integer using a hash function3 modulo the desired dimentionality. 4 Experiments We are interested in the effects of varied supervision using pipeline and joint training for SRL. To compare to prior work (i.e., submissions to the CoNLL-2009 Shared Task), we also consider the joint task of semantic role labeling and predicate sense disambiguation. Our experiments are subtractive, beginning with all supervision available and then successively removing (a) dependency syntax, (b) morphological features, (c) POS tags, and (d) lemmas. Dependency syntax is the</context>
</contexts>
<marker>Weinberger, Dasgupta, Langford, Smola, Attenberg, 2009</marker>
<rawString>Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola, and Josh Attenberg. 2009. Feature hashing for large scale multitask learning. In L´eon Bottou and Michael Littman, editors, Proceedings of the 26th Annual International Conference on Machine Learning (ICML 2009). Omnipress.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel H Younger</author>
</authors>
<title>Recognition and parsing of context-free languages in time n3.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="14180" citStr="Younger, 1967" startWordPosition="2220" endWordPosition="2221">ld the argument. The label on the edge indicates the type of semantic relationship. Unlike syntactic dependency parsing, the graph is not required to be a tree, nor even a connected graph. Self-loops and crossing arcs are permitted. The constrained syntactic DMV parser treats the semantic graph as observed, and constrains the syntactic parent to be chosen from one of the semantic parents, if there are any. In some cases, imposing this constraint would not permit any projective dependency parses—in this case, we ignore the semantic constraint for that sentence. We parse with the CKY algorithm (Younger, 1967; Aho and Ullman, 1972) by utilizing a PCFG corresponding to the DMV (Cohn et al., 2010). Each chart cell allows only non-terminals compatible with the constrained sets. This can be viewed as a variation of Pereira and Schabes (1992). Semantic Dependency Model As described above, semantic role labeling can be cast as a structured prediction problem where the structure is a labeled semantic dependency graph. We define a conditional random field (CRF) (Lafferty et al., 2001) for this task. Because each word in a sentence may be in a semantic relationship with any other word (including itself), a</context>
</contexts>
<marker>Younger, 1967</marker>
<rawString>Daniel H. Younger. 1967. Recognition and parsing of context-free languages in time n3. Information and Control, 10(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Wenliang Chen</author>
<author>Chunyu Kity</author>
<author>Guodong Zhou</author>
</authors>
<title>Multilingual dependency learning: A huge feature engineering method to semantic dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL</booktitle>
<contexts>
<context position="1622" citStr="Zhao et al., 2009" startWordPosition="244" endWordPosition="247">arbitrary graphical structure.1 1 Introduction The goal of semantic role labeling (SRL) is to identify predicates and arguments and label their semantic contribution in a sentence. Such labeling defines who did what to whom, when, where and how. For example, in the sentence “The kids ran the marathon”, ran assigns a role to kids to denote that they are the runners; and a role to marathon to denote that it is the race course. Models for SRL have increasingly come to rely on an array of NLP tools (e.g., parsers, lemmatizers) in order to obtain state-of-the-art results (Bj¨orkelund et al., 2009; Zhao et al., 2009). Each tool is typically trained on hand-annotated data, thus placing SRL at the end of a very highresource NLP pipeline. However, richly annotated data such as that provided in parsing treebanks is expensive to produce, and may be tied to specific domains (e.g., newswire). Many languages do 1http://www.cs.jhu.edu/˜mrg/software/ not have such supervised resources (low-resource languages), which makes exploring SRL crosslinguistically difficult. The problem of SRL for low-resource languages is an important one to solve, as solutions pave the way for a wide range of applications: Accurate identi</context>
<context position="5793" citStr="Zhao et al., 2009" startWordPosition="879" endWordPosition="882">small number of labeled SRL examples may be essential for good end-task performance, but that the choice of a good model for grammar induction has an even greater impact. 2 Related Work Our work builds upon research in both semantic role labeling and unsupervised grammar induction (Klein and Manning, 2004; Spitkovsky et al., 2010a). Previous related approaches to semantic role labeling include joint classification of semantic arguments (Toutanova et al., 2005; Johansson and Nugues, 2008), latent syntax induction (Boxwell et al., 2011; Naradowsky et al., 2012), and feature engineering for SRL (Zhao et al., 2009; Bj¨orkelund et al., 2009). Toutanova et al. (2005) introduced one of the first joint approaches for SRL and demonstrated that a model that scores the full predicateargument structure of a parse tree could lead to significant error reduction over independent classifiers for each predicate-argument relation. Johansson and Nugues (2008) and Lluis et al. (2013) extend this idea by coupling predictions of a dependency parser with predictions from a semantic role labeler. In the model from Johansson and Nugues (2008), the outputs from an SRL pipeline are reranked based on the full predicateargumen</context>
<context position="9994" citStr="Zhao et al. (2009)" startWordPosition="1538" endWordPosition="1541">arkup (Spitkovsky et al., 2010b) can lead to considerable gains. Recent work in fully unsupervised dependency parsing has supplanted these methods with even higher accuracies (Spitkovsky et al., 2013) by arranging optimizers into networks that suggest informed restarts based on previously identified local optima. We do not reimplement these approaches within the SRL pipeline here, but provide comparison of these methods against our grammar induction approach in isolation in § 4.5. In both pipeline and joint models, we use features adapted from state-of-the-art approaches to SRL. This includes Zhao et al. (2009) features, who use feature templates from combinations of word properties, syntactic positions including head and children, and semantic properties; and features from Bj¨orkelund et al. (2009), who utilize features on syntactic siblings and the dependency path concatenated with the direction of each edge. Features are described further in § 3.3. 3 Approaches We consider an array of models, varying: 1. Pipeline vs. joint training (Figures 1 and 2) 2. Types of supervision 3. The objective function at the level of syntax 3.1 Unsupervised Syntax in the Pipeline Typical SRL systems are trained foll</context>
<context position="19131" citStr="Zhao et al., 2009" startWordPosition="3001" endWordPosition="3004">daptive learning rate (AdaGrad) (Duchi et al., 2011) over mini-batches. The unary and binary factors are defined with exponential family potentials. In the next section, we consider binary features of the observations (the sentence and labels from previous pipeline stages) which are conjoined with the state of the variables in the factor. 3.3 Features for CRF Models Our feature design stems from two key ideas. First, for SRL, it has been observed that feature bigrams (the concatenation of simple features such as a predicate’s POS tag and an argument’s word) are important for state-of-the-art (Zhao et al., 2009; Bj¨orkelund et al., 2009). Second, for syntactic dependency parsing, combining Brown cluster features with word forms or POS tags yields high accuracy even with little training data (Koo et al., 2008). We create binary indicator features for each model using feature templates. Our feature template definitions build from those used by the top performing systems in the CoNLL-2009 Shared Task, Zhao et al. (2009) and Bj¨orkelund et al. (2009) and from features in syntactic dependency parsing (McDonald et al., 2005; Koo et al., 2008). Template Creation Feature templates are defined over triples o</context>
<context position="20587" citStr="Zhao et al., 2009" startWordPosition="3231" endWordPosition="3234">), its rightmost nearest sibling (rightNearSib), etc. Following Zhao et al. (2009), we include the notion of verb and noun supports and sections of the dependency path. Also following Zhao et al. (2009), properties from a set of positions can be put together in three possible orders: as the given sequence, as a sorted list of unique strings, and removing all duplicated neighbored strings. We consider both template unigrams and bigrams, combining two templates in sequence. Additional templates we include are the relative position (Bj¨orkelund et al., 2009), geneological relationship, distance (Zhao et al., 2009), and binned distance (Koo et al., 2008) between two words in the path. From Llu´ıs et al. (2013), we use 1, 2,3- gram ,3- gram path features of words/POS tags (path-grams), and the number of non-consecutive token pairs in a predicate-argument path (continuity). 3.4 Feature Selection Constructing all feature template unigrams and bigrams would yield an unwieldy number of features. We therefore determine the top N template bigrams for a dataset and factor a according to an information gain measure (Martins et al., 2011): p(f, xa) p(f, xa) log2 p(f)p(xa) where Tm is the mth feature template, f i</context>
<context position="24556" citStr="Zhao et al. (2009)" startWordPosition="3863" endWordPosition="3866">09. 5Because we do not include a binary factor between predicate sense and semantic role, we do not include sense as a by Bj¨orkelund et al. (2009). Each of IGC and IGB also include 32 template bigrams selected by information gain on 1000 sentences—we select a different set of template bigrams for each dataset. We compare against the language-specific feature sets detailed in the literature on high-resource top-performing SRL systems: From Bj¨orkelund et al. (2009), these are feature sets for German, English, Spanish and Chinese, obtained by weeks of forward selection (Bde,en,es,zh); and from Zhao et al. (2009), these are features for Catalan Zca.6 4.3 High-resource SRL We first compare our models trained as a pipeline, using all available supervision (syntax, morphology, POS tags, lemmas) from the CoNLL-2009 data. Table 4(a) shows the results of our model with gold syntax and a richer feature set than that of Naradowsky et al. (2012), which only looked at whether a syntactic dependency edge was present. This highlights an important advantage of the pipeline trained model: the features can consider any part of the syntax (e.g., arbitrary subtrees), whereas the joint model is limited to those feature</context>
<context position="26713" citStr="Zhao et al. (2009)" startWordPosition="4207" endWordPosition="4210">e setting with constrained grammar induction (DMV+C). feature for argument prediction. 6This covers all CoNLL languages but Czech, where feature sets were not made publicly available in either work. In Czech, we disallowed template bigrams involving path-grams. 1182 SRL Approach Feature Set Dep. Parser Avg. ca cs de en es zh Pipeline IGC Gold 84.98 84.97 87.65 79.14 86.54 84.22 87.35 Pipeline IGB Gold 84.74 85.15 86.64 79.50 85.77 84.40 86.95 ) Naradowsky et al. (2012) Gold 72.73 69.59 74.84 66.49 78.55 68.93 77.97 Bj¨orkelund et al. (2009) Supervised 81.55 80.01 85.41 79.71 85.63 79.91 78.60 Zhao et al. (2009) Supervised 80.85 80.32 85.19 75.99 85.44 80.46 77.72 Pipeline IGC Supervised 78.03 76.24 83.34 74.19 81.96 76.12 76.35 ) Pipeline Z.a Supervised *77.62 77.62 — — — — — Pipeline Bde,en,es,zh Supervised *76.49 — — 72.17 81.15 76.65 75.99 Pipeline IGB Supervised 75.68 74.59 81.61 69.08 78.86 74.51 75.44 Joint IGC Marginalized 72.48 71.35 81.03 65.15 76.16 71.03 70.14 Joint IGB Marginalized 72.40 71.55 80.04 64.80 75.57 71.21 71.21 Naradowsky et al. (2012) Marginalized 71.27 67.99 73.16 67.26 76.12 66.74 76.32 ) Pipeline IGC DMV+C (bc) 70.08 68.21 79.63 62.25 73.81 68.73 67.86 Pipeline Z.a DMV+C </context>
</contexts>
<marker>Zhao, Chen, Kity, Zhou, 2009</marker>
<rawString>Hai Zhao, Wenliang Chen, Chunyu Kity, and Guodong Zhou. 2009. Multilingual dependency learning: A huge feature engineering method to semantic dependency parsing. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>