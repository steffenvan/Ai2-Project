<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000311">
<title confidence="0.961841">
A Study of Convolution Tree Kernel with Local Alignment
</title>
<author confidence="0.996925">
Lidan Zhang
</author>
<affiliation confidence="0.9637375">
Department of Computer Science
HKU, Hong Kong
</affiliation>
<email confidence="0.934811">
lzhang@cs.hku.hk
</email>
<author confidence="0.976976">
Kwok-Ping Chan
</author>
<affiliation confidence="0.959471">
Department of Computer Science
HKU, Hong Kong
</affiliation>
<email confidence="0.956733">
kpchan@cs.hku.hk
</email>
<sectionHeader confidence="0.991438" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999817090909091">
This paper discusses a new convolu-
tion tree kernel by introducing local
alignments. The main idea of the new
kernel is to allow some syntactic al-
ternations during each match between
subtrees. In this paper, we give an
algorithm to calculate the composite
kernel. The experiment results show
promising improvements on two tasks:
semantic role labeling and question
classification.
</bodyText>
<sectionHeader confidence="0.99816" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999103403225807">
Recently kernel-based methods have become
a state-of-art technique and been widely used
in natural language processing applications.
In this method, a key problem is how to de-
sign a proper kernel function in terms of dif-
ferent data representations. So far, there are
two kinds of data representations. One is to
encode an object with a flat vector whose ele-
ment correspond to an extracted feature from
the object. However the feature vector is sen-
sitive to the structural variations. The ex-
traction schema is heavily dependent on dif-
ferent problems. On the other hand, kernel
function can be directly calculated on the ob-
ject. The advantages are that the original
topological information is to a large extent
preserved and the introduction of additional
noise may be avoided. Thus structure-based
kernels can well model syntactic parse tree
in a variety of applications, such as relation
extraction(Zelenko et al., 2003), named en-
tity recognition(Culotta and Sorensen, 2004),
semantic role labeling(Moschitti et al., 2008)
and so on.
To compute the structural kernel function,
Haussler (1999) introduced a general type of
kernel function, called“ Convolution kernel”.
Based on this work, Collins and Duffy (2002)
proposed a tree kernel calculation by count-
ing the common subtrees. In other words,
two trees are considered if and only if these
two trees are exactly same. In real sentences,
some structural alternations within a given
phrase are permitted without changing its us-
age. Therefore, Moschitti (2004) proposed
partial trees to partially match between sub-
trees. Kashima and Koyanagi (2002) general-
ize the tree kernel to labeled order tree kernel
with more flexible match. And from the idea
of introducing linguistical knowledge, Zhang
et al. (2007) proposed a grammar-driven tree
kernel, in which two subtrees are same if and
only if the corresponding two productions are
in the same manually defined set. In addi-
tion, the problem of hard matching can be al-
leviated by processing or mapping the trees.
For example, Tai mapping (Kuboyama et al.,
2006) generalized the kernel from counting
subtrees to counting the function of mapping.
Moreover multi-source knowledge can benefit
kernel calculation, such as using dependency
information to dynamically determine the tree
span (Qian et al., 2008).
In this paper, we propose a tree kernel cal-
culation algorithm by allowing variations in
productions. The variation is measured with
local alignment score between two derivative
POS sequences. To reduce the computation
complexity, we use the dynamic programming
algorithm to compute the score of any align-
ment. And the top n alignments are consid-
ered in the kernel.
</bodyText>
<note confidence="0.949534">
Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 25–32,
Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.998436">
25
</page>
<bodyText confidence="0.998799842105263">
Another problem in Collins and Duffy’s
tree kernel is context-free. It does not con-
sider any semantic information located at the
leaf nodes of the parsing trees. To lexicalized
tree kernel, Bloehdorn et al. (2007) consid-
ered the associated term similarity by virtue
of WordNet. Shen et al. (2003) constructed a
separate lexical feature containing words on a
given path and merged into the kernel in linear
combination.
The paper is organized as follows. In sec-
tion 2, we describe the commonly used tree
kernel. In section 3, we propose our method
to make use of the local alignment informa-
tion in kernel calculation. Section 4 presents
the results of our experiments for two differ-
ent applications ( Semantic Role Labeling and
Question Classification). Finally section 5
provides our conclusions.
</bodyText>
<sectionHeader confidence="0.877066" genericHeader="method">
2 Convolution Tree Kernel
</sectionHeader>
<bodyText confidence="0.99935575">
The main idea of tree kernel is to count
the number of common subtrees between
two trees T1 and T2. In convolutional
tree kernel (Collins and Duffy, 2002), a
tree(T) is represented as a vector h(T) =
(h1(T), ..., hi(T), ..., hn(T)), where hi(T) is
the number of occurrences of the ith tree frag-
ment in the tree T. Since the number of sub-
trees is exponential with the parse tree size,
it is infeasible to directly count the common
subtrees. To reduce the computation complex-
ity, a recursive kernel calculation algorithm
was presented. Given two trees T1 and T2,
where, NT1 and NT2 are the sets of all nodes in
trees T1 and T2, respectively. Ii(n) is the indi-
cator function to be 1 if i-th subtree is rooted
at node n and 0 otherwise. And 4(n1, n2) is
the number of common subtrees rooted at n1
and n2. It can be computed efficiently accord-
ing to the following rules:
</bodyText>
<listItem confidence="0.965139">
(1) If the productions at n1 and n2 are differ-
ent, 4(n1, n2) = 0
(2) If the productions at n1 and n2 are same,
and n1 and n2 are pre-terminals, then
</listItem>
<equation confidence="0.87519325">
4(n1, n2) = A
(3) Else, 4(n1, n2) = A flnc(n1)
� (1 +
4(ch(n1, j), ch(n2, j)))
</equation>
<bodyText confidence="0.982951181818182">
where nc(n1) is the number of children of
n1 in the tree. Note that n1 = n2 be-
cause the productions at n1 and n2 are same.
ch(n1, j) represents the jth child of node
n1. And 0 &lt; A ≤ 1 is the parameter
to downweight the contribution of larger tree
fragments to the kernel. It corresponds to
K(T1,T2) = Ei Asizeihi(T1)hi(T2), where
sizei is the number of rules in the i’th frag-
ment. The time complexity of computing this
kernel is O(|NT1 |· |NT2|).
</bodyText>
<sectionHeader confidence="0.959445" genericHeader="method">
3 Tree Kernel with Local Alignment
</sectionHeader>
<subsectionHeader confidence="0.720571">
3.1 General Framework
</subsectionHeader>
<bodyText confidence="0.999976818181818">
As we referred, one of problems in the ba-
sic tree kernel is its hard match between two
rules. In other words, at each tree level,
the two subtrees are required to be perfectly
equal. However, in real sentences, some
modifiers can be added into a phrase with-
out changing the phrase’s function. For ex-
ample, two sentences are given in Figure 1.
Considering “A1” role, the similarities be-
tween two subtrees(in circle) are 0 in (Collins
and Duffy, 2002), because the productions
“NP→DT ADJP NN” and “NP→DT NN”
are not identical. From linguistical point of
view, the adjective phrase is optional in real
sentences, which does not change the corre-
sponding semantic role. Thus the modifier
components(like “ADJP” in the above exam-
ple) should be neglected in similarity compar-
isons.
To make the hard match flexible, we can
align two string sequences derived from the
same node. Considering the above example,
</bodyText>
<equation confidence="0.997169285714286">
K(T1, T2) = &lt; h(T1), h(T2) &gt; (1)
�= hi(T1)hi(T2)
i
�= ( � �Ii(n1) Ii(n2))
i n1ENT1 n2ENT2
�= E 4(n1, n2)
n1ENT1 n2ENT2
</equation>
<page confidence="0.995293">
26
</page>
<figure confidence="0.999876666666667">
S S S
NP
VP
NP
VP
NP
VP
Richard
NNP
A0
AUX
has VBN
taken
DT
VP
ADJP
NP
NN
v
a
RBR
JJ
approach
more
audience-friendly
Richard
has VBN
taken
has VBN
NP
Richard
NP
DT
a approach
NN
taken
DT
a
NULL
approach
NN
A1
A1
VP
AUX
AUX
VP
NNP
NNP
A1
(a) (b) (c)
</figure>
<figureCaption confidence="0.999988">
Figure 1: Syntactic parse tree with “A1” semantic role
</figureCaption>
<bodyText confidence="0.9994823125">
an alignment might be “DT ADJP NN” vs
“DT - NN”, by inserting a symbol(-). The
symbol(-) corresponds to a “NULL” subtree
in the parser tree. And the “NULL” subtree
can be regarded as a null character in the sen-
tence, see Figure 1(c).
Convolution kernels, studied in (Haussler,
1999) gave the framework to construct a com-
plex kernel from its simple elements. Suppose
x E X can be decomposed into x1,..., xm �
�x. Let R be a relation over X1 x ... x Xm x X
such that R(x) is true iff x1, ..., xm are parts
of x. R−1(x) = 14R(x, x)}, which returns
all components. For example, x is any string,
then x� can be its characters. The convolution
kernel K is defined as:
</bodyText>
<equation confidence="0.88530675">
X
K(x, y) =
9∈R−1(x),y∈R−1(y)
(2)
</equation>
<bodyText confidence="0.999833625">
Considering our problem, for example, a
derived string sequence x by the rule “n1 —*
x”. R(xi, x) is true iff xi appears in the right
hand of x. Given two POS sequences x and
y derived from two nodes n1 and n2, respec-
tively, A(x, y) denotes all the possible align-
ments of the sequence. The general form of
the kernel with local alignment is defined as:
</bodyText>
<equation confidence="0.9996425">
XK0(n1, n2) = K(ni1, nj2) (3)
(i,j)∈A(x,y)
nc(n1,i)
XO0(n1, n2) = A AS(i,j) Y
(i,j)∈A(x,y) d=1
(1 + O0(ch(n1, i, d), ch(n2, j, d))
</equation>
<bodyText confidence="0.999039333333334">
where, (i, j) denotes the ith and jth variation
for x and y, AS(i,j) is the score for alignment i
and j. And ch(n1, i, d) selects the dth subtree
for the ith aligned schema of node n1.
It is easily to prove the above kernel is pos-
itive semi-definite, since the kernel K(ni1, nj2)
is positive semi-definite. The native computa-
tion is impractical because the number of all
possible alignments(|A(x, y)|) is exponential
with respect to |x |and |y|. In the next sec-
tion, we will discuss how to calculate AS(i,j)
for each alignment.
</bodyText>
<subsectionHeader confidence="0.998934">
3.2 Local Alignment Kernel
</subsectionHeader>
<bodyText confidence="0.9998684">
The local alignment(LA) kernel was usually
used in bioinformatics, to compare the sim-
ilarity between two protein sequences(x and
y) by exploring their alignments(Saigo et al.,
2004).
</bodyText>
<equation confidence="0.9973185">
KLA(x, y) = X expO`(x,y,&apos;r) (4)
7r∈A(x,y)
</equation>
<bodyText confidence="0.9984924">
where &gt; 0 is a parameter, A(x, y) denotes
all possible local alignments between x and y,
and s(x, y, 7r) is the local alignment score for
a given alignment schema 7r, which is equal
to:
</bodyText>
<equation confidence="0.9955596">
S(x&amp;quot;ri1, y&amp;quot;i �)−
|r|−1 X �g(�i+1
1 − 7ri1) + g(�i+1
2 − 7ri2)] (5)
j=1
</equation>
<bodyText confidence="0.9998526">
In equation( 5), S is a substitution matrix, and
g is a gap penalty function. The alignment
score is the sum of the substitution score be-
tween the correspondence at the aligned posi-
tion, minus the sum of the gap penalty for the
</bodyText>
<equation confidence="0.9913586">
m
Y Kd(xd, yd)
d=1
s(x, y, 7r) = X |7r|
i=1
</equation>
<page confidence="0.969708">
27
</page>
<bodyText confidence="0.999784727272727">
case that ‘-’ symbol is inserted. In natural lan-
guage processing, the substitution matrix can
be selected as identity matrix and no penalty
is accounted.
Obviously, the direct computation of the
original KLA is not practical. Saigo (2004)
presented a dynamic programming algorithm
with time complexity O(|x|·|y|). In this paper,
this dynamic algorithm is used to compute the
kernel matrix, whose element(i, j) is used as
AS(i,j) measurement in equation(3).
</bodyText>
<subsectionHeader confidence="0.9951">
3.3 Local Alignment Tree Kernel
</subsectionHeader>
<bodyText confidence="0.901575333333333">
Now we embed the above local alignment
score into the general tree kernel computation.
Equation(3) can be re-written into following:
</bodyText>
<equation confidence="0.9996944">
E4&apos; (n1, n2) = � (expas(x,y,7r)×
7rEA(x,y)
nc(nl,i)
H (1 + 4&apos;(ch(n1, i, k), ch(n2, j, k))))
k=1
</equation>
<bodyText confidence="0.99985775">
To further reduce the computation com-
plexity, a threshold (�) is used to filter out
alignments with low scores. This can help to
avoid over-generated subtrees and only select
the significant alignments. In other words,
by using the threshold (�), we can select the
salient subtree variations for kernels. The fi-
nal kernel calculation is shown below:
</bodyText>
<equation confidence="0.9994435">
4&apos; (n1, n2) = A E (6,3s(x,y,7r)×
7r ∈ A(x, y)
s(x, y, 7r) &gt; �
nc(nl,i)
H (1 + 4&apos;(ch(n1, i, k), ch(n2, j, k))))
k=1
</equation>
<bodyText confidence="0.998058">
After filtering, the kernel is still positive
semi-definite. This can be easily proved using
the theorem in (Shin and Kuboyama, 2008),
since this subset selection is transitive. More
specifically, if s(x, y, 7r) &gt; A s(y, z, 7r&apos;) &gt;
�, then s(x, z, 7r + 7r&apos;) &gt; �.
The algorithm to compute the local align-
ment tree kernel is given in algorithm 1. For
any two nodes pair(xi and yj), the local align-
ment score M(xi, yj) is assigned. In the ker-
nel matrix calculation, the worst case occurs
when the tree is balanced and most of the
alignments are selected.
Algorithm 1 algorithm for local alignment
tree kernel
Require: 2 nodes n1,n2 in parse trees;The
</bodyText>
<equation confidence="0.839126266666667">
productions are n1 → x1, ..., x. and n2 →
y1, ..., yn
return 4&apos;(n1, n2)
if n1 and n2 are not same then
4&apos;(n1, n2) = 0
else
if both n1 and n2 are pre-terminals then
4&apos;(n1, n2) = 1
else
calculate kernel matrix by equation( 4)
for each possible alignment do
calculate 4&apos;(n1, n2) by equation(7)
end for
end if
end if
</equation>
<sectionHeader confidence="0.996068" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.8988455">
4.1 Semantic Role Labeling
4.1.1 Experiment Setup
</subsectionHeader>
<bodyText confidence="0.9994187">
We use the CoNLL-2005 SRL shared task
data(Carreras and Marquez, 2005) as our ex-
perimental data. It is from the Wall Street
Journal part of the Penn Treebank, together
with predicate-arguments information from
the PropBank. According to the shared task,
sections 02-21 are used for training, section
24 for development and section 23 as well as
some data from Brown corpus are left for test.
The data sets are described in Table 1.
</bodyText>
<table confidence="0.9826492">
Sentences Arguments
Training 39,832 239,858
Dev 1,346 8,346
Test WSJ 1,346 8,346
Brown 450 2,350
</table>
<tableCaption confidence="0.999509">
Table 1: Data sets statistics
</tableCaption>
<page confidence="0.998633">
28
</page>
<bodyText confidence="0.999073307692308">
Considering the two steps in semantic role
labeling, i.e. semantic role identification and
recognition. We assume identification has
been done correctly, and only consider the
semantic role classification. In our experi-
ment, we focus on the semantic classes in-
clude 6 core (A0-A5), 12 adjunct(AM-) and
8 reference(R-) arguments.
In our implementation, SVM-Light-TK1
(Moschitti, 2004) is modified. For SVM
multi-classifier, the ONE-vs-ALL (OVA)
strategy is selected. In all, we prepare the data
for each semantic role (r) as following:
</bodyText>
<listItem confidence="0.8825187">
(1) Given a sentence and its correct full syn-
tactic parse tree;
(2) Let P be the predicate. Its potential argu-
ments A are extracted according to (Xue
and Palmer, 2004)
(3) For each pair &lt; p, a &gt;E P x A: if a
covers exactly the words of semantic role
of p, put minimal subtree &lt; p, a &gt; into
positive example set (Tr); else put it in
the negative examples (T,−)
</listItem>
<bodyText confidence="0.889988">
In our experiments, we set 0 = 0.5.
</bodyText>
<subsectionHeader confidence="0.675337">
4.1.2 Experimental Results
</subsectionHeader>
<bodyText confidence="0.979197">
The classification performance is evalu-
ated with respect to accuracy, precision(p),
recall(r) and F1 = 2pr/(p + r).
</bodyText>
<table confidence="0.9856778">
Accuracy(%)
(Collins and Duffy, 2002) 84.35
(Moschitti, 2004) 86.72
(Zhang et al., 2007) 87.96
Our Kernel 88.48
</table>
<tableCaption confidence="0.9855605">
Table 2: Performance comparison between
different kernel performance on WSJ data
</tableCaption>
<footnote confidence="0.967788">
1http://dit.unitn.it/ moschitt/Tree-Kernel.htm
</footnote>
<table confidence="0.9999040625">
P(%) R(%) F,3=1
Development 81.03 68.91 74.48
WSJ Test 84.97 79.45 82.11
Brown Test 76.95 70.94 73.51
WSJ+Brown 82.98 75.40 79.01
WSJ P(%) R(%) F
A0 81.28 83.90 82.56
A1 84.22 66.39 74.25
A2 77.27 62.36 69.02
A3 93.33 21.21 34.57
A4 82.61 51.35 63.33
A5 100.00 40.00 57.41
AM-ADV 74.21 56.21 63.92
AM-CAU 75.00 46.09 57.09
AM-DIR 57.14 16.00 25.00
AM-DIS 77.78 70.00 73.68
AM-EXT 75.00 53.10 62.18
AM-LOC 89.66 74.83 81.57
AM-MNR 84.62 48.20 61.41
AM-MOD 96.64 92.00 94.26
AM-NEG 99.30 95.30 97.26
AM-PNC 48.20 28.31 35.67
AM-PRD 50.00 30.00 37.50
AM-TMP 87.87 73.43 80.00
R-A0 81.08 67.80 73.85
R-A1 77.50 49.60 60.49
R-A2 58.00 42.67 49.17
R-AM-CAU 100.00 25.00 40.00
R-AM-EXT 100.00 100.00 100.00
R-AM-LOC 100.00 55.00 70.97
R-AM-MNR 50.00 25.00 33.33
R-AM-TMP 85.71 52.94 65.46
</table>
<tableCaption confidence="0.996887">
Table 3: top: overall performance result on
</tableCaption>
<bodyText confidence="0.953185571428571">
data sets ; bottom: detail result on WSJ
data
Table 2 compares the performance of our
method and other three famous kernels on
WSJ test data. We implemented these three
methods with the same settings described
in the papers. It shows that our kernel
achieves the best performance with 88.48%
accuracy. The advantages of our approach
are: 1). the alignments allow soft syntactic
structure match; 2). threshold can avoid over-
generation and selected salient alignments.
Table 3 gives our performance on data sets
and the detail result on WSJ test data.
</bodyText>
<page confidence="0.999476">
29
</page>
<tableCaption confidence="0.997469">
Table 4: popular semantic similarity measurements
</tableCaption>
<equation confidence="0.855493666666667">
Similarity
Definition
SimWUP(c1, c2)
2dep(lso(c1,c2))
=
d(c1,lso(c1,c2))+d(c2,lso(c1,c2))+2dep(lso(c1,c2))
Wu and Palmer
SimRES(c1, c2) = − log P(lSO(c1, c2))
2 log P(lso(c1,c2))
SimLIN(c1, c2) = log P(c1)+log P(c2)
Resnik
Lin
</equation>
<subsectionHeader confidence="0.9482335">
4.2 Question Classification
4.2.1 Semantic-enriched Tree Kernel
</subsectionHeader>
<bodyText confidence="0.997364371428571">
Another problem in the tree kernel (Collins
and Duffy, 2002) is the lack of semantic in-
formation, since the match stops at the pre-
terminals. All the lexical information is en-
coded at the leaf nodes of parsing trees. How-
ever, the semantic knowledge is important in
some text applications, like Question Classi-
fication. To introduce semantic similarities
between words into our kernel, we use the
framework in Bloehdorn et al. (2007) and
rewrite the rule (2) in the iterative tree kernel
calculation(in section 2).
(2) If the productions at n1 and
n2 are same, and n1 and n2 are
pre-terminals, then 0(n1, n2) =
Aαkw(w1, w2)
where w1 and w2 are two words derived from
pre-terminals n1 and n2, respectively, and the
parameter α is to control the contribution of
the leaves. Note that each preterminal has
one child or equally covers one word. So
kw(w1, w2) actually calculate the similarity
between two words w1 and w2.
In general, there are two ways to mea-
sure the semantic similarities. One is to de-
rive from semantic networks such as Word-
Net (Mavroeidis et al., 2005; Bloehdorn et
al., 2006). The other way is to use statisti-
cal methods of distributional or co-occurrence
( O´ S´eaghdha and Copestake, 2008) behavior
of the words.
WordNet2 can be regarded as direct graphs
semantically linking concepts by means of
relations. Table 4 gives some similarity
measures between two arbitrary concepts c1
</bodyText>
<footnote confidence="0.849845">
2http://wordnet.princeton.edu/
</footnote>
<bodyText confidence="0.999197409090909">
and c2. For our application, the word-to-
word similarity can be obtained by maximiz-
ing the corresponding concept-based similar-
ity scores. In our implementation, we use
WordNet::Similarity package3(Patwardhan et
al., 2003) and the noun hierarchy of WordNet.
In Table 4, dep is the length of path from a
node to its global root, lSO(c1, c2) represents
the lowest super-ordinate of c1 and c2. The
detail definitions can be found in (Budanitsky
and Hirst, 2006) .
As an alternative, Latent Semantic Anal-
ysis(LSA) is a technique. It calculates the
words similarities by means of occurrence
of terms in documents. Given a term-by-
document matrix X, its singular value decom-
position is: X = UEVT, where E is a diago-
nal matrix with singular values in decreasing
arrangement. The column of U are singular
vectors corresponding to the individual singu-
lar value. Then the latent semantic similarity
kernel of terms ti and tj is:
</bodyText>
<equation confidence="0.679784">
SimLSA =G Uik(Ujk)T &gt; (8)
</equation>
<bodyText confidence="0.998874555555556">
where Uk = IkU is to project U onto its first
k dimensions. Ik is the identity matrix whose
first k diagonal elements are 1 and all the other
elements are 0. And Uik is the i-th row of
the matrix Uk. From equation (8), the LSA-
based similarity between two terms is the in-
ner product of the two projected vectors. The
details of LSA can be found in (Cristianini et
al., 2002; Choi et al., 2001).
</bodyText>
<subsectionHeader confidence="0.796223">
4.2.2 Experiment Results
</subsectionHeader>
<bodyText confidence="0.9994825">
In this set of experiment, we evaluate differ-
ent types of kernels for Question Classifica-
tion(QC) task. The duty of QC is to cat-
egorize questions into different classes. In
</bodyText>
<footnote confidence="0.94901">
3http://search.cpan.org/dist/WordNet-Similarity
</footnote>
<page confidence="0.991579">
30
</page>
<table confidence="0.99940475">
Accuracy(%) 1000 2000 3000 4000 5500
BOW 77.1 83.3 87.2 87.3 89.2
TK 80.2 86.2 87.4 88.6 91.2
LATK 80.4 86.5 87.5 88.8 91.6
WUP 81.3 87.3 88.0 89.8 92.5
α = 1 RES 81.0 87.1 87.9 89.5 92.2
LIN 81.1 87.0 88.0 89.3 92.4
LSA(k = 50) 80.8 86.9 87.8 89.3 91.7
</table>
<tableCaption confidence="0.999797">
Table 5: Classification accuracy of different kernels on different data sets
</tableCaption>
<bodyText confidence="0.999917">
this paper we use the same dataset as intro-
duced in(Li and Roth, 2002). The dataset is
divided4 into 5500 questions for training and
500 questions from TREC 20 for testing. The
total training samples are randomly divided
into 5 subsets with sizes 1,000, 2,000, 3,000,
4,000 and 5,500 respectively. All the ques-
tions are labeled into 6 coarse grained cate-
gories and 50 fine grained categories: Abbre-
viations (abbreviation and expansion), Entity
(animal, body, color, creation, currency, med-
ical, event, food, instrument, language, let-
ter, plant, product, religion, sport, substance,
symbol, technique, term, vehicle, word), De-
scription (definition, description, manner, rea-
son), Human (description, group, individual,
title), Location (city, country, mountain, state)
and Numeric (code, count, date, distance,
money, order, percent, period, speed, temper-
ature, size, weight).
In this paper, we compare the linear ker-
nel based on bag-of-word (BOW), the original
tree kernel (TK), the local alignment tree ker-
nel (section 3, LATK) and its correspondences
with LSA similarity and a set of semantic-
enriched LATK with different similarity met-
rics.
To obtain the parse tree, we use Charniak
parser5 for every question. Like the previ-
ous experiment, SVM-Light-TK software and
the OVA strategy are implemented. In all ex-
periments, we use the default parameter in
SVM(e.g. margin parameter) and set α = 1.
In LSA model, we set k = 50. Finally, we
use multi-classification accuracy to evaluate
</bodyText>
<footnote confidence="0.9971445">
4http://l2r.cs.uiuc.edu/ cogcomp/Data/QA/QC/
5ftp://ftp.cs.brown.edu/pub/nlparser/
</footnote>
<bodyText confidence="0.993349">
the performance.
Table 5 gives the results of the experiments.
We can see that the local alignment tree ker-
nel increase the multi-classification accuracy
of the basic tree kernel by about 0.4%. The
introduction of semantic information further
improves accuracy. Among WordNet-based
metrics, “Wu and Palmer” metric achieves
the best result, i.e. 92.5%. As a whole,
the WordNet-based similarities perform better
than LSA-based measurement.
</bodyText>
<sectionHeader confidence="0.998547" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999967263157895">
In this paper, we propose a tree kernel calcula-
tion by allowing local alignments. More flex-
ible productions are considered in line with
modifiers in real sentences. Considering text
related applications, words similarities have
been merged into the presented tree kernel.
These similarities can be derived from dif-
ferent WordNet-based metrics or document
statistics. Finally experiments are carried on
two different applications (Semantic Role La-
beling and Question Classification).
For further work, we plan to study exploit-
ing semantic knowledge in the kernel. A
promising direction is to study the different
effects of these semantic similarities. We are
interested in some distributional similarities
(Lee, 1999) given certain context. Also the
effectivenss of the semantic-enriched tree ker-
nel in SRL is another problem.
</bodyText>
<sectionHeader confidence="0.99605" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.794637333333333">
Stephan Bloehdorn, Roberto Basili, Marco Cammisa,
and Alessandro Moschitti. 2006. Semantic kernels
for text classification based on topological measures
</reference>
<page confidence="0.99921">
31
</page>
<reference confidence="0.999715371428571">
of feature similarity. In ICDM ’06: Proceedings of
the Sixth International Conference on Data Mining,
pages 808–812, Washington, DC, USA. IEEE Com-
puter Society.
Alexander Budanitsky and Graeme Hirst. 2006. Eval-
uating wordnet-based measures of lexical semantic
relatedness. Computational Linguistics, 32(1):13–
47.
X. Carreras and L. Marquez. 2005. Introduction to the
conll-2005 shared task: Semantic role labeling. In
CoNLL ’05: Proceedings of the 9th Conference on
Computational Natural Language Learning.
Freddy Y. Y. Choi, Peter Wiemer-hastings, and Johanna
Moore. 2001. Latent semantic analysis for text
segmentation. In In Proceedings of EMNLP, pages
109–117.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In
ACL, pages 263–270.
Nello Cristianini, John Shawe-Taylor, and Huma
Lodhi. 2002. Latent semantic kernels. J. Intell.
Inf. Syst., 18(2-3):127–152.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In ACL ’04:
Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 423–
429, Morristown, NJ, USA. Association for Com-
putational Linguistics.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical report.
Tetsuji Kuboyama, Kilho Shin, and Hisashi Kashima.
2006. Flexible tree kernels based on counting the
number of tree mappings. In ECML/PKDD Work-
shop on Mining and Learning with Graphs.
Lillian Lee. 1999. Measures of distributional similar-
ity. In 37th Annual Meeting of the Association for
Computational Linguistics, pages 25–32.
Xin Li and Dan Roth. 2002. Learning question clas-
sifiers. In Proceedings of the 19th international
conference on Computational linguistics, pages 1–
7, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Dimitrios Mavroeidis, George Tsatsaronis, Michalis
Vazirgiannis, Martin Theobald, and Gerhard
Weikum. 2005. Word sense disambiguation for
exploiting hierarchical thesauri in text classification.
In Al´ıpio Jorge, Lu´ıs Torgo, Pavel Brazdil, Rui
Camacho, and Gama Joao, editors, Knowledge
discovery in databases: PKDD 2005 : 9th Eu-
ropean Conference on Principles and Practice
of Knowledge Discovery in Databases, volume
3721 of Lecture Notes in Computer Science, pages
181–192, Porto, Portugal. Springer.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role label-
ing. Comput. Linguist., 34(2):193–224.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In ACL ’04:
Proceedings of the 42nd Annual Meeting on Asso-
ciation for Computational Linguistics, pages 335–
342, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Diarmuid O´ S´eaghdha and Ann Copestake. 2008. Se-
mantic classification with distributional kernels. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
649–656, Manchester, UK, August. Coling 2008 Or-
ganizing Committee.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2003. Using measures of semantic re-
latedness for word sense disambiguation. In In Pro-
ceedings of the Fourth International Conference on
Intelligent Text Processing and Computational Lin-
guistics (CICLING-03), pages 241–257.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming
Zhu, and Peide Qian. 2008. Exploiting constituent
dependencies for tree kernel-based semantic relation
extraction. In Proceedings of the 22nd International
Conference on Computational Linguistics (Coling
2008), pages 697–704, Manchester, UK, August.
Coling 2008 Organizing Committee.
Hiroto Saigo, Jean-Philippe Vert, Nobuhisa Ueda, and
Tatsuya Akutsu. 2004. Protein homology detec-
tion using string alignment kernels. Bioinformatics,
20(11):1682–1689.
Kilho Shin and Tetsuji Kuboyama. 2008. A gener-
alization of haussler’s convolution kernel: mapping
kernel. In ICML, pages 944–951.
Nianwen Xue and Martha Palmer. 2004. Calibrat-
ing features for semantic role labeling. In Dekang
Lin and Dekai Wu, editors, Proceedings of EMNLP
2004, pages 88–94, Barcelona, Spain, July. Associ-
ation for Computational Linguistics.
Dmitry Zelenko, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. J. Mach. Learn. Res., 3:1083–1106.
Min Zhang, Wanxiang Che, Aiti Aw, Chew Lim Tan,
Guodong Zhou, Ting Liu, and Sheng Li. 2007.
A grammar-driven convolution tree kernel for se-
mantic role classification. In Proceedings of the
45th Annual Meeting of the Association of Compu-
tational Linguistics, pages 200–207, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
</reference>
<page confidence="0.999295">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.307810">
<title confidence="0.999803">A Study of Convolution Tree Kernel with Local Alignment</title>
<author confidence="0.943138">Lidan</author>
<affiliation confidence="0.991045">Department of Computer</affiliation>
<address confidence="0.926432">HKU, Hong</address>
<email confidence="0.712927">lzhang@cs.hku.hkKwok-Ping</email>
<affiliation confidence="0.993339">Department of Computer</affiliation>
<address confidence="0.932409">HKU, Hong</address>
<email confidence="0.989909">kpchan@cs.hku.hk</email>
<abstract confidence="0.987861833333333">This paper discusses a new convolution tree kernel by introducing local alignments. The main idea of the new kernel is to allow some syntactic alternations during each match between subtrees. In this paper, we give an algorithm to calculate the composite kernel. The experiment results show promising improvements on two tasks: semantic role labeling and question classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Stephan Bloehdorn</author>
<author>Roberto Basili</author>
<author>Marco Cammisa</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Semantic kernels for text classification based on topological measures of feature similarity.</title>
<date>2006</date>
<booktitle>In ICDM ’06: Proceedings of the Sixth International Conference on Data Mining,</booktitle>
<pages>808--812</pages>
<publisher>IEEE Computer Society.</publisher>
<location>Washington, DC, USA.</location>
<contexts>
<context position="16607" citStr="Bloehdorn et al., 2006" startWordPosition="2840" endWordPosition="2843">rative tree kernel calculation(in section 2). (2) If the productions at n1 and n2 are same, and n1 and n2 are pre-terminals, then 0(n1, n2) = Aαkw(w1, w2) where w1 and w2 are two words derived from pre-terminals n1 and n2, respectively, and the parameter α is to control the contribution of the leaves. Note that each preterminal has one child or equally covers one word. So kw(w1, w2) actually calculate the similarity between two words w1 and w2. In general, there are two ways to measure the semantic similarities. One is to derive from semantic networks such as WordNet (Mavroeidis et al., 2005; Bloehdorn et al., 2006). The other way is to use statistical methods of distributional or co-occurrence ( O´ S´eaghdha and Copestake, 2008) behavior of the words. WordNet2 can be regarded as direct graphs semantically linking concepts by means of relations. Table 4 gives some similarity measures between two arbitrary concepts c1 2http://wordnet.princeton.edu/ and c2. For our application, the word-toword similarity can be obtained by maximizing the corresponding concept-based similarity scores. In our implementation, we use WordNet::Similarity package3(Patwardhan et al., 2003) and the noun hierarchy of WordNet. In Ta</context>
</contexts>
<marker>Bloehdorn, Basili, Cammisa, Moschitti, 2006</marker>
<rawString>Stephan Bloehdorn, Roberto Basili, Marco Cammisa, and Alessandro Moschitti. 2006. Semantic kernels for text classification based on topological measures of feature similarity. In ICDM ’06: Proceedings of the Sixth International Conference on Data Mining, pages 808–812, Washington, DC, USA. IEEE Computer Society.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Budanitsky</author>
<author>Graeme Hirst</author>
</authors>
<title>Evaluating wordnet-based measures of lexical semantic relatedness.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<pages>47</pages>
<contexts>
<context position="17402" citStr="Budanitsky and Hirst, 2006" startWordPosition="2963" endWordPosition="2966">direct graphs semantically linking concepts by means of relations. Table 4 gives some similarity measures between two arbitrary concepts c1 2http://wordnet.princeton.edu/ and c2. For our application, the word-toword similarity can be obtained by maximizing the corresponding concept-based similarity scores. In our implementation, we use WordNet::Similarity package3(Patwardhan et al., 2003) and the noun hierarchy of WordNet. In Table 4, dep is the length of path from a node to its global root, lSO(c1, c2) represents the lowest super-ordinate of c1 and c2. The detail definitions can be found in (Budanitsky and Hirst, 2006) . As an alternative, Latent Semantic Analysis(LSA) is a technique. It calculates the words similarities by means of occurrence of terms in documents. Given a term-bydocument matrix X, its singular value decomposition is: X = UEVT, where E is a diagonal matrix with singular values in decreasing arrangement. The column of U are singular vectors corresponding to the individual singular value. Then the latent semantic similarity kernel of terms ti and tj is: SimLSA =G Uik(Ujk)T &gt; (8) where Uk = IkU is to project U onto its first k dimensions. Ik is the identity matrix whose first k diagonal eleme</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Alexander Budanitsky and Graeme Hirst. 2006. Evaluating wordnet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13– 47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Carreras</author>
<author>L Marquez</author>
</authors>
<title>Introduction to the conll-2005 shared task: Semantic role labeling.</title>
<date>2005</date>
<booktitle>In CoNLL ’05: Proceedings of the 9th Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="11991" citStr="Carreras and Marquez, 2005" startWordPosition="2087" endWordPosition="2090">orst case occurs when the tree is balanced and most of the alignments are selected. Algorithm 1 algorithm for local alignment tree kernel Require: 2 nodes n1,n2 in parse trees;The productions are n1 → x1, ..., x. and n2 → y1, ..., yn return 4&apos;(n1, n2) if n1 and n2 are not same then 4&apos;(n1, n2) = 0 else if both n1 and n2 are pre-terminals then 4&apos;(n1, n2) = 1 else calculate kernel matrix by equation( 4) for each possible alignment do calculate 4&apos;(n1, n2) by equation(7) end for end if end if 4 Experiments 4.1 Semantic Role Labeling 4.1.1 Experiment Setup We use the CoNLL-2005 SRL shared task data(Carreras and Marquez, 2005) as our experimental data. It is from the Wall Street Journal part of the Penn Treebank, together with predicate-arguments information from the PropBank. According to the shared task, sections 02-21 are used for training, section 24 for development and section 23 as well as some data from Brown corpus are left for test. The data sets are described in Table 1. Sentences Arguments Training 39,832 239,858 Dev 1,346 8,346 Test WSJ 1,346 8,346 Brown 450 2,350 Table 1: Data sets statistics 28 Considering the two steps in semantic role labeling, i.e. semantic role identification and recognition. We a</context>
</contexts>
<marker>Carreras, Marquez, 2005</marker>
<rawString>X. Carreras and L. Marquez. 2005. Introduction to the conll-2005 shared task: Semantic role labeling. In CoNLL ’05: Proceedings of the 9th Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
<author>Peter Wiemer-hastings</author>
<author>Johanna Moore</author>
</authors>
<title>Latent semantic analysis for text segmentation. In</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>109--117</pages>
<contexts>
<context position="18280" citStr="Choi et al., 2001" startWordPosition="3124" endWordPosition="3127"> with singular values in decreasing arrangement. The column of U are singular vectors corresponding to the individual singular value. Then the latent semantic similarity kernel of terms ti and tj is: SimLSA =G Uik(Ujk)T &gt; (8) where Uk = IkU is to project U onto its first k dimensions. Ik is the identity matrix whose first k diagonal elements are 1 and all the other elements are 0. And Uik is the i-th row of the matrix Uk. From equation (8), the LSAbased similarity between two terms is the inner product of the two projected vectors. The details of LSA can be found in (Cristianini et al., 2002; Choi et al., 2001). 4.2.2 Experiment Results In this set of experiment, we evaluate different types of kernels for Question Classification(QC) task. The duty of QC is to categorize questions into different classes. In 3http://search.cpan.org/dist/WordNet-Similarity 30 Accuracy(%) 1000 2000 3000 4000 5500 BOW 77.1 83.3 87.2 87.3 89.2 TK 80.2 86.2 87.4 88.6 91.2 LATK 80.4 86.5 87.5 88.8 91.6 WUP 81.3 87.3 88.0 89.8 92.5 α = 1 RES 81.0 87.1 87.9 89.5 92.2 LIN 81.1 87.0 88.0 89.3 92.4 LSA(k = 50) 80.8 86.9 87.8 89.3 91.7 Table 5: Classification accuracy of different kernels on different data sets this paper we use </context>
</contexts>
<marker>Choi, Wiemer-hastings, Moore, 2001</marker>
<rawString>Freddy Y. Y. Choi, Peter Wiemer-hastings, and Johanna Moore. 2001. Latent semantic analysis for text segmentation. In In Proceedings of EMNLP, pages 109–117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>263--270</pages>
<contexts>
<context position="1825" citStr="Collins and Duffy (2002)" startWordPosition="278" endWordPosition="281"> be directly calculated on the object. The advantages are that the original topological information is to a large extent preserved and the introduction of additional noise may be avoided. Thus structure-based kernels can well model syntactic parse tree in a variety of applications, such as relation extraction(Zelenko et al., 2003), named entity recognition(Culotta and Sorensen, 2004), semantic role labeling(Moschitti et al., 2008) and so on. To compute the structural kernel function, Haussler (1999) introduced a general type of kernel function, called“ Convolution kernel”. Based on this work, Collins and Duffy (2002) proposed a tree kernel calculation by counting the common subtrees. In other words, two trees are considered if and only if these two trees are exactly same. In real sentences, some structural alternations within a given phrase are permitted without changing its usage. Therefore, Moschitti (2004) proposed partial trees to partially match between subtrees. Kashima and Koyanagi (2002) generalize the tree kernel to labeled order tree kernel with more flexible match. And from the idea of introducing linguistical knowledge, Zhang et al. (2007) proposed a grammar-driven tree kernel, in which two su</context>
<context position="4451" citStr="Collins and Duffy, 2002" startWordPosition="699" endWordPosition="702">on a given path and merged into the kernel in linear combination. The paper is organized as follows. In section 2, we describe the commonly used tree kernel. In section 3, we propose our method to make use of the local alignment information in kernel calculation. Section 4 presents the results of our experiments for two different applications ( Semantic Role Labeling and Question Classification). Finally section 5 provides our conclusions. 2 Convolution Tree Kernel The main idea of tree kernel is to count the number of common subtrees between two trees T1 and T2. In convolutional tree kernel (Collins and Duffy, 2002), a tree(T) is represented as a vector h(T) = (h1(T), ..., hi(T), ..., hn(T)), where hi(T) is the number of occurrences of the ith tree fragment in the tree T. Since the number of subtrees is exponential with the parse tree size, it is infeasible to directly count the common subtrees. To reduce the computation complexity, a recursive kernel calculation algorithm was presented. Given two trees T1 and T2, where, NT1 and NT2 are the sets of all nodes in trees T1 and T2, respectively. Ii(n) is the indicator function to be 1 if i-th subtree is rooted at node n and 0 otherwise. And 4(n1, n2) is the </context>
<context position="6344" citStr="Collins and Duffy, 2002" startWordPosition="1057" endWordPosition="1060"> sizei is the number of rules in the i’th fragment. The time complexity of computing this kernel is O(|NT1 |· |NT2|). 3 Tree Kernel with Local Alignment 3.1 General Framework As we referred, one of problems in the basic tree kernel is its hard match between two rules. In other words, at each tree level, the two subtrees are required to be perfectly equal. However, in real sentences, some modifiers can be added into a phrase without changing the phrase’s function. For example, two sentences are given in Figure 1. Considering “A1” role, the similarities between two subtrees(in circle) are 0 in (Collins and Duffy, 2002), because the productions “NP→DT ADJP NN” and “NP→DT NN” are not identical. From linguistical point of view, the adjective phrase is optional in real sentences, which does not change the corresponding semantic role. Thus the modifier components(like “ADJP” in the above example) should be neglected in similarity comparisons. To make the hard match flexible, we can align two string sequences derived from the same node. Considering the above example, K(T1, T2) = &lt; h(T1), h(T2) &gt; (1) �= hi(T1)hi(T2) i �= ( � �Ii(n1) Ii(n2)) i n1ENT1 n2ENT2 �= E 4(n1, n2) n1ENT1 n2ENT2 26 S S S NP VP NP VP NP VP Ri</context>
<context position="13594" citStr="Collins and Duffy, 2002" startWordPosition="2356" endWordPosition="2359">e data for each semantic role (r) as following: (1) Given a sentence and its correct full syntactic parse tree; (2) Let P be the predicate. Its potential arguments A are extracted according to (Xue and Palmer, 2004) (3) For each pair &lt; p, a &gt;E P x A: if a covers exactly the words of semantic role of p, put minimal subtree &lt; p, a &gt; into positive example set (Tr); else put it in the negative examples (T,−) In our experiments, we set 0 = 0.5. 4.1.2 Experimental Results The classification performance is evaluated with respect to accuracy, precision(p), recall(r) and F1 = 2pr/(p + r). Accuracy(%) (Collins and Duffy, 2002) 84.35 (Moschitti, 2004) 86.72 (Zhang et al., 2007) 87.96 Our Kernel 88.48 Table 2: Performance comparison between different kernel performance on WSJ data 1http://dit.unitn.it/ moschitt/Tree-Kernel.htm P(%) R(%) F,3=1 Development 81.03 68.91 74.48 WSJ Test 84.97 79.45 82.11 Brown Test 76.95 70.94 73.51 WSJ+Brown 82.98 75.40 79.01 WSJ P(%) R(%) F A0 81.28 83.90 82.56 A1 84.22 66.39 74.25 A2 77.27 62.36 69.02 A3 93.33 21.21 34.57 A4 82.61 51.35 63.33 A5 100.00 40.00 57.41 AM-ADV 74.21 56.21 63.92 AM-CAU 75.00 46.09 57.09 AM-DIR 57.14 16.00 25.00 AM-DIS 77.78 70.00 73.68 AM-EXT 75.00 53.10 62.18</context>
<context position="15577" citStr="Collins and Duffy, 2002" startWordPosition="2660" endWordPosition="2663">ch are: 1). the alignments allow soft syntactic structure match; 2). threshold can avoid overgeneration and selected salient alignments. Table 3 gives our performance on data sets and the detail result on WSJ test data. 29 Table 4: popular semantic similarity measurements Similarity Definition SimWUP(c1, c2) 2dep(lso(c1,c2)) = d(c1,lso(c1,c2))+d(c2,lso(c1,c2))+2dep(lso(c1,c2)) Wu and Palmer SimRES(c1, c2) = − log P(lSO(c1, c2)) 2 log P(lso(c1,c2)) SimLIN(c1, c2) = log P(c1)+log P(c2) Resnik Lin 4.2 Question Classification 4.2.1 Semantic-enriched Tree Kernel Another problem in the tree kernel (Collins and Duffy, 2002) is the lack of semantic information, since the match stops at the preterminals. All the lexical information is encoded at the leaf nodes of parsing trees. However, the semantic knowledge is important in some text applications, like Question Classification. To introduce semantic similarities between words into our kernel, we use the framework in Bloehdorn et al. (2007) and rewrite the rule (2) in the iterative tree kernel calculation(in section 2). (2) If the productions at n1 and n2 are same, and n1 and n2 are pre-terminals, then 0(n1, n2) = Aαkw(w1, w2) where w1 and w2 are two words derived </context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron. In ACL, pages 263–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nello Cristianini</author>
<author>John Shawe-Taylor</author>
<author>Huma Lodhi</author>
</authors>
<title>Latent semantic kernels.</title>
<date>2002</date>
<journal>J. Intell. Inf. Syst.,</journal>
<pages>18--2</pages>
<contexts>
<context position="18260" citStr="Cristianini et al., 2002" startWordPosition="3120" endWordPosition="3123">ere E is a diagonal matrix with singular values in decreasing arrangement. The column of U are singular vectors corresponding to the individual singular value. Then the latent semantic similarity kernel of terms ti and tj is: SimLSA =G Uik(Ujk)T &gt; (8) where Uk = IkU is to project U onto its first k dimensions. Ik is the identity matrix whose first k diagonal elements are 1 and all the other elements are 0. And Uik is the i-th row of the matrix Uk. From equation (8), the LSAbased similarity between two terms is the inner product of the two projected vectors. The details of LSA can be found in (Cristianini et al., 2002; Choi et al., 2001). 4.2.2 Experiment Results In this set of experiment, we evaluate different types of kernels for Question Classification(QC) task. The duty of QC is to categorize questions into different classes. In 3http://search.cpan.org/dist/WordNet-Similarity 30 Accuracy(%) 1000 2000 3000 4000 5500 BOW 77.1 83.3 87.2 87.3 89.2 TK 80.2 86.2 87.4 88.6 91.2 LATK 80.4 86.5 87.5 88.8 91.6 WUP 81.3 87.3 88.0 89.8 92.5 α = 1 RES 81.0 87.1 87.9 89.5 92.2 LIN 81.1 87.0 88.0 89.3 92.4 LSA(k = 50) 80.8 86.9 87.8 89.3 91.7 Table 5: Classification accuracy of different kernels on different data set</context>
</contexts>
<marker>Cristianini, Shawe-Taylor, Lodhi, 2002</marker>
<rawString>Nello Cristianini, John Shawe-Taylor, and Huma Lodhi. 2002. Latent semantic kernels. J. Intell. Inf. Syst., 18(2-3):127–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>423--429</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="1587" citStr="Culotta and Sorensen, 2004" startWordPosition="243" endWordPosition="246"> whose element correspond to an extracted feature from the object. However the feature vector is sensitive to the structural variations. The extraction schema is heavily dependent on different problems. On the other hand, kernel function can be directly calculated on the object. The advantages are that the original topological information is to a large extent preserved and the introduction of additional noise may be avoided. Thus structure-based kernels can well model syntactic parse tree in a variety of applications, such as relation extraction(Zelenko et al., 2003), named entity recognition(Culotta and Sorensen, 2004), semantic role labeling(Moschitti et al., 2008) and so on. To compute the structural kernel function, Haussler (1999) introduced a general type of kernel function, called“ Convolution kernel”. Based on this work, Collins and Duffy (2002) proposed a tree kernel calculation by counting the common subtrees. In other words, two trees are considered if and only if these two trees are exactly same. In real sentences, some structural alternations within a given phrase are permitted without changing its usage. Therefore, Moschitti (2004) proposed partial trees to partially match between subtrees. Kas</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 423– 429, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Haussler</author>
</authors>
<title>Convolution kernels on discrete structures.</title>
<date>1999</date>
<tech>Technical report.</tech>
<contexts>
<context position="1705" citStr="Haussler (1999)" startWordPosition="262" endWordPosition="263">tions. The extraction schema is heavily dependent on different problems. On the other hand, kernel function can be directly calculated on the object. The advantages are that the original topological information is to a large extent preserved and the introduction of additional noise may be avoided. Thus structure-based kernels can well model syntactic parse tree in a variety of applications, such as relation extraction(Zelenko et al., 2003), named entity recognition(Culotta and Sorensen, 2004), semantic role labeling(Moschitti et al., 2008) and so on. To compute the structural kernel function, Haussler (1999) introduced a general type of kernel function, called“ Convolution kernel”. Based on this work, Collins and Duffy (2002) proposed a tree kernel calculation by counting the common subtrees. In other words, two trees are considered if and only if these two trees are exactly same. In real sentences, some structural alternations within a given phrase are permitted without changing its usage. Therefore, Moschitti (2004) proposed partial trees to partially match between subtrees. Kashima and Koyanagi (2002) generalize the tree kernel to labeled order tree kernel with more flexible match. And from th</context>
<context position="7504" citStr="Haussler, 1999" startWordPosition="1275" endWordPosition="1276">E 4(n1, n2) n1ENT1 n2ENT2 26 S S S NP VP NP VP NP VP Richard NNP A0 AUX has VBN taken DT VP ADJP NP NN v a RBR JJ approach more audience-friendly Richard has VBN taken has VBN NP Richard NP DT a approach NN taken DT a NULL approach NN A1 A1 VP AUX AUX VP NNP NNP A1 (a) (b) (c) Figure 1: Syntactic parse tree with “A1” semantic role an alignment might be “DT ADJP NN” vs “DT - NN”, by inserting a symbol(-). The symbol(-) corresponds to a “NULL” subtree in the parser tree. And the “NULL” subtree can be regarded as a null character in the sentence, see Figure 1(c). Convolution kernels, studied in (Haussler, 1999) gave the framework to construct a complex kernel from its simple elements. Suppose x E X can be decomposed into x1,..., xm � �x. Let R be a relation over X1 x ... x Xm x X such that R(x) is true iff x1, ..., xm are parts of x. R−1(x) = 14R(x, x)}, which returns all components. For example, x is any string, then x� can be its characters. The convolution kernel K is defined as: X K(x, y) = 9∈R−1(x),y∈R−1(y) (2) Considering our problem, for example, a derived string sequence x by the rule “n1 —* x”. R(xi, x) is true iff xi appears in the right hand of x. Given two POS sequences x and y derived f</context>
</contexts>
<marker>Haussler, 1999</marker>
<rawString>David Haussler. 1999. Convolution kernels on discrete structures. Technical report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tetsuji Kuboyama</author>
<author>Kilho Shin</author>
<author>Hisashi Kashima</author>
</authors>
<title>Flexible tree kernels based on counting the number of tree mappings.</title>
<date>2006</date>
<booktitle>In ECML/PKDD Workshop on Mining and Learning with Graphs.</booktitle>
<contexts>
<context position="2672" citStr="Kuboyama et al., 2006" startWordPosition="417" endWordPosition="420">ase are permitted without changing its usage. Therefore, Moschitti (2004) proposed partial trees to partially match between subtrees. Kashima and Koyanagi (2002) generalize the tree kernel to labeled order tree kernel with more flexible match. And from the idea of introducing linguistical knowledge, Zhang et al. (2007) proposed a grammar-driven tree kernel, in which two subtrees are same if and only if the corresponding two productions are in the same manually defined set. In addition, the problem of hard matching can be alleviated by processing or mapping the trees. For example, Tai mapping (Kuboyama et al., 2006) generalized the kernel from counting subtrees to counting the function of mapping. Moreover multi-source knowledge can benefit kernel calculation, such as using dependency information to dynamically determine the tree span (Qian et al., 2008). In this paper, we propose a tree kernel calculation algorithm by allowing variations in productions. The variation is measured with local alignment score between two derivative POS sequences. To reduce the computation complexity, we use the dynamic programming algorithm to compute the score of any alignment. And the top n alignments are considered in th</context>
</contexts>
<marker>Kuboyama, Shin, Kashima, 2006</marker>
<rawString>Tetsuji Kuboyama, Kilho Shin, and Hisashi Kashima. 2006. Flexible tree kernels based on counting the number of tree mappings. In ECML/PKDD Workshop on Mining and Learning with Graphs.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>25--32</pages>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In 37th Annual Meeting of the Association for Computational Linguistics, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics,</booktitle>
<volume>1</volume>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="18932" citStr="Li and Roth, 2002" startWordPosition="3237" endWordPosition="3240"> set of experiment, we evaluate different types of kernels for Question Classification(QC) task. The duty of QC is to categorize questions into different classes. In 3http://search.cpan.org/dist/WordNet-Similarity 30 Accuracy(%) 1000 2000 3000 4000 5500 BOW 77.1 83.3 87.2 87.3 89.2 TK 80.2 86.2 87.4 88.6 91.2 LATK 80.4 86.5 87.5 88.8 91.6 WUP 81.3 87.3 88.0 89.8 92.5 α = 1 RES 81.0 87.1 87.9 89.5 92.2 LIN 81.1 87.0 88.0 89.3 92.4 LSA(k = 50) 80.8 86.9 87.8 89.3 91.7 Table 5: Classification accuracy of different kernels on different data sets this paper we use the same dataset as introduced in(Li and Roth, 2002). The dataset is divided4 into 5500 questions for training and 500 questions from TREC 20 for testing. The total training samples are randomly divided into 5 subsets with sizes 1,000, 2,000, 3,000, 4,000 and 5,500 respectively. All the questions are labeled into 6 coarse grained categories and 50 fine grained categories: Abbreviations (abbreviation and expansion), Entity (animal, body, color, creation, currency, medical, event, food, instrument, language, letter, plant, product, religion, sport, substance, symbol, technique, term, vehicle, word), Description (definition, description, manner, r</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>Xin Li and Dan Roth. 2002. Learning question classifiers. In Proceedings of the 19th international conference on Computational linguistics, pages 1– 7, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Dimitrios Mavroeidis</author>
<author>George Tsatsaronis</author>
<author>Michalis Vazirgiannis</author>
<author>Martin Theobald</author>
<author>Gerhard Weikum</author>
</authors>
<title>Word sense disambiguation for exploiting hierarchical thesauri in text classification.</title>
<date>2005</date>
<booktitle>Knowledge discovery in databases: PKDD 2005 : 9th European Conference on Principles and Practice of Knowledge Discovery in Databases,</booktitle>
<volume>3721</volume>
<pages>181--192</pages>
<editor>In Al´ıpio Jorge, Lu´ıs Torgo, Pavel Brazdil, Rui Camacho, and Gama Joao, editors,</editor>
<publisher>Springer.</publisher>
<location>Porto, Portugal.</location>
<contexts>
<context position="16582" citStr="Mavroeidis et al., 2005" startWordPosition="2836" endWordPosition="2839">e the rule (2) in the iterative tree kernel calculation(in section 2). (2) If the productions at n1 and n2 are same, and n1 and n2 are pre-terminals, then 0(n1, n2) = Aαkw(w1, w2) where w1 and w2 are two words derived from pre-terminals n1 and n2, respectively, and the parameter α is to control the contribution of the leaves. Note that each preterminal has one child or equally covers one word. So kw(w1, w2) actually calculate the similarity between two words w1 and w2. In general, there are two ways to measure the semantic similarities. One is to derive from semantic networks such as WordNet (Mavroeidis et al., 2005; Bloehdorn et al., 2006). The other way is to use statistical methods of distributional or co-occurrence ( O´ S´eaghdha and Copestake, 2008) behavior of the words. WordNet2 can be regarded as direct graphs semantically linking concepts by means of relations. Table 4 gives some similarity measures between two arbitrary concepts c1 2http://wordnet.princeton.edu/ and c2. For our application, the word-toword similarity can be obtained by maximizing the corresponding concept-based similarity scores. In our implementation, we use WordNet::Similarity package3(Patwardhan et al., 2003) and the noun hi</context>
</contexts>
<marker>Mavroeidis, Tsatsaronis, Vazirgiannis, Theobald, Weikum, 2005</marker>
<rawString>Dimitrios Mavroeidis, George Tsatsaronis, Michalis Vazirgiannis, Martin Theobald, and Gerhard Weikum. 2005. Word sense disambiguation for exploiting hierarchical thesauri in text classification. In Al´ıpio Jorge, Lu´ıs Torgo, Pavel Brazdil, Rui Camacho, and Gama Joao, editors, Knowledge discovery in databases: PKDD 2005 : 9th European Conference on Principles and Practice of Knowledge Discovery in Databases, volume 3721 of Lecture Notes in Computer Science, pages 181–192, Porto, Portugal. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Tree kernels for semantic role labeling.</title>
<date>2008</date>
<journal>Comput. Linguist.,</journal>
<volume>34</volume>
<issue>2</issue>
<contexts>
<context position="1635" citStr="Moschitti et al., 2008" startWordPosition="249" endWordPosition="252">om the object. However the feature vector is sensitive to the structural variations. The extraction schema is heavily dependent on different problems. On the other hand, kernel function can be directly calculated on the object. The advantages are that the original topological information is to a large extent preserved and the introduction of additional noise may be avoided. Thus structure-based kernels can well model syntactic parse tree in a variety of applications, such as relation extraction(Zelenko et al., 2003), named entity recognition(Culotta and Sorensen, 2004), semantic role labeling(Moschitti et al., 2008) and so on. To compute the structural kernel function, Haussler (1999) introduced a general type of kernel function, called“ Convolution kernel”. Based on this work, Collins and Duffy (2002) proposed a tree kernel calculation by counting the common subtrees. In other words, two trees are considered if and only if these two trees are exactly same. In real sentences, some structural alternations within a given phrase are permitted without changing its usage. Therefore, Moschitti (2004) proposed partial trees to partially match between subtrees. Kashima and Koyanagi (2002) generalize the tree ker</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2008</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2008. Tree kernels for semantic role labeling. Comput. Linguist., 34(2):193–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow semantic parsing.</title>
<date>2004</date>
<booktitle>In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>335--342</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2123" citStr="Moschitti (2004)" startWordPosition="328" endWordPosition="329">tion(Zelenko et al., 2003), named entity recognition(Culotta and Sorensen, 2004), semantic role labeling(Moschitti et al., 2008) and so on. To compute the structural kernel function, Haussler (1999) introduced a general type of kernel function, called“ Convolution kernel”. Based on this work, Collins and Duffy (2002) proposed a tree kernel calculation by counting the common subtrees. In other words, two trees are considered if and only if these two trees are exactly same. In real sentences, some structural alternations within a given phrase are permitted without changing its usage. Therefore, Moschitti (2004) proposed partial trees to partially match between subtrees. Kashima and Koyanagi (2002) generalize the tree kernel to labeled order tree kernel with more flexible match. And from the idea of introducing linguistical knowledge, Zhang et al. (2007) proposed a grammar-driven tree kernel, in which two subtrees are same if and only if the corresponding two productions are in the same manually defined set. In addition, the problem of hard matching can be alleviated by processing or mapping the trees. For example, Tai mapping (Kuboyama et al., 2006) generalized the kernel from counting subtrees to c</context>
<context position="12866" citStr="Moschitti, 2004" startWordPosition="2227" endWordPosition="2228">n 23 as well as some data from Brown corpus are left for test. The data sets are described in Table 1. Sentences Arguments Training 39,832 239,858 Dev 1,346 8,346 Test WSJ 1,346 8,346 Brown 450 2,350 Table 1: Data sets statistics 28 Considering the two steps in semantic role labeling, i.e. semantic role identification and recognition. We assume identification has been done correctly, and only consider the semantic role classification. In our experiment, we focus on the semantic classes include 6 core (A0-A5), 12 adjunct(AM-) and 8 reference(R-) arguments. In our implementation, SVM-Light-TK1 (Moschitti, 2004) is modified. For SVM multi-classifier, the ONE-vs-ALL (OVA) strategy is selected. In all, we prepare the data for each semantic role (r) as following: (1) Given a sentence and its correct full syntactic parse tree; (2) Let P be the predicate. Its potential arguments A are extracted according to (Xue and Palmer, 2004) (3) For each pair &lt; p, a &gt;E P x A: if a covers exactly the words of semantic role of p, put minimal subtree &lt; p, a &gt; into positive example set (Tr); else put it in the negative examples (T,−) In our experiments, we set 0 = 0.5. 4.1.2 Experimental Results The classification perfor</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernels for shallow semantic parsing. In ACL ’04: Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pages 335– 342, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid O´ S´eaghdha</author>
<author>Ann Copestake</author>
</authors>
<title>Semantic classification with distributional kernels.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>649--656</pages>
<location>Manchester, UK,</location>
<marker>S´eaghdha, Copestake, 2008</marker>
<rawString>Diarmuid O´ S´eaghdha and Ann Copestake. 2008. Semantic classification with distributional kernels. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 649–656, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Using measures of semantic relatedness for word sense disambiguation. In</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics (CICLING-03),</booktitle>
<pages>241--257</pages>
<contexts>
<context position="17166" citStr="Patwardhan et al., 2003" startWordPosition="2920" endWordPosition="2923">such as WordNet (Mavroeidis et al., 2005; Bloehdorn et al., 2006). The other way is to use statistical methods of distributional or co-occurrence ( O´ S´eaghdha and Copestake, 2008) behavior of the words. WordNet2 can be regarded as direct graphs semantically linking concepts by means of relations. Table 4 gives some similarity measures between two arbitrary concepts c1 2http://wordnet.princeton.edu/ and c2. For our application, the word-toword similarity can be obtained by maximizing the corresponding concept-based similarity scores. In our implementation, we use WordNet::Similarity package3(Patwardhan et al., 2003) and the noun hierarchy of WordNet. In Table 4, dep is the length of path from a node to its global root, lSO(c1, c2) represents the lowest super-ordinate of c1 and c2. The detail definitions can be found in (Budanitsky and Hirst, 2006) . As an alternative, Latent Semantic Analysis(LSA) is a technique. It calculates the words similarities by means of occurrence of terms in documents. Given a term-bydocument matrix X, its singular value decomposition is: X = UEVT, where E is a diagonal matrix with singular values in decreasing arrangement. The column of U are singular vectors corresponding to t</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2003</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2003. Using measures of semantic relatedness for word sense disambiguation. In In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics (CICLING-03), pages 241–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Longhua Qian</author>
<author>Guodong Zhou</author>
<author>Fang Kong</author>
<author>Qiaoming Zhu</author>
<author>Peide Qian</author>
</authors>
<title>Exploiting constituent dependencies for tree kernel-based semantic relation extraction.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (Coling</booktitle>
<pages>697--704</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="2915" citStr="Qian et al., 2008" startWordPosition="451" endWordPosition="454"> from the idea of introducing linguistical knowledge, Zhang et al. (2007) proposed a grammar-driven tree kernel, in which two subtrees are same if and only if the corresponding two productions are in the same manually defined set. In addition, the problem of hard matching can be alleviated by processing or mapping the trees. For example, Tai mapping (Kuboyama et al., 2006) generalized the kernel from counting subtrees to counting the function of mapping. Moreover multi-source knowledge can benefit kernel calculation, such as using dependency information to dynamically determine the tree span (Qian et al., 2008). In this paper, we propose a tree kernel calculation algorithm by allowing variations in productions. The variation is measured with local alignment score between two derivative POS sequences. To reduce the computation complexity, we use the dynamic programming algorithm to compute the score of any alignment. And the top n alignments are considered in the kernel. Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 25–32, Athens, Greece, 31 March 2009. c�2009 Association for Computational Linguistics 25 Another problem in Collins and Duffy’s tr</context>
</contexts>
<marker>Qian, Zhou, Kong, Zhu, Qian, 2008</marker>
<rawString>Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming Zhu, and Peide Qian. 2008. Exploiting constituent dependencies for tree kernel-based semantic relation extraction. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 697–704, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroto Saigo</author>
<author>Jean-Philippe Vert</author>
<author>Nobuhisa Ueda</author>
<author>Tatsuya Akutsu</author>
</authors>
<title>Protein homology detection using string alignment kernels.</title>
<date>2004</date>
<journal>Bioinformatics,</journal>
<volume>20</volume>
<issue>11</issue>
<contexts>
<context position="9138" citStr="Saigo et al., 2004" startWordPosition="1575" endWordPosition="1578">, d) selects the dth subtree for the ith aligned schema of node n1. It is easily to prove the above kernel is positive semi-definite, since the kernel K(ni1, nj2) is positive semi-definite. The native computation is impractical because the number of all possible alignments(|A(x, y)|) is exponential with respect to |x |and |y|. In the next section, we will discuss how to calculate AS(i,j) for each alignment. 3.2 Local Alignment Kernel The local alignment(LA) kernel was usually used in bioinformatics, to compare the similarity between two protein sequences(x and y) by exploring their alignments(Saigo et al., 2004). KLA(x, y) = X expO`(x,y,&apos;r) (4) 7r∈A(x,y) where &gt; 0 is a parameter, A(x, y) denotes all possible local alignments between x and y, and s(x, y, 7r) is the local alignment score for a given alignment schema 7r, which is equal to: S(x&amp;quot;ri1, y&amp;quot;i �)− |r|−1 X �g(�i+1 1 − 7ri1) + g(�i+1 2 − 7ri2)] (5) j=1 In equation( 5), S is a substitution matrix, and g is a gap penalty function. The alignment score is the sum of the substitution score between the correspondence at the aligned position, minus the sum of the gap penalty for the m Y Kd(xd, yd) d=1 s(x, y, 7r) = X |7r| i=1 27 case that ‘-’ symbol is </context>
</contexts>
<marker>Saigo, Vert, Ueda, Akutsu, 2004</marker>
<rawString>Hiroto Saigo, Jean-Philippe Vert, Nobuhisa Ueda, and Tatsuya Akutsu. 2004. Protein homology detection using string alignment kernels. Bioinformatics, 20(11):1682–1689.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kilho Shin</author>
<author>Tetsuji Kuboyama</author>
</authors>
<title>A generalization of haussler’s convolution kernel: mapping kernel.</title>
<date>2008</date>
<booktitle>In ICML,</booktitle>
<pages>944--951</pages>
<contexts>
<context position="11030" citStr="Shin and Kuboyama, 2008" startWordPosition="1908" endWordPosition="1911">, k), ch(n2, j, k)))) k=1 To further reduce the computation complexity, a threshold (�) is used to filter out alignments with low scores. This can help to avoid over-generated subtrees and only select the significant alignments. In other words, by using the threshold (�), we can select the salient subtree variations for kernels. The final kernel calculation is shown below: 4&apos; (n1, n2) = A E (6,3s(x,y,7r)× 7r ∈ A(x, y) s(x, y, 7r) &gt; � nc(nl,i) H (1 + 4&apos;(ch(n1, i, k), ch(n2, j, k)))) k=1 After filtering, the kernel is still positive semi-definite. This can be easily proved using the theorem in (Shin and Kuboyama, 2008), since this subset selection is transitive. More specifically, if s(x, y, 7r) &gt; A s(y, z, 7r&apos;) &gt; �, then s(x, z, 7r + 7r&apos;) &gt; �. The algorithm to compute the local alignment tree kernel is given in algorithm 1. For any two nodes pair(xi and yj), the local alignment score M(xi, yj) is assigned. In the kernel matrix calculation, the worst case occurs when the tree is balanced and most of the alignments are selected. Algorithm 1 algorithm for local alignment tree kernel Require: 2 nodes n1,n2 in parse trees;The productions are n1 → x1, ..., x. and n2 → y1, ..., yn return 4&apos;(n1, n2) if n1 and n2 a</context>
</contexts>
<marker>Shin, Kuboyama, 2008</marker>
<rawString>Kilho Shin and Tetsuji Kuboyama. 2008. A generalization of haussler’s convolution kernel: mapping kernel. In ICML, pages 944–951.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Calibrating features for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004,</booktitle>
<pages>88--94</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="13185" citStr="Xue and Palmer, 2004" startWordPosition="2280" endWordPosition="2283">ation and recognition. We assume identification has been done correctly, and only consider the semantic role classification. In our experiment, we focus on the semantic classes include 6 core (A0-A5), 12 adjunct(AM-) and 8 reference(R-) arguments. In our implementation, SVM-Light-TK1 (Moschitti, 2004) is modified. For SVM multi-classifier, the ONE-vs-ALL (OVA) strategy is selected. In all, we prepare the data for each semantic role (r) as following: (1) Given a sentence and its correct full syntactic parse tree; (2) Let P be the predicate. Its potential arguments A are extracted according to (Xue and Palmer, 2004) (3) For each pair &lt; p, a &gt;E P x A: if a covers exactly the words of semantic role of p, put minimal subtree &lt; p, a &gt; into positive example set (Tr); else put it in the negative examples (T,−) In our experiments, we set 0 = 0.5. 4.1.2 Experimental Results The classification performance is evaluated with respect to accuracy, precision(p), recall(r) and F1 = 2pr/(p + r). Accuracy(%) (Collins and Duffy, 2002) 84.35 (Moschitti, 2004) 86.72 (Zhang et al., 2007) 87.96 Our Kernel 88.48 Table 2: Performance comparison between different kernel performance on WSJ data 1http://dit.unitn.it/ moschitt/Tree</context>
</contexts>
<marker>Xue, Palmer, 2004</marker>
<rawString>Nianwen Xue and Martha Palmer. 2004. Calibrating features for semantic role labeling. In Dekang Lin and Dekai Wu, editors, Proceedings of EMNLP 2004, pages 88–94, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Zelenko</author>
<author>Chinatsu Aone</author>
<author>Anthony Richardella</author>
</authors>
<title>Kernel methods for relation extraction.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<pages>3--1083</pages>
<contexts>
<context position="1533" citStr="Zelenko et al., 2003" startWordPosition="236" endWordPosition="239">s. One is to encode an object with a flat vector whose element correspond to an extracted feature from the object. However the feature vector is sensitive to the structural variations. The extraction schema is heavily dependent on different problems. On the other hand, kernel function can be directly calculated on the object. The advantages are that the original topological information is to a large extent preserved and the introduction of additional noise may be avoided. Thus structure-based kernels can well model syntactic parse tree in a variety of applications, such as relation extraction(Zelenko et al., 2003), named entity recognition(Culotta and Sorensen, 2004), semantic role labeling(Moschitti et al., 2008) and so on. To compute the structural kernel function, Haussler (1999) introduced a general type of kernel function, called“ Convolution kernel”. Based on this work, Collins and Duffy (2002) proposed a tree kernel calculation by counting the common subtrees. In other words, two trees are considered if and only if these two trees are exactly same. In real sentences, some structural alternations within a given phrase are permitted without changing its usage. Therefore, Moschitti (2004) proposed </context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Dmitry Zelenko, Chinatsu Aone, and Anthony Richardella. 2003. Kernel methods for relation extraction. J. Mach. Learn. Res., 3:1083–1106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Zhang</author>
<author>Wanxiang Che</author>
<author>Aiti Aw</author>
<author>Chew Lim Tan</author>
<author>Guodong Zhou</author>
<author>Ting Liu</author>
<author>Sheng Li</author>
</authors>
<title>A grammar-driven convolution tree kernel for semantic role classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>200--207</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="2370" citStr="Zhang et al. (2007)" startWordPosition="365" endWordPosition="368"> called“ Convolution kernel”. Based on this work, Collins and Duffy (2002) proposed a tree kernel calculation by counting the common subtrees. In other words, two trees are considered if and only if these two trees are exactly same. In real sentences, some structural alternations within a given phrase are permitted without changing its usage. Therefore, Moschitti (2004) proposed partial trees to partially match between subtrees. Kashima and Koyanagi (2002) generalize the tree kernel to labeled order tree kernel with more flexible match. And from the idea of introducing linguistical knowledge, Zhang et al. (2007) proposed a grammar-driven tree kernel, in which two subtrees are same if and only if the corresponding two productions are in the same manually defined set. In addition, the problem of hard matching can be alleviated by processing or mapping the trees. For example, Tai mapping (Kuboyama et al., 2006) generalized the kernel from counting subtrees to counting the function of mapping. Moreover multi-source knowledge can benefit kernel calculation, such as using dependency information to dynamically determine the tree span (Qian et al., 2008). In this paper, we propose a tree kernel calculation a</context>
<context position="13645" citStr="Zhang et al., 2007" startWordPosition="2364" endWordPosition="2367">n a sentence and its correct full syntactic parse tree; (2) Let P be the predicate. Its potential arguments A are extracted according to (Xue and Palmer, 2004) (3) For each pair &lt; p, a &gt;E P x A: if a covers exactly the words of semantic role of p, put minimal subtree &lt; p, a &gt; into positive example set (Tr); else put it in the negative examples (T,−) In our experiments, we set 0 = 0.5. 4.1.2 Experimental Results The classification performance is evaluated with respect to accuracy, precision(p), recall(r) and F1 = 2pr/(p + r). Accuracy(%) (Collins and Duffy, 2002) 84.35 (Moschitti, 2004) 86.72 (Zhang et al., 2007) 87.96 Our Kernel 88.48 Table 2: Performance comparison between different kernel performance on WSJ data 1http://dit.unitn.it/ moschitt/Tree-Kernel.htm P(%) R(%) F,3=1 Development 81.03 68.91 74.48 WSJ Test 84.97 79.45 82.11 Brown Test 76.95 70.94 73.51 WSJ+Brown 82.98 75.40 79.01 WSJ P(%) R(%) F A0 81.28 83.90 82.56 A1 84.22 66.39 74.25 A2 77.27 62.36 69.02 A3 93.33 21.21 34.57 A4 82.61 51.35 63.33 A5 100.00 40.00 57.41 AM-ADV 74.21 56.21 63.92 AM-CAU 75.00 46.09 57.09 AM-DIR 57.14 16.00 25.00 AM-DIS 77.78 70.00 73.68 AM-EXT 75.00 53.10 62.18 AM-LOC 89.66 74.83 81.57 AM-MNR 84.62 48.20 61.41 </context>
</contexts>
<marker>Zhang, Che, Aw, Tan, Zhou, Liu, Li, 2007</marker>
<rawString>Min Zhang, Wanxiang Che, Aiti Aw, Chew Lim Tan, Guodong Zhou, Ting Liu, and Sheng Li. 2007. A grammar-driven convolution tree kernel for semantic role classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 200–207, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>