<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.060592">
<title confidence="0.997123">
Automated Skimming in Response to Questions for NonVisual Readers
</title>
<author confidence="0.998593">
Debra Yarrington Kathleen F. McCoy
</author>
<affiliation confidence="0.999766">
Dept. of Computer and Information Science Dept. of Computer and Information Science
University of Delaware University of Delaware
</affiliation>
<address confidence="0.725899">
Newark, DE, 19716, USA Newark, DE, 19716, USA
</address>
<email confidence="0.998779">
yarringt@eecis.udel.edu mccoy@cis.udel.edu
</email>
<sectionHeader confidence="0.998599" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960086956522">
This paper presents factors in designing a sys-
tem for automatically skimming text docu-
ments in response to a question. The system
will take a potentially complex question and a
single document and return a Web page con-
taining links to text related to the question.
The goal is that these text areas be those that
visual readers would spend the most time on
when skimming for the answer to a question.
To identify these areas, we had visual readers
skim for an answer to a complex question
while being tracked by an eye-tracking sys-
tem. Analysis of these results indicates that
text with semantic connections to the question
are of interest, but these connections are much
looser than can be identified with traditional
Question-Answering or Information Retrieval
techniques. Instead, we are expanding tradi-
tional semantic treatments by using a Web
search. The goal of this system is to give non-
visual readers information similar to what vis-
ual readers get when skimming through a
document in response to a question.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952909090909">
This paper describes semantic considerations in
developing a system for giving nonvisual readers
information similar to what visual readers glean
when skimming through a document in response to
a question. Our eventual system will be unique in
that it takes both simple and complex questions,
will work in an unrestricted domain, will locate
answers within a single document, and will return
not just an answer to a question, but the informa-
tion visual skimmers acquire when skimming
through a document.
</bodyText>
<page confidence="0.942633">
98
</page>
<subsectionHeader confidence="0.795847">
1.1 Goals
</subsectionHeader>
<bodyText confidence="0.995414">
Production of our skimming system will require
the attainment of three major goals:
</bodyText>
<listItem confidence="0.838879">
1. Achieving an understanding of what information
in the document visual skimmers pay attention
to when skimming in response to a question
2. Developing Natural Language Processing (NLP)
techniques to automatically identify areas of text
visual readers focus on as determined in 1.
3. Developing a user interface to be used in con-
junction with screen reading software to deliver
the visual skimming experience.
</listItem>
<bodyText confidence="0.993164666666667">
In this paper we focus on the first two of these
goals. Section 2 will discuss experiments analyzing
visual skimmers skimming for answers to ques-
tions. Section 3 will discuss developing NLP tech-
niques to replicate the results of Section 2. Section
4 will discuss future work.
</bodyText>
<subsectionHeader confidence="0.80088">
1.2 Impetus
</subsectionHeader>
<bodyText confidence="0.999975866666667">
The impetus for this system was work done by the
author with college students with visual impair-
ments who took significantly longer to complete
homework problems than their visually reading
counterparts. Students used both ScreenReaders,
which read electronic text aloud, and screen mag-
nifiers, which increase the size of text on a screen.
While these students were comfortable listening to
the screenreader reading at rates of up to 500
words per minute, their experience was quite dif-
ferent from their visual-reading peers. Even after
listening to an entire chapter, when they wanted to
return to areas of text that contained text relevant
to the answer, they had to start listening from the
beginning and traverse the document again. Doing
</bodyText>
<note confidence="0.9845045">
Proceedings of the NAACL HLT 2010 Workshop on Speech and Language Processing for Assistive Technologies, pages 98–106,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999636142857143">
homework was a tedious, time-consuming task
which placed these students at a serious disadvan-
tage. It is clear that individuals with visual im-
pairments struggle in terms of education. By
developing a system that levels the playing field in
at least one area, we may make it easier for at least
some individuals to succeed.
</bodyText>
<sectionHeader confidence="0.987661" genericHeader="method">
2 Visual Skimming
</sectionHeader>
<bodyText confidence="0.999974964285714">
If our intention is to convey to nonvisual readers
information similar to what visual readers acquire
when skimming for answers to questions, we first
must determine what information visual readers get
when skimming. For our purposes, we were inter-
ested in what text readers focused on in connection
to a question. While many systems exist that focus
on answering simple, fact-based questions, we
were more interested in more complex questions in
which the answer could not be found using pattern
matching and in which the answer would require at
least a few sentences, not necessarily contiguous
within a document. From an NLP standpoint, lo-
cating longer answers with relevant information
occuring in more than one place that may or may
not have words or word sequences in common with
the question poses an interesting and difficult prob-
lem. The problem becomes making semantic con-
nections within any domain that are more loosely
associated than the synonyms, hypernyms, hypo-
nyms, etc. provided by WordNet (Felbaum, 1998).
Indeed, the questions that students had the most
difficulty with were more complex in nature. Thus
we needed to find out whether visual skimmers
were able to locate text in documents relevant to
complex questions and, if so, what connections
visual skimmers are making in terms of the text
they choose to focus on.
</bodyText>
<subsectionHeader confidence="0.990258">
2.1 Task Description
</subsectionHeader>
<bodyText confidence="0.999945176470588">
To identify how visual readers skim documents to
answer questions, we collected 14 questions ob-
tained from students’ homework assignments,
along with an accompanying document per ques-
tion from which the answer could be obtained. The
questions chosen were on a wide variety of topics
and were complex in nature. An example of a typi-
cal question is, “According to Piaget, what tech-
niques do children use to adjust to their
environment as they grow?” Documents largely
consisted of plain text, although each had a title on
the first page. They held no images and few sub-
titles or other areas users might find visually inter-
esting. Twelve of the documents were two pages in
length, one was eight pages in length, and one was
nine pages long. In each case, the answer to the
question was judged by the researchers to be found
within a single paragraph in the document.
Forty-three visual reading subjects skimmed for
the answer to between 6 – 13 questions. The sub-
jects sat in front of a computer screen to which the
Eye Tracker 1750 by Tobii Technologies was in-
stalled. The questions and accompanying docu-
ments were displayed on the computer screen and,
after being calibrated, subjects were tracked as
they skimmed for the answer. For the two-page
documents, the question appeared at the top of the
first page. For the longer documents, the question
appeared at the top of each page. Subjects had no
time limit for skimming and switched pages by
pressing the space bar. When done skimming each
document, subjects were asked to select a best an-
swer in multiple choice form (to give them a rea-
son to take the skimming task seriously).
</bodyText>
<subsectionHeader confidence="0.852272">
2.2 Results
</subsectionHeader>
<bodyText confidence="0.999657791666667">
Results showed that subjects were reliably able to
correctly answer the multiple choice question after
skimming the document. Of the 510 questions, 423
(about 86%) were answered correctly. The two
questions from longer documents were the least
likely to be answered correctly (one had 10 correct
answers of 21 total answers, and the other had 10
incorrect answers and only one correct answer).
Clearly for the shorter documents, subjects were
able to get appropriate information out of the doc-
ument to successfully answer the question. With
that established, we were interested in analyzing
the eye tracking data to see if there was a connec-
tion between where subjects spent the most time in
the document and the question. If there was an un-
derstandable connection, the goal then became to
automatically replicate those connections and thus
automatically locate places in the text where sub-
jects were most likely to spend the most time.
The Tobii Eye Tracking System tracks the path
and length of time a subject gazes at a particular
point as a subject skims through a document. The
system allows us to define Areas of Interest (AOIs)
and then track the number of prolonged gaze points
</bodyText>
<page confidence="0.998731">
99
</page>
<figureCaption confidence="0.9982035">
Figure 1. Hot spot image results of skimming for the answer to the question, “What are two dietary factors
thought to raise and lower cholesterol?” using the Tobii Eye Tracking System
</figureCaption>
<bodyText confidence="0.999958941176471">
within those areas of interest. For our analysis, we
defined areas of interest as being individual para-
graphs. While we purposely chose documents that
were predominantly text, each had a title as well.
Titles and the few subtitles and lists that occurred
in the documents were also defined as separate
AOIs. For each skimming activity, the eye tracking
system gave us a gaze plot showing the order in
which individuals focused on particular areas, and
a hot spot image showing the gaze points, with
duration indicated with color intensity, that oc-
curred in each AOI (see Figure 1).
In looking at the hot spot images, we found that
subjects used three techniques to peruse a docu-
ment. One technique subjects used was to move
their gaze slowly throughout the entire document,
indicating that they were most likely reading the
document. A second technique used was to move
randomly and quickly from top to bottom of the
document (described as “fixations distributed in a
rough zig-zag down the page” by McLaughlin in
reference to speed reading (1969)), without ever
focusing on one particular area for a longer period
of time. This technique was the least useful to us
because it gave very little information A third
technique was a combination of the first two, in
which the subject’s gaze darted quickly and ran-
domly around the page, and then appeared to focus
on a particular area for an extended period of time.
Figure 1 is a good example of this technique. The
data from this group was clearly relevant to our
task since their fixation points clearly showed what
areas subjects found most interesting while skim-
ming for an answer to a question.
</bodyText>
<subsectionHeader confidence="0.999928">
2.3 Analysis of Skimming Data
</subsectionHeader>
<bodyText confidence="0.999994571428572">
To determine exactly which AOIs subjects focused
on most frequently, we counted the number of gaze
points (or focus points) in each AOI (defined as
paragraphs, titles, subtitles) across all subjects. In
looking at what information individuals focused on
while skimming, we found that individuals did fo-
cus on the title and subtitles that occurred in the
documents. Subjects frequently focused on the first
paragraph or paragraphs of a document. There was
less of a tendency, but still a trend for focusing on
the first paragraph on each page. Interestingly, al-
though a few subjects focused on the first line of
each paragraph, this was not a common practice.
This is significant because it is a technique availa-
ble to users of screenreaders, yet it clearly does not
give these users the same information that visual
skimmers get when skimming through a document.
We also wanted to look at AOIs that did not
have physical features that may have attracted at-
tention. Our conjecture was that these AOIs were
focused on by subjects because of their semantic
</bodyText>
<page confidence="0.956648">
100
</page>
<bodyText confidence="0.999145938775511">
relationship to the question. Indeed, we did find
evidence of this. Results indicated that subjects did
focus on the areas of text containing the answer to
the question. As an example, one of the questions
used in the study was,
“How do people catch the West Nile Vi-
rus?”
The paragraph with the most gaze points for the
most subjects was:
“In the United States, wild birds, especial-
ly crows and jays, are the main reservoir
of West Nile virus, but the virus is actually
spread by certain species of mosquitoes.
Transmission happens when a mosquito
bites a bird infected with the West Nile vi-
rus and the virus enters the mosquito&apos;s
bloodstream. It circulates for a few days
before settling in the salivary glands. Then
the infected mosquito bites an animal or a
human and the virus enters the host&apos;s
bloodstream, where it may cause serious
illness. The virus then probably multiplies
and moves on to the brain, crossing the
blood-brain barrier. Once the virus
crosses that barrier and infects the brain
or its linings, the brain tissue becomes in-
flamed and symptoms arise.”
This paragraph contains the answer to the ques-
tion, yet it has very few words in common with the
question. The word it does have in common with
the question, ‘West Nile Virus’, is the topic of the
document and occurs fairly frequently throughout
the document, and thus cannot account for sub-
jects&apos; focusing on this particular paragraph.
The subjects must have made semantic connec-
tions between the question and the answer that
cannot be explained by simple word matching or
even synonyms, hypernyms and hyponyms. In the
above example, the ability of the user to locate the
answer hinged on their ability to make a connec-
tion between the word ‘catch’ in the question and
its meaning ‘to be infected by’. Clearly simple
keyword matching won’t suffice in this case, yet
equally clearly subjects successfully identified this
paragraph as being relevant to the question. This
suggests that when skimming subjects were able to
make the semantic connections necessary to locate
question answers, even when the answer was of a
very different lexical form than the question.
Other areas of text focused on also appear to
have a semantic relationship with the question. For
example, with the question,
“Why was Monet’s work criticized by the
public?”
the second most frequently focused on paragraph
was:
“In 1874, Manet, Degas, Cezanne, Renoir,
Pissarro, Sisley and Monet put together an
exhibition, which resulted in a large finan-
cial loss for Monet and his friends and
marked a return to financial insecurity for
Monet. It was only through the help of
Manet that Monet was able to remain in
Argenteuil. In an attempt to recoup some
of his losses, Monet tried to sell some of
his paintings at the Hotel Drouot. This,
too, was a failure. Despite the financial
uncertainty, Monet’s paintings never be-
came morose or even all that sombre. In-
stead, Monet immersed himself in the task
of perfecting a style which still had not
been accepted by the world at large. Mo-
net’s compositions from this time were ex-
tremely loosely structured, with color
applied in strong, distinct strokes as if no
reworking of the pigment had been at-
tempted. This technique was calculated to
suggest that the artist had indeed captured
a spontaneous impression of nature.”
Of the 30 subjects who skimmed this document,
15 focused on this paragraph, making it the second
most focused on AOI in the document, second only
to the paragraph that contained the answer (fo-
cused on by 21 of the subjects). The above para-
graph occurred within the middle of the second
page of the document, with no notable physical
attributes that would have attracted attention. Upon
closer inspection of the paragraph, there are refer-
ences to “financial loss,” “financial insecurity,”
“losses,” “failure,” and “financial uncertainty.”
The paragraph also includes “morose” and “somb-
er” and even “had not been accepted by the world
at large.” Subjects appeared to be making a con-
nection between the question topic, Monet’s work
being criticized by the public, and the above terms.
Intuitively, we do seem to make this connection.
Yet the connection being made is not straightfor-
ward and cannot be replicated using the direct se-
</bodyText>
<page confidence="0.995375">
101
</page>
<bodyText confidence="0.97439726984127">
mantic connections that are available via WordNet.
Indeed, the relationships made are more similar to
Hovy and Lin’s (1997) Concept Signatures created
by clustering words in articles with the same edi-
tor-defined classification from the Wall Street
Journal. Our system must be able to replicate these
connections automatically.
Upon further examination, we found other pa-
ragraphs that were focused on by subjects for rea-
sons other than their physical appearance or
location, yet their semantic connection to the ques-
tion was even more tenuous. For instance, when
skimming for the answer to the question,
“How does marijuana affect the brain?”
the second most frequently focused on paragraph
(second to the paragraph with the answer) was,
“The main active chemical in marijuana is
THC (delta-9-tetrahydrocannabinol). The
protein receptors in the membranes of cer-
tain cells bind to THC. Once securely in
place, THC kicks off a series of cellular
reactions that ultimately lead to the high
that users experience when they smoke
marijuana.”
While this paragraph does appear to have loose
semantic connections with the question, the con-
nections are less obvious than paragraphs that fol-
low it, yet it was this paragraph that subjects chose
to focus on. The paragraph is the third to last para-
graph on the first page, so its physical location
could not explain its attraction to subjects. Howev-
er, when we looked more closely at the previous
paragraphs, we saw that the first paragraph deals
with definitions and alternate names for marijuana
(with no semantic links to the question), and the
second and third paragraph deal with statistics on
people who use marijuana (again, with no semantic
connection to the question). The fourth paragraph,
the one focused on, represents a dramatic semantic
shift towards the topic of the question. Intuitively it
makes sense that individuals skimming through the
document would pay more attention to this para-
graph because it seems to represent the start of the
area that may contain the answer, not to mention
conveying topological information about the layout
of the document and general content information
as well.
Data collected from these experiments suggest
that subjects do make and skim for semantic con-
nections. Subjects not only glean information that
directly answers the question, but also on content
within the document that is semantically related to
the question. While physical attributes of text do
attract the attention of skimmers, and thus we must
include methods for accessing this data as well, it
is clear that in order to create a successful skim-
ming device that conveys information similar to
what visual skimmers get when skimming for the
answer to a question, we must come up with a me-
thod for automatically generating loose semantic
connections and then using those semantic connec-
tions to locate text skimmers considered relevant
within the document.
</bodyText>
<sectionHeader confidence="0.999161" genericHeader="method">
3 NLP Techniques
</sectionHeader>
<bodyText confidence="0.999549">
In order to automatically generate the semantic
connections identified above as being those visual
skimmers make, we want to explore Natural Lan-
guage Processing (NLP) techniques.
</bodyText>
<subsectionHeader confidence="0.945594">
3.1 Related Research
</subsectionHeader>
<bodyText confidence="0.999827370370371">
Potentially relevant methodologies may be found
in Open Domain Question Answering Systems.
Open Domain Question Answering Systems in-
volve connecting questions within any domain and
potential answers. These systems usually do not
rely on external knowledge sources and are limited
in the amount of ontological information that can
be included in the system. The questions are usual-
ly fact-based in form (e.g., “How tall is Mt. Ever-
est?”). These systems take a question and query a
potentially large set of documents (e.g., the World
Wide Web) to find the answer. A common tech-
nique is to determine a question type (e.g., “How
many ...?” would be classified as ‘numerical’,
whereas “Who was ...?” would be classified as
‘person’, etc.) and then locate answers of the cor-
rect type (Abney et al., 2000; Kwok et al., 2001;
Srihari and Li, 2000; Galea, 2003). Questions are
also frequently reformulated for pattern matching
(e.g., “Who was the first American Astronaut in
space?” becomes, “The first American Astronaut
in space was” (Kwok et al., 2001; Brill et al.,
2002)). Many systems submit multiple queries to a
document corpus, relying on redundancy of the
answer to handle incorrect answers, poorly con-
structed answers or documents that don’t contain
the answer (e.g., Brill et al., 2002; Kwok et al.,
</bodyText>
<page confidence="0.997845">
102
</page>
<bodyText confidence="0.999631886792453">
2001). For these queries, systems often include
synonyms, hypernyms, hyponyms, etc. in the query
terms used for document and text retrieval (Hovy
et al.,2000; Katz et al., 2005). In an attempt to an-
swer more complex relational queries, Banko et al.
(2007) parsed training data into relational tuples
for use in classifying text tagged for part of speech,
chunked into noun phrases, and then tagged the
relations for probability. Soricut and Brill (2006)
trained data on FAQ knowledge bases from the
World Wide Web, resulting in approximately 1
million question-answer pairs. This system related
potential answers to questions using probability
models computed using the FAQ knowledge base.
Another area of research that may lend useful
techniques for connecting and retrieving relevant
text to a question is query-biased text summariza-
tion. With many summarization schemes, a good
deal of effort has been placed on identifying the
main topic or topics of the document. In query bi-
ased text summarization, however, the topic is
identified a priori, and the task is to locate relevant
text within a document or set of documents. In
multidocument summarization systems, redundan-
cy may be indicative of relevance, but should be
eliminated from the resulting summary. Thus a
concern is measuring relevance versus redundancy
(Carbonell and Goldstein, 1998; Hovy et al., 2005;
Otterbacher et al., 2006). Like Question Answering
systems, many summarization systems simply
match the query terms, expanded to include syn-
onyms, hypernyms, hyponyms, etc., to text in the
document or documents (Varadarajan and Hristi-
dis, 2006; Chali, 2002)
Our system is unique in that it has as its goal not
just to answer a question or create a summary, but
to return information visual skimmers glean while
skimming through a document. Questions posed to
the system will range from simple to complex in
nature, and the answer must be found within a sin-
gle document, regardless of the form the answer
takes. Questions can be on any topic. With com-
plex questions, it is rarely possible to categorize
the type of question (and thus the expected answer
type). Intuitively, it appears equally useless to at-
tempt reformulation of the query for pattern match-
ing. This intuition is born out by Soricut and Brill
(2006) who stated that in their study reformulating
complex questions more often hurt performance
than improved it. Answering complex questions
within a single document when the answer may not
be straightforward in nature poses a challenging
problem.
</bodyText>
<subsectionHeader confidence="0.997274">
3.2 Baseline Processing
</subsectionHeader>
<bodyText confidence="0.999976434782609">
Our baseline system attempted to identify areas of
interest by matching against the query in the tradi-
tion of Open Domain Question Answering. For our
baseline, we used the nonfunction words in each
question as our query terms. The terms were
weighted with a variant of TF/IDF (Salton and
Buckley, 1988) in which terms were weighted by
the inverse of the number of paragraphs they oc-
curred in within the document. This weighting
scheme was designed to give lower weight to
words associated with the document topic and thus
conveying less information about relevance to the
question. Each query term was matched to text in
each paragraph, and paragraphs were ranked for
matching using the summation of, for each query
term, the number of times it occurred in the para-
graph multiplied by its weight.
Results of this baseline ranking were poor. In
none of the 14 documents did this method connect
the question to the text relevant to the answer. This
was expected. This original set of questions was
purposely chosen because of the complex relation-
ship between the question and answer text.
Next we expanded the set of query terms to in-
clude synonyms, hypernyms, and hyponyms as
defined in WordNet (Felbaum, 1998). We included
all senses of each word (query term). Irrelevant
senses resulted in the inclusion of terms that were
no more likely to occur frequently than any other
random word, and thus had no effect on the result-
ing ranking of paragraphs. Again, each of the
words in the expanded set of query terms was
weighted as described above, and paragraphs were
ranked accordingly.
Again, results were poor. Paragraphs ranked
highly were no more likely to contain the answer,
nor were they likely to be areas focused on by the
visual skimmers in our collected skimming data.
Clearly, for complex questions, we need to ex-
pand on these basic techniques to replicate the se-
mantic connections individuals make when
skimming. As our system must work across a vast
array of domains, our system must make these
connections “on the fly” without relying on pre-
viously defined ontological or other general know-
ledge. And our system must work quickly: asking
</bodyText>
<page confidence="0.998638">
103
</page>
<bodyText confidence="0.99972375">
individuals to wait long periods of time while the
system creates semantic connections and locates
appropriate areas of text would defeat the purpose
of a system designed to save its users time.
</bodyText>
<subsectionHeader confidence="0.999586">
3.3 Semantically-Related Word Clusters
</subsectionHeader>
<bodyText confidence="0.999980234567902">
Our solution is to use the World Wide Web to form
clusters of topically-related words, with the topic
being the question. The cluster of words will be
used as query terms and matched to paragraphs as
described above for ranking relevant text.
Using the World Wide Web as our corpus has a
number of advantages. Because of the vast number
of documents that make up the World Wide Web,
we can rely on the redundancy that has proved so
useful for Question Answering and Text Summari-
zation systems. By creating the word clusters from
documents returned from a search using question
words, the words that occur most frequently in the
related document text will most likely be related in
some way to the question words. Even relatively
infrequently occurring word correlations can most
likely be found in some document existing on the
Web, and thus strangely-phrased questions or
questions with odd terms will still most likely
bring up some documents that can be used to form
a cluster. The Web covers virtually all domains.
Somewhere on the Web there is almost certainly an
answer to questions on even the most obscure top-
ics. Thus questions containing words unique to
uncommon domains or questions containing un-
usual word senses will return documents with ap-
propriate cluster words. Finally, the Web is
constantly being updated. Terms that might not
have existed even a year ago will now be found on
the Web.
Our approach is to use the nonstop words in a
question as query terms for a Web search. The
search engine we are using is Google
(www.google.com). For each search engine query,
Google returns an ranked list of URLs it considers
relevant, along with a snippet of text it considers
most relevant to the query (usually because of
words in the snippet that exactly match the query
terms). To create the cluster of words related se-
mantically to the question, we are taking the top 50
URLs, going to their correlating Web page, locat-
ing the snippet of text within the page, and creating
a cluster of words using a 100-word window sur-
rounding the snippet. We are using only nonstop
words in the cluster, and weighting the words
based on their total number of occurrences in the
windows. These word clusters, along with the ex-
panded baseline words, are used to locate and rank
paragraphs in our question document.
Our approach is similar in spirit to other re-
searchers using the Web to identify semantic rela-
tions. Matsuo et al. (2006) looked at the number of
hits of each of two words as a single keyword ver-
sus the number of hits using both words as key-
words to rate the semantic similarity of two words.
Chen et al. (2006) used a similar approach to de-
termine the semantic similarity between two
words: with a Web search using word P as the
query term, they counted the number of times word
Q occurred in the snippet of text returned, and vice
versa. Bollegala et al. (2007) determined semantic
relationships by extracting lexico-syntactic patterns
from the snippets returned from a search on two
keywords (e.g.,“’x’ is a ‘y’”) and extracting the
relationship of the two words based on the pattern.
Sahami and Heilman (2006) used the snippets from
a word search to form a set of words weighted us-
ing TF/IDF, and then determined the semantic si-
milarity of two keywords by the similarity of two
word sets returned in those snippets.
Preliminary results from our approach have been
encouraging. For example, with the question,
“How does Marijuana affect the brain?”, the ex-
panded set of keywords included, “hippocampus,
receptors, THC, memory, neuron”. These words
were present in both the paragraph containing the
answer and the second-most commonly focused on
paragraph in our study. While neither our baseline
nor our expanded baseline identified either para-
graph as an area of interest, the semantically-
related word clusters did.
</bodyText>
<sectionHeader confidence="0.99985" genericHeader="method">
4 Future Work
</sectionHeader>
<bodyText confidence="0.997907">
This system is a work in progress. There are many
facets still under development, including a finer
analysis of visual skimming data, a refinement of
the ranking system for locating areas of interest
within a document, and the development of the
system’s user interface.
</bodyText>
<subsectionHeader confidence="0.997942">
4.1 Skimming Data Analysis
</subsectionHeader>
<bodyText confidence="0.9998205">
For our initial analysis, we focused on the length of
time users spent gazing at text areas. In future
</bodyText>
<page confidence="0.996931">
104
</page>
<bodyText confidence="0.9999979">
analysis, we will look at the order of the gaze
points to determine exactly where the subjects first
gazed before choosing to focus on a particular
area. This may give us even more information
about the type of semantic connection subjects
made before choosing to focus on a particular area.
In addition, in our initial analysis, we defined AOIs
to be paragraphs. We may want to look at smaller
AOIs. For example, with longer paragraphs, the
text that actually caught the subject’s eye may have
occurred only in one portion of the paragraph, yet
as the analysis stands now the entire content of the
paragraph is considered relevant and thus we are
trying to generate semantic relationships between
the question and potentially unrelated text. While
the system only allows us to define AOIs as rec-
tangular areas (and thus we can’t do a sentence-by-
sentence analysis), we may wish to define AOIs as
small as 2 lines of text to narrow in on exactly
where subjects chose to focus.
</bodyText>
<subsectionHeader confidence="0.989218">
4.2 Ranking System Refinement
</subsectionHeader>
<bodyText confidence="0.999994918918919">
It is worth mentioning that, while a good deal of
research has been done on evaluating the goodness
of automatically generated text summaries (Mani
et al.,2002; Lin and Hovy, 2003; Santos et al.,
2004) our system is intended to mimic the actions
of skimmers when answering questions, and thus
our measure of goodness will be our system’s
ability to recreate the retrieval of text focused on
by our visual skimmers. This gives us a distinct
advantage over other systems in measuring good-
ness, as defining a measure of goodness can prove
difficult. In future work, we will be exploring dif-
ferent methods of ranking text such that the system
returns results most similar to the results obtained
from the visual skimming studies. The system will
then be used on other questions and documents and
compared to data to be collected of visual skim-
mers skimming for answers to those questions.
Many variations on the ranking system are poss-
ible. These will be explored to find the best
matches with our collected visual skimming data.
Possibilities include weighting keywords different-
ly according to where they came from (e.g., direct-
ly from the question, from the text in retrieved
Web pages, from text from a Web page ranked
high on the returned URL list or lower, etc.), or
considering how a diversity of documents might
affect results. For instance, if keywords include
‘falcon’ and ‘hawk’ the highest ranking URLs will
most likely be related to birds. However, in G.I.
Joe, there are two characters, Lieutenant Falcon
and General Hawk. To get the less common con-
nection between falcon and hawk and G.I. Joe, one
may have to look for diversity in the topics of the
returned URLs. Another area to be explored will
be the effect of varying the window size surround-
ing the snippet of text to form the bag of words.
</bodyText>
<subsectionHeader confidence="0.994587">
4.3 User Interface
</subsectionHeader>
<bodyText confidence="0.951202142857143">
The user interface for our system poses some inter-
esting questions. It is important that the output of
the system provide the user with information about
(1) document topology, (2) document semantics,
and (3) information most relevant to answering the
question. At the same time, it is important that us-
ing the output be relatively fast. The output of the
system is envisioned as a Web page with ranked
links at the top pointing to sections of the text like-
ly to be relevant to answering the question.
An important issue that must be explored in
depth with potential users of the system is the ex-
act form of the output web page. We need to ex-
plore the best method for indicating text areas of
interest and the overall topology. The goal is that
reading the links simulate what a visual skimmer
gets from lightly skimming. The user would actual-
ly follow the links that appeared to be “worth read-
ing” in more detail in the same way that skimmers
focus in on particular text segments that appear
worth reading.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999923733333333">
This system attempts to correlate NLP techniques
for creating semantic connections with the seman-
tic connections individuals make. Using the World
Wide Web, we may be able to make those seman-
tic connections across any topic in a reasonable
amount of time without any previously defined
knowledge. We have ascertained that people can
and do make semantic links when skimming for
answers to questions, and we are currently explor-
ing the best use of the World Wide Web in repli-
cating those connections. In the long run, we
envision a system that is user-friendly to nonvisual
and low vision readers that will give them an intel-
ligent way to skim through documents for answers
to questions.
</bodyText>
<page confidence="0.998981">
105
</page>
<sectionHeader confidence="0.998346" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999910101123596">
S. Abney, M. Collins, and A. Singhal. 2000. Answer
Extraction. In Proceedings of ANLP 2000, 296-301.
M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead,
and O. Etzioni. 2007. Open information extraction
from the Web. In Proceedings of the 20th Interna-
tional Joint Conference on Artificial Intelligence,
2670-2676.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Mea-
suring semantic similarity between words using Web
search engines. In Proceedings of WWW 2007. 757-
766.
Brill, E., Lin, J., Banko, M., Domais, S. and Ng, A.
2001. Data-Intensive Question Answering. In Pro-
ceedings of the TREC-10 Conference, NIST, Gai-
thersburg, MD, 183-189.
J. Carbonell and J. Goldstein. 1998. The use of MMR,
diversity-based reranking for reordering documents
and producing summaries. In Proceedings of SIGIR
’98, New York, NY, USA, 335-336.
Y. Chali. 2002. Generic and query-based text summari-
zation using lexical cohesion, In Proceedings of the
Fifteenth Canadian Conference on Artificial Intelli-
gence, Calgary, May, 293-303.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking. In
Proceedings of the COLING/ACL 2006. 1009-1016.
C. Felbaum. 1998. WordNet an Electronic Database,
Boston/Cambridge: MIT Press.
A. Galea.2003. Open-domain Surface-Based Question
Answering System. In Proceedings of the Computer
Science Annual Workshop (CSAW), University of
Malta.
E. H. Hovy, L. Gerber, U. Hermjakob, M. Junk, and C.-
Y. Lin. 2000. Question Answering in Webclopedia.
In Proceedings of the TREC-9 Conference. NIST,
Gaithersburg, MD. November 2000. 655-664.
E. Hovy and C.Y. Lin. 1997. Automated Text Summa-
rization in SUMMARIST. In Proceedings of the
Workshop on Intelligent Scalable Text Summariza-
tion, Madrid, Spain, 18-24.
Boris Katz, Gregory Marton, Gary Borchardt, Alexis
Brownell, Sue Felshin, Daniel Loreto, Jesse Louis-
Rosenberg, Ben Lu, Federico Mora, Stephan Stiller,
Ozlem Uzuner, and Angela Wilcox. 2005. External
Knowledge Sources for Question Answering Pro-
ceedings of the 14th Annual Text REtrieval Confe-
rence (TREC2005), November 2005, Gaithersburg,
MD.
C. Kwok, O. Etzioni, and D.S. Weld. 2001. Scaling
Question Answering to the Web. In Proceedings of
the 10th World Wide Web Conference, Hong Kong,
150-161.
M. Sahami and T. Heilman. 2006. A Web-based kernel
function for measuring the similarity of short text
snippets. In Proceedings of 15th International World
Wide Web Conference. 377-386.
Chin-Yew Lin and E.H. Hovy. 2003. Automatic Evalua-
tion of Summaries Using N-gram Co-occurrence Sta-
tistics, In Proceedings of HLT-NAACL, 71–78.
I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin,
and B. Sundheim. 2002. SUMMAC: a text summari-
zation evaluation, Natural Language Engineering, 8
(1):43-68.
Y, Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka.
2006. Graph-based word clustering using Web search
engine. In Proceedings of EMNLP 2006, 542-550.
G. Harry McLaughlin. 1969. Reading at “Impossible”
Speeds. Journal of Reading, 12(6):449-454,502-510.
Radu Soricut and Eric Brill. 2006. Automatic question
answering using the web: Beyond the factoid. Jour-
nal of Information Retrieval - Special Issue on Web
Information Retrieval, 9:191–206.
G. Salton, and C. Buckley. 1988. Term-weighting ap-
proaches in automatic text retrieval. Information
Processing &amp; Management, 24, 5, 513-523.
E. J. Santos, A. A. Mohamed, and Q. Zhao. 2004. &amp;quot;Au-
tomatic Evaluation of Summaries Using Document
Graphs,&amp;quot; Text Summarization Branches Out. Pro-
ceedings of the ACL-04 Workshop, Barcelona, Spain,
66-73.
R. Srihari and W.A. Li. 2000. Question Answering Sys-
tem Supported by Information Extraction. In Pro-
ceedings of the 1st Meeting of the North American
Chapter of the Association for Computational Lin-
guistics (NAACL-00), 166-172.
R. Varadarajan and V. Hristidis. 2006. A system for
query-specific document summarization, ACM 15th
Conference on Information and Knowledge Man-
agement (CIKM), Arlington, VA, 622-631.
</reference>
<page confidence="0.997317">
106
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.953137">
<title confidence="0.998387">Automated Skimming in Response to Questions for NonVisual Readers</title>
<author confidence="0.999906">Debra Yarrington Kathleen F McCoy</author>
<affiliation confidence="0.999962">Dept. of Computer and Information Science Dept. of Computer and Information Science University of Delaware University of Delaware</affiliation>
<address confidence="0.994924">Newark, DE, 19716, USA Newark, DE, 19716, USA</address>
<email confidence="0.999819">yarringt@eecis.udel.edumccoy@cis.udel.edu</email>
<abstract confidence="0.998277333333333">This paper presents factors in designing a system for automatically skimming text documents in response to a question. The system will take a potentially complex question and a single document and return a Web page containing links to text related to the question. The goal is that these text areas be those that visual readers would spend the most time on when skimming for the answer to a question. To identify these areas, we had visual readers skim for an answer to a complex question while being tracked by an eye-tracking system. Analysis of these results indicates that text with semantic connections to the question are of interest, but these connections are much looser than can be identified with traditional Question-Answering or Information Retrieval techniques. Instead, we are expanding traditional semantic treatments by using a Web search. The goal of this system is to give nonvisual readers information similar to what visual readers get when skimming through a document in response to a question.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Abney</author>
<author>M Collins</author>
<author>A Singhal</author>
</authors>
<title>Answer Extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of ANLP</booktitle>
<pages>296--301</pages>
<contexts>
<context position="19225" citStr="Abney et al., 2000" startWordPosition="3175" endWordPosition="3178">d potential answers. These systems usually do not rely on external knowledge sources and are limited in the amount of ontological information that can be included in the system. The questions are usually fact-based in form (e.g., “How tall is Mt. Everest?”). These systems take a question and query a potentially large set of documents (e.g., the World Wide Web) to find the answer. A common technique is to determine a question type (e.g., “How many ...?” would be classified as ‘numerical’, whereas “Who was ...?” would be classified as ‘person’, etc.) and then locate answers of the correct type (Abney et al., 2000; Kwok et al., 2001; Srihari and Li, 2000; Galea, 2003). Questions are also frequently reformulated for pattern matching (e.g., “Who was the first American Astronaut in space?” becomes, “The first American Astronaut in space was” (Kwok et al., 2001; Brill et al., 2002)). Many systems submit multiple queries to a document corpus, relying on redundancy of the answer to handle incorrect answers, poorly constructed answers or documents that don’t contain the answer (e.g., Brill et al., 2002; Kwok et al., 102 2001). For these queries, systems often include synonyms, hypernyms, hyponyms, etc. in the</context>
</contexts>
<marker>Abney, Collins, Singhal, 2000</marker>
<rawString>S. Abney, M. Collins, and A. Singhal. 2000. Answer Extraction. In Proceedings of ANLP 2000, 296-301.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>M J Cafarella</author>
<author>S Soderland</author>
<author>M Broadhead</author>
<author>O Etzioni</author>
</authors>
<title>Open information extraction from the Web.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2670--2676</pages>
<contexts>
<context position="19990" citStr="Banko et al. (2007)" startWordPosition="3299" endWordPosition="3302"> American Astronaut in space?” becomes, “The first American Astronaut in space was” (Kwok et al., 2001; Brill et al., 2002)). Many systems submit multiple queries to a document corpus, relying on redundancy of the answer to handle incorrect answers, poorly constructed answers or documents that don’t contain the answer (e.g., Brill et al., 2002; Kwok et al., 102 2001). For these queries, systems often include synonyms, hypernyms, hyponyms, etc. in the query terms used for document and text retrieval (Hovy et al.,2000; Katz et al., 2005). In an attempt to answer more complex relational queries, Banko et al. (2007) parsed training data into relational tuples for use in classifying text tagged for part of speech, chunked into noun phrases, and then tagged the relations for probability. Soricut and Brill (2006) trained data on FAQ knowledge bases from the World Wide Web, resulting in approximately 1 million question-answer pairs. This system related potential answers to questions using probability models computed using the FAQ knowledge base. Another area of research that may lend useful techniques for connecting and retrieving relevant text to a question is query-biased text summarization. With many summ</context>
</contexts>
<marker>Banko, Cafarella, Soderland, Broadhead, Etzioni, 2007</marker>
<rawString>M. Banko, M. J. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open information extraction from the Web. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, 2670-2676.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bollegala</author>
<author>Y Matsuo</author>
<author>M Ishizuka</author>
</authors>
<title>Measuring semantic similarity between words using Web search engines.</title>
<date>2007</date>
<booktitle>In Proceedings of WWW</booktitle>
<pages>757--766</pages>
<contexts>
<context position="27549" citStr="Bollegala et al. (2007)" startWordPosition="4573" endWordPosition="4576">sed to locate and rank paragraphs in our question document. Our approach is similar in spirit to other researchers using the Web to identify semantic relations. Matsuo et al. (2006) looked at the number of hits of each of two words as a single keyword versus the number of hits using both words as keywords to rate the semantic similarity of two words. Chen et al. (2006) used a similar approach to determine the semantic similarity between two words: with a Web search using word P as the query term, they counted the number of times word Q occurred in the snippet of text returned, and vice versa. Bollegala et al. (2007) determined semantic relationships by extracting lexico-syntactic patterns from the snippets returned from a search on two keywords (e.g.,“’x’ is a ‘y’”) and extracting the relationship of the two words based on the pattern. Sahami and Heilman (2006) used the snippets from a word search to form a set of words weighted using TF/IDF, and then determined the semantic similarity of two keywords by the similarity of two word sets returned in those snippets. Preliminary results from our approach have been encouraging. For example, with the question, “How does Marijuana affect the brain?”, the expand</context>
</contexts>
<marker>Bollegala, Matsuo, Ishizuka, 2007</marker>
<rawString>D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007. Measuring semantic similarity between words using Web search engines. In Proceedings of WWW 2007. 757-766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>J Lin</author>
<author>M Banko</author>
<author>S Domais</author>
<author>A Ng</author>
</authors>
<title>Data-Intensive Question Answering.</title>
<date>2001</date>
<booktitle>In Proceedings of the TREC-10 Conference,</booktitle>
<pages>183--189</pages>
<location>NIST, Gaithersburg, MD,</location>
<marker>Brill, Lin, Banko, Domais, Ng, 2001</marker>
<rawString>Brill, E., Lin, J., Banko, M., Domais, S. and Ng, A. 2001. Data-Intensive Question Answering. In Proceedings of the TREC-10 Conference, NIST, Gaithersburg, MD, 183-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carbonell</author>
<author>J Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGIR ’98,</booktitle>
<pages>335--336</pages>
<location>New York, NY, USA,</location>
<contexts>
<context position="21084" citStr="Carbonell and Goldstein, 1998" startWordPosition="3468" endWordPosition="3471">may lend useful techniques for connecting and retrieving relevant text to a question is query-biased text summarization. With many summarization schemes, a good deal of effort has been placed on identifying the main topic or topics of the document. In query biased text summarization, however, the topic is identified a priori, and the task is to locate relevant text within a document or set of documents. In multidocument summarization systems, redundancy may be indicative of relevance, but should be eliminated from the resulting summary. Thus a concern is measuring relevance versus redundancy (Carbonell and Goldstein, 1998; Hovy et al., 2005; Otterbacher et al., 2006). Like Question Answering systems, many summarization systems simply match the query terms, expanded to include synonyms, hypernyms, hyponyms, etc., to text in the document or documents (Varadarajan and Hristidis, 2006; Chali, 2002) Our system is unique in that it has as its goal not just to answer a question or create a summary, but to return information visual skimmers glean while skimming through a document. Questions posed to the system will range from simple to complex in nature, and the answer must be found within a single document, regardles</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>J. Carbonell and J. Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of SIGIR ’98, New York, NY, USA, 335-336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Chali</author>
</authors>
<title>Generic and query-based text summarization using lexical cohesion,</title>
<date>2002</date>
<booktitle>In Proceedings of the Fifteenth Canadian Conference on Artificial Intelligence,</booktitle>
<pages>293--303</pages>
<location>Calgary,</location>
<contexts>
<context position="21362" citStr="Chali, 2002" startWordPosition="3512" endWordPosition="3513">he topic is identified a priori, and the task is to locate relevant text within a document or set of documents. In multidocument summarization systems, redundancy may be indicative of relevance, but should be eliminated from the resulting summary. Thus a concern is measuring relevance versus redundancy (Carbonell and Goldstein, 1998; Hovy et al., 2005; Otterbacher et al., 2006). Like Question Answering systems, many summarization systems simply match the query terms, expanded to include synonyms, hypernyms, hyponyms, etc., to text in the document or documents (Varadarajan and Hristidis, 2006; Chali, 2002) Our system is unique in that it has as its goal not just to answer a question or create a summary, but to return information visual skimmers glean while skimming through a document. Questions posed to the system will range from simple to complex in nature, and the answer must be found within a single document, regardless of the form the answer takes. Questions can be on any topic. With complex questions, it is rarely possible to categorize the type of question (and thus the expected answer type). Intuitively, it appears equally useless to attempt reformulation of the query for pattern matchin</context>
</contexts>
<marker>Chali, 2002</marker>
<rawString>Y. Chali. 2002. Generic and query-based text summarization using lexical cohesion, In Proceedings of the Fifteenth Canadian Conference on Artificial Intelligence, Calgary, May, 293-303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Chen</author>
<author>M Lin</author>
<author>Y Wei</author>
</authors>
<title>Novel association measures using web search with double checking.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL</booktitle>
<pages>1009--1016</pages>
<contexts>
<context position="27297" citStr="Chen et al. (2006)" startWordPosition="4527" endWordPosition="4530"> using a 100-word window surrounding the snippet. We are using only nonstop words in the cluster, and weighting the words based on their total number of occurrences in the windows. These word clusters, along with the expanded baseline words, are used to locate and rank paragraphs in our question document. Our approach is similar in spirit to other researchers using the Web to identify semantic relations. Matsuo et al. (2006) looked at the number of hits of each of two words as a single keyword versus the number of hits using both words as keywords to rate the semantic similarity of two words. Chen et al. (2006) used a similar approach to determine the semantic similarity between two words: with a Web search using word P as the query term, they counted the number of times word Q occurred in the snippet of text returned, and vice versa. Bollegala et al. (2007) determined semantic relationships by extracting lexico-syntactic patterns from the snippets returned from a search on two keywords (e.g.,“’x’ is a ‘y’”) and extracting the relationship of the two words based on the pattern. Sahami and Heilman (2006) used the snippets from a word search to form a set of words weighted using TF/IDF, and then deter</context>
</contexts>
<marker>Chen, Lin, Wei, 2006</marker>
<rawString>H. Chen, M. Lin, and Y. Wei. 2006. Novel association measures using web search with double checking. In Proceedings of the COLING/ACL 2006. 1009-1016.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Felbaum</author>
</authors>
<title>WordNet an Electronic Database,</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Boston/Cambridge:</location>
<contexts>
<context position="4970" citStr="Felbaum, 1998" startWordPosition="792" endWordPosition="793">nterested in more complex questions in which the answer could not be found using pattern matching and in which the answer would require at least a few sentences, not necessarily contiguous within a document. From an NLP standpoint, locating longer answers with relevant information occuring in more than one place that may or may not have words or word sequences in common with the question poses an interesting and difficult problem. The problem becomes making semantic connections within any domain that are more loosely associated than the synonyms, hypernyms, hyponyms, etc. provided by WordNet (Felbaum, 1998). Indeed, the questions that students had the most difficulty with were more complex in nature. Thus we needed to find out whether visual skimmers were able to locate text in documents relevant to complex questions and, if so, what connections visual skimmers are making in terms of the text they choose to focus on. 2.1 Task Description To identify how visual readers skim documents to answer questions, we collected 14 questions obtained from students’ homework assignments, along with an accompanying document per question from which the answer could be obtained. The questions chosen were on a wi</context>
<context position="23497" citStr="Felbaum, 1998" startWordPosition="3872" endWordPosition="3873">rm was matched to text in each paragraph, and paragraphs were ranked for matching using the summation of, for each query term, the number of times it occurred in the paragraph multiplied by its weight. Results of this baseline ranking were poor. In none of the 14 documents did this method connect the question to the text relevant to the answer. This was expected. This original set of questions was purposely chosen because of the complex relationship between the question and answer text. Next we expanded the set of query terms to include synonyms, hypernyms, and hyponyms as defined in WordNet (Felbaum, 1998). We included all senses of each word (query term). Irrelevant senses resulted in the inclusion of terms that were no more likely to occur frequently than any other random word, and thus had no effect on the resulting ranking of paragraphs. Again, each of the words in the expanded set of query terms was weighted as described above, and paragraphs were ranked accordingly. Again, results were poor. Paragraphs ranked highly were no more likely to contain the answer, nor were they likely to be areas focused on by the visual skimmers in our collected skimming data. Clearly, for complex questions, w</context>
</contexts>
<marker>Felbaum, 1998</marker>
<rawString>C. Felbaum. 1998. WordNet an Electronic Database, Boston/Cambridge: MIT Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Galea 2003</author>
</authors>
<title>Open-domain Surface-Based Question Answering System.</title>
<booktitle>In Proceedings of the Computer Science Annual Workshop</booktitle>
<institution>(CSAW), University of Malta.</institution>
<marker>2003, </marker>
<rawString>A. Galea.2003. Open-domain Surface-Based Question Answering System. In Proceedings of the Computer Science Annual Workshop (CSAW), University of Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Hovy</author>
<author>L Gerber</author>
<author>U Hermjakob</author>
<author>M Junk</author>
<author>C-Y Lin</author>
</authors>
<title>Question Answering in Webclopedia.</title>
<date>2000</date>
<booktitle>In Proceedings of the TREC-9 Conference.</booktitle>
<pages>655--664</pages>
<location>NIST, Gaithersburg, MD.</location>
<marker>Hovy, Gerber, Hermjakob, Junk, Lin, 2000</marker>
<rawString>E. H. Hovy, L. Gerber, U. Hermjakob, M. Junk, and C.-Y. Lin. 2000. Question Answering in Webclopedia. In Proceedings of the TREC-9 Conference. NIST, Gaithersburg, MD. November 2000. 655-664.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>C Y Lin</author>
</authors>
<title>Automated Text Summarization in SUMMARIST.</title>
<date>1997</date>
<booktitle>In Proceedings of the Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>18--24</pages>
<location>Madrid,</location>
<marker>Hovy, Lin, 1997</marker>
<rawString>E. Hovy and C.Y. Lin. 1997. Automated Text Summarization in SUMMARIST. In Proceedings of the Workshop on Intelligent Scalable Text Summarization, Madrid, Spain, 18-24.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Boris Katz</author>
<author>Gregory Marton</author>
</authors>
<location>Gary Borchardt, Alexis Brownell, Sue Felshin, Daniel Loreto, Jesse Louis-</location>
<marker>Katz, Marton, </marker>
<rawString>Boris Katz, Gregory Marton, Gary Borchardt, Alexis Brownell, Sue Felshin, Daniel Loreto, Jesse Louis-</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Lu Rosenberg</author>
<author>Federico Mora</author>
<author>Stephan Stiller</author>
<author>Ozlem Uzuner</author>
<author>Angela Wilcox</author>
</authors>
<title>External Knowledge Sources for Question Answering</title>
<date>2005</date>
<booktitle>Proceedings of the 14th Annual Text REtrieval Conference (TREC2005),</booktitle>
<location>Gaithersburg, MD.</location>
<marker>Rosenberg, Mora, Stiller, Uzuner, Wilcox, 2005</marker>
<rawString>Rosenberg, Ben Lu, Federico Mora, Stephan Stiller, Ozlem Uzuner, and Angela Wilcox. 2005. External Knowledge Sources for Question Answering Proceedings of the 14th Annual Text REtrieval Conference (TREC2005), November 2005, Gaithersburg, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Kwok</author>
<author>O Etzioni</author>
<author>D S Weld</author>
</authors>
<title>Scaling Question Answering to the Web.</title>
<date>2001</date>
<booktitle>In Proceedings of the 10th World Wide Web Conference, Hong Kong,</booktitle>
<pages>150--161</pages>
<contexts>
<context position="19244" citStr="Kwok et al., 2001" startWordPosition="3179" endWordPosition="3182"> These systems usually do not rely on external knowledge sources and are limited in the amount of ontological information that can be included in the system. The questions are usually fact-based in form (e.g., “How tall is Mt. Everest?”). These systems take a question and query a potentially large set of documents (e.g., the World Wide Web) to find the answer. A common technique is to determine a question type (e.g., “How many ...?” would be classified as ‘numerical’, whereas “Who was ...?” would be classified as ‘person’, etc.) and then locate answers of the correct type (Abney et al., 2000; Kwok et al., 2001; Srihari and Li, 2000; Galea, 2003). Questions are also frequently reformulated for pattern matching (e.g., “Who was the first American Astronaut in space?” becomes, “The first American Astronaut in space was” (Kwok et al., 2001; Brill et al., 2002)). Many systems submit multiple queries to a document corpus, relying on redundancy of the answer to handle incorrect answers, poorly constructed answers or documents that don’t contain the answer (e.g., Brill et al., 2002; Kwok et al., 102 2001). For these queries, systems often include synonyms, hypernyms, hyponyms, etc. in the query terms used f</context>
</contexts>
<marker>Kwok, Etzioni, Weld, 2001</marker>
<rawString>C. Kwok, O. Etzioni, and D.S. Weld. 2001. Scaling Question Answering to the Web. In Proceedings of the 10th World Wide Web Conference, Hong Kong, 150-161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahami</author>
<author>T Heilman</author>
</authors>
<title>A Web-based kernel function for measuring the similarity of short text snippets.</title>
<date>2006</date>
<booktitle>In Proceedings of 15th International World Wide Web Conference.</booktitle>
<pages>377--386</pages>
<contexts>
<context position="27799" citStr="Sahami and Heilman (2006)" startWordPosition="4610" endWordPosition="4613">ord versus the number of hits using both words as keywords to rate the semantic similarity of two words. Chen et al. (2006) used a similar approach to determine the semantic similarity between two words: with a Web search using word P as the query term, they counted the number of times word Q occurred in the snippet of text returned, and vice versa. Bollegala et al. (2007) determined semantic relationships by extracting lexico-syntactic patterns from the snippets returned from a search on two keywords (e.g.,“’x’ is a ‘y’”) and extracting the relationship of the two words based on the pattern. Sahami and Heilman (2006) used the snippets from a word search to form a set of words weighted using TF/IDF, and then determined the semantic similarity of two keywords by the similarity of two word sets returned in those snippets. Preliminary results from our approach have been encouraging. For example, with the question, “How does Marijuana affect the brain?”, the expanded set of keywords included, “hippocampus, receptors, THC, memory, neuron”. These words were present in both the paragraph containing the answer and the second-most commonly focused on paragraph in our study. While neither our baseline nor our expand</context>
</contexts>
<marker>Sahami, Heilman, 2006</marker>
<rawString>M. Sahami and T. Heilman. 2006. A Web-based kernel function for measuring the similarity of short text snippets. In Proceedings of 15th International World Wide Web Conference. 377-386.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>E H Hovy</author>
</authors>
<title>Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics,</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL,</booktitle>
<pages>71--78</pages>
<contexts>
<context position="30110" citStr="Lin and Hovy, 2003" startWordPosition="4997" endWordPosition="5000">s now the entire content of the paragraph is considered relevant and thus we are trying to generate semantic relationships between the question and potentially unrelated text. While the system only allows us to define AOIs as rectangular areas (and thus we can’t do a sentence-bysentence analysis), we may wish to define AOIs as small as 2 lines of text to narrow in on exactly where subjects chose to focus. 4.2 Ranking System Refinement It is worth mentioning that, while a good deal of research has been done on evaluating the goodness of automatically generated text summaries (Mani et al.,2002; Lin and Hovy, 2003; Santos et al., 2004) our system is intended to mimic the actions of skimmers when answering questions, and thus our measure of goodness will be our system’s ability to recreate the retrieval of text focused on by our visual skimmers. This gives us a distinct advantage over other systems in measuring goodness, as defining a measure of goodness can prove difficult. In future work, we will be exploring different methods of ranking text such that the system returns results most similar to the results obtained from the visual skimming studies. The system will then be used on other questions and d</context>
</contexts>
<marker>Lin, Hovy, 2003</marker>
<rawString>Chin-Yew Lin and E.H. Hovy. 2003. Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics, In Proceedings of HLT-NAACL, 71–78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>G Klein</author>
<author>D House</author>
<author>L Hirschman</author>
<author>T Firmin</author>
<author>B Sundheim</author>
</authors>
<title>SUMMAC: a text summarization evaluation,</title>
<date>2002</date>
<journal>Natural Language Engineering,</journal>
<volume>8</volume>
<pages>1--43</pages>
<marker>Mani, Klein, House, Hirschman, Firmin, Sundheim, 2002</marker>
<rawString>I. Mani, G. Klein, D. House, L. Hirschman, T. Firmin, and B. Sundheim. 2002. SUMMAC: a text summarization evaluation, Natural Language Engineering, 8 (1):43-68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsuo</author>
<author>T Sakaki</author>
<author>K Uchiyama</author>
<author>M Ishizuka</author>
</authors>
<title>Graph-based word clustering using Web search engine.</title>
<date>2006</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>542--550</pages>
<contexts>
<context position="27107" citStr="Matsuo et al. (2006)" startWordPosition="4488" endWordPosition="4491">of words related semantically to the question, we are taking the top 50 URLs, going to their correlating Web page, locating the snippet of text within the page, and creating a cluster of words using a 100-word window surrounding the snippet. We are using only nonstop words in the cluster, and weighting the words based on their total number of occurrences in the windows. These word clusters, along with the expanded baseline words, are used to locate and rank paragraphs in our question document. Our approach is similar in spirit to other researchers using the Web to identify semantic relations. Matsuo et al. (2006) looked at the number of hits of each of two words as a single keyword versus the number of hits using both words as keywords to rate the semantic similarity of two words. Chen et al. (2006) used a similar approach to determine the semantic similarity between two words: with a Web search using word P as the query term, they counted the number of times word Q occurred in the snippet of text returned, and vice versa. Bollegala et al. (2007) determined semantic relationships by extracting lexico-syntactic patterns from the snippets returned from a search on two keywords (e.g.,“’x’ is a ‘y’”) and </context>
</contexts>
<marker>Matsuo, Sakaki, Uchiyama, Ishizuka, 2006</marker>
<rawString>Y, Matsuo, T. Sakaki, K. Uchiyama, and M. Ishizuka. 2006. Graph-based word clustering using Web search engine. In Proceedings of EMNLP 2006, 542-550.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Harry McLaughlin</author>
</authors>
<title>Reading at “Impossible” Speeds.</title>
<date>1969</date>
<journal>Journal of Reading,</journal>
<pages>12--6</pages>
<marker>McLaughlin, 1969</marker>
<rawString>G. Harry McLaughlin. 1969. Reading at “Impossible” Speeds. Journal of Reading, 12(6):449-454,502-510.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Eric Brill</author>
</authors>
<title>Automatic question answering using the web: Beyond the factoid.</title>
<date>2006</date>
<journal>Journal of Information Retrieval - Special Issue on Web Information Retrieval,</journal>
<pages>9--191</pages>
<contexts>
<context position="20188" citStr="Soricut and Brill (2006)" startWordPosition="3330" endWordPosition="3333">on redundancy of the answer to handle incorrect answers, poorly constructed answers or documents that don’t contain the answer (e.g., Brill et al., 2002; Kwok et al., 102 2001). For these queries, systems often include synonyms, hypernyms, hyponyms, etc. in the query terms used for document and text retrieval (Hovy et al.,2000; Katz et al., 2005). In an attempt to answer more complex relational queries, Banko et al. (2007) parsed training data into relational tuples for use in classifying text tagged for part of speech, chunked into noun phrases, and then tagged the relations for probability. Soricut and Brill (2006) trained data on FAQ knowledge bases from the World Wide Web, resulting in approximately 1 million question-answer pairs. This system related potential answers to questions using probability models computed using the FAQ knowledge base. Another area of research that may lend useful techniques for connecting and retrieving relevant text to a question is query-biased text summarization. With many summarization schemes, a good deal of effort has been placed on identifying the main topic or topics of the document. In query biased text summarization, however, the topic is identified a priori, and t</context>
<context position="22019" citStr="Soricut and Brill (2006)" startWordPosition="3626" endWordPosition="3629">has as its goal not just to answer a question or create a summary, but to return information visual skimmers glean while skimming through a document. Questions posed to the system will range from simple to complex in nature, and the answer must be found within a single document, regardless of the form the answer takes. Questions can be on any topic. With complex questions, it is rarely possible to categorize the type of question (and thus the expected answer type). Intuitively, it appears equally useless to attempt reformulation of the query for pattern matching. This intuition is born out by Soricut and Brill (2006) who stated that in their study reformulating complex questions more often hurt performance than improved it. Answering complex questions within a single document when the answer may not be straightforward in nature poses a challenging problem. 3.2 Baseline Processing Our baseline system attempted to identify areas of interest by matching against the query in the tradition of Open Domain Question Answering. For our baseline, we used the nonfunction words in each question as our query terms. The terms were weighted with a variant of TF/IDF (Salton and Buckley, 1988) in which terms were weighted</context>
</contexts>
<marker>Soricut, Brill, 2006</marker>
<rawString>Radu Soricut and Eric Brill. 2006. Automatic question answering using the web: Beyond the factoid. Journal of Information Retrieval - Special Issue on Web Information Retrieval, 9:191–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>C Buckley</author>
</authors>
<title>Term-weighting approaches in automatic text retrieval.</title>
<date>1988</date>
<journal>Information Processing &amp; Management,</journal>
<volume>24</volume>
<pages>513--523</pages>
<contexts>
<context position="22590" citStr="Salton and Buckley, 1988" startWordPosition="3716" endWordPosition="3719">This intuition is born out by Soricut and Brill (2006) who stated that in their study reformulating complex questions more often hurt performance than improved it. Answering complex questions within a single document when the answer may not be straightforward in nature poses a challenging problem. 3.2 Baseline Processing Our baseline system attempted to identify areas of interest by matching against the query in the tradition of Open Domain Question Answering. For our baseline, we used the nonfunction words in each question as our query terms. The terms were weighted with a variant of TF/IDF (Salton and Buckley, 1988) in which terms were weighted by the inverse of the number of paragraphs they occurred in within the document. This weighting scheme was designed to give lower weight to words associated with the document topic and thus conveying less information about relevance to the question. Each query term was matched to text in each paragraph, and paragraphs were ranked for matching using the summation of, for each query term, the number of times it occurred in the paragraph multiplied by its weight. Results of this baseline ranking were poor. In none of the 14 documents did this method connect the quest</context>
</contexts>
<marker>Salton, Buckley, 1988</marker>
<rawString>G. Salton, and C. Buckley. 1988. Term-weighting approaches in automatic text retrieval. Information Processing &amp; Management, 24, 5, 513-523.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E J Santos</author>
<author>A A Mohamed</author>
<author>Q Zhao</author>
</authors>
<title>Automatic Evaluation of Summaries Using Document Graphs,&amp;quot; Text Summarization Branches Out.</title>
<date>2004</date>
<booktitle>Proceedings of the ACL-04 Workshop,</booktitle>
<pages>66--73</pages>
<location>Barcelona,</location>
<contexts>
<context position="30132" citStr="Santos et al., 2004" startWordPosition="5001" endWordPosition="5004">tent of the paragraph is considered relevant and thus we are trying to generate semantic relationships between the question and potentially unrelated text. While the system only allows us to define AOIs as rectangular areas (and thus we can’t do a sentence-bysentence analysis), we may wish to define AOIs as small as 2 lines of text to narrow in on exactly where subjects chose to focus. 4.2 Ranking System Refinement It is worth mentioning that, while a good deal of research has been done on evaluating the goodness of automatically generated text summaries (Mani et al.,2002; Lin and Hovy, 2003; Santos et al., 2004) our system is intended to mimic the actions of skimmers when answering questions, and thus our measure of goodness will be our system’s ability to recreate the retrieval of text focused on by our visual skimmers. This gives us a distinct advantage over other systems in measuring goodness, as defining a measure of goodness can prove difficult. In future work, we will be exploring different methods of ranking text such that the system returns results most similar to the results obtained from the visual skimming studies. The system will then be used on other questions and documents and compared </context>
</contexts>
<marker>Santos, Mohamed, Zhao, 2004</marker>
<rawString>E. J. Santos, A. A. Mohamed, and Q. Zhao. 2004. &amp;quot;Automatic Evaluation of Summaries Using Document Graphs,&amp;quot; Text Summarization Branches Out. Proceedings of the ACL-04 Workshop, Barcelona, Spain, 66-73.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Srihari</author>
<author>W A Li</author>
</authors>
<title>Question Answering System Supported by Information Extraction.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-00),</booktitle>
<pages>166--172</pages>
<contexts>
<context position="19266" citStr="Srihari and Li, 2000" startWordPosition="3183" endWordPosition="3186">lly do not rely on external knowledge sources and are limited in the amount of ontological information that can be included in the system. The questions are usually fact-based in form (e.g., “How tall is Mt. Everest?”). These systems take a question and query a potentially large set of documents (e.g., the World Wide Web) to find the answer. A common technique is to determine a question type (e.g., “How many ...?” would be classified as ‘numerical’, whereas “Who was ...?” would be classified as ‘person’, etc.) and then locate answers of the correct type (Abney et al., 2000; Kwok et al., 2001; Srihari and Li, 2000; Galea, 2003). Questions are also frequently reformulated for pattern matching (e.g., “Who was the first American Astronaut in space?” becomes, “The first American Astronaut in space was” (Kwok et al., 2001; Brill et al., 2002)). Many systems submit multiple queries to a document corpus, relying on redundancy of the answer to handle incorrect answers, poorly constructed answers or documents that don’t contain the answer (e.g., Brill et al., 2002; Kwok et al., 102 2001). For these queries, systems often include synonyms, hypernyms, hyponyms, etc. in the query terms used for document and text r</context>
</contexts>
<marker>Srihari, Li, 2000</marker>
<rawString>R. Srihari and W.A. Li. 2000. Question Answering System Supported by Information Extraction. In Proceedings of the 1st Meeting of the North American Chapter of the Association for Computational Linguistics (NAACL-00), 166-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Varadarajan</author>
<author>V Hristidis</author>
</authors>
<title>A system for query-specific document summarization,</title>
<date>2006</date>
<booktitle>ACM 15th Conference on Information and Knowledge Management (CIKM),</booktitle>
<pages>622--631</pages>
<location>Arlington, VA,</location>
<contexts>
<context position="21348" citStr="Varadarajan and Hristidis, 2006" startWordPosition="3507" endWordPosition="3511">ed text summarization, however, the topic is identified a priori, and the task is to locate relevant text within a document or set of documents. In multidocument summarization systems, redundancy may be indicative of relevance, but should be eliminated from the resulting summary. Thus a concern is measuring relevance versus redundancy (Carbonell and Goldstein, 1998; Hovy et al., 2005; Otterbacher et al., 2006). Like Question Answering systems, many summarization systems simply match the query terms, expanded to include synonyms, hypernyms, hyponyms, etc., to text in the document or documents (Varadarajan and Hristidis, 2006; Chali, 2002) Our system is unique in that it has as its goal not just to answer a question or create a summary, but to return information visual skimmers glean while skimming through a document. Questions posed to the system will range from simple to complex in nature, and the answer must be found within a single document, regardless of the form the answer takes. Questions can be on any topic. With complex questions, it is rarely possible to categorize the type of question (and thus the expected answer type). Intuitively, it appears equally useless to attempt reformulation of the query for p</context>
</contexts>
<marker>Varadarajan, Hristidis, 2006</marker>
<rawString>R. Varadarajan and V. Hristidis. 2006. A system for query-specific document summarization, ACM 15th Conference on Information and Knowledge Management (CIKM), Arlington, VA, 622-631.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>