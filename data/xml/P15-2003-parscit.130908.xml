<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.077091">
<title confidence="0.9988225">
Improving Distributed Representation of Word Sense
via WordNet Gloss Composition and Context Clustering
</title>
<author confidence="0.999055">
Tao Chen1 Ruifeng Xu1* Yulan He2 Xuan Wang1
</author>
<affiliation confidence="0.996035333333333">
1Shenzhen Engineering Laboratory of Performance Robots at Digital Stage,
Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China
2School of Engineering and Applied Science, Aston University, UK
</affiliation>
<email confidence="0.958436">
chentao1999@gmail.com {xuruifeng,wangxuan}@hitsz.edu.cn
y.he9@aston.ac.uk
</email>
<sectionHeader confidence="0.993792" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99998845">
In recent years, there has been an increas-
ing interest in learning a distributed rep-
resentation of word sense. Traditional
context clustering based models usually
require careful tuning of model parame-
ters, and typically perform worse on infre-
quent word senses. This paper presents a
novel approach which addresses these lim-
itations by first initializing the word sense
embeddings through learning sentence-
level embeddings from WordNet glosses
using a convolutional neural networks.
The initialized word sense embeddings are
used by a context clustering based model
to generate the distributed representations
of word senses. Our learned represen-
tations outperform the publicly available
embeddings on 2 out of 4 metrics in the
word similarity task, and 6 out of 13 sub
tasks in the analogical reasoning task.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999751034482759">
With the rapid development of deep neural net-
works and parallel computing, distributed repre-
sentation of knowledge attracts much research in-
terest. Models for learning distributed representa-
tions of knowledge have been proposed at differ-
ent granularity level, including word sense level
(Huang et al., 2012; Chen et al., 2014; Neelakan-
tan et al., 2014; Tian et al., 2014; Guo et al.,
2014), word level (Rummelhart, 1986; Bengio et
al., 2003; Collobert and Weston, 2008; Mnih and
Hinton, 2009; Mikolov et al., 2010; Mikolov et
al., 2013), phrase level (Socher et al., 2010; Zhang
et al., 2014; Cho et al., 2014), sentence level
(Mikolov et al., 2010; Socher et al., 2013; Kalch-
brenner et al., 2014; Kim, 2014; Le and Mikolov,
2014), discourse level (Ji and Eisenstein, 2014)
and document level (Le and Mikolov, 2014).
In distributed representations of word senses,
each word sense is usually represented by a dense
and real-valued vector in a low-dimensional space
which captures the contextual semantic informa-
tion. Most existing approaches adopted a cluster-
based paradigm, which produces different sense
vectors for each polysemy or homonymy through
clustering the context of a target word. However,
this paradigm usually has two limitations: (1) The
performance of these approaches is sensitive to
the clustering algorithm which requires the setting
of the sense number for each word. For exam-
ple, Neelakantan et al. (2014) proposed two clus-
tering based model: the Multi-Sense Skip-Gram
(MSSG) model and Non-Parametric Multi-Sense
Skip-Gram (NP-MSSG) model. MSSG assumes
each word has the same k-sense (e.g. k = 3),
i.e., the same number of possible senses. How-
ever, the number of senses in WordNet (Miller,
1995) varies from 1 such as “ben” to 75 such as
“break”. As such, fixing the number of senses
for all words would result in poor representations.
NP-MSSG can learn the number of senses for each
word directly from data. But it requires a tuning
of a hyperparameter A which controls the creation
of cluster centroids during training. Different A
needs to be tuned for different datasets. (2) The
initial value of sense representation is critical for
most statistical clustering based approaches. How-
ever, previous approaches usually adopted ran-
dom initialization (Neelakantan et al., 2014) or the
mean average of candidate words in a gloss (Chen
et al., 2014). As a result, they may not produce
optimal clustering results for word senses.
Focusing on the aforementioned two problems,
this paper proposes to learn distributed representa-
tions of word senses through WordNet gloss com-
position and context clustering. The basic idea is
that a word sense is represented as a synonym set
(synset) in WordNet. In this way, instead of as-
signing a fixed sense number to each word as in the
</bodyText>
<page confidence="0.938817">
15
</page>
<bodyText confidence="0.956287068965517">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 15–20,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
previous methods, different word will be assigned
with different number of senses based on their
corresponding entries in WordNet. Moreover, we
notice that each synset has a textual definition
(named as gloss). Naturally, we use a convolu-
tional neural network (CNN) to learn distributed
representations of these glosses (a.k.a. sense vec-
tors) through sentence composition. Then, we
modify MSSG for context clustering by initial-
izing the sense vectors with the representations
learned by our CNN-based sentence composition
model. We expect that word sense vectors ini-
tialized in this way would potentially lead to bet-
ter representations of word senses generated from
context clustering.
The obtained word sense representations are
evaluated on two tasks. One is word similarity
task, the other is analogical reasoning task pro-
vided by WordRep (Gao et al., 2014). The results
show that our approach attains comparable perfor-
mance on learning distributed representations of
word senses. In specific, our learned represen-
tation outperforms publicly available embeddings
on the globalSim and localSim metrics in word
similarity task, and 6 in 13 subtasks in the ana-
logical reasoning task.
</bodyText>
<sectionHeader confidence="0.939739" genericHeader="method">
2 Our Approach
</sectionHeader>
<bodyText confidence="0.999989391304348">
Our proposed approach first train a Continuous
Bag-Of-Words (CBOW) model (Mikolov et al.,
2013) from a large collection of raw text to gen-
erate word embeddings. These word embeddings
are then used by a Sentence Composition Model,
which takes glosses in WordNet as positive train-
ing data and randomly replaces part of the sen-
tences as negative training data to construct the
corresponding word sense vectors based on a one-
dimensional CNN. For example, a WordNet gloss
of word star is “an actor who plays a principal
role”. This is taken as a positive training example
when learning the word sense vector for “star”.
We concatenate the word embedding generated by
the CBOW model for each of the words in the
gloss, take the concatenated word embeddings as
an input to CNN, and get the output vector as one
sense vector of word star.
The learned sense vectors are fed into a vari-
ant of the previously proposed Multi-Sense Skip-
Gram Model (MSSG) to generates distributed rep-
resentations of word senses from a text corpus. We
name our approach as CNN-VMSSG.
</bodyText>
<subsectionHeader confidence="0.9990325">
2.1 Training Sense Vectors From WordNet
Glosses Using CNN
</subsectionHeader>
<bodyText confidence="0.99997025">
In this step, we learn the distributed representation
of each gloss sentence as the representation of the
corresponding synset. The training objective is to
minimize the ranking loss below:
</bodyText>
<equation confidence="0.981687">
�Gs = max{0,1 − f(s) + f(s′)} (1)
sEP
</equation>
<bodyText confidence="0.999969875">
Given a gloss sentence s as a positive training sam-
ple, we randomly replace some words (controlled
by a parameter A) in s to construct a negative train-
ing sample s′. We compute the scores f(s) and
f(s′) where f(�) is the scoring function represent-
ing the whole CNN architecture without the soft-
max layer. We expect f(s) and f(s′) to be close
to 1 and 0 respectively, and f(s) to be larger than
f(s′) by a margin of 1 for all the sentence in posi-
tive training set P.
The CNN architecture used in this component
follows the architecture proposed by (Kim, 2014)1
which is a slight variant of the architecture pro-
posed by (Collobert and Weston, 2008)2. It takes
a gloss matrix s as input where each column corre-
sponds to the distributed representation v,,,z E Rd
of a word wi in the sentence.
The idea behind the one-dimensional convolu-
tion is to take the dot product of the vector w
with each n-gram in the sentence to obtain an-
other sequence c, where n is the width of filter
in the convolutional layer. In order to make c to
cover different words in the negative sample cor-
responding a positive sample, in this work, we ran-
domly replace half of the words in a positive train-
ing sample to construct a negative training sample
(A = 0.5). For example, take the WordNet gloss
“an actor who plays a principal role” as a positive
sample, a negative training sample constructed by
this method may be “x1 actor who x2 x3 principal
x4”, where x1 to x4 are randomly selected words
in a vocabulary collected from a large corpus.
In the pooling layer, a max-overtime pooling
operation (Collobert et al., 2011), which forces the
network to capture the most useful local features
produced by the convolutional layers, is applied.
The model uses multiple filters (with varying win-
dow sizes) to obtain multiple features. These fea-
tures form the penultimate layer and are passed to
a fully connected softmax layer whose output is
</bodyText>
<footnote confidence="0.999975">
1https://github.com/yoonkim/CNN sentence
2http://ronan.collobert.com/senna/
</footnote>
<page confidence="0.998017">
16
</page>
<bodyText confidence="0.999956428571428">
the probability distribution over labels. The train-
ing error propagates back to fine-tune the parame-
ters of the CNN and the input word vectors. The
vector generated in the penultimate layer of the
CNN architecture is regarded as the sense vector
which captures the semantic content of the input
gloss to a certain degree.
</bodyText>
<subsectionHeader confidence="0.999868">
2.2 Context Clustering and VMSSG Model
</subsectionHeader>
<bodyText confidence="0.975446444444445">
Neelakantan et al. (2014) proposed the MSSG
model which extends the skip-gram model to learn
multi-prototype word embeddings by clustering
the word embeddings of context words around
each word. In this model, for each word w, the
corresponding word embedding vw E Rd, k-sense
vector vsk E Rd (k = 1, 2, ... , K) and k-context
cluster with center µk E Rd (k = 1, 2, ... , K) are
initialized randomly. The sense number K of each
word is a fixed parameter in the training algorithm.
We improve the MSSG model by using the
learned CBOW word embedding to initialize vw
and the sense vector trained by the sentence com-
position model to initialize vsk. We also use the
sense number of each word in WordNet Kw to re-
place K. We named this model as a variant of the
MSSG (VMSSG) model.
Algorithm 1 Algorithm of VMSSG model
</bodyText>
<listItem confidence="0.998034388888889">
1: Input: D, d,K1, ..., Kw, ..., KM�, M.
2: Initialize: bw E V, k E 11, ... , Kw}, initial-
ize vw to a pre-trained word vector, vswk to a
pre-trained sense vector for word w with sense
k, and µwk to a vector of random real value
E (�1,1)d.
3: for each w in D do
4: r &lt;-- random number E [1, M]
5: C &lt;-- 1wi−r, ..., wi−1, wi+1,..., wi+r}
1
6: vc2xr EwEC vw
7: k = arg maxk1sim(µwk , vc)}
8: Assign C to context cluster k.
9: Update µk.
10: C′ = NoisySamples(C)
11: Gradient update on vswk , vw in C, C′.
12: end for
13: Output: vswk , vw, bw E V, k E 11, ... , Kw}
</listItem>
<bodyText confidence="0.999814375">
The training algorithm of the VMSSG model is
shown as Algorithm 1, where D is a text corpus,
V is the vocabulary of D, |V  |is the vocabulary
size, M is the size of context window, vw is the
word embedding for w, swk is a kth context cluster
of word w, µwk is the centroid of cluster k for word
w. The function NoisySamples(C) randomly re-
places context words with noisy words from V .
</bodyText>
<sectionHeader confidence="0.988391" genericHeader="evaluation">
3 Evaluation and Discussion
</sectionHeader>
<subsectionHeader confidence="0.978591">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999957235294118">
In all experiments, we train word vectors and
sense vectors on a snapshot of Wikipedia in April
20103 (Shaoul, 2010), previously used in (Huang
et al., 2012; Neelakantan et al., 2014). WordNet
3.1 is used for training the sentence composition
model. A publicly available word vectors trained
by CBOW from Google News4 are used as pre-
trained word vectors for CNN.
For training CNN, we use: rectified linear
units, filter windows of 3, 4, 5 with 100 feature
maps each, AdaDelta decay parameter of 0.95, the
dropout rate of 0.5. For training VMSSG, we use
MSSG-KMeans as the clustering algorithm, and
CBOW for learning sense vectors. We set the size
of word vectors to 300, using boot vectors and
sense vectors. For other parameter, we use default
parameter settings for MSSG.
</bodyText>
<subsectionHeader confidence="0.999254">
3.2 Word Similarity Task
</subsectionHeader>
<bodyText confidence="0.9999869">
We evaluate our embeddings on the Contextual
Word Similarities (SCWS) dataset (Huang et al.,
2012). It contains 2,003 pairs of words and their
sentential contexts. Each pair is associated with
10 to 16 human judgments of similarity on a
scale from 0 to 10. We use the same metrics in
(Neelakantan et al., 2014) to measure the simi-
larity between two words given their respective
context. The avgSim metric computes the aver-
age similarity of all pairs of prototype vectors for
each word, ignoring context. The avgSimC met-
ric weights each similarity term in avgSim by the
likelihood of the word context appearing in its re-
spective cluster. The globalSim metric computes
each word vector ignoring senses. The localSim
metric chooses the most similar sense in context
to estimate the similarity of a words pair.
We report the Spearman’s correlation p x 100
between a model’s similarity scores and the human
judgments in Table 1.5
</bodyText>
<footnote confidence="0.997380166666667">
3http://www.psych.ualberta.ca/ ˜westburylab/downloads/
westburylab.wikicorp.download.html
4https://drive.google.com/file/d/0B7XkCwpI5KDYNl
NUTTlSS21pQmM/edit?usp=sharing
5The localSim metric of Unified-WSR is not reported in
(Chen et al., 2014).
</footnote>
<page confidence="0.997073">
17
</page>
<table confidence="0.998397833333333">
Model avgSim avgSimC globalSim localSim
Huang et al. 50d 62.8 65.7 58.6 26.1
Unified-WSR 200d 66.2 68.9 64.2 -
MSSG 300d 67.2 69.3 65.3 57.3
NP-MSSG 300d 67.3 69.1 65.5 59.8
CNN-VMSSG 300d 65.7 66.4 66.3 61.1
</table>
<tableCaption confidence="0.993705">
Table 1: Experimental results in the SCWS task.
</tableCaption>
<table confidence="0.999803642857143">
Subtask Word Pairs C&amp;W CBOW MSSG NP-MSSG CNN-VMSSG
Antonym 973 0.28 4.57 0.25 0.10 1.01
Attribute 184 0.22 1.18 0.03 0.15 1.63
Causes 26 0.00 1.08 0.31 0.31 1.23
DerivedFrom 6,119 0.05 0.63 0.09 0.05 0.17
Entails 114 0.05 0.38 0.49 0.34 1.29
HasContext 1,149 0.12 0.35 1.73 1.56 1.41
InstanceOf 1,314 0.08 0.58 2.52 2.34 2.46
IsA 10,615 0.07 0.67 0.15 0.08 0.86
MadeOf 63 0.03 0.72 0.80 0.48 1.28
MemberOf 406 0.08 1.06 0.14 0.86 0.90
PartOf 1,029 0.31 1.27 1.50 0.73 0.48
RelatedTo 102 0.00 0.05 0.12 0.11 1.28
SimilarTo 3,489 0.02 0.29 0.03 0.01 0.12
</table>
<tableCaption confidence="0.999347">
Table 2: Experimental results in the analogical reasoning task.
</tableCaption>
<bodyText confidence="0.999934473684211">
It is observed that our model achieves the best
performance on the globalSim and localSim met-
rics. It indicates that the use of pre-trained word
vectors and initializing sense vectors with the em-
beddings learned from WordNet glosses are in-
deed helpful in improving the quality of both
global word vectors and sense-level word vec-
tors. Our approach performs worse on avgSim and
avgSimC. One possible reason is that we set the
number of context clusters for each word to be the
same as the number of its corresponding senses
in WordNet. However, not all senses appear in the
our experimented corpus which could lead to frag-
mented context clustering results. One possible
way to alleviate this problem is to perform post-
processing to merge clusters which have smaller
inter-cluster differences or to remove sense clus-
ters which are under-represented in our data. We
will leave it as our future work.
</bodyText>
<subsectionHeader confidence="0.999644">
3.3 Analogical Reasoning Task
</subsectionHeader>
<bodyText confidence="0.998315541666667">
The analogical reasoning task introduced by
(Mikolov et al., 2013) consists of questions of the
form “a is to b is as c is to ”, where (a, b) and (c,
) are two word pairs. The goal is to find a word
d* in vocabulary V whose representation vector is
the closest to vb − va + v,.
WordRep is a benchmark collection for the re-
search on learning distributed word representa-
tions, which expands the Mikolov et al.’s analog-
ical reasoning questions. In our experiments, we
use one evaluation set in WordRep, the WordNet
collection which consists of 13 sub tasks.
We use the precision p x 100 as metric for each
sub task. Table 2 shows the results on the 13
sub tasks. The Word Pair column is the num-
ber of word pairs of each sub task. The results
of C&amp;W were obtained using the 50-dimensional
word embeddings that were made publicly avail-
able by Turian et al. (2010).6 The CBOW results
were previously reported in (Gao et al., 2014).
It can be observed that among 13 subtasks,
our model outperforms the others by a good mar-
gin in 6 subtasks, Attribute, Causes, Entails, IsA,
MadeOf and RelatedTo.
</bodyText>
<subsectionHeader confidence="0.910295">
3.4 Discussion
</subsectionHeader>
<bodyText confidence="0.99991125">
Although our evaluation results on the word simi-
larity task and the analogical reasoning task show
that our proposed approach outperforms a number
of existing word representation methods in some
</bodyText>
<footnote confidence="0.981915">
6http://metaoptimize.com/projects/wordreprs/
</footnote>
<page confidence="0.998919">
18
</page>
<bodyText confidence="0.999980826086956">
of the subtasks, it is worth noting that both tasks
do not consider the full spectrum of senses. In spe-
cific, the analogical reasoning task was originally
designed for evaluating single-prototype word rep-
resentations which ignore that a word could have
multiple meanings. Compared to single-prototype
word vectors, evaluating sense vectors requires a
significantly larger search space since each word
could be represented by multiple sense vectors de-
pending on the context. One may also argue that
the analogical reasoning task may not be the most
appropriate one in evaluating multiple-prototype
word vectors since the context information is not
available. In the future, we plan to evaluate our
learned multiple-prototype word vectors in more
relevant NLP tasks such as word sense disam-
biguation and question answering.
Our proposed approach initializes sense vec-
tors using the learned sentence embeddings from
WordNet glosses. In other low resourced lan-
guages, it is still possible to intialize sense vectors
based on, for example, the word meanings found
in language-specific dictionaries.
</bodyText>
<sectionHeader confidence="0.981129" genericHeader="conclusions">
4 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99997235">
This paper presents a method of incorporating
WordNet glosses composition and context cluster-
ing based model for learning distributed represen-
tations of word senses. By initializing sense vec-
tors using the embeddings learned by a sentence
composition from WordNet glosses, the context
clustering method is able to generate better dis-
tributed representations of word senses. The ob-
tained word sense representations achieve state-of-
the-art results on the globalSim and localSim met-
rics in the word similarity task and in 6 sub tasks
of the analogical reasoning task. It shows the ef-
fectiveness of our proposed learning algorithm for
generating word sense distributed representations.
Considering the coverage of word senses in our
training data, in future work we plan to filter out
those sense vectors which are under-represented
in the training corpus. We will also further investi-
gate the feasibility of applying the multi-prototype
word embeddings in a wide range of NLP tasks.
</bodyText>
<sectionHeader confidence="0.999032" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.500050555555556">
This work is supported by the National Natural
Science Foundation of China (No. 61370165,
61203378), National 863 Program of China
2015AA015405, the Natural Science Foundation
of Guangdong Province (No. S2013010014475),
Shenzhen Development and Reform Commission
Grant No.[2014]1507, Shenzhen Peacock Plan
Research Grant KQCX20140521144507925 and
Baidu Collaborate Research Funding.
</bodyText>
<sectionHeader confidence="0.929143" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997826604166667">
Yoshua Bengio, Rejean Ducharme, and Pascal Vincent.
2003. A neural probabilistic language model. Jour-
nal of Machine Learning Research, 3:1137–1155.
Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014.
A unified model for word sense representation and
disambiguation. In Proceedings of the 2014 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1025–1035.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations
using RNN encoder-decoder for statistical machine
translation. Proceedings of the 2014 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1724–1734.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th International Conference on
Machine Learning (ICML), pages 160–167. ACM.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
Bin Gao, Jiang Bian, and Tie-Yan Liu. 2014.
Wordrep: A benchmark for research on learning
word representations. In ICML 2014 Workshop on
Knowledge-Powered Deep Learning for Text Mining
(KPDLTM2014).
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Learning sense-specific word embed-
dings by exploiting bilingual resources. In Proceed-
ings of the 25th International Conference on Com-
putational Linguistics (COLING), pages 497–507.
Eric H Huang, Richard Socher, Christopher D Man-
ning, and Andrew Y Ng. 2012. Improving word
representations via global context and multiple word
prototypes. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL), pages 873–882. Association for Compu-
tational Linguistics.
Yangfeng Ji and Jacob Eisenstein. 2014. Repre-
sentation learning for text-level discourse parsing.
In Proceedings of the 52nd Annual Meeting of the
Association for Computational Linguistics (ACL),
pages 13–24, Baltimore, Maryland, June. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.993572">
19
</page>
<reference confidence="0.999220835616438">
Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-
som. 2014. A convolutional neural network for
modelling sentences. In Proceedings of the 52nd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 655–665, Baltimore,
Maryland, June. Association for Computational Lin-
guistics.
Yoon Kim. 2014. Convolutional neural networks for
sentence classification. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1746–1751, Oc-
tober.
Quoc V Le and Tomas Mikolov. 2014. Distributed
representations of sentences and documents. In Pro-
ceedings of the 31th International Conference on
Machine Learning (ICML), pages 1188–1196.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In Pro-
ceedings of INTERSPEECH, pages 1045–1048.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. In Proceedings of Workshop
at the International Conference on Learning Repre-
sentations (ICLR).
George A Miller. 1995. Wordnet: a lexical
database for english. Communications of the ACM,
38(11):39–41.
Andriy Mnih and Geoffrey E Hinton. 2009. A scal-
able hierarchical distributed language model. In
Advances in neural information processing systems,
pages 1081–1088.
Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-
sos, and Andrew McCallum. 2014. Efficient non-
parametric estimation of multiple embeddings per
word in vector space. In Proceedings of the 2014
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 1059–1069.
DE Rummelhart. 1986. Learning representations by
back-propagating errors. Nature, 323(9):533–536.
Cyrus Shaoul. 2010. The westbury lab wikipedia cor-
pus. Edmonton, AB: University of Alberta.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop, pages 1–9.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631–1642. Citeseer.
Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,
Enhong Chen, and Tie-Yan Liu. 2014. A probabilis-
tic model for learning multi-prototype word embed-
dings. In Proceedings of the 25th International Con-
ference on Computational Linguistics (COLING),
pages 151–160.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In Proceedings of the
48th annual meeting of the association for computa-
tional linguistics (ACL), pages 384–394.
Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and
Chengqing Zong. 2014. Bilingually-constrained
phrase embeddings for machine translation. In Pro-
ceedings of the 52nd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
111–121, Baltimore, Maryland, June. Association
for Computational Linguistics.
</reference>
<page confidence="0.994865">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.923530">
<title confidence="0.991587">Improving Distributed Representation of Word via WordNet Gloss Composition and Context Clustering</title>
<author confidence="0.995646">Ruifeng Yulan Xuan</author>
<affiliation confidence="0.987931333333333">Engineering Laboratory of Performance Robots at Digital Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, of Engineering and Applied Science, Aston University,</affiliation>
<email confidence="0.991917">y.he9@aston.ac.uk</email>
<abstract confidence="0.999244285714286">In recent years, there has been an increasing interest in learning a distributed representation of word sense. Traditional context clustering based models usually require careful tuning of model parameters, and typically perform worse on infrequent word senses. This paper presents a novel approach which addresses these limitations by first initializing the word sense embeddings through learning sentencelevel embeddings from WordNet glosses using a convolutional neural networks. The initialized word sense embeddings are used by a context clustering based model to generate the distributed representations of word senses. Our learned representations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Rejean Ducharme</author>
<author>Pascal Vincent</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="1707" citStr="Bengio et al., 2003" startWordPosition="246" endWordPosition="249">tations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task. 1 Introduction With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contextual semantic information. Most existing approaches</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, 2003</marker>
<rawString>Yoshua Bengio, Rejean Ducharme, and Pascal Vincent. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinxiong Chen</author>
<author>Zhiyuan Liu</author>
<author>Maosong Sun</author>
</authors>
<title>A unified model for word sense representation and disambiguation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1025--1035</pages>
<contexts>
<context position="1592" citStr="Chen et al., 2014" startWordPosition="225" endWordPosition="228">a context clustering based model to generate the distributed representations of word senses. Our learned representations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task. 1 Introduction With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-va</context>
<context position="3634" citStr="Chen et al., 2014" startWordPosition="556" endWordPosition="559"> such as “break”. As such, fixing the number of senses for all words would result in poor representations. NP-MSSG can learn the number of senses for each word directly from data. But it requires a tuning of a hyperparameter A which controls the creation of cluster centroids during training. Different A needs to be tuned for different datasets. (2) The initial value of sense representation is critical for most statistical clustering based approaches. However, previous approaches usually adopted random initialization (Neelakantan et al., 2014) or the mean average of candidate words in a gloss (Chen et al., 2014). As a result, they may not produce optimal clustering results for word senses. Focusing on the aforementioned two problems, this paper proposes to learn distributed representations of word senses through WordNet gloss composition and context clustering. The basic idea is that a word sense is represented as a synonym set (synset) in WordNet. In this way, instead of assigning a fixed sense number to each word as in the 15 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers</context>
<context position="13001" citStr="Chen et al., 2014" startWordPosition="2152" endWordPosition="2155"> in avgSim by the likelihood of the word context appearing in its respective cluster. The globalSim metric computes each word vector ignoring senses. The localSim metric chooses the most similar sense in context to estimate the similarity of a words pair. We report the Spearman’s correlation p x 100 between a model’s similarity scores and the human judgments in Table 1.5 3http://www.psych.ualberta.ca/ ˜westburylab/downloads/ westburylab.wikicorp.download.html 4https://drive.google.com/file/d/0B7XkCwpI5KDYNl NUTTlSS21pQmM/edit?usp=sharing 5The localSim metric of Unified-WSR is not reported in (Chen et al., 2014). 17 Model avgSim avgSimC globalSim localSim Huang et al. 50d 62.8 65.7 58.6 26.1 Unified-WSR 200d 66.2 68.9 64.2 - MSSG 300d 67.2 69.3 65.3 57.3 NP-MSSG 300d 67.3 69.1 65.5 59.8 CNN-VMSSG 300d 65.7 66.4 66.3 61.1 Table 1: Experimental results in the SCWS task. Subtask Word Pairs C&amp;W CBOW MSSG NP-MSSG CNN-VMSSG Antonym 973 0.28 4.57 0.25 0.10 1.01 Attribute 184 0.22 1.18 0.03 0.15 1.63 Causes 26 0.00 1.08 0.31 0.31 1.23 DerivedFrom 6,119 0.05 0.63 0.09 0.05 0.17 Entails 114 0.05 0.38 0.49 0.34 1.29 HasContext 1,149 0.12 0.35 1.73 1.56 1.41 InstanceOf 1,314 0.08 0.58 2.52 2.34 2.46 IsA 10,615 0</context>
</contexts>
<marker>Chen, Liu, Sun, 2014</marker>
<rawString>Xinxiong Chen, Zhiyuan Liu, and Maosong Sun. 2014. A unified model for word sense representation and disambiguation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1025–1035.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
<author>Caglar Gulcehre</author>
<author>Fethi Bougares</author>
<author>Holger Schwenk</author>
<author>Yoshua Bengio</author>
</authors>
<title>Learning phrase representations using RNN encoder-decoder for statistical machine translation.</title>
<date>2014</date>
<booktitle>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1724--1734</pages>
<marker>Cho, van Merrienboer, Gulcehre, Bougares, Schwenk, Bengio, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724–1734.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning (ICML),</booktitle>
<pages>160--167</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1735" citStr="Collobert and Weston, 2008" startWordPosition="250" endWordPosition="253">e publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task. 1 Introduction With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contextual semantic information. Most existing approaches adopted a clusterbased para</context>
<context position="7516" citStr="Collobert and Weston, 2008" startWordPosition="1195" endWordPosition="1198">nce s as a positive training sample, we randomly replace some words (controlled by a parameter A) in s to construct a negative training sample s′. We compute the scores f(s) and f(s′) where f(�) is the scoring function representing the whole CNN architecture without the softmax layer. We expect f(s) and f(s′) to be close to 1 and 0 respectively, and f(s) to be larger than f(s′) by a margin of 1 for all the sentence in positive training set P. The CNN architecture used in this component follows the architecture proposed by (Kim, 2014)1 which is a slight variant of the architecture proposed by (Collobert and Weston, 2008)2. It takes a gloss matrix s as input where each column corresponds to the distributed representation v,,,z E Rd of a word wi in the sentence. The idea behind the one-dimensional convolution is to take the dot product of the vector w with each n-gram in the sentence to obtain another sequence c, where n is the width of filter in the convolutional layer. In order to make c to cover different words in the negative sample corresponding a positive sample, in this work, we randomly replace half of the words in a positive training sample to construct a negative training sample (A = 0.5). For example</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th International Conference on Machine Learning (ICML), pages 160–167. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="8466" citStr="Collobert et al., 2011" startWordPosition="1368" endWordPosition="1371"> in the convolutional layer. In order to make c to cover different words in the negative sample corresponding a positive sample, in this work, we randomly replace half of the words in a positive training sample to construct a negative training sample (A = 0.5). For example, take the WordNet gloss “an actor who plays a principal role” as a positive sample, a negative training sample constructed by this method may be “x1 actor who x2 x3 principal x4”, where x1 to x4 are randomly selected words in a vocabulary collected from a large corpus. In the pooling layer, a max-overtime pooling operation (Collobert et al., 2011), which forces the network to capture the most useful local features produced by the convolutional layers, is applied. The model uses multiple filters (with varying window sizes) to obtain multiple features. These features form the penultimate layer and are passed to a fully connected softmax layer whose output is 1https://github.com/yoonkim/CNN sentence 2http://ronan.collobert.com/senna/ 16 the probability distribution over labels. The training error propagates back to fine-tune the parameters of the CNN and the input word vectors. The vector generated in the penultimate layer of the CNN arch</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Gao</author>
<author>Jiang Bian</author>
<author>Tie-Yan Liu</author>
</authors>
<title>Wordrep: A benchmark for research on learning word representations.</title>
<date>2014</date>
<booktitle>In ICML 2014 Workshop on Knowledge-Powered Deep Learning for Text Mining (KPDLTM2014).</booktitle>
<contexts>
<context position="5193" citStr="Gao et al., 2014" startWordPosition="797" endWordPosition="800">l neural network (CNN) to learn distributed representations of these glosses (a.k.a. sense vectors) through sentence composition. Then, we modify MSSG for context clustering by initializing the sense vectors with the representations learned by our CNN-based sentence composition model. We expect that word sense vectors initialized in this way would potentially lead to better representations of word senses generated from context clustering. The obtained word sense representations are evaluated on two tasks. One is word similarity task, the other is analogical reasoning task provided by WordRep (Gao et al., 2014). The results show that our approach attains comparable performance on learning distributed representations of word senses. In specific, our learned representation outperforms publicly available embeddings on the globalSim and localSim metrics in word similarity task, and 6 in 13 subtasks in the analogical reasoning task. 2 Our Approach Our proposed approach first train a Continuous Bag-Of-Words (CBOW) model (Mikolov et al., 2013) from a large collection of raw text to generate word embeddings. These word embeddings are then used by a Sentence Composition Model, which takes glosses in WordNet </context>
<context position="15731" citStr="Gao et al., 2014" startWordPosition="2635" endWordPosition="2638">tion for the research on learning distributed word representations, which expands the Mikolov et al.’s analogical reasoning questions. In our experiments, we use one evaluation set in WordRep, the WordNet collection which consists of 13 sub tasks. We use the precision p x 100 as metric for each sub task. Table 2 shows the results on the 13 sub tasks. The Word Pair column is the number of word pairs of each sub task. The results of C&amp;W were obtained using the 50-dimensional word embeddings that were made publicly available by Turian et al. (2010).6 The CBOW results were previously reported in (Gao et al., 2014). It can be observed that among 13 subtasks, our model outperforms the others by a good margin in 6 subtasks, Attribute, Causes, Entails, IsA, MadeOf and RelatedTo. 3.4 Discussion Although our evaluation results on the word similarity task and the analogical reasoning task show that our proposed approach outperforms a number of existing word representation methods in some 6http://metaoptimize.com/projects/wordreprs/ 18 of the subtasks, it is worth noting that both tasks do not consider the full spectrum of senses. In specific, the analogical reasoning task was originally designed for evaluatin</context>
</contexts>
<marker>Gao, Bian, Liu, 2014</marker>
<rawString>Bin Gao, Jiang Bian, and Tie-Yan Liu. 2014. Wordrep: A benchmark for research on learning word representations. In ICML 2014 Workshop on Knowledge-Powered Deep Learning for Text Mining (KPDLTM2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Guo</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Learning sense-specific word embeddings by exploiting bilingual resources.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>497--507</pages>
<contexts>
<context position="1656" citStr="Guo et al., 2014" startWordPosition="238" endWordPosition="241">resentations of word senses. Our learned representations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task. 1 Introduction With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contex</context>
</contexts>
<marker>Guo, Che, Wang, Liu, 2014</marker>
<rawString>Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Learning sense-specific word embeddings by exploiting bilingual resources. In Proceedings of the 25th International Conference on Computational Linguistics (COLING), pages 497–507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric H Huang</author>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Improving word representations via global context and multiple word prototypes.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>873--882</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1573" citStr="Huang et al., 2012" startWordPosition="221" endWordPosition="224">eddings are used by a context clustering based model to generate the distributed representations of word senses. Our learned representations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task. 1 Introduction With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by </context>
<context position="11189" citStr="Huang et al., 2012" startWordPosition="1871" endWordPosition="1874"> bw E V, k E 11, ... , Kw} The training algorithm of the VMSSG model is shown as Algorithm 1, where D is a text corpus, V is the vocabulary of D, |V |is the vocabulary size, M is the size of context window, vw is the word embedding for w, swk is a kth context cluster of word w, µwk is the centroid of cluster k for word w. The function NoisySamples(C) randomly replaces context words with noisy words from V . 3 Evaluation and Discussion 3.1 Experimental Setup In all experiments, we train word vectors and sense vectors on a snapshot of Wikipedia in April 20103 (Shaoul, 2010), previously used in (Huang et al., 2012; Neelakantan et al., 2014). WordNet 3.1 is used for training the sentence composition model. A publicly available word vectors trained by CBOW from Google News4 are used as pretrained word vectors for CNN. For training CNN, we use: rectified linear units, filter windows of 3, 4, 5 with 100 feature maps each, AdaDelta decay parameter of 0.95, the dropout rate of 0.5. For training VMSSG, we use MSSG-KMeans as the clustering algorithm, and CBOW for learning sense vectors. We set the size of word vectors to 300, using boot vectors and sense vectors. For other parameter, we use default parameter s</context>
</contexts>
<marker>Huang, Socher, Manning, Ng, 2012</marker>
<rawString>Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL), pages 873–882. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yangfeng Ji</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Representation learning for text-level discourse parsing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>13--24</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="2040" citStr="Ji and Eisenstein, 2014" startWordPosition="303" endWordPosition="306">odels for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contextual semantic information. Most existing approaches adopted a clusterbased paradigm, which produces different sense vectors for each polysemy or homonymy through clustering the context of a target word. However, this paradigm usually has two limitations: (1) The performance of these approaches is sensitive to the clustering algorithm which requires the setting of the sense number f</context>
</contexts>
<marker>Ji, Eisenstein, 2014</marker>
<rawString>Yangfeng Ji and Jacob Eisenstein. 2014. Representation learning for text-level discourse parsing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 13–24, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Edward Grefenstette</author>
<author>Phil Blunsom</author>
</authors>
<title>A convolutional neural network for modelling sentences.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>655--665</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="1963" citStr="Kalchbrenner et al., 2014" startWordPosition="290" endWordPosition="294">ng, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contextual semantic information. Most existing approaches adopted a clusterbased paradigm, which produces different sense vectors for each polysemy or homonymy through clustering the context of a target word. However, this paradigm usually has two limitations: (1) The performance of these approaches is sensitive</context>
</contexts>
<marker>Kalchbrenner, Grefenstette, Blunsom, 2014</marker>
<rawString>Nal Kalchbrenner, Edward Grefenstette, and Phil Blunsom. 2014. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 655–665, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoon Kim</author>
</authors>
<title>Convolutional neural networks for sentence classification.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1746--1751</pages>
<contexts>
<context position="1974" citStr="Kim, 2014" startWordPosition="295" endWordPosition="296">ion of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contextual semantic information. Most existing approaches adopted a clusterbased paradigm, which produces different sense vectors for each polysemy or homonymy through clustering the context of a target word. However, this paradigm usually has two limitations: (1) The performance of these approaches is sensitive to the clu</context>
<context position="7428" citStr="Kim, 2014" startWordPosition="1182" endWordPosition="1183">g loss below: �Gs = max{0,1 − f(s) + f(s′)} (1) sEP Given a gloss sentence s as a positive training sample, we randomly replace some words (controlled by a parameter A) in s to construct a negative training sample s′. We compute the scores f(s) and f(s′) where f(�) is the scoring function representing the whole CNN architecture without the softmax layer. We expect f(s) and f(s′) to be close to 1 and 0 respectively, and f(s) to be larger than f(s′) by a margin of 1 for all the sentence in positive training set P. The CNN architecture used in this component follows the architecture proposed by (Kim, 2014)1 which is a slight variant of the architecture proposed by (Collobert and Weston, 2008)2. It takes a gloss matrix s as input where each column corresponds to the distributed representation v,,,z E Rd of a word wi in the sentence. The idea behind the one-dimensional convolution is to take the dot product of the vector w with each n-gram in the sentence to obtain another sequence c, where n is the width of filter in the convolutional layer. In order to make c to cover different words in the negative sample corresponding a positive sample, in this work, we randomly replace half of the words in a</context>
</contexts>
<marker>Kim, 2014</marker>
<rawString>Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746–1751, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quoc V Le and Tomas Mikolov</author>
</authors>
<title>Distributed representations of sentences and documents.</title>
<date>2014</date>
<booktitle>In Proceedings of the 31th International Conference on Machine Learning (ICML),</booktitle>
<pages>1188--1196</pages>
<contexts>
<context position="1997" citStr="Mikolov, 2014" startWordPosition="299" endWordPosition="300">ttracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contextual semantic information. Most existing approaches adopted a clusterbased paradigm, which produces different sense vectors for each polysemy or homonymy through clustering the context of a target word. However, this paradigm usually has two limitations: (1) The performance of these approaches is sensitive to the clustering algorithm which</context>
</contexts>
<marker>Mikolov, 2014</marker>
<rawString>Quoc V Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In Proceedings of the 31th International Conference on Machine Learning (ICML), pages 1188–1196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In Proceedings of INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Proceedings of INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<booktitle>In Proceedings of Workshop at the International Conference on Learning Representations (ICLR).</booktitle>
<contexts>
<context position="1803" citStr="Mikolov et al., 2013" startWordPosition="262" endWordPosition="265">ity task, and 6 out of 13 sub tasks in the analogical reasoning task. 1 Introduction With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contextual semantic information. Most existing approaches adopted a clusterbased paradigm, which produces different sense vectors for each polysemy or ho</context>
<context position="5627" citStr="Mikolov et al., 2013" startWordPosition="862" endWordPosition="865"> clustering. The obtained word sense representations are evaluated on two tasks. One is word similarity task, the other is analogical reasoning task provided by WordRep (Gao et al., 2014). The results show that our approach attains comparable performance on learning distributed representations of word senses. In specific, our learned representation outperforms publicly available embeddings on the globalSim and localSim metrics in word similarity task, and 6 in 13 subtasks in the analogical reasoning task. 2 Our Approach Our proposed approach first train a Continuous Bag-Of-Words (CBOW) model (Mikolov et al., 2013) from a large collection of raw text to generate word embeddings. These word embeddings are then used by a Sentence Composition Model, which takes glosses in WordNet as positive training data and randomly replaces part of the sentences as negative training data to construct the corresponding word sense vectors based on a onedimensional CNN. For example, a WordNet gloss of word star is “an actor who plays a principal role”. This is taken as a positive training example when learning the word sense vector for “star”. We concatenate the word embedding generated by the CBOW model for each of the wo</context>
<context position="14873" citStr="Mikolov et al., 2013" startWordPosition="2471" endWordPosition="2474"> and avgSimC. One possible reason is that we set the number of context clusters for each word to be the same as the number of its corresponding senses in WordNet. However, not all senses appear in the our experimented corpus which could lead to fragmented context clustering results. One possible way to alleviate this problem is to perform postprocessing to merge clusters which have smaller inter-cluster differences or to remove sense clusters which are under-represented in our data. We will leave it as our future work. 3.3 Analogical Reasoning Task The analogical reasoning task introduced by (Mikolov et al., 2013) consists of questions of the form “a is to b is as c is to ”, where (a, b) and (c, ) are two word pairs. The goal is to find a word d* in vocabulary V whose representation vector is the closest to vb − va + v,. WordRep is a benchmark collection for the research on learning distributed word representations, which expands the Mikolov et al.’s analogical reasoning questions. In our experiments, we use one evaluation set in WordRep, the WordNet collection which consists of 13 sub tasks. We use the precision p x 100 as metric for each sub task. Table 2 shows the results on the 13 sub tasks. The Wo</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. In Proceedings of Workshop at the International Conference on Learning Representations (ICLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="2982" citStr="Miller, 1995" startWordPosition="450" endWordPosition="451">se vectors for each polysemy or homonymy through clustering the context of a target word. However, this paradigm usually has two limitations: (1) The performance of these approaches is sensitive to the clustering algorithm which requires the setting of the sense number for each word. For example, Neelakantan et al. (2014) proposed two clustering based model: the Multi-Sense Skip-Gram (MSSG) model and Non-Parametric Multi-Sense Skip-Gram (NP-MSSG) model. MSSG assumes each word has the same k-sense (e.g. k = 3), i.e., the same number of possible senses. However, the number of senses in WordNet (Miller, 1995) varies from 1 such as “ben” to 75 such as “break”. As such, fixing the number of senses for all words would result in poor representations. NP-MSSG can learn the number of senses for each word directly from data. But it requires a tuning of a hyperparameter A which controls the creation of cluster centroids during training. Different A needs to be tuned for different datasets. (2) The initial value of sense representation is critical for most statistical clustering based approaches. However, previous approaches usually adopted random initialization (Neelakantan et al., 2014) or the mean avera</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Geoffrey E Hinton</author>
</authors>
<title>A scalable hierarchical distributed language model.</title>
<date>2009</date>
<booktitle>In Advances in neural information processing systems,</booktitle>
<pages>1081--1088</pages>
<contexts>
<context position="1758" citStr="Mnih and Hinton, 2009" startWordPosition="254" endWordPosition="257">ngs on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task. 1 Introduction With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contextual semantic information. Most existing approaches adopted a clusterbased paradigm, which produces di</context>
</contexts>
<marker>Mnih, Hinton, 2009</marker>
<rawString>Andriy Mnih and Geoffrey E Hinton. 2009. A scalable hierarchical distributed language model. In Advances in neural information processing systems, pages 1081–1088.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arvind Neelakantan</author>
<author>Jeevan Shankar</author>
<author>Alexandre Passos</author>
<author>Andrew McCallum</author>
</authors>
<title>Efficient nonparametric estimation of multiple embeddings per word in vector space.</title>
<date>2014</date>
<booktitle>In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1059--1069</pages>
<contexts>
<context position="1618" citStr="Neelakantan et al., 2014" startWordPosition="229" endWordPosition="233">g based model to generate the distributed representations of word senses. Our learned representations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task. 1 Introduction With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimen</context>
<context position="3564" citStr="Neelakantan et al., 2014" startWordPosition="542" endWordPosition="545"> number of senses in WordNet (Miller, 1995) varies from 1 such as “ben” to 75 such as “break”. As such, fixing the number of senses for all words would result in poor representations. NP-MSSG can learn the number of senses for each word directly from data. But it requires a tuning of a hyperparameter A which controls the creation of cluster centroids during training. Different A needs to be tuned for different datasets. (2) The initial value of sense representation is critical for most statistical clustering based approaches. However, previous approaches usually adopted random initialization (Neelakantan et al., 2014) or the mean average of candidate words in a gloss (Chen et al., 2014). As a result, they may not produce optimal clustering results for word senses. Focusing on the aforementioned two problems, this paper proposes to learn distributed representations of word senses through WordNet gloss composition and context clustering. The basic idea is that a word sense is represented as a synonym set (synset) in WordNet. In this way, instead of assigning a fixed sense number to each word as in the 15 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Inter</context>
<context position="9247" citStr="Neelakantan et al. (2014)" startWordPosition="1487" endWordPosition="1490"> varying window sizes) to obtain multiple features. These features form the penultimate layer and are passed to a fully connected softmax layer whose output is 1https://github.com/yoonkim/CNN sentence 2http://ronan.collobert.com/senna/ 16 the probability distribution over labels. The training error propagates back to fine-tune the parameters of the CNN and the input word vectors. The vector generated in the penultimate layer of the CNN architecture is regarded as the sense vector which captures the semantic content of the input gloss to a certain degree. 2.2 Context Clustering and VMSSG Model Neelakantan et al. (2014) proposed the MSSG model which extends the skip-gram model to learn multi-prototype word embeddings by clustering the word embeddings of context words around each word. In this model, for each word w, the corresponding word embedding vw E Rd, k-sense vector vsk E Rd (k = 1, 2, ... , K) and k-context cluster with center µk E Rd (k = 1, 2, ... , K) are initialized randomly. The sense number K of each word is a fixed parameter in the training algorithm. We improve the MSSG model by using the learned CBOW word embedding to initialize vw and the sense vector trained by the sentence composition mode</context>
<context position="11216" citStr="Neelakantan et al., 2014" startWordPosition="1875" endWordPosition="1878"> , Kw} The training algorithm of the VMSSG model is shown as Algorithm 1, where D is a text corpus, V is the vocabulary of D, |V |is the vocabulary size, M is the size of context window, vw is the word embedding for w, swk is a kth context cluster of word w, µwk is the centroid of cluster k for word w. The function NoisySamples(C) randomly replaces context words with noisy words from V . 3 Evaluation and Discussion 3.1 Experimental Setup In all experiments, we train word vectors and sense vectors on a snapshot of Wikipedia in April 20103 (Shaoul, 2010), previously used in (Huang et al., 2012; Neelakantan et al., 2014). WordNet 3.1 is used for training the sentence composition model. A publicly available word vectors trained by CBOW from Google News4 are used as pretrained word vectors for CNN. For training CNN, we use: rectified linear units, filter windows of 3, 4, 5 with 100 feature maps each, AdaDelta decay parameter of 0.95, the dropout rate of 0.5. For training VMSSG, we use MSSG-KMeans as the clustering algorithm, and CBOW for learning sense vectors. We set the size of word vectors to 300, using boot vectors and sense vectors. For other parameter, we use default parameter settings for MSSG. 3.2 Word </context>
</contexts>
<marker>Neelakantan, Shankar, Passos, McCallum, 2014</marker>
<rawString>Arvind Neelakantan, Jeevan Shankar, Alexandre Passos, and Andrew McCallum. 2014. Efficient nonparametric estimation of multiple embeddings per word in vector space. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1059–1069.</rawString>
</citation>
<citation valid="true">
<authors>
<author>DE Rummelhart</author>
</authors>
<title>Learning representations by back-propagating errors.</title>
<date>1986</date>
<journal>Nature,</journal>
<volume>323</volume>
<issue>9</issue>
<marker>DE Rummelhart, 1986</marker>
<rawString>DE Rummelhart. 1986. Learning representations by back-propagating errors. Nature, 323(9):533–536.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyrus Shaoul</author>
</authors>
<title>The westbury lab wikipedia corpus.</title>
<date>2010</date>
<institution>University of Alberta.</institution>
<location>Edmonton, AB:</location>
<contexts>
<context position="11149" citStr="Shaoul, 2010" startWordPosition="1866" endWordPosition="1867"> 12: end for 13: Output: vswk , vw, bw E V, k E 11, ... , Kw} The training algorithm of the VMSSG model is shown as Algorithm 1, where D is a text corpus, V is the vocabulary of D, |V |is the vocabulary size, M is the size of context window, vw is the word embedding for w, swk is a kth context cluster of word w, µwk is the centroid of cluster k for word w. The function NoisySamples(C) randomly replaces context words with noisy words from V . 3 Evaluation and Discussion 3.1 Experimental Setup In all experiments, we train word vectors and sense vectors on a snapshot of Wikipedia in April 20103 (Shaoul, 2010), previously used in (Huang et al., 2012; Neelakantan et al., 2014). WordNet 3.1 is used for training the sentence composition model. A publicly available word vectors trained by CBOW from Google News4 are used as pretrained word vectors for CNN. For training CNN, we use: rectified linear units, filter windows of 3, 4, 5 with 100 feature maps each, AdaDelta decay parameter of 0.95, the dropout rate of 0.5. For training VMSSG, we use MSSG-KMeans as the clustering algorithm, and CBOW for learning sense vectors. We set the size of word vectors to 300, using boot vectors and sense vectors. For oth</context>
</contexts>
<marker>Shaoul, 2010</marker>
<rawString>Cyrus Shaoul. 2010. The westbury lab wikipedia corpus. Edmonton, AB: University of Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="1838" citStr="Socher et al., 2010" startWordPosition="268" endWordPosition="271">n the analogical reasoning task. 1 Introduction With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contextual semantic information. Most existing approaches adopted a clusterbased paradigm, which produces different sense vectors for each polysemy or homonymy through clustering the conte</context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D Manning, and Andrew Y Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1631--1642</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="1936" citStr="Socher et al., 2013" startWordPosition="286" endWordPosition="289"> and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contextual semantic information. Most existing approaches adopted a clusterbased paradigm, which produces different sense vectors for each polysemy or homonymy through clustering the context of a target word. However, this paradigm usually has two limitations: (1) The performance of th</context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Tian</author>
<author>Hanjun Dai</author>
<author>Jiang Bian</author>
<author>Bin Gao</author>
<author>Rui Zhang</author>
<author>Enhong Chen</author>
<author>Tie-Yan Liu</author>
</authors>
<title>A probabilistic model for learning multi-prototype word embeddings.</title>
<date>2014</date>
<booktitle>In Proceedings of the 25th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>151--160</pages>
<contexts>
<context position="1637" citStr="Tian et al., 2014" startWordPosition="234" endWordPosition="237">the distributed representations of word senses. Our learned representations outperform the publicly available embeddings on 2 out of 4 metrics in the word similarity task, and 6 out of 13 sub tasks in the analogical reasoning task. 1 Introduction With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which </context>
</contexts>
<marker>Tian, Dai, Bian, Gao, Zhang, Chen, Liu, 2014</marker>
<rawString>Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang, Enhong Chen, and Tie-Yan Liu. 2014. A probabilistic model for learning multi-prototype word embeddings. In Proceedings of the 25th International Conference on Computational Linguistics (COLING), pages 151–160.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th annual meeting of the association for computational linguistics (ACL),</booktitle>
<pages>384--394</pages>
<contexts>
<context position="15665" citStr="Turian et al. (2010)" startWordPosition="2624" endWordPosition="2627"> vector is the closest to vb − va + v,. WordRep is a benchmark collection for the research on learning distributed word representations, which expands the Mikolov et al.’s analogical reasoning questions. In our experiments, we use one evaluation set in WordRep, the WordNet collection which consists of 13 sub tasks. We use the precision p x 100 as metric for each sub task. Table 2 shows the results on the 13 sub tasks. The Word Pair column is the number of word pairs of each sub task. The results of C&amp;W were obtained using the 50-dimensional word embeddings that were made publicly available by Turian et al. (2010).6 The CBOW results were previously reported in (Gao et al., 2014). It can be observed that among 13 subtasks, our model outperforms the others by a good margin in 6 subtasks, Attribute, Causes, Entails, IsA, MadeOf and RelatedTo. 3.4 Discussion Although our evaluation results on the word similarity task and the analogical reasoning task show that our proposed approach outperforms a number of existing word representation methods in some 6http://metaoptimize.com/projects/wordreprs/ 18 of the subtasks, it is worth noting that both tasks do not consider the full spectrum of senses. In specific, t</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics (ACL), pages 384–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiajun Zhang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Chengqing Zong</author>
</authors>
<title>Bilingually-constrained phrase embeddings for machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>111--121</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="1858" citStr="Zhang et al., 2014" startWordPosition="272" endWordPosition="275">oning task. 1 Introduction With the rapid development of deep neural networks and parallel computing, distributed representation of knowledge attracts much research interest. Models for learning distributed representations of knowledge have been proposed at different granularity level, including word sense level (Huang et al., 2012; Chen et al., 2014; Neelakantan et al., 2014; Tian et al., 2014; Guo et al., 2014), word level (Rummelhart, 1986; Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2010; Mikolov et al., 2013), phrase level (Socher et al., 2010; Zhang et al., 2014; Cho et al., 2014), sentence level (Mikolov et al., 2010; Socher et al., 2013; Kalchbrenner et al., 2014; Kim, 2014; Le and Mikolov, 2014), discourse level (Ji and Eisenstein, 2014) and document level (Le and Mikolov, 2014). In distributed representations of word senses, each word sense is usually represented by a dense and real-valued vector in a low-dimensional space which captures the contextual semantic information. Most existing approaches adopted a clusterbased paradigm, which produces different sense vectors for each polysemy or homonymy through clustering the context of a target word.</context>
</contexts>
<marker>Zhang, Liu, Li, Zhou, Zong, 2014</marker>
<rawString>Jiajun Zhang, Shujie Liu, Mu Li, Ming Zhou, and Chengqing Zong. 2014. Bilingually-constrained phrase embeddings for machine translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), pages 111–121, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>