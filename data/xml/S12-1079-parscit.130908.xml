<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018103">
<title confidence="0.975708">
ATA-Sem: Chunk-based Determination of Semantic Text Similarity
</title>
<author confidence="0.8559">
Demetrios Glinos
</author>
<affiliation confidence="0.817807">
Advanced Text Analytics, LLC
</affiliation>
<address confidence="0.760529">
Orlando, Florida, USA
</address>
<email confidence="0.987448">
demetrios.glinos@advancedtextanalytics.com
</email>
<sectionHeader confidence="0.993681" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999764777777778">
This paper describes investigations into using
syntactic chunk information as the basis for
determining the similarity of candidate texts at
the semantic level. Two approaches were con-
sidered. The first was a corpus-based method
that extracted lexical and semantic features
from pairs of chunks from each sentence that
were associated through a chunk alignment
algorithm. The features were used as input to
a classifier trained on the same features ex-
tracted from a corpus of gold standard training
data. The second approach involved breadth-
first chunk association and the application of a
rule-based scoring algorithm. Both approaches
were evaluated against the test data for the
SemEval 2012 Semantic Text Similarity task.
The results show that the rule-based chunk
approach is superior.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.965263333333333">
The task of determining whether two texts are sim-
ilar in some sense has important applications in the
field of natural language processing, including but
not limited to document summarization (Evans, et
al., 2005), plagiarism detection (Barr—n-Cede–o, et
al., 2009) and large corpus document retrieval
(Charikar, 2002).
While textual similarity can be performed at the
purely surface lexical level, as in the “simhash”
clustering method described in (Moulton, 2010),
similarity also applies at the semantic level, where
conceptually similar texts may nevertheless be en-
tirely dissimilar at the surface lexical level. For
example, the phrases “restrict or confine” and
“place limits on (extent or access)” share no words
or morphological roots, yet mean very nearly the
same thing at the semantic level.
The Semantic Textual Similarity (STS) task
(Task #6) at SemEval-2012 (Agirre, et al., 2012)
provided a forum for exploring these issues by fur-
nishing training and evaluation data, and also a
common standard for describing degrees of simi-
larity, shown in Table 1.
Score Description
5 The two sentences are completely equiva-
lent, as they mean the same thing.
4 The two sentences are mostly equivalent,
but some unimportant details differ.
3 The two sentences are roughly equivalent,
but some important information dif-
fers/missing.
2 The two sentences are not equivalent, but
share some details.
1 The two sentences are not equivalent, but
are on the same topic.
0 The two sentences are on different topics.
</bodyText>
<tableCaption confidence="0.993311">
Table 1. STS similarity scoring standard.
</tableCaption>
<bodyText confidence="0.999908176470588">
Our corpus-based chunk similarity method par-
ticipated in the formal STS evaluation. Our rule-
based method was completed after the submittal
date, but we report on it here because the method
does not involve training on a corpus, nor any pa-
rameter tuning, and because it significantly outper-
formed the corpus-based method.
The remainder of this paper is organized as fol-
lows. In the next section, we describe the common
processing components for both methods. Section
3 then presents the corpus-based chunk method,
followed in Section 4 by a discussion of the rule-
based chunk similarity method. Section 5 con-
cludes with a presentation of how the two methods
performed against the STS test set, and offers some
observations on the viability of chunk-based simi-
larity determination.
</bodyText>
<page confidence="0.964391">
547
</page>
<note confidence="0.715296">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 547–551,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.94399" genericHeader="method">
2 Common Processing Components
</sectionHeader>
<bodyText confidence="0.99997672">
A common processing core supports both of the
methods, comprising preprocessing components
and also shared components for determining
chunk-level similarity. The preprocessing compo-
nents make use of the U.S. National Library of
Medicine’s Lexical Tools (NLM 2012) to perform
ASCII conversion and tokenization. Candidate sen-
tence pairs are then tagged and chunked using our
own tagger and separate chunker, which were both
trained on CONLL 2000 data using the CRF++
conditional random field toolkit (Taku-ku 2012).
We use chunk labels that augment the standard
BIO tags with appropriate Penn-Treebank phrasal
tags, for example, “B-NP” and “B-ADVP”.
Once the candidate sentences are chunked, the
two methods diverge in their approach to classifi-
cation. However, both approaches use the NLM
Lexical Tools and WordNet (Fellbaum, 1998) for
term expansion. The NLM’s normalization tool is
used to reduce terms to lower case, strip them of
punctuation, stop words, diacritical marks, etc.,
and to expand the terms with lexical variants.
WordNet’s synonyms and hypernyms for the re-
maining terms are then added to expand the term
lists for chunk-level comparisons.
</bodyText>
<sectionHeader confidence="0.983896" genericHeader="method">
3 Corpus-based Chunk Method
</sectionHeader>
<bodyText confidence="0.999954806451613">
The corpus-based method employs a “chunk
alignment” algorithm for selecting pairs of chunks
for detailed comparison, one from each candidate
sentence. The algorithm operates by initializing
pointers to the first chunk in each sentence. Then,
noting the chunk type for the indexed chunk in the
shorter sentence, the algorithm marches down the
longer sentence searching for the first chunk of the
same type. Once it is found, the two chunks are
marked for comparison and the index into the
shorter sentence is incremented to the next chunk.
The process repeats until no more chunk pairs can
be associated. Figure 1 shows an example of chunk
alignment.
The method generates the set of features shown
in Table 2 based on the chunk-level comparisons.
Features 2 to 4 contain numerical values represent-
ing the sums of “matching scores” from the aligned
chunks. A four-valued matching score is assigned
for each chunk comparison depending on the de-
gree of chunk-level similarity. A value of “3” rep-
resents an exact term match or a match on a
synonym. The value “2” is given if the head term
of one of the chunks is in the hypernym tree for the
other chunk. And a value of “1” is given if the two
chunk heads have a common hypernym ancestor.
The default value “0” is given if none of the above
conditions is found. The numbers in brackets in the
table identify the unigram features that are associ-
ated to compose trigram and 4-gram features, re-
spectively.
</bodyText>
<listItem confidence="0.8678209">
Unigrams 0 Total # chunks in Sentence A
1 Total # chunks in Sentence B
2 Sum of aligned VP matching scores
3 Sum of aligned NP matching scores
4 Sum of aligned PP matching scores
5 Number of VP chunks in A
6 Number of NP chunks in A
7 Number of PP chunks in A
8 Number of VP chunks in B
9 Number of NP chunks in B
</listItem>
<table confidence="0.650280333333333">
10 Number of PP chunks in B
Trigrams [ 2, 5, 8 ], [ 3, 6, 9 ], [ 4, 7, 10 ]
4-grams [ 0, 5, 6, 7 ], [ 1, 8, 9, 10 ]
</table>
<tableCaption confidence="0.99633">
Table 2. Similarity classifier features.
</tableCaption>
<bodyText confidence="0.734365333333333">
Once the feature vector is, it is passed to the text
similarity classifier, which generates the 0-5 simi-
larity score. The classifier was trained on the gold
</bodyText>
<figure confidence="0.572001833333333">
Micron’s num- also marked the first quar- in three years for the DRAM
bers terly profit manufacturer
NP ADVP VP NP PP PP
Micron has declared its first quarter- in three years
ly profit
NP VP NP PP
</figure>
<figureCaption confidence="0.994952">
Figure 1. Chunk Alignment Example
</figureCaption>
<page confidence="0.983426">
548
</page>
<bodyText confidence="0.9914695">
standard training data using the CRF++ toolkit us-
ing the same feature set described above.
</bodyText>
<sectionHeader confidence="0.991255" genericHeader="method">
4 Rule-based Chunk Method
</sectionHeader>
<bodyText confidence="0.9949145">
The rule-based chunk similarity method employs a
breadth-first search method for selecting candidate
chunks for further comparison. The algorithm op-
erates by selecting the first chunk of the sentence
with the larger number of chunks. It then marches
down each chunk of the shorter sentence looking
for an exact term match or head term synonym
match. If a match is found, a chunk-level score
value of 3 is assigned, and the next chunk in the
longer sentence is considered. If a match is not
found, then a new search is performed, this time
searching for a hypernym match. If a match is
found in this second pass, a chunk score of 2 is
assigned, and the next index chunk is considered.
If not, then a third and final pass is performed
searching for a related term match. If a match is
found after this third pass, a chunk score of 1 is
assigned; otherwise, the chunks are deemed dis-
similar and receive a chunk score of zero.
We describe this algorithm as “breadth-first”
because it has the effect of conducting up to three
passes across all of the chunks of the target (short-
er) sentence, looking for successively “looser”
matches. For these purposes, we consider a hyper-
nym match to be looser than an exact or synynym
match, and a common-ancestor (related) term
match to be looser than a hypernym match.
The chunk-level matching scores are accumulat-
ed in the above manner, just as for the corpus-
based method. However, in this case, the results
are used directly by the rule-based scoring algo-
rithm. The scoring algorithm treats predicate and
argument chunks separately and generates raw
scores for each. It then combines them to compute
the final similarity score. The predicate raw score
is the accumulated score for all VP chunk compari-
sons, divided by three times the number of such
comparisons. This results in a predicate raw score
that is in the range [0,1], since the maximum
chunk-level matching score is three. The argument
raw score is produced in the same manner and
multiplied by 5.0, producing a value in the range
[0,5].
Where both predicate and argument raw scores
exist, the total similarity score for the sentence pair
is computed as the product of the two raw scores.
This formulation has the benefit of permitting the
degree of similarity for each score type to affect
the overall score. For example, consider “Sarah
bought the book,” and “Sarah read the book.”
Here, the difference in predicate (“bought” versus
“read”) will temper the otherwise exact match on
the arguments. Similarly, for “Sarah bought the
book,” and “Sarah bought the fish,” the inexact
match on arguments will soften the perfect predi-
cate score.
Table 3 illustrates how the basic rule-based al-
gorithm works. The table shows the associated
chunks from each sentence, their chunk type for
scoring purposes, and their chunk-level matching
score values. Thus, for example, “The Korean Air
deal” and “the final agreement” have a matching
score value of 2 because “agreement” is a hyper-
nym of “deal”. Moreover, because there is no men-
tion of “Bob Saling” in the first sentence, the
corresponding matching value is zero.
Based on the chunk-level scores in the table, the
similarity score is calculated as follows. The raw
predicate score for the two predicate chunk pairs is
6 (3 for each, from the table), divided by the max-
S1: Boeing said the final agreement is expected to be signed during the next few weeks.
S2: The Korean Air deal is expected to be finalized “in the next several weeks,” Boeing spokesman Bob
Saling said.
S2 chunk phrase S1 chunk phrase Chunk type Matching score
The Korean Air deal the final agreement argument 2
is expected to be finalized is expected to be signed predicate 3
in the next several weeks during the next several weeks argument 3
Boeing spokesman Boeing argument 3
Bob Saling argument 0
said said predicate 3
</bodyText>
<tableCaption confidence="0.991999">
Table 3. Chunk-level matching scores for rule-based scoring example from training data.
</tableCaption>
<page confidence="0.998252">
549
</page>
<bodyText confidence="0.999970142857143">
imum possible score, which is also 6, yielding a
value of 1.000. The argument raw score is the sum
of the scores for the argument pairs, 8 in this case,
divided by the maximum possible (12 for the four
argument chunks), scaled by 5, yielding a value of
3.333. The final score is their product, 3.333,
which compares favorably with the gold standard
score value of 3.000 for this sentence pair.
If there are no predicate chunk comparisons for
the sentence pair, the rule-based scoring algorithm
uses the raw argument score without modification.
Similarly, where there are no argument chunk
comparisons, the rule uses the raw predicate score
multiplied by 5.0 to scale it to cover the range
[0,5]. By being robust against zero values in this
manner, the algorithm is able to handle compari-
sons of sentence fragments such as “Tunisia”, in
the event it is the entirety of the input “sentence”.
Additionally, the final score that is reported is
the minimum of the combined score described
above and an upper limit value that is initialized at
5.0, but which can be reduced as each chunk-level
comparison is performed. The upper limit value is
reduced to 4.0 if there is a qualifier mismatch (e.g.,
“uncooked pizza” v. “pizza”). It is reduced to 3.0 if
there is a number mismatch, for example, “Two
men are playing chess” versus “Three men are
playing chess.”
</bodyText>
<sectionHeader confidence="0.998675" genericHeader="conclusions">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999652034482759">
Table 4 shows the results for both algorithms
against the STS test suite. The “Corpus-based” and
“Rule-based” columns reports results for the two
chunk-based similarity algorithm. The five lowest
rows represent the five individual data sets in the
suite. The values in the table represent Pearson
correlation values, which range from -1 to +1,
where the closer a value is to 1, the stronger the
positive correlation.
The three upper rows represent the three metrics
that were used to compute global results across all
of the data sets. “All” refers to the computation of
a Pearson value where the five gold standards and
corresponding results were concatenated. The
“Allnrm” row reports correlation values obtained
by scaling and translating system outputs in a
manner that maintains the individual data set corre-
lation values, yet minimizes the combined data set
error. Finally, the “Mean” reports the weighted
average of the individual data set correlation val-
ues, where the weights used were the numbers of
sentence pairs in each data set. There were 750
sentence pairs in each of the MSRpar, MSRvid,
and OnWN data sets, but only 459 in the SMT-eur
data set and 399 in the SMT-news data set, for a
total of 3108 sentence pairs. The characteristics of
the different data sets and greater detail on the
global scoring metrics are discussed further in the
STS task description paper (Agirre, et al., 2012).
</bodyText>
<table confidence="0.9993777">
Category Corpus- Rule-based Improve-
based ment (%)
All .4976 .5306 6.63%
Allnrm .7160 .7646 6.79%
Mean .3215 .5069 57.67%
MSRpar .2312 .4536 96.20%
MSRvid .6595 .7079 7.33%
SMT-eur .1504 .3996 165.68%
On-WN .2735 .5149 88.26%
SMT-news .1426 .3379 136.98%
</table>
<tableCaption confidence="0.99996">
Table 4. Results against STS test suite.1
</tableCaption>
<bodyText confidence="0.999971608695652">
As Table 4 shows, the rule-based method out-
performed the corpus-based method for all indi-
vidual data sets and for all combined measures.
The percentage improvement is noted in the right-
most column in the figure.
We believe the results for the rule-based method
are sufficient to show that chunk-based methods
may have a role to play in text similarity determi-
nations, particularly in high volume applications
where high throughput is essential. Chunking is
computationally cheap to perform. It is also robust
against sentence fragments and against incomplete
or ungrammatical sentence constructions, as may
be found in emails, text messages, and blog posts.
However, chunk-based methods may be restrict-
ed to such applications since, on an absolute scale,
performance was in the bottom one-third of all sys-
tems that reported results against the STS data
suite. Nevertheless, we recognize that our investi-
gations into chunk-based methods were limited in
both time and scope. As a result, we do not believe
we have yet encountered the upper limit on per-
formance for chunk-based text similarity systems.
</bodyText>
<footnote confidence="0.984985">
1 The results for the corpus-based chunk method are reported
under the name “demetrios_glinos/task6-ATA-CHNK” on the
official STS results page, http://www.cs.york.ac.uk/semeval-
2012/task6/index.php?id=results-update.
</footnote>
<page confidence="0.996332">
550
</page>
<sectionHeader confidence="0.995787" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999735823529412">
Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonza-
lez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on
Semantic Textual Similarity. In Proceedings of the
6th International Workshop on Semantic Evaluation
(SemEval 2012), in conjunction with the First Joint
Conference on Lexical and Computational Semantics
(*SEM 2012).
Alberto Barr—n-Cedeiio, Andreas Eiselt, and Paolo Ros-
so. 2009. Monolingual Text Similarity Measures: A
Comparison of Models over Wikipedia Articles Re-
visions. In Proceedings of the ICON-2009: 7th Inter-
national Conference on Natural Language
Processing, pp. 29-38.
Moses S. Charikar. 2002. Similarity Estimation Tech-
niques from Rounding Algorithms. In STOC &apos;02 Pro-
ceedings of the thirty-fourth annual ACM symposium
on theory of computing.
Taku-ku. 2012. CRF++: Yet Another CRF toolkit.
http://crfpp.googlecode.com/svn/trunk/doc/index.htm
l.
David K. Evans, Kathleen McKeown, and Judith L.
Klavans. 2005. Similarity-based Multilingual Multi-
Document Summarization. IEEE Transactions on In-
formation Theory.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Ryan Moulton. 2010. Simple Simhashing: Clustering in
linear time. [Internet]. Version 1. Ryan Moulton&apos;s Ar-
ticles. Available from: http://moultano.wordpress.
com/article/simple-simhashing-3kbzhsxyg4467-6/.
NLM. 2012. U.S. National Library of Medicine, Lexical
Systems Group, Lexical Tools. http://lexsrv3.nlm.
nih.gov/LexSysGroup/Projects/lvg/current/web/index
.html.
</reference>
<page confidence="0.998044">
551
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.428317">
<title confidence="0.883895">ATA-Sem: Chunk-based Determination of Semantic Text Similarity Demetrios</title>
<affiliation confidence="0.665607">Advanced Text Analytics,</affiliation>
<address confidence="0.822037">Orlando, Florida,</address>
<email confidence="0.999958">demetrios.glinos@advancedtextanalytics.com</email>
<abstract confidence="0.998644210526316">This paper describes investigations into using syntactic chunk information as the basis for determining the similarity of candidate texts at the semantic level. Two approaches were considered. The first was a corpus-based method that extracted lexical and semantic features from pairs of chunks from each sentence that were associated through a chunk alignment algorithm. The features were used as input to a classifier trained on the same features extracted from a corpus of gold standard training data. The second approach involved breadthfirst chunk association and the application of a rule-based scoring algorithm. Both approaches were evaluated against the test data for the SemEval 2012 Semantic Text Similarity task. The results show that the rule-based chunk approach is superior.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<contexts>
<context position="1882" citStr="Agirre, et al., 2012" startWordPosition="273" endWordPosition="276">ge corpus document retrieval (Charikar, 2002). While textual similarity can be performed at the purely surface lexical level, as in the “simhash” clustering method described in (Moulton, 2010), similarity also applies at the semantic level, where conceptually similar texts may nevertheless be entirely dissimilar at the surface lexical level. For example, the phrases “restrict or confine” and “place limits on (extent or access)” share no words or morphological roots, yet mean very nearly the same thing at the semantic level. The Semantic Textual Similarity (STS) task (Task #6) at SemEval-2012 (Agirre, et al., 2012) provided a forum for exploring these issues by furnishing training and evaluation data, and also a common standard for describing degrees of similarity, shown in Table 1. Score Description 5 The two sentences are completely equivalent, as they mean the same thing. 4 The two sentences are mostly equivalent, but some unimportant details differ. 3 The two sentences are roughly equivalent, but some important information differs/missing. 2 The two sentences are not equivalent, but share some details. 1 The two sentences are not equivalent, but are on the same topic. 0 The two sentences are on diff</context>
<context position="13840" citStr="Agirre, et al., 2012" startWordPosition="2308" endWordPosition="2311">the individual data set correlation values, yet minimizes the combined data set error. Finally, the “Mean” reports the weighted average of the individual data set correlation values, where the weights used were the numbers of sentence pairs in each data set. There were 750 sentence pairs in each of the MSRpar, MSRvid, and OnWN data sets, but only 459 in the SMT-eur data set and 399 in the SMT-news data set, for a total of 3108 sentence pairs. The characteristics of the different data sets and greater detail on the global scoring metrics are discussed further in the STS task description paper (Agirre, et al., 2012). Category Corpus- Rule-based Improvebased ment (%) All .4976 .5306 6.63% Allnrm .7160 .7646 6.79% Mean .3215 .5069 57.67% MSRpar .2312 .4536 96.20% MSRvid .6595 .7079 7.33% SMT-eur .1504 .3996 165.68% On-WN .2735 .5149 88.26% SMT-news .1426 .3379 136.98% Table 4. Results against STS test suite.1 As Table 4 shows, the rule-based method outperformed the corpus-based method for all individual data sets and for all combined measures. The percentage improvement is noted in the rightmost column in the figure. We believe the results for the rule-based method are sufficient to show that chunk-based m</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Barr—n-Cedeiio</author>
<author>Andreas Eiselt</author>
<author>Paolo Rosso</author>
</authors>
<title>Monolingual Text Similarity Measures: A Comparison of Models over Wikipedia Articles Revisions.</title>
<date>2009</date>
<booktitle>In Proceedings of the ICON-2009: 7th International Conference on Natural Language Processing,</booktitle>
<pages>29--38</pages>
<marker>Barr—n-Cedeiio, Eiselt, Rosso, 2009</marker>
<rawString>Alberto Barr—n-Cedeiio, Andreas Eiselt, and Paolo Rosso. 2009. Monolingual Text Similarity Measures: A Comparison of Models over Wikipedia Articles Revisions. In Proceedings of the ICON-2009: 7th International Conference on Natural Language Processing, pp. 29-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moses S Charikar</author>
</authors>
<title>Similarity Estimation Techniques from Rounding Algorithms. In</title>
<date>2002</date>
<booktitle>STOC &apos;02 Proceedings of the thirty-fourth annual ACM symposium on theory of computing.</booktitle>
<contexts>
<context position="1306" citStr="Charikar, 2002" startWordPosition="186" endWordPosition="187"> second approach involved breadthfirst chunk association and the application of a rule-based scoring algorithm. Both approaches were evaluated against the test data for the SemEval 2012 Semantic Text Similarity task. The results show that the rule-based chunk approach is superior. 1 Introduction The task of determining whether two texts are similar in some sense has important applications in the field of natural language processing, including but not limited to document summarization (Evans, et al., 2005), plagiarism detection (Barr—n-Cede–o, et al., 2009) and large corpus document retrieval (Charikar, 2002). While textual similarity can be performed at the purely surface lexical level, as in the “simhash” clustering method described in (Moulton, 2010), similarity also applies at the semantic level, where conceptually similar texts may nevertheless be entirely dissimilar at the surface lexical level. For example, the phrases “restrict or confine” and “place limits on (extent or access)” share no words or morphological roots, yet mean very nearly the same thing at the semantic level. The Semantic Textual Similarity (STS) task (Task #6) at SemEval-2012 (Agirre, et al., 2012) provided a forum for ex</context>
</contexts>
<marker>Charikar, 2002</marker>
<rawString>Moses S. Charikar. 2002. Similarity Estimation Techniques from Rounding Algorithms. In STOC &apos;02 Proceedings of the thirty-fourth annual ACM symposium on theory of computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Taku-ku</author>
</authors>
<title>CRF++: Yet Another CRF toolkit. http://crfpp.googlecode.com/svn/trunk/doc/index.htm l.</title>
<date>2012</date>
<contexts>
<context position="4033" citStr="Taku-ku 2012" startWordPosition="612" endWordPosition="613">ada, June 7-8, 2012. c�2012 Association for Computational Linguistics 2 Common Processing Components A common processing core supports both of the methods, comprising preprocessing components and also shared components for determining chunk-level similarity. The preprocessing components make use of the U.S. National Library of Medicine’s Lexical Tools (NLM 2012) to perform ASCII conversion and tokenization. Candidate sentence pairs are then tagged and chunked using our own tagger and separate chunker, which were both trained on CONLL 2000 data using the CRF++ conditional random field toolkit (Taku-ku 2012). We use chunk labels that augment the standard BIO tags with appropriate Penn-Treebank phrasal tags, for example, “B-NP” and “B-ADVP”. Once the candidate sentences are chunked, the two methods diverge in their approach to classification. However, both approaches use the NLM Lexical Tools and WordNet (Fellbaum, 1998) for term expansion. The NLM’s normalization tool is used to reduce terms to lower case, strip them of punctuation, stop words, diacritical marks, etc., and to expand the terms with lexical variants. WordNet’s synonyms and hypernyms for the remaining terms are then added to expand </context>
</contexts>
<marker>Taku-ku, 2012</marker>
<rawString>Taku-ku. 2012. CRF++: Yet Another CRF toolkit. http://crfpp.googlecode.com/svn/trunk/doc/index.htm l.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David K Evans</author>
<author>Kathleen McKeown</author>
<author>Judith L Klavans</author>
</authors>
<title>Similarity-based Multilingual MultiDocument Summarization.</title>
<date>2005</date>
<journal>IEEE Transactions on Information Theory.</journal>
<contexts>
<context position="1201" citStr="Evans, et al., 2005" startWordPosition="171" endWordPosition="174">input to a classifier trained on the same features extracted from a corpus of gold standard training data. The second approach involved breadthfirst chunk association and the application of a rule-based scoring algorithm. Both approaches were evaluated against the test data for the SemEval 2012 Semantic Text Similarity task. The results show that the rule-based chunk approach is superior. 1 Introduction The task of determining whether two texts are similar in some sense has important applications in the field of natural language processing, including but not limited to document summarization (Evans, et al., 2005), plagiarism detection (Barr—n-Cede–o, et al., 2009) and large corpus document retrieval (Charikar, 2002). While textual similarity can be performed at the purely surface lexical level, as in the “simhash” clustering method described in (Moulton, 2010), similarity also applies at the semantic level, where conceptually similar texts may nevertheless be entirely dissimilar at the surface lexical level. For example, the phrases “restrict or confine” and “place limits on (extent or access)” share no words or morphological roots, yet mean very nearly the same thing at the semantic level. The Semant</context>
</contexts>
<marker>Evans, McKeown, Klavans, 2005</marker>
<rawString>David K. Evans, Kathleen McKeown, and Judith L. Klavans. 2005. Similarity-based Multilingual MultiDocument Summarization. IEEE Transactions on Information Theory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="4351" citStr="Fellbaum, 1998" startWordPosition="660" endWordPosition="661">l Library of Medicine’s Lexical Tools (NLM 2012) to perform ASCII conversion and tokenization. Candidate sentence pairs are then tagged and chunked using our own tagger and separate chunker, which were both trained on CONLL 2000 data using the CRF++ conditional random field toolkit (Taku-ku 2012). We use chunk labels that augment the standard BIO tags with appropriate Penn-Treebank phrasal tags, for example, “B-NP” and “B-ADVP”. Once the candidate sentences are chunked, the two methods diverge in their approach to classification. However, both approaches use the NLM Lexical Tools and WordNet (Fellbaum, 1998) for term expansion. The NLM’s normalization tool is used to reduce terms to lower case, strip them of punctuation, stop words, diacritical marks, etc., and to expand the terms with lexical variants. WordNet’s synonyms and hypernyms for the remaining terms are then added to expand the term lists for chunk-level comparisons. 3 Corpus-based Chunk Method The corpus-based method employs a “chunk alignment” algorithm for selecting pairs of chunks for detailed comparison, one from each candidate sentence. The algorithm operates by initializing pointers to the first chunk in each sentence. Then, noti</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Moulton</author>
</authors>
<title>Simple Simhashing: Clustering in linear time. [Internet]. Version 1. Ryan Moulton&apos;s Articles. Available from:</title>
<date>2010</date>
<pages>3--4467</pages>
<contexts>
<context position="1453" citStr="Moulton, 2010" startWordPosition="208" endWordPosition="209">st the test data for the SemEval 2012 Semantic Text Similarity task. The results show that the rule-based chunk approach is superior. 1 Introduction The task of determining whether two texts are similar in some sense has important applications in the field of natural language processing, including but not limited to document summarization (Evans, et al., 2005), plagiarism detection (Barr—n-Cede–o, et al., 2009) and large corpus document retrieval (Charikar, 2002). While textual similarity can be performed at the purely surface lexical level, as in the “simhash” clustering method described in (Moulton, 2010), similarity also applies at the semantic level, where conceptually similar texts may nevertheless be entirely dissimilar at the surface lexical level. For example, the phrases “restrict or confine” and “place limits on (extent or access)” share no words or morphological roots, yet mean very nearly the same thing at the semantic level. The Semantic Textual Similarity (STS) task (Task #6) at SemEval-2012 (Agirre, et al., 2012) provided a forum for exploring these issues by furnishing training and evaluation data, and also a common standard for describing degrees of similarity, shown in Table 1.</context>
</contexts>
<marker>Moulton, 2010</marker>
<rawString>Ryan Moulton. 2010. Simple Simhashing: Clustering in linear time. [Internet]. Version 1. Ryan Moulton&apos;s Articles. Available from: http://moultano.wordpress. com/article/simple-simhashing-3kbzhsxyg4467-6/.</rawString>
</citation>
<citation valid="false">
<authors>
<author>U S</author>
</authors>
<booktitle>National Library of Medicine, Lexical Systems Group, Lexical Tools. http://lexsrv3.nlm. nih.gov/LexSysGroup/Projects/lvg/current/web/index .html.</booktitle>
<marker>S, </marker>
<rawString>NLM. 2012. U.S. National Library of Medicine, Lexical Systems Group, Lexical Tools. http://lexsrv3.nlm. nih.gov/LexSysGroup/Projects/lvg/current/web/index .html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>