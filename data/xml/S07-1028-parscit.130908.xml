<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002730">
<title confidence="0.933464">
FBK-IRST: Kernel Methods for Semantic Relation Extraction
</title>
<author confidence="0.792463">
Claudio Giuliano and Alberto Lavelli and Daniele Pighin and Lorenza Romano
</author>
<affiliation confidence="0.466786">
FBK-IRST, Istituto per la Ricerca Scientifica e Tecnologica
</affiliation>
<address confidence="0.682422">
I-38050, Povo (TN), ITALY
</address>
<email confidence="0.980321">
{giuliano,lavelli,pighin,romano}@itc.it
</email>
<sectionHeader confidence="0.995094" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99990105882353">
We present an approach for semantic rela-
tion extraction between nominals that com-
bines shallow and deep syntactic processing
and semantic information using kernel meth-
ods. Two information sources are consid-
ered: (i) the whole sentence where the re-
lation appears, and (ii) WordNet synsets and
hypernymy relations of the candidate nom-
inals. Each source of information is rep-
resented by kernel functions. In particu-
lar, five basic kernel functions are linearly
combined and weighted under different con-
ditions. The experiments were carried out
using support vector machines as classifier.
The system achieves an overall F1 of 71.8%
on the Classification of Semantic Relations
between Nominals task at SemEval-2007.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999727239130435">
The starting point of our research is an approach
for identifying relations between named entities ex-
ploiting only shallow linguistic information, such as
tokenization, sentence splitting, part-of-speech tag-
ging and lemmatization (Giuliano et al., 2006). A
combination of kernel functions is used to represent
two distinct information sources: (i) the global con-
text where entities appear and (ii) their local con-
texts. The whole sentence where the entities appear
(global context) is used to discover the presence of
a relation between two entities. Windows of limited
size around the entities (local contexts) provide use-
ful clues to identify the roles played by the entities
within a relation (e.g., agent and target of a gene in-
teraction). In the task of detecting protein-protein
interactions, we obtained state-of-the-art results on
two biomedical data sets. In addition, promising re-
sults have been recently obtained for relations such
as work for and org based in in the news domain1.
In this paper, we investigate the use of the above
approach to discover semantic relations between
nominals. In addition to the original feature rep-
resentation, we have integrated deep syntactic pro-
cessing of the global context and semantic informa-
tion for each candidate nominals using WordNet as
external knowledge source. Each source of informa-
tion is represented by kernel functions. A tree kernel
(Moschitti, 2004) is used to exploit the deep syn-
tactic processing obtained using the Charniak parser
(Charniak, 2000). On the other hand, bag of syn-
onyms and hypernyms is used to enhance the repre-
sentation of the candidate nominals. The final sys-
tem is based on five basic kernel functions (bag-of-
words kernel, global context kernel, tree kernel, su-
persense kernel, bag of synonyms and hypernyms
kernel) linearly combined and weighted under dif-
ferent conditions. The experiments were carried out
using support vector machines (Vapnik, 1998) as
classifier.
We present results on the Classification of Seman-
tic Relations between Nominals task at SemEval-
2007, in which sentences containing ordered pairs
of marked nominals, possibly semantically related,
have to be classified. On this task, we achieve an
overall F1 of 71.8% (B category evaluation), largely
outperforming all the baselines.
</bodyText>
<footnote confidence="0.992937">
1These results appear in a paper currently under revision.
</footnote>
<page confidence="0.965373">
141
</page>
<bodyText confidence="0.735995">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 141–144,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.987799" genericHeader="method">
2 Kernel Methods for Relation Extraction
</sectionHeader>
<bodyText confidence="0.9911435">
In order to implement the approach based on syntac-
tic and semantic information, we employed a linear
weighted combination of kernels, using support vec-
tor machines as classifier. We designed two families
of basic kernels: syntactic kernels and semantic ker-
nels. These basic kernels are combined by exploit-
ing the closure properties of kernels. We define our
composite kernel KC(x1, x2) as follows
</bodyText>
<equation confidence="0.993025">
Ki(x1, x2) (1)
VJKi(x1, x1)Ki(x2, x2),
</equation>
<bodyText confidence="0.996680833333333">
where each basic kernel Ki is normalized and wi E
10, 11 is the kernel weight. The normalization factor
plays an important role in allowing us to integrate in-
formation from heterogeneous knowledge sources.
All basic kernels, but the tree kernel (see Section
2.1.3), are explicitly calculated as follows
</bodyText>
<equation confidence="0.999792">
Ki(x1,x2) = (0(x1),0(x2)), (2)
</equation>
<bodyText confidence="0.999968">
where 0(·) is the embedding vector. Even though
the resulting feature space has high dimensionality,
an efficient computation of Equation 2 can be carried
out explicitly since the input representations defined
below are extremely sparse.
</bodyText>
<subsectionHeader confidence="0.988758">
2.1 Syntactic Kernels
</subsectionHeader>
<bodyText confidence="0.9999695">
Syntactic kernels are defined over the whole sen-
tence where the candidate nominals appear.
</bodyText>
<subsectionHeader confidence="0.376423">
2.1.1 Global Context Kernel
</subsectionHeader>
<bodyText confidence="0.984761">
Bunescu and Mooney (2005) and Giuliano et al.
(2006) successfully exploited the fact that relations
between named entities are generally expressed us-
ing only words that appear simultaneously in one of
the following three contexts.
Fore-Between Tokens before and between the two
entities, e.g. “the head of [ORG], Dr. [PER]”.
Between Only tokens between the two entities, e.g.
“[ORG] spokesman [PER]”.
Between-After Tokens between and after the two
entities, e.g. “[PER], a [ORG] professor”.
Here, we investigate whether this assumption is
also correct for semantic relations between nomi-
nals. Our global context kernel operates on the con-
texts defined above, where each context is repre-
sented using a bag-of-words. More formally, given
</bodyText>
<figureCaption confidence="0.9984215">
Figure 1: A content-container relation test sentence
parse tree (a) and the corresponding RT structure (b).
</figureCaption>
<bodyText confidence="0.9383255">
a relation example R, we represent a context C as a
row vector
</bodyText>
<equation confidence="0.843973">
OC(R) _ (tf(t1, C), tf(t2, C), ... , tf(tl, C)) E Rl, (3)
</equation>
<bodyText confidence="0.999986916666667">
where the function tf(ti, C) records how many
times a particular token ti is used in C. Note that
this approach differs from the standard bag-of-words
as punctuation and stop words are included in 0C,
while the nominals are not. To improve the classi-
fication performance, we have further extended 0C
to embed n-grams of (contiguous) tokens (up to n =
3). By substituting 0C into Equation 2, we obtain
the n-gram kernel Kn, which counts uni-grams, bi-
grams, ... , n-grams that two patterns have in com-
mon2. The Global Context kernel KGC(R1, R2) is
then defined as
</bodyText>
<equation confidence="0.998532">
KFB(R1, R2) + KB(R1, R2) + KBA(R1, R2), (4)
</equation>
<bodyText confidence="0.999784">
where KFB, KB and KBA are n-gram kernels
that operate on the Fore-Between, Between and
Between-After patterns respectively.
</bodyText>
<subsubsectionHeader confidence="0.508175">
2.1.2 Bag-of-Words Kernel
</subsubsectionHeader>
<bodyText confidence="0.9998765">
The bag-of-words kernel is defined as the previ-
ous kernel but it operates on the whole sentence.
</bodyText>
<subsectionHeader confidence="0.514881">
2.1.3 Tree Kernel
</subsectionHeader>
<bodyText confidence="0.985894">
Tree kernels can trigger automatic feature selec-
tion and represent a viable alternative to the man-
</bodyText>
<footnote confidence="0.881911">
2In the literature, it is also called n-spectrum kernel.
</footnote>
<figure confidence="0.998209891891892">
a) S1
S
NP
PRP
I
VP
.
VBD
found
NP
PP
IN
in
DT NN
some candy
b) S
.
NP
NN
underwear
PRP$
my
VP
VBD
found
PP
NP
NN
target
NP
NNS
agent
IN
in
n
wi
i=1
</figure>
<page confidence="0.981029">
142
</page>
<bodyText confidence="0.994978142857143">
ual design of attribute-value syntactic features (Mos-
chitti, 2004). A tree kernel KT(t1, t2) evaluates
the similarity between two trees t1 and t2 in terms
of the number of fragments they have in common.
Let Nt be the set of nodes of a tree t and F =
{f1, f2, ... , f|F|} be the fragment space of t1 and
t2. Then
</bodyText>
<equation confidence="0.9514234">
KT(t1,t2) = � � nj�Nt2 O(n�, nj) , (5)
n��Nt1
where A(ni, nj) = �|F|
k�1 Ik(ni) × IK(nj) and
Ik(n) = 1 if k is rooted in n, 0 otherwise.
</equation>
<bodyText confidence="0.999500666666667">
For this task, we defined an ad-hoc class of struc-
tured features (Moschitti et al., 2006), the Reduced
Tree (RT), which can be derived from a sentence
parse tree t by the following steps: (1) remove all the
terminal nodes but those labeled as relation entities
and those POS tagged as verbs, auxiliaries, prepo-
sitions, modals or adverbs; (2) remove all the in-
ternal nodes not covering any remaining terminal;
(3) replace the entity words with placeholders that
indicate the direction in which the relation should
hold. Figure 1 shows a parse tree and the resulting
RT structure.
</bodyText>
<subsectionHeader confidence="0.998082">
2.2 Semantic Kernels
</subsectionHeader>
<bodyText confidence="0.999979571428571">
In (Giuliano et al., 2006), we used the local context
kernel to infer semantic information on the candi-
date entities (i.e., roles played by the entities). As
the task organizers provide the WordNet sense and
role for each nominal, we directly use this informa-
tion to enrich the feature space and do not include
the local context kernel in the combination.
</bodyText>
<subsectionHeader confidence="0.985848">
2.2.1 Bag of Synonyms and Hypernyms Kernel
</subsectionHeader>
<bodyText confidence="0.9996386">
By using the WordNet sense key provided, each
nominal is represented by the bag of its synonyms
and hypernyms (direct and inherited hypernyms).
Formally, given a relation example R, each nominal
N is represented as a row vector
</bodyText>
<equation confidence="0.962848">
ON(R) = (f(t1, N), f(t2, N), ... , f(tl, N)) E Rl, (6)
</equation>
<bodyText confidence="0.9999945">
where the binary function f(ti, N) records if a par-
ticular lemma ti is contained into the bag of syn-
onyms and hypernyms of N. The bag of synonyms
and hypernyms kernel KS&amp;H(R1, R2) is defined as
</bodyText>
<equation confidence="0.996172">
Ktarget(R1, R2) + Kagent(R1, R2), (7)
</equation>
<bodyText confidence="0.999977666666667">
where Ktarget and Kagent are defined by substitut-
ing the embedding of the target and agent nominals
into Equation 2 respectively.
</bodyText>
<subsectionHeader confidence="0.618195">
2.2.2 Supersense Kernel
</subsectionHeader>
<bodyText confidence="0.99992125">
WordNet synsets are organized into 45 lexicogra-
pher files, based on syntactic category and logical
groupings. E.g., noun.artifact is for nouns denoting
man-made objects, noun.attribute for nouns denot-
ing attributes for people and objects etc. The super-
sense kernel KSS(R1, R2) is a variant of the previ-
ous kernel that uses the names of the lexicographer
files (i.e., the supersense) to index the feature space.
</bodyText>
<sectionHeader confidence="0.975821" genericHeader="method">
3 Experimental Setup and Results
</sectionHeader>
<bodyText confidence="0.999984869565217">
Sentences have been tokenized, lemmatized, and
POS tagged with TextPro3. We considered each re-
lation as a different binary classification task, and
each sentence in the data set is a positive or negative
example for the relation. The direction of the rela-
tion is considered labelling the first argument of the
relation as agent and the second as target.
All the experiments were performed using the
SVM package SVMLight-TK4, customized to em-
bed our own kernels. We optimized the linear com-
bination weights wi and regularization parameter c
using 10-fold cross-validation on the training set.
We set the cost-factor j to be the ratio between the
number of negative and positive examples.
Table 1 shows the performance on the test set. We
achieve an overall F1 of 71.8% (B category evalua-
tion), largely outperforming all the baselines, rang-
ing from 48.5% to 57.0%. The average training plus
test running time for a relation is about 10 seconds
on a Intel Pentium M755 2.0 GHz. Figure 2 shows
the learning curves on the test set. For all relations
but theme-tool, accurate classifiers can be learned
using a small fraction of training.
</bodyText>
<sectionHeader confidence="0.993133" genericHeader="discussions">
4 Discussion and Conclusion
</sectionHeader>
<bodyText confidence="0.9996348">
Experimental results show that our kernel-based ap-
proach is appropriate also to detect semantic rela-
tions between nominals. However, differently from
relation extraction between named entities, there is
not a common kernel setup for all relations. E.g.,
</bodyText>
<footnote confidence="0.999937">
3http://tcc.itc.it/projects/textpro/
4http://ai-nlp.info.uniroma2.it/moschitti/
</footnote>
<page confidence="0.99831">
143
</page>
<figure confidence="0.992839">
Learning Curve
30 40 50 60 70 80 90 100
Percentage of Training
</figure>
<figureCaption confidence="0.995041">
Figure 2: Learning curves on the test set.
</figureCaption>
<table confidence="0.999966222222222">
Relation P R Fl Acc
Cause-Effect 67.3 90.2 77.1 72.5
Instrument-Agency 76.9 78.9 77.9 78.2
Product-Producer 76.2 77.4 76.8 68.8
Origin-Entity 62.2 63.9 63.0 66.7
Theme-Tool 69.2 62.1 65.5 73.2
Part-Whole 65.5 73.1 69.1 76.4
Content-Container 78.8 68.4 73.2 74.3
Avg 70.9 73.4 71.8 72.9
</table>
<tableCaption confidence="0.99995">
Table 1: Results on the test set.
</tableCaption>
<bodyText confidence="0.99993462962963">
for content-container we obtain the best perfor-
mance combining the tree kernel and the bag of syn-
onyms and hypernyms kernel; on the other hand, for
instrument-agency the best performance is obtained
by combining the global kernel and the supersense
kernel. Surprisingly, the supersense kernel alone
works quite well and obtains results comparable to
the bag of synonyms and hypernyms kernel. This
result is particularly interesting as a supersense tag-
ger can easily provide a satisfactory accuracy (Cia-
ramita and Altun, 2006). On the other hand, ob-
taining an acceptable accuracy in word sense disam-
biguation (required for a realistic application of the
bag of synonyms and hypernyms kernel) is imprac-
tical as a sufficient amount of training for at least all
nouns is currently not available. Hence, the super-
sense could play a crucial role to improve the perfor-
mance when approaching this task without the nomi-
nals disambiguated. To model the global context us-
ing the Fore-Between, Between and Between-After
contexts did not produce a significant improvement
with respect to the bag-of-words model. This is
mainly due to the fact that examples have been col-
lected from the Web using heuristic patterns/queries,
most of which implying Between patterns/contexts
(e.g., for the cause-effect relation “* comes from *”,
“* out of *” etc.).
</bodyText>
<sectionHeader confidence="0.997502" genericHeader="acknowledgments">
5 Acknowledgements
</sectionHeader>
<bodyText confidence="0.981790166666667">
Claudio Giuliano, Alberto Lavelli and Lorenza Ro-
mano are supported by the X-Media project (http:
//www.x-media-project.org), sponsored
by the European Commission as part of the Infor-
mation Society Technologies (IST) programme un-
der EC grant number IST-FP6-026978.
</bodyText>
<sectionHeader confidence="0.998701" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999515484848485">
Razvan Bunescu and Raymond J. Mooney. 2005. Subse-
quence kernels for relation extraction. In Proceedings
of the 19th Conference on Neural Information Pro-
cessing Systems, Vancouver, British Columbia.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the First Meeting of the
North American Chapter of the Association for Com-
putational Linguistics, pages 132–139, San Francisco,
CA, USA. Morgan Kaufmann Publishers Inc.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and information
extraction with a supersense sequence tagger. In Pro-
ceedings of the 2006 Conference on Empirical Meth-
ods in Natural Language Processing, pages 594–602,
Sydney, Australia, July.
Claudio Giuliano, Alberto Lavelli, and Lorenza Romano.
2006. Exploiting shallow linguistic information for re-
lation extraction from biomedical literature. In Pro-
ceedings of the Eleventh Conference of the European
Chapter ofthe Association for Computational Linguis-
tics (EACL-2006), Trento, Italy, 5-7 April.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Semantic role labeling via tree kernel
joint inference. In Proceedings of the Tenth Confer-
ence on Computational Natural Language Learning,
CoNLL-X.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow statistic parsing. In Proceedings
of the 42nd Meeting of the Association for Computa-
tional Linguistics (ACL’04), Main Volume, pages 335–
342, Barcelona, Spain, July.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons, New York, NY.
</reference>
<figure confidence="0.994094222222222">
F1
40
20
90
80
70
60
50
30
10
0
Cause-Effect
Instrument-Agency
Product-Producer
Origin-Entity
Theme-Tool
Part-Whole
Content-Container
</figure>
<page confidence="0.974606">
144
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.507942">
<title confidence="0.999656">FBK-IRST: Kernel Methods for Semantic Relation Extraction</title>
<author confidence="0.999481">Giuliano Lavelli Pighin Romano</author>
<affiliation confidence="0.988419">FBK-IRST, Istituto per la Ricerca Scientifica e Tecnologica</affiliation>
<address confidence="0.972287">I-38050, Povo (TN), ITALY</address>
<abstract confidence="0.962878722222222">We present an approach for semantic relation extraction between nominals that combines shallow and deep syntactic processing and semantic information using kernel methods. Two information sources are considered: (i) the whole sentence where the relation appears, and (ii) WordNet synsets and hypernymy relations of the candidate nominals. Each source of information is represented by kernel functions. In particular, five basic kernel functions are linearly combined and weighted under different conditions. The experiments were carried out using support vector machines as classifier. system achieves an overall 71.8% on the Classification of Semantic Relations between Nominals task at SemEval-2007.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Razvan Bunescu</author>
<author>Raymond J Mooney</author>
</authors>
<title>Subsequence kernels for relation extraction.</title>
<date>2005</date>
<booktitle>In Proceedings of the 19th Conference on Neural Information Processing Systems,</booktitle>
<location>Vancouver, British Columbia.</location>
<contexts>
<context position="4727" citStr="Bunescu and Mooney (2005)" startWordPosition="713" endWordPosition="716">important role in allowing us to integrate information from heterogeneous knowledge sources. All basic kernels, but the tree kernel (see Section 2.1.3), are explicitly calculated as follows Ki(x1,x2) = (0(x1),0(x2)), (2) where 0(·) is the embedding vector. Even though the resulting feature space has high dimensionality, an efficient computation of Equation 2 can be carried out explicitly since the input representations defined below are extremely sparse. 2.1 Syntactic Kernels Syntactic kernels are defined over the whole sentence where the candidate nominals appear. 2.1.1 Global Context Kernel Bunescu and Mooney (2005) and Giuliano et al. (2006) successfully exploited the fact that relations between named entities are generally expressed using only words that appear simultaneously in one of the following three contexts. Fore-Between Tokens before and between the two entities, e.g. “the head of [ORG], Dr. [PER]”. Between Only tokens between the two entities, e.g. “[ORG] spokesman [PER]”. Between-After Tokens between and after the two entities, e.g. “[PER], a [ORG] professor”. Here, we investigate whether this assumption is also correct for semantic relations between nominals. Our global context kernel operat</context>
</contexts>
<marker>Bunescu, Mooney, 2005</marker>
<rawString>Razvan Bunescu and Raymond J. Mooney. 2005. Subsequence kernels for relation extraction. In Proceedings of the 19th Conference on Neural Information Processing Systems, Vancouver, British Columbia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>132--139</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="2508" citStr="Charniak, 2000" startWordPosition="377" endWordPosition="378">ve been recently obtained for relations such as work for and org based in in the news domain1. In this paper, we investigate the use of the above approach to discover semantic relations between nominals. In addition to the original feature representation, we have integrated deep syntactic processing of the global context and semantic information for each candidate nominals using WordNet as external knowledge source. Each source of information is represented by kernel functions. A tree kernel (Moschitti, 2004) is used to exploit the deep syntactic processing obtained using the Charniak parser (Charniak, 2000). On the other hand, bag of synonyms and hypernyms is used to enhance the representation of the candidate nominals. The final system is based on five basic kernel functions (bag-ofwords kernel, global context kernel, tree kernel, supersense kernel, bag of synonyms and hypernyms kernel) linearly combined and weighted under different conditions. The experiments were carried out using support vector machines (Vapnik, 1998) as classifier. We present results on the Classification of Semantic Relations between Nominals task at SemEval2007, in which sentences containing ordered pairs of marked nomina</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proceedings of the First Meeting of the North American Chapter of the Association for Computational Linguistics, pages 132–139, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimiliano Ciaramita</author>
<author>Yasemin Altun</author>
</authors>
<title>Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>594--602</pages>
<location>Sydney, Australia,</location>
<contexts>
<context position="11857" citStr="Ciaramita and Altun, 2006" startWordPosition="1917" endWordPosition="1921">76.4 Content-Container 78.8 68.4 73.2 74.3 Avg 70.9 73.4 71.8 72.9 Table 1: Results on the test set. for content-container we obtain the best performance combining the tree kernel and the bag of synonyms and hypernyms kernel; on the other hand, for instrument-agency the best performance is obtained by combining the global kernel and the supersense kernel. Surprisingly, the supersense kernel alone works quite well and obtains results comparable to the bag of synonyms and hypernyms kernel. This result is particularly interesting as a supersense tagger can easily provide a satisfactory accuracy (Ciaramita and Altun, 2006). On the other hand, obtaining an acceptable accuracy in word sense disambiguation (required for a realistic application of the bag of synonyms and hypernyms kernel) is impractical as a sufficient amount of training for at least all nouns is currently not available. Hence, the supersense could play a crucial role to improve the performance when approaching this task without the nominals disambiguated. To model the global context using the Fore-Between, Between and Between-After contexts did not produce a significant improvement with respect to the bag-of-words model. This is mainly due to the </context>
</contexts>
<marker>Ciaramita, Altun, 2006</marker>
<rawString>Massimiliano Ciaramita and Yasemin Altun. 2006. Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 594–602, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alberto Lavelli</author>
<author>Lorenza Romano</author>
</authors>
<title>Exploiting shallow linguistic information for relation extraction from biomedical literature.</title>
<date>2006</date>
<booktitle>In Proceedings of the Eleventh Conference of the European Chapter ofthe Association for Computational Linguistics (EACL-2006),</booktitle>
<location>Trento,</location>
<contexts>
<context position="1248" citStr="Giuliano et al., 2006" startWordPosition="174" endWordPosition="177">rmation is represented by kernel functions. In particular, five basic kernel functions are linearly combined and weighted under different conditions. The experiments were carried out using support vector machines as classifier. The system achieves an overall F1 of 71.8% on the Classification of Semantic Relations between Nominals task at SemEval-2007. 1 Introduction The starting point of our research is an approach for identifying relations between named entities exploiting only shallow linguistic information, such as tokenization, sentence splitting, part-of-speech tagging and lemmatization (Giuliano et al., 2006). A combination of kernel functions is used to represent two distinct information sources: (i) the global context where entities appear and (ii) their local contexts. The whole sentence where the entities appear (global context) is used to discover the presence of a relation between two entities. Windows of limited size around the entities (local contexts) provide useful clues to identify the roles played by the entities within a relation (e.g., agent and target of a gene interaction). In the task of detecting protein-protein interactions, we obtained state-of-the-art results on two biomedical</context>
<context position="4754" citStr="Giuliano et al. (2006)" startWordPosition="718" endWordPosition="721">to integrate information from heterogeneous knowledge sources. All basic kernels, but the tree kernel (see Section 2.1.3), are explicitly calculated as follows Ki(x1,x2) = (0(x1),0(x2)), (2) where 0(·) is the embedding vector. Even though the resulting feature space has high dimensionality, an efficient computation of Equation 2 can be carried out explicitly since the input representations defined below are extremely sparse. 2.1 Syntactic Kernels Syntactic kernels are defined over the whole sentence where the candidate nominals appear. 2.1.1 Global Context Kernel Bunescu and Mooney (2005) and Giuliano et al. (2006) successfully exploited the fact that relations between named entities are generally expressed using only words that appear simultaneously in one of the following three contexts. Fore-Between Tokens before and between the two entities, e.g. “the head of [ORG], Dr. [PER]”. Between Only tokens between the two entities, e.g. “[ORG] spokesman [PER]”. Between-After Tokens between and after the two entities, e.g. “[PER], a [ORG] professor”. Here, we investigate whether this assumption is also correct for semantic relations between nominals. Our global context kernel operates on the contexts defined </context>
<context position="7921" citStr="Giuliano et al., 2006" startWordPosition="1278" endWordPosition="1281">this task, we defined an ad-hoc class of structured features (Moschitti et al., 2006), the Reduced Tree (RT), which can be derived from a sentence parse tree t by the following steps: (1) remove all the terminal nodes but those labeled as relation entities and those POS tagged as verbs, auxiliaries, prepositions, modals or adverbs; (2) remove all the internal nodes not covering any remaining terminal; (3) replace the entity words with placeholders that indicate the direction in which the relation should hold. Figure 1 shows a parse tree and the resulting RT structure. 2.2 Semantic Kernels In (Giuliano et al., 2006), we used the local context kernel to infer semantic information on the candidate entities (i.e., roles played by the entities). As the task organizers provide the WordNet sense and role for each nominal, we directly use this information to enrich the feature space and do not include the local context kernel in the combination. 2.2.1 Bag of Synonyms and Hypernyms Kernel By using the WordNet sense key provided, each nominal is represented by the bag of its synonyms and hypernyms (direct and inherited hypernyms). Formally, given a relation example R, each nominal N is represented as a row vector</context>
</contexts>
<marker>Giuliano, Lavelli, Romano, 2006</marker>
<rawString>Claudio Giuliano, Alberto Lavelli, and Lorenza Romano. 2006. Exploiting shallow linguistic information for relation extraction from biomedical literature. In Proceedings of the Eleventh Conference of the European Chapter ofthe Association for Computational Linguistics (EACL-2006), Trento, Italy, 5-7 April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Daniele Pighin</author>
<author>Roberto Basili</author>
</authors>
<title>Semantic role labeling via tree kernel joint inference.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X.</booktitle>
<contexts>
<context position="7384" citStr="Moschitti et al., 2006" startWordPosition="1188" endWordPosition="1191">NP NN underwear PRP$ my VP VBD found PP NP NN target NP NNS agent IN in n wi i=1 142 ual design of attribute-value syntactic features (Moschitti, 2004). A tree kernel KT(t1, t2) evaluates the similarity between two trees t1 and t2 in terms of the number of fragments they have in common. Let Nt be the set of nodes of a tree t and F = {f1, f2, ... , f|F|} be the fragment space of t1 and t2. Then KT(t1,t2) = � � nj�Nt2 O(n�, nj) , (5) n��Nt1 where A(ni, nj) = �|F| k�1 Ik(ni) × IK(nj) and Ik(n) = 1 if k is rooted in n, 0 otherwise. For this task, we defined an ad-hoc class of structured features (Moschitti et al., 2006), the Reduced Tree (RT), which can be derived from a sentence parse tree t by the following steps: (1) remove all the terminal nodes but those labeled as relation entities and those POS tagged as verbs, auxiliaries, prepositions, modals or adverbs; (2) remove all the internal nodes not covering any remaining terminal; (3) replace the entity words with placeholders that indicate the direction in which the relation should hold. Figure 1 shows a parse tree and the resulting RT structure. 2.2 Semantic Kernels In (Giuliano et al., 2006), we used the local context kernel to infer semantic informatio</context>
</contexts>
<marker>Moschitti, Pighin, Basili, 2006</marker>
<rawString>Alessandro Moschitti, Daniele Pighin, and Roberto Basili. 2006. Semantic role labeling via tree kernel joint inference. In Proceedings of the Tenth Conference on Computational Natural Language Learning, CoNLL-X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow statistic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume,</booktitle>
<pages>335--342</pages>
<location>Barcelona, Spain,</location>
<contexts>
<context position="2407" citStr="Moschitti, 2004" startWordPosition="361" endWordPosition="362">s, we obtained state-of-the-art results on two biomedical data sets. In addition, promising results have been recently obtained for relations such as work for and org based in in the news domain1. In this paper, we investigate the use of the above approach to discover semantic relations between nominals. In addition to the original feature representation, we have integrated deep syntactic processing of the global context and semantic information for each candidate nominals using WordNet as external knowledge source. Each source of information is represented by kernel functions. A tree kernel (Moschitti, 2004) is used to exploit the deep syntactic processing obtained using the Charniak parser (Charniak, 2000). On the other hand, bag of synonyms and hypernyms is used to enhance the representation of the candidate nominals. The final system is based on five basic kernel functions (bag-ofwords kernel, global context kernel, tree kernel, supersense kernel, bag of synonyms and hypernyms kernel) linearly combined and weighted under different conditions. The experiments were carried out using support vector machines (Vapnik, 1998) as classifier. We present results on the Classification of Semantic Relatio</context>
<context position="6912" citStr="Moschitti, 2004" startWordPosition="1089" endWordPosition="1091">re n-gram kernels that operate on the Fore-Between, Between and Between-After patterns respectively. 2.1.2 Bag-of-Words Kernel The bag-of-words kernel is defined as the previous kernel but it operates on the whole sentence. 2.1.3 Tree Kernel Tree kernels can trigger automatic feature selection and represent a viable alternative to the man2In the literature, it is also called n-spectrum kernel. a) S1 S NP PRP I VP . VBD found NP PP IN in DT NN some candy b) S . NP NN underwear PRP$ my VP VBD found PP NP NN target NP NNS agent IN in n wi i=1 142 ual design of attribute-value syntactic features (Moschitti, 2004). A tree kernel KT(t1, t2) evaluates the similarity between two trees t1 and t2 in terms of the number of fragments they have in common. Let Nt be the set of nodes of a tree t and F = {f1, f2, ... , f|F|} be the fragment space of t1 and t2. Then KT(t1,t2) = � � nj�Nt2 O(n�, nj) , (5) n��Nt1 where A(ni, nj) = �|F| k�1 Ik(ni) × IK(nj) and Ik(n) = 1 if k is rooted in n, 0 otherwise. For this task, we defined an ad-hoc class of structured features (Moschitti et al., 2006), the Reduced Tree (RT), which can be derived from a sentence parse tree t by the following steps: (1) remove all the terminal n</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernels for shallow statistic parsing. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL’04), Main Volume, pages 335– 342, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>John Wiley and Sons,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="2931" citStr="Vapnik, 1998" startWordPosition="445" endWordPosition="446">source of information is represented by kernel functions. A tree kernel (Moschitti, 2004) is used to exploit the deep syntactic processing obtained using the Charniak parser (Charniak, 2000). On the other hand, bag of synonyms and hypernyms is used to enhance the representation of the candidate nominals. The final system is based on five basic kernel functions (bag-ofwords kernel, global context kernel, tree kernel, supersense kernel, bag of synonyms and hypernyms kernel) linearly combined and weighted under different conditions. The experiments were carried out using support vector machines (Vapnik, 1998) as classifier. We present results on the Classification of Semantic Relations between Nominals task at SemEval2007, in which sentences containing ordered pairs of marked nominals, possibly semantically related, have to be classified. On this task, we achieve an overall F1 of 71.8% (B category evaluation), largely outperforming all the baselines. 1These results appear in a paper currently under revision. 141 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 141–144, Prague, June 2007. c�2007 Association for Computational Linguistics 2 Kernel Methods fo</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir Vapnik. 1998. Statistical Learning Theory. John Wiley and Sons, New York, NY.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>