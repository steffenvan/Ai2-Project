<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.993423">
MAYA: A Fast Question-answering System Based On A Predictive
Answer Indexer*
</title>
<author confidence="0.998799">
Harksoo Kim, Kyungsun Kim
</author>
<affiliation confidence="0.9899115">
Dept. of Computer Science,
Sogang University
</affiliation>
<address confidence="0.977540666666667">
1 Sinsu-Dong, Mapo-Gu, Seoul,
121-742, Korea
{ hskim, kksun }
</address>
<email confidence="0.997151">
@nlpzodiac.sogang.ac.kr
</email>
<author confidence="0.98098">
Gary Geunbae Lee
</author>
<affiliation confidence="0.8896915">
Dept. of Computer Science
and Engineering,
Pohang University of
Science and Technology
</affiliation>
<address confidence="0.986461">
San 31, Hyoja-Dong,
Pohang, 790-784, Korea
</address>
<email confidence="0.997592">
gblee@postech.ac.kr
</email>
<author confidence="0.994365">
Jungyun Seo
</author>
<affiliation confidence="0.990417">
Dept. of Computer Science,
Sogang University
</affiliation>
<address confidence="0.947462">
1 Sinsu-Dong, Mapo-Gu,
Seoul, 121-742, Korea
</address>
<email confidence="0.991234">
seojy@ccs.sogang.ac.kr
</email>
<author confidence="0.357401">
(Currently Visiting CSLI Stanford University)
</author>
<sectionHeader confidence="0.990309" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965545454546">
We propose a Question-answering
(QA) system in Korean that uses a
predictive answer indexer. The
predictive answer indexer, first,
extracts all answer candidates in a
document in indexing time. Then, it
gives scores to the adjacent content
words that are closely related with each
answer candidate. Next, it stores the
weighted content words with each
candidate into a database. Using this
technique, along with a complementary
analysis of questions, the proposed QA
system can save response time because
it is not necessary for the QA system to
extract answer candidates with scores
on retrieval time. If the QA system is
combined with a traditional
Information Retrieval system, it can
improve the document retrieval
precision for closed-class questions
after minimum loss of retrieval time.
</bodyText>
<sectionHeader confidence="0.9996" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9860555">
Information Retrieval (IR) systems have been
applied successfully to a large scale of search
area in which indexing and searching speed is
important. Unfortunately, they return a large
</bodyText>
<affiliation confidence="0.68955025">
∗ This research was partly supported by BK21 program of
Ministry of Education and Technology Excellency
Program of Ministry of Information and
Telecommunications.
</affiliation>
<bodyText confidence="0.999834127906977">
amount of documents that include indexing
terms in a user’s query. Hence, the user should
carefully look over the whole text in order to
find a short phrase that precisely answers his/her
question.
Question-answering (QA), an area of IR, is
attracting more attention, as shown in the
proceedings of AAAI (AAAI, 1999) and TREC
(TREC, http://trec.nist.gov/overview.html). A
QA system searches a large collection of texts,
and filters out inadequate phrases or sentences
within the texts. By using the QA system, a user
can promptly approach to his/her answer phrases
without troublesome tasks. However, most of
the current QA systems (Ferret et al., 1999; Hull,
1999; Srihari and Li, 1999; Prager et al., 2000)
have two problems as follows:
It cannot correctly respond to all of the users’
questions. It can answer the questions that are
included in the pre-defined categories such as
person, date, time, and etc.
It requires more indexing or searching time than
traditional IR systems do because it needs a
deep linguistic knowledge such as syntactic or
semantic roles of words.
To solve the problems, we propose a QA
system using a predictive answer indexer -
MAYA (MAke Your Answer). We can easily
add new categories to MAYA by only
supplementing domain dictionaries and rules.
We do not have to revise the searching engine of
MAYA because the indexer is designed as a
separate component that extracts candidate
answers. In addition, a user can promptly obtain
answer phrases on retrieval time because
MAYA indexes answer candidates in advance.
Most of the previous approaches in IR have
been focused on the method to efficiently
represent terms in a document because they
want to index and search a large amount of data
in a short time (Salton et al., 1983; Salton and
McGill, 1983; Salton 1989). These approaches
have been applied successfully to the
commercial search engines (e.g.
http://www.altavista.com) in World Wide Web
(WWW). However, in a real sense of
information retrieval rather than document
retrieval, a user still needs to find an answer
phrase within the vast amount of the retrieved
documents although he/she can promptly find
the relevant documents by using these engines.
Recently, several QA systems are proposed to
avoid the unnecessary answer finding efforts
(Ferret et al., 1999; Hull, 1999; Moldovan et al.
1999; Prager et al., 1999; Srihari and Li, 1999).
Recent researches have combined the
strengths between a traditional IR system and a
QA system (Prager et al., 2000; Prager et al.,
1999; Srihari and Li, 1999). Most of the
combined systems access a huge amount of
electronic information by using IR techniques,
and they improve precision rates by using QA
techniques. In detail, they retrieve a large
amount of documents that are relevant to a
user’s query by using a well-known TF IDF.
Then, they extract answer candidates within the
documents, and filter out the candidates by
using an expected answer type and some rules
on the retrieval time. Although they have been
based on shallow NLP techniques (Sparck-Jones,
1999), they consume much longer retrieval time
than traditional IR systems do because of the
addictive efforts mentioned above. To save
retrieval time, MAYA extracts answer
candidates, and computes the scores of the
candidates on indexing time. On retrieval time,
it just calculates the similarities between a user’s
query and the candidates. As a result, it can
minimize the retrieval time.
This paper is organized as follows. In Section
2, we review the previous works of the QA
systems. In Section 3, we describe the applied
NLP techniques, and present our system. In
Section 4, we analyze the result of our
experiments. Finally, we draw conclusions in
Section 5.
</bodyText>
<sectionHeader confidence="0.986991" genericHeader="method">
2 Previous Works
</sectionHeader>
<bodyText confidence="0.999947051724138">
The current QA approaches can be classified
into two groups; text-snippet extraction systems
and noun-phrase extraction systems (also called
closed-class QA) (Vicedo and Ferrándex, 2000).
The text-snippet extraction approaches are
based on locating and extracting the most
relevant sentences or paragraphs to the query by
assuming that this text will probably contain the
correct answer to the query. These approaches
have been the most commonly used by
participants in last TREC QA Track (Ferret et al.,
1999; Hull, 1999; Moldovan et al., 1999; Prager
et al., 1999; Srihari and Li, 1999). ExtrAns
(Berri et al., 1998) is a representative QA
system in the text-snippet extraction approaches.
The system locates the phrases in a document
from which a user can infer an answer. However,
it is difficult for the system to be converted into
other domains because the system uses syntactic
and semantic information that only covers a very
limited domain (Vicedo and Ferrándex, 2000).
The noun-phrase extraction approaches are
based on finding concrete information, mainly
noun phrases, requested by users’ closed-class
questions. A closed-class question is a question
stated in natural language, which assumes some
definite answer typified by a noun phrase rather
than a procedural answer. MURAX (Kupiec,
1993) is one of the noun-phrase extraction
systems. MURAX uses modules for the shallow
linguistic analysis: a Part-Of-Speech (POS)
tagger and finite-state recognizer for matching
lexico-syntactic pattern. The finite-state
recognizer decides users’ expectations and
filters out various answer hypotheses. For
example, the answers to questions beginning
with the word Who are likely to be people’s
name. Some QA systems participating in Text
REtrieval Conference (TREC) use a shallow
linguistic knowledge and start from similar
approaches as used in MURAX (Hull, 1999;
Vicedo and Ferrándex, 2000). These QA
systems use specialized shallow parsers to
identify the asking point (who, what, when,
where, etc). However, these QA systems take a
long response time because they apply some
rules to each sentence including answer
candidates and give each answer a score on
retrieval time.
MAYA uses shallow linguistic information
such as a POS tagger, a lexico-syntactic parser
similar to finite-state recognizer in MURAX and
a Named Entity (NE) recognizer based on
dictionaries. However, MAYA returns answer
phrases in very short time compared with those
previous systems because the system extracts
answer candidates and gives each answer a score
using pre-defined rules on indexing time.
</bodyText>
<sectionHeader confidence="0.993477" genericHeader="method">
3 MAYA Q/A approach
</sectionHeader>
<bodyText confidence="0.999974962962963">
MAYA has been designed as a separate
component that interfaces with a traditional IR
system. In other words, it can be run without IR
system. It consists of two engines; an indexing
engine and a searching engine.
The indexing engine first extracts all answer
candidates from collected documents. For
answer extraction, it uses the NE recognizer
based on dictionaries and the finite-state
automata. Then, it gives scores to the terms that
surround each candidate. Next, it stores each
candidate and the surrounding terms with scores
in Index DataBase (DB). For example, if n
surrounding terms affects a candidate, n pairs of
the candidate and terms are stored into DB with
n scores. As shown in Figure 1, the indexing
engine keeps separate index DBs that are
classified into pre-defined semantic categories
(i.e. users’ asking points or question types).
The searching engine identifies a user’s
asking point, and selects an index DB that
includes answer candidates of his/her query.
Then, it calculates similarities between terms of
his/her query and the terms surrounding the
candidates. The similarities are based on p-
Norm model (Salton et al., 1983). Next, it ranks
the candidates according to the similarities.
</bodyText>
<figureCaption confidence="0.998338">
Figure 1. A basic architecture of the QA engines
</figureCaption>
<bodyText confidence="0.997965166666667">
Figure 2 shows a total architecture of MAYA
that combines with a traditional IR system. As
shown in Figure 2, the total system has two
index DBs. One is for the IR system that
retrieves relevant documents, and the other is for
MAYA that extracts relevant answer phrases.
</bodyText>
<figureCaption confidence="0.9552065">
Figure 2. A total architecture of the combined
MAYA system
</figureCaption>
<subsectionHeader confidence="0.999162">
3.1 Predictive Answer indexing
</subsectionHeader>
<bodyText confidence="0.974504607142857">
The answer indexing phase can be separated in 2
stages; Answer-finding and Term-scoring. For
answer-finding, we classify users’ asking points
into 14 semantic categories; person, country,
address, organization, telephone number, email
address, homepage Uniform Resource Locator
(URL), the number of people, physical number,
the number of abstract things, rate, price, date,
and time. We think that the 14 semantic
categories are frequently questioned in general
IR systems. To extract answer candidates
belonging to each category from documents, the
indexing engine uses a POS tagger and a NE
recognizer. The NE recognizer makes use of two
dictionaries and a pattern matcher. One of the
dictionaries, which is called PLO dictionary
(487,782 entries), contains the names of people,
countries, cities, and organizations. The other
dictionary, called unit dictionary (430 entries),
contains the units of length (e.g. cm, m, km), the
units of weight (e.g. mg, g, kg), and others. After
looking up the dictionaries, the NE recognizer
assigns a semantic category to each answer
candidate after disambiguation using POS
tagging. For example, the NE recognizer
extracts 4 answer candidates annotated with 4
semantic categories in the sentence, “
( www.yahoo.co.kr)
</bodyText>
<page confidence="0.672047">
6 ❇. (Yahoo Korea
</page>
<subsubsectionHeader confidence="0.284337">
(CEO Jinsup Yeom www.yahoo.co.kr) expanded
</subsubsectionHeader>
<bodyText confidence="0.960428714285714">
...
the size of the storage for free email service to 6
mega-bytes.)”. (Yahoo Korea)
belongs to organization, and (Jinsup
Yeom) is person. www.yahoo.co.kr means
homepage URL, and 6 ▲(6 mega-bytes) is
physical number. Complex lexical candidates
such as www.yahoo.co.kr are extracted by the
pattern matcher. The pattern matcher extracts
formed answers such as telephone number,
email address, and homepage URL. The patterns
are described as regular expressions. For
example, Homepage URL satisfies the following
regular expressions:
</bodyText>
<equation confidence="0.998890285714286">
^(http://)[_A-Za-z0-9 -]+( .[_A-Za-z0-9 -
]+)+(/[_~A-Za-z0-9 - .]+)*$
^[0-9]{3}( .[0-9]{3})( .[0-9]{2,}){2}(/[_~A-
Za-z0-9 - .]{2,})*$
^[0-9]*[_A-Za-z -]{1,}[_A-Za-z0-9 -
]+( .[_A-Za-7z0-9 -]{2,}){2,}(/[_~A-Za-z0-
9 - .]{2,})*$
</equation>
<bodyText confidence="0.9990935">
In the next stage, the indexing engine gives
scores to content words within a context window
that occur with answer candidates. The
maximum size of the context window is 3
sentences; a previous sentence, a current
sentence, and a next sentence. The window size
can be dynamically changed. When the indexing
engine decides the window size, it checks
whether neighboring sentences have anaphora or
lexical chains.
</bodyText>
<figureCaption confidence="0.870576">
Figure 3. An example with the adjusted window
</figureCaption>
<bodyText confidence="0.983845379310345">
size
If the next sentence has anaphors or lexical
chains of the current sentence and the current
sentence does not have anaphors or lexical
chains of the previous sentence, the indexing
engine sets the window size as 2. Unless
neighboring sentences have anaphors or lexical
chains, the window size is 1. Figure 3 shows an
example in which the window size is adjusted.
The scores of the content words indicate the
magnitude of influences that each content word
causes to answer candidates. For example, when
www.yahoo.co.kr is an answer candidate in the
sentence, “ (www.yahoo.co.kr)
. (Yahoo Korea
(www.yahoo.co.kr) starts a new service.)”,
(Yahoo Korea) has the higher score than
(service) because it has much more
strong clue to www.yahoo.co.kr. We call the
score a term score. The indexing engine assigns
term scores to content words according to 5
scoring features described below.
POS: the part-of-speech of a content word. The
indexing engine gives 2 points to each content
word annotated with a proper noun tag and
gives 1 point to each content word annotated
with other tags such as noun, number, and etc.
For example, (Yahoo Korea)
obtains 2 points, and (service) obtains 1
</bodyText>
<figure confidence="0.585428666666667">
point in “ (www.yahoo.co.kr)
. (Yahoo Korea
(www.yahoo.co.kr) starts a new service.)”.
</figure>
<figureCaption confidence="0.535172857142857">
Grammatical Role: the grammatical relations of
the subcategorized functions of the main verb in
a sentence. The indexing engine gives 4 points
to a topic word, 3 points to a subject, 2 points to
an object and 1 point to the rests. The
grammatical roles can be decided by case
markers like � / (un/nun),and
</figureCaption>
<bodyText confidence="0.86776975">
�/�(i/ga) ✉/
(ul/lul) since Korean is a language with well-
developed morphemic markers. For example,
(Yahoo Korea) obtains 3 points
because it is a subject, and (service)
obtains 2 point because it is an object in the
above sample sentence.
Lexical Chain: the re-occurring words in
adjacent sentences. The indexing engine gives 2
points to each word that forms lexical chains
and gives 1 point to others. For example, if the
next sentence of the above sample sentence is
</bodyText>
<equation confidence="0.1948915">
“ 6
. (The members
</equation>
<bodyText confidence="0.739144457142857">
of the service can use the free storages of 6
mega-bytes for email.)”, (service)
obtains 2 points.
Distance: the distance between a sentence
including a target content word and a sentence
including an answer candidate. The indexing
engine gives 2 points to each content word in
the sentence including the answer candidate.
The engine 1 point
➙ gives others. For example,
to
(Yahoo Korea) and (service)
in the above sample sentence obtain 2 points
respectively because the content words are in
the sentence including the answer candidate,
www.yahoo.co.kr.
Apposition: the IS-A relation between a content
word and an answer candidate. The indexing
engine extracts appositive terms by using
syntactic information such as Explicit IS-A
relation, Pre-modification and Post-modification.
For example, ➙(Yahoo Korea) is Pre-
modification relation with www.yahoo.co.kr in
the above sample sentence. The indexing engine
gives 2 points to each appositive word and gives
1 point to others.
The indexing engine adds up the scores of the 5
features, as shown in Equation 1.
tsi is the term score of the ith term, and fij is the
score of the jth feature in the ith term. A, B, C, D
and E are weighting factors that rank 5 features
according to preference. The indexing engine
uses the following ranking order: E &gt; C &gt; B &gt; A
&gt; D. The weighted term scores are normalized,
as shown in Equation 2.
</bodyText>
<equation confidence="0.9887336">
⎛0.5+0.5 tsij )Iog(N/n) ts.. &gt;0 (2)
Max _tsj log(N) 9
0 0
ts =
ij
</equation>
<bodyText confidence="0.998524444444445">
Equation 2 is similar to TF.IDF equation (Fox,
1983). In Equation 2, tsij is the term score of the
ith term in the context window that is relevant to
the jth answer candidate. Max_tsj is the
maximum value among term scores in the
context window that is relevant to the jth answer
candidate. n is the number of answer candidates
that are affected by the ith term. N is the number
of answer candidates of the same semantic
category. The indexing engine saves the
normalized term scores with the position
information of the relevant answer candidate in
the DB. The position information includes a
document number and the distance between the
beginning of the document and the answer
candidate. As a result, the indexing engine
creates 14 DB’s that correspond to the 14
semantic categories. We call them answer DB’s.
</bodyText>
<subsectionHeader confidence="0.999639">
3.2 Lexico-syntactic Query processing
</subsectionHeader>
<bodyText confidence="0.996659304347826">
In the query processing stage, the searching
engine takes a user’s question and converts it
into a suitable form, using a semantic dictionary,
called a query dictionary. The query dictionary
contains the semantic markers of words. Query
words are converted into semantic markers
before pattern matching. For example, the query
“ ? (Who is
the CEO of Yahoo Korea?)” is translated into
jp ef sf (%who
auxiliary-verb %person preposition Yahoo
Korea symbol)”. In the example, %
(%person) and % ➚(%who) are the semantic
markers. The content words out of the query
dictionary keep their lexical forms. The
functional words (e.g. auxiliary verb,
preposition) are converted into POS’s. After
conversion, the searching engine matches the
converted query against one of 88 lexico-
syntactic patterns, and classifies the query into
the one of 14 semantic categories. When two or
more patterns match the query, the searching
engine returns the first matched category.
</bodyText>
<equation confidence="0.99586775">
% (xsn)* (j)?% .* $
(%person (xsn)* (j)? %who .* $)
% (xsn)* (j)? % (j) (% )? .* $
(%person (xsn)* (j)? %name (j) (%what)? .* $)
% (xsn)* (j)? (% )? % .* $
(%person (xsn)* (j)? (%name)? %want_to_know .* $)
% % .* $
(%which %person .* $)
</equation>
<figureCaption confidence="0.66635225">
Figure 4. Lexico-syntactic patterns
Figure 4 shows some lexico-syntactic patterns
for person category. The above sample query
matches the first pattern in Figure 4.
</figureCaption>
<bodyText confidence="0.999865923076923">
After classifying the query into a semantic
category, the searching engine calculates the
term scores of the content words in the query.
As shown in Rule 1, the term scores are
computed by some heuristic rules, and the range
of the term scores is between 0 and 1. Using the
heuristic rules, the searching engine gives high
scores to content words that focus a user’s
intention. For example, when a user inputs the
query “ ? (In
what year is Yahoo founded?)”, he/she wants to
know only the year, rather than the organizer or
the URL of Yahoo. So, the QA searching engine
</bodyText>
<equation confidence="0.8361512">
tsi =
A.fi1+B..f;2+C..f;3+D.fi4+E.L (1)
A+B+C+D+
E
“ j % j %
</equation>
<bodyText confidence="0.688160789473684">
gives a higher score to �(year) than to
(Yahoo) in contrast to the ordinary IR searching
engine.
1. The last content word in a sentence receives a
high score. For example, (CEO) in “
? (The CEO of Yahoo?)” receives a high
score.
2. The next content words of specific interrogatives
such as (which), (what) receive high
scores. For example, (mountain) in “
? (Which mountain is the highest?)”
receives a high score.
3. The next content words of specific prepositions
like (about) receive low scores, and the
previous content words receive high scores. For
example, the score of (article) in “
(the article about China)” is lower
than that of (China).
Rule 1. Heuristic rules for scoring query terms
</bodyText>
<subsectionHeader confidence="0.999393">
3.3 Answer scoring and ranking
</subsectionHeader>
<bodyText confidence="0.999297714285714">
The searching engine calculates the similarities
between query and answer candidates, and ranks
the answer candidates according to the
similarities. To check the similarities, the
searching engine uses the AND operation of a
well-known p-Norm model (Salton et al., 1983),
as shown in Equation 3.
</bodyText>
<equation confidence="0.995043466666667">
p p p
q a
p (1 ) (1 )
− + −
q p a + + −
p (1 )
q a (3)
1 1 2 2 i i
Sim A Q
( , ) 1
= −p
andp p p
q q
+ + + q
1 2 i
</equation>
<bodyText confidence="0.999974485714286">
In Equation 3, A is an answer candidate, and ai
is the ith term score in the context window of
the answer candidate. ai is stored in the answer
DB. qi is the ith term score in the query. p is the
P-value in the p-Norm model.
It takes a relatively short time for answer
scoring and ranking phase because the indexing
engine has already calculated the scores of the
terms that affect answer candidates. In other
words, the searching engine simply adds up the
weights of co-occurring terms, as shown in
Equation 3. Then, the engine ranks answer
candidates according to the similarities. The
method for answer scoring is similar to the
method for document scoring of traditional IR
engines. However, MAYA is different in that it
indexes, retrieves, and ranks answer candidates,
but not documents.
We can easily combine MAYA with a
traditional IR system because MAYA has been
designed by a separate component that
interfaces with the IR system. We implemented
an IR system that is based on TF•IDF weight
and p-Norm model (Lee et al., 1999).
To improve the precision rate of the IR
system, we combine MAYA with the IR system.
The total system merges the outputs of MAYA
with the outputs of the IR system. MAYA can
produce multiple similarity values per document
if two or more answer candidates are within a
document. However, the IR system produces a
similarity value per document. Therefore, the
total system adds up the similarity value of the
IR system and the maximum similarity value of
MAYA, as shown in Equation 4.
</bodyText>
<equation confidence="0.915798">
Sim(D, Q) = a • IRsim(D, Q) + R • QAsimd (A- , Q) (4)
</equation>
<bodyText confidence="0.9999177">
In Equation 4, QAsimd(Ai,Q) is the similarity
value between query Q and the ith answer
candidate Ai in document d. IRsim(D,Q) is the
similarity value between query Q and document
D. and are weighting factors. We set and
to 0.3 and 0.7.
The total system ranks the retrieved
documents by using the combined similarity
values, and shows the sentences including
answer candidates in the documents.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.999874">
4.1 The experiment data
</subsectionHeader>
<bodyText confidence="0.999916375">
In order to experiment on MAYA, we collected
14,321 documents (65,752 kilobytes) from two
web sites: korea.internet.com (6,452 documents)
and www.sogang.ac.kr (7,869 documents). The
former gives the members on-line articles on
Information Technology (IT). The latter is a
homepage of Sogang University. The indexing
engine created the 14 answer DBs (14 semantic
categories).
For the test data, we collected 50 pairs of
question-answers from 10 graduate students.
Table 1 shows the 14 semantic categories and
the numbers of the collected question-answers in
each category. As shown in Table 1, we found 2
question-answers out of the 14 semantic
categories. They are not closed-class question-
</bodyText>
<equation confidence="0.736422">
a+
R
</equation>
<bodyText confidence="0.6461268">
answers but explanation-seeking question-
answers like “Question: How can I search on-
line Loyola library for any books? Answer:
Connect your computer to http://loyola
1.sogang.ac.kr”.
</bodyText>
<table confidence="0.977632125">
Category person country address organization
# of QAs 9 3 3 9
Category telephone email URL people num.
# of QAs 3 5 4 0
Category phy. num. abs. num. rate price
# of QAs 1 1 0 4
Category date time out of cat. total
# of QAs 5 1 2 50
</table>
<tableCaption confidence="0.972293">
Table 1. The number of the collected question-
answers in each category
</tableCaption>
<bodyText confidence="0.999938875">
We use two sorts of evaluation schemes. To
experiment on MAYA, we compute the
performance score as the Reciprocal Answer
Rank (RAR) of the first correct answer given by
each question. To compute the overall
performance, we use the Mean Reciprocal
Answer Rank (MRAR), as shown in Equation 5
(Voorhees and Tice, 1999).
</bodyText>
<equation confidence="0.9251435">
� �MRAR=1/n ranki) (5)
1i
</equation>
<bodyText confidence="0.999762571428572">
With respect to the total system that combines
MAYA with the IR system, we use the
Reciprocal Document Rank (RDR) and the
Mean Reciprocal Document Rank (MRDR).
RDR means the reciprocal rank of the first
document including the correct answers given
by each question.
</bodyText>
<subsectionHeader confidence="0.999017">
4.2 Analysis of experiment results
</subsectionHeader>
<bodyText confidence="0.999604666666667">
The performance of MAYA is shown in Table 2.
We obtained the correct answers for 33
questions out of 50 in Top 1.
</bodyText>
<table confidence="0.9704396">
Rank Top 1 Top 2 Top 3 Top 4
# of answers 33 4 3 2
Rank Top 5 Top 6— Failure Total(MRAR)
# of answers 1 2 5 50
(0.80)
</table>
<tableCaption confidence="0.999813">
Table 2. The performance of the QA system
</tableCaption>
<bodyText confidence="0.999133263157895">
Table 3 shows the performance of the total
system. As shown in Table 3, the total system
significantly improves the document retrieval
performance of underlying IR system about the
closed-class questions.
The average retrieval time of the IR system
is 0.022 second per query. The total system is
0.029 second per query. The difference of the
retrieval times between the IR system and the
total system is not so big, which means that the
retrieval speed of QA-only-system is fast
enough to be negligible. The IR system shows
some sentences including query terms to a user.
However, the total system shows the sentences
including answer candidates to a user. This
function helps the user get out of the trouble that
the user might experience when he/she looks
through the whole document in order to find the
answer phrase.
</bodyText>
<table confidence="0.994707">
Rank Top 1 Top 2 Top 3 Top 4
# of answers 1 22 8 5 2
# of answers 2 36 5 2 1
Rank Top 5 Top 6— Failure Total
(MRDR)
# of answers 1 3 10 0 50
(0.54)
# of answers 2 2 4 0 50
(0.76)
</table>
<tableCaption confidence="0.5602916">
# of answers 1: the number of answers which are ranked at
top n by using the IR system
# of answers 2: the number of answers which are ranked at
top n by using the total system
Table 3. The performance of the total system
</tableCaption>
<bodyText confidence="0.9995781875">
MAYA could not extract the correct answers
to certain questions in this experiment. The
failure cases are the following, and all of them
can be easily solved by extending the resources
and pattern rules:
The lexico-syntactic parser failed to classify
users’ queries into the predefined semantic
categories. We think that most of these failure
queries can be dealt with by supplementing
additional lexico-syntactic grammars.
The NE recognizer failed to extract answer
candidates. To resolve this problem, we should
supplement the entries in PLO dictionary, the
entries in the unit dictionary, and regular
expressions. We also should endeavor to
improve the precision of the NE recognizer.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999996814814815">
We presented a fast and high-precision Korean
QA system using a predictive answer indexer.
The predictive answer indexer extracts answer
candidates and terms surrounding the candidates
in indexing time. Then, it stores each candidate
with the surrounding terms that have specific
scores in answer DB’s. On the retrieval time, the
QA system just calculates the similarities
between a user’s query and the answer
candidates. Therefore, it can minimize the
retrieval time and enhance the precision. Our
system can easily converted into other domains
because it is based on shallow NLP and IR
techniques such as POS tagging, NE recognizing,
pattern matching and term weighting with
TF⋅IDF. The experimental results show that the
QA system can improve the document retrieval
precision for closed-class questions after the
insignificant loss of retrieval time if it is
combined with a traditional IR system. In the
future, we pursue to concentrate on resolving the
semantic ambiguity when a user’s query
matches two or more lexico-syntactic patterns.
Also, we are working on an automatic and
dynamic way of extending the semantic
categories into which the users’ queries can be
more flexibly categorized.
</bodyText>
<sectionHeader confidence="0.999253" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999631625">
AAAI Fall Symposium on Question Answering.
1999.
Berri, J., Molla, D., and Hess, M. 1998. Extraction
automatique de réponses: implémentations du
systéme ExtrAns. In Proceedings of the fifth
conference TALN 1998, pp. 10-12.
Ferret, O., Grau, B., Illouz, G., and Jacquemin C.
1999. QALC – the Question- Answering program
of the Language and Cognition group at LIMSI-
CNRS. In Proceedings of The Eighth Text
REtrieval Conference(TREC-8), http://trec.nist.
gov/pubs/trec8/t8_proceedings.html.
Fox, E.A. 1983. Extending the Boolean and Vector
Space Models of Information Retrieval with P-
norm Queries and Multiple Concept Types, Ph.D.
Thesis, CS, Cornell University.
Hull, D.A. 1999. Xerox TREC-8 Question
Answering Track Report. In Proceedings of The
Eighth Text REtrieval Conference(TREC-8),
http://trec.nist.gov/pubs/trec8/t8_proceedings.html.
Kupiec, J. 1993. Murax: A Robust Linguistic
Approach for Question Answering Using an On-
line Encyclopedia. In Proceedings of SIGIR’93.
Lee, G., Park, M., and Won, H. 1999. Using syntactic
information in handling natural language queries
for extended boolean retrieval model. In
Proceedings of the 4th international workshop on
information retrieval with Asian languages
(IRAL99), pp. 63-70.
Moldovan, D., Harabagiu, S., Pasca, M., Mihalcea,
R., Goodrum, R., Gîrju, R., and Rus, V. 1999.
LASSO: A Tool for Surfing the Answer Net. In
Proceedings of The Eighth Text REtrieval
Conference (TREC-8), http://trec.ni st.gov/pubs
/trec8/t8_proceedings.html.
Prager, J., Brown, E., Coden A., and Radev D. 2000.
Question-Answering by Predictive Annotation. In
Proceedings of SIGIR 2000, pp. 184-191.
Prager, J., Radev, D., Brown, E., and Coden, A. 1999.
The Use of Predictive Annotation for Question
Answering in TREC8. In Proceedings of The
Eighth Text REtrieval Conference (TREC-8),
http://trec.nist.gov/pubs/trec8/t8_proceedings.html.
Salton, G., Fox, E.A., and Wu, H. 1983. Extended
Boolean Information Retrieval, Communication of
the ACM, 26(12):1022-1036.
Salton, G., and McGill, M. 1983. Introduction to
Modern Information Retrieval (Computer Series),
New York:McGraw-Hill.
Salton, G. 1989. Automatic Text Processing: The
Transformation, Analysis and Retrieval of
Information by Computer. Reading, MA:Addison-
Wesley.
TREC (Text REtrieval Conference) Overview,
http://trec.nist.gov/overview.html.
Sparck-Jones, K. 1999. What is the role NLP in Text
Retrieval?. Natural Language Information
Retrieval, Kluwer Academic Publishers.
T.Strzalkowski (ed), pp.1-24.
Srihari, R., and Li, W. 1999. Information Extraction
Supported Question Answering. In Proceedings of
The Eighth Text REtrieval Conference (TREC-8),
http://trec.nist.gov/pubs/trec8/t8_proceedings.html.
Vicedo, J. L., and Ferrándex, A. 2000. Importance of
Pronominal Anaphora resolution in Question
Answering systems. In Proceeding of ACL 2000,
pp. 555-562.
Voorhees, E., and Tice, D. M. 1999. The TREC-8
Question Answering Track Evaluation. In
Proceedings of The Eighth Text REtrieval
Conference (TREC-8), http://trec.nist.gov/pubs
/trec8/t8_proceedings.html.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.025887">
<title confidence="0.998857">MAYA: A Fast Question-answering System Based On A</title>
<author confidence="0.999083">Harksoo Kim</author>
<author confidence="0.999083">Kyungsun</author>
<affiliation confidence="0.647414333333333">Dept. of Computer Sogang 1 Sinsu-Dong, Mapo-Gu,</affiliation>
<address confidence="0.597836">121-742,</address>
<email confidence="0.7016725">{hskim,kksun@nlpzodiac.sogang.ac.kr</email>
<author confidence="0.994817">Gary Geunbae</author>
<affiliation confidence="0.942392">Dept. of Computer and Pohang University Science and San 31,</affiliation>
<address confidence="0.994086">Pohang, 790-784,</address>
<email confidence="0.987636">gblee@postech.ac.kr</email>
<author confidence="0.40346">Jungyun</author>
<affiliation confidence="0.640493666666667">Dept. of Computer Sogang 1 Sinsu-Dong,</affiliation>
<address confidence="0.721606">Seoul, 121-742,</address>
<email confidence="0.821806">seojy@ccs.sogang.ac.kr</email>
<note confidence="0.848503">(Currently Visiting CSLI Stanford University)</note>
<abstract confidence="0.999624956521739">We propose a Question-answering (QA) system in Korean that uses a predictive answer indexer. The predictive answer indexer, first, extracts all answer candidates in a document in indexing time. Then, it gives scores to the adjacent content words that are closely related with each answer candidate. Next, it stores the weighted content words with each candidate into a database. Using this technique, along with a complementary analysis of questions, the proposed QA system can save response time because it is not necessary for the QA system to extract answer candidates with scores on retrieval time. If the QA system is combined with a traditional Information Retrieval system, it can improve the document retrieval precision for closed-class questions after minimum loss of retrieval time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>AAAI Fall Symposium on Question Answering.</title>
<date>1999</date>
<marker>1999</marker>
<rawString>AAAI Fall Symposium on Question Answering. 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Berri</author>
<author>D Molla</author>
<author>M Hess</author>
</authors>
<title>Extraction automatique de réponses: implémentations du systéme ExtrAns.</title>
<date>1998</date>
<booktitle>In Proceedings of the fifth conference TALN</booktitle>
<pages>10--12</pages>
<contexts>
<context position="6091" citStr="Berri et al., 1998" startWordPosition="952" endWordPosition="955"> The current QA approaches can be classified into two groups; text-snippet extraction systems and noun-phrase extraction systems (also called closed-class QA) (Vicedo and Ferrándex, 2000). The text-snippet extraction approaches are based on locating and extracting the most relevant sentences or paragraphs to the query by assuming that this text will probably contain the correct answer to the query. These approaches have been the most commonly used by participants in last TREC QA Track (Ferret et al., 1999; Hull, 1999; Moldovan et al., 1999; Prager et al., 1999; Srihari and Li, 1999). ExtrAns (Berri et al., 1998) is a representative QA system in the text-snippet extraction approaches. The system locates the phrases in a document from which a user can infer an answer. However, it is difficult for the system to be converted into other domains because the system uses syntactic and semantic information that only covers a very limited domain (Vicedo and Ferrándex, 2000). The noun-phrase extraction approaches are based on finding concrete information, mainly noun phrases, requested by users’ closed-class questions. A closed-class question is a question stated in natural language, which assumes some definite</context>
</contexts>
<marker>Berri, Molla, Hess, 1998</marker>
<rawString>Berri, J., Molla, D., and Hess, M. 1998. Extraction automatique de réponses: implémentations du systéme ExtrAns. In Proceedings of the fifth conference TALN 1998, pp. 10-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Ferret</author>
<author>B Grau</author>
<author>G Illouz</author>
<author>C Jacquemin</author>
</authors>
<title>QALC – the Question- Answering program of the Language and Cognition group at LIMSICNRS.</title>
<date>1999</date>
<booktitle>In Proceedings of The Eighth Text REtrieval Conference(TREC-8), http://trec.nist. gov/pubs/trec8/t8_proceedings.html.</booktitle>
<contexts>
<context position="2390" citStr="Ferret et al., 1999" startWordPosition="350" endWordPosition="353">indexing terms in a user’s query. Hence, the user should carefully look over the whole text in order to find a short phrase that precisely answers his/her question. Question-answering (QA), an area of IR, is attracting more attention, as shown in the proceedings of AAAI (AAAI, 1999) and TREC (TREC, http://trec.nist.gov/overview.html). A QA system searches a large collection of texts, and filters out inadequate phrases or sentences within the texts. By using the QA system, a user can promptly approach to his/her answer phrases without troublesome tasks. However, most of the current QA systems (Ferret et al., 1999; Hull, 1999; Srihari and Li, 1999; Prager et al., 2000) have two problems as follows: It cannot correctly respond to all of the users’ questions. It can answer the questions that are included in the pre-defined categories such as person, date, time, and etc. It requires more indexing or searching time than traditional IR systems do because it needs a deep linguistic knowledge such as syntactic or semantic roles of words. To solve the problems, we propose a QA system using a predictive answer indexer - MAYA (MAke Your Answer). We can easily add new categories to MAYA by only supplementing doma</context>
<context position="4042" citStr="Ferret et al., 1999" startWordPosition="620" endWordPosition="623">ex and search a large amount of data in a short time (Salton et al., 1983; Salton and McGill, 1983; Salton 1989). These approaches have been applied successfully to the commercial search engines (e.g. http://www.altavista.com) in World Wide Web (WWW). However, in a real sense of information retrieval rather than document retrieval, a user still needs to find an answer phrase within the vast amount of the retrieved documents although he/she can promptly find the relevant documents by using these engines. Recently, several QA systems are proposed to avoid the unnecessary answer finding efforts (Ferret et al., 1999; Hull, 1999; Moldovan et al. 1999; Prager et al., 1999; Srihari and Li, 1999). Recent researches have combined the strengths between a traditional IR system and a QA system (Prager et al., 2000; Prager et al., 1999; Srihari and Li, 1999). Most of the combined systems access a huge amount of electronic information by using IR techniques, and they improve precision rates by using QA techniques. In detail, they retrieve a large amount of documents that are relevant to a user’s query by using a well-known TF IDF. Then, they extract answer candidates within the documents, and filter out the candid</context>
<context position="5982" citStr="Ferret et al., 1999" startWordPosition="933" endWordPosition="936">tion 4, we analyze the result of our experiments. Finally, we draw conclusions in Section 5. 2 Previous Works The current QA approaches can be classified into two groups; text-snippet extraction systems and noun-phrase extraction systems (also called closed-class QA) (Vicedo and Ferrándex, 2000). The text-snippet extraction approaches are based on locating and extracting the most relevant sentences or paragraphs to the query by assuming that this text will probably contain the correct answer to the query. These approaches have been the most commonly used by participants in last TREC QA Track (Ferret et al., 1999; Hull, 1999; Moldovan et al., 1999; Prager et al., 1999; Srihari and Li, 1999). ExtrAns (Berri et al., 1998) is a representative QA system in the text-snippet extraction approaches. The system locates the phrases in a document from which a user can infer an answer. However, it is difficult for the system to be converted into other domains because the system uses syntactic and semantic information that only covers a very limited domain (Vicedo and Ferrándex, 2000). The noun-phrase extraction approaches are based on finding concrete information, mainly noun phrases, requested by users’ closed-c</context>
</contexts>
<marker>Ferret, Grau, Illouz, Jacquemin, 1999</marker>
<rawString>Ferret, O., Grau, B., Illouz, G., and Jacquemin C. 1999. QALC – the Question- Answering program of the Language and Cognition group at LIMSICNRS. In Proceedings of The Eighth Text REtrieval Conference(TREC-8), http://trec.nist. gov/pubs/trec8/t8_proceedings.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E A Fox</author>
</authors>
<title>Extending the Boolean and Vector Space Models of Information Retrieval with Pnorm Queries and Multiple Concept Types,</title>
<date>1983</date>
<tech>Ph.D. Thesis,</tech>
<institution>CS, Cornell University.</institution>
<contexts>
<context position="15893" citStr="Fox, 1983" startWordPosition="2488" endWordPosition="2489">e. The indexing engine gives 2 points to each appositive word and gives 1 point to others. The indexing engine adds up the scores of the 5 features, as shown in Equation 1. tsi is the term score of the ith term, and fij is the score of the jth feature in the ith term. A, B, C, D and E are weighting factors that rank 5 features according to preference. The indexing engine uses the following ranking order: E &gt; C &gt; B &gt; A &gt; D. The weighted term scores are normalized, as shown in Equation 2. ⎛0.5+0.5 tsij )Iog(N/n) ts.. &gt;0 (2) Max _tsj log(N) 9 0 0 ts = ij Equation 2 is similar to TF.IDF equation (Fox, 1983). In Equation 2, tsij is the term score of the ith term in the context window that is relevant to the jth answer candidate. Max_tsj is the maximum value among term scores in the context window that is relevant to the jth answer candidate. n is the number of answer candidates that are affected by the ith term. N is the number of answer candidates of the same semantic category. The indexing engine saves the normalized term scores with the position information of the relevant answer candidate in the DB. The position information includes a document number and the distance between the beginning of </context>
</contexts>
<marker>Fox, 1983</marker>
<rawString>Fox, E.A. 1983. Extending the Boolean and Vector Space Models of Information Retrieval with Pnorm Queries and Multiple Concept Types, Ph.D. Thesis, CS, Cornell University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Hull</author>
</authors>
<title>Xerox TREC-8 Question Answering Track Report.</title>
<date>1999</date>
<booktitle>In Proceedings of The Eighth Text REtrieval Conference(TREC-8),</booktitle>
<location>http://trec.nist.gov/pubs/trec8/t8_proceedings.html.</location>
<contexts>
<context position="2402" citStr="Hull, 1999" startWordPosition="354" endWordPosition="355">ser’s query. Hence, the user should carefully look over the whole text in order to find a short phrase that precisely answers his/her question. Question-answering (QA), an area of IR, is attracting more attention, as shown in the proceedings of AAAI (AAAI, 1999) and TREC (TREC, http://trec.nist.gov/overview.html). A QA system searches a large collection of texts, and filters out inadequate phrases or sentences within the texts. By using the QA system, a user can promptly approach to his/her answer phrases without troublesome tasks. However, most of the current QA systems (Ferret et al., 1999; Hull, 1999; Srihari and Li, 1999; Prager et al., 2000) have two problems as follows: It cannot correctly respond to all of the users’ questions. It can answer the questions that are included in the pre-defined categories such as person, date, time, and etc. It requires more indexing or searching time than traditional IR systems do because it needs a deep linguistic knowledge such as syntactic or semantic roles of words. To solve the problems, we propose a QA system using a predictive answer indexer - MAYA (MAke Your Answer). We can easily add new categories to MAYA by only supplementing domain dictionar</context>
<context position="4054" citStr="Hull, 1999" startWordPosition="624" endWordPosition="625"> amount of data in a short time (Salton et al., 1983; Salton and McGill, 1983; Salton 1989). These approaches have been applied successfully to the commercial search engines (e.g. http://www.altavista.com) in World Wide Web (WWW). However, in a real sense of information retrieval rather than document retrieval, a user still needs to find an answer phrase within the vast amount of the retrieved documents although he/she can promptly find the relevant documents by using these engines. Recently, several QA systems are proposed to avoid the unnecessary answer finding efforts (Ferret et al., 1999; Hull, 1999; Moldovan et al. 1999; Prager et al., 1999; Srihari and Li, 1999). Recent researches have combined the strengths between a traditional IR system and a QA system (Prager et al., 2000; Prager et al., 1999; Srihari and Li, 1999). Most of the combined systems access a huge amount of electronic information by using IR techniques, and they improve precision rates by using QA techniques. In detail, they retrieve a large amount of documents that are relevant to a user’s query by using a well-known TF IDF. Then, they extract answer candidates within the documents, and filter out the candidates by usin</context>
<context position="5994" citStr="Hull, 1999" startWordPosition="937" endWordPosition="938">e result of our experiments. Finally, we draw conclusions in Section 5. 2 Previous Works The current QA approaches can be classified into two groups; text-snippet extraction systems and noun-phrase extraction systems (also called closed-class QA) (Vicedo and Ferrándex, 2000). The text-snippet extraction approaches are based on locating and extracting the most relevant sentences or paragraphs to the query by assuming that this text will probably contain the correct answer to the query. These approaches have been the most commonly used by participants in last TREC QA Track (Ferret et al., 1999; Hull, 1999; Moldovan et al., 1999; Prager et al., 1999; Srihari and Li, 1999). ExtrAns (Berri et al., 1998) is a representative QA system in the text-snippet extraction approaches. The system locates the phrases in a document from which a user can infer an answer. However, it is difficult for the system to be converted into other domains because the system uses syntactic and semantic information that only covers a very limited domain (Vicedo and Ferrándex, 2000). The noun-phrase extraction approaches are based on finding concrete information, mainly noun phrases, requested by users’ closed-class questio</context>
<context position="7338" citStr="Hull, 1999" startWordPosition="1138" endWordPosition="1139">her than a procedural answer. MURAX (Kupiec, 1993) is one of the noun-phrase extraction systems. MURAX uses modules for the shallow linguistic analysis: a Part-Of-Speech (POS) tagger and finite-state recognizer for matching lexico-syntactic pattern. The finite-state recognizer decides users’ expectations and filters out various answer hypotheses. For example, the answers to questions beginning with the word Who are likely to be people’s name. Some QA systems participating in Text REtrieval Conference (TREC) use a shallow linguistic knowledge and start from similar approaches as used in MURAX (Hull, 1999; Vicedo and Ferrándex, 2000). These QA systems use specialized shallow parsers to identify the asking point (who, what, when, where, etc). However, these QA systems take a long response time because they apply some rules to each sentence including answer candidates and give each answer a score on retrieval time. MAYA uses shallow linguistic information such as a POS tagger, a lexico-syntactic parser similar to finite-state recognizer in MURAX and a Named Entity (NE) recognizer based on dictionaries. However, MAYA returns answer phrases in very short time compared with those previous systems b</context>
</contexts>
<marker>Hull, 1999</marker>
<rawString>Hull, D.A. 1999. Xerox TREC-8 Question Answering Track Report. In Proceedings of The Eighth Text REtrieval Conference(TREC-8), http://trec.nist.gov/pubs/trec8/t8_proceedings.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>Murax: A Robust Linguistic Approach for Question Answering Using an Online Encyclopedia.</title>
<date>1993</date>
<booktitle>In Proceedings of SIGIR’93.</booktitle>
<contexts>
<context position="6778" citStr="Kupiec, 1993" startWordPosition="1058" endWordPosition="1059">The system locates the phrases in a document from which a user can infer an answer. However, it is difficult for the system to be converted into other domains because the system uses syntactic and semantic information that only covers a very limited domain (Vicedo and Ferrándex, 2000). The noun-phrase extraction approaches are based on finding concrete information, mainly noun phrases, requested by users’ closed-class questions. A closed-class question is a question stated in natural language, which assumes some definite answer typified by a noun phrase rather than a procedural answer. MURAX (Kupiec, 1993) is one of the noun-phrase extraction systems. MURAX uses modules for the shallow linguistic analysis: a Part-Of-Speech (POS) tagger and finite-state recognizer for matching lexico-syntactic pattern. The finite-state recognizer decides users’ expectations and filters out various answer hypotheses. For example, the answers to questions beginning with the word Who are likely to be people’s name. Some QA systems participating in Text REtrieval Conference (TREC) use a shallow linguistic knowledge and start from similar approaches as used in MURAX (Hull, 1999; Vicedo and Ferrándex, 2000). These QA </context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>Kupiec, J. 1993. Murax: A Robust Linguistic Approach for Question Answering Using an Online Encyclopedia. In Proceedings of SIGIR’93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Lee</author>
<author>M Park</author>
<author>H Won</author>
</authors>
<title>Using syntactic information in handling natural language queries for extended boolean retrieval model.</title>
<date>1999</date>
<booktitle>In Proceedings of the 4th international workshop on information retrieval with Asian languages (IRAL99),</booktitle>
<pages>63--70</pages>
<contexts>
<context position="20853" citStr="Lee et al., 1999" startWordPosition="3362" endWordPosition="3365">the searching engine simply adds up the weights of co-occurring terms, as shown in Equation 3. Then, the engine ranks answer candidates according to the similarities. The method for answer scoring is similar to the method for document scoring of traditional IR engines. However, MAYA is different in that it indexes, retrieves, and ranks answer candidates, but not documents. We can easily combine MAYA with a traditional IR system because MAYA has been designed by a separate component that interfaces with the IR system. We implemented an IR system that is based on TF•IDF weight and p-Norm model (Lee et al., 1999). To improve the precision rate of the IR system, we combine MAYA with the IR system. The total system merges the outputs of MAYA with the outputs of the IR system. MAYA can produce multiple similarity values per document if two or more answer candidates are within a document. However, the IR system produces a similarity value per document. Therefore, the total system adds up the similarity value of the IR system and the maximum similarity value of MAYA, as shown in Equation 4. Sim(D, Q) = a • IRsim(D, Q) + R • QAsimd (A- , Q) (4) In Equation 4, QAsimd(Ai,Q) is the similarity value between que</context>
</contexts>
<marker>Lee, Park, Won, 1999</marker>
<rawString>Lee, G., Park, M., and Won, H. 1999. Using syntactic information in handling natural language queries for extended boolean retrieval model. In Proceedings of the 4th international workshop on information retrieval with Asian languages (IRAL99), pp. 63-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Moldovan</author>
<author>S Harabagiu</author>
<author>M Pasca</author>
<author>R Mihalcea</author>
<author>R Goodrum</author>
<author>R Gîrju</author>
<author>V Rus</author>
</authors>
<title>LASSO: A Tool for Surfing the Answer Net.</title>
<date>1999</date>
<booktitle>In Proceedings of The Eighth Text REtrieval Conference (TREC-8), http://trec.ni st.gov/pubs /trec8/t8_proceedings.html.</booktitle>
<contexts>
<context position="4076" citStr="Moldovan et al. 1999" startWordPosition="626" endWordPosition="629">ata in a short time (Salton et al., 1983; Salton and McGill, 1983; Salton 1989). These approaches have been applied successfully to the commercial search engines (e.g. http://www.altavista.com) in World Wide Web (WWW). However, in a real sense of information retrieval rather than document retrieval, a user still needs to find an answer phrase within the vast amount of the retrieved documents although he/she can promptly find the relevant documents by using these engines. Recently, several QA systems are proposed to avoid the unnecessary answer finding efforts (Ferret et al., 1999; Hull, 1999; Moldovan et al. 1999; Prager et al., 1999; Srihari and Li, 1999). Recent researches have combined the strengths between a traditional IR system and a QA system (Prager et al., 2000; Prager et al., 1999; Srihari and Li, 1999). Most of the combined systems access a huge amount of electronic information by using IR techniques, and they improve precision rates by using QA techniques. In detail, they retrieve a large amount of documents that are relevant to a user’s query by using a well-known TF IDF. Then, they extract answer candidates within the documents, and filter out the candidates by using an expected answer t</context>
<context position="6017" citStr="Moldovan et al., 1999" startWordPosition="939" endWordPosition="942">our experiments. Finally, we draw conclusions in Section 5. 2 Previous Works The current QA approaches can be classified into two groups; text-snippet extraction systems and noun-phrase extraction systems (also called closed-class QA) (Vicedo and Ferrándex, 2000). The text-snippet extraction approaches are based on locating and extracting the most relevant sentences or paragraphs to the query by assuming that this text will probably contain the correct answer to the query. These approaches have been the most commonly used by participants in last TREC QA Track (Ferret et al., 1999; Hull, 1999; Moldovan et al., 1999; Prager et al., 1999; Srihari and Li, 1999). ExtrAns (Berri et al., 1998) is a representative QA system in the text-snippet extraction approaches. The system locates the phrases in a document from which a user can infer an answer. However, it is difficult for the system to be converted into other domains because the system uses syntactic and semantic information that only covers a very limited domain (Vicedo and Ferrándex, 2000). The noun-phrase extraction approaches are based on finding concrete information, mainly noun phrases, requested by users’ closed-class questions. A closed-class ques</context>
</contexts>
<marker>Moldovan, Harabagiu, Pasca, Mihalcea, Goodrum, Gîrju, Rus, 1999</marker>
<rawString>Moldovan, D., Harabagiu, S., Pasca, M., Mihalcea, R., Goodrum, R., Gîrju, R., and Rus, V. 1999. LASSO: A Tool for Surfing the Answer Net. In Proceedings of The Eighth Text REtrieval Conference (TREC-8), http://trec.ni st.gov/pubs /trec8/t8_proceedings.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>E Brown</author>
<author>A Coden</author>
<author>D Radev</author>
</authors>
<title>Question-Answering by Predictive Annotation.</title>
<date>2000</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>184--191</pages>
<contexts>
<context position="2446" citStr="Prager et al., 2000" startWordPosition="360" endWordPosition="363">d carefully look over the whole text in order to find a short phrase that precisely answers his/her question. Question-answering (QA), an area of IR, is attracting more attention, as shown in the proceedings of AAAI (AAAI, 1999) and TREC (TREC, http://trec.nist.gov/overview.html). A QA system searches a large collection of texts, and filters out inadequate phrases or sentences within the texts. By using the QA system, a user can promptly approach to his/her answer phrases without troublesome tasks. However, most of the current QA systems (Ferret et al., 1999; Hull, 1999; Srihari and Li, 1999; Prager et al., 2000) have two problems as follows: It cannot correctly respond to all of the users’ questions. It can answer the questions that are included in the pre-defined categories such as person, date, time, and etc. It requires more indexing or searching time than traditional IR systems do because it needs a deep linguistic knowledge such as syntactic or semantic roles of words. To solve the problems, we propose a QA system using a predictive answer indexer - MAYA (MAke Your Answer). We can easily add new categories to MAYA by only supplementing domain dictionaries and rules. We do not have to revise the </context>
<context position="4236" citStr="Prager et al., 2000" startWordPosition="653" endWordPosition="656"> (e.g. http://www.altavista.com) in World Wide Web (WWW). However, in a real sense of information retrieval rather than document retrieval, a user still needs to find an answer phrase within the vast amount of the retrieved documents although he/she can promptly find the relevant documents by using these engines. Recently, several QA systems are proposed to avoid the unnecessary answer finding efforts (Ferret et al., 1999; Hull, 1999; Moldovan et al. 1999; Prager et al., 1999; Srihari and Li, 1999). Recent researches have combined the strengths between a traditional IR system and a QA system (Prager et al., 2000; Prager et al., 1999; Srihari and Li, 1999). Most of the combined systems access a huge amount of electronic information by using IR techniques, and they improve precision rates by using QA techniques. In detail, they retrieve a large amount of documents that are relevant to a user’s query by using a well-known TF IDF. Then, they extract answer candidates within the documents, and filter out the candidates by using an expected answer type and some rules on the retrieval time. Although they have been based on shallow NLP techniques (Sparck-Jones, 1999), they consume much longer retrieval time </context>
</contexts>
<marker>Prager, Brown, Coden, Radev, 2000</marker>
<rawString>Prager, J., Brown, E., Coden A., and Radev D. 2000. Question-Answering by Predictive Annotation. In Proceedings of SIGIR 2000, pp. 184-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Prager</author>
<author>D Radev</author>
<author>E Brown</author>
<author>A Coden</author>
</authors>
<title>The Use of Predictive Annotation for Question Answering in TREC8.</title>
<date>1999</date>
<booktitle>In Proceedings of The Eighth Text REtrieval Conference (TREC-8),</booktitle>
<pages>8--8</pages>
<contexts>
<context position="4097" citStr="Prager et al., 1999" startWordPosition="630" endWordPosition="633">alton et al., 1983; Salton and McGill, 1983; Salton 1989). These approaches have been applied successfully to the commercial search engines (e.g. http://www.altavista.com) in World Wide Web (WWW). However, in a real sense of information retrieval rather than document retrieval, a user still needs to find an answer phrase within the vast amount of the retrieved documents although he/she can promptly find the relevant documents by using these engines. Recently, several QA systems are proposed to avoid the unnecessary answer finding efforts (Ferret et al., 1999; Hull, 1999; Moldovan et al. 1999; Prager et al., 1999; Srihari and Li, 1999). Recent researches have combined the strengths between a traditional IR system and a QA system (Prager et al., 2000; Prager et al., 1999; Srihari and Li, 1999). Most of the combined systems access a huge amount of electronic information by using IR techniques, and they improve precision rates by using QA techniques. In detail, they retrieve a large amount of documents that are relevant to a user’s query by using a well-known TF IDF. Then, they extract answer candidates within the documents, and filter out the candidates by using an expected answer type and some rules on</context>
<context position="6038" citStr="Prager et al., 1999" startWordPosition="943" endWordPosition="946">y, we draw conclusions in Section 5. 2 Previous Works The current QA approaches can be classified into two groups; text-snippet extraction systems and noun-phrase extraction systems (also called closed-class QA) (Vicedo and Ferrándex, 2000). The text-snippet extraction approaches are based on locating and extracting the most relevant sentences or paragraphs to the query by assuming that this text will probably contain the correct answer to the query. These approaches have been the most commonly used by participants in last TREC QA Track (Ferret et al., 1999; Hull, 1999; Moldovan et al., 1999; Prager et al., 1999; Srihari and Li, 1999). ExtrAns (Berri et al., 1998) is a representative QA system in the text-snippet extraction approaches. The system locates the phrases in a document from which a user can infer an answer. However, it is difficult for the system to be converted into other domains because the system uses syntactic and semantic information that only covers a very limited domain (Vicedo and Ferrándex, 2000). The noun-phrase extraction approaches are based on finding concrete information, mainly noun phrases, requested by users’ closed-class questions. A closed-class question is a question st</context>
</contexts>
<marker>Prager, Radev, Brown, Coden, 1999</marker>
<rawString>Prager, J., Radev, D., Brown, E., and Coden, A. 1999. The Use of Predictive Annotation for Question Answering in TREC8. In Proceedings of The Eighth Text REtrieval Conference (TREC-8), http://trec.nist.gov/pubs/trec8/t8_proceedings.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>E A Fox</author>
<author>H Wu</author>
</authors>
<date>1983</date>
<journal>Extended Boolean Information Retrieval, Communication of the ACM,</journal>
<pages>26--12</pages>
<contexts>
<context position="3496" citStr="Salton et al., 1983" startWordPosition="538" endWordPosition="541">ive answer indexer - MAYA (MAke Your Answer). We can easily add new categories to MAYA by only supplementing domain dictionaries and rules. We do not have to revise the searching engine of MAYA because the indexer is designed as a separate component that extracts candidate answers. In addition, a user can promptly obtain answer phrases on retrieval time because MAYA indexes answer candidates in advance. Most of the previous approaches in IR have been focused on the method to efficiently represent terms in a document because they want to index and search a large amount of data in a short time (Salton et al., 1983; Salton and McGill, 1983; Salton 1989). These approaches have been applied successfully to the commercial search engines (e.g. http://www.altavista.com) in World Wide Web (WWW). However, in a real sense of information retrieval rather than document retrieval, a user still needs to find an answer phrase within the vast amount of the retrieved documents although he/she can promptly find the relevant documents by using these engines. Recently, several QA systems are proposed to avoid the unnecessary answer finding efforts (Ferret et al., 1999; Hull, 1999; Moldovan et al. 1999; Prager et al., 199</context>
<context position="9226" citStr="Salton et al., 1983" startWordPosition="1435" endWordPosition="1438"> in Index DataBase (DB). For example, if n surrounding terms affects a candidate, n pairs of the candidate and terms are stored into DB with n scores. As shown in Figure 1, the indexing engine keeps separate index DBs that are classified into pre-defined semantic categories (i.e. users’ asking points or question types). The searching engine identifies a user’s asking point, and selects an index DB that includes answer candidates of his/her query. Then, it calculates similarities between terms of his/her query and the terms surrounding the candidates. The similarities are based on pNorm model (Salton et al., 1983). Next, it ranks the candidates according to the similarities. Figure 1. A basic architecture of the QA engines Figure 2 shows a total architecture of MAYA that combines with a traditional IR system. As shown in Figure 2, the total system has two index DBs. One is for the IR system that retrieves relevant documents, and the other is for MAYA that extracts relevant answer phrases. Figure 2. A total architecture of the combined MAYA system 3.1 Predictive Answer indexing The answer indexing phase can be separated in 2 stages; Answer-finding and Term-scoring. For answer-finding, we classify users’</context>
<context position="19678" citStr="Salton et al., 1983" startWordPosition="3125" endWordPosition="3128">e highest?)” receives a high score. 3. The next content words of specific prepositions like (about) receive low scores, and the previous content words receive high scores. For example, the score of (article) in “ (the article about China)” is lower than that of (China). Rule 1. Heuristic rules for scoring query terms 3.3 Answer scoring and ranking The searching engine calculates the similarities between query and answer candidates, and ranks the answer candidates according to the similarities. To check the similarities, the searching engine uses the AND operation of a well-known p-Norm model (Salton et al., 1983), as shown in Equation 3. p p p q a p (1 ) (1 ) − + − q p a + + − p (1 ) q a (3) 1 1 2 2 i i Sim A Q ( , ) 1 = −p andp p p q q + + + q 1 2 i In Equation 3, A is an answer candidate, and ai is the ith term score in the context window of the answer candidate. ai is stored in the answer DB. qi is the ith term score in the query. p is the P-value in the p-Norm model. It takes a relatively short time for answer scoring and ranking phase because the indexing engine has already calculated the scores of the terms that affect answer candidates. In other words, the searching engine simply adds up the we</context>
</contexts>
<marker>Salton, Fox, Wu, 1983</marker>
<rawString>Salton, G., Fox, E.A., and Wu, H. 1983. Extended Boolean Information Retrieval, Communication of the ACM, 26(12):1022-1036.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M McGill</author>
</authors>
<title>Introduction to Modern Information Retrieval (Computer Series),</title>
<date>1983</date>
<location>New York:McGraw-Hill.</location>
<contexts>
<context position="3521" citStr="Salton and McGill, 1983" startWordPosition="542" endWordPosition="545">MAYA (MAke Your Answer). We can easily add new categories to MAYA by only supplementing domain dictionaries and rules. We do not have to revise the searching engine of MAYA because the indexer is designed as a separate component that extracts candidate answers. In addition, a user can promptly obtain answer phrases on retrieval time because MAYA indexes answer candidates in advance. Most of the previous approaches in IR have been focused on the method to efficiently represent terms in a document because they want to index and search a large amount of data in a short time (Salton et al., 1983; Salton and McGill, 1983; Salton 1989). These approaches have been applied successfully to the commercial search engines (e.g. http://www.altavista.com) in World Wide Web (WWW). However, in a real sense of information retrieval rather than document retrieval, a user still needs to find an answer phrase within the vast amount of the retrieved documents although he/she can promptly find the relevant documents by using these engines. Recently, several QA systems are proposed to avoid the unnecessary answer finding efforts (Ferret et al., 1999; Hull, 1999; Moldovan et al. 1999; Prager et al., 1999; Srihari and Li, 1999).</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>Salton, G., and McGill, M. 1983. Introduction to Modern Information Retrieval (Computer Series), New York:McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
</authors>
<title>Automatic Text Processing: The Transformation, Analysis and Retrieval of Information by Computer.</title>
<date>1989</date>
<location>Reading, MA:AddisonWesley.</location>
<contexts>
<context position="3535" citStr="Salton 1989" startWordPosition="546" endWordPosition="547">We can easily add new categories to MAYA by only supplementing domain dictionaries and rules. We do not have to revise the searching engine of MAYA because the indexer is designed as a separate component that extracts candidate answers. In addition, a user can promptly obtain answer phrases on retrieval time because MAYA indexes answer candidates in advance. Most of the previous approaches in IR have been focused on the method to efficiently represent terms in a document because they want to index and search a large amount of data in a short time (Salton et al., 1983; Salton and McGill, 1983; Salton 1989). These approaches have been applied successfully to the commercial search engines (e.g. http://www.altavista.com) in World Wide Web (WWW). However, in a real sense of information retrieval rather than document retrieval, a user still needs to find an answer phrase within the vast amount of the retrieved documents although he/she can promptly find the relevant documents by using these engines. Recently, several QA systems are proposed to avoid the unnecessary answer finding efforts (Ferret et al., 1999; Hull, 1999; Moldovan et al. 1999; Prager et al., 1999; Srihari and Li, 1999). Recent resear</context>
</contexts>
<marker>Salton, 1989</marker>
<rawString>Salton, G. 1989. Automatic Text Processing: The Transformation, Analysis and Retrieval of Information by Computer. Reading, MA:AddisonWesley.</rawString>
</citation>
<citation valid="false">
<booktitle>TREC (Text REtrieval Conference) Overview,</booktitle>
<location>http://trec.nist.gov/overview.html.</location>
<marker></marker>
<rawString>TREC (Text REtrieval Conference) Overview, http://trec.nist.gov/overview.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sparck-Jones</author>
</authors>
<title>What is the role NLP in Text Retrieval?. Natural Language Information Retrieval,</title>
<date>1999</date>
<pages>1--24</pages>
<publisher>Kluwer Academic</publisher>
<note>Publishers. T.Strzalkowski (ed),</note>
<contexts>
<context position="4794" citStr="Sparck-Jones, 1999" startWordPosition="747" endWordPosition="748"> a traditional IR system and a QA system (Prager et al., 2000; Prager et al., 1999; Srihari and Li, 1999). Most of the combined systems access a huge amount of electronic information by using IR techniques, and they improve precision rates by using QA techniques. In detail, they retrieve a large amount of documents that are relevant to a user’s query by using a well-known TF IDF. Then, they extract answer candidates within the documents, and filter out the candidates by using an expected answer type and some rules on the retrieval time. Although they have been based on shallow NLP techniques (Sparck-Jones, 1999), they consume much longer retrieval time than traditional IR systems do because of the addictive efforts mentioned above. To save retrieval time, MAYA extracts answer candidates, and computes the scores of the candidates on indexing time. On retrieval time, it just calculates the similarities between a user’s query and the candidates. As a result, it can minimize the retrieval time. This paper is organized as follows. In Section 2, we review the previous works of the QA systems. In Section 3, we describe the applied NLP techniques, and present our system. In Section 4, we analyze the result o</context>
</contexts>
<marker>Sparck-Jones, 1999</marker>
<rawString>Sparck-Jones, K. 1999. What is the role NLP in Text Retrieval?. Natural Language Information Retrieval, Kluwer Academic Publishers. T.Strzalkowski (ed), pp.1-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Srihari</author>
<author>W Li</author>
</authors>
<title>Information Extraction Supported Question Answering.</title>
<date>1999</date>
<booktitle>In Proceedings of The Eighth Text REtrieval Conference (TREC-8),</booktitle>
<pages>8--8</pages>
<contexts>
<context position="2424" citStr="Srihari and Li, 1999" startWordPosition="356" endWordPosition="359"> Hence, the user should carefully look over the whole text in order to find a short phrase that precisely answers his/her question. Question-answering (QA), an area of IR, is attracting more attention, as shown in the proceedings of AAAI (AAAI, 1999) and TREC (TREC, http://trec.nist.gov/overview.html). A QA system searches a large collection of texts, and filters out inadequate phrases or sentences within the texts. By using the QA system, a user can promptly approach to his/her answer phrases without troublesome tasks. However, most of the current QA systems (Ferret et al., 1999; Hull, 1999; Srihari and Li, 1999; Prager et al., 2000) have two problems as follows: It cannot correctly respond to all of the users’ questions. It can answer the questions that are included in the pre-defined categories such as person, date, time, and etc. It requires more indexing or searching time than traditional IR systems do because it needs a deep linguistic knowledge such as syntactic or semantic roles of words. To solve the problems, we propose a QA system using a predictive answer indexer - MAYA (MAke Your Answer). We can easily add new categories to MAYA by only supplementing domain dictionaries and rules. We do n</context>
<context position="4120" citStr="Srihari and Li, 1999" startWordPosition="634" endWordPosition="637">alton and McGill, 1983; Salton 1989). These approaches have been applied successfully to the commercial search engines (e.g. http://www.altavista.com) in World Wide Web (WWW). However, in a real sense of information retrieval rather than document retrieval, a user still needs to find an answer phrase within the vast amount of the retrieved documents although he/she can promptly find the relevant documents by using these engines. Recently, several QA systems are proposed to avoid the unnecessary answer finding efforts (Ferret et al., 1999; Hull, 1999; Moldovan et al. 1999; Prager et al., 1999; Srihari and Li, 1999). Recent researches have combined the strengths between a traditional IR system and a QA system (Prager et al., 2000; Prager et al., 1999; Srihari and Li, 1999). Most of the combined systems access a huge amount of electronic information by using IR techniques, and they improve precision rates by using QA techniques. In detail, they retrieve a large amount of documents that are relevant to a user’s query by using a well-known TF IDF. Then, they extract answer candidates within the documents, and filter out the candidates by using an expected answer type and some rules on the retrieval time. Al</context>
<context position="6061" citStr="Srihari and Li, 1999" startWordPosition="947" endWordPosition="950">s in Section 5. 2 Previous Works The current QA approaches can be classified into two groups; text-snippet extraction systems and noun-phrase extraction systems (also called closed-class QA) (Vicedo and Ferrándex, 2000). The text-snippet extraction approaches are based on locating and extracting the most relevant sentences or paragraphs to the query by assuming that this text will probably contain the correct answer to the query. These approaches have been the most commonly used by participants in last TREC QA Track (Ferret et al., 1999; Hull, 1999; Moldovan et al., 1999; Prager et al., 1999; Srihari and Li, 1999). ExtrAns (Berri et al., 1998) is a representative QA system in the text-snippet extraction approaches. The system locates the phrases in a document from which a user can infer an answer. However, it is difficult for the system to be converted into other domains because the system uses syntactic and semantic information that only covers a very limited domain (Vicedo and Ferrándex, 2000). The noun-phrase extraction approaches are based on finding concrete information, mainly noun phrases, requested by users’ closed-class questions. A closed-class question is a question stated in natural languag</context>
</contexts>
<marker>Srihari, Li, 1999</marker>
<rawString>Srihari, R., and Li, W. 1999. Information Extraction Supported Question Answering. In Proceedings of The Eighth Text REtrieval Conference (TREC-8), http://trec.nist.gov/pubs/trec8/t8_proceedings.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Vicedo</author>
<author>A Ferrándex</author>
</authors>
<title>Importance of Pronominal Anaphora resolution in Question Answering systems.</title>
<date>2000</date>
<booktitle>In Proceeding of ACL</booktitle>
<pages>555--562</pages>
<contexts>
<context position="5659" citStr="Vicedo and Ferrándex, 2000" startWordPosition="881" endWordPosition="884">. On retrieval time, it just calculates the similarities between a user’s query and the candidates. As a result, it can minimize the retrieval time. This paper is organized as follows. In Section 2, we review the previous works of the QA systems. In Section 3, we describe the applied NLP techniques, and present our system. In Section 4, we analyze the result of our experiments. Finally, we draw conclusions in Section 5. 2 Previous Works The current QA approaches can be classified into two groups; text-snippet extraction systems and noun-phrase extraction systems (also called closed-class QA) (Vicedo and Ferrándex, 2000). The text-snippet extraction approaches are based on locating and extracting the most relevant sentences or paragraphs to the query by assuming that this text will probably contain the correct answer to the query. These approaches have been the most commonly used by participants in last TREC QA Track (Ferret et al., 1999; Hull, 1999; Moldovan et al., 1999; Prager et al., 1999; Srihari and Li, 1999). ExtrAns (Berri et al., 1998) is a representative QA system in the text-snippet extraction approaches. The system locates the phrases in a document from which a user can infer an answer. However, i</context>
<context position="7367" citStr="Vicedo and Ferrándex, 2000" startWordPosition="1140" endWordPosition="1143">rocedural answer. MURAX (Kupiec, 1993) is one of the noun-phrase extraction systems. MURAX uses modules for the shallow linguistic analysis: a Part-Of-Speech (POS) tagger and finite-state recognizer for matching lexico-syntactic pattern. The finite-state recognizer decides users’ expectations and filters out various answer hypotheses. For example, the answers to questions beginning with the word Who are likely to be people’s name. Some QA systems participating in Text REtrieval Conference (TREC) use a shallow linguistic knowledge and start from similar approaches as used in MURAX (Hull, 1999; Vicedo and Ferrándex, 2000). These QA systems use specialized shallow parsers to identify the asking point (who, what, when, where, etc). However, these QA systems take a long response time because they apply some rules to each sentence including answer candidates and give each answer a score on retrieval time. MAYA uses shallow linguistic information such as a POS tagger, a lexico-syntactic parser similar to finite-state recognizer in MURAX and a Named Entity (NE) recognizer based on dictionaries. However, MAYA returns answer phrases in very short time compared with those previous systems because the system extracts an</context>
</contexts>
<marker>Vicedo, Ferrándex, 2000</marker>
<rawString>Vicedo, J. L., and Ferrándex, A. 2000. Importance of Pronominal Anaphora resolution in Question Answering systems. In Proceeding of ACL 2000, pp. 555-562.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Voorhees</author>
<author>D M Tice</author>
</authors>
<title>The TREC-8 Question Answering Track Evaluation.</title>
<date>1999</date>
<booktitle>In Proceedings of The Eighth Text REtrieval Conference (TREC-8), http://trec.nist.gov/pubs /trec8/t8_proceedings.html.</booktitle>
<contexts>
<context position="23315" citStr="Voorhees and Tice, 1999" startWordPosition="3781" endWordPosition="3784">c.kr”. Category person country address organization # of QAs 9 3 3 9 Category telephone email URL people num. # of QAs 3 5 4 0 Category phy. num. abs. num. rate price # of QAs 1 1 0 4 Category date time out of cat. total # of QAs 5 1 2 50 Table 1. The number of the collected questionanswers in each category We use two sorts of evaluation schemes. To experiment on MAYA, we compute the performance score as the Reciprocal Answer Rank (RAR) of the first correct answer given by each question. To compute the overall performance, we use the Mean Reciprocal Answer Rank (MRAR), as shown in Equation 5 (Voorhees and Tice, 1999). � �MRAR=1/n ranki) (5) 1i With respect to the total system that combines MAYA with the IR system, we use the Reciprocal Document Rank (RDR) and the Mean Reciprocal Document Rank (MRDR). RDR means the reciprocal rank of the first document including the correct answers given by each question. 4.2 Analysis of experiment results The performance of MAYA is shown in Table 2. We obtained the correct answers for 33 questions out of 50 in Top 1. Rank Top 1 Top 2 Top 3 Top 4 # of answers 33 4 3 2 Rank Top 5 Top 6— Failure Total(MRAR) # of answers 1 2 5 50 (0.80) Table 2. The performance of the QA syst</context>
</contexts>
<marker>Voorhees, Tice, 1999</marker>
<rawString>Voorhees, E., and Tice, D. M. 1999. The TREC-8 Question Answering Track Evaluation. In Proceedings of The Eighth Text REtrieval Conference (TREC-8), http://trec.nist.gov/pubs /trec8/t8_proceedings.html.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>