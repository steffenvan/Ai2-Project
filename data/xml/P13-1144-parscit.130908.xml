<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000580">
<title confidence="0.916333">
Text-Driven Toponym Resolution using Indirect Supervision
</title>
<author confidence="0.995894">
Michael Speriosu Jason Baldridge
</author>
<affiliation confidence="0.9983">
Department of Linguistics
University of Texas at Austin
</affiliation>
<address confidence="0.819362">
Austin, TX 78712 USA
</address>
<email confidence="0.999672">
{speriosu,jbaldrid}@utexas.edu
</email>
<sectionHeader confidence="0.99482" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99989025">
Toponym resolvers identify the specific lo-
cations referred to by ambiguous place-
names in text. Most resolvers are based on
heuristics using spatial relationships be-
tween multiple toponyms in a document,
or metadata such as population. This pa-
per shows that text-driven disambiguation
for toponyms is far more effective. We ex-
ploit document-level geotags to indirectly
generate training instances for text classi-
fiers for toponym resolution, and show that
textual cues can be straightforwardly in-
tegrated with other commonly used ones.
Results are given for both 19th century
texts pertaining to the American Civil War
and 20th century newswire articles.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999906774193549">
It has been estimated that at least half of the
world’s stored knowledge, both printed and digi-
tal, has geographic relevance, and that geographic
information pervades many more aspects of hu-
manity than previously thought (Petras, 2004;
Skupin and Esperb´e, 2011). Thus, there is value
in connecting linguistic references to places (e.g.
placenames) to formal references to places (coor-
dinates) (Hill, 2006). Allowing for the querying
and exploration of knowledge in a geographically
informed way requires more powerful tools than a
keyword-based search can provide, in part due to
the ambiguity of toponyms (placenames).
Toponym resolution is the task of disambiguat-
ing toponyms in natural language contexts to geo-
graphic locations (Leidner, 2008). It plays an es-
sential role in automated geographic indexing and
information retrieval. This is useful for histori-
cal research that combines age-old geographic is-
sues like territoriality with modern computational
tools (Guldi, 2009), studies of the effect of histor-
ically recorded travel costs on the shaping of em-
pires (Scheidel et al., 2012), and systems that con-
vey the geographic content in news articles (Teitler
et al., 2008; Sankaranarayanan et al., 2009) and
microblogs (Gelernter and Mushegian, 2011).
Entity disambiguation systems such as those of
Kulkarni et al. (2009) and Hoffart et al. (2011)
disambiguate references to people and organiza-
tions as well as locations, but these systems do not
take into account any features or measures unique
to geography such as physical distance. Here we
demonstrate the utility of incorporating distance
measurements in toponym resolution systems.
Most work on toponym resolution relies on
heuristics and hand-built rules. Some use sim-
ple rules based on information from a gazetteer,
such as population or administrative level (city,
state, country, etc.), resolving every instance of
the same toponym type to the same location re-
gardless of context (Ladra et al., 2008). Others use
relationships between multiple toponyms in a con-
text (local or whole document) and look for con-
tainment relationships, e.g. London and England
occurring in the same paragraph or as the bigram
London, England (Li et al., 2003; Amitay et al.,
2004; Zong et al., 2005; Clough, 2005; Li, 2007;
Volz et al., 2007; Jones et al., 2008; Buscaldi and
Rosso, 2008; Grover et al., 2010). Still others first
identify unambiguous toponyms and then disam-
biguate other toponyms based on geopolitical re-
lationships with or distances to the unambiguous
ones (Ding et al., 2000). Many favor resolutions of
toponyms within a local context or document that
cover a smaller geographic area over those that are
more dispersed (Rauch et al., 2003; Leidner, 2008;
Grover et al., 2010; Loureiro et al., 2011; Zhang
et al., 2012). Roberts et al. (2010) use relation-
ships learned between people, organizations, and
locations from Wikipedia to aid in toponym reso-
lution when such named entities are present, but
do not exploit any other textual context.
</bodyText>
<page confidence="0.932179">
1466
</page>
<note confidence="0.9134435">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1466–1476,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99193805882353">
Most of these approaches suffer from a major
weakness: they rely primarily on spatial relation-
ships and metadata about locations (e.g., popu-
lation). As such, they often require nearby to-
ponyms (including unambiguous or containing to-
ponyms) to resolve ambiguous ones. This reliance
can result in poor coverage when the required in-
formation is missing in the context or when a doc-
ument mentions locations that are neither nearby
geographically nor in a geopolitical relationship.
There is a clear opportunity that most ignore:
use non-toponym textual context. Spatially rel-
evant words like downtown that are not explicit
toponyms can be strong cues for resolution (Hol-
lenstein and Purves, 2012). Furthermore, the con-
nection between non-spatial words and locations
has been successfully exploited in data-driven
approaches to document geolocation (Eisenstein
et al., 2010, 2011; Wing and Baldridge, 2011;
Roller et al., 2012) and other tasks (Hao et al.,
2010; Pang et al., 2011; Intagorn and Lerman,
2012; Hecht et al., 2012; Louwerse and Benesh,
2012; Adams and McKenzie, 2013).
In this paper, we learn resolvers that use all
words in local or document context. For example,
the word lobster appearing near the toponym Port-
land indicates the location is Portland in Maine
rather than Oregon or Michigan. Essentially, we
learn a text classifier per toponym. There are no
massive collections of toponyms labeled with lo-
cations, so we train models indirectly using geo-
tagged Wikipedia articles. Our results show these
text classifiers are far more accurate than algo-
rithms based on spatial proximity or metadata.
Furthermore, they are straightforward to combine
with such algorithms and lead to error reductions
for documents that match those algorithms’ as-
sumptions.
Our primary focus is toponym resolution, so we
evaluate on toponyms identified by human anno-
tators. However, it is important to consider the
utility of an end-to-end toponym identification and
resolution system, so we also demonstrate that
performance is still strong when toponyms are de-
tected with a standard named entity recognizer.
We have implemented all the models discussed
in this paper in an open source software package
called Fieldspring, which is available on GitHub:
http://github.com/utcompling/fieldspring
Explicit instructions are provided for preparing
data and running code to reproduce our results.
</bodyText>
<figureCaption confidence="0.995904">
Figure 1: Points representing the United States.
</figureCaption>
<sectionHeader confidence="0.995673" genericHeader="introduction">
2 Data
</sectionHeader>
<subsectionHeader confidence="0.988539">
2.1 Gazetteer
</subsectionHeader>
<bodyText confidence="0.999984612903226">
Toponym resolvers need a gazetteer to obtain can-
didate locations for each toponym. Additionally,
many gazetteers include other information such as
population and geopolitical hierarchy information.
We use GEONAMES, a freely available gazetteer
containing over eight million entries worldwide.1
Each location entry contains a name (sometimes
more than one) and latitude/longitude coordinates.
Entries also include the location’s administrative
level (e.g. city or state) and its position in the
geopolitical hierarchy of countries, states, etc.
GEONAMES gives the locations of regional
items like states, provinces, and countries as single
points. This is clearly problematic when we seek
connections between words and locations: e.g. we
might learn that many words associated with the
USA are connected to a point in Kansas. To get
around this, we represent regional locations as a
set of points derived from the gazetteer. Since re-
gional locations are named in the entries for loca-
tions they contain, all locations contained in the
region are extracted (in some cases over 100,000
of them) and then k-means is run to find a smaller
set of spatial centroids. These act as a tractable
proxy for the spatial extent of the entire region. k
is set to the number of 1◦ by 1◦ grid cells covered
by that region. Figure 1 shows the points com-
puted for the United States.2 A nice property of
this representation is that it does not involve re-
gion shape files and the additional programming
infrastructure they require.
</bodyText>
<footnote confidence="0.9980495">
1Downloaded April 16, 2013 from www.geonames.
org.
2The representation also contains three points each in
Hawaii and Alaska not shown in Figure 1.
</footnote>
<page confidence="0.97929">
1467
</page>
<table confidence="0.999435428571428">
Corpus docs toks types tokst,,p typest,,p ambavg ambmax
TRC-DEV 631 136k 17k 4356 613 15.0 857
TRC-DEV-NER - - - 3165 391 18.2 857
TRC-TEST 315 68k 11k 1903 440 13.7 857
TRC-TEST-NER - - - 1346 305 15.7 857
CWAR-DEV 228 33m 200k 157k 850 29.9 231
CWAR-TEST 113 25m 305k 85k 760 31.5 231
</table>
<tableCaption confidence="0.998833">
Table 1: Statistics of the corpora used for evaluation. Columns subscripted by top give figures for
</tableCaption>
<bodyText confidence="0.913385857142857">
toponyms. The last two columns give the average number of candidate locations per toponym token and
the number of candidate locations for the most ambiguous toponym.
A location for present purposes is thus a set of
points on the earth’s surface. The distance be-
tween two locations is computed as the great circle
distance between the closest pair of representative
points, one from each location.
</bodyText>
<subsectionHeader confidence="0.997653">
2.2 Toponym Resolution Corpora
</subsectionHeader>
<bodyText confidence="0.932813294117647">
We need corpora with toponyms identified and re-
solved by human annotators for evaluation. The
TR-CONLL corpus (Leidner, 2008) contains 946
REUTERS news articles published in August
1996. It has about 204,000 words and articles
range in length from a few hundred words to sev-
eral thousand words. Each toponym in the corpus
was identified and resolved by hand.3 We place
every third article into a test portion (TRC-TEST)
and the rest in a development portion. Since our
methods do not learn from explicitly labeled to-
ponyms, we do not need a training set.
The Perseus Civil War and 19th Century Amer-
ican Collection (CWAR) contains 341 books (58
million words) written primarily about and during
the American Civil War (Crane, 2000). Toponyms
were annotated by a semi-automated process: a
named entity recognizer identified toponyms, and
then coordinates were assigned using simple rules
and corrected by hand. We divide CWAR into de-
velopment (CWAR-DEV) and test (CWAR-TEST)
sets in the same way as TR-CONLL.
Table 1 gives statistics for both corpora, includ-
ing the number and ambiguity of gold standard
toponyms for both as well as NER identified to-
3We found several systematic types of errors in the origi-
nal TR-CONLL corpus, such as coordinates being swapped
for some locations and some longitudes being zero or the neg-
ative of their correct values. We repaired many of these er-
rors, though some more idiosyncratic mistakes remain. We,
along with Jochen Leidner, will release this updated version
shortly and will link to it from our Fieldspring GitHub page.
ponyms for TR-CONLL.4 We use the pre-trained
English NER from the OpenNLP project.5
</bodyText>
<subsectionHeader confidence="0.993478">
2.3 Geolocated Wikipedia Corpus
</subsectionHeader>
<bodyText confidence="0.999986777777778">
The GEOWIKI dataset contains over one million
English articles from the February 11, 2012 dump
of Wikipedia. Each article has human-annotated
latitude/longitude coordinates. We divide the cor-
pus into training (80%), development (10%), and
test (10%) at random and perform preprocessing
to remove markup in the same manner as Wing
and Baldridge (2011). The training portion is used
here to learn models for text-driven resolvers.
</bodyText>
<sectionHeader confidence="0.979958" genericHeader="method">
3 Toponym Resolvers
</sectionHeader>
<bodyText confidence="0.999968625">
Given a set of toponyms provided via annotations
or identified using NER, a resolver must select a
candidate location for each toponym (or, in some
cases, a resolver may abstain). Here, we describe
baseline resolvers, a heuristic resolver based on
the usual cues used in most toponym resolvers,
and several text-driven resolvers. We also discuss
combining heuristic and text-driven resolvers.
</bodyText>
<subsectionHeader confidence="0.999101">
3.1 Baseline Resolvers
</subsectionHeader>
<bodyText confidence="0.999896333333333">
RANDOM For each toponym, the RANDOM re-
solver randomly selects a location from those as-
sociated in the gazetteer with that toponym.
POPULATION The POPULATION resolver se-
lects the location with the greatest population for
each toponym. It is generally quite effective, but
when a toponym has several locations with large
populations, it is often wrong. Also, it can only be
used when such information is available, and it is
</bodyText>
<footnote confidence="0.84919725">
4States and countries are not annotated in CWAR, so we
do not evaluate end-to-end using NER plus toponym resolu-
tion for it as there are many (falsely) false positives.
5opennlp.apache.org
</footnote>
<page confidence="0.992566">
1468
</page>
<bodyText confidence="0.999608">
less effective if the population statistics are from a
time period different from that of the corpus.
</bodyText>
<subsectionHeader confidence="0.99688">
3.2 SPIDER
</subsectionHeader>
<bodyText confidence="0.9994365">
Leidner (2008) describes two general and useful
minimality properties of toponyms:
</bodyText>
<listItem confidence="0.8992876">
• one sense per discourse: multiple tokens of
a toponym in the same text generally do not
refer to different locations in the same text
• spatial minimality: different toponyms in a
text tend refer to spatially near locations
</listItem>
<bodyText confidence="0.999922016129033">
Many toponym resolvers exploit these (Smith and
Crane, 2001; Rauch et al., 2003; Leidner, 2008;
Grover et al., 2010; Loureiro et al., 2011; Zhang
et al., 2012). Here, we define SPIDER (Spatial
Prominence via Iterative Distance Evaluation and
Reweighting) as a strong representative of such
textually unaware approaches. In addition to cap-
turing both minimality properties, it also identifies
the relative prominence of the locations for each
toponym in a given corpus.
SPIDER resolves each toponym by finding the
location for each that minimizes the sum distance
to all locations for all other toponyms in the same
document. On the first iteration, it tends to select
locations that clump spatially: if Paris occurs with
Dallas, it will choose Paris, Texas even though the
topic may be a flight from Texas to France. Further
iterations bring Paris, France into focus by captur-
ing its prominence across the corpus. The key in-
tuition is that most documents will discuss Paris,
France and only a small portion of these mention
places close to Paris, Texas; thus, Paris, France
will be selected on the first iteration for many
documents (though not for the Dallas document).
SPIDER thus assigns each candidate location a
weight (initialized to 1.0), which is re-estimated
on each iteration. The adjusted distance between
two locations is computed as the great circle dis-
tance divided by the product of the two locations’
weights. At the end of an iteration, each candi-
date location’s weight is updated to be the frac-
tion of the times it was chosen times the number
of candidates for that toponym. The weights are
global, with one for each location in the gazetteer,
so the same weight vector is used for each token
of a given toponym on a given iteration.
For example, if after the first iteration Paris,
France is chosen thrice, Paris, Texas once, and
Paris, Arkansas never, the global weights of these
locations are (3/4)*3=2.25, (1/4)*3=.75, and
(0/4)*3=0, respectively (assume, for the exam-
ple, there are no other locations named Paris). The
sum of the weights remains equal to the number
of candidate locations. The updated weights are
used on the next iteration, so Paris, France will
seem “closer” since any distance computed to it
is divided by a number greater than one. Paris,
Texas will seem somewhat further away, and Paris,
Arkansas infinitely far away. The algorithm con-
tinues for a fixed number of iterations or until the
weights do not change more than some thresh-
old. Here, we run SPIDER for 10 iterations; the
weights have generally converged by this point.
When only one toponym is present in a doc-
ument, we simply select the candidate with the
greatest weight. When there is no such weight in-
formation, such as when the toponym does not co-
occur with other toponyms anywhere in the cor-
pus, we select a candidate at random.
SPIDER captures prominence, but we stress it
is not our main innovation: its purpose is to be a
benchmark for text-driven resolvers to beat.
</bodyText>
<subsectionHeader confidence="0.99683">
3.3 Text-Driven Resolvers
</subsectionHeader>
<bodyText confidence="0.999988285714286">
The text-driven resolvers presented in this section
all use local context windows, document context,
or both, to inform disambiguation.
TRIPDL We use a document geolocator
trained on GEOWIKI’s document location labels.
Others—such as Smith and Crane (2001)—have
estimated a document-level location to inform
toponym resolution, but ours is the first we are
aware of to use training data from a different
domain to build a document geolocator that uses
all words (not only toponyms) to estimate a
document’s location. We use the document geolo-
cation method of Wing and Baldridge (2011). It
discretizes the earth’s surface into 1◦ by 1◦ grid
cells and assigns Kullback-Liebler divergences to
each cell given a document, based on language
models learned for each cell from geolocated
Wikipedia articles. We obtain the probability of a
cell c given a document d by the standard method
of exponentiating the negative KL-divergence and
normalizing these values over all cells:
</bodyText>
<equation confidence="0.903676">
exp(−KL(c, d))
P(c|d) = Ec� exp(−KL(c&apos;, d))
</equation>
<bodyText confidence="0.9979675">
This distribution is used for all toponyms t in d
to define distributions PDL(l|t, d) over candidate
</bodyText>
<page confidence="0.735734">
1469
</page>
<equation confidence="0.9834045">
E
l&apos;∈G(t) P(cl&apos;|d)
</equation>
<bodyText confidence="0.999680112903226">
where G(t) is the set of the locations for t in the
gazetteer, and cl is the cell containing l. TRIPDL
(Toponym Resolution Informed by Predicted Doc-
ument Locations) chooses the location that maxi-
mizes PDL.
WISTR While TRIPDL uses an off-the-shelf
document geolocator to capture the geographic
gist of a document, WISTR (Wikipedia Indirectly
Supervised Toponym Resolver) instead directly
targets each toponym. It learns text classifiers
based on local context window features trained on
instances automatically extracted from GEOWIKI.
To create the indirectly supervised training data
for WISTR, the OpenNLP named entity recog-
nizer detects toponyms in GEOWIKI, and can-
didate locations for each toponym are retrieved
from GEONAMES. Each toponym with a loca-
tion within 10km of the document location is con-
sidered a mention of that location. For example,
the Empire State Building Wikipedia article has a
human-provided location label of (40.75,-73.99).
The toponym New York is mentioned several times
in the article, and GEONAMES lists a New York at
(40.71,-74.01). These points are 4.8km apart, so
each mention of New York in the document is con-
sidered a reference to New York City.
Next, context windows w of twenty words to
each side of each toponym are extracted as fea-
tures. The label for a training instance is the
candidate location closest to the document loca-
tion. We extract 1,489,428 such instances for to-
ponyms relevant to our evaluation corpora. These
instances are used to train logistic regression clas-
sifiers P(l|t, w) for location l and toponym t. To
disambiguate a new toponym, WISTR chooses
the location that maximizes this probability.
Few such probabilistic toponym resolvers ex-
ist in the literature. Li (2007) builds a probabil-
ity distribution over locations for each toponym,
but still relies on nearby toponyms that could refer
to regions that contain that toponym and requires
hand construction of distributions. Other learn-
ing approaches to toponym resolution (e.g. Smith
and Mann (2003)) require explicit unambiguous
mentions like Portland, Maine to construct train-
ing instances, while our data gathering methodol-
ogy does not make such an assumption. Overell
and R¨uger (2008) and Overell (2009) only use
nearby toponyms as features. Mani et al. (2010)
and Qin et al. (2010) use other word types but
only in a local context, and they require toponym-
labeled training data. Our approach makes use of
all words in local and document context and re-
quires no explicitly labeled toponym tokens.
TRAWL We bring TRIPDL, WISTR, and
standard toponym resolution cues about ad-
ministrative levels together with TRAWL (To-
ponym Resolution via Administrative levels and
Wikipedia Locations). The general form of a prob-
abilistic resolver that utilizes such information to
select a location lˆ for a toponym t in document d
may be defined as
</bodyText>
<equation confidence="0.918549">
lˆ = arg maxl P(l, al|t, d).
</equation>
<bodyText confidence="0.9999784">
where al is the administrative level (country, state,
city) for l in the gazetteer. This captures the fact
that countries (like Sudan) tend to be referred to
more often than small cities (like Sudan, Texas).
The above term is simplified as follows:
</bodyText>
<equation confidence="0.950531">
P(l, al|t, d) = P(al|t, d)P(l|al, t, d)
� P(al|t)P(l|t, d)
</equation>
<bodyText confidence="0.9997545">
where we approximate the administrative level
prediction as independent of the document, and
the location as independent of administrative level.
The latter term is then expressed as a linear combi-
nation of the local context (WISTR) and the doc-
ument context (TRIPDL):
</bodyText>
<equation confidence="0.761758">
P(l|t, d) = AtP(l|t, ct) + (1−At)PDL(l|t, d).
</equation>
<bodyText confidence="0.999899666666667">
At, the weight of the local context distribution, is
set according to the confidence that a prediction
based on local context is correct:
</bodyText>
<equation confidence="0.8273475">
At = f(t)
f(t)+C,
</equation>
<bodyText confidence="0.9999512">
where f(t) is the fraction of training instances
of toponym t of all instances extracted from
GEOWIKI. C is set experimentally; C=.0001 was
the optimal value for CWAR-DEV. Intuitively, the
larger C is, the greater f(t) must be for the local
context to be trusted over the document context.
We define P(a|t), the administrative level com-
ponent, to be the fraction of representative points
for a location lˆ out of the number of representa-
tives points for all candidate locations l E t,
</bodyText>
<equation confidence="0.988184">
||Rˆl||
E
l&apos;∈t ||Rl&apos;||
locations of t in document d to be the portion of
P(c|d) consistent with the t’s candidate locations:
P (cl|d)
PDL(l|t, d) =
</equation>
<page confidence="0.732121">
1470
</page>
<bodyText confidence="0.999964142857143">
where ||Rl ||is the number of representative points
of l. This boosts states and countries since higher
probability is assigned to locations with more
points (and cities have just one point).
Taken together, the above definitions yield the
TRAWL resolver, which selects the optimal can-
didate location lˆaccording to
</bodyText>
<equation confidence="0.788168">
lˆ = arg maxl P(al|t)(AtP(l|t, ct) + (1−At)PDL(l|t, d)).
</equation>
<subsectionHeader confidence="0.999178">
3.4 Combining Resolvers and Backoff
</subsectionHeader>
<bodyText confidence="0.993017875">
SPIDER begins with uniform weights for each
candidate location of each toponym. WISTR
and TRAWL both output distributions over these
locations based on outside knowledge sources,
and can be used as more informed initializa-
tions of SPIDER than the uniform ones. We
call these combinations WISTR+SPIDER and
TRAWL+SPIDER.6
WISTR fails to predict when encountering a
toponym it has not seen in the training data, and
TRIPDL fails when a toponym only has locations
in cells with no probability mass. TRAWL fails
when both of these are true. In these cases, we
select the candidate location geographically clos-
est to the most likely cell according to TRIPDL’s
P(c|d) distribution.
</bodyText>
<subsectionHeader confidence="0.96023">
3.5 Document Size
</subsectionHeader>
<bodyText confidence="0.999996533333333">
For SPIDER, runtime is quadratic in the size
of documents, so breaking up documents vastly
reduces runtime. It also restricts the minimal-
ity heuristic—appropriately—to smaller spans of
text. For resolvers that take into account the sur-
rounding document when determining how to re-
solve a toponym, such as TRIPDL and TRAWL,
it can often be beneficial to divide documents into
smaller subdocuments in order to get a better esti-
mate of the overall geographic prominence of the
text surrounding a toponym, but at a more coarse-
grained level than the local context models pro-
vide. For these reasons, we simply divide each
book in the CWAR corpus into small subdocu-
ments of at most 20 sentences.
</bodyText>
<sectionHeader confidence="0.999352" genericHeader="method">
4 Evaluation
5 Results
</sectionHeader>
<bodyText confidence="0.999599086956522">
is the same as the gold location. Such a met-
ric can be problematic, however. The gazetteer
used by a resolver may not contain, for a given
toponym, a location whose latitude and longitude
exactly match the gold label for the toponym (Lei-
dner, 2008). Also, some errors are worse than oth-
ers, e.g. predicting a toponym’s location to be on
the other side of the world versus predicting it to
be a different city in the same country—accuracy
does not reflect this difference.
We choose a metric that instead measures the
distance between the correct and predicted loca-
tion for each toponym and compute the mean and
median of all such error distances. This is used
in document geolocation work (Eisenstein et al.,
2010, 2011; Wing and Baldridge, 2011; Roller
et al., 2012) and is related to the root mean squared
distance metric discussed by Leidner (2008).
It is important to understand performance on
plain text (without gold toponyms), which is the
typical use case for applications using toponym
resolvers. Both the accuracy metric and the error-
distance metric encounter problems when the set
of predicted toponyms is not the same as the set
of gold toponyms (regardless of locations), e.g.
when a named entity recognizer is used to iden-
tify toponyms. In this case, we can use precision
and recall, where a true positive is defined as the
prediction of a correctly identified toponym’s lo-
cation to be as close as possible to its gold la-
bel, given the gazetteer used. False positives oc-
cur when the NER incorrectly predicts a toponym,
and false negatives occur when it fails to predict a
toponym identified by the annotator. When a cor-
rectly identified toponym receives an incorrect lo-
cation prediction, this counts as both a false nega-
tive and a false positive. We primarily present re-
sults from experiments with gold toponyms but in-
clude an accuracy measure for comparability with
results from experiments run on plain text with
a named entity recognizer. This accuracy met-
ric simply computes the fraction of toponyms that
were resolved as close as possible to their gold la-
bel given the gazetteer.
Many prior efforts use a simple accuracy metric:
the fraction of toponyms whose predicted location
</bodyText>
<footnote confidence="0.72328425">
6We scale each toponym’s distribution as output by
WISTR or TRAWL by the number of candidate locations
for that toponym, since the total weight for each toponym in
SPIDER is the number of candidate locations, not 1.
</footnote>
<bodyText confidence="0.9990828">
Table 2 gives the performance of the resolvers
on the TR-CONLL and CWAR test sets when
gold toponyms are used. Values for RANDOM
and SPIDER are averaged over three trials. The
ORACLE row gives results when the candidate
</bodyText>
<page confidence="0.973945">
1471
</page>
<table confidence="0.999280636363636">
Resolver TRC-TEST CWAR-TEST
Mean Med. A Mean Med. A
ORACLE 105 19.8 100.0 0.0 0.0 100.0
RANDOM 3915 1412 33.5 2389 1027 11.8
POPULATION 216 23.1 81.0 1749 0.0 59.7
SPIDER10 2180 30.9 55.7 266 0.0 57.5
TRIPDL 1494 29.3 62.0 847 0.0 51.5
WISTR 279 22.6 82.3 855 0.0 69.1
WISTR+SPIDER10 430 23.1 81.8 201 0.0 85.9
TRAWL 235 22.6 81.4 945 0.0 67.8
TRAWL+SPIDER10 297 23.1 80.7 148 0.0 78.2
</table>
<tableCaption confidence="0.998101">
Table 2: Accuracy and error distance metrics on test sets with gold toponyms.
</tableCaption>
<figureCaption confidence="0.988213">
Figure 2: Visualization of how SPIDER clumps
most predicted locations in the same region
(above), on the CWAR-DEV corpus. TRAWL’s
output (below) is much more dispersed.
</figureCaption>
<bodyText confidence="0.999857">
from GEONAMES closest to the annotated loca-
tion is always selected. The ORACLE mean and
median error values on TR-CONLL are nonzero
due to errors in the annotations and inconsisten-
cies stemming from the fact that coordinates from
GEONAMES were not used in the annotation of
TR-CONLL.
On both datasets, SPIDER achieves errors and
accuracies much better than RANDOM, validating
the intuition that authors tend to discuss places
near each other more often than not, while some
locations are more prominent in a given corpus
despite violating the minimality heuristic. The
text-driven resolvers vastly outperform SPIDER,
showing the effectiveness of textual cues for to-
ponym resolution.
The local context resolver WISTR is very
effective: it has the highest accuracy for
TR-CONLL, though two other text-based re-
solvers also beat the challenging POPULATION
baseline’s accuracy. TRAWL achieves a better
mean distance metric for TR-CONLL, and when
used to seed SPIDER, it obtains the lowest mean
error on CWAR by a large margin. SPIDER
seeded with WISTR achieves the highest accu-
racy on CWAR. The overall geographic scope
of CWAR, a collection of documents about the
American Civil War, is much smaller than that of
TR-CONLL (articles about international events).
This makes toponym resolution easier overall (es-
pecially error distances) for minimality resolvers
like SPIDER, which primarily seek tightly clus-
tered sets of locations. This behavior is quite
clear in visualizations of predicted locations such
as Figure 2.
On the CWAR dataset, POPULATION performs
relatively poorly, demonstrating the fragility of
population-based decisions for working with his-
torical corpora. (Also, we note that POPULATION
is not a resolver per se since it only ever predicts
one location for a given toponym, regardless of
context.)
Table 3 gives results on TRC-TEST when NER-
identified toponyms are used. In this case, the
ORACLE results are less than 100% due to the lim-
itations of the NER, and represent the best possible
results given the NER we used.
When resolvers are run on NER-identified to-
ponyms, the text-driven resolvers that use lo-
cal context again easily beat SPIDER. WISTR
achieves the best performance. The named en-
tity recognizer is likely better at detecting com-
mon toponyms than rare toponyms due to the na-
</bodyText>
<page confidence="0.972756">
1472
</page>
<table confidence="0.9999236">
Resolver P R F
ORACLE 82.6 59.9 69.4
RANDOM 25.1 18.2 21.1
POPULATION 71.6 51.9 60.2
SPIDER10 40.5 29.4 34.1
TRIPDL 51.8 37.5 43.5
WISTR 73.9 53.6 62.1
WISTR+SPIDER10 73.2 53.1 61.5
TRAWL 72.5 52.5 60.9
TRAWL+SPIDER10 72.0 52.2 60.5
</table>
<tableCaption confidence="0.854374">
Table 3: Precision, recall, and F-score of resolvers
on TRC-TEST with NER-identified toponyms.
</tableCaption>
<bodyText confidence="0.999931137931034">
ture of its training data, and many more local con-
text training instances were extracted from com-
mon toponyms than from rare ones in Wikipedia.
Thus, our model that uses only these local context
models does best when running on NER-identified
toponyms. We also measured the mean and me-
dian error distance for toponyms correctly identi-
fied by the named entity recognizer, and found that
they tended to be 50-200km worse than for gold
toponyms. This also makes sense given the named
entity recognizer’s tendency to detect common to-
ponyms: common toponyms tend to be more am-
biguous than others.
Results on TR-CONLL indicate much higher
performance than the resolvers presented by Lei-
dner (2008), whose F-scores do not exceed 36.5%
with either gold or NER toponyms.7 TRC-TEST
is a subset of the documents Leidner uses (he did
not split development and test data), but the results
still come from overlapping data. The most direct
comparison is SPIDER’s F-score of 39.7% com-
pared to his LSW03 algorithm’s 35.6% (both are
minimality resolvers). However, our evaluation is
more penalized since SPIDER loses precision for
NER’s false positives (Jack London as a location)
while Leidner only evaluated on actual locations.
It thus seems fair to conclude that the text-driven
classifiers, with F-scores in the mid-50’s, are much
more accurate on the corpus than previous work.
</bodyText>
<sectionHeader confidence="0.997996" genericHeader="method">
6 Error Analysis
</sectionHeader>
<bodyText confidence="0.99991125">
Table 4 shows the ten toponyms that caused the
greatest total error distances from TRC-DEV with
gold toponyms when resolved by TRAWL, the re-
solver that achieves the lowest mean error on that
</bodyText>
<footnote confidence="0.7473175">
7Leidner (2008) reports precision, recall, and F-score val-
ues even with gold toponyms, since his resolvers can abstain.
</footnote>
<bodyText confidence="0.998351901960784">
dataset among all our resolvers.
Washington, the toponym contributing the most
total error, is a typical example of a toponym that
is difficult to resolve, as there are two very promi-
nent locations within the United States with the
name. Choosing one when the other is correct re-
sults in an error of over 4000 kilometers. This oc-
curs, for example, when TRAWL chooses Wash-
ington state in the phrase Israel’s ambassador to
Washington, where more knowledge about the
status of Washington, D.C. as the political cen-
ter of the United States (e.g. in the form of more
or better contextual training instances) could over-
turn the administrative level component’s prefer-
ence for states.
An instance of California in a baseball-related
news article is incorrectly predicted to be the town
California, Pennsylvania. The context is: ...New
York starter Jimmy Key left the game in the first
inning after Seattle shortstop Alex Rodriguez lined
a shot off his left elbow. The Yankees have lost
12 of their last 19 games and their lead in the AL
East over Baltimore fell to five games. At Califor-
nia, Tim Wakefield pitched a six-hitter for his third
complete game of the season and Mo Vaughn and
Troy O’Leary hit solo home runs in the second in-
ning as the surging Boston Red Sox won their third
straight 4-1 over the California Angels. Boston
has won seven of eight and is 20-6... The pres-
ence of many east coast cues—both toponym and
otherwise—make it unsurprising that the resolver
would predict California, Pennsylvania despite the
administrative level component’s heavier weight-
ing of the state.
The average errors for the toponyms Australia
and Russia are fairly small and stem from differ-
ences in how countries are represented across dif-
ferent gazetteers, not true incorrect predictions.
Table 5 shows the toponyms with the great-
est errors from CWAR-DEV with gold toponyms
when resolved by WISTR+SPIDER. Rome is
sometimes predicted as cities in Italy and other
parts of Europe rather than Rome, Georgia, though
it correctly selects the city in Georgia more often
than not due to SPIDER’s preference for tightly
clumped sets of locations. Mexico, however, fre-
quently gets incorrectly selected as a city in Mary-
land near many other locations in the corpus when
TRAWL’s administrative level component is not
present. Many other of the toponyms contributing
to the total error such as Jackson and Lexington are
</bodyText>
<page confidence="0.934092">
1473
</page>
<table confidence="0.999875818181818">
Toponym N Mean Total
Washington 25 3229 80717
Gaza 12 5936 71234
California 8 5475 43797
Montana 3 11635 34905
WA 3 11221 33662
NZ 2 14068 28136
Australia 88 280 24600
Russia 72 260 18712
OR 2 9242 18484
Sydney 12 1422 17067
</table>
<tableCaption confidence="0.6396982">
Table 4: Toponyms with the greatest total error
distances in kilometers from TRC-DEV with gold
toponyms resolved by TRAWL. N is the number
of instances, and the mean error for each toponym
type is also given.
</tableCaption>
<table confidence="0.999891727272727">
Toponym N Mean Total
Mexico 1398 2963 4142102
Jackson 2485 1210 3007541
Monterey 353 2392 844221
Haymarket 41 15663 642170
McMinnville 145 3307 479446
Alexandria 1434 314 450863
Eastport 184 2109 388000
Lexington 796 442 351684
Winton 21 15881 333499
Clinton 170 1401 238241
</table>
<tableCaption confidence="0.870641">
Table 5: Top errors from CWAR-DEV resolved by
TRAWL+SPIDER.
</tableCaption>
<bodyText confidence="0.999520666666667">
simply the result of many American towns sharing
the same names and a lack of clear disambiguating
context.
</bodyText>
<sectionHeader confidence="0.998189" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999965153846154">
Our text-driven resolvers prove highly effective
for both modern day newswire texts and 19th cen-
tury texts pertaining to the Civil War. They eas-
ily outperform standard minimality toponym re-
solvers, but can also be combined with them. This
strategy works particularly well when predicting
toponyms on a corpus with relatively restricted
geographic extents. Performance remains good
when resolving toponyms identified automatically,
indicating that end-to-end systems based on our
models may improve the experience of digital hu-
manities scholars interested in finding and visual-
izing toponyms in large corpora.
</bodyText>
<sectionHeader confidence="0.990993" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999833">
We thank: the three anonymous reviewers, Grant
DeLozier, and the UT Austin Natural Language
Learning reading group, for their helpful feed-
back; Ben Wing, for his document geoloca-
tion software; Jochen Leidner, for providing the
TR-CONLL corpus as well as feedback on earlier
versions of this paper; and Scott Nesbit, for pro-
viding the annotations for the CWAR corpus. This
research was supported by a grant from the Morris
Memorial Trust Fund of the New York Commu-
nity Trust.
</bodyText>
<sectionHeader confidence="0.9584" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.990290323529412">
B. Adams and G. McKenzie. Inferring thematic
places from spatially referenced natural lan-
guage descriptions. Crowdsourcing Geographic
Knowledge, pages 201–221, 2013.
E. Amitay, N. Har’El, R. Sivan, and A. Soffer.
Web-a-Where: geotagging web content. In Pro-
ceedings of the 27th annual international ACM
SIGIR conference on Research and development
in information retrieval, pages 273–280, 2004.
D. Buscaldi and P. Rosso. A conceptual density-
based approach for the disambiguation of to-
ponyms. International Journal of Geographical
Information Science, 22(3):301–313, 2008.
P. Clough. Extracting metadata for spatially-
aware information retrieval on the internet. In
Proceedings of the 2005 workshop on Ge-
ographic information retrieval, pages 25–30.
ACM, 2005.
G. Crane. The Perseus Digital Library, 2000. URL
http://www.perseus.tufts.edu.
J. Ding, L. Gravano, and N. Shivakumar. Comput-
ing geographical scopes of web resources. In
Proceedings of the 26th International Confer-
ence on Very Large Data Bases, pages 545–556,
2000.
J. Eisenstein, B. O’Connor, N. Smith, and E. Xing.
A latent variable model for geographic lexical
variation. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 1277–1287, 2010.
J. Eisenstein, A. Ahmed, and E. Xing. Sparse ad-
ditive generative models of text. In Proceedings
of the 28th International Conference on Ma-
chine Learning, pages 1041–1048, 2011.
</bodyText>
<page confidence="0.994565">
1474
</page>
<reference confidence="0.993631020618557">
J. Gelernter and N. Mushegian. Geo-parsing mes-
sages from microtext. Transactions in GIS, 15
(6):753–773, 2011.
C. Grover, R. Tobin, K. Byrne, M. Woollard,
J. Reid, S. Dunn, and J. Ball. Use of the Ed-
inburgh geoparser for georeferencing digitized
historical collections. Philosophical Transac-
tions of the Royal Society A: Mathematical,
Physical and Engineering Sciences, 368(1925):
3875–3889, 2010.
J. Guldi. The spatial turn. Spatial Humanities: a
Project of the Institute for Enabling, 2009.
Q. Hao, R. Cai, C. Wang, R. Xiao, J. Yang,
Y. Pang, and L. Zhang. Equip tourists with
knowledge mined from travelogues. In Pro-
ceedings of the 19th international conference on
World wide web, pages 401–410, 2010.
B. Hecht, S. Carton, M. Quaderi, J. Sch¨oning,
M. Raubal, D. Gergle, and D. Downey. Ex-
planatory semantic relatedness and explicit spa-
tialization for exploratory search. In Proceed-
ings of the 35th international ACM SIGIR con-
ference on Research and development in infor-
mation retrieval, pages 415–424. ACM, 2012.
L. Hill. Georeferencing: The Geographic Associ-
ations of Information. MIT Press, 2006.
J. Hoffart, M. Yosef, I. Bordino, H. F¨urstenau,
M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and
G. Weikum. Robust disambiguation of named
entities in text. In Proceedings of the Con-
ference on Empirical Methods in Natural Lan-
guage Processing, pages 782–792. Association
for Computational Linguistics, 2011.
L. Hollenstein and R. Purves. Exploring place
through user-generated content: Using Flickr
tags to describe city cores. Journal of Spatial
Information Science, (1):21–48, 2012.
S. Intagorn and K. Lerman. A probabilistic ap-
proach to mining geospatial knowledge from
social annotations. In Conference on Infor-
mation and Knowledge Management (CIKM),
2012.
C. Jones, R. Purves, P. Clough, and H. Joho. Mod-
elling vague places with knowledge from the
web. International Journal of Geographical In-
formation Science, 2008.
S. Kulkarni, A. Singh, G. Ramakrishnan, and
S. Chakrabarti. Collective annotation of
Wikipedia entities in web text. In Proceedings
of the 15th ACM SIGKDD international confer-
ence on Knowledge discovery and data mining,
pages 457–466. ACM, 2009.
S. Ladra, M. Luaces, O. Pedreira, and D. Seco. A
toponym resolution service following the OGC
WPS standard. In Web and Wireless Geograph-
ical Information Systems, volume 5373, pages
75–85. 2008.
J. Leidner. Toponym resolution in text: Anno-
tation, Evaluation and Applications of Spatial
Grounding of Place Names. Universal Press,
Boca Raton, FL, USA, 2008.
H. Li, R. Srihari, C. Niu, and W. Li. InfoXtract lo-
cation normalization: a hybrid approach to geo-
graphic references in information extraction. In
Proceedings of the HLT-NAACL 2003 workshop
on Analysis of geographic references - Volume
1, pages 39–44, 2003.
Y. Li. Probabilistic toponym resolution and geo-
graphic indexing and querying. Master’s thesis,
The University of Melbourne, Melbourne, Aus-
tralia, 2007.
V. Loureiro, I. Anast´acio, and B. Martins. Learn-
ing to resolve geographical and temporal ref-
erences in text. In Proceedings of the 19th
ACM SIGSPATIAL International Conference on
Advances in Geographic Information Systems,
pages 349–352, 2011.
M. Louwerse and N. Benesh. Representing spatial
structure through maps and language: Lord of
the Rings encodes the spatial structure of Mid-
dle Earth. Cognitive science, 36(8):1556–1569,
2012.
I. Mani, C. Doran, D. Harris, J. Hitzeman,
R. Quimby, J. Richer, B. Wellner, S. Mardis,
and S. Clancy. SpatialML: annotation scheme,
resources, and evaluation. Language Resources
and Evaluation, 44(3):263–280, 2010.
S. Overell. Geographic Information Retrieval:
Classification, Disambiguation and Modelling.
PhD thesis, Imperial College London, 2009.
S. Overell and S. R¨uger. Using co-occurrence
models for placename disambiguation. Inter-
national Journal of Geographical Information
Science, 22:265–287, 2008.
Y. Pang, Q. Hao, Y. Yuan, T. Hu, R. Cai, and
L. Zhang. Summarizing tourist destinations
by mining user-generated travelogues and pho-
</reference>
<page confidence="0.808151">
1475
</page>
<reference confidence="0.999733026666667">
tos. Computer Vision and Image Understand-
ing, 115(3):352 – 363, 2011.
V. Petras. Statistical analysis of geographic and
language clues in the MARC record. Technical
report, The University of California at Berkeley,
2004.
T. Qin, R. Xiao, L. Fang, X. Xie, and L. Zhang.
An efficient location extraction algorithm by
leveraging web contextual information. In Pro-
ceedings of the 18th SIGSPATIAL International
Conference on Advances in Geographic Infor-
mation Systems, pages 53–60. ACM, 2010.
E. Rauch, M. Bukatin, and K. Baker. A
confidence-based framework for disambiguat-
ing geographic terms. In Proceedings of the
HLT-NAACL 2003 workshop on Analysis of ge-
ographic references - Volume 1, pages 50–54,
2003.
K. Roberts, C. Bejan, and S. Harabagiu. Toponym
disambiguation using events. In Proceedings of
the 23rd International Florida Artificial Intelli-
gence Research Society Conference, pages 271–
276, 2010.
S. Roller, M. Speriosu, S. Rallapalli, B. Wing, and
J. Baldridge. Supervised text-based geolocation
using language models on an adaptive grid. In
Proceedings of EMNLP 2012, 2012.
J. Sankaranarayanan, H. Samet, B. Teitler,
M. Lieberman, and J. Sperling. TwitterStand:
news in tweets. In Proceedings of the 17th
ACM SIGSPATIAL International Conference on
Advances in Geographic Information Systems,
pages 42–51, 2009.
W. Scheidel, E. Meeks, and J. Weiland. ORBIS:
The Stanford geospatial network model of the
roman world. 2012.
A. Skupin and A. Esperb´e. An alternative map
of the United States based on an n-dimensional
model of geographic space. Journal of Vi-
sual Languages &amp; Computing, 22(4):290–304,
2011.
D. Smith and G. Crane. Disambiguating geo-
graphic names in a historical digital library. In
Proceedings of the 5th European Conference on
Research and Advanced Technology for Digital
Libraries, pages 127–136, 2001.
D. Smith and G. Mann. Bootstrapping toponym
classifiers. In Proceedings of the HLT-NAACL
2003 workshop on Analysis of geographic ref-
erences - Volume 1, pages 45–49, 2003.
B. Teitler, M. Lieberman, D. Panozzo, J. Sankara-
narayanan, H. Samet, and J. Sperling. News-
Stand: a new view on news. In Proceedings of
the 16th ACM SIGSPATIAL international con-
ference on Advances in geographic information
systems, page 18. ACM, 2008.
R. Volz, J. Kleb, and W. Mueller. Towards
ontology-based disambiguation of geographical
identifiers. In Proceedings of the 16th Interna-
tional Conference on World Wide Web, 2007.
B. Wing and J. Baldridge. Simple supervised doc-
ument geolocation with geodesic grids. In Pro-
ceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Hu-
man Language Technologies, pages 955–964,
2011.
Q. Zhang, P. Jin, S. Lin, and L. Yue. Extracting
focused locations for web pages. In Web-Age
Information Management, volume 7142, pages
76–89. 2012.
W. Zong, D. Wu, A. Sun, E. Lim, and D. Goh. On
assigning place names to geography related web
pages. In Proceedings of the 5th ACM/IEEE-
CS joint conference on Digital libraries, pages
354–362, 2005.
</reference>
<page confidence="0.992923">
1476
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.645319">
<title confidence="0.999834">Text-Driven Toponym Resolution using Indirect Supervision</title>
<author confidence="0.999995">Michael Speriosu Jason Baldridge</author>
<affiliation confidence="0.9983195">Department of University of Texas at</affiliation>
<address confidence="0.885008">Austin, TX 78712</address>
<abstract confidence="0.954088235294118">Toponym resolvers identify the specific locations referred to by ambiguous placenames in text. Most resolvers are based on heuristics using spatial relationships between multiple toponyms in a document, or metadata such as population. This paper shows that text-driven disambiguation for toponyms is far more effective. We exploit document-level geotags to indirectly generate training instances for text classifiers for toponym resolution, and show that textual cues can be straightforwardly integrated with other commonly used ones. Results are given for both 19th century texts pertaining to the American Civil War and 20th century newswire articles.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Gelernter</author>
<author>N Mushegian</author>
</authors>
<title>Geo-parsing messages from microtext.</title>
<date>2011</date>
<journal>Transactions in GIS,</journal>
<volume>15</volume>
<pages>6--753</pages>
<contexts>
<context position="2134" citStr="Gelernter and Mushegian, 2011" startWordPosition="314" endWordPosition="317">m resolution is the task of disambiguating toponyms in natural language contexts to geographic locations (Leidner, 2008). It plays an essential role in automated geographic indexing and information retrieval. This is useful for historical research that combines age-old geographic issues like territoriality with modern computational tools (Guldi, 2009), studies of the effect of historically recorded travel costs on the shaping of empires (Scheidel et al., 2012), and systems that convey the geographic content in news articles (Teitler et al., 2008; Sankaranarayanan et al., 2009) and microblogs (Gelernter and Mushegian, 2011). Entity disambiguation systems such as those of Kulkarni et al. (2009) and Hoffart et al. (2011) disambiguate references to people and organizations as well as locations, but these systems do not take into account any features or measures unique to geography such as physical distance. Here we demonstrate the utility of incorporating distance measurements in toponym resolution systems. Most work on toponym resolution relies on heuristics and hand-built rules. Some use simple rules based on information from a gazetteer, such as population or administrative level (city, state, country, etc.), re</context>
</contexts>
<marker>Gelernter, Mushegian, 2011</marker>
<rawString>J. Gelernter and N. Mushegian. Geo-parsing messages from microtext. Transactions in GIS, 15 (6):753–773, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Grover</author>
<author>R Tobin</author>
<author>K Byrne</author>
<author>M Woollard</author>
<author>J Reid</author>
<author>S Dunn</author>
<author>J Ball</author>
</authors>
<title>Use of the Edinburgh geoparser for georeferencing digitized historical collections.</title>
<date>2010</date>
<journal>Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences,</journal>
<volume>368</volume>
<issue>1925</issue>
<pages>3875--3889</pages>
<contexts>
<context position="3233" citStr="Grover et al., 2010" startWordPosition="492" endWordPosition="495"> rules based on information from a gazetteer, such as population or administrative level (city, state, country, etc.), resolving every instance of the same toponym type to the same location regardless of context (Ladra et al., 2008). Others use relationships between multiple toponyms in a context (local or whole document) and look for containment relationships, e.g. London and England occurring in the same paragraph or as the bigram London, England (Li et al., 2003; Amitay et al., 2004; Zong et al., 2005; Clough, 2005; Li, 2007; Volz et al., 2007; Jones et al., 2008; Buscaldi and Rosso, 2008; Grover et al., 2010). Still others first identify unambiguous toponyms and then disambiguate other toponyms based on geopolitical relationships with or distances to the unambiguous ones (Ding et al., 2000). Many favor resolutions of toponyms within a local context or document that cover a smaller geographic area over those that are more dispersed (Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Roberts et al. (2010) use relationships learned between people, organizations, and locations from Wikipedia to aid in toponym resolution when such named entities are pres</context>
<context position="12701" citStr="Grover et al., 2010" startWordPosition="1999" endWordPosition="2002">tion for it as there are many (falsely) false positives. 5opennlp.apache.org 1468 less effective if the population statistics are from a time period different from that of the corpus. 3.2 SPIDER Leidner (2008) describes two general and useful minimality properties of toponyms: • one sense per discourse: multiple tokens of a toponym in the same text generally do not refer to different locations in the same text • spatial minimality: different toponyms in a text tend refer to spatially near locations Many toponym resolvers exploit these (Smith and Crane, 2001; Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Here, we define SPIDER (Spatial Prominence via Iterative Distance Evaluation and Reweighting) as a strong representative of such textually unaware approaches. In addition to capturing both minimality properties, it also identifies the relative prominence of the locations for each toponym in a given corpus. SPIDER resolves each toponym by finding the location for each that minimizes the sum distance to all locations for all other toponyms in the same document. On the first iteration, it tends to select locations that clump spatially: if Paris occurs</context>
</contexts>
<marker>Grover, Tobin, Byrne, Woollard, Reid, Dunn, Ball, 2010</marker>
<rawString>C. Grover, R. Tobin, K. Byrne, M. Woollard, J. Reid, S. Dunn, and J. Ball. Use of the Edinburgh geoparser for georeferencing digitized historical collections. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 368(1925): 3875–3889, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Guldi</author>
</authors>
<title>The spatial turn. Spatial Humanities: a Project of the Institute for Enabling,</title>
<date>2009</date>
<contexts>
<context position="1857" citStr="Guldi, 2009" startWordPosition="271" endWordPosition="272">s to places (coordinates) (Hill, 2006). Allowing for the querying and exploration of knowledge in a geographically informed way requires more powerful tools than a keyword-based search can provide, in part due to the ambiguity of toponyms (placenames). Toponym resolution is the task of disambiguating toponyms in natural language contexts to geographic locations (Leidner, 2008). It plays an essential role in automated geographic indexing and information retrieval. This is useful for historical research that combines age-old geographic issues like territoriality with modern computational tools (Guldi, 2009), studies of the effect of historically recorded travel costs on the shaping of empires (Scheidel et al., 2012), and systems that convey the geographic content in news articles (Teitler et al., 2008; Sankaranarayanan et al., 2009) and microblogs (Gelernter and Mushegian, 2011). Entity disambiguation systems such as those of Kulkarni et al. (2009) and Hoffart et al. (2011) disambiguate references to people and organizations as well as locations, but these systems do not take into account any features or measures unique to geography such as physical distance. Here we demonstrate the utility of i</context>
</contexts>
<marker>Guldi, 2009</marker>
<rawString>J. Guldi. The spatial turn. Spatial Humanities: a Project of the Institute for Enabling, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Hao</author>
<author>R Cai</author>
<author>C Wang</author>
<author>R Xiao</author>
<author>J Yang</author>
<author>Y Pang</author>
<author>L Zhang</author>
</authors>
<title>Equip tourists with knowledge mined from travelogues.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web,</booktitle>
<pages>401--410</pages>
<contexts>
<context position="5034" citStr="Hao et al., 2010" startWordPosition="769" endWordPosition="772">s missing in the context or when a document mentions locations that are neither nearby geographically nor in a geopolitical relationship. There is a clear opportunity that most ignore: use non-toponym textual context. Spatially relevant words like downtown that are not explicit toponyms can be strong cues for resolution (Hollenstein and Purves, 2012). Furthermore, the connection between non-spatial words and locations has been successfully exploited in data-driven approaches to document geolocation (Eisenstein et al., 2010, 2011; Wing and Baldridge, 2011; Roller et al., 2012) and other tasks (Hao et al., 2010; Pang et al., 2011; Intagorn and Lerman, 2012; Hecht et al., 2012; Louwerse and Benesh, 2012; Adams and McKenzie, 2013). In this paper, we learn resolvers that use all words in local or document context. For example, the word lobster appearing near the toponym Portland indicates the location is Portland in Maine rather than Oregon or Michigan. Essentially, we learn a text classifier per toponym. There are no massive collections of toponyms labeled with locations, so we train models indirectly using geotagged Wikipedia articles. Our results show these text classifiers are far more accurate tha</context>
</contexts>
<marker>Hao, Cai, Wang, Xiao, Yang, Pang, Zhang, 2010</marker>
<rawString>Q. Hao, R. Cai, C. Wang, R. Xiao, J. Yang, Y. Pang, and L. Zhang. Equip tourists with knowledge mined from travelogues. In Proceedings of the 19th international conference on World wide web, pages 401–410, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Hecht</author>
<author>S Carton</author>
<author>M Quaderi</author>
<author>J Sch¨oning</author>
<author>M Raubal</author>
<author>D Gergle</author>
<author>D Downey</author>
</authors>
<title>Explanatory semantic relatedness and explicit spatialization for exploratory search.</title>
<date>2012</date>
<booktitle>In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>415--424</pages>
<publisher>ACM,</publisher>
<marker>Hecht, Carton, Quaderi, Sch¨oning, Raubal, Gergle, Downey, 2012</marker>
<rawString>B. Hecht, S. Carton, M. Quaderi, J. Sch¨oning, M. Raubal, D. Gergle, and D. Downey. Explanatory semantic relatedness and explicit spatialization for exploratory search. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 415–424. ACM, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hill</author>
</authors>
<title>Georeferencing: The Geographic Associations of Information.</title>
<date>2006</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>782--792</pages>
<publisher>MIT Press,</publisher>
<contexts>
<context position="1283" citStr="Hill, 2006" startWordPosition="186" endWordPosition="187">ues can be straightforwardly integrated with other commonly used ones. Results are given for both 19th century texts pertaining to the American Civil War and 20th century newswire articles. 1 Introduction It has been estimated that at least half of the world’s stored knowledge, both printed and digital, has geographic relevance, and that geographic information pervades many more aspects of humanity than previously thought (Petras, 2004; Skupin and Esperb´e, 2011). Thus, there is value in connecting linguistic references to places (e.g. placenames) to formal references to places (coordinates) (Hill, 2006). Allowing for the querying and exploration of knowledge in a geographically informed way requires more powerful tools than a keyword-based search can provide, in part due to the ambiguity of toponyms (placenames). Toponym resolution is the task of disambiguating toponyms in natural language contexts to geographic locations (Leidner, 2008). It plays an essential role in automated geographic indexing and information retrieval. This is useful for historical research that combines age-old geographic issues like territoriality with modern computational tools (Guldi, 2009), studies of the effect of</context>
</contexts>
<marker>Hill, 2006</marker>
<rawString>L. Hill. Georeferencing: The Geographic Associations of Information. MIT Press, 2006. J. Hoffart, M. Yosef, I. Bordino, H. F¨urstenau, M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and G. Weikum. Robust disambiguation of named entities in text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 782–792. Association for Computational Linguistics, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hollenstein</author>
<author>R Purves</author>
</authors>
<title>Exploring place through user-generated content: Using Flickr tags to describe city cores.</title>
<date>2012</date>
<journal>Journal of Spatial Information Science,</journal>
<volume>1</volume>
<contexts>
<context position="4770" citStr="Hollenstein and Purves, 2012" startWordPosition="729" endWordPosition="733">y primarily on spatial relationships and metadata about locations (e.g., population). As such, they often require nearby toponyms (including unambiguous or containing toponyms) to resolve ambiguous ones. This reliance can result in poor coverage when the required information is missing in the context or when a document mentions locations that are neither nearby geographically nor in a geopolitical relationship. There is a clear opportunity that most ignore: use non-toponym textual context. Spatially relevant words like downtown that are not explicit toponyms can be strong cues for resolution (Hollenstein and Purves, 2012). Furthermore, the connection between non-spatial words and locations has been successfully exploited in data-driven approaches to document geolocation (Eisenstein et al., 2010, 2011; Wing and Baldridge, 2011; Roller et al., 2012) and other tasks (Hao et al., 2010; Pang et al., 2011; Intagorn and Lerman, 2012; Hecht et al., 2012; Louwerse and Benesh, 2012; Adams and McKenzie, 2013). In this paper, we learn resolvers that use all words in local or document context. For example, the word lobster appearing near the toponym Portland indicates the location is Portland in Maine rather than Oregon or</context>
</contexts>
<marker>Hollenstein, Purves, 2012</marker>
<rawString>L. Hollenstein and R. Purves. Exploring place through user-generated content: Using Flickr tags to describe city cores. Journal of Spatial Information Science, (1):21–48, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Intagorn</author>
<author>K Lerman</author>
</authors>
<title>A probabilistic approach to mining geospatial knowledge from social annotations.</title>
<date>2012</date>
<booktitle>In Conference on Information and Knowledge Management (CIKM),</booktitle>
<contexts>
<context position="5080" citStr="Intagorn and Lerman, 2012" startWordPosition="777" endWordPosition="780">cument mentions locations that are neither nearby geographically nor in a geopolitical relationship. There is a clear opportunity that most ignore: use non-toponym textual context. Spatially relevant words like downtown that are not explicit toponyms can be strong cues for resolution (Hollenstein and Purves, 2012). Furthermore, the connection between non-spatial words and locations has been successfully exploited in data-driven approaches to document geolocation (Eisenstein et al., 2010, 2011; Wing and Baldridge, 2011; Roller et al., 2012) and other tasks (Hao et al., 2010; Pang et al., 2011; Intagorn and Lerman, 2012; Hecht et al., 2012; Louwerse and Benesh, 2012; Adams and McKenzie, 2013). In this paper, we learn resolvers that use all words in local or document context. For example, the word lobster appearing near the toponym Portland indicates the location is Portland in Maine rather than Oregon or Michigan. Essentially, we learn a text classifier per toponym. There are no massive collections of toponyms labeled with locations, so we train models indirectly using geotagged Wikipedia articles. Our results show these text classifiers are far more accurate than algorithms based on spatial proximity or met</context>
</contexts>
<marker>Intagorn, Lerman, 2012</marker>
<rawString>S. Intagorn and K. Lerman. A probabilistic approach to mining geospatial knowledge from social annotations. In Conference on Information and Knowledge Management (CIKM), 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Jones</author>
<author>R Purves</author>
<author>P Clough</author>
<author>H Joho</author>
</authors>
<title>Modelling vague places with knowledge from the web.</title>
<date>2008</date>
<journal>International Journal of Geographical Information Science,</journal>
<contexts>
<context position="3185" citStr="Jones et al., 2008" startWordPosition="484" endWordPosition="487">uristics and hand-built rules. Some use simple rules based on information from a gazetteer, such as population or administrative level (city, state, country, etc.), resolving every instance of the same toponym type to the same location regardless of context (Ladra et al., 2008). Others use relationships between multiple toponyms in a context (local or whole document) and look for containment relationships, e.g. London and England occurring in the same paragraph or as the bigram London, England (Li et al., 2003; Amitay et al., 2004; Zong et al., 2005; Clough, 2005; Li, 2007; Volz et al., 2007; Jones et al., 2008; Buscaldi and Rosso, 2008; Grover et al., 2010). Still others first identify unambiguous toponyms and then disambiguate other toponyms based on geopolitical relationships with or distances to the unambiguous ones (Ding et al., 2000). Many favor resolutions of toponyms within a local context or document that cover a smaller geographic area over those that are more dispersed (Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Roberts et al. (2010) use relationships learned between people, organizations, and locations from Wikipedia to aid in topo</context>
</contexts>
<marker>Jones, Purves, Clough, Joho, 2008</marker>
<rawString>C. Jones, R. Purves, P. Clough, and H. Joho. Modelling vague places with knowledge from the web. International Journal of Geographical Information Science, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kulkarni</author>
<author>A Singh</author>
<author>G Ramakrishnan</author>
<author>S Chakrabarti</author>
</authors>
<title>Collective annotation of Wikipedia entities in web text.</title>
<date>2009</date>
<booktitle>In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>457--466</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="2205" citStr="Kulkarni et al. (2009)" startWordPosition="325" endWordPosition="328">s to geographic locations (Leidner, 2008). It plays an essential role in automated geographic indexing and information retrieval. This is useful for historical research that combines age-old geographic issues like territoriality with modern computational tools (Guldi, 2009), studies of the effect of historically recorded travel costs on the shaping of empires (Scheidel et al., 2012), and systems that convey the geographic content in news articles (Teitler et al., 2008; Sankaranarayanan et al., 2009) and microblogs (Gelernter and Mushegian, 2011). Entity disambiguation systems such as those of Kulkarni et al. (2009) and Hoffart et al. (2011) disambiguate references to people and organizations as well as locations, but these systems do not take into account any features or measures unique to geography such as physical distance. Here we demonstrate the utility of incorporating distance measurements in toponym resolution systems. Most work on toponym resolution relies on heuristics and hand-built rules. Some use simple rules based on information from a gazetteer, such as population or administrative level (city, state, country, etc.), resolving every instance of the same toponym type to the same location re</context>
</contexts>
<marker>Kulkarni, Singh, Ramakrishnan, Chakrabarti, 2009</marker>
<rawString>S. Kulkarni, A. Singh, G. Ramakrishnan, and S. Chakrabarti. Collective annotation of Wikipedia entities in web text. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 457–466. ACM, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ladra</author>
<author>M Luaces</author>
<author>O Pedreira</author>
<author>D Seco</author>
</authors>
<title>A toponym resolution service following the OGC WPS standard.</title>
<date>2008</date>
<booktitle>In Web and Wireless Geographical Information Systems,</booktitle>
<volume>5373</volume>
<pages>75--85</pages>
<contexts>
<context position="2845" citStr="Ladra et al., 2008" startWordPosition="425" endWordPosition="428">2011) disambiguate references to people and organizations as well as locations, but these systems do not take into account any features or measures unique to geography such as physical distance. Here we demonstrate the utility of incorporating distance measurements in toponym resolution systems. Most work on toponym resolution relies on heuristics and hand-built rules. Some use simple rules based on information from a gazetteer, such as population or administrative level (city, state, country, etc.), resolving every instance of the same toponym type to the same location regardless of context (Ladra et al., 2008). Others use relationships between multiple toponyms in a context (local or whole document) and look for containment relationships, e.g. London and England occurring in the same paragraph or as the bigram London, England (Li et al., 2003; Amitay et al., 2004; Zong et al., 2005; Clough, 2005; Li, 2007; Volz et al., 2007; Jones et al., 2008; Buscaldi and Rosso, 2008; Grover et al., 2010). Still others first identify unambiguous toponyms and then disambiguate other toponyms based on geopolitical relationships with or distances to the unambiguous ones (Ding et al., 2000). Many favor resolutions of</context>
</contexts>
<marker>Ladra, Luaces, Pedreira, Seco, 2008</marker>
<rawString>S. Ladra, M. Luaces, O. Pedreira, and D. Seco. A toponym resolution service following the OGC WPS standard. In Web and Wireless Geographical Information Systems, volume 5373, pages 75–85. 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Leidner</author>
</authors>
<title>Toponym resolution in text: Annotation, Evaluation and Applications of Spatial Grounding of Place Names.</title>
<date>2008</date>
<publisher>Universal Press,</publisher>
<location>Boca Raton, FL, USA,</location>
<contexts>
<context position="1624" citStr="Leidner, 2008" startWordPosition="237" endWordPosition="238">t geographic information pervades many more aspects of humanity than previously thought (Petras, 2004; Skupin and Esperb´e, 2011). Thus, there is value in connecting linguistic references to places (e.g. placenames) to formal references to places (coordinates) (Hill, 2006). Allowing for the querying and exploration of knowledge in a geographically informed way requires more powerful tools than a keyword-based search can provide, in part due to the ambiguity of toponyms (placenames). Toponym resolution is the task of disambiguating toponyms in natural language contexts to geographic locations (Leidner, 2008). It plays an essential role in automated geographic indexing and information retrieval. This is useful for historical research that combines age-old geographic issues like territoriality with modern computational tools (Guldi, 2009), studies of the effect of historically recorded travel costs on the shaping of empires (Scheidel et al., 2012), and systems that convey the geographic content in news articles (Teitler et al., 2008; Sankaranarayanan et al., 2009) and microblogs (Gelernter and Mushegian, 2011). Entity disambiguation systems such as those of Kulkarni et al. (2009) and Hoffart et al.</context>
<context position="3596" citStr="Leidner, 2008" startWordPosition="551" endWordPosition="552">ondon and England occurring in the same paragraph or as the bigram London, England (Li et al., 2003; Amitay et al., 2004; Zong et al., 2005; Clough, 2005; Li, 2007; Volz et al., 2007; Jones et al., 2008; Buscaldi and Rosso, 2008; Grover et al., 2010). Still others first identify unambiguous toponyms and then disambiguate other toponyms based on geopolitical relationships with or distances to the unambiguous ones (Ding et al., 2000). Many favor resolutions of toponyms within a local context or document that cover a smaller geographic area over those that are more dispersed (Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Roberts et al. (2010) use relationships learned between people, organizations, and locations from Wikipedia to aid in toponym resolution when such named entities are present, but do not exploit any other textual context. 1466 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1466–1476, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Most of these approaches suffer from a major weakness: they rely primarily on spatial relationships and metadata about</context>
<context position="9132" citStr="Leidner, 2008" startWordPosition="1426" endWordPosition="1427"> for evaluation. Columns subscripted by top give figures for toponyms. The last two columns give the average number of candidate locations per toponym token and the number of candidate locations for the most ambiguous toponym. A location for present purposes is thus a set of points on the earth’s surface. The distance between two locations is computed as the great circle distance between the closest pair of representative points, one from each location. 2.2 Toponym Resolution Corpora We need corpora with toponyms identified and resolved by human annotators for evaluation. The TR-CONLL corpus (Leidner, 2008) contains 946 REUTERS news articles published in August 1996. It has about 204,000 words and articles range in length from a few hundred words to several thousand words. Each toponym in the corpus was identified and resolved by hand.3 We place every third article into a test portion (TRC-TEST) and the rest in a development portion. Since our methods do not learn from explicitly labeled toponyms, we do not need a training set. The Perseus Civil War and 19th Century American Collection (CWAR) contains 341 books (58 million words) written primarily about and during the American Civil War (Crane, </context>
<context position="12291" citStr="Leidner (2008)" startWordPosition="1934" endWordPosition="1935">. POPULATION The POPULATION resolver selects the location with the greatest population for each toponym. It is generally quite effective, but when a toponym has several locations with large populations, it is often wrong. Also, it can only be used when such information is available, and it is 4States and countries are not annotated in CWAR, so we do not evaluate end-to-end using NER plus toponym resolution for it as there are many (falsely) false positives. 5opennlp.apache.org 1468 less effective if the population statistics are from a time period different from that of the corpus. 3.2 SPIDER Leidner (2008) describes two general and useful minimality properties of toponyms: • one sense per discourse: multiple tokens of a toponym in the same text generally do not refer to different locations in the same text • spatial minimality: different toponyms in a text tend refer to spatially near locations Many toponym resolvers exploit these (Smith and Crane, 2001; Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Here, we define SPIDER (Spatial Prominence via Iterative Distance Evaluation and Reweighting) as a strong representative of such textually unawa</context>
<context position="23085" citStr="Leidner, 2008" startWordPosition="3710" endWordPosition="3712">ial to divide documents into smaller subdocuments in order to get a better estimate of the overall geographic prominence of the text surrounding a toponym, but at a more coarsegrained level than the local context models provide. For these reasons, we simply divide each book in the CWAR corpus into small subdocuments of at most 20 sentences. 4 Evaluation 5 Results is the same as the gold location. Such a metric can be problematic, however. The gazetteer used by a resolver may not contain, for a given toponym, a location whose latitude and longitude exactly match the gold label for the toponym (Leidner, 2008). Also, some errors are worse than others, e.g. predicting a toponym’s location to be on the other side of the world versus predicting it to be a different city in the same country—accuracy does not reflect this difference. We choose a metric that instead measures the distance between the correct and predicted location for each toponym and compute the mean and median of all such error distances. This is used in document geolocation work (Eisenstein et al., 2010, 2011; Wing and Baldridge, 2011; Roller et al., 2012) and is related to the root mean squared distance metric discussed by Leidner (20</context>
<context position="29436" citStr="Leidner (2008)" startWordPosition="4762" endWordPosition="4764">e extracted from common toponyms than from rare ones in Wikipedia. Thus, our model that uses only these local context models does best when running on NER-identified toponyms. We also measured the mean and median error distance for toponyms correctly identified by the named entity recognizer, and found that they tended to be 50-200km worse than for gold toponyms. This also makes sense given the named entity recognizer’s tendency to detect common toponyms: common toponyms tend to be more ambiguous than others. Results on TR-CONLL indicate much higher performance than the resolvers presented by Leidner (2008), whose F-scores do not exceed 36.5% with either gold or NER toponyms.7 TRC-TEST is a subset of the documents Leidner uses (he did not split development and test data), but the results still come from overlapping data. The most direct comparison is SPIDER’s F-score of 39.7% compared to his LSW03 algorithm’s 35.6% (both are minimality resolvers). However, our evaluation is more penalized since SPIDER loses precision for NER’s false positives (Jack London as a location) while Leidner only evaluated on actual locations. It thus seems fair to conclude that the text-driven classifiers, with F-score</context>
</contexts>
<marker>Leidner, 2008</marker>
<rawString>J. Leidner. Toponym resolution in text: Annotation, Evaluation and Applications of Spatial Grounding of Place Names. Universal Press, Boca Raton, FL, USA, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>R Srihari</author>
<author>C Niu</author>
<author>W Li</author>
</authors>
<title>InfoXtract location normalization: a hybrid approach to geographic references in information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003 workshop on Analysis of geographic references -</booktitle>
<volume>1</volume>
<pages>39--44</pages>
<contexts>
<context position="3082" citStr="Li et al., 2003" startWordPosition="464" endWordPosition="467">ng distance measurements in toponym resolution systems. Most work on toponym resolution relies on heuristics and hand-built rules. Some use simple rules based on information from a gazetteer, such as population or administrative level (city, state, country, etc.), resolving every instance of the same toponym type to the same location regardless of context (Ladra et al., 2008). Others use relationships between multiple toponyms in a context (local or whole document) and look for containment relationships, e.g. London and England occurring in the same paragraph or as the bigram London, England (Li et al., 2003; Amitay et al., 2004; Zong et al., 2005; Clough, 2005; Li, 2007; Volz et al., 2007; Jones et al., 2008; Buscaldi and Rosso, 2008; Grover et al., 2010). Still others first identify unambiguous toponyms and then disambiguate other toponyms based on geopolitical relationships with or distances to the unambiguous ones (Ding et al., 2000). Many favor resolutions of toponyms within a local context or document that cover a smaller geographic area over those that are more dispersed (Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Roberts et al. (201</context>
</contexts>
<marker>Li, Srihari, Niu, Li, 2003</marker>
<rawString>H. Li, R. Srihari, C. Niu, and W. Li. InfoXtract location normalization: a hybrid approach to geographic references in information extraction. In Proceedings of the HLT-NAACL 2003 workshop on Analysis of geographic references - Volume 1, pages 39–44, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Li</author>
</authors>
<title>Probabilistic toponym resolution and geographic indexing and querying.</title>
<date>2007</date>
<tech>Master’s thesis,</tech>
<institution>The University of Melbourne,</institution>
<location>Melbourne, Australia,</location>
<contexts>
<context position="3146" citStr="Li, 2007" startWordPosition="478" endWordPosition="479">ponym resolution relies on heuristics and hand-built rules. Some use simple rules based on information from a gazetteer, such as population or administrative level (city, state, country, etc.), resolving every instance of the same toponym type to the same location regardless of context (Ladra et al., 2008). Others use relationships between multiple toponyms in a context (local or whole document) and look for containment relationships, e.g. London and England occurring in the same paragraph or as the bigram London, England (Li et al., 2003; Amitay et al., 2004; Zong et al., 2005; Clough, 2005; Li, 2007; Volz et al., 2007; Jones et al., 2008; Buscaldi and Rosso, 2008; Grover et al., 2010). Still others first identify unambiguous toponyms and then disambiguate other toponyms based on geopolitical relationships with or distances to the unambiguous ones (Ding et al., 2000). Many favor resolutions of toponyms within a local context or document that cover a smaller geographic area over those that are more dispersed (Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Roberts et al. (2010) use relationships learned between people, organizations, and </context>
<context position="18459" citStr="Li (2007)" startWordPosition="2944" endWordPosition="2945">k in the document is considered a reference to New York City. Next, context windows w of twenty words to each side of each toponym are extracted as features. The label for a training instance is the candidate location closest to the document location. We extract 1,489,428 such instances for toponyms relevant to our evaluation corpora. These instances are used to train logistic regression classifiers P(l|t, w) for location l and toponym t. To disambiguate a new toponym, WISTR chooses the location that maximizes this probability. Few such probabilistic toponym resolvers exist in the literature. Li (2007) builds a probability distribution over locations for each toponym, but still relies on nearby toponyms that could refer to regions that contain that toponym and requires hand construction of distributions. Other learning approaches to toponym resolution (e.g. Smith and Mann (2003)) require explicit unambiguous mentions like Portland, Maine to construct training instances, while our data gathering methodology does not make such an assumption. Overell and R¨uger (2008) and Overell (2009) only use nearby toponyms as features. Mani et al. (2010) and Qin et al. (2010) use other word types but only</context>
</contexts>
<marker>Li, 2007</marker>
<rawString>Y. Li. Probabilistic toponym resolution and geographic indexing and querying. Master’s thesis, The University of Melbourne, Melbourne, Australia, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Loureiro</author>
<author>I Anast´acio</author>
<author>B Martins</author>
</authors>
<title>Learning to resolve geographical and temporal references in text.</title>
<date>2011</date>
<booktitle>In Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems,</booktitle>
<pages>349--352</pages>
<marker>Loureiro, Anast´acio, Martins, 2011</marker>
<rawString>V. Loureiro, I. Anast´acio, and B. Martins. Learning to resolve geographical and temporal references in text. In Proceedings of the 19th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, pages 349–352, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Louwerse</author>
<author>N Benesh</author>
</authors>
<title>Representing spatial structure through maps and language: Lord of the Rings encodes the spatial structure of Middle Earth.</title>
<date>2012</date>
<journal>Cognitive science,</journal>
<volume>36</volume>
<issue>8</issue>
<contexts>
<context position="5127" citStr="Louwerse and Benesh, 2012" startWordPosition="785" endWordPosition="788">by geographically nor in a geopolitical relationship. There is a clear opportunity that most ignore: use non-toponym textual context. Spatially relevant words like downtown that are not explicit toponyms can be strong cues for resolution (Hollenstein and Purves, 2012). Furthermore, the connection between non-spatial words and locations has been successfully exploited in data-driven approaches to document geolocation (Eisenstein et al., 2010, 2011; Wing and Baldridge, 2011; Roller et al., 2012) and other tasks (Hao et al., 2010; Pang et al., 2011; Intagorn and Lerman, 2012; Hecht et al., 2012; Louwerse and Benesh, 2012; Adams and McKenzie, 2013). In this paper, we learn resolvers that use all words in local or document context. For example, the word lobster appearing near the toponym Portland indicates the location is Portland in Maine rather than Oregon or Michigan. Essentially, we learn a text classifier per toponym. There are no massive collections of toponyms labeled with locations, so we train models indirectly using geotagged Wikipedia articles. Our results show these text classifiers are far more accurate than algorithms based on spatial proximity or metadata. Furthermore, they are straightforward to</context>
</contexts>
<marker>Louwerse, Benesh, 2012</marker>
<rawString>M. Louwerse and N. Benesh. Representing spatial structure through maps and language: Lord of the Rings encodes the spatial structure of Middle Earth. Cognitive science, 36(8):1556–1569, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>C Doran</author>
<author>D Harris</author>
<author>J Hitzeman</author>
<author>R Quimby</author>
<author>J Richer</author>
<author>B Wellner</author>
<author>S Mardis</author>
<author>S Clancy</author>
</authors>
<title>SpatialML: annotation scheme, resources, and evaluation.</title>
<date>2010</date>
<journal>Language Resources and Evaluation,</journal>
<volume>44</volume>
<issue>3</issue>
<contexts>
<context position="19007" citStr="Mani et al. (2010)" startWordPosition="3026" endWordPosition="3029">uch probabilistic toponym resolvers exist in the literature. Li (2007) builds a probability distribution over locations for each toponym, but still relies on nearby toponyms that could refer to regions that contain that toponym and requires hand construction of distributions. Other learning approaches to toponym resolution (e.g. Smith and Mann (2003)) require explicit unambiguous mentions like Portland, Maine to construct training instances, while our data gathering methodology does not make such an assumption. Overell and R¨uger (2008) and Overell (2009) only use nearby toponyms as features. Mani et al. (2010) and Qin et al. (2010) use other word types but only in a local context, and they require toponymlabeled training data. Our approach makes use of all words in local and document context and requires no explicitly labeled toponym tokens. TRAWL We bring TRIPDL, WISTR, and standard toponym resolution cues about administrative levels together with TRAWL (Toponym Resolution via Administrative levels and Wikipedia Locations). The general form of a probabilistic resolver that utilizes such information to select a location lˆ for a toponym t in document d may be defined as lˆ = arg maxl P(l, al|t, d).</context>
</contexts>
<marker>Mani, Doran, Harris, Hitzeman, Quimby, Richer, Wellner, Mardis, Clancy, 2010</marker>
<rawString>I. Mani, C. Doran, D. Harris, J. Hitzeman, R. Quimby, J. Richer, B. Wellner, S. Mardis, and S. Clancy. SpatialML: annotation scheme, resources, and evaluation. Language Resources and Evaluation, 44(3):263–280, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Overell</author>
</authors>
<title>Geographic Information Retrieval: Classification, Disambiguation and Modelling. PhD thesis,</title>
<date>2009</date>
<location>Imperial College London,</location>
<contexts>
<context position="18950" citStr="Overell (2009)" startWordPosition="3018" endWordPosition="3019">s the location that maximizes this probability. Few such probabilistic toponym resolvers exist in the literature. Li (2007) builds a probability distribution over locations for each toponym, but still relies on nearby toponyms that could refer to regions that contain that toponym and requires hand construction of distributions. Other learning approaches to toponym resolution (e.g. Smith and Mann (2003)) require explicit unambiguous mentions like Portland, Maine to construct training instances, while our data gathering methodology does not make such an assumption. Overell and R¨uger (2008) and Overell (2009) only use nearby toponyms as features. Mani et al. (2010) and Qin et al. (2010) use other word types but only in a local context, and they require toponymlabeled training data. Our approach makes use of all words in local and document context and requires no explicitly labeled toponym tokens. TRAWL We bring TRIPDL, WISTR, and standard toponym resolution cues about administrative levels together with TRAWL (Toponym Resolution via Administrative levels and Wikipedia Locations). The general form of a probabilistic resolver that utilizes such information to select a location lˆ for a toponym t in </context>
</contexts>
<marker>Overell, 2009</marker>
<rawString>S. Overell. Geographic Information Retrieval: Classification, Disambiguation and Modelling. PhD thesis, Imperial College London, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Overell</author>
<author>S R¨uger</author>
</authors>
<title>Using co-occurrence models for placename disambiguation.</title>
<date>2008</date>
<journal>International Journal of Geographical Information Science,</journal>
<volume>22</volume>
<marker>Overell, R¨uger, 2008</marker>
<rawString>S. Overell and S. R¨uger. Using co-occurrence models for placename disambiguation. International Journal of Geographical Information Science, 22:265–287, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Pang</author>
<author>Q Hao</author>
<author>Y Yuan</author>
<author>T Hu</author>
<author>R Cai</author>
<author>L Zhang</author>
</authors>
<title>Summarizing tourist destinations by mining user-generated travelogues and photos.</title>
<date>2011</date>
<journal>Computer Vision and Image Understanding,</journal>
<volume>115</volume>
<issue>3</issue>
<contexts>
<context position="5053" citStr="Pang et al., 2011" startWordPosition="773" endWordPosition="776">ontext or when a document mentions locations that are neither nearby geographically nor in a geopolitical relationship. There is a clear opportunity that most ignore: use non-toponym textual context. Spatially relevant words like downtown that are not explicit toponyms can be strong cues for resolution (Hollenstein and Purves, 2012). Furthermore, the connection between non-spatial words and locations has been successfully exploited in data-driven approaches to document geolocation (Eisenstein et al., 2010, 2011; Wing and Baldridge, 2011; Roller et al., 2012) and other tasks (Hao et al., 2010; Pang et al., 2011; Intagorn and Lerman, 2012; Hecht et al., 2012; Louwerse and Benesh, 2012; Adams and McKenzie, 2013). In this paper, we learn resolvers that use all words in local or document context. For example, the word lobster appearing near the toponym Portland indicates the location is Portland in Maine rather than Oregon or Michigan. Essentially, we learn a text classifier per toponym. There are no massive collections of toponyms labeled with locations, so we train models indirectly using geotagged Wikipedia articles. Our results show these text classifiers are far more accurate than algorithms based </context>
</contexts>
<marker>Pang, Hao, Yuan, Hu, Cai, Zhang, 2011</marker>
<rawString>Y. Pang, Q. Hao, Y. Yuan, T. Hu, R. Cai, and L. Zhang. Summarizing tourist destinations by mining user-generated travelogues and photos. Computer Vision and Image Understanding, 115(3):352 – 363, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Petras</author>
</authors>
<title>Statistical analysis of geographic and language clues in the MARC record.</title>
<date>2004</date>
<tech>Technical report,</tech>
<institution>The University of California at Berkeley,</institution>
<contexts>
<context position="1111" citStr="Petras, 2004" startWordPosition="161" endWordPosition="162">onyms is far more effective. We exploit document-level geotags to indirectly generate training instances for text classifiers for toponym resolution, and show that textual cues can be straightforwardly integrated with other commonly used ones. Results are given for both 19th century texts pertaining to the American Civil War and 20th century newswire articles. 1 Introduction It has been estimated that at least half of the world’s stored knowledge, both printed and digital, has geographic relevance, and that geographic information pervades many more aspects of humanity than previously thought (Petras, 2004; Skupin and Esperb´e, 2011). Thus, there is value in connecting linguistic references to places (e.g. placenames) to formal references to places (coordinates) (Hill, 2006). Allowing for the querying and exploration of knowledge in a geographically informed way requires more powerful tools than a keyword-based search can provide, in part due to the ambiguity of toponyms (placenames). Toponym resolution is the task of disambiguating toponyms in natural language contexts to geographic locations (Leidner, 2008). It plays an essential role in automated geographic indexing and information retrieval</context>
</contexts>
<marker>Petras, 2004</marker>
<rawString>V. Petras. Statistical analysis of geographic and language clues in the MARC record. Technical report, The University of California at Berkeley, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Qin</author>
<author>R Xiao</author>
<author>L Fang</author>
<author>X Xie</author>
<author>L Zhang</author>
</authors>
<title>An efficient location extraction algorithm by leveraging web contextual information.</title>
<date>2010</date>
<booktitle>In Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems,</booktitle>
<pages>53--60</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="19029" citStr="Qin et al. (2010)" startWordPosition="3031" endWordPosition="3034">ym resolvers exist in the literature. Li (2007) builds a probability distribution over locations for each toponym, but still relies on nearby toponyms that could refer to regions that contain that toponym and requires hand construction of distributions. Other learning approaches to toponym resolution (e.g. Smith and Mann (2003)) require explicit unambiguous mentions like Portland, Maine to construct training instances, while our data gathering methodology does not make such an assumption. Overell and R¨uger (2008) and Overell (2009) only use nearby toponyms as features. Mani et al. (2010) and Qin et al. (2010) use other word types but only in a local context, and they require toponymlabeled training data. Our approach makes use of all words in local and document context and requires no explicitly labeled toponym tokens. TRAWL We bring TRIPDL, WISTR, and standard toponym resolution cues about administrative levels together with TRAWL (Toponym Resolution via Administrative levels and Wikipedia Locations). The general form of a probabilistic resolver that utilizes such information to select a location lˆ for a toponym t in document d may be defined as lˆ = arg maxl P(l, al|t, d). where al is the admin</context>
</contexts>
<marker>Qin, Xiao, Fang, Xie, Zhang, 2010</marker>
<rawString>T. Qin, R. Xiao, L. Fang, X. Xie, and L. Zhang. An efficient location extraction algorithm by leveraging web contextual information. In Proceedings of the 18th SIGSPATIAL International Conference on Advances in Geographic Information Systems, pages 53–60. ACM, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Rauch</author>
<author>M Bukatin</author>
<author>K Baker</author>
</authors>
<title>A confidence-based framework for disambiguating geographic terms.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003 workshop on Analysis of geographic references -</booktitle>
<volume>1</volume>
<pages>50--54</pages>
<contexts>
<context position="3581" citStr="Rauch et al., 2003" startWordPosition="547" endWordPosition="550">elationships, e.g. London and England occurring in the same paragraph or as the bigram London, England (Li et al., 2003; Amitay et al., 2004; Zong et al., 2005; Clough, 2005; Li, 2007; Volz et al., 2007; Jones et al., 2008; Buscaldi and Rosso, 2008; Grover et al., 2010). Still others first identify unambiguous toponyms and then disambiguate other toponyms based on geopolitical relationships with or distances to the unambiguous ones (Ding et al., 2000). Many favor resolutions of toponyms within a local context or document that cover a smaller geographic area over those that are more dispersed (Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Roberts et al. (2010) use relationships learned between people, organizations, and locations from Wikipedia to aid in toponym resolution when such named entities are present, but do not exploit any other textual context. 1466 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1466–1476, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Most of these approaches suffer from a major weakness: they rely primarily on spatial relationships and</context>
<context position="12665" citStr="Rauch et al., 2003" startWordPosition="1993" endWordPosition="1996">o-end using NER plus toponym resolution for it as there are many (falsely) false positives. 5opennlp.apache.org 1468 less effective if the population statistics are from a time period different from that of the corpus. 3.2 SPIDER Leidner (2008) describes two general and useful minimality properties of toponyms: • one sense per discourse: multiple tokens of a toponym in the same text generally do not refer to different locations in the same text • spatial minimality: different toponyms in a text tend refer to spatially near locations Many toponym resolvers exploit these (Smith and Crane, 2001; Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Here, we define SPIDER (Spatial Prominence via Iterative Distance Evaluation and Reweighting) as a strong representative of such textually unaware approaches. In addition to capturing both minimality properties, it also identifies the relative prominence of the locations for each toponym in a given corpus. SPIDER resolves each toponym by finding the location for each that minimizes the sum distance to all locations for all other toponyms in the same document. On the first iteration, it tends to select locations t</context>
</contexts>
<marker>Rauch, Bukatin, Baker, 2003</marker>
<rawString>E. Rauch, M. Bukatin, and K. Baker. A confidence-based framework for disambiguating geographic terms. In Proceedings of the HLT-NAACL 2003 workshop on Analysis of geographic references - Volume 1, pages 50–54, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Roberts</author>
<author>C Bejan</author>
<author>S Harabagiu</author>
</authors>
<title>Toponym disambiguation using events.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Florida Artificial Intelligence Research Society Conference,</booktitle>
<pages>271--276</pages>
<contexts>
<context position="3684" citStr="Roberts et al. (2010)" startWordPosition="565" endWordPosition="568">nd (Li et al., 2003; Amitay et al., 2004; Zong et al., 2005; Clough, 2005; Li, 2007; Volz et al., 2007; Jones et al., 2008; Buscaldi and Rosso, 2008; Grover et al., 2010). Still others first identify unambiguous toponyms and then disambiguate other toponyms based on geopolitical relationships with or distances to the unambiguous ones (Ding et al., 2000). Many favor resolutions of toponyms within a local context or document that cover a smaller geographic area over those that are more dispersed (Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Roberts et al. (2010) use relationships learned between people, organizations, and locations from Wikipedia to aid in toponym resolution when such named entities are present, but do not exploit any other textual context. 1466 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1466–1476, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Most of these approaches suffer from a major weakness: they rely primarily on spatial relationships and metadata about locations (e.g., population). As such, they often require nearby toponyms (including un</context>
</contexts>
<marker>Roberts, Bejan, Harabagiu, 2010</marker>
<rawString>K. Roberts, C. Bejan, and S. Harabagiu. Toponym disambiguation using events. In Proceedings of the 23rd International Florida Artificial Intelligence Research Society Conference, pages 271– 276, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Roller</author>
<author>M Speriosu</author>
<author>S Rallapalli</author>
<author>B Wing</author>
<author>J Baldridge</author>
</authors>
<title>Supervised text-based geolocation using language models on an adaptive grid.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP 2012,</booktitle>
<contexts>
<context position="5000" citStr="Roller et al., 2012" startWordPosition="762" endWordPosition="765">verage when the required information is missing in the context or when a document mentions locations that are neither nearby geographically nor in a geopolitical relationship. There is a clear opportunity that most ignore: use non-toponym textual context. Spatially relevant words like downtown that are not explicit toponyms can be strong cues for resolution (Hollenstein and Purves, 2012). Furthermore, the connection between non-spatial words and locations has been successfully exploited in data-driven approaches to document geolocation (Eisenstein et al., 2010, 2011; Wing and Baldridge, 2011; Roller et al., 2012) and other tasks (Hao et al., 2010; Pang et al., 2011; Intagorn and Lerman, 2012; Hecht et al., 2012; Louwerse and Benesh, 2012; Adams and McKenzie, 2013). In this paper, we learn resolvers that use all words in local or document context. For example, the word lobster appearing near the toponym Portland indicates the location is Portland in Maine rather than Oregon or Michigan. Essentially, we learn a text classifier per toponym. There are no massive collections of toponyms labeled with locations, so we train models indirectly using geotagged Wikipedia articles. Our results show these text cla</context>
<context position="23604" citStr="Roller et al., 2012" startWordPosition="3798" endWordPosition="3801"> a location whose latitude and longitude exactly match the gold label for the toponym (Leidner, 2008). Also, some errors are worse than others, e.g. predicting a toponym’s location to be on the other side of the world versus predicting it to be a different city in the same country—accuracy does not reflect this difference. We choose a metric that instead measures the distance between the correct and predicted location for each toponym and compute the mean and median of all such error distances. This is used in document geolocation work (Eisenstein et al., 2010, 2011; Wing and Baldridge, 2011; Roller et al., 2012) and is related to the root mean squared distance metric discussed by Leidner (2008). It is important to understand performance on plain text (without gold toponyms), which is the typical use case for applications using toponym resolvers. Both the accuracy metric and the errordistance metric encounter problems when the set of predicted toponyms is not the same as the set of gold toponyms (regardless of locations), e.g. when a named entity recognizer is used to identify toponyms. In this case, we can use precision and recall, where a true positive is defined as the prediction of a correctly ide</context>
</contexts>
<marker>Roller, Speriosu, Rallapalli, Wing, Baldridge, 2012</marker>
<rawString>S. Roller, M. Speriosu, S. Rallapalli, B. Wing, and J. Baldridge. Supervised text-based geolocation using language models on an adaptive grid. In Proceedings of EMNLP 2012, 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sankaranarayanan</author>
<author>H Samet</author>
<author>B Teitler</author>
<author>M Lieberman</author>
<author>J Sperling</author>
</authors>
<title>TwitterStand: news in tweets.</title>
<date>2009</date>
<booktitle>In Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems,</booktitle>
<pages>42--51</pages>
<contexts>
<context position="2087" citStr="Sankaranarayanan et al., 2009" startWordPosition="308" endWordPosition="311"> the ambiguity of toponyms (placenames). Toponym resolution is the task of disambiguating toponyms in natural language contexts to geographic locations (Leidner, 2008). It plays an essential role in automated geographic indexing and information retrieval. This is useful for historical research that combines age-old geographic issues like territoriality with modern computational tools (Guldi, 2009), studies of the effect of historically recorded travel costs on the shaping of empires (Scheidel et al., 2012), and systems that convey the geographic content in news articles (Teitler et al., 2008; Sankaranarayanan et al., 2009) and microblogs (Gelernter and Mushegian, 2011). Entity disambiguation systems such as those of Kulkarni et al. (2009) and Hoffart et al. (2011) disambiguate references to people and organizations as well as locations, but these systems do not take into account any features or measures unique to geography such as physical distance. Here we demonstrate the utility of incorporating distance measurements in toponym resolution systems. Most work on toponym resolution relies on heuristics and hand-built rules. Some use simple rules based on information from a gazetteer, such as population or admini</context>
</contexts>
<marker>Sankaranarayanan, Samet, Teitler, Lieberman, Sperling, 2009</marker>
<rawString>J. Sankaranarayanan, H. Samet, B. Teitler, M. Lieberman, and J. Sperling. TwitterStand: news in tweets. In Proceedings of the 17th ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems, pages 42–51, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Scheidel</author>
<author>E Meeks</author>
<author>J Weiland</author>
</authors>
<title>ORBIS: The Stanford geospatial network model of the roman world.</title>
<date>2012</date>
<contexts>
<context position="1968" citStr="Scheidel et al., 2012" startWordPosition="289" endWordPosition="292">eographically informed way requires more powerful tools than a keyword-based search can provide, in part due to the ambiguity of toponyms (placenames). Toponym resolution is the task of disambiguating toponyms in natural language contexts to geographic locations (Leidner, 2008). It plays an essential role in automated geographic indexing and information retrieval. This is useful for historical research that combines age-old geographic issues like territoriality with modern computational tools (Guldi, 2009), studies of the effect of historically recorded travel costs on the shaping of empires (Scheidel et al., 2012), and systems that convey the geographic content in news articles (Teitler et al., 2008; Sankaranarayanan et al., 2009) and microblogs (Gelernter and Mushegian, 2011). Entity disambiguation systems such as those of Kulkarni et al. (2009) and Hoffart et al. (2011) disambiguate references to people and organizations as well as locations, but these systems do not take into account any features or measures unique to geography such as physical distance. Here we demonstrate the utility of incorporating distance measurements in toponym resolution systems. Most work on toponym resolution relies on heu</context>
</contexts>
<marker>Scheidel, Meeks, Weiland, 2012</marker>
<rawString>W. Scheidel, E. Meeks, and J. Weiland. ORBIS: The Stanford geospatial network model of the roman world. 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Skupin</author>
<author>A Esperb´e</author>
</authors>
<title>An alternative map of the United States based on an n-dimensional model of geographic space.</title>
<date>2011</date>
<journal>Journal of Visual Languages &amp; Computing,</journal>
<volume>22</volume>
<issue>4</issue>
<marker>Skupin, Esperb´e, 2011</marker>
<rawString>A. Skupin and A. Esperb´e. An alternative map of the United States based on an n-dimensional model of geographic space. Journal of Visual Languages &amp; Computing, 22(4):290–304, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Smith</author>
<author>G Crane</author>
</authors>
<title>Disambiguating geographic names in a historical digital library.</title>
<date>2001</date>
<booktitle>In Proceedings of the 5th European Conference on Research and Advanced Technology for Digital Libraries,</booktitle>
<pages>127--136</pages>
<contexts>
<context position="12645" citStr="Smith and Crane, 2001" startWordPosition="1989" endWordPosition="1992">e do not evaluate end-to-end using NER plus toponym resolution for it as there are many (falsely) false positives. 5opennlp.apache.org 1468 less effective if the population statistics are from a time period different from that of the corpus. 3.2 SPIDER Leidner (2008) describes two general and useful minimality properties of toponyms: • one sense per discourse: multiple tokens of a toponym in the same text generally do not refer to different locations in the same text • spatial minimality: different toponyms in a text tend refer to spatially near locations Many toponym resolvers exploit these (Smith and Crane, 2001; Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Here, we define SPIDER (Spatial Prominence via Iterative Distance Evaluation and Reweighting) as a strong representative of such textually unaware approaches. In addition to capturing both minimality properties, it also identifies the relative prominence of the locations for each toponym in a given corpus. SPIDER resolves each toponym by finding the location for each that minimizes the sum distance to all locations for all other toponyms in the same document. On the first iteration, it tends t</context>
<context position="15847" citStr="Smith and Crane (2001)" startWordPosition="2522" endWordPosition="2525">e candidate with the greatest weight. When there is no such weight information, such as when the toponym does not cooccur with other toponyms anywhere in the corpus, we select a candidate at random. SPIDER captures prominence, but we stress it is not our main innovation: its purpose is to be a benchmark for text-driven resolvers to beat. 3.3 Text-Driven Resolvers The text-driven resolvers presented in this section all use local context windows, document context, or both, to inform disambiguation. TRIPDL We use a document geolocator trained on GEOWIKI’s document location labels. Others—such as Smith and Crane (2001)—have estimated a document-level location to inform toponym resolution, but ours is the first we are aware of to use training data from a different domain to build a document geolocator that uses all words (not only toponyms) to estimate a document’s location. We use the document geolocation method of Wing and Baldridge (2011). It discretizes the earth’s surface into 1◦ by 1◦ grid cells and assigns Kullback-Liebler divergences to each cell given a document, based on language models learned for each cell from geolocated Wikipedia articles. We obtain the probability of a cell c given a document </context>
</contexts>
<marker>Smith, Crane, 2001</marker>
<rawString>D. Smith and G. Crane. Disambiguating geographic names in a historical digital library. In Proceedings of the 5th European Conference on Research and Advanced Technology for Digital Libraries, pages 127–136, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Smith</author>
<author>G Mann</author>
</authors>
<title>Bootstrapping toponym classifiers.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003 workshop on Analysis of geographic references -</booktitle>
<volume>1</volume>
<pages>45--49</pages>
<contexts>
<context position="18741" citStr="Smith and Mann (2003)" startWordPosition="2985" endWordPosition="2988">28 such instances for toponyms relevant to our evaluation corpora. These instances are used to train logistic regression classifiers P(l|t, w) for location l and toponym t. To disambiguate a new toponym, WISTR chooses the location that maximizes this probability. Few such probabilistic toponym resolvers exist in the literature. Li (2007) builds a probability distribution over locations for each toponym, but still relies on nearby toponyms that could refer to regions that contain that toponym and requires hand construction of distributions. Other learning approaches to toponym resolution (e.g. Smith and Mann (2003)) require explicit unambiguous mentions like Portland, Maine to construct training instances, while our data gathering methodology does not make such an assumption. Overell and R¨uger (2008) and Overell (2009) only use nearby toponyms as features. Mani et al. (2010) and Qin et al. (2010) use other word types but only in a local context, and they require toponymlabeled training data. Our approach makes use of all words in local and document context and requires no explicitly labeled toponym tokens. TRAWL We bring TRIPDL, WISTR, and standard toponym resolution cues about administrative levels to</context>
</contexts>
<marker>Smith, Mann, 2003</marker>
<rawString>D. Smith and G. Mann. Bootstrapping toponym classifiers. In Proceedings of the HLT-NAACL 2003 workshop on Analysis of geographic references - Volume 1, pages 45–49, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Teitler</author>
<author>M Lieberman</author>
<author>D Panozzo</author>
<author>J Sankaranarayanan</author>
<author>H Samet</author>
<author>J Sperling</author>
</authors>
<title>NewsStand: a new view on news.</title>
<date>2008</date>
<booktitle>In Proceedings of the 16th ACM SIGSPATIAL international conference on Advances in geographic information systems,</booktitle>
<pages>18</pages>
<publisher>ACM,</publisher>
<contexts>
<context position="2055" citStr="Teitler et al., 2008" startWordPosition="304" endWordPosition="307">rovide, in part due to the ambiguity of toponyms (placenames). Toponym resolution is the task of disambiguating toponyms in natural language contexts to geographic locations (Leidner, 2008). It plays an essential role in automated geographic indexing and information retrieval. This is useful for historical research that combines age-old geographic issues like territoriality with modern computational tools (Guldi, 2009), studies of the effect of historically recorded travel costs on the shaping of empires (Scheidel et al., 2012), and systems that convey the geographic content in news articles (Teitler et al., 2008; Sankaranarayanan et al., 2009) and microblogs (Gelernter and Mushegian, 2011). Entity disambiguation systems such as those of Kulkarni et al. (2009) and Hoffart et al. (2011) disambiguate references to people and organizations as well as locations, but these systems do not take into account any features or measures unique to geography such as physical distance. Here we demonstrate the utility of incorporating distance measurements in toponym resolution systems. Most work on toponym resolution relies on heuristics and hand-built rules. Some use simple rules based on information from a gazette</context>
</contexts>
<marker>Teitler, Lieberman, Panozzo, Sankaranarayanan, Samet, Sperling, 2008</marker>
<rawString>B. Teitler, M. Lieberman, D. Panozzo, J. Sankaranarayanan, H. Samet, and J. Sperling. NewsStand: a new view on news. In Proceedings of the 16th ACM SIGSPATIAL international conference on Advances in geographic information systems, page 18. ACM, 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Volz</author>
<author>J Kleb</author>
<author>W Mueller</author>
</authors>
<title>Towards ontology-based disambiguation of geographical identifiers.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th International Conference on World Wide Web,</booktitle>
<contexts>
<context position="3165" citStr="Volz et al., 2007" startWordPosition="480" endWordPosition="483">lution relies on heuristics and hand-built rules. Some use simple rules based on information from a gazetteer, such as population or administrative level (city, state, country, etc.), resolving every instance of the same toponym type to the same location regardless of context (Ladra et al., 2008). Others use relationships between multiple toponyms in a context (local or whole document) and look for containment relationships, e.g. London and England occurring in the same paragraph or as the bigram London, England (Li et al., 2003; Amitay et al., 2004; Zong et al., 2005; Clough, 2005; Li, 2007; Volz et al., 2007; Jones et al., 2008; Buscaldi and Rosso, 2008; Grover et al., 2010). Still others first identify unambiguous toponyms and then disambiguate other toponyms based on geopolitical relationships with or distances to the unambiguous ones (Ding et al., 2000). Many favor resolutions of toponyms within a local context or document that cover a smaller geographic area over those that are more dispersed (Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Roberts et al. (2010) use relationships learned between people, organizations, and locations from Wiki</context>
</contexts>
<marker>Volz, Kleb, Mueller, 2007</marker>
<rawString>R. Volz, J. Kleb, and W. Mueller. Towards ontology-based disambiguation of geographical identifiers. In Proceedings of the 16th International Conference on World Wide Web, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Wing</author>
<author>J Baldridge</author>
</authors>
<title>Simple supervised document geolocation with geodesic grids.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>955--964</pages>
<contexts>
<context position="4978" citStr="Wing and Baldridge, 2011" startWordPosition="758" endWordPosition="761">ance can result in poor coverage when the required information is missing in the context or when a document mentions locations that are neither nearby geographically nor in a geopolitical relationship. There is a clear opportunity that most ignore: use non-toponym textual context. Spatially relevant words like downtown that are not explicit toponyms can be strong cues for resolution (Hollenstein and Purves, 2012). Furthermore, the connection between non-spatial words and locations has been successfully exploited in data-driven approaches to document geolocation (Eisenstein et al., 2010, 2011; Wing and Baldridge, 2011; Roller et al., 2012) and other tasks (Hao et al., 2010; Pang et al., 2011; Intagorn and Lerman, 2012; Hecht et al., 2012; Louwerse and Benesh, 2012; Adams and McKenzie, 2013). In this paper, we learn resolvers that use all words in local or document context. For example, the word lobster appearing near the toponym Portland indicates the location is Portland in Maine rather than Oregon or Michigan. Essentially, we learn a text classifier per toponym. There are no massive collections of toponyms labeled with locations, so we train models indirectly using geotagged Wikipedia articles. Our resul</context>
<context position="11033" citStr="Wing and Baldridge (2011)" startWordPosition="1732" endWordPosition="1735"> idiosyncratic mistakes remain. We, along with Jochen Leidner, will release this updated version shortly and will link to it from our Fieldspring GitHub page. ponyms for TR-CONLL.4 We use the pre-trained English NER from the OpenNLP project.5 2.3 Geolocated Wikipedia Corpus The GEOWIKI dataset contains over one million English articles from the February 11, 2012 dump of Wikipedia. Each article has human-annotated latitude/longitude coordinates. We divide the corpus into training (80%), development (10%), and test (10%) at random and perform preprocessing to remove markup in the same manner as Wing and Baldridge (2011). The training portion is used here to learn models for text-driven resolvers. 3 Toponym Resolvers Given a set of toponyms provided via annotations or identified using NER, a resolver must select a candidate location for each toponym (or, in some cases, a resolver may abstain). Here, we describe baseline resolvers, a heuristic resolver based on the usual cues used in most toponym resolvers, and several text-driven resolvers. We also discuss combining heuristic and text-driven resolvers. 3.1 Baseline Resolvers RANDOM For each toponym, the RANDOM resolver randomly selects a location from those a</context>
<context position="16175" citStr="Wing and Baldridge (2011)" startWordPosition="2576" endWordPosition="2579">lvers to beat. 3.3 Text-Driven Resolvers The text-driven resolvers presented in this section all use local context windows, document context, or both, to inform disambiguation. TRIPDL We use a document geolocator trained on GEOWIKI’s document location labels. Others—such as Smith and Crane (2001)—have estimated a document-level location to inform toponym resolution, but ours is the first we are aware of to use training data from a different domain to build a document geolocator that uses all words (not only toponyms) to estimate a document’s location. We use the document geolocation method of Wing and Baldridge (2011). It discretizes the earth’s surface into 1◦ by 1◦ grid cells and assigns Kullback-Liebler divergences to each cell given a document, based on language models learned for each cell from geolocated Wikipedia articles. We obtain the probability of a cell c given a document d by the standard method of exponentiating the negative KL-divergence and normalizing these values over all cells: exp(−KL(c, d)) P(c|d) = Ec� exp(−KL(c&apos;, d)) This distribution is used for all toponyms t in d to define distributions PDL(l|t, d) over candidate 1469 E l&apos;∈G(t) P(cl&apos;|d) where G(t) is the set of the locations for t</context>
<context position="23582" citStr="Wing and Baldridge, 2011" startWordPosition="3794" endWordPosition="3797">tain, for a given toponym, a location whose latitude and longitude exactly match the gold label for the toponym (Leidner, 2008). Also, some errors are worse than others, e.g. predicting a toponym’s location to be on the other side of the world versus predicting it to be a different city in the same country—accuracy does not reflect this difference. We choose a metric that instead measures the distance between the correct and predicted location for each toponym and compute the mean and median of all such error distances. This is used in document geolocation work (Eisenstein et al., 2010, 2011; Wing and Baldridge, 2011; Roller et al., 2012) and is related to the root mean squared distance metric discussed by Leidner (2008). It is important to understand performance on plain text (without gold toponyms), which is the typical use case for applications using toponym resolvers. Both the accuracy metric and the errordistance metric encounter problems when the set of predicted toponyms is not the same as the set of gold toponyms (regardless of locations), e.g. when a named entity recognizer is used to identify toponyms. In this case, we can use precision and recall, where a true positive is defined as the predict</context>
</contexts>
<marker>Wing, Baldridge, 2011</marker>
<rawString>B. Wing and J. Baldridge. Simple supervised document geolocation with geodesic grids. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 955–964, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zhang</author>
<author>P Jin</author>
<author>S Lin</author>
<author>L Yue</author>
</authors>
<title>Extracting focused locations for web pages.</title>
<date>2012</date>
<journal>In Web-Age Information Management,</journal>
<volume>7142</volume>
<pages>76--89</pages>
<contexts>
<context position="3661" citStr="Zhang et al., 2012" startWordPosition="561" endWordPosition="564"> bigram London, England (Li et al., 2003; Amitay et al., 2004; Zong et al., 2005; Clough, 2005; Li, 2007; Volz et al., 2007; Jones et al., 2008; Buscaldi and Rosso, 2008; Grover et al., 2010). Still others first identify unambiguous toponyms and then disambiguate other toponyms based on geopolitical relationships with or distances to the unambiguous ones (Ding et al., 2000). Many favor resolutions of toponyms within a local context or document that cover a smaller geographic area over those that are more dispersed (Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Roberts et al. (2010) use relationships learned between people, organizations, and locations from Wikipedia to aid in toponym resolution when such named entities are present, but do not exploit any other textual context. 1466 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1466–1476, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Most of these approaches suffer from a major weakness: they rely primarily on spatial relationships and metadata about locations (e.g., population). As such, they often require nearby</context>
<context position="12745" citStr="Zhang et al., 2012" startWordPosition="2007" endWordPosition="2010">e positives. 5opennlp.apache.org 1468 less effective if the population statistics are from a time period different from that of the corpus. 3.2 SPIDER Leidner (2008) describes two general and useful minimality properties of toponyms: • one sense per discourse: multiple tokens of a toponym in the same text generally do not refer to different locations in the same text • spatial minimality: different toponyms in a text tend refer to spatially near locations Many toponym resolvers exploit these (Smith and Crane, 2001; Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Here, we define SPIDER (Spatial Prominence via Iterative Distance Evaluation and Reweighting) as a strong representative of such textually unaware approaches. In addition to capturing both minimality properties, it also identifies the relative prominence of the locations for each toponym in a given corpus. SPIDER resolves each toponym by finding the location for each that minimizes the sum distance to all locations for all other toponyms in the same document. On the first iteration, it tends to select locations that clump spatially: if Paris occurs with Dallas, it will choose Paris, Texas ev</context>
</contexts>
<marker>Zhang, Jin, Lin, Yue, 2012</marker>
<rawString>Q. Zhang, P. Jin, S. Lin, and L. Yue. Extracting focused locations for web pages. In Web-Age Information Management, volume 7142, pages 76–89. 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Zong</author>
<author>D Wu</author>
<author>A Sun</author>
<author>E Lim</author>
<author>D Goh</author>
</authors>
<title>On assigning place names to geography related web pages.</title>
<date>2005</date>
<booktitle>In Proceedings of the 5th ACM/IEEECS joint conference on Digital libraries,</booktitle>
<pages>354--362</pages>
<contexts>
<context position="3122" citStr="Zong et al., 2005" startWordPosition="472" endWordPosition="475">solution systems. Most work on toponym resolution relies on heuristics and hand-built rules. Some use simple rules based on information from a gazetteer, such as population or administrative level (city, state, country, etc.), resolving every instance of the same toponym type to the same location regardless of context (Ladra et al., 2008). Others use relationships between multiple toponyms in a context (local or whole document) and look for containment relationships, e.g. London and England occurring in the same paragraph or as the bigram London, England (Li et al., 2003; Amitay et al., 2004; Zong et al., 2005; Clough, 2005; Li, 2007; Volz et al., 2007; Jones et al., 2008; Buscaldi and Rosso, 2008; Grover et al., 2010). Still others first identify unambiguous toponyms and then disambiguate other toponyms based on geopolitical relationships with or distances to the unambiguous ones (Ding et al., 2000). Many favor resolutions of toponyms within a local context or document that cover a smaller geographic area over those that are more dispersed (Rauch et al., 2003; Leidner, 2008; Grover et al., 2010; Loureiro et al., 2011; Zhang et al., 2012). Roberts et al. (2010) use relationships learned between peo</context>
</contexts>
<marker>Zong, Wu, Sun, Lim, Goh, 2005</marker>
<rawString>W. Zong, D. Wu, A. Sun, E. Lim, and D. Goh. On assigning place names to geography related web pages. In Proceedings of the 5th ACM/IEEECS joint conference on Digital libraries, pages 354–362, 2005.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>