<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000200">
<title confidence="0.9934395">
References Extension for the Automatic Evaluation of MT by
Syntactic Hybridization
</title>
<author confidence="0.999385">
Bo Wang, Tiejun Zhao, Muyun Yang, Sheng Li
</author>
<affiliation confidence="0.998172">
School of Computer Science and Technology
Harbin Institute of Technology
</affiliation>
<address confidence="0.66126">
Harbin, China
</address>
<email confidence="0.99246">
{bowang,tjzhao,ymy,sl}@mtlab.hit.edu.cn
</email>
<sectionHeader confidence="0.995555" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99977">
Because of the variations of the languages, the
coverage of the references is very important to
the reference based automatic evaluation of
machine translation systems. We propose a
method to extend the reference set of the au-
tomatic evaluation only based on multiple
manual references and their syntactic struc-
tures. In our approach, the syntactic equiva-
lents in the reference sentences are identified
and hybridized to generate new references.
The new method need no external knowledge
and can obtain the equivalents of long sub-
segments of reference sentences. The experi-
mental results show that using the extended
reference set the popular automatic evaluation
metrics achieve better correlations with the
human assessments.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937666666667">
While human evaluation of machine translation
output remains the most reliable method to assess
translation quality, it is a costly and time consum-
ing process. The development of automatic ma-
chine translation evaluation metrics enables the
rapid assessment of system output. By providing
immediate feedback on the effectiveness of various
techniques, these metrics have guided machine
translation research and have facilitated rapid ad-
vances in the state of the art. In addition, automatic
evaluation metrics are useful in comparing the per-
formance of multiple machine translation systems
</bodyText>
<page confidence="0.990565">
37
</page>
<bodyText confidence="0.999957147058823">
on a given translation task. Since automatic evalua-
tion metrics are meant to serve as a surrogate for
human judgments, their quality is determined by
how well they correlate with assessors’ preferences
and how accurately they predicts human judg-
ments.
Although current methods for automatically
evaluating machine translation output do not re-
quire humans to assess individual system output,
humans are nevertheless needed to generate a
number of reference translations. The quality of
machine-generated translations is determined by
automatically comparing system output with these
references. All current automatic evaluation met-
rics are based on the various measures of the gen-
eral similarity between the system translation and
manual references. This kind of method has an ob-
vious drawback: it does not account for combina-
tions of lexical and syntactic differences that might
occur between a perfectly fluent and accurately-
translated machine output and a human reference
translation (beyond variations already captured by
the different reference translations themselves).
Moreover, the set of human reference translations
is unlikely to be an exhaustive inventory of “good
translations” for any given foreign language sen-
tence. Therefore, it would be highly desirable to
extend the coverage of the references for the simi-
larity based evaluation methods.
To match the system translation with various
presentation of the same meaning, many work ha-
ven been proposed to extend the references by
generating lexical variations. The first strategy fo-
cuses on the extension based on paraphrase identi-
</bodyText>
<note confidence="0.7315365">
Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 37–44,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<figureCaption confidence="0.603986194444444">
fication (Lepage and Denoual, 2005; Lassner et al. ciently. Experimental results are illustrated in sec-
2005; Zhou et al. 2006; Kauchak and Barzilay, tion 4. We also include some related discussion in
2006; Owczarzak et al. 2006; Owczarzak et al. Section 5. Finally this work is concluded in Sec-
2007). In this kind of method, the quality of system tion 6.
translations can be viewed as the extent to which 2 Syntactic Equivalents
the conveyed meaning matches the semantics of In our approach, we propose a novel method to
the reference translations, independent of sub- obtain the equivalents of the sub-segments from
strings they may share. In short, all paraphrases of the corresponding references to a single source
human-generated references should be considered sentence. A sub-segment can be a word, a phrase
“good” translations. The second strategy extends or longer unit such as a clause. As we know, the
the references with the synonymy (Banerjee and variations of the sentences to the same meaning
Lavie, 2005; Lassner et al. 2005). This is an alter- can be distinguished into two categories. The first
nation to obtain lexical variations with synonymy is the structural variations. In this case, presenta-
dictionaries instead of the paraphrase. In this kind tions employ the same words but arrange them in
of method, the reference is matched against to the different structure. The second is lexical variations.
system translation with the pack of the synonymies In this case, presentations have the same structure
of the reference words instead of the exact match- but employ the different words. In practice, one
ing. reference sentence often has both of the two kinds
Both two strategies can successfully capture the of variations comparing with other corresponding
lexical variations and greatly extend the coverage reference sentences.
of the references. But they still have two common As the previous works, we also focus on the
deficiencies. The first is the demand of the external lexical variations. The approach is that the equiva-
knowledge. Paraphrase based method need a mass lents of the words are not obtained by external
of external corpus to extract paraphrases and syn- knowledge. In our strategy, generally speaking, the
onymy based method need manually constructed equivalents of a sub-segment S in a reference sen-
semantic dictionaries. These demands seriously tence are identified as the sub-segments which play
limit the application on various languages for the same syntactic role in the same structure in the
which the external knowledge is absent. other corresponding references. The equivalents
Another deficiency is that the two strategies obtained in this way are called syntactic equiva-
cannot capture the equivalents of long sub- lents.
segments such as a clause. Synonymy based me- Suppose R1 and R2 is a corresponding reference
thod can only capture the equivalents of single sentence pair. T1 and T2 are the consecutive syntac-
words. Paraphrase based method can capture the tic trees of R1 and R2 respectively. We formally
equivalents of longer units but the length is still define a syntactic equivalent pair between R1 and
very narrow. In many cases, some long sub- R2 with a 4-tuple:
segments can be varied with an entirely different &lt;N1, N2, S1, S2&gt;
</figureCaption>
<bodyText confidence="0.993287275862069">
presentation which cannot be decomposed into the where Ni is a non-terminal node in Ti and Si is the
variations of words or phrases. sub-segment which is covered by Ni. Then, all the
To address these problems we propose a novel syntactic equivalent pair R1 and R2 can be recur-
strategy to generate variations presentation only sively identified using following process:
using existing multiple manual references without • The first syntactic equivalent pair &lt;N1, N2,
any external knowledge. We identify the syntactic S1, S2&gt; is identified where Ni is the root of
components on different level as the replaceable Ti and Si= Ri.
units and determine the syntactic equivalents of the • Suppose &lt;N1, N2, S1, S2&gt; is a syntactic
components in the corresponding references. Then equivalent pair. {N11, N12, ...N1m} and { N21,
the equivalents of the syntactic components are N22, ...N2n} are the child nodes sequences of
hybridized into new references.
The rest of the paper is organized as follows.
Section 2 introduces the concept and identification
of the syntactic equivalents. Section 3 proposes a
process to hybridize the syntactic equivalents effi-
38
N1 and N2 respectively. If n=m and N1i= N2i
(i.e. the child nodes sequence of N1 and N2
are exactly the same), for each node pair N1i
and N2i a syntactic equivalent pair is identi-
fied as &lt; N1i, N2i, S1i, S2i&gt;.
With this process, all equivalent pairs on differ-
ent syntactic level can be identified by synchro-
nously traveling the two trees from top to bottom.
The following is an example of the identification
of the equivalent pairs. Figure 1 gives out a refer-
ence sentence pair and their syntactic trees. The
nodes which are included in certain equivalent pair
are surrounded by a rectangle.
</bodyText>
<figureCaption confidence="0.696366">
Figure 1 An example of the identification of the syn-
tactic equivalent pairs.
</figureCaption>
<bodyText confidence="0.9967045">
In this example, five equivalent pairs can be
identified:
</bodyText>
<listItem confidence="0.999948285714286">
• &lt;S, S, “Machine translation develops con-
stantly”, “MT progresses persistently”&gt;
• &lt;NP, NP, “Machine translation”, “MT”&gt;
• &lt;VP, VP, “develops constantly”, “progresses
persistently”&gt;
• &lt;VV, VV, “develops”, “progresses”&gt;
• &lt;ADV, ADV, “constantly”, “persistently”&gt;
</listItem>
<sectionHeader confidence="0.722547" genericHeader="method">
3 Hybridization of Syntactic Equivalents
</sectionHeader>
<bodyText confidence="0.999902961538462">
The indentified syntactic equivalents pairs include
the sub-segments which sharing the same role in
the same syntactic structure. Because of this, we
can obtain a variation of a reference sentence by
switching the two sub-segments of an equivalent
pair in this sentence. This operation did not change
the structure of the sentence but only replace a sub-
segment in the structure with its equivalent.
Consequently, two new references can be gener-
ated by switching the two sub-segments of an
equivalent pair between two reference sentences.
Furthermore when we switch the sub-segments of
all equivalent pairs between the two references,
multiple new references are generated with various
combinations of the switches. This operation is
called the syntactic hybridization of the references
which can be illustrated by following steps:
Suppose R={ri}i=1...n is a reference set containing
n reference sentences to a single source sentence.
R’ is the new reference set containing the original
reference sentences and the hybridized reference
sentences. R’ can be obtained by formula (1):
where rooti is the root node of the syntactic tree of
ri. Equ(nt) returns the set of all equivalent of the
sub-segments covered by the tree node nt. The de-
tailed process of Equ(nt) is:
</bodyText>
<construct confidence="0.905899166666667">
Equ(nt):
Define set equ = Φ
Add Seg(nt) to equ
If nt is included in an equivalent pair &lt;nt, nt’, s, s’&gt;
Add p’ to equ
Define childi=1...m is the m children of nt
</construct>
<equation confidence="0.780739">
Define hybr = Equ(child1)×Equ(child2)...×
Equ(childm)
Merge hybr into equ
Return equ
</equation>
<bodyText confidence="0.999639">
where Seg(nt) is the sub-segment covered by the
tree node nt. Operation S1× S2 generates the Carte-
sian product of the sub-segment set S1 and S2, i.e.
for each arbitrary sub-segment pair s1 and s2 se-
lected from S1 and S respectively, we concatenate
s1 and s2. Finally, the reduplicate references in R’
are removed.
For the example in Section 2, eight hybridized
references can be generated including the original
two sentences:
</bodyText>
<page confidence="0.99085">
39
</page>
<listItem confidence="0.999988125">
• Machine Translation develops constantly
• Machine Translation develops persistently
• Machine Translation progresses constantly
• Machine Translation progresses persistently
• MT develops constantly
• MT develops persistently
• MT progresses constantly
• MT progresses persistently
</listItem>
<sectionHeader confidence="0.996771" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999902382352941">
We will show experimental results in this section
to verify the effectiveness of the extended set of
hybridized reference sentences. In the experiments,
multiple translations of the source language sen-
tences are evaluated with several popular auto-
matic evaluation metrics. The evaluation is carried
out on sentence level using the original reference
set and the extended reference set respectively.
Finally, the Pearson’s correlations between the
human assessments and evaluation scores using
two reference set are calculated and compared.
The multiple translations and human assess-
ments are obtained from the dataset of the MT
evaluation workshop at ACL05 (LDC2006T04)
and the dataset from NistMATR08 (LDC2008E43).
Table 1 &amp; 2 describes the detail of the two datasets.
The popular automatic evaluation metrics in-
clude BLEU (Papieni et al., 2002), GTM (Me-
lamed et al., 2003), Rouge (Lin and Och, 2004)
and METEOR (Banerjee and Lavie, 2005). The
syntactic trees of the reference sentences are ob-
tained with the Stanford statistical parser (Klein
2003) for LDC2006T04 and Collins parser (Collins
1999) for LDC2008E43.
Table 3 &amp; 4 gives out the correlations using two
reference set on both datasets. The first column is
the name of the used metrics. The second column
is the correlations based on the original reference
set. The third column is the correlations based on
the extended reference set. In the experiment, the
maximum length of N-gram in BLEU is 4. The
exponent of GTM is 2. ROUGE uses skip-bigram
with a window of nine words. And METEOR is
run in “exact” mode.
</bodyText>
<table confidence="0.983986636363636">
Release Year 2006
Genre Newswire
Number of segments 919
Source Language Chinese
Target Language English
Number of system transla- 7
tions
Number of reference trans- 4
lations
Human assessment scores Score 1-5, ade-
quacy &amp; fluency
</table>
<tableCaption confidence="0.928282">
Table 1 Description of LDC2006T04
</tableCaption>
<table confidence="0.989437545454545">
Release Year 2008
Genre Newswire
Number of segments 249
Source Language Arabic
Target Language English
Number of system transla- 8
tions
Number of reference trans- 4
lations
Human assessment scores Score 1-7, ade-
quacy
</table>
<tableCaption confidence="0.996965">
Table 2 Description of LDC2008E43
</tableCaption>
<bodyText confidence="0.996869333333333">
After the hybridization, each source sentence in
LDC2006T04 has 31 corresponding reference sen-
tences in average and each source sentence in
LDC2008E43 has 66 corresponding reference sen-
tences in average. The number of the references is
greatly increased. And as shown in the results, the
usage of the extended reference set improves the
correlations with human assessments for all the
metrics in most cases except the ROUGE on LDC
</bodyText>
<table confidence="0.998229">
2008E43.
Metric Original Extended
BLEU 0.3488 0.3564
GTM 0.3671 0.3681
ROUGE 0.4252 0.4325
METEOR 0.4686 0.4723
</table>
<tableCaption confidence="0.989037">
Table 3 Pearson’s correlations with human assess-
</tableCaption>
<table confidence="0.9478955">
ments on sentence level on LDC2006T04
Metric Original Extended
BLEU 0.6092 0.6109
GTM 0.5434 0.5438
ROUGE 0.6628 0.6582
METEOR 0.7053 0.7089
</table>
<tableCaption confidence="0.999674">
Table 4 Pearson’s correlations with human assess-
</tableCaption>
<bodyText confidence="0.85727375">
ments on sentence level on LDC2008E43
The following is a real instance in the experi-
ments from LDC2008E43:
Four original references:
</bodyText>
<page confidence="0.992466">
40
</page>
<listItem confidence="0.990484666666667">
• Ten churches burned down in 10 days in
the American state of Alabama
• Burning of ten churches in ten days in the
American state of Alabama
• Ten churches set on fire in ten days in
American state of Alabama
• Torching of ten churches within ten days in
American state of Alabama
Six additional references:
• Torching of ten churches in ten days in the
American state of Alabama
• Torching of ten churches within ten days in
the American state of Alabama
• Torching of ten churches in ten days in
American state of Alabama
• Burning of ten churches within ten days in
American state of Alabama
• Burning of ten churches within ten days in
the American state of Alabama
• Burning of ten churches in ten days in
American state of Alabama
</listItem>
<bodyText confidence="0.738468">
The syntactic structure of the original references:
</bodyText>
<listItem confidence="0.957754375">
• (TOP (S (NPB (CD Ten) (NNS Churches))
(VP (VBN Burned) (PP (IN Down) (PP (IN
in) (NP (NPB (CD 10) (NNS Days)) (PP
(IN in) (NP (NPB (DT the) (NNP American)
(NNP State)) (PP (IN of) (NPB (NNP Ala-
bama)))))))))))
• (TOP (NP (NPB (NN Burning)) (PP (IN of)
(NP (NPB (CD Ten) (NNS Churches)) (PP
(IN in) (NP (NPB (CD Ten) (NNS Days))
(PP (IN in) (NP (NPB (DT the) (NNP
American) (NNP State)) (PP (IN of) (NPB
(NNP Alabama)))))))))))
• (TOP (S (NPB (CD Ten) (NNS Churches))
(VP (VB Set) (PP (IN on) (NPB (NN Fire)))
(PP (IN in) (NP (NPB (CD Ten) (NNS
Days)) (PP (IN in) (NP (NPB (NNP Ameri-
can) (NNP State)) (PP (IN of) (NPB (NNP
Alabama))))))))))
• (TOP (NP (NPB (NNP Torching)) (PP (IN
of) (NP (NPB (CD Ten) (NNS Churches))
(PP (IN within) (NP (NPB (CD Ten) (NNS
Days)) (PP (IN in) (NP (NPB (NNP Ameri-
can) (NNP State)) (PP (IN of) (NPB (NNP
Alabama)))))))))))
</listItem>
<bodyText confidence="0.8045858">
To investigate the distribution of the equivalents
we also perform several statistics about the count
and the length of the syntactic nodes. In table 5, we
list the information about the count of the nodes.
The first row is the average words count per refer-
ence sentence. The second and third row is the
count of all tree nodes and equivalent nodes in all
references respectively. The fourth and fifth row is
the average count of tree nodes and equivalent
nodes per reference sentence respectively.
</bodyText>
<table confidence="0.990499">
2006T 2008E4
04 3
Average length of 31.52 34.43
reference
Total tree nodes 21123 62569
1
Total equivalent nodes 21807 10073
Average tree nodes 57.46 62.82
Average equivalent 5.93 10.11
nodes
</table>
<tableCaption confidence="0.9453865">
Table 5 Counts of the tree nodes and equivalent
nodes in references.
</tableCaption>
<bodyText confidence="0.99997816">
We also investigate the distribution of the length
(count of covered words) of the nodes. First, we
count the tree nodes and equivalent nodes whose
length is from 1 word to 50 words. Then we calcu-
late the pro-portion of equivalent nodes and tree
nodes for each length. Figure 2 and 3 illustrate the
distribution of absolute count of the equivalent
nodes. The X-axis is the length of the nodes and
the Y-axis is the count. Figure 4 and 5 illustrate the
distribution of the proportions on two datasets re-
spectively. The X-axis is the length of the nodes
and the Y-axis is the proportion.
The investigation reveals four main messages.
First, the absolute counts of the short equivalents
are much more than those of long equivalents as
expected. Second, the proportion of the long
equivalents is greater than those of short equiva-
lents, this clarify that the reason of large amount of
short equivalents is the large amount of short tree
nodes. Third, also from the proportion of view we
can see that the new method comparably bias to
the long equivalents. This happens because the
method adopts a top-down survey of the tree. Forth,
the multiple references in Arabic-English data
seem to match each other better than the references
</bodyText>
<page confidence="0.998669">
41
</page>
<bodyText confidence="0.999251">
in Chinese-English data. Arabic-English references
have much more equivalents than Chinese-English
data and bias to long equivalents more significant.
</bodyText>
<figureCaption confidence="0.987371375">
Figure 2 Distribution of absolute length of equivalent
node on LDC2006T04
Figure 3 Distribution of absolute length of equivalent
node on LDC2008E43
Figure 4 Distribution of length proportion of equiva-
lent nodes on LDC2006T04
Figure 5 Distribution of length proportion of equiva-
lent nodes on LDC2008E43
</figureCaption>
<sectionHeader confidence="0.997707" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999711">
The experimental results verify the positive effect
of the hybridized reference for the automatic eval-
</bodyText>
<page confidence="0.995426">
42
</page>
<bodyText confidence="0.9999365625">
evaluation in most cases. Though the improvement
of the correlations is not very significant it is stable
across the metrics in various styles.
Compared with the previous works based on pa-
raphrase and synonym the new method has three
important advantages. The first is that the hybrid-
ized reference can switch the long span sub-
segments beyond the words and phrases.
The second is that the switch can be per-formed
in multiple levels, i.e. a sub-segment can not only
be replaced as a single unit but also can be varied
by replacing some child sub-segments of it. It’s
noticeable that the multiple level switches also
make it possible to present some structural varia-
tions by means of the lexical variations. In hybridi-
zation, we can realize some structural variation
between syntactic nodes by switch their parent
node instead of reordering them directly.
The third advantage is that the new method
needs no external knowledge which greatly facili-
tates the application. But this advantage also re-
sults in the main deficiency of this approach: the
hybridization references cannot adopt any novel
equivalents which are absent in existing references.
This deficiency can be overcome by introducing
the paraphrase and synonym into the syntactic hy-
bridization.
It should be indicated that though the hybridiza-
tion process generate many new references not all
of the new references are reasonable.
In table 6 we compare the effect of hybridized
references and manual references with more details
on LDC2006T04. In the table, the first column is
the contents of the references for each source sen-
tence. “Manual” means the manual references and
the number in front of it indicates how many man-
ual references are provided. “Hybr” means the hy-
bridized references generated from the manual
references in front of the “+”. The second column
is the Pearson’s correlations between human as-
sessments and the BLEU scores using the corre-
sponding reference set. Besides the set containing
4 references the other correlations are the average
of the correlations based on all possible subset con-
taining certain number of references. For example
correlation of “2 Manual” is the average of the cor-
relations based on 6 possible subset containing 2
references.
</bodyText>
<table confidence="0.9662775">
Reference Set Correlation
1 Manual 0.2565
2 Manual 0.3057
2 Manual+ Hybr 0.3082
3 Manual 0.3316
3 Manual + Hybr 0.3369
4 Manual 0.3488
4 Manual+ Hybr 0.3564
</table>
<tableCaption confidence="0.9926205">
Table 6 Pearson’s correlations based on incremental
reference set
</tableCaption>
<bodyText confidence="0.999976090909091">
As shown in the Table 6 hybridized references
can improve the correlations with human assess-
ments on different sizes of manual references set.
But it also indicated that though hybridization can
generate a mass of novel references the new refer-
ences is always not more effective than even one
additional manual references. This tells us that the
quality of the hybridized references still need to be
further refined.
Another message revealed by the table is that
with the increase of the number of manual refer-
ences the improvement of correlation made by ad-
ditional manual references is decreasing. However,
the improvement made by the hybridized is in-
creasing. This happens because the number of hy-
bridized references increases much faster than the
number of manual references.
There are still several noticeable deficiencies of
this work. First, it only works when there are more
than two existing references. This make it cannot
be used to extend the single reference in mass bi-
lingual corpus. Second, which is also the most im-
portant one is that this method strongly focuses on
the precision at the cost of recall. Though we have
recognized many equivalents for each sentence but
there are still many equivalents that share different
context cannot be recognized. This will be our
main future work. The last deficiency is the bias to
the long equivalents. This problem is caused by the
same reason with the second deficiency: this
method define the equivalent with the same syntac-
tic context. If two sub-nodes do not share the same
parent it often have different brothers.
</bodyText>
<sectionHeader confidence="0.998968" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999993307692308">
In this work we present a novel method to extend
the coverage of the reference set for the automatic
evaluation of machine translation. The new method
decomposes the existing references into sub-
segments according to the syntactic structure. And
then generate new reference sentences by hybridiz-
ing the equivalents of the segments which play the
same syntactic role in corresponding references. In
this way the new method can not only capture the
equivalents of words and phrases like the other
methods but also capture the equivalents of long
sub-segments which are out of the capability of the
other methods. Another important advantage of the
new method is the no use of the external knowl-
edge which greatly facilitates the application.
Experimental results show that with the ex-
tended reference set the state-of-the-arts automatic
evaluation metrics achieve better correlation with
the human assessments.
In the future work, we will relax the restriction
of the equivalent definition and try to recognize
more equivalents. We will also introduce the para-
phrase and synonyms into our method to see fur-
ther improvement. Another interesting challenge is
to hybridize the equivalents in the different order
and present the structural variations directly.
</bodyText>
<sectionHeader confidence="0.99822" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.987613">
This work is supported by the National Natural
Science Foundation of China under Grant No.
60773066 and 60736014, the National High Tech-
nology Development 863 Program of China under
Grant No. 2006AA010108.
</bodyText>
<sectionHeader confidence="0.998191" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99977565">
Statanjeev Banerjee, Alon Lavie. 2005. METEOR: An
Automatic Metric for MT Evaluation with Im-proved
Correlation with Human Judgements. In Proceedings
of the ACL Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Trans-lation and/or
Summarization.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. PhD Dissertation, Uni-
versity of Pennsylvania.
I. Dan Melamed, Ryan Green, Joseph P. Turian, 2003,
Precision and recall of machine translation, In Pro-
ceedings of HLT/NAACL 2003.
David Kauchak, Regina Barzilay. 2006. Paraphrasing
for Automatic Evaluation, In Proceedings of the
NAACL 2006.
Dan Klein, Christopher Manning. 2003. Accurate Un-
lexicalized Parsing. In Proceedings of the 41th Meet-
ing of the ACL, pp. 423-430.
Yves Lepage, Etienne Denoual. 2005. Automatic gen-
eration of paraphrases to be used as translation refer-
</reference>
<page confidence="0.995569">
43
</page>
<reference confidence="0.999405965517242">
ences in objective evaluation measures of ma-chine
translation, In Proceedings of the IWP 2005.
Karolina Owczarzak, Declan Groves, Josef Van Ge-
nabith ,Andy Way. 2006. Contextual Bitext-Derived
Paraphrases in Automatic MT Evaluation, In Pro-
ceedings of the Workshop on Statistical Ma-chine
Translation.
Karolina Owczarzak, Josef Van Genabith, Andy Way.
2007. Dependency-Based Automatic Evaluation for
Machine Translation, In Proceedings of SSST,
NAACL-HLT 2007 / AMTA Workshop on Syntax
and Structure in Statistical Translation.
Kishore Papieni, Salim Roukos, Todd Ward, Wei-Jing
Zhu. 2002. BLEU: a method for automatic evalua-
tion of machine translation, In Proceedings of the
40th Meeting of the ACL.
Grazia Russo-Lassner, Jimmy Lin, Philip Resnik. 2005.
Re-evaluating Machine Translation Results with Pa-
raphrase Support, Technical Report LAMP-TR-
125/CS-TR-4754/UMIACS-TR-2005-57, University
of Maryland, College Park, MD.
Chin-Yew Lin, Franz Josef Och. 2004. Automatic
evaluation of machine translation quality using long-
est common subsequence and skip-bigram sta-tistics.
In Proceedings of the 42th Meeting of the ACL.
Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006.
Re-evaluating Machine Translation Results with Pa-
raphrase Support, In Proceedings of the EMNLP
2006.
</reference>
<page confidence="0.999294">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.670628">
<title confidence="0.998088">References Extension for the Automatic Evaluation of MT Syntactic Hybridization</title>
<author confidence="0.999782">Bo Wang</author>
<author confidence="0.999782">Tiejun Zhao</author>
<author confidence="0.999782">Muyun Yang</author>
<author confidence="0.999782">Sheng</author>
<affiliation confidence="0.9994545">School of Computer Science and Harbin Institute of</affiliation>
<address confidence="0.685193">Harbin,</address>
<email confidence="0.991314">bowang@mtlab.hit.edu.cn</email>
<email confidence="0.991314">tjzhao@mtlab.hit.edu.cn</email>
<email confidence="0.991314">ymy@mtlab.hit.edu.cn</email>
<email confidence="0.991314">sl@mtlab.hit.edu.cn</email>
<abstract confidence="0.999363944444445">Because of the variations of the languages, the coverage of the references is very important to the reference based automatic evaluation of machine translation systems. We propose a method to extend the reference set of the automatic evaluation only based on multiple manual references and their syntactic structures. In our approach, the syntactic equivalents in the reference sentences are identified and hybridized to generate new references. The new method need no external knowledge and can obtain the equivalents of long subsegments of reference sentences. The experimental results show that using the extended reference set the popular automatic evaluation metrics achieve better correlations with the human assessments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Statanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgements.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Trans-lation and/or Summarization.</booktitle>
<contexts>
<context position="12024" citStr="Banerjee and Lavie, 2005" startWordPosition="1874" endWordPosition="1877"> level using the original reference set and the extended reference set respectively. Finally, the Pearson’s correlations between the human assessments and evaluation scores using two reference set are calculated and compared. The multiple translations and human assessments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43). Table 1 &amp; 2 describes the detail of the two datasets. The popular automatic evaluation metrics include BLEU (Papieni et al., 2002), GTM (Melamed et al., 2003), Rouge (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The syntactic trees of the reference sentences are obtained with the Stanford statistical parser (Klein 2003) for LDC2006T04 and Collins parser (Collins 1999) for LDC2008E43. Table 3 &amp; 4 gives out the correlations using two reference set on both datasets. The first column is the name of the used metrics. The second column is the correlations based on the original reference set. The third column is the correlations based on the extended reference set. In the experiment, the maximum length of N-gram in BLEU is 4. The exponent of GTM is 2. ROUGE uses skip-bigram with a window of nine words. And</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Statanjeev Banerjee, Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgements. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Trans-lation and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>PhD</tech>
<institution>Dissertation, University of Pennsylvania.</institution>
<contexts>
<context position="12184" citStr="Collins 1999" startWordPosition="1900" endWordPosition="1901"> using two reference set are calculated and compared. The multiple translations and human assessments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43). Table 1 &amp; 2 describes the detail of the two datasets. The popular automatic evaluation metrics include BLEU (Papieni et al., 2002), GTM (Melamed et al., 2003), Rouge (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The syntactic trees of the reference sentences are obtained with the Stanford statistical parser (Klein 2003) for LDC2006T04 and Collins parser (Collins 1999) for LDC2008E43. Table 3 &amp; 4 gives out the correlations using two reference set on both datasets. The first column is the name of the used metrics. The second column is the correlations based on the original reference set. The third column is the correlations based on the extended reference set. In the experiment, the maximum length of N-gram in BLEU is 4. The exponent of GTM is 2. ROUGE uses skip-bigram with a window of nine words. And METEOR is run in “exact” mode. Release Year 2006 Genre Newswire Number of segments 919 Source Language Chinese Target Language English Number of system transla</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>M. Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. PhD Dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph P Turian</author>
</authors>
<title>Precision and recall of machine translation,</title>
<date>2003</date>
<booktitle>In Proceedings of HLT/NAACL</booktitle>
<contexts>
<context position="11959" citStr="Melamed et al., 2003" startWordPosition="1862" endWordPosition="1866">evaluation metrics. The evaluation is carried out on sentence level using the original reference set and the extended reference set respectively. Finally, the Pearson’s correlations between the human assessments and evaluation scores using two reference set are calculated and compared. The multiple translations and human assessments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43). Table 1 &amp; 2 describes the detail of the two datasets. The popular automatic evaluation metrics include BLEU (Papieni et al., 2002), GTM (Melamed et al., 2003), Rouge (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The syntactic trees of the reference sentences are obtained with the Stanford statistical parser (Klein 2003) for LDC2006T04 and Collins parser (Collins 1999) for LDC2008E43. Table 3 &amp; 4 gives out the correlations using two reference set on both datasets. The first column is the name of the used metrics. The second column is the correlations based on the original reference set. The third column is the correlations based on the extended reference set. In the experiment, the maximum length of N-gram in BLEU is 4. The exponent of </context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. Dan Melamed, Ryan Green, Joseph P. Turian, 2003, Precision and recall of machine translation, In Proceedings of HLT/NAACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for Automatic Evaluation,</title>
<date>2006</date>
<booktitle>In Proceedings of the NAACL</booktitle>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak, Regina Barzilay. 2006. Paraphrasing for Automatic Evaluation, In Proceedings of the NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Meeting of the ACL,</booktitle>
<pages>423--430</pages>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein, Christopher Manning. 2003. Accurate Unlexicalized Parsing. In Proceedings of the 41th Meeting of the ACL, pp. 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yves Lepage</author>
<author>Etienne Denoual</author>
</authors>
<title>Automatic generation of paraphrases to be used as translation references in objective evaluation measures of ma-chine translation,</title>
<date>2005</date>
<booktitle>In Proceedings of the IWP</booktitle>
<contexts>
<context position="3408" citStr="Lepage and Denoual, 2005" startWordPosition="497" endWordPosition="500">s” for any given foreign language sentence. Therefore, it would be highly desirable to extend the coverage of the references for the similarity based evaluation methods. To match the system translation with various presentation of the same meaning, many work haven been proposed to extend the references by generating lexical variations. The first strategy focuses on the extension based on paraphrase identiProceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 37–44, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics fication (Lepage and Denoual, 2005; Lassner et al. ciently. Experimental results are illustrated in sec2005; Zhou et al. 2006; Kauchak and Barzilay, tion 4. We also include some related discussion in 2006; Owczarzak et al. 2006; Owczarzak et al. Section 5. Finally this work is concluded in Sec2007). In this kind of method, the quality of system tion 6. translations can be viewed as the extent to which 2 Syntactic Equivalents the conveyed meaning matches the semantics of In our approach, we propose a novel method to the reference translations, independent of sub- obtain the equivalents of the sub-segments from strings they may </context>
</contexts>
<marker>Lepage, Denoual, 2005</marker>
<rawString>Yves Lepage, Etienne Denoual. 2005. Automatic generation of paraphrases to be used as translation references in objective evaluation measures of ma-chine translation, In Proceedings of the IWP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Declan Groves</author>
<author>Josef Van Genabith</author>
</authors>
<title>Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation,</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Statistical Ma-chine Translation.</booktitle>
<marker>Owczarzak, Groves, Van Genabith, 2006</marker>
<rawString>Karolina Owczarzak, Declan Groves, Josef Van Genabith ,Andy Way. 2006. Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation, In Proceedings of the Workshop on Statistical Ma-chine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karolina Owczarzak</author>
<author>Josef Van Genabith</author>
<author>Andy Way</author>
</authors>
<title>Dependency-Based Automatic Evaluation for Machine Translation,</title>
<date>2007</date>
<booktitle>In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation.</booktitle>
<marker>Owczarzak, Van Genabith, Way, 2007</marker>
<rawString>Karolina Owczarzak, Josef Van Genabith, Andy Way. 2007. Dependency-Based Automatic Evaluation for Machine Translation, In Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papieni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Wei-Jing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation,</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Meeting of the ACL.</booktitle>
<contexts>
<context position="11931" citStr="Papieni et al., 2002" startWordPosition="1857" endWordPosition="1860">h several popular automatic evaluation metrics. The evaluation is carried out on sentence level using the original reference set and the extended reference set respectively. Finally, the Pearson’s correlations between the human assessments and evaluation scores using two reference set are calculated and compared. The multiple translations and human assessments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43). Table 1 &amp; 2 describes the detail of the two datasets. The popular automatic evaluation metrics include BLEU (Papieni et al., 2002), GTM (Melamed et al., 2003), Rouge (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The syntactic trees of the reference sentences are obtained with the Stanford statistical parser (Klein 2003) for LDC2006T04 and Collins parser (Collins 1999) for LDC2008E43. Table 3 &amp; 4 gives out the correlations using two reference set on both datasets. The first column is the name of the used metrics. The second column is the correlations based on the original reference set. The third column is the correlations based on the extended reference set. In the experiment, the maximum length of N-gram in</context>
</contexts>
<marker>Papieni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papieni, Salim Roukos, Todd Ward, Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation, In Proceedings of the 40th Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grazia Russo-Lassner</author>
<author>Jimmy Lin</author>
<author>Philip Resnik</author>
</authors>
<title>Re-evaluating Machine Translation Results with Paraphrase Support,</title>
<date>2005</date>
<tech>Technical Report LAMP-TR125/CS-TR-4754/UMIACS-TR-2005-57,</tech>
<institution>University of Maryland, College Park, MD.</institution>
<marker>Russo-Lassner, Lin, Resnik, 2005</marker>
<rawString>Grazia Russo-Lassner, Jimmy Lin, Philip Resnik. 2005. Re-evaluating Machine Translation Results with Paraphrase Support, Technical Report LAMP-TR125/CS-TR-4754/UMIACS-TR-2005-57, University of Maryland, College Park, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Franz Josef Och</author>
</authors>
<title>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram sta-tistics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42th Meeting of the ACL.</booktitle>
<contexts>
<context position="11986" citStr="Lin and Och, 2004" startWordPosition="1868" endWordPosition="1871">tion is carried out on sentence level using the original reference set and the extended reference set respectively. Finally, the Pearson’s correlations between the human assessments and evaluation scores using two reference set are calculated and compared. The multiple translations and human assessments are obtained from the dataset of the MT evaluation workshop at ACL05 (LDC2006T04) and the dataset from NistMATR08 (LDC2008E43). Table 1 &amp; 2 describes the detail of the two datasets. The popular automatic evaluation metrics include BLEU (Papieni et al., 2002), GTM (Melamed et al., 2003), Rouge (Lin and Och, 2004) and METEOR (Banerjee and Lavie, 2005). The syntactic trees of the reference sentences are obtained with the Stanford statistical parser (Klein 2003) for LDC2006T04 and Collins parser (Collins 1999) for LDC2008E43. Table 3 &amp; 4 gives out the correlations using two reference set on both datasets. The first column is the name of the used metrics. The second column is the correlations based on the original reference set. The third column is the correlations based on the extended reference set. In the experiment, the maximum length of N-gram in BLEU is 4. The exponent of GTM is 2. ROUGE uses skip-b</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>Chin-Yew Lin, Franz Josef Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram sta-tistics. In Proceedings of the 42th Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Zhou</author>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Re-evaluating Machine Translation Results with Paraphrase Support,</title>
<date>2006</date>
<booktitle>In Proceedings of the EMNLP</booktitle>
<contexts>
<context position="3499" citStr="Zhou et al. 2006" startWordPosition="512" endWordPosition="515">overage of the references for the similarity based evaluation methods. To match the system translation with various presentation of the same meaning, many work haven been proposed to extend the references by generating lexical variations. The first strategy focuses on the extension based on paraphrase identiProceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 37–44, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics fication (Lepage and Denoual, 2005; Lassner et al. ciently. Experimental results are illustrated in sec2005; Zhou et al. 2006; Kauchak and Barzilay, tion 4. We also include some related discussion in 2006; Owczarzak et al. 2006; Owczarzak et al. Section 5. Finally this work is concluded in Sec2007). In this kind of method, the quality of system tion 6. translations can be viewed as the extent to which 2 Syntactic Equivalents the conveyed meaning matches the semantics of In our approach, we propose a novel method to the reference translations, independent of sub- obtain the equivalents of the sub-segments from strings they may share. In short, all paraphrases of the corresponding references to a single source human-g</context>
</contexts>
<marker>Zhou, Lin, Hovy, 2006</marker>
<rawString>Liang Zhou, Chin-Yew Lin, and Eduard Hovy. 2006. Re-evaluating Machine Translation Results with Paraphrase Support, In Proceedings of the EMNLP 2006.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>