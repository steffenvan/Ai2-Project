<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.036375">
<title confidence="0.984433">
WriteAhead: Mining Grammar Patterns in Corpora for Assisted Writing
</title>
<author confidence="0.982578">
Tzu-Hsi Yen, Jian-Cheng Wu+
</author>
<affiliation confidence="0.9822915">
Department of Computer Science
National Tsing Hua University
</affiliation>
<address confidence="0.937905">
Hsinchu, Taiwan, R.O.C. 30013
</address>
<email confidence="0.983308">
{joe, jiancheng}@nlplab.cc
</email>
<author confidence="0.990101">
Joanne Boisson, Jim Chang, Jason Chang
</author>
<affiliation confidence="0.990626">
+National Academy for Educational Research
Minstry of Education
</affiliation>
<address confidence="0.923045">
Taipei, Taiwan, R.O.C. 30013
</address>
<email confidence="0.999473">
{joanne,jim,jason}@nlplab.cc
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999968884615384">
This paper describes WriteAhead, a
resource-rich, Interactive Writing Envi-
ronment that provides L2 learners with
writing prompts, as well as ”get it right”
advice, to helps them write fluently and
accurately. The method involves automat-
ically analyzing reference and learner cor-
pora, extracting grammar patterns with ex-
ample phrases, and computing dubious,
overused patterns. At run-time, as the user
types (or mouses over) a word, the system
automatically retrieves and displays gram-
mar patterns and examples, most relevant
to the word. The user can opt for patterns
from a general corpus, academic corpus,
learner corpus, or commonly overused du-
bious patterns found in a learner corpus.
WriteAhead proactively engages the user
with steady, timely, and spot-on informa-
tion for effective assisted writing. Pre-
liminary experiments show that WriteA-
head fulfills the design goal of foster-
ing learner independence and encouraging
self-editing, and is likely to induce better
writing, and improve writing skills in the
long run.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999314826086956">
The British Council has estimated that roughly
a billion people are learning and using English
around the world (British Council 1997), mostly
as a second language, and the numbers are grow-
ing. Clearly, many of L2 speakers of English feel
themselves to be at a disadvantage in work that
requires communication in English. For exam-
ple, Flowerdew (1999) reports that a third of Hong
Kong academics feel disadvantged in publishing a
paper internationally, as compared to native speak-
ers.
These L2 speakers and learners provide moti-
vation for research and development of computer
assisted language learning, in particular tools that
help identify and correct learners’ writing errors.
Much work has been done on developing tech-
nologies for automated gramatical error correc-
tion (GEC) to assist language learners (Leacock,
Chodorow, Gamon, and Tetreault 2010). How-
ever, such efforts have not led to the development
of a production system (Wampler, 2002).
However, Milton (2010) pointed out that fo-
cusing on fully-automatic, high quality GEC so-
lutions has overlooked the long-term pedagogi-
cal needs of L2 learner writers. Learners could
be more effectively assisted in an interactive
writring environment (IWE) that constantly pro-
vides context-sensitive writing suggestions, right
in the process of writing or self-editing.
Consider an online writer who starts a sentence
with ”This paper discusses ....” The best way the
system can help is probably displaying the patterns
related to the last word discuss such as discuss
something and discusses with someone, that help
the user to write accurately and fluently. If the user
somehow writes or pastes in some incorrect sen-
tence, ”This paper discusses about the influence of
interference and reflection of light.” The best way
the system can help is probably displaying the er-
roneous or overused pattern, discuss about some-
thing, that prompts the user to change the sentence
to ”This paper discusses the influence of interfer-
ence and reflection of light.”
Intuitively, by extracting and displaying such
patterns and examples, distilled from a very large
corpus, we can guide the user towards writing flu-
</bodyText>
<page confidence="0.985819">
139
</page>
<note confidence="0.840028">
Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 139–144,
Beijing, China, July 26-31, 2015. c�2015 ACL and AFNLP
</note>
<figureCaption confidence="0.999774">
Figure 1: Example WriteAhead session where an user typed ”This paper present method”.
</figureCaption>
<bodyText confidence="0.999743837837838">
ently, and free of grammatical errors.
We present a new system, WriteAhead, that
proactively provides just-in-time writing sugges-
tions to assist student writers, while they type
away. Example WriteAhead suggestions for ”We
discussed ...” are shown in Figure 1. WriteAhead
has determined the best patterns and examples ex-
tracted from the underlying corpus. WriteAhead
learns these patterns and examples automatically
during training by analyzing annotated dictionary
examples and automatically tagged sentences in a
corpus. As will be described in Section 4, we used
the information on collocation and syntax (ICS)
for example sentences from online Macmillan En-
glish Dictionary, as well as in the Citeseer x cor-
pus, to develop WriteAhead.
At run-time, WriteAhead activates itself as the
user types in yet another word (e.g., ”discussed”
in the prefix ”We discussed ...”). WriteAhead then
retrieves patterns related to the last word. WriteA-
head goes one step further and re-ranks the sug-
gestions, in an attempt to move most relevant sug-
gestions to the top. WriteAhead can be accessed at
http://writehead.nlpweb.org/.
In our prototype, WriteAhead returns the sug-
gestions to the user directly (see Figure 1); alterna-
tively, the suggestions returned by WriteAhead can
be used as input to an automatic grammar checker
or an essay rater.
The rest of this paper is organized as follows.
We review the related work in the next section.
Then we present our method for automatically
learning normal and overused grammar patterns
and examples for use in an interactive writing en-
vironment (Section 3). Section 5 gives a demon-
stration script of the interactive writing environ-
ment.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999963476190476">
Much work described in a recent survey (Lea-
cock, Chodorow, Gamon, and Tetreault 2010)
show that the elusive goal of fully-automatic and
high-quality grammatical error correction is far
from a reality. Moreover, Milton (2010) pointed
out that we should shift the focus and responsi-
bility to the learner, since no conclusive evidence
shows explicit correction by a teacher or machine
is leads to improved writing skills (Truscott, 1996;
Ferris and Hedgcock, 2005). In this paper, we
develop an interactive writing environment (IWE)
that constantly provides context-sensitive writing
suggestions, right in the process of writing or self-
editing.
Autocompletion has been widely used in many
language production tasks (e.g., search query and
translation). Examples include Google Suggest
and TransType, which pioneered the interactive
user interface for statistical machine translation
(Langlais et al., 2002; Casacuberta et al. 2009).
However, previous work focuses exclusively on
</bodyText>
<page confidence="0.993352">
140
</page>
<sectionHeader confidence="0.976112" genericHeader="method">
3 Method
</sectionHeader>
<bodyText confidence="0.268483">
Procedure ExtractPatterns(Sent, Keywords, Corpus)
</bodyText>
<listItem confidence="0.985429666666667">
(1) Learning phrase templates for grammar patterns of
content words (Section 3.1.1)
(2) Extracting grammar patterns for all keywords in the
given corpus based on phrase templates (Section 3.1.2)
(3) Extracting exemplary instances for all patterns of all
keywords (Section 3.1.3)
</listItem>
<figureCaption confidence="0.998423">
Figure 2: Outline of the pattern extraction process
</figureCaption>
<bodyText confidence="0.999977389830509">
providing surface suggestions lacking in general-
ity to be truely effective for all users in different
writing situation. In contrast, we provide sugges-
tions in the form of theoretical and pedagogically
sound language representation, in the form of Pat-
tern Grammar (Hunston and Francis 2000). We
also provide concise examples much like concor-
dance advocated by Sinclair (1991).
Much work has been done in deriving context-
free grammar from a corpus, while very little work
has been done in deriving pattern grammar. Ma-
son and Hunston (2004) reports on a pilot study
to automatically recognize grammar patterns for
verbs, using only limited linguistic knowledge. It
is unclear whether their method can scale up and
extend to other parts of speech. In contrast, we
show it is feasible to extract grammar patterns for
nouns, verbs, and adjectives on a large scale using
a corpus with hundreds of million words.
Researchers have been extracting error patterns
in the form of word or part of speech (POS) se-
quencesto detect real-word spelling errors (e.g.,
Golding and Schabes, 1996; Verberne, 2002). For
example, the sequence of det. det. n. definitely
indicate an error, while v. prep. adv. might or
might not indicate an error. For this reason, func-
tion words (e.g., prepositions) are not necessarily
reduced to POS tags (e.g., v. to adv.). Sometimes,
even lexicalized patterns are necessary (e.g., go to
adv.) Sun et al. (2007) extend n-grams to non-
continuous sequential patterns allowing arbitrary
gaps between words. In a recent study closer to
our work, Gamon (2011) use high-order part-of-
speech ngram to model and detect learner errors
on the sentence level.
In contrast to the previous research in devel-
oping computer assisted writing environment, we
present a system that automatically learns gram-
mar patterns and examples from an academic writ-
ten corpus as well as learner corpus, with the goal
of providing relevant, in-context suggestions.
Non-native speakers often make grammatically er-
ror, particularly in using some common words in
writing (e.g., discuss vs. discuss *about). In addi-
tion, using dictionaries or mere lexical suggestions
to assist learner in writing is often not sufficient,
and the information could be irrelevant at times.
In this section, we address such a problem. Given
various corpora (e.g., BNC or CiteseerX) in a spe-
cific genre/domain and a unfinished or completed
sentence, we intend to assist the user by retrieving
and displaying a set of suggestions extracted from
each corpus. For this, by a simple and intuitional
method, we extract grammatical error patterns and
correction such that the top ranked suggestions are
likely to contain a pattern that fits well with the
context of the unfinished sentence. We describe
the stage of our solution to this problem in the sub-
sections that followed.
</bodyText>
<subsectionHeader confidence="0.997393">
3.1 Extracting Grammar Patterns
</subsectionHeader>
<bodyText confidence="0.9997624">
We attempt to extract grammatical error patterns
and correction for keywords in a given corpus to
provide writing suggestions, in order to assist ESL
learners in an online writing session. Our extrac-
tion process is shown in Figure 2.
</bodyText>
<figureCaption confidence="0.939272833333333">
3.1.1 Learning Extraction Templates In the
first stage of the extraction process (Step (1) in
Figure 2), we generate a set of phrase templates
for identifying grammar patterns based on infor-
mation on Collocation and Syntax (ICS) in an on-
line dictionary.
</figureCaption>
<bodyText confidence="0.998376947368421">
For example, the dictionary entry of difficulty
may provide examples with ICS pattern, such as
have difficulty/problem (in) doing something:
Six months after the accident, he still has difficulty
walking. This complicated pattern with parenthet-
ical and alternative parts can be expanded to yield
patterns such as have difficulty in doing some-
thing. By generalizing such a pattern into tem-
plates with PoS and phrase tags (e.g., v. np prep.
v np, we can identify instances of such a pattern
in tagged and chunked sentences. For this, we ex-
pand the parentheticals (e.g., (in)) and alternatives
(e.g., difficulty/problem) in ICS.
Then, we replace (non-entry) words in ICS with
the most frequent part of speech tags or phrase
tags, resulting in sequences of POS and phrase la-
bels (e.g., v. difficulty prep. v. np). Then, we
take only the complementation part (e.g., prep.
v. np). Finally, we convert each complementa-
</bodyText>
<page confidence="0.993401">
141
</page>
<bodyText confidence="0.994607215686274">
tion into a regular expression for a RegExp chunk
parser.
Subsequently, we convert each template into a
regular expression of chunk labels, intended to
match instances of potential patterns in tagged
sentences. The chunk labels typically are repre-
sented using B-I-O symbols followed by phrase
type, with each symbol denoting Beginning,
Inside, and Outside of the phrase. Note that in or-
der to identify the head of a phrase, we change the
B-I-O representation to I-H-O, with H signifying
the Head.
3.1.2 Extracting Patterns In the second stage
of the extraction process (Step (2) in Figure 2),
we identify instances of potential patterns for all
keywords. These instances are generated for each
tagged and chunked sentence in the given corpus
and for each chunk templates obtained in the pre-
vious stage.
We adopt the MapReduce framework to extract
salient patterns. At the start of the Map Proce-
dure, we perform part of speech, lemmatization,
and base phrase tagging on the sentences. We
then find all pattern instances anchoring at a key-
word and matching templates obtained in the first
stage. Then, from each matched instance, we ex-
tract the tuple, (grammar pattern, collocation, and
ngrams). Finally, we emit all tuples extracted from
the tagged sentence. The map procedure is applied
to every tagged sentence in the given corpus.
In the reduce part, the ReducePattern Proce-
dure receives a batch of tuples, locally sorted and
grouped by keyword, as is usually done in the
MapReduce paradigm. At the start of the Redu-
cePattern Procedure, we further group the tuple
by pattern. Then we count the number of tuples
of each pattern as well as within-group average
and standard deviation of the counts. Finally, with
these statistics, we filter and identify patterns more
frequent than average by 1 standard deviation. The
ReducePattern Procedure is applied to all tuples
generated in the Map Procedure. Sample output
of this stage is shown in Table 1.
3.1.3 Extracting Exemplary Phrases In the
third and final stage of extraction, we generate ex-
emplary phrases for all patterns of all keywords
of interest using the ReduceCollExm Procedure,
which is done after the Map procedure, and essen-
tially the same as the ReducePattern Procedure in
the second stage (Section 3.1.2).
In the spirit of the GDEX method (Kilgarriff
</bodyText>
<tableCaption confidence="0.996888">
Table 1: Example difficulty patterns extracted.
</tableCaption>
<subsectionHeader confidence="0.93998">
Pattern Count Example
</subsectionHeader>
<bodyText confidence="0.961806055555555">
difficulty of something 2169 of the problem
difficulty in doing something 1790 in solving the problems
difficulty of doing something 1264 of solving this problem
difficulty in something 1219 in the previous analyses
difficulty with something 755 with this approach
difficulty doing something 718 using it
Note: There are 11200 instances of potential difficulty pat-
terns with average count of 215 and a standard deviation of
318
et al. 2008) of selecting good dictionary exam-
ples for a headword via collocations, we propose
a method for selection good example for a pattern.
For this, we count and select salient collocations
(e.g., the heads of phrases, difficulty in process in
pattern instance difficulty in the optimization pro-
cess). For each selected collocation, we choose
the most frequent instance (augmented with con-
text) to show the user the typical situation of using
the collocation.
These examples also facilitate the system in
ranking patterns (as will be described in Section
3.2). For that, we add one chunk before, and one
chunk after the collocational instance. For exam-
ple, the collocation, method for solution of equa-
tion is exemplified using the authentic corpus ex-
ample, ”method for exact solution of homogeneous
linear differential equation” in the context of ”re-
port a new analytical ... with.” We use a similar
procedure as describe in Section 3.1.2 to extract
examples.
After the grammar patterns are extracted from a
reference corpus and a learner corpus, we normal-
ize and compared the counts of the same pattern in
the two corpora and compuate an overuse ratio for
all patterns and retain patterns with a high overuse
ratio.
</bodyText>
<subsectionHeader confidence="0.999606">
3.2 Retrieving and Ranking Suggestions
</subsectionHeader>
<bodyText confidence="0.99787">
Once the patterns and examples are automatically
extracted for each keyword in the given corpus,
they are stored and indexed by keyword. At run-
time in a writing session, WriteAway constantly
probes and gets the last keyword of the unfinished
sentence Sent in the text box (or the word under
the mouse when in editing mode). With the key-
word as a query, WriteAway retrieves and ranks all
relevant patterns and examples (Pat and Exm) aim-
ing to move the most relevant information toward
the top. We compute the longest common subse-
quence (LCS) of Sent and an example, Exm. The
examples and patterns are ranked by
</bodyText>
<page confidence="0.955488">
142
</page>
<equation confidence="0.9993265">
Score(Exm) =  |LCS(Exm, Sent)  |× Count(Exm).
Score(Pat) = E Score(E), where E is an example of Pat
</equation>
<bodyText confidence="0.999640333333333">
To improve ranking, we also try to find the
longest similar subsequence (LSS) between the
user input, Sent and retrieved example, Exm
based on distributional word similarity using the
word2vec (Mikolov et al., 2013) cosine distance.
The new score function is:
</bodyText>
<equation confidence="0.97343475">
Score(Exm) = LSS(Exm, Sent) × Count(Exm),
LSS(Exm, Sent) = max sim(Exmsub, Sentsub),
sim(A, B) = 0, if |A |=6 |B|.
sim(A, B) = E word-sim(Ai, Bi), otherwise.
</equation>
<sectionHeader confidence="0.996036" genericHeader="method">
4 Experiments and Results
</sectionHeader>
<bodyText confidence="0.999803333333333">
For training, we used a collection of approxi-
mately 3,000 examples for 700 headwords ob-
tained from online Macmillan English Dictionary
(Rundel 2007), to develop the templates of pat-
terns. The headwords include nouns, verbs, ad-
jectives, and adverbs. We then proceeded to ex-
tract writing grammar patterns and examples from
the British National Corpus (BNC, with 100 mil-
lion words), CiteseerX corpus (with 460 million
words) and Taiwan Degree Thesis Corpus (with
10 million words). First, we used Tsujii POS Tag-
ger (Tsuruoka and Tsujii 2005) to generate tagged
sentences. We applied the proposed method to
generate suggestions for each of the 700 content
keywords in Academic Keyword List.
</bodyText>
<subsectionHeader confidence="0.998902">
4.1 Technical Architecture
</subsectionHeader>
<bodyText confidence="0.999649111111111">
WriteAhead was implemented in Python and Flask
Web framework. We stored the suggestions in
JSON format using PostgreSQL for faster access.
WriteAhead server obtains client input from a pop-
ular browser (Safari, Chrome, or Firefox) dynam-
ically with AJAX techniques. For uninterrupted
service and ease of scaling up, we chose to host
WriteAhead on Heroku, a cloud-platform-as-a-
service (PaaS) site.
</bodyText>
<subsectionHeader confidence="0.99675">
4.2 Evaluating WriteAhead
</subsectionHeader>
<bodyText confidence="0.999935">
To evaluate the performance of WriteAhead, we
randomly sampled 100 sentences from a learner
corpus with complementation errors. For each
sentence, we identify the keyword related to the er-
ror and checked whether we have identify an over-
used pattern relevant to the error, and if positive
the rank of this pattern. We then use the Mean Re-
ciprocate Rank (MRR) to measure performance.
Evaluation of WriteAhead showed a MMR rate of
.30 and a recall rate of 24%. The Top 1, 2, 3 recall
rates are 31%, 35%, and 38% respectively
</bodyText>
<sectionHeader confidence="0.970297" genericHeader="method">
5 Demo script
</sectionHeader>
<bodyText confidence="0.999957702127659">
In this demo, we will present a new writing assis-
tance system, WriteAhead, which makes it easy to
obtain writing tips as you type away. WriteAhead
does two things really well.
First, it examines the unfinished sentence you
just typed in and then automatically gives you tips
in the form of grammar patterns (accompanied
with examples similar to those found in a good
dictionary ) for continuing your sentence.
Second, WriteAhead automatically ranks sug-
gestions relevant to your writing, so you spend less
time looking at tips, and focus more on writing
your text.
You might type in The paper present method
and you are not sure about how to continue. You
will instantly receive tips on grammar as well as
content as shown in Figure 1. At a quick glance,
you might find a relevant pattern, method for do-
ing something with examples such as This paper
presents/describes a method for generating solu-
tions. That could tip you off as to change the sen-
tence into This paper presents a method, thus get-
ting rid of tense and article errors, and help you
continue to write something like method for ex-
tracting information.
Using WriteAhead this way, you could at once
speed up writing and avoid making common writ-
ing errors. This writing and tip-taking process re-
peats until you finish writing a sentence. And as
you start writing a new, the process starts all over
again.
Most autocompletion systems such as Google
Suggest and TransType offer word-level sugges-
tions, while WriteAhead organizes, summarizes,
and ranks suggestions, so you can, at a glance,
grasp complex linguistic information and make
quick decision. Our philosophy is that it is impor-
tant to show information from general to specific
to reduce the cognitive load, so while minding the
form, you can still focus on the content of writing.
WriteAhead makes writing easy and fun, and it
also turns writing into a continuous learning pro-
cess by combining problem solving and informa-
tion seeking together to create a satisfying user
experience. WriteAhead can even help you beat
Writers Block. WriteAhead can be accessed at
http://writeahead.nlpweb.org/.
</bodyText>
<page confidence="0.998812">
143
</page>
<sectionHeader confidence="0.999368" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999839142857143">
Many avenues exist for future research and im-
provement of WriteAhead. For example, corpora
for different language levels, genres (e.g., emails,
news) could be used to make the suggestions more
relevant to users with diverse proficiency levels
and interests. NLP, IR, and machine learning
techniques could be used to provide more rele-
vant ranking, to pin-point grammatical errors, or
to generate finer-grained semantic patterns (e.g.,
assist someone in something or attend activ-
ity/institution) Additionally, an interesting direc-
tion is identifying grammar patterns using a CRF
sequence labeller.
In summary, in an attempt to assist learner writ-
ers, we have proposed a method for providing
writing suggestion as a user is typewriting. The
method involves extracting, retrieving, and rank-
ing grammar patterns and examples. We have im-
plemented and evaluated the proposed method as
applied to a scholarly corpus with promising re-
sults.
</bodyText>
<sectionHeader confidence="0.998668" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999291948717949">
Casacuberta, Francisco, et al. ”Human interaction for
high-quality machine translation.” Communications
of the ACM 52.10 (2009): 135-138.
Dagneaux, Estelle, Sharon Denness, and Sylviane
Granger. ”Computer-aided error analysis.” System
26.2 (1998): 163-174.
Flowerdew, John. ”Problems in writing for scholarly
publication in English: The case of Hong Kong.”
Journal of Second Language Writing 8.3 (1999):
243-264.
Ferris, Dana, and J. S. Hedgcock. ”Teacher response
to student writing: Issues in oral and written feed-
back.” Teaching ESL composition: Purpose, pro-
cess and practice (2005): 184-222.
Graddol, David. ”The future of English?: A guide to
forecasting the popularity of the English language in
the 21st century.” (1997).
Granger, Sylviane, and Paul Rayson. Automatic profil-
ing of learner texts.” Learner English on computer
(1998): 119-131.
Granger, Sylviane, and Stephanie Tyson. ”Connector
usage in the English essay writing of native and non-
native EFL speakers of English.” World Englishes
15.1 (1996): 17-27.
Golding, Andrew R., and Yves Schabes. ”Combin-
ing trigram-based and feature-based methods for
context-sensitive spelling correction.” Proceedings
of the 34th annual meeting on Association for Com-
putational Linguistics. Association for Computa-
tional Linguistics, 1996.
Gamon, Michael. ”High-order sequence modeling for
language learner error detection.” Proceedings of
the 6th Workshop on Innovative Use of NLP for
Building Educational Applications. Association for
Computational Linguistics, 2011.
Hunston, Susan, and Gill Francis. Pattern grammar:
A corpus-driven approach to the lexical grammar of
English. Amsterdam: John Benjamins, 2000.
Leacock, Claudia, et al. ”Automated grammatical error
detection for language learners.” Synthesis lectures
on human language technologies 3.1 (2010): 1-134.
Milton, John, and Vivying SY Cheng. ”A toolkit
to assist L2 learners become independent writers.”
Proceedings of the NAACL HLT 2010 Workshop
on Computational Linguistics and Writing: Writ-
ing Processes and Authoring Aids. Association for
Computational Linguistics, 2010.
Mason, Oliver, and Susan Hunston. ”The auto-
matic recognition of verb patterns: A feasibility
study.” International journal of corpus linguistics
9.2 (2004): 253-270.
Mikolov, Tomas, et al. ”Distributed representations of
words and phrases and their compositionality.” Ad-
vances in Neural Information Processing Systems.
2013.
Sun, Guihua, et al. ”Detecting erroneous sentences us-
ing automatically mined sequential patterns.” An-
nual Meeting-Association for Computational Lin-
guistics. Vol. 45. No. 1. 2007.
Truscott, John. ”The case against grammar correc-
tion in L2 writing classes.” Language learning 46.2
(1996): 327-369.
Tsuruoka, Yoshimasa, and Jun’ichi Tsujii. ”Chunk
parsing revisited.” Proceedings of the Ninth Inter-
national Workshop on Parsing Technology. Associ-
ation for Computational Linguistics, 2005.
Verberne, Suzan. ”Context-sensitive spell checking
based on word trigram probabilities.” Unpublished
masters thesis, University of Nijmegen (2002).
Sinclair J. (1991) Corpus, Concordance, Collocation.
Oxford University Press, Hong Kong.
P. Langlais, G. Foster, and G. Lapalme. 2000.
TransType: a computer-aided translation typing sys-
tem. In Workshop on Embedded Machine Transla-
tion Systems.
Caragea, Cornelia, et al. ”CiteSeer x: A Scholarly
Big Dataset.” Advances in Information Retrieval.
Springer International Publishing, 2014. 311-322.
</reference>
<page confidence="0.998584">
144
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.646369">
<title confidence="0.999726">Mining Grammar Patterns in Corpora for Assisted Writing</title>
<author confidence="0.998132">Tzu-Hsi Yen</author>
<author confidence="0.998132">Jian-Cheng Wu</author>
<affiliation confidence="0.999742">Department of Computer Science National Tsing Hua University</affiliation>
<address confidence="0.998555">Hsinchu, Taiwan, R.O.C. 30013</address>
<author confidence="0.916484">Joanne Boisson</author>
<author confidence="0.916484">Jim Chang</author>
<author confidence="0.916484">Jason</author>
<affiliation confidence="0.9087075">National Academy for Educational Minstry of</affiliation>
<address confidence="0.738765">Taipei, Taiwan, R.O.C.</address>
<abstract confidence="0.999330333333333">paper describes a resource-rich, Interactive Writing Environment that provides L2 learners with writing prompts, as well as ”get it right” advice, to helps them write fluently and accurately. The method involves automatically analyzing reference and learner corpora, extracting grammar patterns with example phrases, and computing dubious, overused patterns. At run-time, as the user types (or mouses over) a word, the system automatically retrieves and displays grammar patterns and examples, most relevant to the word. The user can opt for patterns from a general corpus, academic corpus, learner corpus, or commonly overused dubious patterns found in a learner corpus. engages the user with steady, timely, and spot-on information for effective assisted writing. Preexperiments show that WriteAthe design goal of fostering learner independence and encouraging self-editing, and is likely to induce better writing, and improve writing skills in the long run.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Francisco Casacuberta</author>
</authors>
<title>Human interaction for high-quality machine translation.”</title>
<date>2009</date>
<journal>Communications of the ACM</journal>
<volume>52</volume>
<pages>135--138</pages>
<marker>Casacuberta, 2009</marker>
<rawString>Casacuberta, Francisco, et al. ”Human interaction for high-quality machine translation.” Communications of the ACM 52.10 (2009): 135-138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Estelle Dagneaux</author>
<author>Sharon Denness</author>
<author>Sylviane Granger</author>
</authors>
<title>Computer-aided error analysis.”</title>
<date>1998</date>
<journal>System</journal>
<volume>26</volume>
<pages>163--174</pages>
<marker>Dagneaux, Denness, Granger, 1998</marker>
<rawString>Dagneaux, Estelle, Sharon Denness, and Sylviane Granger. ”Computer-aided error analysis.” System 26.2 (1998): 163-174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Flowerdew</author>
</authors>
<title>Problems in writing for scholarly publication in English: The case of Hong Kong.”</title>
<date>1999</date>
<journal>Journal of Second Language Writing</journal>
<volume>8</volume>
<pages>243--264</pages>
<contexts>
<context position="1774" citStr="Flowerdew (1999)" startWordPosition="261" endWordPosition="262">for effective assisted writing. Preliminary experiments show that WriteAhead fulfills the design goal of fostering learner independence and encouraging self-editing, and is likely to induce better writing, and improve writing skills in the long run. 1 Introduction The British Council has estimated that roughly a billion people are learning and using English around the world (British Council 1997), mostly as a second language, and the numbers are growing. Clearly, many of L2 speakers of English feel themselves to be at a disadvantage in work that requires communication in English. For example, Flowerdew (1999) reports that a third of Hong Kong academics feel disadvantged in publishing a paper internationally, as compared to native speakers. These L2 speakers and learners provide motivation for research and development of computer assisted language learning, in particular tools that help identify and correct learners’ writing errors. Much work has been done on developing technologies for automated gramatical error correction (GEC) to assist language learners (Leacock, Chodorow, Gamon, and Tetreault 2010). However, such efforts have not led to the development of a production system (Wampler, 2002). H</context>
</contexts>
<marker>Flowerdew, 1999</marker>
<rawString>Flowerdew, John. ”Problems in writing for scholarly publication in English: The case of Hong Kong.” Journal of Second Language Writing 8.3 (1999): 243-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Ferris</author>
<author>J S Hedgcock</author>
</authors>
<title>Teacher response to student writing: Issues in oral and written feedback.” Teaching ESL composition: Purpose, process and practice</title>
<date>2005</date>
<pages>184--222</pages>
<contexts>
<context position="5928" citStr="Ferris and Hedgcock, 2005" startWordPosition="907" endWordPosition="910">nd examples for use in an interactive writing environment (Section 3). Section 5 gives a demonstration script of the interactive writing environment. 2 Related Work Much work described in a recent survey (Leacock, Chodorow, Gamon, and Tetreault 2010) show that the elusive goal of fully-automatic and high-quality grammatical error correction is far from a reality. Moreover, Milton (2010) pointed out that we should shift the focus and responsibility to the learner, since no conclusive evidence shows explicit correction by a teacher or machine is leads to improved writing skills (Truscott, 1996; Ferris and Hedgcock, 2005). In this paper, we develop an interactive writing environment (IWE) that constantly provides context-sensitive writing suggestions, right in the process of writing or selfediting. Autocompletion has been widely used in many language production tasks (e.g., search query and translation). Examples include Google Suggest and TransType, which pioneered the interactive user interface for statistical machine translation (Langlais et al., 2002; Casacuberta et al. 2009). However, previous work focuses exclusively on 140 3 Method Procedure ExtractPatterns(Sent, Keywords, Corpus) (1) Learning phrase te</context>
</contexts>
<marker>Ferris, Hedgcock, 2005</marker>
<rawString>Ferris, Dana, and J. S. Hedgcock. ”Teacher response to student writing: Issues in oral and written feedback.” Teaching ESL composition: Purpose, process and practice (2005): 184-222.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graddol</author>
</authors>
<title>The future of English?: A guide to forecasting the popularity of the English language in the 21st century.”</title>
<date>1997</date>
<marker>Graddol, 1997</marker>
<rawString>Graddol, David. ”The future of English?: A guide to forecasting the popularity of the English language in the 21st century.” (1997).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
<author>Paul Rayson</author>
</authors>
<title>Automatic profiling of learner texts.” Learner English on computer</title>
<date>1998</date>
<pages>119--131</pages>
<marker>Granger, Rayson, 1998</marker>
<rawString>Granger, Sylviane, and Paul Rayson. Automatic profiling of learner texts.” Learner English on computer (1998): 119-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
<author>Stephanie Tyson</author>
</authors>
<title>Connector usage in the English essay writing of native and nonnative</title>
<date>1996</date>
<journal>EFL speakers of English.” World Englishes</journal>
<volume>15</volume>
<pages>17--27</pages>
<marker>Granger, Tyson, 1996</marker>
<rawString>Granger, Sylviane, and Stephanie Tyson. ”Connector usage in the English essay writing of native and nonnative EFL speakers of English.” World Englishes 15.1 (1996): 17-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
<author>Yves Schabes</author>
</authors>
<title>Combining trigram-based and feature-based methods for context-sensitive spelling correction.”</title>
<date>1996</date>
<booktitle>Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="7912" citStr="Golding and Schabes, 1996" startWordPosition="1208" endWordPosition="1211">ile very little work has been done in deriving pattern grammar. Mason and Hunston (2004) reports on a pilot study to automatically recognize grammar patterns for verbs, using only limited linguistic knowledge. It is unclear whether their method can scale up and extend to other parts of speech. In contrast, we show it is feasible to extract grammar patterns for nouns, verbs, and adjectives on a large scale using a corpus with hundreds of million words. Researchers have been extracting error patterns in the form of word or part of speech (POS) sequencesto detect real-word spelling errors (e.g., Golding and Schabes, 1996; Verberne, 2002). For example, the sequence of det. det. n. definitely indicate an error, while v. prep. adv. might or might not indicate an error. For this reason, function words (e.g., prepositions) are not necessarily reduced to POS tags (e.g., v. to adv.). Sometimes, even lexicalized patterns are necessary (e.g., go to adv.) Sun et al. (2007) extend n-grams to noncontinuous sequential patterns allowing arbitrary gaps between words. In a recent study closer to our work, Gamon (2011) use high-order part-ofspeech ngram to model and detect learner errors on the sentence level. In contrast to </context>
</contexts>
<marker>Golding, Schabes, 1996</marker>
<rawString>Golding, Andrew R., and Yves Schabes. ”Combining trigram-based and feature-based methods for context-sensitive spelling correction.” Proceedings of the 34th annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gamon</author>
</authors>
<title>High-order sequence modeling for language learner error detection.”</title>
<date>2011</date>
<booktitle>Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="8403" citStr="Gamon (2011)" startWordPosition="1290" endWordPosition="1291">in the form of word or part of speech (POS) sequencesto detect real-word spelling errors (e.g., Golding and Schabes, 1996; Verberne, 2002). For example, the sequence of det. det. n. definitely indicate an error, while v. prep. adv. might or might not indicate an error. For this reason, function words (e.g., prepositions) are not necessarily reduced to POS tags (e.g., v. to adv.). Sometimes, even lexicalized patterns are necessary (e.g., go to adv.) Sun et al. (2007) extend n-grams to noncontinuous sequential patterns allowing arbitrary gaps between words. In a recent study closer to our work, Gamon (2011) use high-order part-ofspeech ngram to model and detect learner errors on the sentence level. In contrast to the previous research in developing computer assisted writing environment, we present a system that automatically learns grammar patterns and examples from an academic written corpus as well as learner corpus, with the goal of providing relevant, in-context suggestions. Non-native speakers often make grammatically error, particularly in using some common words in writing (e.g., discuss vs. discuss *about). In addition, using dictionaries or mere lexical suggestions to assist learner in </context>
</contexts>
<marker>Gamon, 2011</marker>
<rawString>Gamon, Michael. ”High-order sequence modeling for language learner error detection.” Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications. Association for Computational Linguistics, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susan Hunston</author>
<author>Gill Francis</author>
</authors>
<title>Pattern grammar: A corpus-driven approach to the lexical grammar of English.</title>
<date>2000</date>
<location>Amsterdam: John Benjamins,</location>
<contexts>
<context position="7127" citStr="Hunston and Francis 2000" startWordPosition="1078" endWordPosition="1081">us) (1) Learning phrase templates for grammar patterns of content words (Section 3.1.1) (2) Extracting grammar patterns for all keywords in the given corpus based on phrase templates (Section 3.1.2) (3) Extracting exemplary instances for all patterns of all keywords (Section 3.1.3) Figure 2: Outline of the pattern extraction process providing surface suggestions lacking in generality to be truely effective for all users in different writing situation. In contrast, we provide suggestions in the form of theoretical and pedagogically sound language representation, in the form of Pattern Grammar (Hunston and Francis 2000). We also provide concise examples much like concordance advocated by Sinclair (1991). Much work has been done in deriving contextfree grammar from a corpus, while very little work has been done in deriving pattern grammar. Mason and Hunston (2004) reports on a pilot study to automatically recognize grammar patterns for verbs, using only limited linguistic knowledge. It is unclear whether their method can scale up and extend to other parts of speech. In contrast, we show it is feasible to extract grammar patterns for nouns, verbs, and adjectives on a large scale using a corpus with hundreds of</context>
</contexts>
<marker>Hunston, Francis, 2000</marker>
<rawString>Hunston, Susan, and Gill Francis. Pattern grammar: A corpus-driven approach to the lexical grammar of English. Amsterdam: John Benjamins, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
</authors>
<title>Automated grammatical error detection for language learners.” Synthesis lectures on human language technologies 3.1</title>
<date>2010</date>
<pages>1--134</pages>
<marker>Leacock, 2010</marker>
<rawString>Leacock, Claudia, et al. ”Automated grammatical error detection for language learners.” Synthesis lectures on human language technologies 3.1 (2010): 1-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Milton</author>
<author>Vivying SY Cheng</author>
</authors>
<title>A toolkit to assist L2 learners become independent writers.”</title>
<date>2010</date>
<booktitle>Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing: Writing Processes and Authoring Aids. Association for Computational Linguistics,</booktitle>
<marker>Milton, Cheng, 2010</marker>
<rawString>Milton, John, and Vivying SY Cheng. ”A toolkit to assist L2 learners become independent writers.” Proceedings of the NAACL HLT 2010 Workshop on Computational Linguistics and Writing: Writing Processes and Authoring Aids. Association for Computational Linguistics, 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oliver Mason</author>
<author>Susan Hunston</author>
</authors>
<title>The automatic recognition of verb patterns: A feasibility study.”</title>
<date>2004</date>
<journal>International journal of corpus linguistics</journal>
<volume>9</volume>
<pages>253--270</pages>
<contexts>
<context position="7375" citStr="Mason and Hunston (2004)" startWordPosition="1119" endWordPosition="1123">s of all keywords (Section 3.1.3) Figure 2: Outline of the pattern extraction process providing surface suggestions lacking in generality to be truely effective for all users in different writing situation. In contrast, we provide suggestions in the form of theoretical and pedagogically sound language representation, in the form of Pattern Grammar (Hunston and Francis 2000). We also provide concise examples much like concordance advocated by Sinclair (1991). Much work has been done in deriving contextfree grammar from a corpus, while very little work has been done in deriving pattern grammar. Mason and Hunston (2004) reports on a pilot study to automatically recognize grammar patterns for verbs, using only limited linguistic knowledge. It is unclear whether their method can scale up and extend to other parts of speech. In contrast, we show it is feasible to extract grammar patterns for nouns, verbs, and adjectives on a large scale using a corpus with hundreds of million words. Researchers have been extracting error patterns in the form of word or part of speech (POS) sequencesto detect real-word spelling errors (e.g., Golding and Schabes, 1996; Verberne, 2002). For example, the sequence of det. det. n. de</context>
</contexts>
<marker>Mason, Hunston, 2004</marker>
<rawString>Mason, Oliver, and Susan Hunston. ”The automatic recognition of verb patterns: A feasibility study.” International journal of corpus linguistics 9.2 (2004): 253-270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.”</title>
<date>2013</date>
<booktitle>Advances in Neural Information Processing Systems.</booktitle>
<marker>Mikolov, 2013</marker>
<rawString>Mikolov, Tomas, et al. ”Distributed representations of words and phrases and their compositionality.” Advances in Neural Information Processing Systems. 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guihua Sun</author>
</authors>
<title>Detecting erroneous sentences using automatically mined sequential patterns.”</title>
<date>2007</date>
<journal>Annual Meeting-Association for Computational Linguistics.</journal>
<volume>45</volume>
<marker>Sun, 2007</marker>
<rawString>Sun, Guihua, et al. ”Detecting erroneous sentences using automatically mined sequential patterns.” Annual Meeting-Association for Computational Linguistics. Vol. 45. No. 1. 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Truscott</author>
</authors>
<title>The case against grammar correction in L2 writing classes.” Language learning 46.2</title>
<date>1996</date>
<pages>327--369</pages>
<contexts>
<context position="5900" citStr="Truscott, 1996" startWordPosition="905" endWordPosition="906">ammar patterns and examples for use in an interactive writing environment (Section 3). Section 5 gives a demonstration script of the interactive writing environment. 2 Related Work Much work described in a recent survey (Leacock, Chodorow, Gamon, and Tetreault 2010) show that the elusive goal of fully-automatic and high-quality grammatical error correction is far from a reality. Moreover, Milton (2010) pointed out that we should shift the focus and responsibility to the learner, since no conclusive evidence shows explicit correction by a teacher or machine is leads to improved writing skills (Truscott, 1996; Ferris and Hedgcock, 2005). In this paper, we develop an interactive writing environment (IWE) that constantly provides context-sensitive writing suggestions, right in the process of writing or selfediting. Autocompletion has been widely used in many language production tasks (e.g., search query and translation). Examples include Google Suggest and TransType, which pioneered the interactive user interface for statistical machine translation (Langlais et al., 2002; Casacuberta et al. 2009). However, previous work focuses exclusively on 140 3 Method Procedure ExtractPatterns(Sent, Keywords, Co</context>
</contexts>
<marker>Truscott, 1996</marker>
<rawString>Truscott, John. ”The case against grammar correction in L2 writing classes.” Language learning 46.2 (1996): 327-369.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Chunk parsing revisited.”</title>
<date>2005</date>
<booktitle>Proceedings of the Ninth International Workshop on Parsing Technology. Association for Computational Linguistics,</booktitle>
<contexts>
<context position="16872" citStr="Tsuruoka and Tsujii 2005" startWordPosition="2672" endWordPosition="2675">0, if |A |=6 |B|. sim(A, B) = E word-sim(Ai, Bi), otherwise. 4 Experiments and Results For training, we used a collection of approximately 3,000 examples for 700 headwords obtained from online Macmillan English Dictionary (Rundel 2007), to develop the templates of patterns. The headwords include nouns, verbs, adjectives, and adverbs. We then proceeded to extract writing grammar patterns and examples from the British National Corpus (BNC, with 100 million words), CiteseerX corpus (with 460 million words) and Taiwan Degree Thesis Corpus (with 10 million words). First, we used Tsujii POS Tagger (Tsuruoka and Tsujii 2005) to generate tagged sentences. We applied the proposed method to generate suggestions for each of the 700 content keywords in Academic Keyword List. 4.1 Technical Architecture WriteAhead was implemented in Python and Flask Web framework. We stored the suggestions in JSON format using PostgreSQL for faster access. WriteAhead server obtains client input from a popular browser (Safari, Chrome, or Firefox) dynamically with AJAX techniques. For uninterrupted service and ease of scaling up, we chose to host WriteAhead on Heroku, a cloud-platform-as-aservice (PaaS) site. 4.2 Evaluating WriteAhead To </context>
</contexts>
<marker>Tsuruoka, Tsujii, 2005</marker>
<rawString>Tsuruoka, Yoshimasa, and Jun’ichi Tsujii. ”Chunk parsing revisited.” Proceedings of the Ninth International Workshop on Parsing Technology. Association for Computational Linguistics, 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzan Verberne</author>
</authors>
<title>Context-sensitive spell checking based on word trigram probabilities.” Unpublished masters thesis,</title>
<date>2002</date>
<institution>University of Nijmegen</institution>
<contexts>
<context position="7929" citStr="Verberne, 2002" startWordPosition="1212" endWordPosition="1213">en done in deriving pattern grammar. Mason and Hunston (2004) reports on a pilot study to automatically recognize grammar patterns for verbs, using only limited linguistic knowledge. It is unclear whether their method can scale up and extend to other parts of speech. In contrast, we show it is feasible to extract grammar patterns for nouns, verbs, and adjectives on a large scale using a corpus with hundreds of million words. Researchers have been extracting error patterns in the form of word or part of speech (POS) sequencesto detect real-word spelling errors (e.g., Golding and Schabes, 1996; Verberne, 2002). For example, the sequence of det. det. n. definitely indicate an error, while v. prep. adv. might or might not indicate an error. For this reason, function words (e.g., prepositions) are not necessarily reduced to POS tags (e.g., v. to adv.). Sometimes, even lexicalized patterns are necessary (e.g., go to adv.) Sun et al. (2007) extend n-grams to noncontinuous sequential patterns allowing arbitrary gaps between words. In a recent study closer to our work, Gamon (2011) use high-order part-ofspeech ngram to model and detect learner errors on the sentence level. In contrast to the previous rese</context>
</contexts>
<marker>Verberne, 2002</marker>
<rawString>Verberne, Suzan. ”Context-sensitive spell checking based on word trigram probabilities.” Unpublished masters thesis, University of Nijmegen (2002).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sinclair</author>
</authors>
<title>Corpus, Concordance, Collocation.</title>
<date>1991</date>
<publisher>Oxford University Press,</publisher>
<location>Hong Kong.</location>
<contexts>
<context position="7212" citStr="Sinclair (1991)" startWordPosition="1093" endWordPosition="1094">racting grammar patterns for all keywords in the given corpus based on phrase templates (Section 3.1.2) (3) Extracting exemplary instances for all patterns of all keywords (Section 3.1.3) Figure 2: Outline of the pattern extraction process providing surface suggestions lacking in generality to be truely effective for all users in different writing situation. In contrast, we provide suggestions in the form of theoretical and pedagogically sound language representation, in the form of Pattern Grammar (Hunston and Francis 2000). We also provide concise examples much like concordance advocated by Sinclair (1991). Much work has been done in deriving contextfree grammar from a corpus, while very little work has been done in deriving pattern grammar. Mason and Hunston (2004) reports on a pilot study to automatically recognize grammar patterns for verbs, using only limited linguistic knowledge. It is unclear whether their method can scale up and extend to other parts of speech. In contrast, we show it is feasible to extract grammar patterns for nouns, verbs, and adjectives on a large scale using a corpus with hundreds of million words. Researchers have been extracting error patterns in the form of word o</context>
</contexts>
<marker>Sinclair, 1991</marker>
<rawString>Sinclair J. (1991) Corpus, Concordance, Collocation. Oxford University Press, Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Langlais</author>
<author>G Foster</author>
<author>G Lapalme</author>
</authors>
<title>TransType: a computer-aided translation typing system.</title>
<date>2000</date>
<booktitle>In Workshop on Embedded Machine Translation Systems.</booktitle>
<marker>Langlais, Foster, Lapalme, 2000</marker>
<rawString>P. Langlais, G. Foster, and G. Lapalme. 2000. TransType: a computer-aided translation typing system. In Workshop on Embedded Machine Translation Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cornelia Caragea</author>
</authors>
<title>CiteSeer x: A Scholarly Big Dataset.” Advances in Information Retrieval.</title>
<date>2014</date>
<pages>311--322</pages>
<publisher>Springer International Publishing,</publisher>
<marker>Caragea, 2014</marker>
<rawString>Caragea, Cornelia, et al. ”CiteSeer x: A Scholarly Big Dataset.” Advances in Information Retrieval. Springer International Publishing, 2014. 311-322.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>