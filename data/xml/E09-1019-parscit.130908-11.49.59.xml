<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005620">
<title confidence="0.9960005">
Web augmentation of language models for continuous speech recognition
of SMS text messages
</title>
<author confidence="0.995871">
Mathias Creutz1, Sami Virpioja1,2 and Anna Kovaleva1
</author>
<affiliation confidence="0.9888195">
1Nokia Research Center, Helsinki, Finland
2Adaptive Informatics Research Centre, Helsinki University of Technology, Espoo, Finland
</affiliation>
<email confidence="0.997977">
mathias.creutz@nokia.com, sami.virpioja@tkk.fi, annakov@gmx.de
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999863578947368">
In this paper, we present an efficient query
selection algorithm for the retrieval of web
text data to augment a statistical language
model (LM). The number of retrieved rel-
evant documents is optimized with respect
to the number of queries submitted.
The querying scheme is applied in the do-
main of SMS text messages. Continuous
speech recognition experiments are con-
ducted on three languages: English, Span-
ish, and French. The web data is utilized
for augmenting in-domain LMs in general
and for adapting the LMs to a user-specific
vocabulary. Word error rate reductions
of up to 6.6 % (in LM augmentation) and
26.0 % (in LM adaptation) are obtained in
setups, where the size of the web mixture
LM is limited to the size of the baseline
in-domain LM.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999971859649123">
An automatic speech recognition (ASR) system
consists of acoustic models of speech sounds and
of a statistical language model (LM). The LM
learns the probabilities of word sequences from
text corpora available for training. The perfor-
mance of the model depends on the amount and
style of the text. The more text there is, the better
the model is, in general. It is also important that
the model be trained on text that matches the style
of language used in the ASR application. Well
matching, in-domain, text may be both difficult
and expensive to obtain in the large quantities that
are needed.
A popular solution is to utilize the World Wide
Web as a source of additional text for LM train-
ing. A small in-domain set is used as seed data,
and more data of the same kind is retrieved from
the web. A decade ago, Berger and Miller (1998)
proposed a just-in-time LM that updated the cur-
rent LM by retrieving data from the web using re-
cent recognition hypotheses as queries submitted
to a search engine. Perplexity reductions of up to
10 % were reported.1 Many other works have fol-
lowed. Zhu and Rosenfeld (2001) retrieved page
and phrase counts from the web in order to update
the probabilities of infrequent trigrams that occur
in N-best lists. Word error rate (WER) reductions
of about 3 % were obtained on TREC-7 data.
In more recent work, the focus has turned to
the collection of text rather than n-gram statistics
based on page counts. More effort has been put
into the selection of query strings. Bulyko et al.
(2003; 2007) first extend their baseline vocabulary
with words from a small in-domain training cor-
pus. They then use n-grams with these new words
in their web queries in order to retrieve text of a
certain genre. For instance, they succeed in ob-
taining conversational style phrases, such as “we
were friends but we don’t actually have a relation-
ship.” In a number of experiments, word error
rate reductions of 2-3 % are obtained on English
data, and 6 % on Mandarin. The same method for
web data collection is applied by C¸ etin and Stolcke
(2005) in meeting and lecture transcription tasks.
The web sources reduce perplexity by 10 % and
4.3 %, respectively, and word error rates by 3.5 %
and 2.2 %, respectively.
Sarikaya et al. (2005) chunk the in-domain text
into “n-gram islands” consisting of only content
words and excluding frequently occurring stop
words. An island such as “stock fund portfolio” is
then extended by adding context, producing “my
stock fund portfolio”, for instance. Multiple is-
lands are combined using and and or operations to
form web queries. Significant word error reduc-
tions between 10 and 20 % are obtained; however,
the in-domain data set is very small, 1700 phrases,
</bodyText>
<footnote confidence="0.9949335">
1All reported percentage differences are relative unless
explicitly stated otherwise.
</footnote>
<note confidence="0.9621235">
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 157–165,
Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.996744">
157
</page>
<bodyText confidence="0.999944871428572">
which makes (any) new data a much needed addi-
tion.
Similarly, Misu and Kawahara (2006) obtain
very good word error reductions (20 %) in spo-
ken dialogue systems for software support and
sightseeing guidance. Nouns that have high tf/idf
scores in the in-domain documents are used in the
web queries. The existing in-domain data sets
poorly match the speaking style of the task and
therefore existing dialogue corpora of different do-
mains are included, which improves the perfor-
mance considerably.
Wan and Hain (2006) select query strings by
comparing the n-gram counts within an in-domain
topic model to the corresponding counts in an out-
of-domain background model. Topic-specific n-
grams are used as queries, and perplexity reduc-
tions of 5.4 % are obtained.
It is customary to postprocess and filter the
downloaded web texts. Sentence boundaries are
detected using some heuristics. Text chunks with a
high out-of-vocabulary (OOV) rate are discarded.
Additionally, the chunks are often ranked accord-
ing to their similarity with the in-domain data, and
the lowest ranked chunks are discarded. As a sim-
ilarity measure, the perplexity of the sentence ac-
cording to the in-domain LM can be used; for in-
stance, Bulyko et al. (2007). Another measure
for ranking is relative perplexity (Weilhammer et
al., 2006), where the in-domain perplexity is di-
vided by the perplexity given by an LM trained
on the web data. Also the BLEU score familiar
from the field of machine translation has been used
(Sarikaya et al., 2005).
Some criticism has been raised by Sethy et al.
(2007), who claim that sentence ranking has an
inherent bias towards the center of the in-domain
distribution. They propose a data selection algo-
rithm that selects a sentence from the web set, if
adding the sentence to the already selected set re-
duces the relative entropy with respect to the in-
domain data distribution. The algorithm appears
efficient in producing a rather small subset (1/11)
of the web data, while degrading the WER only
marginally.
The current paper describes a new method for
query selection and its applications in LM aug-
mentation and adaptation using web data. The
language models are part of a continuous speech
recognition system that enables users to use
speech as an input modality on mobile devices,
such as mobile phones. The particular domain of
interest is personal communication: The user dic-
tates a message that is automatically transcribed
into text and sent to a recipient as an SMS text
message. Memory consumption and computa-
tional speed are crucial factors in mobile applica-
tions. While most studies ignore the sizes of the
LMs when comparing models, we aim at improv-
ing the LM without increasing its size when web
data is added.
Another aspect that is typically overlooked is
that the collection of web data costs time and com-
putational resources. This applies to the querying,
downloading and postprocessing of the data. The
query selection scheme proposed in this paper is
economical in the sense that it strives to download
as much relevant text from the web as possible us-
ing as few queries as possible avoiding overlap be-
tween the set of pages found by different queries.
</bodyText>
<sectionHeader confidence="0.616152" genericHeader="method">
2 Query selection and web data retrieval
</sectionHeader>
<bodyText confidence="0.999857066666667">
Our query selection scheme involves multiple
steps. The assumption is that a batch of queries
will be created. These queries are submitted to
a search engine and the matching documents are
downloaded. This procedure is repeated for multi-
ple query batches.
In particular, our scheme attempts to maximize
the number of retrieved relevant documents, when
two restrictions apply: (1) queries are not “free”:
each query costs some time or money; for in-
stance, the number of queries submitted within a
particular period of time is limited, and (2) the
number of documents retrieved for a particular
query is limited to a particular number of “top
hits”.
</bodyText>
<subsectionHeader confidence="0.9804945">
2.1 N-gram selection and prospection
querying
</subsectionHeader>
<bodyText confidence="0.99936225">
Some text reflecting the target domain must be
available. A set of the most frequent n-grams oc-
curring in the text is selected, from unigrams up to
five-grams. Some of these n-grams are character-
istic of the domain of interest (such as “Hogwarts
School of Witchcraft and Wizardry”), others are
just frequent in general (“but they did not say”);
we do not know yet which ones.
All n-grams are submitted as queries to the web
search engine. Exact matches of the n-grams are
required; different inflections or matches of the
words individually are not accepted.
</bodyText>
<page confidence="0.997256">
158
</page>
<bodyText confidence="0.99978925">
The search engine returns the total number of
hits h(qs) for each query qs as well as the URLs
of a predefined maximum number of “top hit” web
pages. The top hit pages are downloaded and post-
processed into plain text, from which duplicate
paragraphs and paragraphs with a high OOV rate
are removed.
N-gram language models are then trained sep-
arately on the in-domain text and the the filtered
web text. If the amount of web text is very large,
only a subset is used, which consists of the parts
of the web data that are the most similar to the
in-domain text. As a similarity measure, relative
perplexity is used. The LM trained on web data is
called a background LM to distinguish it from the
in-domain LM.
</bodyText>
<subsectionHeader confidence="0.995963">
2.2 Focused querying
</subsectionHeader>
<bodyText confidence="0.999968">
Next, the querying is made more specific and tar-
geted on the domain of interest. New queries are
created that consist of n-gram pairs, requiring that
a document contain two n-grams (“but they did not
say”+“Hogwarts School of Witchcraft and Wiz-
ardry”).2
If all possible n-gram pairs are formed from
the n-grams selected in Section 2.1, the number
of pairs is very large, and we cannot afford using
them all as queries. Typical approaches for query
selection include the following: (i) select pairs that
include n-grams that are relatively more frequent
in the in-domain text than in the background text,
(ii) use some extra source of knowledge for select-
ing the best pairs.
</bodyText>
<subsectionHeader confidence="0.583989">
2.2.1 Extra linguistic knowledge
</subsectionHeader>
<bodyText confidence="0.98601215">
We first tested the second (ii) query selection ap-
proach by incorporating some simple linguistic
knowledge: In an experiment on English, queries
were obtained by combining a highly frequent n-
gram with a slightly less frequent n-gram that had
to contain a first- or second-person pronoun (I,
you, we, me, us, my, your, our). Such n-grams
were thought to capture direct speech, which is
characteristic for the desired genre of personal
communication. (Similar techniques are reported
in the literature cited in Section 1.)
Although successful for English, this scheme is
more difficult to apply to other languages, where
person is conveyed as verbal suffixes rather than
single words. Linguistic knowledge is needed for
2Higher order tuples could be used as well, but we have
only tested n-gram pairs.
every language, and it turns out that many of the
queries are “wasted”, because they are too specific
and return only few (if any) documents.
</bodyText>
<subsectionHeader confidence="0.855013">
2.2.2 Statistical approach
</subsectionHeader>
<bodyText confidence="0.999928916666667">
The other proposed query selection technique (i)
allows for an automatic identification of the n-
grams that are characteristic of the in-domain
genre. If the relative frequency of an n-gram is
higher in the in-domain data than in the back-
ground data, then the n-gram is potentially valu-
able. However, as in the linguistic approach, there
is no guarantee that queries are not wasted, since
the identified n-gram may be very rare on the In-
ternet. Pairing it with some other n-gram (which
may also be rare) often results in very few hits.
To get out the most of the queries, we pro-
pose a query selection algorithm that attempts to
optimize the relevance of the query to the target
domain, but also takes into account the expected
amount of data retrieved by the query. Thus, the
potential queries are ranked according to the ex-
pected number of retrieved relevant documents.
Only the highest ranked pairs, which are likely to
produce the highest number of relevant web pages,
are used as queries.
We denote queries that consist of two n-grams
s and t by qs∧t. The expected number of retrieved
relevant documents for the query qs∧t is r(qs∧t):
</bodyText>
<equation confidence="0.998619">
r(qs∧t) = n(qs∧t) · p(qs∧t  |Q), (1)
</equation>
<bodyText confidence="0.9989818125">
where n(qs∧t) is the expected number of retrieved
documents for the query, and p(qs∧t  |Q) is the ex-
pected proportion of relevant documents within all
documents retrieved by the query. The expected
proportion of relevant documents is a value be-
tween zero and one, and as explained below, it is
dependent on all past queries, the query history Q.
Expected number of retrieved documents
n(qs∧t). From the prospection querying phase
(Section 2.1), we know the numbers of hits for
the single n-grams s and t, separately: h(qs) and
h(qt). We make the operational, but overly simpli-
fying, assumption that the n-grams occur evenly
distributed over the web collection, independently
of each other. The expected size of the intersection
qs∧t is then:
</bodyText>
<equation confidence="0.993905">
ˆh(qs∧t) = h(qs) · h(qt)
</equation>
<bodyText confidence="0.999048">
where N is the size of the web collection that our
n-gram selection covers (total number of docu-
</bodyText>
<equation confidence="0.956341">
N
, (2)
</equation>
<page confidence="0.986757">
159
</page>
<bodyText confidence="0.999939666666667">
ments). N is not known, but different estimates
can be used, for instance, N = maxvqs h(qs),
where it is assumed that the most frequent n-gram
occurs in every document in the collection (prob-
ably an underestimate of the actual value).
Ideally, the expected number of retrieved doc-
uments equals the expected number of hits, but
since the search engine returns a limited maximum
number of “top hit” pages, M, we get:
</bodyText>
<equation confidence="0.990878">
n(qsnt) = min( ˆh(qsnt), M). (3)
</equation>
<bodyText confidence="0.998353">
Expected proportion of relevant documents
ρ(qsnt  |Q). As in the case of n(qsnt), an inde-
pendence assumption can be applied in the deriva-
tion of the expected proportion of relevant docu-
ments for the combined query qsnt: We simply
put together the chances of obtaining relevant doc-
uments by the single n-gram queries qs and qt in-
dividually. The union equals:
</bodyText>
<equation confidence="0.999794">
ρ(qsnt  |Q) =
1 − (1 − ρ(qs  |Q)) · (1 − ρ(qt  |Q)). (4)
</equation>
<bodyText confidence="0.999963543478261">
However, we do not know the values for
ρ(qs  |Q) and ρ(qt  |Q). As mentioned earlier, it is
straightforward to obtain a relevance ranking for a
set of n-grams: For each n-gram s, the LM prob-
ability is computed using both the in-domain and
the background LM. The in-domain probability is
divided by the background probability and the n-
grams are sorted, highest relative probability first.
The first n-gram is much more prominent in the
in-domain than the background data, and we wish
to obtain more text with this crucial n-gram. The
opposite is true for the last n-gram.
We need to transform the ranking into ρ(·) val-
ues between zero and one. There is no absolute di-
vision into relevant and irrelevant documents from
the point of view of LM training. We use a proba-
bilistic query ranking scheme, such that we define
that of all documents containing an x % relevant
n-gram, x % are relevant. When the n-grams have
been ranked into a presumed order of relevance,
we decide that the most relevant n-gram is 100 %
relevant and the least relevant n-gram is 0 % rele-
vant; finally, we scale the relevances of the other
n-grams according to rank.
When scoring the remaining n-grams, linear
scaling is avoided, because the majority of the n-
grams are irrelevant or neutral with respect to our
domain of interest, and many of them would ob-
tain fairly high relevance values. Instead, we fix
the relevance value of the “most domain-neutral”
n-gram (the one with the relative probability value
closest to one); we might assume that only 5 % of
all documents containing this n-gram are indeed
relevant. We then fit a polynomial curve through
the three points with known values (0, 0.05, and 1)
to get the missing ρ(·) values for all qs.
Decay factor δ(s  |Q). We noticed that if con-
stant relevance values are used, the top ranked
queries will consist of a rather small set of top
ranked n-grams that are paired with each other in
all possible combinations. However, it is likely
that each time an n-gram is used in a query, the
need for finding more occurrences of this partic-
ular n-gram decreases. Therefore, we introduced
a decay factor δ(s  |Q), by which the initial ρ(·)
value, written ρ0(qs), is multiplied:
</bodyText>
<equation confidence="0.99668725">
ρ(qs  |Q) = ρ0(qs) · δ(s  |Q), (5)
The decay is exponential:
P
δ(s  |Q) = (1 − �) ∀s∈Q 1. (6)
</equation>
<bodyText confidence="0.997565607142857">
E is a small value between zero and one (for in-
stance 0.05), and EvscQ 1 is the number of times
the n-gram s has occurred in past queries.
Overlap with previous queries. Some queries
are likely to retrieve the same set of documents
as other queries. This occurs if two queries share
one n-gram and there is strong correlation be-
tween the second n-grams (for instance, “we wish
you”+“Merry Christmas” vs. “we wish you”+
“and a Happy New Year”). In principle, when as-
sessing the relevance of a query, one should esti-
mate the overlap of that query with all past queries.
We have tested an approximate solution that al-
lows for fast computing. However, the real effect
of this addition was insignificant, and a further de-
scription is omitted in this paper.
Optimal order of the queries. We want to max-
imize the expected number of retrieved relevant
documents while keeping the number of submitted
queries as low as possible. Therefore we sort the
queries best first and submit as many queries we
can afford from the top of the list. However, the
relevance of a query is dependent on the sequence
of past queries (because of the decay factor). Find-
ing the optimal order of the queries takes O(n2)
operations, if n is the total number of queries.
A faster solution is to apply an iterative algo-
rithm: All queries are put in some initial order. For
</bodyText>
<page confidence="0.983933">
160
</page>
<bodyText confidence="0.9997355">
each query, its r(qs∧t) value is computed accord-
ing to Equation 1. The queries are then rearranged
into the order defined by the new r(·) values, best
first. These two steps are repeated until conver-
gence.
Repeated focused querying. Focused querying
can be run multiple times. Some ten thousands of
the top ranked queries are submitted to the search
engine and the documents matching the queries
are downloaded. A new background LM is trained
using the new web data, and a new round of fo-
cused querying can take place.
</bodyText>
<subsectionHeader confidence="0.3887195">
2.2.3 Comparison of the linguistic and
statistical focused querying schemes
</subsectionHeader>
<bodyText confidence="0.999328142857143">
On one language (German), the statical focused
querying algorithm (Section 2.2.2) was shown
to retrieve 50 % more unique web pages and
70 % more words than the linguistic scheme (Sec-
tion 2.2.1) for the same number of queries. Also
results from language modeling and speech recog-
nition experiments favored statistical querying.
</bodyText>
<subsectionHeader confidence="0.994973">
2.3 Web collections obtained
</subsectionHeader>
<bodyText confidence="0.958050147058823">
For the speech recognition experiments described
in the current paper, we have collected web texts
for three languages: US English, European Span-
ish, and Canadian French.
As in-domain data we used 230,000 English
text messages (4 million words), 65,000 Spanish
messages (2 million words), and 60,000 French
messages (1 million words). These text messages
were obtained in data collection projects involving
thousand of participants, who used a web interface
to enter messages according to different scenarios
of personal communication situations.3 A few ex-
ample messages are shown in Figure 1.
The queries were submitted to Yahoo!’s web
search engine. The web pages that were retrieved
by the queries were filtered and cleaned and di-
vided into chunks consisting of single paragraphs.
For English, we obtained 210 million paragraphs
and 13 billion words, for Spanish 160 million
paragraphs and 12 billion words, and for French
44 million paragraphs and 3 billion words.
3Real messages sent from mobile phones would be the
best data, but are hard to get because of privacy protection.
The postprocessing of authentic messages would, however,
require proper handling of artifacts resulting from the limited
input capacities on keypads of mobile devices, such as spe-
cific acronyms: i’ll c u l8er. In our setup, we did not have to
face such issues.
I hope you have a long and happy marriage.
Congratulations!
Remember to pick up Billy at practice at five
o’clock!
Hey Eric, how was the trip with the kids over
winter vacation? Did you go to Texas?
</bodyText>
<figureCaption confidence="0.998627">
Figure 1: Example text messages (US English).
</figureCaption>
<bodyText confidence="0.99997925">
The linguistic focused querying method was ap-
plied in the US English task (because the statisti-
cal method did not yet exist). The Spanish and
Canadian French web collections were obtained
using statistical querying. Since the French set
was smaller than the other sets (“only” 3 billion
words), web crawling was performed, such that
those web sites that had provided us with the most
valuable data (measured by relative perplexity)
were downloaded entirely. As a result, the num-
ber of paragraphs increased to 110 million and the
number of words to 8 billion.
</bodyText>
<sectionHeader confidence="0.991838" genericHeader="method">
3 Speech Recognition Experiments
</sectionHeader>
<bodyText confidence="0.99998780952381">
We have trained language models on the in-
domain data together with web data, and these
models have been used in speech recognition ex-
periments. Two kinds of experiments have been
performed: (1) the in-domain LM is augmented
with web data, and (2) the LM is adapted to a user-
specific vocabulary utilizing web data as an addi-
tional data source.
One hundred native speakers for each language
were recorded reading held-out subsets of the in-
domain text data. The speech data was partitioned
into training and test sets, such that around one
fourth of the speakers were reserved for testing.
We use a continuous speech recognizer opti-
mized for low memory footprint and fast recog-
nition (Olsen et al., 2008). The recognizer
runs on a server (Core2 2.33 GHz) in about
one fourth of real time. The LM probabilities
are quantized and precompiled together with the
speaker-independent acoustic models (intra-word
triphones) into a finite state transducer (FST).
</bodyText>
<subsectionHeader confidence="0.993685">
3.1 Language model augmentation
</subsectionHeader>
<bodyText confidence="0.9997764">
Each paragraph in the web data is treated as a po-
tential text message and scored according to its
similarity to the in-domain data. Relative perplex-
ity is used as the similarity measure. The para-
graphs are sorted, lowest relative perplexity first,
</bodyText>
<page confidence="0.995043">
161
</page>
<table confidence="0.9997392">
US English
FST size [MB] 10 20 40 70
In-domain 42.7 40.1 39.1 –
Web mixture 42.0 37.6 35.7 33.8
Ppl reduction [%] 1.6 6.2 8.7 13.6
European Spanish
FST size [MB] 10 20 25 40
In-domain 68.0 64.6 64.3 –
Web mixture 63.9 58.4 55.0 52.1
Ppl reduction [%] 6.0 9.6 14.5 19.0
Canadian French
FST size [MB] 10 20 25 50
In-domain 57.6 – – –
Web mixture 51.7 47.9 45.9 44.6
Ppl reduction [%] 10.2 16.8 20.3 22.6
</table>
<tableCaption confidence="0.993259">
Table 1: Perplexities.
</tableCaption>
<table confidence="0.9999314">
US English
FST size [MB] 10 20 40 70
In-domain 17.9 17.5 17.3 –
Web mixture 17.5 16.7 16.4 15.8
WER reduction 2.2 4.4 5.2 8.4
European Spanish
FST size [MB] 10 20 25 40
In-domain 18.9 18.7 18.6 –
Web mixture 18.7 17.9 17.4 16.8
WER reduction 1.4 4.1 6.6 9.7
Canadian French
FST size [MB] 10 20 25 50
In-domain 22.6 – – –
Web mixture 22.1 21.7 21.3 20.9
WER reduction 2.3 4.1 5.8 7.5
</table>
<tableCaption confidence="0.997905">
Table 2: Word error rates [%].
</tableCaption>
<bodyText confidence="0.999980384615385">
In the tables, the perplexity and word error rate reductions of the web mixtures are computed with
respect to the in-domain models of the same size, if such models exist; otherwise the comparison is
made to the largest in-domain model available.
and the highest ranked paragraphs are used as LM
training data. The optimal size of the set depends
on the test, but the largest chosen set contains 15
million paragraphs and 500 million words.
Separate LMs are trained on the in-domain data
and web data. The two LMs are then linearly
interpolated into a mixture model. Roughly the
same interpolation weights (0.5) are obtained for
the LMs, when the optimal value is chosen based
on a held-out in-domain development test set.
</bodyText>
<subsectionHeader confidence="0.96727">
3.1.1 Test set perplexities
</subsectionHeader>
<bodyText confidence="0.99997740625">
In Table 1, the prediction abilities of the in-domain
and web mixture language models are compared.
As an evaluation measure we use perplexity cal-
culated on test sets consisting of in-domain text.
The comparison is performed on FSTs of differ-
ent sizes. The FSTs contain the acoustic models,
language model and lexicon, but the LM makes up
for most of the size. The availability of data varies
for the different languages, and therefore the FST
sizes are not exactly the same across languages.
The LMs have been created using the SRI LM
toolkit (Stolcke, 2002). Good-Turing smoothing
with Katz backoff (Katz, 1987) has been used, and
the different model sizes are obtained by pruning
down the full models using entropy-based prun-
ing (Stolcke, 1998). N-gram orders up to five have
been tested: 5-grams always work best on the mix-
ture models, whereas the best in-domain models
are 4- or 5-grams.
For every language and model size, the web
mixture model performs better than the corre-
sponding in-domain model. The perplexity reduc-
tions obtained increase with the size of the model.
Since it is possible to create larger mixture mod-
els than in-domain models, there are no in-domain
results for the largest model sizes.
Especially if large models can be afforded, the
perplexity reductions are considerable. The largest
improvements are observed for French (between
10.2 % and 22.6 % relative). This is not surprising,
as the French in-domain set is the smallest, which
leaves much room for improvement.
</bodyText>
<subsectionHeader confidence="0.982292">
3.1.2 Word error rates
</subsectionHeader>
<bodyText confidence="0.99925375">
Speech recognition results for the different LMs
are given in Table 2. The results are consistent in
the sense that the web mixture models outperform
the in-domain models, and augmentation helps
more with larger models. The largest word error
rate reduction is observed for the largest Span-
ish model (9.7 % relative). All WER reductions
are statistically significant (one-sided Wilcoxon
signed-rank test; level 0.05) except the 10 MB
Spanish setup.
Although the observed word error rate reduc-
tions are mostly smaller than the corresponding
</bodyText>
<page confidence="0.995192">
162
</page>
<bodyText confidence="0.9999795">
perplexity reductions, the results are actually very
good, when we consider the fact that consider-
able reductions in perplexity may typically trans-
late into meager word error reductions; see, for in-
stance, Rosenfeld (2000), Goodman (2001). This
suggests that the web texts are very welcome com-
plementary data that improve on the robustness of
the recognition.
</bodyText>
<subsectionHeader confidence="0.925097">
3.1.3 Modified Kneser-Ney smoothing
</subsectionHeader>
<bodyText confidence="0.99983259375">
In the above experiments, Good-Turing (GT)
smoothing with Katz backoff was used, although
modified Kneser-Ney (KN) interpolation has been
shown to outperform other smoothing methods
(Chen and Goodman, 1999). However, as demon-
strated by Siivola et al. (2007), KN smoothing
is not compatible with simple pruning methods
such as entropy-based pruning. In order to make
a meaningful comparison, we used the revised
Kneser pruning and Kneser-Ney growing tech-
niques proposed by Siivola et al. (2007). For the
three languages, we built KN models that resulted
in FSTs of the same sizes as the largest GT in-
domain models. The perplexities decreased 4–8%,
but in speech recognition, the improvements were
mostly negligible: the error rates were 17.0 for En-
glish, 18.7 for Spanish, and 22.5 for French.
For English, we also created web mixture mod-
els with KN smoothing. The error rates were 16.5,
15.9 and 15.7 for the 20 MB, 40 MB and 70 MB
models, respectively. Thus, Kneser-Ney outper-
formed Good-Turing, but the improvements were
small, and a statistically significant difference was
measured only for the 40 MB LMs. This was ex-
pected, as it has been observed before that very
simple smoothing techniques can perform well on
large data sets, such as web data (Brants et al.,
2007).
For the purpose of demonstrating the usefulness
of our web data retrieval system, we concluded
that there was no significant difference between
GT and KN smoothing in our current setup.
</bodyText>
<subsectionHeader confidence="0.999038">
3.2 Language model adaptation
</subsectionHeader>
<bodyText confidence="0.996913125">
In the second set of experiments we envisage a
system that adapts to the user’s own vocabulary.
Some words that the user needs may not be in-
cluded in the built-in vocabulary of the device,
such as names in the user’s contact list, names of
places or words related to some specific hobby or
other focus of interest.
Two adaptation techniques have been tested:
</bodyText>
<listItem confidence="0.526439833333333">
(1) Unigram adaptation is a simple technique, in
which user-specific words (for instance, names
from the contact list) are added to the vocabulary.
No context information is available, and thus only
unigram probabilities are created for these words.
(2) In message adaptation, the LM is augmented
selectively with paragraphs of web data that con-
tain user-specific words. Now, higher order n-
grams can be estimated, since the words occur
within passages of running text. This idea is not
new: information retrieval has been suggested as a
solution by Bigi et al. (2004) among others.
</listItem>
<bodyText confidence="0.999872">
In our message adaptation, we have not created
web queries dynamically on demand. Instead, we
used the large web collections described in Sec-
tion 2.3, from which we selected paragraphs con-
taining user-specific words. We have tested both
adaptation by pooling (adding the paragraphs to
the original training data), and adaptation by in-
terpolation (using the new data to train a sepa-
rate LM, which is interpolated with the original
LM). One million words from the web data were
selected for each language. The adaptation was
thought to take place off-line on a server.
</bodyText>
<subsectionHeader confidence="0.925358">
3.2.1 Data sets
</subsectionHeader>
<bodyText confidence="0.999940538461539">
For each language, the adaptation takes place on
two baseline models, which are the in-domain
and web mixture LMs of Section 3.1; however,
the amount of in-domain training data is reduced
slightly (as explained below).
In order to evaluate the success of the adapta-
tion, a simulated user-specific test set is created.
This set is obtained by selecting a subset of a
larger potential test set. Words that occur both in
the training set and the potential test set and that
are infrequent in the training set are chosen as the
user-specific vocabulary. For Spanish and French,
a training set frequency threshold of one is used,
resulting in 606 and 275 user-specific words, re-
spectively. For English the threshold is 5, which
results in 99 words. All messages in the potential
test set containing any of these words are selected
into the user-specific test set. Any message con-
taining user-specific words is removed from the
in-domain training set. In this manner, we obtain
a test set with a certain over-representation of a
specific vocabulary, without biasing the word fre-
quency distribution of the training set to any no-
ticeable degree.
For comparison, performance is additionally
computed on a generic in-domain test set, as be-
</bodyText>
<page confidence="0.995658">
163
</page>
<table confidence="0.998940185185185">
US English, 23 MB models
Model WER (reduction)
user-specific in-domain
In-domain 29.1 (–) 17.9 (–)
+unigram adapt. 24.4 (16.3) 17.1 (4.7)
+message adapt. 21.6 (26.0) 16.8 (6.0)
Web mixture 25.7 (11.8) 16.9 (5.9)
+unigram adapt. 23.1 (20.6) 16.3 (8.8)
+message adapt. 22.2 (23.8) 16.4 (8.5)
European Spanish, 23 MB models
Model WER (reduction)
user-specific in-domain
In-domain 25.3 (–) 18.6 (–)
+unigram adapt. 23.4 (7.7) 18.5 (0.3)
+message adapt. 21.7 (14.4) 18.0 (3.2)
Web mixture 21.9 (13.7) 17.5 (5.8)
+unigram adapt. 21.5 (15.3) 17.7 (5.0)
+message adapt. 21.2 (16.5) 17.7 (4.7)
Canadian French, 21 MB models
Model WER (reduction)
user-specific in-domain
In-domain 30.3 (–) 22.6 (–)
+unigram adapt. 28.3 (6.4) 22.5 (0.4)
+message adapt. 26.6 (12.1) 22.2 (1.8)
Web mixture 26.7 (11.8) 21.4 (5.1)
+unigram adapt. 26.0 (14.3) 21.4 (5.4)
+message adapt. 26.0 (14.2) 21.6 (4.3)
</table>
<tableCaption confidence="0.998869">
Table 3: Adaptation, word error rates [%]. Six
</tableCaption>
<bodyText confidence="0.9735832">
models have been evaluated on two types of test
sets: a user-specific test set with a higher number
of user-specific words and a generic in-domain test
set. The numbers in brackets are relative WER re-
ductions [%] compared to the in-domain model.
WER values for the unigram adaptation are ren-
dered in italics, if the improvement obtained is sta-
tistically significant compared to the correspond-
ing non-adapted model. WER values for the mes-
sage adaptation are in italics, if there is a statisti-
cally significant reduction with respect to unigram
adaptation.
fore. User-specific and generic development test
sets are used for the estimation of optimal interpo-
lation weights.
</bodyText>
<sectionHeader confidence="0.96341" genericHeader="evaluation">
3.2.2 Results
</sectionHeader>
<bodyText confidence="0.999956032258065">
The adaptation experiments are summarized in Ta-
ble 3. Only medium sized FSTs (21–23 MB)
have been tested. The two baseline models have
been adapted using the simple unigram reweight-
ing scheme and using selective web message aug-
mentation. For the in-domain baseline, pooling
works the best, that is, adding the web messages
to the original in-domain training set. For the web
mixture baseline, a mixture model is the only op-
tion; that is, one more layer of interpolation is
added.
In the adaptation of the in-domain LMs, mes-
sage selection is almost twice as effective as uni-
gram adaptation for all data sets. Also the perfor-
mance on the generic in-domain test set is slightly
improved, because more training data is available.
Except for English, the best results on the user-
specific test sets are produced by the adaptation of
the web mixture models. The benefit of using mes-
sage adaptation instead of simple unigram adapta-
tion is smaller when we have a web mixture model
as a baseline rather than an in-domain-only LM.
On the generic test sets, the adaptation of the
web mixture makes a difference only for English.
Since there were practically no singleton words
in the English in-domain data, the user-specific
vocabulary consists of words occurring at most
five times. Thus, the English user-specific words
are more frequent than their Spanish and French
equivalents, which shows in larger WER reduc-
tions for English in all types of adaptation.
</bodyText>
<sectionHeader confidence="0.997658" genericHeader="conclusions">
4 Discussion and conclusion
</sectionHeader>
<bodyText confidence="0.999952380952381">
Mobile applications need to run in small memory,
but not much attention is usually paid to memory
consumption in related LM work. We have shown
that LM augmentation using web data can be suc-
cessful, even when the resulting mixture model is
not allowed to grow any larger than the initial in-
domain model. Yet, the benefit of the web data is
larger, the larger model can be used.
The largest WER reductions were observed in
the adaptation to a user-specific vocabulary. This
can be compared to Misu and Kawahara (2006),
who obtained similar accuracy improvements with
clever selection of web data, when there was ini-
tially no in-domain data available with both the
correct topic and speaking style.
We used relative perplexity ranking to filter the
downloaded web data. More elaborate algorithms
could be exploited, such as the one proposed by
Sethy et al. (2007). Initially, we have experi-
mented along those lines, but it did not pay off;
maybe future refinements will be more successful.
</bodyText>
<page confidence="0.998041">
164
</page>
<sectionHeader confidence="0.998339" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999883927083333">
Adam Berger and Robert Miller. 1998. Just-in-time
language modeling. In In ICASSP-98, pages 705–
708.
Brigitte Bigi, Yan Huang, and Renato De Mori. 2004.
Vocabulary and language model adaptation using in-
formation retrieval. In Proc. Interspeech 2004 – IC-
SLP, pages 1361–1364, Jeju Island, Korea.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language
models in machine translation. In Proceedings
of the 2007 Joint Conference on Empirical Meth-
ods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 858–867.
Ivan Bulyko, Mari Ostendorf, and Andreas Stolcke.
2003. Getting more mileage from web text sources
for conversational speech language modeling using
class-dependent mixtures. In NAACL ’03: Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 7–
9, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Ivan Bulyko, Mari Ostendorf, Manhung Siu, Tim Ng,
Andreas Stolcke, and ¨Ozg¨ur C¸etin. 2007. Web
resources for language modeling in conversational
speech recognition. ACM Trans. Speech Lang. Pro-
cess., 5(1):1–25.
¨Ozg¨ur C¸etin and Andreas Stolcke. 2005. Lan-
guage modeling in the ICSI-SRI spring 2005 meet-
ing speech recognition evaluation system. Technical
Report 05-006, International Computer Science In-
stitute, Berkeley, CA, USA, July.
S. F. Chen and J. Goodman. 1999. An empirical
study of smoothing techniques for language model-
ing. Computer Speech and Language, 13:359–394.
Joshua T. Goodman. 2001. A bit of progress in lan-
guage modeling. Computer Speech and Language,
15:403–434.
Slava M. Katz. 1987. Estimation of probabilities
from sparse data for the language model compo-
nent of a speech recognizer. IEEE Transactions
on Acoustics, Speech and Signal Processing, ASSP-
35(3):400–401, March.
Teruhisa Misu and Tatsuya Kawahara. 2006. A boot-
strapping approach for developing language model
of new spoken dialogue systems by selecting web
texts. In Proc. INTERSPEECH ’06, pages 9–13,
Pittsburgh, PA, USA, September, 17–21.
Jesper Olsen, Yang Cao, Guohong Ding, and Xinxing
Yang. 2008. A decoder for large vocabulary contin-
uous short message dictation on embedded devices.
In Proc. ICASSP 2008, Las Vegas, Nevada.
Ronald Rosenfeld. 2000. Two decades of language
modeling: Where do we go from here? Proceedings
of the IEEE, 88(8):1270–1278.
Ruhi Sarikaya, Augustin Gravano, and Yuqing Gao.
2005. Rapid language model development using ex-
ternal resources for new spoken dialog domains. In
Proc. IEEE International Conference on Acoustics,
Speech, and Signal Processing (ICASSP ’05), vol-
ume I, pages 573–576.
Abhinav Sethy, Shrikanth Narayanan, and Bhuvana
Ramabhadran. 2007. Data driven approach for lan-
guage model adaptation using stepwise relative en-
tropy minimization. In Proc. IEEE International
Conference on Acoustics, Speech, and Signal Pro-
cessing (ICASSP ’07), volume IV, pages 177–180.
Vesa Siivola, Teemu Hirsim¨aki, and Sami Virpi-
oja. 2007. On growing and pruning Kneser-
Ney smoothed n-gram models. IEEE Transac-
tions on Audio, Speech and Language Processing,
15(5):1617–1624.
A. Stolcke. 1998. Entropy-based pruning of backoff
language models. In Proc. DARPA BNTU Work-
shop, pages 270–274, Lansdowne, VA, USA.
A. Stolcke. 2002. SRILM – an extensible
language modeling toolkit. In Proc. ICSLP,
pages 901–904. http://www.speech.sri.com/
projects/srilm/.
Vincent Wan and Thomas Hain. 2006. Strategies for
language model web-data collection. In Proc. IEEE
International Conference on Acoustics, Speech, and
Signal Processing (ICASSP ’06), volume I, pages
1069–1072.
Karl Weilhammer, Matthew N. Stuttle, and Steve
Young. 2006. Bootstrapping language models for
dialogue systems. In Proc. INTERSPEECH 2006
- ICSLP Ninth International Conference on Spo-
ken Language Processing, Pittsburgh, PA, USA,
September 17–21.
Xiaojin Zhu and R. Rosenfeld. 2001. Improving tri-
gram language modeling with the world wide web.
In Proc. IEEE International Conference on Acous-
tics, Speech, and Signal Processing (ICASSP ’01).,
volume 1, pages 533–536.
</reference>
<page confidence="0.998738">
165
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.419829">
<title confidence="0.9701445">Web augmentation of language models for continuous speech recognition of SMS text messages</title>
<author confidence="0.875962">Sami</author>
<affiliation confidence="0.6191145">Research Center, Helsinki, Finland Informatics Research Centre, Helsinki University of Technology, Espoo, Finland</affiliation>
<email confidence="0.973568">mathias.creutz@nokia.com,sami.virpioja@tkk.fi,annakov@gmx.de</email>
<abstract confidence="0.99458545">In this paper, we present an efficient query selection algorithm for the retrieval of web text data to augment a statistical language model (LM). The number of retrieved relevant documents is optimized with respect to the number of queries submitted. The querying scheme is applied in the domain of SMS text messages. Continuous speech recognition experiments are conducted on three languages: English, Spanish, and French. The web data is utilized for augmenting in-domain LMs in general and for adapting the LMs to a user-specific vocabulary. Word error rate reductions of up to 6.6 % (in LM augmentation) and 26.0 % (in LM adaptation) are obtained in setups, where the size of the web mixture LM is limited to the size of the baseline in-domain LM.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Robert Miller</author>
</authors>
<title>Just-in-time language modeling.</title>
<date>1998</date>
<booktitle>In In ICASSP-98,</booktitle>
<pages>705--708</pages>
<contexts>
<context position="1950" citStr="Berger and Miller (1998)" startWordPosition="316" endWordPosition="319">r training. The performance of the model depends on the amount and style of the text. The more text there is, the better the model is, in general. It is also important that the model be trained on text that matches the style of language used in the ASR application. Well matching, in-domain, text may be both difficult and expensive to obtain in the large quantities that are needed. A popular solution is to utilize the World Wide Web as a source of additional text for LM training. A small in-domain set is used as seed data, and more data of the same kind is retrieved from the web. A decade ago, Berger and Miller (1998) proposed a just-in-time LM that updated the current LM by retrieving data from the web using recent recognition hypotheses as queries submitted to a search engine. Perplexity reductions of up to 10 % were reported.1 Many other works have followed. Zhu and Rosenfeld (2001) retrieved page and phrase counts from the web in order to update the probabilities of infrequent trigrams that occur in N-best lists. Word error rate (WER) reductions of about 3 % were obtained on TREC-7 data. In more recent work, the focus has turned to the collection of text rather than n-gram statistics based on page coun</context>
</contexts>
<marker>Berger, Miller, 1998</marker>
<rawString>Adam Berger and Robert Miller. 1998. Just-in-time language modeling. In In ICASSP-98, pages 705– 708.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brigitte Bigi</author>
<author>Yan Huang</author>
<author>Renato De Mori</author>
</authors>
<title>Vocabulary and language model adaptation using information retrieval.</title>
<date>2004</date>
<booktitle>In Proc. Interspeech 2004 – ICSLP,</booktitle>
<pages>1361--1364</pages>
<location>Jeju Island,</location>
<marker>Bigi, Huang, De Mori, 2004</marker>
<rawString>Brigitte Bigi, Yan Huang, and Renato De Mori. 2004. Vocabulary and language model adaptation using information retrieval. In Proc. Interspeech 2004 – ICSLP, pages 1361–1364, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Ashok C Popat</author>
<author>Peng Xu</author>
<author>Franz J Och</author>
<author>Jeffrey Dean</author>
</authors>
<title>Large language models in machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>858--867</pages>
<contexts>
<context position="27198" citStr="Brants et al., 2007" startWordPosition="4598" endWordPosition="4601"> speech recognition, the improvements were mostly negligible: the error rates were 17.0 for English, 18.7 for Spanish, and 22.5 for French. For English, we also created web mixture models with KN smoothing. The error rates were 16.5, 15.9 and 15.7 for the 20 MB, 40 MB and 70 MB models, respectively. Thus, Kneser-Ney outperformed Good-Turing, but the improvements were small, and a statistically significant difference was measured only for the 40 MB LMs. This was expected, as it has been observed before that very simple smoothing techniques can perform well on large data sets, such as web data (Brants et al., 2007). For the purpose of demonstrating the usefulness of our web data retrieval system, we concluded that there was no significant difference between GT and KN smoothing in our current setup. 3.2 Language model adaptation In the second set of experiments we envisage a system that adapts to the user’s own vocabulary. Some words that the user needs may not be included in the built-in vocabulary of the device, such as names in the user’s contact list, names of places or words related to some specific hobby or other focus of interest. Two adaptation techniques have been tested: (1) Unigram adaptation </context>
</contexts>
<marker>Brants, Popat, Xu, Och, Dean, 2007</marker>
<rawString>Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 858–867.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ivan Bulyko</author>
<author>Mari Ostendorf</author>
<author>Andreas Stolcke</author>
</authors>
<title>Getting more mileage from web text sources for conversational speech language modeling using class-dependent mixtures.</title>
<date>2003</date>
<booktitle>In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology,</booktitle>
<pages>pages</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2635" citStr="Bulyko et al. (2003" startWordPosition="435" endWordPosition="438">ing data from the web using recent recognition hypotheses as queries submitted to a search engine. Perplexity reductions of up to 10 % were reported.1 Many other works have followed. Zhu and Rosenfeld (2001) retrieved page and phrase counts from the web in order to update the probabilities of infrequent trigrams that occur in N-best lists. Word error rate (WER) reductions of about 3 % were obtained on TREC-7 data. In more recent work, the focus has turned to the collection of text rather than n-gram statistics based on page counts. More effort has been put into the selection of query strings. Bulyko et al. (2003; 2007) first extend their baseline vocabulary with words from a small in-domain training corpus. They then use n-grams with these new words in their web queries in order to retrieve text of a certain genre. For instance, they succeed in obtaining conversational style phrases, such as “we were friends but we don’t actually have a relationship.” In a number of experiments, word error rate reductions of 2-3 % are obtained on English data, and 6 % on Mandarin. The same method for web data collection is applied by C¸ etin and Stolcke (2005) in meeting and lecture transcription tasks. The web sourc</context>
</contexts>
<marker>Bulyko, Ostendorf, Stolcke, 2003</marker>
<rawString>Ivan Bulyko, Mari Ostendorf, and Andreas Stolcke. 2003. Getting more mileage from web text sources for conversational speech language modeling using class-dependent mixtures. In NAACL ’03: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology, pages 7– 9, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Bulyko</author>
<author>Mari Ostendorf</author>
<author>Manhung Siu</author>
<author>Tim Ng</author>
<author>Andreas Stolcke</author>
<author>¨Ozg¨ur C¸etin</author>
</authors>
<title>Web resources for language modeling in conversational speech recognition.</title>
<date>2007</date>
<journal>ACM Trans. Speech Lang. Process.,</journal>
<volume>5</volume>
<issue>1</issue>
<marker>Bulyko, Ostendorf, Siu, Ng, Stolcke, C¸etin, 2007</marker>
<rawString>Ivan Bulyko, Mari Ostendorf, Manhung Siu, Tim Ng, Andreas Stolcke, and ¨Ozg¨ur C¸etin. 2007. Web resources for language modeling in conversational speech recognition. ACM Trans. Speech Lang. Process., 5(1):1–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>¨Ozg¨ur C¸etin</author>
<author>Andreas Stolcke</author>
</authors>
<title>Language modeling in the ICSI-SRI spring 2005 meeting speech recognition evaluation system.</title>
<date>2005</date>
<tech>Technical Report 05-006,</tech>
<institution>International Computer Science Institute,</institution>
<location>Berkeley, CA, USA,</location>
<marker>C¸etin, Stolcke, 2005</marker>
<rawString>¨Ozg¨ur C¸etin and Andreas Stolcke. 2005. Language modeling in the ICSI-SRI spring 2005 meeting speech recognition evaluation system. Technical Report 05-006, International Computer Science Institute, Berkeley, CA, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S F Chen</author>
<author>J Goodman</author>
</authors>
<title>An empirical study of smoothing techniques for language modeling. Computer Speech and Language,</title>
<date>1999</date>
<pages>13--359</pages>
<contexts>
<context position="26131" citStr="Chen and Goodman, 1999" startWordPosition="4417" endWordPosition="4420">he corresponding 162 perplexity reductions, the results are actually very good, when we consider the fact that considerable reductions in perplexity may typically translate into meager word error reductions; see, for instance, Rosenfeld (2000), Goodman (2001). This suggests that the web texts are very welcome complementary data that improve on the robustness of the recognition. 3.1.3 Modified Kneser-Ney smoothing In the above experiments, Good-Turing (GT) smoothing with Katz backoff was used, although modified Kneser-Ney (KN) interpolation has been shown to outperform other smoothing methods (Chen and Goodman, 1999). However, as demonstrated by Siivola et al. (2007), KN smoothing is not compatible with simple pruning methods such as entropy-based pruning. In order to make a meaningful comparison, we used the revised Kneser pruning and Kneser-Ney growing techniques proposed by Siivola et al. (2007). For the three languages, we built KN models that resulted in FSTs of the same sizes as the largest GT indomain models. The perplexities decreased 4–8%, but in speech recognition, the improvements were mostly negligible: the error rates were 17.0 for English, 18.7 for Spanish, and 22.5 for French. For English, </context>
</contexts>
<marker>Chen, Goodman, 1999</marker>
<rawString>S. F. Chen and J. Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 13:359–394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua T Goodman</author>
</authors>
<title>A bit of progress in language modeling.</title>
<date>2001</date>
<journal>Computer Speech and Language,</journal>
<pages>15--403</pages>
<contexts>
<context position="25767" citStr="Goodman (2001)" startWordPosition="4366" endWordPosition="4367">ls, and augmentation helps more with larger models. The largest word error rate reduction is observed for the largest Spanish model (9.7 % relative). All WER reductions are statistically significant (one-sided Wilcoxon signed-rank test; level 0.05) except the 10 MB Spanish setup. Although the observed word error rate reductions are mostly smaller than the corresponding 162 perplexity reductions, the results are actually very good, when we consider the fact that considerable reductions in perplexity may typically translate into meager word error reductions; see, for instance, Rosenfeld (2000), Goodman (2001). This suggests that the web texts are very welcome complementary data that improve on the robustness of the recognition. 3.1.3 Modified Kneser-Ney smoothing In the above experiments, Good-Turing (GT) smoothing with Katz backoff was used, although modified Kneser-Ney (KN) interpolation has been shown to outperform other smoothing methods (Chen and Goodman, 1999). However, as demonstrated by Siivola et al. (2007), KN smoothing is not compatible with simple pruning methods such as entropy-based pruning. In order to make a meaningful comparison, we used the revised Kneser pruning and Kneser-Ney g</context>
</contexts>
<marker>Goodman, 2001</marker>
<rawString>Joshua T. Goodman. 2001. A bit of progress in language modeling. Computer Speech and Language, 15:403–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="24077" citStr="Katz, 1987" startWordPosition="4096" endWordPosition="4097"> Table 1, the prediction abilities of the in-domain and web mixture language models are compared. As an evaluation measure we use perplexity calculated on test sets consisting of in-domain text. The comparison is performed on FSTs of different sizes. The FSTs contain the acoustic models, language model and lexicon, but the LM makes up for most of the size. The availability of data varies for the different languages, and therefore the FST sizes are not exactly the same across languages. The LMs have been created using the SRI LM toolkit (Stolcke, 2002). Good-Turing smoothing with Katz backoff (Katz, 1987) has been used, and the different model sizes are obtained by pruning down the full models using entropy-based pruning (Stolcke, 1998). N-gram orders up to five have been tested: 5-grams always work best on the mixture models, whereas the best in-domain models are 4- or 5-grams. For every language and model size, the web mixture model performs better than the corresponding in-domain model. The perplexity reductions obtained increase with the size of the model. Since it is possible to create larger mixture models than in-domain models, there are no in-domain results for the largest model sizes.</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava M. Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing, ASSP35(3):400–401, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Teruhisa Misu</author>
<author>Tatsuya Kawahara</author>
</authors>
<title>A bootstrapping approach for developing language model of new spoken dialogue systems by selecting web texts.</title>
<date>2006</date>
<booktitle>In Proc. INTERSPEECH ’06,</booktitle>
<pages>9--13</pages>
<location>Pittsburgh, PA, USA,</location>
<contexts>
<context position="4182" citStr="Misu and Kawahara (2006)" startWordPosition="691" endWordPosition="694">ing context, producing “my stock fund portfolio”, for instance. Multiple islands are combined using and and or operations to form web queries. Significant word error reductions between 10 and 20 % are obtained; however, the in-domain data set is very small, 1700 phrases, 1All reported percentage differences are relative unless explicitly stated otherwise. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 157–165, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 157 which makes (any) new data a much needed addition. Similarly, Misu and Kawahara (2006) obtain very good word error reductions (20 %) in spoken dialogue systems for software support and sightseeing guidance. Nouns that have high tf/idf scores in the in-domain documents are used in the web queries. The existing in-domain data sets poorly match the speaking style of the task and therefore existing dialogue corpora of different domains are included, which improves the performance considerably. Wan and Hain (2006) select query strings by comparing the n-gram counts within an in-domain topic model to the corresponding counts in an outof-domain background model. Topic-specific ngrams </context>
</contexts>
<marker>Misu, Kawahara, 2006</marker>
<rawString>Teruhisa Misu and Tatsuya Kawahara. 2006. A bootstrapping approach for developing language model of new spoken dialogue systems by selecting web texts. In Proc. INTERSPEECH ’06, pages 9–13, Pittsburgh, PA, USA, September, 17–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jesper Olsen</author>
<author>Yang Cao</author>
<author>Guohong Ding</author>
<author>Xinxing Yang</author>
</authors>
<title>A decoder for large vocabulary continuous short message dictation on embedded devices.</title>
<date>2008</date>
<booktitle>In Proc. ICASSP</booktitle>
<location>Las Vegas, Nevada.</location>
<contexts>
<context position="21340" citStr="Olsen et al., 2008" startWordPosition="3614" endWordPosition="3617">hese models have been used in speech recognition experiments. Two kinds of experiments have been performed: (1) the in-domain LM is augmented with web data, and (2) the LM is adapted to a userspecific vocabulary utilizing web data as an additional data source. One hundred native speakers for each language were recorded reading held-out subsets of the indomain text data. The speech data was partitioned into training and test sets, such that around one fourth of the speakers were reserved for testing. We use a continuous speech recognizer optimized for low memory footprint and fast recognition (Olsen et al., 2008). The recognizer runs on a server (Core2 2.33 GHz) in about one fourth of real time. The LM probabilities are quantized and precompiled together with the speaker-independent acoustic models (intra-word triphones) into a finite state transducer (FST). 3.1 Language model augmentation Each paragraph in the web data is treated as a potential text message and scored according to its similarity to the in-domain data. Relative perplexity is used as the similarity measure. The paragraphs are sorted, lowest relative perplexity first, 161 US English FST size [MB] 10 20 40 70 In-domain 42.7 40.1 39.1 – W</context>
</contexts>
<marker>Olsen, Cao, Ding, Yang, 2008</marker>
<rawString>Jesper Olsen, Yang Cao, Guohong Ding, and Xinxing Yang. 2008. A decoder for large vocabulary continuous short message dictation on embedded devices. In Proc. ICASSP 2008, Las Vegas, Nevada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>Two decades of language modeling: Where do we go from here?</title>
<date>2000</date>
<booktitle>Proceedings of the IEEE,</booktitle>
<volume>88</volume>
<issue>8</issue>
<contexts>
<context position="25751" citStr="Rosenfeld (2000)" startWordPosition="4364" endWordPosition="4365">the in-domain models, and augmentation helps more with larger models. The largest word error rate reduction is observed for the largest Spanish model (9.7 % relative). All WER reductions are statistically significant (one-sided Wilcoxon signed-rank test; level 0.05) except the 10 MB Spanish setup. Although the observed word error rate reductions are mostly smaller than the corresponding 162 perplexity reductions, the results are actually very good, when we consider the fact that considerable reductions in perplexity may typically translate into meager word error reductions; see, for instance, Rosenfeld (2000), Goodman (2001). This suggests that the web texts are very welcome complementary data that improve on the robustness of the recognition. 3.1.3 Modified Kneser-Ney smoothing In the above experiments, Good-Turing (GT) smoothing with Katz backoff was used, although modified Kneser-Ney (KN) interpolation has been shown to outperform other smoothing methods (Chen and Goodman, 1999). However, as demonstrated by Siivola et al. (2007), KN smoothing is not compatible with simple pruning methods such as entropy-based pruning. In order to make a meaningful comparison, we used the revised Kneser pruning </context>
</contexts>
<marker>Rosenfeld, 2000</marker>
<rawString>Ronald Rosenfeld. 2000. Two decades of language modeling: Where do we go from here? Proceedings of the IEEE, 88(8):1270–1278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruhi Sarikaya</author>
<author>Augustin Gravano</author>
<author>Yuqing Gao</author>
</authors>
<title>Rapid language model development using external resources for new spoken dialog domains. In</title>
<date>2005</date>
<booktitle>Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ’05), volume I,</booktitle>
<pages>573--576</pages>
<contexts>
<context position="3366" citStr="Sarikaya et al. (2005)" startWordPosition="564" endWordPosition="567">se n-grams with these new words in their web queries in order to retrieve text of a certain genre. For instance, they succeed in obtaining conversational style phrases, such as “we were friends but we don’t actually have a relationship.” In a number of experiments, word error rate reductions of 2-3 % are obtained on English data, and 6 % on Mandarin. The same method for web data collection is applied by C¸ etin and Stolcke (2005) in meeting and lecture transcription tasks. The web sources reduce perplexity by 10 % and 4.3 %, respectively, and word error rates by 3.5 % and 2.2 %, respectively. Sarikaya et al. (2005) chunk the in-domain text into “n-gram islands” consisting of only content words and excluding frequently occurring stop words. An island such as “stock fund portfolio” is then extended by adding context, producing “my stock fund portfolio”, for instance. Multiple islands are combined using and and or operations to form web queries. Significant word error reductions between 10 and 20 % are obtained; however, the in-domain data set is very small, 1700 phrases, 1All reported percentage differences are relative unless explicitly stated otherwise. Proceedings of the 12th Conference of the European</context>
<context position="5602" citStr="Sarikaya et al., 2005" startWordPosition="923" endWordPosition="926"> chunks with a high out-of-vocabulary (OOV) rate are discarded. Additionally, the chunks are often ranked according to their similarity with the in-domain data, and the lowest ranked chunks are discarded. As a similarity measure, the perplexity of the sentence according to the in-domain LM can be used; for instance, Bulyko et al. (2007). Another measure for ranking is relative perplexity (Weilhammer et al., 2006), where the in-domain perplexity is divided by the perplexity given by an LM trained on the web data. Also the BLEU score familiar from the field of machine translation has been used (Sarikaya et al., 2005). Some criticism has been raised by Sethy et al. (2007), who claim that sentence ranking has an inherent bias towards the center of the in-domain distribution. They propose a data selection algorithm that selects a sentence from the web set, if adding the sentence to the already selected set reduces the relative entropy with respect to the indomain data distribution. The algorithm appears efficient in producing a rather small subset (1/11) of the web data, while degrading the WER only marginally. The current paper describes a new method for query selection and its applications in LM augmentati</context>
</contexts>
<marker>Sarikaya, Gravano, Gao, 2005</marker>
<rawString>Ruhi Sarikaya, Augustin Gravano, and Yuqing Gao. 2005. Rapid language model development using external resources for new spoken dialog domains. In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ’05), volume I, pages 573–576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhinav Sethy</author>
<author>Shrikanth Narayanan</author>
<author>Bhuvana Ramabhadran</author>
</authors>
<title>Data driven approach for language model adaptation using stepwise relative entropy minimization.</title>
<date>2007</date>
<booktitle>In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ’07),</booktitle>
<volume>volume IV,</volume>
<pages>177--180</pages>
<contexts>
<context position="5657" citStr="Sethy et al. (2007)" startWordPosition="933" endWordPosition="936">rded. Additionally, the chunks are often ranked according to their similarity with the in-domain data, and the lowest ranked chunks are discarded. As a similarity measure, the perplexity of the sentence according to the in-domain LM can be used; for instance, Bulyko et al. (2007). Another measure for ranking is relative perplexity (Weilhammer et al., 2006), where the in-domain perplexity is divided by the perplexity given by an LM trained on the web data. Also the BLEU score familiar from the field of machine translation has been used (Sarikaya et al., 2005). Some criticism has been raised by Sethy et al. (2007), who claim that sentence ranking has an inherent bias towards the center of the in-domain distribution. They propose a data selection algorithm that selects a sentence from the web set, if adding the sentence to the already selected set reduces the relative entropy with respect to the indomain data distribution. The algorithm appears efficient in producing a rather small subset (1/11) of the web data, while degrading the WER only marginally. The current paper describes a new method for query selection and its applications in LM augmentation and adaptation using web data. The language models a</context>
</contexts>
<marker>Sethy, Narayanan, Ramabhadran, 2007</marker>
<rawString>Abhinav Sethy, Shrikanth Narayanan, and Bhuvana Ramabhadran. 2007. Data driven approach for language model adaptation using stepwise relative entropy minimization. In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ’07), volume IV, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vesa Siivola</author>
<author>Teemu Hirsim¨aki</author>
<author>Sami Virpioja</author>
</authors>
<title>On growing and pruning KneserNey smoothed n-gram models.</title>
<date>2007</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>15</volume>
<issue>5</issue>
<marker>Siivola, Hirsim¨aki, Virpioja, 2007</marker>
<rawString>Vesa Siivola, Teemu Hirsim¨aki, and Sami Virpioja. 2007. On growing and pruning KneserNey smoothed n-gram models. IEEE Transactions on Audio, Speech and Language Processing, 15(5):1617–1624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>Entropy-based pruning of backoff language models.</title>
<date>1998</date>
<booktitle>In Proc. DARPA BNTU Workshop,</booktitle>
<pages>270--274</pages>
<location>Lansdowne, VA, USA.</location>
<contexts>
<context position="24211" citStr="Stolcke, 1998" startWordPosition="4118" endWordPosition="4119">erplexity calculated on test sets consisting of in-domain text. The comparison is performed on FSTs of different sizes. The FSTs contain the acoustic models, language model and lexicon, but the LM makes up for most of the size. The availability of data varies for the different languages, and therefore the FST sizes are not exactly the same across languages. The LMs have been created using the SRI LM toolkit (Stolcke, 2002). Good-Turing smoothing with Katz backoff (Katz, 1987) has been used, and the different model sizes are obtained by pruning down the full models using entropy-based pruning (Stolcke, 1998). N-gram orders up to five have been tested: 5-grams always work best on the mixture models, whereas the best in-domain models are 4- or 5-grams. For every language and model size, the web mixture model performs better than the corresponding in-domain model. The perplexity reductions obtained increase with the size of the model. Since it is possible to create larger mixture models than in-domain models, there are no in-domain results for the largest model sizes. Especially if large models can be afforded, the perplexity reductions are considerable. The largest improvements are observed for Fre</context>
</contexts>
<marker>Stolcke, 1998</marker>
<rawString>A. Stolcke. 1998. Entropy-based pruning of backoff language models. In Proc. DARPA BNTU Workshop, pages 270–274, Lansdowne, VA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM – an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. ICSLP,</booktitle>
<pages>901--904</pages>
<note>http://www.speech.sri.com/ projects/srilm/.</note>
<contexts>
<context position="24023" citStr="Stolcke, 2002" startWordPosition="4089" endWordPosition="4090">main development test set. 3.1.1 Test set perplexities In Table 1, the prediction abilities of the in-domain and web mixture language models are compared. As an evaluation measure we use perplexity calculated on test sets consisting of in-domain text. The comparison is performed on FSTs of different sizes. The FSTs contain the acoustic models, language model and lexicon, but the LM makes up for most of the size. The availability of data varies for the different languages, and therefore the FST sizes are not exactly the same across languages. The LMs have been created using the SRI LM toolkit (Stolcke, 2002). Good-Turing smoothing with Katz backoff (Katz, 1987) has been used, and the different model sizes are obtained by pruning down the full models using entropy-based pruning (Stolcke, 1998). N-gram orders up to five have been tested: 5-grams always work best on the mixture models, whereas the best in-domain models are 4- or 5-grams. For every language and model size, the web mixture model performs better than the corresponding in-domain model. The perplexity reductions obtained increase with the size of the model. Since it is possible to create larger mixture models than in-domain models, there</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM – an extensible language modeling toolkit. In Proc. ICSLP, pages 901–904. http://www.speech.sri.com/ projects/srilm/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Wan</author>
<author>Thomas Hain</author>
</authors>
<title>Strategies for language model web-data collection. In</title>
<date>2006</date>
<booktitle>Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ’06), volume I,</booktitle>
<pages>1069--1072</pages>
<contexts>
<context position="4610" citStr="Wan and Hain (2006)" startWordPosition="760" endWordPosition="763">s 157–165, Athens, Greece, 30 March – 3 April 2009. c�2009 Association for Computational Linguistics 157 which makes (any) new data a much needed addition. Similarly, Misu and Kawahara (2006) obtain very good word error reductions (20 %) in spoken dialogue systems for software support and sightseeing guidance. Nouns that have high tf/idf scores in the in-domain documents are used in the web queries. The existing in-domain data sets poorly match the speaking style of the task and therefore existing dialogue corpora of different domains are included, which improves the performance considerably. Wan and Hain (2006) select query strings by comparing the n-gram counts within an in-domain topic model to the corresponding counts in an outof-domain background model. Topic-specific ngrams are used as queries, and perplexity reductions of 5.4 % are obtained. It is customary to postprocess and filter the downloaded web texts. Sentence boundaries are detected using some heuristics. Text chunks with a high out-of-vocabulary (OOV) rate are discarded. Additionally, the chunks are often ranked according to their similarity with the in-domain data, and the lowest ranked chunks are discarded. As a similarity measure, </context>
</contexts>
<marker>Wan, Hain, 2006</marker>
<rawString>Vincent Wan and Thomas Hain. 2006. Strategies for language model web-data collection. In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ’06), volume I, pages 1069–1072.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Weilhammer</author>
<author>Matthew N Stuttle</author>
<author>Steve Young</author>
</authors>
<title>Bootstrapping language models for dialogue systems.</title>
<date>2006</date>
<booktitle>In Proc. INTERSPEECH 2006 - ICSLP Ninth International Conference on Spoken Language Processing,</booktitle>
<location>Pittsburgh, PA, USA,</location>
<contexts>
<context position="5396" citStr="Weilhammer et al., 2006" startWordPosition="886" endWordPosition="889">ic ngrams are used as queries, and perplexity reductions of 5.4 % are obtained. It is customary to postprocess and filter the downloaded web texts. Sentence boundaries are detected using some heuristics. Text chunks with a high out-of-vocabulary (OOV) rate are discarded. Additionally, the chunks are often ranked according to their similarity with the in-domain data, and the lowest ranked chunks are discarded. As a similarity measure, the perplexity of the sentence according to the in-domain LM can be used; for instance, Bulyko et al. (2007). Another measure for ranking is relative perplexity (Weilhammer et al., 2006), where the in-domain perplexity is divided by the perplexity given by an LM trained on the web data. Also the BLEU score familiar from the field of machine translation has been used (Sarikaya et al., 2005). Some criticism has been raised by Sethy et al. (2007), who claim that sentence ranking has an inherent bias towards the center of the in-domain distribution. They propose a data selection algorithm that selects a sentence from the web set, if adding the sentence to the already selected set reduces the relative entropy with respect to the indomain data distribution. The algorithm appears ef</context>
</contexts>
<marker>Weilhammer, Stuttle, Young, 2006</marker>
<rawString>Karl Weilhammer, Matthew N. Stuttle, and Steve Young. 2006. Bootstrapping language models for dialogue systems. In Proc. INTERSPEECH 2006 - ICSLP Ninth International Conference on Spoken Language Processing, Pittsburgh, PA, USA, September 17–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
<author>R Rosenfeld</author>
</authors>
<title>Improving trigram language modeling with the world wide web. In</title>
<date>2001</date>
<booktitle>Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ’01).,</booktitle>
<volume>1</volume>
<pages>533--536</pages>
<contexts>
<context position="2223" citStr="Zhu and Rosenfeld (2001)" startWordPosition="363" endWordPosition="366">tching, in-domain, text may be both difficult and expensive to obtain in the large quantities that are needed. A popular solution is to utilize the World Wide Web as a source of additional text for LM training. A small in-domain set is used as seed data, and more data of the same kind is retrieved from the web. A decade ago, Berger and Miller (1998) proposed a just-in-time LM that updated the current LM by retrieving data from the web using recent recognition hypotheses as queries submitted to a search engine. Perplexity reductions of up to 10 % were reported.1 Many other works have followed. Zhu and Rosenfeld (2001) retrieved page and phrase counts from the web in order to update the probabilities of infrequent trigrams that occur in N-best lists. Word error rate (WER) reductions of about 3 % were obtained on TREC-7 data. In more recent work, the focus has turned to the collection of text rather than n-gram statistics based on page counts. More effort has been put into the selection of query strings. Bulyko et al. (2003; 2007) first extend their baseline vocabulary with words from a small in-domain training corpus. They then use n-grams with these new words in their web queries in order to retrieve text </context>
</contexts>
<marker>Zhu, Rosenfeld, 2001</marker>
<rawString>Xiaojin Zhu and R. Rosenfeld. 2001. Improving trigram language modeling with the world wide web. In Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP ’01)., volume 1, pages 533–536.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>