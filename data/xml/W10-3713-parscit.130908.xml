<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012067">
<title confidence="0.99784">
An Efficient, Generic Approach to Extracting Multi-Word Expressions
from Dependency Trees
</title>
<author confidence="0.948378">
Scott Martens and Vincent Vandeghinste
</author>
<affiliation confidence="0.8540505">
Centrum voor Computerlinguistiek
Katholieke Universiteit Leuven
</affiliation>
<email confidence="0.992293">
scott@ccl.kuleuven.be &amp; vincent@ccl.kuleuven.be
</email>
<sectionHeader confidence="0.993663" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999923909090909">
The Varro toolkit offers an intuitive mech-
anism for extracting syntactically mo-
tivated multi-word expressions (MWEs)
from dependency treebanks by looking for
recurring connected subtrees instead of
subsequences in strings. This approach
can find MWEs that are in varying orders
and have words inserted into their compo-
nents. This paper also proposes descrip-
tion length gain as a statistical correlation
measure well-suited to tree structures.
</bodyText>
<sectionHeader confidence="0.998796" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999482772727273">
Automatic MWE extraction techniques operate
by using either statistical correlation tests on the
distributions of words in corpora, syntactic pat-
tern matching techniques, or by using hypothe-
ses about the semantic non-compositionality of
MWEs. This paper proposes a purely statistical
technique for MWE extraction that incorporates
syntactic considerations by operating entirely on
dependency treebanks. On the whole, dependency
trees have one node for each word in the sentence,
although most dependency schemes vary from this
to some extent in practice. See Figure 1 for an
example dependency tree produced automatically
by the Stanford parser from the English language
data in the Europarl corpus. (Marneffe, 2008;
Koehn, 2005)
Identifying MWEs with subtrees in dependency
trees is not a new idea. It is close to the formal def-
inition offered in Mel’ˇcuk (1998), and is applied
computationally in Debusmann (2004) However,
using dependency treebanks to automatically ex-
tract MWEs is fairly new and few MWE extrac-
</bodyText>
<figureCaption confidence="0.904567">
Figure 1. A dependency tree of the sentence
“The Minutes ofyesterday’s sitting have been dis-
tributed.”
</figureCaption>
<bodyText confidence="0.999311333333333">
tion projects to date take advantage of dependency
information directly. There are a number of rea-
sons why this is the case:
</bodyText>
<listItem confidence="0.998647142857143">
• String-based algorithms are not readily ap-
plicable to trees.
• Tree structures yield a potentially combina-
torial number of candidate MWEs, a prob-
lem shared with methods that look for strings
with gaps.
• Statistical techniques used in MWE extrac-
</listItem>
<bodyText confidence="0.840839833333333">
tion, like pointwise mutual information, are
two-variable tests that are not easy to apply
to larger sets of words.
The tool and statistical procedures used in this
research are not language dependent and can op-
erate on MWE of any size, producing depen-
</bodyText>
<page confidence="0.996103">
85
</page>
<figure confidence="0.74334125">
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 85–88,
Beijing, August 2010
(a) “The Minutes (...) (b) “(...) Minutes of
have been distributed” (...) distributed.”
</figure>
<figureCaption confidence="0.902087">
Figure 2. Two induced subtrees of the dependency
tree in Figure 1. Note that both correspond to dis-
continuous phrases in the original sentence.
</figureCaption>
<bodyText confidence="0.9999335625">
dency pairs, short phrases of any syntactic cate-
gory, lengthy formulas and idioms. There are no
underlying linguistic assumptions in this method-
ology except that a MWE must consist of words
that have a fixed set of dependency links in a
treebank. Even word order and distance between
words is not directly assumed to be significant.
The input, however, requires substantial linguis-
tic pre-processing – particularly, the identification
of at least some of the dependency relations in
the corpora used. Retrieving MWEs that contain
abstract categories, like information about the ar-
guments of verbs or part-of-speech information
for unincluded elements, requires using treebanks
that contain that information, rather than purely
lexical dependency trees.
</bodyText>
<sectionHeader confidence="0.910228" genericHeader="method">
2 Varro Toolkit for Frequent Subtree
Discovery
</sectionHeader>
<bodyText confidence="0.970508083333333">
The Varro toolkit is an open-source application for
efficiently extracting frequent closed unordered
induced subtrees from treebanks with labeled
nodes and edges. It is publicly available under an
open source license.1 For a fuller description of
Varro, including the algorithm and data structures
used and a formal definition offrequent closed un-
ordered induced subtrees, see Martens (2010).
Given some tree like the one in Figure 1, an in-
duced subtree is a connected subset of its nodes
and the edges that connect them, as shown in
Figure 2. Subtrees do not necessarily represent
</bodyText>
<footnote confidence="0.919311">
1http://varro.sourceforge.net/
</footnote>
<bodyText confidence="0.999658294117647">
fixed sequences of words in the original text,
they include syntactically motivated discontinu-
ous phrases. This dramatically reduces the num-
ber of candidate discontinuous MWEs when com-
pared to string methods. An unordered induced
subtree is a subtree where the words may appear
with different word orders, but the subtree is still
identified as the same if the dependency structure
is the same. A frequent closed subtree is a sub-
tree of a treebank that appears more than some
fixed number of times and where there is no sub-
tree that contains it and appears the same number
of times. Finding only closed subtrees reduces the
combinatorial explosion of possible subtrees, and
ensures that each candidate MWE includes all the
words the that co-occur with it every time it ap-
pears.
</bodyText>
<sectionHeader confidence="0.960965" genericHeader="method">
3 Preprocessing and Extracting Subtrees
</sectionHeader>
<bodyText confidence="0.997168">
The English language portion of the Europarl
Corpus, version 3 was parsed using the Stanford
parser, which produces both a constituentcy parse
and a dependency tree as its output.2 The depen-
dency information for each sentence was trans-
formed into the XML input format used by Varro.
The result is a treebank of 1.4 million individual
parse trees, each representing a sentence, and a to-
tal of 36 million nodes.
In order to test the suitability of Varro for large
treebanks and intensive extractions, all recurring
closed subtrees that appear at least twice were ex-
tracted. This took a total of 129,312.27 seconds
(just over 34 hours), producing 9,976,355 frequent
subtrees, of which 9,909,269 contain more than
one word and are therefore candidate MWEs.
A fragment of the Varro output can be seen in
Figure 3. The nodes of the subtrees returned are
not in a grammatical surface order. However, the
original source order can be recovered by using
the locations where each subtree appears to find
the order in the treebank. Doing so for the tree
in Figure 3 shows what kinds of MWEs this ap-
proach can extract from treebanks. The under-
lined words in the following sentences are the
ones included in the subtree in Figure 3:
</bodyText>
<footnote confidence="0.98254">
2This portion of the work was done by our colleagues
J¨org Tiedemann and Gideon Kotz´e at RU Groningen.
</footnote>
<page confidence="0.99447">
86
</page>
<figureCaption confidence="0.756250166666667">
Figure 3. An example of a found subtree and can-
didate MWE. This subtree appears in 2581 unique
locations in the treebank, and only the locations
of the first few places in the treebank where it ap-
pears are reproduced here, but all 2581 are in the
Marro output data.
</figureCaption>
<bodyText confidence="0.96480625">
The vote will take place tomorrow at 9 a.m.
The vote will take place today at noon.
The vote will take place tomorrow, Wednesday
at 11:30 a.m.
</bodyText>
<sectionHeader confidence="0.8776745" genericHeader="method">
4 Statistical Methods for Evaluating
Subtrees as MWEs
</sectionHeader>
<bodyText confidence="0.999949133333333">
To evaluate the quality of subtrees as MWEs,
we propose to use a simplified form of de-
scription length gain (DLG), a metric derived
from algorithmic information theory and Mini-
mum Description Length methods (MDL). (Ris-
sanen, 1978; Gr¨unwald, 2005) Given a quantity of
data of any kind that can be stored as a digital in-
formation in a computer, and some process which
transforms the data in a way that can be reversed,
DLG is the measure of how the space required to
store that data changes when it is transformed.
To calculate DLG, one must first decide how to
encode the trees in the treebank. It is not neces-
sary to actually encode the treebank in any par-
ticular format. All that is necessary is to be able
to calculate how many bits the treebank would re-
quire to encode it.
Space prevents the full description of the en-
coding mechanism used or the way DLG is cal-
culated. The encoding mechanism is largely the
same as the one described in Luccio et al. (2001)
Converting the trees to strings makes it possible to
calculate the encoding size by calculating the en-
tropy of the treebank in that encoding using clas-
sical information theoric methods.
In effect, the procedure for calculating DLG is
to calculate the entropy of the whole treebank,
given the encoding method chosen, and then to
recalculate its entropy given some subtree which
is removed from the treebank and replaced with a
symbol that acts as an abbreviation. That subtree
is then be added back to the treebank once as part
of a look-up table. These methods are largely the
same as those used by common data compression
software.
DLG is the difference between these two en-
tropy measures.3
Because of the sensitivity of DLG to low fre-
quencies, it can be viewed as a kind of non-
parametric significance test. Any frequent struc-
ture that cannot be used to compress the treebank
has a negative DLG and is not frequent enough or
large enough to be considered significant.
Marro reports several statistics related to DLG
for each extracted subtree, as shown in Figure 3:
</bodyText>
<listItem confidence="0.994330692307693">
• Unique appearances (reported by the root-
Count attribute) is the number of times the
extracted subtree appears with a different
root node.
• Entropy is the entropy of the extracted sub-
tree, given the encoding scheme that Marro
uses to calculate DLG.
• Algorithmic mutual information (AMI) (re-
ported with the mi attribute) is the DLG of
the extracted subtree divided by its number
of unique appearances in the treebank.
• Compression is the AMI divided by the en-
tropy.
</listItem>
<bodyText confidence="0.753683">
AMI is comparable to pointwise mutual infor-
mation (PMI) in that both are measures of redun-
dant bits, while compression is comparable to nor-
malized mutual information metrics.
</bodyText>
<footnote confidence="0.996254">
3This is a very simplified picture of MDL and DLG met-
rics.
</footnote>
<page confidence="0.999362">
87
</page>
<sectionHeader confidence="0.996713" genericHeader="evaluation">
5 Results and Conclusions
</sectionHeader>
<bodyText confidence="0.945440333333333">
We used the metrics described above to sort the
nearly 10 million frequent subtrees of the parsed
English Europarl corpus. We found that:
</bodyText>
<listItem confidence="0.937217">
• Compression and AMI metrics strongly fa-
vor very large subtrees that represent highly
formulaic language.
• DLG alone finds smaller, high frequency ex-
pressions more like MWEs favoured by ter-
minologists and collocation analysis.
</listItem>
<bodyText confidence="0.9987255">
For example, the highest DLG subtree matches
the phrase “the European Union”. This is not
unexpected given the source of the data and con-
stitutes a very positive result. Among the nearly
10 million candidate MWEs extracted, it also
places near the top discontinuous phrases like
“... am speaking ... in my ... capacity as ...”.
Using both compression ratio and AMI, the
same subtree appears first. It is present 26 times
in the treebank, with a compression score of 0.894
and an AMI of 386.92 bits. It corresponds to the
underlined words in the sentence below:
The next item is the recommendation for
second reading (A4-0245/99), on behalf of
the Committee on Transport and Tourism, on
the common position adopted by the Council
(13651/3/98 - C4-0037/99-96/0182 (COD) with
a view to adopting a Council Directive on the
charging of heavy goods vehicles for the use of
certain infrastructures.
This is precisely the kind of formulaic speech,
with various gaps to fill in, which is of great inter-
est for sub-sentential translation memory systems.
(Gotti et al., 2005; Vandeghinste and Martens,
2010)
We believe this kind of strategy can substan-
tially enhance MWE extraction techniques. It in-
tegrates syntax into MWE extraction in an intu-
itive way. Furthermore, description length gain
offers a unified statistical account of an MWE as
a linguistically motivated structure that can com-
press relevant corpus data. It is similar to the types
of statistical tests already used, but is also non-
parametric and suitable for the study of arbitrary
MWEs, not just two-word MWEs or phrases that
occur without gaps.
</bodyText>
<sectionHeader confidence="0.998333" genericHeader="conclusions">
6 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9929342">
This research is supported by the AMASS++
Project,4 directly funded by the Institute for the
Promotion of Innovation by Science and Technol-
ogy in Flanders (IWT) (SBO IWT 060051) and by
the PaCo-MT project (STE-07007).
</bodyText>
<sectionHeader confidence="0.998477" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999598097560976">
Debusmann, Ralph. 2004. Multiword expressions as
dependency subgraphs. Proceedings of the 2004
ACL Workshop on Multiword Expressions, pp. 56-
63.
Gotti, Fabrizio, Philippe Langlais, Eliott Macklovitch,
Didier Bourigault, Benoit Robichaud and Claude
Coulombe. 2005. 3GTM: A third-generation trans-
lation memory. Proceedings of the 3rd Computa-
tional Linguistics in the North-East Workshop, pp.
8–15.
Gr¨unwald, Peter. 2005. A tutorial introduction to
the minimum description length principle. In: Ad-
vances in Minimum Description Length: Theory
and Applications, (Peter Gr¨unwald, In Jae Myung,
Mark Pitt, eds.), MIT Press, pp. 23–81.
Koehn, Philipp. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. Proceedings of the
10th MT Summit, pp. 79–86.
Luccio, Fabrizio, Antonio Enriquez, Pablo Rieumont
and Linda Pagli. 2001. Exact Rooted Subtree
Matching in Sublinear Time. Universit`a di Pisa
Technical Report TR-01-14.
de Marneffe, Marie-Catherine and Christopher D.
Manning. 2008. The Stanford typed dependencies
representation. Proceedings of the 2008 CoLing
Workshop on Cross-framework and Cross-domain
Parser Evaluation, pp. 1–8.
Martens, Scott. 2010. Varro: An Algorithm and
Toolkit for Regular Structure Discovery in Tree-
banks. Proceedings of the 2010 Int’l Conf. on Com-
putational Linguistics (CoLing), in press.
Mel’ˇcuk, Igor. 1998. Collocations and Lexical Func-
tions. In: Phraseology. Theory, Analysis, and Ap-
plications, (Anthony Cowie ed.), pp. 23–53.
Rissanen, Jorma. 1978. Modeling by shortest data
description. Automatica, vol. 14, pp. 465–471.
Vandeghinste, Vincent and Scott Martens. 2010.
Bottom-up transfer in Example-based Machine
Translation. Proceedings of the 2010 Conf. of the
European Association for Machine Translation, in
press.
</reference>
<footnote confidence="0.844922">
4http://www.cs.kuleuven.be/˜liir/projects/amass/
</footnote>
<page confidence="0.998645">
88
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.196795">
<title confidence="0.991525">An Efficient, Generic Approach to Extracting Multi-Word from Dependency Trees</title>
<author confidence="0.532349333333333">Scott Martens</author>
<author confidence="0.532349333333333">Vincent Centrum voor Katholieke Universiteit</author>
<abstract confidence="0.9991505">offers an intuitive mechfor extracting moexpressions (MWEs) from dependency treebanks by looking for recurring connected subtrees instead of subsequences in strings. This approach can find MWEs that are in varying orders and have words inserted into their compo- This paper also proposes descriplength gain a statistical correlation measure well-suited to tree structures.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ralph Debusmann</author>
</authors>
<title>Multiword expressions as dependency subgraphs.</title>
<date>2004</date>
<booktitle>Proceedings of the 2004 ACL Workshop on Multiword Expressions,</booktitle>
<pages>56--63</pages>
<contexts>
<context position="1623" citStr="Debusmann (2004)" startWordPosition="229" endWordPosition="230">WE extraction that incorporates syntactic considerations by operating entirely on dependency treebanks. On the whole, dependency trees have one node for each word in the sentence, although most dependency schemes vary from this to some extent in practice. See Figure 1 for an example dependency tree produced automatically by the Stanford parser from the English language data in the Europarl corpus. (Marneffe, 2008; Koehn, 2005) Identifying MWEs with subtrees in dependency trees is not a new idea. It is close to the formal definition offered in Mel’ˇcuk (1998), and is applied computationally in Debusmann (2004) However, using dependency treebanks to automatically extract MWEs is fairly new and few MWE extracFigure 1. A dependency tree of the sentence “The Minutes ofyesterday’s sitting have been distributed.” tion projects to date take advantage of dependency information directly. There are a number of reasons why this is the case: • String-based algorithms are not readily applicable to trees. • Tree structures yield a potentially combinatorial number of candidate MWEs, a problem shared with methods that look for strings with gaps. • Statistical techniques used in MWE extraction, like pointwise mutua</context>
</contexts>
<marker>Debusmann, 2004</marker>
<rawString>Debusmann, Ralph. 2004. Multiword expressions as dependency subgraphs. Proceedings of the 2004 ACL Workshop on Multiword Expressions, pp. 56-63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Gotti</author>
<author>Philippe Langlais</author>
<author>Eliott Macklovitch</author>
<author>Didier Bourigault</author>
<author>Benoit Robichaud</author>
<author>Claude Coulombe</author>
</authors>
<title>3GTM: A third-generation translation memory.</title>
<date>2005</date>
<booktitle>Proceedings of the 3rd Computational Linguistics in the North-East Workshop,</booktitle>
<pages>8--15</pages>
<contexts>
<context position="11012" citStr="Gotti et al., 2005" startWordPosition="1805" endWordPosition="1808">compression score of 0.894 and an AMI of 386.92 bits. It corresponds to the underlined words in the sentence below: The next item is the recommendation for second reading (A4-0245/99), on behalf of the Committee on Transport and Tourism, on the common position adopted by the Council (13651/3/98 - C4-0037/99-96/0182 (COD) with a view to adopting a Council Directive on the charging of heavy goods vehicles for the use of certain infrastructures. This is precisely the kind of formulaic speech, with various gaps to fill in, which is of great interest for sub-sentential translation memory systems. (Gotti et al., 2005; Vandeghinste and Martens, 2010) We believe this kind of strategy can substantially enhance MWE extraction techniques. It integrates syntax into MWE extraction in an intuitive way. Furthermore, description length gain offers a unified statistical account of an MWE as a linguistically motivated structure that can compress relevant corpus data. It is similar to the types of statistical tests already used, but is also nonparametric and suitable for the study of arbitrary MWEs, not just two-word MWEs or phrases that occur without gaps. 6 Acknowledgements This research is supported by the AMASS++ </context>
</contexts>
<marker>Gotti, Langlais, Macklovitch, Bourigault, Robichaud, Coulombe, 2005</marker>
<rawString>Gotti, Fabrizio, Philippe Langlais, Eliott Macklovitch, Didier Bourigault, Benoit Robichaud and Claude Coulombe. 2005. 3GTM: A third-generation translation memory. Proceedings of the 3rd Computational Linguistics in the North-East Workshop, pp. 8–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Gr¨unwald</author>
</authors>
<title>A tutorial introduction to the minimum description length principle. In:</title>
<date>2005</date>
<booktitle>Advances in Minimum Description Length: Theory</booktitle>
<pages>23--81</pages>
<editor>and Applications, (Peter Gr¨unwald, In Jae Myung, Mark Pitt, eds.),</editor>
<publisher>MIT Press,</publisher>
<marker>Gr¨unwald, 2005</marker>
<rawString>Gr¨unwald, Peter. 2005. A tutorial introduction to the minimum description length principle. In: Advances in Minimum Description Length: Theory and Applications, (Peter Gr¨unwald, In Jae Myung, Mark Pitt, eds.), MIT Press, pp. 23–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>Proceedings of the 10th MT Summit,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="1437" citStr="Koehn, 2005" startWordPosition="198" endWordPosition="199">ds in corpora, syntactic pattern matching techniques, or by using hypotheses about the semantic non-compositionality of MWEs. This paper proposes a purely statistical technique for MWE extraction that incorporates syntactic considerations by operating entirely on dependency treebanks. On the whole, dependency trees have one node for each word in the sentence, although most dependency schemes vary from this to some extent in practice. See Figure 1 for an example dependency tree produced automatically by the Stanford parser from the English language data in the Europarl corpus. (Marneffe, 2008; Koehn, 2005) Identifying MWEs with subtrees in dependency trees is not a new idea. It is close to the formal definition offered in Mel’ˇcuk (1998), and is applied computationally in Debusmann (2004) However, using dependency treebanks to automatically extract MWEs is fairly new and few MWE extracFigure 1. A dependency tree of the sentence “The Minutes ofyesterday’s sitting have been distributed.” tion projects to date take advantage of dependency information directly. There are a number of reasons why this is the case: • String-based algorithms are not readily applicable to trees. • Tree structures yield </context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Koehn, Philipp. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. Proceedings of the 10th MT Summit, pp. 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabrizio Luccio</author>
<author>Antonio Enriquez</author>
<author>Pablo Rieumont</author>
<author>Linda Pagli</author>
</authors>
<title>Exact Rooted Subtree Matching in Sublinear Time. Universit`a di Pisa</title>
<date>2001</date>
<tech>Technical Report TR-01-14.</tech>
<contexts>
<context position="7782" citStr="Luccio et al. (2001)" startWordPosition="1260" endWordPosition="1263">r, and some process which transforms the data in a way that can be reversed, DLG is the measure of how the space required to store that data changes when it is transformed. To calculate DLG, one must first decide how to encode the trees in the treebank. It is not necessary to actually encode the treebank in any particular format. All that is necessary is to be able to calculate how many bits the treebank would require to encode it. Space prevents the full description of the encoding mechanism used or the way DLG is calculated. The encoding mechanism is largely the same as the one described in Luccio et al. (2001) Converting the trees to strings makes it possible to calculate the encoding size by calculating the entropy of the treebank in that encoding using classical information theoric methods. In effect, the procedure for calculating DLG is to calculate the entropy of the whole treebank, given the encoding method chosen, and then to recalculate its entropy given some subtree which is removed from the treebank and replaced with a symbol that acts as an abbreviation. That subtree is then be added back to the treebank once as part of a look-up table. These methods are largely the same as those used by </context>
</contexts>
<marker>Luccio, Enriquez, Rieumont, Pagli, 2001</marker>
<rawString>Luccio, Fabrizio, Antonio Enriquez, Pablo Rieumont and Linda Pagli. 2001. Exact Rooted Subtree Matching in Sublinear Time. Universit`a di Pisa Technical Report TR-01-14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>Proceedings of the 2008 CoLing Workshop on Cross-framework and Cross-domain Parser Evaluation,</booktitle>
<pages>1--8</pages>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>de Marneffe, Marie-Catherine and Christopher D. Manning. 2008. The Stanford typed dependencies representation. Proceedings of the 2008 CoLing Workshop on Cross-framework and Cross-domain Parser Evaluation, pp. 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Martens</author>
</authors>
<title>Varro: An Algorithm and Toolkit for Regular Structure Discovery in Treebanks.</title>
<date>2010</date>
<booktitle>Proceedings of the 2010 Int’l Conf. on Computational Linguistics (CoLing),</booktitle>
<note>in press.</note>
<contexts>
<context position="3991" citStr="Martens (2010)" startWordPosition="598" endWordPosition="599"> arguments of verbs or part-of-speech information for unincluded elements, requires using treebanks that contain that information, rather than purely lexical dependency trees. 2 Varro Toolkit for Frequent Subtree Discovery The Varro toolkit is an open-source application for efficiently extracting frequent closed unordered induced subtrees from treebanks with labeled nodes and edges. It is publicly available under an open source license.1 For a fuller description of Varro, including the algorithm and data structures used and a formal definition offrequent closed unordered induced subtrees, see Martens (2010). Given some tree like the one in Figure 1, an induced subtree is a connected subset of its nodes and the edges that connect them, as shown in Figure 2. Subtrees do not necessarily represent 1http://varro.sourceforge.net/ fixed sequences of words in the original text, they include syntactically motivated discontinuous phrases. This dramatically reduces the number of candidate discontinuous MWEs when compared to string methods. An unordered induced subtree is a subtree where the words may appear with different word orders, but the subtree is still identified as the same if the dependency struct</context>
<context position="11045" citStr="Martens, 2010" startWordPosition="1811" endWordPosition="1812"> of 386.92 bits. It corresponds to the underlined words in the sentence below: The next item is the recommendation for second reading (A4-0245/99), on behalf of the Committee on Transport and Tourism, on the common position adopted by the Council (13651/3/98 - C4-0037/99-96/0182 (COD) with a view to adopting a Council Directive on the charging of heavy goods vehicles for the use of certain infrastructures. This is precisely the kind of formulaic speech, with various gaps to fill in, which is of great interest for sub-sentential translation memory systems. (Gotti et al., 2005; Vandeghinste and Martens, 2010) We believe this kind of strategy can substantially enhance MWE extraction techniques. It integrates syntax into MWE extraction in an intuitive way. Furthermore, description length gain offers a unified statistical account of an MWE as a linguistically motivated structure that can compress relevant corpus data. It is similar to the types of statistical tests already used, but is also nonparametric and suitable for the study of arbitrary MWEs, not just two-word MWEs or phrases that occur without gaps. 6 Acknowledgements This research is supported by the AMASS++ Project,4 directly funded by the </context>
</contexts>
<marker>Martens, 2010</marker>
<rawString>Martens, Scott. 2010. Varro: An Algorithm and Toolkit for Regular Structure Discovery in Treebanks. Proceedings of the 2010 Int’l Conf. on Computational Linguistics (CoLing), in press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Mel’ˇcuk</author>
</authors>
<title>Collocations and Lexical Functions.</title>
<date>1998</date>
<pages>23--53</pages>
<editor>In: Phraseology. Theory, Analysis, and Applications, (Anthony Cowie ed.),</editor>
<marker>Mel’ˇcuk, 1998</marker>
<rawString>Mel’ˇcuk, Igor. 1998. Collocations and Lexical Functions. In: Phraseology. Theory, Analysis, and Applications, (Anthony Cowie ed.), pp. 23–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorma Rissanen</author>
</authors>
<title>Modeling by shortest data description.</title>
<date>1978</date>
<journal>Automatica,</journal>
<volume>14</volume>
<pages>465--471</pages>
<contexts>
<context position="7050" citStr="Rissanen, 1978" startWordPosition="1122" endWordPosition="1124">s in 2581 unique locations in the treebank, and only the locations of the first few places in the treebank where it appears are reproduced here, but all 2581 are in the Marro output data. The vote will take place tomorrow at 9 a.m. The vote will take place today at noon. The vote will take place tomorrow, Wednesday at 11:30 a.m. 4 Statistical Methods for Evaluating Subtrees as MWEs To evaluate the quality of subtrees as MWEs, we propose to use a simplified form of description length gain (DLG), a metric derived from algorithmic information theory and Minimum Description Length methods (MDL). (Rissanen, 1978; Gr¨unwald, 2005) Given a quantity of data of any kind that can be stored as a digital information in a computer, and some process which transforms the data in a way that can be reversed, DLG is the measure of how the space required to store that data changes when it is transformed. To calculate DLG, one must first decide how to encode the trees in the treebank. It is not necessary to actually encode the treebank in any particular format. All that is necessary is to be able to calculate how many bits the treebank would require to encode it. Space prevents the full description of the encoding </context>
</contexts>
<marker>Rissanen, 1978</marker>
<rawString>Rissanen, Jorma. 1978. Modeling by shortest data description. Automatica, vol. 14, pp. 465–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vincent Vandeghinste</author>
<author>Scott Martens</author>
</authors>
<title>Bottom-up transfer in Example-based Machine Translation.</title>
<date>2010</date>
<booktitle>Proceedings of the</booktitle>
<note>in press.</note>
<contexts>
<context position="11045" citStr="Vandeghinste and Martens, 2010" startWordPosition="1809" endWordPosition="1812"> 0.894 and an AMI of 386.92 bits. It corresponds to the underlined words in the sentence below: The next item is the recommendation for second reading (A4-0245/99), on behalf of the Committee on Transport and Tourism, on the common position adopted by the Council (13651/3/98 - C4-0037/99-96/0182 (COD) with a view to adopting a Council Directive on the charging of heavy goods vehicles for the use of certain infrastructures. This is precisely the kind of formulaic speech, with various gaps to fill in, which is of great interest for sub-sentential translation memory systems. (Gotti et al., 2005; Vandeghinste and Martens, 2010) We believe this kind of strategy can substantially enhance MWE extraction techniques. It integrates syntax into MWE extraction in an intuitive way. Furthermore, description length gain offers a unified statistical account of an MWE as a linguistically motivated structure that can compress relevant corpus data. It is similar to the types of statistical tests already used, but is also nonparametric and suitable for the study of arbitrary MWEs, not just two-word MWEs or phrases that occur without gaps. 6 Acknowledgements This research is supported by the AMASS++ Project,4 directly funded by the </context>
</contexts>
<marker>Vandeghinste, Martens, 2010</marker>
<rawString>Vandeghinste, Vincent and Scott Martens. 2010. Bottom-up transfer in Example-based Machine Translation. Proceedings of the 2010 Conf. of the European Association for Machine Translation, in press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>