<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.087334">
<title confidence="0.983909">
Discovering Phonotactic Finite-State Automata by Genetic Search
</title>
<author confidence="0.993593">
Anja Belz
</author>
<affiliation confidence="0.9948335">
School of Cognitive and Computing Sciences
University of Sussex
</affiliation>
<address confidence="0.994062">
Brighton BN1 9QH, UK
</address>
<email confidence="0.964315">
email: an j ab@cogs . susx . ac . uk
</email>
<sectionHeader confidence="0.992826" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997778">
This paper presents a genetic algorithm based
approach to the automatic discovery of finite-
state automata (FSAs) from positive data.
FSAs are commonly used in computational
phonology, but — given the limited learnabil-
ity of FSAs from arbitrary language subsets
— are usually constructed manually. The ap-
proach presented here offers a practical auto-
matic method that helps reduce the cost of man-
ual FSA construction.
</bodyText>
<sectionHeader confidence="0.987898" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.997823066666667">
Finite-state techniques have always been cen-
tral to computational phonology. Definite
FSAs are commonly used to encode phonotac-
tic constraints in a variety of NLP contexts.
Such FSAs are usually constructed in a time-
consuming manual process.
Following notational convention, an FSA is a
quintuple (S, I, 8, so, F), in which S is a finite
nonempty set of states, I is a finite nonempty
alphabet, 8 is the state transition function, so
is the initial state, and F is a nonempty set
of final states in S. The language L accepted
by the finite automaton A, denoted L(A), is
{x16(so, x) E 11.
Generally, the problem considered here is that
of identifying a language L from a fixed finite
sample D = (D, D), where D+ C L and
n L = 0 (D— may be empty). If D— is
empty, and D+ is structurally complete with re-
gard to L, the problem is not complex, and there
exist a number of algorithms based on first con-
structing a canonical automaton from the data
set which is then reduced or merged in various
ways. If D+ is an arbitrary strict subset of L,
the problem is less clearly defined. Since any fi-
nite sample is consistent with an infinite number
of languages, L cannot be identified uniquely
from D+. &amp;quot;...the best we can hope to do is to
infer a grammar that will describe the strings
in D+ and predict other strings that in some
sense are of the same nature as those contained
in D+.&amp;quot; (Fu and Booth, 1986, p. 345)
To constrain the set of possible languages L,
the inferred grammar is typically required to
be as small as possible. However, the prob-
lem of finding a minimal grammar consistent
with a given sample D was shown to be NP-
hard by Gold (1978). Nonapproximability re-
sults of varying strength have been added since.
In the special case where D contains all strings
of symbols over a finite alphabet I of length
shorter than k, a polynomial-time algorithm can
be found (Trakhtenbrot and Barzdin, 1973), but
if even a small fraction of examples is missing,
the problem is again NP-hard (Angluin, 1978).
</bodyText>
<sectionHeader confidence="0.978004" genericHeader="introduction">
2 Task Description
</sectionHeader>
<bodyText confidence="0.9999930625">
Given a known finite alphabet of symbols I, a
target finite-state language L, and a data sam-
ple D+ C L C I*, the task is to find an FSA A,
such that L(A) is consistent with D+, L(A) is
a superset of D+ encoding generalisation over
the structural regularities of D+, and the size
of S is as small as possible. Where the target
language is known in advance, the degree of lan-
guage and size approximation can be measured,
and its adequacy relative to training set size and
representativeness can be described. In the case
of inference of automata that encode (part of)
a phonological grammar, language approxima-
tion and its degree of adequacy can be described
relative to a set of theoretical linguistic assump-
tions that describes a target grammar.
</bodyText>
<sectionHeader confidence="0.979247" genericHeader="method">
3 Search Method
</sectionHeader>
<bodyText confidence="0.9963265">
By direct analogy with natural evolution, ge-
netic algorithms (GAs) work with a population
</bodyText>
<page confidence="0.966136">
1472
</page>
<bodyText confidence="0.999984791666667">
of individuals each of which represents a candi-
date solution to the given problem. These indi-
viduals are assigned a fitness score and on its ba-
sis are selected to &apos;mate&apos;, and produce the next
generation. This process is typically iterated
until the population has converged, i.e. when
individuals have reached a degree of similarity
beyond which further improvement becomes im-
possible. Characteristics that form part of good
solutions are passed on through the generations
and begin to combine in the offspring to ap-
proach global optima, an effect that has been
explained in terms of the building block hypothe-
sis (Goldberg, 1989). Unlike other search meth-
ods, GAs sample different areas of the search
space simultaneously, and are therefore able to
escape local optima and to avoid areas of low
fitness.
The main issues in GA design are encoding
the candidate solutions (individuals) as data
structures for the GA to work on, defining a
fitness function that accurately expresses the
goodness of candidate solutions, and designing
genetic operators that combine and alter the
genetic material of good solutions (parents) to
produce new, better solutions (offspring).
In the present GA1, the state-transition ma-
trices of FSAs are directly converted into geno-
types. Mutation randomly adds or deletes one
transition in each FSA and a variant of uniform
crossover tends to preserve the general struc-
ture of the fitter parent, while adding some sub-
structures from the weaker parent (offspring can
be larger or smaller than both parents). Fit-
ness is evaluated according to three fitness cri-
teria. The first two follow directly from the
task description: (1) size of S (smallness), and
(2) ability to parse strings in D+ (consistency),
where ability to partially parse strings is also
rewarded. Used on their own, however, these
criteria lead search in the direction of universal
automata that produce all strings x E /* up to
the length of the longest string in D+. To avoid
this, (3) an overgeneration criterion is added
that requires automata to achieve a given degree
of generalisation, such that the size of L(A) is
equal to the size of the target language (where
the target language is not known, its size is es-
</bodyText>
<table confidence="0.976628666666667">
&apos;Developed jointly with Berkan Eskikaya, COGS,
University of Sussex, and described in more detail in
(Belz and Eskikaya, 1998)
S+ Succ. Target Conver- Accurate
size (Avg) found gence (Target)
(Avg) at gen. automata
8 0 — 201 0
16 3(1.4) 33(49) 211 25(21)
32 6(3.7) 40(67) 172 58(49)
48 10(5.2) 46(83) 171 99(92)
64 8(5.4) 45(68) 191 79(73)
80 9(6.0) 37(73) 142 79(79)
96 9(6.2) 35(75) 114 80(76)
112 9(6.1) 46(73) 158 80(80)
128 9(6.4) 36(77) 135 89(89)
</table>
<tableCaption confidence="0.999941">
Table 1: Toy data set results.
</tableCaption>
<bodyText confidence="0.995609">
timated). Transitions that are not required to
parse any string in D+ are eliminated. Fitness
criteria 1-3 are weighted (reflecting their rela-
tive importance to fitness evaluation). These
weights can be manipulated to directly affect
the structural and functional properties of au-
tomata.
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999805444444445">
Toy Data Set The data used in the first set
of tests was generated with the grammar S --+
cA, A —+ civiB, A c2v2B, B c. Here, c
abbreviates a set of 4 consonants, c1 2 sharped
consonants, c2 2 non-sharped consonants, v1 2
front vowels, and v2 2 non-front vowels. The
grammar (generating a total of 128 strings) is
a simple version of a phonotactic constraint in
Russian, where non-sharped consonants cannot
be followed by front vowels, (e.g. Halle (1971)).
Tests were carried out for different-size ran-
domly selected language subsets. Different com-
binations of weights for the three fitness crite-
ria were tested. Table 1 gives results for the
best weight combination averaged over 10 runs
differing only in random population initialisa-
tion (and results averaged over all other weight
combinations in brackets). The first column in-
dicates how many successful runs there were out
of 10 for the best weight combination (and the
average for all weight combinations in brack-
ets). A run was deemed successful if the target
automaton was found before convergence. The
last column shows how many accurate automata
were in final populations, and in brackets how
many of these also matched the target FSA in
size.
</bodyText>
<page confidence="0.93569">
1473
</page>
<table confidence="0.988326666666667">
Train. Test. General. States
Best 100% 61% 100 7
Avg 94% 49% 100.3 7.1
</table>
<tableCaption confidence="0.994249">
Table 2: Results for Russian noun data set.
</tableCaption>
<bodyText confidence="0.992016902439024">
The general effects of reducing the size of D+
are that successful runs become less likely, and
that the weight assigned to the degree of over-
generation becomes more important and tends
to have to be increased. The larger the data set,
the more similar the results that can be achieved
with different weights.
Russian Noun Data Set The data used in
the second series of tests were bisyllabic femi-
nine Russian nouns ending in -a. The alphabet
consisted of 36 phonemic symbols2. The train-
ing set contained 100 strings, and a related set
of 100 strings was used as a test set.
Results are shown in Table 2. The target
degree of overgeneration was set to 100 times
the size of the data set. Tests were carried out
for different weights assigned to the overgener-
ation criterion. Results are given for the best
automaton found in 10 runs for the best weight
settings, and for the corresponding averages for
all 10 runs.
Figure 1 shows the fittest automaton from
Table 1. Phonemes are grouped together (as
label sets on arcs) in several linguistically use-
ful ways. Vowels and consonants are separated
completely. Vowels are separated into the set
of those that can be preceded by sharped con-
sonants (capitalised symbols) and those that
cannot. Correspondingly, sharped consonants
tend to be separated from nonsharped equiva-
lents. The phonemes k, r, t are singled out
(arc 4 -4 5) because they combine only with
nonsharped consonants to form stem-final clus-
ters. The groupings S (0 6) and L,M,P,R
(6 --- 1) reflect stem-initial consonant clusters.
These groupings are typical of the automata
discovered in all 10 runs. Occasionally ka (the
feminine diminutive ending) was singled out as
a separate ending, and the stem vowels were
frequently grouped together more efficiently.
Different degrees of generalisaton were
</bodyText>
<figureCaption confidence="0.517301">
&apos;The set of phonemic symbols used here is based on
Halle (Halle, 1971). Capital symbols represent sharped
versions of non-capitalised counterparts.
Figure 1: Best automaton for Russian noun set.
</figureCaption>
<bodyText confidence="0.618075333333333">
achieved with different weight settings. The au-
tomaton shown here corresponds most closely to
(Halle, 1971).
</bodyText>
<sectionHeader confidence="0.981023" genericHeader="conclusions">
5 Conclusion and Further Research
</sectionHeader>
<bodyText confidence="0.99997475">
This paper presented a GA for FSA induction
and preliminary results for a toy data set and a
simple set of Russian nouns. These results in-
dicate that genetic search can successfully be
applied to the automatic discovery of finite-
state automata that encode (parts of) phono-
logical grammars from arbitrary subsets of pos-
itive data. A more detailed description of the
GA and a report on subsequent results can be
found in Belz and Eskikaya (1998). The method
is currently being extended to allow for sets of
negative examples.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998483090909091">
D. Angluin. 1978. On the complexity of min-
imum inference of regular sets. Information
and Control, 39:337-350.
A. Belz and B. Eskikaya. 1998. A genetic al-
gorithm for finite-state automaton induction.
Cognitive Science Research Paper 487, School
of Cognitive and Computing Sciences, Uni-
versity of Sussex.
K. S. Fu and T. L. Booth. 1986. Grammati-
cal inference: introduction and survey. IEEE
Transactions on Pattern Analysis and Ma-
chine Intelligence, PAMI-8:343-375.
E. M. Gold. 1978. Complexity of automaton
identification from given data. Information
and Control, 37:302-320.
D. E. Goldberg. 1989. Genetic Algorithms in
search, optimization and machine learning.
Addison-Wesley.
M. Halle. 1971. The Sound Pattern of Russian.
Mouton, The Hague.
B. B. Trakhtenbrot and Ya. Barzdin. 1973. Fi-
nite Automata. North Holland, Amsterdam.
</reference>
<page confidence="0.995017">
1474
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000091">
<title confidence="0.999528">Discovering Phonotactic Finite-State Automata by Genetic Search</title>
<author confidence="0.999226">Anja Belz</author>
<affiliation confidence="0.9997735">School of Cognitive and Computing Sciences University of Sussex</affiliation>
<address confidence="0.99819">Brighton BN1 9QH, UK</address>
<email confidence="0.448392">jab@cogs.susx.ac.uk</email>
<abstract confidence="0.999066688">This paper presents a genetic algorithm based approach to the automatic discovery of finitestate automata (FSAs) from positive data. FSAs are commonly used in computational phonology, but — given the limited learnability of FSAs from arbitrary language subsets — are usually constructed manually. The approach presented here offers a practical automatic method that helps reduce the cost of manual FSA construction. 1 Background techniques have always been central to computational phonology. Definite FSAs are commonly used to encode phonotactic constraints in a variety of NLP contexts. Such FSAs are usually constructed in a timeconsuming manual process. Following notational convention, an FSA is a I, 8, F), which a finite set of states, a finite nonempty 8 is the state transition function, the initial state, and a nonempty set final states in language the finite automaton Generally, the problem considered here is that identifying a language a fixed finite = (D, D), C L L = 0 may be empty). If is and structurally complete with reto problem is not complex, and there exist a number of algorithms based on first constructing a canonical automaton from the data set which is then reduced or merged in various If an arbitrary strict subset of the problem is less clearly defined. Since any finite sample is consistent with an infinite number languages, be identified uniquely best we can hope to do is to infer a grammar that will describe the strings predict other strings that in some sense are of the same nature as those contained in D+.&amp;quot; (Fu and Booth, 1986, p. 345) constrain the set of possible languages the inferred grammar is typically required to be as small as possible. However, the problem of finding a minimal grammar consistent a given sample shown to be NPhard by Gold (1978). Nonapproximability results of varying strength have been added since. the special case where all strings symbols over a finite alphabet length than polynomial-time algorithm can be found (Trakhtenbrot and Barzdin, 1973), but if even a small fraction of examples is missing, the problem is again NP-hard (Angluin, 1978). 2 Task Description a known finite alphabet of symbols finite-state language a data sam- C L C I*, task is to find an FSA that L(A) superset of structural regularities of the size as possible. Where the target language is known in advance, the degree of language and size approximation can be measured, and its adequacy relative to training set size and representativeness can be described. In the case of inference of automata that encode (part of) a phonological grammar, language approximation and its degree of adequacy can be described relative to a set of theoretical linguistic assumptions that describes a target grammar. 3 Search Method By direct analogy with natural evolution, genetic algorithms (GAs) work with a population 1472 of individuals each of which represents a candidate solution to the given problem. These individuals are assigned a fitness score and on its basis are selected to &apos;mate&apos;, and produce the next generation. This process is typically iterated until the population has converged, i.e. when individuals have reached a degree of similarity beyond which further improvement becomes impossible. Characteristics that form part of good solutions are passed on through the generations and begin to combine in the offspring to approach global optima, an effect that has been in terms of the block hypothe- 1989). Unlike other search methods, GAs sample different areas of the search space simultaneously, and are therefore able to escape local optima and to avoid areas of low fitness. main issues in GA design are the candidate solutions (individuals) as data structures for the GA to work on, defining a function accurately expresses the goodness of candidate solutions, and designing operators combine and alter the genetic material of good solutions (parents) to produce new, better solutions (offspring). the present the state-transition matrices of FSAs are directly converted into genotypes. Mutation randomly adds or deletes one transition in each FSA and a variant of uniform crossover tends to preserve the general structure of the fitter parent, while adding some substructures from the weaker parent (offspring can be larger or smaller than both parents). Fitness is evaluated according to three fitness criteria. The first two follow directly from the description: (1) size of (smallness), ability to parse strings in (consistency), where ability to partially parse strings is also rewarded. Used on their own, however, these criteria lead search in the direction of universal that produce all strings /* up to length of the longest string in D+.To avoid this, (3) an overgeneration criterion is added that requires automata to achieve a given degree generalisation, such that the size of equal to the size of the target language (where target language is not known, its size is es-</abstract>
<affiliation confidence="0.57818">apos;Developed jointly with Berkan Eskikaya, COGS, University of Sussex, and described in more detail in</affiliation>
<note confidence="0.78002">(Belz and Eskikaya, 1998) S+ Succ. Target Conver- Accurate size (Avg) found gence (Target) (Avg) at gen. automata</note>
<phone confidence="0.537242333333333">8 0 — 201 0 16 3(1.4) 33(49) 211 25(21) 32 6(3.7) 40(67) 172 58(49) 48 10(5.2) 46(83) 171 99(92) 64 8(5.4) 45(68) 191 79(73) 80 9(6.0) 37(73) 142 79(79) 96 9(6.2) 35(75) 114 80(76) 112 9(6.1) 46(73) 158 80(80) 128 9(6.4) 36(77) 135 89(89)</phone>
<abstract confidence="0.976210076190476">Table 1: Toy data set results. timated). Transitions that are not required to any string in eliminated. Fitness criteria 1-3 are weighted (reflecting their relative importance to fitness evaluation). These weights can be manipulated to directly affect the structural and functional properties of automata. 4 Results Data Set data used in the first set tests was generated with the grammar --+ —+ A Here, c a set of 4 consonants, 2 sharped 2 non-sharped consonants, 2 vowels, and 2 non-front vowels. The grammar (generating a total of 128 strings) is a simple version of a phonotactic constraint in Russian, where non-sharped consonants cannot be followed by front vowels, (e.g. Halle (1971)). Tests were carried out for different-size randomly selected language subsets. Different combinations of weights for the three fitness criteria were tested. Table 1 gives results for the best weight combination averaged over 10 runs differing only in random population initialisation (and results averaged over all other weight combinations in brackets). The first column indicates how many successful runs there were out of 10 for the best weight combination (and the average for all weight combinations in brackets). A run was deemed successful if the target automaton was found before convergence. The last column shows how many accurate automata were in final populations, and in brackets how many of these also matched the target FSA in size. 1473 Train. Test. General. States Best 100% 61% 100 7 Avg 94% 49% 100.3 7.1 Table 2: Results for Russian noun data set. general effects of reducing the size of are that successful runs become less likely, and that the weight assigned to the degree of overgeneration becomes more important and tends to have to be increased. The larger the data set, the more similar the results that can be achieved with different weights. Noun Data Set data used in the second series of tests were bisyllabic femi- Russian nouns ending in alphabet of 36 phonemic The training set contained 100 strings, and a related set of 100 strings was used as a test set. Results are shown in Table 2. The target degree of overgeneration was set to 100 times the size of the data set. Tests were carried out for different weights assigned to the overgeneration criterion. Results are given for the best automaton found in 10 runs for the best weight settings, and for the corresponding averages for all 10 runs. Figure 1 shows the fittest automaton from Table 1. Phonemes are grouped together (as label sets on arcs) in several linguistically useful ways. Vowels and consonants are separated completely. Vowels are separated into the set of those that can be preceded by sharped consonants (capitalised symbols) and those that cannot. Correspondingly, sharped consonants tend to be separated from nonsharped equiva- The phonemes t singled out (arc 4 -4 5) because they combine only with consonants to form stem-final clus- The groupings S (0 6) and L,M,P,R --reflect stem-initial consonant clusters. These groupings are typical of the automata in all 10 runs. Occasionally feminine diminutive ending) was singled out as a separate ending, and the stem vowels were frequently grouped together more efficiently. Different degrees of generalisaton were &apos;The set of phonemic symbols used here is based on Halle (Halle, 1971). Capital symbols represent sharped versions of non-capitalised counterparts. Figure 1: Best automaton for Russian noun set. achieved with different weight settings. The automaton shown here corresponds most closely to (Halle, 1971). and Further Research This paper presented a GA for FSA induction and preliminary results for a toy data set and a simple set of Russian nouns. These results indicate that genetic search can successfully be applied to the automatic discovery of finitestate automata that encode (parts of) phonological grammars from arbitrary subsets of positive data. A more detailed description of the GA and a report on subsequent results can be found in Belz and Eskikaya (1998). The method is currently being extended to allow for sets of negative examples. References D. Angluin. 1978. On the complexity of mininference of regular sets.</abstract>
<note confidence="0.57060825">Control, A. Belz and B. Eskikaya. 1998. A genetic algorithm for finite-state automaton induction. Cognitive Science Research Paper 487, School</note>
<affiliation confidence="0.755903">of Cognitive and Computing Sciences, University of Sussex.</affiliation>
<intro confidence="0.54186">S. Fu and T. 1986. Grammati-</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Angluin</author>
</authors>
<title>On the complexity of minimum inference of regular sets.</title>
<date>1978</date>
<journal>Information and Control,</journal>
<pages>39--337</pages>
<contexts>
<context position="2632" citStr="Angluin, 1978" startWordPosition="464" endWordPosition="465"> Booth, 1986, p. 345) To constrain the set of possible languages L, the inferred grammar is typically required to be as small as possible. However, the problem of finding a minimal grammar consistent with a given sample D was shown to be NPhard by Gold (1978). Nonapproximability results of varying strength have been added since. In the special case where D contains all strings of symbols over a finite alphabet I of length shorter than k, a polynomial-time algorithm can be found (Trakhtenbrot and Barzdin, 1973), but if even a small fraction of examples is missing, the problem is again NP-hard (Angluin, 1978). 2 Task Description Given a known finite alphabet of symbols I, a target finite-state language L, and a data sample D+ C L C I*, the task is to find an FSA A, such that L(A) is consistent with D+, L(A) is a superset of D+ encoding generalisation over the structural regularities of D+, and the size of S is as small as possible. Where the target language is known in advance, the degree of language and size approximation can be measured, and its adequacy relative to training set size and representativeness can be described. In the case of inference of automata that encode (part of) a phonologica</context>
</contexts>
<marker>Angluin, 1978</marker>
<rawString>D. Angluin. 1978. On the complexity of minimum inference of regular sets. Information and Control, 39:337-350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Belz</author>
<author>B Eskikaya</author>
</authors>
<title>A genetic algorithm for finite-state automaton induction.</title>
<date>1998</date>
<journal>Cognitive Science Research Paper</journal>
<volume>487</volume>
<institution>School of Cognitive and Computing Sciences, University of Sussex.</institution>
<contexts>
<context position="5847" citStr="Belz and Eskikaya, 1998" startWordPosition="1001" endWordPosition="1004">+ (consistency), where ability to partially parse strings is also rewarded. Used on their own, however, these criteria lead search in the direction of universal automata that produce all strings x E /* up to the length of the longest string in D+. To avoid this, (3) an overgeneration criterion is added that requires automata to achieve a given degree of generalisation, such that the size of L(A) is equal to the size of the target language (where the target language is not known, its size is es&apos;Developed jointly with Berkan Eskikaya, COGS, University of Sussex, and described in more detail in (Belz and Eskikaya, 1998) S+ Succ. Target Conver- Accurate size (Avg) found gence (Target) (Avg) at gen. automata 8 0 — 201 0 16 3(1.4) 33(49) 211 25(21) 32 6(3.7) 40(67) 172 58(49) 48 10(5.2) 46(83) 171 99(92) 64 8(5.4) 45(68) 191 79(73) 80 9(6.0) 37(73) 142 79(79) 96 9(6.2) 35(75) 114 80(76) 112 9(6.1) 46(73) 158 80(80) 128 9(6.4) 36(77) 135 89(89) Table 1: Toy data set results. timated). Transitions that are not required to parse any string in D+ are eliminated. Fitness criteria 1-3 are weighted (reflecting their relative importance to fitness evaluation). These weights can be manipulated to directly affect the str</context>
</contexts>
<marker>Belz, Eskikaya, 1998</marker>
<rawString>A. Belz and B. Eskikaya. 1998. A genetic algorithm for finite-state automaton induction. Cognitive Science Research Paper 487, School of Cognitive and Computing Sciences, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Fu</author>
<author>T L Booth</author>
</authors>
<title>Grammatical inference: introduction and survey.</title>
<date>1986</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<pages>8--343</pages>
<contexts>
<context position="2030" citStr="Fu and Booth, 1986" startWordPosition="358" endWordPosition="361">urally complete with regard to L, the problem is not complex, and there exist a number of algorithms based on first constructing a canonical automaton from the data set which is then reduced or merged in various ways. If D+ is an arbitrary strict subset of L, the problem is less clearly defined. Since any finite sample is consistent with an infinite number of languages, L cannot be identified uniquely from D+. &amp;quot;...the best we can hope to do is to infer a grammar that will describe the strings in D+ and predict other strings that in some sense are of the same nature as those contained in D+.&amp;quot; (Fu and Booth, 1986, p. 345) To constrain the set of possible languages L, the inferred grammar is typically required to be as small as possible. However, the problem of finding a minimal grammar consistent with a given sample D was shown to be NPhard by Gold (1978). Nonapproximability results of varying strength have been added since. In the special case where D contains all strings of symbols over a finite alphabet I of length shorter than k, a polynomial-time algorithm can be found (Trakhtenbrot and Barzdin, 1973), but if even a small fraction of examples is missing, the problem is again NP-hard (Angluin, 197</context>
</contexts>
<marker>Fu, Booth, 1986</marker>
<rawString>K. S. Fu and T. L. Booth. 1986. Grammatical inference: introduction and survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-8:343-375.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Gold</author>
</authors>
<title>Complexity of automaton identification from given data.</title>
<date>1978</date>
<journal>Information and Control,</journal>
<pages>37--302</pages>
<contexts>
<context position="2277" citStr="Gold (1978)" startWordPosition="406" endWordPosition="407">of L, the problem is less clearly defined. Since any finite sample is consistent with an infinite number of languages, L cannot be identified uniquely from D+. &amp;quot;...the best we can hope to do is to infer a grammar that will describe the strings in D+ and predict other strings that in some sense are of the same nature as those contained in D+.&amp;quot; (Fu and Booth, 1986, p. 345) To constrain the set of possible languages L, the inferred grammar is typically required to be as small as possible. However, the problem of finding a minimal grammar consistent with a given sample D was shown to be NPhard by Gold (1978). Nonapproximability results of varying strength have been added since. In the special case where D contains all strings of symbols over a finite alphabet I of length shorter than k, a polynomial-time algorithm can be found (Trakhtenbrot and Barzdin, 1973), but if even a small fraction of examples is missing, the problem is again NP-hard (Angluin, 1978). 2 Task Description Given a known finite alphabet of symbols I, a target finite-state language L, and a data sample D+ C L C I*, the task is to find an FSA A, such that L(A) is consistent with D+, L(A) is a superset of D+ encoding generalisatio</context>
</contexts>
<marker>Gold, 1978</marker>
<rawString>E. M. Gold. 1978. Complexity of automaton identification from given data. Information and Control, 37:302-320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Goldberg</author>
</authors>
<title>Genetic Algorithms in search, optimization and machine learning.</title>
<date>1989</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="4140" citStr="Goldberg, 1989" startWordPosition="721" endWordPosition="722">each of which represents a candidate solution to the given problem. These individuals are assigned a fitness score and on its basis are selected to &apos;mate&apos;, and produce the next generation. This process is typically iterated until the population has converged, i.e. when individuals have reached a degree of similarity beyond which further improvement becomes impossible. Characteristics that form part of good solutions are passed on through the generations and begin to combine in the offspring to approach global optima, an effect that has been explained in terms of the building block hypothesis (Goldberg, 1989). Unlike other search methods, GAs sample different areas of the search space simultaneously, and are therefore able to escape local optima and to avoid areas of low fitness. The main issues in GA design are encoding the candidate solutions (individuals) as data structures for the GA to work on, defining a fitness function that accurately expresses the goodness of candidate solutions, and designing genetic operators that combine and alter the genetic material of good solutions (parents) to produce new, better solutions (offspring). In the present GA1, the state-transition matrices of FSAs are </context>
</contexts>
<marker>Goldberg, 1989</marker>
<rawString>D. E. Goldberg. 1989. Genetic Algorithms in search, optimization and machine learning. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Halle</author>
</authors>
<title>The Sound Pattern of Russian.</title>
<date>1971</date>
<publisher>Mouton, The Hague.</publisher>
<contexts>
<context position="6958" citStr="Halle (1971)" startWordPosition="1195" endWordPosition="1196">lative importance to fitness evaluation). These weights can be manipulated to directly affect the structural and functional properties of automata. 4 Results Toy Data Set The data used in the first set of tests was generated with the grammar S --+ cA, A —+ civiB, A c2v2B, B c. Here, c abbreviates a set of 4 consonants, c1 2 sharped consonants, c2 2 non-sharped consonants, v1 2 front vowels, and v2 2 non-front vowels. The grammar (generating a total of 128 strings) is a simple version of a phonotactic constraint in Russian, where non-sharped consonants cannot be followed by front vowels, (e.g. Halle (1971)). Tests were carried out for different-size randomly selected language subsets. Different combinations of weights for the three fitness criteria were tested. Table 1 gives results for the best weight combination averaged over 10 runs differing only in random population initialisation (and results averaged over all other weight combinations in brackets). The first column indicates how many successful runs there were out of 10 for the best weight combination (and the average for all weight combinations in brackets). A run was deemed successful if the target automaton was found before convergenc</context>
<context position="9707" citStr="Halle, 1971" startWordPosition="1657" endWordPosition="1658">nants tend to be separated from nonsharped equivalents. The phonemes k, r, t are singled out (arc 4 -4 5) because they combine only with nonsharped consonants to form stem-final clusters. The groupings S (0 6) and L,M,P,R (6 --- 1) reflect stem-initial consonant clusters. These groupings are typical of the automata discovered in all 10 runs. Occasionally ka (the feminine diminutive ending) was singled out as a separate ending, and the stem vowels were frequently grouped together more efficiently. Different degrees of generalisaton were &apos;The set of phonemic symbols used here is based on Halle (Halle, 1971). Capital symbols represent sharped versions of non-capitalised counterparts. Figure 1: Best automaton for Russian noun set. achieved with different weight settings. The automaton shown here corresponds most closely to (Halle, 1971). 5 Conclusion and Further Research This paper presented a GA for FSA induction and preliminary results for a toy data set and a simple set of Russian nouns. These results indicate that genetic search can successfully be applied to the automatic discovery of finitestate automata that encode (parts of) phonological grammars from arbitrary subsets of positive data. A </context>
</contexts>
<marker>Halle, 1971</marker>
<rawString>M. Halle. 1971. The Sound Pattern of Russian. Mouton, The Hague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barzdin</author>
</authors>
<title>Finite Automata.</title>
<date>1973</date>
<publisher>North</publisher>
<location>Holland, Amsterdam.</location>
<contexts>
<context position="2533" citStr="Barzdin, 1973" startWordPosition="447" endWordPosition="448"> predict other strings that in some sense are of the same nature as those contained in D+.&amp;quot; (Fu and Booth, 1986, p. 345) To constrain the set of possible languages L, the inferred grammar is typically required to be as small as possible. However, the problem of finding a minimal grammar consistent with a given sample D was shown to be NPhard by Gold (1978). Nonapproximability results of varying strength have been added since. In the special case where D contains all strings of symbols over a finite alphabet I of length shorter than k, a polynomial-time algorithm can be found (Trakhtenbrot and Barzdin, 1973), but if even a small fraction of examples is missing, the problem is again NP-hard (Angluin, 1978). 2 Task Description Given a known finite alphabet of symbols I, a target finite-state language L, and a data sample D+ C L C I*, the task is to find an FSA A, such that L(A) is consistent with D+, L(A) is a superset of D+ encoding generalisation over the structural regularities of D+, and the size of S is as small as possible. Where the target language is known in advance, the degree of language and size approximation can be measured, and its adequacy relative to training set size and representa</context>
</contexts>
<marker>Barzdin, 1973</marker>
<rawString>B. B. Trakhtenbrot and Ya. Barzdin. 1973. Finite Automata. North Holland, Amsterdam.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>