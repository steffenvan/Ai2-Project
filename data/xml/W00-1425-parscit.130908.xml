<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.995991">
Capturing the Interaction between Aggregation and Text
Planning in Two Generation Systems
</title>
<author confidence="0.991843">
Hua Cheng and Chris Mellish
</author>
<affiliation confidence="0.998758">
Division of Informatics, University of Edinburgh
</affiliation>
<address confidence="0.77908">
80 South Bridge, Edinburgh Ef11 11-IN, UK
</address>
<email confidence="0.535732">
huac, chrisrnOdai. ed. ac. uk
</email>
<sectionHeader confidence="0.957444" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999902">
In natural language generation, different gener-
ation tasks often interact with each other in a
complex way. We think that how to resolve the
complex interactions inside and between tasks
is more important to the generation of a co-
herent text than how to model each individual
factor. This paper focuses on the interaction be-
tween aggregation and text planning, and tries
to explore what preferences exist among the fea-
tures considered by the two tasks. The prefer-
ences are implemented in two generation sys-
tems, namely ILEX-TS and a text planner us-
ing a Genetic Algorithm. The evaluation em-
phasises the second implementation and shows
that capturing these preferences properly can
lead to coherent text.
</bodyText>
<sectionHeader confidence="0.3530185" genericHeader="keywords">
1 Discourse coherence and
aggregation
</sectionHeader>
<bodyText confidence="0.999862690909091">
In NLG, theories based on domain-independent
rhetorical relations, in particular, Rhetorical
Structure Theory (Mann and Thompson, 1987),
are often used in text planning, whose task
is to select the relevant information to be ex-
pressed and organise it into a hierarchical struc-
ture which captures certain discourse prefer-
ences such as preferences for the use of rhetori-
cal relations_
In the theory of discourse structure developed
by Grosz and Shiner (1986), each discourse seg-
ment exhibits two types of coherence: local co-
herence among utterances inside the segment,
and global coherence between this segment and
other discourse segments. Discourse segments
are connected by either a dominance relation or
a satisfaction-precedence relation.
There has been an effort to synthesise the
two accounts of discourse structnre. \ loser and
Moore (199(i) argue that the two theories have
considerable common ground, which lies in the
correspondence between the notion of domi-
nance and nuclearity. It is possible to map
between Grosz and Sidner&apos;s linguistic structure
and RST text structure, and relation-based co-
herence and global coherence capture similar
discourse properties.
Oberlander et al. (1999) propose a dis-
tinction between two types of discourse coher-
ence: proposition-based coherence, which ex-
ists between text spans connected by RST re-
lations except for object-attribute elaboration,
and entity-based coherence, which exists be-
tween spans of text in virtue of shared entities.
entity-based coherence captures the coherence
among adjacent propositions, which resembles
local coherence in Grosz and Sidner&apos;s theory.
To generate a coherent text, the text planning
process must try to achieve both local (entity-
based) and global (relation-based) coherence.
Since the task of aggregation is to combine sim-
ple representations together to form a complex
one, which in the mean time leads to a shorter
text as a whole, aggregation could affect the or-
dering of text plans and the length of the whole
text. Therefore, it is closely related to the task
of maintaining both types of coherence. Here
we treat embedding as a type of aggregation.
There is no consensus as to where aggregation
should happen or how it. is related to other gen-
eration processes (Wilkinson, 1995; Reape and
Mellish, 1999). In many NLG systems, aggre-
gation is a post planning process whose prefer-
ences are only partially taken into account by
the text planner.
</bodyText>
<subsectionHeader confidence="0.985229">
1.1 Aggregation and local coherence
</subsectionHeader>
<bodyText confidence="0.999252333333333">
In a structured text plan produced by the text
planner, local coherence is normally maintained
t hrough the ordering of the selected facts, where
</bodyText>
<page confidence="0.997607">
186
</page>
<bodyText confidence="0.999951423076923">
certain types of center transition (e.g. cen-
ter continuation) .are :preferred -over-others (e.g..
center shifting) (Centering Theory (Grosz et al.,
1995)). Aggregation may affect text planning
by taking away facts from a sequence featuring
preferred center movements for embedding or
subordination. As a result, the preferred cen-
ter transitions in the original sequences could
be cut off. For example, comparing the first
two descriptions of .a .necklace in Figure 1, 2 is
less coherent than I because of the shifting from
the description of the necklace to that of the de-
signer, which is a side effect of embedding.
Since the centers of sentences are normally
NPs and embedding adds non-restrictive com-
ponents into an NP, it could affect the way a Cb
is realised (e.g. preventing it from being a pro-
noun). As pointed out in (Grosz et al., 1995),
different realisations (e.g. pronoun vs. definite
description) are not equivalent with respect to
their effect on coherence. Therefore, embedding
could influence local coherence by forcing a dif-
ferent realisation from that preferred by Center-
ing Theory. There is an obvious need to balance
the consideration for local coherence and stylis-
tic preferences.
</bodyText>
<subsectionHeader confidence="0.978133">
1.2 Aggregation and global coherence
</subsectionHeader>
<bodyText confidence="0.997556622222222">
Different types of aggregation need to be com-
patible among themselves, in particular, embed-
ding and semantic parataxis and hypotaxis. Us-
ing the abstraction of RST, semantic parataxis
concerns facts related by explicit multi-nuclear
semantic relations (e.g. sequence and contrast)
or by implicit connections like parallel common
parts. If two facts have at least two identi-
cal parallel components, we say that a conjunct
or disjunct relation exists between them, and
these relations are multi-nuclear relations_ Se-
mantic hypotaxis concerns facts connected by
nucleus-satellite relations (e.g. cause). Seman-
tic parataxis and hypotaxis feature in relation-
based coherence and they depend on the text
planner to put the related facts next to each
other in order to perform a combination.
(Cheng, 1998) describes interactions that
need to be taken into account in aggrega-
tion. Firstly, complex embedded components
like non-restrictive clauses may interrupt the
semantic connection or syntactic similarity be-
tween a set of clauses. Secondly, the possibilities
of other types of aggregation should be consid-
ered for both the main fact and the fact to be
-; embedded -during :embedding decision . maki ng._ _
And thirdly, performing parataxis inside a hy-
potaxis could convey wrong information.
We argue that the effect of aggregation is not
limited to the particular NP or sentence where
aggregation happens, but to the coherence of
the text as a whole. The complex interactions
demand the features of aggregation to be eval-
uated together with other coherence features
and aggregation to be planned as a part of text
structuring. This requires better coordination
between aggregation and other generation tasks
as well as among different types of aggregation
than is present in current NLG systems.
In this paper, we describe how to capture the
above interactions as preferences among related
features, and the implementation of the prefer-
ences in two very different generation architec-
tures to produce descriptions of museum objects
on display.
</bodyText>
<sectionHeader confidence="0.9007235" genericHeader="method">
2 Preferences among coherence
features
</sectionHeader>
<bodyText confidence="0.999921714285714">
We claim that it is the relative preferences
among features rather than the absolute magni-
tude of each individual one that play the crucial
role in the production of a coherent text. In this
section we discuss the preferences among fea-
tures related to text planning, based on which
those for embedding can be introduced.
</bodyText>
<subsectionHeader confidence="0.655677">
2.1 Preferences for global coherence
</subsectionHeader>
<bodyText confidence="0.999631947368421">
A semantic relation other than conjunct or dis-
junct is preferred to be used whenever possible
because it usually conveys interesting informa-
tion about domain objects and leads to a coher-
ent text span. If a conjunct relation shares a fact
with a semantic relation, the conjunct should
be suppressed. For example., in 3 of Figure 1.
apart from other relations, there is an amplifica-
tion relation signalled by indeed and a conjunct
between the last two propositions. Compared
with 3, 4 is less preferred because it misses the
amplification and the center transition.frorn the
necklace to an Arts and Crafts style jewel is not
so smooth, whereas 3 expresses the amplifica-
tion explicitly and the conjunct implicitly.
However, a semantic relation can only be used
if the knowledge assumed to be shared by the
hearer is introduced in the previous discourse
(Mellish et al.. 1998a). We assume the strategy
</bodyText>
<page confidence="0.981277">
187
</page>
<listItem confidence="0.996568090909091">
1. This necklace is in the Arts and Crafts style. Arts and Crafts style jewels usually have an elaborate
design. They tend to have floral motifs. For instance, this necklace has floral motifs. It was designed
by Jessie King. King was Scottish. She once lived in London.
2. This necklace, which was designed by Jessie King, is in the Arts and Crafts style. Arts and
Crafts style jewels usually have an elaborate design. They tend to have floral motifs. For instance,
this necklace has floral motifs. King was Scottish. She once lived in London.
3. The necklace is in the Arts and Crafts style. It is set with jewels in that it features cabnchon
stones. Indeed, an Arts and Crafts style jewel usually uses cabochon stones. It usually uses oval
stones.
4. The necklace is in the Arts and Crafts style. It is set. with jewels in that it features cabochon
stones. An Arts and Crafts style jewel usually uses cabochon stones and oval stones.
</listItem>
<figureCaption confidence="0.99872">
Figure 1: Aggregation examples
</figureCaption>
<bodyText confidence="0.9976751">
of (Mellish et al., 1998a) which uses a Joint re-
lation to connect every two text spans that do
not have a semantic relation other than object-
attribute elaboration and conjunct/disjunct in
between. Although joint is not preferred when
other relations are present, it is better than
missing presuppositions or embedding a con-
junct relation inside a semantic relation. There-
fore, we have the following heuristics, where
&amp;quot;A&gt;B&amp;quot; means that A is preferred over B.
</bodyText>
<figure confidence="0.9188986">
Heuristic 1 Preferences among features for
global coherence:
a semantic relation &gt; Conjunct/Disjunct &gt;
Joint &gt; presuppositions not met
Joint &gt; Conjunct inside a semantic relation
</figure>
<subsectionHeader confidence="0.933805">
2.2 Preferences for local coherence
</subsectionHeader>
<bodyText confidence="0.989505348837209">
One way to achieve local coherence is to con-
trol center transitions among utterances. In
Centering Theory, Rule 2 specifies preferences
among center movement in a locally coherent
discourse segment: sequences of continuation
are preferred over sequences of retaining, which
are then preferred over sequences of shifting.
Brennan et al. (1987) also describe typical
discourse topic movements in terms of center
transitions between pairs of utterances. They
argue that the order of coherence among the
transitions is continuing &gt; retaining &gt; smooth
shifting &gt; abrupt shifting. Instead of claiming
that these are the best models, we use them
simply as an example of linguistic models being
used for evaluating features of text planning.
A type of center transition that appears fre-
quently in descriptive text is that the descrip-
tion starts with an object, but shifts to associ-
ated objects or perspectives of that object_ This
is a type of abrupt shifting, but it is appropriate
as long as the objects are highly associated to
the original object (Schank, 1977). This phe-
nomenon is handled in the system of (Grosz,
1977), where subparts of an object are included
into a focus space as the implicit foci when the
object itself is to be included.
We call this center movement an associate
shifting, where the center moves from a trig-
ger entity to a closely associated entity. Our
informal observation from museum descriptions
shows that associate shifting is preferred by hu-
man writers to all other types of center move-
ments except for continuation. There are two
types of associate shifting: where the trigger
is in the previous utterance or two entities in
two adjacent utterances have the same trigger.
There is no preference between them.
Heuristic 2 summarises the above preferences.
We admit that these are strict heuristics and
that human texts are sometimes more flexible.
Heuristic 2 Preferences among center transi-
tions:
</bodyText>
<construct confidence="0.8133935">
Continuation &gt; Associate shifting &gt; Retain-
ing &gt; Smooth shifting &gt; Abrupt shifting
</construct>
<bodyText confidence="0.8281466">
2.3 Preferences for both types of
coherence
Two propositions can be connected in differ-
ent ways, e.g. through a semantic relation or a
smooth center transition only. Since a semantic
relation is always preferred, we have the follow-
ing heuristic:
Heuristic 3 Preferences among semantic rela-
tions and center transitions:
a semantic relation &gt; Joint Continuation
</bodyText>
<page confidence="0.988576">
188
</page>
<subsectionHeader confidence="0.773815">
2.4 Preferences for embedding
</subsectionHeader>
<bodyText confidence="0.994919142857143">
We distinguish between a-good-,-normal&apos;and-bad
embedding based on the features it bears. We do
not claim that the set of features is complete.
In a different context, more criteria might have
to be considered.
A good embedding is one satisfying all the fol-
lowing conditions:
</bodyText>
<listItem confidence="0.930020909090909">
1. The referring part is an indefinite, a. demon-
strative or a bridging description (as de-
fined in (Poesio et al., 1997)).
2. The embedded part can be realised as an
adjective or a prepositional phrase (Scott
and de Souza, 1990).
3. In the resulting text, the embedded part
does not lie between text spans connected
by semantic parataxis and hypotaxis.
4. There is an available syntactic slot to hold
the embedded part.
</listItem>
<bodyText confidence="0.985401428571429">
A good embedding is highly preferred and
should be performed whenever possible. A nor-
mal embedding is one satisfying condition 1, 3
and 4 and the embedded part is a relative clause
which provides additional information about
the referent. Bad embeddings are all those left,
for example, if there is no available syntactic
slot for the embedded part.
Since semantic parataxis has a higher priority
than embedding (Cheng, 1998), a good embed-
ding should be less preferred than using a con-
junct relation, but it should be preferred over a
center continuation for it to happen.
To decide the interaction between an embed-
ding and a center transition, we use the first two
examples in Figure 1 again. The only difference
between I and 2 is the position of the sentence
&amp;quot;This necklace Was designed by Jessie King&amp;quot;,
which can be represented in terms of features of
local coherence and embedding as follows:
the last three sentences in 1: Joint + Contin-
</bodyText>
<table confidence="0.391412571428571">
uation + Joint Smooth shifting
the last two sentences plus embedding in 2:
Joint + Abrupt shifting + Normal embedding
1 is preferred over 2 because the center moves
more smoothly in 1. The heuristics derived from
the above discussions are summarised below:
Heuristic 4 Preferences among features for
embedding and center transition:
Good embedding &gt; Normal embedding &gt;
Joint &gt; Bud- embedding,.. ;
Continuation + Smooth shifting + Joint &gt;
Abrupt shifting + Normal embedding
Good embedding &gt; Continuation + Joint
Conjunct &gt; Good embedding
</table>
<bodyText confidence="0.998659666666667">
The +&apos; symbol can be interpreted in different
ways, depending on how the features are used
in an NLG system. In a traditional system,. it
means the coexistence of two features.. In a sys-
tem using numbers for planning, it can have the
same meaning as the arithmetic symbol.
</bodyText>
<sectionHeader confidence="0.887681" genericHeader="method">
3 Capturing the preferences in ILEX
</sectionHeader>
<bodyText confidence="0.999310972972973">
The architecture of text planning has a great
effect on aggregation possibilities. In object de-
scriptive text generation, there lacks a central
overriding communicative goal which could be
decomposed in a structured way into subgoals.
The main goal is to provide interesting infor-
mation about the target object. There are gen-
erally only a small number of relations, mainly
object-attribute elaboration and joint. For such a
genre, a domain-dependent bottom-up planner
(Marcu, 1997) or opportunistic planner (Mel-
lish et al., 1998b) suits better than a domain-
independent top-down planner. In these archi-
tectures, aggregation is important to text plan-
ning because it changes the order in which infor-
mation is expressed. The first implementation
we will describe is based on ILEX (Oberlander
et al., 1998).
ILEX is an adaptive hypertext generation
system, providing natural language descriptions
for museum objects. The bottom-up text plan-
ning is fulfilled in two steps: a content selection
procedure, where a set of fact nodes with high
relevance is selected from the Content Potential
(following a search algorithm), and a content
structuring procedure, where selected facts are
reorganised to form entity-chains (based on the
theory of entity-based coherence), which repre-
sent a coherent text arrangement.
To make it possible for the ILEX planner to
take into account aggregation, we use a revised
version of Nleteer&apos;s Text Structure (Meteer.
1992; Panaget, 1997) as the intermediate level of
representation between text planning and sen-
tence realisation to provide abstract syntactic
constraints to the planning. We call this *-s-
tem ILEX-TS (ILEX based on Text Structure).
</bodyText>
<page confidence="0.998033">
189
</page>
<bodyText confidence="0.999842375">
In ILEX-TS, abstract referring expression de-
termination and.aggregation are performed dur-
ing text structuring. For each fact whose Text
Structure is being built, if an NP in the fact can
take modifiers, the embedding process will find
a list of elaboration facts to the referent and
make embedding decisions based on the con-
straints imposed by the NP form. The decisions
include what to embed and what syntactic form
the embedded part should use.
Heuristic 1, 2 and 3 are followed naturally
by the ILEX text planner, which calculates the
best RS tree and puts facts connected by the
imaginary conjunct relation next to each other.
It tries to feature center continuations as often
as possible. When it needs to shift topic, it uses
a smooth shifting.
ILEX-TS has a set of embedding rules, where
those rules featuring good embedding are al-
ways used first, then a rule featuring a normal
embedding. Bad embedding is not allowed at
all. To coordinate different types of aggrega-
tion, the algorithm checks parataxis and hy-
potaxis possibilities for each nucleus fact and
the fact to be embedded before it applies an
embedding rule. These realise most of Heuris-
tic 4 (except for the second set). However, be-
cause the various factors are optimised in order
(with no backtracking), there is no guarantee
that the best overall text will be found. In addi-
tion, complex interactions between aggregation
and center transition cannot be easily captured.
</bodyText>
<sectionHeader confidence="0.649347" genericHeader="method">
4 Text planning using a GA
</sectionHeader>
<bodyText confidence="0.999159">
Although most heuristics can be followed in
ILEX-TS, some interactions are missing, for ex-
ample, 2 of Figure 1 will probably he generated.
For better coordination, we adopt the text plan-
ner based on a Genetic Algorithm (GA) as de-
scribed in (Mellish et al., 1998a). The task is,
given a set of facts arid a set of relations between
facts, to produce a legal rhetoricatstructure tree
using all the facts and some relations.
A fact is represented in terms of a subject,
a verb and a complement (as well as a unique
identifier). A relation is represented in terms of
the relation name, the two facts that are con-
nected by the relation and a list of precondition
facts which need to have been mentioned before
the relation can be used&apos;.
A genetic algorithm is suitable for such a
problem because. the .number of .possible.com-
binations is huge and the search space is not
perfectly smooth and unimodal (there can be
many good combinations). Also the generation
task does not require a global optimum to be
found. What we need is a combination that is
coherent enough for people to understand.
(Mellish et al., 1998a) summarises the genetic
algorithm roughly as follows:
</bodyText>
<listItem confidence="0.988760222222222">
1. Enumerate a set of random initial se-
quences by loosely following sequences of
facts where consecutive facts mention the
same entity.
2. Evaluate sequences by evaluating the
rhetorical structure trees they give rise to.
3. Perform mutation and crossover on the se-
quences.
4. Stop after a given number of iterations, and
</listItem>
<bodyText confidence="0.961404068965517">
return the tree for the &amp;quot;best&amp;quot; sequence.
The advantage of this approach is that it pro-
vides a mechanism to integrate planning factors
in the evaluation function and search for the
best combinations of them. So it is an excellent
framework for experimenting with the interac-
tion between aggregation and text planning.
In the algorithm, the RS trees are right-
branching and are almost deterministically built
from sequences of facts. Given two sequences,
crossover inserts a random segment from one
sequence in a random position in the other to
produce two new sequences. Mutation selects
a random segment of a sequence and moves it
into a random position in the same sequence.
To explore the whole space of aggregation.
we decide not to perform aggregation on struc-
tured facts or on adjacent facts in a linear se-
quence because they might restrict the possibil-
ities and even miss out good candidates. In-
stead, we define a third operator called embed-
ding mvtation. Suppose we have a sequence
1.14 where we call each element
of the sequence a -unit, which can be either a fact
or a list of facts or units with no depth limit.
For a list, we call its very first fact the main fact,
system is limited in all aspects. It clops net have a real
realisation component. so the parts we are less interested
As this is an experimental system, the ability of the in are realised by canned phrases for readability.
</bodyText>
<page confidence="0.965833">
190
</page>
<figure confidence="0.920435631578947">
. Features/Factors Values (raters)
1 2
Semantic relations -20 -46
a joint
a conjunct or disjunct 10 11
a relation other than joint, conjunct or disjunct 21 69
a conjunct inside other semantic relations -50 -63
a precondition not satisfied -30 -61
Focus moves 20 7
a continuing
an associate shifting 16 1
a smooth shifting 14 -3
resuming a previous focus 6 -43
Embedding 6 3
a good embedding
a normal embedding 3 0
a bad embedding -30 -64
Others -10 -12
topic not mentioned in the first sentence
</figure>
<tableCaption confidence="0.913945">
Table I: Two different raters satisfying the same constraints
</tableCaption>
<figureCaption confidence="0.985695">
Figure 2: Scores for four museum descriptive texts
</figureCaption>
<figure confidence="0.91781">
CHOKER
HOUSE
.CENSER
TURCINE
</figure>
<bodyText confidence="0.99982880952381">
into which the remaining facts in the list are to
be embedded. The embedding mutation ran-
domly selects a unit U from the sequence and
an entity in its main fact. It then collects all
the units mentioning this entity and randomly
chooses one Uk. The list containing these two
units [Lli,&amp;quot;Uk] represents a random embedding
and will be treated as a single unit in later op-
erations. It takes the. position of I.JE to produce
a new sequence [151,U9,...,[li-E,Uk],...,15,1 and all
repetitions outside II.:;,Uk] are removed. This
sequence is then evaluated and ordered in the
population.
The probabilities of applying the three Opera-
tor S are: 63% for crossover. 30% for embedding
mutation and 3% for normal mutation. This is
because the first two are more likely to produce
sequences bearing desired properties by either
combining the good bits of two sequences or
performing a reasonable amount of embedding,
whereas normal mutation is entirely random2.
</bodyText>
<sectionHeader confidence="0.5293175" genericHeader="method">
5 Justifying the GA evaluation
function
</sectionHeader>
<bodyText confidence="0.994412">
The linguistic theories discussed in Section 2
only give evidence in qualitative terms. For a
GA-based planner to work, we have to come up
Nvith actual numbers that can be used to evalu-
</bodyText>
<footnote confidence="0.9970505">
2The values For crossover and mutation rate used in
our algorithm are fairly standard.
</footnote>
<page confidence="0.99757">
191
</page>
<bodyText confidence="0.950971375">
The small portable throne from the time of the Qianlong Emperor 1736-95 is made
of lacquered wood-with decoration-in-gold-and-red.-ft -was-used-in-the-private apartrtentS.
of the Imperial Palaces. The cover from the reign of Jiaquing, 1796-1820 is woven in yellow
silk, which is the imperial colour of the Qing Dynasty,1644-1911. It would have covered
the throne when not in use.
The design on the seat is a imperial five clawed dragon in a circular medallion. On the
inside of the arm pieCes are small shelves. Precious possessions can be placed in small shelves
and can be studied as an aid to contemplation.
</bodyText>
<figureCaption confidence="0.983207">
Figure 3: A generated text scored the highest, with the embedded parts highlighted
</figureCaption>
<table confidence="0.999660333333333">
Score2 Score3 Score4 Score5 Score6
Score1 .9567 .9337 .9631 .9419 .9515
Score2 .9435 .8819 .9280 .9185
Score3 .8650 .8462 .9574
Score4 .9503 .8940
Score5 .8486
</table>
<tableCaption confidence="0.999765">
Table 2: Correlations between six raters
</tableCaption>
<bodyText confidence="0.999292129032258">
ate an RS tree. Mellish et al. (1998a) present
some scores for evaluating the basic features of a
tree, but they make it clear that the scores are
there for descriptive purpose, not for making
any serious claim about the best way of evalu-
ating RS trees.
The methodology we adopted was that we
took the existing evaluation function and ex-
tended it to take into account features for local
coherence, embedding and semantic parataxis.
This resulted in rater 1 in Table 1, which sat-
isfied all the heuristics mentioned in Section 2.
We manually broke down four human written
museum descriptions into individual facts and
relations and reconstructed sequences of facts
with the same orderings and aggregations as in
the original texts. We then used our evaluation
function to score the RS trees built from these
sequences. In the mean time, we ran the GA
algorithm for 5000 iterations on the facts and
relations for 10 times. The results are shown in
Figure 2, where the four line styles correspond
to the four texts. The jagged lines represent the
scores of the machine generated texts and the
straight lines represent the scores for the corre-
sponding human texts.
All human texts were scored among the high-
est and machine generated texts can get scores
very close to human ones sometimes. Since the
human texts were written and revised by nm-
seum experts. they can be treated as &amp;quot;nearly
best texts&amp;quot;. The figure shows that the evalu-
ation function based on our heuristics can find
good and correct combinations. The reason for
a relatively bad text being generated sometimes
might be that really bad sequences were pro-
duced at the beginning. This could be improved
by using certain heuristics to get better initial
sequences. Also when the number of facts be-
comes larger, more iterations are needed to get
readable texts. Figure 3 gives a text generated
using rater 1.
To justify our claim that it is the preferences
among generation factors that decide the coher-
ence of a text, we fed the preferences into a con-
straint based program. If a feature can take a
range of values, the program randomly selects
a. number in that range. A number of raters
compatible with the constraints were generated
and one of them is given in Table 1 as rater 2.
We then generated all possible combinations, in-
cluding embedding, of se-veil facts from a human
text and used six randomly produced raters to
score each of them.
The qualities the, generated texts are nor-
mal distributed according to all raters. The
raters distinguish between good and bad texts
and they classify the majority of texts as of
moderate quality and only very small percent-
ages as very good or very bad texts. The be-
haviours of the raters are very similar as the
histograms are of roughly the same shape.
</bodyText>
<page confidence="0.994453">
192
</page>
<bodyText confidence="0.999952">
To see to what extent the six raters agree with
each other, we calculated the Pearson correla-
tion coefficient between them, which is shown
in Table 2. We can claim from the table that
for this data, the scores from the six raters cor-
relate, and we have a fairly good chance to be-
lieve that the six raters, randomly produced in
a sense, agree with each other on evaluating the
text and they measure basically the same thing.
</bodyText>
<sectionHeader confidence="0.99593" genericHeader="conclusions">
6 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999712111111111">
This paper describes an experiment with the
preferences among features concerning aggrega-
tion and text planning, in particular, we present
an mechanism for how relevant features can be
scored to contribute together to the planning of
a coherent text. The statistical results partially
justify our claim that it is the preferences among
generation features that decide the coherence of
a text.
Our experiment could be extended in many
ways, for example, validating the evaluation
function through empirical analysis of human
assessments of the generated texts, and us-
ing more texts to test the correlation between
raters. The architecture based on the Genetic
Algorithm can also be used for testing interac-
tions between or within other text generation
modules.
</bodyText>
<sectionHeader confidence="0.999242" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999909275">
Susan Brennan, Marilyn Walker Friedman, and Carl
Pollard. 1987. A centering apporach to pronouns.
In Proceedings of the 25th Annual Meeting of the
Association for Computational Linguistics, pages
155-162, Stanford, CA.
Hua Cheng. 1998. Embedding new information into
referring expressions. In Proceedings of COLING-
A CL &apos;98, pages 1478-1480, Montreal, Canada.
Barbara Grosz and Candace Sidner. 1986. Atten-
tions, intentions and the structure of discourse.
Computational Linguistics, 12:175-204_
Barbara Grosz, Aravind Joshi, and Scott Weinstein.
1995_ Centering: A framework for modelling the
local coherence of discourse. Computational Lin-
guistics, 21(2):203-226.
Barbara Grosz. 1977. The representation and use of
focus in dialogue understanding. Technical report
151, SRI International.
William Mann and Sandra Thompson. 1987.
Rhetorical structure theory: A theory of text or-
ganization. Technical Report ISI/R-87-19O. In-
formation Sciences Institute. University of South-
ern California.
Daniel Marcu. 1997. From local to global coherence:
bottom,up approach. to..text.planning. In Pro-
ceedings of the Fourteenth National Conference on
Artificial Intelligence, pages 629-635, Providence,
Rhode Island.
Chris Mellish, Alistair Knott, Jon Oberlander,
and Mick O&apos;Donnell. 1998a. Experiments using
stochastic search for text planning. In Proceed-
ings of the 9th International Workshop on Natural
Language Generation, Ontario, Canada.
Chris,Mellish,, Mick 0 lionnell,, J on Oberlander, and
Alistair Knott. 1998b. An architecture for op-
portunistic text generation. In Proceedings of the
9th International Workshop on Natural Language
Generation, Ontario, Canada.
Marie Meteer. 1992. Expressibility and The Prob-
lem of Efficient Text Planning. Communication
in Artificial Intelligence. Pinter Publishers Lim-
ited, London.
Megan Moser and Johanna Moore. 1996. Toward a
synthesis of two accounts of discourse structure.
Computational Linguistics, 22(3):409-419.
Jon Oberlander, Mick O&apos;Donnell, Ali Knott, and
Chris Mellish. 1998. Conversation in the mu-
seum: Experiments in dynamic hypermedia with
the intelligent labelling explorer. New Review of
Hypermedia and Multimedia, 4:11-32.
Jon Oberlander, Alistair Knott, Mick O&apos;Donnell,
and Chris Mellish. 1999, Beyond elaboration:
Generating descriptive texts containing it-clefts.
In T Sanders, J Schilperoord, and W Spooren,
editors, Text Representation: Linguistic and Psy-
cholinguistic Aspects. Benjamins, Amsterdam.
Franck Panaget. 1997. Micro-planning: A uni-
fied representation of lexical and grammatical re-
sources. In Proceeding of the 6th European Work-
shop on Natural Language Generation, pages 97-
106.
Massimo Poesio, Renata Vieira., and Simone Teufel.
1997. Resolving bridging references in Imre-
stricted text. Research paper hcrc-rp87, Centre
for Cognitive Science, University of Edinburgh.
Michael Reape and Chris Mellish. 1999. Just what
is aggregation anyway? In Proceedings of the 7t0
European Workshop on Natural Language Gener-
ation, pages 20-29, Toulouse, France.
Roger Schank. 1977. Rules and topics in conversa-
tion. Cognitive Science, 101:421-441.
Donia Scott and Clarisse Sieckenius de Souza. 1990.
• Getting the&apos; messageacross in rst-based text gen-
eration. In R. Dale, C. Mellish, and M. Zook, edi-
tors. Current Research in Natural Language Cr&apos;en-
cration, pages 47--73. Academic Press_
John Wilkinson. 1995. Aggregation in Natural Lan-
guage Generation: Another Look. Technical re-
port, Computer Science Department. University
of Waterloo.
</reference>
<page confidence="0.999251">
193
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998891">Capturing the Interaction between Aggregation and Planning in Two Generation Systems</title>
<author confidence="0.868166">Hua Cheng</author>
<author confidence="0.868166">Chris</author>
<affiliation confidence="0.97641">Division of Informatics, University of</affiliation>
<address confidence="0.91653">South Bridge, Edinburgh Ef11</address>
<abstract confidence="0.99734500959693">chrisrnOdai. ed. Abstract language generation, different generation tasks often interact with each other in a complex way. We think that how to resolve the complex interactions inside and between tasks is more important to the generation of a coherent text than how to model each individual factor. This paper focuses on the interaction between aggregation and text planning, and tries to explore what preferences exist among the features considered by the two tasks. The preferences are implemented in two generation systems, namely ILEX-TS and a text planner using a Genetic Algorithm. The evaluation emphasises the second implementation and shows that capturing these preferences properly can lead to coherent text. 1 Discourse coherence and aggregation NLG, based on domain-independent rhetorical relations, in particular, Rhetorical Structure Theory (Mann and Thompson, 1987), are often used in text planning, whose task is to select the relevant information to be expressed and organise it into a hierarchical structure which captures certain discourse preferences such as preferences for the use of rhetorical relations_ In the theory of discourse structure developed by Grosz and Shiner (1986), each discourse segment exhibits two types of coherence: local coherence among utterances inside the segment, and global coherence between this segment and other discourse segments. Discourse segments connected by either a or There has been an effort to synthesise the two accounts of discourse structnre. \ loser and Moore (199(i) argue that the two theories have considerable common ground, which lies in the correspondence between the notion of dominance and nuclearity. It is possible to map between Grosz and Sidner&apos;s linguistic structure and RST text structure, and relation-based coherence and global coherence capture similar discourse properties. et al. propose a tinction between two types of discourse cohercoherence, exists between text spans connected by RST reexcept for elaboration, exists between spans of text in virtue of shared entities. coherence the coherence among adjacent propositions, which resembles coherence Grosz and Sidner&apos;s theory. a coherent text, text planning process must try to achieve both local (entitybased) and global (relation-based) coherence. Since the task of aggregation is to combine simple representations together to form a complex one, which in the mean time leads to a shorter text as a whole, aggregation could affect the ordering of text plans and the length of the whole text. Therefore, it is closely related to the task of maintaining both types of coherence. Here we treat embedding as a type of aggregation. There is no consensus as to where aggregation should happen or how it. is related to other generation processes (Wilkinson, 1995; Reape and Mellish, 1999). In many NLG systems, aggregation is a post planning process whose preferences are only partially taken into account by the text planner. local coherence In a structured text plan produced by the text planner, local coherence is normally maintained t hrough the ordering of the selected facts, where 186 certain types of center transition (e.g. cencontinuation) .are :preferred (e.g.. center shifting) (Centering Theory (Grosz et al., 1995)). Aggregation may affect text planning by taking away facts from a sequence featuring preferred center movements for embedding or subordination. As a result, the preferred center transitions in the original sequences could be cut off. For example, comparing the first descriptions of .a .necklace in Figure 2 coherent than of the shifting from the description of the necklace to that of the designer, which is a side effect of embedding. Since the centers of sentences are normally NPs and embedding adds non-restrictive cominto an NP, it could affect the way a is realised (e.g. preventing it from being a pronoun). As pointed out in (Grosz et al., 1995), different realisations (e.g. pronoun vs. definite description) are not equivalent with respect to their effect on coherence. Therefore, embedding could influence local coherence by forcing a different realisation from that preferred by Centering Theory. There is an obvious need to balance the consideration for local coherence and stylistic preferences. 1.2 Aggregation and global coherence types aggregation to be comamong in particular, embedding and semantic parataxis and hypotaxis. Usthe abstraction of semantic parataxis concerns facts related by explicit multi-nuclear relations (e.g. or by implicit connections like parallel common If facts have at least two identiparallel components, we say that a exists between them, and these relations are multi-nuclear relations_ Semantic hypotaxis concerns facts connected by relations (e.g. Semantic parataxis and hypotaxis feature in relationbased coherence and they depend on the text planner to put the related facts next to each other in order to perform a combination. (Cheng, 1998) describes interactions that need to be taken into account in aggregation. Firstly, complex embedded components like non-restrictive clauses may interrupt the semantic connection or syntactic similarity between a set of clauses. Secondly, the possibilities of other types of aggregation should be considered for both the main fact and the fact to be -; embedded -during :embedding decision . maki ng._ _ And thirdly, performing parataxis inside a hypotaxis could convey wrong information. We argue that the effect of aggregation is not limited to the particular NP or sentence where but to the coherence of the text as a whole. The complex interactions the features of aggregation to be evalwith other coherence features and aggregation to be planned as a part of text structuring. This requires better coordination between aggregation and other generation tasks as well as among different types of aggregation than is present in current NLG systems. In this paper, we describe how to capture the above interactions as preferences among related the of the prefervery different generation architectures to produce descriptions of museum objects on display. 2 Preferences among coherence features We claim that it is the relative preferences among features rather than the absolute magnitude of each individual one that play the crucial role in the production of a coherent text. In this we discuss preferences among features related to text planning, based on which those for embedding can be introduced. 2.1 Preferences for global coherence relation other than dispreferred to be used whenever usually conveys interesting information about domain objects and leads to a cohertext span. If a shares a fact a semantic relation, the suppressed. For example., in Figure from other relations, there is an amplificasignalled by a between the last two propositions. Compared less preferred because it misses the the center transition.frorn Arts and Crafts style jewel not smooth, whereas 3 expresses the amplificaand the However, a semantic relation can only be used if the knowledge assumed to be shared by the hearer is introduced in the previous discourse (Mellish et al.. 1998a). We assume the strategy 187 1. This necklace is in the Arts and Crafts style. Arts and Crafts style jewels usually have an elaborate design. They tend to have floral motifs. For instance, this necklace has floral motifs. It was designed by Jessie King. King was Scottish. She once lived in London. This necklace, designed by Jessie King, in the Arts and Crafts style. Arts and Crafts style jewels usually have an elaborate design. They tend to have floral motifs. For instance, this necklace has floral motifs. King was Scottish. She once lived in London. The in the Arts is set with jewels in that it features cabnchon Indeed, an Arts and Crafts style jewel usually uses cabochon stones. It usually uses stones. 4. The necklace is in the Arts and Crafts style. It is set. with jewels in that it features cabochon stones. An Arts and Crafts style jewel usually uses cabochon stones and oval stones. Figure 1: Aggregation examples (Mellish et al., 1998a) which uses a relation to connect every two text spans that do have a semantic relation other than objectelaboration Although not preferred when other relations are present, it is better than presuppositions or embedding a coninside a semantic relation. Therefore, we have the following heuristics, where &amp;quot;A&gt;B&amp;quot; means that A is preferred over B. 1 among features for global coherence: a semantic relation &gt; Conjunct/Disjunct &gt; Joint &gt; presuppositions not met Joint &gt; Conjunct inside a semantic relation for local coherence One way to achieve local coherence is to control center transitions among utterances. In Centering Theory, Rule 2 specifies preferences among center movement in a locally coherent segment: sequences of preferred over sequences of then preferred over sequences of Brennan et al. (1987) also describe typical discourse topic movements in terms of center transitions between pairs of utterances. They argue that the order of coherence among the is &gt; retaining &gt; smooth &gt; abrupt shifting. of claiming that these are the best models, we use them simply as an example of linguistic models being used for evaluating features of text planning. A type of center transition that appears frequently in descriptive text is that the description starts with an object, but shifts to associated objects or perspectives of that object_ This is a type of abrupt shifting, but it is appropriate as long as the objects are highly associated to the original object (Schank, 1977). This phenomenon is handled in the system of (Grosz, 1977), where subparts of an object are included into a focus space as the implicit foci when the object itself is to be included. call this center movement an the center moves from a trigger entity to a closely associated entity. Our informal observation from museum descriptions that shifting preferred by human writers to all other types of center moveexcept for are two of shifting: the trigger is in the previous utterance or two entities in two adjacent utterances have the same trigger. There is no preference between them. Heuristic 2 summarises the above preferences. We admit that these are strict heuristics and that human texts are sometimes more flexible. 2 among center transitions: Continuation &gt; Associate shifting &gt; Retaining &gt; Smooth shifting &gt; Abrupt shifting 2.3 Preferences for both types of coherence Two propositions can be connected in different ways, e.g. through a semantic relation or a smooth center transition only. Since a semantic relation is always preferred, we have the following heuristic: 3 among semantic relations and center transitions: a semantic relation &gt; Joint Continuation 188 2.4 Preferences for embedding on the features it bears. We do not claim that the set of features is complete. In a different context, more criteria might have to be considered. embedding one satisfying all the following conditions: 1. The referring part is an indefinite, a. demonstrative or a bridging description (as defined in (Poesio et al., 1997)). 2. The embedded part can be realised as an adjective or a prepositional phrase (Scott and de Souza, 1990). 3. In the resulting text, the embedded part does not lie between text spans connected by semantic parataxis and hypotaxis. 4. There is an available syntactic slot to hold the embedded part. embedding highly preferred and be performed whenever possible. A norembedding one satisfying condition 1, 3 and 4 and the embedded part is a relative clause which provides additional information about referent. embeddings all those left, for example, if there is no available syntactic slot for the embedded part. Since semantic parataxis has a higher priority embedding (Cheng, 1998), a embedbe less preferred than using a conbut it should be preferred over a center continuation for it to happen. To decide the interaction between an embedding and a center transition, we use the first two examples in Figure 1 again. The only difference the of the sentence &amp;quot;This necklace Was designed by Jessie King&amp;quot;, which can be represented in terms of features of local coherence and embedding as follows: last sentences in Joint + Continuation + Joint Smooth shifting last sentences plus embedding in Joint + Abrupt shifting + Normal embedding preferred the center moves smoothly in heuristics derived from above are summarised below: 4 among features for embedding and center transition: Good embedding &gt; Normal embedding &gt; &gt; Continuation + Smooth shifting + Joint &gt; Abrupt shifting + Normal embedding Good embedding &gt; Continuation + Joint Conjunct &gt; Good embedding The +&apos; symbol can be interpreted in different ways, depending on how the features are used in an NLG system. In a traditional system,. it the coexistence of two features.. system using numbers for planning, it can have the same meaning as the arithmetic symbol. Capturing the preferences in The architecture of text planning has a great effect on aggregation possibilities. In object descriptive text generation, there lacks a central overriding communicative goal which could be decomposed in a structured way into subgoals. The main goal is to provide interesting information about the target object. There are generally only a small number of relations, mainly elaboration such a genre, a domain-dependent bottom-up planner (Marcu, 1997) or opportunistic planner (Mellish et al., 1998b) suits better than a domainindependent top-down planner. In these archiis important to text planning because it changes the order in which information is expressed. The first implementation we will describe is based on ILEX (Oberlander et al., 1998). ILEX is an adaptive hypertext generation system, providing natural language descriptions for museum objects. The bottom-up text planning is fulfilled in two steps: a content selection procedure, where a set of fact nodes with high relevance is selected from the Content Potential (following a search algorithm), and a content structuring procedure, where selected facts are reorganised to form entity-chains (based on the theory of entity-based coherence), which represent a coherent text arrangement. To make it possible for the ILEX planner to take into account aggregation, we use a revised version of Nleteer&apos;s Text Structure (Meteer. 1992; Panaget, 1997) as the intermediate level of representation between text planning and sentence realisation to provide abstract syntactic to the planning. We call this tem ILEX-TS (ILEX based on Text Structure). 189 In ILEX-TS, abstract referring expression determination and.aggregation are performed during text structuring. For each fact whose Text Structure is being built, if an NP in the fact can take modifiers, the embedding process will find a list of elaboration facts to the referent and make embedding decisions based on the constraints imposed by the NP form. The decisions include what to embed and what syntactic form the embedded part should use. Heuristic 1, 2 and 3 are followed naturally by the ILEX text planner, which calculates the best RS tree and puts facts connected by the next to each other. It tries to feature center continuations as often as possible. When it needs to shift topic, it uses a smooth shifting. ILEX-TS has a set of embedding rules, where those rules featuring good embedding are always used first, then a rule featuring a normal embedding. Bad embedding is not allowed at all. To coordinate different types of aggregation, the algorithm checks parataxis and hypotaxis possibilities for each nucleus fact and the fact to be embedded before it applies an embedding rule. These realise most of Heuristic 4 (except for the second set). However, because the various factors are optimised in order (with no backtracking), there is no guarantee that the best overall text will be found. In addition, complex interactions between aggregation transition cannot be easily captured. planning using a GA Although most heuristics can be followed in ILEX-TS, some interactions are missing, for ex- 2 of Figure probably he generated. For better coordination, we adopt the text planner based on a Genetic Algorithm (GA) as described in (Mellish et al., 1998a). The task is, given a set of facts arid a set of relations between facts, to produce a legal rhetoricatstructure tree all facts and some relations. fact is represented terms of a subject, and a complement (as well as a unique A relation is in terms of the relation name, the two facts that are connected by the relation and a list of precondition facts which need to have been mentioned before the relation can be used&apos;. A genetic algorithm is suitable for such a problem because. the .number of .possible.combinations is huge and the search space is not perfectly smooth and unimodal (there can be many good combinations). Also the generation task does not require a global optimum to be found. What we need is a combination that is coherent enough for people to understand. (Mellish et al., 1998a) summarises the genetic algorithm roughly as follows: 1. Enumerate a set of random initial sequences by loosely following sequences of facts where consecutive facts mention the same entity. 2. Evaluate sequences by evaluating the rhetorical structure trees they give rise to. 3. Perform mutation and crossover on the sequences. 4. Stop after a given number of iterations, return the tree for the &amp;quot;best&amp;quot; sequence. The advantage of this approach is that it provides a mechanism to integrate planning factors in the evaluation function and search for the best combinations of them. So it is an excellent framework for experimenting with the interaction between aggregation and text planning. the algorithm, the RS trees rightbranching and are almost deterministically built from sequences of facts. Given two sequences, crossover inserts a random segment from one sequence in a random position in the other to two sequences. selects a random segment of a sequence and moves it into a random position in the same sequence. To explore the whole space of aggregation. we decide not to perform aggregation on strucfacts or on adjacent facts in a linear sethey might restrict the possibilities and even miss out good candidates. Inwe define third operator called embedmvtation. we have a sequence we call each the sequence a can be either a fact or a list of facts or units with no depth limit. a list, we call its very first fact the fact, system is limited in all aspects. It clops net have a real realisation component. so the parts we are less interested As this is an experimental system, the ability of the in are realised by canned phrases for readability. 190 . Features/Factors Values (raters) 1 2 Semantic relations a joint -20 -46 a conjunct or disjunct 10 11 other than joint, conjunct or disjunct 21 69 a conjunct inside other semantic relations -50 -63 not satisfied -30 -61 a continuing 20 7 an associate shifting 16 1 a smooth shifting 14 -3 resuming a previous focus 6 -43 Embedding a good embedding 6 3 a normal embedding 3 0 a bad embedding -30 -64 Others -10 -12 topic not mentioned in the first sentence Table I: Two different raters satisfying the same constraints Figure 2: Scores for four museum descriptive texts CHOKER HOUSE .CENSER TURCINE into which the remaining facts in the list are to be embedded. The embedding mutation randomly selects a unit U from the sequence and entity its It then collects all units entity and randomly chooses one Uk. The list containing these two units [Lli,&amp;quot;Uk] represents a random embedding and will be treated as a single unit in later op- It takes the. position of to produce new sequence and all outside are removed. This sequence is then evaluated and ordered in the population. The probabilities of applying the three Operator S are: 63% for crossover. 30% for embedding mutation and 3% for normal mutation. This is because the first two are more likely to produce sequences bearing desired properties by either combining the good bits of two sequences or performing a reasonable amount of embedding, normal mutation entirely 5 Justifying the GA evaluation function The linguistic theories discussed in Section 2 only give evidence in qualitative terms. For a GA-based planner to work, we have to come up numbers that can be used to evaluvalues For crossover and mutation rate used in our algorithm are fairly standard. 191 The small portable throne from the time of the Qianlong Emperor 1736-95 is made lacquered wood-with of the Imperial Palaces. The cover from the reign of Jiaquing, 1796-1820 is woven in yellow silk, which is the imperial colour of the Qing Dynasty,1644-1911. It would have covered the throne when not in use. The design on the seat is a imperial five clawed dragon in a circular medallion. On the of the arm pieCes are small shelves. Precious possessions can be placed in shelves can be studied as aid to contemplation.</abstract>
<note confidence="0.4269265">Figure 3: A generated text scored the highest, with the embedded parts highlighted Score2 Score3 Score4 Score5 Score6 Score1 .9567 .9337 .9631 .9419 .9515 Score2 .9435 .8819 .9280 .9185 Score3 .8650 .8462 .9574 Score4 .9503 .8940 Score5 .8486 Table 2: Correlations between six raters</note>
<abstract confidence="0.999522144444445">ate an RS tree. Mellish et al. (1998a) present some scores for evaluating the basic features of a but they make it clear that the scores for purpose, not for making serious about the best way of evaluating RS trees. The methodology we adopted was that we took the existing evaluation function and extended it to take into account features for local coherence, embedding and semantic parataxis. This resulted in rater 1 in Table 1, which satisfied all the heuristics mentioned in Section 2. broke down four human written descriptions into individual facts and reconstructed facts with the same orderings and aggregations as in the original texts. We then used our evaluation to RS from these sequences. In the mean time, we ran the GA algorithm for 5000 iterations on the facts and 10 in Figure 2, where the four line styles correspond the four texts. The jagged lines of the generated texts and the lines represent the scores for corretexts were scored among the highand generated can get scores to human ones sometimes. Since the human texts were written and revised by nmexperts. they can be treated &amp;quot;nearly best texts&amp;quot;. The figure shows that the evaluation function based on our heuristics can find good and correct combinations. The reason for a relatively bad text being generated sometimes might be that really bad sequences were produced at the beginning. This could be improved by using certain heuristics to get better initial sequences. Also when the number of facts belarger, more iterations are needed to texts. Figure 3 gives a generated using rater 1. justify our claim that it the preferences that decide the cohertext, we fed the preferences a constraint based program. If a feature can take a of values, the randomly selects a. number in that range. A number of raters compatible with the constraints were generated one of them is given in Table 1 rater 2. generated all possible combinations, inembedding, of facts from a human text and used six randomly produced raters to of qualities generated texts are noraccording to raters. The distinguish between good bad and they classify the majority of texts as of moderate quality and only very small percentages as very good or very bad texts. The behaviours of the raters are very similar as the histograms are of roughly the same shape. 192 To see to what extent the six raters agree with each other, we calculated the Pearson correlation coefficient between them, which is shown in Table 2. We can claim from the table that for this data, the scores from the six raters correlate, and we have a fairly good chance to believe that the six raters, randomly produced in a sense, agree with each other on evaluating the text and they measure basically the same thing. 6 Conclusions and future work This paper describes an experiment with the preferences among features concerning aggregation and text planning, in particular, we present an mechanism for how relevant features can be scored to contribute together to the planning of a coherent text. The statistical results partially justify our claim that it is the preferences among generation features that decide the coherence of a text. Our experiment could be extended in many ways, for example, validating the evaluation function through empirical analysis of human assessments of the generated texts, and using more texts to test the correlation between raters. The architecture based on the Genetic Algorithm can also be used for testing interacbetween or within other generation modules.</abstract>
<note confidence="0.850809777777778">References Susan Brennan, Marilyn Walker Friedman, and Carl Pollard. 1987. A centering apporach to pronouns. of the 25th Annual Meeting of the for Computational Linguistics, 155-162, Stanford, CA. Hua Cheng. 1998. Embedding new information into expressions. In of COLING- CL &apos;98, 1478-1480, Montreal, Canada. Barbara Grosz and Candace Sidner. 1986. Attentions, intentions and the structure of discourse. Linguistics, Barbara Grosz, Aravind Joshi, and Scott Weinstein. 1995_ Centering: A framework for modelling the coherence of discourse. Lin- Barbara Grosz. 1977. The representation and use of focus in dialogue understanding. Technical report 151, SRI International. William Mann and Sandra Thompson. 1987. Rhetorical structure theory: A theory of text organization. Technical Report ISI/R-87-19O. Information Sciences Institute. University of Southern California. 1997. From local to global coherence: In Proceedings of the Fourteenth National Conference on Intelligence, Providence, Mellish, Alistair Knott, Oberlander, Mick O&apos;Donnell. Experiments using search for text In Proceedings of the 9th International Workshop on Natural Generation, Canada. Chris,Mellish,, Mick 0 lionnell,, J on Oberlander, and Knott. 1998b. architecture for optext generation. In of the 9th International Workshop on Natural Language</note>
<address confidence="0.679617">Ontario, Meteer. 1992. and The Prob-</address>
<abstract confidence="0.662522357142857">of Efficient Text Planning. Artificial Intelligence. Pinter Publishers Lim- Moser and Johanna Moore. 1996. a two accounts of discourse structure. Linguistics, Jon Oberlander, Mick O&apos;Donnell, Ali Knott, and Mellish. 1998. in the muin dynamic hypermedia with intelligent labelling explorer. of Multimedia, 4:11-32. Jon Oberlander, Alistair Knott, Mick O&apos;Donnell, and Chris Mellish. 1999, Beyond elaboration: descriptive containing T J Schilperoord, and W</abstract>
<note confidence="0.868721636363636">Representation: Linguistic and Psy- Aspects. Amsterdam. Franck Panaget. 1997. Micro-planning: A unified representation of lexical and grammatical re- In of the 6th European Workon Natural Language Generation, 97- 106. Massimo Poesio, Renata Vieira., and Simone Teufel. 1997. Resolving bridging references in Imrestricted text. Research paper hcrc-rp87, Centre for Cognitive Science, University of Edinburgh. Michael Reape and Chris Mellish. 1999. Just what aggregation anyway? In of the 7t0 European Workshop on Natural Language Gener- 20-29, Toulouse, France. Roger Schank. 1977. Rules and topics in conversa- Science, Donia Scott and Clarisse Sieckenius de Souza. 1990. • Getting the&apos; messageacross in rst-based text generation. In R. Dale, C. Mellish, and M. Zook, edi- Research in Natural Language Cr&apos;en- 47--73. Academic Press_</note>
<author confidence="0.870969">Aggregation in Natural Lan-</author>
<affiliation confidence="0.930845">guage Generation: Another Look. Technical report, Computer Science Department. University</affiliation>
<address confidence="0.5711625">of Waterloo. 193</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Susan Brennan</author>
<author>Marilyn Walker Friedman</author>
<author>Carl Pollard</author>
</authors>
<title>A centering apporach to pronouns.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>155--162</pages>
<location>Stanford, CA.</location>
<contexts>
<context position="10163" citStr="Brennan et al. (1987)" startWordPosition="1621" endWordPosition="1624">owing heuristics, where &amp;quot;A&gt;B&amp;quot; means that A is preferred over B. Heuristic 1 Preferences among features for global coherence: a semantic relation &gt; Conjunct/Disjunct &gt; Joint &gt; presuppositions not met Joint &gt; Conjunct inside a semantic relation 2.2 Preferences for local coherence One way to achieve local coherence is to control center transitions among utterances. In Centering Theory, Rule 2 specifies preferences among center movement in a locally coherent discourse segment: sequences of continuation are preferred over sequences of retaining, which are then preferred over sequences of shifting. Brennan et al. (1987) also describe typical discourse topic movements in terms of center transitions between pairs of utterances. They argue that the order of coherence among the transitions is continuing &gt; retaining &gt; smooth shifting &gt; abrupt shifting. Instead of claiming that these are the best models, we use them simply as an example of linguistic models being used for evaluating features of text planning. A type of center transition that appears frequently in descriptive text is that the description starts with an object, but shifts to associated objects or perspectives of that object_ This is a type of abrupt</context>
</contexts>
<marker>Brennan, Friedman, Pollard, 1987</marker>
<rawString>Susan Brennan, Marilyn Walker Friedman, and Carl Pollard. 1987. A centering apporach to pronouns. In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, pages 155-162, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hua Cheng</author>
</authors>
<title>Embedding new information into referring expressions.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGA CL &apos;98,</booktitle>
<pages>1478--1480</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="5642" citStr="Cheng, 1998" startWordPosition="885" endWordPosition="886">oncerns facts related by explicit multi-nuclear semantic relations (e.g. sequence and contrast) or by implicit connections like parallel common parts. If two facts have at least two identical parallel components, we say that a conjunct or disjunct relation exists between them, and these relations are multi-nuclear relations_ Semantic hypotaxis concerns facts connected by nucleus-satellite relations (e.g. cause). Semantic parataxis and hypotaxis feature in relationbased coherence and they depend on the text planner to put the related facts next to each other in order to perform a combination. (Cheng, 1998) describes interactions that need to be taken into account in aggregation. Firstly, complex embedded components like non-restrictive clauses may interrupt the semantic connection or syntactic similarity between a set of clauses. Secondly, the possibilities of other types of aggregation should be considered for both the main fact and the fact to be -; embedded -during :embedding decision . maki ng._ _ And thirdly, performing parataxis inside a hypotaxis could convey wrong information. We argue that the effect of aggregation is not limited to the particular NP or sentence where aggregation happe</context>
<context position="13341" citStr="Cheng, 1998" startWordPosition="2145" endWordPosition="2146">In the resulting text, the embedded part does not lie between text spans connected by semantic parataxis and hypotaxis. 4. There is an available syntactic slot to hold the embedded part. A good embedding is highly preferred and should be performed whenever possible. A normal embedding is one satisfying condition 1, 3 and 4 and the embedded part is a relative clause which provides additional information about the referent. Bad embeddings are all those left, for example, if there is no available syntactic slot for the embedded part. Since semantic parataxis has a higher priority than embedding (Cheng, 1998), a good embedding should be less preferred than using a conjunct relation, but it should be preferred over a center continuation for it to happen. To decide the interaction between an embedding and a center transition, we use the first two examples in Figure 1 again. The only difference between I and 2 is the position of the sentence &amp;quot;This necklace Was designed by Jessie King&amp;quot;, which can be represented in terms of features of local coherence and embedding as follows: the last three sentences in 1: Joint + Continuation + Joint Smooth shifting the last two sentences plus embedding in 2: Joint +</context>
</contexts>
<marker>Cheng, 1998</marker>
<rawString>Hua Cheng. 1998. Embedding new information into referring expressions. In Proceedings of COLINGA CL &apos;98, pages 1478-1480, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Candace Sidner</author>
</authors>
<title>Attentions, intentions and the structure of discourse.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--175</pages>
<marker>Grosz, Sidner, 1986</marker>
<rawString>Barbara Grosz and Candace Sidner. 1986. Attentions, intentions and the structure of discourse. Computational Linguistics, 12:175-204_</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Aravind Joshi</author>
<author>Scott Weinstein</author>
</authors>
<title>Centering: A framework for modelling the local coherence of discourse.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--2</pages>
<contexts>
<context position="3760" citStr="Grosz et al., 1995" startWordPosition="585" endWordPosition="588">onsensus as to where aggregation should happen or how it. is related to other generation processes (Wilkinson, 1995; Reape and Mellish, 1999). In many NLG systems, aggregation is a post planning process whose preferences are only partially taken into account by the text planner. 1.1 Aggregation and local coherence In a structured text plan produced by the text planner, local coherence is normally maintained t hrough the ordering of the selected facts, where 186 certain types of center transition (e.g. center continuation) .are :preferred -over-others (e.g.. center shifting) (Centering Theory (Grosz et al., 1995)). Aggregation may affect text planning by taking away facts from a sequence featuring preferred center movements for embedding or subordination. As a result, the preferred center transitions in the original sequences could be cut off. For example, comparing the first two descriptions of .a .necklace in Figure 1, 2 is less coherent than I because of the shifting from the description of the necklace to that of the designer, which is a side effect of embedding. Since the centers of sentences are normally NPs and embedding adds non-restrictive components into an NP, it could affect the way a Cb i</context>
</contexts>
<marker>Grosz, Joshi, Weinstein, 1995</marker>
<rawString>Barbara Grosz, Aravind Joshi, and Scott Weinstein. 1995_ Centering: A framework for modelling the local coherence of discourse. Computational Linguistics, 21(2):203-226.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
</authors>
<title>The representation and use of focus in dialogue understanding.</title>
<date>1977</date>
<tech>Technical report 151,</tech>
<publisher>SRI International.</publisher>
<contexts>
<context position="10937" citStr="Grosz, 1977" startWordPosition="1753" endWordPosition="1754">sitions is continuing &gt; retaining &gt; smooth shifting &gt; abrupt shifting. Instead of claiming that these are the best models, we use them simply as an example of linguistic models being used for evaluating features of text planning. A type of center transition that appears frequently in descriptive text is that the description starts with an object, but shifts to associated objects or perspectives of that object_ This is a type of abrupt shifting, but it is appropriate as long as the objects are highly associated to the original object (Schank, 1977). This phenomenon is handled in the system of (Grosz, 1977), where subparts of an object are included into a focus space as the implicit foci when the object itself is to be included. We call this center movement an associate shifting, where the center moves from a trigger entity to a closely associated entity. Our informal observation from museum descriptions shows that associate shifting is preferred by human writers to all other types of center movements except for continuation. There are two types of associate shifting: where the trigger is in the previous utterance or two entities in two adjacent utterances have the same trigger. There is no pref</context>
</contexts>
<marker>Grosz, 1977</marker>
<rawString>Barbara Grosz. 1977. The representation and use of focus in dialogue understanding. Technical report 151, SRI International.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Mann</author>
<author>Sandra Thompson</author>
</authors>
<title>Rhetorical structure theory: A theory of text organization.</title>
<date>1987</date>
<tech>Technical Report ISI/R-87-19O.</tech>
<institution>Information Sciences Institute. University of Southern California.</institution>
<contexts>
<context position="1125" citStr="Mann and Thompson, 1987" startWordPosition="170" endWordPosition="173"> model each individual factor. This paper focuses on the interaction between aggregation and text planning, and tries to explore what preferences exist among the features considered by the two tasks. The preferences are implemented in two generation systems, namely ILEX-TS and a text planner using a Genetic Algorithm. The evaluation emphasises the second implementation and shows that capturing these preferences properly can lead to coherent text. 1 Discourse coherence and aggregation In NLG, theories based on domain-independent rhetorical relations, in particular, Rhetorical Structure Theory (Mann and Thompson, 1987), are often used in text planning, whose task is to select the relevant information to be expressed and organise it into a hierarchical structure which captures certain discourse preferences such as preferences for the use of rhetorical relations_ In the theory of discourse structure developed by Grosz and Shiner (1986), each discourse segment exhibits two types of coherence: local coherence among utterances inside the segment, and global coherence between this segment and other discourse segments. Discourse segments are connected by either a dominance relation or a satisfaction-precedence rel</context>
</contexts>
<marker>Mann, Thompson, 1987</marker>
<rawString>William Mann and Sandra Thompson. 1987. Rhetorical structure theory: A theory of text organization. Technical Report ISI/R-87-19O. Information Sciences Institute. University of Southern California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>From local to global coherence: bottom,up approach. to..text.planning.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence,</booktitle>
<pages>629--635</pages>
<location>Providence, Rhode Island.</location>
<contexts>
<context position="15186" citStr="Marcu, 1997" startWordPosition="2452" endWordPosition="2453">In a system using numbers for planning, it can have the same meaning as the arithmetic symbol. 3 Capturing the preferences in ILEX The architecture of text planning has a great effect on aggregation possibilities. In object descriptive text generation, there lacks a central overriding communicative goal which could be decomposed in a structured way into subgoals. The main goal is to provide interesting information about the target object. There are generally only a small number of relations, mainly object-attribute elaboration and joint. For such a genre, a domain-dependent bottom-up planner (Marcu, 1997) or opportunistic planner (Mellish et al., 1998b) suits better than a domainindependent top-down planner. In these architectures, aggregation is important to text planning because it changes the order in which information is expressed. The first implementation we will describe is based on ILEX (Oberlander et al., 1998). ILEX is an adaptive hypertext generation system, providing natural language descriptions for museum objects. The bottom-up text planning is fulfilled in two steps: a content selection procedure, where a set of fact nodes with high relevance is selected from the Content Potentia</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Daniel Marcu. 1997. From local to global coherence: bottom,up approach. to..text.planning. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, pages 629-635, Providence, Rhode Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Mellish</author>
<author>Alistair Knott</author>
<author>Jon Oberlander</author>
<author>Mick O&apos;Donnell</author>
</authors>
<title>Experiments using stochastic search for text planning.</title>
<date>1998</date>
<booktitle>In Proceedings of the 9th International Workshop on Natural Language Generation,</booktitle>
<location>Ontario, Canada.</location>
<contexts>
<context position="9173" citStr="Mellish et al., 1998" startWordPosition="1469" endWordPosition="1472"> style jewels usually have an elaborate design. They tend to have floral motifs. For instance, this necklace has floral motifs. King was Scottish. She once lived in London. 3. The necklace is in the Arts and Crafts style. It is set with jewels in that it features cabnchon stones. Indeed, an Arts and Crafts style jewel usually uses cabochon stones. It usually uses oval stones. 4. The necklace is in the Arts and Crafts style. It is set. with jewels in that it features cabochon stones. An Arts and Crafts style jewel usually uses cabochon stones and oval stones. Figure 1: Aggregation examples of (Mellish et al., 1998a) which uses a Joint relation to connect every two text spans that do not have a semantic relation other than objectattribute elaboration and conjunct/disjunct in between. Although joint is not preferred when other relations are present, it is better than missing presuppositions or embedding a conjunct relation inside a semantic relation. Therefore, we have the following heuristics, where &amp;quot;A&gt;B&amp;quot; means that A is preferred over B. Heuristic 1 Preferences among features for global coherence: a semantic relation &gt; Conjunct/Disjunct &gt; Joint &gt; presuppositions not met Joint &gt; Conjunct inside a semant</context>
<context position="15233" citStr="Mellish et al., 1998" startWordPosition="2457" endWordPosition="2461"> it can have the same meaning as the arithmetic symbol. 3 Capturing the preferences in ILEX The architecture of text planning has a great effect on aggregation possibilities. In object descriptive text generation, there lacks a central overriding communicative goal which could be decomposed in a structured way into subgoals. The main goal is to provide interesting information about the target object. There are generally only a small number of relations, mainly object-attribute elaboration and joint. For such a genre, a domain-dependent bottom-up planner (Marcu, 1997) or opportunistic planner (Mellish et al., 1998b) suits better than a domainindependent top-down planner. In these architectures, aggregation is important to text planning because it changes the order in which information is expressed. The first implementation we will describe is based on ILEX (Oberlander et al., 1998). ILEX is an adaptive hypertext generation system, providing natural language descriptions for museum objects. The bottom-up text planning is fulfilled in two steps: a content selection procedure, where a set of fact nodes with high relevance is selected from the Content Potential (following a search algorithm), and a content</context>
<context position="18110" citStr="Mellish et al., 1998" startWordPosition="2928" endWordPosition="2931">n embedding rule. These realise most of Heuristic 4 (except for the second set). However, because the various factors are optimised in order (with no backtracking), there is no guarantee that the best overall text will be found. In addition, complex interactions between aggregation and center transition cannot be easily captured. 4 Text planning using a GA Although most heuristics can be followed in ILEX-TS, some interactions are missing, for example, 2 of Figure 1 will probably he generated. For better coordination, we adopt the text planner based on a Genetic Algorithm (GA) as described in (Mellish et al., 1998a). The task is, given a set of facts arid a set of relations between facts, to produce a legal rhetoricatstructure tree using all the facts and some relations. A fact is represented in terms of a subject, a verb and a complement (as well as a unique identifier). A relation is represented in terms of the relation name, the two facts that are connected by the relation and a list of precondition facts which need to have been mentioned before the relation can be used&apos;. A genetic algorithm is suitable for such a problem because. the .number of .possible.combinations is huge and the search space is</context>
<context position="23571" citStr="Mellish et al. (1998" startWordPosition="3856" endWordPosition="3859">Dynasty,1644-1911. It would have covered the throne when not in use. The design on the seat is a imperial five clawed dragon in a circular medallion. On the inside of the arm pieCes are small shelves. Precious possessions can be placed in small shelves and can be studied as an aid to contemplation. Figure 3: A generated text scored the highest, with the embedded parts highlighted Score2 Score3 Score4 Score5 Score6 Score1 .9567 .9337 .9631 .9419 .9515 Score2 .9435 .8819 .9280 .9185 Score3 .8650 .8462 .9574 Score4 .9503 .8940 Score5 .8486 Table 2: Correlations between six raters ate an RS tree. Mellish et al. (1998a) present some scores for evaluating the basic features of a tree, but they make it clear that the scores are there for descriptive purpose, not for making any serious claim about the best way of evaluating RS trees. The methodology we adopted was that we took the existing evaluation function and extended it to take into account features for local coherence, embedding and semantic parataxis. This resulted in rater 1 in Table 1, which satisfied all the heuristics mentioned in Section 2. We manually broke down four human written museum descriptions into individual facts and relations and recons</context>
</contexts>
<marker>Mellish, Knott, Oberlander, O&apos;Donnell, 1998</marker>
<rawString>Chris Mellish, Alistair Knott, Jon Oberlander, and Mick O&apos;Donnell. 1998a. Experiments using stochastic search for text planning. In Proceedings of the 9th International Workshop on Natural Language Generation, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>lionnell</author>
</authors>
<title>An architecture for opportunistic text generation.</title>
<date>1998</date>
<booktitle>In Proceedings of the 9th International Workshop on Natural Language Generation,</booktitle>
<location>Ontario, Canada.</location>
<marker>lionnell, 1998</marker>
<rawString>Chris,Mellish,, Mick 0 lionnell,, J on Oberlander, and Alistair Knott. 1998b. An architecture for opportunistic text generation. In Proceedings of the 9th International Workshop on Natural Language Generation, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Meteer</author>
</authors>
<title>Expressibility and The Problem of Efficient Text Planning. Communication in Artificial Intelligence.</title>
<date>1992</date>
<publisher>Pinter Publishers Limited,</publisher>
<location>London.</location>
<marker>Meteer, 1992</marker>
<rawString>Marie Meteer. 1992. Expressibility and The Problem of Efficient Text Planning. Communication in Artificial Intelligence. Pinter Publishers Limited, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Megan Moser</author>
<author>Johanna Moore</author>
</authors>
<title>Toward a synthesis of two accounts of discourse structure.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<pages>22--3</pages>
<marker>Moser, Moore, 1996</marker>
<rawString>Megan Moser and Johanna Moore. 1996. Toward a synthesis of two accounts of discourse structure. Computational Linguistics, 22(3):409-419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Oberlander</author>
<author>Mick O&apos;Donnell</author>
<author>Ali Knott</author>
<author>Chris Mellish</author>
</authors>
<title>Conversation in the museum: Experiments in dynamic hypermedia with the intelligent labelling explorer. New Review of Hypermedia and Multimedia,</title>
<date>1998</date>
<pages>4--11</pages>
<contexts>
<context position="15506" citStr="Oberlander et al., 1998" startWordPosition="2502" endWordPosition="2505">ch could be decomposed in a structured way into subgoals. The main goal is to provide interesting information about the target object. There are generally only a small number of relations, mainly object-attribute elaboration and joint. For such a genre, a domain-dependent bottom-up planner (Marcu, 1997) or opportunistic planner (Mellish et al., 1998b) suits better than a domainindependent top-down planner. In these architectures, aggregation is important to text planning because it changes the order in which information is expressed. The first implementation we will describe is based on ILEX (Oberlander et al., 1998). ILEX is an adaptive hypertext generation system, providing natural language descriptions for museum objects. The bottom-up text planning is fulfilled in two steps: a content selection procedure, where a set of fact nodes with high relevance is selected from the Content Potential (following a search algorithm), and a content structuring procedure, where selected facts are reorganised to form entity-chains (based on the theory of entity-based coherence), which represent a coherent text arrangement. To make it possible for the ILEX planner to take into account aggregation, we use a revised vers</context>
</contexts>
<marker>Oberlander, O&apos;Donnell, Knott, Mellish, 1998</marker>
<rawString>Jon Oberlander, Mick O&apos;Donnell, Ali Knott, and Chris Mellish. 1998. Conversation in the museum: Experiments in dynamic hypermedia with the intelligent labelling explorer. New Review of Hypermedia and Multimedia, 4:11-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jon Oberlander</author>
<author>Alistair Knott</author>
<author>Mick O&apos;Donnell</author>
<author>Chris Mellish</author>
</authors>
<title>Beyond elaboration: Generating descriptive texts containing it-clefts. In</title>
<date>1999</date>
<editor>T Sanders, J Schilperoord, and W Spooren, editors, Text</editor>
<location>Benjamins, Amsterdam.</location>
<contexts>
<context position="2186" citStr="Oberlander et al. (1999)" startWordPosition="334" endWordPosition="337">lobal coherence between this segment and other discourse segments. Discourse segments are connected by either a dominance relation or a satisfaction-precedence relation. There has been an effort to synthesise the two accounts of discourse structnre. \ loser and Moore (199(i) argue that the two theories have considerable common ground, which lies in the correspondence between the notion of dominance and nuclearity. It is possible to map between Grosz and Sidner&apos;s linguistic structure and RST text structure, and relation-based coherence and global coherence capture similar discourse properties. Oberlander et al. (1999) propose a distinction between two types of discourse coherence: proposition-based coherence, which exists between text spans connected by RST relations except for object-attribute elaboration, and entity-based coherence, which exists between spans of text in virtue of shared entities. entity-based coherence captures the coherence among adjacent propositions, which resembles local coherence in Grosz and Sidner&apos;s theory. To generate a coherent text, the text planning process must try to achieve both local (entitybased) and global (relation-based) coherence. Since the task of aggregation is to c</context>
</contexts>
<marker>Oberlander, Knott, O&apos;Donnell, Mellish, 1999</marker>
<rawString>Jon Oberlander, Alistair Knott, Mick O&apos;Donnell, and Chris Mellish. 1999, Beyond elaboration: Generating descriptive texts containing it-clefts. In T Sanders, J Schilperoord, and W Spooren, editors, Text Representation: Linguistic and Psycholinguistic Aspects. Benjamins, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franck Panaget</author>
</authors>
<title>Micro-planning: A unified representation of lexical and grammatical resources.</title>
<date>1997</date>
<booktitle>In Proceeding of the 6th European Workshop on Natural Language Generation,</booktitle>
<pages>97--106</pages>
<contexts>
<context position="16167" citStr="Panaget, 1997" startWordPosition="2604" endWordPosition="2605">em, providing natural language descriptions for museum objects. The bottom-up text planning is fulfilled in two steps: a content selection procedure, where a set of fact nodes with high relevance is selected from the Content Potential (following a search algorithm), and a content structuring procedure, where selected facts are reorganised to form entity-chains (based on the theory of entity-based coherence), which represent a coherent text arrangement. To make it possible for the ILEX planner to take into account aggregation, we use a revised version of Nleteer&apos;s Text Structure (Meteer. 1992; Panaget, 1997) as the intermediate level of representation between text planning and sentence realisation to provide abstract syntactic constraints to the planning. We call this *-stem ILEX-TS (ILEX based on Text Structure). 189 In ILEX-TS, abstract referring expression determination and.aggregation are performed during text structuring. For each fact whose Text Structure is being built, if an NP in the fact can take modifiers, the embedding process will find a list of elaboration facts to the referent and make embedding decisions based on the constraints imposed by the NP form. The decisions include what t</context>
</contexts>
<marker>Panaget, 1997</marker>
<rawString>Franck Panaget. 1997. Micro-planning: A unified representation of lexical and grammatical resources. In Proceeding of the 6th European Workshop on Natural Language Generation, pages 97-106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Massimo Poesio</author>
<author>Renata Vieira</author>
<author>Simone Teufel</author>
</authors>
<title>Resolving bridging references in Imrestricted text.</title>
<date>1997</date>
<institution>Centre for Cognitive Science, University of Edinburgh.</institution>
<note>Research paper hcrc-rp87,</note>
<contexts>
<context position="12616" citStr="Poesio et al., 1997" startWordPosition="2023" endWordPosition="2026">nce a semantic relation is always preferred, we have the following heuristic: Heuristic 3 Preferences among semantic relations and center transitions: a semantic relation &gt; Joint Continuation 188 2.4 Preferences for embedding We distinguish between a-good-,-normal&apos;and-bad embedding based on the features it bears. We do not claim that the set of features is complete. In a different context, more criteria might have to be considered. A good embedding is one satisfying all the following conditions: 1. The referring part is an indefinite, a. demonstrative or a bridging description (as defined in (Poesio et al., 1997)). 2. The embedded part can be realised as an adjective or a prepositional phrase (Scott and de Souza, 1990). 3. In the resulting text, the embedded part does not lie between text spans connected by semantic parataxis and hypotaxis. 4. There is an available syntactic slot to hold the embedded part. A good embedding is highly preferred and should be performed whenever possible. A normal embedding is one satisfying condition 1, 3 and 4 and the embedded part is a relative clause which provides additional information about the referent. Bad embeddings are all those left, for example, if there is n</context>
</contexts>
<marker>Poesio, Vieira, Teufel, 1997</marker>
<rawString>Massimo Poesio, Renata Vieira., and Simone Teufel. 1997. Resolving bridging references in Imrestricted text. Research paper hcrc-rp87, Centre for Cognitive Science, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Reape</author>
<author>Chris Mellish</author>
</authors>
<title>Just what is aggregation anyway?</title>
<date>1999</date>
<booktitle>In Proceedings of the 7t0 European Workshop on Natural Language Generation,</booktitle>
<pages>20--29</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="3282" citStr="Reape and Mellish, 1999" startWordPosition="510" endWordPosition="513">rocess must try to achieve both local (entitybased) and global (relation-based) coherence. Since the task of aggregation is to combine simple representations together to form a complex one, which in the mean time leads to a shorter text as a whole, aggregation could affect the ordering of text plans and the length of the whole text. Therefore, it is closely related to the task of maintaining both types of coherence. Here we treat embedding as a type of aggregation. There is no consensus as to where aggregation should happen or how it. is related to other generation processes (Wilkinson, 1995; Reape and Mellish, 1999). In many NLG systems, aggregation is a post planning process whose preferences are only partially taken into account by the text planner. 1.1 Aggregation and local coherence In a structured text plan produced by the text planner, local coherence is normally maintained t hrough the ordering of the selected facts, where 186 certain types of center transition (e.g. center continuation) .are :preferred -over-others (e.g.. center shifting) (Centering Theory (Grosz et al., 1995)). Aggregation may affect text planning by taking away facts from a sequence featuring preferred center movements for embe</context>
</contexts>
<marker>Reape, Mellish, 1999</marker>
<rawString>Michael Reape and Chris Mellish. 1999. Just what is aggregation anyway? In Proceedings of the 7t0 European Workshop on Natural Language Generation, pages 20-29, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Schank</author>
</authors>
<title>Rules and topics in conversation.</title>
<date>1977</date>
<journal>Cognitive Science,</journal>
<pages>101--421</pages>
<contexts>
<context position="10878" citStr="Schank, 1977" startWordPosition="1742" endWordPosition="1743">ances. They argue that the order of coherence among the transitions is continuing &gt; retaining &gt; smooth shifting &gt; abrupt shifting. Instead of claiming that these are the best models, we use them simply as an example of linguistic models being used for evaluating features of text planning. A type of center transition that appears frequently in descriptive text is that the description starts with an object, but shifts to associated objects or perspectives of that object_ This is a type of abrupt shifting, but it is appropriate as long as the objects are highly associated to the original object (Schank, 1977). This phenomenon is handled in the system of (Grosz, 1977), where subparts of an object are included into a focus space as the implicit foci when the object itself is to be included. We call this center movement an associate shifting, where the center moves from a trigger entity to a closely associated entity. Our informal observation from museum descriptions shows that associate shifting is preferred by human writers to all other types of center movements except for continuation. There are two types of associate shifting: where the trigger is in the previous utterance or two entities in two </context>
</contexts>
<marker>Schank, 1977</marker>
<rawString>Roger Schank. 1977. Rules and topics in conversation. Cognitive Science, 101:421-441.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donia Scott</author>
<author>Clarisse Sieckenius de Souza</author>
</authors>
<date>1990</date>
<marker>Scott, de Souza, 1990</marker>
<rawString>Donia Scott and Clarisse Sieckenius de Souza. 1990.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Getting</author>
</authors>
<title>the&apos; messageacross in rst-based text generation.</title>
<booktitle>Current Research in Natural Language Cr&apos;encration,</booktitle>
<pages>47--73</pages>
<editor>In R. Dale, C. Mellish, and M. Zook, editors.</editor>
<publisher>Academic Press_</publisher>
<marker>Getting, </marker>
<rawString>• Getting the&apos; messageacross in rst-based text generation. In R. Dale, C. Mellish, and M. Zook, editors. Current Research in Natural Language Cr&apos;encration, pages 47--73. Academic Press_</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Wilkinson</author>
</authors>
<title>Aggregation in Natural Language Generation: Another Look.</title>
<date>1995</date>
<tech>Technical report,</tech>
<institution>Computer Science Department. University of Waterloo.</institution>
<contexts>
<context position="3256" citStr="Wilkinson, 1995" startWordPosition="508" endWordPosition="509">e text planning process must try to achieve both local (entitybased) and global (relation-based) coherence. Since the task of aggregation is to combine simple representations together to form a complex one, which in the mean time leads to a shorter text as a whole, aggregation could affect the ordering of text plans and the length of the whole text. Therefore, it is closely related to the task of maintaining both types of coherence. Here we treat embedding as a type of aggregation. There is no consensus as to where aggregation should happen or how it. is related to other generation processes (Wilkinson, 1995; Reape and Mellish, 1999). In many NLG systems, aggregation is a post planning process whose preferences are only partially taken into account by the text planner. 1.1 Aggregation and local coherence In a structured text plan produced by the text planner, local coherence is normally maintained t hrough the ordering of the selected facts, where 186 certain types of center transition (e.g. center continuation) .are :preferred -over-others (e.g.. center shifting) (Centering Theory (Grosz et al., 1995)). Aggregation may affect text planning by taking away facts from a sequence featuring preferred</context>
</contexts>
<marker>Wilkinson, 1995</marker>
<rawString>John Wilkinson. 1995. Aggregation in Natural Language Generation: Another Look. Technical report, Computer Science Department. University of Waterloo.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>