<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000135">
<title confidence="0.951447">
Sense-Tagging Chinese Corpus
</title>
<author confidence="0.851768">
Hsin-Hsi Chen
</author>
<affiliation confidence="0.7903315">
Department of Computer Science and
Information Engineering
National Taiwan University
Taipei, TAIWAN
</affiliation>
<email confidence="0.940577">
hh_chen@csie.ntu.edu.tw
</email>
<sectionHeader confidence="0.91438" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999802833333333">
Contextual information and the mapping
from WordNet synsets to Cilin sense tags
deal with word sense disambiguation. The
average performance is 63.36% when small
categories are used, and 1, 2 and 3
candidates are proposed for low, middle and
high ambiguous words. The performance
of tagging unknown words is 34.35%, which
is much better than that of baseline mode.
The sense tagger achieves the performance
of 76.04%, when unambiguous, ambiguous,
and unknown words are tagged.
</bodyText>
<sectionHeader confidence="0.905212" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.97476162962963">
Tagging task, which adds lexical, syntactic or
semantic information to raw text, makes
materials more valuable. The researches on
part of speech (POS) tagging have been a long
history, and achieve very good results. Many
POS-tagged corpora are available. The
accuracy for POS-tagging is in the range of 95%
to 97%1. In contrast, although the researches
on word sense disambiguation (WSD) are also
very early (Kelly and Stone, 1975), large-scale
sense-tagged corpus is relatively few. In
English, only some sense-tagged corpora such as
HECTOR (Atkins, 1993), DSO (Ng and Lee,
1996), SEMCOR (Fellbaum, 1997), and
SENSEVAL (ICilgarriff, 1998) are available.
For evaluating word sense disambiguation
systems, the first SENSEVAL (Kilgarriff and
Rosenzweig, 2000) reports that the performance
for a fine-grained word sense disambiguation
task is at around 75%.
1 The performance includes tagging unambiguous
words. Marshall (1987) reported that the
performance of CLAWS tagger is 94%.
Approximately 65% of words were tagged
unambiguously, and the disambiguation program
achieved better than 80% success on the ambiguous
words.
</bodyText>
<subsectionHeader confidence="0.463121">
Chi-Ching Lin
</subsectionHeader>
<bodyText confidence="0.954147022222223">
Department of Computer Science and
Information Engineering
National Taiwan University
Taipei, TAIWAN
cclin@n1g2.csie.ntu.edu.tw
Tagging accuracy depends on several issues
(Manning and Schutze, 1999), e.g., the amount
of training data, the granularity of the tagging set,
the occurrences of unknown words, and so on.
Three approaches have been proposed for WSD,
including dictionary/thesaurus-based approach,
supervised learning, and unsupervised learning.
The major differences are what kinds of
resources are used, i.e., dictionary versus text
corpus, and sense-tagged corpus versus
untagged corpus. A good survey refers to the
paper (Ide and Veronis, 1998). Compared with
English, Chinese does not have large-scale
sense-tagged corpus. The widely available
corpus is Academic Sinica Balanced Corpus
abbreviated as ASBC hereafter (Huang and
Chen, 1995), which is a POS-tagged corpus.
Thus, a computer-aided tool to sense-tag
Chinese corpus is indispensable.
This paper presents a sense tagger for
Mandarin Chinese. It is organized as follows.
Section 2 discusses the degree of polysemy in
Mandarin Chinese from several viewpoints.
Section 3 presents WSD algorithms for tagging
ambiguous words and unknown words.
Section 4 shows our experimental results.
Finally, Section 5 concludes the remarks.
2 Degree of Polysemy in Mandarin Chinese
The degree of polysemy is defined as the
average number of senses of words. We adopt
tagging set from tong2yi4ci2ci2lin2 (]t111
*) abbreviated as Cilin (Mei, et al., 1982). It
is composed of 12 large categories, 94 middle
categories, and 1,428 small categories.
Small categories (more fine granularity) are
used to compute the distribution of word senses.
Besides Cilin, ASBC is employed to count
frequency of a word. Total 28,321 word types
appear both in Cilin and in ASBC corpus.
Here a word type corresponds to a dictionary
</bodyText>
<page confidence="0.999876">
7
</page>
<tableCaption confidence="0.999899">
Table 1. The Distribution of Word Senses
</tableCaption>
<table confidence="0.999655818181818">
Low Ambiguity Middle Ambiguity Degree High Ambiguity Degree #Word Types
Degree #Word Types Mord Types
Degree #Word Types
2 4261 (71.95%) 5 186 (3.14%) 9 14 (0.24%) 14 1(0.02%)
3 948 (16.01%) 6 77 (1.30%) 10 8 (0.14%) 15 1(0.02%)
42 (0.71%) 11 3 (0.05%) 17 1 (0.02%)
4 - 344 (5.81%) 7
8 25 (0.42%) 12 4 (0.07%) 18 1(0.02%)
13 5 (0.08%) 20 1 (0.02%)
Sum 5553 (93.77%) Sum 330 (5.57%) Sum 39 (0.66%)
Total Word Types 5922
</table>
<tableCaption confidence="0.999878">
Table 2. The Distribution of Word Senses with Consideration of POS
</tableCaption>
<figure confidence="0.9874008125">
------..........1„..30t.„ N V A F K
Degree
Low 2 1441 (81.05%) 1056 (71.79%) 580 (79.67%) 14 (77.78%) 101 (73.72%)
3 238 (13.39%) 238 (16.18%) 115 (15.80%) 4 (22.22%) 25 (18.25%)
4 55 (3.09%) 99 (6.73%) 20 (2.75%) 7 (5.11%)
Middle 5 26 (1.46%) 41(2.79%) 9 (1.24%) 3 (2.19%)
6 12 (0.67%) 13 (0.88%) 2 (0.27%) 1 (0.73%)
7 3 (0.17%) 13 (0.88%) 2 (0.27%)
8 2(0.11%) 6(0.40%) ,
High 9 1 (0.06%) 1 (0.07%)
11 1 (0.07%)
12 1 (0.07%)
13 1 (0.07%)
19 1 (0.07%)
Total Word 1778 1471 728 18 137
Types
</figure>
<bodyText confidence="0.998024918918919">
entry. Of these, 5,922 words are polysemous,
i.e., they have more than one sense. Table 1
lists the statistics. We divide the ambiguity
degree into three levels according to the number
of senses of a word. It includes low (2-4),
middle (5-8), and high ambiguity (&gt;8). The
statistics shows that 93.77% of word types
belong to the class of low ambiguity.
We further consider POS when computing
the distribution of word senses. Table 2 shows
the statistics. N, V, A, F, and K denote nouns,
verbs, adjectives, numerals, and auxiliaries
(adverbs), respectively. We can find most of
words belong to the class of low ambiguity no
matter which POSes they are. Besides, the
ambiguity is decreased when POS is considered.
The number of polysemous words is down to
4,132. For A and K, the number of senses is
no more than 7, and the percentages in the class
of low degrees are 98.22% and 97.08%,
respectively. For N and V, there are some high
ambiguous words. In particular, the verb (47,
da3) has 19 senses2. The percentages in the
class of low degrees are 97.53% and 94.70%,
respectively.
Then, the frequency of word types is
considered. ASBC corpus is used to compute
the occurrences of word types. Table 3 lists
the statistics. A word token is an occurrence of
a type in the corpus. On the average, the words
of low, middle and high ambiguity appear
205.96, 1926.65, and 4480.28 times,
respectively. Table 1 shows 93.77% of
polysemous words belong to the class of low
ambiguity, but Table 3 illustrates they only
2 The word (#T, da3) has 20 senses. Besides verb
usage, it also functions as an auxiliary.
</bodyText>
<page confidence="0.999024">
8
</page>
<tableCaption confidence="0.999394">
Table 3. The Distribution of Word Senses with Consideration of Frequencies
</tableCaption>
<table confidence="0.9994386">
Low Ambiguity Middle Ambiguity High Ambiguity
Types Tokens #Tokens/ Types Tokens #Tokens/ Types Tokens #Tokens/
#Types #Types #TYPes
5553 1143686 205.96 330 635796 1926.65 39 174731 4480.28
93.77% 58.52% 5.57% 32.53% 0.66% 8.94%
</table>
<tableCaption confidence="0.999159">
Table 4. The Distribution of Word Senses and Fre uencies with Consideration of POS
</tableCaption>
<table confidence="0.9992848125">
Frequency Low Middle High Sum Percentage
Ambiguity 96.64%;
Low Types (C) 3112 734 147 3993
Tokens (A) 70131 230955 735819 1036905 85.52%
A/C 22.54 314.65 5005.57 259.68
Middle Types (C) 42 62 29 133 3.22%
Tokens (A) 1905 14667 153307 169879 14.01%
A/C 45.36 236.56 5286.45 1277.29
High Types (C) 0 2 4 6 0.15°A,&apos;
Tokens (A) 0- 843 4847 5690 0.47%
A/C 0 421.5 1211.75 948.33
Sum Types (C) 3154 798 180 4132
Tokens (A) 72036 246465 893973 1212474
A/C 22.84 308.85 4966.52
% Types (C) 76.33°/0- 19.31% 4.36%
Tokens (A) 5.94% 20.33% 73.73%
</table>
<bodyText confidence="0.988306357142857">
occupy 58.52% of tokens in ASBC corpus.
Table 4 summarizes the distribution of
word senses and frequencies. Low frequency
denotes the number of occurrences less than 100,
middle frequency denotes the number of
occurrences between 100 and 1000, and high
frequency denotes the number of occurrences
more than 1000. Rows C and A in Table 4
denote number of word types and word tokens,
respectively. The last column denotes
percentage for each ambiguity degree. For
example, the percentage of word types with low
ambiguity is 96.64% (i.e., 3993/4132). This
table shows the following two phenomena:
</bodyText>
<listItem confidence="0.969203">
(1) POS information reduces the degree of
ambiguities. Total 8.94% of word tokens are
high ambiguous in Table 3. It decreases to
0.47% in Table 4.
(2) High ambiguous words tend to be high
frequent. From the row of low ambiguity,
there are 3,112 low-frequent words. They
</listItem>
<bodyText confidence="0.9991487">
occur 70,131 times in ASBC corpus.
Comparatively, there are only 881 middle- or
high-frequent words, but they occur 966,774
times. That is, 23.67% of word types are
middle- or high-frequent words, and they
occupy 94.06% of word tokens. From the row
of high ambiguity, there are only a few words,
but they occur frequently in the ASBC corpus.
It shows that semantic tagging is a challengeable
problem in Mandarin Chinese.
</bodyText>
<sectionHeader confidence="0.981654" genericHeader="method">
3 Semantic Tagging
</sectionHeader>
<subsectionHeader confidence="0.999335">
3.1 Tagging Unambiguous Words
</subsectionHeader>
<bodyText confidence="0.999916">
In the semantic tagging, the small categories are
selected. We postulate that the sense definition
for each word in Cilin is complete. That is, a
word that has only one sense in Cilin is called an
unambiguous word or a monosemous word. If
POS information is also considered, a word may
be unambiguous under a specific POS.
Because we do not have a semantically tagged
corpus for training, we try to acquire the context
</bodyText>
<page confidence="0.98953">
9
</page>
<bodyText confidence="0.999800333333333">
for each semantic tag starting from the
unambiguous words.
ASBC corpus is the target we study. At
the first stage, only those words that are
unambiguous in Cilin, and also appear in ASBC
corpus are tagged. Figure 1 shows this case.
</bodyText>
<subsectionHeader confidence="0.61071">
Unambiguous Words
</subsectionHeader>
<figureCaption confidence="0.99435">
Figure 1. Tagging Unambiguous Words
</figureCaption>
<bodyText confidence="0.996801833333333">
An unambiguous word (and hence its sense
tag) is characterized by the words surrounding it.
The window size is set to 6, and stop words are
removed. A list of stop words is trained from
ASBC corpus. The words of POSes Neu (4fc
119), DE (0 4 3,6), SHI (k), FW
ate), C (1.4141), T (09), and I (Alm)
are regarded as stop words. A sense tag Ctag
is in terms of a vector (wl, w2, wn), where n
is the vocabulary size and wi is a weight of word
cw. The weight can be determined by the
following two ways.
</bodyText>
<listItem confidence="0.982246">
(1) MI metric (Church, et al., 1989)
MI (Ctag ,cw).
</listItem>
<equation confidence="0.9795565">
P (Ctag , cw)
log 2 P(Ctag)P(cw)—
f (Ctag , cw)
log 2 f (Ctag) (ew)x N
</equation>
<bodyText confidence="0.9517387">
where P(Ctag) is the probability of Ctag,
P(cw) is the probability of cw,
P(Ctag, cw) is the cooccurrence
probability of Ctag and cw,
J(Crag) is the frequency of Ctag,
J(cw) is the frequency of cw,
f(Ctag, cw) is the cooccurrence
frequency of Ctag and cw, and
Nis total number of words in the
corpus.
</bodyText>
<listItem confidence="0.840067">
(2) EM metric (Ballesteros and Croft, 1998)
em (Crag, cw) =
</listItem>
<construct confidence="0.436733333333333">
(f(Ctag, cw) — En (Ctag, cw) 0)
f (Ctag ) f (cw)
En (Ctag , cw)=
</construct>
<subsectionHeader confidence="0.994272">
3.2 Tagging Ambiguous Words
</subsectionHeader>
<bodyText confidence="0.989244">
At the second stage, we deal with those words
that have more than one sense in the Cilin.
Figure 2 shows the words we consider.
</bodyText>
<figure confidence="0.844981666666667">
Unambiguous Words
ASBC
Ambiguous Words
</figure>
<figureCaption confidence="0.999533">
Figure 2. Tagging Ambiguous Words
</figureCaption>
<bodyText confidence="0.9853565">
The approach we adopted on semantic
tagging rests on an underlying assumption: each
sense has a characteristic context that is
different from the context of all the other senses.
In addition, all words expressing the same sense
share the same characteristic context. We will
apply the information trained at the first stage to
selecting the best sense tag from the candidates
of each ambiguous word. Recall that a vector
corresponds to a sense tag. We employ the
similar way specified in Section 3.1 to identify
the context vector of an ambiguous word. A
cosine formula shown as follows measures the
similarity between a sense vector and a context
vector, where w and v are a sense vector and a
context vector, respectively. The sense tag of
the highest similarity score is chosen.
cos (W , V ) =
We retrain the sense vector for each sense tag
after the unambiguous words are resolved.
</bodyText>
<subsectionHeader confidence="0.998929">
3.3 Tagging Unknown Words
</subsectionHeader>
<bodyText confidence="0.999528">
Those words that appear in ASBC corpus, but
are not gathered in Cilin are called unknown
words. All the 1,428 sense tags are the
possible candidates. Intuitively, the algorithm
in Section 3.2 can be applied directly to select a
sense tag from the 1,428 candidates. However,
the candidate set is very large. Here we adopt
outside evidences from the mapping among
WordNet synsets (Fellbaum, 1998) and Cilin
</bodyText>
<figure confidence="0.882076444444444">
max
f(Ctag) f(cw)
W • V
lw I lv I
10
ewl syni c=&gt; Candidate
sYni List
sYn21 Ctagi
syn22 i=&gt; Ctag2
sYn23 Ctag3
Mappine Table
among
WordNet
synsets and
Cilin tags
ew2
Cw
E:=&gt;
</figure>
<figureCaption confidence="0.999913">
Figure 3. Flow of Semantic Tagging
</figureCaption>
<bodyText confidence="0.929636666666667">
sense tags to narrow down the candidate set.
Figure 3 summarizes the flow of our algorithm.
It is illustrated as follows.
</bodyText>
<listItem confidence="0.9482251875">
(1) Find all the English translations of an
unknown Chinese word by looking up a
Chinese-English dictionary.
(2) Find all the synsets of the English
translations by looking up WordNet We do
not resolve translation ambiguity and target
polysemy at these two steps, thus the retrieved
synsets may cover more senses than that of the
original Chinese word.
(3) Transform the synsets back to Cilin sense
tags by looking up a mapping table. How the
mapping table is set up will be discussed in
Section 3.3.1.
(4) Select a sense tag from the candidates
proposed at step (3) by using the WSD in
Section 3.2.
</listItem>
<bodyText confidence="0.969921">
Figure 4 shows the unknown words we deal
with at this stage. Those words that are not
gathered in our Chinese-English dictionary are
not considered, so that only parts of unknown
words are resolved. In other words, there
remain words without sense tags.
Unambiguous Words
</bodyText>
<subsubsectionHeader confidence="0.905258">
3.3.1 Mapping SynSets to Cilin Sense Tags
</subsubsectionHeader>
<bodyText confidence="0.9503989">
At first, we put unambiguous words (specified
in Section 3.1) into WordNet by looking up a
Chinese-English dictionary. Although these
words do not have translation ambiguity, the
corresponding English translation may have
target polysemy problem. In other words, the
English translation may cover irrelevant senses
besides the correct one. The following
algorithm will find the most similar synset with
Chinese sense tag.
</bodyText>
<listItem confidence="0.997424434782609">
(1) If the English translation corresponds to
only one synset, this synset is the solution.
(2) If the English translation corresponds to
more than one synset, POS is considered:
(a) If the Chinese sense tag belongs to one of
categories A-D in Cilin (i.e., a noun sense),
and there is only one noun synset, then the
synset is adopted. Otherwise, we translate
the context vector of the Chinese sense into
English, compare it with vectors of the
synsets, and select the most similar synset
(b) If the Chinese sense tag belongs to one of
categories FJ&amp;quot; in Cilin (i.e., a verb sense),
we try to find a verb synset in the similar
way as (a). If it fails, we try noun and
adjective synsets instead.
(c) If the Chinese sense tag belongs to category
E in Cilin (i.e., an adjective sense), we try
adjective, adverb, noun and verb synsets in
sequence.
(d) If the Chinese sense tag belongs to category
K in Cilin (i.e., an adverb sense), only
adverb synsets are considered.
</listItem>
<bodyText confidence="0.993146666666667">
Next, we consider the ambiguous words.
Chinese-English dictionary lookup finds all the
English translations. WordNet search collects
</bodyText>
<figure confidence="0.965819">
ASBC
Unknown
Words Ambiguous Words
</figure>
<figureCaption confidence="0.990357">
Figure 4. Tagging Unknown Words
</figureCaption>
<page confidence="0.996915">
11
</page>
<bodyText confidence="0.9991828">
the synset candidates for the translations.
Some synsets are selected and regarded as the
mapping of the Cilin sense tag. Here the
problems of translation ambiguity and target
polysemy must be faced. In other words, not
all English translations cover the Chin sense.
Because the goal is to find a mapping table
between WordNet synsets and Cilin sense tags,
we neglect the problem of translation ambiguity
and follow the method in the previous paragraph
to choose the most similar synsets.
During mapping, English translations of a
word may not be found in the Chinese-English
dictionary, and WordNet may not gather the
English translations even dictionary look-up is
successful. Thus, only 1,328 of 1,428 Chin
ta&amp;s are mapped to WordNet synsets. From the
other view, there remains some WordNet
synsets that do not correspond to any Cilin sense
tags. Let such a synset be Si. We follow the
relational pointers like hypemym, hyponym,
similar, derived, antonym, or participle to
collect the neighboring synsets denoted by Si.
The following method selects suitable Chin
tag(s) for Si.
</bodyText>
<listItem confidence="0.86324325">
(1) If Si is the only one synset that has been
mapped to Cilin tags, we choose a Chin
tag and map Si to it.
(2) If there exists more than one S; (say, Sp,
</listItem>
<bodyText confidence="0.999705909090909">
Sj2, Sjn) that has been mapped to
Cilin tags, we choose the Cilin tags that
more synsets map to.
The above method is called a more restrictive
scheme. An alternative method (called less
restrictive method) is: all the Cilin tags that the
neighboring synsets map to are selected. If
Cilin tags cannot be found from neighboring
synsets, we extend the range one more, and
repeat the selection procedure again until all the
synsets are considered.
</bodyText>
<sectionHeader confidence="0.998199" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998176">
4.1 Test Materials
</subsectionHeader>
<bodyText confidence="0.999980555555556">
We sample documents of different categories
from ASBC corpus, including philosophy (10%),
science (10%), society (35%), art (5%), life
(20%) and literary (20%). There are 35,921
words in the test corpus. Research associates
tag this corpus manually. At first, they mark
up the ambiguous words by looking up the Chin
dictionary. Next, they tag the unknown words.
A list of candidates is proposed by looking up
the mapping table. Because the mapping table
may have errors, the annotators assign a tag
&amp;quot;none&amp;quot; when they cannot choose a solution from
the proposed candidates. Total 435 of 1,979
words are tagged with &amp;quot;none&amp;quot; with the more
restrictive method. In contrast, only 346 words
are labeled with &amp;quot;none&amp;quot; with the less restrictive
method. The tag mapper achieves 82.52% of
performance approximately.
</bodyText>
<subsectionHeader confidence="0.99878">
4.2 Tagging Ambiguous Words
</subsectionHeader>
<bodyText confidence="0.999944621621622">
Table 5 shows the performance of tagging
ambiguous words. MI defined in Section 3.1 is
used. Total 11,101 words are tagged. The
performance of tagging low, middle, and high
ambiguous words are 62.60%, 31.36%, and
27.00%, respectively. Table 6 shows that the
performance is improved, in particular, the
classes of middle- and high- ambiguity, when
EM (defined in Section 3.1) is used. The
overall performance is increased from 49.55%
to 52.85%.
In the previous experiments, only one sense is
reported for each word. If we report more than
one sense for middle and high ambiguous words,
the performance is improved. Table 7 shows
that the first 2 and 3 candidates are selected.
From the diagonal of this table, the performance
for tagging low ambiguity (2-4), middle
ambiguity (5-8) and high ambiguity (&gt;8) is
similar (i.e., 63.98%, 60.92% and 67.95%) when
1 candidate, 2 candidates, and 3 candidates are
proposed, respectively. In this case, 7,034 of
11,101 words are tagged correctly. That is, the
performance is 63.36%.
In the next experiment, we adopt middle
categories (i.e., 94 categories) rather than the
above small categories (i.e., 1428 categories).
Table 8 shows that the overall performance is
improved by 11.05%. It also lists the results
with the combinations of first-n and middle
categories. Under the middle categories and
1-3 proposed candidates, the performance for
tagging low, middle and high ambiguous words
are 71.02%, 73.88%, and 75.94%, respectively.
Total 8,033 of 11,101 words are tagged
correctly. In other words, the performance is
72.36%.
</bodyText>
<page confidence="0.998964">
12
</page>
<tableCaption confidence="0.999534">
Table 5. Performance of Tagging Ambiguous Words using MI
</tableCaption>
<table confidence="0.997431">
guitY Low Middle High Summary
Word Tokens .
Total Tokens 6601 3511 989 11101
Correct Tokens 4132 1101 267 5500
Correct Rate 62.60% 31.36% 27.00% 49.55%
</table>
<tableCaption confidence="0.665718">
Table 6. Performance of Ta22in Ambiauous Words using EM
</tableCaption>
<table confidence="0.9995992">
biguity Low Middle High Summary
Word Tokens
Total Tokens 6601 3511 989 11101
Correct Tokens 4223 1334 310 5867
Correct Rate 63.98% 37.99% 31.34% 52.85%
</table>
<tableCaption confidence="0.997085">
Table 7. Performance of Tagging using the First-n and EM
</tableCaption>
<table confidence="0.999322">
biguity Low Middle High Middle and High
First-n
1 63.98% 37.99% 31.34% 36.53%
2 60.92% 53.99% 59.40%
3 71.35% 67.95% 70.60%
</table>
<tableCaption confidence="0.996161">
Table 8. Performance of Tagging using First-n and Middle Categories
</tableCaption>
<table confidence="0.9996345">
Ambiguity Low Middle High Middle and High
First-n/Categon
1 Small 63.98% 37.99% 31.34% 36.53%
Middle 71.02% 56.19% 43.78% 53.47%
2 Small 60.92% 53.99% 59.40%
Middle 73.88% 65.72% 72.09%
3 Small 71.35% 67.95% 70.60%
Middle 79.27% 75.94% 78.53%
</table>
<subsectionHeader confidence="0.991262">
4.3 Tagging Unknown Words
</subsectionHeader>
<bodyText confidence="0.999975771428571">
There are 1,979 unknown words in our test
corpus. Total 1,663 words have been tagged
manually. In the experiments, we consider the
effects from training corpus and mapping table.
Table 9 shows the performance. M1 and P1
employ more restrictive mapping table, while
M2 and P2 adopt less restrictive mapping table.
M1 and M2 use the training result in Section 3.1
(i.e., unambiguous words), while P1 and P2
utilize the training result in Section 3.2 (i.e.,
unambiguous and ambiguous words). In the
baseline model, all 1428 Cilin tags are the
candidates of unknown words. The
performance is worse. On the average, the
precision is 1.22%. MI is the best because
more restrictive mapping table reduces the
possibility of mapping errors. This table also
lists the performance of each category. It
meets our expectation, i.e., tagging verb is
harder than tagging other categories. Next we
use POS to improve the performance. POS
narrows down the number of candidates, so that
the overall performance is enhanced from
27.13%% to 34.35%%.
In summary, we consider the overall
performance of tagging our sample data.
Recall that there are 35,921 words in the test
corpus. Except the stop words that are not
tagged by the sense tagger, there remain 13,586
unambiguous words, 11,101 ambiguous words,
and 1,633 unknown words for tagging. From
Tables 6 and 9, we know 5,867 unambiguous
words and 561 unknown words are tagged
correctly. The sense tagger achieves the
performance of 76.04%.
</bodyText>
<sectionHeader confidence="0.994179" genericHeader="conclusions">
5. Conclusion
</sectionHeader>
<bodyText confidence="0.9125176">
This paper analyzes the polysemy degree in
Mandarin Chinese. We consider the
distribution of word senses from POS and
frequency. Under the Cilin small categories,
23.67% of word types in ASBC corpus are
</bodyText>
<page confidence="0.999554">
13
</page>
<tableCaption confidence="0.999491">
Table 9. Performance of TaIn2 Unknown Words
</tableCaption>
<table confidence="0.999796">
Categories #Tokens Baseline M1 M2 P1 P2 Ml(POS)
All 1633 Correct 20 443 395 438 396 561
Precision 1.22% 27.13% 24.19% 26.82% 24.25% 34.35%
N 858 Correct 11 255 228 255 231 320
Precision 1.28% 29.72% 26.57% 29.72% 26.92% 37.30%
V 619 Correct 5 144 124 137 120 167
Precision 0.81% 23.26% 20.03% 22.13% 19.39% 26.98%
A 58 Correct 0 5 5 5 5 28
Precision 0 8.62% 8.62% 8.62% 8.62% 48.28%
F 4 Correct 1 1 1 1 1 4
Precision 25.00% 25.00% 25.00% 25.00% 25.00% 100.00%
K 94 Correct 3 38 37 40 39 42
Precision 3.19% 40.43% 39.36 42.55 41.49 44.68%
</table>
<bodyText confidence="0.99987505">
middle or high frequent words, but they occupy
94.06% of word tokens. We adopt contextual
information and mapping from WordNet synsets
to Cilin sense tags to deal with this
challengeable problem. The performances for
tagging low, middle and high ambiguous words
are 63.98%, 60.92%, and 67.95% when small
proposed. Comparatively, the performances
categories are used and 1-3 candidates are
71.02%, 73.88%, and 75.94% by using middle
categories. The performance of tagging
unknown words is 34.35%. It is worse than
that of tagging ambiguous words, but is much
better than that of the baseline mode. The
overall performance is the sense tagger is
76.04%. Although sense tagging does not
achieve the performance of POS tagging, the
sense tagger proposed in this paper is still a
useful computer-aided tool to reduce the human
cost on tagging a large-scale corpus.
</bodyText>
<sectionHeader confidence="0.99928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998478444444444">
Atkins, S. (1993) &amp;quot;Tools for Computer-Aided
Lexicography: the Hector Project,&amp;quot; Acta
Linguistica Hungarica, 41, pp. 5-72.
Ballesteros, L. and Croft, W.B. (1998) &amp;quot;Resolving
Ambiguity for Cross-Language Information
Retrieval,&amp;quot; Proceedings of the 21st Annual
International ACM SIGIR Conference, pp. 64-71.
Church, K.W., et al. (1989) &amp;quot;Parsing, Word
Associations and Typical Predicate-Argument
Relations.&amp;quot; Proceedings of International
Workshop on Parsing Technologies, pp. 389-398.
Huang C.R and Chen, K.J. (1995) &amp;quot;Academic
Sinica Balanced Corpus,&amp;quot; Technical Report
95-02/98-04, Academic Sinica, Taipei, Taiwan.
Fellbaum, C. editor (1998) WordNet: An Electronic
Lexical Database, MIT Press, Cambridge, Mass.
Ide, N. and Veronis, J. (1998) &amp;quot;Word Sense
Disambiguation: The State of Art,&amp;quot;
Computational Linguistics, 24(1), pp. 1-40.
Kelly, E. and Stone, P. (1975) Computer Recognition
of English Word Senses, North-Holland,
Amsterdam.
Kilgarriff, A. (1998) &amp;quot;SENSEVAL: An Exercise in
Evaluating Word Sense Disambiguation
Programs,&amp;quot; Proceedings of First International
Conference on Language Resources and
Evaluation, Granada, pp. 581-588.
Kilgarriff, A. and Rosenzweig, J. (2000) &amp;quot;English
SENSEVAL: Report and Results,&amp;quot; Proceedings
of Second International Conference on Language
Resources and Evaluation.
Manning, C.D. and Schulze, IL (1999) Foundations
of Statistical Natural Language Processing, MIT
Press, Cambridge, Mass.
Marshall, I. (1987) &amp;quot;Tag Selection using Probabilistic
Methods,&amp;quot; in Roger Garside, Geoffrey Leech and
Geoffrey Sampson (editors), The Computational
Analysis of English, Longman, pp. 42-56.
Mei, J.; et al. (1982) tong2yi4ci2ci2lin2. Shanghai
Dictionary Press.
Ng RT. and Lee, RB. (1996) &amp;quot;Integrating Multiple
Knowledge Sources to Disambiguate Word Sense:
An Exemplar-Based Approach,&amp;quot; Proceedings of
34th Annual Meeting of Association for
Computational Linguistics, pp. 40-47.
</reference>
<page confidence="0.999271">
14
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.894362">
<title confidence="0.999752">Sense-Tagging Chinese Corpus</title>
<author confidence="0.946606">Hsin-Hsi</author>
<affiliation confidence="0.997254">Department of Computer Science Information National Taiwan</affiliation>
<address confidence="0.99136">Taipei, TAIWAN</address>
<email confidence="0.970968">hh_chen@csie.ntu.edu.tw</email>
<abstract confidence="0.999087384615384">Contextual information and the mapping from WordNet synsets to Cilin sense tags deal with word sense disambiguation. The average performance is 63.36% when small categories are used, and 1, 2 and 3 candidates are proposed for low, middle and high ambiguous words. The performance of tagging unknown words is 34.35%, which is much better than that of baseline mode. The sense tagger achieves the performance of 76.04%, when unambiguous, ambiguous, and unknown words are tagged.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Atkins</author>
</authors>
<title>Tools for Computer-Aided Lexicography: the Hector Project,&amp;quot;</title>
<date>1993</date>
<journal>Acta Linguistica Hungarica,</journal>
<volume>41</volume>
<pages>5--72</pages>
<contexts>
<context position="1225" citStr="Atkins, 1993" startWordPosition="180" endWordPosition="181"> ambiguous, and unknown words are tagged. 1 Introduction Tagging task, which adds lexical, syntactic or semantic information to raw text, makes materials more valuable. The researches on part of speech (POS) tagging have been a long history, and achieve very good results. Many POS-tagged corpora are available. The accuracy for POS-tagging is in the range of 95% to 97%1. In contrast, although the researches on word sense disambiguation (WSD) are also very early (Kelly and Stone, 1975), large-scale sense-tagged corpus is relatively few. In English, only some sense-tagged corpora such as HECTOR (Atkins, 1993), DSO (Ng and Lee, 1996), SEMCOR (Fellbaum, 1997), and SENSEVAL (ICilgarriff, 1998) are available. For evaluating word sense disambiguation systems, the first SENSEVAL (Kilgarriff and Rosenzweig, 2000) reports that the performance for a fine-grained word sense disambiguation task is at around 75%. 1 The performance includes tagging unambiguous words. Marshall (1987) reported that the performance of CLAWS tagger is 94%. Approximately 65% of words were tagged unambiguously, and the disambiguation program achieved better than 80% success on the ambiguous words. Chi-Ching Lin Department of Compute</context>
</contexts>
<marker>Atkins, 1993</marker>
<rawString>Atkins, S. (1993) &amp;quot;Tools for Computer-Aided Lexicography: the Hector Project,&amp;quot; Acta Linguistica Hungarica, 41, pp. 5-72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ballesteros</author>
<author>W B Croft</author>
</authors>
<title>Resolving Ambiguity for Cross-Language Information Retrieval,&amp;quot;</title>
<date>1998</date>
<booktitle>Proceedings of the 21st Annual International ACM SIGIR Conference,</booktitle>
<pages>64--71</pages>
<contexts>
<context position="10217" citStr="Ballesteros and Croft, 1998" startWordPosition="1695" endWordPosition="1698">Ctag is in terms of a vector (wl, w2, wn), where n is the vocabulary size and wi is a weight of word cw. The weight can be determined by the following two ways. (1) MI metric (Church, et al., 1989) MI (Ctag ,cw). P (Ctag , cw) log 2 P(Ctag)P(cw)— f (Ctag , cw) log 2 f (Ctag) (ew)x N where P(Ctag) is the probability of Ctag, P(cw) is the probability of cw, P(Ctag, cw) is the cooccurrence probability of Ctag and cw, J(Crag) is the frequency of Ctag, J(cw) is the frequency of cw, f(Ctag, cw) is the cooccurrence frequency of Ctag and cw, and Nis total number of words in the corpus. (2) EM metric (Ballesteros and Croft, 1998) em (Crag, cw) = (f(Ctag, cw) — En (Ctag, cw) 0) f (Ctag ) f (cw) En (Ctag , cw)= 3.2 Tagging Ambiguous Words At the second stage, we deal with those words that have more than one sense in the Cilin. Figure 2 shows the words we consider. Unambiguous Words ASBC Ambiguous Words Figure 2. Tagging Ambiguous Words The approach we adopted on semantic tagging rests on an underlying assumption: each sense has a characteristic context that is different from the context of all the other senses. In addition, all words expressing the same sense share the same characteristic context. We will apply the info</context>
</contexts>
<marker>Ballesteros, Croft, 1998</marker>
<rawString>Ballesteros, L. and Croft, W.B. (1998) &amp;quot;Resolving Ambiguity for Cross-Language Information Retrieval,&amp;quot; Proceedings of the 21st Annual International ACM SIGIR Conference, pp. 64-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>Parsing, Word Associations and Typical Predicate-Argument Relations.&amp;quot;</title>
<date>1989</date>
<booktitle>Proceedings of International Workshop on Parsing Technologies,</booktitle>
<pages>389--398</pages>
<marker>Church, 1989</marker>
<rawString>Church, K.W., et al. (1989) &amp;quot;Parsing, Word Associations and Typical Predicate-Argument Relations.&amp;quot; Proceedings of International Workshop on Parsing Technologies, pp. 389-398.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C R Huang</author>
<author>K J Chen</author>
</authors>
<title>Academic Sinica Balanced Corpus,&amp;quot;</title>
<date>1995</date>
<tech>Technical Report 95-02/98-04,</tech>
<institution>Academic Sinica,</institution>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="2657" citStr="Huang and Chen, 1995" startWordPosition="377" endWordPosition="380">a, the granularity of the tagging set, the occurrences of unknown words, and so on. Three approaches have been proposed for WSD, including dictionary/thesaurus-based approach, supervised learning, and unsupervised learning. The major differences are what kinds of resources are used, i.e., dictionary versus text corpus, and sense-tagged corpus versus untagged corpus. A good survey refers to the paper (Ide and Veronis, 1998). Compared with English, Chinese does not have large-scale sense-tagged corpus. The widely available corpus is Academic Sinica Balanced Corpus abbreviated as ASBC hereafter (Huang and Chen, 1995), which is a POS-tagged corpus. Thus, a computer-aided tool to sense-tag Chinese corpus is indispensable. This paper presents a sense tagger for Mandarin Chinese. It is organized as follows. Section 2 discusses the degree of polysemy in Mandarin Chinese from several viewpoints. Section 3 presents WSD algorithms for tagging ambiguous words and unknown words. Section 4 shows our experimental results. Finally, Section 5 concludes the remarks. 2 Degree of Polysemy in Mandarin Chinese The degree of polysemy is defined as the average number of senses of words. We adopt tagging set from tong2yi4ci2ci</context>
</contexts>
<marker>Huang, Chen, 1995</marker>
<rawString>Huang C.R and Chen, K.J. (1995) &amp;quot;Academic Sinica Balanced Corpus,&amp;quot; Technical Report 95-02/98-04, Academic Sinica, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Lexical Database,</title>
<date>1998</date>
<editor>Fellbaum, C. editor</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>1998</marker>
<rawString>Fellbaum, C. editor (1998) WordNet: An Electronic Lexical Database, MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>J Veronis</author>
</authors>
<title>Word Sense Disambiguation: The State of Art,&amp;quot;</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>1--40</pages>
<contexts>
<context position="2462" citStr="Ide and Veronis, 1998" startWordPosition="350" endWordPosition="353">nformation Engineering National Taiwan University Taipei, TAIWAN cclin@n1g2.csie.ntu.edu.tw Tagging accuracy depends on several issues (Manning and Schutze, 1999), e.g., the amount of training data, the granularity of the tagging set, the occurrences of unknown words, and so on. Three approaches have been proposed for WSD, including dictionary/thesaurus-based approach, supervised learning, and unsupervised learning. The major differences are what kinds of resources are used, i.e., dictionary versus text corpus, and sense-tagged corpus versus untagged corpus. A good survey refers to the paper (Ide and Veronis, 1998). Compared with English, Chinese does not have large-scale sense-tagged corpus. The widely available corpus is Academic Sinica Balanced Corpus abbreviated as ASBC hereafter (Huang and Chen, 1995), which is a POS-tagged corpus. Thus, a computer-aided tool to sense-tag Chinese corpus is indispensable. This paper presents a sense tagger for Mandarin Chinese. It is organized as follows. Section 2 discusses the degree of polysemy in Mandarin Chinese from several viewpoints. Section 3 presents WSD algorithms for tagging ambiguous words and unknown words. Section 4 shows our experimental results. Fin</context>
</contexts>
<marker>Ide, Veronis, 1998</marker>
<rawString>Ide, N. and Veronis, J. (1998) &amp;quot;Word Sense Disambiguation: The State of Art,&amp;quot; Computational Linguistics, 24(1), pp. 1-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Kelly</author>
<author>P Stone</author>
</authors>
<title>Computer Recognition of English Word Senses,</title>
<date>1975</date>
<location>North-Holland, Amsterdam.</location>
<contexts>
<context position="1100" citStr="Kelly and Stone, 1975" startWordPosition="161" endWordPosition="164">rds is 34.35%, which is much better than that of baseline mode. The sense tagger achieves the performance of 76.04%, when unambiguous, ambiguous, and unknown words are tagged. 1 Introduction Tagging task, which adds lexical, syntactic or semantic information to raw text, makes materials more valuable. The researches on part of speech (POS) tagging have been a long history, and achieve very good results. Many POS-tagged corpora are available. The accuracy for POS-tagging is in the range of 95% to 97%1. In contrast, although the researches on word sense disambiguation (WSD) are also very early (Kelly and Stone, 1975), large-scale sense-tagged corpus is relatively few. In English, only some sense-tagged corpora such as HECTOR (Atkins, 1993), DSO (Ng and Lee, 1996), SEMCOR (Fellbaum, 1997), and SENSEVAL (ICilgarriff, 1998) are available. For evaluating word sense disambiguation systems, the first SENSEVAL (Kilgarriff and Rosenzweig, 2000) reports that the performance for a fine-grained word sense disambiguation task is at around 75%. 1 The performance includes tagging unambiguous words. Marshall (1987) reported that the performance of CLAWS tagger is 94%. Approximately 65% of words were tagged unambiguously</context>
</contexts>
<marker>Kelly, Stone, 1975</marker>
<rawString>Kelly, E. and Stone, P. (1975) Computer Recognition of English Word Senses, North-Holland, Amsterdam.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
</authors>
<title>SENSEVAL: An Exercise in Evaluating Word Sense Disambiguation Programs,&amp;quot;</title>
<date>1998</date>
<booktitle>Proceedings of First International Conference on Language Resources and Evaluation, Granada,</booktitle>
<pages>581--588</pages>
<marker>Kilgarriff, 1998</marker>
<rawString>Kilgarriff, A. (1998) &amp;quot;SENSEVAL: An Exercise in Evaluating Word Sense Disambiguation Programs,&amp;quot; Proceedings of First International Conference on Language Resources and Evaluation, Granada, pp. 581-588.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>J Rosenzweig</author>
</authors>
<title>English SENSEVAL: Report and Results,&amp;quot;</title>
<date>2000</date>
<booktitle>Proceedings of Second International Conference on Language Resources and Evaluation.</booktitle>
<contexts>
<context position="1426" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="205" endWordPosition="208">s on part of speech (POS) tagging have been a long history, and achieve very good results. Many POS-tagged corpora are available. The accuracy for POS-tagging is in the range of 95% to 97%1. In contrast, although the researches on word sense disambiguation (WSD) are also very early (Kelly and Stone, 1975), large-scale sense-tagged corpus is relatively few. In English, only some sense-tagged corpora such as HECTOR (Atkins, 1993), DSO (Ng and Lee, 1996), SEMCOR (Fellbaum, 1997), and SENSEVAL (ICilgarriff, 1998) are available. For evaluating word sense disambiguation systems, the first SENSEVAL (Kilgarriff and Rosenzweig, 2000) reports that the performance for a fine-grained word sense disambiguation task is at around 75%. 1 The performance includes tagging unambiguous words. Marshall (1987) reported that the performance of CLAWS tagger is 94%. Approximately 65% of words were tagged unambiguously, and the disambiguation program achieved better than 80% success on the ambiguous words. Chi-Ching Lin Department of Computer Science and Information Engineering National Taiwan University Taipei, TAIWAN cclin@n1g2.csie.ntu.edu.tw Tagging accuracy depends on several issues (Manning and Schutze, 1999), e.g., the amount of tr</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>Kilgarriff, A. and Rosenzweig, J. (2000) &amp;quot;English SENSEVAL: Report and Results,&amp;quot; Proceedings of Second International Conference on Language Resources and Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>IL Schulze</author>
</authors>
<date>1999</date>
<booktitle>Foundations of Statistical Natural Language Processing,</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>Manning, Schulze, 1999</marker>
<rawString>Manning, C.D. and Schulze, IL (1999) Foundations of Statistical Natural Language Processing, MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Marshall</author>
</authors>
<title>Tag Selection using Probabilistic Methods,&amp;quot;</title>
<date>1987</date>
<booktitle>The Computational Analysis of English, Longman,</booktitle>
<pages>42--56</pages>
<editor>in Roger Garside, Geoffrey Leech and Geoffrey Sampson (editors),</editor>
<contexts>
<context position="1593" citStr="Marshall (1987)" startWordPosition="231" endWordPosition="232"> 97%1. In contrast, although the researches on word sense disambiguation (WSD) are also very early (Kelly and Stone, 1975), large-scale sense-tagged corpus is relatively few. In English, only some sense-tagged corpora such as HECTOR (Atkins, 1993), DSO (Ng and Lee, 1996), SEMCOR (Fellbaum, 1997), and SENSEVAL (ICilgarriff, 1998) are available. For evaluating word sense disambiguation systems, the first SENSEVAL (Kilgarriff and Rosenzweig, 2000) reports that the performance for a fine-grained word sense disambiguation task is at around 75%. 1 The performance includes tagging unambiguous words. Marshall (1987) reported that the performance of CLAWS tagger is 94%. Approximately 65% of words were tagged unambiguously, and the disambiguation program achieved better than 80% success on the ambiguous words. Chi-Ching Lin Department of Computer Science and Information Engineering National Taiwan University Taipei, TAIWAN cclin@n1g2.csie.ntu.edu.tw Tagging accuracy depends on several issues (Manning and Schutze, 1999), e.g., the amount of training data, the granularity of the tagging set, the occurrences of unknown words, and so on. Three approaches have been proposed for WSD, including dictionary/thesaur</context>
</contexts>
<marker>Marshall, 1987</marker>
<rawString>Marshall, I. (1987) &amp;quot;Tag Selection using Probabilistic Methods,&amp;quot; in Roger Garside, Geoffrey Leech and Geoffrey Sampson (editors), The Computational Analysis of English, Longman, pp. 42-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Mei</author>
</authors>
<date>1982</date>
<pages>2--4</pages>
<publisher>Shanghai Dictionary Press.</publisher>
<marker>Mei, 1982</marker>
<rawString>Mei, J.; et al. (1982) tong2yi4ci2ci2lin2. Shanghai Dictionary Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RB Lee</author>
</authors>
<title>Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach,&amp;quot;</title>
<date>1996</date>
<booktitle>Proceedings of 34th Annual Meeting of Association for Computational Linguistics,</booktitle>
<pages>40--47</pages>
<contexts>
<context position="1249" citStr="Lee, 1996" startWordPosition="185" endWordPosition="186">ds are tagged. 1 Introduction Tagging task, which adds lexical, syntactic or semantic information to raw text, makes materials more valuable. The researches on part of speech (POS) tagging have been a long history, and achieve very good results. Many POS-tagged corpora are available. The accuracy for POS-tagging is in the range of 95% to 97%1. In contrast, although the researches on word sense disambiguation (WSD) are also very early (Kelly and Stone, 1975), large-scale sense-tagged corpus is relatively few. In English, only some sense-tagged corpora such as HECTOR (Atkins, 1993), DSO (Ng and Lee, 1996), SEMCOR (Fellbaum, 1997), and SENSEVAL (ICilgarriff, 1998) are available. For evaluating word sense disambiguation systems, the first SENSEVAL (Kilgarriff and Rosenzweig, 2000) reports that the performance for a fine-grained word sense disambiguation task is at around 75%. 1 The performance includes tagging unambiguous words. Marshall (1987) reported that the performance of CLAWS tagger is 94%. Approximately 65% of words were tagged unambiguously, and the disambiguation program achieved better than 80% success on the ambiguous words. Chi-Ching Lin Department of Computer Science and Informatio</context>
</contexts>
<marker>Lee, 1996</marker>
<rawString>Ng RT. and Lee, RB. (1996) &amp;quot;Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach,&amp;quot; Proceedings of 34th Annual Meeting of Association for Computational Linguistics, pp. 40-47.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>