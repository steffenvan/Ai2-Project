<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000103">
<title confidence="0.9980285">
Adaptation of the F-measure to Cluster Based Lexicon Quality
Evaluation
</title>
<author confidence="0.990474">
Angelo Dalli
</author>
<affiliation confidence="0.993657">
NLP Research Group
Department of Computer Science
University of Sheffield
</affiliation>
<bodyText confidence="0.417423">
a . dalli@dcs shef. .ac.uk
</bodyText>
<sectionHeader confidence="0.970101" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9986151">
An external lexicon quality measure
called the L-measure is derived from the
F-measure (Rijsbergen, 1979; Larsen and
Aone, 1999). The typically small sample
sizes available for minority languages and
the evaluation of Semitic language lexi-
cons are two main factors considered.
Large-scale evaluation results for the
Maltilex Corpus are presented (Rosner et
al., 1999).
</bodyText>
<sectionHeader confidence="0.998787" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999917866666667">
Computational Lexicons form a fundamental
component of any NLP system. Unfortunately,
good quality lexicons are hard to create and
maintain. The labour intensive process of lexicon
creation is further compounded when minority
languages are concerned. Inevitably, computa-
tional lexicons for minor languages tend to be
quite small when compared to computational
lexicons available for more common languages
such as English.
The Maltilex Corpus is used in this paper to
evaluate a cluster based lexicon quality measure
adapted from the F-measure. The Maltilex Cor-
pus is the first large-scale computational lexicon
for Maltese (Rosner et al., 1999). The choice of
Maltese as the evaluation language presented
some additional problems due to the Semitic
morphology and grammar of Maltese (Mifsud,
1995). An innovative approach to lexicon crea-
tion using an automated technique called the
Lexicon Structuring Technique (LST) was used
to create an initial computational lexicon from a
wordlist (Dalli, 2002a). LST decreased the
amount of work that is normally required to cre-
ate a lexicon from scratch by adapting a number
of clustering, alignment, and approximate match-
ing techniques to produce a set of clusters con-
taining related wordforms. Lexicon clusters are
thus analogous to lemmas in more traditional
lexicons.
This approach has many advantages for a lan-
guage having a Semitic morphology and gram-
mar due to the large number of wordforms that
can be derived for a single lemma. Instead of
processing every wordform individually, the
whole cluster can be treated as a single entity,
reducing processing requirements significantly.
The close relationship of this lexicon defini-
tion and standard clustering systems (with lem-
mas corresponding to clusters), enabled the re-
use of cluster quality evaluation measures to the
task of lexicon quality evaluation. There are two
main ways of evaluating cluster quality which are
summarised in (Steinbach et al., 1999 pg. 6) as
follows:
</bodyText>
<listItem confidence="0.993506166666667">
• Internal Quality Measure — Clusters are
compared without reference to external
knowledge against some predefined set of
desirable qualities.
• External Quality Measure — Clusters are
compared to known external classes.
</listItem>
<bodyText confidence="0.999101142857143">
Internal quality measures are not always desir-
able, since their very existence implies that better
quality can be achieved by applying an internal
quality measure in conjunction with some opti-
misation technique. An internal quality measure
for cluster-based lexicons was not available ei-
ther.
The two main external quality measures appli-
cable lexicon quality evaluation tasks are entropy
(Shannon, 1948) and the F-measure (van
Rijsbergen, 1979; Larsen and Aone, 1999).
Entropy based quality measures assert that the
best entropy that can be obtained is when each
cluster contains the optimal number of members.
In our context this corresponds to having clusters
(corresponding to lemmas) that contain exactly
all the wordforms associated with that cluster.
The class distribution of the data is calculated by
considering the probability of every member be-
longing to some class. The entropy of every clus-
ter j is calculated using the standard entropy
</bodyText>
<equation confidence="0.7426734">
formula E(j) = —1p u log(p ) where pu de-
notes the probability that a member of cluster j
belongs to class i. The total entropy is then calcu-
lated as E = j • E(j) where n, is the
n j=1
</equation>
<bodyText confidence="0.999890857142857">
size of cluster j, m the number of clusters, and n
the total number of data points.
The F-measure treats every cluster as a query
and every class as the desired result set for a
query. The recall and precision values for each
given class are then calculated using information
retrieval concepts. The F-measure of cluster j and
</bodyText>
<equation confidence="0.998167333333333">
\ 2. r (1 , j) • p(i , j)
class i is given by TV,j)— .\ .\
711,J)+ pv,j)
</equation>
<bodyText confidence="0.995579">
where r denotes recall and p the precision. Recall
</bodyText>
<sectionHeader confidence="0.438909" genericHeader="method">
\ nu
</sectionHeader>
<bodyText confidence="0.791239833333333">
is defined as r(i , j) = —n and precision is de-
where ny is the number of
ni
class i members in cluster j, while n, and n, are
the sizes of cluster j and class i respectively. The
overall F-measure for the entire data set of size n
</bodyText>
<equation confidence="0.707259333333333">
n.
is given by F* max[F(1, j)1 .
n
</equation>
<sectionHeader confidence="0.758591" genericHeader="method">
2 Lexicon Quality Measure
</sectionHeader>
<bodyText confidence="0.999969666666667">
Computational lexicons have an additional do-
main-specific external quality measure available
in the form of existing non-computational lan-
guage dictionaries. Dictionaries can be used to
compare the results generated by the automated
system against those produced by human experts.
Generally it can be assumed that reputable
printed dictionaries are of a very high quality and
thus provide a gold standard for comparison. For
some languages, especially minority languages,
the only available quality data would be in
printed dictionary form. Unfortunately most non-
computational dictionaries are not amenable to
automated analysis techniques since the process
of re-inputting and re-structuring data into a
computational dictionary format is generally so
labour intensive that it becomes too expensive.
Additionally, since every cluster and class
correspond to a lemma, the number of classes to
be considered is expected to number in the thou-
sands. This would make a straightforward appli-
cation of the F-measure an overly long process.
A modified statistical sampling technique based
on the F-measure that gives results that are ap-
proximately as good as the full application of the
F-measure and that caters for the particular nu-
ances of lexicon quality evaluation is thus
needed.
The L-measure is such a new measure based
on the F-measure that attempts to measure the
quality of a given lexicon in relation to other ex-
isting lexicons that are possibly non-
computational lexicons (i.e. human compiled
language dictionaries), taking into consideration
that a full population analysis may not be practi-
cal under most circumstances.
</bodyText>
<subsectionHeader confidence="0.99767">
2.1 Lexicon Extraction from Dictionaries
</subsectionHeader>
<bodyText confidence="0.996893983333334">
The L-Measure works by comparing two lexi-
cons, one derived from a gold standard represen-
tation in the form of human compiled dictionaries
and the other being a computational lexicon
whose quality is being assessed. In order to avoid
confusion, formal definitions of the terms dic-
tionary, lexicon and wordlists are now presented.
A dictionary D is formally modeled as a se-
quence &lt;t1 th&gt; of tuples of the form (1, def)
where / denotes a lemma (i.e. a dictionary head-
fined as
word in a more traditional sense) and def is a 5-
tuple (m, r, c, 1, o) with m containing morpho-
logical information that enables members of the
lemma to be inferred or generated, r a set of rela-
tions to other lemmas, c a description of the dif-
ferent contexts where the lemma may be
normally used, i containing meta-information
about lemma / itself, and o an object containing
additional information (such as etymology, ex-
amples of common use, etc.) Since multiple en-
tries of the same headword may be present in D
the sequence is not injective, i.e. the sequence
can contain duplicate elements.
The main two differences between a dictionary
and a lexicon are that different types of informa-
tion are stored about every lemma in the def
component, and secondly, that a lexicon has an
injective sequence of tuples (i.e. a sequence that
does not have duplicates and where the exact or-
der is important) while a dictionary does not
(since a dictionary does not need to force a
headword to have one unique entry, especially in
the case of printed dictionaries that often have the
same headword appearing in multiple top-level
entries).
A dictionary D can be thus transformed into a
lexicon L, denoted by L = lex(D), by filtering the
tuple sequence &lt;ti th&gt; making up D to include
only the / components of every tuple. The filtered
sequence is then transformed into an injective
sequence of unique lemmas &lt;// /.&gt;, satisfying
the requirements for a lexicon. Appropriate trans-
formations have to be defined to transform the
def component from dictionary to lexicon format.
The sequence of lemmas is then expanded to a
canonical wordlist W. A canonical wordlist W is
a sequence &lt;w1 wu&gt; of sets of strings generated
from a lexicon L, denoted by W = can(L), by list-
ing all possible instances of every lemma in the
lexicon (i.e. all possible wordforms of a particu-
lar lemma), in effect creating a full form lexicon.
The canonical wordlist W thus has u sets of
strings corresponding to u lemmas in the lexicon.
The particular lemma used to generate a word-
form w is obtained by the operator lem(w). The
sequence of lemmas used to generate W is de-
noted as lemmas( W). The union of two wordlists
L-.) W2 is defined to be the union of all sets of
strings in both wordlists,
</bodyText>
<equation confidence="0.924508333333333">
i.e. eX, G G W2 • W2 (xi
provided that /em(x1) = /em(y3) v /em(x)
lemmas(W2)v /em(y3) lemmas(W) holds.
</equation>
<bodyText confidence="0.996067210526316">
This definition ensures maximum coverage of
the resulting canonical wordlist. An empty or null
canonical wordlist results if no pair of strings
obey the previously stated condition while the
union of a wordlist with a null wordlist is the
original wordlist itself
Similarly the intersection of two wordlists W
n W2 is defined to be the union of all sets of
strings in both wordlists that have corresponding
lemmas appearing in both wordlists, i.e.
Vx, G ,yj G W2 • WI n W2 =(x, yj)
provided that lem(xi)= /em(y3) holds.
Note that this definition is concerned mainly
with the lemmas and their associated wordforms
themselves. Since lexicons are not just a list of
lemmas and wordforms, other linguistic annota-
tions will have to be evaluated using other tech-
niques appropriate to the particular linguistic
annotations added to the lemma entries.
</bodyText>
<subsectionHeader confidence="0.982131">
2.2 L-Measure Definition
</subsectionHeader>
<bodyText confidence="0.997986857142857">
Given a lexicon L and a set of dictionaries D =
{D1 Dk} transform the set of dictionaries D into
a set of lexicons L&apos; = {L1 .. Lk} using the lex
transformation on every dictionary, thus
L&apos;=Ulex(R). Define Was the canonical word-
list obtained from L, W = can(L) and W as the
canonical wordlist obtained from L&apos;,
</bodyText>
<equation confidence="0.594308">
=Ucan(L) under canonical wordlist union.
</equation>
<bodyText confidence="0.983572181818182">
Define Y to be the canonical wordlist of words
common to both W and W, Y= Wn W. The
sample size S used for the L-measure is defined
as adlemmas(Y)1 where a is some value in the
range (0..1) that controls the random sample size.
Typically a should be set to somewhere between
0.01 and 0.1. It is expected that the sample size
will be large enough to assume that the sample is
representative of the whole population.
The L-measure of a lemma j in lemmas(W) and
lemma i in lemmas(Y) is given by
</bodyText>
<equation confidence="0.94923925">
L(i , — r (1 , j) + p(i ,
call and p is the precision. Recall is defined as
ri , j) = —
ni
</equation>
<bodyText confidence="0.8912778">
where ny is the number of lemma i
ni
members in lemma j, while n, and n, are the sizes
of lemma j and lemma i respectively. The overall
L-measure for the entire sample of size n is given
</bodyText>
<equation confidence="0.826274666666667">
ni
by L* = L- max[L(i, j)]. L* is always in the
in
</equation>
<bodyText confidence="0.999959">
range [0..1] and is proportional to the lexicon
quality, with an L* score of 1 representing a per-
fect quality lexicon with respect to the lexicon
being used as a standard.
Y is used instead of W since lexical word cov-
erage is largely determined by the quality of the
corpus used to create the lexicon. While this kind
of analysis might be useful in determining the
coverage of a lexicon the L-measure is oriented
towards measuring quality rather than quantity,
independently of the corpus that was used to cre-
ate the lexicon.
</bodyText>
<sectionHeader confidence="0.999943" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.981581879310345">
The L-measure has been used to measure the
quality of the Maltilex Computational Lexicon in
relation to existing paper based dictionaries. The
most comprehensive dictionary of Maltese was
used to produce L&apos;, the comparison standard lexi-
con (Aquilina, 1987-1990). The capability of the
L-measure to work with a statistical sample made
a manual analysis of results possible without hav-
ing L&apos; in digital form.
The value for the sample size S was deter-
mined through a parameter a that was set to 0.01,
meaning that 1% of all lemmas in the Maltilex
Computational Lexicon were covered by the sta-
tistical sample. Since around 63,000 lemmas ex-
ist in the combined lexicon the sample size S was
determined to be 630. The set of 630 lemmas
chosen at random from the Maltilex Corpus con-
tained a total of 5,887 wordforms taken from the
combined lexicon.
The precision and recall for the samples were
calculated individually to obtain the individual L-
measure for a range of lemmas. A fully worked
out example of the calculation of the L-measure
for the lemma miss ier (father) is given. Lem-
mas in the Maltilex Computational Lexicon are
aligned automatically using a technique adopted
from bioinformatics and hence the presentation
of the wordforms in their aligned format (Dalli,
2000b; Gusfield, 1997).
The lemma missier (the Maltese word forfa-
ther with the cluster showing different forms like
my father, your father, etc.) taken from the
Maltilex Computational Lexicon, which repre-
sents lemma i, contains seven members as dis-
played below:
missier
missierek
missier
missier kom
missi ri
missieri
missier
The lemma missier, taken from Aquilina&apos;s Dic-
tionary, which represents lemma j, can be used to
generate the following ten members as displayed
below:
missier
missierek
missier a
missier kom
missi ri iietna
missieri
missiera
missier
missier
missi ri
For this example, n, and n, are thus equal to 10
and 7 respectively. Recall and precision values
</bodyText>
<equation confidence="0.8136362">
\ 7
are calculated as r(missier,missier&apos;)=— =1
7
\ 7
Amissier,missier&apos;)=— = 0.7 respectively.
</equation>
<page confidence="0.84857">
10
</page>
<bodyText confidence="0.9637595">
The L-measure for the lemma missier is
\ 2 .1. 0.7 1.4
</bodyText>
<equation confidence="0.973589">
L(missier,missier&apos;)= = = 0.8235
1+0.7 1.7
2• r , j) • p(i , j)
</equation>
<bodyText confidence="0.844542">
where r denotes re-
and precision is defined as
</bodyText>
<figure confidence="0.782569769230769">
— na
jiet
hom
hom
iet
The overall L-measure for the entire sample of
5,887 wordforms is given by
L* = max [L(i, A . The contribution of
5887
the lemma missier to the final L* score is thus
7
given by 0.8235= 0.000979226. A high
5 887
</figure>
<bodyText confidence="0.99465">
precision floating point library was used to repre-
sent the individual contribution values since these
are generally very small. Figures 1 and 2 show
the precision and recall curves for the whole
sample respectively.
</bodyText>
<figure confidence="0.808399">
0 -
Lemtnam 63 94 125 156 187 218 249 280 311 342 373 404 435 466 497 528 559 590 621
</figure>
<figureCaption confidence="0.932071">
Figure 1 Precision
</figureCaption>
<figure confidence="0.851207">
Lenl.a5 49 73 97 121 145 169 193 217 241 285 289 313 337 381 385 409 433 457 481 505 529 553 577 601 625
</figure>
<figureCaption confidence="0.781545">
Figure 2 Recall
</figureCaption>
<figure confidence="0.989369666666667">
1.2
0.8
0.6
0.4
0.2
Lemtnam 63 94 125 156 187 218 249 280 311 342 373 404 435 466 497 528 559 590 62
</figure>
<figureCaption confidence="0.999273">
Figure 3 Precision and Recall Trends
</figureCaption>
<bodyText confidence="0.9989712">
Figure 3 shows moving average trendlines for
precision and recall (precision is shown in a bold
line on top, recall is the fainter line underneath).
The average precision was 0.91748 and the aver-
age rate of recall was 0.661359.
</bodyText>
<figure confidence="0.915829818181818">
1.2
1
0.8
0.6
0.4
0.2
0
IIII!1111T1 1! &apos;till r1111111
111111 &apos;II
II LII i1i Ii Ii IL IiIIhL
1 55 109 163 217 271 325 379 433 487 541 595
</figure>
<figureCaption confidence="0.99699">
Figure 5 Individual L-Measure Values Trend
</figureCaption>
<bodyText confidence="0.997931809523809">
Figure 4 shows the individual L-measure val-
ues for the sample. The values displayed in Fig-
ure 4 are those used to calculate the final L*
value. Figure 5 shows the moving average trend-
line for the individual L-measure values.
The average individual L-measure was
0.707256882 while the average individual
contribution of a lemma to the L* value was
0.000748924. The variance in the L-measure in-
dividual values was 0.065504369.
The correlation between the L-measure and
precision was 0.163665769 while the correlation
between the L-measure and recall was
0.922214452.
The overall L* score for the Maltilex Computa-
tional Lexicon was 0.4718. This score is quite
intuitive when the various problems in the exist-
ing Maltese corpus used to create the Computa-
tional Lexicon are considered. This score means
that the number of wordforms that are stored or
that can be generated by the current lexicon
</bodyText>
<figure confidence="0.9957343">
1.2
0.8
0.6
0.4
0.2
i U1111M J111111111
I [
1101.11 11 I 1J1LI Ii
IL Ili [IL L1111E11111111
1 55 109 163 217 271 325 379 433 487 541
</figure>
<figureCaption confidence="0.764299">
Figure 4 Individual L-Measure Values
</figureCaption>
<figure confidence="0.99593125">
1.2
1
0.6
0.8
millifAvetWousvmAiNke
595
0.4
0.2
</figure>
<bodyText confidence="0.909223333333333">
needs to be expanded by around 53% in order to
match the quality of the lexicon underlying
Aquilina&apos; s dictionary (Aquilina, 1987-1990).
Angelo Dalli. 2002b. Biologically Inspired Lexicon
Structuring Technique. HLT2002, San Diego, Cali-
fornia.
</bodyText>
<sectionHeader confidence="0.998282" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999991617647059">
The L-measure is a useful evaluation metric that
can be used to measure the quality of a computa-
tional lexicon based on clustering concepts. The
small data sample required by L-measure to give
meaningful results makes it a practical measure
to use in a variety of situations where massive
amounts of data might not be available. This
makes L-measure ideal for use in the evaluation
of Language Resources for minority languages
and also for quick benchmark studies that evalu-
ate the quality of a computational lexicon as it is
being created.
Compared with the F-measure, the L-measure
will give highly similar results using less data.
Naturally the validity of the L-measure results
depends on the choice of the a value, which in
turn determines the sample size.
The lemma/cluster based approach of the L-
measure is suitable for the evaluation of Semitic
language lexicons that often prove problematic to
evaluation techniques based on English or Ro-
mance languages.
The L-measure also has potential future appli-
cations in the comparison and evaluation of dif-
ferent lexicons. The individual L-measure scores
can also be used to identify areas of similarities
and differences between different lexicons
quickly.
The L-measure can also be adapted to other
areas of Computational Linguistics as long as the
concept of a cluster and some means of determin-
ing its precision and recall exist. Minimal
changes are needed to adapt the L-measure to
other domains making future adaptations likely.
</bodyText>
<sectionHeader confidence="0.969735" genericHeader="acknowledgments">
Acknowledgment
</sectionHeader>
<bodyText confidence="0.996355">
This work has been made possible with the col-
laboration of the Mattilex Project at the Univer-
sity of Malta.
</bodyText>
<sectionHeader confidence="0.998573" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997993307692308">
Angelo Dalli. 2002a. Computational Lexicon for Mal-
tese. M. Sc. Dissertation. Department of Computer
Science and AT, University of Malta, Malta.
Bjorner Larsen and Chinatsu Aone. 1999. Fast and
Effective Text Mining Using Linear-time Docu-
ment Clustering. KDD-99, San Diego, California.
C. Van Rijsbergen. 1979. Information Retrieval, 2nd
ed. Butterworth, London.
Claude E. Shannon 1948. A mathematical theory of
communication. Bell System Technical Journal 27:
379-423, 623-656.
Dan Gusfield. 1997. Algorithms on Strings, Trees and
Sequences. Cambridge University Press, Cam-
bridge, UK.
Joseph Aquilina 1987-1990. Maltese-English Dic-
tionary. Midsea Books, 2 Volumes, Valletta, Malta.
Manwel Mifsud. 1995. Loan verbs in Maltese a de-
scriptive and comparative study. Studies in Semitic
languages and linguistics, Brill, Leiden.
Michael Rosner et. al. 1999. Linguistic and Computa-
tional Aspects of Maltilex. ATLAS Symposium, Tu-
nis.
Michael Steinbach, George Karypis, and Vipin
Kumar. 1999. A comparison of document cluster-
ing techniques, University of Minnesota, Technical
Report 00-034.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.228756">
<title confidence="0.99946">Adaptation of the F-measure to Cluster Based Lexicon Quality Evaluation</title>
<author confidence="0.995021">Angelo</author>
<affiliation confidence="0.997445">NLP Research Department of Computer University of Sheffield</affiliation>
<email confidence="0.835617">a.dalli@dcsshef..ac.uk</email>
<abstract confidence="0.997714625">An external lexicon quality measure called the L-measure is derived from the F-measure (Rijsbergen, 1979; Larsen and Aone, 1999). The typically small sample sizes available for minority languages and the evaluation of Semitic language lexicons are two main factors considered.</abstract>
<note confidence="0.480432333333333">Large-scale evaluation results for the Maltilex Corpus are presented (Rosner et al., 1999).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Angelo Dalli</author>
</authors>
<date>2002</date>
<journal>Computational Lexicon</journal>
<institution>Department of Computer Science and AT, University of Malta,</institution>
<contexts>
<context position="1577" citStr="Dalli, 2002" startWordPosition="231" endWordPosition="232"> common languages such as English. The Maltilex Corpus is used in this paper to evaluate a cluster based lexicon quality measure adapted from the F-measure. The Maltilex Corpus is the first large-scale computational lexicon for Maltese (Rosner et al., 1999). The choice of Maltese as the evaluation language presented some additional problems due to the Semitic morphology and grammar of Maltese (Mifsud, 1995). An innovative approach to lexicon creation using an automated technique called the Lexicon Structuring Technique (LST) was used to create an initial computational lexicon from a wordlist (Dalli, 2002a). LST decreased the amount of work that is normally required to create a lexicon from scratch by adapting a number of clustering, alignment, and approximate matching techniques to produce a set of clusters containing related wordforms. Lexicon clusters are thus analogous to lemmas in more traditional lexicons. This approach has many advantages for a language having a Semitic morphology and grammar due to the large number of wordforms that can be derived for a single lemma. Instead of processing every wordform individually, the whole cluster can be treated as a single entity, reducing process</context>
</contexts>
<marker>Dalli, 2002</marker>
<rawString>Angelo Dalli. 2002a. Computational Lexicon for Maltese. M. Sc. Dissertation. Department of Computer Science and AT, University of Malta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bjorner Larsen</author>
<author>Chinatsu Aone</author>
</authors>
<title>Fast and Effective Text Mining Using Linear-time Document Clustering. KDD-99,</title>
<date>1999</date>
<location>San Diego, California.</location>
<contexts>
<context position="3241" citStr="Larsen and Aone, 1999" startWordPosition="487" endWordPosition="490">ce to external knowledge against some predefined set of desirable qualities. • External Quality Measure — Clusters are compared to known external classes. Internal quality measures are not always desirable, since their very existence implies that better quality can be achieved by applying an internal quality measure in conjunction with some optimisation technique. An internal quality measure for cluster-based lexicons was not available either. The two main external quality measures applicable lexicon quality evaluation tasks are entropy (Shannon, 1948) and the F-measure (van Rijsbergen, 1979; Larsen and Aone, 1999). Entropy based quality measures assert that the best entropy that can be obtained is when each cluster contains the optimal number of members. In our context this corresponds to having clusters (corresponding to lemmas) that contain exactly all the wordforms associated with that cluster. The class distribution of the data is calculated by considering the probability of every member belonging to some class. The entropy of every cluster j is calculated using the standard entropy formula E(j) = —1p u log(p ) where pu denotes the probability that a member of cluster j belongs to class i. The tota</context>
</contexts>
<marker>Larsen, Aone, 1999</marker>
<rawString>Bjorner Larsen and Chinatsu Aone. 1999. Fast and Effective Text Mining Using Linear-time Document Clustering. KDD-99, San Diego, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Van Rijsbergen</author>
</authors>
<date>1979</date>
<booktitle>Information Retrieval, 2nd ed.</booktitle>
<location>Butterworth, London.</location>
<marker>Van Rijsbergen, 1979</marker>
<rawString>C. Van Rijsbergen. 1979. Information Retrieval, 2nd ed. Butterworth, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claude E Shannon</author>
</authors>
<title>A mathematical theory of communication.</title>
<date>1948</date>
<journal>Bell System Technical Journal</journal>
<volume>27</volume>
<pages>379--423</pages>
<contexts>
<context position="3177" citStr="Shannon, 1948" startWordPosition="479" endWordPosition="480"> Quality Measure — Clusters are compared without reference to external knowledge against some predefined set of desirable qualities. • External Quality Measure — Clusters are compared to known external classes. Internal quality measures are not always desirable, since their very existence implies that better quality can be achieved by applying an internal quality measure in conjunction with some optimisation technique. An internal quality measure for cluster-based lexicons was not available either. The two main external quality measures applicable lexicon quality evaluation tasks are entropy (Shannon, 1948) and the F-measure (van Rijsbergen, 1979; Larsen and Aone, 1999). Entropy based quality measures assert that the best entropy that can be obtained is when each cluster contains the optimal number of members. In our context this corresponds to having clusters (corresponding to lemmas) that contain exactly all the wordforms associated with that cluster. The class distribution of the data is calculated by considering the probability of every member belonging to some class. The entropy of every cluster j is calculated using the standard entropy formula E(j) = —1p u log(p ) where pu denotes the pro</context>
</contexts>
<marker>Shannon, 1948</marker>
<rawString>Claude E. Shannon 1948. A mathematical theory of communication. Bell System Technical Journal 27: 379-423, 623-656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on Strings, Trees and Sequences.</title>
<date>1997</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="13017" citStr="Gusfield, 1997" startWordPosition="2205" endWordPosition="2206"> size S was determined to be 630. The set of 630 lemmas chosen at random from the Maltilex Corpus contained a total of 5,887 wordforms taken from the combined lexicon. The precision and recall for the samples were calculated individually to obtain the individual Lmeasure for a range of lemmas. A fully worked out example of the calculation of the L-measure for the lemma miss ier (father) is given. Lemmas in the Maltilex Computational Lexicon are aligned automatically using a technique adopted from bioinformatics and hence the presentation of the wordforms in their aligned format (Dalli, 2000b; Gusfield, 1997). The lemma missier (the Maltese word forfather with the cluster showing different forms like my father, your father, etc.) taken from the Maltilex Computational Lexicon, which represents lemma i, contains seven members as displayed below: missier missierek missier missier kom missi ri missieri missier The lemma missier, taken from Aquilina&apos;s Dictionary, which represents lemma j, can be used to generate the following ten members as displayed below: missier missierek missier a missier kom missi ri iietna missieri missiera missier missier missi ri For this example, n, and n, are thus equal to 10</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield. 1997. Algorithms on Strings, Trees and Sequences. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Aquilina</author>
</authors>
<title>1987-1990. Maltese-English Dictionary.</title>
<date></date>
<journal>Midsea Books,</journal>
<volume>2</volume>
<location>Volumes, Valletta,</location>
<marker>Aquilina, </marker>
<rawString>Joseph Aquilina 1987-1990. Maltese-English Dictionary. Midsea Books, 2 Volumes, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manwel Mifsud</author>
</authors>
<title>Loan verbs in Maltese a descriptive and comparative study.</title>
<date>1995</date>
<booktitle>Studies in Semitic languages and linguistics,</booktitle>
<location>Brill, Leiden.</location>
<contexts>
<context position="1376" citStr="Mifsud, 1995" startWordPosition="201" endWordPosition="202">ation is further compounded when minority languages are concerned. Inevitably, computational lexicons for minor languages tend to be quite small when compared to computational lexicons available for more common languages such as English. The Maltilex Corpus is used in this paper to evaluate a cluster based lexicon quality measure adapted from the F-measure. The Maltilex Corpus is the first large-scale computational lexicon for Maltese (Rosner et al., 1999). The choice of Maltese as the evaluation language presented some additional problems due to the Semitic morphology and grammar of Maltese (Mifsud, 1995). An innovative approach to lexicon creation using an automated technique called the Lexicon Structuring Technique (LST) was used to create an initial computational lexicon from a wordlist (Dalli, 2002a). LST decreased the amount of work that is normally required to create a lexicon from scratch by adapting a number of clustering, alignment, and approximate matching techniques to produce a set of clusters containing related wordforms. Lexicon clusters are thus analogous to lemmas in more traditional lexicons. This approach has many advantages for a language having a Semitic morphology and gram</context>
</contexts>
<marker>Mifsud, 1995</marker>
<rawString>Manwel Mifsud. 1995. Loan verbs in Maltese a descriptive and comparative study. Studies in Semitic languages and linguistics, Brill, Leiden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Rosner</author>
</authors>
<date>1999</date>
<booktitle>Linguistic and Computational Aspects of Maltilex. ATLAS Symposium,</booktitle>
<location>Tunis.</location>
<marker>Rosner, 1999</marker>
<rawString>Michael Rosner et. al. 1999. Linguistic and Computational Aspects of Maltilex. ATLAS Symposium, Tunis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Steinbach</author>
<author>George Karypis</author>
<author>Vipin Kumar</author>
</authors>
<title>A comparison of document clustering techniques,</title>
<date>1999</date>
<tech>Technical Report 00-034.</tech>
<institution>University of Minnesota,</institution>
<contexts>
<context position="2533" citStr="Steinbach et al., 1999" startWordPosition="381" endWordPosition="384">ny advantages for a language having a Semitic morphology and grammar due to the large number of wordforms that can be derived for a single lemma. Instead of processing every wordform individually, the whole cluster can be treated as a single entity, reducing processing requirements significantly. The close relationship of this lexicon definition and standard clustering systems (with lemmas corresponding to clusters), enabled the reuse of cluster quality evaluation measures to the task of lexicon quality evaluation. There are two main ways of evaluating cluster quality which are summarised in (Steinbach et al., 1999 pg. 6) as follows: • Internal Quality Measure — Clusters are compared without reference to external knowledge against some predefined set of desirable qualities. • External Quality Measure — Clusters are compared to known external classes. Internal quality measures are not always desirable, since their very existence implies that better quality can be achieved by applying an internal quality measure in conjunction with some optimisation technique. An internal quality measure for cluster-based lexicons was not available either. The two main external quality measures applicable lexicon quality </context>
</contexts>
<marker>Steinbach, Karypis, Kumar, 1999</marker>
<rawString>Michael Steinbach, George Karypis, and Vipin Kumar. 1999. A comparison of document clustering techniques, University of Minnesota, Technical Report 00-034.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>