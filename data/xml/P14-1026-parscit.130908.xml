<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.871023">
Learning to Automatically Solve Algebra Word Problems
</title>
<author confidence="0.834361">
Nate Kushmant, Yoav Artzit, Luke Zettlemoyert, and Regina Barzilayt
</author>
<affiliation confidence="0.668106">
t Computer Science and Articial Intelligence Laboratory, Massachusetts Institute of Technology
</affiliation>
<email confidence="0.925042">
{nkushman, regina}@csail.mit.edu
</email>
<note confidence="0.887325">
t Computer Science &amp; Engineering, University of Washington
</note>
<email confidence="0.995822">
{yoav, lsz}@cs.washington.edu
</email>
<sectionHeader confidence="0.997262" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999861875">
We present an approach for automatically
learning to solve algebra word problems.
Our algorithm reasons across sentence
boundaries to construct and solve a sys-
tem of linear equations, while simultane-
ously recovering an alignment of the vari-
ables and numbers in these equations to
the problem text. The learning algorithm
uses varied supervision, including either
full equations or just the final answers. We
evaluate performance on a newly gathered
corpus of algebra word problems, demon-
strating that the system can correctly an-
swer almost 70% of the questions in the
dataset. This is, to our knowledge, the first
learning result for this task.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999844666666667">
Algebra word problems concisely describe a world
state and pose questions about it. The described
state can be modeled with a system of equations
whose solution specifies the questions’ answers.
For example, Figure 1 shows one such problem.
The reader is asked to infer how many children and
adults were admitted to an amusement park, based
on constraints provided by ticket prices and overall
sales. This paper studies the task of learning to
automatically solve such problems given only the
natural language.1
Solving these problems requires reasoning
across sentence boundaries to find a system of
equations that concisely models the described se-
mantic relationships. For example, in Figure 1,
the total ticket revenue computation in the second
equation summarizes facts about ticket prices and
total sales described in the second, third, and fifth
</bodyText>
<footnote confidence="0.993254333333333">
1The code and data for this work are available
at http://groups.csail.mit.edu/rbg/code/
wordprobs/.
</footnote>
<bodyText confidence="0.914419875">
Word problem
An amusement park sells 2 kinds of tickets.
Tickets for children cost $1.50. Adult tickets
cost $4. On a certain day, 278 people entered
the park. On that same day the admission fees
collected totaled $792. How many children
were admitted on that day? How many adults
were admitted?
</bodyText>
<equation confidence="0.9417424">
Equations
x + y = 278
1.5x + 4y = 792
Solution
x = 128 y = 150
</equation>
<figureCaption confidence="0.77215125">
Figure 1: An example algebra word problem. Our
goal is to map a given problem to a set of equations
representing its algebraic meaning, which are then
solved to get the problem’s answer.
</figureCaption>
<bodyText confidence="0.99986855">
sentences. Furthermore, the first equation models
an implicit semantic relationship, namely that the
children and adults admitted are non-intersecting
subsets of the set of people who entered the park.
Our model defines a joint log-linear distribu-
tion over full systems of equations and alignments
between these equations and the text. The space
of possible equations is defined by a set of equa-
tion templates, which we induce from the train-
ing examples, where each template has a set of
slots. Number slots are filled by numbers from
the text, and unknown slots are aligned to nouns.
For example, the system in Figure 1 is gener-
ated by filling one such template with four spe-
cific numbers (1.5, 4, 278, and 792) and align-
ing two nouns (“Tickets” in “Tickets for children”,
and “tickets” in “Adult tickets”). These inferred
correspondences are used to define cross-sentence
features that provide global cues to the model.
For instance, in our running example, the string
</bodyText>
<page confidence="0.966487">
271
</page>
<note confidence="0.830781">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 271–281,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999874428571428">
pairs (“$1.50”, “children”) and (“$4”,“adults”)
both surround the word “cost,” suggesting an out-
put equation with a sum of two constant-variable
products.
We consider learning with two different levels
of supervision. In the first scenario, we assume ac-
cess to each problem’s numeric solution (see Fig-
ure 1) for most of the data, along with a small
set of seed examples labeled with full equations.
During learning, a solver evaluates competing hy-
potheses to drive the learning process. In the sec-
ond scenario, we are provided with a full system
of equations for each problem. In both cases, the
available labeled equations (either the seed set, or
the full set) are abstracted to provide the model’s
equation templates, while the slot filling and align-
ment decisions are latent variables whose settings
are estimated by directly optimizing the marginal
data log-likelihood.
The approach is evaluated on a new corpus of
514 algebra word problems and associated equa-
tion systems gathered from Algebra.com. Pro-
vided with full equations during training, our al-
gorithm successfully solves over 69% of the word
problems from our test set. Furthermore, we find
the algorithm can robustly handle weak supervi-
sion, achieving more than 70% of the above per-
formance when trained exclusively on answers.
</bodyText>
<sectionHeader confidence="0.999931" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999930152777778">
Our work is related to three main areas of research:
situated semantic interpretation, information ex-
traction, and automatic word problem solvers.
Situated Semantic Interpretation There is a
large body of research on learning to map nat-
ural language to formal meaning representations,
given varied forms of supervision. Reinforcement
learning can be used to learn to read instructions
and perform actions in an external world (Brana-
van et al., 2009; Branavan et al., 2010; Vogel
and Jurafsky, 2010). Other approaches have re-
lied on access to more costly annotated logical
forms (Zelle and Mooney, 1996; Thompson and
Mooney, 2003; Wong and Mooney, 2006; Zettle-
moyer and Collins, 2005; Kwiatkowski et al.,
2010). These techniques have been generalized
more recently to learn from sentences paired with
indirect feedback from a controlled application.
Examples include question answering (Clarke et
al., 2010; Cai and Yates, 2013a; Cai and Yates,
2013b; Berant et al., 2013; Kwiatkowski et al.,
2013), dialog systems (Artzi and Zettlemoyer,
2011), robot instruction (Chen and Mooney, 2011;
Chen, 2012; Kim and Mooney, 2012; Matuszek et
al., 2012; Artzi and Zettlemoyer, 2013), and pro-
gram executions (Kushman and Barzilay, 2013;
Lei et al., 2013). We focus on learning from varied
supervision, including question answers and equa-
tion systems, both can be obtained reliably from
annotators with no linguistic training and only ba-
sic math knowledge.
Nearly all of the above work processed sin-
gle sentences in isolation. Techniques that con-
sider multiple sentences typically do so in a se-
rial fashion, processing each in turn with limited
cross-sentence reasoning (Branavan et al., 2009;
Zettlemoyer and Collins, 2009; Chen and Mooney,
2011; Artzi and Zettlemoyer, 2013). We focus on
analyzing multiple sentences simultaneously, as
is necessary to generate the global semantic rep-
resentations common in domains such as algebra
word problems.
Information Extraction Our approach is related
to work on template-based information extraction,
where the goal is to identify instances of event
templates in text and extract their slot fillers. Most
work has focused on the supervised case, where
the templates are manually defined and data is la-
beled with alignment information, e.g. (Grishman
et al., 2005; Maslennikov and Chua, 2007; Ji and
Grishman, 2008; Reichart and Barzilay, 2012).
However, some recent work has studied the au-
tomatic induction of the set of possible templates
from data (Chambers and Jurafsky, 2011; Ritter et
al., 2012). In our approach, systems of equations
are relatively easy to specify, providing a type of
template structure, and the alignment of the slots
in these templates to the text is modeled primar-
ily with latent variables during learning. Addition-
ally, mapping to a semantic representation that can
be executed allows us to leverage weaker supervi-
sion during learning.
Automatic Word Problem Solvers Finally, there
has been research on automatically solving vari-
ous types of mathematical word problems. The
dominant existing approach is to hand engineer
rule-based systems to solve math problem in spe-
cific domains (Mukherjee and Garain, 2008; Lev
et al., 2004). Our focus is on learning a model
for the end-to-end task of solving word problems
given only a training corpus of questions paired
with equations or answers.
</bodyText>
<page confidence="0.99585">
272
</page>
<table confidence="0.984626428571428">
Derivation 1
Word An amusement park sells 2 kinds of tickets. Tickets for children cost $ 1.50 . Adult
problem tickets cost $ 4 . On a certain day, 278 people entered the park. On that same day the
admission fees collected totaled $ 792 . How many children were admitted on that
day? How many adults were admitted?
Aligned 1 1 2 2
template u1 + u2 − n1 = 0 n2 × u1 + n3 × u2 − n4 = 0
Instantiated x + y − 278 = 0 1.5x + 4y − 792 = 0
equations
Answer x = 128
y = 150
Derivation 2
Word A motorist drove 2 hours at one speed and then for 3 hours at another speed. He
problem covered a distance of 252 kilometers. If he had traveled 4 hours at the first speed and
1 hour at the second speed, he would have covered 244 kilometers. Find two speeds?
Aligned n1 × u1 1 + n2 × u1 2 − n3 = 0 n4 × u2 1 + n5 × u2 2 − n6 = 0
template
Instantiated 2x + 3y − 252 = 0 4x + 1y − 244 = 0
equations
Answer x = 48
y = 52
</table>
<figureCaption confidence="0.875804">
Figure 2: Two complete derivations for two different word problems. Derivation 1 shows an alignment
</figureCaption>
<bodyText confidence="0.896948666666667">
where two instances of the same slot are aligned to the same word (e.g., u11 and u21 both are aligned to
“Tickets”). Derivation 2 includes an alignment where four identical nouns are each aligned to different
slot instances in the template (e.g., the first “speed” in the problem is aligned to u11).
</bodyText>
<sectionHeader confidence="0.913245" genericHeader="method">
3 Mapping Word Problems to Equations
</sectionHeader>
<bodyText confidence="0.999947923076923">
We define a two step process to map word prob-
lems to equations. First, a template is selected
to define the overall structure of the equation sys-
tem. Next, the template is instantiated with num-
bers and nouns from the text. During inference we
consider these two steps jointly.
Figure 2 shows both steps for two derivations.
The template dictates the form of the equations in
the system and the type of slots in each: u slots
represent unknowns and n slots are for numbers
that must be filled from the text. In Derivation 1,
the selected template has two unknown slots, u1
and u2, and four number slots, n1 to n4. Slots
can be shared between equations, for example, the
unknown slots u1 and u2 in the example appear
in both equations. A slot may have different in-
stances, for example u11 and u21 are the two in-
stances of u1 in the example.
We align each slot instance to a word in the
problem. Each number slot n is aligned to a num-
ber, and each unknown slot u is aligned to a noun.
For example, Derivation 1 aligns the number 278
to n1, 1.50 to n2, 4 to n3, and 792 to n4. It also
aligns both instances of u1 (e.g., u11 and u21) to
“Tickets”, and both instances of u2 to “tickets”.
In contrast, in Derivation 2, instances of the same
unknown slot (e.g. u11 and u21) are aligned to two
different words in the problem (different occur-
rences of the word “speed”). This allows for a
tighter mapping between the natural language and
the system template, where the words aligned to
the first equation in the template come from the
first two sentences, and the words aligned to the
second equation come from the third.
Given an alignment, the template can then be
instantiated: each number slot n is replaced with
the aligned number, and each unknown slot u with
a variable. This output system of equations is then
automatically solved to generate the final answer.
</bodyText>
<page confidence="0.988721">
273
</page>
<subsectionHeader confidence="0.988983">
3.1 Derivations
</subsectionHeader>
<bodyText confidence="0.9997434">
Definitions Let X be the set of all word problems.
A word problem x ∈ X is a sequence of k words
hw1,... wki. Also, define an equation template t
to be a formula A = B, where A and B are expres-
sions. An expression A is one of the following:
</bodyText>
<listItem confidence="0.9589876">
• A number constant f.
• A number slot n.
• An unknown slot u.
• An application of a mathematical relation R
to two expressions (e.g., n1 × u1).
</listItem>
<bodyText confidence="0.999834710526316">
We define a system template T to be a set of l
equation templates {t0, ... , tl}. T is the set of
all system templates. A slot may occur more than
once in a system template, to allow variables to
be reused in different equations. We denote a spe-
cific instance i of a slot, u for example, as ui. For
brevity, we omit the instance index when a slot ap-
pears only once. To capture a correspondence be-
tween the text of x and a template T, we define an
alignment p to be a set of pairs (w, s), where w is
a token in x and s is a slot instance in T.
Given the above definitions, an equation e can
be constructed from a template t where each num-
ber slot n is replaced with a real number, each un-
known slot u is replaced with a variable, and each
number constant f is kept as is. We call the pro-
cess of turning a template into an equation tem-
plate instantiation. Similarly, an equation system
E is a set of l equations {e0, ... , el}, which can
be constructed by instantiating each of the equa-
tion templates in a system template T. Finally, an
answer a is a tuple of real numbers.
We define a derivation y from a word problem
to an answer as a tuple (T, p, a), where T is the se-
lected system template, p is an alignment between
T and x, and a is the answer generated by instan-
tiating T using x through p and solving the gener-
ated equations. Let Y be the set of all derivations.
The Space of Possible Derivations We aim to
map each word problem x to an equation system
E. The space of equation systems considered is
defined by the set of possible system templates T
and the words in the original problem x, that are
available for filling slots. In practice, we gener-
ate T from the training data, as described in Sec-
tion 4.1. Given a system template T ∈ T, we
create an alignment p between T and x. The set
of possible alignment pairs is constrained as fol-
</bodyText>
<equation confidence="0.9727215">
u11 + u12 − n1 = 0
n2 × u21 + n3 × u22 − n4 = 0
</equation>
<figureCaption confidence="0.865527">
Figure 3: The first example problem and selected
</figureCaption>
<bodyText confidence="0.9760644">
system template from Figure 2 with all potential
aligned words marked. Nouns (boldfaced) may be
aligned to unknown slot instances uji , and num-
ber words (highlighted) may be aligned to number
slots ni.
lows: each number slot n ∈ T can be aligned to
any number in the text, a number word can only
be aligned to a single slot n, and must be aligned
to all instances of that slot. Additionally, an un-
known slot instance u ∈ T can only be aligned to
a noun word. A complete derivation’s alignment
pairs all slots in T with words in x.
Figure 3 illustrates the space of possible align-
ments for the first problem and system template
from Figure 2. Nouns (shown in boldface) can
be aligned to any of the unknown slot instances
in the selected template (u11, ui, u12, and u22 for the
template selected). Numbers (highlighted) can be
aligned to any of the number slots (n1, n2, n3, and
n4 in the template).
</bodyText>
<subsectionHeader confidence="0.994641">
3.2 Probabilistic Model
</subsectionHeader>
<bodyText confidence="0.9989865">
Due to the ambiguity in selecting the system tem-
plate and alignment, there will be many possible
derivations y ∈ Y for each word problem x ∈ X.
We discriminate between competing analyses us-
ing a log-linear model, which has a feature func-
tion φ : X × Y → Rd and a parameter vector
θ ∈ Rd. The probability of a derivation y given a
problem x is defined as:
</bodyText>
<equation confidence="0.9827945">
p(y|x; θ) = eθ·φ(x,y�)
y�EY
</equation>
<bodyText confidence="0.994093">
Section 6 defines the full set of features used.
The inference problem at test time requires us
to find the most likely answer a given a problem
An amusement park sells 2 kinds of tickets.
Tickets for children cost $ 1.50 . Adult tick-
ets cost $ 4 . On a certain day, 278 people
entered the park. On that same day the ad-
mission fees collected totaled $ 792 . How
many children were admitted on that day?
How many adults were admitted?
</bodyText>
<equation confidence="0.629594">
eθ·φ(x,y)
</equation>
<page confidence="0.921453">
274
</page>
<bodyText confidence="0.893416">
x, assuming the parameters 0 are known:
</bodyText>
<equation confidence="0.872595">
f(x) = arg max
a
</equation>
<bodyText confidence="0.9993445">
Here, the probability of the answer is marginalized
over template selection and alignment:
</bodyText>
<equation confidence="0.977891666666667">
p(a|x; 0) = � p(y|x; 0) (1)
yEY
s.t. AN(y)=a
</equation>
<bodyText confidence="0.9999658">
where AN(y) extracts the answer a out of deriva-
tion y. In this way, the distribution over deriva-
tions y is modeled as a latent variable. We use a
beam search inference procedure to approximately
compute Equation 1, as described in Section 5.
</bodyText>
<sectionHeader confidence="0.995161" genericHeader="method">
4 Learning
</sectionHeader>
<bodyText confidence="0.999995">
To learn our model, we need to induce the struc-
ture of system templates in T and estimate the
model parameters 0.
</bodyText>
<subsectionHeader confidence="0.995597">
4.1 Template Induction
</subsectionHeader>
<bodyText confidence="0.999953230769231">
It is possible to generate system templates T when
provided access to a set of n training examples
{(xi, Ei) : i = 1, ... , n}, where xi is a word
problem and Ei is a set of equations. We general-
ize each E to a system template T by (a) replacing
each variable with an unknown slot, and (b) re-
placing each number mentioned in the text with a
number slot. Numbers not mentioned in the prob-
lem text remain in the template as constants. This
allows us to solve problems that require numbers
that are implied by the problem semantics rather
than appearing directly in the text, such as the per-
cent problem in Figure 4.
</bodyText>
<subsectionHeader confidence="0.987106">
4.2 Parameter Estimation
</subsectionHeader>
<bodyText confidence="0.900993956521739">
For parameter estimation, we assume access to
n training examples {(xi, Vi) : i = 1, ... , n},
each containing a word problem xi and a val-
idation function Vi. The validation function
V : Y → {0, 1} maps a derivation y E Y to 1 if
it is correct, or 0 otherwise.
We can vary the validation function to learn
from different types of supervision. In Sec-
tion 8, we will use validation functions that check
whether the derivation y has either (1) the cor-
rect system of equations E, or (2) the correct an-
swer a. Also, using different types of validation
functions on different subsets of the data enables
semi-supervised learning. This approach is related
to Artzi and Zettlemoyer (2013).
Figure 4: During template induction, we automat-
ically detect the numbers in the problem (high-
lighted above) to generalize the labeled equations
to templates. Numbers not present in the text are
considered part of the induced template.
We estimate 0 by maximizing the conditional
log-likelihood of the data, marginalizing over all
valid derivations:
</bodyText>
<equation confidence="0.995634333333333">
O = � � log p(y|xi; 0)
i yEY
s.t. Vi(y)=1
</equation>
<bodyText confidence="0.999673">
We use L-BFGS (Nocedal and Wright, 2006) to
optimize the parameters. The gradient of the indi-
vidual parameter 0j is given by:
</bodyText>
<equation confidence="0.9928555">
Ep(y|xi,Vi(y)=1;e) [Oj(xi, y)] − (2)
Ep(y|xi;e) [Oj(xi, y)]
</equation>
<bodyText confidence="0.999965">
Section 5 describes how we approximate the
two terms of the gradient using beam search.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="method">
5 Inference
</sectionHeader>
<bodyText confidence="0.977611941176471">
Computing the normalization constant for Equa-
tion 1 requires summing over all templates and all
possible ways to instantiate them. This results in
a search space exponential in the number of slots
in the largest template in T, the set of available
system templates. Therefore, we approximate this
computation using beam search. We initialize the
beam with all templates in T and iteratively align
slots from the templates in the beam to words in
the problem text. For each template, the next slot
Word problem
A chemist has a solution that is 18 % alco-
hol and one that is 50 % alcohol. He wants
to make 80 liters of a 30 % solution. How
many liters of the 18 % solution should he
add? How many liters of the 30 % solution
should he add?
</bodyText>
<equation confidence="0.809645363636364">
Labeled equations
18x0.01xx+50x0.01xy=30x0.01x80
x + y = 80
Induced template system
n1 x 0.01 x ui+ n2 x 0.01 x u12 = n3 x 0.01 x n4
u21 + u22 = n5
p(a|x; 0)
�=
i
∂O
∂0j
</equation>
<page confidence="0.980086">
275
</page>
<bodyText confidence="0.999949">
to be considered is selected according to a pre-
defined canonicalized ordering for that template.
After each iteration we prune the beam to keep the
top-k partial derivations according to the model
score. When pruning the beam, we allow at most l
partial derivations for each template, to ensure that
a small number of templates don’t monopolize the
beam. We continue this process until all templates
in the beam are fully instantiated.
During learning we compute the second term in
the gradient (Equation 2) using our beam search
approximation. Depending on the available vali-
dation function V (as defined in Section 4.2), we
can also accurately prune the beam for the com-
putation of the first half of the gradient. Specifi-
cally, when assuming access to labeled equations,
we can constrain the search to consider only par-
tial hypotheses that could possibly be completed
to produce the labeled equations.
</bodyText>
<sectionHeader confidence="0.9959" genericHeader="method">
6 Model Details
</sectionHeader>
<bodyText confidence="0.996776827586207">
Template Canonicalization There are many syn-
tactically different but semantically equivalent
ways to express a given system of equations. For
example, the phrase “John is 3 years older than
Bill” can be written as j = b + 3 or j − 3 = b.
To avoid such ambiguity, we canonicalize tem-
plates into a normal form representation. We per-
form this canonicalization by obtaining the sym-
bolic solution for the unknown slots in terms of
the number slots and constants using the mathe-
matical solver Maxima (Maxima, 2014).
Slot Signature In a template like s1 +s2 = s3, the
slot s1 is distinct from the slot s2, but we would
like them to share many of the features used in de-
ciding their alignment. To facilitate this, we gener-
ate signatures for each slot and slot pair. The sig-
nature for a slot indicates the system of equations
it appears in, the specific equation it is in, and the
terms of the equation it is a part of. Pairwise slot
signatures concatenate the signatures for the two
slots as well as indicating which terms are shared.
This allows, for example, n2 and n3 in Derivation
1 in Figure 2 to have the same signature, while the
pairs (n2, u1) and (n3, u1) have different ones. To
share features across templates, slot and slot-pair
signatures are generated for both the full template,
as well as for each of the constituent equations.
Features The features O(x, y) are computed for a
derivation y and problem x and cover all deriva-
</bodyText>
<table confidence="0.999850875">
Document level
Unigrams
Bigrams
Single slot
Has the same lemma as a question object
Is a question object
Is in a question sentence
Is equal to one or two (for numbers)
Word lemma X nearby constant
Slot pair
Dep. path contains: Word
Dep. path contains: Dep. Type
Dep. path contains: Word X Dep. Type
Are the same word instance
Have the same lemma
In the same sentence
In the same phrase
Connected by a preposition
Numbers are equal
One number is larger than the other
Equivalent relationship
Solution Features
Is solution all positive
Is solution all integer
</table>
<tableCaption confidence="0.99991">
Table 1: The features divided into categories.
</tableCaption>
<bodyText confidence="0.993753181818182">
tion decisions, including template and alignment
selection. When required, we use standard tools
to generate part-of-speech tags, lematizations, and
dependency parses to compute features.2 For each
number word in y we also identify the closest noun
in the dependency parse. For example, the noun
for 278 in Derivation 1, Figure 2 would be “peo-
ple.” The features are calculated based on these
nouns, rather than the number words.
We use four types of features: document level
features, features that look at a single slot entry,
features that look at pairs of slot entries, and fea-
tures that look at the numeric solutions. Table 1
lists all the features used. Unless otherwise noted,
when computing slot and slot pair features, a sep-
arate feature is generated for each of the signature
types discussed earlier.
Document level features Oftentimes the natural
language in x will contain words or phrases which
are indicative of a certain template, but are not as-
sociated with any of the words aligned to slots in
the template. For example, the word “chemist”
</bodyText>
<footnote confidence="0.910438">
2In our experiments these are generated using the Stan-
ford parser (de Marneffe et al., 2006)
</footnote>
<page confidence="0.997978">
276
</page>
<bodyText confidence="0.999568019607843">
might indicate a template like the one seen in Fig-
ure 4. We include features that connect each tem-
plate with the unigrams and bigrams in the word
problem. We also include an indicator feature for
each system template, providing a bias for its use.
Single Slot Features The natural language x al-
ways contains one or more questions or commands
indicating the queried quantities. For example, the
first problem in Figure 2 asks “How many children
were admitted on that day?” The queried quanti-
ties, the number of children in this case, must be
represented by an unknown in the system of equa-
tions. We generate a set of features which look at
both the word overlap and the noun phrase overlap
between slot words and the objects of a question or
command sentence. We also compute a feature in-
dicating whether a slot is filled from a word in a
question sentence. Additionally, algebra problems
frequently use phrases such as “2 kinds of tickets”
(e.g., Figure 2). These numbers do not typically
appear in the equations. To account for this, we
add a single feature indicating whether a number
is one or two. Lastly, many templates contain con-
stants which are identifiable from words used in
nearby slots. For example, in Figure 4 the con-
stant 0.01 is related to the use of “%” in the text.
To capture such usage, we include a set of lexical-
ized features which concatenate the word lemma
with nearby constants in the equation. These fea-
tures do not include the slot signature.
Slot Pair Features The majority of features we
compute account for relationships between slot
words. This includes features that trigger for
various equivalence relations between the words
themselves, as well as features of the dependency
path between them. We also include features that
look at the numerical relationship of two num-
bers, where the numeric values of the unknowns
are generated by solving the system of equations.
This helps recognize that, for example, the total of
a sum is typically larger than each of the (typically
positive) summands.
Additionally, we also have a single feature look-
ing at shared relationships between pairs of slots.
For example, in Figure 2 the relationship between
“tickets for children” and “$1.50” is “cost”. Sim-
ilarly the relationship between “Adult tickets” and
“$4” is also “cost”. Since the actual nature of this
relationship is not important, this feature is not
lexicalized, instead it is only triggered for the pres-
ence of equality. We consider two cases: subject-
</bodyText>
<table confidence="0.9994609">
# problems 514
# sentences 1616
# words 19357
Vocabulary size 2352
Mean words per problem 37
Mean sentences per problem 3.1
Mean nouns per problem 13.4
# unique equation systems 28
Mean slots per system 7
Mean derivations per problem 4M
</table>
<tableCaption confidence="0.999296">
Table 2: Dataset statistics.
</tableCaption>
<bodyText confidence="0.9994924">
object relationships where the intervening verb
is equal, and noun-to-preposition object relation-
ships where the intervening preposition is equal.
Solution Features By grounding our semantics in
math, we are able to include features which look
at the final answer, a, to learn which answers are
reasonable for the algebra problems we typically
see. For example, the solution to many, but not all,
of the problems involves the size of some set of
objects which must be both positive and integer.
</bodyText>
<sectionHeader confidence="0.997824" genericHeader="method">
7 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999988269230769">
Dataset We collected a new dataset of alge-
bra word problems from Algebra.com, a crowd-
sourced tutoring website. The questions were
posted by students for members of the community
to respond with solutions. Therefore, the problems
are highly varied, and are taken from real prob-
lems given to students. We heuristically filtered
the data to get only linear algebra questions which
did not require any explicit background knowl-
edge. From these we randomly chose a set of
1024 questions. As the questions are posted to a
web forum, the posts often contained additional
comments which were not part of the word prob-
lems and the solutions are embedded in long free-
form natural language descriptions. To clean the
data we asked Amazon Mechanical Turk workers
to extract from the text: the algebra word prob-
lem itself, the solution equations, and the numeric
answer. We manually verified both the equations
and the numbers to ensure they were correct. To
ensure each problem type is seen at least a few
times in the training data, we removed the infre-
quent problem types. Specifically, we induced the
system template from each equation system, as de-
scribed in Section 4.1, and removed all problems
for which the associated system template appeared
</bodyText>
<page confidence="0.990374">
277
</page>
<bodyText confidence="0.999044357142857">
less than 6 times in the dataset. This left us with
514 problems. Table 2 provides the data statistics.
Forms of Supervision We consider both semi-
supervised and supervised learning. In the semi-
supervised scenario, we assume access to the nu-
merical answers of all problems in the training cor-
pus and to a small number of problems paired with
full equation systems. To select which problems
to annotate with equations, we identified the five
most common types of questions in the data and
annotated a randomly sampled question of each
type. 5EQ+ANS uses this form of weak supervi-
sion. To show the benefit of using the weakly su-
pervised data, we also provide results for a base-
line scenario 5EQ, where the training data includes
only the five seed questions annotated with equa-
tion systems. In the fully supervised scenario
ALLEQ, we assume access to full equation sys-
tems for the entire training set.
Evaluation Protocol We run all our experiments
using 5-fold cross-validation. Since our model
generates a solution for every problem, we report
only accuracy. We report two metrics: equation
accuracy to measure how often the system gener-
ates the correct equation system, and answer accu-
racy to evaluate how often the generated numerical
answer is correct. When comparing equations, we
avoid spurious differences by canonicalizing the
equation system, as described in Section 6. To
compare answer tuples we disregard the ordering
and require each number appearing in the refer-
ence answer to appear in the generated answer.
Parameters and Solver In our experiments we set
k in our beam search algorithm (Section 5) to 200,
and l to 20. We run the L-BFGS computation for
50 iterations. We regularize our learning objec-
tive using the L2-norm and a A value of 0.1. The
set of mathematical relations supported by our im-
plementation is {+, −, x, /}.Our implementation
uses the Gaussian Elimination function in the Effi-
cient Java Matrix Library (EJML) (Abeles, 2014)
to generate answers given a set of equations.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="evaluation">
8 Results
</sectionHeader>
<subsectionHeader confidence="0.999933">
8.1 Impact of Supervision
</subsectionHeader>
<bodyText confidence="0.999472">
Table 3 summarizes the results. As expected, hav-
ing access to the full system of equations (ALLEQ)
at training time results in the best learned model,
with nearly 69% accuracy. However, training
from primarily answer annotations (5EQ+ANS)
</bodyText>
<table confidence="0.9986258">
Equation Answer
accuracy accuracy
5EQ 20.4 20.8
5EQ+ANS 45.7 46.1
ALLEQ 66.1 68.7
</table>
<tableCaption confidence="0.9988155">
Table 3: Cross-validation accuracy results for var-
ious forms of supervision.
</tableCaption>
<table confidence="0.999961166666667">
Equation Answer % of
accuracy accuracy data
&lt; 10 43.6 50.8 25.5
11 − 15 46.6 45.1 10.5
16 − 20 44.2 52.0 11.3
&gt; 20 85.7 86.1 52.7
</table>
<tableCaption confidence="0.9899575">
Table 4: Performance on different template fre-
quencies for ALLEQ.
</tableCaption>
<bodyText confidence="0.9788035">
results in performance which is almost 70% of
ALLEQ, demonstrating the value of weakly super-
vised data. In contrast, 5EQ, which cannot use this
weak supervision, performs much worse.
</bodyText>
<subsectionHeader confidence="0.993164">
8.2 Performance and Template Frequency
</subsectionHeader>
<bodyText confidence="0.999996428571429">
To better understand the results, we also measured
equation accuracy as a function of the frequency
of each equation template in the data set. Table 4
reports results for ALLEQ after grouping the prob-
lems into four different frequency bins. We can
see that the system correctly answers more than
85% of the question types which occur frequently
while still achieving more than 50% accuracy on
those that occur relatively infrequently. We do not
include template frequency results for 5EQ+ANS
since in this setup our system is given only the top
five most common templates. This limited set of
templates covers only those questions in the &gt; 20
bin, or about 52% of the data. However, on this
subset 5EQ+ANS performs very well, answering
88% of them correctly, which is approximately the
same as the 86% achieved by ALLEQ. Thus while
the weak supervision is not helpful in generating
the space of possible equations, it is very helpful
in learning to generate the correct answer when
given an appropriate space of equations.
</bodyText>
<subsectionHeader confidence="0.999675">
8.3 Ablation Analysis
</subsectionHeader>
<bodyText confidence="0.99997">
Table 5 shows ablation results for each group of
features. The results along the diagonal show the
performance when a single group of features is
ablated, while the off-diagonal numbers show the
</bodyText>
<page confidence="0.99249">
278
</page>
<table confidence="0.999594166666667">
w/o w/o w/o w/o
pair document solution single
w/o pair 42.8 25.7 19.0 39.6
w/o document – 63.8 50.4 57.6
w/o solution – – 63.6 62.0
w/o single – – – 65.9
</table>
<tableCaption confidence="0.992766">
Table 5: Cross-validation accuracy results with
</tableCaption>
<bodyText confidence="0.936867571428571">
different feature groups ablated for ALLEQ. Re-
sults are for answer accuracy which is 68.7% with-
out any features ablated.
performance when two groups of features are ab-
lated together. We can see that all of the features
contribute to the overall performance, and that the
pair features are the most important followed by
the document and solution features. We also see
that the pair features can compensate for the ab-
sence of other features. For example, the perfor-
mance drops only slightly when either the docu-
ment or solution features are removed in isolation.
However, the drop is much more dramatic when
they are removed along with the pair features.
</bodyText>
<subsectionHeader confidence="0.990268">
8.4 Qualitative Error Analysis
</subsectionHeader>
<bodyText confidence="0.999524428571429">
We examined our system output on one fold of
ALLEQ and identified two main classes of errors.
The first, accounting for approximately one-
quarter of the cases, includes mistakes where
more background or world knowledge might have
helped. For example, Problem 1 in Figure 5 re-
quires understanding the relation between the di-
mensions of a painting, and how this relation is
maintained when the painting is printed, and Prob-
lem 2 relies on understanding concepts of com-
merce, including cost, sale price, and profit. While
these relationships could be learned in our model
with enough data, as it does for percentage prob-
lems (e.g., Figure 4), various outside resources,
such as knowledge bases (e.g. Freebase) or distri-
butional statistics from a large text corpus, might
help us learn them with less training data.
The second category, which accounts for about
half of the errors, includes mistakes that stem from
compositional language. For example, the second
sentence in Problem 3 in Figure 5 could generate
the equation 2x−y = 5, with the phrase “twice of
one of them” generating the expression 2x. Given
the typical shallow nesting, it’s possible to learn
templates for these cases given enough data, and in
the future it might also be possible to develop new,
cross-sentence semantic parsers to enable better
generalization from smaller datasets.
</bodyText>
<figureCaption confidence="0.931872">
Figure 5: Examples of problems our system does
not solve correctly.
</figureCaption>
<sectionHeader confidence="0.997295" genericHeader="conclusions">
9 Conclusion
</sectionHeader>
<bodyText confidence="0.999977315789474">
We presented an approach for automatically learn-
ing to solve algebra word problems. Our algorithm
constructs systems of equations, while aligning
their variables and numbers to the problem text.
Using a newly gathered corpus we measured the
effects of various forms of weak supervision on
performance. To the best of our knowledge, we
present the first learning result for this task.
There are still many opportunities to improve
the reported results, and extend the approach to
related domains. We would like to develop tech-
niques to learn compositional models of mean-
ing for generating new equations. Furthermore,
the general representation of mathematics lends it-
self to many different domains including geome-
try, physics, and chemistry. Eventually, we hope
to extend the techniques to synthesize even more
complex structures, such as computer programs,
from natural language.
</bodyText>
<sectionHeader confidence="0.998811" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9210484">
The authors acknowledge the support of Battelle
Memorial Institute (PO#300662) and NSF (grant
IIS-0835652). We thank Nicholas FitzGerald, the
MIT NLP group, the UW NLP group and the
ACL reviewers for their suggestions and com-
ments. Any opinions, findings, conclusions, or
recommendations expressed in this paper are those
of the authors, and do not necessarily reflect the
views of the funding organizations.
A painting is 10 inches tall and 15 inches
(1) wide. A print of the painting is 25 inches
tall, how wide is the print in inches?
A textbook costs a bookstore 44 dollars,
and the store sells it for 55 dollars. Find
the amount of profit based on the selling
price.
The sum of two numbers is 85. The dif-
(3) ference of twice of one of them and the
other one is 5. Find both numbers.
The difference between two numbers is
</bodyText>
<figure confidence="0.686262">
(4) 6. If you double both numbers, the sum
is 36. Find the two numbers.
(2)
</figure>
<page confidence="0.870184">
279
</page>
<note confidence="0.8101812">
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In
Proceedings of the Conference on Language Re-
sources and Evaluation.
</note>
<sectionHeader confidence="0.293513" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.443472333333333">
Peter Abeles. 2014. Efficient java matrix library.
https://code.google.com/p/efficient
-java-matrix-library/.
</bodyText>
<reference confidence="0.994344272727273">
Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrap-
ping semantic parsers from conversations. In Pro-
ceedings of the Conference on Empirical Methods
in Natural Language Processing.
Yoav Artzi and Luke Zettlemoyer. 2013. Weakly su-
pervised learning of semantic parsers for mapping
instructions to actions. Transactions of the Associa-
tion for Computational Linguistics.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on freebase from
question-answer pairs. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and
Regina Barzilay. 2009. Reinforcement learning for
mapping instructions to actions. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
S.R.K Branavan, Luke Zettlemoyer, and Regina Barzi-
lay. 2010. Reading between the lines: Learning to
map high-level instructions to commands. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics.
Qingqing Cai and Alexander Yates. 2013a. Large-
scale semantic parsing via schema matching and lex-
icon extension. In Proceedings of the Annual Meet-
ing of the Association for Computational Linguis-
tics.
Qingqing Cai and Alexander Yates. 2013b. Seman-
tic parsing freebase: Towards open-domain seman-
tic parsing. In Proceedings of the Joint Conference
on Lexical and Computational Semantics.
Nathanael Chambers and Dan Jurafsky. 2011.
Template-based information extraction without the
templates. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics.
David Chen and Raymond Mooney. 2011. Learning
to interpret natural language navigation instructions
from observations. In Proceedings of the Confer-
ence on Artificial Intelligence.
David Chen. 2012. Fast online lexicon learning for
grounded language acquisition. In Proceedings of
the Annual Meeting of the Association for Computa-
tional Linguistics.
James Clarke, Dan Goldwasser, Ming-Wei Chang, and
Dan Roth. 2010. Driving semantic parsing from the
world’s response. In Proceedings of the Conference
on Computational Natural Language Learning. As-
sociation for Computational Linguistics.
Ralph Grishman, David Westbrook, and Adam Mey-
ers. 2005. NYUs English ACE 2005 System De-
scription. In Proceedings of the Automatic Content
Extraction Evaluation Workshop.
Heng Ji and Ralph Grishman. 2008. Refining event ex-
traction through cross-document inference. In Pro-
ceedings of the Annual Meeting of the Association
for Computational Linguistics.
Joohyun Kim and Raymond Mooney. 2012. Unsuper-
vised pcfg induction for grounded language learning
with highly ambiguous supervision. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Nate Kushman and Regina Barzilay. 2013. Using se-
mantic unification to generate regular expressions
from natural language. In Proceeding of the Annual
Meeting of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-
ter, and Mark Steedman. 2010. Inducing proba-
bilistic ccg grammars from logical form with higher-
order unification. In Proceedings of the Conference
on Empirical Methods on Natural Language Pro-
cessing.
Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke
Zettlemoyer. 2013. Scaling semantic parsers with
on-the-fly ontology matching. In Proceedings of
Empirical Methods in Natural Language Process-
ing.
Tao Lei, Fan Long, Regina Barzilay, and Martin Ri-
nard. 2013. From natural language specifications to
program input parsers. In Proceeding of the Associ-
ation for Computational Linguistics.
Iddo Lev, Bill MacCartney, Christopher Manning, and
Roger Levy. 2004. Solving logic puzzles: From
robust processing to precise semantics. In Proceed-
ings of the Workshop on Text Meaning and Interpre-
tation. Association for Computational Linguistics.
Mstislav Maslennikov and Tat-Seng Chua. 2007. A
multi-resolution framework for information extrac-
tion from free text. In Proceedings of the Annual
Meeting of the Association for Computational Lin-
guistics.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A joint
model of language and perception for grounded at-
tribute learning. In Proceedings of the International
Conference on Machine Learning.
Maxima. 2014. Maxima, a computer algebra system.
version 5.32.1.
</reference>
<page confidence="0.951425">
280
</page>
<reference confidence="0.999639113636363">
Anirban Mukherjee and Utpal Garain. 2008. A review
of methods for automatic understanding of natural
language mathematical problems. Artificial Intelli-
gence Review, 29(2).
Jorge Nocedal and Stephen Wright. 2006. Numeri-
cal optimization, series in operations research and
financial engineering. Springer, New York.
Roi Reichart and Regina Barzilay. 2012. Multi-event
extraction guided by global constraints. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics.
Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.
2012. Open domain event extraction from twitter. In
Proceedings of the Conference on Knowledge Dis-
covery and Data Mining.
Cynthia Thompson and Raymond Mooney. 2003.
Acquiring word-meaning mappings for natural lan-
guage interfaces. Journal of Artificial Intelligence
Research, 18(1).
Adam Vogel and Dan Jurafsky. 2010. Learning to
follow navigational directions. In Proceedings of
the Annual Meeting of the Association for Compu-
tational Linguistics.
Yuk Wah Wong and Raymond Mooney. 2006. Learn-
ing for semantic parsing with statistical machine
translation. In Proceedings of the Annual Meeting
of the North American Chapter of the Association of
Computational Linguistics. Association for Compu-
tational Linguistics.
John Zelle and Raymond Mooney. 1996. Learning
to parse database queries using inductive logic pro-
gramming. In Proceedings of the Conference on Ar-
tificial Intelligence.
Luke Zettlemoyer and Michael Collins. 2005. Learn-
ing to map sentences to logical form: Structured
classification with probabilistic categorial gram-
mars. In Proceedings of the Conference on Uncer-
tainty in Artificial Intelligence.
Luke Zettlemoyer and Michael Collins. 2009. Learn-
ing context-dependent mappings from sentences to
logical form. In Proceedings of the Joint Confer-
ence of the Association for Computational Linguis-
tics and International Joint Conference on Natural
Language Processing.
</reference>
<page confidence="0.997869">
281
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.343230">
<title confidence="0.999351">Learning to Automatically Solve Algebra Word Problems</title>
<author confidence="0.673642">Yoav Luke</author>
<author confidence="0.673642">Regina</author>
<affiliation confidence="0.390635">Science and Articial Intelligence Laboratory, Massachusetts Institute of Science &amp; Engineering, University of</affiliation>
<abstract confidence="0.999904764705882">We present an approach for automatically learning to solve algebra word problems. Our algorithm reasons across sentence boundaries to construct and solve a system of linear equations, while simultaneously recovering an alignment of the variables and numbers in these equations to the problem text. The learning algorithm uses varied supervision, including either full equations or just the final answers. We evaluate performance on a newly gathered corpus of algebra word problems, demonstrating that the system can correctly answer almost 70% of the questions in the dataset. This is, to our knowledge, the first learning result for this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Bootstrapping semantic parsers from conversations.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6028" citStr="Artzi and Zettlemoyer, 2011" startWordPosition="945" endWordPosition="948">rnal world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques that consider multiple sentences typically do so in a serial fashion, processing each in turn with limited cross-sent</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2011</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2011. Bootstrapping semantic parsers from conversations. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="6157" citStr="Artzi and Zettlemoyer, 2013" startWordPosition="965" endWordPosition="968">re costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques that consider multiple sentences typically do so in a serial fashion, processing each in turn with limited cross-sentence reasoning (Branavan et al., 2009; Zettlemoyer and Collins, 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). We foc</context>
<context position="17538" citStr="Artzi and Zettlemoyer (2013)" startWordPosition="3098" endWordPosition="3101">ning examples {(xi, Vi) : i = 1, ... , n}, each containing a word problem xi and a validation function Vi. The validation function V : Y → {0, 1} maps a derivation y E Y to 1 if it is correct, or 0 otherwise. We can vary the validation function to learn from different types of supervision. In Section 8, we will use validation functions that check whether the derivation y has either (1) the correct system of equations E, or (2) the correct answer a. Also, using different types of validation functions on different subsets of the data enables semi-supervised learning. This approach is related to Artzi and Zettlemoyer (2013). Figure 4: During template induction, we automatically detect the numbers in the problem (highlighted above) to generalize the labeled equations to templates. Numbers not present in the text are considered part of the induced template. We estimate 0 by maximizing the conditional log-likelihood of the data, marginalizing over all valid derivations: O = � � log p(y|xi; 0) i yEY s.t. Vi(y)=1 We use L-BFGS (Nocedal and Wright, 2006) to optimize the parameters. The gradient of the individual parameter 0j is given by: Ep(y|xi,Vi(y)=1;e) [Oj(xi, y)] − (2) Ep(y|xi;e) [Oj(xi, y)] Section 5 describes h</context>
</contexts>
<marker>Artzi, Zettlemoyer, 2013</marker>
<rawString>Yoav Artzi and Luke Zettlemoyer. 2013. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Andrew Chou</author>
<author>Roy Frostig</author>
<author>Percy Liang</author>
</authors>
<title>Semantic parsing on freebase from question-answer pairs.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5955" citStr="Berant et al., 2013" startWordPosition="935" endWordPosition="938">sed to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques that consider multiple sentences typically d</context>
</contexts>
<marker>Berant, Chou, Frostig, Liang, 2013</marker>
<rawString>Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Harr Chen</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reinforcement learning for mapping instructions to actions.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5433" citStr="Branavan et al., 2009" startWordPosition="853" endWordPosition="857">urthermore, we find the algorithm can robustly handle weak supervision, achieving more than 70% of the above performance when trained exclusively on answers. 2 Related Work Our work is related to three main areas of research: situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), rob</context>
<context position="6665" citStr="Branavan et al., 2009" startWordPosition="1046" endWordPosition="1049">ion (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques that consider multiple sentences typically do so in a serial fashion, processing each in turn with limited cross-sentence reasoning (Branavan et al., 2009; Zettlemoyer and Collins, 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). We focus on analyzing multiple sentences simultaneously, as is necessary to generate the global semantic representations common in domains such as algebra word problems. Information Extraction Our approach is related to work on template-based information extraction, where the goal is to identify instances of event templates in text and extract their slot fillers. Most work has focused on the supervised case, where the templates are manually defined and data is labeled with alignment information, e.g. (Grishma</context>
</contexts>
<marker>Branavan, Chen, Zettlemoyer, Barzilay, 2009</marker>
<rawString>S.R.K Branavan, Harr Chen, Luke Zettlemoyer, and Regina Barzilay. 2009. Reinforcement learning for mapping instructions to actions. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S R K Branavan</author>
<author>Luke Zettlemoyer</author>
<author>Regina Barzilay</author>
</authors>
<title>Reading between the lines: Learning to map high-level instructions to commands.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5456" citStr="Branavan et al., 2010" startWordPosition="858" endWordPosition="861"> algorithm can robustly handle weak supervision, achieving more than 70% of the above performance when trained exclusively on answers. 2 Related Work Our work is related to three main areas of research: situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen an</context>
</contexts>
<marker>Branavan, Zettlemoyer, Barzilay, 2010</marker>
<rawString>S.R.K Branavan, Luke Zettlemoyer, and Regina Barzilay. 2010. Reading between the lines: Learning to map high-level instructions to commands. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Largescale semantic parsing via schema matching and lexicon extension.</title>
<date>2013</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5911" citStr="Cai and Yates, 2013" startWordPosition="927" endWordPosition="930">supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques </context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013a. Largescale semantic parsing via schema matching and lexicon extension. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qingqing Cai</author>
<author>Alexander Yates</author>
</authors>
<title>Semantic parsing freebase: Towards open-domain semantic parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="5911" citStr="Cai and Yates, 2013" startWordPosition="927" endWordPosition="930">supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques </context>
</contexts>
<marker>Cai, Yates, 2013</marker>
<rawString>Qingqing Cai and Alexander Yates. 2013b. Semantic parsing freebase: Towards open-domain semantic parsing. In Proceedings of the Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Template-based information extraction without the templates.</title>
<date>2011</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7495" citStr="Chambers and Jurafsky, 2011" startWordPosition="1173" endWordPosition="1176">ons common in domains such as algebra word problems. Information Extraction Our approach is related to work on template-based information extraction, where the goal is to identify instances of event templates in text and extract their slot fillers. Most work has focused on the supervised case, where the templates are manually defined and data is labeled with alignment information, e.g. (Grishman et al., 2005; Maslennikov and Chua, 2007; Ji and Grishman, 2008; Reichart and Barzilay, 2012). However, some recent work has studied the automatic induction of the set of possible templates from data (Chambers and Jurafsky, 2011; Ritter et al., 2012). In our approach, systems of equations are relatively easy to specify, providing a type of template structure, and the alignment of the slots in these templates to the text is modeled primarily with latent variables during learning. Additionally, mapping to a semantic representation that can be executed allows us to leverage weaker supervision during learning. Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve m</context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Template-based information extraction without the templates. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chen</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning to interpret natural language navigation instructions from observations.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="6070" citStr="Chen and Mooney, 2011" startWordPosition="951" endWordPosition="954">., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques that consider multiple sentences typically do so in a serial fashion, processing each in turn with limited cross-sentence reasoning (Branavan et al., 2009; Zet</context>
</contexts>
<marker>Chen, Mooney, 2011</marker>
<rawString>David Chen and Raymond Mooney. 2011. Learning to interpret natural language navigation instructions from observations. In Proceedings of the Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chen</author>
</authors>
<title>Fast online lexicon learning for grounded language acquisition.</title>
<date>2012</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6082" citStr="Chen, 2012" startWordPosition="955" endWordPosition="956">fsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques that consider multiple sentences typically do so in a serial fashion, processing each in turn with limited cross-sentence reasoning (Branavan et al., 2009; Zettlemoyer and</context>
</contexts>
<marker>Chen, 2012</marker>
<rawString>David Chen. 2012. Fast online lexicon learning for grounded language acquisition. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Clarke</author>
<author>Dan Goldwasser</author>
<author>Ming-Wei Chang</author>
<author>Dan Roth</author>
</authors>
<title>Driving semantic parsing from the world’s response.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5890" citStr="Clarke et al., 2010" startWordPosition="923" endWordPosition="926">iven varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in i</context>
</contexts>
<marker>Clarke, Goldwasser, Chang, Roth, 2010</marker>
<rawString>James Clarke, Dan Goldwasser, Ming-Wei Chang, and Dan Roth. 2010. Driving semantic parsing from the world’s response. In Proceedings of the Conference on Computational Natural Language Learning. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Grishman</author>
<author>David Westbrook</author>
<author>Adam Meyers</author>
</authors>
<title>NYUs English ACE</title>
<date>2005</date>
<booktitle>In Proceedings of the Automatic Content Extraction Evaluation Workshop.</booktitle>
<contexts>
<context position="7279" citStr="Grishman et al., 2005" startWordPosition="1139" endWordPosition="1142">., 2009; Zettlemoyer and Collins, 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). We focus on analyzing multiple sentences simultaneously, as is necessary to generate the global semantic representations common in domains such as algebra word problems. Information Extraction Our approach is related to work on template-based information extraction, where the goal is to identify instances of event templates in text and extract their slot fillers. Most work has focused on the supervised case, where the templates are manually defined and data is labeled with alignment information, e.g. (Grishman et al., 2005; Maslennikov and Chua, 2007; Ji and Grishman, 2008; Reichart and Barzilay, 2012). However, some recent work has studied the automatic induction of the set of possible templates from data (Chambers and Jurafsky, 2011; Ritter et al., 2012). In our approach, systems of equations are relatively easy to specify, providing a type of template structure, and the alignment of the slots in these templates to the text is modeled primarily with latent variables during learning. Additionally, mapping to a semantic representation that can be executed allows us to leverage weaker supervision during learning</context>
</contexts>
<marker>Grishman, Westbrook, Meyers, 2005</marker>
<rawString>Ralph Grishman, David Westbrook, and Adam Meyers. 2005. NYUs English ACE 2005 System Description. In Proceedings of the Automatic Content Extraction Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heng Ji</author>
<author>Ralph Grishman</author>
</authors>
<title>Refining event extraction through cross-document inference.</title>
<date>2008</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7330" citStr="Ji and Grishman, 2008" startWordPosition="1147" endWordPosition="1150">oney, 2011; Artzi and Zettlemoyer, 2013). We focus on analyzing multiple sentences simultaneously, as is necessary to generate the global semantic representations common in domains such as algebra word problems. Information Extraction Our approach is related to work on template-based information extraction, where the goal is to identify instances of event templates in text and extract their slot fillers. Most work has focused on the supervised case, where the templates are manually defined and data is labeled with alignment information, e.g. (Grishman et al., 2005; Maslennikov and Chua, 2007; Ji and Grishman, 2008; Reichart and Barzilay, 2012). However, some recent work has studied the automatic induction of the set of possible templates from data (Chambers and Jurafsky, 2011; Ritter et al., 2012). In our approach, systems of equations are relatively easy to specify, providing a type of template structure, and the alignment of the slots in these templates to the text is modeled primarily with latent variables during learning. Additionally, mapping to a semantic representation that can be executed allows us to leverage weaker supervision during learning. Automatic Word Problem Solvers Finally, there has</context>
</contexts>
<marker>Ji, Grishman, 2008</marker>
<rawString>Heng Ji and Ralph Grishman. 2008. Refining event extraction through cross-document inference. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joohyun Kim</author>
<author>Raymond Mooney</author>
</authors>
<title>Unsupervised pcfg induction for grounded language learning with highly ambiguous supervision.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6104" citStr="Kim and Mooney, 2012" startWordPosition="957" endWordPosition="960"> Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques that consider multiple sentences typically do so in a serial fashion, processing each in turn with limited cross-sentence reasoning (Branavan et al., 2009; Zettlemoyer and Collins, 2009; Chen a</context>
</contexts>
<marker>Kim, Mooney, 2012</marker>
<rawString>Joohyun Kim and Raymond Mooney. 2012. Unsupervised pcfg induction for grounded language learning with highly ambiguous supervision. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nate Kushman</author>
<author>Regina Barzilay</author>
</authors>
<title>Using semantic unification to generate regular expressions from natural language.</title>
<date>2013</date>
<booktitle>In Proceeding of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6209" citStr="Kushman and Barzilay, 2013" startWordPosition="973" endWordPosition="976">996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques that consider multiple sentences typically do so in a serial fashion, processing each in turn with limited cross-sentence reasoning (Branavan et al., 2009; Zettlemoyer and Collins, 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). We focus on analyzing multiple sentences simultaneously, a</context>
</contexts>
<marker>Kushman, Barzilay, 2013</marker>
<rawString>Nate Kushman and Regina Barzilay. 2013. Using semantic unification to generate regular expressions from natural language. In Proceeding of the Annual Meeting of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Luke Zettlemoyer</author>
<author>Sharon Goldwater</author>
<author>Mark Steedman</author>
</authors>
<title>Inducing probabilistic ccg grammars from logical form with higherorder unification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Conference on Empirical Methods on Natural Language Processing.</booktitle>
<contexts>
<context position="5694" citStr="Kwiatkowski et al., 2010" startWordPosition="896" endWordPosition="899">n, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question</context>
</contexts>
<marker>Kwiatkowski, Zettlemoyer, Goldwater, Steedman, 2010</marker>
<rawString>Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwater, and Mark Steedman. 2010. Inducing probabilistic ccg grammars from logical form with higherorder unification. In Proceedings of the Conference on Empirical Methods on Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Kwiatkowski</author>
<author>Eunsol Choi</author>
<author>Yoav Artzi</author>
<author>Luke Zettlemoyer</author>
</authors>
<title>Scaling semantic parsers with on-the-fly ontology matching.</title>
<date>2013</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="5982" citStr="Kwiatkowski et al., 2013" startWordPosition="939" endWordPosition="942">instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques that consider multiple sentences typically do so in a serial fashion, p</context>
</contexts>
<marker>Kwiatkowski, Choi, Artzi, Zettlemoyer, 2013</marker>
<rawString>Tom Kwiatkowski, Eunsol Choi, Yoav Artzi, and Luke Zettlemoyer. 2013. Scaling semantic parsers with on-the-fly ontology matching. In Proceedings of Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Lei</author>
<author>Fan Long</author>
<author>Regina Barzilay</author>
<author>Martin Rinard</author>
</authors>
<title>From natural language specifications to program input parsers.</title>
<date>2013</date>
<booktitle>In Proceeding of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="6228" citStr="Lei et al., 2013" startWordPosition="977" endWordPosition="980">03; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques that consider multiple sentences typically do so in a serial fashion, processing each in turn with limited cross-sentence reasoning (Branavan et al., 2009; Zettlemoyer and Collins, 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). We focus on analyzing multiple sentences simultaneously, as is necessary to g</context>
</contexts>
<marker>Lei, Long, Barzilay, Rinard, 2013</marker>
<rawString>Tao Lei, Fan Long, Regina Barzilay, and Martin Rinard. 2013. From natural language specifications to program input parsers. In Proceeding of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Iddo Lev</author>
<author>Bill MacCartney</author>
<author>Christopher Manning</author>
<author>Roger Levy</author>
</authors>
<title>Solving logic puzzles: From robust processing to precise semantics.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Text Meaning and Interpretation. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8173" citStr="Lev et al., 2004" startWordPosition="1280" endWordPosition="1283"> are relatively easy to specify, providing a type of template structure, and the alignment of the slots in these templates to the text is modeled primarily with latent variables during learning. Additionally, mapping to a semantic representation that can be executed allows us to leverage weaker supervision during learning. Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee and Garain, 2008; Lev et al., 2004). Our focus is on learning a model for the end-to-end task of solving word problems given only a training corpus of questions paired with equations or answers. 272 Derivation 1 Word An amusement park sells 2 kinds of tickets. Tickets for children cost $ 1.50 . Adult problem tickets cost $ 4 . On a certain day, 278 people entered the park. On that same day the admission fees collected totaled $ 792 . How many children were admitted on that day? How many adults were admitted? Aligned 1 1 2 2 template u1 + u2 − n1 = 0 n2 × u1 + n3 × u2 − n4 = 0 Instantiated x + y − 278 = 0 1.5x + 4y − 792 = 0 equ</context>
</contexts>
<marker>Lev, MacCartney, Manning, Levy, 2004</marker>
<rawString>Iddo Lev, Bill MacCartney, Christopher Manning, and Roger Levy. 2004. Solving logic puzzles: From robust processing to precise semantics. In Proceedings of the Workshop on Text Meaning and Interpretation. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mstislav Maslennikov</author>
<author>Tat-Seng Chua</author>
</authors>
<title>A multi-resolution framework for information extraction from free text.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7307" citStr="Maslennikov and Chua, 2007" startWordPosition="1143" endWordPosition="1146">d Collins, 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). We focus on analyzing multiple sentences simultaneously, as is necessary to generate the global semantic representations common in domains such as algebra word problems. Information Extraction Our approach is related to work on template-based information extraction, where the goal is to identify instances of event templates in text and extract their slot fillers. Most work has focused on the supervised case, where the templates are manually defined and data is labeled with alignment information, e.g. (Grishman et al., 2005; Maslennikov and Chua, 2007; Ji and Grishman, 2008; Reichart and Barzilay, 2012). However, some recent work has studied the automatic induction of the set of possible templates from data (Chambers and Jurafsky, 2011; Ritter et al., 2012). In our approach, systems of equations are relatively easy to specify, providing a type of template structure, and the alignment of the slots in these templates to the text is modeled primarily with latent variables during learning. Additionally, mapping to a semantic representation that can be executed allows us to leverage weaker supervision during learning. Automatic Word Problem Sol</context>
</contexts>
<marker>Maslennikov, Chua, 2007</marker>
<rawString>Mstislav Maslennikov and Tat-Seng Chua. 2007. A multi-resolution framework for information extraction from free text. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Matuszek</author>
<author>Nicholas FitzGerald</author>
<author>Luke Zettlemoyer</author>
<author>Liefeng Bo</author>
<author>Dieter Fox</author>
</authors>
<title>A joint model of language and perception for grounded attribute learning.</title>
<date>2012</date>
<booktitle>In Proceedings of the International Conference on Machine Learning.</booktitle>
<contexts>
<context position="6127" citStr="Matuszek et al., 2012" startWordPosition="961" endWordPosition="964"> relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques that consider multiple sentences typically do so in a serial fashion, processing each in turn with limited cross-sentence reasoning (Branavan et al., 2009; Zettlemoyer and Collins, 2009; Chen and Mooney, 2011; Artzi </context>
</contexts>
<marker>Matuszek, FitzGerald, Zettlemoyer, Bo, Fox, 2012</marker>
<rawString>Cynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A joint model of language and perception for grounded attribute learning. In Proceedings of the International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maxima</author>
</authors>
<title>Maxima, a computer algebra system. version 5.32.1.</title>
<date>2014</date>
<contexts>
<context position="20556" citStr="Maxima, 2014" startWordPosition="3627" endWordPosition="3628">ly partial hypotheses that could possibly be completed to produce the labeled equations. 6 Model Details Template Canonicalization There are many syntactically different but semantically equivalent ways to express a given system of equations. For example, the phrase “John is 3 years older than Bill” can be written as j = b + 3 or j − 3 = b. To avoid such ambiguity, we canonicalize templates into a normal form representation. We perform this canonicalization by obtaining the symbolic solution for the unknown slots in terms of the number slots and constants using the mathematical solver Maxima (Maxima, 2014). Slot Signature In a template like s1 +s2 = s3, the slot s1 is distinct from the slot s2, but we would like them to share many of the features used in deciding their alignment. To facilitate this, we generate signatures for each slot and slot pair. The signature for a slot indicates the system of equations it appears in, the specific equation it is in, and the terms of the equation it is a part of. Pairwise slot signatures concatenate the signatures for the two slots as well as indicating which terms are shared. This allows, for example, n2 and n3 in Derivation 1 in Figure 2 to have the same </context>
</contexts>
<marker>Maxima, 2014</marker>
<rawString>Maxima. 2014. Maxima, a computer algebra system. version 5.32.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anirban Mukherjee</author>
<author>Utpal Garain</author>
</authors>
<title>A review of methods for automatic understanding of natural language mathematical problems.</title>
<date>2008</date>
<journal>Artificial Intelligence Review,</journal>
<volume>29</volume>
<issue>2</issue>
<contexts>
<context position="8154" citStr="Mukherjee and Garain, 2008" startWordPosition="1276" endWordPosition="1279">proach, systems of equations are relatively easy to specify, providing a type of template structure, and the alignment of the slots in these templates to the text is modeled primarily with latent variables during learning. Additionally, mapping to a semantic representation that can be executed allows us to leverage weaker supervision during learning. Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specific domains (Mukherjee and Garain, 2008; Lev et al., 2004). Our focus is on learning a model for the end-to-end task of solving word problems given only a training corpus of questions paired with equations or answers. 272 Derivation 1 Word An amusement park sells 2 kinds of tickets. Tickets for children cost $ 1.50 . Adult problem tickets cost $ 4 . On a certain day, 278 people entered the park. On that same day the admission fees collected totaled $ 792 . How many children were admitted on that day? How many adults were admitted? Aligned 1 1 2 2 template u1 + u2 − n1 = 0 n2 × u1 + n3 × u2 − n4 = 0 Instantiated x + y − 278 = 0 1.5x</context>
</contexts>
<marker>Mukherjee, Garain, 2008</marker>
<rawString>Anirban Mukherjee and Utpal Garain. 2008. A review of methods for automatic understanding of natural language mathematical problems. Artificial Intelligence Review, 29(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge Nocedal</author>
<author>Stephen Wright</author>
</authors>
<title>Numerical optimization, series in operations research and financial engineering.</title>
<date>2006</date>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="17971" citStr="Nocedal and Wright, 2006" startWordPosition="3169" endWordPosition="3172">orrect answer a. Also, using different types of validation functions on different subsets of the data enables semi-supervised learning. This approach is related to Artzi and Zettlemoyer (2013). Figure 4: During template induction, we automatically detect the numbers in the problem (highlighted above) to generalize the labeled equations to templates. Numbers not present in the text are considered part of the induced template. We estimate 0 by maximizing the conditional log-likelihood of the data, marginalizing over all valid derivations: O = � � log p(y|xi; 0) i yEY s.t. Vi(y)=1 We use L-BFGS (Nocedal and Wright, 2006) to optimize the parameters. The gradient of the individual parameter 0j is given by: Ep(y|xi,Vi(y)=1;e) [Oj(xi, y)] − (2) Ep(y|xi;e) [Oj(xi, y)] Section 5 describes how we approximate the two terms of the gradient using beam search. 5 Inference Computing the normalization constant for Equation 1 requires summing over all templates and all possible ways to instantiate them. This results in a search space exponential in the number of slots in the largest template in T, the set of available system templates. Therefore, we approximate this computation using beam search. We initialize the beam wit</context>
</contexts>
<marker>Nocedal, Wright, 2006</marker>
<rawString>Jorge Nocedal and Stephen Wright. 2006. Numerical optimization, series in operations research and financial engineering. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roi Reichart</author>
<author>Regina Barzilay</author>
</authors>
<title>Multi-event extraction guided by global constraints.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="7360" citStr="Reichart and Barzilay, 2012" startWordPosition="1151" endWordPosition="1154">ettlemoyer, 2013). We focus on analyzing multiple sentences simultaneously, as is necessary to generate the global semantic representations common in domains such as algebra word problems. Information Extraction Our approach is related to work on template-based information extraction, where the goal is to identify instances of event templates in text and extract their slot fillers. Most work has focused on the supervised case, where the templates are manually defined and data is labeled with alignment information, e.g. (Grishman et al., 2005; Maslennikov and Chua, 2007; Ji and Grishman, 2008; Reichart and Barzilay, 2012). However, some recent work has studied the automatic induction of the set of possible templates from data (Chambers and Jurafsky, 2011; Ritter et al., 2012). In our approach, systems of equations are relatively easy to specify, providing a type of template structure, and the alignment of the slots in these templates to the text is modeled primarily with latent variables during learning. Additionally, mapping to a semantic representation that can be executed allows us to leverage weaker supervision during learning. Automatic Word Problem Solvers Finally, there has been research on automaticall</context>
</contexts>
<marker>Reichart, Barzilay, 2012</marker>
<rawString>Roi Reichart and Regina Barzilay. 2012. Multi-event extraction guided by global constraints. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Oren Etzioni Mausam</author>
<author>Sam Clark</author>
</authors>
<title>Open domain event extraction from twitter.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Knowledge Discovery and Data Mining.</booktitle>
<contexts>
<context position="7517" citStr="Ritter et al., 2012" startWordPosition="1177" endWordPosition="1180"> algebra word problems. Information Extraction Our approach is related to work on template-based information extraction, where the goal is to identify instances of event templates in text and extract their slot fillers. Most work has focused on the supervised case, where the templates are manually defined and data is labeled with alignment information, e.g. (Grishman et al., 2005; Maslennikov and Chua, 2007; Ji and Grishman, 2008; Reichart and Barzilay, 2012). However, some recent work has studied the automatic induction of the set of possible templates from data (Chambers and Jurafsky, 2011; Ritter et al., 2012). In our approach, systems of equations are relatively easy to specify, providing a type of template structure, and the alignment of the slots in these templates to the text is modeled primarily with latent variables during learning. Additionally, mapping to a semantic representation that can be executed allows us to leverage weaker supervision during learning. Automatic Word Problem Solvers Finally, there has been research on automatically solving various types of mathematical word problems. The dominant existing approach is to hand engineer rule-based systems to solve math problem in specifi</context>
</contexts>
<marker>Ritter, Mausam, Clark, 2012</marker>
<rawString>Alan Ritter, Mausam, Oren Etzioni, and Sam Clark. 2012. Open domain event extraction from twitter. In Proceedings of the Conference on Knowledge Discovery and Data Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Thompson</author>
<author>Raymond Mooney</author>
</authors>
<title>Acquiring word-meaning mappings for natural language interfaces.</title>
<date>2003</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>18</volume>
<issue>1</issue>
<contexts>
<context position="5613" citStr="Thompson and Mooney, 2003" startWordPosition="883" endWordPosition="886"> work is related to three main areas of research: situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Le</context>
</contexts>
<marker>Thompson, Mooney, 2003</marker>
<rawString>Cynthia Thompson and Raymond Mooney. 2003. Acquiring word-meaning mappings for natural language interfaces. Journal of Artificial Intelligence Research, 18(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Vogel</author>
<author>Dan Jurafsky</author>
</authors>
<title>Learning to follow navigational directions.</title>
<date>2010</date>
<booktitle>In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5483" citStr="Vogel and Jurafsky, 2010" startWordPosition="862" endWordPosition="865"> handle weak supervision, achieving more than 70% of the above performance when trained exclusively on answers. 2 Related Work Our work is related to three main areas of research: situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012;</context>
</contexts>
<marker>Vogel, Jurafsky, 2010</marker>
<rawString>Adam Vogel and Dan Jurafsky. 2010. Learning to follow navigational directions. In Proceedings of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuk Wah Wong</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning for semantic parsing with statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the Annual Meeting of the North American Chapter of the Association of Computational Linguistics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="5636" citStr="Wong and Mooney, 2006" startWordPosition="887" endWordPosition="890">ain areas of research: situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We foc</context>
</contexts>
<marker>Wong, Mooney, 2006</marker>
<rawString>Yuk Wah Wong and Raymond Mooney. 2006. Learning for semantic parsing with statistical machine translation. In Proceedings of the Annual Meeting of the North American Chapter of the Association of Computational Linguistics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Zelle</author>
<author>Raymond Mooney</author>
</authors>
<title>Learning to parse database queries using inductive logic programming.</title>
<date>1996</date>
<booktitle>In Proceedings of the Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="5586" citStr="Zelle and Mooney, 1996" startWordPosition="879" endWordPosition="882">wers. 2 Related Work Our work is related to three main areas of research: situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kus</context>
</contexts>
<marker>Zelle, Mooney, 1996</marker>
<rawString>John Zelle and Raymond Mooney. 1996. Learning to parse database queries using inductive logic programming. In Proceedings of the Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="5667" citStr="Zettlemoyer and Collins, 2005" startWordPosition="891" endWordPosition="895">situated semantic interpretation, information extraction, and automatic word problem solvers. Situated Semantic Interpretation There is a large body of research on learning to map natural language to formal meaning representations, given varied forms of supervision. Reinforcement learning can be used to learn to read instructions and perform actions in an external world (Branavan et al., 2009; Branavan et al., 2010; Vogel and Jurafsky, 2010). Other approaches have relied on access to more costly annotated logical forms (Zelle and Mooney, 1996; Thompson and Mooney, 2003; Wong and Mooney, 2006; Zettlemoyer and Collins, 2005; Kwiatkowski et al., 2010). These techniques have been generalized more recently to learn from sentences paired with indirect feedback from a controlled application. Examples include question answering (Clarke et al., 2010; Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; Kwiatkowski et al., 2013), dialog systems (Artzi and Zettlemoyer, 2011), robot instruction (Chen and Mooney, 2011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supe</context>
</contexts>
<marker>Zettlemoyer, Collins, 2005</marker>
<rawString>Luke Zettlemoyer and Michael Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. In Proceedings of the Conference on Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Luke Zettlemoyer</author>
<author>Michael Collins</author>
</authors>
<title>Learning context-dependent mappings from sentences to logical form.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing.</booktitle>
<contexts>
<context position="6696" citStr="Zettlemoyer and Collins, 2009" startWordPosition="1050" endWordPosition="1053">011; Chen, 2012; Kim and Mooney, 2012; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013), and program executions (Kushman and Barzilay, 2013; Lei et al., 2013). We focus on learning from varied supervision, including question answers and equation systems, both can be obtained reliably from annotators with no linguistic training and only basic math knowledge. Nearly all of the above work processed single sentences in isolation. Techniques that consider multiple sentences typically do so in a serial fashion, processing each in turn with limited cross-sentence reasoning (Branavan et al., 2009; Zettlemoyer and Collins, 2009; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). We focus on analyzing multiple sentences simultaneously, as is necessary to generate the global semantic representations common in domains such as algebra word problems. Information Extraction Our approach is related to work on template-based information extraction, where the goal is to identify instances of event templates in text and extract their slot fillers. Most work has focused on the supervised case, where the templates are manually defined and data is labeled with alignment information, e.g. (Grishman et al., 2005; Maslennikov and</context>
</contexts>
<marker>Zettlemoyer, Collins, 2009</marker>
<rawString>Luke Zettlemoyer and Michael Collins. 2009. Learning context-dependent mappings from sentences to logical form. In Proceedings of the Joint Conference of the Association for Computational Linguistics and International Joint Conference on Natural Language Processing.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>