<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.986988">
Adaptation of Maximum Entropy Capitalizer: Little Data Can Help a Lot
</title>
<author confidence="0.94585">
Ciprian Chelba and Alex Acero
</author>
<affiliation confidence="0.913856">
Microsoft Research
</affiliation>
<address confidence="0.9014725">
One Microsoft Way
Redmond, WA 98052
</address>
<email confidence="0.99927">
{chelba,alexac}@microsoft.com
</email>
<sectionHeader confidence="0.993903" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946636363636">
A novel technique for maximum “a posteriori”
(MAP) adaptation of maximum entropy (MaxEnt)
and maximum entropy Markov models (MEMM) is
presented.
The technique is applied to the problem of recov-
ering the correct capitalization of uniformly cased
text: a “background” capitalizer trained on 20Mwds
of Wall Street Journal (WSJ) text from 1987 is
adapted to two Broadcast News (BN) test sets —
one containing ABC Primetime Live text and the
other NPR Morning News/CNN Morning Edition
text — from 1996.
The “in-domain” performance of the WSJ capi-
talizer is 45% better than that of the 1-gram base-
line, when evaluated on a test set drawn from WSJ
1994. When evaluating on the mismatched “out-of-
domain” test data, the 1-gram baseline is outper-
formed by 60%; the improvement brought by the
adaptation technique using a very small amount of
matched BN data — 25-70kwds — is about 20-25%
relative. Overall, automatic capitalization error rate
of 1.4% is achieved on BN data.
</bodyText>
<sectionHeader confidence="0.99913" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999882696428572">
Automatic capitalization is a practically relevant
problem: speech recognition output needs to be
capitalized; also, modern word processors perform
capitalization among other text proofing algorithms
such as spelling correction and grammar checking.
Capitalization can be also used as a preprocessing
step in named entity extraction or machine trans-
lation. We study the impact of using increasing
amounts of training data as well as using a small
amount of adaptation data on this simple problem
that is well suited to data-driven approaches since
vast amounts of “training” data are easily obtainable
by simply wiping the case information in text.
As in previous approaches, the problem is framed
as an instance of the class of sequence labeling
problems. A case frequently encountered in prac-
tice is that of using mismatched — out-of-domain,
in this particular case we used Broadcast News —
test data. For example, one may wish to use a capi-
talization engine developed on newswire text for e-
mail or office documents. This typically affects neg-
atively the performance of a given model, and more
sophisticated models tend to be more brittle. In the
capitalization case we have studied, the relative per-
formance improvement of the MEMM capitalizer
over the 1-gram baseline drops from in-domain —
WSJ — performance of 45% to 35-40% when used
on the slightly mismatched BN data.
In order to take advantage of the adaptation data
in our scenario, a maximum a-posteriori (MAP)
adaptation technique for maximum entropy (Max-
Ent) models is developed. The adaptation procedure
proves to be quite effective in further reducing the
capitalization error of the WSJ MEMM capitalizer
on BN test data. It is also quite general and could
improve performance of MaxEnt models in any sce-
nario where model adaptation is desirable. A further
relative improvement of about 20% is obtained by
adapting the WSJ model to Broadcast News (BN)
text. Overall, the MEMM capitalizer adapted to BN
data achieves 60% relative improvement in accuracy
over the 1-gram baseline.
The paper is organized as follows: the next sec-
tion frames automatic capitalization as a sequence
labeling problem, presents previous approaches as
well as the widespread and highly sub-optimal 1-
gram capitalization technique that is used as a base-
line in most experiments in this work and others.
The MEMM sequence labeling technique is briefly
reviewed in Section 3. Section 4 describes the
MAP adaptation technique used for the capitaliza-
tion of out-of-domain text. The detailed mathemat-
ical derivation is presented in Appendix A. The ex-
perimental results are presented in Section 5, fol-
lowed by conclusions and suggestions for future
work.
</bodyText>
<sectionHeader confidence="0.782642" genericHeader="introduction">
2 Capitalization as Sequence Tagging
</sectionHeader>
<bodyText confidence="0.999336">
Automatic capitalization can be seen as a sequence
tagging problem: each lower-case word receives a
tag that describes its capitalization form. Similar to
the work in (Lita et al., 2003), we tag each word in
a sentence with one of the tags:
</bodyText>
<listItem confidence="0.990333272727273">
• LOC lowercase
• CAP capitalized
• MXC mixed case; no further guess is made as to
the capitalization of such words. A possibility
is to use the most frequent one encountered in
the training data.
• AUC all upper case
• PNC punctuation; we decided to have a sep-
arate tag for punctuation since it is quite fre-
quent and models well the syntactic context in
a parsimonious way
</listItem>
<bodyText confidence="0.879249">
For training a given capitalizer one needs to convert
running text into uniform case text accompanied by
the above capitalization tags. For example,
PrimeTime continues on ABC .PERIOD
Now ,COMMA from Los Angeles ,COMMA
Diane Sawyer .PERIOD
</bodyText>
<equation confidence="0.915637166666667">
becomes
primetime_MXC continues_LOC on_LOC
abc_AUC .period_PNC
now_CAP ,comma_PNC from_LOC los_CAP
angeles_CAP ,comma_PNC diane_CAP
sawyer_CAP .period_PNC
</equation>
<bodyText confidence="0.998736857142857">
The text is assumed to be already segmented into
sentences. Any sequence labeling algorithm can
then be trained for tagging lowercase word se-
quences with capitalization tags.
At test time, the uniform case text to be capital-
ized is first segmented into sentences1 after which
each sentence is tagged.
</bodyText>
<subsectionHeader confidence="0.994748">
2.1 1-gram capitalizer
</subsectionHeader>
<bodyText confidence="0.995000333333333">
A widespread algorithm used for capitalization is
the 1-gram tagger: for every word in a given vo-
cabulary (usually large, 100kwds or more) use the
most frequent tag encountered in a large amount of
training data. As a special case for automatic capi-
talization, the most frequent tag for the first word in
a sentence is overridden by CAP, thus capitalizing
on the fact that the first word in a sentence is most
likely capitalized2.
</bodyText>
<footnote confidence="0.99613925">
1Unlike the training phase, the sentence segmenter at test
time is assumed to operate on uniform case text.
2As with everything in natural language, it is not hard to
find exceptions to this “rule”.
</footnote>
<bodyText confidence="0.999327571428571">
Due to its popularity, both our work and that
of (Lita et al., 2003) uses the 1-gram capitalizer as
a baseline. The work in (Kim and Woodland, 2004)
indicates that the same 1-gram algorithm is used in
Microsoft Word 2000 and is consequently used as
a baseline for evaluating the performance of their
algorithm as well.
</bodyText>
<subsectionHeader confidence="0.993656">
2.2 Previous Work
</subsectionHeader>
<bodyText confidence="0.996407130434783">
We share the approach to capitalization as sequence
tagging with that of (Lita et al., 2003). In their ap-
proach, a language model is built on pairs (word,
tag) and then used to disambiguate over all possible
tag assignments to a sentence using dynamic pro-
gramming techniques.
The same idea is explored in (Kim and Woodland,
2004) in the larger context of automatic punctuation
generation and capitalization from speech recogni-
tion output. A second approach they consider for
capitalization is the use a rule-based tagger as de-
scribed by (Brill, 1994), which they show to outper-
form the case sensitive language modeling approach
and be quite robust to speech recognition errors and
punctuation generation errors.
Departing from their work, our approach builds
on a standard technique for sequence tagging,
namely MEMMs, which has been successfully ap-
plied to part-of-speech tagging (Ratnaparkhi, 1996).
The MEMM approach models the tag sequence T
conditionally on the word sequence W, which has a
few substantial advantages over the 1-gram tagging
approach:
</bodyText>
<listItem confidence="0.997840846153846">
• discriminative training of probability model
P(T|W) using conditional maximum likeli-
hood is well correlated with tagging accuracy
• ability to use a rich set of word-level fea-
tures in a parsimonious way: sub-word fea-
tures such as prefixes and suffixes, as well as
future words3 are easily incorporated in the
probability model
• no concept of “out-of-vocabulary” word: sub-
word features are very useful in dealing with
words not seen in the training data
• ability to integrate rich contextual features into
the model
</listItem>
<bodyText confidence="0.8655686">
More recently, certain drawbacks of MEMM mod-
els have been addressed by the conditional random
field (CRF) approach (Lafferty et al., 2001) which
slightly outperforms MEMMs on a standard part-
of-speech tagging task. In a similar vein, the work
</bodyText>
<footnote confidence="0.670971">
3Relative to the current word, whose tag is assigned a prob-
ability value by the MEMM.
</footnote>
<bodyText confidence="0.999948">
of (Collins, 2002) explores the use of discrimina-
tively trained HMMs for sequence labeling prob-
lems, a fair baseline for such cases that is often over-
looked in favor of the inadequate maximum likeli-
hood HMMs.
The work on adapting the MEMM model param-
eters using MAP smoothing builds on the Gaussian
prior model used for smoothing MaxEnt models, as
presented in (Chen and Rosenfeld, 2000). We are
not aware of any previous work on MAP adapta-
tion of MaxEnt models using a prior, be it Gaus-
sian or a different one, such as the exponential prior
of (Goodman, 2004). Although we do not have a
formal derivation, the adaptation technique should
easily extend to the CRF scenario.
A final remark contrasts rule-based approaches
to sequence tagging such as (Brill, 1994) with
the probabilistic approach taken in (Ratnaparkhi,
1996): having a weight on each feature in the Max-
Ent model and a sound probabilistic model allows
for a principled way of adapting the model to a new
domain; performing such adaptation in a rule-based
model is unclear, if at all possible.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="method">
3 MEMM for Sequence Labeling
</sectionHeader>
<bodyText confidence="0.996682">
A simple approach to sequence labeling is the max-
imum entropy Markov model. The model assigns a
probability P(T|W) to any possible tag sequence
</bodyText>
<equation confidence="0.916407666666667">
T = t1 ... tn = T1n for a given word sequence
W = w1 ... wn. The probability assignment is
done according to:
P(ti|xi(W,Ti−1
1 ))
where ti is the tag corresponding to word i and
xi(W,Ti−1
1 ) is the conditioning information at posi-
tion i in the word sequence on which the probability
</equation>
<bodyText confidence="0.945395176470588">
model is built.
The approach we took is the one in (Rat-
naparkhi, 1996), which uses xi(W, T i−1
1 ) =
{wi, wi−1, wi+1, ti−1, ti−2}. We note that the prob-
ability model is causal in the sequencing of tags (the
probability assignment for ti only depends on previ-
ous tags ti−1, ti−2) which allows for efficient algo-
rithms that search for the most likely tag sequence
T∗(W) = arg maxT P(T |W) as well as ensures a
properly normalized conditional probability model
P(T|W).
The probability P(ti|xi(W,T i−1
1 )) is modeled
using a maximum entropy model. The next section
briefly describes the training procedure; for details
the reader is referred to (Berger et al., 1996).
</bodyText>
<subsectionHeader confidence="0.999184">
3.1 Maximum Entropy State Transition Model
</subsectionHeader>
<bodyText confidence="0.999591">
The sufficient statistics that are ex-
tracted from the training data are tuples
</bodyText>
<equation confidence="0.7898235">
(y, #, x) = (ti, #, xi(W, Ti−1
1 )) where ti is
</equation>
<bodyText confidence="0.918246944444444">
the tag assigned in context xi(W, T i−1
1 ) =
{wi, wi−1, wi+1, ti−1, ti−2} and # denotes the
count with which this event has been observed in
the training data. By way of example, the event
associated with the first word in the example in
Section 2 is (*bdw* denotes a special boundary
type):
MXC 1
currentword=primetime
previousword=*bdw*
nextword=continues
t1=*bdw* t1,2=*bdw*,*bdw*
prefix1=p prefix2=pr prefix3=pri
suffix1=e suffix2=me suffix3=ime
The maximum entropy probability model P(y|x)
uses features which are indicator functions of the
type:
</bodyText>
<equation confidence="0.7531015">
f(y, x) = {1, if y = MXC and x.wi = primetime
0, o/w
</equation>
<bodyText confidence="0.9666795">
Assuming a set of features F whose cardinality is
F, the probability assignment is made according to:
</bodyText>
<equation confidence="0.9895895">
�i=1F 11PA(y|x) = Z−1(x,A) · exp E Aifi(x, y)J
�i=1F 11exp EAifi(x,y)J
</equation>
<bodyText confidence="0.9994305">
where A = {A1 ... AF} E RF is the set of real-
valued model parameters.
</bodyText>
<subsectionHeader confidence="0.831479">
3.1.1 Feature Selection
</subsectionHeader>
<bodyText confidence="0.999974">
We used a simple count cut-off feature selection al-
gorithm which counts the number of occurrences of
all features in a predefined set after which it discards
the features whose count is less than a pre-specified
threshold. The parameter of the feature selection al-
gorithm is the threshold value; a value of 0 will keep
all features encountered in the training data.
</bodyText>
<subsubsectionHeader confidence="0.588755">
3.1.2 Parameter Estimation
</subsubsectionHeader>
<bodyText confidence="0.9973418">
The model parameters A are estimated such that
the model assigns maximum log-likelihood to the
training data subject to a Gaussian prior centered
at 0, A ∼ N(0, diag(u2i )), that ensures smooth-
ing (Chen and Rosenfeld, 2000):
</bodyText>
<equation confidence="0.9978181">
L(A) = E ˜p(x, y)log pA(y|x) (1)
x,y
n
P(T|W) =
i=1
E
Z(x, A) =
y
λi
2σ&apos; + const(A)i
</equation>
<bodyText confidence="0.993800666666667">
As shown in (Chen and Rosenfeld, 2000) — and re-
derived in Appendix A for the non-zero mean case
— the update equations are:
</bodyText>
<equation confidence="0.9983536">
λ(t+1) i= λ(t) i+ δi, where δi satisfies:
λi
˜p(x, y)fi(x, y) − =
σ2 i
˜p(x)pΛ(y|x)fi(x, y)exp(δif#(x, y))
</equation>
<bodyText confidence="0.999448">
In our experiments the variances are tied to σi = σ
whose value is determined by line search on devel-
opment data such that it yields the best tagging ac-
curacy.
</bodyText>
<sectionHeader confidence="0.9920865" genericHeader="method">
4 MAP Adaptation of Maximum Entropy
Models
</sectionHeader>
<bodyText confidence="0.999596125">
In the adaptation scenario we already have a Max-
Ent model trained on the background data and we
wish to make best use of the adaptation data by bal-
ancing the two. A simple way to accomplish this is
to use MAP adaptation using a prior distribution on
the model parameters.
A Gaussian prior for the model parameters A
has been previously used in (Chen and Rosen-
feld, 2000) for smoothing MaxEnt models. The
prior has 0 mean and diagonal covariance: A ∼
N(0, diag(σ2 i)). In the adaptation scenario, the
prior distribution used is centered at the parameter
values A0 estimated from the background data in-
stead of 0: A ∼ N(A0, diag(σ2i )).
The regularized log-likelihood of the adaptation
training data becomes:
</bodyText>
<equation confidence="0.9950352">
L(A) = � ˜p(x, y)log pΛ(y|x) (3)
x,y
(λi − λ0i )2
+ const(A)
2σ2 i
</equation>
<bodyText confidence="0.892448">
The adaptation is performed in stages:
</bodyText>
<listItem confidence="0.870850875">
• apply feature selection algorithm on adaptation
data and determine set of features Fadapt.
• build new model by taking the union of the
background and the adaptation feature sets:
F = Fbackground ∪ Fadapt; each of the
background features receives the correspond-
ing weight λi determined on the background
training data; the new features
</listItem>
<bodyText confidence="0.828351">
Fadapt \ Fbackground4 introduced in the model
receive 0 weight. The resulting model is thus
equivalent with the background model.
• train the model such that the regularized log-
likelihood of the adaptation training data is
maximized. The prior mean is set at A0 =
Abackground · 0; · denotes concatenation be-
tween the parameter vector for the background
model and a 0-valued vector of length |Fadapt\
Fbackground |corresponding to the weights for
the new features.
As shown in Appendix A, the update equations are
very similar to the 0-mean case:
</bodyText>
<equation confidence="0.995851285714286">
E AX, y)fi(x, y) − (λi −2λi )
x,y i
_( 0 σ2 + (4)
δi
i
E ˜p(x)pΛ(y|x)fi(x, y)exp(δif#(x, y))
x,y
</equation>
<bodyText confidence="0.999962071428572">
The effect of the prior is to keep the model param-
eters λi close to the background ones. The cost of
moving away from the mean for each feature fi is
specified by the magnitude of the variance σi: a
small variance σi will keep the weight λi close to
its mean; a large variance σi will make the regu-
larized log-likelihood (see Eq. 3) insensitive to the
prior on λi, allowing the use of the best value λi for
modeling the adaptation data.
Another observation is that not only the features
observed in the adaptation data get updated: even
if E˜p(x,y)[fi] = 0, the weight λi for feature fi will
still get updated if the feature fi triggers for a con-
text x encountered in the adaptation data and some
predicted value y — not necessarily present in the
adaptation data in context x.
In our experiments the variances were tied to
σi = σ whose value was determined by line search
on development data drawn from the adaptation
data. The common variance σ will thus balance
optimally the log-likelihood of the adaptation data
with the A0 mean values obtained from the back-
ground data.
Other tying schemes are possible: separate val-
ues could be used for the Fadapt \ Fbackground and
Fbackground feature sets, respectively. We did not
experiment with various tying schemes although
this is a promising research direction.
</bodyText>
<subsectionHeader confidence="0.853912">
4.1 Relationship with Minimum Divergence
Training
</subsectionHeader>
<bodyText confidence="0.970538">
Another possibility to adapt the background model
is to do minimum KL divergence (MinDiv) train-
</bodyText>
<equation confidence="0.972537">
4We use A \ B to denote set difference.
F
−
i=1
E
x,y
δi + (2)
σ2i
E
x,y
F
−
i=1
</equation>
<bodyText confidence="0.99962944">
ing (Pietra et al., 1995) between the background
exponential model B — assumed fixed — and an
exponential model A built using the Fbackground U
Fadapt feature set. It can be shown that, if we
smooth the A model with a Gaussian prior on the
feature weights that is centered at 0 — following
the approach in (Chen and Rosenfeld, 2000) for
smoothing maximum entropy models — then the
MinDiv update equations for estimating A on the
adaptation data are identical to the MAP adaptation
procedure we proposed5.
However, we wish to point out that the equiva-
lence holds only if the feature set for the new model
A is Fbackground U Fadapt. The straightforward ap-
plication of MinDiv training — by using only the
Fadapt feature set for A — will not result in an
equivalent procedure to ours. In fact, the differ-
ence in performance between this latter approach
and ours could be quite large since the cardinality
of Fbackground is typically several orders of mag-
nitude larger than that of Fadapt and our approach
also updates the weights corresponding to features
in Fbackground \ Fadapt. Further experiments are
needed to compare the performance of the two ap-
proaches.
</bodyText>
<sectionHeader confidence="0.999355" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999106875">
The baseline 1-gram and the background MEMM
capitalizer were trained on various amounts of
WSJ (Paul and Baker, 1992) data from 1987 — files
WS87_{001-126}. The in-domain test data used
was file WS94_000 (8.7kwds).
As for the adaptation experiments, two different
sets of BN data were used, whose sizes are summa-
rized in Table 1:
</bodyText>
<listItem confidence="0.956086">
1. BN CNN/NPR data. The train-
ing/development/test partition consisted of a
3-way random split of file BN624BTS. The
resulting sets are denoted CNN-trn/dev/tst,
respectively
2. BN ABC Primetime data. The training set con-
sisted of file BN623ATS whereas the develop-
ment/test set consisted of a 2-way random split
of file BN624ATS
</listItem>
<subsectionHeader confidence="0.800309">
5.1 In-Domain Experiments
</subsectionHeader>
<bodyText confidence="0.9959378">
We have proceeded building both 1-gram and
MEMM capitalizers using various amounts of back-
ground training data. The model sizes for the 1-
gram and MEMM capitalizer are presented in Ta-
ble 2. Count cut-off feature selection has been used
</bodyText>
<footnote confidence="0.983726">
5Thanks to one of the anonymous reviewers for pointing out
this possible connection.
</footnote>
<table confidence="0.9996342">
Data set train Partition test
devel
WSJ 2-20M — 8.7k
CNN 73k 73k 73k
ABC 25k 8k 8k
</table>
<tableCaption confidence="0.993554">
Table 1: Background and adaptation training, devel-
opment, and test data partition sizes
</tableCaption>
<bodyText confidence="0.9715978">
for the MEMM capitalizer with the threshold set at
5, so the MEMM model size is a function of the
training data. The 1-gram capitalizer used a vocab-
ulary of the most likely 100k wds derived from the
training data.
</bodyText>
<table confidence="0.999627">
Model No. Param. (103)
Training Data Size (106) 2.0 3.5 20.0
1-gram 100 100 100
MEMM 76 102 238
</table>
<tableCaption confidence="0.9388265">
Table 2: Background models size (number of pa-
rameters) for various amounts of training data
</tableCaption>
<bodyText confidence="0.999145">
We first evaluated the in-domain and out-of-
domain relative performance of the 1-gram and the
MEMM capitalizers as a function of the amount of
training data. The results are presented in Table 3.
The MEMM capitalizer performs about 45% better
</bodyText>
<table confidence="0.99896125">
Model Test Data Cap ERR (%)
Training Data Size (106) 2.0 3.5 20.0
1-gram WSJ-tst 5.4 5.2 4.4
MEMM WSJ-tst 2.9 2.5 2.3
1-gram ABC-dev 3.1 2.9 2.6
MEMM ABC-dev 2.2 2.0 1.6
1-gram CNN-dev 4.4 4.2 3.5
MEMM CNN-dev 2.7 2.5 2.1
</table>
<tableCaption confidence="0.997269">
Table 3: Background models performance on in-
</tableCaption>
<bodyText confidence="0.95678525">
domain (WSJ-test) and out-of-domain (BN-dev)
data for various amounts of training data
than the 1-gram one when trained and evaluated on
Wall Street Journal text. The relative performance
improvement of the MEMM capitalizer over the 1-
gram baseline drops to 35-40% when using out-of-
domain Broadcast News data. Both models benefit
from using more training data.
</bodyText>
<subsectionHeader confidence="0.997884">
5.2 Adaptation Experiments
</subsectionHeader>
<bodyText confidence="0.999960923076923">
We have then adapted the best MEMM model built
on 20Mwds on the two BN data sets (CNN/ABC)
and compared performance against the 1-gram and
the unadapted MEMM models.
There are a number of parameters to be tuned
on development data. Table 4 presents the varia-
tion in model size with different count cut-off values
for the feature selection procedure on the adaptation
data. As can be seen, very few features are added to
the background model. Table 5 presents the varia-
tion in log-likelihood and capitalization accuracy on
the CNN adaptation training and development data,
respectively. The adaptation procedure was found
</bodyText>
<table confidence="0.751174">
Cut-off 0 5 106
No. features 243,262 237,745 237,586
</table>
<tableCaption confidence="0.839112">
Table 4: Adapted model size as a function of count
</tableCaption>
<bodyText confidence="0.9774902">
cut-off threshold used for feature selection on CNN-
trn adaptation data; the entry corresponding to the
cut-off threshold of 106 represents the number of
features in the background model
to be insensitive to the number of reestimation it-
erations, and, more surprisingly, to the number of
features added to the background model from the
adaptation data, as shown in 5. The most sensitive
parameter is the prior variance σ2, as shown in Fig-
ure 1; its value is chosen to maximize classification
accuracy on development data. As expected, low
values of σ2 result in no adaptation at all, whereas
high values of σ2 fit the training data very well, and
result in a dramatic increase of training data log-
likelihood and accuracies approaching 100%.
</bodyText>
<table confidence="0.9997605">
Cut- σ2 LogL Cap ACC (%)
off (nats) CNN-trn CNN-dev
0 0.01 -4258.58 98.00 97.98
0 3.0 -1194.45 99.63 98.62
5 0.01 -4269.72 98.00 97.98
5 3.0 -1369.26 99.55 98.60
106 0.01 -4424.58 98.00 97.96
106 3.0 -1467.46 99.52 98.57
</table>
<tableCaption confidence="0.989453">
Table 5: Adapted model performance for various
</tableCaption>
<bodyText confidence="0.9711004">
count cut-off and σ2 variance values; log-likelihood
and accuracy on adaptation data CNN-trn as well
as accuracy on held-out data CNN-dev; the back-
ground model results (no new features added) are
the entries corresponding to the cut-off threshold of
</bodyText>
<page confidence="0.492661">
106
</page>
<bodyText confidence="0.9995214">
Finally, Table 6 presents the results on test data
for 1-gram, background and adapted MEMM. As
can be seen, the background MEMM outperforms
the 1-gram model on both BN test sets by about
35-40% relative. Adaptation improves performance
even further by another 20-25% relative. Overall,
the adapted models achieve 60% relative reduction
in capitalization error over the 1-gram baseline on
both BN test sets. An intuitively satisfying result
is the fact that the cross-test set performance (CNN
</bodyText>
<figure confidence="0.8295545">
Adaptation: Training LogL (nats) with σ2
σ2 variance
</figure>
<figureCaption confidence="0.895572333333333">
Figure 1: Variation of training data log-likelihood,
and training/development data (- -/– line) capitaliza-
tion accuracy as a function of the prior variance σ2
</figureCaption>
<table confidence="0.998818">
Model Adapt Data Cap ERR (%)
ABC-tst CNN-tst
1-gram — 2.7 3.7
MEMM — 1.8 2.2
MEMM ABC-trn 1.4 1.7
MEMM CNN-trn 2.4 1.4
</table>
<tableCaption confidence="0.951827666666667">
Table 6: Background and adapted models perfor-
mance on BN test data; two adaptation/test sets are
used: ABC and CNN
</tableCaption>
<bodyText confidence="0.767593">
adapted model evaluated on ABC data and the other
way around) is worse than the adapted one.
</bodyText>
<sectionHeader confidence="0.990122" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999869421052632">
The MEMM tagger is very effective in reducing
both in-domain and out-of-domain capitalization er-
ror by 35%-45% relative over a 1-gram capitaliza-
tion model.
We have also presented a general technique for
adapting MaxEnt probability models. It was shown
to be very effective in adapting a background
MEMM capitalization model, improving the accu-
racy by 20-25% relative. An overall 50-60% reduc-
tion in capitalization error over the standard 1-gram
baseline is achieved. A surprising result is that the
adaptation performance gain is not due to adding
more, domain-specific features but rather making
better use of the background features for modeling
the in-domain data.
As expected, adding more background training
data improves performance but a very small amount
of domain specific data also helps significantly if
one can make use of it in an effective way. The
</bodyText>
<figure confidence="0.99591105">
−1000
−1500
−2000
−2500
−3000
−3500
−4000
−4500
0 1 2 3 4 5 6
LogL(train)
Adaptation: Training and Development Capitalization Accuracy with σ2
100
99.5
99
98.5
98
97.5
0 1 2 3 4 5 6
Accuracy
σ2 variance
</figure>
<bodyText confidence="0.972658307692308">
“There’s no data like more data” rule-of-thumb
could be amended by “..., especially if it’s the right
data!”.
As future work we plan to investigate the best
way to blend increasing amounts of less-specific
background training data with specific, in-domain
data for this and other problems.
Another interesting research direction is to ex-
plore the usefulness of the MAP adaptation of Max-
Ent models for other problems among which we
wish to include language modeling, part-of-speech
tagging, parsing, machine translation, information
extraction, text routing.
</bodyText>
<sectionHeader confidence="0.998563" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997048">
Special thanks to Adwait Ratnaparkhi for making
available the code for his MEMM tagger and Max-
Ent trainer.
</bodyText>
<sectionHeader confidence="0.998929" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.981542833333333">
A. L. Berger, S. A. Della Pietra, and V. J. Della
Pietra. 1996. A Maximum Entropy Approach
to Natural Language Processing. Computational
Linguistics, 22(1):39–72, March.
Eric Brill. 1994. Some Advances in
Transformation-Based Part of Speech Tag-
ging. In National Conference on Artificial
Intelligence, pages 722–727.
Stanley F. Chen and Ronald Rosenfeld. 2000. A
Survey of Smoothing Techniques for Maximum
Entropy Models. IEEE Transactions on Speech
and Audio Processing, 8(1):37–50.
Michael Collins. 2002. Discriminative Training
Methods for Hidden Markov Models: Theory
and Experiments with Perceptron Algorithms.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, pages
1–8, University of Pennsylvania, Philadelphia,
PA, July. ACL.
Joshua Goodman. 2004. Exponential Priors for
Maximum Entropy Models. In Daniel Marcu Su-
san Dumais and Salim Roukos, editors, HLT-
NAACL 2004: Main Proceedings, pages 305–
312, Boston, Massachusetts, USA, May 2 - May
7. Association for Computational Linguistics.
Ji-Hwan Kim and Philip C. Woodland. 2004. Au-
tomatic Capitalization Generation for Speech In-
put. Computer Speech and Language, 18(1):67–
90, January.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields:
Probabilistic Models for Segmenting and La-
beling Sequence Data. In Proc. 18th Interna-
tional Conf. on Machine Learning, pages 282–
289. Morgan Kaufmann, San Francisco, CA.
L. Lita, A. Ittycheriah, S. Roukos, and N. Kamb-
hatla. 2003. tRuEcasIng. In Proccedings of
ACL, pages 152–159, Sapporo, Japan.
Doug B. Paul and Janet M. Baker. 1992. The design
for the Wall Street Journal-based CSR corpus. In
Proceedings of the DARPA SLS Workshop. Febru-
ary.
S. Della Pietra, V. Della Pietra, and J. Lafferty.
1995. Inducing features of random fields. Tech-
nical Report CMU-CS-95-144, School of Com-
puter Science, Carnegie Mellon University, Pitts-
burg, PA.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-of-Speech Tagging. In Eric Brill
and Kenneth Church, editors, Proceedings of
the Conference on Empirical Methods in Natu-
ral Language Processing, pages 133–142. Asso-
ciation for Computational Linguistics, Somerset,
New Jersey.
</reference>
<sectionHeader confidence="0.860269" genericHeader="acknowledgments">
Appendix
A Modified IIS for MaxEnt MAP
</sectionHeader>
<subsectionHeader confidence="0.555923">
Adaptation Using a Gaussian Prior
</subsectionHeader>
<bodyText confidence="0.928148">
The regularized log-likelihood of the training data
— to be maximized by the MAP adaptation training
algorithm — is:
</bodyText>
<equation confidence="0.987696833333334">
L(Λ) = � ˜p(x, y)log pΛ(y|x)
x,y
(λi − λ0i)2
+ const(Λ)
2σ2 i
E= ˜p(x, y) F λifi(x, y) −
x,y i=1
λifi(x, y) −
�i=1F 1exp Eλifi(x,y)J
(λi −λ0i )2
+ const(Λ)
2σ2 i
</equation>
<bodyText confidence="0.9999195">
where the last equality holds because the argument
of the log is independent of y.
</bodyText>
<equation confidence="0.938056842105263">
E � �i=1F 1exp Eλifi(x,y)J
x,y ˜p(x,y)log
y,
+ const(Λ)
2σ2 i
F
−
i=1
(λi − λ0i )2
F
−
i=1
E= ˜p(x, y) F
x,y i=1
E ˜p(x) log �
x y,
F
−
i=1
</equation>
<bodyText confidence="0.9999431">
The derivation of the updates follows very closely
the one in (Chen and Rosenfeld, 2000) for smooth-
ing a MaxEnt model by using a Gaussian prior
with 0 mean and diagonal covariance matrix. At
each iteration we seek to find a set of updates for
Λ, ∆ = {δi}, that increase the regularized log-
likelihood L(Λ) by the largest amount possible.
After a few basic algebraic manipulations, the
difference in log-likelihood caused by a ∆ change
in the Λ values becomes:
</bodyText>
<equation confidence="0.9960016">
L(Λ + ∆) − L(Λ)
δifi(x, y) −
�i=1F 1PΛ(y|x)exp Eδifi(x,y)]
2(λi − λ0i)δi + δi2
2σ2 i
</equation>
<bodyText confidence="0.99991225">
Following the same lower bounding technique as
in (Chen and Rosenfeld, 2000) by using log x &lt;
x−1 and Jensen’s inequality for the U-convexity of
the exponential we obtain:
</bodyText>
<equation confidence="0.999711">
L(Λ + ∆) − L(Λ)
δifi(x, y) + 1 −
</equation>
<bodyText confidence="0.968694875">
the maximum value of A(∆, Λ) – which is non-
negative and thus guarantees that the regularized
log-likelihood does not decrease at each iteration:
L(Λ + ∆*) − L(Λ) &gt; 0.
Solving for the root of ∂A(∆,Λ)
∂δi = 0 results in the
update Eq. 4 and is equivalent to finding the solution
to:
</bodyText>
<equation confidence="0.9745035">
E˜p(x,y) [fi] − (λi − λ0 i )= δiσ2 σ2 +
i i
E ˜p(x)pΛ(y|x)fi(x, y)exp(δif#(x, y))
x,y
</equation>
<bodyText confidence="0.77757">
A convenient way to solve this equation is to substi-
</bodyText>
<equation confidence="0.950376">
tute αi = exp(δi) and ai = E˜p(x,y) [fi] − (λi−λ0i )
σ2 i
</equation>
<bodyText confidence="0.998369">
and then use Newton’s method for finding the solu-
tion to ai = f(αi), where f(α) is:
</bodyText>
<equation confidence="0.9719445">
f (α) = log 2α + E˜p(x)pΛ(y |x)fi(x, y)αf#(x,y)
i x,y
</equation>
<bodyText confidence="0.9993065">
The summation on the right hand side reduces to
accumulating the coefficients of a polynomial in
α whose maximum degree is the highest possible
value of f#(x, y) on any context x encountered in
the training data and any allowed predicted value
yEY.
</bodyText>
<equation confidence="0.997512133333333">
E= ˜p(x, y) EF
x,y i=1
E ˜p(x) log E
x y
−
EF
i=1
E ˜p(x, y) EF
&gt; i=1
x,y
E ˜p(x)pΛ(y|x) EF f#(x,y)exp(δif#(x, y))
x,y i=1 fi(x, y)
−
EF
i=1
</equation>
<bodyText confidence="0.8726178">
2(λi − λ0i )δi + δi2 = A(∆, Λ)
2σ2i
where f#(x, y) = EF i=1 fi(x, y). Taking the first
and second partial derivative of A(∆, Λ) with re-
spect to δi we obtain, respectively:
</bodyText>
<equation confidence="0.928403727272727">
∂A(∆, Λ) = E˜p(x,y)[fi] − (λi −λ0 i ) +
∂δi σ2
i
[ ]
− E˜p(x)pΛ(y|x) fi · exp(δif#)
and
∂2A(∆, Λ)
∂δi2 = −
[ ]
−E˜p(x)pΛ(y|x) fi · f# · exp(δif#) &lt; 0
Since A(0, Λ) = 0 and ∂2A(∆,Λ)
</equation>
<bodyText confidence="0.84387">
∂δi2 &lt; 0, by solving
for the unique root ∆* of ∂A(∆,Λ) = 0 we obtain
</bodyText>
<page confidence="0.414518">
∂δi
</page>
<figure confidence="0.57815725">
δi
σ2i
1
σ2i
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.260465">
<title confidence="0.806503">Adaptation of Maximum Entropy Capitalizer: Little Data Can Help a Lot</title>
<author confidence="0.372389">Chelba</author>
<affiliation confidence="0.722457">Microsoft</affiliation>
<address confidence="0.8254595">One Microsoft Redmond, WA</address>
<abstract confidence="0.997746739130435">A novel technique for maximum “a posteriori” (MAP) adaptation of maximum entropy (MaxEnt) and maximum entropy Markov models (MEMM) is presented. The technique is applied to the problem of recovering the correct capitalization of uniformly cased text: a “background” capitalizer trained on 20Mwds of Wall Street Journal (WSJ) text from 1987 is adapted to two Broadcast News (BN) test sets — one containing ABC Primetime Live text and the other NPR Morning News/CNN Morning Edition text — from 1996. The “in-domain” performance of the WSJ capitalizer is 45% better than that of the 1-gram baseline, when evaluated on a test set drawn from WSJ 1994. When evaluating on the mismatched “out-ofdomain” test data, the 1-gram baseline is outperformed by 60%; the improvement brought by the adaptation technique using a very small amount of matched BN data — 25-70kwds — is about 20-25% relative. Overall, automatic capitalization error rate 1.4%is achieved on BN data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="10282" citStr="Berger et al., 1996" startWordPosition="1693" endWordPosition="1696"> one in (Ratnaparkhi, 1996), which uses xi(W, T i−1 1 ) = {wi, wi−1, wi+1, ti−1, ti−2}. We note that the probability model is causal in the sequencing of tags (the probability assignment for ti only depends on previous tags ti−1, ti−2) which allows for efficient algorithms that search for the most likely tag sequence T∗(W) = arg maxT P(T |W) as well as ensures a properly normalized conditional probability model P(T|W). The probability P(ti|xi(W,T i−1 1 )) is modeled using a maximum entropy model. The next section briefly describes the training procedure; for details the reader is referred to (Berger et al., 1996). 3.1 Maximum Entropy State Transition Model The sufficient statistics that are extracted from the training data are tuples (y, #, x) = (ti, #, xi(W, Ti−1 1 )) where ti is the tag assigned in context xi(W, T i−1 1 ) = {wi, wi−1, wi+1, ti−1, ti−2} and # denotes the count with which this event has been observed in the training data. By way of example, the event associated with the first word in the example in Section 2 is (*bdw* denotes a special boundary type): MXC 1 currentword=primetime previousword=*bdw* nextword=continues t1=*bdw* t1,2=*bdw*,*bdw* prefix1=p prefix2=pr prefix3=pri suffix1=e </context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. A. Della Pietra, and V. J. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):39–72, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Some Advances in Transformation-Based Part of Speech Tagging.</title>
<date>1994</date>
<booktitle>In National Conference on Artificial Intelligence,</booktitle>
<pages>722--727</pages>
<contexts>
<context position="6751" citStr="Brill, 1994" startWordPosition="1096" endWordPosition="1097">the performance of their algorithm as well. 2.2 Previous Work We share the approach to capitalization as sequence tagging with that of (Lita et al., 2003). In their approach, a language model is built on pairs (word, tag) and then used to disambiguate over all possible tag assignments to a sentence using dynamic programming techniques. The same idea is explored in (Kim and Woodland, 2004) in the larger context of automatic punctuation generation and capitalization from speech recognition output. A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors. Departing from their work, our approach builds on a standard technique for sequence tagging, namely MEMMs, which has been successfully applied to part-of-speech tagging (Ratnaparkhi, 1996). The MEMM approach models the tag sequence T conditionally on the word sequence W, which has a few substantial advantages over the 1-gram tagging approach: • discriminative training of probability model P(T|W) using conditional maximum likelihood is</context>
<context position="8867" citStr="Brill, 1994" startWordPosition="1443" endWordPosition="1444">looked in favor of the inadequate maximum likelihood HMMs. The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000). We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004). Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario. A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing such adaptation in a rule-based model is unclear, if at all possible. 3 MEMM for Sequence Labeling A simple approach to sequence labeling is the maximum entropy Markov model. The model assigns a probability P(T|W) to any possible tag sequence T = t1 ... tn = T1n for a given word sequence W = w1 ... wn. The probability assignment is done according to: P(ti|xi(W,Ti−1 1 )) wh</context>
</contexts>
<marker>Brill, 1994</marker>
<rawString>Eric Brill. 1994. Some Advances in Transformation-Based Part of Speech Tagging. In National Conference on Artificial Intelligence, pages 722–727.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Ronald Rosenfeld</author>
</authors>
<title>A Survey of Smoothing Techniques for Maximum Entropy Models.</title>
<date>2000</date>
<booktitle>IEEE Transactions on Speech and Audio Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="8492" citStr="Chen and Rosenfeld, 2000" startWordPosition="1376" endWordPosition="1379">dom field (CRF) approach (Lafferty et al., 2001) which slightly outperforms MEMMs on a standard partof-speech tagging task. In a similar vein, the work 3Relative to the current word, whose tag is assigned a probability value by the MEMM. of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs. The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000). We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004). Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario. A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing </context>
<context position="11943" citStr="Chen and Rosenfeld, 2000" startWordPosition="1974" endWordPosition="1977">election We used a simple count cut-off feature selection algorithm which counts the number of occurrences of all features in a predefined set after which it discards the features whose count is less than a pre-specified threshold. The parameter of the feature selection algorithm is the threshold value; a value of 0 will keep all features encountered in the training data. 3.1.2 Parameter Estimation The model parameters A are estimated such that the model assigns maximum log-likelihood to the training data subject to a Gaussian prior centered at 0, A ∼ N(0, diag(u2i )), that ensures smoothing (Chen and Rosenfeld, 2000): L(A) = E ˜p(x, y)log pA(y|x) (1) x,y n P(T|W) = i=1 E Z(x, A) = y λi 2σ&apos; + const(A)i As shown in (Chen and Rosenfeld, 2000) — and rederived in Appendix A for the non-zero mean case — the update equations are: λ(t+1) i= λ(t) i+ δi, where δi satisfies: λi ˜p(x, y)fi(x, y) − = σ2 i ˜p(x)pΛ(y|x)fi(x, y)exp(δif#(x, y)) In our experiments the variances are tied to σi = σ whose value is determined by line search on development data such that it yields the best tagging accuracy. 4 MAP Adaptation of Maximum Entropy Models In the adaptation scenario we already have a MaxEnt model trained on the backgr</context>
<context position="16127" citStr="Chen and Rosenfeld, 2000" startWordPosition="2727" endWordPosition="2730">ous tying schemes although this is a promising research direction. 4.1 Relationship with Minimum Divergence Training Another possibility to adapt the background model is to do minimum KL divergence (MinDiv) train4We use A \ B to denote set difference. F − i=1 E x,y δi + (2) σ2i E x,y F − i=1 ing (Pietra et al., 1995) between the background exponential model B — assumed fixed — and an exponential model A built using the Fbackground U Fadapt feature set. It can be shown that, if we smooth the A model with a Gaussian prior on the feature weights that is centered at 0 — following the approach in (Chen and Rosenfeld, 2000) for smoothing maximum entropy models — then the MinDiv update equations for estimating A on the adaptation data are identical to the MAP adaptation procedure we proposed5. However, we wish to point out that the equivalence holds only if the feature set for the new model A is Fbackground U Fadapt. The straightforward application of MinDiv training — by using only the Fadapt feature set for A — will not result in an equivalent procedure to ours. In fact, the difference in performance between this latter approach and ours could be quite large since the cardinality of Fbackground is typically sev</context>
</contexts>
<marker>Chen, Rosenfeld, 2000</marker>
<rawString>Stanley F. Chen and Ronald Rosenfeld. 2000. A Survey of Smoothing Techniques for Maximum Entropy Models. IEEE Transactions on Speech and Audio Processing, 8(1):37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1--8</pages>
<publisher>ACL.</publisher>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, PA,</location>
<contexts>
<context position="8123" citStr="Collins, 2002" startWordPosition="1315" endWordPosition="1316">es, as well as future words3 are easily incorporated in the probability model • no concept of “out-of-vocabulary” word: subword features are very useful in dealing with words not seen in the training data • ability to integrate rich contextual features into the model More recently, certain drawbacks of MEMM models have been addressed by the conditional random field (CRF) approach (Lafferty et al., 2001) which slightly outperforms MEMMs on a standard partof-speech tagging task. In a similar vein, the work 3Relative to the current word, whose tag is assigned a probability value by the MEMM. of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs. The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000). We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004). Although we do not have a formal derivation, the adaptat</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Michael Collins. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1–8, University of Pennsylvania, Philadelphia, PA, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joshua Goodman</author>
</authors>
<title>Exponential Priors for Maximum Entropy Models.</title>
<date>2004</date>
<booktitle>HLTNAACL 2004: Main Proceedings,</booktitle>
<volume>2</volume>
<pages>305--312</pages>
<editor>In Daniel Marcu Susan Dumais and Salim Roukos, editors,</editor>
<location>Boston, Massachusetts, USA,</location>
<contexts>
<context position="8665" citStr="Goodman, 2004" startWordPosition="1412" endWordPosition="1413">hose tag is assigned a probability value by the MEMM. of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs. The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000). We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004). Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario. A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing such adaptation in a rule-based model is unclear, if at all possible. 3 MEMM for Sequence Labeling A simple approach to sequence labeling is the maximum entropy Markov model</context>
</contexts>
<marker>Goodman, 2004</marker>
<rawString>Joshua Goodman. 2004. Exponential Priors for Maximum Entropy Models. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLTNAACL 2004: Main Proceedings, pages 305– 312, Boston, Massachusetts, USA, May 2 - May 7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji-Hwan Kim</author>
<author>Philip C Woodland</author>
</authors>
<title>Automatic Capitalization Generation for Speech Input.</title>
<date>2004</date>
<journal>Computer Speech and Language,</journal>
<volume>18</volume>
<issue>1</issue>
<pages>90</pages>
<contexts>
<context position="6012" citStr="Kim and Woodland, 2004" startWordPosition="972" endWordPosition="975">quent tag encountered in a large amount of training data. As a special case for automatic capitalization, the most frequent tag for the first word in a sentence is overridden by CAP, thus capitalizing on the fact that the first word in a sentence is most likely capitalized2. 1Unlike the training phase, the sentence segmenter at test time is assumed to operate on uniform case text. 2As with everything in natural language, it is not hard to find exceptions to this “rule”. Due to its popularity, both our work and that of (Lita et al., 2003) uses the 1-gram capitalizer as a baseline. The work in (Kim and Woodland, 2004) indicates that the same 1-gram algorithm is used in Microsoft Word 2000 and is consequently used as a baseline for evaluating the performance of their algorithm as well. 2.2 Previous Work We share the approach to capitalization as sequence tagging with that of (Lita et al., 2003). In their approach, a language model is built on pairs (word, tag) and then used to disambiguate over all possible tag assignments to a sentence using dynamic programming techniques. The same idea is explored in (Kim and Woodland, 2004) in the larger context of automatic punctuation generation and capitalization from</context>
</contexts>
<marker>Kim, Woodland, 2004</marker>
<rawString>Ji-Hwan Kim and Philip C. Woodland. 2004. Automatic Capitalization Generation for Speech Input. Computer Speech and Language, 18(1):67– 90, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In</title>
<date>2001</date>
<booktitle>Proc. 18th International Conf. on Machine Learning,</booktitle>
<pages>282--289</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Francisco, CA.</location>
<contexts>
<context position="7915" citStr="Lafferty et al., 2001" startWordPosition="1277" endWordPosition="1280">ility model P(T|W) using conditional maximum likelihood is well correlated with tagging accuracy • ability to use a rich set of word-level features in a parsimonious way: sub-word features such as prefixes and suffixes, as well as future words3 are easily incorporated in the probability model • no concept of “out-of-vocabulary” word: subword features are very useful in dealing with words not seen in the training data • ability to integrate rich contextual features into the model More recently, certain drawbacks of MEMM models have been addressed by the conditional random field (CRF) approach (Lafferty et al., 2001) which slightly outperforms MEMMs on a standard partof-speech tagging task. In a similar vein, the work 3Relative to the current word, whose tag is assigned a probability value by the MEMM. of (Collins, 2002) explores the use of discriminatively trained HMMs for sequence labeling problems, a fair baseline for such cases that is often overlooked in favor of the inadequate maximum likelihood HMMs. The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000). We are not aware of a</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proc. 18th International Conf. on Machine Learning, pages 282– 289. Morgan Kaufmann, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lita</author>
<author>A Ittycheriah</author>
<author>S Roukos</author>
<author>N Kambhatla</author>
</authors>
<title>tRuEcasIng.</title>
<date>2003</date>
<booktitle>In Proccedings of ACL,</booktitle>
<pages>152--159</pages>
<location>Sapporo, Japan.</location>
<contexts>
<context position="4086" citStr="Lita et al., 2003" startWordPosition="649" endWordPosition="652">ine in most experiments in this work and others. The MEMM sequence labeling technique is briefly reviewed in Section 3. Section 4 describes the MAP adaptation technique used for the capitalization of out-of-domain text. The detailed mathematical derivation is presented in Appendix A. The experimental results are presented in Section 5, followed by conclusions and suggestions for future work. 2 Capitalization as Sequence Tagging Automatic capitalization can be seen as a sequence tagging problem: each lower-case word receives a tag that describes its capitalization form. Similar to the work in (Lita et al., 2003), we tag each word in a sentence with one of the tags: • LOC lowercase • CAP capitalized • MXC mixed case; no further guess is made as to the capitalization of such words. A possibility is to use the most frequent one encountered in the training data. • AUC all upper case • PNC punctuation; we decided to have a separate tag for punctuation since it is quite frequent and models well the syntactic context in a parsimonious way For training a given capitalizer one needs to convert running text into uniform case text accompanied by the above capitalization tags. For example, PrimeTime continues on</context>
<context position="5932" citStr="Lita et al., 2003" startWordPosition="958" endWordPosition="961">ord in a given vocabulary (usually large, 100kwds or more) use the most frequent tag encountered in a large amount of training data. As a special case for automatic capitalization, the most frequent tag for the first word in a sentence is overridden by CAP, thus capitalizing on the fact that the first word in a sentence is most likely capitalized2. 1Unlike the training phase, the sentence segmenter at test time is assumed to operate on uniform case text. 2As with everything in natural language, it is not hard to find exceptions to this “rule”. Due to its popularity, both our work and that of (Lita et al., 2003) uses the 1-gram capitalizer as a baseline. The work in (Kim and Woodland, 2004) indicates that the same 1-gram algorithm is used in Microsoft Word 2000 and is consequently used as a baseline for evaluating the performance of their algorithm as well. 2.2 Previous Work We share the approach to capitalization as sequence tagging with that of (Lita et al., 2003). In their approach, a language model is built on pairs (word, tag) and then used to disambiguate over all possible tag assignments to a sentence using dynamic programming techniques. The same idea is explored in (Kim and Woodland, 2004) i</context>
</contexts>
<marker>Lita, Ittycheriah, Roukos, Kambhatla, 2003</marker>
<rawString>L. Lita, A. Ittycheriah, S. Roukos, and N. Kambhatla. 2003. tRuEcasIng. In Proccedings of ACL, pages 152–159, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug B Paul</author>
<author>Janet M Baker</author>
</authors>
<title>The design for the Wall Street Journal-based CSR corpus.</title>
<date>1992</date>
<booktitle>In Proceedings of the DARPA SLS Workshop.</booktitle>
<contexts>
<context position="17084" citStr="Paul and Baker, 1992" startWordPosition="2888" endWordPosition="2891">nDiv training — by using only the Fadapt feature set for A — will not result in an equivalent procedure to ours. In fact, the difference in performance between this latter approach and ours could be quite large since the cardinality of Fbackground is typically several orders of magnitude larger than that of Fadapt and our approach also updates the weights corresponding to features in Fbackground \ Fadapt. Further experiments are needed to compare the performance of the two approaches. 5 Experiments The baseline 1-gram and the background MEMM capitalizer were trained on various amounts of WSJ (Paul and Baker, 1992) data from 1987 — files WS87_{001-126}. The in-domain test data used was file WS94_000 (8.7kwds). As for the adaptation experiments, two different sets of BN data were used, whose sizes are summarized in Table 1: 1. BN CNN/NPR data. The training/development/test partition consisted of a 3-way random split of file BN624BTS. The resulting sets are denoted CNN-trn/dev/tst, respectively 2. BN ABC Primetime data. The training set consisted of file BN623ATS whereas the development/test set consisted of a 2-way random split of file BN624ATS 5.1 In-Domain Experiments We have proceeded building both 1-</context>
</contexts>
<marker>Paul, Baker, 1992</marker>
<rawString>Doug B. Paul and Janet M. Baker. 1992. The design for the Wall Street Journal-based CSR corpus. In Proceedings of the DARPA SLS Workshop. February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing features of random fields.</title>
<date>1995</date>
<tech>Technical Report CMU-CS-95-144,</tech>
<institution>School of Computer Science, Carnegie Mellon University,</institution>
<location>Pittsburg, PA.</location>
<contexts>
<context position="15820" citStr="Pietra et al., 1995" startWordPosition="2671" endWordPosition="2674">ce σ will thus balance optimally the log-likelihood of the adaptation data with the A0 mean values obtained from the background data. Other tying schemes are possible: separate values could be used for the Fadapt \ Fbackground and Fbackground feature sets, respectively. We did not experiment with various tying schemes although this is a promising research direction. 4.1 Relationship with Minimum Divergence Training Another possibility to adapt the background model is to do minimum KL divergence (MinDiv) train4We use A \ B to denote set difference. F − i=1 E x,y δi + (2) σ2i E x,y F − i=1 ing (Pietra et al., 1995) between the background exponential model B — assumed fixed — and an exponential model A built using the Fbackground U Fadapt feature set. It can be shown that, if we smooth the A model with a Gaussian prior on the feature weights that is centered at 0 — following the approach in (Chen and Rosenfeld, 2000) for smoothing maximum entropy models — then the MinDiv update equations for estimating A on the adaptation data are identical to the MAP adaptation procedure we proposed5. However, we wish to point out that the equivalence holds only if the feature set for the new model A is Fbackground U Fa</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1995</marker>
<rawString>S. Della Pietra, V. Della Pietra, and J. Lafferty. 1995. Inducing features of random fields. Technical Report CMU-CS-95-144, School of Computer Science, Carnegie Mellon University, Pittsburg, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-of-Speech Tagging.</title>
<date>1996</date>
<booktitle>Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>133--142</pages>
<editor>In Eric Brill and Kenneth Church, editors,</editor>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="7101" citStr="Ratnaparkhi, 1996" startWordPosition="1147" endWordPosition="1148">me idea is explored in (Kim and Woodland, 2004) in the larger context of automatic punctuation generation and capitalization from speech recognition output. A second approach they consider for capitalization is the use a rule-based tagger as described by (Brill, 1994), which they show to outperform the case sensitive language modeling approach and be quite robust to speech recognition errors and punctuation generation errors. Departing from their work, our approach builds on a standard technique for sequence tagging, namely MEMMs, which has been successfully applied to part-of-speech tagging (Ratnaparkhi, 1996). The MEMM approach models the tag sequence T conditionally on the word sequence W, which has a few substantial advantages over the 1-gram tagging approach: • discriminative training of probability model P(T|W) using conditional maximum likelihood is well correlated with tagging accuracy • ability to use a rich set of word-level features in a parsimonious way: sub-word features such as prefixes and suffixes, as well as future words3 are easily incorporated in the probability model • no concept of “out-of-vocabulary” word: subword features are very useful in dealing with words not seen in the t</context>
<context position="8928" citStr="Ratnaparkhi, 1996" startWordPosition="1451" endWordPosition="1452">Ms. The work on adapting the MEMM model parameters using MAP smoothing builds on the Gaussian prior model used for smoothing MaxEnt models, as presented in (Chen and Rosenfeld, 2000). We are not aware of any previous work on MAP adaptation of MaxEnt models using a prior, be it Gaussian or a different one, such as the exponential prior of (Goodman, 2004). Although we do not have a formal derivation, the adaptation technique should easily extend to the CRF scenario. A final remark contrasts rule-based approaches to sequence tagging such as (Brill, 1994) with the probabilistic approach taken in (Ratnaparkhi, 1996): having a weight on each feature in the MaxEnt model and a sound probabilistic model allows for a principled way of adapting the model to a new domain; performing such adaptation in a rule-based model is unclear, if at all possible. 3 MEMM for Sequence Labeling A simple approach to sequence labeling is the maximum entropy Markov model. The model assigns a probability P(T|W) to any possible tag sequence T = t1 ... tn = T1n for a given word sequence W = w1 ... wn. The probability assignment is done according to: P(ti|xi(W,Ti−1 1 )) where ti is the tag corresponding to word i and xi(W,Ti−1 1 ) i</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Model for Part-of-Speech Tagging. In Eric Brill and Kenneth Church, editors, Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 133–142. Association for Computational Linguistics, Somerset, New Jersey.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>