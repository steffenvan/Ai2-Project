<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000154">
<title confidence="0.980017">
Arabic Tokenization System
</title>
<author confidence="0.970688">
Mohammed A. Attia
</author>
<affiliation confidence="0.979753">
School of Informatics / The University of Manchester, PO Box
</affiliation>
<address confidence="0.670886">
88, Sackville Street, Manchester M60 1QD, UK
</address>
<email confidence="0.999325">
mohammed.attia@postgrad.manchester.ac.uk
</email>
<sectionHeader confidence="0.995647" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999865076923077">
Tokenization is a necessary and non-trivial
step in natural language processing. In the
case of Arabic, where a single word can
comprise up to four independent tokens,
morphological knowledge needs to be in-
corporated into the tokenizer. In this paper
we describe a rule-based tokenizer that
handles tokenization as a full-rounded
process with a preprocessing stage (white
space normalizer), and a post-processing
stage (token filter). We also show how it
handles multiword expressions, and how
ambiguity is resolved.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999522">
Tokenization is a non-trivial problem as it is
“closely related to the morphological analysis”
(Chanod and Tapanainen 1994). This is even more
the case with languages with rich and complex
morphology such as Arabic. The function of a to-
kenizer is to split a running text into tokens, so that
they can be fed into a morphological transducer or
POS tagger for further processing. The tokenizer is
responsible for defining word boundaries, demar-
cating clitics, multiword expressions, abbreviations
and numbers.
Clitics are syntactic units that do not have free
forms but are instead attached to other words. De-
ciding whether a morpheme is an affix or a clitic
can be confusing. However, we can generally say
that affixes carry morpho-syntactic features (such
as tense, person, gender or number), while clitics
serve syntactic functions (such as negation, defini-
tion, conjunction or preposition) that would other-
wise be served by an independent lexical item.
</bodyText>
<page confidence="0.99769">
65
</page>
<bodyText confidence="0.999163324324324">
Therefore tokenization is a crucial step for a syn-
tactic parser that needs to build a tree from syntac-
tic units. An example of clitics in English is the
genitive suffix “’s” in the student’s book.
Arabic clitics, however, are not as easily recog-
nizable. Clitics use the same alphabet as that of
words with no demarcating mark as the English
apostrophe, and they can be concatenated one after
the other. Without sufficient morphological knowl-
edge, it is impossible to detect and mark clitics. In
this paper we will show different levels of imple-
mentation of the Arabic tokenizer, according to the
levels of linguistic depth involved.
Arabic Tokenization has been described in vari-
ous researches and implemented in many solutions
as it is a required preliminary stage for further
processing. These solutions include morphological
analysis (Beesley 2001; Buckwalter 2002), diacri-
tization (Nelken and Shieber 2005), Information
Retrieval (Larkey and Connell 2002), and POS
Tagging (Diab et al 2004; Habash and Rambow
2005). None of these projects, however, show how
multiword expressions are treated, or how ambigu-
ity is filtered out.
In our research, tokenization is handled in a
rule-based system as an independent process. We
show how the tokenizer interacts with other trans-
ducers, and how multiword expressions are identi-
fied and delimited. We also show how incorrect
tokenizations are filtered out, and how undesired
tokenizations are marked. All tools in this research
are developed in Finite State Technology (Beesley
and Karttunen 2003). These tools have been devel-
oped to serve an Arabic Lexical Functional Gram-
mar parser using XLE (Xerox Linguistics Envi-
ronment) platform as part of the ParGram Project
(Butt et al 2002).
</bodyText>
<note confidence="0.9061775">
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 65–72,
Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.957116" genericHeader="method">
2 Arabic Tokens
</sectionHeader>
<bodyText confidence="0.999973384615385">
A token is the minimal syntactic unit; it can be a
word, a part of a word (or a clitic), a multiword
expression, or a punctuation mark. A tokenizer
needs to know a list of all word boundaries, such
as white spaces and punctuation marks, and also
information about the token boundaries inside
words when a word is composed of a stem and cli-
tics. Throughout this research full form words, i.e.
stems with or without clitics, as well as numbers
will be termed main tokens. All main tokens are
delimited either by a white space or a punctuation
mark. Full form words can then be divided into
sub-tokens, where clitics and stems are separated.
</bodyText>
<subsectionHeader confidence="0.99155">
2.1 Main Tokens
</subsectionHeader>
<bodyText confidence="0.99995925">
A tokenizer relies mainly on white spaces and
punctuation marks as delimiters of word bounda-
ries (or main tokens). Additional punctuation
marks are used in Arabic such as the comma ‘،’,
question mark ‘p’ and semicolon ‘؛’. Numbers are
also considered as main tokens. A few Arab coun-
tries use the Arabic numerals as in English, while
most Arab countries use the Hindi numerals such
as ‘2’ (2) and ‘3’ (3). Therefore a list of all
punctuation marks and number characters must be
fed to the system to allow it to demarcate main
tokens in the text.
</bodyText>
<subsectionHeader confidence="0.999655">
2.2 Sub-Tokens
</subsectionHeader>
<bodyText confidence="0.999814888888889">
Arabic morphotactics allow words to be pre-
fixed or suffixed with clitics (Attia 2006b). Clitics
themselves can be concatenated one after the other.
Furthermore, clitics undergo assimilation with
word stems and with each other, which makes
them even harder to handle in any superficial way.
A verb can comprise up four sub-tokens (a con-
junction, a complementizer, a verb stem and an
object pronoun) as illustrated by Figure 1.
</bodyText>
<figureCaption confidence="0.99759">
Figure 1: Possible sub-tokens in Arabic verbs
</figureCaption>
<bodyText confidence="0.9998475">
Similarly a noun can comprise up to four sub-
tokens. Although Figure 2 shows five sub-tokens
but we must note that the definite article and the
genitive pronoun are mutually exclusive.
</bodyText>
<figureCaption confidence="0.992491">
Figure 2: Possible sub-tokens in Arabic nouns
</figureCaption>
<bodyText confidence="0.999777285714286">
Moreover there are various rules that govern the
combination of words with affixes and clitics.
These rules are called grammar-lexis specifications
(Abbès et al 2004; Dichy 2001; Dichy and Fargaly
2003). An example of these specifications is a rule
that states that adjectives and proper nouns do not
combine with possessive pronouns.
</bodyText>
<sectionHeader confidence="0.978308" genericHeader="method">
3 Development in Finite State Technology
</sectionHeader>
<bodyText confidence="0.999847857142857">
Finite state technology has successfully been
used in developing morphologies and text process-
ing tools for many natural languages, including
Semitic languages. We will explain briefly how
finite state technology works, then we will proceed
into showing how different tokenization models
are implemented.
</bodyText>
<equation confidence="0.9857404">
(1) LEXICON Proclitic
al@U.Def.On@ Root;
Root;
LEXICON Root
kitab Enclitic;
LEXICON Suffix
an Enclitic;
Enclitic;
LEXICON Enclitic
hi@U.Def.Off@ #;
</equation>
<bodyText confidence="0.998431333333333">
In a standard finite state system, lexical entries
along with all possible affixes and clitics are en-
coded in the lexc language which is a right recur-
sive phrase structure grammar (Beesley and Kart-
tunen 2003). A lexc file contains a number of lexi-
cons connected through what is known as “con-
tinuation classes” which determine the path of
concatenation. In example (1) above the lexicon
Proclitic has a lexical form al, which is linked to a
</bodyText>
<page confidence="0.982615">
66
</page>
<bodyText confidence="0.999870695652174">
continuation class named Root. This means that the
forms in Root will be appended to the right of al.
The lexicon Proclitic also has an empty string,
which means that Proclitic itself is optional and
that the path can proceed without it. The bulk of all
lexical entries are presumably listed under Root in
the example.
Sometimes an affix or a clitic requires or for-
bids the existence of another affix or clitic. This is
what is termed “long distance dependencies”
(Beesley and Karttunen 2003). So Flag Diacritics
are introduced to serve as filters on possible con-
catenations to a stem. As we want to prevent Pro-
clitic and Enclitic from co-occurring, for the defi-
nite article and the possessive pronoun are mutu-
ally excusive, we add a Flag Diacritic to each of
them with the same feature name “U.Def”, but
with different value “On/Off”, as shown in (1)
above. At the end we have a transducer with a bi-
nary relation between two sets of strings: the lower
language that contains the surface forms, and the
upper language that contains the analysis, as shown
in (2) for the noun نﺎﺑﺎﺘﻛ kitaban (two books).
</bodyText>
<listItem confidence="0.7803075">
(2) Lower Language: نﺎﺑﺎﺘﻛ
Upper Language: بﺎﺘﻛ+noun+dual+sg
</listItem>
<sectionHeader confidence="0.973223" genericHeader="method">
4 Tokenization Solutions
</sectionHeader>
<bodyText confidence="0.999990137931035">
There are different levels at which an Arabic
tokenizer can be developed, depending on the
depth of the linguistic analysis involved. During
our work with the Arabic grammar we developed
three different solutions, or three models, for Ara-
bic tokenization. These models vary greatly in their
robustness, compliance with the concept of modu-
larity, and the ability to avoid unnecessary ambi-
guities.
The tokenizer relies on white spaces and punc-
tuation marks to demarcate main tokens. In demar-
cating sub-tokens, however, the tokenizer needs
more morphological information. This information
is provided either deterministically by a morpho-
logical transducer, or indeterministically by a to-
ken guesser. Eventually both main tokens and sub-
tokens are marked by the same token boundary,
which is the sign ‘@’ throughout this paper. The
classification into main and sub-tokens is a con-
ceptual idea that helps in assigning the task of
identification to different components.
Identifying main tokens is considered a straight-
forward process that looks for white spaces and
punctuation marks and divides the text accord-
ingly. No further details of main tokens are given
beyond this point. The three models described be-
low are different ways to identify and divide sub-
tokens, or clitics and stems within a full form
word.
</bodyText>
<subsectionHeader confidence="0.9997505">
4.1 Model 1: Tokenization Combined with
Morphological Analysis
</subsectionHeader>
<bodyText confidence="0.986980842105263">
In this implementation the tokenizer and the
morphological analyzer are one and the same. A
single transducer provides both morphological
analysis and tokenization. Examples of the token-
izer/analyser output are shown in (3). The ‘+’ sign
precedes morphological features, while the ‘@’
sign indicates token boundaries.
(3)ﺮﻜﺸﯿﻟو (waliyashkur: and to thank)
و+conj@ل+comp@+verb+pres+sgﺮﻜﺷ@
This sort of implementation is the most linguis-
tically motivated. This is also the most common
form of implementation for Arabic tokenization
(Habash and Rambow 2005). However, it violates
the design concept of modularity which requires
systems to have separate modules for undertaking
separate tasks. For a syntactic parser that requires
the existence of a tokenizer besides a morphologi-
cal analyzer, this implementation is not workable,
and either Model 2 or Model 3 is used instead.
</bodyText>
<subsectionHeader confidence="0.995873">
4.2 Model 2: Tokenization Guesser
</subsectionHeader>
<bodyText confidence="0.999911882352941">
In this model tokenization is separated from
morphological analysis. The tokenizer only detects
and demarcates clitic boundaries. Yet information
on what may constitute a clitic is still needed. This
is why two additional components are required: a
clitics guesser to be integrated with the tokenizer,
and a clitics transducer to be integrated with the
morphological transducer.
Clitics Guesser. We developed a guesser for
Arabic words with all possible clitics and all possi-
ble assimilations. Please refer to (Beesley and
Karttunen 2003) on how to create a basic guesser.
The core idea of a guesser is to assume that a stem
is composed of any arbitrary sequence of Arabic
alphabets, and this stem can be prefixed or/and
suffixed with a limited set of tokens. This guesser
is then used by the tokenizer to mark clitic bounda-
</bodyText>
<page confidence="0.996958">
67
</page>
<bodyText confidence="0.978070282051282">
ries. Due to the nondeterministic nature of a
guesser, there will be increased tokenization ambi-
guities.
(4) ﻞﺟﺮﻠﻟو (and to the man)
@ ﻞﺟر @لا@ل
Clitics Transducer. We must note that Arabic
clitics do not occur individually in natural texts.
They are always attached to words. Therefore a
specialized small-scale morphological transducer is
needed to handle these newly separated forms. We
developed a lexc transducer for clitics only, treat-
ing them as separate words. The purpose of this
transducer is to provide analysis for morphemes
that do not occur independently.
(5) و+conj
ل+prep
لا+art+def
This small-scale specialized transducer is then
unioned (or integrated) with the main morphologi-
cal transducer. Before making the union it is nec-
essary to remove all paths that contain any clitics
in the main morphological transducer to eliminate
redundancies.
In our opinion this is the best model, the advan-
tages are robustness as it is able to deal with any
words whether they are known to the morphologi-
cal transducer or not, and abiding by the concept of
modularity as it separates the process of tokeniza-
tion from morphological analysis.
There are disadvantages, however, for this
model, and among them is that the morphological
analyzer and the syntactic parser have to deal with
increased tokenization ambiguities. The tokenizer
is highly non-deterministic as it depends on a
guesser which, by definition, is non-deterministic.
For a simple sentence of three words, we are faced
with eight different tokenization solutions. None-
theless, this can be handled as explained in subsec-
tion 5.1 on discarding spurious ambiguities.
</bodyText>
<subsectionHeader confidence="0.997386">
4.3 Model 3: Tokenization Dependent on the
Morphological Analyser
</subsectionHeader>
<bodyText confidence="0.999965">
In the above solution, the tokenizer defines the
possible Arabic stem as any arbitrary sequence of
Arabic letters. In this solution, however, the word
stem is not guessed, but taken as a list of actual
words. A possible word in the tokenizer in this
model is any word found in the morphological
transducer. The morphological transducer here is
the same as the one described in subsection 4.1 but
with one difference, that is the output does not in-
clude any morphological features, but only token
boundaries between clitics and stems.
This is a relatively deterministic tokenizer that
handles clitics properly. The main downfall is that
only words found in the morphological transducer
are tokenized. It is not robust, yet it may be more
convenient during grammar debugging, as it pro-
vides much fewer analyses than model 2. Here
spurious ambiguities are successfully avoided.
</bodyText>
<equation confidence="0.425124">
(6) ﻞﺟﺮﻠﻟو (and to the man)
@ ﻞﺟر @لا@ل
</equation>
<bodyText confidence="0.999961777777778">
One advantage of this implementation is that
the tool becomes more deterministic and more
manageable in debugging. Its lack of robustness,
however, makes it mostly inapplicable as no single
morphological transducer can claim to comprise all
the words in a language. In our XLE grammar, this
model is only 0.05% faster than Model 2. This is
not statistically significant advantage compared to
its limitations.
</bodyText>
<subsectionHeader confidence="0.996118">
4.4 Tokenizing Multiword Expressions
</subsectionHeader>
<bodyText confidence="0.999861">
Multiword Expressions (MWEs) are two or
more words that behave like a single word syntac-
tically and semantically. They are defined, more
formally, as “idiosyncratic interpretations that
cross word boundaries” (Sag et al 2001). MWEs
cover expressions that are traditionally classified as
idioms (e.g. down the drain), prepositional verbs
(e.g. rely on), verbs with particles (e.g. give up),
compound nouns (e.g. traffic lights) and colloca-
tions (e.g. do a favour).
With regard to syntactic and morphological
flexibility, MWEs are classified into three types:
fixed, semi-fixed and syntactically flexible expres-
sions (Baldwin 2004; Oflazer et al 2004; Sag et al
2001).
a. Fixed Expressions. These expressions are
lexically, syntactically and morphologically rigid.
An expression of this type is considered as a word
with spaces (a single word that happens to contain
</bodyText>
<figure confidence="0.768250428571428">
@و
@و
@ﻞﺟﺮﻟا@ل
@و
@ﻞﺟﺮﻠﻟ
@ﻞﺟﺮﻠﻟو
@و
</figure>
<page confidence="0.991414">
68
</page>
<bodyText confidence="0.9969902">
spaces), such as ﻂﺳ وﻷا قﺮﺸﻟا al-sharq al-awsat (the
Middle East) and ﻢﺤﻟ ﺖﯿﺑ bait lahem (Bethlehem).
b. Semi-Fixed Expressions. These expressions
can undergo variations, but still the components of
the expression are adjacent. The variations are of
two types, morphological variations where lexical
items can express person, number, tense, gender,
etc., such as the examples in (7), and lexical varia-
tions, where one word can be replaced by another
as in (8).
</bodyText>
<equation confidence="0.835271875">
(7.a) ﺔﯿﻟﺎﻘﺘﻧا ةﺮﺘﻓ
fatratah intiqaliyyah
translational.sg.fem period.sg.fem
(7.b) نﺎﺘﯿﻟﺎﻘﺘﻧا نﺎﺗﺮﺘﻓ
fatratan intiqaliyyatan
translational.dual.fem period.dual.fem
(8) ﺔﻄﯿﺴ ﺒﻟا/ ضرﻷا ﮫﺟو / ﺮﮭﻇ ﻰﻠﻋ
ala zahr/wajh al-ard/al-basitah
</equation>
<bodyText confidence="0.991972583333334">
on the face/surface of the land/earth
(on the face of the earth)
c. Syntactically Flexible Expressions. These
are the expressions that can either undergo reorder-
ing, such as passivization (e.g. the cat was let out
of the bag), or allow external elements to intervene
between the components such as (9.b), where the
adjacency of the MWE is disrupted.
(9.a)ﺔﯾرﺎﻧ ﺔﺟارد
darrajah nariyyah
bike fiery (motorbike)
(9.b) ﺔﯾرﺎﻨﻟا ﺪﻟﻮﻟا ﺔﺟارد
darrajat al-walad al-nariyyah
the-bike the-boy the-fiery (the boy’s motorbike)
Fixed and semi-fixed expressions are identified
and marked by the tokenizer, while syntactically
flexible expressions can only be handled by a syn-
tactic parser (Attia 2006a).
The tokenizer is responsible for treating MWEs
in a special way. They should be marked as single
tokens with the inner space(s) preserved. For this
purpose, as well as for the purpose of morphologi-
cal analysis, a specialized transducer is developed
for MWEs that lists all variations of MWEs and
provides analyses for them (Attia 2006a).
One way to allow the tokenizer to handle
MWEs is to embed the MWEs in the Tokenizer
(Beesley and Karttunen 2003). Yet a better ap-
proach, described by (Karttunen et al 1996), is to
develop one or several multiword transducers or
“staplers” that are composed on the tokenizer. We
will explain here how this is implemented in our
solution, where the list of MWEs is extracted from
the MWE transducer and composed on the token-
izer. Let’s look at the composition regular expres-
sion:
</bodyText>
<equation confidence="0.98900275">
(10) 1 singleTokens.i
2 .o. ?* 0:&amp;quot;[[[&amp;quot; (MweTokens.l) 0:&amp;quot;]]]&amp;quot; ?*
3 .o. &amp;quot;@&amp;quot; -&gt; &amp;quot; &amp;quot;  ||&amp;quot;[[[&amp;quot; [Alphabet*  |&amp;quot;@&amp;quot;*] _
4 .o. &amp;quot;[[[&amp;quot; -&gt; [] .o. &amp;quot;]]]&amp;quot; -&gt; []].i;
</equation>
<bodyText confidence="0.956056307692308">
Single words separated by the ‘@’ sign are defined
in the variable singleTokens and the MWE trans-
ducer is defined in MweTokens. In the MWE
transducer all spaces in the lower language are re-
placed by “@” so that the lower language can be
matched against singleTokens. In line 1 the single-
Tokens is inverted (the upper language is shifted
down) by the operator “.i” so that composition
goes on the side that contains the relevant strings.
From the MWE transducer we take only the lower
language (or the surface form) by the operator “.l”
in line 2. Single words are searched and if they
contain any MWEs, the expressions will (option-
ally) be enclosed by three brackets on either side.
Line 3 replaces all “@” signs with spaces in side
MWEs only. The two compositions in line 4 re-
move the intermediary brackets.
Let’s now show this with a working example.
For the phrase in (11), the tokenizer first gives the
output in (12). Then after the MWEs are composed
with the tokenizer, we obtain the result in (13) with
the MWE identified as a single token.
(11) ﺎﮭﺘﯿﺟرﺎﺧ ﺮﯾزﻮﻟو
wa-liwazir kharijiyatiha
and-to-foreign minister-its
(and to its foreign minister)
</bodyText>
<equation confidence="0.88871375">
(12) @ﺎھ@ﺔﯿﺟرﺎﺧ@ﺮﯾزو@ل@و
(approx. and@to@foreign@minister@its@)
(13) ﺎھ@ﺔﯿﺟرﺎﺧ ﺮﯾزو@ل@و
(approx. and@to@foreign minister@its@)
</equation>
<subsectionHeader confidence="0.992621">
4.5 Normalizing White Spaces
</subsectionHeader>
<bodyText confidence="0.99913875">
White space normalization is a preliminary
stage to tokenization where redundant and mis-
placed white spaces are corrected, to enable the
tokenizer to work on a clean and predictable text.
</bodyText>
<page confidence="0.998435">
69
</page>
<bodyText confidence="0.9998245">
In real-life data spaces may not be as regularly and
consistently used as expected. There may be two or
more spaces, or even tabs, instead of a single
space. Spaces might even be added before or after
punctuation marks in the wrong manner. There-
fore, there is a need for a tool that eliminates in-
consistency in using white spaces, so that when the
text is fed into a tokenizer or morphological ana-
lyzer, words and expressions can be correctly iden-
tified and analyzed. Table 1 shows where spaces
are not expected before or after some punctuation
marks.
</bodyText>
<table confidence="0.9937092">
No Space Before No Space After
) (
} {
] [
” “
</table>
<tableCaption confidence="0.8778575">
Table 1. Space distribution with some punctuation
marks
</tableCaption>
<bodyText confidence="0.999970142857143">
We have developed a white space normalizer
whose function is to go through real-life texts and
correct mistakes related to the placement of white
spaces. When it is fed an input such as the one in
(14.a) in which additional spaces are inserted and
some spaces are misplaced, it corrects the errors
and gives the output in (14.b):
</bodyText>
<equation confidence="0.589667">
(14.a) . مﻼﺴﻟا ﻰﻟإ دﻮﻘﯿﺳ ( ﺔﯿﻃاﺮﻘﻤﯾﺪﻟا ) ﺮﺸﻧ
(14.b) . مﻼﺴﻟا ﻰﻟإ دﻮﻘﯿﺳ ( ﺔﯿﻃاﺮﻘﻤﯾﺪﻟا ) ﺮﺸﻧ
</equation>
<sectionHeader confidence="0.954729" genericHeader="method">
5 Resolving Ambiguity
</sectionHeader>
<bodyText confidence="0.9999744">
There are different types of ambiguity. There
are spurious ambiguities created by the guesser.
There are also ambiguities which do not exist in
the text before tokenization but are only created
during the tokenization process. Finally there are
real ambiguities, where a form can be read as a
single word or two sub-tokens, or where an MWE
has a compositional reading. These three types are
treated by the following three subsections respec-
tively.
</bodyText>
<subsectionHeader confidence="0.999301">
5.1 Discarding Spurious Ambiguities
</subsectionHeader>
<bodyText confidence="0.999955166666667">
Tokenization Model 2 discussed above in subsec-
tion 4.2 is chosen as the optimal implementation
due to its efficiency and robustness, yet it is highly
nondeterministic and produces a large number of
spurious ambiguities. Therefore, a morphological
transducer is needed to filter out the tokenization
paths that contain incorrect sub-tokens. Recall ex-
ample (4) which contained the output of the nonde-
terministic tokenizer. In (15) below, after the out-
put is fed into a morphological transducer, only
one solution is accepted and the rest are discarded,
as underlined words do not constitute valid stems.
</bodyText>
<equation confidence="0.6696928">
(15) ﻞﺟﺮﻠﻟو (and to the man)
@ﻞﺟر@لا@ل@و - Passed.
@ﻞﺟﺮﻟا@ل@و - Discarded.
@ﻞﺟﺮﻠﻟ@و - Discarded.
@ﻞﺟﺮﻠﻟو - Discarded.
</equation>
<subsectionHeader confidence="0.999513">
5.2 Handling Tokenization Ambiguities
</subsectionHeader>
<bodyText confidence="0.999991192307692">
Among the function of a tokenizer is separate
clitics from stems. Some clitics, however, when
separated, become ambiguous with other clitics
and also with other free forms. For example the
word ﻢﮭﺑﺎﺘﻛ kitabahum has only one morphological
reading (meaning their book), but after tokeniza-
tion ﻢھ@بﺎﺘﻛ there are three different readings, as
the second token ﻢھ can either be a clitic genitive
pronoun (the intended reading) or a free pronoun
they (a book, they) or a noun meaning worry
(forming the compound book of worry).
This problem is solved by inserting a mark that
precedes enclitics and follows proclitics to distin-
guish them from each other as well as from free
forms (Ron M. Kaplan and Martin Forst, personal
communications, Oxford, UK, 20 September
2006). The mark we choose is the Arabic elonga-
tion short line called cashida which is originally
used for graphical decorative purposes and looks
natural with most clitics. To illustrate the usage, a
two-word string (16.a) will be rendered without
cashidas as in (16.b), and a single-word string that
contains clitics (17.a) will be rendered with a dis-
tinctive cashida before the enclitic pronoun as in
(17.b). This indicates that the pronoun is attached
to the preceding word and not standing alone.
</bodyText>
<equation confidence="0.8347084">
(16.a) ﻢھ بﺎﺘﻛ
kitab hum/hamm (book of worry/a book, they)
(16.b) ﻢھ@بﺎﺘﻛ
(17.a) ﻢﮭﺑﺎﺘﻛ kitabuhum (their book)
(17.b) ﻢﮭـ@بﺎﺘﻛ
</equation>
<page confidence="0.973998">
70
</page>
<bodyText confidence="0.999840235294118">
This implementation will also resolve a similar
ambiguity, that is ambiguity arising between pro-
clitics and enclitics. The proclitic preposition ك ka
(as) always occurs initially. There is a homo-
graphic enclitic object pronoun ك ka (you) that
always occurs in the final position. This can create
ambiguity in instances such as the made-up sen-
tence in (18.a). The sentence has the initial tokeni-
zation of (18.b) without a cashida, and therefore
the central token becomes ambiguous as it can now
be attached either to the preceding or following
word leading either to the readings in (18.a) or
(18.c). The cashida placement, however, resolves
this ambiguity as in (18.d). The cashida is added
after the token, indicating that it is attached to the
following word and now only the reading in (18.a)
is possible.
</bodyText>
<equation confidence="0.833887166666667">
(18.a) ﺮﯿﻣﻷﺎﻛ ﺖﯿﻄﻋأ
a’taitu ka-lamir (I gave like a prince)
(18.b) ﺮﯿﻣﻷا@ك@ﺖﯿﻄﻋأ
(18.c) ﺮﯿﻣﻷا ﻚﺘﯿﻄﻋأ
a’taitu-ka alamir (I gave you the prince)
(18.d) ﺮﯿﻣﻷا @ـﻛ@ ﺖﯿﻄﻋأ
</equation>
<subsectionHeader confidence="0.997977">
5.3 Handling Real Ambiguities
</subsectionHeader>
<bodyText confidence="0.999981157894737">
Some tokenization readings are legal, yet highly
infrequent and undesired in real-life data. These
undesired readings create onerous ambiguities, as
they are confused with more common and more
acceptable forms. For example the Arabic preposi-
tion ﺪﻌﺑ ba’d (after) has the possible remote reading
of being split into two tokens ﺪﻋ@ـﺑ, which is made
of two elements: ـﺑ bi (with) and ﺪﻋ ‘add (counting).
Similarly ﻦﯿﺑ baina (between) has the possible re-
mote reading ﻦﯾ@ـﺑ, which is made of two tokens
as well: ـﺑ bi (with) and ﻦﯾ yin (Yen).
The same problem occurs with MWEs. The op-
timal handling of MWEs is to treat them as single
tokens and leave internal spaces intact. Yet a non-
deterministic tokenizer allows MWEs to be ana-
lysed compositionally as individual words. So the
MWE لﻮﺠﺘﻟا ﺮﻈﺣ hazr al-tajawwul (curfew) has
two analyses, as in (19), although the composi-
tional reading in (19.b) is undesired.
</bodyText>
<equation confidence="0.831140666666667">
(19.a) @لﻮﺠﺘﻟا ﺮﻈﺣ hazr al-tajawwul (curfew)
(19.b) لﻮﺠﺘﻟا@ ﺮﻈﺣ
hazr (forbidding) al-tajawwul (walking)
</equation>
<bodyText confidence="0.990841307692308">
The solution to this problem is to mark the un-
desired readings. This is implemented by develop-
ing a filter, or a finite state transducer that contains
all possible undesired tokenization possibilities and
attaches the “+undesired” tag to each one of them.
Undesired tokens, such as ﻦﯾ@ـﺑ and ﺪﻋ@ـﺑ,
explained above, can be included in a custom list
in the token filter. As for MWEs, the token filter
imports a list from the MWE transducer and re-
places the spaces with the token delimiter ‘@’ to
denote the undesired tokenization solutions. The
token filter then matches the lists against the output
of the tokenizer. If the output contains a matching
string a mark is added, giving the output in (20).
Notice how (20.b) is marked with the “+undesired”
tag.
(20.a) لﻮﺠﺘﻟا ﺮﻈﺣ@ [hazr al-tajawwul (curfew)]
(20.b) لﻮﺠﺘﻟا@ﺮﻈﺣ+undesired
This transducer or filter is composed on top of
the core tokenizer. The overall design of the token-
izer and its interaction with other finite state com-
ponents is shown in Figure 3. WE must note that
the tokenizer, in its interaction with the morpho-
logical transducer and the MWE transducer, does
not seek morpho-syntactic information, but it que-
ries for lists and possible combinations.
</bodyText>
<figureCaption confidence="0.991373">
Figure 3: Design of the Arabic Tokenizer
</figureCaption>
<sectionHeader confidence="0.99899" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99995">
Tokenization is a process that is closely connected
to and dependent on morphological analysis. In our
research we show how different models of tokeni-
zation are implemented at different levels of lin-
guistic depth. We also explain how the tokenizer
</bodyText>
<page confidence="0.995332">
71
</page>
<bodyText confidence="0.9999095">
interacts with other components1, and how it re-
solves complexity and filters ambiguity. By apply-
ing token filters we gain control over the tokeniza-
tion output.
</bodyText>
<sectionHeader confidence="0.996453" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873614457831">
Abbès R, Dichy J, Hassoun M (2004): The Architecture
of a Standard Arabic lexical database: some figures,
ratios and categories from the DIINAR.1 source pro-
gram, The Workshop on Computational Approaches
to Arabic Script-based Languages, COLING 2004.
Geneva, Switzerland.
Attia M (2006a): Accommodating Multiword Expres-
sions in an Arabic LFG Grammar. In Salakoski T,
Ginter F, Pyysalo S, Pahikkala T (eds), Advances in
Natural Language Processing, 5th International Con-
ference on NLP, FinTAL 2006, Turku, Finland, Vol
4139. Turku, Finland: Springer-Verlag Berlin Hei-
delberg, pp 87-98.
Attia M (2006b): An Ambiguity-Controlled Morpho-
logical Analyzer for Modern Standard Arabic Model-
ling Finite State Networks, The Challenge of Arabic
for NLP/MT Conference. The British Computer So-
ciety, London, UK.
Baldwin T (2004): Multiword Expressions, an Ad-
vanced Course, The Australasian Language Technol-
ogy Summer School (ALTSS 2004). Sydney, Austra-
lia.
Beesley KR (2001): Finite-State Morphological Analy-
sis and Generation of Arabic at Xerox Research:
Status and Plans in 2001, Proceedings of the Arabic
Language Processing: Status and Prospect--39th An-
nual Meeting of the Association for Computational
Linguistics. Toulouse, France.
Beesley KR, Karttunen L (2003): Finite State Morphol-
ogy. Stanford, Calif.: CSLI.
Buckwalter T (2002): Buckwalter Arabic Morphologi-
cal Analyzer Version 1.0., Linguistic Data Consor-
tium. Catalog number LDC2002L49, and ISBN 1-
58563-257-0.
Butt M, Dyvik H, King TH, Masuichi H, Rohrer C
(2002): The Parallel Grammar Project, COLING-
2002 Workshop on Grammar Engineering and
Evaluation. Taipei, Taiwan.
1 The tokenizer along with a number of other Arabic
finite state tools are made available for evaluation on
the website: www.attiapace.com
Chanod J-P, Tapanainen P (1994): A Non-Deterministic
Tokenizer for Finite-State Parsing, ECAI&apos;96. Buda-
pest, Hungary.
Diab M, Hacioglu K, Jurafsky D (2004): Automatic
Tagging of Arabic Text: From Raw Text to Base
Phrase Chunks, Proceedings of NAACL-HLT 2004.
Boston.
Dichy J (2001): On lemmatization in Arabic. A formal
definition of the Arabic entries of multilingual lexical
databases, ACL 39th Annual Meeting. Workshop on
Arabic Language Processing; Status and Prospect.
Toulouse, pp 23-30.
Dichy J, Fargaly A (2003): Roots &amp; Patterns vs. Stems
plus Grammar-Lexis Specifications: on what basis
should a multilingual lexical database centred on
Arabic be built?, Proceedings of the MT-Summit IX
workshop on Machine Translation for Semitic Lan-
guages. New-Orleans.
Habash N, Rambow O (2005): Arabic Tokenization,
Part-of-Speech Tagging and Morphological Disam-
biguation in One Fell Swoop, Proceedings of ACL
2005. Michigan.
Karttunen L, Chanod J-P, Grefenstette G, Schiller A
(1996): Regular expressions for language engineer-
ing. Natural Language Engineering 2:305-328.
Larkey LS, Connell ME (2002): Arabic Information
Retrieval at UMass. In Voorhees EM, Harman DK
(eds), The Tenth Text Retrieval Conference, TREC
2001. Maryland: NIST Special Publication, pp 562-
570.
Nelken R, Shieber SM (2005): Arabic Diacritization
Using Weighted Finite-State Transducers, Proceed-
ings of the 2005 ACL Workshop on Computational
Approaches to Semitic Languages. Michigan.
Oflazer K, Uglu ÖÇ, Say B (2004): Integrating Mor-
phology with Multi-word Expression Processing in
Turkish, Second ACL Workshop on Multiword Ex-
pressions: Integrating Processing. Spain, pp 64-71.
Sag IA, Baldwin T, Bond F, Copestake A, Flickinger D
(2001): Multi-word Expressions: A Pain in the Neck
for NLP, LinGO Working Papers. Stanford Univer-
sity, CA.
</reference>
<page confidence="0.998716">
72
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.979165">
<title confidence="0.999914">Arabic Tokenization System</title>
<author confidence="0.999979">Mohammed A Attia</author>
<affiliation confidence="0.999147">School of Informatics / The University of Manchester, PO Box</affiliation>
<address confidence="0.991335">88, Sackville Street, Manchester M60 1QD, UK</address>
<email confidence="0.998257">mohammed.attia@postgrad.manchester.ac.uk</email>
<abstract confidence="0.999295571428571">Tokenization is a necessary and non-trivial step in natural language processing. In the case of Arabic, where a single word can comprise up to four independent tokens, morphological knowledge needs to be incorporated into the tokenizer. In this paper we describe a rule-based tokenizer that handles tokenization as a full-rounded process with a preprocessing stage (white space normalizer), and a post-processing stage (token filter). We also show how it handles multiword expressions, and how ambiguity is resolved.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Abbès</author>
<author>J Dichy</author>
<author>M Hassoun</author>
</authors>
<title>The Architecture of a Standard Arabic lexical database: some figures, ratios and categories from the DIINAR.1 source program,</title>
<date>2004</date>
<booktitle>The Workshop on Computational Approaches to Arabic Script-based Languages, COLING</booktitle>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="5680" citStr="Abbès et al 2004" startWordPosition="910" endWordPosition="913">rder to handle in any superficial way. A verb can comprise up four sub-tokens (a conjunction, a complementizer, a verb stem and an object pronoun) as illustrated by Figure 1. Figure 1: Possible sub-tokens in Arabic verbs Similarly a noun can comprise up to four subtokens. Although Figure 2 shows five sub-tokens but we must note that the definite article and the genitive pronoun are mutually exclusive. Figure 2: Possible sub-tokens in Arabic nouns Moreover there are various rules that govern the combination of words with affixes and clitics. These rules are called grammar-lexis specifications (Abbès et al 2004; Dichy 2001; Dichy and Fargaly 2003). An example of these specifications is a rule that states that adjectives and proper nouns do not combine with possessive pronouns. 3 Development in Finite State Technology Finite state technology has successfully been used in developing morphologies and text processing tools for many natural languages, including Semitic languages. We will explain briefly how finite state technology works, then we will proceed into showing how different tokenization models are implemented. (1) LEXICON Proclitic al@U.Def.On@ Root; Root; LEXICON Root kitab Enclitic; LEXICON </context>
</contexts>
<marker>Abbès, Dichy, Hassoun, 2004</marker>
<rawString>Abbès R, Dichy J, Hassoun M (2004): The Architecture of a Standard Arabic lexical database: some figures, ratios and categories from the DIINAR.1 source program, The Workshop on Computational Approaches to Arabic Script-based Languages, COLING 2004. Geneva, Switzerland.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Attia</author>
</authors>
<title>(2006a): Accommodating Multiword Expressions in an Arabic LFG Grammar. In</title>
<booktitle>Advances in Natural Language Processing, 5th International Conference on NLP, FinTAL 2006,</booktitle>
<pages>87--98</pages>
<publisher>Springer-Verlag</publisher>
<location>Turku, Finland, Vol 4139. Turku, Finland:</location>
<marker>Attia, </marker>
<rawString>Attia M (2006a): Accommodating Multiword Expressions in an Arabic LFG Grammar. In Salakoski T, Ginter F, Pyysalo S, Pahikkala T (eds), Advances in Natural Language Processing, 5th International Conference on NLP, FinTAL 2006, Turku, Finland, Vol 4139. Turku, Finland: Springer-Verlag Berlin Heidelberg, pp 87-98.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M Attia</author>
</authors>
<title>(2006b): An Ambiguity-Controlled Morphological Analyzer for Modern Standard Arabic Modelling Finite State Networks, The Challenge of Arabic for NLP/MT Conference. The British Computer Society,</title>
<location>London, UK.</location>
<marker>Attia, </marker>
<rawString>Attia M (2006b): An Ambiguity-Controlled Morphological Analyzer for Modern Standard Arabic Modelling Finite State Networks, The Challenge of Arabic for NLP/MT Conference. The British Computer Society, London, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Baldwin</author>
</authors>
<title>Multiword Expressions, an Advanced Course,</title>
<date>2004</date>
<booktitle>The Australasian Language Technology Summer School (ALTSS 2004).</booktitle>
<location>Sydney, Australia.</location>
<contexts>
<context position="14767" citStr="Baldwin 2004" startWordPosition="2351" endWordPosition="2352">sions (MWEs) are two or more words that behave like a single word syntactically and semantically. They are defined, more formally, as “idiosyncratic interpretations that cross word boundaries” (Sag et al 2001). MWEs cover expressions that are traditionally classified as idioms (e.g. down the drain), prepositional verbs (e.g. rely on), verbs with particles (e.g. give up), compound nouns (e.g. traffic lights) and collocations (e.g. do a favour). With regard to syntactic and morphological flexibility, MWEs are classified into three types: fixed, semi-fixed and syntactically flexible expressions (Baldwin 2004; Oflazer et al 2004; Sag et al 2001). a. Fixed Expressions. These expressions are lexically, syntactically and morphologically rigid. An expression of this type is considered as a word with spaces (a single word that happens to contain @و @و @ﻞﺟﺮﻟا@ل @و @ﻞﺟﺮﻠﻟ @ﻞﺟﺮﻠﻟو @و 68 spaces), such as ﻂﺳ وﻷا قﺮﺸﻟا al-sharq al-awsat (the Middle East) and ﻢﺤﻟ ﺖﯿﺑ bait lahem (Bethlehem). b. Semi-Fixed Expressions. These expressions can undergo variations, but still the components of the expression are adjacent. The variations are of two types, morphological variations where lexical items can express person</context>
</contexts>
<marker>Baldwin, 2004</marker>
<rawString>Baldwin T (2004): Multiword Expressions, an Advanced Course, The Australasian Language Technology Summer School (ALTSS 2004). Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beesley KR</author>
</authors>
<title>Finite-State Morphological Analysis and Generation of Arabic at Xerox Research: Status and Plans</title>
<date>2001</date>
<booktitle>in 2001, Proceedings of the Arabic Language Processing: Status and Prospect--39th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<location>Toulouse, France.</location>
<marker>KR, 2001</marker>
<rawString>Beesley KR (2001): Finite-State Morphological Analysis and Generation of Arabic at Xerox Research: Status and Plans in 2001, Proceedings of the Arabic Language Processing: Status and Prospect--39th Annual Meeting of the Association for Computational Linguistics. Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beesley KR</author>
<author>L Karttunen</author>
</authors>
<title>Finite State Morphology.</title>
<date>2003</date>
<publisher>CSLI.</publisher>
<location>Stanford, Calif.:</location>
<marker>KR, Karttunen, 2003</marker>
<rawString>Beesley KR, Karttunen L (2003): Finite State Morphology. Stanford, Calif.: CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Buckwalter</author>
</authors>
<title>Buckwalter Arabic Morphological Analyzer Version 1.0., Linguistic Data Consortium.</title>
<date>2002</date>
<booktitle>Catalog number LDC2002L49, and ISBN</booktitle>
<pages>1--58563</pages>
<contexts>
<context position="2559" citStr="Buckwalter 2002" startWordPosition="396" endWordPosition="397">. Clitics use the same alphabet as that of words with no demarcating mark as the English apostrophe, and they can be concatenated one after the other. Without sufficient morphological knowledge, it is impossible to detect and mark clitics. In this paper we will show different levels of implementation of the Arabic tokenizer, according to the levels of linguistic depth involved. Arabic Tokenization has been described in various researches and implemented in many solutions as it is a required preliminary stage for further processing. These solutions include morphological analysis (Beesley 2001; Buckwalter 2002), diacritization (Nelken and Shieber 2005), Information Retrieval (Larkey and Connell 2002), and POS Tagging (Diab et al 2004; Habash and Rambow 2005). None of these projects, however, show how multiword expressions are treated, or how ambiguity is filtered out. In our research, tokenization is handled in a rule-based system as an independent process. We show how the tokenizer interacts with other transducers, and how multiword expressions are identified and delimited. We also show how incorrect tokenizations are filtered out, and how undesired tokenizations are marked. All tools in this resea</context>
</contexts>
<marker>Buckwalter, 2002</marker>
<rawString>Buckwalter T (2002): Buckwalter Arabic Morphological Analyzer Version 1.0., Linguistic Data Consortium. Catalog number LDC2002L49, and ISBN 1-58563-257-0.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Butt</author>
<author>H Dyvik</author>
<author>King TH</author>
<author>H Masuichi</author>
<author>C Rohrer</author>
</authors>
<date>2002</date>
<booktitle>The Parallel Grammar Project, COLING2002 Workshop on Grammar Engineering and Evaluation.</booktitle>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="3418" citStr="Butt et al 2002" startWordPosition="530" endWordPosition="533">ty is filtered out. In our research, tokenization is handled in a rule-based system as an independent process. We show how the tokenizer interacts with other transducers, and how multiword expressions are identified and delimited. We also show how incorrect tokenizations are filtered out, and how undesired tokenizations are marked. All tools in this research are developed in Finite State Technology (Beesley and Karttunen 2003). These tools have been developed to serve an Arabic Lexical Functional Grammar parser using XLE (Xerox Linguistics Environment) platform as part of the ParGram Project (Butt et al 2002). Proceedings of the 5th Workshop on Important Unresolved Matters, pages 65–72, Prague, Czech Republic, June 2007. c�2007 Association for Computational Linguistics 2 Arabic Tokens A token is the minimal syntactic unit; it can be a word, a part of a word (or a clitic), a multiword expression, or a punctuation mark. A tokenizer needs to know a list of all word boundaries, such as white spaces and punctuation marks, and also information about the token boundaries inside words when a word is composed of a stem and clitics. Throughout this research full form words, i.e. stems with or without clitic</context>
</contexts>
<marker>Butt, Dyvik, TH, Masuichi, Rohrer, 2002</marker>
<rawString>Butt M, Dyvik H, King TH, Masuichi H, Rohrer C (2002): The Parallel Grammar Project, COLING2002 Workshop on Grammar Engineering and Evaluation. Taipei, Taiwan.</rawString>
</citation>
<citation valid="false">
<title>1 The tokenizer along with a number of other Arabic finite state tools are made available for evaluation on the website: www.attiapace.com</title>
<marker></marker>
<rawString>1 The tokenizer along with a number of other Arabic finite state tools are made available for evaluation on the website: www.attiapace.com</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-P Chanod</author>
<author>P Tapanainen</author>
</authors>
<title>A Non-Deterministic Tokenizer for Finite-State Parsing,</title>
<date>1994</date>
<location>ECAI&apos;96. Budapest, Hungary.</location>
<contexts>
<context position="856" citStr="Chanod and Tapanainen 1994" startWordPosition="119" endWordPosition="122">and non-trivial step in natural language processing. In the case of Arabic, where a single word can comprise up to four independent tokens, morphological knowledge needs to be incorporated into the tokenizer. In this paper we describe a rule-based tokenizer that handles tokenization as a full-rounded process with a preprocessing stage (white space normalizer), and a post-processing stage (token filter). We also show how it handles multiword expressions, and how ambiguity is resolved. 1 Introduction Tokenization is a non-trivial problem as it is “closely related to the morphological analysis” (Chanod and Tapanainen 1994). This is even more the case with languages with rich and complex morphology such as Arabic. The function of a tokenizer is to split a running text into tokens, so that they can be fed into a morphological transducer or POS tagger for further processing. The tokenizer is responsible for defining word boundaries, demarcating clitics, multiword expressions, abbreviations and numbers. Clitics are syntactic units that do not have free forms but are instead attached to other words. Deciding whether a morpheme is an affix or a clitic can be confusing. However, we can generally say that affixes carry</context>
</contexts>
<marker>Chanod, Tapanainen, 1994</marker>
<rawString>Chanod J-P, Tapanainen P (1994): A Non-Deterministic Tokenizer for Finite-State Parsing, ECAI&apos;96. Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
<author>K Hacioglu</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic Tagging of Arabic Text: From Raw Text to Base Phrase Chunks,</title>
<date>2004</date>
<booktitle>Proceedings of NAACL-HLT 2004.</booktitle>
<location>Boston.</location>
<contexts>
<context position="2684" citStr="Diab et al 2004" startWordPosition="413" endWordPosition="416">ted one after the other. Without sufficient morphological knowledge, it is impossible to detect and mark clitics. In this paper we will show different levels of implementation of the Arabic tokenizer, according to the levels of linguistic depth involved. Arabic Tokenization has been described in various researches and implemented in many solutions as it is a required preliminary stage for further processing. These solutions include morphological analysis (Beesley 2001; Buckwalter 2002), diacritization (Nelken and Shieber 2005), Information Retrieval (Larkey and Connell 2002), and POS Tagging (Diab et al 2004; Habash and Rambow 2005). None of these projects, however, show how multiword expressions are treated, or how ambiguity is filtered out. In our research, tokenization is handled in a rule-based system as an independent process. We show how the tokenizer interacts with other transducers, and how multiword expressions are identified and delimited. We also show how incorrect tokenizations are filtered out, and how undesired tokenizations are marked. All tools in this research are developed in Finite State Technology (Beesley and Karttunen 2003). These tools have been developed to serve an Arabic</context>
</contexts>
<marker>Diab, Hacioglu, Jurafsky, 2004</marker>
<rawString>Diab M, Hacioglu K, Jurafsky D (2004): Automatic Tagging of Arabic Text: From Raw Text to Base Phrase Chunks, Proceedings of NAACL-HLT 2004. Boston.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Dichy</author>
</authors>
<title>On lemmatization in Arabic. A formal definition of the Arabic entries of multilingual lexical databases,</title>
<date>2001</date>
<booktitle>ACL 39th Annual Meeting. Workshop on Arabic Language Processing; Status</booktitle>
<pages>23--30</pages>
<contexts>
<context position="5692" citStr="Dichy 2001" startWordPosition="914" endWordPosition="915">any superficial way. A verb can comprise up four sub-tokens (a conjunction, a complementizer, a verb stem and an object pronoun) as illustrated by Figure 1. Figure 1: Possible sub-tokens in Arabic verbs Similarly a noun can comprise up to four subtokens. Although Figure 2 shows five sub-tokens but we must note that the definite article and the genitive pronoun are mutually exclusive. Figure 2: Possible sub-tokens in Arabic nouns Moreover there are various rules that govern the combination of words with affixes and clitics. These rules are called grammar-lexis specifications (Abbès et al 2004; Dichy 2001; Dichy and Fargaly 2003). An example of these specifications is a rule that states that adjectives and proper nouns do not combine with possessive pronouns. 3 Development in Finite State Technology Finite state technology has successfully been used in developing morphologies and text processing tools for many natural languages, including Semitic languages. We will explain briefly how finite state technology works, then we will proceed into showing how different tokenization models are implemented. (1) LEXICON Proclitic al@U.Def.On@ Root; Root; LEXICON Root kitab Enclitic; LEXICON Suffix an En</context>
</contexts>
<marker>Dichy, 2001</marker>
<rawString>Dichy J (2001): On lemmatization in Arabic. A formal definition of the Arabic entries of multilingual lexical databases, ACL 39th Annual Meeting. Workshop on Arabic Language Processing; Status and Prospect. Toulouse, pp 23-30.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Dichy</author>
</authors>
<title>Fargaly A (2003): Roots &amp; Patterns vs. Stems plus Grammar-Lexis Specifications: on what basis should a multilingual lexical database centred on Arabic be built?,</title>
<booktitle>Proceedings of the MT-Summit IX workshop on Machine Translation for Semitic Languages.</booktitle>
<location>New-Orleans.</location>
<marker>Dichy, </marker>
<rawString>Dichy J, Fargaly A (2003): Roots &amp; Patterns vs. Stems plus Grammar-Lexis Specifications: on what basis should a multilingual lexical database centred on Arabic be built?, Proceedings of the MT-Summit IX workshop on Machine Translation for Semitic Languages. New-Orleans.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Habash</author>
<author>O Rambow</author>
</authors>
<title>Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop,</title>
<date>2005</date>
<booktitle>Proceedings of ACL 2005.</booktitle>
<location>Michigan.</location>
<contexts>
<context position="2709" citStr="Habash and Rambow 2005" startWordPosition="417" endWordPosition="420"> other. Without sufficient morphological knowledge, it is impossible to detect and mark clitics. In this paper we will show different levels of implementation of the Arabic tokenizer, according to the levels of linguistic depth involved. Arabic Tokenization has been described in various researches and implemented in many solutions as it is a required preliminary stage for further processing. These solutions include morphological analysis (Beesley 2001; Buckwalter 2002), diacritization (Nelken and Shieber 2005), Information Retrieval (Larkey and Connell 2002), and POS Tagging (Diab et al 2004; Habash and Rambow 2005). None of these projects, however, show how multiword expressions are treated, or how ambiguity is filtered out. In our research, tokenization is handled in a rule-based system as an independent process. We show how the tokenizer interacts with other transducers, and how multiword expressions are identified and delimited. We also show how incorrect tokenizations are filtered out, and how undesired tokenizations are marked. All tools in this research are developed in Finite State Technology (Beesley and Karttunen 2003). These tools have been developed to serve an Arabic Lexical Functional Gramm</context>
<context position="9900" citStr="Habash and Rambow 2005" startWordPosition="1580" endWordPosition="1583">word. 4.1 Model 1: Tokenization Combined with Morphological Analysis In this implementation the tokenizer and the morphological analyzer are one and the same. A single transducer provides both morphological analysis and tokenization. Examples of the tokenizer/analyser output are shown in (3). The ‘+’ sign precedes morphological features, while the ‘@’ sign indicates token boundaries. (3)ﺮﻜﺸﯿﻟو (waliyashkur: and to thank) و+conj@ل+comp@+verb+pres+sgﺮﻜﺷ@ This sort of implementation is the most linguistically motivated. This is also the most common form of implementation for Arabic tokenization (Habash and Rambow 2005). However, it violates the design concept of modularity which requires systems to have separate modules for undertaking separate tasks. For a syntactic parser that requires the existence of a tokenizer besides a morphological analyzer, this implementation is not workable, and either Model 2 or Model 3 is used instead. 4.2 Model 2: Tokenization Guesser In this model tokenization is separated from morphological analysis. The tokenizer only detects and demarcates clitic boundaries. Yet information on what may constitute a clitic is still needed. This is why two additional components are required:</context>
</contexts>
<marker>Habash, Rambow, 2005</marker>
<rawString>Habash N, Rambow O (2005): Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop, Proceedings of ACL 2005. Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Karttunen</author>
<author>J-P Chanod</author>
<author>G Grefenstette</author>
<author>Schiller</author>
</authors>
<title>A</title>
<date>1996</date>
<journal>Natural Language Engineering</journal>
<pages>2--305</pages>
<contexts>
<context position="16931" citStr="Karttunen et al 1996" startWordPosition="2685" endWordPosition="2688">e tokenizer, while syntactically flexible expressions can only be handled by a syntactic parser (Attia 2006a). The tokenizer is responsible for treating MWEs in a special way. They should be marked as single tokens with the inner space(s) preserved. For this purpose, as well as for the purpose of morphological analysis, a specialized transducer is developed for MWEs that lists all variations of MWEs and provides analyses for them (Attia 2006a). One way to allow the tokenizer to handle MWEs is to embed the MWEs in the Tokenizer (Beesley and Karttunen 2003). Yet a better approach, described by (Karttunen et al 1996), is to develop one or several multiword transducers or “staplers” that are composed on the tokenizer. We will explain here how this is implemented in our solution, where the list of MWEs is extracted from the MWE transducer and composed on the tokenizer. Let’s look at the composition regular expression: (10) 1 singleTokens.i 2 .o. ?* 0:&amp;quot;[[[&amp;quot; (MweTokens.l) 0:&amp;quot;]]]&amp;quot; ?* 3 .o. &amp;quot;@&amp;quot; -&gt; &amp;quot; &amp;quot; ||&amp;quot;[[[&amp;quot; [Alphabet* |&amp;quot;@&amp;quot;*] _ 4 .o. &amp;quot;[[[&amp;quot; -&gt; [] .o. &amp;quot;]]]&amp;quot; -&gt; []].i; Single words separated by the ‘@’ sign are defined in the variable singleTokens and the MWE transducer is defined in MweTokens. In the MWE transduc</context>
</contexts>
<marker>Karttunen, Chanod, Grefenstette, Schiller, 1996</marker>
<rawString>Karttunen L, Chanod J-P, Grefenstette G, Schiller A (1996): Regular expressions for language engineering. Natural Language Engineering 2:305-328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larkey LS</author>
<author>Connell ME</author>
</authors>
<title>Arabic Information Retrieval at UMass.</title>
<date>2002</date>
<booktitle>In Voorhees EM, Harman DK (eds), The Tenth Text Retrieval Conference, TREC 2001. Maryland: NIST Special Publication,</booktitle>
<pages>562--570</pages>
<marker>LS, ME, 2002</marker>
<rawString>Larkey LS, Connell ME (2002): Arabic Information Retrieval at UMass. In Voorhees EM, Harman DK (eds), The Tenth Text Retrieval Conference, TREC 2001. Maryland: NIST Special Publication, pp 562-570.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Nelken</author>
<author>Shieber SM</author>
</authors>
<title>Arabic Diacritization Using Weighted Finite-State Transducers,</title>
<date>2005</date>
<booktitle>Proceedings of the 2005 ACL Workshop on Computational Approaches to Semitic Languages.</booktitle>
<location>Michigan.</location>
<marker>Nelken, SM, 2005</marker>
<rawString>Nelken R, Shieber SM (2005): Arabic Diacritization Using Weighted Finite-State Transducers, Proceedings of the 2005 ACL Workshop on Computational Approaches to Semitic Languages. Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Oflazer</author>
<author>Uglu ÖÇ</author>
<author>B Say</author>
</authors>
<date>2004</date>
<booktitle>Integrating Morphology with Multi-word Expression Processing in Turkish, Second ACL Workshop on Multiword Expressions: Integrating Processing.</booktitle>
<pages>64--71</pages>
<publisher>Spain,</publisher>
<contexts>
<context position="14787" citStr="Oflazer et al 2004" startWordPosition="2353" endWordPosition="2356">re two or more words that behave like a single word syntactically and semantically. They are defined, more formally, as “idiosyncratic interpretations that cross word boundaries” (Sag et al 2001). MWEs cover expressions that are traditionally classified as idioms (e.g. down the drain), prepositional verbs (e.g. rely on), verbs with particles (e.g. give up), compound nouns (e.g. traffic lights) and collocations (e.g. do a favour). With regard to syntactic and morphological flexibility, MWEs are classified into three types: fixed, semi-fixed and syntactically flexible expressions (Baldwin 2004; Oflazer et al 2004; Sag et al 2001). a. Fixed Expressions. These expressions are lexically, syntactically and morphologically rigid. An expression of this type is considered as a word with spaces (a single word that happens to contain @و @و @ﻞﺟﺮﻟا@ل @و @ﻞﺟﺮﻠﻟ @ﻞﺟﺮﻠﻟو @و 68 spaces), such as ﻂﺳ وﻷا قﺮﺸﻟا al-sharq al-awsat (the Middle East) and ﻢﺤﻟ ﺖﯿﺑ bait lahem (Bethlehem). b. Semi-Fixed Expressions. These expressions can undergo variations, but still the components of the expression are adjacent. The variations are of two types, morphological variations where lexical items can express person, number, tense, gen</context>
</contexts>
<marker>Oflazer, ÖÇ, Say, 2004</marker>
<rawString>Oflazer K, Uglu ÖÇ, Say B (2004): Integrating Morphology with Multi-word Expression Processing in Turkish, Second ACL Workshop on Multiword Expressions: Integrating Processing. Spain, pp 64-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sag IA</author>
<author>T Baldwin</author>
<author>F Bond</author>
<author>A Copestake</author>
<author>D Flickinger</author>
</authors>
<title>Multi-word Expressions: A Pain in the Neck for NLP, LinGO Working Papers.</title>
<date>2001</date>
<location>Stanford University, CA.</location>
<marker>IA, Baldwin, Bond, Copestake, Flickinger, 2001</marker>
<rawString>Sag IA, Baldwin T, Bond F, Copestake A, Flickinger D (2001): Multi-word Expressions: A Pain in the Neck for NLP, LinGO Working Papers. Stanford University, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>