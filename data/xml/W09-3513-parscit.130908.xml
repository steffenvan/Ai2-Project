<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005307">
<title confidence="0.9929245">
Fast decoding and Easy Implementation:
Transliteration as Sequential Labeling
</title>
<author confidence="0.972554">
Eiji ARAMAKI
</author>
<affiliation confidence="0.985547">
The University of Tokyo
</affiliation>
<email confidence="0.99118">
eiji.aramaki@gmail.com
</email>
<author confidence="0.896428">
Takeshi ABEKAWWA
</author>
<affiliation confidence="0.941232">
National Institute of Informatics
</affiliation>
<email confidence="0.992715">
abekawa@nii.ac.jp
</email>
<sectionHeader confidence="0.997309" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999876384615385">
Although most of previous translitera-
tion methods are based on a generative
model, this paper presents a discrimi-
native transliteration model using condi-
tional random Þelds. We regard charac-
ter(s) as a kind of label, which enables
us to consider a transliteration process as
a sequential labeling process. This ap-
proach has two advantages: (1) fast decod-
ing and (2) easy implementation. Experi-
mental results yielded competitive perfor-
mance, demonstrating the feasibility of the
proposed approach.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.910078083333334">
To date, most transliteration methods have relied
on a generative model which resembles a statisti-
cal machine translation (SMT) model. Although
the generative approach has appealing feasibility,
it usually suffers from parameter settings, length
biases and decoding time.
We assume a transliteration process as a kind
of sequential labeling that is widely employed for
various tasks, such as Named Entity Recognition
(NER), part-of-speech (POS) labeling, and so on.
Figure 1 shows a lattice of both the transliteration
and POS labeling. As shown in that Þgure, both
tasks share a similar work frame: (1) an input se-
quence is decomposed into several segments; then
(2) each segments produces a label. Although the
label represents a POS in POS labeling, it repre-
sents a character (or a character sequence) in the
transliteration task.
The proposed approach entails three risks.
1. Numerous Label Variation: Although POS
requires only 10–20 labels at most, a translit-
eration process requires numerous labels. In
fact, Japanese katakana requires more than
260 labels in the following experiment (we
</bodyText>
<figureCaption confidence="0.962909">
Figure 1: (i) Part-of-Speech Lattice and (ii)
Transliteration Lattice.
</figureCaption>
<bodyText confidence="0.938150333333333">
consider combinations of characters as a la-
bel). Such a huge label set might require ex-
tremely heavy calculation.
</bodyText>
<listItem confidence="0.945348375">
2. No Gold Standard Data: We build the gold
standard label from character alignment us-
ing GIZA++ 1. Of course, such gold standard
data contain alignment errors, which might
decrease labeling performance.
3. No Language Model: The proposed ap-
proach cannot incorporate the target language
model.
</listItem>
<bodyText confidence="0.641005">
In spite of the disadvantages listed above, the
proposed method offers two strong advantages.
</bodyText>
<listItem confidence="0.817246">
1. Fast Decoding: Decoding (more pre-
</listItem>
<bodyText confidence="0.70877825">
cisely labeling) is extremely fast (0.12–0.58
s/input). Such rapid decoding is useful for
various applications, for example, a query ex-
pansion for a search engine and so on 2.
</bodyText>
<footnote confidence="0.999574">
1http://www.fjoch.com/GIZA++.html
2A fast transliteration demonstration is available at the
web site; http://akebia.hcc.h.u-tokyo.ac.jp/NEWS/
</footnote>
<page confidence="0.99158">
65
</page>
<note confidence="0.9940345">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 65–68,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<figureCaption confidence="0.947532">
Figure 2: Conversion from Training set to Gold
Standard Labels
</figureCaption>
<bodyText confidence="0.953756111111111">
2. Easy Implementation: Because sequential
labeling is a traditional research topic, vari-
ous algorithms and tools are available. Using
them, we can easily realize various transliter-
ation systems in any language pairs.
The experimental results empirically demon-
strate that the proposed method is competitive
in several language directions (e.g. English–
Chinese).
</bodyText>
<sectionHeader confidence="0.960697" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.999809">
We developed a two-stage labeling system. First,
an input term is decomposed into several segments
(STEP1). Next, each segmentation produces sym-
bol(s) (STEP2).
</bodyText>
<subsectionHeader confidence="0.951673">
2.1 STEP1: Chunking
</subsectionHeader>
<bodyText confidence="0.999903266666667">
For a given noun phrase, consisting n characters,
the system gave a label (L1...Ln) that represents
segmentations.
The segmentation is expressed as two types of
labels (label B and I), where B signifies a begin-
ning of the segmentation, and I signifies the end
of segmentation. This representation is similar to
the IOB representation, which is used in Named
Entity Recognition (NER) or chunking.
For label prediction, we used Conditional Ran-
dom Fields (CRFs), which is a state-of-the-art la-
beling algorithm. We regard a source character it-
self as a CRF feature. The window size is three
(the current character and previous/next charac-
ter).
</bodyText>
<subsectionHeader confidence="0.992751">
2.2 STEP2: Symbol production
</subsectionHeader>
<bodyText confidence="0.997106">
Next, the system estimates labels (T1...T,,t) for
each segmentation, where m is the number of seg-
</bodyText>
<tableCaption confidence="0.997745">
Table 1: Corpora and Sizes
</tableCaption>
<table confidence="0.998469875">
Notation Language Train Test
EN-CH English–Chinese 31,961 2,896
EN-JA English–Japanese 27,993 1,489
EN-KO English–Korean 4,840 989
EN-HI English–Hindi 10,014 1,000
EN-TA English–Tamil 8,037 1,000
EN-KA English–Kannada 8,065 1,000
EN-RU English–Russian 5,977 1,000
</table>
<bodyText confidence="0.974032272727273">
* EN-CH is provided by (Li et al., 2004); EN-
TA, EN-KA, EN-HI and EN-RU are from (Kumaran
and Kellner, 2007); EN-JA and EN-KO are from
http://www.cjk.org/.
mentations (the number of B labels in STEP1).
The label of this step directly represents a target
language character(s). The method of building a
gold standard label is described in the next sub-
section.
Like STEP1, we use CRFs, and regard source
characters as a feature (window size=3).
</bodyText>
<subsectionHeader confidence="0.996514">
2.3 Conversion from Alignment to Labels
</subsectionHeader>
<bodyText confidence="0.999959125">
First, character alignment is estimated using
GIZA++ as shown at the top of Fig. 2. The align-
ment direction is a target- language-to-English, as-
suming that n English characters correspond to a
target language character.
The STEP1 label is generated for each English
character. If the alignment is 1:1, we give the char-
acter a B label. If the alignment is n : 1, we assign
the first character a B label, and give the others I.
Note that we regard null alignment as a continu-
ance of the last segmentation (I).
The STEP2 label is generated for each English
segmentation (B or BI*). If a segmentation cor-
responds to two or more characters in the target
side, we regard the entire sequence as a label (see
T5 in Fig. 2).
</bodyText>
<sectionHeader confidence="0.999963" genericHeader="background">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999898">
3.1 Corpus, Evaluation, and Setting
</subsectionHeader>
<bodyText confidence="0.994675428571429">
To evaluate the performance of our system,
we used a training-set and test-set provided by
NEWS3(Table 1).
We used the following six metrics (Table 2) us-
ing 10 output candidates. A white paper4 presents
the detailed definitions. For learning, we used
CRF++5 with standard parameters (f=20, c=.5).
</bodyText>
<footnote confidence="0.999974">
3http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/
4https://translit.i2r.a-star.edu.sg/news2009/whitepaper/
5http://crfpp.sourceforge.net/
</footnote>
<page confidence="0.988306">
66
</page>
<tableCaption confidence="0.999832">
Table 3: Results in Test-set
</tableCaption>
<table confidence="0.999970375">
ACC MeanF MRR MAP,,f MAPIO MAPsys
EN–CH 0.580 0.826 0.653 0.580 0.199 0.199
EN–RU 0.531 0.912 0.635 0.531 0.219 0.219
EN–JA 0.457 0.828 0.576 0.445 0.194 0.194
EN–TA 0.365 0.884 0.504 0.360 0.172 0.172
EN–HI 0.363 0.864 0.503 0.360 0.170 0.170
EN–KA 0.324 0.856 0.438 0.315 0.148 0.148
EN–KO 0.170 0.512 0.218 0.170 0.069 0.069
</table>
<tableCaption confidence="0.7602265">
Table 4: Average Test time, Training Time, and
the number of labels (label variation).
</tableCaption>
<table confidence="0.999218125">
Language Test Train # of labels
EN–KO 0.436s 11m09.5s 536
EN–CH 0.201s 6m18.9s 283
EN–JA 0.247s 4m44.3s 269
EN–KA 0.190s 2m26.6s 231
EN–HI 0.302s 1m55.6s 268
EN–TA 0.124s 1m32.9s 207
EN–RU 0.580s 0m26.3s 131
</table>
<bodyText confidence="0.9233672">
* Test time is the average labeling time for an input. Training
time is the average training time for 1000 labels.
both training time and performance. To investi-
gate what gave effects on test time is a subject for
our future work.
</bodyText>
<tableCaption confidence="0.988231">
Table 2: Evaluation Metrics
</tableCaption>
<figureCaption confidence="0.708183428571429">
ACC Word Accuracy in Top 1.
MeanF The meanF measures the fuzzy accu-
racy that is deÞned by the edit dis-
tance and Longest Common Subse-
quence (LCS).
MRR Mean Reciprocal Rank. 1/MRR tells
approximately the average rank of the
correct transliteration.
MAP&amp;quot;f Measures the precision in the n−best
candidates tightly for each reference.
MAP10 Measures the precision in the 10-best
candidates.
MAPsys Measures the precision in the top Ki-
best candidates produced by the system.
</figureCaption>
<subsectionHeader confidence="0.982655">
3.2 Results and Discussion
</subsectionHeader>
<bodyText confidence="0.999948083333333">
Table 3 presents the performance. As shown in the
table, a signiÞcant difference was found between
languages (from low (0.17) to high (0.58)).
The high accuracy results(EN-CH or EN-RU)
are competitive with other systems (the middle
rank among the NEWS participating systems).
However, several language results (such as EN-
KO) were found to have poor performance.
We investigated the difference between high-
performance languages and the others. Table 4
shows the training/test times and the number of
labels. As shown in the table, wide divergence is
apparent in the number of labels. For example,
although EN–KO requires numerous labels (536
labels), EN–RU needs only 131 labels. This diver-
gence roughly corresponds to both training-time
and accuracy as follows: (1) EN–KO requires long
training time (11 minutes) which gave poor per-
formance (0.17 ACC), and (2) EN–RU requires
short training (only 26.3 seconds) which gave high
performance (0.53 ACC). This suggests that if the
number of labels is small, we successfully convert
transliteration into a sequential labeling task.
The test time seemed to have no relation to
</bodyText>
<sectionHeader confidence="0.999727" genericHeader="related work">
4 Related Works
</sectionHeader>
<bodyText confidence="0.998252833333333">
Most previous transliteration studies have re-
lied on a generative model resembling the IBM
model(Brown et al., 1993). This approach is ap-
plicable to various languages: for Japanese (Goto
et al., 2004; Knight and Graehl,1998), Korean(Oh
and Choi, 2002; Oh and Choi, 2005; Oh and
Isahara, 2007), Arabic(Stalls and Knight, 1998;
Sherif and Kondrak, 2007), Chinese(Li et al.,
2007), and Persian(Karimi et al., 2007). As de-
scribed previously, the proposed discriminative
approach differs from them.
Another perspective is that of how to repre-
sent transliteration phenomena. Methods can be
classiÞed into three main types: (1) grapheme-
based (Li et al., 2004), (2) phoneme-based (Knight
and Graehl, 1998), and (3) combinations of these
methods (hybrid-model(Bilac and Tanaka, 2004),
and a correspondence-based model(Oh and Choi,
2002; Oh and Choi, 2005) re-ranking model (Oh
and Isahara, 2007)). Our proposed method em-
ploys a grapheme-based approach. Employing
phonemes is a challenge reserved for future stud-
ies.
Aramaki et al. (2008) proposed a discrimina-
</bodyText>
<page confidence="0.998796">
67
</page>
<bodyText confidence="0.999936">
tive transliteration approach using Support Vector
Machines (SVMs). However, their goal, which is
to judge whether two terms come from the same
English words or not, differs from this paper goal.
</bodyText>
<sectionHeader confidence="0.996033" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.99995">
This paper presents a discriminative translitera-
tion model using a sequential labeling technique.
Experimental results yielded competitive perfor-
mance, demonstrating the feasibility of the pro-
posed approach. In the future, how to incorporate
more rich information, such as language model
and phoneme, is remaining problem. We believe
this task conversion, from generation to sequential
labeling, can be useful for several practical appli-
cations.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="acknowledgments">
ACKNOWLEDGMENT
</sectionHeader>
<bodyText confidence="0.973584666666667">
Part of this research is supported by Japanese
Grant-in-Aid for ScientiÞc Research (A) Num-
ber:20680006.
</bodyText>
<sectionHeader confidence="0.9984" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999282442857143">
Eiji Aramaki, Takeshi Imai, Kengo Miyo, and
Kazuhiko Ohe. 2008. Orthographic disambiguation
incorporating transliterated probability. In Proceed-
ings of International Joint Conference on Natural
Language Processing (IJCNLP2008), pages 48–55.
Slaven Bilac and Hozumi Tanaka. 2004. A hybrid
back-transliteration system for Japanese. In Pro-
ceedings of The 20th International Conference on
Computational Linguistics (COLING2004), pages
597–603.
Peter F. Brown, Stephen A. Della Pietra, Vi cent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics,
19(2).
Isao Goto, Naoto Kato, Terumasa Ehara, and Hideki
Tanaka. 2004. Back transliteration from Japanese
to English using target English context. In Proceed-
ings of The 20th International Conference on Com-
putational Linguistics (COLING2004), pages 827–
833.
Sarvnaz Karimi, Falk Scholer, and Andrew Turpin.
2007. Collapsed consonant and vowel models: New
approaches for English-Persian transliteration and
back-transliteration. In Proceedings of the Annual
Meeting of the Association of Computational Lin-
guistics (ACL2007), pages 648–655.
Kevin Knight and Jonathan Graehl. 1998. Ma-
chine transliteration. Computational Linguistics,
24(4):599–612.
A. Kumaran and Tobias Kellner. 2007. A generic
framework for machine transliteration. In SIGIR
’07: Proceedings of the 30th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 721–722.
Haizhou Li, Min Zhang, and Jian Su. 2004. A joint
source-channel model for machine transliteration.
In Proceedings of the Meeting of the Association for
Computational Linguistics (ACL2004), pages 159–
166.
Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui
Dong. 2007. Semantic transliteration of per-
sonal names. In Proceedings of the Annual Meet-
ing of the Association of Computational Linguistics
(ACL2007), pages 120–127.
Jong-Hoon Oh and Key-Sun Choi. 2002. An English-
Korean transliteration model using pronunciation
and contextual rules. In Proceedings of The 19th In-
ternational Conference on Computational Linguis-
tics (COLING2002), pages 758–764.
Jong-Hoon Oh and Key-Sun Choi. 2005. An ensemble
of grapheme and phoneme for machine translitera-
tion. In Proceedings of Second International Joint
Conference on Natural Language Processing (IJC-
NLP2005), pages 450–461.
Jong-Hoon Oh and Hitoshi Isahara. 2007. Machine
transliteration using multiple transliteration engines
and hypothesis re-ranking. In Proceedings of MT
Summit XI, pages 353–360.
Tarek Sherif and Grzegorz Kondrak. 2007. Substring-
based transliteration. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics (ACL2007), pages 944–951.
Bonnie Glover Stalls and Kevin Knight. 1998. Trans-
lating names and technical terms in arabic text.
In Proceedings of The International Conference
on Computational Linguistics and the 36th Annual
Meeting of the Association of Computational Lin-
guistics (COLING-ACL1998) Workshop on Compu-
tational Approaches to Semitic Languages.
</reference>
<page confidence="0.99944">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.526096">
<title confidence="0.999508">Fast decoding and Easy Transliteration as Sequential Labeling</title>
<author confidence="0.940158">Eiji</author>
<affiliation confidence="0.993846">The University of</affiliation>
<email confidence="0.997743">eiji.aramaki@gmail.com</email>
<author confidence="0.848377">Takeshi</author>
<affiliation confidence="0.976851">National Institute of</affiliation>
<email confidence="0.720783">abekawa@nii.ac.jp</email>
<abstract confidence="0.993563357142857">Although most of previous transliteration methods are based on a generative model, this paper presents a discriminative transliteration model using condirandom We regard character(s) as a kind of label, which enables us to consider a transliteration process as a sequential labeling process. This approach has two advantages: (1) fast decoding and (2) easy implementation. Experimental results yielded competitive performance, demonstrating the feasibility of the proposed approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eiji Aramaki</author>
<author>Takeshi Imai</author>
<author>Kengo Miyo</author>
<author>Kazuhiko Ohe</author>
</authors>
<title>Orthographic disambiguation incorporating transliterated probability.</title>
<date>2008</date>
<booktitle>In Proceedings of International Joint Conference on Natural Language Processing (IJCNLP2008),</booktitle>
<pages>48--55</pages>
<contexts>
<context position="9819" citStr="Aramaki et al. (2008)" startWordPosition="1513" endWordPosition="1516">007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classiÞed into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our proposed method employs a grapheme-based approach. Employing phonemes is a challenge reserved for future studies. Aramaki et al. (2008) proposed a discrimina67 tive transliteration approach using Support Vector Machines (SVMs). However, their goal, which is to judge whether two terms come from the same English words or not, differs from this paper goal. 5 Conclusions This paper presents a discriminative transliteration model using a sequential labeling technique. Experimental results yielded competitive performance, demonstrating the feasibility of the proposed approach. In the future, how to incorporate more rich information, such as language model and phoneme, is remaining problem. We believe this task conversion, from gene</context>
</contexts>
<marker>Aramaki, Imai, Miyo, Ohe, 2008</marker>
<rawString>Eiji Aramaki, Takeshi Imai, Kengo Miyo, and Kazuhiko Ohe. 2008. Orthographic disambiguation incorporating transliterated probability. In Proceedings of International Joint Conference on Natural Language Processing (IJCNLP2008), pages 48–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slaven Bilac</author>
<author>Hozumi Tanaka</author>
</authors>
<title>A hybrid back-transliteration system for Japanese.</title>
<date>2004</date>
<booktitle>In Proceedings of The 20th International Conference on Computational Linguistics (COLING2004),</booktitle>
<pages>597--603</pages>
<contexts>
<context position="9565" citStr="Bilac and Tanaka, 2004" startWordPosition="1474" endWordPosition="1477">rious languages: for Japanese (Goto et al., 2004; Knight and Graehl,1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classiÞed into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our proposed method employs a grapheme-based approach. Employing phonemes is a challenge reserved for future studies. Aramaki et al. (2008) proposed a discrimina67 tive transliteration approach using Support Vector Machines (SVMs). However, their goal, which is to judge whether two terms come from the same English words or not, differs from this paper goal. 5 Conclusions This paper presents a discriminative transliteration model using a sequential labeling technique. Experimental </context>
</contexts>
<marker>Bilac, Tanaka, 2004</marker>
<rawString>Slaven Bilac and Hozumi Tanaka. 2004. A hybrid back-transliteration system for Japanese. In Proceedings of The 20th International Conference on Computational Linguistics (COLING2004), pages 597–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vi cent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="8907" citStr="Brown et al., 1993" startWordPosition="1375" endWordPosition="1378">s (536 labels), EN–RU needs only 131 labels. This divergence roughly corresponds to both training-time and accuracy as follows: (1) EN–KO requires long training time (11 minutes) which gave poor performance (0.17 ACC), and (2) EN–RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to 4 Related Works Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl,1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classiÞed into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinati</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vi cent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Naoto Kato</author>
<author>Terumasa Ehara</author>
<author>Hideki Tanaka</author>
</authors>
<title>Back transliteration from Japanese to English using target English context.</title>
<date>2004</date>
<booktitle>In Proceedings of The 20th International Conference on Computational Linguistics (COLING2004),</booktitle>
<pages>827--833</pages>
<contexts>
<context position="8990" citStr="Goto et al., 2004" startWordPosition="1389" endWordPosition="1392">oth training-time and accuracy as follows: (1) EN–KO requires long training time (11 minutes) which gave poor performance (0.17 ACC), and (2) EN–RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to 4 Related Works Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl,1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classiÞed into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-ba</context>
</contexts>
<marker>Goto, Kato, Ehara, Tanaka, 2004</marker>
<rawString>Isao Goto, Naoto Kato, Terumasa Ehara, and Hideki Tanaka. 2004. Back transliteration from Japanese to English using target English context. In Proceedings of The 20th International Conference on Computational Linguistics (COLING2004), pages 827– 833.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarvnaz Karimi</author>
<author>Falk Scholer</author>
<author>Andrew Turpin</author>
</authors>
<title>Collapsed consonant and vowel models: New approaches for English-Persian transliteration and back-transliteration.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL2007),</booktitle>
<pages>648--655</pages>
<contexts>
<context position="9202" citStr="Karimi et al., 2007" startWordPosition="1421" endWordPosition="1424">erformance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to 4 Related Works Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl,1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classiÞed into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our proposed method employs a grapheme-based approach. Employing phonemes is a challenge reserved for future studies. Aram</context>
</contexts>
<marker>Karimi, Scholer, Turpin, 2007</marker>
<rawString>Sarvnaz Karimi, Falk Scholer, and Andrew Turpin. 2007. Collapsed consonant and vowel models: New approaches for English-Persian transliteration and back-transliteration. In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL2007), pages 648–655.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Knight</author>
<author>Jonathan Graehl</author>
</authors>
<date>1998</date>
<booktitle>Machine transliteration. Computational Linguistics,</booktitle>
<pages>24--4</pages>
<contexts>
<context position="9488" citStr="Knight and Graehl, 1998" startWordPosition="1464" endWordPosition="1467">esembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl,1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classiÞed into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our proposed method employs a grapheme-based approach. Employing phonemes is a challenge reserved for future studies. Aramaki et al. (2008) proposed a discrimina67 tive transliteration approach using Support Vector Machines (SVMs). However, their goal, which is to judge whether two terms come from the same English words or not, differs from this paper goal. 5 Conclusions This paper presents a discriminati</context>
</contexts>
<marker>Knight, Graehl, 1998</marker>
<rawString>Kevin Knight and Jonathan Graehl. 1998. Machine transliteration. Computational Linguistics, 24(4):599–612.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kumaran</author>
<author>Tobias Kellner</author>
</authors>
<title>A generic framework for machine transliteration.</title>
<date>2007</date>
<booktitle>In SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>721--722</pages>
<contexts>
<context position="4667" citStr="Kumaran and Kellner, 2007" startWordPosition="695" endWordPosition="698">tself as a CRF feature. The window size is three (the current character and previous/next character). 2.2 STEP2: Symbol production Next, the system estimates labels (T1...T,,t) for each segmentation, where m is the number of segTable 1: Corpora and Sizes Notation Language Train Test EN-CH English–Chinese 31,961 2,896 EN-JA English–Japanese 27,993 1,489 EN-KO English–Korean 4,840 989 EN-HI English–Hindi 10,014 1,000 EN-TA English–Tamil 8,037 1,000 EN-KA English–Kannada 8,065 1,000 EN-RU English–Russian 5,977 1,000 * EN-CH is provided by (Li et al., 2004); ENTA, EN-KA, EN-HI and EN-RU are from (Kumaran and Kellner, 2007); EN-JA and EN-KO are from http://www.cjk.org/. mentations (the number of B labels in STEP1). The label of this step directly represents a target language character(s). The method of building a gold standard label is described in the next subsection. Like STEP1, we use CRFs, and regard source characters as a feature (window size=3). 2.3 Conversion from Alignment to Labels First, character alignment is estimated using GIZA++ as shown at the top of Fig. 2. The alignment direction is a target- language-to-English, assuming that n English characters correspond to a target language character. The S</context>
</contexts>
<marker>Kumaran, Kellner, 2007</marker>
<rawString>A. Kumaran and Tobias Kellner. 2007. A generic framework for machine transliteration. In SIGIR ’07: Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval, pages 721–722.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Min Zhang</author>
<author>Jian Su</author>
</authors>
<title>A joint source-channel model for machine transliteration.</title>
<date>2004</date>
<booktitle>In Proceedings of the Meeting of the Association for Computational Linguistics (ACL2004),</booktitle>
<pages>159--166</pages>
<contexts>
<context position="4600" citStr="Li et al., 2004" startWordPosition="683" endWordPosition="686">he-art labeling algorithm. We regard a source character itself as a CRF feature. The window size is three (the current character and previous/next character). 2.2 STEP2: Symbol production Next, the system estimates labels (T1...T,,t) for each segmentation, where m is the number of segTable 1: Corpora and Sizes Notation Language Train Test EN-CH English–Chinese 31,961 2,896 EN-JA English–Japanese 27,993 1,489 EN-KO English–Korean 4,840 989 EN-HI English–Hindi 10,014 1,000 EN-TA English–Tamil 8,037 1,000 EN-KA English–Kannada 8,065 1,000 EN-RU English–Russian 5,977 1,000 * EN-CH is provided by (Li et al., 2004); ENTA, EN-KA, EN-HI and EN-RU are from (Kumaran and Kellner, 2007); EN-JA and EN-KO are from http://www.cjk.org/. mentations (the number of B labels in STEP1). The label of this step directly represents a target language character(s). The method of building a gold standard label is described in the next subsection. Like STEP1, we use CRFs, and regard source characters as a feature (window size=3). 2.3 Conversion from Alignment to Labels First, character alignment is estimated using GIZA++ as shown at the top of Fig. 2. The alignment direction is a target- language-to-English, assuming that n </context>
<context position="9443" citStr="Li et al., 2004" startWordPosition="1458" endWordPosition="1461">s have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl,1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classiÞed into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our proposed method employs a grapheme-based approach. Employing phonemes is a challenge reserved for future studies. Aramaki et al. (2008) proposed a discrimina67 tive transliteration approach using Support Vector Machines (SVMs). However, their goal, which is to judge whether two terms come from the same English words or not, differs from this paper goal. 5 C</context>
</contexts>
<marker>Li, Zhang, Su, 2004</marker>
<rawString>Haizhou Li, Min Zhang, and Jian Su. 2004. A joint source-channel model for machine transliteration. In Proceedings of the Meeting of the Association for Computational Linguistics (ACL2004), pages 159– 166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Haizhou Li</author>
<author>Khe Chai Sim</author>
<author>Jin-Shea Kuo</author>
<author>Minghui Dong</author>
</authors>
<title>Semantic transliteration of personal names.</title>
<date>2007</date>
<booktitle>In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL2007),</booktitle>
<pages>120--127</pages>
<contexts>
<context position="9168" citStr="Li et al., 2007" startWordPosition="1416" endWordPosition="1419">6.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to 4 Related Works Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl,1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classiÞed into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our proposed method employs a grapheme-based approach. Employing phonemes is a challenge</context>
</contexts>
<marker>Li, Sim, Kuo, Dong, 2007</marker>
<rawString>Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and Minghui Dong. 2007. Semantic transliteration of personal names. In Proceedings of the Annual Meeting of the Association of Computational Linguistics (ACL2007), pages 120–127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Key-Sun Choi</author>
</authors>
<title>An EnglishKorean transliteration model using pronunciation and contextual rules.</title>
<date>2002</date>
<booktitle>In Proceedings of The 19th International Conference on Computational Linguistics (COLING2002),</booktitle>
<pages>758--764</pages>
<contexts>
<context position="9041" citStr="Oh and Choi, 2002" startWordPosition="1396" endWordPosition="1399">O requires long training time (11 minutes) which gave poor performance (0.17 ACC), and (2) EN–RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to 4 Related Works Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl,1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classiÞed into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-</context>
</contexts>
<marker>Oh, Choi, 2002</marker>
<rawString>Jong-Hoon Oh and Key-Sun Choi. 2002. An EnglishKorean transliteration model using pronunciation and contextual rules. In Proceedings of The 19th International Conference on Computational Linguistics (COLING2002), pages 758–764.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Key-Sun Choi</author>
</authors>
<title>An ensemble of grapheme and phoneme for machine transliteration.</title>
<date>2005</date>
<booktitle>In Proceedings of Second International Joint Conference on Natural Language Processing (IJCNLP2005),</booktitle>
<pages>450--461</pages>
<contexts>
<context position="9060" citStr="Oh and Choi, 2005" startWordPosition="1400" endWordPosition="1403">ining time (11 minutes) which gave poor performance (0.17 ACC), and (2) EN–RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to 4 Related Works Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl,1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classiÞed into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh a</context>
</contexts>
<marker>Oh, Choi, 2005</marker>
<rawString>Jong-Hoon Oh and Key-Sun Choi. 2005. An ensemble of grapheme and phoneme for machine transliteration. In Proceedings of Second International Joint Conference on Natural Language Processing (IJCNLP2005), pages 450–461.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Hitoshi Isahara</author>
</authors>
<title>Machine transliteration using multiple transliteration engines and hypothesis re-ranking.</title>
<date>2007</date>
<booktitle>In Proceedings of MT Summit XI,</booktitle>
<pages>353--360</pages>
<contexts>
<context position="9083" citStr="Oh and Isahara, 2007" startWordPosition="1404" endWordPosition="1407">tes) which gave poor performance (0.17 ACC), and (2) EN–RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to 4 Related Works Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl,1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classiÞed into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our</context>
</contexts>
<marker>Oh, Isahara, 2007</marker>
<rawString>Jong-Hoon Oh and Hitoshi Isahara. 2007. Machine transliteration using multiple transliteration engines and hypothesis re-ranking. In Proceedings of MT Summit XI, pages 353–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tarek Sherif</author>
<author>Grzegorz Kondrak</author>
</authors>
<title>Substringbased transliteration.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL2007),</booktitle>
<pages>944--951</pages>
<contexts>
<context position="9142" citStr="Sherif and Kondrak, 2007" startWordPosition="1412" endWordPosition="1415">–RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to 4 Related Works Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl,1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classiÞed into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our proposed method employs a grapheme-based approach. Employi</context>
</contexts>
<marker>Sherif, Kondrak, 2007</marker>
<rawString>Tarek Sherif and Grzegorz Kondrak. 2007. Substringbased transliteration. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL2007), pages 944–951.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Glover Stalls</author>
<author>Kevin Knight</author>
</authors>
<title>Translating names and technical terms in arabic text.</title>
<date>1998</date>
<booktitle>In Proceedings of The International Conference on Computational Linguistics and the 36th Annual Meeting of the Association of Computational Linguistics (COLING-ACL1998) Workshop on Computational Approaches to Semitic Languages.</booktitle>
<contexts>
<context position="9115" citStr="Stalls and Knight, 1998" startWordPosition="1408" endWordPosition="1411">ce (0.17 ACC), and (2) EN–RU requires short training (only 26.3 seconds) which gave high performance (0.53 ACC). This suggests that if the number of labels is small, we successfully convert transliteration into a sequential labeling task. The test time seemed to have no relation to 4 Related Works Most previous transliteration studies have relied on a generative model resembling the IBM model(Brown et al., 1993). This approach is applicable to various languages: for Japanese (Goto et al., 2004; Knight and Graehl,1998), Korean(Oh and Choi, 2002; Oh and Choi, 2005; Oh and Isahara, 2007), Arabic(Stalls and Knight, 1998; Sherif and Kondrak, 2007), Chinese(Li et al., 2007), and Persian(Karimi et al., 2007). As described previously, the proposed discriminative approach differs from them. Another perspective is that of how to represent transliteration phenomena. Methods can be classiÞed into three main types: (1) graphemebased (Li et al., 2004), (2) phoneme-based (Knight and Graehl, 1998), and (3) combinations of these methods (hybrid-model(Bilac and Tanaka, 2004), and a correspondence-based model(Oh and Choi, 2002; Oh and Choi, 2005) re-ranking model (Oh and Isahara, 2007)). Our proposed method employs a graph</context>
</contexts>
<marker>Stalls, Knight, 1998</marker>
<rawString>Bonnie Glover Stalls and Kevin Knight. 1998. Translating names and technical terms in arabic text. In Proceedings of The International Conference on Computational Linguistics and the 36th Annual Meeting of the Association of Computational Linguistics (COLING-ACL1998) Workshop on Computational Approaches to Semitic Languages.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>