<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000050">
<title confidence="0.983135">
Bilingual Word Spectral Clustering for Statistical Machine Translation
</title>
<author confidence="0.997286">
Bing Zhao† Eric P. Xing† ‡ Alex Waibel††Language Technologies Institute
</author>
<affiliation confidence="0.817841666666667">
‡Center for Automated Learning and Discovery
Carnegie Mellon University
Pittsburgh, Pennsylvania 15213
</affiliation>
<email confidence="0.996378">
{bzhao,epxing,ahw}@cs.cmu.edu
</email>
<sectionHeader confidence="0.982864" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999455">
In this paper, a variant of a spectral clus-
tering algorithm is proposed for bilingual
word clustering. The proposed algorithm
generates the two sets of clusters for both
languages efficiently with high seman-
tic correlation within monolingual clus-
ters, and high translation quality across
the clusters between two languages. Each
cluster level translation is considered as
a bilingual concept, which generalizes
words in bilingual clusters. This scheme
improves the robustness for statistical ma-
chine translation models. Two HMM-
based translation models are tested to use
these bilingual clusters. Improved per-
plexity, word alignment accuracy, and
translation quality are observed in our ex-
periments.
</bodyText>
<sectionHeader confidence="0.995174" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991460396226415">
Statistical natural language processing usually suf-
fers from the sparse data problem. Comparing to
the available monolingual data, we have much less
training data especially for statistical machine trans-
lation (SMT). For example, in language modelling,
there are more than 1.7 billion words corpora avail-
able: English Gigaword by (Graff, 2003). However,
for machine translation tasks, there are typically less
than 10 million words of training data.
Bilingual word clustering is a process of form-
ing corresponding word clusters suitable for ma-
chine translation. Previous work from (Wang et al.,
1996) showed improvements in perplexity-oriented
measures using mixture-based translation lexicon
(Brown et al., 1993). A later study by (Och,
25
1999) showed improvements on perplexity of bilin-
gual corpus, and word translation accuracy using a
template-based translation model. Both approaches
are optimizing the maximum likelihood of parallel
corpus, in which a data point is a sentence pair: an
English sentence and its translation in another lan-
guage such as French. These algorithms are es-
sentially the same as monolingual word clusterings
(Kneser and Ney, 1993)—an iterative local search.
In each iteration, a two-level loop over every possi-
ble word-cluster assignment is tested for better like-
lihood change. This kind of approach has two draw-
backs: first it is easily to get stuck in local op-
tima; second, the clustering of English and the other
language are basically two separated optimization
processes, and cluster-level translation is modelled
loosely. These drawbacks make their approaches
generally not very effective in improving translation
models.
In this paper, we propose a variant of the spec-
tral clustering algorithm (Ng et al., 2001) for bilin-
gual word clustering. Given parallel corpus, first, the
word’s bilingual context is used directly as features
- for instance, each English word is represented by
its bilingual word translation candidates. Second,
latent eigenstructure analysis is carried out in this
bilingual feature space, which leads to clusters of
words with similar translations. Essentially an affin-
ity matrix is computed using these cross-lingual fea-
tures. It is then decomposed into two sub-spaces,
which are meaningful for translation tasks: the left
subspace corresponds to the representation of words
in English vocabulary, and the right sub-space cor-
responds to words in French. Each eigenvector is
considered as one bilingual concept, and the bilin-
gual clusters are considered to be its realizations in
two languages. Finally, a general K-means cluster-
</bodyText>
<note confidence="0.9983025">
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 25–32,
Ann Arbor, June 2005. c�Association for Computational Linguistics, 2005
</note>
<bodyText confidence="0.9941666">
ing algorithm is used to find out word clusters in the
two sub-spaces.
The remainder of the paper is structured as fol-
lows: in section 2, concepts of translation models
are introduced together with two extended HMMs;
in section 3, our proposed bilingual word cluster-
ing algorithm is explained in detail, and the related
works are analyzed; in section 4, evaluation metrics
are defined and the experimental results are given;
in section 5, the discussions and conclusions.
</bodyText>
<sectionHeader confidence="0.943675" genericHeader="method">
2 Statistical Machine Translation
</sectionHeader>
<bodyText confidence="0.9998422">
The task of translation is to translate one sentence
in some source language F into a target language E.
For example, given a French sentence with J words
denoted as fJ1 = f1f2...fJ, an SMT system auto-
matically translates it into an English sentence with
I words denoted by eI1 = e1e2...eI. The SMT sys-
tem first proposes multiple English hypotheses in its
model space. Among all the hypotheses, the system
selects the one with the highest conditional proba-
bility according to Bayes’s decision rule:
</bodyText>
<equation confidence="0.998961">
eI1 = arg max P(eI1|fJ1 ) = arg max P(fJ1 |eI1)P(eI1),
{eI1} {eI1}
(1)
</equation>
<bodyText confidence="0.9934235">
where P(fJ1 |eI1) is called translation model, and
P(eI1) is called language model. The translation
model is the key component, which is the focus in
this paper.
</bodyText>
<subsectionHeader confidence="0.984374">
2.1 HMM-based Translation Model
</subsectionHeader>
<bodyText confidence="0.99994175">
HMM is one of the effective translation models (Vo-
gel et al., 1996), which is easily scalable to very
large training corpus.
To model word-to-word translation, we introduce
the mapping j —* aj, which assigns a French word
fj in position j to a English word ei in position
i = aj denoted as eaj. Each French word fj is
an observation, and it is generated by a HMM state
defined as [eaj, aj], where the alignment aj for po-
sition j is considered to have a dependency on the
previous alignment aj−1. Thus the first-order HMM
is defined as follows:
</bodyText>
<equation confidence="0.999328333333333">
�P(fJ 1 |eI 1) = J P(fj|eaj)P(aj|aj−1), (2)
aJ H
1 j=1
</equation>
<bodyText confidence="0.998769071428571">
where P(aj|aj−1) is the transition probability. This
model captures the assumption that words close in
the source sentence are aligned to words close in
the target sentence. An additional pseudo word of
“NULL” is used as the beginning of English sen-
tence for HMM to start with. The (Och and Ney,
2003) model includes other refinements such as spe-
cial treatment of a jump to a Null word, and a uni-
form smoothing prior. The HMM with these refine-
ments is used as our baseline. Motivated by the work
in both (Och and Ney, 2000) and (Toutanova et al.,
2002), we propose the two following simplest ver-
sions of extended HMMs to utilize bilingual word
clusters.
</bodyText>
<subsectionHeader confidence="0.998679">
2.2 Extensions to HMM with word clusters
</subsectionHeader>
<bodyText confidence="0.998135857142857">
Let F denote the cluster mapping fj —* F(fj), which
assigns French word fj to its cluster ID Fj = F(fj).
Similarly E maps English word ei to its cluster ID
of Ei = E(ei). In this paper, we assume each word
belongs to one cluster only.
With bilingual word clusters, we can extend the
HMM model in Eqn. 1 in the following two ways:
</bodyText>
<equation confidence="0.9927855">
P(fJ1 |eI1) = Eai H 1 P(fj |eaj)&apos;
P(aj|aj−1, E(eaj−1), F(fj−1)),
</equation>
<bodyText confidence="0.9997095">
where E(eaj−1) and F(fj−1) are non overlapping
word clusters (Eaj−1, Fj−1)for English and French
respectively.
Another explicit way of utilizing bilingual word
clusters can be considered as a two-stream HMM as
follows:
</bodyText>
<equation confidence="0.988305666666667">
P(fJ1 , F1J |eI1, EI1) =
� �J j=1 P(fj|eaj)P(Fj|Eaj)P(aj|aj−1).
aJ 1
</equation>
<bodyText confidence="0.999561272727273">
This model introduces the translation of bilingual
word clusters directly as an extra factor to Eqn. 2.
Intuitively, the role of this factor is to boost the trans-
lation probabilities for words sharing the same con-
cept. This is a more expressive model because it
models both word and the cluster level translation
equivalence. Also, compared with the model in Eqn.
3, this model is easier to train, as it uses a two-
dimension table instead of a four-dimension table.
However, we do not want this P(Fj|Eaj) to dom-
inate the HMM transition structure, and the obser-
</bodyText>
<page confidence="0.678809">
26
</page>
<bodyText confidence="0.990817333333333">
vation probability of P(fj|eaj) during the EM itera-
tions. Thus a uniform prior P(Fj) = 1/|F |is intro-
duced as a smoothing factor for P(Fj|Eaj):
</bodyText>
<equation confidence="0.999928">
P(Fj|Eaj) = AP(Fj|Eaj) + (1 − A)P(Fj), (5)
</equation>
<bodyText confidence="0.999462571428572">
where |F |is the total number of word clusters in
French (we use the same number of clusters for both
languages). A can be chosen to get optimal perfor-
mance on a development set. In our case, we fix it to
be 0.5 in all our experiments.
is a class of its own. There exists efficient leave-one-
out style algorithm (Kneser and Ney, 1993), which
can automatically determine the number of clusters.
For the bilingual part P(fJ1 |eI1, F, E), we can
slightly modify the same algorithm as in (Kneser
and Ney, 1993). Given the word alignment {�J1}
between fJ1 and eI1 collected from the Viterbi path
in HMM-based translation model, we can infer F as
follows:
</bodyText>
<equation confidence="0.9110455">
F = arg max P(fJ1 |eI1, F, E)
{F}
</equation>
<sectionHeader confidence="0.967012" genericHeader="method">
3 Bilingual Word Clustering
</sectionHeader>
<bodyText confidence="0.9999762">
In bilingual word clustering, the task is to build word
clusters F and E to form partitions of the vocabular-
ies of the two languages respectively. The two par-
titions for the vocabularies of F and E are aimed to
be suitable for machine translation in the sense that
the cluster/partition level translation equivalence is
reliable and focused to handle data sparseness; the
translation model using these clusters explains the
parallel corpus {(fJ1 , eI1)} better in terms of perplex-
ity or joint likelihood.
</bodyText>
<subsectionHeader confidence="0.996249">
3.1 From Monolingual to Bilingual
</subsectionHeader>
<bodyText confidence="0.989941666666667">
To infer bilingual word clusters of (F, E), one can
optimize the joint probability of the parallel corpus
{(fJ1 , eI1)} using the clusters as follows:
</bodyText>
<equation confidence="0.985549">
( F, E) = arg max P(fJ1 , eI1|F, E)
(F,E)
= arg max P(eI1|E)P(fJ1 |eI1, F, E).(6)
(F,E)
</equation>
<bodyText confidence="0.997880333333333">
Eqn. 6 separates the optimization process into two
parts: the monolingual part for E, and the bilingual
part for F given fixed E. The monolingual part is
considered as a prior probability:P(eI1|E), and E can
be inferred using corpus bigram statistics in the fol-
lowing equation:
</bodyText>
<equation confidence="0.999523">
E = arg max P(eI1|E)
{E}
I
= arg max P(Ei|Ei−1)P(ei|Ei). (7)
{E} i=1
</equation>
<bodyText confidence="0.998602">
We need to fix the number of clusters beforehand,
otherwise the optimum is reached when each word
</bodyText>
<equation confidence="0.976698">
P(Fj|Eaj)P(fj|Fj). (8)
</equation>
<bodyText confidence="0.9999325">
Overall, this bilingual word clustering algorithm is
essentially a two-step approach. In the first step, E
is inferred by optimizing the monolingual likelihood
of English data, and secondly F is inferred by op-
timizing the bilingual part without changing E. In
this way, the algorithm is easy to implement without
much change from the monolingual correspondent.
This approach was shown to give the best results
in (Och, 1999). We use it as our baseline to compare
with.
</bodyText>
<subsectionHeader confidence="0.999606">
3.2 Bilingual Word Spectral Clustering
</subsectionHeader>
<bodyText confidence="0.999975111111111">
Instead of using word alignment to bridge the par-
allel sentence pair, and optimize the likelihood in
two separate steps, we develop an alignment-free al-
gorithm using a variant of spectral clustering algo-
rithm. The goal is to build high cluster-level trans-
lation quality suitable for translation modelling, and
at the same time maintain high intra-cluster similar-
ity , and low inter-cluster similarity for monolingual
clusters.
</bodyText>
<subsectionHeader confidence="0.723979">
3.2.1 Notations
</subsectionHeader>
<bodyText confidence="0.9998802">
We define the vocabulary VF as the French vo-
cabulary with a size of |VF |; VE as the English vo-
cabulary with size of |VE|. A co-occurrence matrix
C{F,E} is built with |VF  |rows and |VE |columns;
each element represents the co-occurrence counts of
the corresponding French word fj and English word
ei. In this way, each French word forms a row vec-
tor with a dimension of |VE|, and each dimensional-
ity is a co-occurring English word. The elements in
the vector are the co-occurrence counts. We can also
</bodyText>
<figure confidence="0.770495833333333">
= arg max
{F}
J
H
j=1
27
</figure>
<bodyText confidence="0.9822325">
view each column as a vector for English word, and
we’ll have similar interpretations as above.
</bodyText>
<subsectionHeader confidence="0.866412">
3.2.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.998457">
With C{F,E}, we can infer two affinity matrixes
as follows:
</bodyText>
<equation confidence="0.997209666666667">
AE = CT{F,E}C{F,E}
T
AF = C{F,E}C{F,E},
</equation>
<bodyText confidence="0.999963">
where AE is an |VE |x |VE |affinity matrix for En-
glish words, with rows and columns representing
English words and each element the inner product
between two English words column vectors. Corre-
spondingly, AF is an affinity matrix of size |VF |x
|VF |for French words with similar definitions. Both
AE and AF are symmetric and non-negative. Now
we can compute the eigenstructure for both AE and
AF. In fact, the eigen vectors of the two are corre-
spondingly the right and left sub-spaces of the orig-
inal co-occurrence matrix of C{F,E} respectively.
This can be computed using singular value decom-
position (SVD): C{F,E} = USVT, AE = VS2VT,
and AF = US2UT, where U is the left sub-space,
and V the right sub-space of the co-occurrence ma-
trix C{F,E}. S is a diagonal matrix, with the singular
values ranked from large to small along the diagonal.
Obviously, the left sub-space U is the eigenstructure
for AF; the right sub-space V is the eigenstructure
for AE.
By choosing the top K singular values (the square
root of the eigen values for both AE and AF), the
sub-spaces will be reduced to: U|VF |×K and V|VE|×K
respectively. Based on these subspaces, we can carry
out K-means or other clustering algorithms to in-
fer word clusters for both languages. Our algorithm
goes as follows:
</bodyText>
<listItem confidence="0.999356285714286">
• Initialize bilingual co-occurrence matrix
C{F,E} with rows representing French words,
and columns English words. Cji is the co-
occurrence raw counts of French word fj and
English word ei;
• Form the affinity matrix AE = CT{F,E}C{F,E}
and AF = CT{F,E}C{F,E}. Kernels can also be
</listItem>
<equation confidence="0.587028">
C{F,E)C{F, E1
</equation>
<bodyText confidence="0.986353666666667">
applied here such as AE = exp( σ2
for English words. Set AEii = 0 and AF ii = 0,
and normalize each row to be unit length;
</bodyText>
<listItem confidence="0.9996594">
• Compute the eigen structure of the normalized
matrix AE, and find the k largest eigen vectors:
v1, v2, ..., vk; Similarly, find the k largest eigen
vectors of AF: u1, u2, ..., uk;
• Stack the k eigenvectors of v1, v2, ..., vk in
the columns of YE, and stack the eigenvectors
u1, u2, ..., uk in the columns for YF; Normalize
rows of both YE and YF to have unit length. YE
is size of |VE |x k and YF is size of |VF |x k;
• Treat each row of YE as a point in R|VE|×k, and
cluster them into K English word clusters us-
ing K-means. Treat each row of YF as a point in
R|VF |×k, and cluster them into K French word
clusters.
• Finally, assign original word ei to cluster Ek
</listItem>
<bodyText confidence="0.940494636363636">
if row i of the matrix YE is clustered as Ek;
similar assignments are for French words.
Here AE and AF are affinity matrixes of pair-wise
inner products between the monolingual words. The
more similar the two words, the larger the value.
In our implementations, we did not apply a kernel
function like the algorithm in (Ng et al., 2001). But
the kernel function such as the exponential func-
tion mentioned above can be applied here to control
how rapidly the similarity falls, using some carefully
chosen scaling parameter.
</bodyText>
<sectionHeader confidence="0.478458" genericHeader="method">
3.2.3 Related Clustering Algorithms
</sectionHeader>
<bodyText confidence="0.999927666666667">
The above algorithm is very close to the variants
of a big family of the spectral clustering algorithms
introduced in (Meila and Shi, 2000) and studied in
(Ng et al., 2001). Spectral clustering refers to a class
of techniques which rely on the eigenstructure of
a similarity matrix to partition points into disjoint
clusters with high intra-cluster similarity and low
inter-cluster similarity. It’s shown to be computing
the k-way normalized cut: K − trYT D− 21 AD− 21 Y
for any matrix Y E RM×N. A is the affinity matrix,
and Y in our algorithm corresponds to the subspaces
of U and V .
Experimentally, it has been observed that using
more eigenvectors and directly computing a k-way
partitioning usually gives better performance. In our
implementations, we used the top 500 eigen vectors
to construct the subspaces of U and V for K-means
clustering.
</bodyText>
<page confidence="0.642502">
28
</page>
<subsectionHeader confidence="0.680648">
3.2.4 K-means
</subsectionHeader>
<bodyText confidence="0.999961833333333">
The K-means here can be considered as a post-
processing step in our proposed bilingual word clus-
tering. For initial centroids, we first compute the
center of the whole data set. The farthest centroid
from the center is then chosen to be the first initial
centroid; and after that, the other K-1 centroids are
chosen one by one to well separate all the previous
chosen centroids.
The stopping criterion is: if the maximal change
of the clusters’ centroids is less than the threshold of
1e-3 between two iterations, the clustering algorithm
then stops.
</bodyText>
<sectionHeader confidence="0.999205" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999912642857143">
To test our algorithm, we applied it to the TIDES
Chinese-English small data track evaluation test set.
After preprocessing, such as English tokenization,
Chinese word segmentation, and parallel sentence
splitting, there are in total 4172 parallel sentence
pairs for training. We manually labeled word align-
ments for 627 test sentence pairs randomly sampled
from the dry-run test data in 2001, which has four
human translations for each Chinese sentence. The
preprocessing for the test data is different from the
above, as it is designed for humans to label word
alignments correctly by removing ambiguities from
tokenization and word segmentation as much as pos-
sible. The data statistics are shown in Table 1.
</bodyText>
<table confidence="0.998450333333333">
English Chinese
Sent. Pairs 4172
Train Words 133598 105331
Voc Size 8359 7984
Sent. Pairs 627
Test Words 25500 19726
Voc Size 4084 4827
Unseen Voc Size 1278 1888
Alignment Links 14769
</table>
<tableCaption confidence="0.999893">
Table 1: Training and Test data statistics
</tableCaption>
<subsectionHeader confidence="0.998657">
4.1 Building Co-occurrence Matrix
</subsectionHeader>
<bodyText confidence="0.956852176470588">
Bilingual word co-occurrence counts are collected
from the training data for constructing the matrix
of C{F,E}. Raw counts are collected without word
alignment between the parallel sentences. Practi-
cally, we can use word alignment as used in (Och,
1999). Given an initial word alignment inferred by
HMM, the counts are collected from the aligned
word pair. If the counts are L-1 normalized, then
the co-occurrence matrix is essentially the bilingual
word-to-word translation lexicon such as P ( fj  |ea .) .
We can remove very small entries (P (f |e) G 1e−�),
so that the matrix of C{F,E} is more sparse for eigen-
structure computation. The proposed algorithm is
then carried out to generate the bilingual word clus-
ters for both English and Chinese.
Figure 1 shows the ranked Eigen values for the
co-occurrence matrix of C{F,E}.
</bodyText>
<note confidence="0.4136975">
Eigen values of affinity matrices
Top 1000 Eigen Values
</note>
<figureCaption confidence="0.957313">
Figure 1: Top-1000 Eigen Values of Co-occurrence
Matrix
</figureCaption>
<bodyText confidence="0.99939">
It is clear, that using the initial HMM word align-
ment for co-occurrence matrix makes a difference.
The top Eigen value using word alignment in plot a.
(the deep blue curve) is 3.1946. The two plateaus
indicate how many top K eigen vectors to choose to
reduce the feature space. The first one indicates that
K is in the range of 50 to 120, and the second plateau
indicates K is in the range of 500 to 800. Plot b. is
inferred from the raw co-occurrence counts with the
top eigen value of 2.7148. There is no clear plateau,
which indicates that the feature space is less struc-
tured than the one built with initial word alignment.
We find 500 top eigen vectors are good enough
for bilingual clustering in terms of efficiency and ef-
fectiveness.
</bodyText>
<figure confidence="0.967431909090909">
3.5
2.5
0.5
0 100 200 300 400 500 600 700 800 900 1000
1.5
3
2
1
(a) co−occur counts from init word alignment
(b) raw co−occur counts from data
29
</figure>
<subsectionHeader confidence="0.996397">
4.2 Clustering Results
</subsectionHeader>
<bodyText confidence="0.999766866666667">
Clusters built via the two described methods are
compared. The first method bil1 is the two-step op-
timization approach: first optimizing the monolin-
gual clusters for target language (English), and af-
terwards optimizing clusters for the source language
(Chinese). The second method bil2 is our proposed
algorithm to compute the eigenstructure of the co-
occurrence matrix, which builds the left and right
subspaces, and finds clusters in such spaces. Top
500 eigen vectors are used to construct these sub-
spaces. For both methods, 1000 clusters are inferred
for English and Chinese respectively. The number
of clusters is chosen in a way that the final word
alignment accuracy was optimal. Table 2 provides
the clustering examples using the two algorithms.
</bodyText>
<table confidence="0.998640928571429">
settings cluster examples
mono-E1 entirely,mainly,merely
mono-E2 10th,13th,14th,16th,17th,18th,19th
20th,21st,23rd,24th,26th
mono-E3 drink,anglophobia,carota,giant,gymnasium
bil1-C3 4, M Rf, Ig, Mip i, VU, Al�
bil2-E1 alcoholic cognac distilled drink
scotch spirits whiskey
bil2-C1 �ip i, ip i, ff, 4�, �, A�A,
��, �, ���, �, ���, _�
bil2-E2 evrec harmony luxury people sedan sedans
tour tourism tourist toward travel
bil2-C2 I R�z%ijljr&apos;, &apos; Y, LTJ, r-Ik, *1, -,
YKii, YK�Y, J�., )�R, ftJ�.
</table>
<tableCaption confidence="0.993691">
Table 2: Bilingual Cluster Examples
</tableCaption>
<bodyText confidence="0.9997352">
The monolingual word clusters often contain
words with similar syntax functions. This hap-
pens with esp. frequent words (eg. mono-E1 and
mono-E2). The algorithm tends to put rare words
such as “carota, anglophobia” into a very big cluster
(eg. mono-E3). In addition, the words within these
monolingual clusters rarely share similar transla-
tions such as the typical cluster of “week, month,
year”. This indicates that the corresponding Chi-
nese clusters inferred by optimizing Eqn. 7 are not
close in terms of translational similarity. Overall, the
method of bil1 does not give us a good translational
correspondence between clusters of two languages.
The English cluster of mono-E3 and its best aligned
candidate of bil1-C3 are not well correlated either.
Our proposed bilingual cluster algorithm bil2
generates the clusters with stronger semantic mean-
ing within a cluster. The cluster of bil2-E1 relates
to the concept of “wine” in English. The mono-
lingual word clustering tends to scatter those words
into several big noisy clusters. This cluster also has a
good translational correspondent in bil2-C1 in Chi-
nese. The clusters of bil2-E2 and bil2-C2 are also
correlated very well. We noticed that the Chinese
clusters are slightly more noisy than their English
corresponding ones. This comes from the noise in
the parallel corpus, and sometimes from ambiguities
of the word segmentation in the preprocessing steps.
To measure the quality of the bilingual clusters,
we can use the following two kind of metrics:
</bodyText>
<listItem confidence="0.903584222222222">
• Average c-mirror (Wang et al., 1996): The c-
mirror of a class EZ is the set of clusters in
Chinese which have a translation probability
greater than c. In our case, c is 0.05, the same
value used in (Och, 1999).
• Perplexity: The perplexity is defined as pro-
portional to the negative log likelihood of the
HMM model Viterbi alignment path for each
sentence pair. We use the bilingual word clus-
</listItem>
<bodyText confidence="0.983916">
ters in two extended HMM models, and mea-
sure the perplexities of the unseen test data af-
ter seven forward-backward training iterations.
The two perplexities are defined as PP1 =
</bodyText>
<equation confidence="0.775168666666667">
exp(− E j�1 log(P(fj|ea,)P(aj|aj−1, Ea,−1,
Fj−1))/J) and PP2 = exp(−J−1 E j�1 log(
P(fj|ea,)P(aj|aj−1)P(Fj−1|Ea,−1))) for the
</equation>
<bodyText confidence="0.967407333333333">
two extended HMM models in Eqn 3 and 4.
Both metrics measure the extent to which the trans-
lation probability is spread out. The smaller the bet-
ter. The following table summarizes the results on
c-mirror and perplexity using different methods on
the unseen test data.
</bodyText>
<table confidence="0.9832165">
algorithms 2-mirror HMM-1 Perp HMM-2 Perp
baseline - 1717.82
bil1 3.97 1810.55 352.28
bil2 2.54 1610.86 343.64
</table>
<bodyText confidence="0.999006714285714">
The baseline uses no word clusters. bil1 and bil2
are defined as above. It is clear that our proposed
method gives overall lower perplexity: 1611 from
the baseline of 1717 using the extended HMM-1.
If we use HMM-2, the perplexity goes down even
more using bilingual clusters: 352.28 using bil1, and
343.64 using bil2. As stated, the four-dimensional
</bodyText>
<page confidence="0.756799">
30
</page>
<bodyText confidence="0.989161428571429">
table of P(aj|aj_1, E(eaj_1), F(fj_1)) is easily
subject to overfitting, and usually gives worse per-
plexities.
Average 2-mirror for the two-step bilingual clus-
tering algorithm is 3.97, and for spectral cluster-
ing algorithm is 2.54. This means our proposed al-
gorithm generates more focused clusters of transla-
tional equivalence. Figure 2 shows the histogram for
the cluster pairs (Fj, Ez), of which the cluster level
translation probabilities P(Fj|Ez) E [0.05, 1]. The
interval [0.05, 1] is divided into 10 bins, with first bin
[0.05, 0.1], and 9 bins divides[0.1, 1] equally. The
percentage for clusters pairs with P(Fj|Ez) falling
in each bin is drawn.
</bodyText>
<figureCaption confidence="0.983506">
Figure 2: Histogram of cluster pairs (Fj, Ez)
</figureCaption>
<bodyText confidence="0.999963363636364">
Our algorithm generates much better aligned clus-
ter pairs than the two-step optimization algorithm.
There are 120 cluster pairs aligned with P(Fj|Ez) &gt;
0.9 using clusters from our algorithm, while there
are only 8 such cluster pairs using the two-step ap-
proach. Figure 3 compares the 2-mirror at different
numbers of clusters using the two approaches. Our
algorithm has a much better 2-mirror than the two-
step approach over different number of clusters.
Overall, the extended HMM-2 is better than
HMM-1 in terms of perplexity, and is easier to train.
</bodyText>
<subsectionHeader confidence="0.997231">
4.3 Applications in Word Alignment
</subsectionHeader>
<bodyText confidence="0.99992625">
We also applied our bilingual word clustering in a
word alignment setting. The training data is the
TIDES small data track. The word alignments are
manually labeled for 627 sentences sampled from
the dryrun test data in 2001. In this manually
aligned data, we include one-to-one, one-to-many,
and many-to-many word alignments. Figure 4 sum-
marizes the word alignment accuracy for different
</bodyText>
<figureCaption confidence="0.983409">
Figure 3: 2-mirror with different settings
</figureCaption>
<bodyText confidence="0.9999415">
methods. The baseline is the standard HMM trans-
lation model defined in Eqn. 2; the HMM1 is de-
fined in Eqn 3, and HMM2 is defined in Eqn 4. The
algorithm is applying our proposed bilingual word
clustering algorithm to infer 1000 clusters for both
languages. As expected, Figure 4 shows that using
</bodyText>
<figureCaption confidence="0.984139">
Figure 4: Word Alignment Over Iterations
</figureCaption>
<bodyText confidence="0.999697818181818">
word clusters is helpful for word alignment. HMM2
gives the best performance in terms of F-measure of
word alignment. One quarter of the words in the test
vocabulary are unseen as shown in Table 1. These
unseen words related alignment links (4778 out of
14769) will be left unaligned by translation models.
Thus the oracle (best possible) recall we could get
is 67.65%. Our standard t-test showed that signifi-
cant interval is 0.82% at the 95% confidence level.
The improvement at the last iteration of HMM is
marginally significant.
</bodyText>
<subsectionHeader confidence="0.995586">
4.4 Applications in Phrase-based Translations
</subsectionHeader>
<bodyText confidence="0.999906">
Our pilot word alignment on unseen data showed
improvements. However, we find it more effective
in our phrase extraction, in which three key scores
</bodyText>
<figure confidence="0.998258756097561">
0.6
0.5
0.4
0.3
0.2
0.1
0
0.05 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Ten bins for P(F|E) ranging from [0.05, 1.0]
Histogram of (F,E) pairs with P(F|E) &gt; 0.05
spec-bi-clustering
two-step-bi-clustering
4.5
2.5
3.5
0.5
1.5
4
2
3
0
1
e-mirror over different settings
BIL2: Co-occur raw counts
BIL2: Co-occur counts from init word-align
BIL1: Two-step optimization
number of clusters
45.00%
44.00%
43.00%
42.00%
41.00%
40.00%
39.00%
38.00%
1 2 3 4 5 6 7
HMM Viterbi Iterations
F-measure of word alignment
Baseline HMM
Extended HMM-1
Extended HMM-2
</figure>
<page confidence="0.970589">
31
</page>
<bodyText confidence="0.999838769230769">
are computed: phrase level fertilities, distortions,
and lexicon scores. These scores are used in a lo-
cal greedy search to extract phrase pairs (Zhao and
Vogel, 2005). This phrase extraction is more sen-
sitive to the differences in P(fj|ez) than the HMM
Viterbi word aligner.
The evaluation conditions are defined in NIST
2003 Small track. Around 247K test set (919 Chi-
nese sentences) specific phrase pairs are extracted
with up to 7-gram in source phrase. A trigram
language model is trained using Gigaword XinHua
news part. With a monotone phrase-based decoder,
the translation results are reported in Table 3. The
</bodyText>
<table confidence="0.983399">
Eval. Baseline Bil1 Bil2
NIST 6.417 6.507 6.582
BLEU 0.1558 0.1575 0.1644
</table>
<tableCaption confidence="0.998545">
Table 3: NIST’03 C-E Small Data Track Evaluation
</tableCaption>
<bodyText confidence="0.999777">
baseline is using the lexicon P(fj|ez) trained from
standard HMM in Eqn. 2, which gives a BLEU
score of 0.1558 +/- 0.0113. Bil1 and Bil2 are using
P (fj|ez) from HMM in Eqn. 4 with 1000 bilingual
word clusters inferred from the two-step algorithm
and the proposed one respectively. Using the clus-
ters from the two-step algorithm gives a BLEU score
of 0.1575, which is close to the baseline. Using clus-
ters from our algorithm, we observe more improve-
ments with BLEU score of 0.1644 and a NIST score
of 6.582.
</bodyText>
<sectionHeader confidence="0.986881" genericHeader="conclusions">
5 Discussions and Conclusions
</sectionHeader>
<bodyText confidence="0.9999796">
In this paper, a new approach for bilingual word
clustering using eigenstructure in bilingual feature
space is proposed. Eigenvectors from this feature
space are considered as bilingual concepts. Bilin-
gual clusters from the subspaces expanded by these
concepts are inferred with high semantic correla-
tions within each cluster, and high translation quali-
ties across clusters from the two languages.
Our empirical study also showed effectiveness of
using bilingual word clusters in extended HMMs for
statistical machine translation. The K-means based
clustering algorithm can be easily extended to do hi-
erarchical clustering. However, extensions of trans-
lation models are needed to leverage the hierarchical
clusters appropriately.
</bodyText>
<sectionHeader confidence="0.996464" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999902734693878">
P.F. Brown, Stephen A. Della Pietra, Vincent. J.
Della Pietra, and Robert L. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter es-
timation. In Computational Linguistics, volume 19(2),
pages 263–331.
David Graff. 2003. Ldc gigaword corpora: English gi-
gaword (ldc catalog no: Ldc2003t05). In LDC link:
http://www.ldc.upenn.edu/Catalog/index.jsp.
R. Kneser and Hermann Ney. 1993. Improved clus-
tering techniques for class-based statistical language
modelling. In European Conference on Speech Com-
munication and Technology, pages 973–976.
Marina Meila and Jianbo Shi. 2000. Learning segmenta-
tion by random walks. In Advances in Neural Informa-
tion Processing Systems. (NIPS2000), pages 873–879.
A. Ng, M. Jordan, and Y. Weiss. 2001. On spectral
clustering: Analysis and an algorithm. In Advances in
Neural Information Processing Systems 14: Proceed-
ings of the 2001.
Franz J. Och and Hermann Ney. 2000. A comparison of
alignment models for statistical machine translation.
In COLING’00: The 18th Int. Conf. on Computational
Linguistics, pages 1086–1090, Saarbrucken, Germany,
July.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models. In
Computational Linguistics, volume 29, pages 19–51.
Franz J. Och. 1999. An efficient method for determin-
ing bilingal word classes. In Ninth Conf. of the Europ.
Chapter of the Association for Computational Linguis-
tics (EACL’99), pages 71–76.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2002. Extensions to hmm-based statistical
word alignment models. In Proc. of the Conference on
Empirical Methods in Natural Language Processing.
S. Vogel, Hermann Ney, and C. Tillmann. 1996. Hmm
based word alignment in statistical machine transla-
tion. In Proc. The 16th Int. Conf. on Computational
Lingustics, (Coling’96), pages 836–841.
Yeyi Wang, John Lafferty, and Alex Waibel. 1996.
Word clustering with parallel spoken language cor-
pora. In proceedings of the 4th International Con-
ference on Spoken Language Processing (ICSLP’96),
pages 2364–2367.
Bing Zhao and Stephan Vogel. 2005. A generalized
alignment-free phrase extraction algorithm. In ACL
2005 Workshop: Building and Using Parallel Cor-
pora: Data-driven Machine Translation and Beyond,
Ann Arbor, Michigan.
</reference>
<page confidence="0.945824">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.446011">
<title confidence="0.8433125">Bilingual Word Spectral Clustering for Statistical Machine Translation P. ‡ Technologies</title>
<author confidence="0.738077">for Automated Learning</author>
<affiliation confidence="0.993864">Carnegie Mellon</affiliation>
<address confidence="0.997461">Pittsburgh, Pennsylvania</address>
<abstract confidence="0.988489684210526">In this paper, a variant of a spectral clustering algorithm is proposed for bilingual word clustering. The proposed algorithm generates the two sets of clusters for both languages efficiently with high semantic correlation within monolingual clusters, and high translation quality across the clusters between two languages. Each cluster level translation is considered as a bilingual concept, which generalizes words in bilingual clusters. This scheme improves the robustness for statistical machine translation models. Two HMMbased translation models are tested to use these bilingual clusters. Improved perplexity, word alignment accuracy, and translation quality are observed in our experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>19</volume>
<issue>2</issue>
<pages>263--331</pages>
<marker>Pietra, Mercer, 1993</marker>
<rawString>P.F. Brown, Stephen A. Della Pietra, Vincent. J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. In Computational Linguistics, volume 19(2), pages 263–331.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Graff</author>
</authors>
<title>Ldc gigaword corpora: English gigaword (ldc catalog no:</title>
<date>2003</date>
<booktitle>Ldc2003t05). In LDC link:</booktitle>
<pages>http://www.ldc.upenn.edu/Catalog/index.jsp.</pages>
<contexts>
<context position="1342" citStr="Graff, 2003" startWordPosition="186" endWordPosition="187">scheme improves the robustness for statistical machine translation models. Two HMMbased translation models are tested to use these bilingual clusters. Improved perplexity, word alignment accuracy, and translation quality are observed in our experiments. 1 Introduction Statistical natural language processing usually suffers from the sparse data problem. Comparing to the available monolingual data, we have much less training data especially for statistical machine translation (SMT). For example, in language modelling, there are more than 1.7 billion words corpora available: English Gigaword by (Graff, 2003). However, for machine translation tasks, there are typically less than 10 million words of training data. Bilingual word clustering is a process of forming corresponding word clusters suitable for machine translation. Previous work from (Wang et al., 1996) showed improvements in perplexity-oriented measures using mixture-based translation lexicon (Brown et al., 1993). A later study by (Och, 25 1999) showed improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model. Both approaches are optimizing the maximum likelihood of parallel cor</context>
</contexts>
<marker>Graff, 2003</marker>
<rawString>David Graff. 2003. Ldc gigaword corpora: English gigaword (ldc catalog no: Ldc2003t05). In LDC link: http://www.ldc.upenn.edu/Catalog/index.jsp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kneser</author>
<author>Hermann Ney</author>
</authors>
<title>Improved clustering techniques for class-based statistical language modelling.</title>
<date>1993</date>
<booktitle>In European Conference on Speech Communication and Technology,</booktitle>
<pages>973--976</pages>
<contexts>
<context position="2161" citStr="Kneser and Ney, 1993" startWordPosition="308" endWordPosition="311">machine translation. Previous work from (Wang et al., 1996) showed improvements in perplexity-oriented measures using mixture-based translation lexicon (Brown et al., 1993). A later study by (Och, 25 1999) showed improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model. Both approaches are optimizing the maximum likelihood of parallel corpus, in which a data point is a sentence pair: an English sentence and its translation in another language such as French. These algorithms are essentially the same as monolingual word clusterings (Kneser and Ney, 1993)—an iterative local search. In each iteration, a two-level loop over every possible word-cluster assignment is tested for better likelihood change. This kind of approach has two drawbacks: first it is easily to get stuck in local optima; second, the clustering of English and the other language are basically two separated optimization processes, and cluster-level translation is modelled loosely. These drawbacks make their approaches generally not very effective in improving translation models. In this paper, we propose a variant of the spectral clustering algorithm (Ng et al., 2001) for bilingu</context>
<context position="8075" citStr="Kneser and Ney, 1993" startWordPosition="1311" endWordPosition="1314">le. However, we do not want this P(Fj|Eaj) to dominate the HMM transition structure, and the obser26 vation probability of P(fj|eaj) during the EM iterations. Thus a uniform prior P(Fj) = 1/|F |is introduced as a smoothing factor for P(Fj|Eaj): P(Fj|Eaj) = AP(Fj|Eaj) + (1 − A)P(Fj), (5) where |F |is the total number of word clusters in French (we use the same number of clusters for both languages). A can be chosen to get optimal performance on a development set. In our case, we fix it to be 0.5 in all our experiments. is a class of its own. There exists efficient leave-oneout style algorithm (Kneser and Ney, 1993), which can automatically determine the number of clusters. For the bilingual part P(fJ1 |eI1, F, E), we can slightly modify the same algorithm as in (Kneser and Ney, 1993). Given the word alignment {�J1} between fJ1 and eI1 collected from the Viterbi path in HMM-based translation model, we can infer F as follows: F = arg max P(fJ1 |eI1, F, E) {F} 3 Bilingual Word Clustering In bilingual word clustering, the task is to build word clusters F and E to form partitions of the vocabularies of the two languages respectively. The two partitions for the vocabularies of F and E are aimed to be suitable</context>
</contexts>
<marker>Kneser, Ney, 1993</marker>
<rawString>R. Kneser and Hermann Ney. 1993. Improved clustering techniques for class-based statistical language modelling. In European Conference on Speech Communication and Technology, pages 973–976.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Meila</author>
<author>Jianbo Shi</author>
</authors>
<title>Learning segmentation by random walks.</title>
<date>2000</date>
<booktitle>In Advances in Neural Information Processing Systems. (NIPS2000),</booktitle>
<pages>873--879</pages>
<contexts>
<context position="14448" citStr="Meila and Shi, 2000" startWordPosition="2428" endWordPosition="2431">ch words. Here AE and AF are affinity matrixes of pair-wise inner products between the monolingual words. The more similar the two words, the larger the value. In our implementations, we did not apply a kernel function like the algorithm in (Ng et al., 2001). But the kernel function such as the exponential function mentioned above can be applied here to control how rapidly the similarity falls, using some carefully chosen scaling parameter. 3.2.3 Related Clustering Algorithms The above algorithm is very close to the variants of a big family of the spectral clustering algorithms introduced in (Meila and Shi, 2000) and studied in (Ng et al., 2001). Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters with high intra-cluster similarity and low inter-cluster similarity. It’s shown to be computing the k-way normalized cut: K − trYT D− 21 AD− 21 Y for any matrix Y E RM×N. A is the affinity matrix, and Y in our algorithm corresponds to the subspaces of U and V . Experimentally, it has been observed that using more eigenvectors and directly computing a k-way partitioning usually gives better performance. In our i</context>
</contexts>
<marker>Meila, Shi, 2000</marker>
<rawString>Marina Meila and Jianbo Shi. 2000. Learning segmentation by random walks. In Advances in Neural Information Processing Systems. (NIPS2000), pages 873–879.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ng</author>
<author>M Jordan</author>
<author>Y Weiss</author>
</authors>
<title>On spectral clustering: Analysis and an algorithm.</title>
<date>2001</date>
<booktitle>In Advances in Neural Information Processing Systems 14: Proceedings of the</booktitle>
<contexts>
<context position="2749" citStr="Ng et al., 2001" startWordPosition="400" endWordPosition="403">rings (Kneser and Ney, 1993)—an iterative local search. In each iteration, a two-level loop over every possible word-cluster assignment is tested for better likelihood change. This kind of approach has two drawbacks: first it is easily to get stuck in local optima; second, the clustering of English and the other language are basically two separated optimization processes, and cluster-level translation is modelled loosely. These drawbacks make their approaches generally not very effective in improving translation models. In this paper, we propose a variant of the spectral clustering algorithm (Ng et al., 2001) for bilingual word clustering. Given parallel corpus, first, the word’s bilingual context is used directly as features - for instance, each English word is represented by its bilingual word translation candidates. Second, latent eigenstructure analysis is carried out in this bilingual feature space, which leads to clusters of words with similar translations. Essentially an affinity matrix is computed using these cross-lingual features. It is then decomposed into two sub-spaces, which are meaningful for translation tasks: the left subspace corresponds to the representation of words in English </context>
<context position="14086" citStr="Ng et al., 2001" startWordPosition="2371" endWordPosition="2374">|x k and YF is size of |VF |x k; • Treat each row of YE as a point in R|VE|×k, and cluster them into K English word clusters using K-means. Treat each row of YF as a point in R|VF |×k, and cluster them into K French word clusters. • Finally, assign original word ei to cluster Ek if row i of the matrix YE is clustered as Ek; similar assignments are for French words. Here AE and AF are affinity matrixes of pair-wise inner products between the monolingual words. The more similar the two words, the larger the value. In our implementations, we did not apply a kernel function like the algorithm in (Ng et al., 2001). But the kernel function such as the exponential function mentioned above can be applied here to control how rapidly the similarity falls, using some carefully chosen scaling parameter. 3.2.3 Related Clustering Algorithms The above algorithm is very close to the variants of a big family of the spectral clustering algorithms introduced in (Meila and Shi, 2000) and studied in (Ng et al., 2001). Spectral clustering refers to a class of techniques which rely on the eigenstructure of a similarity matrix to partition points into disjoint clusters with high intra-cluster similarity and low inter-clu</context>
</contexts>
<marker>Ng, Jordan, Weiss, 2001</marker>
<rawString>A. Ng, M. Jordan, and Y. Weiss. 2001. On spectral clustering: Analysis and an algorithm. In Advances in Neural Information Processing Systems 14: Proceedings of the 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A comparison of alignment models for statistical machine translation.</title>
<date>2000</date>
<booktitle>In COLING’00: The 18th Int. Conf. on Computational Linguistics,</booktitle>
<pages>1086--1090</pages>
<location>Saarbrucken, Germany,</location>
<contexts>
<context position="6138" citStr="Och and Ney, 2000" startWordPosition="968" endWordPosition="971"> first-order HMM is defined as follows: �P(fJ 1 |eI 1) = J P(fj|eaj)P(aj|aj−1), (2) aJ H 1 j=1 where P(aj|aj−1) is the transition probability. This model captures the assumption that words close in the source sentence are aligned to words close in the target sentence. An additional pseudo word of “NULL” is used as the beginning of English sentence for HMM to start with. The (Och and Ney, 2003) model includes other refinements such as special treatment of a jump to a Null word, and a uniform smoothing prior. The HMM with these refinements is used as our baseline. Motivated by the work in both (Och and Ney, 2000) and (Toutanova et al., 2002), we propose the two following simplest versions of extended HMMs to utilize bilingual word clusters. 2.2 Extensions to HMM with word clusters Let F denote the cluster mapping fj —* F(fj), which assigns French word fj to its cluster ID Fj = F(fj). Similarly E maps English word ei to its cluster ID of Ei = E(ei). In this paper, we assume each word belongs to one cluster only. With bilingual word clusters, we can extend the HMM model in Eqn. 1 in the following two ways: P(fJ1 |eI1) = Eai H 1 P(fj |eaj)&apos; P(aj|aj−1, E(eaj−1), F(fj−1)), where E(eaj−1) and F(fj−1) are no</context>
</contexts>
<marker>Och, Ney, 2000</marker>
<rawString>Franz J. Och and Hermann Ney. 2000. A comparison of alignment models for statistical machine translation. In COLING’00: The 18th Int. Conf. on Computational Linguistics, pages 1086–1090, Saarbrucken, Germany, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<booktitle>In Computational Linguistics,</booktitle>
<volume>29</volume>
<pages>pages</pages>
<contexts>
<context position="5916" citStr="Och and Ney, 2003" startWordPosition="925" endWordPosition="928">oted as eaj. Each French word fj is an observation, and it is generated by a HMM state defined as [eaj, aj], where the alignment aj for position j is considered to have a dependency on the previous alignment aj−1. Thus the first-order HMM is defined as follows: �P(fJ 1 |eI 1) = J P(fj|eaj)P(aj|aj−1), (2) aJ H 1 j=1 where P(aj|aj−1) is the transition probability. This model captures the assumption that words close in the source sentence are aligned to words close in the target sentence. An additional pseudo word of “NULL” is used as the beginning of English sentence for HMM to start with. The (Och and Ney, 2003) model includes other refinements such as special treatment of a jump to a Null word, and a uniform smoothing prior. The HMM with these refinements is used as our baseline. Motivated by the work in both (Och and Ney, 2000) and (Toutanova et al., 2002), we propose the two following simplest versions of extended HMMs to utilize bilingual word clusters. 2.2 Extensions to HMM with word clusters Let F denote the cluster mapping fj —* F(fj), which assigns French word fj to its cluster ID Fj = F(fj). Similarly E maps English word ei to its cluster ID of Ei = E(ei). In this paper, we assume each word </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz J. Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. In Computational Linguistics, volume 29, pages 19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz J Och</author>
</authors>
<title>An efficient method for determining bilingal word classes.</title>
<date>1999</date>
<booktitle>In Ninth Conf. of the Europ. Chapter of the Association for Computational Linguistics (EACL’99),</booktitle>
<pages>71--76</pages>
<contexts>
<context position="10123" citStr="Och, 1999" startWordPosition="1660" endWordPosition="1661">arg max P(eI1|E) {E} I = arg max P(Ei|Ei−1)P(ei|Ei). (7) {E} i=1 We need to fix the number of clusters beforehand, otherwise the optimum is reached when each word P(Fj|Eaj)P(fj|Fj). (8) Overall, this bilingual word clustering algorithm is essentially a two-step approach. In the first step, E is inferred by optimizing the monolingual likelihood of English data, and secondly F is inferred by optimizing the bilingual part without changing E. In this way, the algorithm is easy to implement without much change from the monolingual correspondent. This approach was shown to give the best results in (Och, 1999). We use it as our baseline to compare with. 3.2 Bilingual Word Spectral Clustering Instead of using word alignment to bridge the parallel sentence pair, and optimize the likelihood in two separate steps, we develop an alignment-free algorithm using a variant of spectral clustering algorithm. The goal is to build high cluster-level translation quality suitable for translation modelling, and at the same time maintain high intra-cluster similarity , and low inter-cluster similarity for monolingual clusters. 3.2.1 Notations We define the vocabulary VF as the French vocabulary with a size of |VF |</context>
<context position="16966" citStr="Och, 1999" startWordPosition="2842" endWordPosition="2843">kenization and word segmentation as much as possible. The data statistics are shown in Table 1. English Chinese Sent. Pairs 4172 Train Words 133598 105331 Voc Size 8359 7984 Sent. Pairs 627 Test Words 25500 19726 Voc Size 4084 4827 Unseen Voc Size 1278 1888 Alignment Links 14769 Table 1: Training and Test data statistics 4.1 Building Co-occurrence Matrix Bilingual word co-occurrence counts are collected from the training data for constructing the matrix of C{F,E}. Raw counts are collected without word alignment between the parallel sentences. Practically, we can use word alignment as used in (Och, 1999). Given an initial word alignment inferred by HMM, the counts are collected from the aligned word pair. If the counts are L-1 normalized, then the co-occurrence matrix is essentially the bilingual word-to-word translation lexicon such as P ( fj |ea .) . We can remove very small entries (P (f |e) G 1e−�), so that the matrix of C{F,E} is more sparse for eigenstructure computation. The proposed algorithm is then carried out to generate the bilingual word clusters for both English and Chinese. Figure 1 shows the ranked Eigen values for the co-occurrence matrix of C{F,E}. Eigen values of affinity m</context>
<context position="21566" citStr="Och, 1999" startWordPosition="3601" endWordPosition="3602">. The clusters of bil2-E2 and bil2-C2 are also correlated very well. We noticed that the Chinese clusters are slightly more noisy than their English corresponding ones. This comes from the noise in the parallel corpus, and sometimes from ambiguities of the word segmentation in the preprocessing steps. To measure the quality of the bilingual clusters, we can use the following two kind of metrics: • Average c-mirror (Wang et al., 1996): The cmirror of a class EZ is the set of clusters in Chinese which have a translation probability greater than c. In our case, c is 0.05, the same value used in (Och, 1999). • Perplexity: The perplexity is defined as proportional to the negative log likelihood of the HMM model Viterbi alignment path for each sentence pair. We use the bilingual word clusters in two extended HMM models, and measure the perplexities of the unseen test data after seven forward-backward training iterations. The two perplexities are defined as PP1 = exp(− E j�1 log(P(fj|ea,)P(aj|aj−1, Ea,−1, Fj−1))/J) and PP2 = exp(−J−1 E j�1 log( P(fj|ea,)P(aj|aj−1)P(Fj−1|Ea,−1))) for the two extended HMM models in Eqn 3 and 4. Both metrics measure the extent to which the translation probability is s</context>
</contexts>
<marker>Och, 1999</marker>
<rawString>Franz J. Och. 1999. An efficient method for determining bilingal word classes. In Ninth Conf. of the Europ. Chapter of the Association for Computational Linguistics (EACL’99), pages 71–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>H Tolga Ilhan</author>
<author>Christopher D Manning</author>
</authors>
<title>Extensions to hmm-based statistical word alignment models.</title>
<date>2002</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="6167" citStr="Toutanova et al., 2002" startWordPosition="973" endWordPosition="976">ned as follows: �P(fJ 1 |eI 1) = J P(fj|eaj)P(aj|aj−1), (2) aJ H 1 j=1 where P(aj|aj−1) is the transition probability. This model captures the assumption that words close in the source sentence are aligned to words close in the target sentence. An additional pseudo word of “NULL” is used as the beginning of English sentence for HMM to start with. The (Och and Ney, 2003) model includes other refinements such as special treatment of a jump to a Null word, and a uniform smoothing prior. The HMM with these refinements is used as our baseline. Motivated by the work in both (Och and Ney, 2000) and (Toutanova et al., 2002), we propose the two following simplest versions of extended HMMs to utilize bilingual word clusters. 2.2 Extensions to HMM with word clusters Let F denote the cluster mapping fj —* F(fj), which assigns French word fj to its cluster ID Fj = F(fj). Similarly E maps English word ei to its cluster ID of Ei = E(ei). In this paper, we assume each word belongs to one cluster only. With bilingual word clusters, we can extend the HMM model in Eqn. 1 in the following two ways: P(fJ1 |eI1) = Eai H 1 P(fj |eaj)&apos; P(aj|aj−1, E(eaj−1), F(fj−1)), where E(eaj−1) and F(fj−1) are non overlapping word clusters (</context>
</contexts>
<marker>Toutanova, Ilhan, Manning, 2002</marker>
<rawString>Kristina Toutanova, H. Tolga Ilhan, and Christopher D. Manning. 2002. Extensions to hmm-based statistical word alignment models. In Proc. of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>Hermann Ney</author>
<author>C Tillmann</author>
</authors>
<title>Hmm based word alignment in statistical machine translation.</title>
<date>1996</date>
<booktitle>In Proc. The 16th Int. Conf. on Computational Lingustics, (Coling’96),</booktitle>
<pages>836--841</pages>
<contexts>
<context position="5083" citStr="Vogel et al., 1996" startWordPosition="772" endWordPosition="776">ranslates it into an English sentence with I words denoted by eI1 = e1e2...eI. The SMT system first proposes multiple English hypotheses in its model space. Among all the hypotheses, the system selects the one with the highest conditional probability according to Bayes’s decision rule: eI1 = arg max P(eI1|fJ1 ) = arg max P(fJ1 |eI1)P(eI1), {eI1} {eI1} (1) where P(fJ1 |eI1) is called translation model, and P(eI1) is called language model. The translation model is the key component, which is the focus in this paper. 2.1 HMM-based Translation Model HMM is one of the effective translation models (Vogel et al., 1996), which is easily scalable to very large training corpus. To model word-to-word translation, we introduce the mapping j —* aj, which assigns a French word fj in position j to a English word ei in position i = aj denoted as eaj. Each French word fj is an observation, and it is generated by a HMM state defined as [eaj, aj], where the alignment aj for position j is considered to have a dependency on the previous alignment aj−1. Thus the first-order HMM is defined as follows: �P(fJ 1 |eI 1) = J P(fj|eaj)P(aj|aj−1), (2) aJ H 1 j=1 where P(aj|aj−1) is the transition probability. This model captures </context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, Hermann Ney, and C. Tillmann. 1996. Hmm based word alignment in statistical machine translation. In Proc. The 16th Int. Conf. on Computational Lingustics, (Coling’96), pages 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yeyi Wang</author>
<author>John Lafferty</author>
<author>Alex Waibel</author>
</authors>
<title>Word clustering with parallel spoken language corpora.</title>
<date>1996</date>
<booktitle>In proceedings of the 4th International Conference on Spoken Language Processing (ICSLP’96),</booktitle>
<pages>2364--2367</pages>
<contexts>
<context position="1599" citStr="Wang et al., 1996" startWordPosition="224" endWordPosition="227">s. 1 Introduction Statistical natural language processing usually suffers from the sparse data problem. Comparing to the available monolingual data, we have much less training data especially for statistical machine translation (SMT). For example, in language modelling, there are more than 1.7 billion words corpora available: English Gigaword by (Graff, 2003). However, for machine translation tasks, there are typically less than 10 million words of training data. Bilingual word clustering is a process of forming corresponding word clusters suitable for machine translation. Previous work from (Wang et al., 1996) showed improvements in perplexity-oriented measures using mixture-based translation lexicon (Brown et al., 1993). A later study by (Och, 25 1999) showed improvements on perplexity of bilingual corpus, and word translation accuracy using a template-based translation model. Both approaches are optimizing the maximum likelihood of parallel corpus, in which a data point is a sentence pair: an English sentence and its translation in another language such as French. These algorithms are essentially the same as monolingual word clusterings (Kneser and Ney, 1993)—an iterative local search. In each it</context>
<context position="21393" citStr="Wang et al., 1996" startWordPosition="3564" endWordPosition="3567">English. The monolingual word clustering tends to scatter those words into several big noisy clusters. This cluster also has a good translational correspondent in bil2-C1 in Chinese. The clusters of bil2-E2 and bil2-C2 are also correlated very well. We noticed that the Chinese clusters are slightly more noisy than their English corresponding ones. This comes from the noise in the parallel corpus, and sometimes from ambiguities of the word segmentation in the preprocessing steps. To measure the quality of the bilingual clusters, we can use the following two kind of metrics: • Average c-mirror (Wang et al., 1996): The cmirror of a class EZ is the set of clusters in Chinese which have a translation probability greater than c. In our case, c is 0.05, the same value used in (Och, 1999). • Perplexity: The perplexity is defined as proportional to the negative log likelihood of the HMM model Viterbi alignment path for each sentence pair. We use the bilingual word clusters in two extended HMM models, and measure the perplexities of the unseen test data after seven forward-backward training iterations. The two perplexities are defined as PP1 = exp(− E j�1 log(P(fj|ea,)P(aj|aj−1, Ea,−1, Fj−1))/J) and PP2 = exp</context>
</contexts>
<marker>Wang, Lafferty, Waibel, 1996</marker>
<rawString>Yeyi Wang, John Lafferty, and Alex Waibel. 1996. Word clustering with parallel spoken language corpora. In proceedings of the 4th International Conference on Spoken Language Processing (ICSLP’96), pages 2364–2367.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Zhao</author>
<author>Stephan Vogel</author>
</authors>
<title>A generalized alignment-free phrase extraction algorithm.</title>
<date>2005</date>
<booktitle>In ACL 2005 Workshop: Building and Using Parallel Corpora: Data-driven Machine Translation and Beyond,</booktitle>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="26276" citStr="Zhao and Vogel, 2005" startWordPosition="4370" endWordPosition="4373">nging from [0.05, 1.0] Histogram of (F,E) pairs with P(F|E) &gt; 0.05 spec-bi-clustering two-step-bi-clustering 4.5 2.5 3.5 0.5 1.5 4 2 3 0 1 e-mirror over different settings BIL2: Co-occur raw counts BIL2: Co-occur counts from init word-align BIL1: Two-step optimization number of clusters 45.00% 44.00% 43.00% 42.00% 41.00% 40.00% 39.00% 38.00% 1 2 3 4 5 6 7 HMM Viterbi Iterations F-measure of word alignment Baseline HMM Extended HMM-1 Extended HMM-2 31 are computed: phrase level fertilities, distortions, and lexicon scores. These scores are used in a local greedy search to extract phrase pairs (Zhao and Vogel, 2005). This phrase extraction is more sensitive to the differences in P(fj|ez) than the HMM Viterbi word aligner. The evaluation conditions are defined in NIST 2003 Small track. Around 247K test set (919 Chinese sentences) specific phrase pairs are extracted with up to 7-gram in source phrase. A trigram language model is trained using Gigaword XinHua news part. With a monotone phrase-based decoder, the translation results are reported in Table 3. The Eval. Baseline Bil1 Bil2 NIST 6.417 6.507 6.582 BLEU 0.1558 0.1575 0.1644 Table 3: NIST’03 C-E Small Data Track Evaluation baseline is using the lexic</context>
</contexts>
<marker>Zhao, Vogel, 2005</marker>
<rawString>Bing Zhao and Stephan Vogel. 2005. A generalized alignment-free phrase extraction algorithm. In ACL 2005 Workshop: Building and Using Parallel Corpora: Data-driven Machine Translation and Beyond, Ann Arbor, Michigan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>