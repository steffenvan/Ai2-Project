<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000539">
<title confidence="0.9994015">
A Probabilistic Setting and Lexical Cooccurrence Model
for Textual Entailment
</title>
<author confidence="0.982735">
Oren Glickman and Ido Dagan
</author>
<affiliation confidence="0.9740095">
Department of Computer Science
Bar Ilan University
</affiliation>
<email confidence="0.997603">
{glikmao,Dagan}@cs.biu.ac.il
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999794545454545">
This paper proposes a general probabilis-
tic setting that formalizes a probabilistic
notion of textual entailment. We further
describe a particular preliminary model
for lexical-level entailment, based on
document cooccurrence probabilities,
which follows the general setting. The
model was evaluated on two application
independent datasets, suggesting the rele-
vance of such probabilistic approaches for
entailment modeling.
</bodyText>
<sectionHeader confidence="0.99938" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99838647826087">
Many Natural Language Processing (NLP)
applications need to recognize when the meaning
of one text can be expressed by, or inferred from,
another text. Information Retrieval (IR), Question
Answering (QA), Information Extraction (IE), text
summarization and Machine Translation (MT)
evaluation are examples of applications that need
to assess this semantic relationship between text
segments. The Textual Entailment Recognition
task (Dagan et al., 2005) has recently been pro-
posed as an application independent framework for
modeling such inferences.
Within the textual entailment framework, a text
t is said to entail a textual hypothesis h if the truth
of h can be inferred from t. Textual entailment cap-
tures generically a broad range of inferences that
are relevant for multiple applications. For example,
a QA system has to identify texts that entail a hy-
pothesized answer. Given the question &amp;quot;Does John
Speak French?&amp;quot;, a text that includes the sentence
&amp;quot;John is a fluent French speaker&amp;quot; entails the sug-
gested answer &amp;quot;John speaks French.&amp;quot; In many
cases, though, entailment inference is uncertain
</bodyText>
<page confidence="0.996663">
43
</page>
<bodyText confidence="0.999951073170732">
and has a probabilistic nature. For example, a text
that includes the sentence &amp;quot;John was born in
France.&amp;quot; does not strictly entail the above answer.
Yet, it is clear that it does increase substantially the
likelihood that the hypothesized answer is true.
The uncertain nature of textual entailment calls
for its explicit modeling in probabilistic terms. We
therefore propose a general generative probabilistic
setting for textual entailment, which allows a clear
formulation of concrete probabilistic models for
this task. We suggest that the proposed setting may
provide a unifying framework for modeling uncer-
tain semantic inferences from texts.
An important sub task of textual entailment,
which we term lexical entailment, is recognizing if
the lexical concepts in a hypothesis h are entailed
from a given text t, even if the relations which hold
between these concepts may not be entailed from t.
This is typically a necessary, but not sufficient,
condition for textual entailment. For example, in
order to infer from a text the hypothesis &amp;quot;Chrysler
stock rose,&amp;quot; it is a necessary that the concepts of
Chrysler, stock and rise must be inferred from the
text. However, for proper entailment it is further
needed that the right relations hold between these
concepts. In this paper we demonstrate the rele-
vance of the general probabilistic setting for mod-
eling lexical entailment, by devising a preliminary
model that is based on document co-occurrence
probabilities in a bag of words representation.
Although our proposed lexical system is rela-
tively simple, as it doesn’t rely on syntactic or
other deeper analysis, it nevertheless was among
the top ranking systems in the first Recognising
Textual Entailment (RTE) Challenge (Glickman et
al., 2005a). The model was evaluated also on an
additional dataset, where it compares favorably
with a state-of-the-art heuristic score. These results
suggest that the proposed probabilistic framework
is a promising basis for devising improved models
that incorporate richer information.
</bodyText>
<note confidence="0.751078">
Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 43–48,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<table confidence="0.8760048">
example text hypothesis
1 John is a French Speaker John speaks French
2 John was born in France
3 Harry&apos;s birthplace is Iowa Harry was born in Iowa
4 Harry is returning to his Iowa hometown
</table>
<tableCaption confidence="0.999566">
Table 1: example sentence pairs
</tableCaption>
<sectionHeader confidence="0.899672" genericHeader="method">
2 Probabilistic Textual Entailment
</sectionHeader>
<subsectionHeader confidence="0.75084">
2.1 Motivation
</subsectionHeader>
<bodyText confidence="0.999922166666667">
A common definition of entailment in formal se-
mantics (Chierchia. and McConnell-Ginet, 1990)
specifies that a text t entails another text h (hy-
pothesis, in our terminology) if h is true in every
circumstance (possible world) in which t is true.
For example, in examples 1 and 3 from Table 1
we’d assume humans to agree that the hypothesis
is necessarily true in any circumstance for which
the text is true. In such intuitive cases, textual en-
tailment may be perceived as being certain, or, tak-
ing a probabilistic perspective, as having a
probability of 1.
In many other cases, though, entailment infer-
ence is uncertain and has a probabilistic nature. In
example 2, the text doesn’t contain enough infor-
mation to infer the hypothesis’ truth. And in exam-
ple 4, the meaning of the word hometown is
ambiguous and therefore one cannot infer for cer-
tain that the hypothesis is true. In both of these
cases there are conceivable circumstances for
which the text is true and the hypothesis false. Yet,
it is clear that in both examples, the text does in-
crease substantially the likelihood of the correct-
ness of the hypothesis, which naturally extends the
classical notion of certain entailment. Given the
text, we expect the probability that the hypothesis
is indeed true to be relatively high, and signifi-
cantly higher than its probability of being true
without reading the text. Aiming to model applica-
tion needs, we suggest that the probability of the
hypothesis being true given the text reflects an ap-
propriate confidence score for the correctness of a
particular textual inference. In the next sub-
sections we propose a concrete probabilistic setting
that formalizes the notion of truth probabilities in
such cases.
</bodyText>
<subsectionHeader confidence="0.998981">
2.2 A Probabilistic Setting
</subsectionHeader>
<bodyText confidence="0.9999702">
Let T denote a space of possible texts, and teT a
specific text. Let H denote the set of all possible
hypotheses. A hypothesis heH is a propositional
statement which can be assigned a truth value. For
now it is assumed that h is represented as a textual
statement, but in principle it could also be ex-
pressed as a formula in some propositional lan-
guage.
A semantic state of affairs is captured by a
mapping from H to {0=false, 1=true}, denoted by
w: H —&gt; {0, 1} (called here possible world, follow-
ing common terminology). A possible world w
represents a concrete set of truth value assignments
for all possible propositions. Accordingly, W de-
notes the set of all possible worlds.
</bodyText>
<subsectionHeader confidence="0.906347">
2.2.1 A Generative Model
</subsectionHeader>
<bodyText confidence="0.9999861">
We assume a probabilistic generative model for
texts and possible worlds. In particular, we assume
that texts are generated along with a concrete state
of affairs, represented by a possible world. Thus,
whenever the source generates a text t, it generates
also corresponding hidden truth assignments that
constitute a possible world w.
The probability distribution of the source, over
all possible texts and truth assignments T x W, is
assumed to reflect inferences that are based on the
generated texts. That is, we assume that the distri-
bution of truth assignments is not bound to reflect
the state of affairs in a particular &amp;quot;real&amp;quot; world, but
only the inferences about propositions&apos; truth which
are related to the text. In particular, the probability
for generating a true hypothesis h that is not related
at all to the corresponding text is determined by
some prior probability P(h). For example, h=&amp;quot;Paris
is the capital of France&amp;quot; might have a prior smaller
than 1 and might well be false when the generated
text is not related at all to Paris or France. In fact,
we may as well assume that the notion of textual
entailment is relevant only for hypotheses for
which P(h) &lt; 1, as otherwise (i.e. for tautologies)
there is no need to consider texts that would sup-
port h&apos;s truth. On the other hand, we assume that
the probability of h being true (generated within w)
would be higher than the prior when the corre-
sponding t does contribute information that sup-
ports h&apos;s truth.
</bodyText>
<page confidence="0.995057">
44
</page>
<bodyText confidence="0.9860384">
We define two types of events over the prob-
ability space for T x W:
I) For a hypothesis h, we denote as Trh the random
variable whose value is the truth value assigned to
h in a given world. Correspondingly, Trh=1 is the
event of h being assigned a truth value of 1 (true).
II) For a text t, we use t itself to denote also the
event that the generated text is t (as usual, it is
clear from the context whether t denotes the text or
the corresponding event).
</bodyText>
<subsectionHeader confidence="0.931707">
2.3 Probabilistic textual entailment
definition
</subsectionHeader>
<bodyText confidence="0.999862875">
We say that a text t probabilistically entails a hy-
pothesis h (denoted as t =&gt; h) if t increases the like-
lihood of h being true, that is, if P(Trh = 1 |t) &gt;
P(Trh = 1) or equivalently if the pointwise mutual
information, I(Trh=1,t), is greater then 0. Once
knowing that t=&gt;h, P(Trh=1 |t) serves as a probabil-
istic confidence value for h being true given t.
Application settings would typically require
that P(Trh = 1 |t) obtains a high value; otherwise,
the text would not be considered sufficiently rele-
vant to support h&apos;s truth (e.g. a supporting text in
QA or IE should entail the extracted information
with high confidence). Finally, we ignore here the
case in which t contributes negative information
about h, leaving this relevant case for further in-
vestigation.
</bodyText>
<subsectionHeader confidence="0.990614">
2.4 Model Properties
</subsectionHeader>
<bodyText confidence="0.994830266666667">
It is interesting to notice the following properties
and implications of our model:
A) Textual entailment is defined as a relationship
between texts and propositions whose representa-
tion is typically based on text as well, unlike logi-
cal entailment which is a relationship between
propositions only. Accordingly, textual entail-
ment confidence is conditioned on the actual gen-
eration of a text, rather than its truth. For
illustration, we would expect that the text “His
father was born in Italy” would logically entail
the hypothesis “He was born in Italy” with high
probability – since most people who’s father was
born in Italy were also born there. However we
expect that the text would actually not probabilis-
tically textually entail the hypothesis since most
people for whom it is specifically reported that
their father was born in Italy were not born in
Italy.1
B) We assign probabilities to propositions (hy-
potheses) in a similar manner to certain probabil-
istic reasoning approaches (e.g. Bacchus, 1990;
Halpern, 1990). However, we also assume a gen-
erative model of text, similar to probabilistic lan-
guage and machine translation models, which
supplies the needed conditional probability distri-
bution. Furthermore, since our conditioning is on
texts rather than propositions we do not assume
any specific logic representation language for text
meaning, and only assume that textual hypotheses
can be assigned truth values.
C) Our framework does not distinguish between
textual entailment inferences that are based on
knowledge of language semantics (such as mur-
dering =&gt; killing) and inferences based on domain
or world knowledge (such as live in Paris =&gt; live
in France). Both are needed in applications and it
is not clear at this stage where and how to put
such a borderline.
D) An important feature of the proposed frame-
work is that for a given text many hypotheses are
likely to be true. Consequently, for a given text t
and hypothesis h, Y—hP(Trh=1|t) does not sum to 1.
This differs from typical generative settings for
IR and MT (Ponte and croft, 1998; Brown et al.,
1993), where all conditioned events are disjoint
by construction. In the proposed model, it is
rather the case that P(Trh=1|t) + P(Trh=0|t) = 1, as
we are interested in the probability that a single
particular hypothesis is true (or false).
E) An implemented model that corresponds to our
probabilistic setting is expected to produce an
estimate for P(Trh = 1 |t). This estimate is ex-
pected to reflect all probabilistic aspects involved
in the modeling, including inherent uncertainty of
the entailment inference itself (as in example 2 of
Table 1), possible uncertainty regarding the cor-
rect disambiguation of the text (example 4), as
well as probabilistic estimates that stem from the
particular model structure.
</bodyText>
<sectionHeader confidence="0.998507" genericHeader="method">
3 A Lexical Entailment Model
</sectionHeader>
<bodyText confidence="0.9986935">
We suggest that the proposed setting above pro-
vides the necessary grounding for probabilistic
</bodyText>
<footnote confidence="0.942332">
1 This seems to be the case, when analyzing the results of en-
tering the above text in a web search engine.
</footnote>
<page confidence="0.999474">
45
</page>
<bodyText confidence="0.999972">
modeling of textual entailment. Since modeling the
full extent of the textual entailment problem is
clearly a long term research goal, in this paper we
rather focus on the above mentioned sub-task of
lexical entailment - identifying when the lexical
elements of a textual hypothesis h are inferred
from a given text t.
To model lexical entailment we first assume that
the meanings of the individual content words in a
hypothesis can be assigned truth values. One pos-
sible interpretation for such truth values is that
lexical concepts are assigned existential meanings.
For example, for a given text t, Trbook=1 if it can be
inferred in t’s state of affairs that a book exists.
Our model does not depend on any such particular
interpretation, though, as we only assume that truth
values can be assigned for lexical items but do not
explicitly annotate or evaluate this sub-task.
Given this setting, a hypothesis is assumed to be
true if and only if all its lexical components are
true as well. This captures our target perspective of
lexical entailment, while not modeling here other
entailment aspects. When estimating the entailment
probability we assume that the truth probability of
a term u in a hypothesis h is independent of the
truth of the other terms in h, obtaining:
</bodyText>
<equation confidence="0.9880145">
P(Trh = 1 |t) = IIu.hP(Tru=1|t) (1)
P(Trh = 1) = IIuEhP(Tru=1)
</equation>
<bodyText confidence="0.99988725">
In order to estimate P(Tru=1|v1, ..., vn) for a
given word u and text t={v1, ..., vn}, we further
assume that the majority of the probability mass
comes from a specific entailing word in t:
</bodyText>
<equation confidence="0.76694">
P(Tru =1 |t) = max v,=-t P(Tru =1 |Tv) (2)
</equation>
<bodyText confidence="0.950642166666667">
where Tv denotes the event that a generated text
contains the word v. This corresponds to expecting
that each word in h will be entailed from a specific
word in t (rather than from the accumulative con-
text of t as a whole2). Alternatively, one can view
(2) as inducing an alignment between terms in the
h to the terms in the t, somewhat similar to align-
ment models in statistical MT (Brown et al., 1993).
Thus we propose estimating the entailment
probability based on lexical entailment probabili-
ties from (1) and (2) as follows:
P(Tr=1 |t) =��uhmat P(Tru =1 |Tv) (3)
</bodyText>
<subsectionHeader confidence="0.9901245">
3.1 Estimating Lexical Entailment
Probabilities
</subsectionHeader>
<bodyText confidence="0.969924129032258">
We perform unsupervised empirical estimation of
the lexical entailment probabilities, P(Tru=1|Tv),
based on word co-occurrence frequencies in a cor-
pus. Following our proposed probabilistic model
(cf. Section 2.2.1), we assume that the domain
corpus is a sample generated by a language source.
Each document represents a generated text and a
(hidden) possible world. Given that the possible
world of the text is not observed we do not know
the truth assignments of hypotheses for the ob-
served texts. We therefore further make the sim-
plest assumption that all hypotheses stated
verbatim in a document are true and all others are
false and hence P(Tru=1|Tv) = P(Tu |Tv). This simple
co-occurrence probability, which we denote as
lexical entailment probability – lep(u,v), is easily
estimated from the corpus based on maximum like-
lihood counts:
lep (u, v) = P (Tru = 1  |T) &apos;zt� nu,v (4)
nv
where nv is the number of documents containing
word v and nu,v is the number of documents con-
taining both u and v.
Given our definition of the textual entailment
relationship (cf. Section 2.3) for a given word v we
only consider for entailment words u for which
P(Tru=1|Tv)&gt; P(Tru=1) or based on our estimations,
for which nu,v/nu &gt; nv/N (N is total number of
documents in the corpus).
We denote as tep the textual entailment probability
estimation as derived from (3) and (4) above:
</bodyText>
<subsectionHeader confidence="0.997888">
3.2 Baseline model
</subsectionHeader>
<bodyText confidence="0.9996222">
As a baseline model for comparison, we use a
score developed within the context of text summa-
rization. (Monz and de Rijke, 2001) propose mod-
eling the directional entailment between two texts
t1, t2 via the following score:
</bodyText>
<equation confidence="0.830344153846154">
I idf (w)
w t t
� �
entscore t t
( , ) ( 1 2 )
� (6)
1 2 �
idf (w)
Hueh max ve
tep (t, h) =t lep u v
( , ) (5)
w t
� 2
</equation>
<bodyText confidence="0.987101333333333">
where idf(w) = log(N/nw), N is total number of
documents in corpus and nw is number of docu-
2 Such a model is proposed in (Glickman et al., 2005b)
</bodyText>
<page confidence="0.869605">
46
</page>
<bodyText confidence="0.997540896103896">
ments containing word w. A practically equivalent pansion (Nie and Brisebois, 1996; Yang and Chua,
measure was independently proposed in the con- 2002). Using WordNet, we expanded the hypothe-
text of QA by (Saggion et al., 2004)3. This baseline ses’ terms with morphological alternations and
measure captures word overlap, considering only semantically related words4.
words that appear in both texts and weighs them For each hypothesis stop words were removed
based on their inverse document frequency. and all content words were expanded as described
4 The RTE challenge dataset above. Boolean Search included a conjunction of
The RTE dataset (Dagan et al., 2005) consists the disjunction of the term’s expansions and was
of sentence pairs annotated for entailment. Fo this performed at the paragraph level over the full
dataset we used word cooccurrence frequencies Reuters corpus, as common in IR for QA. Since we
obtained from a web search engine. The details of wanted to focus our research on semantic variabil-
this experiment are described in Glickman et al., ity we excluded from the result set paragraphs that
2005a. The resulting accuracy on the test set was contain all original words of the hypothesis or their
59% and the resulting confidence weighted score morphological derivations. The resulting dataset
was 0.57. Both are statistically significantly better consists of 50 hypotheses and over a million re-
than chance at the 0.01 level. The baseline model trieved paragraphs (10 hypotheses had only exact
(6) from Section 3.2, which takes into account only matches). The number of paragraphs retrieved per
terms appearing in both the text and hypothesis, hypothesis range from 1 to 400,000.5
achieved an accuracy of only 56%. Although our
proposed lexical system is relatively simple, as it
doesn’t rely on syntactic or other deeper analysis,
it nevertheless was among the top ranking systems
in the RTE Challenge.
5.1 Evaluation
The model’s entailment probability, tep, was com-
pared to the following two baseline models. The
first, denoted as base, is the naïve baseline in
which all retrieved texts are presumed to entail the
hypothesis with equal confidence. This baseline
corresponds to systems which perform blind ex-
pansion with no weighting. The second baseline,
entscore, is the entailment score (6) from 3.2.
The top 20 best results for all methods were
given to judges to be annotated for entailment.
Judges were asked to annotate an example as true
if given the text they can infer with high confi-
dence that the hypothesis is true (similar to the
guidelines published for the RTE Challenge data-
set). Accordingly, they were instructed to annotate
the example as false if either they believe the hy-
pothesis is false given the text or if the text is unre-
lated to the hypothesis. In total there were 1683
text-hypothesis pairs, which were randomly di-
vided between two judges. In order to measure
agreement, we had 200 of the pairs annotated by
both judges, yielding a moderate agreement (a
Kappa of 0.6).
5 RCV1 dataset
In addition to the RTE dataset we were interested
in evaluating the model on a more representative
set of texts and hypotheses that better corresponds
to applicative settings. We focused on the informa-
tion seeking setting, common in applications such
as QA and IR, in which a hypothesis is given and it
is necessary to identify texts that entail it.
An annotator was asked to choose 60 hypothe-
ses based on sentences from the first few docu-
ments in the Reuters Corpus Volume 1 (Rose et al.,
2002). The annotator was instructed to choose sen-
tential hypotheses such that their truth could easily
be evaluated. We further required that the hypothe-
ses convey a reasonable information need in such a
way that they might correspond to potential ques-
tions, semantic queries or IE relations. Table 2
shows a few of the hypotheses.
In order to create a set of candidate entailing
texts for the given set of test hypotheses, we fol-
lowed the common practice of WordNet based ex-
4 The following WordNet relations were used: Synonyms, see
also, similar to, hypernyms/hyponyms, meronyms/holonyms,
pertainyms, attribute, entailment, cause and domain
5 The dataset is available at:
http://ir-srv.cs.biu.ac.il:64080/emsee05_dataset.zip
3 (Saggion et al., 2004) actually proposed the above score with
no normalizing denominator. However for a given hypothesis
it results with the same ranking of candidate entailing texts.
47
</bodyText>
<sectionHeader confidence="0.586611" genericHeader="evaluation">
5.2 Results
</sectionHeader>
<table confidence="0.707333333333333">
base entscore tep
precision 0.464 0.568 0.647
cws 0.396 0.509 0.575
</table>
<tableCaption confidence="0.76152775">
Table 2: Results
Table 2 includes the results of macro averaging the
precision at top-20 and the average confidence
weighted score (cws) achieved for the 50 hypothe-
</tableCaption>
<bodyText confidence="0.972361076923077">
ses. Applying Wilcoxon Signed-Rank Test, our
model performs significantly better (at the 0.01
level) than entscore and base for both precision and
cws. Analyzing the results showed that many of
the mistakes were not due to wrong expansion but
rather to a lack of a deeper analysis of the text and
hypothesis (e.g. example 3 in Table 2). Indeed this
is a common problem with lexical models. Incor-
porating additional linguistic levels into the prob-
abilistic entailment model, such as syntactic
matching, co-reference resolution and word sense
disambiguation, becomes a challenging target for
future research.
</bodyText>
<sectionHeader confidence="0.999604" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.9999699375">
This paper proposes a generative probabilistic set-
ting that formalizes the notion of probabilistic tex-
tual entailment, which is based on the conditional
probability that a hypothesis is true given the text.
This probabilistic setting provided the necessary
grounding for a concrete probabilistic model of
lexical entailment that is based on document co-
occurrence statistics in a bag of words representa-
tion. Although the illustrated lexical system is
relatively simple, as it doesn’t rely on syntactic or
other deeper analysis, it nevertheless achieved en-
couraging results. The results suggest that such a
probabilistic framework is a promising basis for
improved implementations incorporating deeper
types of knowledge and a common test-bed for
more sophisticated models.
</bodyText>
<sectionHeader confidence="0.99888" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.954172333333333">
This work was supported in part by the IST Pro-
gramme of the European Community, under the
PASCAL Network of Excellence, IST-2002-
506778. This publication only reflects the authors&apos;
views. We would also like to thank Ruthie Mandel
and Tal Itzhak Ron for their annotation work.
</bodyText>
<sectionHeader confidence="0.986611" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999595086956521">
Fahiem Bacchus. 1990. Representing and Reasoning
with Probabilistic Knowledge, M.I.T. Press.
Peter F. Brown, Vincent J. Della Pietra, Stephen A.
Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263–311.
Chierchia, Gennaro, and Sally McConnell-Ginet. 2001.
Meaning and grammar: An introduction to seman-
tics, 2nd. edition. Cambridge, MA: MIT Press.
Ido Dagan, Oren Glickman and Bernardo Magnini.
2005. The PASCAL Recognising Textual Entailment
Challenge. In Proceedings of the PASCAL Chal-
lenges Workshop for Recognizing Textual Entail-
ment. Southampton, U.K.
Oren Glickman, Ido Dagan and Moshe Koppel. 2005a.
Web Based Probabilistic Textual Entailment,
PASCAL Challenges Workshop for Recognizing
Textual Entailment.
Oren Glickman, Ido Dagan and Moshe Koppel. 2005b.
A Probabilistic Classification Approach for Lexical
Textual Entailment, Twentieth National Conference
on Artificial Intelligence (AAAI-05).
Joseph Y. Halpern. 1990. An analysis of first-order lo-
gics of probability. Artificial Intelligence 46:311-350.
Christof Monz, Maarten de Rijke. 2001. Light-Weight
Entailment Checking for Computational Semantics.
In Proc. of the third workshop on inference in com-
putational semantics (ICoS-3).
Jian-Yun Nie and Martin Brisebois. 1996. An Inferential
Approach to Information Retrieval and Its Implemen-
tation Using a Manual Thesaurus. Artificial Intelli-
gence Revue 10(5-6): 409-439.
Jay M. Ponte, W. Bruce Croft, 1998. A Language Mod-
eling Approach to Information Retrieval. SIGIR con-
ference on Research and Development in Information
Retrieval.
Tony G. Rose, Mary Stevenson, and Miles Whitehead.
2002. The Reuters Corpus volume 1 - from yester-
day’s news to tomorrow’s language resources. Third
International Conference on Language Resources and
Evaluation (LREC).
Hui Yang and Tat-Seng Chua. 2002. The integration of
lexical knowledge and external resources for ques-
tion answering. The eleventh Text REtrieval Confer-
ence (TREC-11).
</reference>
<page confidence="0.999351">
48
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.903883">
<title confidence="0.997437">A Probabilistic Setting and Lexical Cooccurrence for Textual Entailment</title>
<author confidence="0.997274">Oren Glickman</author>
<author confidence="0.997274">Ido</author>
<affiliation confidence="0.9785775">Department of Computer Bar Ilan University</affiliation>
<email confidence="0.986803">glikmao@cs.biu.ac.il</email>
<email confidence="0.986803">Dagan@cs.biu.ac.il</email>
<abstract confidence="0.996890166666666">This paper proposes a general probabilistic setting that formalizes a probabilistic notion of textual entailment. We further describe a particular preliminary model for lexical-level entailment, based on document cooccurrence probabilities, which follows the general setting. The model was evaluated on two application independent datasets, suggesting the relevance of such probabilistic approaches for entailment modeling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Fahiem Bacchus</author>
</authors>
<title>Representing and Reasoning with Probabilistic Knowledge,</title>
<date>1990</date>
<publisher>M.I.T. Press.</publisher>
<contexts>
<context position="10451" citStr="Bacchus, 1990" startWordPosition="1707" endWordPosition="1708">her than its truth. For illustration, we would expect that the text “His father was born in Italy” would logically entail the hypothesis “He was born in Italy” with high probability – since most people who’s father was born in Italy were also born there. However we expect that the text would actually not probabilistically textually entail the hypothesis since most people for whom it is specifically reported that their father was born in Italy were not born in Italy.1 B) We assign probabilities to propositions (hypotheses) in a similar manner to certain probabilistic reasoning approaches (e.g. Bacchus, 1990; Halpern, 1990). However, we also assume a generative model of text, similar to probabilistic language and machine translation models, which supplies the needed conditional probability distribution. Furthermore, since our conditioning is on texts rather than propositions we do not assume any specific logic representation language for text meaning, and only assume that textual hypotheses can be assigned truth values. C) Our framework does not distinguish between textual entailment inferences that are based on knowledge of language semantics (such as murdering =&gt; killing) and inferences based o</context>
</contexts>
<marker>Bacchus, 1990</marker>
<rawString>Fahiem Bacchus. 1990. Representing and Reasoning with Probabilistic Knowledge, M.I.T. Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Stephen A Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="11529" citStr="Brown et al., 1993" startWordPosition="1884" endWordPosition="1887">between textual entailment inferences that are based on knowledge of language semantics (such as murdering =&gt; killing) and inferences based on domain or world knowledge (such as live in Paris =&gt; live in France). Both are needed in applications and it is not clear at this stage where and how to put such a borderline. D) An important feature of the proposed framework is that for a given text many hypotheses are likely to be true. Consequently, for a given text t and hypothesis h, Y—hP(Trh=1|t) does not sum to 1. This differs from typical generative settings for IR and MT (Ponte and croft, 1998; Brown et al., 1993), where all conditioned events are disjoint by construction. In the proposed model, it is rather the case that P(Trh=1|t) + P(Trh=0|t) = 1, as we are interested in the probability that a single particular hypothesis is true (or false). E) An implemented model that corresponds to our probabilistic setting is expected to produce an estimate for P(Trh = 1 |t). This estimate is expected to reflect all probabilistic aspects involved in the modeling, including inherent uncertainty of the entailment inference itself (as in example 2 of Table 1), possible uncertainty regarding the correct disambiguati</context>
<context position="14448" citStr="Brown et al., 1993" startWordPosition="2387" endWordPosition="2390">ate P(Tru=1|v1, ..., vn) for a given word u and text t={v1, ..., vn}, we further assume that the majority of the probability mass comes from a specific entailing word in t: P(Tru =1 |t) = max v,=-t P(Tru =1 |Tv) (2) where Tv denotes the event that a generated text contains the word v. This corresponds to expecting that each word in h will be entailed from a specific word in t (rather than from the accumulative context of t as a whole2). Alternatively, one can view (2) as inducing an alignment between terms in the h to the terms in the t, somewhat similar to alignment models in statistical MT (Brown et al., 1993). Thus we propose estimating the entailment probability based on lexical entailment probabilities from (1) and (2) as follows: P(Tr=1 |t) =��uhmat P(Tru =1 |Tv) (3) 3.1 Estimating Lexical Entailment Probabilities We perform unsupervised empirical estimation of the lexical entailment probabilities, P(Tru=1|Tv), based on word co-occurrence frequencies in a corpus. Following our proposed probabilistic model (cf. Section 2.2.1), we assume that the domain corpus is a sample generated by a language source. Each document represents a generated text and a (hidden) possible world. Given that the possib</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gennaro Chierchia</author>
<author>Sally McConnell-Ginet</author>
</authors>
<title>Meaning and grammar: An introduction to semantics, 2nd. edition.</title>
<date>2001</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>Chierchia, McConnell-Ginet, 2001</marker>
<rawString>Chierchia, Gennaro, and Sally McConnell-Ginet. 2001. Meaning and grammar: An introduction to semantics, 2nd. edition. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2005</date>
<booktitle>In Proceedings of the PASCAL Challenges Workshop for Recognizing Textual Entailment.</booktitle>
<location>Southampton, U.K.</location>
<contexts>
<context position="1086" citStr="Dagan et al., 2005" startWordPosition="141" endWordPosition="144">ng. The model was evaluated on two application independent datasets, suggesting the relevance of such probabilistic approaches for entailment modeling. 1 Introduction Many Natural Language Processing (NLP) applications need to recognize when the meaning of one text can be expressed by, or inferred from, another text. Information Retrieval (IR), Question Answering (QA), Information Extraction (IE), text summarization and Machine Translation (MT) evaluation are examples of applications that need to assess this semantic relationship between text segments. The Textual Entailment Recognition task (Dagan et al., 2005) has recently been proposed as an application independent framework for modeling such inferences. Within the textual entailment framework, a text t is said to entail a textual hypothesis h if the truth of h can be inferred from t. Textual entailment captures generically a broad range of inferences that are relevant for multiple applications. For example, a QA system has to identify texts that entail a hypothesized answer. Given the question &amp;quot;Does John Speak French?&amp;quot;, a text that includes the sentence &amp;quot;John is a fluent French speaker&amp;quot; entails the suggested answer &amp;quot;John speaks French.&amp;quot; In many c</context>
<context position="17205" citStr="Dagan et al., 2005" startWordPosition="2865" endWordPosition="2868">lent pansion (Nie and Brisebois, 1996; Yang and Chua, measure was independently proposed in the con- 2002). Using WordNet, we expanded the hypothetext of QA by (Saggion et al., 2004)3. This baseline ses’ terms with morphological alternations and measure captures word overlap, considering only semantically related words4. words that appear in both texts and weighs them For each hypothesis stop words were removed based on their inverse document frequency. and all content words were expanded as described 4 The RTE challenge dataset above. Boolean Search included a conjunction of The RTE dataset (Dagan et al., 2005) consists the disjunction of the term’s expansions and was of sentence pairs annotated for entailment. Fo this performed at the paragraph level over the full dataset we used word cooccurrence frequencies Reuters corpus, as common in IR for QA. Since we obtained from a web search engine. The details of wanted to focus our research on semantic variabilthis experiment are described in Glickman et al., ity we excluded from the result set paragraphs that 2005a. The resulting accuracy on the test set was contain all original words of the hypothesis or their 59% and the resulting confidence weighted </context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman and Bernardo Magnini. 2005. The PASCAL Recognising Textual Entailment Challenge. In Proceedings of the PASCAL Challenges Workshop for Recognizing Textual Entailment. Southampton, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
<author>Moshe Koppel</author>
</authors>
<title>Web Based Probabilistic Textual Entailment, PASCAL Challenges Workshop for Recognizing Textual Entailment.</title>
<date>2005</date>
<contexts>
<context position="3489" citStr="Glickman et al., 2005" startWordPosition="525" endWordPosition="528"> must be inferred from the text. However, for proper entailment it is further needed that the right relations hold between these concepts. In this paper we demonstrate the relevance of the general probabilistic setting for modeling lexical entailment, by devising a preliminary model that is based on document co-occurrence probabilities in a bag of words representation. Although our proposed lexical system is relatively simple, as it doesn’t rely on syntactic or other deeper analysis, it nevertheless was among the top ranking systems in the first Recognising Textual Entailment (RTE) Challenge (Glickman et al., 2005a). The model was evaluated also on an additional dataset, where it compares favorably with a state-of-the-art heuristic score. These results suggest that the proposed probabilistic framework is a promising basis for devising improved models that incorporate richer information. Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 43–48, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics example text hypothesis 1 John is a French Speaker John speaks French 2 John was born in France 3 Harry&apos;s birthplace is Iowa Harry was born in </context>
<context position="16535" citStr="Glickman et al., 2005" startWordPosition="2761" endWordPosition="2764">ts in the corpus). We denote as tep the textual entailment probability estimation as derived from (3) and (4) above: 3.2 Baseline model As a baseline model for comparison, we use a score developed within the context of text summarization. (Monz and de Rijke, 2001) propose modeling the directional entailment between two texts t1, t2 via the following score: I idf (w) w t t � � entscore t t ( , ) ( 1 2 ) � (6) 1 2 � idf (w) Hueh max ve tep (t, h) =t lep u v ( , ) (5) w t � 2 where idf(w) = log(N/nw), N is total number of documents in corpus and nw is number of docu2 Such a model is proposed in (Glickman et al., 2005b) 46 ments containing word w. A practically equivalent pansion (Nie and Brisebois, 1996; Yang and Chua, measure was independently proposed in the con- 2002). Using WordNet, we expanded the hypothetext of QA by (Saggion et al., 2004)3. This baseline ses’ terms with morphological alternations and measure captures word overlap, considering only semantically related words4. words that appear in both texts and weighs them For each hypothesis stop words were removed based on their inverse document frequency. and all content words were expanded as described 4 The RTE challenge dataset above. Boolean</context>
</contexts>
<marker>Glickman, Dagan, Koppel, 2005</marker>
<rawString>Oren Glickman, Ido Dagan and Moshe Koppel. 2005a. Web Based Probabilistic Textual Entailment, PASCAL Challenges Workshop for Recognizing Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Glickman</author>
<author>Ido Dagan</author>
<author>Moshe Koppel</author>
</authors>
<title>A Probabilistic Classification Approach for Lexical Textual Entailment,</title>
<date>2005</date>
<booktitle>Twentieth National Conference on Artificial Intelligence (AAAI-05).</booktitle>
<contexts>
<context position="3489" citStr="Glickman et al., 2005" startWordPosition="525" endWordPosition="528"> must be inferred from the text. However, for proper entailment it is further needed that the right relations hold between these concepts. In this paper we demonstrate the relevance of the general probabilistic setting for modeling lexical entailment, by devising a preliminary model that is based on document co-occurrence probabilities in a bag of words representation. Although our proposed lexical system is relatively simple, as it doesn’t rely on syntactic or other deeper analysis, it nevertheless was among the top ranking systems in the first Recognising Textual Entailment (RTE) Challenge (Glickman et al., 2005a). The model was evaluated also on an additional dataset, where it compares favorably with a state-of-the-art heuristic score. These results suggest that the proposed probabilistic framework is a promising basis for devising improved models that incorporate richer information. Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 43–48, Ann Arbor, June 2005. c�2005 Association for Computational Linguistics example text hypothesis 1 John is a French Speaker John speaks French 2 John was born in France 3 Harry&apos;s birthplace is Iowa Harry was born in </context>
<context position="16535" citStr="Glickman et al., 2005" startWordPosition="2761" endWordPosition="2764">ts in the corpus). We denote as tep the textual entailment probability estimation as derived from (3) and (4) above: 3.2 Baseline model As a baseline model for comparison, we use a score developed within the context of text summarization. (Monz and de Rijke, 2001) propose modeling the directional entailment between two texts t1, t2 via the following score: I idf (w) w t t � � entscore t t ( , ) ( 1 2 ) � (6) 1 2 � idf (w) Hueh max ve tep (t, h) =t lep u v ( , ) (5) w t � 2 where idf(w) = log(N/nw), N is total number of documents in corpus and nw is number of docu2 Such a model is proposed in (Glickman et al., 2005b) 46 ments containing word w. A practically equivalent pansion (Nie and Brisebois, 1996; Yang and Chua, measure was independently proposed in the con- 2002). Using WordNet, we expanded the hypothetext of QA by (Saggion et al., 2004)3. This baseline ses’ terms with morphological alternations and measure captures word overlap, considering only semantically related words4. words that appear in both texts and weighs them For each hypothesis stop words were removed based on their inverse document frequency. and all content words were expanded as described 4 The RTE challenge dataset above. Boolean</context>
</contexts>
<marker>Glickman, Dagan, Koppel, 2005</marker>
<rawString>Oren Glickman, Ido Dagan and Moshe Koppel. 2005b. A Probabilistic Classification Approach for Lexical Textual Entailment, Twentieth National Conference on Artificial Intelligence (AAAI-05).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Y Halpern</author>
</authors>
<title>An analysis of first-order logics of probability.</title>
<date>1990</date>
<journal>Artificial Intelligence</journal>
<pages>46--311</pages>
<contexts>
<context position="10467" citStr="Halpern, 1990" startWordPosition="1709" endWordPosition="1710">uth. For illustration, we would expect that the text “His father was born in Italy” would logically entail the hypothesis “He was born in Italy” with high probability – since most people who’s father was born in Italy were also born there. However we expect that the text would actually not probabilistically textually entail the hypothesis since most people for whom it is specifically reported that their father was born in Italy were not born in Italy.1 B) We assign probabilities to propositions (hypotheses) in a similar manner to certain probabilistic reasoning approaches (e.g. Bacchus, 1990; Halpern, 1990). However, we also assume a generative model of text, similar to probabilistic language and machine translation models, which supplies the needed conditional probability distribution. Furthermore, since our conditioning is on texts rather than propositions we do not assume any specific logic representation language for text meaning, and only assume that textual hypotheses can be assigned truth values. C) Our framework does not distinguish between textual entailment inferences that are based on knowledge of language semantics (such as murdering =&gt; killing) and inferences based on domain or worl</context>
</contexts>
<marker>Halpern, 1990</marker>
<rawString>Joseph Y. Halpern. 1990. An analysis of first-order logics of probability. Artificial Intelligence 46:311-350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christof Monz</author>
<author>Maarten de Rijke</author>
</authors>
<title>Light-Weight Entailment Checking for Computational Semantics.</title>
<date>2001</date>
<booktitle>In Proc. of the third workshop on inference in computational semantics (ICoS-3).</booktitle>
<marker>Monz, de Rijke, 2001</marker>
<rawString>Christof Monz, Maarten de Rijke. 2001. Light-Weight Entailment Checking for Computational Semantics. In Proc. of the third workshop on inference in computational semantics (ICoS-3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian-Yun Nie</author>
<author>Martin Brisebois</author>
</authors>
<title>An Inferential Approach to Information Retrieval and Its Implementation Using a Manual Thesaurus.</title>
<date>1996</date>
<journal>Artificial Intelligence Revue</journal>
<volume>10</volume>
<issue>5</issue>
<pages>409--439</pages>
<contexts>
<context position="16623" citStr="Nie and Brisebois, 1996" startWordPosition="2774" endWordPosition="2777">erived from (3) and (4) above: 3.2 Baseline model As a baseline model for comparison, we use a score developed within the context of text summarization. (Monz and de Rijke, 2001) propose modeling the directional entailment between two texts t1, t2 via the following score: I idf (w) w t t � � entscore t t ( , ) ( 1 2 ) � (6) 1 2 � idf (w) Hueh max ve tep (t, h) =t lep u v ( , ) (5) w t � 2 where idf(w) = log(N/nw), N is total number of documents in corpus and nw is number of docu2 Such a model is proposed in (Glickman et al., 2005b) 46 ments containing word w. A practically equivalent pansion (Nie and Brisebois, 1996; Yang and Chua, measure was independently proposed in the con- 2002). Using WordNet, we expanded the hypothetext of QA by (Saggion et al., 2004)3. This baseline ses’ terms with morphological alternations and measure captures word overlap, considering only semantically related words4. words that appear in both texts and weighs them For each hypothesis stop words were removed based on their inverse document frequency. and all content words were expanded as described 4 The RTE challenge dataset above. Boolean Search included a conjunction of The RTE dataset (Dagan et al., 2005) consists the disj</context>
</contexts>
<marker>Nie, Brisebois, 1996</marker>
<rawString>Jian-Yun Nie and Martin Brisebois. 1996. An Inferential Approach to Information Retrieval and Its Implementation Using a Manual Thesaurus. Artificial Intelligence Revue 10(5-6): 409-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Ponte</author>
<author>W Bruce Croft</author>
</authors>
<title>A Language Modeling Approach to Information Retrieval.</title>
<date>1998</date>
<booktitle>SIGIR conference on Research and Development in Information Retrieval.</booktitle>
<marker>Ponte, Croft, 1998</marker>
<rawString>Jay M. Ponte, W. Bruce Croft, 1998. A Language Modeling Approach to Information Retrieval. SIGIR conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony G Rose</author>
<author>Mary Stevenson</author>
<author>Miles Whitehead</author>
</authors>
<title>The Reuters Corpus volume 1 - from yesterday’s news to tomorrow’s language resources.</title>
<date>2002</date>
<booktitle>Third International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="20076" citStr="Rose et al., 2002" startWordPosition="3345" endWordPosition="3348">greement, we had 200 of the pairs annotated by both judges, yielding a moderate agreement (a Kappa of 0.6). 5 RCV1 dataset In addition to the RTE dataset we were interested in evaluating the model on a more representative set of texts and hypotheses that better corresponds to applicative settings. We focused on the information seeking setting, common in applications such as QA and IR, in which a hypothesis is given and it is necessary to identify texts that entail it. An annotator was asked to choose 60 hypotheses based on sentences from the first few documents in the Reuters Corpus Volume 1 (Rose et al., 2002). The annotator was instructed to choose sentential hypotheses such that their truth could easily be evaluated. We further required that the hypotheses convey a reasonable information need in such a way that they might correspond to potential questions, semantic queries or IE relations. Table 2 shows a few of the hypotheses. In order to create a set of candidate entailing texts for the given set of test hypotheses, we followed the common practice of WordNet based ex4 The following WordNet relations were used: Synonyms, see also, similar to, hypernyms/hyponyms, meronyms/holonyms, pertainyms, at</context>
</contexts>
<marker>Rose, Stevenson, Whitehead, 2002</marker>
<rawString>Tony G. Rose, Mary Stevenson, and Miles Whitehead. 2002. The Reuters Corpus volume 1 - from yesterday’s news to tomorrow’s language resources. Third International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Yang</author>
<author>Tat-Seng Chua</author>
</authors>
<title>The integration of lexical knowledge and external resources for question answering.</title>
<date>2002</date>
<booktitle>The eleventh Text REtrieval Conference (TREC-11).</booktitle>
<marker>Yang, Chua, 2002</marker>
<rawString>Hui Yang and Tat-Seng Chua. 2002. The integration of lexical knowledge and external resources for question answering. The eleventh Text REtrieval Conference (TREC-11).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>