<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.971309">
Automatic acquisition of word interaction patterns from corpora
</title>
<author confidence="0.990755">
Veska Noncheva
</author>
<affiliation confidence="0.988256666666667">
Faculty of Mathematics
and Computer Science
Plovdiv University
</affiliation>
<email confidence="0.993814">
wesnon@pu.acad.bg
</email>
<note confidence="0.5392265">
Joaquim Ferreira da Silva
Faculdade de Ciencias e
Tecnologia
Universidade Nova de Lisboa
</note>
<email confidence="0.964534">
jfs@di.fct.unl.pt
</email>
<author confidence="0.771294">
Gabriel Lopes
</author>
<affiliation confidence="0.602604">
Faculdade de Ciencias e
</affiliation>
<note confidence="0.417995">
Tecnologia
Universidade Nova de Lisboa
</note>
<email confidence="0.975065">
gpl@di.fct.unl.pt
</email>
<sectionHeader confidence="0.993119" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999111875">
A major challenge in computational lin-
guistics is to uncover word interactions in
linguistic expressions. In this paper a new
framework for discovering interaction be-
tween the words constituting multi-word
relevant expressions is proposed. This
framework is built on an algorithm for
relevant expression extraction called Lo-
calMaxs algorithm, partitioning round
medoids clustering method and Bayesian
networks. Bayesian networks are attrac-
tive for their ability to represent depend-
encies and to learn from observations.
This new technology facilitates text com-
prehension. It may also enable control of
highly ambiguous text input.
</bodyText>
<sectionHeader confidence="0.999086" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999835344827586">
Relevant expressions are sequences of text units
(words, characters, part-of-speech (POS) tags,
etc.) that can be extracted automatically from
plain text corpora. In this work we concentrate on
multi-word relevant expressions.
The relationships among words are the funda-
mental building blocks in the process of consti-
tuting multi-word linguistic expressions.
One of our previous aims was to cluster multi-
word relevant expressions in order to separate
those that denote concepts, or represent linguistic
important objects (locutions, for instance), from
those that clearly do not obey the above charac-
teristics, in order to filter out the last ones (Non-
cheva, et al., 2002).
In this work our aim is to discover patterns of
interaction between the words constituting multi
word relevant expressions and to represent them
by means of Bayesian networks.
By &amp;quot;interaction between the words constituting
multi-word relevant expressions&amp;quot; we mean any
relation holding between the lexical items, which
are simultaneously presented in a single struc-
ture. For example, an interaction between the
words constituting bigrams could be verb-
complement relation, or the relation holding be-
tween constituents of a compound or other rela-
tions.
From such a broad definition, it appears quite
clear that on principle an interaction covers a va-
riety of linguistic phenomena ranging from collo-
cations and compounds to selection restrictions.
Information typically associated with word inter-
action is usually expressed in terms of semantic
categories, possibly specified at different degrees
of granularity (in terms of base types or of syno-
nym clusters), and extended through use of func-
tional restrictions. Another representational
option is listing typical lexical fillers of a given
word position. This kind of option could be use-
ful when an appropriate semantic category cannot
be found. Sometimes the selection involves spe-
cific lexical items rather than a general semantic
class. Therefore, on the basis of the application
requirements and the available linguistic
information, we can decide whether to encode
word restrictions through use of semantic
categories, or to encode them by listing the
typical lexical fillers of a given word position
(this option is particularly useful when the
appropriate semantic category is missing), or to
combine the different encoding.
We will give a standard representation of the
interaction between the words constituting multi-
word relevant expressions through Bayesian net-
works.
The remainder of this paper is organized as
follows. In Section 2 we make reference to the
</bodyText>
<page confidence="0.996505">
25
</page>
<bodyText confidence="0.999928846153846">
algorithm for extracting relevant expressions. In
Section 3 we discuss the cluster approach used
for extracting classes of relevant expressions.
Every class contains relevant expressions that are
similar in the sense that they have both the same
patterns of dependence structure and similar fea-
tures. In Section 4 we show how Bayesian net-
works are used for presenting relevant expression
patterns. Bayesian models describing bigrams are
presented. The experimental results are obtained
from the corpus &amp;quot;European Legislation in force
on social policy, environment, customs and ra-
tional use of energy&amp;quot;.
</bodyText>
<sectionHeader confidence="0.9604575" genericHeader="method">
2 Extraction of Relevant Expres-
sions
</sectionHeader>
<bodyText confidence="0.9938165">
The approach LocalMaxs for finding multi-word
relevant expressions from unarmotated text cor-
pora is presented in (Silva et al., 1999; Silva and
Lopes, 1999). This approach is based on the idea
that each n-gram has a kind of cohesion force
sticking the word units together within the n-
gram. In order to measure the cohesion value of
each n-gram of any size in the corpus, a new co-
hesion measure using the probabilities of the n-
grams (n1) in the corpus is proposed. As a result
a data set containing thousands of relevant ex-
pressions is available. Most of them express con-
cepts or linguistic objects that are semantically
rich or technically feasible. Also, not every rele-
vant expression denotes an interesting concept.
Some examples of selected n-grams, are: Human
Rights, Human Rights in East Timor, common
agricultural policy, economia energetica (in Por-
tuguese) and publication au Jounal officiell des
Communautes (in French).
</bodyText>
<sectionHeader confidence="0.941529" genericHeader="method">
3 Classes of Relevant Expressions
</sectionHeader>
<bodyText confidence="0.9994595">
In order to find classes of Relevant Expressions,
some features must be considered.
</bodyText>
<subsectionHeader confidence="0.99805">
3.1 Importance measures
</subsectionHeader>
<bodyText confidence="0.994040294117647">
We would like to distinguish different written
forms of words and relevant expressions. For
example: party, Party, and PARTY.
We define the notion importance of a relevant
expression extracted from a corpus in the follow-
ing way:
Let n be the number of words wi, i=1,2,
in one relevant expression (n-gram) and Li be the
number of the different written forms of the word
. With w: we denote the form of the word wi
in the n-gram under consideration. Let
,1 = 1,2,..., L,} be the set of all possible
forms of the word w1. Let f (w,1 ) be the fre-
quency of word form w.
We will define importance
*
imp RE (V] 2,—, n) of a relevant expression
</bodyText>
<equation confidence="0.965783461538462">
.
, w2 m the following way:
1 f (wt)
imp RE (1&apos;111 }V2 )•••, wn ) = L,
n &gt; 1, L, = 1,2,..., n .
When n =1 the importance of a word is
imp ( = f (w ) .
f (w1)
1=1
When n = 2 the importance of a bigram is
1 f (w;` ) f (w*2)
imP RE (14&apos;; ,w2,— 2 ( LI + L2
f() f(w)
</equation>
<bodyText confidence="0.885882444444445">
For example, the importance of the bigrams
Mac Sharry, heavy-metal concentrations, and
Fundamental Freedoms in the corpus are:
impRE(heavy-metal,concentration)=.99,
impRE(Mac,Sharry)= 1,
impRE(Fundamental,Freedoms) =.44.
Note that using the importance measure we
will classify the bigrams Mac Sharry and Fun-
damental Freedoms in two clusters.
</bodyText>
<subsectionHeader confidence="0.999597">
3.2 Probabilistic measures
</subsectionHeader>
<bodyText confidence="0.98526675">
Probabilistic information about words, phrases,
and other linguistic structures is represented in
the minds of language users and plays a role in
language comprehension. The probability of a
word&apos;s occurrence is conditioned on many as-
pects of its contexts, including neighbouring
words, syntactic and lexical structure, semantic
expectations, and discourse factors (Jurafsky, et
al., 2001).
We will focus on the role of local probabilis-
tic relations between words in the process of ac-
, where
</bodyText>
<page confidence="0.9728">
26
</page>
<bodyText confidence="0.999717">
quisition of classes of relevant expressions. Con-
sider the following measures of probabilistic rela-
tions between words:
</bodyText>
<listItem confidence="0.996186">
• Prior probability
</listItem>
<bodyText confidence="0.953166333333333">
The prior probability is the probability of a
word&apos;s occurrence independent of context. The
prior probability is usually estimated by using the
word frequency J(w1) in a sufficiently large cor-
pus in the following way: p(w) — where
N is the total number of word tokens in the cor-
pus. The relative frequencies p(w,) are estimates
of the prior probabilities p(w,). The most frequent
first words of the extracted bigrams wovi hi are the
function words the, of, to, in, a, for, be, shall, by,
and on. Their frequencies J(w1) in the corpus
available are as follows: Rthe&apos;)= 142222,
Ron= 100294, j(`-to&apos;)= 49984, Rin&apos;)= 41293,
a&apos;)= 22821, for&apos;)= 21146, Rbe&apos;)= 20414,
J(` shall&apos; )= 17234, by&apos; )= 15519, j(` on&apos; )= 13682.
The set of the most frequent second words con-
sists of the same function words and conjunctions
and and or with frequencies and&apos;)= 47078,
Ror&apos;)= 15666.
The content words exhibit weaker effects of
surrounding context, but strong effects of relative
frequency (Jurafsky, et al., 2001). The frequen-
cies of the most frequent seven content words in
the corpus are as follows RArticle&apos;) = 13413,
J(`Member&apos;) = 11006, f(`Comrnission&apos;) = 7457,
f(`Directive&apos;) = 7452, States&apos;) = 7081,
J(`Council&apos;) = 6130, J(`State&apos;) = 5431.
</bodyText>
<listItem confidence="0.967261">
• Joint probability
</listItem>
<bodyText confidence="0.858186470588235">
The joint probability p(we_iwi) of the bigram
wz_iw, may be thought of as the prior probability
of these two words taken together in the same
order, and could be estimated by relative fre-
quency of the bigram: (w ) f ,
where Nis the total number of bigrams.
The extracted bigrams with the highest joint
probabilities are of the, to the, in the. They are
clustered in the following two clusters {of the}
and {to the, in the} . Other bigrams with high
joint probabilities are at least, in particular,
Member States, Member State, the Commission,
Council Directive, and European Communities.
Extracted bigrams with low joint probabilities
are either voted, the Bank&apos;s, the Group&apos;s, not
weaken, their non-degradability, Commission
undertook, its middle, other media.
</bodyText>
<listItem confidence="0.985453">
• Conditional probability given previous
wordp(w11w11)
</listItem>
<bodyText confidence="0.971651095238095">
It is estimated from p(w, w. = f
f
The conditional probability p(w ilw 1) would be
high if the second word was particularly likely to
follow the first.
We have received high conditional probabili-
ties of target words given previous word
p(w,lw,_i) of the bigrams Eastern Europe, Canary
Islands, aquatic environment, lifelong learning,
and low-skilled workers, and low ones of job-
seekers and, by the, with the, failing this, and pa-
ternity or.
According to the results presented in (Jurafsky,
et al., 2001) the function words of and to are most
likely to collocate with the previous word. For
example, kind of, able to.
Our experimental results show high condi-
tional probabilities of target function words given
previous words p(w11w14) of the bigrams de-
mountable as, insofar as, chaired by, emanating
from, and annexed to.
</bodyText>
<listItem confidence="0.9322355">
• Conditional probability given next word
P(wi
</listItem>
<bodyText confidence="0.9635001875">
It is estimated from p(w, f
f (w
We have received high conditional probability
of target given next word p(wilwi+i) of the bi-
grams title Mester&apos;, ISO 9001, global warming,
be reconciled, a bunk, and low ones of that coun-
try&apos;s, this island, the World, all inventories, and
this resolution. We expect low conditional prob-
abilities of target function words given next
words p(wilw „El). For example &amp;quot;the old&amp;quot;, &amp;quot;you
must&amp;quot;, &amp;quot;and filter&amp;quot;. If these probabilities are high
probably these bigrams are parts of n-grams, n&gt;2.
According to the results presented in (Jurafsky,
et al., 2001) the function words I, a, the, and in
tend to collocate with the followings words. For
example, a lot, the same, in terms, I mean.
</bodyText>
<listItem confidence="0.8889715">
• Conditional probability given surround-
ing words p(w,lw1,w,±1)
</listItem>
<page confidence="0.997211">
27
</page>
<bodyText confidence="0.998327333333333">
The words w, and wi_, are content words that
appear frequently and are likely to collocate with
each other.
</bodyText>
<note confidence="0.5130282">
Examples: Ebro Valley, Pergamon Press, NON-
CALCAREOUS SAND, DIRECTIVITY INDEX, On-
board Observer, Merchant Shipping, Time-frame Ac-
tors, EAGGF Guidance, TOWER CRANES, White
Paper, Structural Funds, Atomic Energy.
</note>
<listItem confidence="0.997238">
• A class with noun phrases having property:
</listItem>
<bodyText confidence="0.9946535">
The first word is not predictable from the sec-
ond and vice versa.
</bodyText>
<construct confidence="0.9064048">
Examples: economic viability, economic reforms,
economic feasibility, economic optimum, economic
restructuring, appropriate modifications, appropriate
fashion, social acceptability, social forces, social
spheres, new constructions, new apparatus.
</construct>
<bodyText confidence="0.9999352">
We have shown that probabilistic measures in-
fluence both extraction and clustering of relevant
expressions. Now we will apply a probabilistic
model called Bayesian Network for presenting
and analyzing classes with relevant expressions.
</bodyText>
<sectionHeader confidence="0.9260745" genericHeader="method">
4 Relevant Expression Pattern
Structure
</sectionHeader>
<bodyText confidence="0.999965857142857">
Groups of relevant expressions that present
equivalent patterns have been extracted. A more
ambitious goal for analysis is revealing the pat-
tern structure.
This clearly is a hard problem. Mainly since
relevant expression data alone give only a partial
structure that does not always reflect key linguis-
tic events. In addition, there is a noise in data.
In this work, we introduce a new approach for
analyzing relevant expression patterns by exam-
ining statistical properties of conditional (in)de-
pendence in the data.
The Bayesian network model is a promising
tool for analyzing relevant expression patterns.
First, they are particularly useful for describing
processes composed of locally interacting com-
ponents. Second, algorithms for inference- and
learning- Bayesian networks are developed and
have been used successfully in many applica-
tions. Finally, Bayesian networks provide models
of causal influence (Pearl, 2000).
</bodyText>
<subsectionHeader confidence="0.994741">
4.1 Bayesian Networks Language
</subsectionHeader>
<bodyText confidence="0.999761863636364">
Let xi be a set of words. By X,• we denote the
word i of an n-gram, i=1,2,...,n. We say that X,• is
a random variable with the set of its possible val-
ues xi. We can represent the dependencies be-
tween words X/X2...X„ using a graph, in which
each variable is denoted by a node. When two
variables are dependent we draw an edge be-
tween them.
Consider the variables XI, X2 and X3. If X, does
not directly affect X3, then we should say that the
effect of word Xi on word X3 is mediated through
word X2. Once we know the word X2, the word X,
does not give new information about word X3
We formalize this in the following way:
P(X/IX2,X3)=P(X/IX2) or P(X31Xi,X2)=P(X3IX2). In
this case we say that Xi and X3 are conditionally
independent, given X2. We expect that once X2 is
fixed we will observe that X1 and X3 are inde-
pendent. In the graph representation at Figure 1
there is not an edge between X1 and X3, and the
relation between them is represented as a direct
path through X2.
</bodyText>
<figureCaption confidence="0.992213">
Figure /. A simple Bayesian network structure
</figureCaption>
<bodyText confidence="0.6408767">
Consider Figure 2. As before, the three pairs of
words X1X2, X2X3 and X/X3 are correlated. Words
X2 and X4 are independent once we know the
word Xi. Thus, word Xi explains the relation be-
tween X2 and X4. In such a situation, we say that
word Xi is a common cause of words X2 and X4.
We model graphically this relation as shown in
Figure 2. If the word Xi is not known, then X2
and X4 would appear dependent in data and we
would have drawn an edge between them. In
</bodyText>
<equation confidence="0.296245">
X2,X3, X4, X5)=P(xI)P(X2 X5)P(X31X2)13(X4IXI)P(X5)
</equation>
<figureCaption confidence="0.99678">
Figure 2. An example of a Bayesian network
</figureCaption>
<page confidence="0.994428">
30
</page>
<bodyText confidence="0.93853148">
such a case we call Xir a hidden common cause.
In addition to a graph that describes dependen-
cies between variables, we associate with each
variable X a conditional probability model that
specifies the probability of X,• given its parents
rt,(X). We denote this probability as p(x,170,
where x, is a value of X1, ni are the values of its
parents.
A Bayesian network is a pair (D,P), where D is
a directed acyclic graph, P={p(x/Ini), p(x217c2),• •
p(x„lic„)} is a set of n conditional probability dis-
tributions, one for each variable, and 7r, are the
values of parents of node X, in D. Then the set P
defines the associated joint probability distribu-
tion as p(x1,x2,...,.;) = ilp(x, I Ki) (see
i=1
Figure 2).
Using stochastic models is natural in word in-
teraction because of two main reasons: the lin-
guistic processes are stochastic and the data are
noisy.
In the model described above, the sets of vari-
ables&apos; values were sets of words. Let us note that
the set of the possible values of a variable could
also be a set of tags.
</bodyText>
<subsectionHeader confidence="0.9685325">
4.2 Bayesian networks describing bi-
grams
</subsectionHeader>
<bodyText confidence="0.978743714285714">
Bayesian networks describing the bigrams are
discussed below. Data from the corpus have been
used to learn the probabilities assigned to pat-
terns.
proper names, terminologies, idiomatical verbs.
Examples: Ferro Rodrigues, Solbes Mira, Bern-
hard, FRIEDMANN, PINTO PIZARRO, Arctic
Oceans, Bahamas Bermuda, Grand Duchy, Great
Britain, United Kingdom, cathode ray, mega-
electron volt, prima facie, tetra acetate, photo-
chemical oxidants, suture stapler, bituminous
shale, speech therapist, explosive atmospheres,
vinyl chloride, arbitral tribunal, laid down
Pattern 2 of dependency:
</bodyText>
<equation confidence="0.539027857142857">
Probabilistic model:
P(xl,x2)= P(x1)13(x2lx
x - content words: proper names, lists of nouns,
lists of verbs, lists of adjectives, list of specific
words.
2
X - lists of nouns, verb forms and prepositions.
</equation>
<bodyText confidence="0.704537727272727">
Types of linguistic expressions:
Basic grammatical forms (it is, must be, they are)
and subcategorisation patterns (arrange for, pub-
lished in), and terminologies.
Examples: Coliform bacteria, Milk-based bever-
ages, Q-L relationships, Arable crops, Labrador
coast, Corinth Canal, Tarapoto Process, UKAEA
Windscale, READY-TO-USE PAINTS, Devon
Island, Nicosia Charter, Ross Sea, whale prod-
ucts, vertebrate animals, verifiable criteria.
Pattern 1 of dependency: Pattern 3 of dependency:
</bodyText>
<subsubsectionHeader confidence="0.28115">
Probabilistic model:
</subsubsectionHeader>
<bodyText confidence="0.826655470588235">
The directed graph is a directed cycle
X X2,X2 , representing mutual causation.
xi- content words: proper names, lists of nouns,
lists of verbs, lists of adjectives, lists of specific
words.
X2- content words: surnames, lists of nouns, lists
of adverbs, lists of specific words.
Types of linguistic expressions:
Probabilistic model:
p(xi,x2)= p(x2)p(xilx2)
x - lists of adjectives, verb forms and preposi-
tions, and lists of specific words.
X2 -lists of nouns, verbs and lists of specific
words.
Types of linguistic expressions:
Basic grammatical patterns (shall send, be elabo-
rated), locutions (in particular, for example),
</bodyText>
<page confidence="0.999612">
31
</page>
<bodyText confidence="0.969759714285714">
proper names and terminologies.
Examples: World War, Ozone Layer, Wild
Fauna, guide dogs, sensitive detector, telecom-
munications terminal, sugar beet, German De-
mocratic, ISIC Nomenclature, bare ropes, traffic
arteries, social affairs, commercial whaling.
Pattern 4 of dependency: 0
</bodyText>
<equation confidence="0.9138405">
Probabilistic model:
13(x 1,x2) = p(xi)p(x2)
</equation>
<bodyText confidence="0.951786884615385">
- lists of adjectives, verb forms and preposi-
tions, and lists of specific words.
2
X - lists of nouns, adjectives, verb forms and lists
of specific words.
Types of linguistic expressions:
Noun and adjective phrases, and the most fre-
quent bigrams: locutions (as regards, at least),
basic grammatical forms of verbs (has been, are
not), noun phrases introducing the main text top-
ics (European Communities).
Examples: as regards, at least, has been, are not,
European Communities, appropriate modifica-
tions, appropriate fashion, social acceptability,
social forces, social spheres, new constructions
,new apparatus, national focal.
The bigram class extraction result is surpris-
ingly accurate. We are currently extending this
approach in studying n-grams (n&gt;2). When n&gt;2
we expect each variable to depend on just a small
subset of other variables. It will permit us to de-
compose the joint probability distribution func-
tion of an n-gram into several distributions
involving a small subset of variables and then to
piece them together coherently to answer ques-
tion of global nature.
</bodyText>
<sectionHeader confidence="0.999398" genericHeader="conclusions">
5 Related Works and Conclusions
</sectionHeader>
<bodyText confidence="0.9999654">
This work is an alternative approach to the lan-
guage-dependent extractors based on morpho-
syntactic filters such as Xtract (Smadja, 1990),
ACABIT (Daille, 1994), etc. We have applied a
probabilistic model called Bayesian Network for
presenting and analyzing classes with relevant
expressions. As a result certain patterns of de-
pendency have been extracted. They reveal frag-
ments of the underlying linguistic structures.
Data from corpora are used to estimate both the
dependency structures and the probabilities as-
signed to those structures.
This language modelling approach could be
built into text entry methods in order to support
the user in text production.
</bodyText>
<sectionHeader confidence="0.999188" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999108710526316">
Jurafsky Daniel, Alan Bell, Michelle Gregory, and
William D. Raymond. 2001. Probabilistic relations
between words: Evidence from reduction in lexical
production. In Bybee, Joan and Paul Hopper (eds.).
Frequency and the emergence of linguistic struc-
ture. Amsterdam: John Benjamins, 229-254.
Kaufman L. and P.J. Rousseeuw. 1990. Finding
Groups in Data: An Introduction in Cluster Analy-
sis, Wiley, New York.
Noncheva V., J.F Silva.. and G.P Lopes. Clustering
Automatically Extracted Relevant Expressions,
Technical report: CENTRIA, DI- Faculdade de
Ciencias e Tecnologia, Universidade Nova de
Lisboa, Portugal.
Pearl J. 2000. Causality. Cambridge University Press,
Cambridge, UK.
Silva, J.F., Gael Dias, Sylvie Guillore, and José
Gabriel P. Lopes. 1999. Using LocalMaxs Algo-
rithm for the Extraction of Contiguous and Non-
contiguous Multiword Lexical Units. In: P. Bara-
hona (ed.) Progress in Artificial Intelligence: 9th
Portuguese Conference on Al, EPL4&apos;93, Evora Por-
tugal, September 1999, Proceedings. Lecture Notes
in Artificial Intelligence, Springer-Verlag, Vol.
1695, p. 113-132 (1999).
Silva. J.F. and J.G.P.Lopes. 1999. A Local Maxima
method and a Fair Dispersion Normalization for ex-
tracting multi-word units from corpora. In Proceed-
ings of the Sixth Meeting on Mathematics of
Language (MOL6), Orlando, Florida July 23-25,
pp. 369-381.
Smadja F. and K. McKeown. 1990. Automatically
Extracting and Representing Collocations for Lan-
guage Generation, Proceedings of the 28th annual
meeting of the ACL, Pittsburgh, PA.
Daile B. 1994. Approche mixte pour l&apos;extraction de
terminologie: statistique lexicale et filtres linguis-
tiques. PhD dissertation, Universite Paris.
</reference>
<page confidence="0.999295">
32
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.358658">
<title confidence="0.993483">Automatic acquisition of word interaction patterns from corpora</title>
<author confidence="0.667117">Veska</author>
<affiliation confidence="0.908192">Faculty of and Computer Science Plovdiv University</affiliation>
<email confidence="0.926317">wesnon@pu.acad.bg</email>
<author confidence="0.818231">Joaquim Ferreira da</author>
<affiliation confidence="0.962021">Faculdade de Ciencias Universidade Nova de Lisboa</affiliation>
<email confidence="0.977746">jfs@di.fct.unl.pt</email>
<author confidence="0.948547">Gabriel</author>
<affiliation confidence="0.9612265">Faculdade de Ciencias Universidade Nova de Lisboa</affiliation>
<email confidence="0.991368">gpl@di.fct.unl.pt</email>
<abstract confidence="0.988398647058824">A major challenge in computational linguistics is to uncover word interactions in linguistic expressions. In this paper a new framework for discovering interaction between the words constituting multi-word relevant expressions is proposed. This framework is built on an algorithm for relevant expression extraction called LocalMaxs algorithm, partitioning round medoids clustering method and Bayesian networks. Bayesian networks are attractive for their ability to represent dependencies and to learn from observations. This new technology facilitates text comprehension. It may also enable control of highly ambiguous text input.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jurafsky Daniel</author>
<author>Alan Bell</author>
<author>Michelle Gregory</author>
<author>William D Raymond</author>
</authors>
<title>Probabilistic relations between words: Evidence from reduction in lexical production.</title>
<date>2001</date>
<pages>229--254</pages>
<editor>In Bybee, Joan and Paul Hopper (eds.).</editor>
<publisher>John Benjamins,</publisher>
<location>Amsterdam:</location>
<marker>Daniel, Bell, Gregory, Raymond, 2001</marker>
<rawString>Jurafsky Daniel, Alan Bell, Michelle Gregory, and William D. Raymond. 2001. Probabilistic relations between words: Evidence from reduction in lexical production. In Bybee, Joan and Paul Hopper (eds.). Frequency and the emergence of linguistic structure. Amsterdam: John Benjamins, 229-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kaufman</author>
<author>P J Rousseeuw</author>
</authors>
<title>Finding Groups in Data: An Introduction in Cluster Analysis,</title>
<date>1990</date>
<publisher>Wiley,</publisher>
<location>New York.</location>
<marker>Kaufman, Rousseeuw, 1990</marker>
<rawString>Kaufman L. and P.J. Rousseeuw. 1990. Finding Groups in Data: An Introduction in Cluster Analysis, Wiley, New York.</rawString>
</citation>
<citation valid="false">
<authors>
<author>V Noncheva</author>
<author>J F Silva</author>
<author>G P Lopes</author>
</authors>
<title>Clustering Automatically Extracted Relevant Expressions,</title>
<booktitle>Technical report: CENTRIA, DI- Faculdade de Ciencias e Tecnologia, Universidade Nova de</booktitle>
<location>Lisboa, Portugal.</location>
<marker>Noncheva, Silva, Lopes, </marker>
<rawString>Noncheva V., J.F Silva.. and G.P Lopes. Clustering Automatically Extracted Relevant Expressions, Technical report: CENTRIA, DI- Faculdade de Ciencias e Tecnologia, Universidade Nova de Lisboa, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pearl</author>
</authors>
<title>Causality.</title>
<date>2000</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="12806" citStr="Pearl, 2000" startWordPosition="2019" endWordPosition="2020">ts. In addition, there is a noise in data. In this work, we introduce a new approach for analyzing relevant expression patterns by examining statistical properties of conditional (in)dependence in the data. The Bayesian network model is a promising tool for analyzing relevant expression patterns. First, they are particularly useful for describing processes composed of locally interacting components. Second, algorithms for inference- and learning- Bayesian networks are developed and have been used successfully in many applications. Finally, Bayesian networks provide models of causal influence (Pearl, 2000). 4.1 Bayesian Networks Language Let xi be a set of words. By X,• we denote the word i of an n-gram, i=1,2,...,n. We say that X,• is a random variable with the set of its possible values xi. We can represent the dependencies between words X/X2...X„ using a graph, in which each variable is denoted by a node. When two variables are dependent we draw an edge between them. Consider the variables XI, X2 and X3. If X, does not directly affect X3, then we should say that the effect of word Xi on word X3 is mediated through word X2. Once we know the word X2, the word X, does not give new information a</context>
</contexts>
<marker>Pearl, 2000</marker>
<rawString>Pearl J. 2000. Causality. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J F Silva</author>
<author>Gael Dias</author>
<author>Sylvie Guillore</author>
<author>José Gabriel P Lopes</author>
</authors>
<title>Using LocalMaxs Algorithm for the Extraction of Contiguous and Noncontiguous Multiword Lexical Units. In:</title>
<date>1999</date>
<booktitle>Progress in Artificial Intelligence: 9th Portuguese Conference on Al, EPL4&apos;93, Evora Portugal, September 1999, Proceedings. Lecture Notes in Artificial Intelligence,</booktitle>
<volume>1695</volume>
<pages>113--132</pages>
<editor>P. Barahona (ed.)</editor>
<publisher>Springer-Verlag,</publisher>
<contexts>
<context position="4382" citStr="Silva et al., 1999" startWordPosition="647" endWordPosition="650">lass contains relevant expressions that are similar in the sense that they have both the same patterns of dependence structure and similar features. In Section 4 we show how Bayesian networks are used for presenting relevant expression patterns. Bayesian models describing bigrams are presented. The experimental results are obtained from the corpus &amp;quot;European Legislation in force on social policy, environment, customs and rational use of energy&amp;quot;. 2 Extraction of Relevant Expressions The approach LocalMaxs for finding multi-word relevant expressions from unarmotated text corpora is presented in (Silva et al., 1999; Silva and Lopes, 1999). This approach is based on the idea that each n-gram has a kind of cohesion force sticking the word units together within the ngram. In order to measure the cohesion value of each n-gram of any size in the corpus, a new cohesion measure using the probabilities of the ngrams (n1) in the corpus is proposed. As a result a data set containing thousands of relevant expressions is available. Most of them express concepts or linguistic objects that are semantically rich or technically feasible. Also, not every relevant expression denotes an interesting concept. Some examples </context>
</contexts>
<marker>Silva, Dias, Guillore, Lopes, 1999</marker>
<rawString>Silva, J.F., Gael Dias, Sylvie Guillore, and José Gabriel P. Lopes. 1999. Using LocalMaxs Algorithm for the Extraction of Contiguous and Noncontiguous Multiword Lexical Units. In: P. Barahona (ed.) Progress in Artificial Intelligence: 9th Portuguese Conference on Al, EPL4&apos;93, Evora Portugal, September 1999, Proceedings. Lecture Notes in Artificial Intelligence, Springer-Verlag, Vol. 1695, p. 113-132 (1999).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J F</author>
<author>J G P Lopes</author>
</authors>
<title>A Local Maxima method and a Fair Dispersion Normalization for extracting multi-word units from corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the Sixth Meeting on Mathematics of Language (MOL6),</booktitle>
<pages>369--381</pages>
<location>Orlando, Florida</location>
<marker>F, Lopes, 1999</marker>
<rawString>Silva. J.F. and J.G.P.Lopes. 1999. A Local Maxima method and a Fair Dispersion Normalization for extracting multi-word units from corpora. In Proceedings of the Sixth Meeting on Mathematics of Language (MOL6), Orlando, Florida July 23-25, pp. 369-381.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Smadja</author>
<author>K McKeown</author>
</authors>
<title>Automatically Extracting and Representing Collocations for Language Generation,</title>
<date>1990</date>
<booktitle>Proceedings of the 28th annual meeting of the ACL,</booktitle>
<location>Pittsburgh, PA.</location>
<marker>Smadja, McKeown, 1990</marker>
<rawString>Smadja F. and K. McKeown. 1990. Automatically Extracting and Representing Collocations for Language Generation, Proceedings of the 28th annual meeting of the ACL, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Daile</author>
</authors>
<title>Approche mixte pour l&apos;extraction de terminologie: statistique lexicale et filtres linguistiques.</title>
<date>1994</date>
<institution>Universite Paris.</institution>
<note>PhD dissertation,</note>
<marker>Daile, 1994</marker>
<rawString>Daile B. 1994. Approche mixte pour l&apos;extraction de terminologie: statistique lexicale et filtres linguistiques. PhD dissertation, Universite Paris.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>