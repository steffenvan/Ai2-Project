<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001223">
<title confidence="0.934233">
Language Models for Image Captioning: The Quirks and What Works
</title>
<author confidence="0.832478666666667">
Jacob Devlin*, Hao Cheng*, Hao Fang*, Saurabh Gupta♣,
Li Deng, Xiaodong He*, Geoffrey Zweig*, Margaret Mitchell*
Microsoft Research
</author>
<affiliation confidence="0.846689666666667">
y� Corresponding authors: {jdevlin,xiaohe,gzweig,memitc}@microsoft.com
♠ University of Washington
♣ University of California at Berkeley
</affiliation>
<sectionHeader confidence="0.98933" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99985436">
Two recent approaches have achieved
state-of-the-art results in image caption-
ing. The first uses a pipelined process
where a set of candidate words is gen-
erated by a convolutional neural network
(CNN) trained on images, and then a max-
imum entropy (ME) language model is
used to arrange these words into a coherent
sentence. The second uses the penultimate
activation layer of the CNN as input to a
recurrent neural network (RNN) that then
generates the caption sequence. In this pa-
per, we compare the merits of these dif-
ferent language modeling approaches for
the first time by using the same state-of-
the-art CNN as input. We examine is-
sues in the different approaches, includ-
ing linguistic irregularities, caption repe-
tition, and data set overlap. By combining
key aspects of the ME and RNN methods,
we achieve a new record performance over
previously published results on the bench-
mark COCO dataset. However, the gains
we see in BLEU do not translate to human
judgments.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999820341463415">
Recent progress in automatic image captioning
has shown that an image-conditioned language
model can be very effective at generating captions.
Two leading approaches have been explored for
this task. The first decomposes the problem into
an initial step that uses a convolutional neural net-
work to predict a bag of words that are likely to
be present in a caption; then in a second step, a
maximum entropy language model (ME LM) is
used to generate a sentence that covers a mini-
mum number of the detected words (Fang et al.,
2015). The second approach uses the activations
from final hidden layer of an object detection CNN
as the input to a recurrent neural network lan-
guage model (RNN LM). This is referred to as a
Multimodal Recurrent Neural Network (MRNN)
(Karpathy and Fei-Fei, 2015; Mao et al., 2015;
Chen and Zitnick, 2015). Similar in spirit is the
the log-bilinear (LBL) LM of Kiros et al. (2014).
In this paper, we study the relative merits of
these approaches. By using an identical state-of-
the-art CNN as the input to RNN-based and ME-
based models, we are able to empirically com-
pare the strengths and weaknesses of the lan-
guage modeling components. We find that the
approach of directly generating the text with an
MRNN1 outperforms the ME LM when measured
by BLEU on the COCO dataset (Lin et al., 2014),2
but this recurrent model tends to reproduce cap-
tions in the training set. In fact, a simple k-nearest
neighbor approach, which is common in earlier re-
lated work (Farhadi et al., 2010; Mason and Char-
niak, 2014), performs similarly to the MRNN. In
contrast, the ME LM generates the most novel
captions, and does the best at captioning images
for which there is no close match in the training
data. With a Deep Multimodal Similarity Model
(DMSM) incorporated,3 the ME LM significantly
outperforms other methods according to human
judgments. In sum, the contributions of this pa-
per are as follows:
</bodyText>
<listItem confidence="0.983903125">
1. We compare the use of discrete detections
and continuous valued CNN activations as
the conditioning information for language
models trained to generate image captions.
2. We show that a simple k-nearest neighbor re-
trieval method performs at near state-of-the-
art for this task and dataset.
3. We demonstrate that a state-of-the-art
</listItem>
<footnote confidence="0.875571142857143">
1In our case, a gated recurrent neural network (GRNN) is
used (Cho et al., 2014), similar to an LSTM.
2This is the largest image captioning dataset to date.
3As described by Fang et al. (2015).
100
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 100–105,
</footnote>
<note confidence="0.270744">
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.6865946">
MRNN-based approach tends to reconstruct
previously seen captions; in contrast, the
two stage ME LM approach achieves similar
or better performance while generating
relatively novel captions.
</bodyText>
<listItem confidence="0.995907222222222">
4. We advance the state-of-the-art BLEU scores
on the COCO dataset.
5. We present human evaluation results on the
systems with the best performance as mea-
sured by automatic metrics.
6. We explore several issues with the statistical
models and the underlying COCO dataset, in-
cluding linguistic irregularities, caption repe-
tition, and data set overlap.
</listItem>
<sectionHeader confidence="0.983428" genericHeader="introduction">
2 Models
</sectionHeader>
<bodyText confidence="0.999214875">
All language models compared here are trained
using output from the same state-of-the-art CNN.
The CNN used is the 16-layer variant of VGGNet
(Simonyan and Zisserman, 2014) which was ini-
tially trained for the ILSVRC2014 classification
task (Russakovsky et al., 2015), and then fine-
tuned on the Microsoft COCO data set (Fang et
al., 2015; Lin et al., 2014).
</bodyText>
<subsectionHeader confidence="0.959347">
2.1 Detector Conditioned Models
</subsectionHeader>
<bodyText confidence="0.999975608695652">
We study the effect of leveraging an explicit de-
tection step to find key objects/attributes in images
before generation, examining both an ME LM ap-
proach as reported in previous work (Fang et al.,
2015), and a novel LSTM approach introduced
here. Both use a CNN trained to output a bag of
words indicating the words that are likely to ap-
pear in a caption, and both use a beam search to
find a top-scoring sentence that contains a subset
of the words. This set of words is dynamically ad-
justed to remove words as they are mentioned.
We refer the reader to Fang et al. (2015) for a
full description of their ME LM approach, whose
500-best outputs we analyze here.4 We also in-
clude the output from their ME LM that leverages
scores from a Deep Multimodal Similarity Model
(DMSM) during n-best re-ranking. Briefly, the
DMSM is a non-generative neural network model
which projects both the image pixels and caption
text into a comparable vector space, and scores
their similarity.
In the LSTM approach, similar to the ME LM
approach, we maintain a set of likely words D that
</bodyText>
<footnote confidence="0.81915">
4We will refer to this system as D-ME.
</footnote>
<bodyText confidence="0.995457727272727">
have not yet been mentioned in the caption un-
der construction. This set is initialized to all the
words predicted by the CNN above some thresh-
old α.5 The words already mentioned in the
sentence history h are then removed to produce
a set of conditioning words D \ {h}. We in-
corporate this information within the LSTM by
adding an additional input encoded to represent
the remaining visual attributes D \ {h} as a con-
tinuous valued auxiliary feature vector (Mikolov
and Zweig, 2012). This is encoded as f(sh_1 +
</bodyText>
<equation confidence="0.8214225">
E
vED\{h} gv + Uqh,D), where sh_1 and gv are
</equation>
<bodyText confidence="0.99939175">
respectively the continuous-space representations
for last word h−1 and detector v E D \ {h}, U is
learned matrix for recurrent histories, and f(·) is
the sigmoid transformation.
</bodyText>
<subsectionHeader confidence="0.996577">
2.2 Multimodal Recurrent Neural Network
</subsectionHeader>
<bodyText confidence="0.999974409090909">
In this section, we explore a model directly con-
ditioned on the CNN activations rather than a set
of word detections. Our implementation is very
similar to captioning models described in Karpa-
thy and Fei-Fei (2015), Vinyals et al. (2014), Mao
et al. (2015), and Donahue et al. (2014). This
joint vision-language RNN is referred to as a Mul-
timodal Recurrent Neural Network (MRNN).
In this model, we feed each image into our
CNN and retrieve the 4096-dimensional final hid-
den layer, denoted as fc7. The fc7 vector is
then fed into a hidden layer H to obtain a 500-
dimensional representation that serves as the ini-
tial hidden state to a gated recurrent neural net-
work (GRNN) (Cho et al., 2014). The GRNN
is trained jointly with H to produce the caption
one word at a time, conditioned on the previous
word and the previous recurrent state. For decod-
ing, we perform a beam search of size 10 to emit
tokens until an END token is produced. We use
a 500-dimensional GRNN hidden layer and 200-
dimensional word embeddings.
</bodyText>
<subsectionHeader confidence="0.998779">
2.3 k-Nearest Neighbor Model
</subsectionHeader>
<bodyText confidence="0.9998375">
Both Donahue et al. (2015) and Karpathy and Fei-
Fei (2015) present a 1-nearest neighbor baseline.
As a first step, we replicated these results using the
cosine similarity of the fc7 layer between each
test set image t and training image r. We randomly
emit one caption from t’s most similar training im-
age as the caption of t. As reported in previous
results, performance is quite poor, with a BLEU
</bodyText>
<footnote confidence="0.978075">
5In all experiments in this paper, α=0.5.
</footnote>
<page confidence="0.996231">
101
</page>
<figureCaption confidence="0.99240575">
Figure 1: Example of the set of candidate captions for an
image, the highest scoring m captions (green) and the con-
sensus caption (orange). This is a real example visualized in
two dimensions.
</figureCaption>
<bodyText confidence="0.999659615384615">
score of 11.2%.
However, we explore the idea that we may be
able to find an optimal k-nearest neighbor consen-
sus caption. We first select the k = 90 nearest
training images of a test image t as above. We de-
note the union of training captions in this set as
C = c1, ..., c5k.6 For each caption ci, we com-
pute the n-gram overlap F-score between ci and
each other caption in C. We define the consen-
sus caption c∗ to be caption with the highest mean
n-gram overlap with the other captions in C. We
have found it is better to only compute this average
among ci’s m = 125 most similar captions, rather
than all of C. The hyperparameters k and m were
obtained by a grid search on the validation set.
A visual example of the consensus caption is
given in Figure 1. Intuitively, we are choosing
a single caption that may describe many different
images that are similar to t, rather than a caption
that describes the single image that is most similar
to t. We believe that this is a reasonable approach
to take for a retrieval-based method for captioning,
as it helps ensure incorrect information is not men-
tioned. Further details on retrieval-based methods
are available in, e.g., (Ordonez et al., 2011; Ho-
dosh et al., 2013).
</bodyText>
<sectionHeader confidence="0.999493" genericHeader="method">
3 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999559">
3.1 The Microsoft COCO Dataset
</subsectionHeader>
<bodyText confidence="0.999992833333333">
We work with the Microsoft COCO dataset (Lin
et al., 2014), with 82,783 training images, and
the validation set split into 20,243 validation im-
ages and 20,244 testval images. Most images con-
tain multiple objects and significant contextual in-
formation, and each image comes with 5 human-
</bodyText>
<footnote confidence="0.756782">
6Each training image has 5 captions.
</footnote>
<table confidence="0.999922833333333">
LM PPLX BLEU METEOR
D-ME† 18.1 23.6 22.8
D-LSTM 14.3 22.4 22.6
MRNN 13.2 25.7 22.6
k-Nearest Neighbor - 26.0 22.5
1-Nearest Neighbor - 11.2 17.3
</table>
<tableCaption confidence="0.9631845">
Table 1: Model performance on testval. †: From (Fang et al.,
2015).
</tableCaption>
<table confidence="0.991118833333333">
D-ME+DMSM a plate with a sandwich and a cup of coffee
MRNN a close up of a plate of food
D-ME+DMSM+MRNN a plate of food and a cup of coffee
k-NN a cup of coffee on a plate with a spoon
D-ME+DMSM a black bear walking across a lush green forest
MRNN a couple of bears walking across a dirt road
D-ME+DMSM+MRNN a black bear walking through a wooded area
k-NN a black bear that is walking in the woods
D-ME+DMSM a gray and white cat sitting on top of it
MRNN a cat sitting in front of a mirror
D-ME+DMSM+MRNN a close up of a cat looking at the camera
k-NN a cat sitting on top of a wooden table
</table>
<tableCaption confidence="0.99949">
Table 2: Example generated captions.
</tableCaption>
<bodyText confidence="0.988923">
annotated captions. The images create a challeng-
ing testbed for image captioning and are widely
used in recent automatic image captioning work.
</bodyText>
<subsectionHeader confidence="0.99534">
3.2 Metrics
</subsectionHeader>
<bodyText confidence="0.999949933333333">
The quality of generated captions is measured au-
tomatically using BLEU (Papineni et al., 2002)
and METEOR (Denkowski and Lavie, 2014).
BLEU roughly measures the fraction of Ar-grams
(up to 4 grams) that are in common between a hy-
pothesis and one or more references, and penalizes
short hypotheses by a brevity penalty term.7 ME-
TEOR (Denkowski and Lavie, 2014) measures un-
igram precision and recall, extending exact word
matches to include similar words based on Word-
Net synonyms and stemmed tokens. We also re-
port the perplexity (PPLX) of studied detection-
conditioned LMs. The PPLX is in many ways
the natural measure of a statistical LM, but can be
loosely correlated with BLEU (Auli et al., 2013).
</bodyText>
<subsectionHeader confidence="0.999023">
3.3 Model Comparison
</subsectionHeader>
<bodyText confidence="0.916286272727273">
In Table 1, we summarize the generation perfor-
mance of our different models. The discrete de-
tection based models are prefixed with “D”. Some
example generated results are show in Table 2.
We see that the detection-conditioned LSTM
LM produces much lower PPLX than the
detection-conditioned ME LM, but its BLEU
score is no better. The MRNN has the lowest
PPLX, and highest BLEU among all LMs stud-
7We use the length of the reference that is closest to the
length of the hypothesis to compute the brevity penalty.
</bodyText>
<page confidence="0.970613">
102
</page>
<table confidence="0.999902">
Re-Ranking Features BLEU METEOR
D-ME † 23.6 22.8
+ DMSM † 25.7 23.6
+ MRNN 26.8 23.3
+ DMSM + MRNN 27.3 23.6
</table>
<tableCaption confidence="0.97713325">
Table 3: Model performance on testval after re-ranking.
†: previously reported and reconfirmed BLEU scores from
(Fang et al., 2015). +DMSM had resulted in the highest score
yet reported.
</tableCaption>
<bodyText confidence="0.9992648">
ied in our experiments. It significantly improves
BLEU by 2.1 absolutely over the D-ME LM base-
line. METEOR is similar across all three LM-
based methods.
Perhaps most surprisingly, the k-nearest neigh-
bor algorithm achieves a higher BLEU score than
all other models. However, as we will demonstrate
in Section 3.5, the generated captions perform sig-
nificantly better than the nearest neighbor captions
in terms of human quality judgements.
</bodyText>
<subsectionHeader confidence="0.99751">
3.4 n-best Re-Ranking
</subsectionHeader>
<bodyText confidence="0.996991933333334">
In addition to comparing the ME-based and RNN-
based LMs independently, we explore whether
combining these models results in an additive im-
provement. To this end, we use the 500-best list
from the D-ME and add a score for each hypoth-
esis from the MRNN.8 We then re-rank the hy-
potheses using MERT (Och, 2003). As in previous
work (Fang et al., 2015), model weights were opti-
mized to maximize BLEU score on the validation
set. We further extend this combination approach
to the D-ME model with DMSM scores included
during re-ranking (Fang et al., 2015).
Results are show in Table 3. We find that com-
bining the D-ME, DMSM, and MRNN achieves a
1.6 BLEU improvement over the D-ME+DMSM.
</bodyText>
<subsectionHeader confidence="0.987926">
3.5 Human Evaluation
</subsectionHeader>
<bodyText confidence="0.9999759">
Because automatic metrics do not always corre-
late with human judgments (Callison-Burch et al.,
2006; Hodosh et al., 2013), we also performed hu-
man evaluations using the same procedure as in
Fang et al. (2015). Here, human judges were pre-
sented with an image, a system generated caption,
and a human generated caption, and were asked
which caption was “better”.9 For each condition,
5 judgments were obtained for 1000 images from
the testval set.
</bodyText>
<footnote confidence="0.998609333333333">
8The MRNN does not produce a diverse n-best list.
9The captions were randomized and the users were not
informed which was which.
</footnote>
<bodyText confidence="0.957262666666667">
Results are shown in Table 4. The D-
ME+DMSM outperforms the MRNN by 5 per-
centage points for the “Better Or Equal to Hu-
man” judgment, despite both systems achieving
the same BLEU score. The k-Nearest Neighbor
system performs 1.4 percentage points worse than
the MRNN, despite achieving a slightly higher
BLEU score. Finally, the combined model does
not outperform the D-ME+DMSM in terms of hu-
man judgments despite a 1.6 BLEU improvement.
Although we cannot pinpoint the exact reason
for this mismatch between automated scores and
human evaluation, a more detailed analysis of the
difference between systems is performed in Sec-
tions 4 and 5.
</bodyText>
<table confidence="0.999215142857143">
Approach Human Judgements
Better Better BLEU
or Equal
D-ME+DMSM 7.8% 34.0% 25.7
MRNN 8.8% 29.0% 25.7
D-ME+DMSM+MRNN 5.7% 34.2% 27.3
k-Nearest Neighbor 5.5% 27.6% 26.0
</table>
<tableCaption confidence="0.95149875">
Table 4: Results when comparing produced captions to those
written by humans, as judged by humans. These are the per-
cent of captions judged to be “better than” or “better than or
equal to” a caption written by a human.
</tableCaption>
<sectionHeader confidence="0.980419" genericHeader="method">
4 Language Analysis
</sectionHeader>
<bodyText confidence="0.9998728125">
Examples of common mistakes we observe on the
testval set are shown in Table 5. The D-ME system
has difficulty with anaphora, particularly within
the phrase “on top of it”, as shown in examples
(1), (2), and (3). This is likely due to the fact that is
maintains a local context window. In contrast, the
MRNN approach tends to generate such anaphoric
relationships correctly.
However, the D-ME LM maintains an explicit
coverage state vector tracking which attributes
have already been emitted. The MRNN implicitly
maintains the full state using its recurrent layer,
which sometimes results in multiple emission mis-
takes, where the same attribute is emitted more
than once. This is particularly evident when coor-
dination (“and”) is present (examples (4) and (5)).
</bodyText>
<subsectionHeader confidence="0.990378">
4.1 Repeated Captions
</subsectionHeader>
<bodyText confidence="0.999344166666667">
All of our models produce a large number of cap-
tions seen in the training and repeated for differ-
ent images in the test set, as shown in Table 6
(also observed by Vinyals et al. (2014) for their
LSTM-based model). There are at least two po-
tential causes for this repetition.
</bodyText>
<page confidence="0.996528">
103
</page>
<table confidence="0.952931777777778">
D-ME+DMSM MRNN
1 a slice of pizza sitting on top of it a bed with a red blanket on top of it
2 a black and white bird perched on a birthday cake with candles on top
top of it of it
3 a little boy that is brushing his a little girl brushing her teeth with a
teeth with a toothbrush in her toothbrush
mouth
4 a large bed sitting in a bedroom a bedroom with a bed and a bed
5 a man wearing a bow tie a man wearing a tie and a tie
</table>
<tableCaption confidence="0.9874522">
Table 5: Example errors in the two basic approaches.
Table 6: Percentage unique (Unique Captions) and novel
(Seen In Training) captions for testval images. For example,
28.5% unique means 5,776 unique strings were generated for
all 20,244 images.
</tableCaption>
<bodyText confidence="0.999912">
First, the systems often produce generic cap-
tions such as “a close up of a plate of food”, which
may be applied to many publicly available im-
ages. This may suggest a deeper issue in the train-
ing and evaluation of our models, which warrants
more discussion in future work. Second, although
the COCO dataset and evaluation server10 has en-
couraged rapid progress in image captioning, there
may be a lack of diversity in the data. We also note
that although caption duplication is an issue in all
systems, it is a greater issue in the MRNN than the
D-ME+DMSM.
</bodyText>
<sectionHeader confidence="0.98806" genericHeader="method">
5 Image Diversity
</sectionHeader>
<bodyText confidence="0.9999299375">
The strong performance of the k-nearest neighbor
algorithm and the large number of repeated cap-
tions produced by the systems here suggest a lack
of diversity in the training and test data.11
We believe that one reason to work on image
captioning is to be able to caption compositionally
novel images, where the individual components of
the image may be seen in the training, but the en-
tire composition is often not.
In order to evaluate results for only compo-
sitionally novel images, we bin the test images
based on visual overlap with the training data.
For each test image, we compute the fc7 cosine
similarity with each training image, and the mean
value of the 50 closest images. We then compute
BLEU on the 20% least overlapping and 20% most
</bodyText>
<footnote confidence="0.99536475">
10http://mscoco.org/dataset/
11This is partially an artifact of the manner in which the
Microsoft COCO data set was constructed, since each image
was chosen to be in one of 80 pre-defined object categories.
</footnote>
<table confidence="0.999894625">
Condition Train/Test Visual Overlap
BLEU
Whole 20% 20%
Set Least Most
D-ME+DMSM 25.7 20.9 29.9
MRNN 25.7 18.8 32.0
D-ME+DMSM+MRNN 27.3 21.7 32.0
k-Nearest Neighbor 26.0 18.4 33.2
</table>
<tableCaption confidence="0.998186">
Table 7: Performance for different portions of testval, based
on visual overlap with the training.
</tableCaption>
<bodyText confidence="0.973520705882353">
overlapping subsets.
Results are shown in Table 7. The D-
ME+DMSM outperforms the k-nearest neighbor
approach by 2.5 BLEU on the “20% Least” set,
even though performance on the whole set is com-
parable. Additionally, the D-ME+DMSM out-
performs the MRNN by 2.1 BLEU on the “20%
Least” set, but performs 2.1 BLEU worse on
the “20% Most” set. This is evidence that D-
ME+DMSM generalizes better on novel images
than the MRNN; this is further supported by the
relatively low percentage of captions it gener-
ates seen in the training data (Table 6) while still
achieving reasonable captioning performance. We
hypothesize that these are the main reasons for
the strong human evaluation results of the D-
ME+DMSM shown in Section 3.5.
</bodyText>
<sectionHeader confidence="0.999419" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999787">
We have shown that a gated RNN conditioned di-
rectly on CNN activations (an MRNN) achieves
better BLEU performance than an ME LM or
LSTM conditioned on a set of discrete activations;
and a similar BLEU performance to an ME LM
combined with a DMSM. However, the ME LM
+ DMSM method significantly outperforms the
MRNN in terms of human quality judgments. We
hypothesize that this is partially due to the lack of
novelty in the captions produced by the MRNN.
In fact, a k-nearest neighbor retrieval algorithm
introduced in this paper performs similarly to the
MRNN in terms of both automatic metrics and hu-
man judgements.
When we use the MRNN system alongside the
DMSM to provide additional scores in MERT re-
ranking of the n-best produced by the image-
conditioned ME LM, we advance by 1.6 BLEU
points on the best previously published results on
the COCO dataset. Unfortunately, this improve-
ment in BLEU does not translate to improved hu-
man quality judgments.
</bodyText>
<figure confidence="0.99885595">
Human
4.8%
99.4%
33.1%
MRNN
System
Unique
Captions
Seen In
Training
D-ME+DMSM
47.0%
30.0%
D-ME+DMSM+MRNN
k-Nearest Neighbor
28.5%
36.6%
60.3%
61.3%
100%
</figure>
<page confidence="0.992768">
104
</page>
<sectionHeader confidence="0.995777" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999560340425532">
Michael Auli, Michel Galley, Chris Quirk, and Ge-
offrey Zweig. 2013. Joint language and transla-
tion modeling with recurrent neural networks. In
Proc. Conf. Empirical Methods Natural Language
Process. (EMNLP), pages 1044–1054.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluation the role of bleu in
machine translation research. In EACL, volume 6,
pages 249–256.
Xinlei Chen and C. Lawrence Zitnick. 2015. Mind’s
eye: A recurrent visual representation for image cap-
tion generation. In Proc. Conf. Comput. Vision and
Pattern Recognition (CVPR).
Kyunghyun Cho, Bart van Merrienboer, Caglar Gul-
cehre, Fethi Bougares, Holger Schwenk, and Yoshua
Bengio. 2014. Learning phrase representations
using RNN encoder-decoder for statistical machine
translation. CoRR.
Michael Denkowski and Alon Lavie. 2014. Meteor
universal: language specific translation evaluation
for any target language. In Proc. EACL 2014 Work-
shop Statistical Machine Translation.
Jeff Donahue, Lisa Anne Hendricks, Sergio Guadar-
rama, Marcus Rohrbach, Subhashini Venugopalan,
Kate Saenko, and Trevor Darrell. 2014. Long-term
recurrent convolutional networks for visual recogni-
tion and description. arXiv:1411.4389 [cs.CV].
Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadar-
rama, Marcus Rohrbach, Subhashini Venugopalan,
Kate Saenko, and Trevor Darrell. 2015. Long-term
recurrent convolutional networks for visual recogni-
tion and description. In Proc. Conf. Comput. Vision
and Pattern Recognition (CVPR).
Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Sri-
vastava, Li Deng, Piotr Doll´a, Margaret Mitchell,
John C. Platt, C. Lawrence Zitnick, and Geoffrey
Zweig. 2015. From captionons to visual concepts
and back. In Proc. Conf. Comput. Vision and Pat-
tern Recognition (CVPR).
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences from images.
In Proc. European Conf. Comput. Vision (ECCV),
pages 15–29.
Micah Hodosh, Peter Young, and Julia Hockenmaier.
2013. Framing image description as a ranking task:
data models and evaluation metrics. J. Artificial In-
tell. Research, pages 853–899.
Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In Proc. Conf. Comput. Vision and Pattern
Recognition (CVPR).
Ryan Kiros, Ruslan Salakhutdinov, and Richard Zemel.
2014. Multimodal neural language models. In Proc.
Int. Conf. Machine Learning (ICML).
Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar,
and C. Lawrence Zitnick. 2014. Microsoft COCO:
Common objects in context. arXiv:1405.0312
[cs.CV].
Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and
Alan L. Yuille. 2015. Deep captioning with multi-
modal recurrent neural networks (m-RNN). In Proc.
Int. Conf. Learning Representations (ICLR).
Rebecca Mason and Eugene Charniak. 2014. Domain-
specific image captioning. In CoNLL.
Tomas Mikolov and Geoffrey Zweig. 2012. Context
dependent recurrent neural network language model.
In SLT, pages 234–239.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL, ACL ’03.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2Text: Describing images using 1 million
captioned photogrphs. In Proc. Annu. Conf. Neural
Inform. Process. Syst. (NIPS).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. Assoc.
for Computational Linguistics (ACL), pages 311–
318.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,
Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-
drej Karpathy, Aditya Khosla, Michael Bernstein,
Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet
Large Scale Visual Recognition Challenge. Interna-
tional Journal of Computer Vision (IJCV).
Karen Simonyan and Andrew Zisserman. 2014. Very
deep convolutional networks for large-scale image
recognition. arXiv preprint.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and
Dumitru Erhan. 2014. Show and tell: a neural im-
age caption generator. In Proc. Conf. Comput. Vi-
sion and Pattern Recognition (CVPR).
</reference>
<page confidence="0.999008">
105
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.403426">
<title confidence="0.999863">Language Models for Image Captioning: The Quirks and What Works</title>
<author confidence="0.945175">Hao Hao Saurabh Deng</author>
<author confidence="0.945175">Xiaodong Geoffrey Margaret</author>
<affiliation confidence="0.951745">Microsoft Research</affiliation>
<note confidence="0.629518">Corresponding authors: of Washington of California at Berkeley</note>
<abstract confidence="0.996089423076923">Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-ofthe-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP),</booktitle>
<pages>1044--1054</pages>
<contexts>
<context position="11845" citStr="Auli et al., 2013" startWordPosition="2026" endWordPosition="2029"> et al., 2002) and METEOR (Denkowski and Lavie, 2014). BLEU roughly measures the fraction of Ar-grams (up to 4 grams) that are in common between a hypothesis and one or more references, and penalizes short hypotheses by a brevity penalty term.7 METEOR (Denkowski and Lavie, 2014) measures unigram precision and recall, extending exact word matches to include similar words based on WordNet synonyms and stemmed tokens. We also report the perplexity (PPLX) of studied detectionconditioned LMs. The PPLX is in many ways the natural measure of a statistical LM, but can be loosely correlated with BLEU (Auli et al., 2013). 3.3 Model Comparison In Table 1, we summarize the generation performance of our different models. The discrete detection based models are prefixed with “D”. Some example generated results are show in Table 2. We see that the detection-conditioned LSTM LM produces much lower PPLX than the detection-conditioned ME LM, but its BLEU score is no better. The MRNN has the lowest PPLX, and highest BLEU among all LMs stud7We use the length of the reference that is closest to the length of the hypothesis to compute the brevity penalty. 102 Re-Ranking Features BLEU METEOR D-ME † 23.6 22.8 + DMSM † 25.7</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP), pages 1044–1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Miles Osborne</author>
<author>Philipp Koehn</author>
</authors>
<title>Re-evaluation the role of bleu in machine translation research.</title>
<date>2006</date>
<booktitle>In EACL,</booktitle>
<volume>6</volume>
<pages>249--256</pages>
<contexts>
<context position="13937" citStr="Callison-Burch et al., 2006" startWordPosition="2381" endWordPosition="2384">500-best list from the D-ME and add a score for each hypothesis from the MRNN.8 We then re-rank the hypotheses using MERT (Och, 2003). As in previous work (Fang et al., 2015), model weights were optimized to maximize BLEU score on the validation set. We further extend this combination approach to the D-ME model with DMSM scores included during re-ranking (Fang et al., 2015). Results are show in Table 3. We find that combining the D-ME, DMSM, and MRNN achieves a 1.6 BLEU improvement over the D-ME+DMSM. 3.5 Human Evaluation Because automatic metrics do not always correlate with human judgments (Callison-Burch et al., 2006; Hodosh et al., 2013), we also performed human evaluations using the same procedure as in Fang et al. (2015). Here, human judges were presented with an image, a system generated caption, and a human generated caption, and were asked which caption was “better”.9 For each condition, 5 judgments were obtained for 1000 images from the testval set. 8The MRNN does not produce a diverse n-best list. 9The captions were randomized and the users were not informed which was which. Results are shown in Table 4. The DME+DMSM outperforms the MRNN by 5 percentage points for the “Better Or Equal to Human” ju</context>
</contexts>
<marker>Callison-Burch, Osborne, Koehn, 2006</marker>
<rawString>Chris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluation the role of bleu in machine translation research. In EACL, volume 6, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xinlei Chen</author>
<author>C Lawrence Zitnick</author>
</authors>
<title>Mind’s eye: A recurrent visual representation for image caption generation.</title>
<date>2015</date>
<booktitle>In Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context position="2160" citStr="Chen and Zitnick, 2015" startWordPosition="348" endWordPosition="351">st decomposes the problem into an initial step that uses a convolutional neural network to predict a bag of words that are likely to be present in a caption; then in a second step, a maximum entropy language model (ME LM) is used to generate a sentence that covers a minimum number of the detected words (Fang et al., 2015). The second approach uses the activations from final hidden layer of an object detection CNN as the input to a recurrent neural network language model (RNN LM). This is referred to as a Multimodal Recurrent Neural Network (MRNN) (Karpathy and Fei-Fei, 2015; Mao et al., 2015; Chen and Zitnick, 2015). Similar in spirit is the the log-bilinear (LBL) LM of Kiros et al. (2014). In this paper, we study the relative merits of these approaches. By using an identical state-ofthe-art CNN as the input to RNN-based and MEbased models, we are able to empirically compare the strengths and weaknesses of the language modeling components. We find that the approach of directly generating the text with an MRNN1 outperforms the ME LM when measured by BLEU on the COCO dataset (Lin et al., 2014),2 but this recurrent model tends to reproduce captions in the training set. In fact, a simple k-nearest neighbor a</context>
</contexts>
<marker>Chen, Zitnick, 2015</marker>
<rawString>Xinlei Chen and C. Lawrence Zitnick. 2015. Mind’s eye: A recurrent visual representation for image caption generation. In Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyunghyun Cho</author>
<author>Bart van Merrienboer</author>
</authors>
<title>Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio.</title>
<date>2014</date>
<publisher>CoRR.</publisher>
<marker>Cho, van Merrienboer, 2014</marker>
<rawString>Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Denkowski</author>
<author>Alon Lavie</author>
</authors>
<title>Meteor universal: language specific translation evaluation for any target language. In</title>
<date>2014</date>
<booktitle>Proc. EACL 2014 Workshop Statistical Machine Translation.</booktitle>
<contexts>
<context position="11280" citStr="Denkowski and Lavie, 2014" startWordPosition="1929" endWordPosition="1932">d D-ME+DMSM+MRNN a black bear walking through a wooded area k-NN a black bear that is walking in the woods D-ME+DMSM a gray and white cat sitting on top of it MRNN a cat sitting in front of a mirror D-ME+DMSM+MRNN a close up of a cat looking at the camera k-NN a cat sitting on top of a wooden table Table 2: Example generated captions. annotated captions. The images create a challenging testbed for image captioning and are widely used in recent automatic image captioning work. 3.2 Metrics The quality of generated captions is measured automatically using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). BLEU roughly measures the fraction of Ar-grams (up to 4 grams) that are in common between a hypothesis and one or more references, and penalizes short hypotheses by a brevity penalty term.7 METEOR (Denkowski and Lavie, 2014) measures unigram precision and recall, extending exact word matches to include similar words based on WordNet synonyms and stemmed tokens. We also report the perplexity (PPLX) of studied detectionconditioned LMs. The PPLX is in many ways the natural measure of a statistical LM, but can be loosely correlated with BLEU (Auli et al., 2013). 3.3 Model Comparison In Table 1, </context>
</contexts>
<marker>Denkowski, Lavie, 2014</marker>
<rawString>Michael Denkowski and Alon Lavie. 2014. Meteor universal: language specific translation evaluation for any target language. In Proc. EACL 2014 Workshop Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Donahue</author>
<author>Lisa Anne Hendricks</author>
<author>Sergio Guadarrama</author>
<author>Marcus Rohrbach</author>
<author>Subhashini Venugopalan</author>
<author>Kate Saenko</author>
<author>Trevor Darrell</author>
</authors>
<title>Long-term recurrent convolutional networks for visual recognition and description.</title>
<date>2014</date>
<note>arXiv:1411.4389 [cs.CV].</note>
<contexts>
<context position="7158" citStr="Donahue et al. (2014)" startWordPosition="1187" endWordPosition="1190">ary feature vector (Mikolov and Zweig, 2012). This is encoded as f(sh_1 + E vED\{h} gv + Uqh,D), where sh_1 and gv are respectively the continuous-space representations for last word h−1 and detector v E D \ {h}, U is learned matrix for recurrent histories, and f(·) is the sigmoid transformation. 2.2 Multimodal Recurrent Neural Network In this section, we explore a model directly conditioned on the CNN activations rather than a set of word detections. Our implementation is very similar to captioning models described in Karpathy and Fei-Fei (2015), Vinyals et al. (2014), Mao et al. (2015), and Donahue et al. (2014). This joint vision-language RNN is referred to as a Multimodal Recurrent Neural Network (MRNN). In this model, we feed each image into our CNN and retrieve the 4096-dimensional final hidden layer, denoted as fc7. The fc7 vector is then fed into a hidden layer H to obtain a 500- dimensional representation that serves as the initial hidden state to a gated recurrent neural network (GRNN) (Cho et al., 2014). The GRNN is trained jointly with H to produce the caption one word at a time, conditioned on the previous word and the previous recurrent state. For decoding, we perform a beam search of siz</context>
</contexts>
<marker>Donahue, Hendricks, Guadarrama, Rohrbach, Venugopalan, Saenko, Darrell, 2014</marker>
<rawString>Jeff Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. 2014. Long-term recurrent convolutional networks for visual recognition and description. arXiv:1411.4389 [cs.CV].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Donahue</author>
<author>Lisa Anne Hendricks</author>
<author>Sergio Guadarrama</author>
<author>Marcus Rohrbach</author>
<author>Subhashini Venugopalan</author>
<author>Kate Saenko</author>
<author>Trevor Darrell</author>
</authors>
<title>Long-term recurrent convolutional networks for visual recognition and description.</title>
<date>2015</date>
<booktitle>In Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context position="7946" citStr="Donahue et al. (2015)" startWordPosition="1327" endWordPosition="1330">imensional final hidden layer, denoted as fc7. The fc7 vector is then fed into a hidden layer H to obtain a 500- dimensional representation that serves as the initial hidden state to a gated recurrent neural network (GRNN) (Cho et al., 2014). The GRNN is trained jointly with H to produce the caption one word at a time, conditioned on the previous word and the previous recurrent state. For decoding, we perform a beam search of size 10 to emit tokens until an END token is produced. We use a 500-dimensional GRNN hidden layer and 200- dimensional word embeddings. 2.3 k-Nearest Neighbor Model Both Donahue et al. (2015) and Karpathy and FeiFei (2015) present a 1-nearest neighbor baseline. As a first step, we replicated these results using the cosine similarity of the fc7 layer between each test set image t and training image r. We randomly emit one caption from t’s most similar training image as the caption of t. As reported in previous results, performance is quite poor, with a BLEU 5In all experiments in this paper, α=0.5. 101 Figure 1: Example of the set of candidate captions for an image, the highest scoring m captions (green) and the consensus caption (orange). This is a real example visualized in two d</context>
</contexts>
<marker>Donahue, Hendricks, Guadarrama, Rohrbach, Venugopalan, Saenko, Darrell, 2015</marker>
<rawString>Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. 2015. Long-term recurrent convolutional networks for visual recognition and description. In Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Fang</author>
<author>Saurabh Gupta</author>
<author>Forrest Iandola</author>
<author>Rupesh Srivastava</author>
<author>Li Deng</author>
<author>Piotr Doll´a</author>
<author>Margaret Mitchell</author>
<author>John C Platt</author>
<author>C Lawrence Zitnick</author>
<author>Geoffrey Zweig</author>
</authors>
<title>From captionons to visual concepts and back.</title>
<date>2015</date>
<booktitle>In Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).</booktitle>
<marker>Fang, Gupta, Iandola, Srivastava, Deng, Doll´a, Mitchell, Platt, Zitnick, Zweig, 2015</marker>
<rawString>Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh Srivastava, Li Deng, Piotr Doll´a, Margaret Mitchell, John C. Platt, C. Lawrence Zitnick, and Geoffrey Zweig. 2015. From captionons to visual concepts and back. In Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ali Farhadi</author>
<author>Mohsen Hejrati</author>
<author>Mohammad Amin Sadeghi</author>
<author>Peter Young</author>
<author>Cyrus Rashtchian</author>
<author>Julia Hockenmaier</author>
<author>David Forsyth</author>
</authors>
<title>Every picture tells a story: generating sentences from images. In</title>
<date>2010</date>
<booktitle>Proc. European Conf. Comput. Vision (ECCV),</booktitle>
<pages>15--29</pages>
<contexts>
<context position="2830" citStr="Farhadi et al., 2010" startWordPosition="467" endWordPosition="470">LM of Kiros et al. (2014). In this paper, we study the relative merits of these approaches. By using an identical state-ofthe-art CNN as the input to RNN-based and MEbased models, we are able to empirically compare the strengths and weaknesses of the language modeling components. We find that the approach of directly generating the text with an MRNN1 outperforms the ME LM when measured by BLEU on the COCO dataset (Lin et al., 2014),2 but this recurrent model tends to reproduce captions in the training set. In fact, a simple k-nearest neighbor approach, which is common in earlier related work (Farhadi et al., 2010; Mason and Charniak, 2014), performs similarly to the MRNN. In contrast, the ME LM generates the most novel captions, and does the best at captioning images for which there is no close match in the training data. With a Deep Multimodal Similarity Model (DMSM) incorporated,3 the ME LM significantly outperforms other methods according to human judgments. In sum, the contributions of this paper are as follows: 1. We compare the use of discrete detections and continuous valued CNN activations as the conditioning information for language models trained to generate image captions. 2. We show that a</context>
</contexts>
<marker>Farhadi, Hejrati, Sadeghi, Young, Rashtchian, Hockenmaier, Forsyth, 2010</marker>
<rawString>Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David Forsyth. 2010. Every picture tells a story: generating sentences from images. In Proc. European Conf. Comput. Vision (ECCV), pages 15–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micah Hodosh</author>
<author>Peter Young</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Framing image description as a ranking task: data models and evaluation metrics.</title>
<date>2013</date>
<journal>J. Artificial Intell. Research,</journal>
<pages>853--899</pages>
<contexts>
<context position="9772" citStr="Hodosh et al., 2013" startWordPosition="1658" endWordPosition="1662"> than all of C. The hyperparameters k and m were obtained by a grid search on the validation set. A visual example of the consensus caption is given in Figure 1. Intuitively, we are choosing a single caption that may describe many different images that are similar to t, rather than a caption that describes the single image that is most similar to t. We believe that this is a reasonable approach to take for a retrieval-based method for captioning, as it helps ensure incorrect information is not mentioned. Further details on retrieval-based methods are available in, e.g., (Ordonez et al., 2011; Hodosh et al., 2013). 3 Experimental Results 3.1 The Microsoft COCO Dataset We work with the Microsoft COCO dataset (Lin et al., 2014), with 82,783 training images, and the validation set split into 20,243 validation images and 20,244 testval images. Most images contain multiple objects and significant contextual information, and each image comes with 5 human6Each training image has 5 captions. LM PPLX BLEU METEOR D-ME† 18.1 23.6 22.8 D-LSTM 14.3 22.4 22.6 MRNN 13.2 25.7 22.6 k-Nearest Neighbor - 26.0 22.5 1-Nearest Neighbor - 11.2 17.3 Table 1: Model performance on testval. †: From (Fang et al., 2015). D-ME+DMSM</context>
<context position="13959" citStr="Hodosh et al., 2013" startWordPosition="2385" endWordPosition="2388">nd add a score for each hypothesis from the MRNN.8 We then re-rank the hypotheses using MERT (Och, 2003). As in previous work (Fang et al., 2015), model weights were optimized to maximize BLEU score on the validation set. We further extend this combination approach to the D-ME model with DMSM scores included during re-ranking (Fang et al., 2015). Results are show in Table 3. We find that combining the D-ME, DMSM, and MRNN achieves a 1.6 BLEU improvement over the D-ME+DMSM. 3.5 Human Evaluation Because automatic metrics do not always correlate with human judgments (Callison-Burch et al., 2006; Hodosh et al., 2013), we also performed human evaluations using the same procedure as in Fang et al. (2015). Here, human judges were presented with an image, a system generated caption, and a human generated caption, and were asked which caption was “better”.9 For each condition, 5 judgments were obtained for 1000 images from the testval set. 8The MRNN does not produce a diverse n-best list. 9The captions were randomized and the users were not informed which was which. Results are shown in Table 4. The DME+DMSM outperforms the MRNN by 5 percentage points for the “Better Or Equal to Human” judgment, despite both s</context>
</contexts>
<marker>Hodosh, Young, Hockenmaier, 2013</marker>
<rawString>Micah Hodosh, Peter Young, and Julia Hockenmaier. 2013. Framing image description as a ranking task: data models and evaluation metrics. J. Artificial Intell. Research, pages 853–899.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrej Karpathy</author>
<author>Li Fei-Fei</author>
</authors>
<title>Deep visualsemantic alignments for generating image descriptions.</title>
<date>2015</date>
<booktitle>In Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context position="2117" citStr="Karpathy and Fei-Fei, 2015" startWordPosition="340" endWordPosition="343">ches have been explored for this task. The first decomposes the problem into an initial step that uses a convolutional neural network to predict a bag of words that are likely to be present in a caption; then in a second step, a maximum entropy language model (ME LM) is used to generate a sentence that covers a minimum number of the detected words (Fang et al., 2015). The second approach uses the activations from final hidden layer of an object detection CNN as the input to a recurrent neural network language model (RNN LM). This is referred to as a Multimodal Recurrent Neural Network (MRNN) (Karpathy and Fei-Fei, 2015; Mao et al., 2015; Chen and Zitnick, 2015). Similar in spirit is the the log-bilinear (LBL) LM of Kiros et al. (2014). In this paper, we study the relative merits of these approaches. By using an identical state-ofthe-art CNN as the input to RNN-based and MEbased models, we are able to empirically compare the strengths and weaknesses of the language modeling components. We find that the approach of directly generating the text with an MRNN1 outperforms the ME LM when measured by BLEU on the COCO dataset (Lin et al., 2014),2 but this recurrent model tends to reproduce captions in the training </context>
<context position="7089" citStr="Karpathy and Fei-Fei (2015)" startWordPosition="1173" endWordPosition="1177">esent the remaining visual attributes D \ {h} as a continuous valued auxiliary feature vector (Mikolov and Zweig, 2012). This is encoded as f(sh_1 + E vED\{h} gv + Uqh,D), where sh_1 and gv are respectively the continuous-space representations for last word h−1 and detector v E D \ {h}, U is learned matrix for recurrent histories, and f(·) is the sigmoid transformation. 2.2 Multimodal Recurrent Neural Network In this section, we explore a model directly conditioned on the CNN activations rather than a set of word detections. Our implementation is very similar to captioning models described in Karpathy and Fei-Fei (2015), Vinyals et al. (2014), Mao et al. (2015), and Donahue et al. (2014). This joint vision-language RNN is referred to as a Multimodal Recurrent Neural Network (MRNN). In this model, we feed each image into our CNN and retrieve the 4096-dimensional final hidden layer, denoted as fc7. The fc7 vector is then fed into a hidden layer H to obtain a 500- dimensional representation that serves as the initial hidden state to a gated recurrent neural network (GRNN) (Cho et al., 2014). The GRNN is trained jointly with H to produce the caption one word at a time, conditioned on the previous word and the pr</context>
</contexts>
<marker>Karpathy, Fei-Fei, 2015</marker>
<rawString>Andrej Karpathy and Li Fei-Fei. 2015. Deep visualsemantic alignments for generating image descriptions. In Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan Kiros</author>
<author>Ruslan Salakhutdinov</author>
<author>Richard Zemel</author>
</authors>
<title>Multimodal neural language models.</title>
<date>2014</date>
<booktitle>In Proc. Int. Conf. Machine Learning (ICML).</booktitle>
<contexts>
<context position="2235" citStr="Kiros et al. (2014)" startWordPosition="362" endWordPosition="365"> network to predict a bag of words that are likely to be present in a caption; then in a second step, a maximum entropy language model (ME LM) is used to generate a sentence that covers a minimum number of the detected words (Fang et al., 2015). The second approach uses the activations from final hidden layer of an object detection CNN as the input to a recurrent neural network language model (RNN LM). This is referred to as a Multimodal Recurrent Neural Network (MRNN) (Karpathy and Fei-Fei, 2015; Mao et al., 2015; Chen and Zitnick, 2015). Similar in spirit is the the log-bilinear (LBL) LM of Kiros et al. (2014). In this paper, we study the relative merits of these approaches. By using an identical state-ofthe-art CNN as the input to RNN-based and MEbased models, we are able to empirically compare the strengths and weaknesses of the language modeling components. We find that the approach of directly generating the text with an MRNN1 outperforms the ME LM when measured by BLEU on the COCO dataset (Lin et al., 2014),2 but this recurrent model tends to reproduce captions in the training set. In fact, a simple k-nearest neighbor approach, which is common in earlier related work (Farhadi et al., 2010; Mas</context>
</contexts>
<marker>Kiros, Salakhutdinov, Zemel, 2014</marker>
<rawString>Ryan Kiros, Ruslan Salakhutdinov, and Richard Zemel. 2014. Multimodal neural language models. In Proc. Int. Conf. Machine Learning (ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tsung-Yi Lin</author>
<author>Michael Maire</author>
<author>Serge Belongie</author>
<author>James Hays</author>
<author>Pietro Perona</author>
<author>Deva Ramanan</author>
<author>Piotr Doll´ar</author>
<author>C Lawrence Zitnick</author>
</authors>
<title>Microsoft COCO: Common objects in context.</title>
<date>2014</date>
<note>arXiv:1405.0312 [cs.CV].</note>
<marker>Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll´ar, Zitnick, 2014</marker>
<rawString>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common objects in context. arXiv:1405.0312 [cs.CV].</rawString>
</citation>
<citation valid="true">
<authors>
<author>Junhua Mao</author>
<author>Wei Xu</author>
<author>Yi Yang</author>
<author>Jiang Wang</author>
<author>Alan L Yuille</author>
</authors>
<title>Deep captioning with multimodal recurrent neural networks (m-RNN).</title>
<date>2015</date>
<booktitle>In Proc. Int. Conf. Learning Representations (ICLR).</booktitle>
<contexts>
<context position="2135" citStr="Mao et al., 2015" startWordPosition="344" endWordPosition="347">this task. The first decomposes the problem into an initial step that uses a convolutional neural network to predict a bag of words that are likely to be present in a caption; then in a second step, a maximum entropy language model (ME LM) is used to generate a sentence that covers a minimum number of the detected words (Fang et al., 2015). The second approach uses the activations from final hidden layer of an object detection CNN as the input to a recurrent neural network language model (RNN LM). This is referred to as a Multimodal Recurrent Neural Network (MRNN) (Karpathy and Fei-Fei, 2015; Mao et al., 2015; Chen and Zitnick, 2015). Similar in spirit is the the log-bilinear (LBL) LM of Kiros et al. (2014). In this paper, we study the relative merits of these approaches. By using an identical state-ofthe-art CNN as the input to RNN-based and MEbased models, we are able to empirically compare the strengths and weaknesses of the language modeling components. We find that the approach of directly generating the text with an MRNN1 outperforms the ME LM when measured by BLEU on the COCO dataset (Lin et al., 2014),2 but this recurrent model tends to reproduce captions in the training set. In fact, a si</context>
<context position="7131" citStr="Mao et al. (2015)" startWordPosition="1182" endWordPosition="1185">ontinuous valued auxiliary feature vector (Mikolov and Zweig, 2012). This is encoded as f(sh_1 + E vED\{h} gv + Uqh,D), where sh_1 and gv are respectively the continuous-space representations for last word h−1 and detector v E D \ {h}, U is learned matrix for recurrent histories, and f(·) is the sigmoid transformation. 2.2 Multimodal Recurrent Neural Network In this section, we explore a model directly conditioned on the CNN activations rather than a set of word detections. Our implementation is very similar to captioning models described in Karpathy and Fei-Fei (2015), Vinyals et al. (2014), Mao et al. (2015), and Donahue et al. (2014). This joint vision-language RNN is referred to as a Multimodal Recurrent Neural Network (MRNN). In this model, we feed each image into our CNN and retrieve the 4096-dimensional final hidden layer, denoted as fc7. The fc7 vector is then fed into a hidden layer H to obtain a 500- dimensional representation that serves as the initial hidden state to a gated recurrent neural network (GRNN) (Cho et al., 2014). The GRNN is trained jointly with H to produce the caption one word at a time, conditioned on the previous word and the previous recurrent state. For decoding, we p</context>
</contexts>
<marker>Mao, Xu, Yang, Wang, Yuille, 2015</marker>
<rawString>Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L. Yuille. 2015. Deep captioning with multimodal recurrent neural networks (m-RNN). In Proc. Int. Conf. Learning Representations (ICLR).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Mason</author>
<author>Eugene Charniak</author>
</authors>
<title>Domainspecific image captioning.</title>
<date>2014</date>
<booktitle>In CoNLL.</booktitle>
<contexts>
<context position="2857" citStr="Mason and Charniak, 2014" startWordPosition="471" endWordPosition="475">14). In this paper, we study the relative merits of these approaches. By using an identical state-ofthe-art CNN as the input to RNN-based and MEbased models, we are able to empirically compare the strengths and weaknesses of the language modeling components. We find that the approach of directly generating the text with an MRNN1 outperforms the ME LM when measured by BLEU on the COCO dataset (Lin et al., 2014),2 but this recurrent model tends to reproduce captions in the training set. In fact, a simple k-nearest neighbor approach, which is common in earlier related work (Farhadi et al., 2010; Mason and Charniak, 2014), performs similarly to the MRNN. In contrast, the ME LM generates the most novel captions, and does the best at captioning images for which there is no close match in the training data. With a Deep Multimodal Similarity Model (DMSM) incorporated,3 the ME LM significantly outperforms other methods according to human judgments. In sum, the contributions of this paper are as follows: 1. We compare the use of discrete detections and continuous valued CNN activations as the conditioning information for language models trained to generate image captions. 2. We show that a simple k-nearest neighbor </context>
</contexts>
<marker>Mason, Charniak, 2014</marker>
<rawString>Rebecca Mason and Eugene Charniak. 2014. Domainspecific image captioning. In CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Context dependent recurrent neural network language model.</title>
<date>2012</date>
<booktitle>In SLT,</booktitle>
<pages>234--239</pages>
<contexts>
<context position="6581" citStr="Mikolov and Zweig, 2012" startWordPosition="1089" endWordPosition="1092">milarity. In the LSTM approach, similar to the ME LM approach, we maintain a set of likely words D that 4We will refer to this system as D-ME. have not yet been mentioned in the caption under construction. This set is initialized to all the words predicted by the CNN above some threshold α.5 The words already mentioned in the sentence history h are then removed to produce a set of conditioning words D \ {h}. We incorporate this information within the LSTM by adding an additional input encoded to represent the remaining visual attributes D \ {h} as a continuous valued auxiliary feature vector (Mikolov and Zweig, 2012). This is encoded as f(sh_1 + E vED\{h} gv + Uqh,D), where sh_1 and gv are respectively the continuous-space representations for last word h−1 and detector v E D \ {h}, U is learned matrix for recurrent histories, and f(·) is the sigmoid transformation. 2.2 Multimodal Recurrent Neural Network In this section, we explore a model directly conditioned on the CNN activations rather than a set of word detections. Our implementation is very similar to captioning models described in Karpathy and Fei-Fei (2015), Vinyals et al. (2014), Mao et al. (2015), and Donahue et al. (2014). This joint vision-lan</context>
</contexts>
<marker>Mikolov, Zweig, 2012</marker>
<rawString>Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. In SLT, pages 234–239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum error rate training in statistical machine translation.</title>
<date>2003</date>
<booktitle>In ACL, ACL ’03.</booktitle>
<contexts>
<context position="13443" citStr="Och, 2003" startWordPosition="2300" endWordPosition="2301">rhaps most surprisingly, the k-nearest neighbor algorithm achieves a higher BLEU score than all other models. However, as we will demonstrate in Section 3.5, the generated captions perform significantly better than the nearest neighbor captions in terms of human quality judgements. 3.4 n-best Re-Ranking In addition to comparing the ME-based and RNNbased LMs independently, we explore whether combining these models results in an additive improvement. To this end, we use the 500-best list from the D-ME and add a score for each hypothesis from the MRNN.8 We then re-rank the hypotheses using MERT (Och, 2003). As in previous work (Fang et al., 2015), model weights were optimized to maximize BLEU score on the validation set. We further extend this combination approach to the D-ME model with DMSM scores included during re-ranking (Fang et al., 2015). Results are show in Table 3. We find that combining the D-ME, DMSM, and MRNN achieves a 1.6 BLEU improvement over the D-ME+DMSM. 3.5 Human Evaluation Because automatic metrics do not always correlate with human judgments (Callison-Burch et al., 2006; Hodosh et al., 2013), we also performed human evaluations using the same procedure as in Fang et al. (20</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In ACL, ACL ’03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vicente Ordonez</author>
<author>Girish Kulkarni</author>
<author>Tamara L Berg</author>
</authors>
<title>Im2Text: Describing images using 1 million captioned photogrphs.</title>
<date>2011</date>
<booktitle>In Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS).</booktitle>
<contexts>
<context position="9750" citStr="Ordonez et al., 2011" startWordPosition="1654" endWordPosition="1657">milar captions, rather than all of C. The hyperparameters k and m were obtained by a grid search on the validation set. A visual example of the consensus caption is given in Figure 1. Intuitively, we are choosing a single caption that may describe many different images that are similar to t, rather than a caption that describes the single image that is most similar to t. We believe that this is a reasonable approach to take for a retrieval-based method for captioning, as it helps ensure incorrect information is not mentioned. Further details on retrieval-based methods are available in, e.g., (Ordonez et al., 2011; Hodosh et al., 2013). 3 Experimental Results 3.1 The Microsoft COCO Dataset We work with the Microsoft COCO dataset (Lin et al., 2014), with 82,783 training images, and the validation set split into 20,243 validation images and 20,244 testval images. Most images contain multiple objects and significant contextual information, and each image comes with 5 human6Each training image has 5 captions. LM PPLX BLEU METEOR D-ME† 18.1 23.6 22.8 D-LSTM 14.3 22.4 22.6 MRNN 13.2 25.7 22.6 k-Nearest Neighbor - 26.0 22.5 1-Nearest Neighbor - 11.2 17.3 Table 1: Model performance on testval. †: From (Fang et</context>
</contexts>
<marker>Ordonez, Kulkarni, Berg, 2011</marker>
<rawString>Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. 2011. Im2Text: Describing images using 1 million captioned photogrphs. In Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proc. Assoc. for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<contexts>
<context position="11241" citStr="Papineni et al., 2002" startWordPosition="1923" endWordPosition="1926"> of bears walking across a dirt road D-ME+DMSM+MRNN a black bear walking through a wooded area k-NN a black bear that is walking in the woods D-ME+DMSM a gray and white cat sitting on top of it MRNN a cat sitting in front of a mirror D-ME+DMSM+MRNN a close up of a cat looking at the camera k-NN a cat sitting on top of a wooden table Table 2: Example generated captions. annotated captions. The images create a challenging testbed for image captioning and are widely used in recent automatic image captioning work. 3.2 Metrics The quality of generated captions is measured automatically using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). BLEU roughly measures the fraction of Ar-grams (up to 4 grams) that are in common between a hypothesis and one or more references, and penalizes short hypotheses by a brevity penalty term.7 METEOR (Denkowski and Lavie, 2014) measures unigram precision and recall, extending exact word matches to include similar words based on WordNet synonyms and stemmed tokens. We also report the perplexity (PPLX) of studied detectionconditioned LMs. The PPLX is in many ways the natural measure of a statistical LM, but can be loosely correlated with BLEU (Auli et al., 2</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proc. Assoc. for Computational Linguistics (ACL), pages 311– 318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olga Russakovsky</author>
<author>Jia Deng</author>
<author>Hao Su</author>
<author>Jonathan Krause</author>
<author>Sanjeev Satheesh</author>
<author>Sean Ma</author>
<author>Zhiheng Huang</author>
<author>Andrej Karpathy</author>
<author>Aditya Khosla</author>
<author>Michael Bernstein</author>
<author>Alexander C Berg</author>
<author>Li Fei-Fei</author>
</authors>
<title>ImageNet Large Scale Visual Recognition Challenge.</title>
<date>2015</date>
<journal>International Journal of Computer Vision (IJCV).</journal>
<contexts>
<context position="4868" citStr="Russakovsky et al., 2015" startWordPosition="783" endWordPosition="786">. We advance the state-of-the-art BLEU scores on the COCO dataset. 5. We present human evaluation results on the systems with the best performance as measured by automatic metrics. 6. We explore several issues with the statistical models and the underlying COCO dataset, including linguistic irregularities, caption repetition, and data set overlap. 2 Models All language models compared here are trained using output from the same state-of-the-art CNN. The CNN used is the 16-layer variant of VGGNet (Simonyan and Zisserman, 2014) which was initially trained for the ILSVRC2014 classification task (Russakovsky et al., 2015), and then finetuned on the Microsoft COCO data set (Fang et al., 2015; Lin et al., 2014). 2.1 Detector Conditioned Models We study the effect of leveraging an explicit detection step to find key objects/attributes in images before generation, examining both an ME LM approach as reported in previous work (Fang et al., 2015), and a novel LSTM approach introduced here. Both use a CNN trained to output a bag of words indicating the words that are likely to appear in a caption, and both use a beam search to find a top-scoring sentence that contains a subset of the words. This set of words is dynam</context>
</contexts>
<marker>Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, Berg, Fei-Fei, 2015</marker>
<rawString>Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Simonyan</author>
<author>Andrew Zisserman</author>
</authors>
<title>Very deep convolutional networks for large-scale image recognition. arXiv preprint.</title>
<date>2014</date>
<contexts>
<context position="4774" citStr="Simonyan and Zisserman, 2014" startWordPosition="769" endWordPosition="772">E LM approach achieves similar or better performance while generating relatively novel captions. 4. We advance the state-of-the-art BLEU scores on the COCO dataset. 5. We present human evaluation results on the systems with the best performance as measured by automatic metrics. 6. We explore several issues with the statistical models and the underlying COCO dataset, including linguistic irregularities, caption repetition, and data set overlap. 2 Models All language models compared here are trained using output from the same state-of-the-art CNN. The CNN used is the 16-layer variant of VGGNet (Simonyan and Zisserman, 2014) which was initially trained for the ILSVRC2014 classification task (Russakovsky et al., 2015), and then finetuned on the Microsoft COCO data set (Fang et al., 2015; Lin et al., 2014). 2.1 Detector Conditioned Models We study the effect of leveraging an explicit detection step to find key objects/attributes in images before generation, examining both an ME LM approach as reported in previous work (Fang et al., 2015), and a novel LSTM approach introduced here. Both use a CNN trained to output a bag of words indicating the words that are likely to appear in a caption, and both use a beam search </context>
</contexts>
<marker>Simonyan, Zisserman, 2014</marker>
<rawString>Karen Simonyan and Andrew Zisserman. 2014. Very deep convolutional networks for large-scale image recognition. arXiv preprint.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oriol Vinyals</author>
<author>Alexander Toshev</author>
<author>Samy Bengio</author>
<author>Dumitru Erhan</author>
</authors>
<title>Show and tell: a neural image caption generator.</title>
<date>2014</date>
<booktitle>In Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).</booktitle>
<contexts>
<context position="7112" citStr="Vinyals et al. (2014)" startWordPosition="1178" endWordPosition="1181">tributes D \ {h} as a continuous valued auxiliary feature vector (Mikolov and Zweig, 2012). This is encoded as f(sh_1 + E vED\{h} gv + Uqh,D), where sh_1 and gv are respectively the continuous-space representations for last word h−1 and detector v E D \ {h}, U is learned matrix for recurrent histories, and f(·) is the sigmoid transformation. 2.2 Multimodal Recurrent Neural Network In this section, we explore a model directly conditioned on the CNN activations rather than a set of word detections. Our implementation is very similar to captioning models described in Karpathy and Fei-Fei (2015), Vinyals et al. (2014), Mao et al. (2015), and Donahue et al. (2014). This joint vision-language RNN is referred to as a Multimodal Recurrent Neural Network (MRNN). In this model, we feed each image into our CNN and retrieve the 4096-dimensional final hidden layer, denoted as fc7. The fc7 vector is then fed into a hidden layer H to obtain a 500- dimensional representation that serves as the initial hidden state to a gated recurrent neural network (GRNN) (Cho et al., 2014). The GRNN is trained jointly with H to produce the caption one word at a time, conditioned on the previous word and the previous recurrent state.</context>
<context position="16426" citStr="Vinyals et al. (2014)" startWordPosition="2799" endWordPosition="2802">ic relationships correctly. However, the D-ME LM maintains an explicit coverage state vector tracking which attributes have already been emitted. The MRNN implicitly maintains the full state using its recurrent layer, which sometimes results in multiple emission mistakes, where the same attribute is emitted more than once. This is particularly evident when coordination (“and”) is present (examples (4) and (5)). 4.1 Repeated Captions All of our models produce a large number of captions seen in the training and repeated for different images in the test set, as shown in Table 6 (also observed by Vinyals et al. (2014) for their LSTM-based model). There are at least two potential causes for this repetition. 103 D-ME+DMSM MRNN 1 a slice of pizza sitting on top of it a bed with a red blanket on top of it 2 a black and white bird perched on a birthday cake with candles on top top of it of it 3 a little boy that is brushing his a little girl brushing her teeth with a teeth with a toothbrush in her toothbrush mouth 4 a large bed sitting in a bedroom a bedroom with a bed and a bed 5 a man wearing a bow tie a man wearing a tie and a tie Table 5: Example errors in the two basic approaches. Table 6: Percentage uniqu</context>
</contexts>
<marker>Vinyals, Toshev, Bengio, Erhan, 2014</marker>
<rawString>Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2014. Show and tell: a neural image caption generator. In Proc. Conf. Comput. Vision and Pattern Recognition (CVPR).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>