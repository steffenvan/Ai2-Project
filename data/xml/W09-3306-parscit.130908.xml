<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.231083">
<title confidence="0.99152">
Evaluating a Statistical CCG Parser on Wikipedia
</title>
<author confidence="0.998027">
Matthew Honnibal Joel Nothman James R. Curran
</author>
<affiliation confidence="0.995776">
School of Information Technologies
University of Sydney
</affiliation>
<address confidence="0.437371">
NSW 2006, Australia
</address>
<email confidence="0.997809">
{mhonn,joel,james}@it.usyd.edu.au
</email>
<sectionHeader confidence="0.997372" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999945666666667">
The vast majority of parser evaluation is
conducted on the 1984 Wall Street Journal
(WSJ). In-domain evaluation of this kind
is important for system development, but
gives little indication about how the parser
will perform on many practical problems.
Wikipedia is an interesting domain for
parsing that has so far been under-
explored. We present statistical parsing re-
sults that for the first time provide infor-
mation about what sort of performance a
user parsing Wikipedia text can expect.
We find that the C&amp;C parser’s standard
model is 4.3% less accurate on Wikipedia
text, but that a simple self-training ex-
ercise reduces the gap to 3.8%. The
self-training also speeds up the parser on
newswire text by 20%.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999936816666667">
Modern statistical parsers are able to retrieve accu-
rate syntactic analyses for sentences that closely
match the domain of the parser’s training data.
Breaking this domain dependence is now one
of the main challenges for increasing the indus-
trial viability of statistical parsers. Substantial
progress has been made in adapting parsers from
newswire domains to scientific domains, espe-
cially for biomedical literature (Nivre et al., 2007).
However, there is also substantial interest in pars-
ing encyclopedia text, particularly Wikipedia.
Wikipedia has become an influential resource
for NLP for many reasons. In addition to its va-
riety of interesting metadata, it is massive, con-
stantly updated, and multilingual. Wikipedia is
now given its own submission keyword in general
CL conferences, and there are workshops largely
centred around exploiting it and other collabora-
tive semantic resources.
Despite this interest, there have been few in-
vestigations into how accurately existing NLP pro-
cessing tools work on Wikipedia text. If it is found
that Wikipedia text poses new challenges for our
processing tools, then our results will constitute
a baseline for future development. On the other
hand, if we find that models trained on newswire
text perform well, we will have discovered another
interesting way Wikipedia text can be exploited.
This paper presents the first evaluation of a sta-
tistical parser on Wikipedia text. The only pre-
vious published results we are aware of were de-
scribed by Ytrestøl et al. (2009), who ran the
LinGo HPSG parser over Wikipedia, and found
that the correct parse was in the top 500 returned
parses for 60% of sentences. This is an interesting
result, but one that gives little indication of how
well a user could expect a parser to actually anno-
tate Wikipedia text, or how to go about adjusting
one if its performance is inadequate.
To investigate this, we randomly selected 200
sentences from Wikipedia, and hand-labelled them
with CCG annotation in order to evaluate the C&amp;C
parser (Clark and Curran, 2007). C&amp;C is the fastest
deep-grammar parser, making it a likely choice for
parsing Wikipedia, given its size.
Even at the parser’s WSJ speeds, it would
take about 18 days to parse the current English
Wikipedia on a single CPU. We find that the parser
is 54% slower on Wikipedia text, so parsing a full
dump is inconvenient at best. The parser is only
4.3% less accurate, however.
We then examine how these figures might be
improved. We try a simple domain adaptation
experiment, using self-training. One of our ex-
periments, which involves self-training using the
Simple English Wikipedia, improves the accuracy
of the parser’s standard model on Wikipedia by
0.8%. The bootstrapping also makes the parser
faster. Parse speeds on newswire text improve
20%, and speeds on Wikipedia improve by 34%.
</bodyText>
<page confidence="0.994224">
38
</page>
<note confidence="0.990485">
Proceedings of the 2009 Workshop on the People’s Web Meets NLP, ACL-IJCNLP 2009, pages 38–41,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<table confidence="0.99480575">
Corpus Sentences Mean length
WSJ 02-21 39,607 23.5
FEW 889,027 (586,724) 22.4 (16.6)
SEW 224,251 (187,321) 16.5 (14.1)
</table>
<tableCaption confidence="0.999595">
Table 1: Sentence lengths before (and after) length filter.
</tableCaption>
<sectionHeader confidence="0.965956" genericHeader="method">
2 CCG Parsing
</sectionHeader>
<bodyText confidence="0.999152">
Combinatory Categorial Grammar (CCG) (Steed-
man, 2000) is a linguistically motivated grammar
formalism with several advantages for NLP. Like
HPSG, LFG and LTAG, a CCG parse recovers the
semantic structure of a sentence, including long-
range dependencies and complement/adjunct dis-
tinctions, providing substantially more informa-
tion than skeletal brackets.
Clark and Curran (2007) describe how a fast and
accurate CCG parser can be trained from CCGbank
(Hockenmaier and Steedman, 2007). One of the
keys to the system’s success is supertagging (Ban-
galore and Joshi, 1999). Supertagging is the as-
signment of lexical categories before parsing. The
parser is given only tags assigned a high proba-
bility, greatly restricting the search space it must
explore. We use this system, referred to as C&amp;C,
for our parsing experiments.
</bodyText>
<sectionHeader confidence="0.869049" genericHeader="method">
3 Processing Wikipedia Data
</sectionHeader>
<bodyText confidence="0.999955565217391">
We began by processing all articles from the
March 2009 dump of Simple English Wikipedia
(SEW) and the matching Full English Wikipedia
(FEW) articles. SEW is an online encyclopedia
written in basic English. It has stylistic guidelines
that instruct contributors to use basic vocabulary
and syntax, to improve the articles’ readability.
This might make SEW text easier to parse, mak-
ing it useful for our self-training experiments.
mwlib (PediaPress, 2007) was used to parse
the MediaWiki markup. We did not expand tem-
plates, and retained only paragraph text tokenized
according to the WSJ, after it was split into sen-
tences using the NLTK (Loper and Bird, 2002) im-
plementation of Punkt (Kiss and Strunk, 2006) pa-
rameterised on Wikipedia text. Finally, we dis-
carded incorrectly parsed markup and other noise.
We also introduced a sentence length filter for
the domain adaptation data (but not the evaluation
data), discarding sentences longer than 25 words
or shorter than 3 words. The length filter was used
to gather sentences that would be easier to parse.
The effect of this filter is shown in Table 1.
</bodyText>
<sectionHeader confidence="0.988043" genericHeader="method">
4 Self-training Methodology
</sectionHeader>
<bodyText confidence="0.999969729166666">
To investigate how the parser could be improved
on Wikipedia text, we experimented with semi-
supervised learning. We chose a simple method,
self-training. Unlabelled data is annotated by the
system, and the predictions are taken as truth and
integrated into the training system.
Steedman et al. (2003) showed that the selec-
tion of sentences for semi-supervised parsing is
very important. There are two issues: the accu-
racy with which the data can be parsed, which de-
termines how noisy the new training data will be;
and the utility of the examples, which determines
how informative the examples will be.
We experimented with a novel source of data
to balance these two concerns. Simple English
Wikipedia imposes editorial guidelines on the
length and syntactic style authors can use. This
text should be easier to parse, lowering the noise,
but the syntactic restrictions might mean its exam-
ples have lower utility for adapting the parser to
the full English Wikipedia.
We train the C&amp;C supertagger and parser (Clark
and Curran, 2007) on sections 02-21 of the Wall
Street Journal (WSJ) marked up with CCG annota-
tions (Hockenmaier and Steedman, 2007) in the
standard way. We then parse all of the Sim-
ple English Wikipedia remaining after our pre-
processing. We discard the 826 sentences the
parser could not find an analysis for, and set aside
1,486 randomly selected sentences as a future de-
velopment set, leaving a corpus of 185,000 auto-
matically parsed sentences (2.6 million words).
We retrain the supertagger on a simple concate-
nation of the 39,607 WSJ training sentences and
the Wikipedia sentences, and then use it with the
normal-form derivations and hybrid dependencies
model distributed with the parser1.
We repeated our experiments using text from
the full English Wikipedia (FEW) for articles
whose names match an article in SEW. We ran-
domly selected a sample of 185,000 sentences
from these, to match the size of the SEW corpus.
We also performed a set of experiments where
we re-parsed the corpus using the updated su-
pertagger and retrained on output, the logic being
that the updated model might make fewer errors,
producing higher quality training data. This itera-
tive retraining was found to have no effect.
</bodyText>
<footnote confidence="0.989996">
1http://svn.ask.it.usyd.edu.au/trac/candc
</footnote>
<page confidence="0.993697">
39
</page>
<table confidence="0.9999565">
Model P WSJ Section 23 cov P R Wiki 200 cov Wiki 90k
R F speed F speed speed cov
WSJ derivs 85.51 84.62 85.06 545 99.58 81.20 80.51 80.86 394 99.00 239 98.81
SEW derivs 85.06 84.11 84.59 634 99.75 81.96 81.34 81.65 739 99.50 264 99.11
FEW derivs 85.24 84.32 84.78 653 99.79 81.94 81.36 81.65 776 99.50 296 99.15
WSJ hybrid 86.20 84.80 85.50 481 99.58 81.93 80.51 81.22 372 99.00 221 98.81
SEW hybrid 85.80 84.30 85.05 571 99.75 82.16 80.49 81.32 643 99.50 257 99.11
FEW hybrid 85.94 84.46 85.19 577 99.79 82.49 81.03 81.75 665 99.50 275 99.15
</table>
<tableCaption confidence="0.997746">
Table 2: Parsing results with automatic POS tags. SEW and FEW models incorporate self-training.
</tableCaption>
<sectionHeader confidence="0.954902" genericHeader="method">
5 Annotating the Wikipedia Data
</sectionHeader>
<bodyText confidence="0.999927578947368">
We manually annotated a Full English Wikipedia
evaluation set of 200 sentences. The sentences
were sampled at random from the 5000 articles
that were linked to most often by Wikipedia pages.
Articles used for self-training were excluded.
The annotation was conducted by one annota-
tor. First, we parsed the sentences using the C&amp;C
parser. We then manually corrected the supertags,
supplied them back to the parser, and corrected
the parses using a GUI. The interface allowed the
annotator to specify bracket constraints until the
parser selected the correct analysis. The annota-
tion took about 20 hours in total.
We used the CCGbank manual (Hockenmaier
and Steedman, 2005) as the guidelines for our
annotation. There were, however, some system-
atic differences from CCGbank, due to the faulty
noun phrase bracketing and complement/adjunct
distinctions inherited from the Penn Treebank.
</bodyText>
<sectionHeader confidence="0.999966" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.9999438">
The results in this section refer to precision, re-
call and F-Score over labelled CCG dependencies,
which are 5-tuples (head, child, category, slot,
range). Speed is reported as words per second, us-
ing a single core 2.6 GHz Pentium 4 Xeon.
</bodyText>
<subsectionHeader confidence="0.990624">
6.1 Out-of-the-Box Performance
</subsectionHeader>
<bodyText confidence="0.999987159090909">
Our experiments were performed using two mod-
els provided with v1.02 of the C&amp;C parser. The
derivs model is calculated using features from the
Eisner (1996) normal form derivation. This is the
model C&amp;C recommend for general use, because
it is simpler and faster to train. The hybrid model
achieves the best published results for CCG pars-
ing (Clark and Curran, 2007), so we also experi-
mented with this model. The models’ performance
is shown in the WSJ rows of Table 2. We report ac-
curacy using automatic POS tags, since we did not
correct the POS tags in the Wikipedia data.
The derivs and hybrid models show a simi-
lar drop in performance on Wikipedia, of about
4.3%. Since this is the first accuracy evalua-
tion conducted on Wikipedia, it is possible that
Wikipedia data is simply harder to parse, possi-
bly due to its wider vocabulary. It is also possible
that our manual annotation made the task slightly
harder, because we did not reproduce the CCGbank
noun phrase bracketing and complement/adjunct
distinction errors.
We also report the parser’s speed and coverage
on Wikipedia. Since these results do not require
labelled data, we used a sample of 90,000 sen-
tences to obtain more reliable figures. Speeds var-
ied enormously between this sample and the 200
annotated sentences. A length comparison reveals
that our manually annotated sentences are slightly
shorter, with a mean of 20 tokens per sentence.
Shorter sentences are often easier to parse, so this
issue may have affected our accuracy results, too.
The 54% drop in speed on Wikipedia text is ex-
plained by the way the supertagger and parser are
integrated. The supertagger supplies the parser
with a beam of categories. If parsing fails, the
chart is reinitialised with a wider beam and it tries
again. These failures occur more often when the
supertagger cannot produce a high quality tag se-
quence, particularly if the problem is in the tag
dictionary, which constrains the supertagger’s se-
lections for frequent words. This is why we fo-
cused on the supertagger in our domain adaptation
experiments.
</bodyText>
<subsectionHeader confidence="0.999561">
6.2 Domain Adaptation Experiments
</subsectionHeader>
<bodyText confidence="0.999967">
The inclusion of parsed data from Wikipedia ar-
ticles in the supertagger’s training data improves
its accuracy on Wikipedia data, with the FEW en-
hanced model achieving 89.86% accuracy, com-
pared with the original accuracy of 88.77%. The
SEW enhanced supertagger achieved 89.45% ac-
curacy. The derivs model parser improves in ac-
curacy by 0.8%, the hybrid model by 0.5%.
</bodyText>
<page confidence="0.996187">
40
</page>
<bodyText confidence="0.9999835">
The out-of-domain training data had little im-
pact on the models’ accuracy on the WSJ, but
did improve parse speed by 20%, as it did on
Wikipedia. The speed increases because the su-
pertagger’s beam width is decided by its confi-
dence scores, which are more narrowly distributed
after the model has been trained with more data.
After self-training, the derivs and hybrid mod-
els performed equally accurately. With no reason
to use the hybrid model, the total speed increase is
34%. With our pre-processing, the full Wikipedia
dump had close to 1 billion words, so speed is an
important factor.
Overall, our simple self-training experiment
was quite successful. This result may seem sur-
prising given that the CoNLL 2007 participants
generally failed to use similar resources to adapt
dependency parsers to biomedical text (Dredze
et al., 2007). However, our results confirm Rimell
and Clark’s (2009) finding that the C&amp;C parser’s
division of labour between the supertagger and
parser make it easier to adapt to new domains.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999991851851852">
We have presented the first investigation into sta-
tistical parsing on Wikipedia data. The parser’s
accuracy dropped 4.3%, suggesting that the sys-
tem is still useable out-of-the-box. The parser is
also 54% slower on Wikipedia text. Parsing a full
Wikipedia dump would therefore take about 52
days of CPU time using our 5-year-old architec-
ture, which is inconvenient, but manageable over
multiple processors.
Using simple domain adaptation techniques,
we are able to increase the parser’s accuracy on
Wikipedia, with the fastest model improving in ac-
curacy by 0.8%. This closed the gap in accuracy
between the two parser models, removing the need
to use the slower hybrid model. This allowed us to
achieve an overall speed improvement of 34%.
Our results reflect the general trend that
NLP systems perform worse on foreign domains
(Gildea, 2001). Our results also support Rimell
and Clark’s (2009) conclusion that because C&amp;C
is highly lexicalised, domain adaptation is largely
a process of adapting the supertagger.
A particularly promising aspect of these results
is that the parse speeds on the Wall Street Journal
improved, by 15%. This improvement came with
no loss in accuracy, and suggests that further boot-
strapping experiments are likely to be successful.
</bodyText>
<sectionHeader confidence="0.997586" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.9876142">
We would like to thank Stephen Clark and the
anonymous reviewers for their helpful feedback.
Joel was supported by a Capital Markets CRC
PhD scholarship and a University of Sydney Vice-
Chancellor’s Research Scholarship.
</bodyText>
<sectionHeader confidence="0.993331" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.987233037735849">
Srinivas Bangalore and Aravind Joshi. 1999. Supertagging:
An approach to almost parsing. Computational Linguis-
tics, 25(2):237–265.
Stephen Clark and James R. Curran. 2007. Wide-coverage ef-
ficient statistical parsing with CCG and log-linear models.
Computational Linguistics, 33(4):493–552.
Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuzman
Ganchev, Jo˜ao Graca, and Fernando Pereira. 2007. Frus-
tratingly hard domain adaptation for dependency pars-
ing. In Proceedings of the CoNLL Shared Task Session
of EMNLP-CoNLL 2007, pages 1051–1055. ACL, Prague,
Czech Republic.
Jason Eisner. 1996. Efficient normal-form parsing for Com-
binatory Categorial Grammar. In Proceedings of the Asso-
ciation for Computational Linguistics, pages 79–86. Santa
Cruz, CA, USA.
Daniel Gildea. 2001. Corpus variation and parser perfor-
mance. In Proceedings of the EMNLP Conference, pages
167–202. Pittsburgh, PA.
Julia Hockenmaier and Mark Steedman. 2005. CCGbank
manual. Technical Report MS-CIS-05-09, Department of
Computer Science, University of Pennsylvania.
Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a
corpus of CCG derivations and dependency structures ex-
tracted from the Penn Treebank. Computational Linguis-
tics, 33(3):355–396.
Tibor Kiss and Jan Strunk. 2006. Unsupervised multilingual
sentence boundary detection. Computational Linguistics,
32(4):485–525.
Edward Loper and Steven Bird. 2002. NLTK: The natural
language toolkit.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald,
Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007.
The CoNLL 2007 shared task on dependency parsing. In
Proceedings of the CoNLL Shared Task Session, pages
915–932. Prague, Czech Republic.
PediaPress. 2007. mwlib MediaWiki parsing library.
http://code.pediapress.com.
Laura Rimell and Stephen Clark. 2009. Porting a lexicalized-
grammar parser to the biomedical domain. Journal of
Biomedical Informatics. (in press).
Mark Steedman. 2000. The Syntactic Process. The MIT
Press, Cambridge, MA.
Mark Steedman, Rebecca Hwa, Stephen Clark, Miles Os-
borne, Anoop Sarkar, Julia Hockenmaier, Paul Ruhlen,
Steven Baker, and Jeremiah Crim. 2003. Example selec-
tion for bootstrapping statistical parsers. In Proceedings
of HLT-NAACL 2003. Edmonton, Alberta.
Gisle Ytrestøl, Stephan Oepen, and Daniel Flickinger. 2009.
Extracting and annotating Wikipedia sub-domains. In
Proceedings of the 7th International Workshop on Tree-
banks and Linguistic Theories, pages 185–197. Gronin-
gen, Netherlands.
</reference>
<page confidence="0.999447">
41
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.453244">
<title confidence="0.999948">Evaluating a Statistical CCG Parser on Wikipedia</title>
<author confidence="0.999993">Matthew Honnibal Joel Nothman James R Curran</author>
<affiliation confidence="0.98695">School of Information University of</affiliation>
<address confidence="0.514512">NSW 2006,</address>
<abstract confidence="0.992709263157895">The vast majority of parser evaluation is conducted on the 1984 Wall Street Journal In-domain evaluation of this kind is important for system development, but gives little indication about how the parser will perform on many practical problems. Wikipedia is an interesting domain for parsing that has so far been underexplored. We present statistical parsing results that for the first time provide information about what sort of performance a user parsing Wikipedia text can expect. find that the standard model is 4.3% less accurate on Wikipedia text, but that a simple self-training exercise reduces the gap to 3.8%. The self-training also speeds up the parser on newswire text by 20%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Srinivas Bangalore</author>
<author>Aravind Joshi</author>
</authors>
<title>Supertagging: An approach to almost parsing.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="4687" citStr="Bangalore and Joshi, 1999" startWordPosition="740" endWordPosition="744">ce lengths before (and after) length filter. 2 CCG Parsing Combinatory Categorial Grammar (CCG) (Steedman, 2000) is a linguistically motivated grammar formalism with several advantages for NLP. Like HPSG, LFG and LTAG, a CCG parse recovers the semantic structure of a sentence, including longrange dependencies and complement/adjunct distinctions, providing substantially more information than skeletal brackets. Clark and Curran (2007) describe how a fast and accurate CCG parser can be trained from CCGbank (Hockenmaier and Steedman, 2007). One of the keys to the system’s success is supertagging (Bangalore and Joshi, 1999). Supertagging is the assignment of lexical categories before parsing. The parser is given only tags assigned a high probability, greatly restricting the search space it must explore. We use this system, referred to as C&amp;C, for our parsing experiments. 3 Processing Wikipedia Data We began by processing all articles from the March 2009 dump of Simple English Wikipedia (SEW) and the matching Full English Wikipedia (FEW) articles. SEW is an online encyclopedia written in basic English. It has stylistic guidelines that instruct contributors to use basic vocabulary and syntax, to improve the articl</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>Srinivas Bangalore and Aravind Joshi. 1999. Supertagging: An approach to almost parsing. Computational Linguistics, 25(2):237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Wide-coverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="2983" citStr="Clark and Curran, 2007" startWordPosition="470" endWordPosition="473">ext. The only previous published results we are aware of were described by Ytrestøl et al. (2009), who ran the LinGo HPSG parser over Wikipedia, and found that the correct parse was in the top 500 returned parses for 60% of sentences. This is an interesting result, but one that gives little indication of how well a user could expect a parser to actually annotate Wikipedia text, or how to go about adjusting one if its performance is inadequate. To investigate this, we randomly selected 200 sentences from Wikipedia, and hand-labelled them with CCG annotation in order to evaluate the C&amp;C parser (Clark and Curran, 2007). C&amp;C is the fastest deep-grammar parser, making it a likely choice for parsing Wikipedia, given its size. Even at the parser’s WSJ speeds, it would take about 18 days to parse the current English Wikipedia on a single CPU. We find that the parser is 54% slower on Wikipedia text, so parsing a full dump is inconvenient at best. The parser is only 4.3% less accurate, however. We then examine how these figures might be improved. We try a simple domain adaptation experiment, using self-training. One of our experiments, which involves self-training using the Simple English Wikipedia, improves the a</context>
<context position="4497" citStr="Clark and Curran (2007)" startWordPosition="709" endWordPosition="712">ntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP Corpus Sentences Mean length WSJ 02-21 39,607 23.5 FEW 889,027 (586,724) 22.4 (16.6) SEW 224,251 (187,321) 16.5 (14.1) Table 1: Sentence lengths before (and after) length filter. 2 CCG Parsing Combinatory Categorial Grammar (CCG) (Steedman, 2000) is a linguistically motivated grammar formalism with several advantages for NLP. Like HPSG, LFG and LTAG, a CCG parse recovers the semantic structure of a sentence, including longrange dependencies and complement/adjunct distinctions, providing substantially more information than skeletal brackets. Clark and Curran (2007) describe how a fast and accurate CCG parser can be trained from CCGbank (Hockenmaier and Steedman, 2007). One of the keys to the system’s success is supertagging (Bangalore and Joshi, 1999). Supertagging is the assignment of lexical categories before parsing. The parser is given only tags assigned a high probability, greatly restricting the search space it must explore. We use this system, referred to as C&amp;C, for our parsing experiments. 3 Processing Wikipedia Data We began by processing all articles from the March 2009 dump of Simple English Wikipedia (SEW) and the matching Full English Wiki</context>
<context position="7134" citStr="Clark and Curran, 2007" startWordPosition="1140" endWordPosition="1143">he accuracy with which the data can be parsed, which determines how noisy the new training data will be; and the utility of the examples, which determines how informative the examples will be. We experimented with a novel source of data to balance these two concerns. Simple English Wikipedia imposes editorial guidelines on the length and syntactic style authors can use. This text should be easier to parse, lowering the noise, but the syntactic restrictions might mean its examples have lower utility for adapting the parser to the full English Wikipedia. We train the C&amp;C supertagger and parser (Clark and Curran, 2007) on sections 02-21 of the Wall Street Journal (WSJ) marked up with CCG annotations (Hockenmaier and Steedman, 2007) in the standard way. We then parse all of the Simple English Wikipedia remaining after our preprocessing. We discard the 826 sentences the parser could not find an analysis for, and set aside 1,486 randomly selected sentences as a future development set, leaving a corpus of 185,000 automatically parsed sentences (2.6 million words). We retrain the supertagger on a simple concatenation of the 39,607 WSJ training sentences and the Wikipedia sentences, and then use it with the norma</context>
<context position="10568" citStr="Clark and Curran, 2007" startWordPosition="1708" endWordPosition="1711">in this section refer to precision, recall and F-Score over labelled CCG dependencies, which are 5-tuples (head, child, category, slot, range). Speed is reported as words per second, using a single core 2.6 GHz Pentium 4 Xeon. 6.1 Out-of-the-Box Performance Our experiments were performed using two models provided with v1.02 of the C&amp;C parser. The derivs model is calculated using features from the Eisner (1996) normal form derivation. This is the model C&amp;C recommend for general use, because it is simpler and faster to train. The hybrid model achieves the best published results for CCG parsing (Clark and Curran, 2007), so we also experimented with this model. The models’ performance is shown in the WSJ rows of Table 2. We report accuracy using automatic POS tags, since we did not correct the POS tags in the Wikipedia data. The derivs and hybrid models show a similar drop in performance on Wikipedia, of about 4.3%. Since this is the first accuracy evaluation conducted on Wikipedia, it is possible that Wikipedia data is simply harder to parse, possibly due to its wider vocabulary. It is also possible that our manual annotation made the task slightly harder, because we did not reproduce the CCGbank noun phras</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Wide-coverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dredze</author>
<author>John Blitzer</author>
</authors>
<title>Partha Pratim Talukdar, Kuzman Ganchev, Jo˜ao Graca, and Fernando Pereira.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>1051--1055</pages>
<publisher>ACL,</publisher>
<location>Prague, Czech Republic.</location>
<marker>Dredze, Blitzer, 2007</marker>
<rawString>Mark Dredze, John Blitzer, Partha Pratim Talukdar, Kuzman Ganchev, Jo˜ao Graca, and Fernando Pereira. 2007. Frustratingly hard domain adaptation for dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1051–1055. ACL, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Efficient normal-form parsing for Combinatory Categorial Grammar.</title>
<date>1996</date>
<booktitle>In Proceedings of the Association for Computational Linguistics,</booktitle>
<pages>79--86</pages>
<location>Santa Cruz, CA, USA.</location>
<contexts>
<context position="10358" citStr="Eisner (1996)" startWordPosition="1674" endWordPosition="1675">on. There were, however, some systematic differences from CCGbank, due to the faulty noun phrase bracketing and complement/adjunct distinctions inherited from the Penn Treebank. 6 Results The results in this section refer to precision, recall and F-Score over labelled CCG dependencies, which are 5-tuples (head, child, category, slot, range). Speed is reported as words per second, using a single core 2.6 GHz Pentium 4 Xeon. 6.1 Out-of-the-Box Performance Our experiments were performed using two models provided with v1.02 of the C&amp;C parser. The derivs model is calculated using features from the Eisner (1996) normal form derivation. This is the model C&amp;C recommend for general use, because it is simpler and faster to train. The hybrid model achieves the best published results for CCG parsing (Clark and Curran, 2007), so we also experimented with this model. The models’ performance is shown in the WSJ rows of Table 2. We report accuracy using automatic POS tags, since we did not correct the POS tags in the Wikipedia data. The derivs and hybrid models show a similar drop in performance on Wikipedia, of about 4.3%. Since this is the first accuracy evaluation conducted on Wikipedia, it is possible that</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Efficient normal-form parsing for Combinatory Categorial Grammar. In Proceedings of the Association for Computational Linguistics, pages 79–86. Santa Cruz, CA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of the EMNLP Conference,</booktitle>
<pages>167--202</pages>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="14542" citStr="Gildea, 2001" startWordPosition="2365" endWordPosition="2366"> Parsing a full Wikipedia dump would therefore take about 52 days of CPU time using our 5-year-old architecture, which is inconvenient, but manageable over multiple processors. Using simple domain adaptation techniques, we are able to increase the parser’s accuracy on Wikipedia, with the fastest model improving in accuracy by 0.8%. This closed the gap in accuracy between the two parser models, removing the need to use the slower hybrid model. This allowed us to achieve an overall speed improvement of 34%. Our results reflect the general trend that NLP systems perform worse on foreign domains (Gildea, 2001). Our results also support Rimell and Clark’s (2009) conclusion that because C&amp;C is highly lexicalised, domain adaptation is largely a process of adapting the supertagger. A particularly promising aspect of these results is that the parse speeds on the Wall Street Journal improved, by 15%. This improvement came with no loss in accuracy, and suggests that further bootstrapping experiments are likely to be successful. 8 Acknowledgements We would like to thank Stephen Clark and the anonymous reviewers for their helpful feedback. Joel was supported by a Capital Markets CRC PhD scholarship and a Un</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of the EMNLP Conference, pages 167–202. Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank manual.</title>
<date>2005</date>
<tech>Technical Report MS-CIS-05-09,</tech>
<institution>Department of Computer Science, University of Pennsylvania.</institution>
<contexts>
<context position="9710" citStr="Hockenmaier and Steedman, 2005" startWordPosition="1570" endWordPosition="1573">ipedia evaluation set of 200 sentences. The sentences were sampled at random from the 5000 articles that were linked to most often by Wikipedia pages. Articles used for self-training were excluded. The annotation was conducted by one annotator. First, we parsed the sentences using the C&amp;C parser. We then manually corrected the supertags, supplied them back to the parser, and corrected the parses using a GUI. The interface allowed the annotator to specify bracket constraints until the parser selected the correct analysis. The annotation took about 20 hours in total. We used the CCGbank manual (Hockenmaier and Steedman, 2005) as the guidelines for our annotation. There were, however, some systematic differences from CCGbank, due to the faulty noun phrase bracketing and complement/adjunct distinctions inherited from the Penn Treebank. 6 Results The results in this section refer to precision, recall and F-Score over labelled CCG dependencies, which are 5-tuples (head, child, category, slot, range). Speed is reported as words per second, using a single core 2.6 GHz Pentium 4 Xeon. 6.1 Out-of-the-Box Performance Our experiments were performed using two models provided with v1.02 of the C&amp;C parser. The derivs model is </context>
</contexts>
<marker>Hockenmaier, Steedman, 2005</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2005. CCGbank manual. Technical Report MS-CIS-05-09, Department of Computer Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julia Hockenmaier</author>
<author>Mark Steedman</author>
</authors>
<title>CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>3</issue>
<contexts>
<context position="4602" citStr="Hockenmaier and Steedman, 2007" startWordPosition="726" endWordPosition="729">7 23.5 FEW 889,027 (586,724) 22.4 (16.6) SEW 224,251 (187,321) 16.5 (14.1) Table 1: Sentence lengths before (and after) length filter. 2 CCG Parsing Combinatory Categorial Grammar (CCG) (Steedman, 2000) is a linguistically motivated grammar formalism with several advantages for NLP. Like HPSG, LFG and LTAG, a CCG parse recovers the semantic structure of a sentence, including longrange dependencies and complement/adjunct distinctions, providing substantially more information than skeletal brackets. Clark and Curran (2007) describe how a fast and accurate CCG parser can be trained from CCGbank (Hockenmaier and Steedman, 2007). One of the keys to the system’s success is supertagging (Bangalore and Joshi, 1999). Supertagging is the assignment of lexical categories before parsing. The parser is given only tags assigned a high probability, greatly restricting the search space it must explore. We use this system, referred to as C&amp;C, for our parsing experiments. 3 Processing Wikipedia Data We began by processing all articles from the March 2009 dump of Simple English Wikipedia (SEW) and the matching Full English Wikipedia (FEW) articles. SEW is an online encyclopedia written in basic English. It has stylistic guidelines</context>
<context position="7249" citStr="Hockenmaier and Steedman, 2007" startWordPosition="1159" endWordPosition="1162">d the utility of the examples, which determines how informative the examples will be. We experimented with a novel source of data to balance these two concerns. Simple English Wikipedia imposes editorial guidelines on the length and syntactic style authors can use. This text should be easier to parse, lowering the noise, but the syntactic restrictions might mean its examples have lower utility for adapting the parser to the full English Wikipedia. We train the C&amp;C supertagger and parser (Clark and Curran, 2007) on sections 02-21 of the Wall Street Journal (WSJ) marked up with CCG annotations (Hockenmaier and Steedman, 2007) in the standard way. We then parse all of the Simple English Wikipedia remaining after our preprocessing. We discard the 826 sentences the parser could not find an analysis for, and set aside 1,486 randomly selected sentences as a future development set, leaving a corpus of 185,000 automatically parsed sentences (2.6 million words). We retrain the supertagger on a simple concatenation of the 39,607 WSJ training sentences and the Wikipedia sentences, and then use it with the normal-form derivations and hybrid dependencies model distributed with the parser1. We repeated our experiments using te</context>
</contexts>
<marker>Hockenmaier, Steedman, 2007</marker>
<rawString>Julia Hockenmaier and Mark Steedman. 2007. CCGbank: a corpus of CCG derivations and dependency structures extracted from the Penn Treebank. Computational Linguistics, 33(3):355–396.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tibor Kiss</author>
<author>Jan Strunk</author>
</authors>
<title>Unsupervised multilingual sentence boundary detection.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<contexts>
<context position="5676" citStr="Kiss and Strunk, 2006" startWordPosition="901" endWordPosition="904">ia (SEW) and the matching Full English Wikipedia (FEW) articles. SEW is an online encyclopedia written in basic English. It has stylistic guidelines that instruct contributors to use basic vocabulary and syntax, to improve the articles’ readability. This might make SEW text easier to parse, making it useful for our self-training experiments. mwlib (PediaPress, 2007) was used to parse the MediaWiki markup. We did not expand templates, and retained only paragraph text tokenized according to the WSJ, after it was split into sentences using the NLTK (Loper and Bird, 2002) implementation of Punkt (Kiss and Strunk, 2006) parameterised on Wikipedia text. Finally, we discarded incorrectly parsed markup and other noise. We also introduced a sentence length filter for the domain adaptation data (but not the evaluation data), discarding sentences longer than 25 words or shorter than 3 words. The length filter was used to gather sentences that would be easier to parse. The effect of this filter is shown in Table 1. 4 Self-training Methodology To investigate how the parser could be improved on Wikipedia text, we experimented with semisupervised learning. We chose a simple method, self-training. Unlabelled data is an</context>
</contexts>
<marker>Kiss, Strunk, 2006</marker>
<rawString>Tibor Kiss and Jan Strunk. 2006. Unsupervised multilingual sentence boundary detection. Computational Linguistics, 32(4):485–525.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>NLTK: The natural language toolkit.</title>
<date>2002</date>
<contexts>
<context position="5628" citStr="Loper and Bird, 2002" startWordPosition="893" endWordPosition="896">m the March 2009 dump of Simple English Wikipedia (SEW) and the matching Full English Wikipedia (FEW) articles. SEW is an online encyclopedia written in basic English. It has stylistic guidelines that instruct contributors to use basic vocabulary and syntax, to improve the articles’ readability. This might make SEW text easier to parse, making it useful for our self-training experiments. mwlib (PediaPress, 2007) was used to parse the MediaWiki markup. We did not expand templates, and retained only paragraph text tokenized according to the WSJ, after it was split into sentences using the NLTK (Loper and Bird, 2002) implementation of Punkt (Kiss and Strunk, 2006) parameterised on Wikipedia text. Finally, we discarded incorrectly parsed markup and other noise. We also introduced a sentence length filter for the domain adaptation data (but not the evaluation data), discarding sentences longer than 25 words or shorter than 3 words. The length filter was used to gather sentences that would be easier to parse. The effect of this filter is shown in Table 1. 4 Self-training Methodology To investigate how the parser could be improved on Wikipedia text, we experimented with semisupervised learning. We chose a sim</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. NLTK: The natural language toolkit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session,</booktitle>
<pages>915--932</pages>
<location>Prague, Czech Republic.</location>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proceedings of the CoNLL Shared Task Session, pages 915–932. Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>PediaPress</author>
</authors>
<title>mwlib MediaWiki parsing library.</title>
<date>2007</date>
<note>http://code.pediapress.com.</note>
<contexts>
<context position="5422" citStr="PediaPress, 2007" startWordPosition="858" endWordPosition="859">robability, greatly restricting the search space it must explore. We use this system, referred to as C&amp;C, for our parsing experiments. 3 Processing Wikipedia Data We began by processing all articles from the March 2009 dump of Simple English Wikipedia (SEW) and the matching Full English Wikipedia (FEW) articles. SEW is an online encyclopedia written in basic English. It has stylistic guidelines that instruct contributors to use basic vocabulary and syntax, to improve the articles’ readability. This might make SEW text easier to parse, making it useful for our self-training experiments. mwlib (PediaPress, 2007) was used to parse the MediaWiki markup. We did not expand templates, and retained only paragraph text tokenized according to the WSJ, after it was split into sentences using the NLTK (Loper and Bird, 2002) implementation of Punkt (Kiss and Strunk, 2006) parameterised on Wikipedia text. Finally, we discarded incorrectly parsed markup and other noise. We also introduced a sentence length filter for the domain adaptation data (but not the evaluation data), discarding sentences longer than 25 words or shorter than 3 words. The length filter was used to gather sentences that would be easier to par</context>
</contexts>
<marker>PediaPress, 2007</marker>
<rawString>PediaPress. 2007. mwlib MediaWiki parsing library. http://code.pediapress.com.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Porting a lexicalizedgrammar parser to the biomedical domain.</title>
<date>2009</date>
<journal>Journal of Biomedical Informatics.</journal>
<note>(in press).</note>
<marker>Rimell, Clark, 2009</marker>
<rawString>Laura Rimell and Stephen Clark. 2009. Porting a lexicalizedgrammar parser to the biomedical domain. Journal of Biomedical Informatics. (in press).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4173" citStr="Steedman, 2000" startWordPosition="664" endWordPosition="666">Wikipedia, improves the accuracy of the parser’s standard model on Wikipedia by 0.8%. The bootstrapping also makes the parser faster. Parse speeds on newswire text improve 20%, and speeds on Wikipedia improve by 34%. 38 Proceedings of the 2009 Workshop on the People’s Web Meets NLP, ACL-IJCNLP 2009, pages 38–41, Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP Corpus Sentences Mean length WSJ 02-21 39,607 23.5 FEW 889,027 (586,724) 22.4 (16.6) SEW 224,251 (187,321) 16.5 (14.1) Table 1: Sentence lengths before (and after) length filter. 2 CCG Parsing Combinatory Categorial Grammar (CCG) (Steedman, 2000) is a linguistically motivated grammar formalism with several advantages for NLP. Like HPSG, LFG and LTAG, a CCG parse recovers the semantic structure of a sentence, including longrange dependencies and complement/adjunct distinctions, providing substantially more information than skeletal brackets. Clark and Curran (2007) describe how a fast and accurate CCG parser can be trained from CCGbank (Hockenmaier and Steedman, 2007). One of the keys to the system’s success is supertagging (Bangalore and Joshi, 1999). Supertagging is the assignment of lexical categories before parsing. The parser is g</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. The MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
<author>Rebecca Hwa</author>
<author>Stephen Clark</author>
<author>Miles Osborne</author>
<author>Anoop Sarkar</author>
<author>Julia Hockenmaier</author>
<author>Paul Ruhlen</author>
<author>Steven Baker</author>
<author>Jeremiah Crim</author>
</authors>
<title>Example selection for bootstrapping statistical parsers.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003.</booktitle>
<location>Edmonton, Alberta.</location>
<contexts>
<context position="6401" citStr="Steedman et al. (2003)" startWordPosition="1018" endWordPosition="1021">lso introduced a sentence length filter for the domain adaptation data (but not the evaluation data), discarding sentences longer than 25 words or shorter than 3 words. The length filter was used to gather sentences that would be easier to parse. The effect of this filter is shown in Table 1. 4 Self-training Methodology To investigate how the parser could be improved on Wikipedia text, we experimented with semisupervised learning. We chose a simple method, self-training. Unlabelled data is annotated by the system, and the predictions are taken as truth and integrated into the training system. Steedman et al. (2003) showed that the selection of sentences for semi-supervised parsing is very important. There are two issues: the accuracy with which the data can be parsed, which determines how noisy the new training data will be; and the utility of the examples, which determines how informative the examples will be. We experimented with a novel source of data to balance these two concerns. Simple English Wikipedia imposes editorial guidelines on the length and syntactic style authors can use. This text should be easier to parse, lowering the noise, but the syntactic restrictions might mean its examples have </context>
</contexts>
<marker>Steedman, Hwa, Clark, Osborne, Sarkar, Hockenmaier, Ruhlen, Baker, Crim, 2003</marker>
<rawString>Mark Steedman, Rebecca Hwa, Stephen Clark, Miles Osborne, Anoop Sarkar, Julia Hockenmaier, Paul Ruhlen, Steven Baker, and Jeremiah Crim. 2003. Example selection for bootstrapping statistical parsers. In Proceedings of HLT-NAACL 2003. Edmonton, Alberta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gisle Ytrestøl</author>
<author>Stephan Oepen</author>
<author>Daniel Flickinger</author>
</authors>
<title>Extracting and annotating Wikipedia sub-domains.</title>
<date>2009</date>
<booktitle>In Proceedings of the 7th International Workshop on Treebanks and Linguistic Theories,</booktitle>
<pages>185--197</pages>
<location>Groningen, Netherlands.</location>
<contexts>
<context position="2457" citStr="Ytrestøl et al. (2009)" startWordPosition="380" endWordPosition="383">. Despite this interest, there have been few investigations into how accurately existing NLP processing tools work on Wikipedia text. If it is found that Wikipedia text poses new challenges for our processing tools, then our results will constitute a baseline for future development. On the other hand, if we find that models trained on newswire text perform well, we will have discovered another interesting way Wikipedia text can be exploited. This paper presents the first evaluation of a statistical parser on Wikipedia text. The only previous published results we are aware of were described by Ytrestøl et al. (2009), who ran the LinGo HPSG parser over Wikipedia, and found that the correct parse was in the top 500 returned parses for 60% of sentences. This is an interesting result, but one that gives little indication of how well a user could expect a parser to actually annotate Wikipedia text, or how to go about adjusting one if its performance is inadequate. To investigate this, we randomly selected 200 sentences from Wikipedia, and hand-labelled them with CCG annotation in order to evaluate the C&amp;C parser (Clark and Curran, 2007). C&amp;C is the fastest deep-grammar parser, making it a likely choice for pa</context>
</contexts>
<marker>Ytrestøl, Oepen, Flickinger, 2009</marker>
<rawString>Gisle Ytrestøl, Stephan Oepen, and Daniel Flickinger. 2009. Extracting and annotating Wikipedia sub-domains. In Proceedings of the 7th International Workshop on Treebanks and Linguistic Theories, pages 185–197. Groningen, Netherlands.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>