<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.120786">
<title confidence="0.998861">
A Japanese corpus of referring expressions used in a situated
collaboration task
</title>
<author confidence="0.989915">
Philipp Spanger Yasuhara Masaaki Iida Ryu Tokunaga Takenobu
</author>
<affiliation confidence="0.999601">
Department of Computer Science
Tokyo Institute of Technology
</affiliation>
<email confidence="0.987324">
{philipp, yasuhara, ryu-i, take}@cl.cs.titech.ac.jp
</email>
<sectionHeader confidence="0.9935" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998021">
In order to pursue research on generating
referring expressions in a situated collab-
oration task, we set up a data-collection
experiment based on the Tangram puzzle.
For a pair of participants we recorded ev-
ery utterance in synchronisation with the
current state of the puzzle as well as all
operations by the participants. Referring
expressions were annotated with their ref-
erents in order to build a referring expres-
sion corpus in Japanese. We provide pre-
liminary results on the analysis of the cor-
pus from various standpoints, focussing on
action-mentioning expressions.
</bodyText>
<sectionHeader confidence="0.998798" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985224803030303">
Referring expressions are a linguistic device to re-
fer to a certain object, enabling smooth collabo-
ration between humans and agents where physical
operations are involved. Previous research often
either selectively focussed only on a limited num-
ber of expression-types or set up overly controlled
experiments. In contrast, we intend to work to-
wards analysing the whole breadth of referring ex-
pressions in a situated domain. For this purpose
we created a corpus (in Japanese) and analysed it
from various standpoints.
From very early on in referring expression re-
search, there has been some interest in the col-
laborative aspect of the reference process (Clark
and Wilkes-Gibbs, 1986). This has more recently
developed into creating situated corpora in order
to analyse the referring expressions occurring in
situated collaborative tasks. The COCONUT cor-
pus (Di Eugenio et al., 2000) is collected from
keyboard-input dialogues between two partici-
pants who are collaboratively working on a sim-
ple 2-D design task (buying and arranging furni-
ture for two rooms). In contrast, the QUAKE cor-
pus (Byron et al., 2005) – as well as the more re-
cent SCARE corpus (Stoia et al., 2008), which is
an extension of QUAKE – is based on an interac-
tion captured in a 3-D virtual reality (VR) world
where two participants collaboratively carry out
a treasure hunting task. There has been ongoing
work to exploit these two resources for research on
different aspects of referring expressions (Pamela
W. Jordan, 2005; Byron, 2005).
However, while these resources have inspired
new research into different aspects of referring ex-
pressions, at the same time they have clear limi-
tations. The COCONUT corpus is collected from
dialogues in which participants refer to symbol-
like objects in a 2-D world. It thus resem-
bles the more recent (non-collaborative) TUNA-
corpus (van Deemter, 2007) in tending to en-
courage very simple types of expressions. Fur-
thermore, limiting participants’ interaction to key-
board input makes the dialogue less natural. While
the QUAKE corpus deals with a more complex do-
main (3-D virtual world), the participating sub-
jects were only able to carry out limited kinds of
actions (pushing buttons, picking up or dropping
objects) as compared with the complexity of the
three-dimensional target domain.
In contrast to these two corpora, we set up a
comparatively simple collaborative task (Tangram
Puzzle) allowing participants to freely communi-
cate via speech and to perform actions various
enough to accomplish the given task, e.g. pick-
ing, moving, turning and rotating pieces. All ut-
terances by participants were recorded in synchro-
nisation with operations on objects and the object
arrangement. The utterances were transcribed and
all referring expressions found were annotated to-
gether with their referents. Thus, this corpus al-
lows us to study in detail human-human interac-
tion, particularly referring expressions in a situ-
ated setting. In what follows, we first describe de-
tails of the building of the corpus and then provide
Proceedings of the 12th European Workshop on Natural Language Generation, pages 110–113,
Athens, Greece, 30 – 31 March 2009. c�2009 Association for Computational Linguistics
</bodyText>
<page confidence="0.997127">
110
</page>
<bodyText confidence="0.92381">
results of our preliminary analysis. This analysis
reveals a novel type of referring expression men-
tioning an action on objects, which we call action-
mentioning expressions.
2 Building the corpus
</bodyText>
<figureCaption confidence="0.999167">
Figure 1: Screenshot of the Tangram simulator
</figureCaption>
<subsectionHeader confidence="0.991878">
2.1 Experimental setting
</subsectionHeader>
<bodyText confidence="0.999986">
We recruited 12 Japanese graduate students (4 fe-
males, 8 males) and split them into 6 pairs. Each
pair was instructed to solve the Tangram puzzle
(an ancient Chinese geometrical puzzle) coopera-
tively. The goal of Tangram is to construct a given
shape by arranging seven pieces of simple figures
as shown in Figure 1.
In order to record detailed information of the
interaction (position of pieces, participants’ ac-
tions), we implemented a Tangram simulator in
which the pieces on the computer display can be
moved, rotated and flipped with simple mouse op-
erations. Figure 1 shows the simulator interface in
which the left shows the goal shape area and the
right the working area. We assigned two differ-
ent roles to participants, a solver and an operator;
the solver thinks of the arrangement of the pieces
to make the goal shape and gives instructions to
the operator, while the operator manipulates the
pieces with the mouse according to the solver’s in-
structions.
A solver and an operator sit side by side in front
of their own computer display. Both participants
share the same working area of the simulator. The
operator can manipulate the pieces, but cannot see
the goal shape. In contrast, the solver sees the goal
shape but cannot move pieces. A shield screen was
set between the participants in order to prevent
them from peeking at their partner’s display. In
this asymmetrical interaction, we can expect many
referring expressions during the interaction.
Each pair is assigned four exercises and the par-
ticipants exchanged roles after two exercises. We
set a time limit of 15 minutes for an exercise.
Utterances by the participants are recorded sep-
arately in stereo through headset microphones in
synchronisation with the position of the pieces and
the mouse actions. In total, we collected 24 dia-
logues of about four hours. The average length of
a dialogue was 10 minutes 43 seconds.
</bodyText>
<subsectionHeader confidence="0.999142">
2.2 Annotation
</subsectionHeader>
<bodyText confidence="0.999814083333333">
Recorded dialogues were transcribed with a time
code attached to each utterance. Since our main
concern is collecting referring expressions, we de-
fined an utterance to be a complete sentence to
prevent a referring expression being split into sev-
eral utterances. Referring expressions were an-
notated together with their referents by using the
multi-purpose annotation tool SLAT (Noguchi et
al., 2008). Two annotators (two of the authors) an-
notated four dialogue texts separately. We anno-
tated all 24 dialogue texts and corrected discrep-
ancies by discussion between the annotators.
</bodyText>
<sectionHeader confidence="0.766441" genericHeader="method">
3 Analysis of the corpus
</sectionHeader>
<bodyText confidence="0.999988083333333">
We collected a total of 1,509 tokens and 449 types
of referring expressions in 24 dialogues. Our
asymmetric experimental setting tended to encour-
age referring expressions from the solver, while
the operator was constrained to confirming his un-
derstanding of the solver’s instructions. This is re-
flected in the number of referring expressions by
the solver (1,287) largely outnumbering those of
the operator (222). There are a number of expres-
sions (215 expressions; 15% of the total) referring
to multiple objects (referring to 2 or more pieces)
and we excluded them from our current analysis.
We exclusively deal here with expressions refer-
ring to a specific single piece or indefinite expres-
sions, i.e. those that have no definite referent (in
total 1,294 tokens).
We found the following syntactic/semantic fea-
tures used in the expressions: i) demonstratives
(adjectives and pronouns), ii) object attribute-
values, iii) spatial relations, iv) actions on an ob-
ject and v) others. The number of these features is
summarised in Table 1. (Note that multiple fea-
tures can be used in a single expression.) The
right-most column shows an example with its En-
</bodyText>
<page confidence="0.998911">
111
</page>
<tableCaption confidence="0.995068">
Table 1: Features of referring expressions
</tableCaption>
<table confidence="0.9908605">
Feature types tokens Example
demonstrative 118 745
adjective 100 196 “ano migigawa no sankakkei (that triangle at the right side)”
pronoun 19 551 “kore (this)”
attribute 303 641
size 165 267 “tittyai sankakkei (the small triangle)”
shape 271 605 “ˆokii sankakkei (the large triangle)”
direction 6 6 “ano sita muiteru dekai sankakkei (that large triangle facing to the bottom)”
spatial relations 129 148
projective 125 144 “hidari no okkii sankakkei (the small triangle on the left)”
topological 2 2 “ˆokii hanareteiru yatu (the big distant one)”
overlapping 2 2 “sono sita ni aru sankakkei (the triangle underneath it)”
action-mentioning 78 85 “migi ue ni doketa sankakkei (the triangle you put away to the top right)”
others 29 30
remaining 15 15 “nokotteiru ˆokii sankakkei (the remaining large triangle)”
similarity 14 15 “sore to onazi katati no (the one of the same shape as that one)”
</table>
<bodyText confidence="0.990023230769231">
glish translation. The identified feature in the re-
ferring expression is underlined.
We note here a tendency to employ object at-
tributes, particularly the attribute “shape” as well
as use of demonstratives, particularly demonstra-
tive pronouns. These kinds of referring expres-
sions are quite general and appear in a variety of
other non-situated settings as well. In addition,
we found another kind of expression not usually
employed by humans outside of situated collabo-
ration tasks; referring expressions mentioning an
action on an object. We have 85 expressions (over
6% of the total) of this type in our corpus.
</bodyText>
<sectionHeader confidence="0.995494" genericHeader="method">
4 Action-mentioning expressions
</sectionHeader>
<bodyText confidence="0.999964127272727">
We further analysed those expressions that men-
tion an action on an object, which we call action-
mentioning expressions hereafter. Although there
was significant variation in usage of action-
mentioning expressions among the pairs, all 6
pairs of participants used at least one action-
mentioning expression, indicating that it is a fun-
damental type of expression for this task set-
ting. Action-mentioning expressions are different
from haptic-ostensive referring expressions (Fos-
ter et al., 2008) since action-mentioning expres-
sions are not necessarily accompanied by simulta-
neous physical operation on an object.
Action-mentioning expressions can be again di-
vided into three categories: i) combination of a
temporal adverbial with a verb indicating an ac-
tion (“turned”, “put”, “moved”, etc) (55 tokens or
about 65% of action-mentioning expression), ii)
use of temporal adverbials without a verb, i.e. verb
ellipsis (22 tokens or about 26%) and iii) expres-
sions with a verb without temporal adverbials (8
tokens or about 9%). The second category includ-
ing verb ellipsis would be rare in English, but it is
quite natural in Japanese.
Only less than 10% of this kind of expression
did not include any temporal adverbial, indicating
that humans tend to describe the temporal aspect
of an action. This needs to be integrated into any
generation algorithm for this task domain. The
temporal adverbials used by the participants were
the Japanese “sakki no NP (the NP [verb-ed] just
before)” or “ima no NP (the current NP/the NP
[you are verb-ing] now/the NP [verb-ed] just be-
fore)”. “Ima” generally refers to the current time
point (“now”). It can, however, refer to a past time
point as well, thus it is ambiguous.
Participants tended to use “ima” largely in its
perfect meaning (completed action). The fre-
quency of use of “ima” in its perfect meaning in
comparison to its progressive meaning was about
2:1. The distribution of the two types of tempo-
ral adverbials “sakki” and “ima” was about 2:3.
The slight preference here for “ima” might be ex-
plained by its dual meanings (progressive and per-
fect) in contrast to the exclusive use of “sakki” for
past actions.
Figure 2 shows the distribution of “sakki (just
before)” and past-cases of “ima (now)” dependent
on the time-distance to the action they refer to. For
actions occurring within a timeframe of about 10
seconds previous to uttering an expression, partic-
ipants had an overwhelming preference for “ima”.
The frequency of “ima” decreases quickly for ac-
tions that occurred 10-20 seconds prior to the ut-
terance. In contrast, after 20 seconds from the ac-
</bodyText>
<page confidence="0.989994">
112
</page>
<figure confidence="0.872395">
time (sec)
</figure>
<figureCaption confidence="0.9990425">
Figure 2: Frequency of “sakki” and “ima” over the
time-distance to referenced action
</figureCaption>
<bodyText confidence="0.994542">
tion, participants prefered “sakki”.
In addition, we investigated what actions oc-
curred in between the utterance and the action
mentioned. The actions we take into account here
are basic manipulations of an object like “move”,
“flip”, “click” and so on. Referring to an immedi-
ately preceding action, participants had a strong
preference for using “ima”. Interestingly, with
only one other action in between, the participants’
preference becomes opposite (i.e. “sakki” is pre-
ferred.). For referring to actions further in the past
(i.e. more actions in between), there was a con-
tinous preference for “sakki” over “ima”. Further
analysis should also investigate the phenomenon
of the difference in use of temporal adverbials for
other languages and whether this is related to char-
acteristics of the Japanese language or rather an in-
herent property of the use of temporal adverbials
in natural language.
</bodyText>
<sectionHeader confidence="0.974184" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999990894736842">
We collected a corpus of Japanese referring ex-
pressions as a first step towards developing algo-
rithms for generating referring expressions in a sit-
uated collaboration. We carried out an initial anal-
ysis of the collected expressions, focussing on ex-
pressions that include a mention of an action on
an object. We noted that they are often combined
with temporal adverbials with participants seek-
ing to make a temporal ordering of actions. In
addition, we intend to further analyse other types
of expressisons (demonstratives, etc) and work on
developing generation algorithms for this domain.
In future work, we intend to generalise this exper-
iment in the Tangram-domain to other domains.
Furthermore, information such as gestures and eye
movements should be incorporated in data collec-
tion. This will lay the basis for the development of
more general models for the generation of refer-
ring expressions in a situated collaborative task.
</bodyText>
<sectionHeader confidence="0.998531" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998843976190476">
Donna Byron, Thomas Mampilly, Vinay Sharma, and
Tianfang Xu. 2005. Utilizing visual attention for
cross-modal coreference interpretation. In CON-
TEXT 2005, pages 83–96.
Donna K. Byron. 2005. The OSU Quake 2004 cor-
pus of two-party situated problem-solving dialogs.
Technical report, Department of Computer Science
and Enginerring, The Ohio State University.
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as
a collaborative process. Cognition, 22:1–39.
B. Di Eugenio, P. W. Jordan, R. H. Thomason, and J. D
Moore. 2000. The agreement process: An empirical
investigation of human-human computer-mediated
collaborative dialogues. International Journal of
Human-Computer Studies, 53(6):1017–1076.
Mary Ellen Foster, Ellen Gurman Bard, Markus Guhe,
Robin L. Hill, Jon Oberlander, and Alois Knoll.
2008. The roles of haptic-ostensive referring expres-
sions in cooperative, task-based human-robot dia-
logue. In Proceedings of 3rd Human-Robot Inter-
action, pages 295–302.
Masaki Noguchi, Kenta Miyoshi, Takenobu Tokunaga,
Ryu Iida, Mamoru Komachi, and Kentaro Inui.
2008. Multiple purpose annotation using SLAT –
Segment and link-based annotation tool. In Pro-
ceedings of 2nd Linguistic Annotation Workshop,
pages 61–64.
Marilyn A. Walker Pamela W. Jordan. 2005. Learning
content selection rules for generating object descrip-
tions in dialogue. Journal of Artificial Intelligence
Research, 24:157–194.
Laura Stoia, Darla Magdalene Shockley, Donna K. By-
ron, and Eric Fosler-Lussier. 2008. SCARE: A sit-
uated corpus with annotated referring expressions.
In Proceedings of the Sixth International Confer-
ence on Language Resources and Evaluation (LREC
2008).
Kees van Deemter. 2007. TUNA: Towards a UNified
Algorithm for the generation of referring expres-
sions. Technical report, Aberdeen University.
www.csd.abdn.ac.uk/research/tuna/pubs/TUNA-
final-report.pdf.
</reference>
<figure confidence="0.993849">
&amp;quot;sakki (just before)&amp;quot;
&amp;quot;ima (now)&amp;quot;
frequency 16
12
8
4
0
0^10
10^20
20^30
30^40
40^50
50^60
60^70
70^80
80^90
90^100
100^
</figure>
<page confidence="0.987608">
113
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.597795">
<title confidence="0.9868535">A Japanese corpus of referring expressions used in a collaboration task</title>
<author confidence="0.99983">Philipp Spanger Yasuhara Masaaki Iida Ryu Tokunaga</author>
<affiliation confidence="0.99032">Department of Computer Tokyo Institute of</affiliation>
<email confidence="0.908644">yasuhara,ryu-i,</email>
<abstract confidence="0.999838307692308">In order to pursue research on generating referring expressions in a situated collaboration task, we set up a data-collection experiment based on the Tangram puzzle. For a pair of participants we recorded every utterance in synchronisation with the current state of the puzzle as well as all operations by the participants. Referring expressions were annotated with their referents in order to build a referring expression corpus in Japanese. We provide preliminary results on the analysis of the cor-</abstract>
<intro confidence="0.68795">pus from various standpoints, focussing on</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Donna Byron</author>
<author>Thomas Mampilly</author>
<author>Vinay Sharma</author>
<author>Tianfang Xu</author>
</authors>
<title>Utilizing visual attention for cross-modal coreference interpretation. In CONTEXT</title>
<date>2005</date>
<pages>83--96</pages>
<contexts>
<context position="1960" citStr="Byron et al., 2005" startWordPosition="298" endWordPosition="301">t from various standpoints. From very early on in referring expression research, there has been some interest in the collaborative aspect of the reference process (Clark and Wilkes-Gibbs, 1986). This has more recently developed into creating situated corpora in order to analyse the referring expressions occurring in situated collaborative tasks. The COCONUT corpus (Di Eugenio et al., 2000) is collected from keyboard-input dialogues between two participants who are collaboratively working on a simple 2-D design task (buying and arranging furniture for two rooms). In contrast, the QUAKE corpus (Byron et al., 2005) – as well as the more recent SCARE corpus (Stoia et al., 2008), which is an extension of QUAKE – is based on an interaction captured in a 3-D virtual reality (VR) world where two participants collaboratively carry out a treasure hunting task. There has been ongoing work to exploit these two resources for research on different aspects of referring expressions (Pamela W. Jordan, 2005; Byron, 2005). However, while these resources have inspired new research into different aspects of referring expressions, at the same time they have clear limitations. The COCONUT corpus is collected from dialogues</context>
</contexts>
<marker>Byron, Mampilly, Sharma, Xu, 2005</marker>
<rawString>Donna Byron, Thomas Mampilly, Vinay Sharma, and Tianfang Xu. 2005. Utilizing visual attention for cross-modal coreference interpretation. In CONTEXT 2005, pages 83–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donna K Byron</author>
</authors>
<title>The OSU Quake</title>
<date>2005</date>
<tech>Technical report,</tech>
<institution>Department of Computer Science and Enginerring, The Ohio State University.</institution>
<contexts>
<context position="2359" citStr="Byron, 2005" startWordPosition="369" endWordPosition="370">from keyboard-input dialogues between two participants who are collaboratively working on a simple 2-D design task (buying and arranging furniture for two rooms). In contrast, the QUAKE corpus (Byron et al., 2005) – as well as the more recent SCARE corpus (Stoia et al., 2008), which is an extension of QUAKE – is based on an interaction captured in a 3-D virtual reality (VR) world where two participants collaboratively carry out a treasure hunting task. There has been ongoing work to exploit these two resources for research on different aspects of referring expressions (Pamela W. Jordan, 2005; Byron, 2005). However, while these resources have inspired new research into different aspects of referring expressions, at the same time they have clear limitations. The COCONUT corpus is collected from dialogues in which participants refer to symbollike objects in a 2-D world. It thus resembles the more recent (non-collaborative) TUNAcorpus (van Deemter, 2007) in tending to encourage very simple types of expressions. Furthermore, limiting participants’ interaction to keyboard input makes the dialogue less natural. While the QUAKE corpus deals with a more complex domain (3-D virtual world), the participa</context>
</contexts>
<marker>Byron, 2005</marker>
<rawString>Donna K. Byron. 2005. The OSU Quake 2004 corpus of two-party situated problem-solving dialogs. Technical report, Department of Computer Science and Enginerring, The Ohio State University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>D Wilkes-Gibbs</author>
</authors>
<title>Referring as a collaborative process.</title>
<date>1986</date>
<journal>Cognition,</journal>
<pages>22--1</pages>
<contexts>
<context position="1534" citStr="Clark and Wilkes-Gibbs, 1986" startWordPosition="230" endWordPosition="233">r to a certain object, enabling smooth collaboration between humans and agents where physical operations are involved. Previous research often either selectively focussed only on a limited number of expression-types or set up overly controlled experiments. In contrast, we intend to work towards analysing the whole breadth of referring expressions in a situated domain. For this purpose we created a corpus (in Japanese) and analysed it from various standpoints. From very early on in referring expression research, there has been some interest in the collaborative aspect of the reference process (Clark and Wilkes-Gibbs, 1986). This has more recently developed into creating situated corpora in order to analyse the referring expressions occurring in situated collaborative tasks. The COCONUT corpus (Di Eugenio et al., 2000) is collected from keyboard-input dialogues between two participants who are collaboratively working on a simple 2-D design task (buying and arranging furniture for two rooms). In contrast, the QUAKE corpus (Byron et al., 2005) – as well as the more recent SCARE corpus (Stoia et al., 2008), which is an extension of QUAKE – is based on an interaction captured in a 3-D virtual reality (VR) world wher</context>
</contexts>
<marker>Clark, Wilkes-Gibbs, 1986</marker>
<rawString>H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as a collaborative process. Cognition, 22:1–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Di Eugenio</author>
<author>P W Jordan</author>
<author>R H Thomason</author>
<author>J D Moore</author>
</authors>
<title>The agreement process: An empirical investigation of human-human computer-mediated collaborative dialogues.</title>
<date>2000</date>
<journal>International Journal of Human-Computer Studies,</journal>
<volume>53</volume>
<issue>6</issue>
<marker>Di Eugenio, Jordan, Thomason, Moore, 2000</marker>
<rawString>B. Di Eugenio, P. W. Jordan, R. H. Thomason, and J. D Moore. 2000. The agreement process: An empirical investigation of human-human computer-mediated collaborative dialogues. International Journal of Human-Computer Studies, 53(6):1017–1076.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Ellen Foster</author>
<author>Ellen Gurman Bard</author>
<author>Markus Guhe</author>
<author>Robin L Hill</author>
<author>Jon Oberlander</author>
<author>Alois Knoll</author>
</authors>
<title>The roles of haptic-ostensive referring expressions in cooperative, task-based human-robot dialogue.</title>
<date>2008</date>
<booktitle>In Proceedings of 3rd Human-Robot Interaction,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="10062" citStr="Foster et al., 2008" startWordPosition="1598" endWordPosition="1602">an action on an object. We have 85 expressions (over 6% of the total) of this type in our corpus. 4 Action-mentioning expressions We further analysed those expressions that mention an action on an object, which we call actionmentioning expressions hereafter. Although there was significant variation in usage of actionmentioning expressions among the pairs, all 6 pairs of participants used at least one actionmentioning expression, indicating that it is a fundamental type of expression for this task setting. Action-mentioning expressions are different from haptic-ostensive referring expressions (Foster et al., 2008) since action-mentioning expressions are not necessarily accompanied by simultaneous physical operation on an object. Action-mentioning expressions can be again divided into three categories: i) combination of a temporal adverbial with a verb indicating an action (“turned”, “put”, “moved”, etc) (55 tokens or about 65% of action-mentioning expression), ii) use of temporal adverbials without a verb, i.e. verb ellipsis (22 tokens or about 26%) and iii) expressions with a verb without temporal adverbials (8 tokens or about 9%). The second category including verb ellipsis would be rare in English, </context>
</contexts>
<marker>Foster, Bard, Guhe, Hill, Oberlander, Knoll, 2008</marker>
<rawString>Mary Ellen Foster, Ellen Gurman Bard, Markus Guhe, Robin L. Hill, Jon Oberlander, and Alois Knoll. 2008. The roles of haptic-ostensive referring expressions in cooperative, task-based human-robot dialogue. In Proceedings of 3rd Human-Robot Interaction, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaki Noguchi</author>
<author>Kenta Miyoshi</author>
<author>Takenobu Tokunaga</author>
<author>Ryu Iida</author>
<author>Mamoru Komachi</author>
<author>Kentaro Inui</author>
</authors>
<title>Multiple purpose annotation using SLAT – Segment and link-based annotation tool.</title>
<date>2008</date>
<booktitle>In Proceedings of 2nd Linguistic Annotation Workshop,</booktitle>
<pages>61--64</pages>
<contexts>
<context position="6630" citStr="Noguchi et al., 2008" startWordPosition="1053" endWordPosition="1056">set microphones in synchronisation with the position of the pieces and the mouse actions. In total, we collected 24 dialogues of about four hours. The average length of a dialogue was 10 minutes 43 seconds. 2.2 Annotation Recorded dialogues were transcribed with a time code attached to each utterance. Since our main concern is collecting referring expressions, we defined an utterance to be a complete sentence to prevent a referring expression being split into several utterances. Referring expressions were annotated together with their referents by using the multi-purpose annotation tool SLAT (Noguchi et al., 2008). Two annotators (two of the authors) annotated four dialogue texts separately. We annotated all 24 dialogue texts and corrected discrepancies by discussion between the annotators. 3 Analysis of the corpus We collected a total of 1,509 tokens and 449 types of referring expressions in 24 dialogues. Our asymmetric experimental setting tended to encourage referring expressions from the solver, while the operator was constrained to confirming his understanding of the solver’s instructions. This is reflected in the number of referring expressions by the solver (1,287) largely outnumbering those of </context>
</contexts>
<marker>Noguchi, Miyoshi, Tokunaga, Iida, Komachi, Inui, 2008</marker>
<rawString>Masaki Noguchi, Kenta Miyoshi, Takenobu Tokunaga, Ryu Iida, Mamoru Komachi, and Kentaro Inui. 2008. Multiple purpose annotation using SLAT – Segment and link-based annotation tool. In Proceedings of 2nd Linguistic Annotation Workshop, pages 61–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker Pamela W Jordan</author>
</authors>
<title>Learning content selection rules for generating object descriptions in dialogue.</title>
<date>2005</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>24--157</pages>
<contexts>
<context position="2345" citStr="Jordan, 2005" startWordPosition="367" endWordPosition="368"> is collected from keyboard-input dialogues between two participants who are collaboratively working on a simple 2-D design task (buying and arranging furniture for two rooms). In contrast, the QUAKE corpus (Byron et al., 2005) – as well as the more recent SCARE corpus (Stoia et al., 2008), which is an extension of QUAKE – is based on an interaction captured in a 3-D virtual reality (VR) world where two participants collaboratively carry out a treasure hunting task. There has been ongoing work to exploit these two resources for research on different aspects of referring expressions (Pamela W. Jordan, 2005; Byron, 2005). However, while these resources have inspired new research into different aspects of referring expressions, at the same time they have clear limitations. The COCONUT corpus is collected from dialogues in which participants refer to symbollike objects in a 2-D world. It thus resembles the more recent (non-collaborative) TUNAcorpus (van Deemter, 2007) in tending to encourage very simple types of expressions. Furthermore, limiting participants’ interaction to keyboard input makes the dialogue less natural. While the QUAKE corpus deals with a more complex domain (3-D virtual world),</context>
</contexts>
<marker>Jordan, 2005</marker>
<rawString>Marilyn A. Walker Pamela W. Jordan. 2005. Learning content selection rules for generating object descriptions in dialogue. Journal of Artificial Intelligence Research, 24:157–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Stoia</author>
<author>Darla Magdalene Shockley</author>
<author>Donna K Byron</author>
<author>Eric Fosler-Lussier</author>
</authors>
<title>SCARE: A situated corpus with annotated referring expressions.</title>
<date>2008</date>
<booktitle>In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC</booktitle>
<contexts>
<context position="2023" citStr="Stoia et al., 2008" startWordPosition="312" endWordPosition="315">ression research, there has been some interest in the collaborative aspect of the reference process (Clark and Wilkes-Gibbs, 1986). This has more recently developed into creating situated corpora in order to analyse the referring expressions occurring in situated collaborative tasks. The COCONUT corpus (Di Eugenio et al., 2000) is collected from keyboard-input dialogues between two participants who are collaboratively working on a simple 2-D design task (buying and arranging furniture for two rooms). In contrast, the QUAKE corpus (Byron et al., 2005) – as well as the more recent SCARE corpus (Stoia et al., 2008), which is an extension of QUAKE – is based on an interaction captured in a 3-D virtual reality (VR) world where two participants collaboratively carry out a treasure hunting task. There has been ongoing work to exploit these two resources for research on different aspects of referring expressions (Pamela W. Jordan, 2005; Byron, 2005). However, while these resources have inspired new research into different aspects of referring expressions, at the same time they have clear limitations. The COCONUT corpus is collected from dialogues in which participants refer to symbollike objects in a 2-D wor</context>
</contexts>
<marker>Stoia, Shockley, Byron, Fosler-Lussier, 2008</marker>
<rawString>Laura Stoia, Darla Magdalene Shockley, Donna K. Byron, and Eric Fosler-Lussier. 2008. SCARE: A situated corpus with annotated referring expressions. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC 2008).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
</authors>
<title>TUNA: Towards a UNified Algorithm for the generation of referring expressions.</title>
<date>2007</date>
<tech>Technical report,</tech>
<institution>Aberdeen University.</institution>
<note>www.csd.abdn.ac.uk/research/tuna/pubs/TUNAfinal-report.pdf.</note>
<marker>van Deemter, 2007</marker>
<rawString>Kees van Deemter. 2007. TUNA: Towards a UNified Algorithm for the generation of referring expressions. Technical report, Aberdeen University. www.csd.abdn.ac.uk/research/tuna/pubs/TUNAfinal-report.pdf.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>