<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.088118">
<title confidence="0.989553">
Overgeneration and ranking for spoken dialogue systems
</title>
<author confidence="0.990436">
Sebastian Varges
</author>
<affiliation confidence="0.9734055">
Center for the Study of Language and Information
Stanford University
</affiliation>
<address confidence="0.567905">
Stanford, CA 94305, USA
</address>
<email confidence="0.996445">
varges@stanford.edu
</email>
<sectionHeader confidence="0.993795" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998159818181818">
We describe an implemented generator
for a spoken dialogue system that fol-
lows the ‘overgeneration and ranking’ ap-
proach. We find that overgeneration based
on bottom-up chart generation is well-
suited to a) model phenomena such as
alignment and variation in dialogue, and b)
address robustness issues in the face of im-
perfect generation input. We report evalu-
ation results of a first user study involving
20 subjects.
</bodyText>
<sectionHeader confidence="0.998756" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998927388888889">
Overgeneration and ranking approaches have be-
come increasingly popular in recent years (Langk-
ilde, 2002; Varges, 2002). However, most work on
generation for practical dialogue systems makes
use of generation components that work toward
a single output, often using simple templates. In
the following, we first describe our dialogue sys-
tem and then turn to the generator which is based
on the overgeneration and ranking paradigm. We
outline the results of a user study, followed by a
discussion section.
The dialogue system: Dialogue processing
starts with the output of a speech recognizer
(Nuance) which is analyzed by both a statistical
dependency parser and a topic classifier. Parse
trees and topic labels are matched by the ‘di-
alogue move scripts’ of the dialogue manager
(DM) (Mirkovic and Cavedon, 2005). The
dialogue system is fully implemented and has
been used in restaurant selection and MP3 player
tasks (Weng et al., 2004). There are 41 task-
independent, generic dialogue rules, 52 restaurant
selection rules and 89 MP3 player rules. Query
constraints are built by dialogue move scripts if
the parse tree matches input patterns specified
in the scripts. For example, a request “I want
to find an inexpensive Japanese restaurant that
takes reservations” results in constraints such as
restaurant:Cuisine = restaurant:japanese
and restaurant:PriceLevel = 0-10. If the
database query constructed from these constraints
returns no results, various constraint modification
strategies such as constraint relaxation or removal
can be employed. For example, ‘Japanese food’
can be relaxed to ‘Asian food’ since cuisine types
are hierarchically organized.
</bodyText>
<sectionHeader confidence="0.992117" genericHeader="method">
2 Overgeneration for spoken dialogue
</sectionHeader>
<bodyText confidence="0.999205777777778">
Table 1 shows some example outputs of the sys-
tem. The wording of the realizations is informed
by a wizard-of-oz data collection. The task of
the generator is to produce these verbalizations
given dialogue strategy, constraints and further
discourse context, i.e. the input to the generator
is non-linguistic. We perform mild overgenera-
tion of candidate moves, followed by ranking. The
highest-ranked candidate is selected for output.
</bodyText>
<subsectionHeader confidence="0.994392">
2.1 Chart generation
</subsectionHeader>
<bodyText confidence="0.999992083333333">
We follow a bottom-up chart generation approach
(Kay, 1996) for production systems similar to
(Varges, 2005). The rule-based core of the gen-
erator is a set of productions written in a produc-
tion system. Productions map individual database
constraints to phrases such as “open for lunch”,
“within 3 miles”, “a formal dress code”, and re-
cursively combine them into NPs. This includes
the use of coordination to produce “restaurants
with a 5-star rating and a formal dress code”,
for example. The NPs are integrated into sen-
tence templates, several of which can be combined
</bodyText>
<page confidence="0.807365">
20
</page>
<bodyText confidence="0.579944">
Proceedings of the Fourth International Natural Language Generation Conference, pages 20–22,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</bodyText>
<table confidence="0.7549093">
|result |mod example realization fexp
s1 0 no I’m sorry but I found no restaurants on Mayfield Road that serve Mediterranean food. 0
s2 small: no There are 2 cheap Thai restaurants in Lincoln in my database : Thai Mee Choke and 61
&gt; 0, &lt; t1 Noodle House.
s3 medium: no I found 9 restaurants with a two star rating and a formal dress code that are open 212
&gt;= t1, &lt; t2 for dinner and serve French food . Here are the first ones :
s4 large: no I found 258 restaurants on Page Mill Road, for example Maya Restaurant, 300
&gt;= t2 Green Frog and Pho Hoa Restaurant. Would you like to try searching by cuisine ?
s5 large yes I found no restaurants that ... However, there are NUM restaurants that ... Would you like to ...? 16
s6 (any) yes/no I found 18 items. 2
</table>
<tableCaption confidence="0.9956075">
Table 1: Some system responses (‘IresultI’: size of database result set, ‘mod’: performed modifications).
Last column: frequency in user study (180 tasks, 596 constraint inputs to generator)
</tableCaption>
<bodyText confidence="0.999845090909091">
to form an output candidate turn. For example,
a constraint realizing template “I found no [NP-
original] but there are [NUM] [NP-optimized] in
my database” can be combined with a follow-up
sentence template such as “You could try to look
for [NP-constraint-suggestion]”. ‘NP-original’ re-
alizes constraints directly constructed from the
user utterance; ‘NP-optimized’ realizes potentially
modified constraints used to obtain the actual
query result. To avoid generating separate sets of
NPs independently for these two – often largely
overlapping – constraint sets, we assign unique in-
dices to the input constraints, overgenerate NPs
and check their indices.
The generator maintains state across dialogue
turns, allowing it to track its previous decisions
(see ‘variation’ below). Both input constraints and
chart edges are indexed by turn numbers to avoid
confusing edges of different turns.
We currently use 102 productions overall in the
restaurant and MP3 domains, 38 of them to gener-
ate NPs that realize 19 input constraints.
</bodyText>
<subsectionHeader confidence="0.98183">
2.2 Ranking: alignment &amp; variation
</subsectionHeader>
<bodyText confidence="0.999787404761905">
Alignment Alignment is a key to successful nat-
ural language dialogue (Brockmann et al., 2005).
We perform alignment of system utterances with
user utterances by computing an ngram-based
overlap score. For example, a user utterance “I
want to find a Chinese restaurant” is presented by
the bag-of-words {‘I’, ‘want’, ‘to’, ‘find’, ...} and
the bag-of-bigrams {‘I want’, ‘want to’, ‘to find’,
...}. We compute the overlap with candidate sys-
tem utterances represented in the same way and
combine the unigram and bigram match scores.
Words are lemmatized and proper nouns of exam-
ple items removed from the utterances.
Alignment allows us to prefer “restaurants that
serve Chinese food” over “Chinese restaurants”
if the user used a wording more similar to the
first. The Gricean Maxim of Brevity, applied to
NLG in (Dale and Reiter, 1995), suggests a prefer-
ence for the second, shorter realization. However,
if the user thought it necessary to use “serves”,
maybe to correct an earlier mislabeling by the
classifier/parse-matching patterns, then the system
should make it clear that it understood the user
correctly by using those same words. On the other
hand, a general preference for brevity is desirable
in spoken dialogue systems: users are generally
not willing to listen to lengthy synthesized speech.
Variation We use a variation score to ‘cycle’
over sentence-level paraphrases. Alternative can-
didates for realizing a certain input move are
given a unique alternation (‘alt’) number in in-
creasing order. For example, for the simple move
continuation query we may assign the follow-
ing alt values: “Do you want more?” (alt=1) and
“Do you want me to continue?” (alt=2). The sys-
tem cycles over these alternatives in turn. Once
we reach alt=2, it starts over from alt=1. The ac-
tual alt ‘score’ is inversely related to recency and
normalized to [0...1].
Score combination The final candidate score is
a linear combination of alignment and variation
scores:
</bodyText>
<equation confidence="0.9999485">
scorefinal = A1 · alignuni,bi +(1 − A1) · variation(1)
alignuni,bi = A2 · alignuni +(1 − A2) · alignbi (2)
</equation>
<bodyText confidence="0.999713">
where A1, A2 E {0...1}. A high value of A1
places more emphasis on alignment, a low value
yields candidates that are more different from pre-
viously chosen ones. In our experience, align-
ment should be given a higher weight than vari-
ation, and, within alignment, bigrams should be
</bodyText>
<page confidence="0.998064">
21
</page>
<bodyText confidence="0.999775">
weighted higher than unigrams, i.e. A1 &gt; 0.5 and
A2 &lt; 0.5. Deriving weights empirically from cor-
pus data is an avenue for future research.
</bodyText>
<sectionHeader confidence="0.88447" genericHeader="method">
3 User study
</sectionHeader>
<bodyText confidence="0.99944605">
Each of 20 subjects in a restaurant selection task
was given 9 scenario descriptions involving 3 con-
straints. We use a back-end database of 2500
restaurants containing the 13 attributes/constraints
for each restaurant.
On average, the generator produced 16 output
candidates for inputs of two constraints, 160 can-
didates for typical inputs of 3 constraints and 320
candidates for 4 constraints. For larger constraint
sets, we currently reduce the level of overgenera-
tion but in the future intend to interleave overgen-
eration with ranking similar to (Varges, 2002).
Task completion in the experiments was high:
the subjects met all target constraints in 170 out of
180 tasks, i.e. completion rate was 94.44%. To
the question “The responses of the system were
appropriate, helpful, and clear.” (on a scale where
1 = ‘strongly agree’, 5 = ‘strongly disagree’), the
subjects gave the following ratings: 1: 7, 2: 9, 3:
2, 4: 2 and 5: 0, i.e. the mean user rating is 1.95.
</bodyText>
<sectionHeader confidence="0.989934" genericHeader="method">
4 Discussion &amp; Conclusions
</sectionHeader>
<bodyText confidence="0.993313860465116">
Where NLG affects the dialogue system: Dis-
course entities introduced by NLG add items to the
system’s salience list as an equal partner to NLU.
Robustness: due to imperfect ASR and NLU,
we relax completeness requirements when doing
overgeneration, and reason about the generation
input by adding defaults for missing constraints,
checking ranges of attribute values etc. Moreover,
we use a template generator as a fall-back if NLG
fails to at least give some feedback to the user (s6
in table 1).
What-to-say vs how-to-say-it: the classic sep-
aration of NLG into separate modules also holds
in our dialogue system, albeit with some mod-
ifications: ‘content determination’ is ultimately
performed by the user and the constraint opti-
mizer. The presentation dialogue moves do micro-
planning, for example by deciding to present re-
trieved database items either as examples (s4 in
table 1) or as part of a larger answer list of items.
The chart generator performs realization.
In sum, flexible and expressive NLG is cru-
cial for the robustness of the entire speech-based
dialogue system by verbalizing what the system
understood and what actions it performed as a
consequence of this understanding. We find that
overgeneration and ranking techniques allow us to
model alignment and variation even in situations
where no corpus data is available by using the dis-
course history as a ‘corpus’.
Acknowledgments This work is supported by the
US government’s NIST Advanced Technology Program.
Collaborating partners are CSLI, Robert Bosch Corporation,
VW America, and SRI International. We thank the many
people involved in this project, in particular Fuliang Weng
and Heather Pon-Barry for developing the content optimiza-
tion module; Annie Lien, Badri Raghunathan, Brian Lathrop,
Fuliang Weng, Heather Pon-Barry, Jeff Russell, and Tobias
Scheideck for performing the evaluations and compiling the
results; Matthew Purver and Florin Ratiu for work on the
CSLI dialogue manager. The content optimizer, knowledge
manager, and the NLU module have been developed by the
Bosch Research and Technology Center.
</bodyText>
<sectionHeader confidence="0.998417" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999685870967742">
Carsten Brockmann, Amy Isard, Jon Oberlander, and
Michael White. 2005. Modelling alignment for af-
fective dialogue. In Proc. of the UM’05 Workshop
on Adapting the Interaction Style to Affective Fac-
tors.
Robert Dale and Ehud Reiter. 1995. Computational
Interpretations of the Gricean Maxims in the Gener-
ation of Referring Expressions. Cognitive Science,
19:233–263.
Martin Kay. 1996. Chart Generation. In Proceedings
ofACL-96, pages 200–204.
Irene Langkilde. 2002. An Empirical Verification
of Coverage and Correctness for a General-Purpose
Sentence Generator. In Proc. ofINLG-02.
Danilo Mirkovic and Lawrence Cavedon. 2005. Prac-
tical Plug-and-Play Dialogue Management. In Pro-
ceedings of the 6th Meeting of the Pacific Associa-
tion for Computational Linguistics (PACLING).
Sebastian Varges. 2002. Fluency and Completeness
in Instance-based Natural Language Generation. In
Proc. of COLING-02.
Sebastian Varges. 2005. Chart generation using pro-
duction systems (short paper). In Proc. of 10th Eu-
ropean Workshop On Natural Language Generation.
Fuliang Weng, L. Cavedon, B. Raghunathan,
D. Mirkovic, H. Cheng, H. Schmidt, H. Bratt,
R. Mishra, S. Peters, L. Zhao, S. Upson, E. Shriberg,
and C. Bergmann. 2004. Developing a conversa-
tional dialogue system for cognitively overloaded
users. In Proceedings of the International Congress
on Intelligent Transportation Systems (ICSLP).
</reference>
<page confidence="0.999025">
22
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.659236">
<title confidence="0.999963">Overgeneration and ranking for spoken dialogue systems</title>
<author confidence="0.987429">Sebastian</author>
<affiliation confidence="0.864013">Center for the Study of Language and Stanford</affiliation>
<address confidence="0.991659">Stanford, CA 94305,</address>
<email confidence="0.99962">varges@stanford.edu</email>
<abstract confidence="0.993121666666667">We describe an implemented generator for a spoken dialogue system that follows the ‘overgeneration and ranking’ approach. We find that overgeneration based on bottom-up chart generation is wellsuited to a) model phenomena such as alignment and variation in dialogue, and b) address robustness issues in the face of imperfect generation input. We report evaluation results of a first user study involving 20 subjects.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Carsten Brockmann</author>
<author>Amy Isard</author>
<author>Jon Oberlander</author>
<author>Michael White</author>
</authors>
<title>Modelling alignment for affective dialogue.</title>
<date>2005</date>
<booktitle>In Proc. of the UM’05 Workshop on Adapting the Interaction Style to Affective Factors.</booktitle>
<contexts>
<context position="5615" citStr="Brockmann et al., 2005" startWordPosition="884" endWordPosition="887">verlapping – constraint sets, we assign unique indices to the input constraints, overgenerate NPs and check their indices. The generator maintains state across dialogue turns, allowing it to track its previous decisions (see ‘variation’ below). Both input constraints and chart edges are indexed by turn numbers to avoid confusing edges of different turns. We currently use 102 productions overall in the restaurant and MP3 domains, 38 of them to generate NPs that realize 19 input constraints. 2.2 Ranking: alignment &amp; variation Alignment Alignment is a key to successful natural language dialogue (Brockmann et al., 2005). We perform alignment of system utterances with user utterances by computing an ngram-based overlap score. For example, a user utterance “I want to find a Chinese restaurant” is presented by the bag-of-words {‘I’, ‘want’, ‘to’, ‘find’, ...} and the bag-of-bigrams {‘I want’, ‘want to’, ‘to find’, ...}. We compute the overlap with candidate system utterances represented in the same way and combine the unigram and bigram match scores. Words are lemmatized and proper nouns of example items removed from the utterances. Alignment allows us to prefer “restaurants that serve Chinese food” over “Chine</context>
</contexts>
<marker>Brockmann, Isard, Oberlander, White, 2005</marker>
<rawString>Carsten Brockmann, Amy Isard, Jon Oberlander, and Michael White. 2005. Modelling alignment for affective dialogue. In Proc. of the UM’05 Workshop on Adapting the Interaction Style to Affective Factors.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<date>1995</date>
<booktitle>Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions. Cognitive Science,</booktitle>
<pages>19--233</pages>
<contexts>
<context position="6356" citStr="Dale and Reiter, 1995" startWordPosition="1004" endWordPosition="1007">, a user utterance “I want to find a Chinese restaurant” is presented by the bag-of-words {‘I’, ‘want’, ‘to’, ‘find’, ...} and the bag-of-bigrams {‘I want’, ‘want to’, ‘to find’, ...}. We compute the overlap with candidate system utterances represented in the same way and combine the unigram and bigram match scores. Words are lemmatized and proper nouns of example items removed from the utterances. Alignment allows us to prefer “restaurants that serve Chinese food” over “Chinese restaurants” if the user used a wording more similar to the first. The Gricean Maxim of Brevity, applied to NLG in (Dale and Reiter, 1995), suggests a preference for the second, shorter realization. However, if the user thought it necessary to use “serves”, maybe to correct an earlier mislabeling by the classifier/parse-matching patterns, then the system should make it clear that it understood the user correctly by using those same words. On the other hand, a general preference for brevity is desirable in spoken dialogue systems: users are generally not willing to listen to lengthy synthesized speech. Variation We use a variation score to ‘cycle’ over sentence-level paraphrases. Alternative candidates for realizing a certain inp</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions. Cognitive Science, 19:233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Chart Generation.</title>
<date>1996</date>
<booktitle>In Proceedings ofACL-96,</booktitle>
<pages>200--204</pages>
<contexts>
<context position="2836" citStr="Kay, 1996" startWordPosition="428" endWordPosition="429">an food’ since cuisine types are hierarchically organized. 2 Overgeneration for spoken dialogue Table 1 shows some example outputs of the system. The wording of the realizations is informed by a wizard-of-oz data collection. The task of the generator is to produce these verbalizations given dialogue strategy, constraints and further discourse context, i.e. the input to the generator is non-linguistic. We perform mild overgeneration of candidate moves, followed by ranking. The highest-ranked candidate is selected for output. 2.1 Chart generation We follow a bottom-up chart generation approach (Kay, 1996) for production systems similar to (Varges, 2005). The rule-based core of the generator is a set of productions written in a production system. Productions map individual database constraints to phrases such as “open for lunch”, “within 3 miles”, “a formal dress code”, and recursively combine them into NPs. This includes the use of coordination to produce “restaurants with a 5-star rating and a formal dress code”, for example. The NPs are integrated into sentence templates, several of which can be combined 20 Proceedings of the Fourth International Natural Language Generation Conference, pages</context>
</contexts>
<marker>Kay, 1996</marker>
<rawString>Martin Kay. 1996. Chart Generation. In Proceedings ofACL-96, pages 200–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Irene Langkilde</author>
</authors>
<title>An Empirical Verification of Coverage and Correctness for a General-Purpose Sentence Generator. In</title>
<date>2002</date>
<booktitle>Proc. ofINLG-02.</booktitle>
<contexts>
<context position="729" citStr="Langkilde, 2002" startWordPosition="107" endWordPosition="109">mation Stanford University Stanford, CA 94305, USA varges@stanford.edu Abstract We describe an implemented generator for a spoken dialogue system that follows the ‘overgeneration and ranking’ approach. We find that overgeneration based on bottom-up chart generation is wellsuited to a) model phenomena such as alignment and variation in dialogue, and b) address robustness issues in the face of imperfect generation input. We report evaluation results of a first user study involving 20 subjects. 1 Introduction Overgeneration and ranking approaches have become increasingly popular in recent years (Langkilde, 2002; Varges, 2002). However, most work on generation for practical dialogue systems makes use of generation components that work toward a single output, often using simple templates. In the following, we first describe our dialogue system and then turn to the generator which is based on the overgeneration and ranking paradigm. We outline the results of a user study, followed by a discussion section. The dialogue system: Dialogue processing starts with the output of a speech recognizer (Nuance) which is analyzed by both a statistical dependency parser and a topic classifier. Parse trees and topic </context>
</contexts>
<marker>Langkilde, 2002</marker>
<rawString>Irene Langkilde. 2002. An Empirical Verification of Coverage and Correctness for a General-Purpose Sentence Generator. In Proc. ofINLG-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danilo Mirkovic</author>
<author>Lawrence Cavedon</author>
</authors>
<title>Practical Plug-and-Play Dialogue Management.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th Meeting of the Pacific Association for Computational Linguistics (PACLING).</booktitle>
<contexts>
<context position="1436" citStr="Mirkovic and Cavedon, 2005" startWordPosition="219" endWordPosition="222"> makes use of generation components that work toward a single output, often using simple templates. In the following, we first describe our dialogue system and then turn to the generator which is based on the overgeneration and ranking paradigm. We outline the results of a user study, followed by a discussion section. The dialogue system: Dialogue processing starts with the output of a speech recognizer (Nuance) which is analyzed by both a statistical dependency parser and a topic classifier. Parse trees and topic labels are matched by the ‘dialogue move scripts’ of the dialogue manager (DM) (Mirkovic and Cavedon, 2005). The dialogue system is fully implemented and has been used in restaurant selection and MP3 player tasks (Weng et al., 2004). There are 41 taskindependent, generic dialogue rules, 52 restaurant selection rules and 89 MP3 player rules. Query constraints are built by dialogue move scripts if the parse tree matches input patterns specified in the scripts. For example, a request “I want to find an inexpensive Japanese restaurant that takes reservations” results in constraints such as restaurant:Cuisine = restaurant:japanese and restaurant:PriceLevel = 0-10. If the database query constructed from </context>
</contexts>
<marker>Mirkovic, Cavedon, 2005</marker>
<rawString>Danilo Mirkovic and Lawrence Cavedon. 2005. Practical Plug-and-Play Dialogue Management. In Proceedings of the 6th Meeting of the Pacific Association for Computational Linguistics (PACLING).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Varges</author>
</authors>
<title>Fluency and Completeness in Instance-based Natural Language Generation.</title>
<date>2002</date>
<booktitle>In Proc. of COLING-02.</booktitle>
<contexts>
<context position="744" citStr="Varges, 2002" startWordPosition="110" endWordPosition="111">niversity Stanford, CA 94305, USA varges@stanford.edu Abstract We describe an implemented generator for a spoken dialogue system that follows the ‘overgeneration and ranking’ approach. We find that overgeneration based on bottom-up chart generation is wellsuited to a) model phenomena such as alignment and variation in dialogue, and b) address robustness issues in the face of imperfect generation input. We report evaluation results of a first user study involving 20 subjects. 1 Introduction Overgeneration and ranking approaches have become increasingly popular in recent years (Langkilde, 2002; Varges, 2002). However, most work on generation for practical dialogue systems makes use of generation components that work toward a single output, often using simple templates. In the following, we first describe our dialogue system and then turn to the generator which is based on the overgeneration and ranking paradigm. We outline the results of a user study, followed by a discussion section. The dialogue system: Dialogue processing starts with the output of a speech recognizer (Nuance) which is analyzed by both a statistical dependency parser and a topic classifier. Parse trees and topic labels are matc</context>
<context position="8582" citStr="Varges, 2002" startWordPosition="1373" endWordPosition="1374">ta is an avenue for future research. 3 User study Each of 20 subjects in a restaurant selection task was given 9 scenario descriptions involving 3 constraints. We use a back-end database of 2500 restaurants containing the 13 attributes/constraints for each restaurant. On average, the generator produced 16 output candidates for inputs of two constraints, 160 candidates for typical inputs of 3 constraints and 320 candidates for 4 constraints. For larger constraint sets, we currently reduce the level of overgeneration but in the future intend to interleave overgeneration with ranking similar to (Varges, 2002). Task completion in the experiments was high: the subjects met all target constraints in 170 out of 180 tasks, i.e. completion rate was 94.44%. To the question “The responses of the system were appropriate, helpful, and clear.” (on a scale where 1 = ‘strongly agree’, 5 = ‘strongly disagree’), the subjects gave the following ratings: 1: 7, 2: 9, 3: 2, 4: 2 and 5: 0, i.e. the mean user rating is 1.95. 4 Discussion &amp; Conclusions Where NLG affects the dialogue system: Discourse entities introduced by NLG add items to the system’s salience list as an equal partner to NLU. Robustness: due to imperf</context>
</contexts>
<marker>Varges, 2002</marker>
<rawString>Sebastian Varges. 2002. Fluency and Completeness in Instance-based Natural Language Generation. In Proc. of COLING-02.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Varges</author>
</authors>
<title>Chart generation using production systems (short paper).</title>
<date>2005</date>
<booktitle>In Proc. of 10th European Workshop On Natural Language Generation.</booktitle>
<contexts>
<context position="2885" citStr="Varges, 2005" startWordPosition="435" endWordPosition="436">y organized. 2 Overgeneration for spoken dialogue Table 1 shows some example outputs of the system. The wording of the realizations is informed by a wizard-of-oz data collection. The task of the generator is to produce these verbalizations given dialogue strategy, constraints and further discourse context, i.e. the input to the generator is non-linguistic. We perform mild overgeneration of candidate moves, followed by ranking. The highest-ranked candidate is selected for output. 2.1 Chart generation We follow a bottom-up chart generation approach (Kay, 1996) for production systems similar to (Varges, 2005). The rule-based core of the generator is a set of productions written in a production system. Productions map individual database constraints to phrases such as “open for lunch”, “within 3 miles”, “a formal dress code”, and recursively combine them into NPs. This includes the use of coordination to produce “restaurants with a 5-star rating and a formal dress code”, for example. The NPs are integrated into sentence templates, several of which can be combined 20 Proceedings of the Fourth International Natural Language Generation Conference, pages 20–22, Sydney, July 2006. c�2006 Association for</context>
</contexts>
<marker>Varges, 2005</marker>
<rawString>Sebastian Varges. 2005. Chart generation using production systems (short paper). In Proc. of 10th European Workshop On Natural Language Generation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fuliang Weng</author>
<author>L Cavedon</author>
<author>B Raghunathan</author>
<author>D Mirkovic</author>
<author>H Cheng</author>
<author>H Schmidt</author>
<author>H Bratt</author>
<author>R Mishra</author>
<author>S Peters</author>
<author>L Zhao</author>
<author>S Upson</author>
<author>E Shriberg</author>
<author>C Bergmann</author>
</authors>
<title>Developing a conversational dialogue system for cognitively overloaded users.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Congress on Intelligent Transportation Systems (ICSLP).</booktitle>
<contexts>
<context position="1561" citStr="Weng et al., 2004" startWordPosition="240" endWordPosition="243">e our dialogue system and then turn to the generator which is based on the overgeneration and ranking paradigm. We outline the results of a user study, followed by a discussion section. The dialogue system: Dialogue processing starts with the output of a speech recognizer (Nuance) which is analyzed by both a statistical dependency parser and a topic classifier. Parse trees and topic labels are matched by the ‘dialogue move scripts’ of the dialogue manager (DM) (Mirkovic and Cavedon, 2005). The dialogue system is fully implemented and has been used in restaurant selection and MP3 player tasks (Weng et al., 2004). There are 41 taskindependent, generic dialogue rules, 52 restaurant selection rules and 89 MP3 player rules. Query constraints are built by dialogue move scripts if the parse tree matches input patterns specified in the scripts. For example, a request “I want to find an inexpensive Japanese restaurant that takes reservations” results in constraints such as restaurant:Cuisine = restaurant:japanese and restaurant:PriceLevel = 0-10. If the database query constructed from these constraints returns no results, various constraint modification strategies such as constraint relaxation or removal can</context>
</contexts>
<marker>Weng, Cavedon, Raghunathan, Mirkovic, Cheng, Schmidt, Bratt, Mishra, Peters, Zhao, Upson, Shriberg, Bergmann, 2004</marker>
<rawString>Fuliang Weng, L. Cavedon, B. Raghunathan, D. Mirkovic, H. Cheng, H. Schmidt, H. Bratt, R. Mishra, S. Peters, L. Zhao, S. Upson, E. Shriberg, and C. Bergmann. 2004. Developing a conversational dialogue system for cognitively overloaded users. In Proceedings of the International Congress on Intelligent Transportation Systems (ICSLP).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>