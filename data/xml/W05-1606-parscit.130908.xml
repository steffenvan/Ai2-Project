<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.992163">
Generating Referential Descriptions Under Conditions of Uncertainty
</title>
<author confidence="0.944562">
Helmut Horacek
</author>
<affiliation confidence="0.913114">
Universität des Saarlandes
</affiliation>
<address confidence="0.6924995">
F.R. 6.2 Informatik
Postfach 151150, D-66041 Saarbrücken, Germany
</address>
<email confidence="0.999587">
email: horacek@cs.uni-sb.de
</email>
<sectionHeader confidence="0.995651" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999939823529412">
Algorithms for generating referring expressions
typically assume that an object in a scenary can be
identified through a set of commonly agreed
properties. This is a strong assumption, since in
reality properties of objects may be perceived differ-
ently among people, due to a number of factors
including vagueness, knowledge discrepancies, and
limited perception capabilities. Taking these discre-
pancies into account, we reinterpret concepts of
algorithms generating referring expressions in view
of uncertainties about the appearance of objects. Our
model includes two complementary measures of
likelihood in object identification, and adapted
property selection and termination criteria. The
approach is relevant for situations with potential
perception problems and for scenarios with knowl-
edge discrepancies between conversants.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965333333333">
Generating referring expressions is a traditional, standard
task in natural language generation. Over the past two
decades, a number of algorithms have been proposed which
differ among each other in terms of efficiency and coverage.
To the best of our knowledge, all algorithms share the
assumption that objects can be identified by a description
consisting of attribute values ascribed to these objects.
Moreover, the results are specified in a way that implicitly
assumes complete agreement about these properties,
provided they are known to the audience. We feel that this
assumption may be too strong in reality so that, for
instance, a dialog system in which the reference generation
algorithm is embedded is unlikely to behave adequately when
a misunderstanding occurs due to a perception mismatch.
In this paper, we address this problem by incorporating
measures to deal with uncertainties into a standard algorithm
that generates referring expressions. In order to represent
uncertainties, we propose two complementary measures
expressing the likelihood of object identification. We define
computation schemes for combining descriptions with
boolean combinations of attribute values, and we extend the
incremental standard reference generation algorithm by
adapting property selection and termination criteria.
This paper is organized as follows. First, we motivate our
approach in more detail. Then we introduce our method for
representing aspects of uncertainty. We follow by illustrating
the propagation of uncertainty assessments for several attri-
bute values, including boolean combinations, and we give
examples of the effects. Then we describe extensions to the
incremental algorithm, and we discuss their impact.
</bodyText>
<sectionHeader confidence="0.989148" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999964491228071">
In the scope of this paper, we adopt the terminology origin-
ally formulated in [Dale 1988] and later used by several
others. A referential description [Donellan 1966] serves the
purpose of letting the hearer or reader identify a particular
object or set of objects in a given situation. The referring
expression to be generated is required to be a distinguishing
description, that is a description of the enitties being referred
to, but not to any other object in the context set. A context
set is defined as the set of the entities the addressee is
currently assumed to be attending to – this is similar to the
set of entities in the focus spaces of the discourse focus stack
in Grosz&apos; and Sidner&apos;s [1986] theory of discourse structure.
Moreover, the contrast set (or the set of potential
distractors [McDonald 1981]) is defined to entail all
elements of the context set except the intended referents.
Generating referring expressions is pursued since the
eighties [Appelt 1985, Kronfeld 1986, Appelt and Kronfeld
1987]. Subsequent years were characterized by a debate about
computational efficiency versus minimality of the elements
appearing in the resulting referring expression [Dale 1988,
Reiter 1990, Reiter and Dale 1992]. In the mid-nineties, this
debate seemed to be settled in favor of the incremental
approach [Dale and Reiter 1995] – motivated by results of
psychological experiments [Levelt 1989, Pechmann 1989],
certain non-minimal expressions are tolerated in favor of
adopting the fast strategy of incrementally selecting ambi-
guity-reducing attributes from a domain-dependent preference
list. Recently, algorithms have been applied to the identifi-
cation of sets of objects rather than individuals [Bateman
1999, Stone 2000, Krahmer, v. Erk, and Verweg 2001], and
the repertoire of descriptions has been extended to boolean
combinations of attributes, including negations [van Deemter
2002]. To avoid the generation of redundant descriptions that
is typical for incremental approaches, Gardent [2002] and
Horacek [2003] proposed exhaustive resp. best-first searches.
All these procedures more or less share the design of the
underlying knowledge base. Objects are conceived in terms
of sets of attributes, each with an atomic value as its filler.
Some models distinguish specializations of these values
according to a taxonomic hierarchy, so that the most accu-
rate value can be replaced by one of its generalizations if
there are reasons to assume this alternative is preferable –
due to insufficient knowledge attributed to the audience, or
to prevent unintended implications. A few approaches also
deal with relations to other objects, whose representation
differs from that of attributes only by the reference to the
related object. Typically, a user model is assumed to guide
the choice among available descriptors; the user model
expresses taxonomic knowledge attributed to the user, indic-
ating for a descriptor whether it is known to the user or not.
While a knowledge base developed and interpreted in this
manner is adequate for generating referring expressions in
most application-relevant settings, there may be circum-
stances in which uncertainties are prominent, so that the
simple boolean attribution of properties to objects becomes
problematic and may prove insufficient. Uncertainties may
manifest themselves in at least the following three factors:
</bodyText>
<listItem confidence="0.943035">
• Uncertainty about knowledge
</listItem>
<bodyText confidence="0.9991295">
There may not be sufficient evidence to assume that
the user is or is not acquainted with a specific term. In
fact, most of today&apos;s user model components assign
some probability to statements about a user&apos;s knowl-
edge or capabilities, for example on the basis of infer-
ences obtained through a belief network [Pearl 1988].
</bodyText>
<listItem confidence="0.949033">
• Uncertainty about perception capabilities
</listItem>
<bodyText confidence="0.999958222222222">
There is an increasing number of applications with
natural language interaction where the objects of the
discourse do not appear on the computer screen (e.g.,
ubiquitous tools guiding a user in environments such
as airports and tourist attraction areas, e.g., [Wahlster
2004]). In such situations, perception and recognition
of object properties is much harder to assess; for
example, the visibility of some object or of one of its
parts may not be derivable with complete certainty.
</bodyText>
<listItem confidence="0.97862">
• Uncertainty about conceptual agreement
</listItem>
<bodyText confidence="0.999729197368421">
While ascribing a value to an attribute is straightfor-
ward for certain categories of attributes, problems may
occur, e.g., in connection with vagueness. This
concept may be relevant for a number of commonly
used properties such as size and shape, and even with
colors, transitions between adjacent color tones may
not be firmly categorized as one of the two candidates.
To illustrate these manifestations of uncertainty, let us
consider a scenario with three similar dogs, one of which is
a bassett, which is also the intended referent. In addition,
the bassett is brownish and has a long tail. The other two
dogs have shorter tails and their skin is also brown, but
with some white resp. black portions. Furthermore, we
assume that the audience has little knowledge about dog
specifics, that is, it is not very likely that they may recog-
nize the intended referent as a bassett. We also assume that
the tails of the dogs cannot be observed easily by the
audience under the given local circumstances.
Hence, the three attributes “category”, “color”, and “tail
length” each fall into one of the categories of uncertainty
introduced above: the categorization of the intended referent
as a bassett is associated with uncertainty about knowledge,
the limited visibility which may not enable the spectators
to see the tails of the dogs in each moment constitutes an
uncertainty about perception capabilities, and the similarity
of the dogs&apos; colors may yield uncertainty about conceptual
agreement, that is, it is doubtfull whether the descriptor
“brownish” is attributed only to the intended referent or also
to some of the other dogs in the given situation.
Apparently, these uncertainties have consequences on
building human-adequate referring expressions, especially in
contexts where most of the descriptors available are asso-
ciated with some kind of uncertainty. Intuitively, we would
expect people to produce referring expressions with several
of these descriptors, being redundant in case they are all
recognized, but also hoping that the identification will
succeed if the audience can identify only some part of the
overall description in the given situation. Moreover, we
would expect people only to use descriptors that have some
reasonable chance of being understood.
Unfortunately, traditional generation algorithms do not
enable us to model such a behavior, since none of the
options available does justice to the uncertainty involved. If
a descriptor is modeled as applying to all entities (e.g., for
“brownish”), it will never be chosen since it yields no
discrimination. A similar consequence is obtained when the
capabilities of the audience are interpreted pessimistically.
Finally, if a descriptor is assumed to be understood, it
might be chosen without considering any of the other candi-
dates associated with uncertainty. Thus, modeling in the
existing algorithms forces us to make crisp decisions, with
strong impacts on the result of the algorithm. Redundant
expressions motivated by uncertainties about recognition
cannot be generated under any modeling alternative.
There are only a few computational approaches which
address the problem of uncertainty about the recognition of
referring expressions. For example, [Edmonds 1994] and
[Heeman and Hirst 1995] describe both plan-based methods,
where a vague and partial description is produced initially,
which is narrowed and ultimately confirmed in the subse-
quent discourse. However, the documented examples do only
emphasize incomplete, but never incorrect interpretations.
An approach that fits better to our intentions is the work
by Goodman [1987], which emphasizes reference identi-
fication and associated failures in task-oriented dialogs
[Goodman 1986]. This case study demonstrates various
impacts of limitations and discrepancies of expertise on
referential identification: subjects exhibit uncertainty in
identification, which manifests itself in tentative actions and
changes of mind, they misinterpret descriptions (e.g.,
&apos;outlet&apos; interpreted as &apos;hole&apos;), and they may find no
appropriate referent at all. In the latter case, subject even
undertake attempts to repair an otherwise uninterpretable
description by relaxing descriptors. In the following, we
interpret some of these findings for our model of uncer-
tainty, including a model of a repair mechanism.
</bodyText>
<sectionHeader confidence="0.972862" genericHeader="method">
3 Representing Uncertainties
</sectionHeader>
<bodyText confidence="0.999840107142858">
Basically, our model of uncertainty combines the three kinds
of uncertainty described in the previous section. Each of
them is expressed in terms of a probability, associated with
a triple consisting of an object, an attribute applicable to
that object, and the value ascribed to this pair. The follow-
ing probabilities each express the likelihood that the user
recognizes a description correctly from the perspectives of:
pK The user is acquainted with the terms mentioned
pP The user can perceive the properties uttered
pA The user agrees to the applicability of the terms used
In order to identify an intended referent successfully, all
three factors must be assessed positively, so that the
probability of recognition p becomes the product of these
three probabilities. Since the individual properties refer to
factors outside the scope of proper generation, we only deal
with p in the scope of this paper, although it is clear that
this assessment requires contributions from several sources.
The concept of using individual probabilities to represent
manifestations of uncertainty is not only simple, it also fits
to knowledge sources where data about these probabilities
could be found. For example, user models, the potential
sources for assessing pK, typically assign assessments to
user capabilities on the basis of belief networks. Similar
considerations hold for representations of vague properties,
which fall under the concept of term agreement. These
properties can be modeled by fuzzy logic systems [Zadeh
1984, 1996], which allow for an interpretation in terms of a
single probability value, representing the likelihood that a
precise value is perceived as a given vague term.
The association of a probability with the applicability of
a descriptor to an object not only expresses the somehow
direct likelihood of success of this task, but the application
of this likelihood to several candidate objects also gives an
indication of the likelihood of success of the overall identi-
fication goal. If a descriptor is assumed to be associated with
several candidate objects by the audience, with certain
degrees typically different among these objects, several cases
can be distinguished: (1) correct identification, where the
audience relates the description only to those objects to
which this descriptor indeed applies, (2) misinterpretation,
where none of these objects, but others are associated with
the descriptor by the audience, (3) ambiguity, which is a
combination of (1) and (2), and, finally, (4) the case of an
uninterpretable description, where the audience does not
relate the descriptor to any of the candidate objects. In the
last case, people are known to make an attempt to repair
their unsuccessful interpretation, since they assume that the
expression communicated is indeed intended to refer to some
object or objects in the domain of discourse, according to the
work by Goodman. In order to simulate the effect of this
behavior, we compute the probability of the occurrence of an
uninterpretable description, which we call the repair
factor, and we increase the probability of identification of
the candidate objects (which we call our repair mechanism),
based on the amount of the repair factor and the context of
these objects in the overall identification task.
</bodyText>
<equation confidence="0.96371">
R(k, p , , pn)
1 ... = E E Õ ( F ( f ( j , m, n), p )|f(j , m, n)|)
i
j= 0 m j
= =
i 1
</equation>
<bodyText confidence="0.99552725">
f(j,m,n) recursively enumerates all combinations of m out of
n elements (here: natural numbers 1 ... n) and returns the j-th
combination as a set of numbers M={i1,...,im} with 1&lt;ik&lt;n
(k= 1,...,m)
</bodyText>
<figureCaption confidence="0.998201">
Figure 1. Repair factor for insufficient recognition of k objects
</figureCaption>
<bodyText confidence="0.989063">
In concrete terms, if we have n objects for which a descriptor
is recognized with probability pi for object i, the probability
that none of the objects is recognized by a referring
expression built from that descriptor is II(1-pi), (1 &lt; i &lt; n).
Although this number tends to be small if there are several
objects to which the description matches with some reason-
able degree of confidence, the associated need for invoking a
repair mechanism becomes increasingly urgent when further
descriptors are added to the description built so far, as well as
when the task is to identify multiple referents rather than a
single one. For the case of 2 objects, the need for invoking a
repair mechanism can be quantified by the repair factor
2IIi=1,n(1-pi)+Σi=1,n(piIIj=1,n(j≠i)(1-pj)). The general case, if
needed, gets increasingly complex, as illustrated in Figure 1,
for k objects to be identified, out of n candidates (k &lt; n).
Thus, for the likelihood of recognition failure, a
mechanism is required that simulates identification repair
under these conditions. Apart from the likelihood of failure,
repair should be guided by potential confusability of objects
in view of some given descriptor. Hence, while we think it
is virtually impossible to confuse an animal and a piece of
equipment, at least under any reasonable conditions of visibi-
lity, we assume that objects of some degree of appearance
similarity (size and shape) may potentially be confused with
each other. Hence, we consider a potentially confusable
object a candidate for being interpreted as an intended referent
in case a repair of a reference failure is required. Confusion in
this sense may be interpreted in two ways: from the
perspective of the speaker, those objects are candidates which
the speaker thinks the hearer could confuse. From the hearers
perspective, those objects are candidates which the hearer
thinks the speaker might have confused in producing a badly
interpretable description. Since the latter constellation
corresponds to the situation present for repair attempts, we
model potential candidates quasi “objectively” by incorpor-
ating annotations in the knowledge base. The dependency of
user capabilities as assessed by a user model influences these
assessments indirectly through the probability of recognition
attributed to the user for each descriptor-object pair.
i
if
if
</bodyText>
<equation confidence="0.998248666666667">
(1 p)
− i
i M
∈ M⎬⎭
i M
∉
p
F(M,p i ) = j⎩
k− 1 (mø n
</equation>
<bodyText confidence="0.889925">
Determine probability of identification (D, k, O1, ..., On)
O1, ..., Om Objects to which descriptor D applies
Om+1, ..., On Objects to which repair with D is applicable
pi, ..., pm Probability that D is recognized for Oi
Objects ordered along degrees of recognition confidence:
∀i,j(1 ≤ i, j ≤ m): (pi &gt; pj) → (i &gt; j)
</bodyText>
<figure confidence="0.966836875">
Rprop ← R(k, p1, ..., pm), i ← 1
1. if (i≤m)
then Rc ← Min(Rprop/n, 1-pi), p-idi ← pi+Rc
else Rc ← Rprop/n, p-idi ← Rc
endif
if (i&lt;n)
then i ← i+1, Rprop ← Rprop - Rc, goto 1
endif
</figure>
<figureCaption confidence="0.999948">
Figure 2. Assessing identification probabilities including repair
</figureCaption>
<bodyText confidence="0.994310025641026">
In order to keep the repair mechanism simple, we approxi-
mate confusability of an object by augmenting its represen-
tation with annotations of all property-value combinations
that do not apply to it, but which could somehow be
perceived as holding for this object. The potentially large
amount of data created this way can be significantly reduced
by making use of inheritance. For example, one can state
that blue and purple (physical) objects can be confused, by
making annotations about confusability with blue for purple
objects, and vice-versa. This annotation is then inherited to
all entities that are specializations of (physical) objects.
The proper repair is then simulated by collecting all
candidates to which the descriptor in question could arguably
apply, and by assigning these candidates a probability of
identification through repair, according to the repair factor,
as assessed above. There are two kinds of candidates: (1)
those to which the descriptor is recognized with some
probability, and (2) those to which it could apply with some
relaxation, that is, which contains a suitable confusability
annotation. The repair factor, which is computed according
to the schema in Figure 1, is then evenly distributed among
these two sets of candidates, provided the added probabilities
of recognition and repair do not get greater than 1 for some
object; this can only be the case if the number of objects to
identify is close to the number of candidates. In such a case,
the extra amount is distributed recursively among the
remaining candidates, always respecting the upper limit of 1.
If the number of objects to identify even exceeds the number
of candidates, the effect of the repair mechanism results in a
modification of the number of objects to identify, reducing it
to the number of available candidates. The computation of
the probability of identification through repair is illustrated
in Figure 2. Three examples in Figure 3 illustrate the effect
of the repair mechanism in quantitative terms. They empha-
size the relation between expectations about the number of
objects to be identified and probabilities of identification.
For k objects to be identified out of n, judging identifi-
cation by descriptor D, which may involve repair measures
(D applies to m out of these n with probabilities pi,...,pm)
</bodyText>
<equation confidence="0.990879666666667">
1. k=1, n=4, m=2 (p1 =0.8, p2 =0.4): Rprop = 0.12
p-id1 = 0.83, p-id2 = 0.43, p-id3 = p-id4 = 0.03
2. k=2, n=4, m=2 (p1 =0.8, p2 =0.4): Rprop = 0.96
p-id1 = 1, p-id2 = 0.6533, p-id3 = p-id4 = 0.2533
3. k=3, n=4, m=2 (p1 =0.8, p2 =0.4): Rprop = 2.1
p-id1 = 1, p-id2 = 1, p-id3 = p-id4 = 0.65
</equation>
<figureCaption confidence="0.998749">
Figure 3. Examples of assessing identification probabilities
</figureCaption>
<bodyText confidence="0.999967666666667">
Specifically, the increasing contributions of the repair
facility are shown, which will be even more pronounced with
several attributes associated with limited recognition expect-
ations. We will see this effect in context with building
descriptor combination in the next section, as well as in the
detailed exposition of an example in Appendix II.
</bodyText>
<sectionHeader confidence="0.978365" genericHeader="method">
4 Identifiability of Descriptor Compositions
</sectionHeader>
<bodyText confidence="0.999942382352941">
Since a single descriptor is rarely sufficient for identifying
one or several objects in scenarios of interesting complexity,
boolean compositions of descriptors are generated for this
purpose, conjunctions being required for building identifying
expressions for single objects. Their probability of recog-
nition is a simple extension of the case of single descriptors.
If pi is the probability of recognition of descriptor Di for
some object O, an expression consisting of several Di
(i=1,pn) is identified with O through recognition if all Di are
attributed to O. The probability of this coincidence amounts
to the product of all probabilities Πpi (i=1,pn).
The probability of identification through repair is
computed by distributing the repair factor R(k,P1,...,Pm),
where each Pj=Πpji (j=1,m;i=1,pn), among all objects quali-
fying for the repair measure. While this distribution is an
equal one for the case of a single descriptor, apart from using
the upper limit of 1 for the total probability, such an even
distribution would not do full justice here. We propose to
distribute the likelihood proportionally to the probabilities of
recognition for each descriptor, which makes repair more
likely applicable to those objects which are also more likely
to be identified anyway. In order to perform this operation
properly, “average” probabilities (ap) for only reparable
descriptors must be estimated. Moreover, we want to favor
repairs for objects which require fewer “average” probabilities
for this computation, by incorporating a &amp;quot;scale-down factor”
(sdf) for each additional repair. The computation schema is
given in Figure 4. For concrete computations, we choose 0.5
for both factors ap and sdf – see the examples in Figure 5.
The first one demonstrates the partitioning of the repair
factor according to the number of attributes which require
repair. Specifically, the first three objects get the same share
of the repair factor, while the fourth object gets only half of
it, since its identification is the only one which requires
</bodyText>
<figure confidence="0.685984260869565">
Compute identification probability (D1,...,Dnp,k,O1,...,On)
O1,...,Om Objects to which all D1,...,np are applicable
Om+1,...,On Objects with repair possible for all D1,...,np
pi1,...,pinp Probability that D1,...,np is attributed to Oi
Objects ordered along degrees of identification confidence:
∀i,j(1≤i,j≤m): (Πl=1,nppil &gt; Πl=1,nppjl) → (i &gt; j)
for i from 1 to n do
Pi ← 1, sdfi ← 1/sdf
for j from 1 to np do
if pij &gt; 0
then Pi ← Pipij
else Pi ← Piap, sdfi ← sdfisdf
endif
endfor
endfor
Rprop ← R(k,Pi,...,Pm), i ← 1, P ← Σi=1,nPi
1. if (i≤m)
then Rc ← Min(Rprop(Pi/P),1-Pi), p-idi ← Pi+Rc
else Rc ← Rprop(Pisdfi/P), p-idi ← Rc
endif
if (i&lt;n)
then i ← i+1, Rprop ← Rprop - Rc, goto 1
endif
</figure>
<figureCaption confidence="0.999924">
Figure 4. Identification probabilities for several descriptors
</figureCaption>
<bodyText confidence="0.998812551724138">
repair regarding two descriptors. The second example features
the impact of multiple intended referents on the repair factor,
which increases the probabilities of identification substan-
tially. The last example illustrates the compensative effect
between comparably low probabilities of recognition and
higher ones in connection with the requirement of using the
repair facility. Specifically, this example demonstrates that
the probability of identification for an object (the second
one) that is only identifiable through the repair mechanism
can even become higher than the probability of identification
for an object (the second one) that does not require repair for
being identified. However, such an effect is only possible in
the context of descriptors applicable with some degree of
confidence to both candidates, but strongly favoring the
object whose identification relies on the repair mechanism
due to mismatch with another descriptor. This is the most
critical effect in choosing descriptors.
The incorproation of disjunctions and negations is more
local, since this extension only generalizes the probability
of recognition of a single property. This is because these
operators appear only in embedded boolean combinations
[van Deemter 2002], which are the basis for building larger
varieties of expressions [Horacek 2004]. For disjunctions of
two descriptors with associated probabilities p1 and p2, the
joint probability amounts to p1+p2-p1p2, assuming indepen-
dence, which is quite normal for descriptors originating from
For k objects to be identified out of n, judging identifi-
cation by np descriptors D, at least repair possible for all
(Dj applies to object i with probability pji, ∀i≤m: pji &gt; 0)
</bodyText>
<equation confidence="0.986764888888889">
1. k=1, n=4, m=1, np=2 (p11 =0.5, p21 =0.5, p12 =0.5,
p22 =0, p13 =0, p23 =0.5, p14 =0, p24 =0): Rprop = 0.75
p-id1 = 0.464, p-id2 = 0.214, p-id3 = 0.214, p-id4 = 0.107
2. k=2, n=3, m=1, np=2 (p11 =0.5, p21 =0.6, p12 =0.6,
p22 =0.5, p13 =0, p23 =0.55): Rprop = 1.4
p-id1 = p-id2 = 0.766, p-id3 = 0.466
3. k=1, n=2, m=1, np=3 (p11 =0.5, p21 =0.5, p31 =0.5,
p12 =0.9, p22 =0.9, p32 =0): Rprop = 0.875
p-id1 = 0.331, p-id2 = 0.668
</equation>
<figureCaption confidence="0.998164">
Figure 5. Examples of assessing identification probabilities
</figureCaption>
<bodyText confidence="0.999875666666667">
distinct properties. For some properties, prominently those
associated with vagueness, building disjunctions of
descriptors originating from the same property may be
beneficial. For example, disjunctions of similar colors or
shapes may reduce the uncertainty through combining the
identifiability of both. A simple way to model this constel-
lation is by assigning probabilities to the set of applicable
values so that their sum does not exceed 1, thereby modeling
exclusion of the co-occurrence of more than one value.
Consequently, the associated probabilities can simply be
added. Propagation of the “confusable” annotation is treated
similarly – if at least one of the descriptors is marked as
“confusable”, this also holds for the disjunction. For dealing
with negation, the probability of identification is simply
inverted (1-p). The treatment of the “confusable” annotation,
however, is a bit problematic. The invertion operation needs
modification through anticipating the amount of the repair
factor, but this cannot be done locally. Therefore, this factor,
rf, must be estimated in advance. For concrete computations
we use a value of 0.1, so that ¬p for a “confusable” p
amounts to 0.9.
</bodyText>
<sectionHeader confidence="0.98793" genericHeader="method">
5 An Algorithm Incorporating Uncertainties
</sectionHeader>
<bodyText confidence="0.999952333333334">
In this section, we describe extensions to the algorithm by
Dale and Reiter [1995] that take into account the measures
addressing uncertainty introduced in previous sections. This
reference algorithm takes an intended referent r (the generali-
zation to several referents is straightforward), the attributes P
that describe r, and a contrast set C, and incrementally builds
an identifying description L, if possible. The algorithm
assumes an environment with three interface functions:
BasicLevelValue, accessing basic level categories of objects
[Rosch 1978], MoreSpecificValue for accessing incremen-
tally specialized values of an attribute according to a taxo-
nomic hierarchy, and UserKnows for judging whether the
user is familiar with the attribute value of an object.
The algorithm basically iterates over the attributes P,
according to some predetermined ordering which reflects
preferences in the domain of application. For each attribute
in P, a value assumed to be known to the user is determined,
so that this value describes the intended referent and rules out
at least one potential distractor which is still in the contrast
set C in the iteration step considered. If such a value can be
found, a pair consisting of the attribute and this value is
included in the identifying description L. This step is
repeated until the list P is exhausted or a distinguishing
description is found, that is, the contrast set C is empty.
Unless the distinguishing description L does not contain a
descriptor expressible as a head noun, such a descriptor is
added. Choosing the value of an attribute is done by an
embedded iteration. It starts with the basic level value attri-
buted to r, after which more specific values also attributed to
r and assumed to be known to the user are tested for their
discriminatory power. Finally, the least specific value that
excludes the largest number of potential distractors and is
known to the user is chosen. The schema of this procedure
is given in Appendix I. The only modification we have done
to the original version is the result of L as a non-distin-
guishing description in case of identification failure.
The algorithm by Dale and Reiter contains the principal
operations that also other algorithms for generating referring
expressions apply. The extension to boolean combinations
of descriptors by van Deemter is essentially realized as an
iteration around the Dale and Reiter algorithm, through
building increasingly complex combinations, which other
control regimes generate and maintain more effectively.
In order to control effects of facilities dealing with uncer-
tainty, the extended algorithm has four control parameters:
</bodyText>
<listItem confidence="0.957182083333333">
• pmin, the minimal probability of recognition required
for an attribute-value pair applicable to the intended
referent, to justify its inclusion in the description,
• 4p1, the minimal improvement in terms of probabi-
lity of identification of the intended referent over a
potential distractor obtained through an additional
attribute-value pair,
• 4p2, the minimal preference in terms of probability
of identification of the intended referent over all poten-
tial distractors obtained through a description, and
• Complexity-limit, an upper bound on the number of
descriptors collected in the distinguishing description.
</listItem>
<bodyText confidence="0.9945515">
In order to incorporate our concepts of representing
uncertainty in this algorithm, we have to replace the inter-
face functions which access crisp data and we must modify
yes-no decisions. These enhancements concern:
</bodyText>
<listItem confidence="0.988238333333333">
• the decision about whether a descriptor excludes a
potential distractor (in the function RulesOut),
• the choice of a value for an attribute (in the function
FindBestvalue), and
• the termination of the overall procedure (in the
function MakeReferringExpression)
</listItem>
<bodyText confidence="0.999489272727273">
Modifications of the reference algorithm are given in
detail in the extended version in Appendix I – some lines are
marked by labels [Ni] for references from the text.
Expressions of the form pr(r,L) compute the probability of
identification of referent r through the description L,
according to the schema described in the previous sections.
Under conditions of uncertainty, determining whether a
descriptor excludes a potential distractor may become a
proper decision rather than a mere computation. A clear-cut
case is only present if the repair facility is not applicable to
one of the members of the contrast set, so that its associated
probability of identification amounts to 0. This condition
replaces the criterion that the user must know that this
descriptor does not apply to some potential distractor in the
function RulesOut [N7]. However, it would be a rather
restrictive strategy to accept only those descriptors which
definitely exclude a potential distractor. In fact, none of the
descriptors that make up the example in Appendix II yield
such a crisp discrimination. In addition to that, a descriptor is
also valuable if it contributes to a better identification of the
intended referent by increasing the difference to a potential
distractor in the associated probabilities of identification by a
significant margin (4p1). This criterion is added to the crisp
criterion described above, encapsulated in the function Domi-
nate [N8], which is used for this decision instead of the
function RulesOut [N2]. The idea is that subsequently chosen
descriptors have comparable effects on the identification of
some of the other potential distractors, so that the intended
referent ultimately gains over all of them. The significance
of this margin must be tuned in such a way that the gain
over some potential distractors is not outweighted by a loss
over some other potential distractors.
The suitability of a value for an attribute depends on two
factors associated with uncertainty: the probability of recog-
nition associated with that value for the present user, and the
effect of this value on excluding elements from the set of
potential distractors. These two factors have adverse effects:
while a more specific value has the potential of excluding an
increasing number of potential distractors, its probability of
recognition when applied to the intended referent may be
lower than that of a less specific value. Consequently, it is
not necessarily the case that an improved discriminatory
power leads to a better overall effect. Hence, the choice of a
value requires a minimal probability of recognition (pmin,
[N6]), and calls to Dominate replace calls to RulesOut. Addi-
tional variants of descriptors can be generated by enhancing
the interface function MoreSpecificValue, also building
disjunctions of values excluding each other, to cover cases
described at the end of Section 4, that is, building
disjunctions of descriptors by composing descriptors
(possibly vague ones) that cover adjacent value ranges.
The third factor, the termination criterion, is adapted to
uncertainties by enhancing it in two ways: (1) a complexity
limit is applied to the specifications in the description L
[N3]; while this cut-off may serve practical considerations
also without conditions of uncertainty (for a partitioning into
sequences of descriptions [Horacek 2004]), it gains on rele-
vance in uncertain environments. (2) a certain degree of being
Dominant in the probability of identification over all
potential distractors is considered sufficient (4p2, [N4]) rather
than requiring the ultimate exclusion of all potential
distractors. Finally, the conditions under which descriptors
are selected, give rise to an optional optimization step. The
prerequisite for this step is the distinction between
descriptors which definitively exclude at least one potential
distractor (Lro in the extended algorithm, [N1]) and others
which only affect their associated probabilities of identifi-
cation, but do not make them 0. Then all subsets of the
description built which contain at least Lro are examined
[N5] whether they yield a better preference over all potential
distractors in terms of their probabilities of identification
[N9]. Through this measure, an early chosen descriptor with
a probability of identification lower for the intended referent
than for some potential distractors can finally be discarded,
provided the discriminating effect on other potential
distractors is also achieved by later chosen descriptors. In the
example in Appendix II, all descriptors are categorized as
optional ones, but for the one expressing the head noun –
which is precisely the reason why it is not optional.
Altogether, the algorithm selects descriptors which either
exclude some potential distractors definitively, makes some
of them rely on the repair mechanism, or simply increases
the probability of identification of the intended referent
considerably in comparison to elements of the contrast set.
While this selection process works reasonably in most
cases, it may turn out as problematic when several of the
descriptors chosen are associated with limited probabilities
of recognition for the intended referent in comparison to
potential distractors not completely excluded. As a conse-
quence, these potential distractors may be judged superior in
terms of the probability of identification even though they
rely on the repair mechanism (see example 3 in Figure 5).
This risk can be circumvented by using a relatively high
pmin parameter, but this measure may easily lead to the
exclusion of an otherwise beneficial descriptor under normal
conditions. An improvement can be obtained by the call to
the procedure Optimize. If one of the first two descriptors
used in example 3 in Figure 5 does not definitively exclude a
potential distractor, the procedure Optimize tests descriptor
combinations without it, and one of those may yield a better
result – see also the example in Appendix II. A possible
variations would be to allow just a single violation of the
pmin restriction, for a descriptor with very good discrimi-
natory power.
So far, we have only elaborated changes for incorpo-
rating uncertainty concepts to the reference algorithm per se.
Handling boolean combinations of descriptors through
applying the reference algorithm to increasingly complex
combinations also works with uncertainties, since all
computations required are defined. More difficulties arise
with ambitious control regimes, which rely on cut-off
techniques, in addition to the complexity cut-off, such as
dominance and value cut-offs, as introduced in [Horacek
2004]. A complexity cut-off is already included in the
extended reference algorithm. The two other cut-offs can be
generalized, but this is likely to be associated with a
significant loss of efficiency. In order for a descriptor to
dominate another one, the dominating one must not only
exclude all potential distractors that its competitor does, but
it must also favor the intended referents over all potential
distractors in terms of the associated probabilities of identi-
fication – this requirement reduces the application frequency
of this cut-off considerably. A value cut-off, in turn, is
applicable to a partial solution if a solution has already been
found, and there are no descriptor combinations untested for
the partial solution which may yield a solution with less
complex specifications. This condition can also be met in
the environment associated with uncertainties. In this
environment, however, there is another factor that has an
impact on the quality of the solution, that is the probability
of identification, which cannot be assessed prior to actually
choosing a descriptor and testing its effects.
</bodyText>
<sectionHeader confidence="0.994734" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99945525">
In this paper, we have presented an approach for generating
referential descriptions under conditions of uncertainty. The
approach combines a proper recognition of objects associated
with some degree of uncertainty, as well as identification
through a repair mechanism, motivated by the need to
identify objects even for descriptions that originally appear
uninterpretable. On these lines, we have reinterpreted
concepts of algorithms generating referring expressions in
view of uncertainties about the appearance of objects.
Incorporating measures of uncertainty in such an algorithm
attacks strong assumptions and effects underlying most of
the existing algorithms:
</bodyText>
<listItem confidence="0.916754142857143">
• They typically require crisp specifications concerning
attribution of descriptors to referents and knowledge of
the audience. Especially the connection to modern user
models may require coarse-grained interpretations here.
• A single result is produced even if several reasonable
variants exist, and this choice is implicitly determined
by the preference ordering imposed on the descriptors.
• The interaction with other components of an NL gener-
ation system and an embedding dialog system is rather
limited. Reference generation is typically conceived as
a pure functional service, with no feedback, taking into
account syntactic constraints, at best (e.g., [Horacek
1997]). An embedding dialog system has no chance to
find out possible sources for an identification failure.
</listItem>
<bodyText confidence="0.99786">
The algorithm incorporating measures to deal with uncer-
tainties provides facilities to improve this situation:
</bodyText>
<listItem confidence="0.878424375">
• Specifications concerning attribution of descriptors to
referents and knowledge of the audience can be done in
a direct fashion, requiring no interpretations.
• There are some parameters to control the choice of
descriptors, the conciseness and expected effectiveness
of the result, including an afterwards optimization
which only requires re-calculation of probabilities.
• The probabilities of identification associated with the
</listItem>
<bodyText confidence="0.700778857142857">
intended referents and those potential distractors that
fall under the repair facility give an indication about
the likelihood of success of the identification task and
also about potential sources for a failure. Moreover,
the situation about probabilities and descriptors may
suggest variants in building surface expressions, such
as putting emphasis on a critical descriptor.
</bodyText>
<sectionHeader confidence="0.949302" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.921889045045045">
[Appelt 1985] Doug Appelt. Planning English Referring
Expressions. Artificial Intelligence 26:1-33, 1985.
[Appelt and Kronfeld 1987] Doug Appelt and Amichai
Kronfeld. A Computational Model of Referring. In Proc.
of the 10th International Joint Conference on Artificial
Intelligence (IJCAI-87), pp. 640-647, Milano, Italy,
1987.
[Bateman 1999] John Bateman. Using Aggregation for
Selecting Content when Generating Referring
Expressions. In Proc. of the 37th Annual Meeting of the
Association for Computational Linguistics (ACL-99),
pp. 127-134, University of Maryland, 1999.
[Dale 1988] Robert Dale. Generating Referring Expressions
in a Domain of Objects and Processes. PhD Thesis,
Centre for Cognitive Science, University of Edinburgh,
1988.
[Dale and Reiter 1995] Robert Dale and Ehud Reiter.
Computational Interpretations of the Gricean Maxims in
the Generation of Referring Expressions. Cognitive
Science 18:233-263, 1995.
[Donellan 1966] K. Donellan. Reference and Definite
Description. Philosophical Review 75:281-304, 1966.
[Edmonds 1994] Phil Edmonds. Collaboration on Reference
to Objects that are not Mutually Known. In Proc. of the
15th International Conference on Computational Lingu-
istics (COLING-94), pp. 1118-1122, 1994.
[Gardent 2002] Claire Gardent. Generating Minimal Definite
Descriptions. In Proc. of the 40th Annual Meeting of
the Association for Computational Linguistics (ACL-
2002), pp. 96-103, Philadelphia, Pennsylvania, 2002.
[Goodman 1986] Bradley Goodman. Reference Identification
and Reference Identification Failures. Computational
Linguistics 12:273-305, 1986.
[Goodman 1987] Bradley Goodman. Communication and
Miscommunication. Association of Computational
Linguistics Series of Cambridge University Press,
London, England, 1987.
[Grosz and Sidner 1986] Barbara Grosz and Candace Sidner.
Attention, Intention, and the Structure of Discourse.
Computational Linguistics 12:175-206, 1986.
[Heeman and Hirst 1995] Peter Heeman and Graeme Hirst.
Collaborating on Referring Expressions. Computational
Linguistics 21:351-382, 1995.
[Horacek 1997] Helmut Horacek. An Algorithm for
Generating Referential Descriptions with Flexible Inter-
faces. In Proc. of the 35th Annual Meeting of the
Association for Computational Linguistics and 8th
Conference of the European Chapter of the Association
for Computational Linguistics (ACL-EACL&apos;97), pp.
206-213, Madrid, Spain, 1997.
[Horacek 2003] Helmut Horacek. A Best-First Search
Algorithm for Generating Referring Expressions. In
Proc. of the 10th Conference of the European Chapter
of the Association for Computational Linguistics
(EACL-2003), Conference Companion (short paper), pp.
103-106, Budapest, Hungary, 2003.
[Horacek 2004] Helmut Horacek. On Referring to Sets of
Objects Naturally. In Proc. of the Third International
Conference on Natural Language Generation (INLG-
2004), pp. 70-79, Brockenhurst, UK, 2004.
[Krahmer, v. Erk and Verleg 2001] Emiel Krahmer, S. v.
Erk, André Verleg. A Meta-Algorithm for the Generation
of Referring Expressions. In Proc. of the 8th European
Workshop on Netural Language Generation (EWNLG-
2001), pp. 29-39, Toulouse, France, 2001.
[Kronfeld 1986] Amichai Kronfeld. Donellan&apos;s Distinction
and a Computational Model of Reference. In Proc. of the
24th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-86), pp. 186-191, New York,
NY, 1986.
[Levelt 1989] William Levelt. Speaking: From Intention to
Articulation. MIT Press, 1989.
[McDonald 1981] David McDonald. Natural Language Gener-
ation as a Process of Decision Making under Constraints.
PhD thesis, MIT, 1981.
[Pearl 1988] Judea Pearl. Probabilistic Reasoning in Intel-
ligent Systems: Networks of Plausible Inferences.
Morgan Kaufman, San Mateo, California, 1988.
[Pechmann 1989] Thomas Pechmann. Incremental Speech
Production and Referential Overspecification. Linguistics
27:89-110, 1989.
[Reiter 1990] Ehud Reiter. The Computational Complexity
of Avoiding Conversational Implicatures. In Proc. of the
28th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-90), pp. 97-104, Pittsburgh,
Pennsylvania, 1990.
[Reiter and Dale 1992] Ehud Reiter and Robert Dale.
Generating Definite NP Referring Expressions. In Proc.
of the 14th International Conference on Computational
Lingustics (COLING-92), pp. 232-238, Nantes, France,
1992.
[Rosch 1978] Eleanor Rosch. Principles of Categorization.
In E. Rosch and B. Llyod (eds.) Cognition and Catego-
rization, pp. 27-48, Hillsdale, NJ: Lawrence Erlbaum,
1978.
[Stone 2000] Matthew Stone. On Identifying Sets. In Proc.
of the First International Conference on Natural Langu-
age Generation (INLG-2000), pp. 116-123, Mitzpe
Ramon, Israel, 2000.
[van Deemter 2002] Kees van Deemter. Generating Referring
Expressions: Boolean Extensions of the Incremental
Algorithm. Computational Linguistics, 28(1):37-52,
2002.
[Wahlster 2004] Wolfgang Wahlster. REAL: REssourcen-
Adaptive Lokalisation. Project in SFB 378, Saarland
University, 2004.
[Zadeh 1984] Lofti Zadeh. Making Computers Think like
People. IEEE Spektrum, 8:26-32, 1984.
[Zadeh 1996] Lofti Zadeh. Fuzzy Logic and the Calculi of
Fuzzy Rules and Fuzzy Graphs. International Journal of
Multi-Valued Logic, 8:1-39, 1996.
</reference>
<sectionHeader confidence="0.375551" genericHeader="method">
Appendix I: Reference Algorithm ([Dale and Reiter 1995], left) and Extended Algorithm (right)
</sectionHeader>
<equation confidence="0.8670915">
MakeReferringExpression (r,C,P)
L ← {}
for each member Ai of list P do
V = FindBestValue(r,Ai,BasicLevelValue(r,Ai))
</equation>
<figure confidence="0.926738737704918">
if RulesOut(&lt;Ai,V&gt;) ≠ nil
then L ← L ∪ {&lt;Ai,V&gt;}
C ← C - RulesOut(&lt;Ai,V&gt;)
endif
if C = {} then
if &lt;type,X&gt; ∈ L for some X
then return L (an identifying description)
else return L ∪ {&lt;type,BasicLevelValue(r,type)&gt;}
endif
endif
return L (a non-identifying description)
FindBestValue (r,A,initial-value)
if UserKnows(r,&lt;A,initial-value&gt;) = true
then value ← initial-value
else value ← no-value
endif
if (spec-value ← MoreSpecificValue(r,A,value)) ≠ nil A
(new-value ← FindBestValue(r,A,spec-value)) ≠ nil A
(|RulesOut(&lt;A,new-value&gt;) |&gt; |RulesOut(&lt;A,value&gt;)|)
then value ← new-value
endif
return value
RulesOut (A,V)
if V = no-value
then return nil
else return {x: x ∈ C A UserKnows(x,&lt;A,V&gt;) = false}
endif
MakeReferringExpression (r,C,P)
L ← {}, Lro ← {} [N1]
for each member Ai of list P do
V = FindBestValue(r,Ai,BasicLevelValue(r,Ai))
if Dominate(Ai,V) ≠ nil [N2]
then L ← L ∪ {&lt;Ai,V&gt;}
C ← C - RulesOut(&lt;Ai,V&gt;)
if RulesOut(&lt;Ai,V&gt;) ≠ nil
then Lro ← Lro ∪ {&lt;Ai,V&gt;}
endif endif
if (C = {}) ∨ (|L |&gt; Complexity-limit) ∨ [N3]
(∀x ∈ C: (pr(r,L) - pr(x,L)) &gt; Δp2) then [N4]
L ← Optimize(L,Lro) (optional) [N5]
if &lt;type,X&gt; ∈ L for some X
then return L (a likely identifying description)
else return L ∪ {&lt;type,BasicLevelValue(r,type)&gt;}
endif endif
return L (an unlikely identifying description)
FindBestValue (r,A,initial-value)
if pr(r,{&lt;A,initial-value&gt;}) &gt; pmin [N6]
then value ← initial-value
else value ← no-value
endif
if (spec-value ← MoreSpecificValue(r,A,value)) ≠ nil A
(new-value ← FindBestValue(r,A,spec-value)) ≠ nil A
(|Dominate(A,new-value) |&gt; |Dominate(A,value)|)
then value ← new-value
endif
return value
RulesOut (A,V)
if V = no-value then return nil
else return {x: x ∈ C A (pr(x,L ∪ {&lt;A,V&gt;}) = 0)} [N7]
endif
Dominate (A,V)
</figure>
<bodyText confidence="0.607742">
if V = no-value then return nil
</bodyText>
<equation confidence="0.8914015">
else return {x: x ∈ C A ((pr(x,L ∪ {&lt;A,V&gt;}) = 0) ∨
((pr(r,L ∪ {&lt;A,V&gt;}) - pr(x,L ∪ {&lt;A,V&gt;})) - [N8]
(pr(r,L) - pr(x,L))) &gt; Δp1)}
endif
Optimize (L1,L2)
Lopt ← L1, ppopt ← Min∀x ∈ C(pr(r,L1) - pr(x,L1)) [N9]
forall L (L1 ⊇ L ⊇ L2) do
pp ← Min∀x ∈ C(pr(r,L) - pr(x,L))
</equation>
<bodyText confidence="0.918275">
if ppopt &lt; pp then Lopt ← L, ppopt ← pp endif
endforall
return Lopt
</bodyText>
<sectionHeader confidence="0.970715" genericHeader="method">
Appendix II: An Example of the Extended Algorithm at Work
</sectionHeader>
<bodyText confidence="0.9546975">
In this section, we demonstrate the functionality of the
extended algorithm by an example that illustrates several
features of this algorithm. As in the discussion in Section
2, the scenario consists of three similar dogs, one of which
is a bassett, which is also the intended referent. In addition,
the bassett is brownish and has a long tail. The other two
dogs have shorter tails and their skin is also brownish, but
with some white resp. black portions, which makes the
descriptor &apos;brownish&apos; less appropriate than for the bassett.
To make the example suitable for our purposes, the
audience is assumed to have little knowledge about dog
specifics, that is, they may recognize the intended referent
as a bassett, but this is not very likely (we assume the
category identification has a likelihood of 30%). In
addition, it is assumed that the tails of the dogs, specific-
ally the one of the bassett, cannot be observed easily by the
audience (again, we assume the recognition has a likelihood
of 30%). Both, dog category and tail length are potentially
confusable for all dogs. These properties and associated
probabilities of identification as listed below.
Objects category Attributes tail-length
color
dog1 bassett brownish long
dog2 dog brown-white short
dog3 dog brown-black short
Probabilities of identification (per attribute-value and object)
bassett: p(dog1) = 0.3, p(dog2) = 0.0, p(dog3) = 0.0
brownish: p(dog1) = 0.9, p(dog2) = 0.8, p(dog3) = 0.8
long-tail: p(dog1) = 0.3, p(dog2) = 0.0, p(dog3) = 0.0
Hence, the intended referent r is {dog1}, and the contrast set
C is {dog2, dog3}. For demonstration purposes, we choose
the following parameterizations:
</bodyText>
<listItem confidence="0.961097">
• The attributes are considered according to the ordered
preference list P = (color, category, tail-length), which
in some sense reflects the ease of perception of color
• We choose 5% (0.05) for Ap1, which indicates suffi-
cient dominance, and 30% (0.3) for Apmin, which
indicates sufficient identification potenial (it always
succeeds in the example); similarly, we choose 50%
(0.5) for Ap2 which indicates sufficient discrimination
(it never succeeds here, hence all descriptors are tried)
• We allow a maximum complexity of 3 descriptors, so
that this cut-off criterion does not apply in our simple
example, and we use the optional optimization step
• In order to compute probabilities of identification, we
need to choose a values for the ”scale-down factor”,
</listItem>
<bodyText confidence="0.99336525">
which will be 0.5, as mentioned in Section 4. Since
no disjunctions and negations of descriptors are needed
for our example, no further parameters are required.
We now illustrate the generation process step by step.
In the first step, “brownish” is chosen as the value of the
attribute “color” of dog1, and its contribution to discriminate
the intended referent from the elements of the contrast set is
checked. To start with, its identification potential, 0.9, is far
higher than pmin (0.25).Since this descriptor is also applic-
able to both other dogs, dog2 and dog3, but with lower
probability, these two objects still remain in the contrast set.
Despite this limited discrimination, the attribute is chosen
because it achieves more than the minimal dominance
required: 0.9 - 0.8 equals 0.1, which is higher than Ap2
(0.05). Thus, the situation after step 1 is as follows (we
neglect the repair factor, which is as low as 0.1 x 0.2 x 0.2):
</bodyText>
<equation confidence="0.9852065">
pr(dog1) = 0.9, pr(dog2) = pr(dog3) = 0.8
Apr = pr(dog1) - Max(pr(dog2), pr(dog3)) = 0.1 &gt; Ap1
</equation>
<bodyText confidence="0.999893933333333">
In the next step, “bassett” is chosen as the value of the
attribute “category” of dog1, and again its contribution to
discriminate the intended referent from the elements of the
contrast set is checked. Its identification potential, 0.3, is
sufficient, since it is higher than pmin (0.25). This descriptor
is not applicable to the other dogs, dog2 and dog3. Never-
theless, they still remain in the contrast set since they poten-
tially are subject to the repair mechanism. The probabilities
of identification are computed according to the schema in
Figure 4: the product of the probabilities of &amp;quot;bassett&amp;quot; and
&amp;quot;brownish&amp;quot; associated with dog1 yields 0.27. The repair
factor, the complementing 0.73, is distributed evenly among
all three dogs. Hence, the degree of dominance of this
descriptor amounts to 0.27, which is higher than Ap2 (0.05).
Thus, the situation after step 2 is as follows:
</bodyText>
<equation confidence="0.909245">
pr(dog1) = 0.513, pr(dog2) = pr(dog3) = 0.243
Apr = pr(dog1) - Max(pr(dog2), pr(dog3)) = 0.27 &gt; Ap1
</equation>
<bodyText confidence="0.999990666666667">
In the last step, “long” is chosen as the value of the attri-
bute “tail-length”. Again, its identification potential, 0.3,
is sufficient, since it is higher than pmin (0.25). As in the
previous step, dog2 and dog3 still remain in the contrast set
since they potentially are subject to the repair mechanism.
The product of the probabilities for dog1 results from the
previous one, multiplied by 0.3, which yields 0.081. The
repair factor, the complementing 0.919, is distributed by
giving two parts to dog1 (the scale-down factor applies) and
one part to each of the other dogs. Hence, the degree of
dominance of this descriptor amounts to 0.31075, which is
higher than Ap2 (0.05), which gives the final situation:
</bodyText>
<equation confidence="0.922879">
pr(dog1) = 0.5405, pr(dog2) = pr(dog3) = 0.22975
Apr = pr(dog1) - Max(pr(dog2), pr(dog3)) = 0.31075 &gt; Ap1
</equation>
<bodyText confidence="0.999887428571429">
Optimization attempts show that “bassett” only is slightly
inferior (pr(dog1) = 0.53, pr(dog2) = pr(dog3) = 0.23, Apr =
0.3), while “bassett” together with “long-tailed” is slightly
superior (pr(dog1) = 0.545, pr(dog2) = pr(dog3) = 0.225, Apr
= 0.32) to the combination of all descriptors. Hence, the
example demonstrates benefits and risks are comparable when
only limited discrimination is possible by each descriptor.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.355233">
<title confidence="0.997232">Generating Referential Descriptions Under Conditions of Uncertainty</title>
<author confidence="0.481827">Helmut</author>
<affiliation confidence="0.569595">Universität des</affiliation>
<address confidence="0.6957515">F.R. 6.2 Postfach 151150, D-66041 Saarbrücken, Germany</address>
<email confidence="0.997321">horacek@cs.uni-sb.de</email>
<abstract confidence="0.998737555555556">Algorithms for generating referring expressions typically assume that an object in a scenary can be identified through a set of commonly agreed properties. This is a strong assumption, since in reality properties of objects may be perceived differently among people, due to a number of factors including vagueness, knowledge discrepancies, and limited perception capabilities. Taking these discrepancies into account, we reinterpret concepts of algorithms generating referring expressions in view of uncertainties about the appearance of objects. Our model includes two complementary measures of likelihood in object identification, and adapted property selection and termination criteria. The approach is relevant for situations with potential perception problems and for scenarios with knowledge discrepancies between conversants.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Doug Appelt</author>
</authors>
<title>Planning English Referring Expressions.</title>
<date>1985</date>
<journal>Artificial Intelligence</journal>
<pages>26--1</pages>
<marker>[Appelt 1985]</marker>
<rawString>Doug Appelt. Planning English Referring Expressions. Artificial Intelligence 26:1-33, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Appelt</author>
<author>Amichai Kronfeld</author>
</authors>
<title>A Computational Model of Referring.</title>
<date>1987</date>
<booktitle>In Proc. of the 10th International Joint Conference on Artificial Intelligence (IJCAI-87),</booktitle>
<pages>640--647</pages>
<location>Milano, Italy,</location>
<marker>[Appelt and Kronfeld 1987]</marker>
<rawString>Doug Appelt and Amichai Kronfeld. A Computational Model of Referring. In Proc. of the 10th International Joint Conference on Artificial Intelligence (IJCAI-87), pp. 640-647, Milano, Italy, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Bateman</author>
</authors>
<title>Using Aggregation for Selecting Content when Generating Referring Expressions.</title>
<date>1999</date>
<booktitle>In Proc. of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-99),</booktitle>
<pages>127--134</pages>
<institution>University of Maryland,</institution>
<marker>[Bateman 1999]</marker>
<rawString>John Bateman. Using Aggregation for Selecting Content when Generating Referring Expressions. In Proc. of the 37th Annual Meeting of the Association for Computational Linguistics (ACL-99), pp. 127-134, University of Maryland, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Generating Referring Expressions in a Domain of Objects and Processes.</title>
<date>1988</date>
<tech>PhD Thesis,</tech>
<institution>Centre for Cognitive Science, University of Edinburgh,</institution>
<marker>[Dale 1988]</marker>
<rawString>Robert Dale. Generating Referring Expressions in a Domain of Objects and Processes. PhD Thesis, Centre for Cognitive Science, University of Edinburgh, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<date>1995</date>
<journal>Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions. Cognitive Science</journal>
<pages>18--233</pages>
<marker>[Dale and Reiter 1995]</marker>
<rawString>Robert Dale and Ehud Reiter. Computational Interpretations of the Gricean Maxims in the Generation of Referring Expressions. Cognitive Science 18:233-263, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Donellan</author>
</authors>
<title>Reference and Definite Description. Philosophical Review 75:281-304,</title>
<date>1966</date>
<marker>[Donellan 1966]</marker>
<rawString>K. Donellan. Reference and Definite Description. Philosophical Review 75:281-304, 1966.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Edmonds</author>
</authors>
<title>Collaboration on Reference to Objects that are not Mutually Known.</title>
<date>1994</date>
<booktitle>In Proc. of the 15th International Conference on Computational Linguistics (COLING-94),</booktitle>
<pages>1118--1122</pages>
<marker>[Edmonds 1994]</marker>
<rawString>Phil Edmonds. Collaboration on Reference to Objects that are not Mutually Known. In Proc. of the 15th International Conference on Computational Linguistics (COLING-94), pp. 1118-1122, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Gardent</author>
</authors>
<title>Generating Minimal Definite Descriptions.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL2002),</booktitle>
<pages>96--103</pages>
<location>Philadelphia, Pennsylvania,</location>
<marker>[Gardent 2002]</marker>
<rawString>Claire Gardent. Generating Minimal Definite Descriptions. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL2002), pp. 96-103, Philadelphia, Pennsylvania, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Goodman</author>
</authors>
<title>Reference Identification and Reference Identification Failures.</title>
<date>1986</date>
<journal>Computational Linguistics</journal>
<pages>12--273</pages>
<marker>[Goodman 1986]</marker>
<rawString>Bradley Goodman. Reference Identification and Reference Identification Failures. Computational Linguistics 12:273-305, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bradley Goodman</author>
</authors>
<title>Communication and Miscommunication. Association of Computational Linguistics Series of Cambridge</title>
<date>1987</date>
<publisher>University Press,</publisher>
<location>London, England,</location>
<marker>[Goodman 1987]</marker>
<rawString>Bradley Goodman. Communication and Miscommunication. Association of Computational Linguistics Series of Cambridge University Press, London, England, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Grosz</author>
<author>Candace Sidner</author>
</authors>
<date>1986</date>
<booktitle>Attention, Intention, and the Structure of Discourse. Computational Linguistics 12:175-206,</booktitle>
<marker>[Grosz and Sidner 1986]</marker>
<rawString>Barbara Grosz and Candace Sidner. Attention, Intention, and the Structure of Discourse. Computational Linguistics 12:175-206, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Heeman</author>
<author>Graeme Hirst</author>
</authors>
<title>Collaborating on Referring Expressions.</title>
<date>1995</date>
<journal>Computational Linguistics</journal>
<pages>21--351</pages>
<marker>[Heeman and Hirst 1995]</marker>
<rawString>Peter Heeman and Graeme Hirst. Collaborating on Referring Expressions. Computational Linguistics 21:351-382, 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Horacek</author>
</authors>
<title>An Algorithm for Generating Referential Descriptions with Flexible Interfaces.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics (ACL-EACL&apos;97),</booktitle>
<pages>206--213</pages>
<location>Madrid,</location>
<marker>[Horacek 1997]</marker>
<rawString>Helmut Horacek. An Algorithm for Generating Referential Descriptions with Flexible Interfaces. In Proc. of the 35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter of the Association for Computational Linguistics (ACL-EACL&apos;97), pp. 206-213, Madrid, Spain, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Horacek</author>
</authors>
<title>A Best-First Search Algorithm for Generating Referring Expressions.</title>
<date>2003</date>
<booktitle>In Proc. of the 10th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2003), Conference Companion (short paper),</booktitle>
<pages>103--106</pages>
<location>Budapest, Hungary,</location>
<marker>[Horacek 2003]</marker>
<rawString>Helmut Horacek. A Best-First Search Algorithm for Generating Referring Expressions. In Proc. of the 10th Conference of the European Chapter of the Association for Computational Linguistics (EACL-2003), Conference Companion (short paper), pp. 103-106, Budapest, Hungary, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Horacek</author>
</authors>
<title>On Referring to Sets of Objects Naturally.</title>
<date>2004</date>
<booktitle>In Proc. of the Third International Conference on Natural Language Generation (INLG2004),</booktitle>
<pages>70--79</pages>
<location>Brockenhurst, UK,</location>
<marker>[Horacek 2004]</marker>
<rawString>Helmut Horacek. On Referring to Sets of Objects Naturally. In Proc. of the Third International Conference on Natural Language Generation (INLG2004), pp. 70-79, Brockenhurst, UK, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>S v Erk</author>
</authors>
<title>André Verleg. A Meta-Algorithm for the Generation of Referring Expressions.</title>
<date>2001</date>
<booktitle>In Proc. of the 8th European Workshop on Netural Language Generation (EWNLG2001),</booktitle>
<pages>29--39</pages>
<location>Toulouse, France,</location>
<marker>[Krahmer, v. Erk and Verleg 2001]</marker>
<rawString>Emiel Krahmer, S. v. Erk, André Verleg. A Meta-Algorithm for the Generation of Referring Expressions. In Proc. of the 8th European Workshop on Netural Language Generation (EWNLG2001), pp. 29-39, Toulouse, France, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amichai Kronfeld</author>
</authors>
<title>Donellan&apos;s Distinction and a Computational Model of Reference.</title>
<date>1986</date>
<booktitle>In Proc. of the 24th Annual Meeting of the Association for Computational Linguistics (ACL-86),</booktitle>
<pages>186--191</pages>
<location>New York, NY,</location>
<marker>[Kronfeld 1986]</marker>
<rawString>Amichai Kronfeld. Donellan&apos;s Distinction and a Computational Model of Reference. In Proc. of the 24th Annual Meeting of the Association for Computational Linguistics (ACL-86), pp. 186-191, New York, NY, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Levelt</author>
</authors>
<title>Speaking: From Intention to Articulation.</title>
<date>1989</date>
<publisher>MIT Press,</publisher>
<marker>[Levelt 1989]</marker>
<rawString>William Levelt. Speaking: From Intention to Articulation. MIT Press, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McDonald</author>
</authors>
<title>Natural Language Generation as a Process of Decision Making under Constraints.</title>
<date>1981</date>
<tech>PhD thesis, MIT,</tech>
<marker>[McDonald 1981]</marker>
<rawString>David McDonald. Natural Language Generation as a Process of Decision Making under Constraints. PhD thesis, MIT, 1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Judea Pearl</author>
</authors>
<title>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inferences.</title>
<date>1988</date>
<publisher>Morgan</publisher>
<location>Kaufman, San Mateo, California,</location>
<marker>[Pearl 1988]</marker>
<rawString>Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inferences. Morgan Kaufman, San Mateo, California, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Pechmann</author>
</authors>
<title>Incremental Speech Production and Referential Overspecification. Linguistics 27:89-110,</title>
<date>1989</date>
<marker>[Pechmann 1989]</marker>
<rawString>Thomas Pechmann. Incremental Speech Production and Referential Overspecification. Linguistics 27:89-110, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
</authors>
<title>The Computational Complexity of Avoiding Conversational Implicatures.</title>
<date>1990</date>
<booktitle>In Proc. of the 28th Annual Meeting of the Association for Computational Linguistics (ACL-90),</booktitle>
<pages>97--104</pages>
<location>Pittsburgh, Pennsylvania,</location>
<marker>[Reiter 1990]</marker>
<rawString>Ehud Reiter. The Computational Complexity of Avoiding Conversational Implicatures. In Proc. of the 28th Annual Meeting of the Association for Computational Linguistics (ACL-90), pp. 97-104, Pittsburgh, Pennsylvania, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ehud Reiter</author>
<author>Robert Dale</author>
</authors>
<title>Generating Definite NP Referring Expressions.</title>
<date>1992</date>
<booktitle>In Proc. of the 14th International Conference on Computational Lingustics (COLING-92),</booktitle>
<pages>232--238</pages>
<location>Nantes,</location>
<marker>[Reiter and Dale 1992]</marker>
<rawString>Ehud Reiter and Robert Dale. Generating Definite NP Referring Expressions. In Proc. of the 14th International Conference on Computational Lingustics (COLING-92), pp. 232-238, Nantes, France, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleanor Rosch</author>
</authors>
<title>Principles of Categorization. In</title>
<date>1978</date>
<booktitle>Cognition and Categorization,</booktitle>
<pages>27--48</pages>
<editor>E. Rosch and B. Llyod (eds.)</editor>
<location>Hillsdale, NJ: Lawrence Erlbaum,</location>
<marker>[Rosch 1978]</marker>
<rawString>Eleanor Rosch. Principles of Categorization. In E. Rosch and B. Llyod (eds.) Cognition and Categorization, pp. 27-48, Hillsdale, NJ: Lawrence Erlbaum, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Stone</author>
</authors>
<title>On Identifying Sets.</title>
<date>2000</date>
<booktitle>In Proc. of the First International Conference on Natural Language Generation (INLG-2000),</booktitle>
<pages>116--123</pages>
<location>Mitzpe Ramon, Israel,</location>
<marker>[Stone 2000]</marker>
<rawString>Matthew Stone. On Identifying Sets. In Proc. of the First International Conference on Natural Language Generation (INLG-2000), pp. 116-123, Mitzpe Ramon, Israel, 2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kees van Deemter</author>
</authors>
<title>Generating Referring Expressions: Boolean Extensions of the Incremental Algorithm.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<pages>28--1</pages>
<marker>[van Deemter 2002]</marker>
<rawString>Kees van Deemter. Generating Referring Expressions: Boolean Extensions of the Incremental Algorithm. Computational Linguistics, 28(1):37-52, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Wahlster</author>
</authors>
<title>REAL: REssourcenAdaptive Lokalisation.</title>
<date>2004</date>
<booktitle>Project in SFB 378,</booktitle>
<institution>Saarland University,</institution>
<marker>[Wahlster 2004]</marker>
<rawString>Wolfgang Wahlster. REAL: REssourcenAdaptive Lokalisation. Project in SFB 378, Saarland University, 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lofti Zadeh</author>
</authors>
<title>Making Computers Think like People.</title>
<date>1984</date>
<journal>IEEE Spektrum,</journal>
<pages>8--26</pages>
<marker>[Zadeh 1984]</marker>
<rawString>Lofti Zadeh. Making Computers Think like People. IEEE Spektrum, 8:26-32, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lofti Zadeh</author>
</authors>
<title>Fuzzy Logic and the Calculi of Fuzzy Rules and Fuzzy Graphs.</title>
<date>1996</date>
<journal>International Journal of Multi-Valued Logic,</journal>
<pages>8--1</pages>
<marker>[Zadeh 1996]</marker>
<rawString>Lofti Zadeh. Fuzzy Logic and the Calculi of Fuzzy Rules and Fuzzy Graphs. International Journal of Multi-Valued Logic, 8:1-39, 1996.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>