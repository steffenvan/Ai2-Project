<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000081">
<title confidence="0.989836">
Automatically Detecting Action Items in Audio Meeting Recordings
</title>
<author confidence="0.98755">
Jason M. Brenier
</author>
<affiliation confidence="0.99655">
Department of Linguistics
Center for Spoken Language Research
Institute of Cognitive Science
University of Colorado at Boulder
</affiliation>
<address confidence="0.973737">
594 UCB
Boulder, Colorado 80309-0594
</address>
<email confidence="0.996559">
jbrenier@colorado.edu
</email>
<author confidence="0.996346">
William Morgan Pi-Chuan Chang Surabhi Gupta
</author>
<affiliation confidence="0.9872575">
Department of Computer Science
Stanford University
</affiliation>
<address confidence="0.926861">
353 Serra Mall
Stanford, CA 94305-9205
</address>
<email confidence="0.994192666666667">
ruby@cs.stanford.edu
pcchang@cs.stanford.edu
surabhi@cs.stanford.edu
</email>
<sectionHeader confidence="0.993835" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999960285714286">
Identification of action items in meeting
recordings can provide immediate access
to salient information in a medium noto-
riously difficult to search and summarize.
To this end, we use a maximum entropy
model to automatically detect action item-
related utterances from multi-party audio
meeting recordings. We compare the ef-
fect of lexical, temporal, syntactic, seman-
tic, and prosodic features on system per-
formance. We show that on a corpus of ac-
tion item annotations on the ICSI meeting
recordings, characterized by high imbal-
ance and low inter-annotator agreement,
the system performs at an F measure of
31.92%. While this is low compared to
better-studied tasks on more mature cor-
pora, the relative usefulness of the features
towards this task is indicative of their use-
fulness on more consistent annotations, as
well as to related tasks.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999925228571429">
Meetings are a ubiquitous feature of workplace
environments, and recordings of meetings pro-
vide obvious benefit in that they can be replayed
or searched through at a later date. As record-
ing technology becomes more easily available and
storage space becomes less costly, the feasibil-
ity of producing and storing these recordings in-
creases. This is particularly true for audio record-
ings, which are cheaper to produce and store than
full audio-video recordings.
However, audio recordings are notoriously diffi-
cult to search or to summarize. This is doubly true
of multi-party recordings, which, in addition to the
difficulties presented by single-party recordings,
typically contain backchannels, elaborations, and
side topics, all of which further confound search
and summarization processes. Making efficient
use of large meeting corpora thus requires intel-
ligent summary and review techniques.
One possible user goal given a corpus of meet-
ing recordings is to discover the action items de-
cided within the meetings. Action items are deci-
sions made within the meeting that require post-
meeting attention or labor. Rapid identification
of action items can provide immediate access to
salient portions of the meetings. A review of ac-
tion items can also function as (part of) a summary
of the meeting content.
To this end, we explore the task of applying
maximum entropy classifiers to the task of auto-
matically detecting action item utterances in au-
dio recordings of multi-party meetings. Although
available corpora for action items are not ideal, it
is hoped that the feature analysis presented here
will be of use to later work on other corpora.
</bodyText>
<sectionHeader confidence="0.99969" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999879285714286">
Multi-party meetings have attracted a significant
amount of recent research attention. The creation
of the ICSI corpus (Janin et al., 2003), comprised
of 72 hours of meeting recordings with an average
of 6 speakers per meeting, with associated tran-
scripts, has spurred further annotations for var-
ious types of information, including dialog acts
(Shriberg et al., 2004), topic hierarchies and action
items (Gruenstein et al., 2005), and “hot spots”
(Wrede and Shriberg, 2003).
The classification of individual utterances based
on their role in the dialog, i.e. as opposed to their
semantic payload, has a long history, especially
in the context of dialog act (DA) classification.
</bodyText>
<page confidence="0.880023">
96
</page>
<bodyText confidence="0.990327966666667">
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 96–103,
Sydney, July 2006. c�2006 Association for Computational Linguistics
Research on DA classification initially focused
on two-party conversational speech (Mast et al.,
1996; Stolcke et al., 1998; Shriberg et al., 1998)
and, more recently, has extended to multi-party
audio recordings like the ICSI corpus (Shriberg
et al., 2004). Machine learning techniques such
as graphical models (Ji and Bilmes, 2005), maxi-
mum entropy models (Ang et al., 2005), and hid-
den Markov models (Zimmermann et al., 2005)
have been used to classify utterances from multi-
party conversations.
It is only more recently that work focused
specifically on action items themselves has been
developed. SVMs have been successfully applied
to the task of extracting action items from email
messages (Bennett and Carbonell, 2005; Corston-
Oliver et al., 2004). Bennett and Carbonell, in par-
ticular, distinguish the task of action item detec-
tion in email from the more well-studied task of
text classification, noting the finer granularity of
the action item task and the difference of seman-
tics vs. intent. (Although recent work has begun to
blur this latter division, e.g. Cohen et al. (2004).)
In the audio domain, annotations for action item
utterances on several recorded meeting corpora,
including the ICSI corpus, have recently become
available (Gruenstein et al., 2005), enabling work
on this topic.
</bodyText>
<sectionHeader confidence="0.997291" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.9999426">
We use action item annotations produced by Gru-
enstein et al. (2005). This corpus provides topic
hierarchy and action item annotations for the ICSI
meeting corpus as well as other corpora of meet-
ings; due to the ready availability of other types of
annotations for the ICSI corpus, we focus solely
on the annotations for these meetings. Figure 1
gives an example of the annotations.
The corpus covers 54 ICSI meetings annotated
by two human annotators, and several other meet-
ings annotated by one annotator. Of the 54 meet-
ings with dual annotations, 6 contain no action
items. For this study we consider only those meet-
ings which contain action items and which are an-
notated by both annotators.
As the annotations were produced by a small
number of untrained annotators, an immediate
question is the degree of consistency and reliabil-
ity. Inter-annotator agreement is typically mea-
sured by the kappa statistic (Carletta, 1996), de-
</bodyText>
<figure confidence="0.430232">
kappa
</figure>
<figureCaption confidence="0.956209">
Figure 2: Distribution of K (inter-annotator agree-
</figureCaption>
<bodyText confidence="0.983133666666667">
ment) across the 54 ICSI meetings tagged by two
annotators. Of the two meetings with K = 1.0, one
has only two action items and the other only four.
</bodyText>
<equation confidence="0.963424666666667">
fined as:
P(O) − P(E)
1 − P(E)
</equation>
<bodyText confidence="0.999115838709677">
where P(O) is the probability of the observed
agreement, and P(E) the probability of the “ex-
pected agreement” (i.e., under the assumption the
two sets of annotations are independent). The
kappa statistic ranges from −1 to 1, indicating per-
fect disagreement and perfect agreement, respec-
tively.
Overall inter-annotator agreement as measured
by K on the action item corpus is poor, as noted in
Purver et al. (2006), with an overall K of 0.364 and
values for individual meetings ranging from 1.0 to
less than zero. Figure 2 shows the distribution of
K across all 54 annotated ICSI meetings.
To reduce the effect of poor inter-annotator
agreement, we focus on the top 15 meetings as
ranked by K; the minimum K in this set is 0.435.
Although this reduces the total amount of data
available, our intention is that this subset of the
most consistent annotations will form a higher-
quality corpus.
While the corpus classifies related action item
utterances into action item “groups,” in this study
we wish to treat the annotations as simply binary
attributes. Visual analysis of annotations for sev-
eral meetings outside the set of chosen 15 suggests
that the union of the two sets of annotations yields
the most consistent resulting annotation; thus, for
this study, we consider an utterance to be an action
item if at least one of the annotators marked it as
such.
The 15-meeting subset contains 24,250 utter-
</bodyText>
<equation confidence="0.900214166666667">
0.0 0.2 0.4 0.6 0.8 1.0
frequency
0 2 4 6 8
K =
97
A1 A2
X X So that will be sort of the assignment for next week, is to—
X X to—for slides and whatever net you picked and what it can do and—and how far
you’ve gotten. Pppt!
X - Well, I’d like to also,
X X though, uh, ha- have a first cut at what the
X X belief-net looks like.
</equation>
<listItem confidence="0.5882145">
- X Even if it’s really crude.
- - OK? So, you know,
- - here a- here are—
- X So we’re supposed to @@ about features and whatnot, and—
</listItem>
<figureCaption confidence="0.953508">
Figure 1: Example transcript and action item annotations (marked “X”) from annotators A1 and A2.
</figureCaption>
<bodyText confidence="0.5964415">
“@@” signifies an unintelligible word. This transcript is from an ICSI meeting recording and has r =
0.373, ranking it 16th out of 54 meetings in annotator agreement.
</bodyText>
<figure confidence="0.895057">
0 500 1000 1500 2000 2500
</figure>
<figureCaption confidence="0.9763">
Figure 3: Number of total and action item utter-
</figureCaption>
<bodyText confidence="0.925124705882353">
ances across the 15 selected meetings. There are
24,250 utterances total, 590 of which (2.4%) are
action item utterances.
ances total; under the union strategy above, 590 of
these are action item utterances. Figure 3 shows
the number of action item utterances and the num-
ber of total utterances in the 15 selected meetings.
One noteworthy feature of the ICSI corpus un-
derlying the action item annotations is the “digit
reading task,” in which the participants of meet-
ings take turns reading aloud strings of digits.
This task was designed to provide a constrained-
vocabulary training set of speech recognition de-
velopers interested in multi-party speech. In this
study we did not remove these sections; the net
effect is that some portions of the data consist of
these fairly atypical utterances.
</bodyText>
<sectionHeader confidence="0.998703" genericHeader="method">
4 Experimental methodology
</sectionHeader>
<bodyText confidence="0.987668285714286">
We formulate the action item detection task as one
of binary classification of utterances. We apply a
maximum entropy (maxent) model (Berger et al.,
1996) to this task.
Maxent models seek to maximize the condi-
tional probability of a class c given the observa-
tions X using the exponential form
</bodyText>
<equation confidence="0.9984495">
P(c|X) = Z(1X) exp L� λZ,, fZ,,(X )J
Z
</equation>
<bodyText confidence="0.9997399375">
where fZ,,(X) is the ith feature of the data X
in class c, λZ,, is the corresponding weight, and
Z(X) is a normalization term. Maxent models
choose the weights λZ,, so as to maximize the en-
tropy of the induced distribution while remaining
consistent with the data and labels; the intuition is
that such a distribution makes the fewest assump-
tions about the underlying data.
Our maxent model is regularized by a quadratic
prior and uses quasi-Newton parameter optimiza-
tion. Due to the limited amount of training data
(see Section 3) and to avoid overfitting, we em-
ploy 10-fold cross validation in each experiment.
To evaluate system performance, we calculate
the F measure (F) of precision (P) and recall (R),
defined as:
</bodyText>
<equation confidence="0.999313333333333">
P = |A ∩ C|
|A|
R = |A ∩ C|
|C|
2PR
F= P + R
</equation>
<bodyText confidence="0.998743333333333">
where A is the set of utterances marked as action
items by the system, and C is the set of (all) cor-
rect action item utterances.
</bodyText>
<page confidence="0.995019">
98
</page>
<bodyText confidence="0.999958923076923">
The use of precision and recall is motivated by
the fact that the large imbalance between posi-
tive and negative examples in the corpus (Sec-
tion 3) means that simpler metrics like accuracy
are insufficient—a system that simply classifies
every utterance as negative will achieve an accu-
racy of 97.5%, which clearly is not a good reflec-
tion of desired behavior. Recall and F measure for
such a system, however, will be zero.
Likewise, a system that flips a coin weighted in
proportion to the number of positive examples in
the entire corpus will have an accuracy of 95.25%,
but will only achieve P = R = F = 2.4%.
</bodyText>
<sectionHeader confidence="0.999237" genericHeader="method">
5 Features
</sectionHeader>
<bodyText confidence="0.9998015">
As noted in Section 3, we treat the task of produc-
ing action item annotations as a binary classifica-
tion task. To this end, we consider the following
sets of features. (Note that all real-valued features
were range-normalized so as to lie in [0, 1] and that
no binning was employed.)
</bodyText>
<subsectionHeader confidence="0.955158">
5.1 Immediate lexical features
</subsectionHeader>
<bodyText confidence="0.999903375">
We extract word unigram and bigram features
from the transcript for each utterance. We nor-
malize for case and for certain contractions; for
example, “I’ll” is transformed into “I will”.
Note that these are oracle features, as the tran-
scripts are human-produced and not the product
of automatic speech recognizer (ASR) system out-
put.
</bodyText>
<subsectionHeader confidence="0.997141">
5.2 Contextual lexical features
</subsectionHeader>
<bodyText confidence="0.999592333333333">
We extract word unigram and bigram features
from the transcript for the previous and next ut-
terances across all speakers in the meeting.
</bodyText>
<subsectionHeader confidence="0.993541">
5.3 Syntactic features
</subsectionHeader>
<bodyText confidence="0.999983857142857">
Under the hypothesis that action item utterances
will exhibit particular syntactic patterns, we use
a conditional Markov model part-of-speech (POS)
tagger (Toutanova and Manning, 2000) trained on
the Switchboard corpus (Godfrey et al., 1992) to
tag utterance words for part of speech. We use the
following binary POS features:
</bodyText>
<listItem confidence="0.997601363636364">
• Presence of UH tag, denoting the presence of
an “interjection” (including filled pauses, un-
filled pauses, and discourse markers).
• Presence of MD tag, denoting presence of a
modal verb.
• Number of NN* tags, denoting the number of
nouns.
• Number of VB* tags, denoting the number of
verbs.
• Presence of VBD tag, denoting the presence
of a past-tense verb.
</listItem>
<subsectionHeader confidence="0.970907">
5.4 Prosodic features
</subsectionHeader>
<bodyText confidence="0.999736">
Under the hypothesis that action item utterances
will exhibit particular prosodic behavior—for ex-
ample, that they are emphasized, or are pitched a
certain way—we performed pitch extraction using
an auto-correlation method within the sound anal-
ysis package Praat (Boersma and Weenink, 2005).
From the meeting audio files we extract the fol-
lowing prosodic features, on a per-utterance basis:
(pitch measures are in Hz; intensity in energy; nor-
malization in all cases is z-normalization)
</bodyText>
<listItem confidence="0.99945575">
• Pitch and intensity range, minimum, and
maximum.
• Pitch and intensity mean.
• Pitch and intensity median (0.5 quantile).
• Pitch and intensity standard deviation.
• Pitch slope, processed to eliminate halv-
ing/doubling.
• Number of voiced frames.
• Duration-normalized pitch and intensity
ranges and voiced frame count.
• Speaker-normalized pitch and intensity
means.
</listItem>
<subsectionHeader confidence="0.944162">
5.5 Temporal features
</subsectionHeader>
<bodyText confidence="0.9993038">
Under the hypothesis that the length of an utter-
ance or its location within the meeting as a whole
will determine its likelihood of being an action
item—for example, shorter statements near the
end of the meeting might be more likely to be ac-
tion items—we extract the duration of each utter-
ance and the time from its occurrence until the end
of the meeting. (Note that the use of this feature
precludes operating in an online setting, where the
end of the meeting may not be known in advance.)
</bodyText>
<subsectionHeader confidence="0.961151">
5.6 General semantic features
</subsectionHeader>
<bodyText confidence="0.999937166666667">
Under the hypothesis that action item utterances
will frequently involve temporal expressions—e.g.
“Let’s have the paper written by next Tuesday”—
we use Identifinder (Bikel et al., 1997) to mark
temporal expressions (“TIMEX” tags) in utterance
transcripts, and create a binary feature denoting
</bodyText>
<page confidence="0.993913">
99
</page>
<bodyText confidence="0.999797">
the existence of a temporal expression in each ut-
terance.
Note that as Identifinder was trained on broad-
cast news corpora, applying it to the very different
domain of multi-party meeting transcripts may not
result in optimal behavior.
</bodyText>
<subsectionHeader confidence="0.986895">
5.7 Dialog-specific semantic features
</subsectionHeader>
<bodyText confidence="0.999211277777778">
Under the hypothesis that action item utterances
may be closely correlated with specific dialog
act tags, we use the dialog act annotations from
the ICSI Meeting Recorder Dialog Act Corpus.
(Shriberg et al., 2004) As these DA annotations
do not correspond one-to-one with utterances in
the ICSI corpus, we align them in the most liberal
way possible, i.e., if at least one word in an utter-
ance is annotated for a particular DA, we mark the
entirety of that utterance as exhibiting that DA.
We consider both fine-grained and coarse-
grained dialog acts.1 The former yields 56 fea-
tures, indicating occurrence of DA tags such
as “appreciation,” “rhetorical question,” and
“task management”; the latter consists of only
7 classes—“disruption,” “backchannel,” “filler,”
“statement,” “question,” “unlabeled,” and “un-
known.”
</bodyText>
<sectionHeader confidence="0.999935" genericHeader="method">
6 Results
</sectionHeader>
<bodyText confidence="0.999920047619048">
The final performance for the maxent model
across different feature sets is given in Table 1.
F measures scores range from 13.81 to 31.92.
Figure 4 shows the interpolated precision-recall
curves for several of these feature sets; these
graphs display the level of precision that can be
achieved if one is willing to sacrifice some recall,
and vice versa.
Although ideally, all combinations of features
should be evaluated separately, the large number
of features in this precludes this strategy. The
combination of features explored here was cho-
sen so as to start from simpler features and suc-
cessively add more complex ones. We start with
transcript features that are immediate and context-
independent (“unigram”, “bigram”, “TIMEX”);
then add transcript features that require context
(“temporal”, “context”), then non-transcript (i.e.
audio signal) features (“prosodic”), and finally add
features that require both the transcript and the au-
dio signal (“DA”).
</bodyText>
<footnote confidence="0.560775333333333">
1We use the map 01 grouping defined in the MRDA cor-
pus to collapse the tags.
recall
</footnote>
<figureCaption confidence="0.9033802">
Figure 4: Interpolated precision-recall curve for
several (cumulative) feature sets. This graph sug-
gests the level of precision that can be achieved
if one is willing to sacrifice some recall, and vice
versa.
</figureCaption>
<bodyText confidence="0.924543875">
In total, nine combinations of features were
considered. In every case except that of syn-
tactic and coarse-grained dialog act features, the
additional features improved system performance
and these features were used in succeeding exper-
iments. Syntactic and coarse-grained DA features
resulted in a drop in performance and were dis-
carded from succeeding systems.
</bodyText>
<sectionHeader confidence="0.985903" genericHeader="method">
7 Analysis
</sectionHeader>
<bodyText confidence="0.99987952173913">
The unigram and bigram features provide signif-
icant discriminative power. Tables 2 and 3 give
the top features, as determined by weight, for the
models trained only on these features. It is clear
from Table 3 that the detailed end-of-utterance
punctuation in the human-generated transcripts
provide valuable discriminative power.
The performance gain from adding TIMEX tag-
ging features is small and likely not statistically
significant. Post-hoc analysis of the TIMEX tag-
ging (Section 5.6) suggests that Identifinder tag-
ging accuracy is quite plausible in general, but ex-
hibits an unfortunate tendency to mark the digit-
reading (see Section 3) portion of the meetings as
temporal expressions. It is plausible that remov-
ing these utterances from the meetings would al-
low this feature a higher accuracy.
Based on the low feature weight assigned, utter-
ance length appears to provide no significant value
to the model. However, the time until the meet-
ing is over ranks as the highest-weighted feature
in the unigram+bigram+TIMEX+temporal feature
set. This feature is thus responsible for the 39.25%
</bodyText>
<figure confidence="0.9842115">
0.0 0.2 0.4 0.6 0.8 1.0
unigram
bigram
temporal
context+prosodic
fine−grained DAs
precision
0.0 0.2 0.4 0.6 0.8 1.0
</figure>
<page confidence="0.772667">
100
</page>
<table confidence="0.9980823">
features number F % imp.
unigram 6844 13.81
unigram+bigram 61281 16.72 21.07
unigram+bigram+TIMEX 61284 16.84 0.72
unigram+bigram+TIMEX+temporal 61286 23.45 39.25
unigram+bigram+TIMEX+temporal+syntactic 61291 21.94 -6.44
unigram+bigram+TIMEX+temporal+context 183833 25.62 9.25
unigram+bigram+TIMEX+temporal+context+prosodic 183871 27.44 7.10
unigram+bigram+TIMEX+temporal+context+prosodic+coarse DAs 183878 26.47 -3.53
unigram+bigram+TIMEX+temporal+context+prosodic+fine DAs 183927 31.92 16.33
</table>
<tableCaption confidence="0.972763333333333">
Table 1: Performance of the maxent classifier as measured by F measure, the relative improvement from
the preceding feature set, and the number of features, across all feature sets tried. Italicized lines denote
the addition of features which do not improve performance; these are omitted from succeeding systems.
</tableCaption>
<table confidence="0.999587454545455">
feature +/- λ
“pull” + 2.2100
“email” + 1.7883
“needs” + 1.7212
“added” + 1.6613
“mm-hmm” - 1.5937
“present” + 1.5740
“nine” - 1.5019
“!” - 1.5001
“five” - 1.4944
“together” + 1.4882
</table>
<tableCaption confidence="0.995118">
Table 2: Features, evidence type (positive denotes
</tableCaption>
<bodyText confidence="0.519231">
action item), and weight for the top ten features
in the unigram-only model. “Nine” and “five” are
common words in the digit-reading task (see Sec-
tion 3).
</bodyText>
<table confidence="0.999330363636364">
feature +/- λ
“- $” - 1.4308
“i will” + 1.4128
“, $” - 1.3115
“uh $” - 1.2752
“w- $” - 1.2419
“. $” - 1.2247
“email” + 1.2062
“six $” - 1.1874
“* in” - 1.1833
“so $” - 1.1819
</table>
<tableCaption confidence="0.998269">
Table 3: Features, evidence type and weight for
</tableCaption>
<bodyText confidence="0.861438428571428">
the top ten features in the unigram+bigram model.
The symbol * denotes the beginning of an utter-
ance and $ the end. All of the top ten features are
bigrams except for the unigrams “email”.
feature +/- λ
mean intensity (norm.) - 1.4288
mean pitch (norm.) - 1.0661
</bodyText>
<figure confidence="0.7695825">
intensity range + 1.0510
“i will” + 0.8657
“email” + 0.8113
reformulate/summarize (DA) + 0.7946
“just go” (next) + 0.7190
“i will” (prev.) + 0.7074
“the paper” + 0.6788
understanding check (DA) + 0.6547
</figure>
<tableCaption confidence="0.822406">
Table 4: Features, evidence type and weight for
</tableCaption>
<bodyText confidence="0.998850909090909">
the top ten features on the best-performing model.
Bigrams labeled “prev.” and “next” correspond to
the lexemes from previous and next utterances, re-
spectively. Prosodic features labeled as “norm.”
have been normalized on a per-speaker basis.
boost in F measure in row 3 of Table 1.
The addition of part-of-speech tags actually de-
creases system performance. It is unclear why this
is the case. It may be that the unigram and bi-
gram features already adequately capture any dis-
tinctions these features make, or simply that these
features are generally not useful for distinguishing
action items.
Contextual features, on the other hand, im-
prove system performance significantly. A post-
hoc analysis of the action item annotations makes
clear why: action items are often split across mul-
tiple utterances (e.g. as in Figure 1), only a portion
of which contain lexical cues sufficient to distin-
guish them as such. Contextual features thus allow
utterances immediately surrounding these “obvi-
ous” action items to be tagged as well.
</bodyText>
<page confidence="0.997668">
101
</page>
<bodyText confidence="0.99996075">
Prosodic features yield a 7.10% increase in
F measure, and analysis shows that speaker-
normalized intensity and pitch, and the range in
intensity of an utterance, are valuable discrimina-
tive features. The subsequent addition of coarse-
grained dialog act tags does not further improve
system performance. It is likely this is due to rea-
sons similar to those for POS tags—either the cat-
egories are insufficient to distinguish action item
utterances, or whatever usefulness they provide is
subsumed by other features.
Table 4 shows the feature weights for the top-
ranked features on the best-scoring system. The
addition of the fine-grained DA tags results in a
significant increase in performance.The F measure
of this best feature set is 31.92%.
</bodyText>
<sectionHeader confidence="0.996369" genericHeader="method">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999847931034483">
We have shown that several classes of features are
useful for the task of action item annotation from
multi-party meeting corpora. Simple lexical fea-
tures, their contextual versions, the time until the
end of the meeting, prosodic features, and fine-
grained dialog acts each contribute significant in-
creases in system performance.
While the raw system performance numbers of
Table 1 are low relative to other, better-studied
tasks on other, more mature corpora, we believe
the relative usefulness of the features towards this
task is indicative of their usefulness on more con-
sistent annotations, as well as to related tasks.
The Gruenstein et al. (2005) corpus provides
a valuable and necessary resource for research in
this area, but several factors raise the question of
annotation quality. The low κ scores in Section 3
are indicative of annotation problems. Post-hoc
error analysis yields many examples of utterances
which are somewhat difficult to imagine as pos-
sible, never mind desirable, to tag. The fact that
the extremely useful oracular information present
in the fine-grained DA annotation does not raise
performance to the high levels that one might ex-
pect further suggests that the annotations are not
ideal—or, at the least, that they are inconsistent
with the DA annotations.2
This analysis is consistent with the findings of
Purver et al. (2006), who achieve an F measure of
</bodyText>
<footnote confidence="0.95681425">
2Which is not to say they are devoid of significant value—
training and testing our best system on the corpus with the
590 positive classifications randomly shuffled across all ut-
terances yields an F measure of only 4.82.
</footnote>
<bodyText confidence="0.999054">
less than 25% when applying SVMs to the classi-
fication task to the same corpus, and motivate the
development of a new corpus of action item anno-
tations.
</bodyText>
<sectionHeader confidence="0.99678" genericHeader="discussions">
9 Future work
</sectionHeader>
<bodyText confidence="0.999963644444445">
In Section 6 we showed that contextual lexical
features are useful for the task of action item de-
tection, at least in the fairly limited manner em-
ployed in our implementation, which simply looks
at immediate previous and immediate next utter-
ances. It seems likely that applying a sequence
model such as an HMM or conditional random
field (CRFs) will act as a generalization of this fea-
ture and may further improve performance.
Addition of features such as speaker change and
“hot spots” (Wrede and Shriberg, 2003) may also
aid classification. Conversely, it is possible that
feature selection techniques may improve perfor-
mance by helping to eliminate poor-quality fea-
tures. In this work we have followed an “ev-
erything but the kitchen sink” approach, in part
because we were curious about which features
would prove useful. The effect of adding POS and
coarse-grained DA features illustrates that this is
not necessarily the ideal strategy in terms of ulti-
mate system performance.
In general, the features evaluated in this
work are an indiscriminate mix of human- and
automatically-generated features; of the human-
generated features, some are plausible to generate
automatically, at some loss of quality (e.g. tran-
scripts) while others are unlikely to be automati-
cally generated in the foreseeable future (e.g. fine-
grained dialog acts). Future work may focus on
the effects that automatic generation of the former
has on overall system performance (although this
may require higher-quality annotations to be use-
ful.) For example, the detailed end-of-utterance
punctuation present in the human transcripts pro-
vides valuable discriminative power (Table 3), but
current ASR systems are not likely to be able to
provide this level of detail. Switching to ASR out-
put will have a negative effect on performance.
One final issue is that of utterance segmenta-
tion. The scheme used in the ICSI meeting corpus
does not necessarily correspond to the ideal seg-
mentation for other tasks. The action item annota-
tions were performed on these segmentations, and
in this study we did not attempt resegmentation,
but in the future it may prove valuable to collapse,
</bodyText>
<page confidence="0.996449">
102
</page>
<bodyText confidence="0.9998795">
for example, successive un-interrupted utterances
from the same speaker into a single utterance.
In conclusion, while overall system perfor-
mance does not approach levels typical of better-
studied classification tasks such as named-entity
recognition, we believe that this is a largely a prod-
uct of the current action item annotation quality.
We believe that the feature analysis presented here
is useful, for this task and for other related tasks,
and that, provided with a set of more consistent
action item annotations, the current system can be
used as is to achieve better performance.
</bodyText>
<sectionHeader confidence="0.998278" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999811">
The authors wish to thank Dan Jurafsky, Chris
Manning, Stanley Peters, Matthew Purver, and
several anonymous reviewers for valuable advice
and comments.
</bodyText>
<sectionHeader confidence="0.999425" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999739296296296">
Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005.
Automatic dialog act segmentation and classifica-
tion in multiparty meetings. In Proceedings of the
ICASSP.
Paul N. Bennett and Jaime Carbonell. 2005. Detecting
action-items in e-mail. In Proceedings of SIGIR.
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to nat-
ural language processing. Computational Linguis-
tics, 22(1):39–71.
D. Bikel, S. Miller, R. Schwartz, and R. Weischedel.
1997. Nymble: a high-performance learning name-
finder. In Proceedings of the Conference on Applied
NLP.
Paul Boersma and David Weenink. 2005. Praat: doing
phonetics by computer v4.4.12 (computer program).
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: The kappa statistic. Computational Lin-
guistics, 22(2):249–254.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
”speech acts”. In Proceedings of EMNLP.
Simon Corston-Oliver, Eric Ringger, Michael Ga-
mon, and Richard Campbell. 2004. Task-focused
summarization of email. In Text Summarization
Branches Out: Proceedings of the ACL Workshop.
J. Godfrey, E. Holliman, and J.McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for
research and development. In Proceedings of
ICAASP.
Alexander Gruenstein, John Niekrasz, and Matthew
Purver. 2005. Meeting structure annotation: Data
and tools. In Proceedings of the 6th SIGDIAL Work-
shop on Discourse and Dialogue.
Adam Janin, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Nelson Morgan, Barbara Peskin,
Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke,
and Chuck Wooters. 2003. The ICSI meeting cor-
pus. In Proceedings of the ICASSP.
Gang Ji and Jeff Bilmes. 2005. Dialog act tag-
ging using graphical models. In Proceedings of the
ICASSP.
Marion Mast, R. Kompe, S. Harbeck, A. Kießling,
H. Niemann, E. N¨oth, E.G. Schukat-Talamazzini,
and V. Warnke. 1996. Dialog act classification with
the help of prosody. In Proceedings of the ICSLP.
Matthew Purver, Patrick Ehlen, and John Niekrasz.
2006. Detecting action items in multi-party meet-
ings: Annotation and initial experiments. In Pro-
ceedings of the 3rd Joint Workshop on MLMI.
Elizabeth Shriberg, Rebecca Bates, Andreas Stolcke,
Paul Taylor, Daniel Jurafsky, Klaus Ries, Noah Coc-
caro, Rachel Martin, Marie Meteer, and Carol Van
EssDykema. 1998. Can prosody aid the auto-
matic classification of dialog acts in conversational
speech? Language and Speech, 41(3–4):439–487.
Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy
Ang, and Hannah Carvey. 2004. The ICSI meeting
recorder dialog act (MRDA) corpus. In Proceedings
of the 5th SIGDIAL Workshop on Discourse and Di-
alogue.
Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates,
Noah Coccaro, Daniel Jurafsky, Rachel Mar-
tin, Marie Meteer, Klaus Ries, Paul Taylor, and
Carol Van EssDykema. 1998. Dialog act model-
ing for conversational speech. In Proceedings of
the AAAI Spring Symposium on Applying Machine
Learning to Discourse Processing.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in a
maximum entropy part-of-speech tagger. In Pro-
ceedings ofEMNLP.
Britta Wrede and Elizabeth Shriberg. 2003. Spot-
ting “hot spots” in meetings: Human judgments and
prosodic cues. In Proceedings of the European Con-
ference on Speech Communication and Technology.
Matthias Zimmermann, Yang Liu, Elizabeth Shriberg,
and Andreas Stolcke. 2005. Toward joint segmen-
tation and classification of dialog acts in multiparty
meetings. In Proceedings of the 2nd Joint Workshop
on MLMI.
</reference>
<page confidence="0.999288">
103
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.593864">
<title confidence="0.99994">Automatically Detecting Action Items in Audio Meeting Recordings</title>
<author confidence="0.99997">M Jason</author>
<affiliation confidence="0.9994175">Department of Center for Spoken Language Institute of Cognitive University of Colorado at</affiliation>
<address confidence="0.9783965">594 Boulder, Colorado</address>
<email confidence="0.999869">jbrenier@colorado.edu</email>
<author confidence="0.998333">William Morgan Pi-Chuan Chang Surabhi</author>
<affiliation confidence="0.849544">Department of Computer Stanford</affiliation>
<address confidence="0.9569625">353 Serra Stanford, CA</address>
<email confidence="0.999044">surabhi@cs.stanford.edu</email>
<abstract confidence="0.998533181818182">Identification of action items in meeting recordings can provide immediate access to salient information in a medium notoriously difficult to search and summarize. To this end, we use a maximum entropy model to automatically detect action itemrelated utterances from multi-party audio meeting recordings. We compare the effect of lexical, temporal, syntactic, semantic, and prosodic features on system performance. We show that on a corpus of action item annotations on the ICSI meeting recordings, characterized by high imbalance and low inter-annotator agreement, the system performs at an F measure of 31.92%. While this is low compared to better-studied tasks on more mature corpora, the relative usefulness of the features towards this task is indicative of their usefulness on more consistent annotations, as well as to related tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jeremy Ang</author>
<author>Yang Liu</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Automatic dialog act segmentation and classification in multiparty meetings.</title>
<date>2005</date>
<booktitle>In Proceedings of the ICASSP.</booktitle>
<contexts>
<context position="4203" citStr="Ang et al., 2005" startWordPosition="637" endWordPosition="640">ayload, has a long history, especially in the context of dialog act (DA) classification. 96 Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 96–103, Sydney, July 2006. c�2006 Association for Computational Linguistics Research on DA classification initially focused on two-party conversational speech (Mast et al., 1996; Stolcke et al., 1998; Shriberg et al., 1998) and, more recently, has extended to multi-party audio recordings like the ICSI corpus (Shriberg et al., 2004). Machine learning techniques such as graphical models (Ji and Bilmes, 2005), maximum entropy models (Ang et al., 2005), and hidden Markov models (Zimmermann et al., 2005) have been used to classify utterances from multiparty conversations. It is only more recently that work focused specifically on action items themselves has been developed. SVMs have been successfully applied to the task of extracting action items from email messages (Bennett and Carbonell, 2005; CorstonOliver et al., 2004). Bennett and Carbonell, in particular, distinguish the task of action item detection in email from the more well-studied task of text classification, noting the finer granularity of the action item task and the difference </context>
</contexts>
<marker>Ang, Liu, Shriberg, 2005</marker>
<rawString>Jeremy Ang, Yang Liu, and Elizabeth Shriberg. 2005. Automatic dialog act segmentation and classification in multiparty meetings. In Proceedings of the ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul N Bennett</author>
<author>Jaime Carbonell</author>
</authors>
<title>Detecting action-items in e-mail.</title>
<date>2005</date>
<booktitle>In Proceedings of SIGIR.</booktitle>
<contexts>
<context position="4551" citStr="Bennett and Carbonell, 2005" startWordPosition="691" endWordPosition="694">1996; Stolcke et al., 1998; Shriberg et al., 1998) and, more recently, has extended to multi-party audio recordings like the ICSI corpus (Shriberg et al., 2004). Machine learning techniques such as graphical models (Ji and Bilmes, 2005), maximum entropy models (Ang et al., 2005), and hidden Markov models (Zimmermann et al., 2005) have been used to classify utterances from multiparty conversations. It is only more recently that work focused specifically on action items themselves has been developed. SVMs have been successfully applied to the task of extracting action items from email messages (Bennett and Carbonell, 2005; CorstonOliver et al., 2004). Bennett and Carbonell, in particular, distinguish the task of action item detection in email from the more well-studied task of text classification, noting the finer granularity of the action item task and the difference of semantics vs. intent. (Although recent work has begun to blur this latter division, e.g. Cohen et al. (2004).) In the audio domain, annotations for action item utterances on several recorded meeting corpora, including the ICSI corpus, have recently become available (Gruenstein et al., 2005), enabling work on this topic. 3 Data We use action it</context>
</contexts>
<marker>Bennett, Carbonell, 2005</marker>
<rawString>Paul N. Bennett and Jaime Carbonell. 2005. Detecting action-items in e-mail. In Proceedings of SIGIR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Berger</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="9478" citStr="Berger et al., 1996" startWordPosition="1545" endWordPosition="1548">I corpus underlying the action item annotations is the “digit reading task,” in which the participants of meetings take turns reading aloud strings of digits. This task was designed to provide a constrainedvocabulary training set of speech recognition developers interested in multi-party speech. In this study we did not remove these sections; the net effect is that some portions of the data consist of these fairly atypical utterances. 4 Experimental methodology We formulate the action item detection task as one of binary classification of utterances. We apply a maximum entropy (maxent) model (Berger et al., 1996) to this task. Maxent models seek to maximize the conditional probability of a class c given the observations X using the exponential form P(c|X) = Z(1X) exp L� λZ,, fZ,,(X )J Z where fZ,,(X) is the ith feature of the data X in class c, λZ,, is the corresponding weight, and Z(X) is a normalization term. Maxent models choose the weights λZ,, so as to maximize the entropy of the induced distribution while remaining consistent with the data and labels; the intuition is that such a distribution makes the fewest assumptions about the underlying data. Our maxent model is regularized by a quadratic p</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>Adam Berger, Stephen Della Pietra, and Vincent Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bikel</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Nymble: a high-performance learning namefinder.</title>
<date>1997</date>
<booktitle>In Proceedings of the Conference on Applied NLP.</booktitle>
<contexts>
<context position="14314" citStr="Bikel et al., 1997" startWordPosition="2363" endWordPosition="2366"> whole will determine its likelihood of being an action item—for example, shorter statements near the end of the meeting might be more likely to be action items—we extract the duration of each utterance and the time from its occurrence until the end of the meeting. (Note that the use of this feature precludes operating in an online setting, where the end of the meeting may not be known in advance.) 5.6 General semantic features Under the hypothesis that action item utterances will frequently involve temporal expressions—e.g. “Let’s have the paper written by next Tuesday”— we use Identifinder (Bikel et al., 1997) to mark temporal expressions (“TIMEX” tags) in utterance transcripts, and create a binary feature denoting 99 the existence of a temporal expression in each utterance. Note that as Identifinder was trained on broadcast news corpora, applying it to the very different domain of multi-party meeting transcripts may not result in optimal behavior. 5.7 Dialog-specific semantic features Under the hypothesis that action item utterances may be closely correlated with specific dialog act tags, we use the dialog act annotations from the ICSI Meeting Recorder Dialog Act Corpus. (Shriberg et al., 2004) As</context>
</contexts>
<marker>Bikel, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>D. Bikel, S. Miller, R. Schwartz, and R. Weischedel. 1997. Nymble: a high-performance learning namefinder. In Proceedings of the Conference on Applied NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Boersma</author>
<author>David Weenink</author>
</authors>
<title>Praat: doing phonetics by computer v4.4.12 (computer program).</title>
<date>2005</date>
<contexts>
<context position="13015" citStr="Boersma and Weenink, 2005" startWordPosition="2153" endWordPosition="2156">f an “interjection” (including filled pauses, unfilled pauses, and discourse markers). • Presence of MD tag, denoting presence of a modal verb. • Number of NN* tags, denoting the number of nouns. • Number of VB* tags, denoting the number of verbs. • Presence of VBD tag, denoting the presence of a past-tense verb. 5.4 Prosodic features Under the hypothesis that action item utterances will exhibit particular prosodic behavior—for example, that they are emphasized, or are pitched a certain way—we performed pitch extraction using an auto-correlation method within the sound analysis package Praat (Boersma and Weenink, 2005). From the meeting audio files we extract the following prosodic features, on a per-utterance basis: (pitch measures are in Hz; intensity in energy; normalization in all cases is z-normalization) • Pitch and intensity range, minimum, and maximum. • Pitch and intensity mean. • Pitch and intensity median (0.5 quantile). • Pitch and intensity standard deviation. • Pitch slope, processed to eliminate halving/doubling. • Number of voiced frames. • Duration-normalized pitch and intensity ranges and voiced frame count. • Speaker-normalized pitch and intensity means. 5.5 Temporal features Under the hy</context>
</contexts>
<marker>Boersma, Weenink, 2005</marker>
<rawString>Paul Boersma and David Weenink. 2005. Praat: doing phonetics by computer v4.4.12 (computer program).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: The kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>2</issue>
<contexts>
<context position="6060" citStr="Carletta, 1996" startWordPosition="941" endWordPosition="942">tions for these meetings. Figure 1 gives an example of the annotations. The corpus covers 54 ICSI meetings annotated by two human annotators, and several other meetings annotated by one annotator. Of the 54 meetings with dual annotations, 6 contain no action items. For this study we consider only those meetings which contain action items and which are annotated by both annotators. As the annotations were produced by a small number of untrained annotators, an immediate question is the degree of consistency and reliability. Inter-annotator agreement is typically measured by the kappa statistic (Carletta, 1996), dekappa Figure 2: Distribution of K (inter-annotator agreement) across the 54 ICSI meetings tagged by two annotators. Of the two meetings with K = 1.0, one has only two action items and the other only four. fined as: P(O) − P(E) 1 − P(E) where P(O) is the probability of the observed agreement, and P(E) the probability of the “expected agreement” (i.e., under the assumption the two sets of annotations are independent). The kappa statistic ranges from −1 to 1, indicating perfect disagreement and perfect agreement, respectively. Overall inter-annotator agreement as measured by K on the action i</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>J. Carletta. 1996. Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2):249–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
<author>Vitor R Carvalho</author>
<author>Tom M Mitchell</author>
</authors>
<title>Learning to classify email into ”speech acts”.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="4914" citStr="Cohen et al. (2004)" startWordPosition="752" endWordPosition="755">rom multiparty conversations. It is only more recently that work focused specifically on action items themselves has been developed. SVMs have been successfully applied to the task of extracting action items from email messages (Bennett and Carbonell, 2005; CorstonOliver et al., 2004). Bennett and Carbonell, in particular, distinguish the task of action item detection in email from the more well-studied task of text classification, noting the finer granularity of the action item task and the difference of semantics vs. intent. (Although recent work has begun to blur this latter division, e.g. Cohen et al. (2004).) In the audio domain, annotations for action item utterances on several recorded meeting corpora, including the ICSI corpus, have recently become available (Gruenstein et al., 2005), enabling work on this topic. 3 Data We use action item annotations produced by Gruenstein et al. (2005). This corpus provides topic hierarchy and action item annotations for the ICSI meeting corpus as well as other corpora of meetings; due to the ready availability of other types of annotations for the ICSI corpus, we focus solely on the annotations for these meetings. Figure 1 gives an example of the annotation</context>
</contexts>
<marker>Cohen, Carvalho, Mitchell, 2004</marker>
<rawString>William W. Cohen, Vitor R. Carvalho, and Tom M. Mitchell. 2004. Learning to classify email into ”speech acts”. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Eric Ringger</author>
<author>Michael Gamon</author>
<author>Richard Campbell</author>
</authors>
<title>Task-focused summarization of email.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL Workshop.</booktitle>
<marker>Corston-Oliver, Ringger, Gamon, Campbell, 2004</marker>
<rawString>Simon Corston-Oliver, Eric Ringger, Michael Gamon, and Richard Campbell. 2004. Task-focused summarization of email. In Text Summarization Branches Out: Proceedings of the ACL Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Godfrey</author>
<author>E Holliman</author>
<author>J McDaniel</author>
</authors>
<title>SWITCHBOARD: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proceedings of ICAASP.</booktitle>
<contexts>
<context position="12258" citStr="Godfrey et al., 1992" startWordPosition="2030" endWordPosition="2033">le, “I’ll” is transformed into “I will”. Note that these are oracle features, as the transcripts are human-produced and not the product of automatic speech recognizer (ASR) system output. 5.2 Contextual lexical features We extract word unigram and bigram features from the transcript for the previous and next utterances across all speakers in the meeting. 5.3 Syntactic features Under the hypothesis that action item utterances will exhibit particular syntactic patterns, we use a conditional Markov model part-of-speech (POS) tagger (Toutanova and Manning, 2000) trained on the Switchboard corpus (Godfrey et al., 1992) to tag utterance words for part of speech. We use the following binary POS features: • Presence of UH tag, denoting the presence of an “interjection” (including filled pauses, unfilled pauses, and discourse markers). • Presence of MD tag, denoting presence of a modal verb. • Number of NN* tags, denoting the number of nouns. • Number of VB* tags, denoting the number of verbs. • Presence of VBD tag, denoting the presence of a past-tense verb. 5.4 Prosodic features Under the hypothesis that action item utterances will exhibit particular prosodic behavior—for example, that they are emphasized, or</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J. Godfrey, E. Holliman, and J.McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and development. In Proceedings of ICAASP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Gruenstein</author>
<author>John Niekrasz</author>
<author>Matthew Purver</author>
</authors>
<title>Meeting structure annotation: Data and tools.</title>
<date>2005</date>
<booktitle>In Proceedings of the 6th SIGDIAL Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="3426" citStr="Gruenstein et al., 2005" startWordPosition="519" endWordPosition="522">lti-party meetings. Although available corpora for action items are not ideal, it is hoped that the feature analysis presented here will be of use to later work on other corpora. 2 Related work Multi-party meetings have attracted a significant amount of recent research attention. The creation of the ICSI corpus (Janin et al., 2003), comprised of 72 hours of meeting recordings with an average of 6 speakers per meeting, with associated transcripts, has spurred further annotations for various types of information, including dialog acts (Shriberg et al., 2004), topic hierarchies and action items (Gruenstein et al., 2005), and “hot spots” (Wrede and Shriberg, 2003). The classification of individual utterances based on their role in the dialog, i.e. as opposed to their semantic payload, has a long history, especially in the context of dialog act (DA) classification. 96 Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 96–103, Sydney, July 2006. c�2006 Association for Computational Linguistics Research on DA classification initially focused on two-party conversational speech (Mast et al., 1996; Stolcke et al., 1998; Shriberg et al., 1998) and, more recently, has extended to multi-party aud</context>
<context position="5097" citStr="Gruenstein et al., 2005" startWordPosition="778" endWordPosition="781">sk of extracting action items from email messages (Bennett and Carbonell, 2005; CorstonOliver et al., 2004). Bennett and Carbonell, in particular, distinguish the task of action item detection in email from the more well-studied task of text classification, noting the finer granularity of the action item task and the difference of semantics vs. intent. (Although recent work has begun to blur this latter division, e.g. Cohen et al. (2004).) In the audio domain, annotations for action item utterances on several recorded meeting corpora, including the ICSI corpus, have recently become available (Gruenstein et al., 2005), enabling work on this topic. 3 Data We use action item annotations produced by Gruenstein et al. (2005). This corpus provides topic hierarchy and action item annotations for the ICSI meeting corpus as well as other corpora of meetings; due to the ready availability of other types of annotations for the ICSI corpus, we focus solely on the annotations for these meetings. Figure 1 gives an example of the annotations. The corpus covers 54 ICSI meetings annotated by two human annotators, and several other meetings annotated by one annotator. Of the 54 meetings with dual annotations, 6 contain no </context>
<context position="22735" citStr="Gruenstein et al. (2005)" startWordPosition="3682" endWordPosition="3685">asses of features are useful for the task of action item annotation from multi-party meeting corpora. Simple lexical features, their contextual versions, the time until the end of the meeting, prosodic features, and finegrained dialog acts each contribute significant increases in system performance. While the raw system performance numbers of Table 1 are low relative to other, better-studied tasks on other, more mature corpora, we believe the relative usefulness of the features towards this task is indicative of their usefulness on more consistent annotations, as well as to related tasks. The Gruenstein et al. (2005) corpus provides a valuable and necessary resource for research in this area, but several factors raise the question of annotation quality. The low κ scores in Section 3 are indicative of annotation problems. Post-hoc error analysis yields many examples of utterances which are somewhat difficult to imagine as possible, never mind desirable, to tag. The fact that the extremely useful oracular information present in the fine-grained DA annotation does not raise performance to the high levels that one might expect further suggests that the annotations are not ideal—or, at the least, that they are</context>
</contexts>
<marker>Gruenstein, Niekrasz, Purver, 2005</marker>
<rawString>Alexander Gruenstein, John Niekrasz, and Matthew Purver. 2005. Meeting structure annotation: Data and tools. In Proceedings of the 6th SIGDIAL Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Janin</author>
<author>Don Baron</author>
<author>Jane Edwards</author>
<author>Dan Ellis</author>
<author>David Gelbart</author>
<author>Nelson Morgan</author>
<author>Barbara Peskin</author>
<author>Thilo Pfau</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
<author>Chuck Wooters</author>
</authors>
<title>The ICSI meeting corpus.</title>
<date>2003</date>
<booktitle>In Proceedings of the ICASSP.</booktitle>
<contexts>
<context position="3135" citStr="Janin et al., 2003" startWordPosition="474" endWordPosition="477">o salient portions of the meetings. A review of action items can also function as (part of) a summary of the meeting content. To this end, we explore the task of applying maximum entropy classifiers to the task of automatically detecting action item utterances in audio recordings of multi-party meetings. Although available corpora for action items are not ideal, it is hoped that the feature analysis presented here will be of use to later work on other corpora. 2 Related work Multi-party meetings have attracted a significant amount of recent research attention. The creation of the ICSI corpus (Janin et al., 2003), comprised of 72 hours of meeting recordings with an average of 6 speakers per meeting, with associated transcripts, has spurred further annotations for various types of information, including dialog acts (Shriberg et al., 2004), topic hierarchies and action items (Gruenstein et al., 2005), and “hot spots” (Wrede and Shriberg, 2003). The classification of individual utterances based on their role in the dialog, i.e. as opposed to their semantic payload, has a long history, especially in the context of dialog act (DA) classification. 96 Proceedings of the 7th SIGdial Workshop on Discourse and </context>
</contexts>
<marker>Janin, Baron, Edwards, Ellis, Gelbart, Morgan, Peskin, Pfau, Shriberg, Stolcke, Wooters, 2003</marker>
<rawString>Adam Janin, Don Baron, Jane Edwards, Dan Ellis, David Gelbart, Nelson Morgan, Barbara Peskin, Thilo Pfau, Elizabeth Shriberg, Andreas Stolcke, and Chuck Wooters. 2003. The ICSI meeting corpus. In Proceedings of the ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gang Ji</author>
<author>Jeff Bilmes</author>
</authors>
<title>Dialog act tagging using graphical models.</title>
<date>2005</date>
<booktitle>In Proceedings of the ICASSP.</booktitle>
<contexts>
<context position="4160" citStr="Ji and Bilmes, 2005" startWordPosition="629" endWordPosition="632">he dialog, i.e. as opposed to their semantic payload, has a long history, especially in the context of dialog act (DA) classification. 96 Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 96–103, Sydney, July 2006. c�2006 Association for Computational Linguistics Research on DA classification initially focused on two-party conversational speech (Mast et al., 1996; Stolcke et al., 1998; Shriberg et al., 1998) and, more recently, has extended to multi-party audio recordings like the ICSI corpus (Shriberg et al., 2004). Machine learning techniques such as graphical models (Ji and Bilmes, 2005), maximum entropy models (Ang et al., 2005), and hidden Markov models (Zimmermann et al., 2005) have been used to classify utterances from multiparty conversations. It is only more recently that work focused specifically on action items themselves has been developed. SVMs have been successfully applied to the task of extracting action items from email messages (Bennett and Carbonell, 2005; CorstonOliver et al., 2004). Bennett and Carbonell, in particular, distinguish the task of action item detection in email from the more well-studied task of text classification, noting the finer granularity </context>
</contexts>
<marker>Ji, Bilmes, 2005</marker>
<rawString>Gang Ji and Jeff Bilmes. 2005. Dialog act tagging using graphical models. In Proceedings of the ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marion Mast</author>
<author>R Kompe</author>
<author>S Harbeck</author>
<author>A Kießling</author>
<author>H Niemann</author>
<author>E N¨oth</author>
<author>E G Schukat-Talamazzini</author>
<author>V Warnke</author>
</authors>
<title>Dialog act classification with the help of prosody.</title>
<date>1996</date>
<booktitle>In Proceedings of the ICSLP.</booktitle>
<marker>Mast, Kompe, Harbeck, Kießling, Niemann, N¨oth, Schukat-Talamazzini, Warnke, 1996</marker>
<rawString>Marion Mast, R. Kompe, S. Harbeck, A. Kießling, H. Niemann, E. N¨oth, E.G. Schukat-Talamazzini, and V. Warnke. 1996. Dialog act classification with the help of prosody. In Proceedings of the ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Purver</author>
<author>Patrick Ehlen</author>
<author>John Niekrasz</author>
</authors>
<title>Detecting action items in multi-party meetings: Annotation and initial experiments.</title>
<date>2006</date>
<booktitle>In Proceedings of the 3rd Joint Workshop on MLMI.</booktitle>
<contexts>
<context position="6712" citStr="Purver et al. (2006)" startWordPosition="1052" endWordPosition="1055"> of K (inter-annotator agreement) across the 54 ICSI meetings tagged by two annotators. Of the two meetings with K = 1.0, one has only two action items and the other only four. fined as: P(O) − P(E) 1 − P(E) where P(O) is the probability of the observed agreement, and P(E) the probability of the “expected agreement” (i.e., under the assumption the two sets of annotations are independent). The kappa statistic ranges from −1 to 1, indicating perfect disagreement and perfect agreement, respectively. Overall inter-annotator agreement as measured by K on the action item corpus is poor, as noted in Purver et al. (2006), with an overall K of 0.364 and values for individual meetings ranging from 1.0 to less than zero. Figure 2 shows the distribution of K across all 54 annotated ICSI meetings. To reduce the effect of poor inter-annotator agreement, we focus on the top 15 meetings as ranked by K; the minimum K in this set is 0.435. Although this reduces the total amount of data available, our intention is that this subset of the most consistent annotations will form a higherquality corpus. While the corpus classifies related action item utterances into action item “groups,” in this study we wish to treat the an</context>
<context position="23444" citStr="Purver et al. (2006)" startWordPosition="3795" endWordPosition="3798"> factors raise the question of annotation quality. The low κ scores in Section 3 are indicative of annotation problems. Post-hoc error analysis yields many examples of utterances which are somewhat difficult to imagine as possible, never mind desirable, to tag. The fact that the extremely useful oracular information present in the fine-grained DA annotation does not raise performance to the high levels that one might expect further suggests that the annotations are not ideal—or, at the least, that they are inconsistent with the DA annotations.2 This analysis is consistent with the findings of Purver et al. (2006), who achieve an F measure of 2Which is not to say they are devoid of significant value— training and testing our best system on the corpus with the 590 positive classifications randomly shuffled across all utterances yields an F measure of only 4.82. less than 25% when applying SVMs to the classification task to the same corpus, and motivate the development of a new corpus of action item annotations. 9 Future work In Section 6 we showed that contextual lexical features are useful for the task of action item detection, at least in the fairly limited manner employed in our implementation, which</context>
</contexts>
<marker>Purver, Ehlen, Niekrasz, 2006</marker>
<rawString>Matthew Purver, Patrick Ehlen, and John Niekrasz. 2006. Detecting action items in multi-party meetings: Annotation and initial experiments. In Proceedings of the 3rd Joint Workshop on MLMI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Andreas Stolcke</author>
<author>Paul Taylor</author>
<author>Daniel Jurafsky</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Rachel Martin</author>
<author>Marie Meteer</author>
<author>Carol Van EssDykema</author>
</authors>
<title>Can prosody aid the automatic classification of dialog acts in conversational speech? Language and Speech,</title>
<date>1998</date>
<pages>41--3</pages>
<marker>Shriberg, Bates, Stolcke, Taylor, Jurafsky, Ries, Coccaro, Martin, Meteer, Van EssDykema, 1998</marker>
<rawString>Elizabeth Shriberg, Rebecca Bates, Andreas Stolcke, Paul Taylor, Daniel Jurafsky, Klaus Ries, Noah Coccaro, Rachel Martin, Marie Meteer, and Carol Van EssDykema. 1998. Can prosody aid the automatic classification of dialog acts in conversational speech? Language and Speech, 41(3–4):439–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elizabeth Shriberg</author>
<author>Raj Dhillon</author>
<author>Sonali Bhagat</author>
<author>Jeremy Ang</author>
<author>Hannah Carvey</author>
</authors>
<title>The ICSI meeting recorder dialog act (MRDA) corpus.</title>
<date>2004</date>
<booktitle>In Proceedings of the 5th SIGDIAL Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="3364" citStr="Shriberg et al., 2004" startWordPosition="510" endWordPosition="513">y detecting action item utterances in audio recordings of multi-party meetings. Although available corpora for action items are not ideal, it is hoped that the feature analysis presented here will be of use to later work on other corpora. 2 Related work Multi-party meetings have attracted a significant amount of recent research attention. The creation of the ICSI corpus (Janin et al., 2003), comprised of 72 hours of meeting recordings with an average of 6 speakers per meeting, with associated transcripts, has spurred further annotations for various types of information, including dialog acts (Shriberg et al., 2004), topic hierarchies and action items (Gruenstein et al., 2005), and “hot spots” (Wrede and Shriberg, 2003). The classification of individual utterances based on their role in the dialog, i.e. as opposed to their semantic payload, has a long history, especially in the context of dialog act (DA) classification. 96 Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 96–103, Sydney, July 2006. c�2006 Association for Computational Linguistics Research on DA classification initially focused on two-party conversational speech (Mast et al., 1996; Stolcke et al., 1998; Shriberg et </context>
<context position="14911" citStr="Shriberg et al., 2004" startWordPosition="2455" endWordPosition="2458">nder (Bikel et al., 1997) to mark temporal expressions (“TIMEX” tags) in utterance transcripts, and create a binary feature denoting 99 the existence of a temporal expression in each utterance. Note that as Identifinder was trained on broadcast news corpora, applying it to the very different domain of multi-party meeting transcripts may not result in optimal behavior. 5.7 Dialog-specific semantic features Under the hypothesis that action item utterances may be closely correlated with specific dialog act tags, we use the dialog act annotations from the ICSI Meeting Recorder Dialog Act Corpus. (Shriberg et al., 2004) As these DA annotations do not correspond one-to-one with utterances in the ICSI corpus, we align them in the most liberal way possible, i.e., if at least one word in an utterance is annotated for a particular DA, we mark the entirety of that utterance as exhibiting that DA. We consider both fine-grained and coarsegrained dialog acts.1 The former yields 56 features, indicating occurrence of DA tags such as “appreciation,” “rhetorical question,” and “task management”; the latter consists of only 7 classes—“disruption,” “backchannel,” “filler,” “statement,” “question,” “unlabeled,” and “unknown</context>
</contexts>
<marker>Shriberg, Dhillon, Bhagat, Ang, Carvey, 2004</marker>
<rawString>Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy Ang, and Hannah Carvey. 2004. The ICSI meeting recorder dialog act (MRDA) corpus. In Proceedings of the 5th SIGDIAL Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Noah Coccaro</author>
<author>Daniel Jurafsky</author>
<author>Rachel Martin</author>
<author>Marie Meteer</author>
<author>Klaus Ries</author>
<author>Paul Taylor</author>
<author>Carol Van EssDykema</author>
</authors>
<title>Dialog act modeling for conversational speech.</title>
<date>1998</date>
<booktitle>In Proceedings of the AAAI Spring Symposium on Applying Machine Learning to Discourse Processing.</booktitle>
<marker>Stolcke, Shriberg, Bates, Coccaro, Jurafsky, Martin, Meteer, Ries, Taylor, Van EssDykema, 1998</marker>
<rawString>Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates, Noah Coccaro, Daniel Jurafsky, Rachel Martin, Marie Meteer, Klaus Ries, Paul Taylor, and Carol Van EssDykema. 1998. Dialog act modeling for conversational speech. In Proceedings of the AAAI Spring Symposium on Applying Machine Learning to Discourse Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
</authors>
<title>Enriching the knowledge sources used in a maximum entropy part-of-speech tagger.</title>
<date>2000</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="12201" citStr="Toutanova and Manning, 2000" startWordPosition="2021" endWordPosition="2024">e. We normalize for case and for certain contractions; for example, “I’ll” is transformed into “I will”. Note that these are oracle features, as the transcripts are human-produced and not the product of automatic speech recognizer (ASR) system output. 5.2 Contextual lexical features We extract word unigram and bigram features from the transcript for the previous and next utterances across all speakers in the meeting. 5.3 Syntactic features Under the hypothesis that action item utterances will exhibit particular syntactic patterns, we use a conditional Markov model part-of-speech (POS) tagger (Toutanova and Manning, 2000) trained on the Switchboard corpus (Godfrey et al., 1992) to tag utterance words for part of speech. We use the following binary POS features: • Presence of UH tag, denoting the presence of an “interjection” (including filled pauses, unfilled pauses, and discourse markers). • Presence of MD tag, denoting presence of a modal verb. • Number of NN* tags, denoting the number of nouns. • Number of VB* tags, denoting the number of verbs. • Presence of VBD tag, denoting the presence of a past-tense verb. 5.4 Prosodic features Under the hypothesis that action item utterances will exhibit particular pr</context>
</contexts>
<marker>Toutanova, Manning, 2000</marker>
<rawString>Kristina Toutanova and Christopher D. Manning. 2000. Enriching the knowledge sources used in a maximum entropy part-of-speech tagger. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Britta Wrede</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Spotting “hot spots” in meetings: Human judgments and prosodic cues.</title>
<date>2003</date>
<booktitle>In Proceedings of the European Conference on Speech Communication and Technology.</booktitle>
<contexts>
<context position="3470" citStr="Wrede and Shriberg, 2003" startWordPosition="526" endWordPosition="529">ora for action items are not ideal, it is hoped that the feature analysis presented here will be of use to later work on other corpora. 2 Related work Multi-party meetings have attracted a significant amount of recent research attention. The creation of the ICSI corpus (Janin et al., 2003), comprised of 72 hours of meeting recordings with an average of 6 speakers per meeting, with associated transcripts, has spurred further annotations for various types of information, including dialog acts (Shriberg et al., 2004), topic hierarchies and action items (Gruenstein et al., 2005), and “hot spots” (Wrede and Shriberg, 2003). The classification of individual utterances based on their role in the dialog, i.e. as opposed to their semantic payload, has a long history, especially in the context of dialog act (DA) classification. 96 Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 96–103, Sydney, July 2006. c�2006 Association for Computational Linguistics Research on DA classification initially focused on two-party conversational speech (Mast et al., 1996; Stolcke et al., 1998; Shriberg et al., 1998) and, more recently, has extended to multi-party audio recordings like the ICSI corpus (Shriberg</context>
<context position="24376" citStr="Wrede and Shriberg, 2003" startWordPosition="3957" endWordPosition="3960">task to the same corpus, and motivate the development of a new corpus of action item annotations. 9 Future work In Section 6 we showed that contextual lexical features are useful for the task of action item detection, at least in the fairly limited manner employed in our implementation, which simply looks at immediate previous and immediate next utterances. It seems likely that applying a sequence model such as an HMM or conditional random field (CRFs) will act as a generalization of this feature and may further improve performance. Addition of features such as speaker change and “hot spots” (Wrede and Shriberg, 2003) may also aid classification. Conversely, it is possible that feature selection techniques may improve performance by helping to eliminate poor-quality features. In this work we have followed an “everything but the kitchen sink” approach, in part because we were curious about which features would prove useful. The effect of adding POS and coarse-grained DA features illustrates that this is not necessarily the ideal strategy in terms of ultimate system performance. In general, the features evaluated in this work are an indiscriminate mix of human- and automatically-generated features; of the hu</context>
</contexts>
<marker>Wrede, Shriberg, 2003</marker>
<rawString>Britta Wrede and Elizabeth Shriberg. 2003. Spotting “hot spots” in meetings: Human judgments and prosodic cues. In Proceedings of the European Conference on Speech Communication and Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Zimmermann</author>
<author>Yang Liu</author>
<author>Elizabeth Shriberg</author>
<author>Andreas Stolcke</author>
</authors>
<title>Toward joint segmentation and classification of dialog acts in multiparty meetings.</title>
<date>2005</date>
<booktitle>In Proceedings of the 2nd Joint Workshop on MLMI.</booktitle>
<contexts>
<context position="4255" citStr="Zimmermann et al., 2005" startWordPosition="646" endWordPosition="649"> context of dialog act (DA) classification. 96 Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 96–103, Sydney, July 2006. c�2006 Association for Computational Linguistics Research on DA classification initially focused on two-party conversational speech (Mast et al., 1996; Stolcke et al., 1998; Shriberg et al., 1998) and, more recently, has extended to multi-party audio recordings like the ICSI corpus (Shriberg et al., 2004). Machine learning techniques such as graphical models (Ji and Bilmes, 2005), maximum entropy models (Ang et al., 2005), and hidden Markov models (Zimmermann et al., 2005) have been used to classify utterances from multiparty conversations. It is only more recently that work focused specifically on action items themselves has been developed. SVMs have been successfully applied to the task of extracting action items from email messages (Bennett and Carbonell, 2005; CorstonOliver et al., 2004). Bennett and Carbonell, in particular, distinguish the task of action item detection in email from the more well-studied task of text classification, noting the finer granularity of the action item task and the difference of semantics vs. intent. (Although recent work has b</context>
</contexts>
<marker>Zimmermann, Liu, Shriberg, Stolcke, 2005</marker>
<rawString>Matthias Zimmermann, Yang Liu, Elizabeth Shriberg, and Andreas Stolcke. 2005. Toward joint segmentation and classification of dialog acts in multiparty meetings. In Proceedings of the 2nd Joint Workshop on MLMI.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>