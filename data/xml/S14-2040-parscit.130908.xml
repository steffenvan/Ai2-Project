<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.088986">
<title confidence="0.9977225">
Duluth: Measuring Cross–Level Semantic Similarity
with First and Second–Order Dictionary Overlaps
</title>
<author confidence="0.999021">
Ted Pedersen
</author>
<affiliation confidence="0.998637">
Department of Computer Science
University of Minnesota
</affiliation>
<address confidence="0.592388">
Duluth, MN 55812
</address>
<email confidence="0.998014">
tpederse@d.umn.edu
</email>
<sectionHeader confidence="0.993869" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999964466666667">
This paper describes the Duluth systems
that participated in the Cross–Level Se-
mantic Similarity task of SemEval–2014.
These three systems were all unsupervised
and relied on a dictionary melded together
from various sources, and used first–order
(Lesk) and second–order (Vector) over-
laps to measure similarity. The first–order
overlaps fared well according to Spear-
man’s correlation (top 5) but less so rela-
tive to Pearson’s. Most systems performed
at comparable levels for both Spearman’s
and Pearson’s measure, which suggests
the Duluth approach is potentially unique
among the participating systems.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.96466973015873">
Cross–Level Semantic Similarity (CLSS) is a
novel variation on the problem of semantic simi-
larity. As traditionally formulated, pairs of words,
pairs of phrases, or pairs of sentences are scored
for similarity. However, the CLSS shared task
(Jurgens et al., 2014) included 4 subtasks where
pairs of different granularity were measured for
semantic similarity. These included : word-2-
sense (w2s), phrase-2-word (p2w), sentence-2-
phrase (s2p), and paragraph-2-sentence (g2s). In
addition to different levels of granularity, these
pairs included slang, jargon and other examples of
non–standard English.
We were drawn to this task because of our long–
standing interest in semantic similarity. We have
pursued approaches ranging from those that rely
on structured knowledge sources like WordNet
(e.g., WordNet::Similarity) (Pedersen et al., 2004)
to those that use distributional information found
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
in raw text (e.g., SenseClusters) (Purandare and
Pedersen, 2004). Our approach in this shared task
is a bit of both, but relies on using definitions for
each item in a pair so that similarity can be mea-
sured using first or second–order overlaps.
A first–order approach finds direct matches be-
tween the words in a pair of definitions. In a
second–order approach each word in a definition
is replaced by a vector of the words it co–occurs
with, and then the vectors for all the words in a
definition are averaged together to represent the
definition. Then, similarity can be measured by
finding the cosine between pairs of these vectors.
We decided on a definition based approach since
it had the potential to normalize the differences in
granularity of the pairs.
The main difficulty in comparing definitions is
that they can be very brief or may not even ex-
ist at all. This is why we combined various dif-
ferent kinds of resources to arrive at our dictio-
nary. While we achieved near total coverage of
words and senses, phrases were sparsely covered,
and sentences and paragraphs had no coverage. In
those cases we used the text of the phrase, sentence
or paragraph to serve as its own definition.
The Duluth systems were implemented using
the UMLS::Similarity package (McInnes et al.,
2009) (version 1.35)1, which includes support for
user–defined dictionaries, first–order Lesk meth-
ods, and second–order Vector methods. As a result
the Duluth systems required minimal implementa-
tion, so once a dictionary was ready experiments
could begin immediately.
This paper is organized as follows. First, the
first–order Lesk and second–order Vector mea-
sures are described. Then we discuss the details
of the three Duluth systems that participated in
this task. Finally, we review the task results and
consider future directions for this problem and our
system.
</bodyText>
<footnote confidence="0.990725">
1http://umls-similarity.sourceforge.net
</footnote>
<page confidence="0.909884">
247
</page>
<note confidence="0.8303445">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 247–251,
Dublin, Ireland, August 23-24, 2014.
</note>
<sectionHeader confidence="0.98956" genericHeader="introduction">
2 Measures
</sectionHeader>
<bodyText confidence="0.99974325">
The Duluth systems use first–order Lesk meth-
ods (Duluth1 and Duluth3) and second–order Vec-
tor methods (Duluth2). These require that defini-
tions be available for both items in a pair, with the
caveat that we use the term definition somewhat
loosely to mean both traditional dictionary defini-
tions as well as various proxies when those are not
available.
</bodyText>
<subsectionHeader confidence="0.982464">
2.1 First–order Overlaps : Lesk
</subsectionHeader>
<bodyText confidence="0.99994162264151">
The Lesk measure (Lesk, 1986) was originally a
method of word sense disambiguation that mea-
sured the overlap among the definitions of the
possible senses of an ambiguous word with those
of surrounding words (Lesk, 1986). The senses
which have the largest number of overlaps are pre-
sumed to be the correct or intended senses for the
given context. A modified approach compares the
glosses of an ambiguous word with the surround-
ing context (Kilgarriff and Rosenzweig, 2000).
These are both first–order methods where defini-
tions are directly compared with each other, or
with the surrounding context.
In the Duluth systems, we measure overlaps by
summing the number of words shared between
definitions. Sequences of words that match are
weighted more heavily and contribute the square
of their length, while individual matching words
just count as one. For example, given the defini-
tions a small noisy collie and a small noisy bor-
der collie the stop word a would not be matched,
and then small noisy would match (and be given
a score of 4) and then collie would also match
(receiving a score of 1). So, the total Lesk score
would be 5. The scores of the Duluth systems were
normalized by dividing by the maximum Lesk
score for any pair in a subtask. This moves the
scores to a 0–1 scale, where 1.00 means the def-
initions are exactly the same, and where 0 means
they share no words.
One of the main drawbacks of the original Lesk
method is that glosses tend to be very short. Vari-
ous methods have been proposed to overcome this.
For example, (Banerjee and Pedersen, 2003) intro-
duced the Extended Gloss Overlap measure which
creates super–glosses by augmenting the glosses
of the senses to be measured with the glosses of
semantically related senses (which are connected
via relation links in WordNet). This adaptation
of the Lesk measure was first implemented in
WordNet::Similarity (Pedersen et al., 2004) and
then later in UMLS::Similarity (McInnes et al.,
2009). It has been applied to both word sense
disambiguation and semantic similarity, and gen-
erally found to improve on original Lesk (Baner-
jee, 2002; Banerjee and Pedersen, 2002; Patward-
han et al., 2003; McInnes and Pedersen, 2013).
However, the Duluth systems do not build super–
glosses in this way since many of the items in the
pairs are not found in WordNet. However, def-
initions are expanded in a simpler way, by merg-
ing together various different resources to increase
both coverage and the length of definitions.
</bodyText>
<subsectionHeader confidence="0.99468">
2.2 Second–order Overlaps : Vector
</subsectionHeader>
<bodyText confidence="0.999005611111111">
The main limitation of first–order Lesk ap-
proaches is that if terminology differs from one
definition to another, then meaningful matches
may not be found. For example, consider the def-
initions a small noisy collie and a dog that barks
a lot. A first–order overlap approach would find
no similarity (other than the stop word a) between
these definitions.
In cases like this some form of term expansion
could improve the chances of matching. Synonym
expansion is a well–known possibility, although in
the Duluth systems we opted to expand words with
their co–occurrence vectors. This follows from an
approach to word sense discrimination developed
by (Sch¨utze, 1998). Once words are expanded
then all the vectors in a definition are averaged to-
gether and this averaged vector becomes the rep-
resentation of the definition. This idea was first
implemented in WordNet::Similarity (Pedersen et
al., 2004) and then later in UMLS::Similarity
(McInnes et al., 2009), and has been applied to
word sense disambiguation and semantic similar-
ity (Patwardhan, 2003; Patwardhan and Pedersen,
2006; Liu et al., 2012).
The co–occurrences for the words in the defi-
nitions can come from any corpus of text. Once
a co–occurrence matrix is constructed, then each
word in each definition is replaced by its vector
from that matrix. If no such vector is found the
word is removed from the definition. Then, all the
vectors representing a definition are averaged to-
gether, and this vector is used to measure against
other vectors created in the same way. The scores
returned by the Vector measure are between 0 and
1 (inclusive) where 1.00 means exactly the same
and 0 means no similarity.
</bodyText>
<page confidence="0.995429">
248
</page>
<sectionHeader confidence="0.994826" genericHeader="method">
3 Duluth Systems
</sectionHeader>
<bodyText confidence="0.999886833333334">
There were three Duluth systems. Duluth1 and
Duluth3 use first–order Lesk, and Duluth2 uses
second–order Vector. Duluth3 was an ensemble
made up of Duluth1 and a close variant of it (Du-
luth1a, where the only difference was the stop list
employed).
Duluth1 and Duluth2 use the NSP stoplist2
which includes approximately 390 words and
comes from the SMART stoplist. Duluth1a treated
any word with 4 or fewer characters as a stop
word. Stemming was performed by all Duluth sys-
tems using the Porter algorithm as implemented in
the Lingua::Stem::en Perl module.
Before processing, all of the similarity pairs and
the dictionary entries were converted to lower case
and any non alpha-numeric characters were re-
placed with spaces. Also, any stop listed words
were removed.
</bodyText>
<subsectionHeader confidence="0.97818">
3.1 Dictionary Creation
</subsectionHeader>
<bodyText confidence="0.999908928571429">
The key step for all the Duluth systems is the
creation of the dictionary. We elected to treat
senses as word forms, and so our dictionary did
not make sense distinctions (and would include all
the senses of a word or phrase in its entry).
Since the words and phrases used in some pairs
are slang or non–standard English, traditional lex-
ical resources like WordNet do not provide ad-
equate coverage. However, WordNet provides
a good foundation for coverage of standard En-
glish, so we began by extracting the glosses from
WordNet v3.0 using the WordNet::QueryData Perl
module.
Wiktionary is a crowd sourced lexical resource
that includes more slang and jargon, so we also ex-
tracted entries from it using the Wiktionary::Parser
Perl module. In hopes of increasing our coverage
of phrases in particular, we looked up words and
phrases in Wikipedia using the WWW::Wikipedia
Perl module and used the first paragraph of an en-
try (up to the first heading) as a definition. Finally,
we also used the dict program in Linux which
we configured to use the following resources :
the Collaborative International Dictionary of En-
glish v.0.48 (gcide), Moby Thesaurus II by Grady
Ward, 1.0 (moby-thes), V.E.R.A. – Virtual Entity
of Relevant Acronyms (June 2006) (vera), the Jar-
gon File (version 4.4.7, 29 Dec 2003) (argon), the
</bodyText>
<footnote confidence="0.9172815">
2http://cpansearch.perl.org/src/TPEDERSE/Text-NSP-
1.27/bin/utils/stoplist-nsp.regex
</footnote>
<bodyText confidence="0.99689324137931">
Free On-line Dictionary of Computing (26 July
2010) (foldoc), and the CIA World Factbook 2002
(world02).
The most obvious question that arises about
these resources is how much coverage they pro-
vide for the pairs in the task. Based on experi-
ments on the trial data, we found that none of the
resources individually provided satisfactory cov-
erage, but if they were all combined then coverage
was reasonably good (although still not complete).
In the test data, it turned out there were only 20
items in the w2s subtask for which we did not have
a dictionary entry (out of 1000). However, for p2w
(phrase-2-word) there were 407 items not included
in the dictionary (most of which were phrases).
In the s2p (sentence-2-phrase) subtask there were
only 15 phrases which had definitions, so for this
subtask and also for g2s (paragraph-2-sentence)
the items themselves were the definitions for es-
sentially all the pairs.
Also of interest might be the total size of the
dictionaries created. The number of tokens in
g2s (paragraph-2-sentence) was 46,252, and in s2p
(sentence-2-phrase) it was 12,361. This is simply
the token count for the pairs included in each sub-
task. However, the dictionaries were much larger
for p2w (phrase-2-word), where the token count
was 262,876, and for w2s (word-2-sense) where it
was 499,767.
</bodyText>
<subsectionHeader confidence="0.999208">
3.2 Co–occurrence Matrix for Vector
</subsectionHeader>
<bodyText confidence="0.999895416666667">
In the Duluth systems, the co–occurrence matrix
comes from treating the WordNet glosses as a cor-
pus. Any pair of words that occur together in a
WordNet gloss are considered a co–occurrence.
There are 117,659 glosses, made up of
1,460,921 words. This resulted in a matrix of
90,516 rows and 99,493 columns, representing
708,152 unique bigrams. The matrix is not sym-
metric since the co–occurrences are bigrams, so
dog house is treated differently than house dog.
The WordNet glosses were extracted from ver-
sion 3.0 using the glossExtract Perl program3.
</bodyText>
<sectionHeader confidence="0.999933" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.947006">
Results for the CLSS task were ranked both
by Pearson’s and Spearman’s Correlation coeffi-
cients. Duluth system results are shown in Tables
1 and 2. These tables also include the results of
</bodyText>
<footnote confidence="0.974224">
3http://www.d.umn.edu/˜tpederse/Code/glossExtract-
v0.03.tar.gz
</footnote>
<page confidence="0.999135">
249
</page>
<tableCaption confidence="0.998361">
Table 1: Spearman’s Results
</tableCaption>
<table confidence="0.999753285714286">
g2s s2p p2w w2s rank
(of 38)
Top .801 .728 .424 .343 1
Duluth3 .725 .660 .399 .327 3
Duluth1 .726 .658 .385 .316 5
Duluth2 .553 .473 .235 .231 21
Baseline .613 .626 .162 .128
</table>
<tableCaption confidence="0.857812">
Table 2: Pearson’s Results
</tableCaption>
<table confidence="0.999523428571429">
g2s s2p p2w w2s rank
(of 38)
Top .811 .742 .415 .355 1
Duluth2 .501 .450 .241 .224 23
Duluth1 .458 .440 .075 .076 30
Duluth3 .455 .426 .075 .080 31
Baseline .527 .562 .165 .110
</table>
<bodyText confidence="0.99707246875">
the top ranked system (which was the same sys-
tem according to both measures) and results from
a baseline system that measures the Least Com-
mon Substring between the terms in a pair, except
in the w2s subtask, where it measured the LCS be-
tween the associated WordNet glosses.
Table 1 shows that the Duluth3 system offers a
slight improvement upon Duluth1. Recall that Du-
luth3 is an ensemble that includes Duluth1 and its
minor variant Duluth1a. Both of these are first-
order
rstorder methods, and significantly outperform the
second–order method Duluth2.
However, Table 2 tells a completely different
story. There the second–order system Duluth2
performs better, although overall rankings suffer
according to Pearson’s measure. It is also very ap-
parent that the ranks between Pearson’s and Spear-
man’s for Duluth1 and Duluth3 differ significantly
(from 3 to 30 and 5 to 31). This is very atypical,
and most systems maintained approximately the
same rankings between the two correlation mea-
sures. Note that Duluth2 behaves in this way,
where the relative ranking is 21 and 23.
Table 3 shows the number of pairs in each sub-
task which returned a score of 0. This could be due
to missing definitions, or no matches occurring be-
tween the definitions. Interestingly Duluth2 has a
much smaller number of 0 valued scores, which
shows the second–order method provides greater
coverage due to its more flexible notion of match-
ing. However, despite much higher numbers of
</bodyText>
<tableCaption confidence="0.997719">
Table 3: Number of Pairs with Score of 0
</tableCaption>
<table confidence="0.9773245">
g2s s2p p2w w2s
Duluth1 107 197 211 23
Duluth2 9 101 40 15
Duluth3 101 196 205 23
</table>
<bodyText confidence="0.997030666666667">
0s, Duluth1 and Duluth3 perform much better with
Spearman’s rank correlation coefficient. This sug-
gests that there is a kind of precision–recall trade-
off between these systems, where Duluth2 has
higher recall and Duluth1 and Duluth3 have higher
precision.
</bodyText>
<sectionHeader confidence="0.999273" genericHeader="conclusions">
5 Future Directions
</sectionHeader>
<bodyText confidence="0.999990571428572">
The relatively good performance of the first–order
Duluth systems (at least with respect to rank cor-
relation) shows again the important role of lexical
resources. Our first–order method was not appre-
ciably more complex than the baseline method, yet
it performed significantly better (especially for the
p2w and w2s tasks). This is no doubt due to the
more extensive dictionary that we employed.
That said, our approach to building the dictio-
nary was relatively crude, and could be substan-
tially improved. For example, we could be more
selective in the content we add to the entries for
words or phrases. We could also do more than
simply use the sentences and paragraphs as their
own definitions. For example, we could replace
words or phrases in sentences and paragraphs with
their definitions, and then carry out first or second–
order matching.
Second–order matching did not perform as well
as we had hoped. We believe this is due to the
somewhat noisy nature of the dictionaries we con-
structed, and expanding those definitions by re-
placing words with vectors created even more
noise. We believe that a more refined approach
to creating dictionaries would certainly improve
these results, as would a more selective method of
combining the co–occurrence vectors (rather than
simply averaging them).
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999096">
The Duluth systems relied heavily on the freely
available software package UMLS::Similarity. We
are grateful to Bridget McInnes and Ying Liu for
their work in developing this package, and in par-
ticular for the --dict functionality.
</bodyText>
<page confidence="0.992369">
250
</page>
<sectionHeader confidence="0.98932" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999618571428572">
Satanjeev Banerjee and Ted Pedersen. 2002. An
adapted Lesk algorithm for word sense disambigua-
tion using WordNet. In Proceedings of the Third In-
ternational Conference on Intelligent Text Process-
ing and Computational Linguistics, pages 136–145,
Mexico City, February.
Satanjeev Banerjee and Ted Pedersen. 2003. Ex-
tended gloss overlaps as a measure of semantic re-
latedness. In Proceedings of the Eighteenth Inter-
national Joint Conference on Artificial Intelligence,
pages 805–810, Acapulco, August.
Satanjeev Banerjee. 2002. Adapting the Lesk algo-
rithm for word sense disambiguation to WordNet.
Master’s thesis, University of Minnesota, Duluth,
December.
David Jurgens, Mohammad Taher Pilehvar, and
Roberto Navigli. 2014. Semeval-2014 task 3:
Cross–level semantic similarity. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014), Dublin, August.
Adam Kilgarriff and Joseph Rosenzweig. 2000. Spe-
cial issue on SENSEVAL: Framework and results
for english SENSEVAL. Computers and the Hu-
manities, 34(1–2):15–48.
Michael Lesk. 1986. Automatic sense disambiguation
using machine readable dictionaries: how to tell a
pine cone from an ice cream cone. In Proceedings of
the 5th annual international conference on Systems
documentation, pages 24–26. ACM Press.
Ying Liu, Bridget McInnes, Ted Pedersen, Genevieve
Melton-Meaux, and Serguei Pakhomov. 2012. Se-
mantic relatedness study using second–order co–
occurrence vectors computed from biomedical cor-
pora, UMLS, and WordNet. In Proceedings of the
2nd ACM SIGHIT International Health Informatics
Symposium, pages 363–371, Miami, FL.
Bridget McInnes and Ted Pedersen. 2013. Evaluating
measures of semantic similarity and relatedness to
disambiguate terms in biomedical text. Journal of
Biomedical Informatics, 46:1116–1124.
Bridget McInnes, Ted Pedersen, and Serguei Pakho-
mov. 2009. UMLS-Interface and UMLS-Similarity
: Open source software for measuring paths and
semantic similarity. In Proceedings of the Annual
Symposium ofthe American Medical Informatics As-
sociation, pages 431–435, San Francisco.
Siddharth Patwardhan and Ted Pedersen. 2006. Us-
ing WordNet-based Context Vectors to Estimate the
Semantic Relatedness of Concepts. In Proceed-
ings of the EACL 2006 Workshop on Making Sense
of Sense: Bringing Computational Linguistics and
Psycholinguistics Together, pages 1–8, Trento, Italy,
April.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted
Pedersen. 2003. Using measures of semantic re-
latedness for word sense disambiguation. In Pro-
ceedings of the Fourth International Conference on
Intelligent Text Processing and Computational Lin-
guistics, pages 241–257, Mexico City, February.
Siddharth Patwardhan. 2003. Incorporating dictionary
and corpus information into a context vector mea-
sure of semantic relatedness. Master’s thesis, Uni-
versity of Minnesota, Duluth, August.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::Similarity -Measuring the
relatedness of concepts. In Proceedings ofFifth An-
nual Meeting of the North American Chapter of the
Association for Computational Linguistics, pages
38–41, Boston, MA.
Amruta Purandare and Ted Pedersen. 2004. Word
sense discrimination by clustering contexts in vector
and similarity spaces. In Proceedings of the Confer-
ence on Computational Natural Language Learning,
pages 41–48, Boston, MA.
Hinrich Sch¨utze. 1998. Automatic word sense dis-
crimination. Computational Linguistics, 24(1):97–
123.
</reference>
<page confidence="0.997869">
251
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.647575">
<title confidence="0.998408">Duluth: Measuring Cross–Level Semantic with First and Second–Order Dictionary Overlaps</title>
<author confidence="0.992981">Ted</author>
<affiliation confidence="0.885335">Department of Computer University of Duluth, MN</affiliation>
<email confidence="0.998878">tpederse@d.umn.edu</email>
<abstract confidence="0.9998655625">This paper describes the Duluth systems that participated in the Cross–Level Semantic Similarity task of SemEval–2014. These three systems were all unsupervised and relied on a dictionary melded together from various sources, and used first–order (Lesk) and second–order (Vector) overlaps to measure similarity. The first–order overlaps fared well according to Spearman’s correlation (top 5) but less so relative to Pearson’s. Most systems performed at comparable levels for both Spearman’s and Pearson’s measure, which suggests the Duluth approach is potentially unique among the participating systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An adapted Lesk algorithm for word sense disambiguation using WordNet.</title>
<date>2002</date>
<booktitle>In Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>136--145</pages>
<location>Mexico City,</location>
<contexts>
<context position="6502" citStr="Banerjee and Pedersen, 2002" startWordPosition="1019" endWordPosition="1022"> to overcome this. For example, (Banerjee and Pedersen, 2003) introduced the Extended Gloss Overlap measure which creates super–glosses by augmenting the glosses of the senses to be measured with the glosses of semantically related senses (which are connected via relation links in WordNet). This adaptation of the Lesk measure was first implemented in WordNet::Similarity (Pedersen et al., 2004) and then later in UMLS::Similarity (McInnes et al., 2009). It has been applied to both word sense disambiguation and semantic similarity, and generally found to improve on original Lesk (Banerjee, 2002; Banerjee and Pedersen, 2002; Patwardhan et al., 2003; McInnes and Pedersen, 2013). However, the Duluth systems do not build super– glosses in this way since many of the items in the pairs are not found in WordNet. However, definitions are expanded in a simpler way, by merging together various different resources to increase both coverage and the length of definitions. 2.2 Second–order Overlaps : Vector The main limitation of first–order Lesk approaches is that if terminology differs from one definition to another, then meaningful matches may not be found. For example, consider the definitions a small noisy collie and a </context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An adapted Lesk algorithm for word sense disambiguation using WordNet. In Proceedings of the Third International Conference on Intelligent Text Processing and Computational Linguistics, pages 136–145, Mexico City, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>805--810</pages>
<location>Acapulco,</location>
<contexts>
<context position="5936" citStr="Banerjee and Pedersen, 2003" startWordPosition="932" endWordPosition="935">he stop word a would not be matched, and then small noisy would match (and be given a score of 4) and then collie would also match (receiving a score of 1). So, the total Lesk score would be 5. The scores of the Duluth systems were normalized by dividing by the maximum Lesk score for any pair in a subtask. This moves the scores to a 0–1 scale, where 1.00 means the definitions are exactly the same, and where 0 means they share no words. One of the main drawbacks of the original Lesk method is that glosses tend to be very short. Various methods have been proposed to overcome this. For example, (Banerjee and Pedersen, 2003) introduced the Extended Gloss Overlap measure which creates super–glosses by augmenting the glosses of the senses to be measured with the glosses of semantically related senses (which are connected via relation links in WordNet). This adaptation of the Lesk measure was first implemented in WordNet::Similarity (Pedersen et al., 2004) and then later in UMLS::Similarity (McInnes et al., 2009). It has been applied to both word sense disambiguation and semantic similarity, and generally found to improve on original Lesk (Banerjee, 2002; Banerjee and Pedersen, 2002; Patwardhan et al., 2003; McInnes</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence, pages 805–810, Acapulco, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
</authors>
<title>Adapting the Lesk algorithm for word sense disambiguation to WordNet. Master’s thesis,</title>
<date>2002</date>
<institution>University of Minnesota,</institution>
<location>Duluth,</location>
<contexts>
<context position="6473" citStr="Banerjee, 2002" startWordPosition="1016" endWordPosition="1018">ve been proposed to overcome this. For example, (Banerjee and Pedersen, 2003) introduced the Extended Gloss Overlap measure which creates super–glosses by augmenting the glosses of the senses to be measured with the glosses of semantically related senses (which are connected via relation links in WordNet). This adaptation of the Lesk measure was first implemented in WordNet::Similarity (Pedersen et al., 2004) and then later in UMLS::Similarity (McInnes et al., 2009). It has been applied to both word sense disambiguation and semantic similarity, and generally found to improve on original Lesk (Banerjee, 2002; Banerjee and Pedersen, 2002; Patwardhan et al., 2003; McInnes and Pedersen, 2013). However, the Duluth systems do not build super– glosses in this way since many of the items in the pairs are not found in WordNet. However, definitions are expanded in a simpler way, by merging together various different resources to increase both coverage and the length of definitions. 2.2 Second–order Overlaps : Vector The main limitation of first–order Lesk approaches is that if terminology differs from one definition to another, then meaningful matches may not be found. For example, consider the definition</context>
</contexts>
<marker>Banerjee, 2002</marker>
<rawString>Satanjeev Banerjee. 2002. Adapting the Lesk algorithm for word sense disambiguation to WordNet. Master’s thesis, University of Minnesota, Duluth, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Mohammad Taher Pilehvar</author>
<author>Roberto Navigli</author>
</authors>
<title>Semeval-2014 task 3: Cross–level semantic similarity.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014),</booktitle>
<location>Dublin,</location>
<contexts>
<context position="1093" citStr="Jurgens et al., 2014" startWordPosition="152" endWordPosition="155">ond–order (Vector) overlaps to measure similarity. The first–order overlaps fared well according to Spearman’s correlation (top 5) but less so relative to Pearson’s. Most systems performed at comparable levels for both Spearman’s and Pearson’s measure, which suggests the Duluth approach is potentially unique among the participating systems. 1 Introduction Cross–Level Semantic Similarity (CLSS) is a novel variation on the problem of semantic similarity. As traditionally formulated, pairs of words, pairs of phrases, or pairs of sentences are scored for similarity. However, the CLSS shared task (Jurgens et al., 2014) included 4 subtasks where pairs of different granularity were measured for semantic similarity. These included : word-2- sense (w2s), phrase-2-word (p2w), sentence-2- phrase (s2p), and paragraph-2-sentence (g2s). In addition to different levels of granularity, these pairs included slang, jargon and other examples of non–standard English. We were drawn to this task because of our long– standing interest in semantic similarity. We have pursued approaches ranging from those that rely on structured knowledge sources like WordNet (e.g., WordNet::Similarity) (Pedersen et al., 2004) to those that us</context>
</contexts>
<marker>Jurgens, Pilehvar, Navigli, 2014</marker>
<rawString>David Jurgens, Mohammad Taher Pilehvar, and Roberto Navigli. 2014. Semeval-2014 task 3: Cross–level semantic similarity. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), Dublin, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Joseph Rosenzweig</author>
</authors>
<date>2000</date>
<booktitle>Special issue on SENSEVAL: Framework and results for english SENSEVAL. Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="4840" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="736" endWordPosition="739">nition somewhat loosely to mean both traditional dictionary definitions as well as various proxies when those are not available. 2.1 First–order Overlaps : Lesk The Lesk measure (Lesk, 1986) was originally a method of word sense disambiguation that measured the overlap among the definitions of the possible senses of an ambiguous word with those of surrounding words (Lesk, 1986). The senses which have the largest number of overlaps are presumed to be the correct or intended senses for the given context. A modified approach compares the glosses of an ambiguous word with the surrounding context (Kilgarriff and Rosenzweig, 2000). These are both first–order methods where definitions are directly compared with each other, or with the surrounding context. In the Duluth systems, we measure overlaps by summing the number of words shared between definitions. Sequences of words that match are weighted more heavily and contribute the square of their length, while individual matching words just count as one. For example, given the definitions a small noisy collie and a small noisy border collie the stop word a would not be matched, and then small noisy would match (and be given a score of 4) and then collie would also match (</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>Adam Kilgarriff and Joseph Rosenzweig. 2000. Special issue on SENSEVAL: Framework and results for english SENSEVAL. Computers and the Humanities, 34(1–2):15–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th annual international conference on Systems documentation,</booktitle>
<pages>24--26</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="4398" citStr="Lesk, 1986" startWordPosition="665" endWordPosition="666">tem. 1http://umls-similarity.sourceforge.net 247 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 247–251, Dublin, Ireland, August 23-24, 2014. 2 Measures The Duluth systems use first–order Lesk methods (Duluth1 and Duluth3) and second–order Vector methods (Duluth2). These require that definitions be available for both items in a pair, with the caveat that we use the term definition somewhat loosely to mean both traditional dictionary definitions as well as various proxies when those are not available. 2.1 First–order Overlaps : Lesk The Lesk measure (Lesk, 1986) was originally a method of word sense disambiguation that measured the overlap among the definitions of the possible senses of an ambiguous word with those of surrounding words (Lesk, 1986). The senses which have the largest number of overlaps are presumed to be the correct or intended senses for the given context. A modified approach compares the glosses of an ambiguous word with the surrounding context (Kilgarriff and Rosenzweig, 2000). These are both first–order methods where definitions are directly compared with each other, or with the surrounding context. In the Duluth systems, we measu</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 5th annual international conference on Systems documentation, pages 24–26. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ying Liu</author>
<author>Bridget McInnes</author>
<author>Ted Pedersen</author>
<author>Genevieve Melton-Meaux</author>
<author>Serguei Pakhomov</author>
</authors>
<title>Semantic relatedness study using second–order co– occurrence vectors computed from biomedical corpora, UMLS, and WordNet.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium,</booktitle>
<pages>363--371</pages>
<location>Miami, FL.</location>
<contexts>
<context position="7982" citStr="Liu et al., 2012" startWordPosition="1258" endWordPosition="1261">ty, although in the Duluth systems we opted to expand words with their co–occurrence vectors. This follows from an approach to word sense discrimination developed by (Sch¨utze, 1998). Once words are expanded then all the vectors in a definition are averaged together and this averaged vector becomes the representation of the definition. This idea was first implemented in WordNet::Similarity (Pedersen et al., 2004) and then later in UMLS::Similarity (McInnes et al., 2009), and has been applied to word sense disambiguation and semantic similarity (Patwardhan, 2003; Patwardhan and Pedersen, 2006; Liu et al., 2012). The co–occurrences for the words in the definitions can come from any corpus of text. Once a co–occurrence matrix is constructed, then each word in each definition is replaced by its vector from that matrix. If no such vector is found the word is removed from the definition. Then, all the vectors representing a definition are averaged together, and this vector is used to measure against other vectors created in the same way. The scores returned by the Vector measure are between 0 and 1 (inclusive) where 1.00 means exactly the same and 0 means no similarity. 248 3 Duluth Systems There were th</context>
</contexts>
<marker>Liu, McInnes, Pedersen, Melton-Meaux, Pakhomov, 2012</marker>
<rawString>Ying Liu, Bridget McInnes, Ted Pedersen, Genevieve Melton-Meaux, and Serguei Pakhomov. 2012. Semantic relatedness study using second–order co– occurrence vectors computed from biomedical corpora, UMLS, and WordNet. In Proceedings of the 2nd ACM SIGHIT International Health Informatics Symposium, pages 363–371, Miami, FL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bridget McInnes</author>
<author>Ted Pedersen</author>
</authors>
<title>Evaluating measures of semantic similarity and relatedness to disambiguate terms in biomedical text.</title>
<date>2013</date>
<journal>Journal of Biomedical Informatics,</journal>
<pages>46--1116</pages>
<contexts>
<context position="6556" citStr="McInnes and Pedersen, 2013" startWordPosition="1028" endWordPosition="1031">, 2003) introduced the Extended Gloss Overlap measure which creates super–glosses by augmenting the glosses of the senses to be measured with the glosses of semantically related senses (which are connected via relation links in WordNet). This adaptation of the Lesk measure was first implemented in WordNet::Similarity (Pedersen et al., 2004) and then later in UMLS::Similarity (McInnes et al., 2009). It has been applied to both word sense disambiguation and semantic similarity, and generally found to improve on original Lesk (Banerjee, 2002; Banerjee and Pedersen, 2002; Patwardhan et al., 2003; McInnes and Pedersen, 2013). However, the Duluth systems do not build super– glosses in this way since many of the items in the pairs are not found in WordNet. However, definitions are expanded in a simpler way, by merging together various different resources to increase both coverage and the length of definitions. 2.2 Second–order Overlaps : Vector The main limitation of first–order Lesk approaches is that if terminology differs from one definition to another, then meaningful matches may not be found. For example, consider the definitions a small noisy collie and a dog that barks a lot. A first–order overlap approach w</context>
</contexts>
<marker>McInnes, Pedersen, 2013</marker>
<rawString>Bridget McInnes and Ted Pedersen. 2013. Evaluating measures of semantic similarity and relatedness to disambiguate terms in biomedical text. Journal of Biomedical Informatics, 46:1116–1124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bridget McInnes</author>
<author>Ted Pedersen</author>
<author>Serguei Pakhomov</author>
</authors>
<title>UMLS-Interface and UMLS-Similarity : Open source software for measuring paths and semantic similarity.</title>
<date>2009</date>
<booktitle>In Proceedings of the Annual Symposium ofthe American Medical Informatics Association,</booktitle>
<pages>431--435</pages>
<location>San Francisco.</location>
<contexts>
<context position="3229" citStr="McInnes et al., 2009" startWordPosition="489" endWordPosition="492"> approach since it had the potential to normalize the differences in granularity of the pairs. The main difficulty in comparing definitions is that they can be very brief or may not even exist at all. This is why we combined various different kinds of resources to arrive at our dictionary. While we achieved near total coverage of words and senses, phrases were sparsely covered, and sentences and paragraphs had no coverage. In those cases we used the text of the phrase, sentence or paragraph to serve as its own definition. The Duluth systems were implemented using the UMLS::Similarity package (McInnes et al., 2009) (version 1.35)1, which includes support for user–defined dictionaries, first–order Lesk methods, and second–order Vector methods. As a result the Duluth systems required minimal implementation, so once a dictionary was ready experiments could begin immediately. This paper is organized as follows. First, the first–order Lesk and second–order Vector measures are described. Then we discuss the details of the three Duluth systems that participated in this task. Finally, we review the task results and consider future directions for this problem and our system. 1http://umls-similarity.sourceforge.n</context>
<context position="6329" citStr="McInnes et al., 2009" startWordPosition="991" endWordPosition="994"> and where 0 means they share no words. One of the main drawbacks of the original Lesk method is that glosses tend to be very short. Various methods have been proposed to overcome this. For example, (Banerjee and Pedersen, 2003) introduced the Extended Gloss Overlap measure which creates super–glosses by augmenting the glosses of the senses to be measured with the glosses of semantically related senses (which are connected via relation links in WordNet). This adaptation of the Lesk measure was first implemented in WordNet::Similarity (Pedersen et al., 2004) and then later in UMLS::Similarity (McInnes et al., 2009). It has been applied to both word sense disambiguation and semantic similarity, and generally found to improve on original Lesk (Banerjee, 2002; Banerjee and Pedersen, 2002; Patwardhan et al., 2003; McInnes and Pedersen, 2013). However, the Duluth systems do not build super– glosses in this way since many of the items in the pairs are not found in WordNet. However, definitions are expanded in a simpler way, by merging together various different resources to increase both coverage and the length of definitions. 2.2 Second–order Overlaps : Vector The main limitation of first–order Lesk approach</context>
<context position="7839" citStr="McInnes et al., 2009" startWordPosition="1236" endWordPosition="1239">hese definitions. In cases like this some form of term expansion could improve the chances of matching. Synonym expansion is a well–known possibility, although in the Duluth systems we opted to expand words with their co–occurrence vectors. This follows from an approach to word sense discrimination developed by (Sch¨utze, 1998). Once words are expanded then all the vectors in a definition are averaged together and this averaged vector becomes the representation of the definition. This idea was first implemented in WordNet::Similarity (Pedersen et al., 2004) and then later in UMLS::Similarity (McInnes et al., 2009), and has been applied to word sense disambiguation and semantic similarity (Patwardhan, 2003; Patwardhan and Pedersen, 2006; Liu et al., 2012). The co–occurrences for the words in the definitions can come from any corpus of text. Once a co–occurrence matrix is constructed, then each word in each definition is replaced by its vector from that matrix. If no such vector is found the word is removed from the definition. Then, all the vectors representing a definition are averaged together, and this vector is used to measure against other vectors created in the same way. The scores returned by the</context>
</contexts>
<marker>McInnes, Pedersen, Pakhomov, 2009</marker>
<rawString>Bridget McInnes, Ted Pedersen, and Serguei Pakhomov. 2009. UMLS-Interface and UMLS-Similarity : Open source software for measuring paths and semantic similarity. In Proceedings of the Annual Symposium ofthe American Medical Informatics Association, pages 431–435, San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Ted Pedersen</author>
</authors>
<title>Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together,</booktitle>
<pages>1--8</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="7963" citStr="Patwardhan and Pedersen, 2006" startWordPosition="1254" endWordPosition="1257">nsion is a well–known possibility, although in the Duluth systems we opted to expand words with their co–occurrence vectors. This follows from an approach to word sense discrimination developed by (Sch¨utze, 1998). Once words are expanded then all the vectors in a definition are averaged together and this averaged vector becomes the representation of the definition. This idea was first implemented in WordNet::Similarity (Pedersen et al., 2004) and then later in UMLS::Similarity (McInnes et al., 2009), and has been applied to word sense disambiguation and semantic similarity (Patwardhan, 2003; Patwardhan and Pedersen, 2006; Liu et al., 2012). The co–occurrences for the words in the definitions can come from any corpus of text. Once a co–occurrence matrix is constructed, then each word in each definition is replaced by its vector from that matrix. If no such vector is found the word is removed from the definition. Then, all the vectors representing a definition are averaged together, and this vector is used to measure against other vectors created in the same way. The scores returned by the Vector measure are between 0 and 1 (inclusive) where 1.00 means exactly the same and 0 means no similarity. 248 3 Duluth Sy</context>
</contexts>
<marker>Patwardhan, Pedersen, 2006</marker>
<rawString>Siddharth Patwardhan and Ted Pedersen. 2006. Using WordNet-based Context Vectors to Estimate the Semantic Relatedness of Concepts. In Proceedings of the EACL 2006 Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together, pages 1–8, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Using measures of semantic relatedness for word sense disambiguation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>241--257</pages>
<location>Mexico City,</location>
<contexts>
<context position="6527" citStr="Patwardhan et al., 2003" startWordPosition="1023" endWordPosition="1027">e, (Banerjee and Pedersen, 2003) introduced the Extended Gloss Overlap measure which creates super–glosses by augmenting the glosses of the senses to be measured with the glosses of semantically related senses (which are connected via relation links in WordNet). This adaptation of the Lesk measure was first implemented in WordNet::Similarity (Pedersen et al., 2004) and then later in UMLS::Similarity (McInnes et al., 2009). It has been applied to both word sense disambiguation and semantic similarity, and generally found to improve on original Lesk (Banerjee, 2002; Banerjee and Pedersen, 2002; Patwardhan et al., 2003; McInnes and Pedersen, 2013). However, the Duluth systems do not build super– glosses in this way since many of the items in the pairs are not found in WordNet. However, definitions are expanded in a simpler way, by merging together various different resources to increase both coverage and the length of definitions. 2.2 Second–order Overlaps : Vector The main limitation of first–order Lesk approaches is that if terminology differs from one definition to another, then meaningful matches may not be found. For example, consider the definitions a small noisy collie and a dog that barks a lot. A f</context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2003</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2003. Using measures of semantic relatedness for word sense disambiguation. In Proceedings of the Fourth International Conference on Intelligent Text Processing and Computational Linguistics, pages 241–257, Mexico City, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
</authors>
<title>Incorporating dictionary and corpus information into a context vector measure of semantic relatedness. Master’s thesis,</title>
<date>2003</date>
<institution>University of Minnesota,</institution>
<location>Duluth,</location>
<contexts>
<context position="7932" citStr="Patwardhan, 2003" startWordPosition="1252" endWordPosition="1253">hing. Synonym expansion is a well–known possibility, although in the Duluth systems we opted to expand words with their co–occurrence vectors. This follows from an approach to word sense discrimination developed by (Sch¨utze, 1998). Once words are expanded then all the vectors in a definition are averaged together and this averaged vector becomes the representation of the definition. This idea was first implemented in WordNet::Similarity (Pedersen et al., 2004) and then later in UMLS::Similarity (McInnes et al., 2009), and has been applied to word sense disambiguation and semantic similarity (Patwardhan, 2003; Patwardhan and Pedersen, 2006; Liu et al., 2012). The co–occurrences for the words in the definitions can come from any corpus of text. Once a co–occurrence matrix is constructed, then each word in each definition is replaced by its vector from that matrix. If no such vector is found the word is removed from the definition. Then, all the vectors representing a definition are averaged together, and this vector is used to measure against other vectors created in the same way. The scores returned by the Vector measure are between 0 and 1 (inclusive) where 1.00 means exactly the same and 0 means</context>
</contexts>
<marker>Patwardhan, 2003</marker>
<rawString>Siddharth Patwardhan. 2003. Incorporating dictionary and corpus information into a context vector measure of semantic relatedness. Master’s thesis, University of Minnesota, Duluth, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::Similarity -Measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings ofFifth Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>38--41</pages>
<location>Boston, MA.</location>
<contexts>
<context position="1676" citStr="Pedersen et al., 2004" startWordPosition="233" endWordPosition="236">CLSS shared task (Jurgens et al., 2014) included 4 subtasks where pairs of different granularity were measured for semantic similarity. These included : word-2- sense (w2s), phrase-2-word (p2w), sentence-2- phrase (s2p), and paragraph-2-sentence (g2s). In addition to different levels of granularity, these pairs included slang, jargon and other examples of non–standard English. We were drawn to this task because of our long– standing interest in semantic similarity. We have pursued approaches ranging from those that rely on structured knowledge sources like WordNet (e.g., WordNet::Similarity) (Pedersen et al., 2004) to those that use distributional information found This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ in raw text (e.g., SenseClusters) (Purandare and Pedersen, 2004). Our approach in this shared task is a bit of both, but relies on using definitions for each item in a pair so that similarity can be measured using first or second–order overlaps. A first–order approach finds direct matches between the words in a pair of definitions. In</context>
<context position="6271" citStr="Pedersen et al., 2004" startWordPosition="982" endWordPosition="985">ale, where 1.00 means the definitions are exactly the same, and where 0 means they share no words. One of the main drawbacks of the original Lesk method is that glosses tend to be very short. Various methods have been proposed to overcome this. For example, (Banerjee and Pedersen, 2003) introduced the Extended Gloss Overlap measure which creates super–glosses by augmenting the glosses of the senses to be measured with the glosses of semantically related senses (which are connected via relation links in WordNet). This adaptation of the Lesk measure was first implemented in WordNet::Similarity (Pedersen et al., 2004) and then later in UMLS::Similarity (McInnes et al., 2009). It has been applied to both word sense disambiguation and semantic similarity, and generally found to improve on original Lesk (Banerjee, 2002; Banerjee and Pedersen, 2002; Patwardhan et al., 2003; McInnes and Pedersen, 2013). However, the Duluth systems do not build super– glosses in this way since many of the items in the pairs are not found in WordNet. However, definitions are expanded in a simpler way, by merging together various different resources to increase both coverage and the length of definitions. 2.2 Second–order Overlaps</context>
<context position="7781" citStr="Pedersen et al., 2004" startWordPosition="1227" endWordPosition="1230">d find no similarity (other than the stop word a) between these definitions. In cases like this some form of term expansion could improve the chances of matching. Synonym expansion is a well–known possibility, although in the Duluth systems we opted to expand words with their co–occurrence vectors. This follows from an approach to word sense discrimination developed by (Sch¨utze, 1998). Once words are expanded then all the vectors in a definition are averaged together and this averaged vector becomes the representation of the definition. This idea was first implemented in WordNet::Similarity (Pedersen et al., 2004) and then later in UMLS::Similarity (McInnes et al., 2009), and has been applied to word sense disambiguation and semantic similarity (Patwardhan, 2003; Patwardhan and Pedersen, 2006; Liu et al., 2012). The co–occurrences for the words in the definitions can come from any corpus of text. Once a co–occurrence matrix is constructed, then each word in each definition is replaced by its vector from that matrix. If no such vector is found the word is removed from the definition. Then, all the vectors representing a definition are averaged together, and this vector is used to measure against other v</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::Similarity -Measuring the relatedness of concepts. In Proceedings ofFifth Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 38–41, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amruta Purandare</author>
<author>Ted Pedersen</author>
</authors>
<title>Word sense discrimination by clustering contexts in vector and similarity spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning,</booktitle>
<pages>41--48</pages>
<location>Boston, MA.</location>
<contexts>
<context position="2004" citStr="Purandare and Pedersen, 2004" startWordPosition="276" endWordPosition="279"> slang, jargon and other examples of non–standard English. We were drawn to this task because of our long– standing interest in semantic similarity. We have pursued approaches ranging from those that rely on structured knowledge sources like WordNet (e.g., WordNet::Similarity) (Pedersen et al., 2004) to those that use distributional information found This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ in raw text (e.g., SenseClusters) (Purandare and Pedersen, 2004). Our approach in this shared task is a bit of both, but relies on using definitions for each item in a pair so that similarity can be measured using first or second–order overlaps. A first–order approach finds direct matches between the words in a pair of definitions. In a second–order approach each word in a definition is replaced by a vector of the words it co–occurs with, and then the vectors for all the words in a definition are averaged together to represent the definition. Then, similarity can be measured by finding the cosine between pairs of these vectors. We decided on a definition b</context>
</contexts>
<marker>Purandare, Pedersen, 2004</marker>
<rawString>Amruta Purandare and Ted Pedersen. 2004. Word sense discrimination by clustering contexts in vector and similarity spaces. In Proceedings of the Conference on Computational Natural Language Learning, pages 41–48, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<pages>123</pages>
<marker>Sch¨utze, 1998</marker>
<rawString>Hinrich Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97– 123.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>