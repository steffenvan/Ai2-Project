<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011788">
<note confidence="0.816035">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 87-90, Lisbon, Portugal, 2000.
</note>
<title confidence="0.9984375">
Generating Synthetic Speech Prosody with Lazy Learning
in Tree Structures
</title>
<author confidence="0.913527">
Laurent Blin Laurent Miclet
</author>
<affiliation confidence="0.315841">
IRISA-ENSSAT IRISA-ENSSAT
</affiliation>
<address confidence="0.904797">
F-22305 Lannion, France F-22305 Lannion, France
</address>
<email confidence="0.997716">
blinOenssat.fr miclet@enssat.fr
</email>
<sectionHeader confidence="0.996572" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999949357142857">
We present ongoing work on prosody predic-
tion for speech synthesis. This approach con-
siders sentences as tree structures and infers
the prosody from a corpus of such structures
using machine learning techniques. The predic-
tion is achieved from the prosody of the closest
sentence of the corpus through tree similarity
measurements, using either the nearest neigh-
bour algorithm or an analogy-based approach.
We introduce two different tree structure rep-
resentations, the tree similarity metrics consid-
ered, and then we discuss the different predic-
tion methods. Experiments are currently under
process to qualify this approach.
</bodyText>
<sectionHeader confidence="0.998522" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999927576923077">
Natural prosody production remains a problem
in speech synthesis systems. Several automatic
prediction methods have already been tried for
this, including decision trees (Ross, 1995), neu-
ral networks (Traber, 1992), and HMMs (Jensen
et al., 1994). The original aspect of our pre-
diction approach is a tree structure representa-
tion of sentences, and the use of tree similar-
ity measurements to achieve the prosody pre-
diction. We think that reasoning on a whole
structure rather than on local features of a sen-
tence should better reflect the many relations
influencing the prosody. This approach is an
attempt to achieve such a goal.
The data used in this work is a part of the
Boston University Radio (WBUR) News Cor-
pus (Ostendorf et al., 1995). The prosodic infor-
mation consists of ToBI labeling of accents and
breaks (Silverman et al., 1992). The syntactic
and part-of-speech informations were obtained
from the part of the corpus processed in the
Penn Treebank project (Marcus et al., 1993).
We firstly describe the tree structures defined
for this work, then present the tree metrics that
we are using, and finally discuss how they are
manipulated to achieve the prosody prediction.
</bodyText>
<sectionHeader confidence="0.980511" genericHeader="method">
2 Tree Structures
</sectionHeader>
<bodyText confidence="0.998726428571429">
So far we have considered two types of struc-
tures in this work: a simple syntactic structure
and a performance structure (Gee and Grosjean,
1983). Their comparison in use should provide
some interesting knowledge about the usefulness
or the limitations of the elements of information
included in each one.
</bodyText>
<subsectionHeader confidence="0.995801">
2.1 Syntactic Structure
</subsectionHeader>
<bodyText confidence="0.999793076923077">
The syntactic structure considered is built ex-
clusively from the syntactic parsing of the given
sentences. This parsing, with the relative syn-
tactic tags, constitute the backbone of the struc-
ture. Below this structure lie the words of the
sentence, with their part-of-speech tags. Addi-
tional levels of nodes can be added deeper in
the structure to represent the syllables of each
word, and the phonemes of each syllable.
The syntactic structure corresponding to the
sentence &amp;quot;Hennessy will be a hard act to follow&amp;quot;
is presented in Figure 1 as an example (the syl-
lable level has been omitted for clarity).
</bodyText>
<subsectionHeader confidence="0.999039">
2.2 Performance Structure
</subsectionHeader>
<bodyText confidence="0.999925875">
The performance structure used in our approach
is a combination of syntactic and phonological
informations. Its upper part is a binary tree
where each node represents a break between the
two parts of the sentence contained into the sub-
trees of the node. This binary structure defines
a hierarchy: the closer to the root the node is,
the more salient (or stronger) the break is.
</bodyText>
<page confidence="0.996135">
87
</page>
<figure confidence="0.9685682">
to [TO] follow [VB]
1=wffl,
Hennessy [NNP] be [VB] NP
a [D [JJI- 31.&amp;quot;---S
VP
</figure>
<bodyText confidence="0.998010222222222">
our system in a full synthesis context where
no prosodic value is available). The currently
used method (Bachenko and Fitzpatrick, 1990)
provides rules to infer a default phrasing for
a sentence. Not only the effects of this esti-
mation will have to be quantified, but we plan
to develop a more accurate solution to predict
this structure accordingly to any corpus speaker
characteristics.
</bodyText>
<figureCaption confidence="0.694801">
Figure 1: Syntactic structure for the sentence
</figureCaption>
<bodyText confidence="0.998552571428571">
&amp;quot;Hennessy will be a hard act to follow&amp;quot;. (Syn-
tactic labels: S: simple declarative clause, NP:
noun phrase, VP: verb phrase. Part-of-speech
labels: NNP: proper noun, MD: modal, VB:
base form verb, DT: determiner, JJ: adjective,
NN: singular noun, TO: special label for &amp;quot;to&amp;quot;).
The lower part represents the phonological
phrases into which the whole sentence is divided
by the binary structure, and uses the same rep-
resentation levels as in the syntactic structure.
The only difference comes from a simplification
performed by joining the words into phonolog-
ical words (composed of one content word —
noun, adjective, verb or adverb — and of the
surrounding function words). Each phonologi-
cal phrase is labeled with a syntactic category
(the main one), and no break is supposed to
occur inside.
A possible performance structure for the same
example: &amp;quot;Hennessy will be a hard act to fol-
low&amp;quot; is shown in Figure 2.
</bodyText>
<equation confidence="0.6608506">
1
Hennessy [NNP] V-P-MP
I
I ZN
will be [VB] a hard [JJ] act INN]
</equation>
<figureCaption confidence="0.812244">
Figure 2: Performance structure for the sen-
</figureCaption>
<bodyText confidence="0.988104777777778">
tence &amp;quot;Hennessy will be a hard act to follow&amp;quot;.
The syntactic and part-of-speech labels have the
same meaning as in Figure 1. B1, B2 and B3
are the break-related nodes.
Unlike the syntactic structure, a first step of
prediction is done in the performance structure
with the break values. This prosody informa-
tion is known for the sentences in the corpus,
but has to be predicted for new ones (to put
</bodyText>
<sectionHeader confidence="0.991101" genericHeader="method">
3 Tree Metrics
</sectionHeader>
<bodyText confidence="0.999887333333333">
Now that the tree structures are defined, we
need the tools to predict the prosody. We have
considered two similarity metrics to calculate
the &amp;quot;distance&amp;quot; between two tree structures, in-
spired from the Wagner and Fisher&apos;s editing dis-
tance (Wagner and Fisher, 1974).
</bodyText>
<subsectionHeader confidence="0.998132">
3.1 Principles
</subsectionHeader>
<bodyText confidence="0.999963416666667">
Introducing a small set of elementary transfor-
mation operators upon trees (insertion or dele-
tion of a node, substitution of a node by an-
other one) it is possible to determine a set of
specific operation sequences that transform any
given tree into another one. Specifying costs
for each elementary operation (possibly a func-
tion of the node values) allows the evaluation
of a whole transformation cost by adding the
operation costs in the sequence. Therefore the
tree distance can be defined as the cost of the
sequence minimizing this sum.
</bodyText>
<subsectionHeader confidence="0.997905">
3.2 Considered Metrics
</subsectionHeader>
<bodyText confidence="0.999858736842106">
Many metrics can be defined from this princi-
ple. The differences come from the application
conditions which can be set on the operators. In
our experiments, two metrics are tested. They
both preserve the order of the nodes in the trees,
an essential condition in our application.
The first one (Selkow, 1977) allows only sub-
stitutions between nodes at the same depth level
in the trees. Moreover, the insertion or deletion
of a node involves respectively the insertion or
deletion of the whole subtree depending of the
node. These strict conditions should be able to
locate very close structures.
The other one (Zhang, 1995) allows the sub-
stitutions of nodes whatever theirs locations are
inside the structures. It also allows the insertion
or deletion of lonely nodes inside the structures.
Compared to the previous metric, these less rig-
orous stipulations should not only retrieve the
</bodyText>
<figure confidence="0.3811635">
TP
to follow [VB]
</figure>
<page confidence="0.992606">
88
</page>
<bodyText confidence="0.999719">
very close structures, but also other ones which
wouldn&apos;t have been found.
Moreover, these two algorithms also provide
a mapping between the nodes of the trees. This
mapping illustrates the operations which led to
the final distance value: the parts of the trees
which were inserted or deleted, and the ones
which were substituted or unchanged.
</bodyText>
<subsectionHeader confidence="0.993224">
3.3 Operation Costs
</subsectionHeader>
<bodyText confidence="0.999962785714286">
As exposed in section 3.1, a tree is &amp;quot;close&amp;quot; to
another one because of the definition of the op-
erators costs. In this work, they have been set
to allow the only comparison of nodes of same
structural nature (break-related nodes together,
syllable-related nodes together...), and to repre-
sent the linguistic &amp;quot;similarity&amp;quot; between compa-
rable elements (to set that an adjective may be
&amp;quot;closer&amp;quot; to a noun than to a determiner...).
These operation costs are currently manually
set. To decide on the scale of values to affect
is not an easy task, and it needs some human
expertise. One possibility would be to further
automate the process for setting these values.
</bodyText>
<sectionHeader confidence="0.965771" genericHeader="method">
4 Prosody Prediction
</sectionHeader>
<bodyText confidence="0.999111333333333">
The tree representations and the metrics can
now be used to predict the prosody of a sen-
tence.
</bodyText>
<subsectionHeader confidence="0.99465">
4.1 Nearest Neighbour Prediction
</subsectionHeader>
<bodyText confidence="0.999995941176471">
The simple method that we have firstly used is
the nearest neighbour algorithm: given a new
sentence, the closest match among the corpus
of sentences of known prosody is retrieved and
used to infer the prosody of the new sentence.
The mapping from the tree distance computa-
tions can be used to give a simple way to know
where to apply the prosody of one sentence onto
the other one, from the words linked inside.
Unfortunately, this process may not be as
easy. The ideal mapping would be that each
word of the new sentence had a corresponding
word in the other sentence. Hopeless, the two
sentences may not be as closed as desired, and
some words may have been inserted or deleted.
To decide on the prosody for these unlinked
parts is a problem.
</bodyText>
<subsectionHeader confidence="0.813239">
4.2 Analogy-Based Prediction
</subsectionHeader>
<bodyText confidence="0.9999404">
A potential way to improve the prediction is
based on analogy. The previous mapping be-
tween the two structures defines a tree transfor-
mation. The idea of this approach is based on
the knowledge brought by other pairs of struc-
tures from the corpus sharing the same trans-
formation.
This approach can be connected to the ana-
logical framework defined by Pirrelli and Yvon,
where inference processes are presented for sym-
bolic and string values by the mean of two no-
tions: the analogical proportion, and the ana-
logical transfer (Pinelli and Yvon, 1999).
Concerning our problem, and given three
known tree structures T1, T2 T3 and a new one
, an analogical proportion would be expressed
as: T1 is to T2 as T3 is t 0 Ti if and only if the set
of operations transforming T1 into T2 is equiva-
lent to the one transforming T3 into T&apos;, accord-
ingly to a specific tree metric. There are various
levels for defining this transformation equiva-
lence. A strict identity would be for instance
the insertion of the same structure at the same
place, representing the same word (and having
the same syntactic function in the sentence). A
less strict equivalence could be the insertion of
a different word having the same number of syl-
lables. Weaker and weaker conditions can be
set. As a consequence, these different possibili-
ties have to be tested accordingly to the amount
of diversity in the corpus to prove the efficiency
of this equivalence.
Next, the analogical transfer would be to ap-
ply on the phrase described by T3 the prosody
transformation defined between T1 and T2 as to
get the prosody of the phrase of T&apos;. The for-
malization of this prosody transfer is still under
development.
From these two notions, the analogical infer-
ence would be therefore defined as:
</bodyText>
<listItem confidence="0.99598175">
• firstly, to retrieve all analogical proportions
involving T&apos; and three known structures in
the corpus;
• secondly, to compute the analogical trans-
</listItem>
<bodyText confidence="0.924067125">
fer for each 3-tuple of known structures,
and to store its result in a set of possible
outputs if the transfer succeeds.
This analogical inference as described above
may be a long task in the retrieval of every 3-
tuple of known structures since a tree trans-
formation can be defined between any pair of
them. For very dissimilar structures, the set of
</bodyText>
<page confidence="0.999506">
89
</page>
<bodyText confidence="0.999943">
operations would be very complex and uneasy
to employ. A first way to improve this search
is to keep the structure resulting of the near-
est neighbour computation as T3. The trans-
formation between 14 and T3 should be one of
the simplest (accordingly to the operations cost;
see section 3.3), and then the search would be
limited to the retrieval of a pair (71,T2) sharing
an equivalent transformation. However, this is
still time-consuming, and we are trying to de-
fine a general way to limit the search in such a
tree structure space, for example based on tree
indexing for efficiency (Daelemans et al., 1997).
</bodyText>
<sectionHeader confidence="0.994548" genericHeader="method">
5 First Results
</sectionHeader>
<bodyText confidence="0.99998624">
Because of the uncompleted development of
this approach, most experiments are still under
progress. So far they were run to find the clos-
est match of held-out corpus sentences using the
syntactic structure and the performance struc-
ture, for each of the distance metrics. We are
using both the &amp;quot;actual&amp;quot; and estimated perfor-
mance structures to quantify the effects of this
estimation. Cross-validation tests have been
chosen to validate our method.
These experiments are not all complete, but
an initial analysis of the results doesn&apos;t seem to
show many differences between the tree metrics
considered. We believe that this is due to the
small size of the corpus we are using. With only
around 300 sentences, most structures are very
different, so the majority of pairwise compar-
isons should be very distant. We are currently
running experiments where the tree structures
are generated at the phrase level. This strat-
egy implies to adapt the tree metrics to take
into consideration the location of the phrases in
the sentences (two similar structures should be
privileged if they have the same location in their
respective sentences).
</bodyText>
<sectionHeader confidence="0.998997" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999625">
We have presented a new prosody prediction
method. Its original aspect is to consider sen-
tences as tree structures. Tree similarity metrics
and analogy-based learning in a corpus of such
structures are used to predict the prosody of a
new sentence. Further experiments are needed
to validate this approach.
An additional development of our method
would be the introduction of focus labels. In
a dialogue context, some extra information can
refine the intonation. With the tree structures
that we are using, it is easy to introduce spe-
cial markers upon the nodes of the structure.
According to their nature and location, they
can indicate some focus either on a word, on a
phrase or on a whole sentence. With the adap-
tation of the tree metrics, the prediction process
is kept unchanged.
</bodyText>
<sectionHeader confidence="0.998428" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999778545454546">
J. Bachenko and E. Fitzpatrick. 1990. A compu-
tational grammar of discourse-neutral prosodic
phrasing in English. Comp. Ling., 16(3):155-170.
W. Daelemans, A. van den Bosch, and T. Weijters.
1997. IGTree: Using trees for compression and
classification in lazy learning algorithms. In Artif.
Intel. Review, volume 11, pages 407-423. Kluwer
Academic Publishers.
J. P. Gee and F. Grosjean. 1983. Performance struc-
tures: a psycholinguistic and linguistic appraisal.
Cognitive Psychology, 15:411-458.
U. Jensen, R. K. Moore, P. Dalsgaard, and B. Lind-
berg. 1994. Modelling intonation contours at
the phrase level using continuous density HMMs.
Comp. Speech and Lang., 8:247-260.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large anno-
tated corpus of English: the Penn Treebank.
Comp. Ling., 19.
M. Ostendorf, P. J. Price, and S. Shattuck-Hufnagel.
1995. The Boston University Radio News Corpus.
Technical Report ECS-95-001, Boston U.
V. Pirrelli and F. Yvon. 1999. The hidden dimen-
sion: a paradigmatic view of data-driven NLP. J.
of Exp. and Theo. Artif. Intel., 11(3):391-408.
K. Ross. 1995. Modeling of intonation for speech
synthesis. Ph.D. thesis, Col. of Eng., Boston U.
S. M. Selkow. 1977. The tree-to-tree editing prob-
lem. Inf. Processing Letters, 6(6):184-186.
K. Silverman, M. E. Beckman, J. Pitrelli, M. Osten-
dorf, C. W. Wightman, P. J. Price, J. B. Pier-
rhumbert, and J. Hirschberg. 1992. TOBI: A
standard for labelling English prosody. In Int.
Conf. on Spoken Lang. Processing, pages 867-870.
C. Traber, 1992. Talking machines: theories, mod-
els and designs, chapter FO generation with a
database of natural FO patterns and with a neural
network, pages 287-304.
R. A. Wagner and M. J. Fisher. 1974. The string-
to-string correction problem. J. of the Asso. for
Computing Machinery, 21(1):168-173.
K. Zhang. 1995. Algorithms for the constrained
editing distance between ordered labeled trees and
related problems. Pattern Reco., 28(3):463-474.
</reference>
<page confidence="0.998633">
90
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.820282">
<note confidence="0.975676">of CoNLL-2000 and LLL-2000, 87-90, Lisbon, Portugal, 2000.</note>
<title confidence="0.99793">Generating Synthetic Speech Prosody with Lazy in Tree Structures</title>
<author confidence="0.999414">Laurent Blin Laurent Miclet</author>
<affiliation confidence="0.999661">IRISA-ENSSAT IRISA-ENSSAT</affiliation>
<address confidence="0.997777">F-22305 Lannion, France F-22305 Lannion, France</address>
<email confidence="0.984914">blinOenssat.frmiclet@enssat.fr</email>
<abstract confidence="0.9902898">We present ongoing work on prosody prediction for speech synthesis. This approach considers sentences as tree structures and infers the prosody from a corpus of such structures using machine learning techniques. The prediction is achieved from the prosody of the closest sentence of the corpus through tree similarity measurements, using either the nearest neighbour algorithm or an analogy-based approach. We introduce two different tree structure representations, the tree similarity metrics considered, and then we discuss the different prediction methods. Experiments are currently under process to qualify this approach.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bachenko</author>
<author>E Fitzpatrick</author>
</authors>
<title>A computational grammar of discourse-neutral prosodic phrasing in English.</title>
<date>1990</date>
<pages>16--3</pages>
<publisher>Comp. Ling.,</publisher>
<contexts>
<context position="3696" citStr="Bachenko and Fitzpatrick, 1990" startWordPosition="584" endWordPosition="587"> clarity). 2.2 Performance Structure The performance structure used in our approach is a combination of syntactic and phonological informations. Its upper part is a binary tree where each node represents a break between the two parts of the sentence contained into the subtrees of the node. This binary structure defines a hierarchy: the closer to the root the node is, the more salient (or stronger) the break is. 87 to [TO] follow [VB] 1=wffl, Hennessy [NNP] be [VB] NP a [D [JJI- 31.&amp;quot;---S VP our system in a full synthesis context where no prosodic value is available). The currently used method (Bachenko and Fitzpatrick, 1990) provides rules to infer a default phrasing for a sentence. Not only the effects of this estimation will have to be quantified, but we plan to develop a more accurate solution to predict this structure accordingly to any corpus speaker characteristics. Figure 1: Syntactic structure for the sentence &amp;quot;Hennessy will be a hard act to follow&amp;quot;. (Syntactic labels: S: simple declarative clause, NP: noun phrase, VP: verb phrase. Part-of-speech labels: NNP: proper noun, MD: modal, VB: base form verb, DT: determiner, JJ: adjective, NN: singular noun, TO: special label for &amp;quot;to&amp;quot;). The lower part represents</context>
</contexts>
<marker>Bachenko, Fitzpatrick, 1990</marker>
<rawString>J. Bachenko and E. Fitzpatrick. 1990. A computational grammar of discourse-neutral prosodic phrasing in English. Comp. Ling., 16(3):155-170.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
<author>T Weijters</author>
</authors>
<title>IGTree: Using trees for compression and classification in lazy learning algorithms.</title>
<date>1997</date>
<journal>In Artif. Intel. Review,</journal>
<volume>11</volume>
<pages>407--423</pages>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Daelemans, van den Bosch, Weijters, 1997</marker>
<rawString>W. Daelemans, A. van den Bosch, and T. Weijters. 1997. IGTree: Using trees for compression and classification in lazy learning algorithms. In Artif. Intel. Review, volume 11, pages 407-423. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Gee</author>
<author>F Grosjean</author>
</authors>
<title>Performance structures: a psycholinguistic and linguistic appraisal.</title>
<date>1983</date>
<journal>Cognitive Psychology,</journal>
<pages>15--411</pages>
<contexts>
<context position="2286" citStr="Gee and Grosjean, 1983" startWordPosition="351" endWordPosition="354">orf et al., 1995). The prosodic information consists of ToBI labeling of accents and breaks (Silverman et al., 1992). The syntactic and part-of-speech informations were obtained from the part of the corpus processed in the Penn Treebank project (Marcus et al., 1993). We firstly describe the tree structures defined for this work, then present the tree metrics that we are using, and finally discuss how they are manipulated to achieve the prosody prediction. 2 Tree Structures So far we have considered two types of structures in this work: a simple syntactic structure and a performance structure (Gee and Grosjean, 1983). Their comparison in use should provide some interesting knowledge about the usefulness or the limitations of the elements of information included in each one. 2.1 Syntactic Structure The syntactic structure considered is built exclusively from the syntactic parsing of the given sentences. This parsing, with the relative syntactic tags, constitute the backbone of the structure. Below this structure lie the words of the sentence, with their part-of-speech tags. Additional levels of nodes can be added deeper in the structure to represent the syllables of each word, and the phonemes of each syll</context>
</contexts>
<marker>Gee, Grosjean, 1983</marker>
<rawString>J. P. Gee and F. Grosjean. 1983. Performance structures: a psycholinguistic and linguistic appraisal. Cognitive Psychology, 15:411-458.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Jensen</author>
<author>R K Moore</author>
<author>P Dalsgaard</author>
<author>B Lindberg</author>
</authors>
<title>Modelling intonation contours at the phrase level using continuous density HMMs.</title>
<date>1994</date>
<journal>Comp. Speech and Lang.,</journal>
<pages>8--247</pages>
<contexts>
<context position="1185" citStr="Jensen et al., 1994" startWordPosition="165" endWordPosition="168">est sentence of the corpus through tree similarity measurements, using either the nearest neighbour algorithm or an analogy-based approach. We introduce two different tree structure representations, the tree similarity metrics considered, and then we discuss the different prediction methods. Experiments are currently under process to qualify this approach. 1 Introduction Natural prosody production remains a problem in speech synthesis systems. Several automatic prediction methods have already been tried for this, including decision trees (Ross, 1995), neural networks (Traber, 1992), and HMMs (Jensen et al., 1994). The original aspect of our prediction approach is a tree structure representation of sentences, and the use of tree similarity measurements to achieve the prosody prediction. We think that reasoning on a whole structure rather than on local features of a sentence should better reflect the many relations influencing the prosody. This approach is an attempt to achieve such a goal. The data used in this work is a part of the Boston University Radio (WBUR) News Corpus (Ostendorf et al., 1995). The prosodic information consists of ToBI labeling of accents and breaks (Silverman et al., 1992). The </context>
</contexts>
<marker>Jensen, Moore, Dalsgaard, Lindberg, 1994</marker>
<rawString>U. Jensen, R. K. Moore, P. Dalsgaard, and B. Lindberg. 1994. Modelling intonation contours at the phrase level using continuous density HMMs. Comp. Speech and Lang., 8:247-260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Comp. Ling.,</journal>
<volume>19</volume>
<contexts>
<context position="1929" citStr="Marcus et al., 1993" startWordPosition="292" endWordPosition="295">rity measurements to achieve the prosody prediction. We think that reasoning on a whole structure rather than on local features of a sentence should better reflect the many relations influencing the prosody. This approach is an attempt to achieve such a goal. The data used in this work is a part of the Boston University Radio (WBUR) News Corpus (Ostendorf et al., 1995). The prosodic information consists of ToBI labeling of accents and breaks (Silverman et al., 1992). The syntactic and part-of-speech informations were obtained from the part of the corpus processed in the Penn Treebank project (Marcus et al., 1993). We firstly describe the tree structures defined for this work, then present the tree metrics that we are using, and finally discuss how they are manipulated to achieve the prosody prediction. 2 Tree Structures So far we have considered two types of structures in this work: a simple syntactic structure and a performance structure (Gee and Grosjean, 1983). Their comparison in use should provide some interesting knowledge about the usefulness or the limitations of the elements of information included in each one. 2.1 Syntactic Structure The syntactic structure considered is built exclusively fr</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Comp. Ling., 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Ostendorf</author>
<author>P J Price</author>
<author>S Shattuck-Hufnagel</author>
</authors>
<date>1995</date>
<tech>Technical Report ECS-95-001,</tech>
<institution>The Boston University Radio News Corpus.</institution>
<location>Boston U.</location>
<contexts>
<context position="1680" citStr="Ostendorf et al., 1995" startWordPosition="253" endWordPosition="256">ready been tried for this, including decision trees (Ross, 1995), neural networks (Traber, 1992), and HMMs (Jensen et al., 1994). The original aspect of our prediction approach is a tree structure representation of sentences, and the use of tree similarity measurements to achieve the prosody prediction. We think that reasoning on a whole structure rather than on local features of a sentence should better reflect the many relations influencing the prosody. This approach is an attempt to achieve such a goal. The data used in this work is a part of the Boston University Radio (WBUR) News Corpus (Ostendorf et al., 1995). The prosodic information consists of ToBI labeling of accents and breaks (Silverman et al., 1992). The syntactic and part-of-speech informations were obtained from the part of the corpus processed in the Penn Treebank project (Marcus et al., 1993). We firstly describe the tree structures defined for this work, then present the tree metrics that we are using, and finally discuss how they are manipulated to achieve the prosody prediction. 2 Tree Structures So far we have considered two types of structures in this work: a simple syntactic structure and a performance structure (Gee and Grosjean,</context>
</contexts>
<marker>Ostendorf, Price, Shattuck-Hufnagel, 1995</marker>
<rawString>M. Ostendorf, P. J. Price, and S. Shattuck-Hufnagel. 1995. The Boston University Radio News Corpus. Technical Report ECS-95-001, Boston U.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Pirrelli</author>
<author>F Yvon</author>
</authors>
<title>The hidden dimension: a paradigmatic view of data-driven NLP.</title>
<date>1999</date>
<journal>J. of Exp. and Theo. Artif. Intel.,</journal>
<pages>11--3</pages>
<marker>Pirrelli, Yvon, 1999</marker>
<rawString>V. Pirrelli and F. Yvon. 1999. The hidden dimension: a paradigmatic view of data-driven NLP. J. of Exp. and Theo. Artif. Intel., 11(3):391-408.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ross</author>
</authors>
<title>Modeling of intonation for speech synthesis.</title>
<date>1995</date>
<journal>Inf. Processing Letters,</journal>
<tech>Ph.D. thesis,</tech>
<pages>6--6</pages>
<institution>Col. of Eng.,</institution>
<location>Boston</location>
<contexts>
<context position="1121" citStr="Ross, 1995" startWordPosition="156" endWordPosition="157">The prediction is achieved from the prosody of the closest sentence of the corpus through tree similarity measurements, using either the nearest neighbour algorithm or an analogy-based approach. We introduce two different tree structure representations, the tree similarity metrics considered, and then we discuss the different prediction methods. Experiments are currently under process to qualify this approach. 1 Introduction Natural prosody production remains a problem in speech synthesis systems. Several automatic prediction methods have already been tried for this, including decision trees (Ross, 1995), neural networks (Traber, 1992), and HMMs (Jensen et al., 1994). The original aspect of our prediction approach is a tree structure representation of sentences, and the use of tree similarity measurements to achieve the prosody prediction. We think that reasoning on a whole structure rather than on local features of a sentence should better reflect the many relations influencing the prosody. This approach is an attempt to achieve such a goal. The data used in this work is a part of the Boston University Radio (WBUR) News Corpus (Ostendorf et al., 1995). The prosodic information consists of To</context>
</contexts>
<marker>Ross, 1995</marker>
<rawString>K. Ross. 1995. Modeling of intonation for speech synthesis. Ph.D. thesis, Col. of Eng., Boston U. S. M. Selkow. 1977. The tree-to-tree editing problem. Inf. Processing Letters, 6(6):184-186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Silverman</author>
<author>M E Beckman</author>
<author>J Pitrelli</author>
<author>M Ostendorf</author>
<author>C W Wightman</author>
<author>P J Price</author>
<author>J B Pierrhumbert</author>
<author>J Hirschberg</author>
</authors>
<title>TOBI: A standard for labelling English prosody.</title>
<date>1992</date>
<booktitle>In Int. Conf. on Spoken Lang. Processing,</booktitle>
<pages>867--870</pages>
<contexts>
<context position="1779" citStr="Silverman et al., 1992" startWordPosition="269" endWordPosition="272">nd HMMs (Jensen et al., 1994). The original aspect of our prediction approach is a tree structure representation of sentences, and the use of tree similarity measurements to achieve the prosody prediction. We think that reasoning on a whole structure rather than on local features of a sentence should better reflect the many relations influencing the prosody. This approach is an attempt to achieve such a goal. The data used in this work is a part of the Boston University Radio (WBUR) News Corpus (Ostendorf et al., 1995). The prosodic information consists of ToBI labeling of accents and breaks (Silverman et al., 1992). The syntactic and part-of-speech informations were obtained from the part of the corpus processed in the Penn Treebank project (Marcus et al., 1993). We firstly describe the tree structures defined for this work, then present the tree metrics that we are using, and finally discuss how they are manipulated to achieve the prosody prediction. 2 Tree Structures So far we have considered two types of structures in this work: a simple syntactic structure and a performance structure (Gee and Grosjean, 1983). Their comparison in use should provide some interesting knowledge about the usefulness or t</context>
</contexts>
<marker>Silverman, Beckman, Pitrelli, Ostendorf, Wightman, Price, Pierrhumbert, Hirschberg, 1992</marker>
<rawString>K. Silverman, M. E. Beckman, J. Pitrelli, M. Ostendorf, C. W. Wightman, P. J. Price, J. B. Pierrhumbert, and J. Hirschberg. 1992. TOBI: A standard for labelling English prosody. In Int. Conf. on Spoken Lang. Processing, pages 867-870.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Traber</author>
</authors>
<title>Talking machines: theories, models and designs, chapter FO generation with a database of natural FO patterns and with a neural network,</title>
<date>1992</date>
<pages>287--304</pages>
<contexts>
<context position="1153" citStr="Traber, 1992" startWordPosition="161" endWordPosition="162">m the prosody of the closest sentence of the corpus through tree similarity measurements, using either the nearest neighbour algorithm or an analogy-based approach. We introduce two different tree structure representations, the tree similarity metrics considered, and then we discuss the different prediction methods. Experiments are currently under process to qualify this approach. 1 Introduction Natural prosody production remains a problem in speech synthesis systems. Several automatic prediction methods have already been tried for this, including decision trees (Ross, 1995), neural networks (Traber, 1992), and HMMs (Jensen et al., 1994). The original aspect of our prediction approach is a tree structure representation of sentences, and the use of tree similarity measurements to achieve the prosody prediction. We think that reasoning on a whole structure rather than on local features of a sentence should better reflect the many relations influencing the prosody. This approach is an attempt to achieve such a goal. The data used in this work is a part of the Boston University Radio (WBUR) News Corpus (Ostendorf et al., 1995). The prosodic information consists of ToBI labeling of accents and break</context>
</contexts>
<marker>Traber, 1992</marker>
<rawString>C. Traber, 1992. Talking machines: theories, models and designs, chapter FO generation with a database of natural FO patterns and with a neural network, pages 287-304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Wagner</author>
<author>M J Fisher</author>
</authors>
<title>The stringto-string correction problem.</title>
<date>1974</date>
<journal>J. of the Asso. for Computing Machinery,</journal>
<pages>21--1</pages>
<contexts>
<context position="5690" citStr="Wagner and Fisher, 1974" startWordPosition="923" endWordPosition="926">ntactic and part-of-speech labels have the same meaning as in Figure 1. B1, B2 and B3 are the break-related nodes. Unlike the syntactic structure, a first step of prediction is done in the performance structure with the break values. This prosody information is known for the sentences in the corpus, but has to be predicted for new ones (to put 3 Tree Metrics Now that the tree structures are defined, we need the tools to predict the prosody. We have considered two similarity metrics to calculate the &amp;quot;distance&amp;quot; between two tree structures, inspired from the Wagner and Fisher&apos;s editing distance (Wagner and Fisher, 1974). 3.1 Principles Introducing a small set of elementary transformation operators upon trees (insertion or deletion of a node, substitution of a node by another one) it is possible to determine a set of specific operation sequences that transform any given tree into another one. Specifying costs for each elementary operation (possibly a function of the node values) allows the evaluation of a whole transformation cost by adding the operation costs in the sequence. Therefore the tree distance can be defined as the cost of the sequence minimizing this sum. 3.2 Considered Metrics Many metrics can be</context>
</contexts>
<marker>Wagner, Fisher, 1974</marker>
<rawString>R. A. Wagner and M. J. Fisher. 1974. The stringto-string correction problem. J. of the Asso. for Computing Machinery, 21(1):168-173.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Zhang</author>
</authors>
<title>Algorithms for the constrained editing distance between ordered labeled trees and related problems.</title>
<date>1995</date>
<journal>Pattern Reco.,</journal>
<pages>28--3</pages>
<contexts>
<context position="6895" citStr="Zhang, 1995" startWordPosition="1125" endWordPosition="1126"> can be defined from this principle. The differences come from the application conditions which can be set on the operators. In our experiments, two metrics are tested. They both preserve the order of the nodes in the trees, an essential condition in our application. The first one (Selkow, 1977) allows only substitutions between nodes at the same depth level in the trees. Moreover, the insertion or deletion of a node involves respectively the insertion or deletion of the whole subtree depending of the node. These strict conditions should be able to locate very close structures. The other one (Zhang, 1995) allows the substitutions of nodes whatever theirs locations are inside the structures. It also allows the insertion or deletion of lonely nodes inside the structures. Compared to the previous metric, these less rigorous stipulations should not only retrieve the TP to follow [VB] 88 very close structures, but also other ones which wouldn&apos;t have been found. Moreover, these two algorithms also provide a mapping between the nodes of the trees. This mapping illustrates the operations which led to the final distance value: the parts of the trees which were inserted or deleted, and the ones which we</context>
</contexts>
<marker>Zhang, 1995</marker>
<rawString>K. Zhang. 1995. Algorithms for the constrained editing distance between ordered labeled trees and related problems. Pattern Reco., 28(3):463-474.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>