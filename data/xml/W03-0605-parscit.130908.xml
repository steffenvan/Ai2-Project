<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000039">
<title confidence="0.992379">
An Architecture for Word Learning using Bidirectional Multimodal
Structural Alignment
</title>
<author confidence="0.911352">
Keith Bonawitz Anthony Kim Seth Tardiff
</author>
<email confidence="0.910171">
bonawitz@mit.edu tkim42@mit.edu stardiff@mit.edu
</email>
<note confidence="0.552349666666667">
MIT Artificial Intelligence Lab
545 Technology Square
Cambridge, MA 02139
</note>
<sectionHeader confidence="0.981433" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999813047619048">
Learning of new words is assisted by contex-
tual information. This context can come in
several forms, including observations in non-
linguistic semantic domains, as well as the lin-
guistic context in which the new word was pre-
sented. We outline a general architecture for
word learning, in which structural alignment
coordinates this contextual information in or-
der to restrict the possible interpretations of un-
known words. We identify spatial relations as
an applicable semantic domain, and describe a
system-in-progress for implementing the gen-
eral architecture using video sequences as our
non-linguistic input. For example, when the
complete system is presented with “The bird
dove to the rock,” with a video sequence of a
bird flying from a tree to a rock, and with the
meanings for all the words except the preposi-
tion “to,” the system will register the unknown
“to” with the corresponding aspect of the bird’s
trajectory.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995099">
Multimodal word learning can be viewed as a problem in
which inputs are presented concurrently in both the lin-
guistic domain and at least one non-linguistic semantic
domain. It is the responsibility of the word learner to (1)
infer the correspondence of each word to some fragment
of the semantic domain, and (2) refine the model of the
word based on this correspondence (by generalizing the
word semantics, for example).
In this paper1, we propose a system aimed at the first
half of the problem: inferring a correspondence between
</bodyText>
<footnote confidence="0.920557">
1This research is supported, in part, by the National Science
Foundation, Award Number IIS-0218861.
</footnote>
<bodyText confidence="0.999651">
words and non-linguistic semantic domain fragments. In
particular, we are interested in using the context of the
word’s introduction to limit possible interpretations. As-
suming linguistic inputs are syntactic multiword utter-
ances (e.g. phrases and sentences), this context includes
the semantic and syntactic relationship of the new word to
other words in the linguistic input. In a multimodal learn-
ing environment, the context also includes input observed
in the non-linguistic domain. Our system is designed to
coordinate all of these contextual clues in order to restrict
the set of possible interpretations of the new word. By
leveraging previously learned words to enable the learn-
ing of new words, we create a bootstrapping system for
word learning.
In Section 2, we outline a general architecture based
on symbolic structural alignment (Gentner and Markman,
1997) for solving the stated problem. In this outline,
we identify necessary subsystems and requirements the
system must satisfy. In Section 3, we identify the vi-
sual domain of spatial relations as a potential semantic
domain, and in Sections 4–6 we describe a system-in-
progress which instantiates the general architecture for
this semantic domain.
</bodyText>
<sectionHeader confidence="0.989961" genericHeader="introduction">
2 General Architecture
</sectionHeader>
<bodyText confidence="0.999837076923077">
We propose an architecture to answer the following ques-
tion: assuming that a new word is embedded in a phrase
with other previously acquired words, how can we exploit
this linguistic context to focus on the fragments of non-
linguistic input most likely to correspond with the new
word?
The semantic principle of compositionality states that
the meaning of any expression (such as a phrase)
is a function of the meaning of its sub-expressions,
where the particular function is determined by the
method of composition. For example, the expression
“The quick fox jumped over the log” can be consid-
ered a composition of the sub-expressions “The quick
</bodyText>
<figure confidence="0.701110142857143">
X
/
A A
/ \ / \
2 C B 1
/ \  |/ \
E F D E F
</figure>
<figureCaption confidence="0.7068678">
Figure 1: Structural alignment between these two struc-
tures infers the correspondences CH1 and 2HB(D).
Structural alignment between semantic representations
will bring unknown words into correspondence with their
probable semantics.
</figureCaption>
<bodyText confidence="0.999957178571428">
fox” and “over the log”, where syntactic composi-
tion with “jumped” is the method of composition. In
other words, the semantics of this sentence can be ex-
pressed: jumped(SEMANTICS(“The quick fox”), SE-
MANTICS(“over the log”)). Recursive application of this
principle reveals that the semantic value of an expression
is a structured representation.
Intuitively, then, we can approach our bootstrapping
problem by structural alignment (Gentner and Markman,
1997). Structural alignment is a process in which corre-
sponding elements in two structured representations are
identified by matching. Correspondence between non-
matching elements is then implied by the structural con-
straints of the representations.
For example, in Figure 1, structural alignment first
matches A, E, and F between the two representations.
Then, based on structural constraints, 1 is inferred to cor-
respond with C, and 2 with B(D). In our architecture,
structural alignment of the semantics of known words
(and linguistic constituents formed thereof) with seman-
tic structures observed in the non-linguistic domain will
cause an alignment of unknown words with probable cor-
responding semantic fragments, thereby achieving our
word learning goal of exploiting linguistic context to fo-
cus on fragments of the semantic input.
The remainder of this section describes the representa-
tions and methods required by a system seeking to imple-
ment this general architecture.
</bodyText>
<subsectionHeader confidence="0.994148">
2.1 Semantic Representation
</subsectionHeader>
<bodyText confidence="0.990486">
In order to perform structural alignment, the representa-
tion for the semantic domain must have several key prop-
erties:
</bodyText>
<listItem confidence="0.9967725">
• The domain must be a structural representation and
it must be symbolic, in order to allow alignment of
symbols.
• For inferences made from structural alignment to be
valid, the representation must obey the principle of
compositionality.
• The representation should contain orthogonal ele-
ments (i.e. the same piece of semantics is not
encoded into multiple symbols) so that there are
canonical ways of expressing particular meanings.
• Finally, the semantic representation must be lexical-
ized, implying that the semantics of any linguistic
phrase can be cleanly divided amongst the phrase’s
constituent words. Each word should get a single
connected semantic structure that does not share se-
mantic symbols with any other word.
</listItem>
<subsectionHeader confidence="0.998941">
2.2 Semantic Processing
</subsectionHeader>
<bodyText confidence="0.999972">
It is likely that the actual non-linguistic input modality
will not be an appropriate structured symbolic represen-
tation. For example, the visual, aural, and kinesthetic
modalities are non-symbolic. In any system dealing with
such an input modality, it will be necessary to have mod-
ules that extract structured symbolic representations from
the unstructured input.
</bodyText>
<subsectionHeader confidence="0.998289">
2.3 Linguistic Processing
</subsectionHeader>
<bodyText confidence="0.999757296296296">
One challenge in performing structural alignment against
language input is that the structured semantic representa-
tion of the linguistic input is implicit rather than explicit.
Therefore, we need methods for parsing and an appropri-
ate grammar. The grammar and parsing algorithms we
choose must support several non-standard features.
First, we expect to encounter word meanings which are
unknown, so our selected techniques must support gaps in
the parse. We also require a reversible grammar, so that,
when presented with the meaning of an entire expression
and the meaning of some of its subexpressions, we can
infer the meaning of the remaining subexpressions.2
Although it may not be required, parsing techniques
that use partial structural alignment are preferred. Words
and phrases have many possible interpretations and this
problem is exacerbated by unknown words in the linguis-
tic input. Since targets for the parse are available in the
semantic input domain, use of these targets to guide the
search through the space of possible linguistic interpreta-
tions is advantageous. Increasing structural alignment be-
tween the parsed semantics and the input semantics could
be such a guiding heuristic. As a side effect of using
structural alignment as a parsing heuristic, we should ex-
pect the parser to manipulate partial semantic and syntac-
tic structures throughout the parsing process, as opposed
to generating semantics from a completed syntactic parse
tree after parsing is completed.
</bodyText>
<footnote confidence="0.746178428571429">
2Mathematically, this is equivalent to saying that there will
be cases where we know a, f, and x in the equality a = f(x, y),
and we want to be able to infer the value of y. In order to do
so, we must be able to compute the functional inverse of f with
respect to y. That is, we want the function f−&apos;such that y =
f−&apos;
, (x, a).
</footnote>
<figureCaption confidence="0.654933">
Figure 2: General system structure of our proposed im-
plementation. Our learning-enabling structure is based
</figureCaption>
<listItem confidence="0.72439">
on a semantic representation (Section 4) which is ob-
tained by translating video inputs (Section 6). We then
use a bidirectional search process to parse the linguistic
input and to structurally align linguistic semantics with
the non-linguistic semantics. (Section 5).
</listItem>
<subsectionHeader confidence="0.995522">
2.4 Structural Alignment
</subsectionHeader>
<bodyText confidence="0.999873375">
Gentner and Markman (1997) describe the requisite com-
ponents of a structural alignment system as (1) methods
for matching structural atoms, (2) methods for identify-
ing sets of compatible atom matches (for example, ruling
out cases in which two atoms in one structure map to the
same atom in another structure), and (3) methods using
atom matches to guide the matching of large portions of
structure.
</bodyText>
<sectionHeader confidence="0.751362" genericHeader="method">
3 Implementation with Visual Domain of
</sectionHeader>
<subsectionHeader confidence="0.686212">
Spatial Relations
</subsectionHeader>
<bodyText confidence="0.999971230769231">
In order to validate our general architecture, we outline a
system-in-progress which instantiates the architecture for
the particular semantic domain of spatial relations. The
domain of spatial relations captures the relative position-
ing, orientation, and movement of objects in space. Ex-
amples of sentences capturing spatial semantics include
“The boy threw the ball into the box on the table”, “The
path went from the tree to the lakeside”, and “The sign
points to the door”.
The following sections describe the methods and rep-
resentations we have chosen to satisfy the requirements
outlined in Section 2. Figure 2 shows how the system is
designed and how the rest of this paper is organized.
</bodyText>
<table confidence="0.982996941176471">
���������������������� |THING Bird
���
���|THING Pole
���
���PLACE At
��
��PATH−ELEMENT From
��
��
���
���|THING Deborah
���
���PLACE At
��
��PATH−ELEMENT To
�� PATH
EVENT Go
</table>
<figureCaption confidence="0.971973">
Figure 3: Lexical-Conceptual Semantics is our semantic
domain representation. This structure is the LCS model
of “The bird flew from the pole to Deborah.”
</figureCaption>
<sectionHeader confidence="0.996656" genericHeader="method">
4 Lexical-Conceptual Semantics
</sectionHeader>
<bodyText confidence="0.998316272727273">
We use Lexical-Conceptual Semantics (LCS) (Jackend-
off, 1983) as our semantic representation. LCS is a cog-
nitive representation that focuses on trajectories and spa-
tial relations. Unlike other representations such as Log-
ical Form (LF) and Conceptual Dependency (CD), LCS
delineates notions of PATHs and PLACEs. LCS is more
formally outlined in (Jackendoff, 1983) and is compared
to other semantic representations in (Bender, 2001).
The following productions yield a simplified portion of
the LCS space. For a complete description, refer to (Jack-
endoff, 1983).
</bodyText>
<listItem confidence="0.999506153846154">
• [THING]
• [PLACE] ←− PLACE-FUNC([THING])
where PLACE-FUNC E {AT, ABOVE, BELOW,
ON, IN, etc.}
• [PATH] ←− PATH(PATH-ELEMENT, PATH-
ELEMENT, · · ·)
• [PATH-ELEMENT] ←− PATH-FUNC([PLACE])
where PATH-FUNC E {TO, FROM, TOWARD,
AWAY-FROM, VIA, etc.}
• [EVENT] ←− GO([THING],[PATH])
• [STATE] ←− BE([THING],[PLACE])
• [CAUSE] ←− CAUSE([THING],[EVENT])
• [CAUSE] ←− CAUSE([EVENT],[EVENT])
</listItem>
<bodyText confidence="0.999291142857143">
Using LCS, the trajectory expressed in the sentence
“The bird flew from the pole to Deborah,” is represented
as in Figure 3
Lexical-Conceptual Semantics focuses on spatial rela-
tions in the physical world. However, it is easily exten-
sible to other domains, such as the temporal and posses-
sive domains (Jackendoff, 1983; Dorr, 1992). Research
</bodyText>
<figure confidence="0.99270876">
§5 Parsing and §6 Extract Semantic
Structural Alignment Structures from Vision
|THING Bird
���
���
���|THING Pole
���
���PLACE At
��
��
��PATH−ELEMENT From
��
���
��
���|THING Deborah
����
��
���PLACE At
��
��PATH−ELEMENT To
�� PATH
EVENT Go
§4 ��������������������������
“The bird flew from the
pole to Deborah”
</figure>
<bodyText confidence="0.9992854">
focusing on using LCS in the abstract domain of social
politics is also ongoing in our lab. Furthermore, it seems
that much of language is spatial in nature. For example,
there is significant psychological evidence that humans
use spatial relations to talk about abstract domains such
as time (Boroditsky, 2000). As a consequence, we believe
that techniques for learning Lexical-Conceptual Seman-
tics for words, developed here using the concrete spatial
relations domain, will be extendible to many other do-
mains.
</bodyText>
<sectionHeader confidence="0.970937" genericHeader="method">
5 Language Parsing and Structural
Alignment
</sectionHeader>
<bodyText confidence="0.999967285714286">
This section will describe the methods used to simultane-
ously parse linguistic input strings and align the resultant
semantic structures with those from vision. The primary
architecture of the system is a constraint propagation net-
work using grammatical rules as constraints. A custom
constraint network topology is generated for each linguis-
tic input string using a bidirectional search algorithm.
</bodyText>
<subsectionHeader confidence="0.994969">
5.1 Parsing/Alignment as Constraint Propagation
</subsectionHeader>
<bodyText confidence="0.999984421052632">
The parsing and structural alignment system may be
viewed as a single large constraint, as in Figure 4. This
constraint has two inputs: on one side, it takes a set
of semantic representations originating from the vision
processor. On the other side, it takes a linguistic input
string, together with possible meanings for each word
in the string, as determined by a lexicon (treating un-
known words as having any possible meaning). As out-
put, the constraint eliminates, in each input set, all mean-
ings which do not lead to a successful structurally aligned
parse.
In order to achieve such a complicated constraint, it
is useful to decompose the constraint into a network of
simpler constraints, each working over a local domain of
only a few constituents rather than over the domain of an
entire sentence, as in Figure 5. We can then base these
subconstraints on grammatical rules over a fixed number
of constituents, and trust the composed network to handle
the complete sentence.
</bodyText>
<subsectionHeader confidence="0.987893">
5.2 Grammatical Framework for Constraints
</subsectionHeader>
<bodyText confidence="0.992289088235294">
The grammar framework chosen for our system is Com-
binatorial Categorial Grammar (CCG) (Steedman, 2000).
CCG has many advantages in a system like ours. First,
there are only a handful of rules for combining con-
stituents, and these rules are explicit and well defined.
These qualities facilitate the implementation of con-
straints. In addition, CCG is adept at parsing around
missing information, because it was designed to handle
Figure 4: The parsing and structural alignment system
functions as a constraint on linguistic and visual inter-
pretations, requiring that expressions follow grammatical
rules and that they align with the semantic domain input.
This example shows the system presented with the sen-
tence “The dove flies,” and with a corresponding concep-
tual structure (from vision). In this situation, all the words
were known, so the system will simply eliminate the in-
terpretations of “dove” as a GO and flies as THINGs. If
the word “dove” had not been known, the system would
still select the verb form of flies (by alignment), which
brings “dove” into alignment with the appropriate frag-
ment of semantic structure (the THING frame).
linguistic phenomena such as parasitic-gapping3. The
ability to gracefully handle incomplete phrases is crucial
in our system, because it enables us to parse around un-
known words.
In CCG, syntactic categories can either be atomic el-
ements, or functors of those elements. The atomic ele-
ments are usually {S, N, NP} corresponding to Sentence,
Noun, and Noun Phrase. Syntactic category functors are
expressed in argument-rightmost curried notation, using
/ or \ to indicate whether the argument is expected to the
left or right, respectively. Thus NP/N indicates a NP re-
quiring a N to the right (and is therefore the syntactic
category of a determiner), while (S\NP)/NP indicates an
</bodyText>
<footnote confidence="0.936922833333333">
3An example of a sentence with parasitic gapping is “John
hates and Mary loves the movie,” where both verbs share the
same object. CCG handles this by treating “John hates” and
“Mary loves” as constituents, which can then be conjoined by
“and” into a single “John hates and Mary loves” constituent
(traditional grammars are unable to recognize “John hates” as
</footnote>
<figure confidence="0.987469893617021">
a constituent.)
����������
S :
�
���� �
�����
��� PATH
GO
|THING Dove−Bird
�
�����
���� �
{
f
“The”
{ �
NP/N : λx.x
“dove”
N : |THING Dove−Bird
S\NP : λx.
“flies”
N : |THING Fly−Insect
S\NP : λx.
�
�����
���� �
i
x ���
PATH
GO fly
��������
x ���
PATH
GO Dive
��������
Parsing &amp; Alignment System
“The” “dove” “flies”
Alignment
Parsing &amp;
System
Lexical Conceptual Semantics (from Vision)
Grammar Rule
Constraint
“The dove”
“The dove flies”
Grammar Rule
Constraint
</figure>
<figureCaption confidence="0.998893">
Figure 5: The parsing and alignment constraint in Fig-
</figureCaption>
<bodyText confidence="0.953768071428571">
ure 4 is actually implemented as a network of simpler
constraints, as shown here. Each constraint implements a
grammatical rule, as shown in Figure 6. The topology of
the constraint network is dependent on the particular lin-
guistic string, and is constructed by bidirectional search,
as described in Section 5.3.
S requiring one NP to the left and one to the right (this
is the category of a monotransitive verb). For semantics,
the notation is extended to X:f, indicating semantic cat-
egory X with lambda calculus semantics f. These ele-
ments combine using a few simple productions, such as
the following functional application rules4:
X/Y:f Y:a ⇒ X:fa (&gt; forward application)
Y:a X\Y:f ⇒ X:fa (&lt; backward application)
</bodyText>
<subsectionHeader confidence="0.998229">
5.3 Constructing the Constraint Network
</subsectionHeader>
<bodyText confidence="0.999810888888889">
The constraints we use are parse rules; therefore our con-
straint network topology embodies a parse tree for any
sentence it can handle. Since our inputs do not include
the parse tree, we must consider how to generate an ap-
propriate constraint network topology.
One option is to use the same network topology to
handle all sentences of the same length. Such a net-
work would have to contain every possible parse tree,
and thus would essentially result in an exhaustive search
of the parse space. A better solution would be to avoid
the exhaustive search by constructing a custom constraint
topology for each sentence, using standard heuristic parse
techniques. The drawback to this approach is that we
are not interested in finding just any potential parse of
a phrase/sentence, nor even the most statistically proba-
ble parse. Since our intent is to perform structural align-
ment with input from the non-linguistic domain, our goal
in parsing is to find the semantic parse structure which
</bodyText>
<footnote confidence="0.913174">
4For a full description and analysis of CCG, see (Steedman,
2000)
</footnote>
<figureCaption confidence="0.648803">
Figure 6: This figure shows one of the CCG Rules, For-
ward Functional Application, being treated as a constraint
in a constraint propagation network. Any one of the three
inputs can be left unspecified, and the constraint can com-
pletely determine the value based on the other two inputs.
</figureCaption>
<bodyText confidence="0.996537125">
aligns best with the semantic structure input from the
non-linguistic domain. It follows that we should use the
non-linguistic input to guide our search.
Our system applies bidirectional search to the
parse/alignment problem. In contrast to traditional search
techniques, bidirectional search treats both the “source”
and “goal” symmetrically; the search-space is traversed
both forward from the source and backward from the
goal. The search processes operating in each direction
interact with each other whenever their paths reach the
same state in the search-space. This interaction provides
hints for quickly completing the remainder of the search.
For example, if the forward and backward paths reach
the same search-state, then the forward searcher quickly
reaches the goal by tracing the backward-path.
The specific style of bidirectional search we are inves-
tigating is based on Streams and Counterstreams (Ull-
man, 1996), in which forward and backward search paths
interact with each other by means of primed pathways.
For each transition, two priming values are maintained:
a forward priming and backward priming. Primings are
used when a decision must be made between several pos-
sible transitions that could extend a search path; those
transitions that have a higher priming (using the for-
</bodyText>
<figure confidence="0.9784705">
“The boy went to the lake”
|THING Boy
���
���
���|THING Lake
���
���PLACE At
��
��PATH−ELEMENT To
�� PATH
EVENT Go
CCG Constraint
Backward Application (&lt;)
Y:a X\Y:f ⇒ Y:fa
S : �����������������
“The boy” ����������������� “went to the lake”
NP : |THING Boy x
S\NP : λx. ���
���
���|THING Lake
���
���PLACE At
��
��PATH−ELEMENT To
�� PATH
EVENT Go
</figure>
<bodyText confidence="0.999590444444444">
ward priming for forward searches, backward priming for
backward searchers) are preferred for expansion. Transi-
tion primings in a particular direction (either forward or
backward) are increased whenever a search path traverses
the transition in the opposite direction. The net influence
of the primings is that transitions previously traversed in
one direction are more likely to be explored in the op-
posite direction, if the opportunity arises. By extension,
primings provide clues for finding a path from any state
to the target state.
The Streams and Counterstreams approach to bidirec-
tional search facilitates incorporation of other types of
context. For example, some situational context can be
captured by allowing primings from previous parses of
recent sentences to influence the current parse. Also, sta-
tistical cues such as Lexical Attraction (Yuret, 1999) can
be integrated into the system by using heuristics to bias
primings.
</bodyText>
<subsectionHeader confidence="0.989625">
5.4 Structural Alignment
</subsectionHeader>
<bodyText confidence="0.9999769">
The three components of structural alignment specified
in Section 2.4 (atomic alignment, identification of com-
patible match sets, and structurally implied matches) are
woven into the bidirectional search construction of the
constraint network topology. When a constraint net-
work fragment is constructed which bridges between a
small portion of the linguistic input and the non-linguistic
semantics, this “atomic alignment” primes the bidirec-
tional search to be more likely to repeat this match while
constructing larger constraint network fragments; hence
atomic alignment leads to larger structural alignment.
The constraints in the constraint network ensure that all
active atomic alignments are compatible. Finally, when
the constraint network bridges large portions of the lin-
guistic and non-linguistic inputs, the non-linguistic se-
mantic structure gets partitioned across the words in the
linguistic input by the grammatical constraints. This
completes the structural alignment by bringing unknown
words into correspondence with their probable seman-
tics.
</bodyText>
<subsectionHeader confidence="0.998533">
5.5 Handling Uncertainty
</subsectionHeader>
<bodyText confidence="0.99976828">
Throughout this discussion, we have considered words as
being either completely learned or completely unlearned.
Clearly, though, there is much middle ground, including
words whose meanings are still ambiguous among sev-
eral options, as well as words for which some meanings
have been well acquired, while other valid meanings have
yet to be learned. How can our system handle this degree-
of-acquisition continuum?
Let us consider what we can expect from the meaning-
refinement module. First, it should be able to report a set
of witnessed possible meanings for each word, together
with a correctness strength for each interpretation. This
would be based on how regularly that interpretation has
been witnessed. Furthermore, the module should be able
to report the likelihood that the word still has unwitnessed
interpretations; for initial occurrences of a word, this like-
lihood would be quite high, but with more exposure to the
word, this likelihood would fall off.5
Returning to our system, we can now treat each word
as having a set of known meanings together with a wild-
card unknown meaning. When using bidirectional search
to construct the constraint network topology, we bias the
primings of transitions which reduce a word’s potential
meaning set, using the likelihood estimates given by the
meaning-refinement module.
</bodyText>
<sectionHeader confidence="0.942064" genericHeader="method">
6 Lexical-Conceptual Structures from
Video
</sectionHeader>
<bodyText confidence="0.9999709">
The proposed system includes a vision component that is
responsible for converting pixel data from a video input
into the semantic structure described in Section 4. This
vision system is an implementation of the ideas presented
by John Bender (2001). Following Bender’s prescrip-
tions, the vision system does not perform object recog-
nition. Instead, the goal of the system is to analyze the
different paths and places that are present in a scene and,
by relating these paths and places to one another, to con-
struct an LCS representation of the actions.
</bodyText>
<subsectionHeader confidence="0.998923">
6.1 Data Flow
</subsectionHeader>
<bodyText confidence="0.999987666666667">
The vision system consists of two parts. First, video
frames are analyzed in sequence and the objects present
in each scene are tracked using traditional vision algo-
rithms and techniques6. For each object, information
about the object’s size, shape, and position over the life
of the scene is stored in a data structure that we call a
Blob. This name was chosen to highlight the fact that the
vision system makes no attempt at object recognition or
fine-grained analysis and is instead concerned only with
paths along which the objects (blobs) move.
Second, the data regarding each object’s progression
through the scene is interpreted by an implementation of
Bender’s algorithm DESCRIBE v.2 to produce the seman-
tic representation that is used by the other components of
the system.
</bodyText>
<subsectionHeader confidence="0.999261">
6.2 Pixels to Blobs
</subsectionHeader>
<bodyText confidence="0.997886333333333">
The low-level portion of the vision system is fed se-
quences of pixel matrices by an external system that cap-
tures video data. In the current implementation, this pixel
</bodyText>
<footnote confidence="0.9966514">
5These likelihood estimations could be generated, among
other ways, by a meaning-refinement module incorporating a
Bayesian model.
6For details concerning image labeling and object extraction
algorithms see (Horn, 1986)
</footnote>
<figureCaption confidence="0.72228">
Figure 7: Start and end states of an example scene pro-
duced by the simulator. The left image represents the start
state and the right image represents the final state.
</figureCaption>
<bodyText confidence="0.997332033333333">
data is sent from a simulator in which the actions of sim-
ple objects take place. The pixel matrices include integer
definitions of each pixel’s value, supplying all color in-
formation.
When the analysis of a particular scene begins, the vi-
sion system captures a snapshot of the background that
it uses as a reference for all subsequent frames related to
the same scene. As new video frames are input, the stored
background is subtracted and the new video frames are
converted to binary images. A noise removal algorithm
is applied to the binary images to remove any residual
elements of the original background.
Once converted to a binary representation, each video
frame is labeled using an object labeling algorithm and
each distinct object is identified. Each object present
within a frame is overlaid with a shape that will be used
in the Blob representation passed along to the next com-
ponent of the system. Each of the overlaid shapes is
(possibly) matched to a shape observed in a previous
frame. This matching procedure attempts to identify ob-
jects persisting between frames based on proximity in
size, shape, color, and position using a 4-dimensional
nearest-neighbors approach. If a shape matches with a
previously known entry, the Blob structure correspond-
ing to that particular object is assigned a new shape for
its progression. If no match is found, a new Blob struc-
ture is created for the newly-observed object.
Once the analysis of all frames of a scene is complete,
the list of Blobs is fed to the next portion of the vision
system for further interpretation.
</bodyText>
<figureCaption confidence="0.672492666666667">
Figures 7 and 8 show an example of this portion of
the vision system in use. Figure 7 shows the raw images
representing the start and end states of the scene. Figure 8
shows a visualization of the object data created by the
low-level portion of the system. The trace represents the
path along which the object moved during the scene.
Figure 8: Trace of objects during the example scene.
The moving object’s position changes are tracked and the
trace of its path is generated.
</figureCaption>
<figure confidence="0.998893444444445">
|THING blob1
���
���|THING blob0
���
���PLACE Above
��
��PATH−ELEMENT To
�� PATH
EVENT Go
</figure>
<figureCaption confidence="0.7036365">
Figure 9: LCS frame produced by the vision system
based on the example scene presented in Figure 7. Note
that no object recognition is in use, so the objects are
given temporary names (blob0 and blob1).
</figureCaption>
<subsectionHeader confidence="0.997726">
6.3 Blobs to LCS
</subsectionHeader>
<bodyText confidence="0.994823727272727">
The generation of semantic structures from vision data
concludes with an analysis of the Blobs generated by the
low-level vision system. This analysis is performed by
implementing an algorithm described, but never imple-
mented in a system, by Bender (2001).
The algorithm first examines the list of objects present
in the scene and computes the simple exists? and mov-
ing? predicates. If an object is found and moving, an LCS
GO frame is instantiated and the object is compared to all
others present so the appropriate path and place functions
can be calculated. The calculation of path and place func-
tions is based on a set of routines suggested by Bender.
These routines compute the direction, intersection, and
place-descriptions (above, on, left-of, etc.) for each pair
of objects. Finally, the path and place functions described
in Section 4 are found by examining the output of the vi-
sual routines and are added to the LCS frame.
Figure 9 shows the LCS frame constructed by the sys-
tem based on the example shown in Figure 7. The frame
can now be used by the remainder of our system in the
structural alignment phase.
��������������
</bodyText>
<sectionHeader confidence="0.999765" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999978972972973">
This work has parallels to MAIMRA, a system for
word learning from non-linguistic input (Siskind, 1990).
MAIMRA’s semantic structure is also Jackendoff LCS,
and its architecture consists of three modules: a parser
(which produces syntactic parse trees from linguistic in-
put strings), an inference component (which produces se-
mantic structures from non-linguistic input), and a linker
(which establishes correspondence between the syntactic
and semantic structures). Observing that the parser, infer-
ence, and linker components respectively fill the linguis-
tic processing, semantic processing, and structural align-
ment requirements outlined in Section 2, MAIMRA can
be viewed as an instance of the general architecture we
have described.
However, our system is also significantly differ-
ent from MAIMRA in two important respects. First,
MAIMRA is designed with the model-refinement aspect
of word learning intertwined with the correspondence-
inference aspect. In contrast, our architecture seeks to
systematically isolate these two problems, so that prob-
lems of model refinement and correspondence establish-
ment may be pursued independently. Second, MAIMRA’s
design results in exhaustive searches of many spaces (for
example, the parser must generate all possible parses).
Instead, our system seeks to use what we know as soon
as possible, for example by using bidirectional search
to guide the parse process. This implementation detail
becomes important in practical applications because ex-
haustive searches of all possible parses severely limits the
complexity of sentences that can be parsed.
The current work is part of larger initiative, the Bridge
Project. Based on the work of the Genesis Group at
MIT’s Artificial Intelligence Lab, this project seeks to
build cognitively complete systems—systems in which
language, vision, motor, and other AI domains work co-
operatively to achieve results which would have other-
wise been unattainable.
</bodyText>
<sectionHeader confidence="0.997616" genericHeader="conclusions">
8 Contributions
</sectionHeader>
<bodyText confidence="0.995346535714286">
Effectively learning the meanings of words from non-
linguistic input requires the development of representa-
tions and algorithms to determine correspondences be-
tween the linguistic and non-linguistic domains. Through
this research, our contributions to this goal include:
• We propose a general architecture, based on struc-
tural alignment, for employing linguistic and non-
linguistic context in word learning. The system
bootstraps itself by using acquired words to learn
new words. We define the necessary properties of
semantic representations used in such a system. We
also define the modules this system will require.
• We outline a system which implements this architec-
ture for the specific semantic domain of vision. We
identify LCS structures as an appropriate semantic
representation, and we demonstrate techniques for
extracting LCS from video. We also show a bidirec-
tional approach to the parsing and alignment prob-
lem.
We currently have the components described in our im-
plementation functional in isolation. The true merit of
the system will be determined as we bring together all
the pieces; thus our final contribution is the actual imple-
mentation of the systems described herein. It is our hope
that our research will act as a springboard for the devel-
opment of model refinement algorithms which have the
advantage of support from semantic alignment systems
such as ours.
</bodyText>
<sectionHeader confidence="0.999413" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99877403125">
John R. Bender. 2001. Connecting language and vision
using a conceptual semantics. Master’s thesis, Mas-
sachusetts Institute of Technology.
Lera Boroditsky. 2000. Metamorphic structuring: under-
standing time through spatial metaphors. Cognition,
75(1):1–28.
Bonnie Dorr. 1992. The use of lexical semantics in in-
terlingual machine translation. Machine Translation,
7(3):135–193.
Dedre Gentner and Arthur B. Markman. 1997. Structure
mapping in analogy and similarity. American Psychol-
ogist, 52(1):45–56.
Berthold Klaus Paul Horn. 1986. Robot Vision.
McGraw-Hill, New York, New York.
Ray Jackendoff. 1983. Semantics and Cognition, vol-
ume 8 of Current Studies in Linguistics Series. MIT
Press, Cambridge, Massachusetts.
Jeffrey Mark Siskind. 1990. Acquiring core meanings
of words, representated as jackendoff-style concep-
tual structures, from correlated streams of linguistic
and non-linguistic input. In Proceedings of the 28th
Annual Meeting of the Association for Computational
Linguistics (ACL-1990).
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, Massachusetts.
Shimon Ullman, 1996. High Level Vision, chapter 10,
pages 317–358. Sequence Seeking and Counter
Streams: A Model for Information Flow in the Visual
Cortex. MIT Press, Cambridge, Massachusetts.
Deniz Yuret. 1999. Lexical attraction models of lan-
guage. Submitted to The Sixteenth National Confer-
ence on Artificial Intelligence.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.880190">
<title confidence="0.997695">An Architecture for Word Learning using Bidirectional Structural Alignment</title>
<author confidence="0.999655">Keith Anthony Seth</author>
<email confidence="0.99731">bonawitz@mit.edutkim42@mit.edustardiff@mit.edu</email>
<affiliation confidence="0.994286">MIT Artificial Intelligence</affiliation>
<address confidence="0.9844945">545 Technology Cambridge, MA 02139</address>
<abstract confidence="0.996016136363636">Learning of new words is assisted by contextual information. This context can come in several forms, including observations in nonlinguistic semantic domains, as well as the linguistic context in which the new word was presented. We outline a general architecture for word learning, in which structural alignment coordinates this contextual information in order to restrict the possible interpretations of unknown words. We identify spatial relations as an applicable semantic domain, and describe a system-in-progress for implementing the general architecture using video sequences as our non-linguistic input. For example, when the complete system is presented with “The bird dove to the rock,” with a video sequence of a bird flying from a tree to a rock, and with the meanings for all the words except the preposition “to,” the system will register the unknown “to” with the corresponding aspect of the bird’s trajectory.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John R Bender</author>
</authors>
<title>Connecting language and vision using a conceptual semantics. Master’s thesis,</title>
<date>2001</date>
<institution>Massachusetts Institute of Technology.</institution>
<contexts>
<context position="10874" citStr="Bender, 2001" startWordPosition="1703" endWordPosition="1704">gure 3: Lexical-Conceptual Semantics is our semantic domain representation. This structure is the LCS model of “The bird flew from the pole to Deborah.” 4 Lexical-Conceptual Semantics We use Lexical-Conceptual Semantics (LCS) (Jackendoff, 1983) as our semantic representation. LCS is a cognitive representation that focuses on trajectories and spatial relations. Unlike other representations such as Logical Form (LF) and Conceptual Dependency (CD), LCS delineates notions of PATHs and PLACEs. LCS is more formally outlined in (Jackendoff, 1983) and is compared to other semantic representations in (Bender, 2001). The following productions yield a simplified portion of the LCS space. For a complete description, refer to (Jackendoff, 1983). • [THING] • [PLACE] ←− PLACE-FUNC([THING]) where PLACE-FUNC E {AT, ABOVE, BELOW, ON, IN, etc.} • [PATH] ←− PATH(PATH-ELEMENT, PATHELEMENT, · · ·) • [PATH-ELEMENT] ←− PATH-FUNC([PLACE]) where PATH-FUNC E {TO, FROM, TOWARD, AWAY-FROM, VIA, etc.} • [EVENT] ←− GO([THING],[PATH]) • [STATE] ←− BE([THING],[PLACE]) • [CAUSE] ←− CAUSE([THING],[EVENT]) • [CAUSE] ←− CAUSE([EVENT],[EVENT]) Using LCS, the trajectory expressed in the sentence “The bird flew from the pole to Debor</context>
<context position="24173" citStr="Bender (2001)" startWordPosition="3811" endWordPosition="3812">reat each word as having a set of known meanings together with a wildcard unknown meaning. When using bidirectional search to construct the constraint network topology, we bias the primings of transitions which reduce a word’s potential meaning set, using the likelihood estimates given by the meaning-refinement module. 6 Lexical-Conceptual Structures from Video The proposed system includes a vision component that is responsible for converting pixel data from a video input into the semantic structure described in Section 4. This vision system is an implementation of the ideas presented by John Bender (2001). Following Bender’s prescriptions, the vision system does not perform object recognition. Instead, the goal of the system is to analyze the different paths and places that are present in a scene and, by relating these paths and places to one another, to construct an LCS representation of the actions. 6.1 Data Flow The vision system consists of two parts. First, video frames are analyzed in sequence and the objects present in each scene are tracked using traditional vision algorithms and techniques6. For each object, information about the object’s size, shape, and position over the life of the</context>
<context position="28466" citStr="Bender (2001)" startWordPosition="4532" endWordPosition="4533">tracked and the trace of its path is generated. |THING blob1 ��� ���|THING blob0 ��� ���PLACE Above �� ��PATH−ELEMENT To �� PATH EVENT Go Figure 9: LCS frame produced by the vision system based on the example scene presented in Figure 7. Note that no object recognition is in use, so the objects are given temporary names (blob0 and blob1). 6.3 Blobs to LCS The generation of semantic structures from vision data concludes with an analysis of the Blobs generated by the low-level vision system. This analysis is performed by implementing an algorithm described, but never implemented in a system, by Bender (2001). The algorithm first examines the list of objects present in the scene and computes the simple exists? and moving? predicates. If an object is found and moving, an LCS GO frame is instantiated and the object is compared to all others present so the appropriate path and place functions can be calculated. The calculation of path and place functions is based on a set of routines suggested by Bender. These routines compute the direction, intersection, and place-descriptions (above, on, left-of, etc.) for each pair of objects. Finally, the path and place functions described in Section 4 are found </context>
</contexts>
<marker>Bender, 2001</marker>
<rawString>John R. Bender. 2001. Connecting language and vision using a conceptual semantics. Master’s thesis, Massachusetts Institute of Technology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lera Boroditsky</author>
</authors>
<title>Metamorphic structuring: understanding time through spatial metaphors.</title>
<date>2000</date>
<journal>Cognition,</journal>
<volume>75</volume>
<issue>1</issue>
<contexts>
<context position="12350" citStr="Boroditsky, 2000" startWordPosition="1931" endWordPosition="1932">ing and §6 Extract Semantic Structural Alignment Structures from Vision |THING Bird ��� ��� ���|THING Pole ��� ���PLACE At �� �� ��PATH−ELEMENT From �� ��� �� ���|THING Deborah ���� �� ���PLACE At �� ��PATH−ELEMENT To �� PATH EVENT Go §4 �������������������������� “The bird flew from the pole to Deborah” focusing on using LCS in the abstract domain of social politics is also ongoing in our lab. Furthermore, it seems that much of language is spatial in nature. For example, there is significant psychological evidence that humans use spatial relations to talk about abstract domains such as time (Boroditsky, 2000). As a consequence, we believe that techniques for learning Lexical-Conceptual Semantics for words, developed here using the concrete spatial relations domain, will be extendible to many other domains. 5 Language Parsing and Structural Alignment This section will describe the methods used to simultaneously parse linguistic input strings and align the resultant semantic structures with those from vision. The primary architecture of the system is a constraint propagation network using grammatical rules as constraints. A custom constraint network topology is generated for each linguistic input st</context>
</contexts>
<marker>Boroditsky, 2000</marker>
<rawString>Lera Boroditsky. 2000. Metamorphic structuring: understanding time through spatial metaphors. Cognition, 75(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
</authors>
<title>The use of lexical semantics in interlingual machine translation.</title>
<date>1992</date>
<journal>Machine Translation,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="11715" citStr="Dorr, 1992" startWordPosition="1830" endWordPosition="1831">TH] ←− PATH(PATH-ELEMENT, PATHELEMENT, · · ·) • [PATH-ELEMENT] ←− PATH-FUNC([PLACE]) where PATH-FUNC E {TO, FROM, TOWARD, AWAY-FROM, VIA, etc.} • [EVENT] ←− GO([THING],[PATH]) • [STATE] ←− BE([THING],[PLACE]) • [CAUSE] ←− CAUSE([THING],[EVENT]) • [CAUSE] ←− CAUSE([EVENT],[EVENT]) Using LCS, the trajectory expressed in the sentence “The bird flew from the pole to Deborah,” is represented as in Figure 3 Lexical-Conceptual Semantics focuses on spatial relations in the physical world. However, it is easily extensible to other domains, such as the temporal and possessive domains (Jackendoff, 1983; Dorr, 1992). Research §5 Parsing and §6 Extract Semantic Structural Alignment Structures from Vision |THING Bird ��� ��� ���|THING Pole ��� ���PLACE At �� �� ��PATH−ELEMENT From �� ��� �� ���|THING Deborah ���� �� ���PLACE At �� ��PATH−ELEMENT To �� PATH EVENT Go §4 �������������������������� “The bird flew from the pole to Deborah” focusing on using LCS in the abstract domain of social politics is also ongoing in our lab. Furthermore, it seems that much of language is spatial in nature. For example, there is significant psychological evidence that humans use spatial relations to talk about abstract doma</context>
</contexts>
<marker>Dorr, 1992</marker>
<rawString>Bonnie Dorr. 1992. The use of lexical semantics in interlingual machine translation. Machine Translation, 7(3):135–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dedre Gentner</author>
<author>Arthur B Markman</author>
</authors>
<title>Structure mapping in analogy and similarity.</title>
<date>1997</date>
<journal>American Psychologist,</journal>
<volume>52</volume>
<issue>1</issue>
<contexts>
<context position="2702" citStr="Gentner and Markman, 1997" startWordPosition="414" endWordPosition="417">and sentences), this context includes the semantic and syntactic relationship of the new word to other words in the linguistic input. In a multimodal learning environment, the context also includes input observed in the non-linguistic domain. Our system is designed to coordinate all of these contextual clues in order to restrict the set of possible interpretations of the new word. By leveraging previously learned words to enable the learning of new words, we create a bootstrapping system for word learning. In Section 2, we outline a general architecture based on symbolic structural alignment (Gentner and Markman, 1997) for solving the stated problem. In this outline, we identify necessary subsystems and requirements the system must satisfy. In Section 3, we identify the visual domain of spatial relations as a potential semantic domain, and in Sections 4–6 we describe a system-inprogress which instantiates the general architecture for this semantic domain. 2 General Architecture We propose an architecture to answer the following question: assuming that a new word is embedded in a phrase with other previously acquired words, how can we exploit this linguistic context to focus on the fragments of nonlinguistic</context>
<context position="4449" citStr="Gentner and Markman, 1997" startWordPosition="695" endWordPosition="698">s the correspondences CH1 and 2HB(D). Structural alignment between semantic representations will bring unknown words into correspondence with their probable semantics. fox” and “over the log”, where syntactic composition with “jumped” is the method of composition. In other words, the semantics of this sentence can be expressed: jumped(SEMANTICS(“The quick fox”), SEMANTICS(“over the log”)). Recursive application of this principle reveals that the semantic value of an expression is a structured representation. Intuitively, then, we can approach our bootstrapping problem by structural alignment (Gentner and Markman, 1997). Structural alignment is a process in which corresponding elements in two structured representations are identified by matching. Correspondence between nonmatching elements is then implied by the structural constraints of the representations. For example, in Figure 1, structural alignment first matches A, E, and F between the two representations. Then, based on structural constraints, 1 is inferred to correspond with C, and 2 with B(D). In our architecture, structural alignment of the semantics of known words (and linguistic constituents formed thereof) with semantic structures observed in th</context>
<context position="8958" citStr="Gentner and Markman (1997)" startWordPosition="1401" endWordPosition="1404"> we want to be able to infer the value of y. In order to do so, we must be able to compute the functional inverse of f with respect to y. That is, we want the function f−&apos;such that y = f−&apos; , (x, a). Figure 2: General system structure of our proposed implementation. Our learning-enabling structure is based on a semantic representation (Section 4) which is obtained by translating video inputs (Section 6). We then use a bidirectional search process to parse the linguistic input and to structurally align linguistic semantics with the non-linguistic semantics. (Section 5). 2.4 Structural Alignment Gentner and Markman (1997) describe the requisite components of a structural alignment system as (1) methods for matching structural atoms, (2) methods for identifying sets of compatible atom matches (for example, ruling out cases in which two atoms in one structure map to the same atom in another structure), and (3) methods using atom matches to guide the matching of large portions of structure. 3 Implementation with Visual Domain of Spatial Relations In order to validate our general architecture, we outline a system-in-progress which instantiates the architecture for the particular semantic domain of spatial relation</context>
</contexts>
<marker>Gentner, Markman, 1997</marker>
<rawString>Dedre Gentner and Arthur B. Markman. 1997. Structure mapping in analogy and similarity. American Psychologist, 52(1):45–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berthold Klaus Paul Horn</author>
</authors>
<title>Robot Vision.</title>
<date>1986</date>
<publisher>McGraw-Hill,</publisher>
<location>New York, New York.</location>
<contexts>
<context position="25680" citStr="Horn, 1986" startWordPosition="4056" endWordPosition="4057">ch object’s progression through the scene is interpreted by an implementation of Bender’s algorithm DESCRIBE v.2 to produce the semantic representation that is used by the other components of the system. 6.2 Pixels to Blobs The low-level portion of the vision system is fed sequences of pixel matrices by an external system that captures video data. In the current implementation, this pixel 5These likelihood estimations could be generated, among other ways, by a meaning-refinement module incorporating a Bayesian model. 6For details concerning image labeling and object extraction algorithms see (Horn, 1986) Figure 7: Start and end states of an example scene produced by the simulator. The left image represents the start state and the right image represents the final state. data is sent from a simulator in which the actions of simple objects take place. The pixel matrices include integer definitions of each pixel’s value, supplying all color information. When the analysis of a particular scene begins, the vision system captures a snapshot of the background that it uses as a reference for all subsequent frames related to the same scene. As new video frames are input, the stored background is subtra</context>
</contexts>
<marker>Horn, 1986</marker>
<rawString>Berthold Klaus Paul Horn. 1986. Robot Vision. McGraw-Hill, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Semantics and Cognition,</title>
<date>1983</date>
<booktitle>of Current Studies in Linguistics Series.</booktitle>
<volume>8</volume>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="10505" citStr="Jackendoff, 1983" startWordPosition="1646" endWordPosition="1648">s describe the methods and representations we have chosen to satisfy the requirements outlined in Section 2. Figure 2 shows how the system is designed and how the rest of this paper is organized. ���������������������� |THING Bird ��� ���|THING Pole ��� ���PLACE At �� ��PATH−ELEMENT From �� �� ��� ���|THING Deborah ��� ���PLACE At �� ��PATH−ELEMENT To �� PATH EVENT Go Figure 3: Lexical-Conceptual Semantics is our semantic domain representation. This structure is the LCS model of “The bird flew from the pole to Deborah.” 4 Lexical-Conceptual Semantics We use Lexical-Conceptual Semantics (LCS) (Jackendoff, 1983) as our semantic representation. LCS is a cognitive representation that focuses on trajectories and spatial relations. Unlike other representations such as Logical Form (LF) and Conceptual Dependency (CD), LCS delineates notions of PATHs and PLACEs. LCS is more formally outlined in (Jackendoff, 1983) and is compared to other semantic representations in (Bender, 2001). The following productions yield a simplified portion of the LCS space. For a complete description, refer to (Jackendoff, 1983). • [THING] • [PLACE] ←− PLACE-FUNC([THING]) where PLACE-FUNC E {AT, ABOVE, BELOW, ON, IN, etc.} • [PAT</context>
</contexts>
<marker>Jackendoff, 1983</marker>
<rawString>Ray Jackendoff. 1983. Semantics and Cognition, volume 8 of Current Studies in Linguistics Series. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Mark Siskind</author>
</authors>
<title>Acquiring core meanings of words, representated as jackendoff-style conceptual structures, from correlated streams of linguistic and non-linguistic input.</title>
<date>1990</date>
<booktitle>In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics (ACL-1990).</booktitle>
<contexts>
<context position="29465" citStr="Siskind, 1990" startWordPosition="4704" endWordPosition="4705">sted by Bender. These routines compute the direction, intersection, and place-descriptions (above, on, left-of, etc.) for each pair of objects. Finally, the path and place functions described in Section 4 are found by examining the output of the visual routines and are added to the LCS frame. Figure 9 shows the LCS frame constructed by the system based on the example shown in Figure 7. The frame can now be used by the remainder of our system in the structural alignment phase. �������������� 7 Related Work This work has parallels to MAIMRA, a system for word learning from non-linguistic input (Siskind, 1990). MAIMRA’s semantic structure is also Jackendoff LCS, and its architecture consists of three modules: a parser (which produces syntactic parse trees from linguistic input strings), an inference component (which produces semantic structures from non-linguistic input), and a linker (which establishes correspondence between the syntactic and semantic structures). Observing that the parser, inference, and linker components respectively fill the linguistic processing, semantic processing, and structural alignment requirements outlined in Section 2, MAIMRA can be viewed as an instance of the general</context>
</contexts>
<marker>Siskind, 1990</marker>
<rawString>Jeffrey Mark Siskind. 1990. Acquiring core meanings of words, representated as jackendoff-style conceptual structures, from correlated streams of linguistic and non-linguistic input. In Proceedings of the 28th Annual Meeting of the Association for Computational Linguistics (ACL-1990).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2000</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="14168" citStr="Steedman, 2000" startWordPosition="2216" endWordPosition="2217">not lead to a successful structurally aligned parse. In order to achieve such a complicated constraint, it is useful to decompose the constraint into a network of simpler constraints, each working over a local domain of only a few constituents rather than over the domain of an entire sentence, as in Figure 5. We can then base these subconstraints on grammatical rules over a fixed number of constituents, and trust the composed network to handle the complete sentence. 5.2 Grammatical Framework for Constraints The grammar framework chosen for our system is Combinatorial Categorial Grammar (CCG) (Steedman, 2000). CCG has many advantages in a system like ours. First, there are only a handful of rules for combining constituents, and these rules are explicit and well defined. These qualities facilitate the implementation of constraints. In addition, CCG is adept at parsing around missing information, because it was designed to handle Figure 4: The parsing and structural alignment system functions as a constraint on linguistic and visual interpretations, requiring that expressions follow grammatical rules and that they align with the semantic domain input. This example shows the system presented with the</context>
<context position="18575" citStr="Steedman, 2000" startWordPosition="2949" endWordPosition="2950">tially result in an exhaustive search of the parse space. A better solution would be to avoid the exhaustive search by constructing a custom constraint topology for each sentence, using standard heuristic parse techniques. The drawback to this approach is that we are not interested in finding just any potential parse of a phrase/sentence, nor even the most statistically probable parse. Since our intent is to perform structural alignment with input from the non-linguistic domain, our goal in parsing is to find the semantic parse structure which 4For a full description and analysis of CCG, see (Steedman, 2000) Figure 6: This figure shows one of the CCG Rules, Forward Functional Application, being treated as a constraint in a constraint propagation network. Any one of the three inputs can be left unspecified, and the constraint can completely determine the value based on the other two inputs. aligns best with the semantic structure input from the non-linguistic domain. It follows that we should use the non-linguistic input to guide our search. Our system applies bidirectional search to the parse/alignment problem. In contrast to traditional search techniques, bidirectional search treats both the “so</context>
</contexts>
<marker>Steedman, 2000</marker>
<rawString>Mark Steedman. 2000. The Syntactic Process. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shimon Ullman</author>
</authors>
<title>High Level Vision, chapter 10, pages 317–358. Sequence Seeking and Counter Streams: A Model for Information Flow in the Visual Cortex.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="19789" citStr="Ullman, 1996" startWordPosition="3135" endWordPosition="3137">rce” and “goal” symmetrically; the search-space is traversed both forward from the source and backward from the goal. The search processes operating in each direction interact with each other whenever their paths reach the same state in the search-space. This interaction provides hints for quickly completing the remainder of the search. For example, if the forward and backward paths reach the same search-state, then the forward searcher quickly reaches the goal by tracing the backward-path. The specific style of bidirectional search we are investigating is based on Streams and Counterstreams (Ullman, 1996), in which forward and backward search paths interact with each other by means of primed pathways. For each transition, two priming values are maintained: a forward priming and backward priming. Primings are used when a decision must be made between several possible transitions that could extend a search path; those transitions that have a higher priming (using the for“The boy went to the lake” |THING Boy ��� ��� ���|THING Lake ��� ���PLACE At �� ��PATH−ELEMENT To �� PATH EVENT Go CCG Constraint Backward Application (&lt;) Y:a X\Y:f ⇒ Y:fa S : ����������������� “The boy” ����������������� “went t</context>
</contexts>
<marker>Ullman, 1996</marker>
<rawString>Shimon Ullman, 1996. High Level Vision, chapter 10, pages 317–358. Sequence Seeking and Counter Streams: A Model for Information Flow in the Visual Cortex. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
</authors>
<title>Lexical attraction models of language.</title>
<date>1999</date>
<booktitle>Submitted to The Sixteenth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="21373" citStr="Yuret, 1999" startWordPosition="3390" endWordPosition="3391">ite direction. The net influence of the primings is that transitions previously traversed in one direction are more likely to be explored in the opposite direction, if the opportunity arises. By extension, primings provide clues for finding a path from any state to the target state. The Streams and Counterstreams approach to bidirectional search facilitates incorporation of other types of context. For example, some situational context can be captured by allowing primings from previous parses of recent sentences to influence the current parse. Also, statistical cues such as Lexical Attraction (Yuret, 1999) can be integrated into the system by using heuristics to bias primings. 5.4 Structural Alignment The three components of structural alignment specified in Section 2.4 (atomic alignment, identification of compatible match sets, and structurally implied matches) are woven into the bidirectional search construction of the constraint network topology. When a constraint network fragment is constructed which bridges between a small portion of the linguistic input and the non-linguistic semantics, this “atomic alignment” primes the bidirectional search to be more likely to repeat this match while co</context>
</contexts>
<marker>Yuret, 1999</marker>
<rawString>Deniz Yuret. 1999. Lexical attraction models of language. Submitted to The Sixteenth National Conference on Artificial Intelligence.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>