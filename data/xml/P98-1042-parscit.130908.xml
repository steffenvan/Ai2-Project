<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000419">
<title confidence="0.998652">
An Experiment in Hybrid Dictionary
and Statistical Sentence Alignment
</title>
<author confidence="0.931885">
Nigel Collier, Kenji Ono and Hideki Hirakawa
</author>
<affiliation confidence="0.7759705">
Communication and Information Systems Laboratories
Research and Development Center, Toshiba Corporation
</affiliation>
<address confidence="0.745187">
1 Komukai Toshiba-cho, Kawasaki-shi, Kanagawa 210-8582, Japan
</address>
<email confidence="0.850343">
Inigel,ono,hirakawaleeel.rdc.toshiba.co.jp
</email>
<sectionHeader confidence="0.976354" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956571428572">
The task of aligning sentences in parallel corpora of
two languages has been well studied using pure sta-
tistical or linguistic models. We developed a linguis-
tic method based on lexical matching with a bilin-
gual dictionary and two statistical methods based
on sentence length ratios and sentence offset prob-
abilities. This paper seeks to further our knowl-
edge of the alignment task by comparing the per-
formance of the alignment models when used sepa-
rately and together, i.e. as a hybrid system. Our
results show that for our English-Japanese corpus of
newspaper articles, the hybrid system using lexical
matching and sentence length ratios outperforms the
pure methods.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999061942857143">
There have been many approaches proposed to solve
the problem of aligning corresponding sentences in
parallel corpora. With a few notable exceptions
however, much of this work has focussed on ei-
ther corpora containing European language pairs or
clean-parallel corpora where there is little reformat-
ting. In our work we have focussed on developing
a method for robust matching of English-Japanese
sentences, based primarily on lexical matching. The
method combines statistical information from byte
length ratios. We show in this paper that this hybrid
model is more effective than its constituent parts
used separately.
The task of sentence alignment is a critical first
step in many automatic applications involving the
analysis of bilingual texts such as extraction of bilin-
gual vocabulary, extraction of translation templates,
word sense disambiguation, word and phrase align-
ment, and extraction of parameters for statistical
translation models. Many software products which
aid human translators now contain sentence align-
ment tools as an aid to speeding up editing and ter-
minology searching.
Various methods have been developed for sentence
alignment which we can categorise as either lexical
such as (Chen, 1993), based on a large-scale bilin-
gual lexicon; statistical such as (Brown et al., 1991)
(Church, 1993)(Gale and Church, 1993)(Kay and
Roshcheisen, 1993), based on distributional regular-
ities of words or byte-length ratios and possibly in-
ducing a bilingual lexicon as a by-product, or hybrid
such as (Utsuro et al., 1994) (Wu, 1994), based on
some combination of the other two. Neither of the
pure approaches is entirely satisfactory for the fol-
lowing reasons:
</bodyText>
<listItem confidence="0.995686157894737">
• Text volume limits the usefulness of statistical
approaches. We would often like to be able to
align small amounts of text, or texts from var-
ious domains which do not share the same sta-
tistical properties.
• Bilingual dictionary coverage limitations mean
that we will often encounter problems establish-
ing a correspondence in non-general domains.
• Dictionary-based approaches are founded on an
assumption of lexical correspondence between
language pairs. We cannot always rely on this
for non-cognate language pairs, such as English
and Japanese.
• Texts are often heavily reformatted in trans-
lation, so we cannot assume that the corpus
will be clean, i.e. contain many one-to-one sen-
tence mappings. In this case statistical methods
which rely on structure correspondence such as
byte-length ratios may not perform well.
</listItem>
<bodyText confidence="0.999819125">
These factors suggest that some hybrid method
may give us the best combination of coverage and
accuracy when we have a variety of text domains,
text sizes and language pairs. In this paper we seek
to fill a gap in our understanding and to show how
the various components of the hybrid method influ-
ence the quality of sentence alignment for Japanese
and English newspaper articles.
</bodyText>
<sectionHeader confidence="0.980511" genericHeader="method">
2 Bilingual Sentence Alignment
</sectionHeader>
<bodyText confidence="0.998306666666667">
The task of sentence alignment is to match corre-
sponding sentences in a text from one language to
sentences in a translation of that text in another
language. Of particular interest to us is the ap-
plication to Asian language pairs. Previous stud-
ies such as (Fung and Wu, 1994) have commented
</bodyText>
<page confidence="0.995361">
268
</page>
<bodyText confidence="0.99841645">
that methods developed for Indo-European language
pairs using alphabetic characters have not addressed
important issues which occur with European-Asian
language pairs. For example, the language pairs are
unlikely to be cognates, and they may place sentence
boundaries at different points in the text. It has also
been suggested by (Wu, 1994) that sentence length
ratio correlations may arise partly out of historic
cognate-based relationships between Indo-European
languages. Methods which perform well for Indo-
European language pairs have therefore been found
to be less effective for non-Indo-European language
pairs.
In our experiments the languages we use are En-
glish (source) and Japanese (translation). Although
in our corpus (described below) we observe that, in
general, sentences correspond one-to-one we must
also consider multiple sentence correspondences as
well as one-to-zero correspondences. These cases are
summarised below.
</bodyText>
<listItem confidence="0.997383545454546">
1. 1:1 The sentences match one-to-one.
2. 1:n One English sentence matches to more than
one Japanese sentence.
3. m:1 More than one English sentence matches ot
one Japanese sentence.
4. m:n More than one English sentence matches to
more than one Japanese sentence.
5. m:0 The English sentence/s have no correspond-
ing Japanese sentence.
6. 0:n The Japanese sentence/s have no corre-
sponding English sentence.
</listItem>
<bodyText confidence="0.9987305">
In the case of 1:n, m:1 and m:n correspondences,
translation has involved some reformatting and the
meaning correspondence is no longer solely at the
sentence level. Ideally we would like smaller units of
text to match because it is easier later on to establish
word alignment correspondences. In the worst case
of multiple correspondence, the translation is spread
across multiple non-consecutive sentences.
</bodyText>
<sectionHeader confidence="0.994308" genericHeader="method">
3 Corpus
</sectionHeader>
<bodyText confidence="0.998195382978724">
Our primarily motivation is knowledge acquisition
for machine translation and consequently we are in-
terested to acquire vocabulary and other bilingual
knowledge which will be useful for users of such sys-
tems. Recently there has been a move towards In-
ternet page translation and we consider that one in-
teresting domain for users is international news.
The bilingual corpus we use in our experiments is
made from Reuter news articles which were trans-
lated by the Gakken translation agency from En-
glish into Japanesel . The translations are quite lit-
eral and the contents cover international news for
I The corpus was generously made available to us by special
arrangement with Gakken
the period February 1995 to December 1996. We
currently have over 20,000 articles (approximately
47 Mb). From this corpus we randomly chose 50
article pairs and aligned them by hand using a hu-
man bilingual checker to form a judgement set. The
judgement set consists of 380 English sentences and
453 Japanese sentences. On average each English
article has 8 lines and each Japanese article 9 lines.
The articles themselves form a boundary within
which to align constituent sentences. The corpus
is quite well behaved. We observe many 1:1 corre-
spondences, but also a large proportion of 1:2 and
1:3 correspondences as well as reorderings. Omis-
sions seem to be quite rare, so we didn&apos;t see many
m:0 or 0:n correspondences.
An example news article is shown in Figure 1
which highlights several interesting points. Al-
though the news article texts are clean and in
machine-tractable format we still found that it was
a significant challenge to reliably identify sentence
boundaries. A simple illustration of this is shown by
the first Japanese line J1 which usually corresponds
to the first two English lines El and E2. This is
a result of our general-purpose sentence segmenta-
tion algorithm which has difficulty separating the
Japanese title from the first sentence.
Sentences usually corresponded linearly in our
corpus, with few reorderings, so the major chal-
lenge was to identify multiple correspondences and
zero correspondences. We can see an example of a
zero correspondence as E5 has no translation in the
Japanese text. A 1:n correspondence is shown by E7
aligning to both J5 and J6.
</bodyText>
<sectionHeader confidence="0.994633" genericHeader="method">
4 Alignment Models
</sectionHeader>
<bodyText confidence="0.9996256">
In our investigation we examined the performance of
three different matching models (lexical matching,
byte-length ratios and offset probabilities). The ba-
sic models incorporate dynamic programming to find
the least cost alignment path over the set of English
and Japanese sentences. Cost being determined by
the model&apos;s scores. The alignment space includes all
possible combinations of multiple matches upto and
including 3:3 alignments. The basic models are now
outlined below.
</bodyText>
<subsectionHeader confidence="0.999389">
4.1 Model 1: Lexical vector matching
</subsectionHeader>
<bodyText confidence="0.999957">
The lexical approach is perhaps the most robust for
aligning texts in cognate language pairs, or where
there is a large amount of reformatting in trans-
lation. It has also been shown to be particularly
successful within the vector space model in multilin-
gual information retrieval tasks, e.g. (Collier et al.,
1998a),(Collier et al., 1998b), for aligning texts in
non-cognate languages at the article level.
The major limitation with lexical matching is
clearly the assumption of lexical correspondence -
</bodyText>
<page confidence="0.987867">
269
</page>
<bodyText confidence="0.719383">
El. Taiwan ruling party sees power struggle in China
E2. TAIPEI , Feb 9 ( Reuter ) - Taiwan&apos;s ruling Nationalist Party said a struggle to succeed Deng
Xiaoping as China&apos;s most powerful man may have already begun.
</bodyText>
<listItem confidence="0.9911072">
E3. &amp;quot;Once Deng Xiaoping dies, a high tier power struggle among the Chinese communists is in-
evitable,&amp;quot; a Nationalist Party report said.
E4. China and Taiwan have been rivals since the Nationalists lost the Chinese civil war in 1949 and
fled to Taiwan.
E5. Both Beijing and Taipei sometimes portray each other in an unfavourable light.
E6. The report said that the position of Deng&apos;s chosen successor, President Jiang Zemin, may have
been subtly undermined of late.
E7. It based its opinion on the fact that two heavyweight political figures have recently used the
phrase the &amp;quot;solid central collective leadership and its core&amp;quot; instead of the accepted &amp;quot;collective leader-
ship centred on Jiang Zemin&amp;quot; to describe the current leadership structure.
E8. &amp;quot;Such a sensitive statement should not be an unintentional mistake ...
E9. Does this mean the power struggle has gradually surfaced while Deng Xiaoping is still alive ?,&amp;quot;
said the report , distributed to journalists.
E10. &amp;quot;At least the information sends a warning signal that the &apos;core of Jiang&apos; has encountered some
subtle changes,&amp;quot; it added .
</listItem>
<equation confidence="0.852159333333333">
J1. tallilVA-4**Is 1411:11JtaktriCzoitt.0ANILMO [tilt 9[1 t2
o4k,Matlt,r1,11E0AAtAt.,1-17/1-ToMiA-15-ottst-ci:MI.0-c-06gfthIA
, 4U t:,,
J2.Fait, E4-64:62.3b*Litctii**optca,--C, r17/1-*t&amp;E.1k,
hIE- oNtft 5 j 3.1-&lt;tt0
J3. 111111L fIllX5t7, 1 9 4 9 4:1=1:11A#akL oPYM:PVL-C, OATA
</equation>
<figure confidence="0.751112">
TAI:b)60
J4. NiPitlZ1 F (1)1kgitg-1:1-41f41,-Co6iIiRRIII*TgotVht, )%id, MO4:fit -Dtz:
Mg14175 ‘o
J5.PkoRATIts rPrilgEfrq(Dtt 2 MI&apos; , 1Wid058 itog)tchl-C&apos;s NAotriinnotx3A1-, &amp;quot;mvsq,
Atunwoi opt-AiLs&amp;quot; 5 LI:X-Jo-006 5.
J6. LS&amp;quot; &amp;quot; riltbqsti-)tvr f::.
J7. #MITit , 1.7_01 5tatt0,Ti- htso,
Li&apos;Llt, J5&apos;,/.1\T&apos;Lki71,--CL■6c1l.:, 1UcttZ.1
J9. i_PtS&lt; t. &amp;quot; il/MVAL &amp;quot;4-6 &amp;quot; 5 floyi:, MOtsViE-ptc.: LIJ:UMV-:f Lit-&lt;-co6
</figure>
<figureCaption confidence="0.999806">
Figure 1: Example English-Japanese news article pair
</figureCaption>
<bodyText confidence="0.9999509">
which is particularly weak for English and Asian
language pairs where structural and semantic dif-
ferences mean that transfer often occurs at a level
above the lexicon. This is a motivation for incor-
porating statistics into the alignment process, but
in the initial stage we wanted to treat pure lexical
matching as our baseline performance.
We translated each Japanese sentence into En-
glish using dictionary term lookup. Each Japanese
content word was assigned a list of possible English
translations and these were used to match against
the normalised English words in the English sen-
tences. For an English text segment E and the En-
glish term list produced from a Japanese text seg-
ment J, which we considered to be a possible unit
of correspondence, we calculated similarity using
Dice&apos;s coefficient score shown in Equation 1. This
rather simple measure captures frequency, but not
positional information. The weights of words are
their frequencies inside a sentence.
</bodyText>
<equation confidence="0.94515">
Dice(E,J) — (1)
IE-F fJ
</equation>
<bodyText confidence="0.999589285714286">
where fEj is the nun iber of lexical items which
match in E and J, fE is the number of lexical items
in E and fj is the number of lexical items in J.
The translation lists for each Japanese word are used
disjunctively, so if one word in the list matches then
we do not consider the other terms in the list. In
this way we maintain term independence.
</bodyText>
<figure confidence="0.915253666666667">
2fEj
270
120
100
60
60
40
20
*.4
</figure>
<bodyText confidence="0.999648625">
Our transfer dictionary contained some 79,000 En-
glish words in full form together with the list of
translations in Japanese. Of these English words
some 14,000 were proper nouns which were directly
relevant to the vocabulary typically found in interna-
tional news stories. Additionally we perform lexical
normalisation before calculating the matching score
and remove function words with a stop list.
</bodyText>
<subsectionHeader confidence="0.998802">
4.2 Model 2: Byte-length ratios
</subsectionHeader>
<bodyText confidence="0.999994476190476">
For Asian language pairs we cannot rely entirely
on dictionary term matching. Moreover, algorithms
which rely on matching cognates cannot be applied
easily to English and some Asian language. We
were motivated by statistical alignment models such
as (Gale and Church, 1991) to investigate whether
byte-length probabilities could improve or replace
the lexical matching based method. The underlying
assumption is that characters in an English sentence
are responsible for generating some fraction of each
character in the corresponding Japanese sentence.
We derived a probability density function by mak-
ing the assumption that English and Japanese sen-
tence length ratios are normally distributed. The
parameters required for the model are the mean, p
and variance, a, which we calculated from a training
set of 450 hand-aligned sentences. These are then
entered into Equation 2 to find the probability of
any two sentences (or combinations of sentences for
multiple alignments) being in an alignment relation
given that they have a length ratio of x.
</bodyText>
<equation confidence="0.81566">
F(s)— L-1;712.7r1 [;-L4}2 (2)
</equation>
<bodyText confidence="0.999945565217391">
The byte length ratios were calculated as the
length of the Japanese text segment divided by the
length of the English text segment. So in this way we
can incorporate multiple sentence correspondences
into our model. Byte lengths for English sentences
are calculated according to the number of non-white
space characters, with a weighting of 1 for each valid
character including punctuation. For the Japanese
text we counted 2 for each non-white space char-
acter. White spaces were treated as having length
0. The ratios for the training set are shown as a
histogram in Figure 2 and seem to support the as-
sumption of a normal distribution.
The resulting normal curve with a = 0.33 and
p = 0.76 is given in Figure 3, and this can then be
used to provide a probability score for any English
and Japanese sentence being aligned in the Reuters&apos;
corpus.
Clearly it is not enough simply to assume that our
sentence pair lengths follow the normal distribution.
We tested this assumption using a standard test, by
plotting the ordered ratio scores against the values
calculated for the normal curve in Figure 3. If the
</bodyText>
<figureCaption confidence="0.999984">
Figure 2: Sentence length ratios in training set
Figure 3: Sentence lc, igth ratio normal curve
</figureCaption>
<bodyText confidence="0.987228166666667">
distribution is indeed noi mai then we would expect
the plot in Figure 4 to yi,,ld a straight line. We can
see that this is the case f..r most, although not all,
of the observed scores.
Although the curve in Figure 4 shows that our
training set deviated from the normal distribution at
</bodyText>
<figure confidence="0.973093142857143">
•
•
-2
-a
-4
0 0.2 0.4 0.6 0.0 1 1.2 1.4 1.5 1.5 2
nat.
</figure>
<figureCaption confidence="0.998986">
Figure 4: Sentence length ratio normal check curve
</figureCaption>
<page confidence="0.929677">
271
</page>
<figure confidence="0.798147">
;
</figure>
<figureCaption confidence="0.999604">
Figure 5: Sentence offsets in training set
</figureCaption>
<bodyText confidence="0.99997475">
the extremes we nevertheless proceeded to continue
with our simulations using this model considering
that the deviations occured at the extreme ends of
the distribution where relatively few samples were
found. The weakness of this assumption however
does add extra evidence to doubts which have been
raised, e.g. (Wu, 1994), about whether the byte-
length model by itself can perform well.
</bodyText>
<subsectionHeader confidence="0.99946">
4.3 Model 3: Offset ratios
</subsectionHeader>
<bodyText confidence="0.99999265">
We calculated the offsets in the sentence indexes for
English and Japanese sentences in an alignment re-
lation in the hand-aligned training set. An offset
difference was calculated as the Japanese sentence
index minus the English sentence index within a
bilingual news article pair. The values are shown
as a histogram in Figure 5.
As with the byte-length ratio model, we started
from an assumption that sentence correspondence
offsets were normally distributed. We then cal-
culated the mean and variance for our sample set
shown in Figure 5 and used this to form a normal
probability density function (where a = 0.50 and
p = 1.45) shown in Figure 6.
The test for normality of the distribution is the
same as for byte-length ratios and is given in Figure
7. We can see that the assumption of normality is
particularly weak for the offset distribution, but we
are motivated to see whether such a noisy probabil-
ity model can improve alignment results.
</bodyText>
<sectionHeader confidence="0.999011" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.998228875">
In this section we present the results of using dif-
ferent combinations of the three basic methods. We
combined the basic methods to make hybrid models
simply by taking the product of the scores for the
models given above. Although this is simplistic we
felt that in the first stage of our investigation it was
better to give equal weight to each method.
The seven methods we tested are coded as follows:
</bodyText>
<page confidence="0.768693">
-3
</page>
<figureCaption confidence="0.999935">
Figure 6: Sentence offsets normal curve
Figure 7: Sentence offsets normal check curve
</figureCaption>
<bodyText confidence="0.978128666666667">
DICE: sentence alignment using bilingual dictionary
and Dice&apos;s coefficient scores; LEN: sentence align-
ment using sentence length ratios; OFFSET: sen-
tence alignment using offsct probabilities.
We performed sentence alignment on our test set
of 380 English sentences and 453 Japanese sentences.
The results are shown as recall and precision which
we define in the usual way as follows:
#correctly matched sentences retrieved
</bodyText>
<equation confidence="0.76477075">
recall =
#matched sentences in the test collection
#correctly matched sentences retrieved
precision —
</equation>
<bodyText confidence="0.96877">
# matched sentences retrieved
The results are shown in Table 1. We see that the
baseline method using lexical matching with a bilin-
gual lexicon, DICE, performs better than either of
the two statistical methods LEN or OFFSET used
separately. Offset probabilities in particular per-
formed poorly showing that we cannot expect the
correctly matching sentence to appear constantly in
</bodyText>
<figure confidence="0.98012475">
0.03
060
130
100
</figure>
<page confidence="0.984218">
272
</page>
<table confidence="0.945123">
the same highest probability position.
Method Rec. (%) Pr. (%)
DICE (baseline) 84 85
LEN 82 83
OFFSET 50 57
LEN+OFFSET 70 70
DICE+LEN 89 87
DICE+OFFSET 80 80
DICE+LEN+OFFSET 88 85
</table>
<tableCaption confidence="0.8782945">
Table 1: Sentence alignment results as recall and
precision.
</tableCaption>
<bodyText confidence="0.996234714285714">
Considering the hybrid methods, we see signifi-
cantly that DICE+LEN provides a clearly better re-
sult for both recall and precision to either DICE or
LEN used separately. On inspection we found that
DICE by itself could not distinguish clearly between
many candidate sentences. This occured for two rea-
sons.
</bodyText>
<listItem confidence="0.879856714285714">
1. As a result of the limited domain in which news
articles report, there was a strong lexical over-
lap between candidate sentences in a news arti-
cle.
2. Secondly, where the lexical overlap was poor be-
tween the English sentence and the Japanese
translation, this leads to low DICE scores.
</listItem>
<bodyText confidence="0.99998121875">
The second reason can be attributed to low cov-
erage in the bilingual lexicon with the domain of
the news articles. If we had set a minimum thresh-
old limit for overlap frequency then we would have
ruled out many correct matches which were found.
In both cases LEN provides a decisive clue and en-
ables us to find the correct result more reliably. Fur-
thermore, we found that LEN was particularly ef-
fective at identifying multi-sentence correspondences
compared to DICE, possibly because some sentences
are very small and provide weak evidence for lexi-
cal matching, whereas when they are combined with
neighbours they provide significant evidence for the
LEN model.
Using all methods together however in
DICE+LEN+OFFSET seems less promising and we
believe that the offset probabilities are not a reliable
model. Possibly this is due to lack of data in the
training stage when we calculated a and u, or the
data set may not in fact be normally distributed as
indicated by Figure 7.
Finally, we noticed that a consistent factor in the
English and Japanese text pairs was that the first
two lines of the English were always matched to the
first line of the Japanese. This was because the En-
glish text separated the title and first line, whereas
our sentence segmenter could not do this for the
Japanese. This factor was consistent for all the 50
article pairs in our test collection and may have led
to a small deterioration in the results, so the figures
we present are the minimum of what we can expect
when sentence segmentation is performed correctly.
</bodyText>
<sectionHeader confidence="0.998618" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99998904">
The assumption that a partial alignment at the word
level from lexical correspondences can clearly in-
dicate full sentence alignment is flawed when the
texts contain many sentences with similar vocabu-
lary. This is the case with the news stories used in
our experiments and even technical vocabulary and
proper nouns are not adequate to clearly discrimi-
nate between alternative alignment choices because
the vocabulary range inside the news article is not
large. Moreover, the basic assumption of the lexical
approach, that the coverage of the bilingual dictio-
nary is adequate, cannot be relied on if we require
robustness. This has shown the need for some hybrid
model.
For our corpus of newspaper articles, the hybrid
model has been shown to clearly improve sentence
alignment results compared with the pure models
used separately. In the future we would like to make
extensions to the lexical model by incorporating
term weighting methods from information retrieval
such as inverse document frequency which may help
to identify more important terms for matching. In
order to test the generalisability of our method we
also want to extend our investigation to parallel cor-
pora in other domains.
</bodyText>
<sectionHeader confidence="0.996004" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999875">
We would like to thank Reuters and Gakken for al-
lowing us to use the corpus of news stories in our
work. We are grateful to Miwako Shimazu for hand
aligning the judgement set used in the experiments
and to Akira Kumano and Satoshi Kinoshita for
useful discussions. Finally we would also like ex-
press our appreciation to the anonymous reviewers
for their helpful comments.
</bodyText>
<sectionHeader confidence="0.999226" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998347769230769">
P. Brown, J. Lai, and It. Mercer. 1991. Aligning sen-
tences in parallel corpora. In 29th Annual Meeting
of the Association for Computational Linguistics,
Berkeley, California, USA.
S. Chen. 1993. Aligning sentences in bilingual cor-
pora using lexical information. 31st Annual Meet-
ing of the Association of Computational Linguis-
tics, Ohio, USA, 22-26 June.
K. Church. 1993. Char_align: a program for align-
ing parallel texts at the character level. In 31st
Annual Meeting of the Association for Computa-
tional Linguistics, Ohio, USA, pages 1-8, 22-26
June.
</reference>
<page confidence="0.980645">
273
</page>
<reference confidence="0.998340583333333">
N. Collier, H. Hirakawa, and A. Kumano. 1998a.
Creating a noisy parallel corpus from newswire
articles using multi-lingual information retrieval.
Trans. of Information Processing Society of Japan
(to appear).
N. Collier, H. Hirakawa, and A. Kumano. 1998b.
Machine translation vs. dictionary term transla-
tion - a comparison for English-Japanese news
article alignment. In Proceedings of COLING-
ACL &apos;98, University of Montreal, Canada, 10th
August.
P. Fung and D. Wu. 1994. Statistical augmenta-
tion of a Chinese machine readable dictionary. In
Second Annual Workshop on Very Large Corpora,
pages 69-85, August.
W. Gale and K. Church. 1991. A program for align-
ing sentences in bilingual corpora. In Proceedings
of the 29th Annual Conference of the Association
for Computational Linguistics (ACL-91), Berke-
ley, California, pages 177-184.
W. Gale and K. Church. 1993. A program for align-
ing sentences in a bilingual corpora. Computa-
tional Linguistics, 19(1):75-102.
M. Kay and M. Roshcheisen. 1993. Text-translation
alignment. Computational Linguistics, 19:121-
142.
T. Utsuro, H. Ikeda, M. Yamane, Y. Matsumoto,
and N. Nagao. 1994. Bilingual text match-
ing using bilingual dictionary and statistics. In
COLING-94, 15th International Conference, Ky-
oto, Japan, volume 2, August 5-9.
D. Wu. 1994. Aligning a parallel English-Chinese
corpus statistically with lexical criteria. In 32nd
Annual Meeting of the Association for Computa-
tional Linguistics, New Mexico, USA, pages 80-
87, June 27-30.
</reference>
<page confidence="0.998281">
274
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.396689">
<title confidence="0.95949">An Experiment in Hybrid Dictionary and Statistical Sentence Alignment Collier, Kenji Ono Hirakawa Communication and Information Systems Laboratories</title>
<note confidence="0.784241666666667">Research and Development Center, Toshiba Corporation 1 Komukai Toshiba-cho, Kawasaki-shi, Kanagawa 210-8582, Japan Inigel,ono,hirakawaleeel.rdc.toshiba.co.jp</note>
<abstract confidence="0.9991842">The task of aligning sentences in parallel corpora of two languages has been well studied using pure statistical or linguistic models. We developed a linguistic method based on lexical matching with a bilingual dictionary and two statistical methods based on sentence length ratios and sentence offset probabilities. This paper seeks to further our knowledge of the alignment task by comparing the performance of the alignment models when used separately and together, i.e. as a hybrid system. Our results show that for our English-Japanese corpus of newspaper articles, the hybrid system using lexical matching and sentence length ratios outperforms the pure methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Mercer</author>
</authors>
<title>Aligning sentences in parallel corpora.</title>
<date>1991</date>
<booktitle>In 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Berkeley, California, USA.</location>
<marker>Mercer, 1991</marker>
<rawString>P. Brown, J. Lai, and It. Mercer. 1991. Aligning sentences in parallel corpora. In 29th Annual Meeting of the Association for Computational Linguistics, Berkeley, California, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Chen</author>
</authors>
<title>Aligning sentences in bilingual corpora using lexical information.</title>
<date>1993</date>
<booktitle>31st Annual Meeting of the Association of Computational Linguistics,</booktitle>
<location>Ohio, USA,</location>
<contexts>
<context position="2235" citStr="Chen, 1993" startWordPosition="328" endWordPosition="329"> separately. The task of sentence alignment is a critical first step in many automatic applications involving the analysis of bilingual texts such as extraction of bilingual vocabulary, extraction of translation templates, word sense disambiguation, word and phrase alignment, and extraction of parameters for statistical translation models. Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching. Various methods have been developed for sentence alignment which we can categorise as either lexical such as (Chen, 1993), based on a large-scale bilingual lexicon; statistical such as (Brown et al., 1991) (Church, 1993)(Gale and Church, 1993)(Kay and Roshcheisen, 1993), based on distributional regularities of words or byte-length ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (Utsuro et al., 1994) (Wu, 1994), based on some combination of the other two. Neither of the pure approaches is entirely satisfactory for the following reasons: • Text volume limits the usefulness of statistical approaches. We would often like to be able to align small amounts of text, or texts from var</context>
</contexts>
<marker>Chen, 1993</marker>
<rawString>S. Chen. 1993. Aligning sentences in bilingual corpora using lexical information. 31st Annual Meeting of the Association of Computational Linguistics, Ohio, USA, 22-26 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>Char_align: a program for aligning parallel texts at the character level.</title>
<date>1993</date>
<booktitle>In 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1--8</pages>
<location>Ohio, USA,</location>
<contexts>
<context position="2334" citStr="Church, 1993" startWordPosition="344" endWordPosition="345">ns involving the analysis of bilingual texts such as extraction of bilingual vocabulary, extraction of translation templates, word sense disambiguation, word and phrase alignment, and extraction of parameters for statistical translation models. Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching. Various methods have been developed for sentence alignment which we can categorise as either lexical such as (Chen, 1993), based on a large-scale bilingual lexicon; statistical such as (Brown et al., 1991) (Church, 1993)(Gale and Church, 1993)(Kay and Roshcheisen, 1993), based on distributional regularities of words or byte-length ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (Utsuro et al., 1994) (Wu, 1994), based on some combination of the other two. Neither of the pure approaches is entirely satisfactory for the following reasons: • Text volume limits the usefulness of statistical approaches. We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties. • Bilingual dictionary coverage li</context>
</contexts>
<marker>Church, 1993</marker>
<rawString>K. Church. 1993. Char_align: a program for aligning parallel texts at the character level. In 31st Annual Meeting of the Association for Computational Linguistics, Ohio, USA, pages 1-8, 22-26 June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Collier</author>
<author>H Hirakawa</author>
<author>A Kumano</author>
</authors>
<title>Creating a noisy parallel corpus from newswire articles using multi-lingual information retrieval.</title>
<date>1998</date>
<journal>Trans. of Information Processing Society of Japan</journal>
<note>(to appear).</note>
<contexts>
<context position="9108" citStr="Collier et al., 1998" startWordPosition="1419" endWordPosition="1422">least cost alignment path over the set of English and Japanese sentences. Cost being determined by the model&apos;s scores. The alignment space includes all possible combinations of multiple matches upto and including 3:3 alignments. The basic models are now outlined below. 4.1 Model 1: Lexical vector matching The lexical approach is perhaps the most robust for aligning texts in cognate language pairs, or where there is a large amount of reformatting in translation. It has also been shown to be particularly successful within the vector space model in multilingual information retrieval tasks, e.g. (Collier et al., 1998a),(Collier et al., 1998b), for aligning texts in non-cognate languages at the article level. The major limitation with lexical matching is clearly the assumption of lexical correspondence - 269 El. Taiwan ruling party sees power struggle in China E2. TAIPEI , Feb 9 ( Reuter ) - Taiwan&apos;s ruling Nationalist Party said a struggle to succeed Deng Xiaoping as China&apos;s most powerful man may have already begun. E3. &amp;quot;Once Deng Xiaoping dies, a high tier power struggle among the Chinese communists is inevitable,&amp;quot; a Nationalist Party report said. E4. China and Taiwan have been rivals since the Nationali</context>
</contexts>
<marker>Collier, Hirakawa, Kumano, 1998</marker>
<rawString>N. Collier, H. Hirakawa, and A. Kumano. 1998a. Creating a noisy parallel corpus from newswire articles using multi-lingual information retrieval. Trans. of Information Processing Society of Japan (to appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Collier</author>
<author>H Hirakawa</author>
<author>A Kumano</author>
</authors>
<title>Machine translation vs. dictionary term translation - a comparison for English-Japanese news article alignment.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL &apos;98,</booktitle>
<institution>University of Montreal,</institution>
<contexts>
<context position="9108" citStr="Collier et al., 1998" startWordPosition="1419" endWordPosition="1422">least cost alignment path over the set of English and Japanese sentences. Cost being determined by the model&apos;s scores. The alignment space includes all possible combinations of multiple matches upto and including 3:3 alignments. The basic models are now outlined below. 4.1 Model 1: Lexical vector matching The lexical approach is perhaps the most robust for aligning texts in cognate language pairs, or where there is a large amount of reformatting in translation. It has also been shown to be particularly successful within the vector space model in multilingual information retrieval tasks, e.g. (Collier et al., 1998a),(Collier et al., 1998b), for aligning texts in non-cognate languages at the article level. The major limitation with lexical matching is clearly the assumption of lexical correspondence - 269 El. Taiwan ruling party sees power struggle in China E2. TAIPEI , Feb 9 ( Reuter ) - Taiwan&apos;s ruling Nationalist Party said a struggle to succeed Deng Xiaoping as China&apos;s most powerful man may have already begun. E3. &amp;quot;Once Deng Xiaoping dies, a high tier power struggle among the Chinese communists is inevitable,&amp;quot; a Nationalist Party report said. E4. China and Taiwan have been rivals since the Nationali</context>
</contexts>
<marker>Collier, Hirakawa, Kumano, 1998</marker>
<rawString>N. Collier, H. Hirakawa, and A. Kumano. 1998b. Machine translation vs. dictionary term translation - a comparison for English-Japanese news article alignment. In Proceedings of COLINGACL &apos;98, University of Montreal, Canada, 10th August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Fung</author>
<author>D Wu</author>
</authors>
<title>Statistical augmentation of a Chinese machine readable dictionary.</title>
<date>1994</date>
<booktitle>In Second Annual Workshop on Very Large Corpora,</booktitle>
<pages>69--85</pages>
<contexts>
<context position="4204" citStr="Fung and Wu, 1994" startWordPosition="648" endWordPosition="651">bination of coverage and accuracy when we have a variety of text domains, text sizes and language pairs. In this paper we seek to fill a gap in our understanding and to show how the various components of the hybrid method influence the quality of sentence alignment for Japanese and English newspaper articles. 2 Bilingual Sentence Alignment The task of sentence alignment is to match corresponding sentences in a text from one language to sentences in a translation of that text in another language. Of particular interest to us is the application to Asian language pairs. Previous studies such as (Fung and Wu, 1994) have commented 268 that methods developed for Indo-European language pairs using alphabetic characters have not addressed important issues which occur with European-Asian language pairs. For example, the language pairs are unlikely to be cognates, and they may place sentence boundaries at different points in the text. It has also been suggested by (Wu, 1994) that sentence length ratio correlations may arise partly out of historic cognate-based relationships between Indo-European languages. Methods which perform well for IndoEuropean language pairs have therefore been found to be less effectiv</context>
</contexts>
<marker>Fung, Wu, 1994</marker>
<rawString>P. Fung and D. Wu. 1994. Statistical augmentation of a Chinese machine readable dictionary. In Second Annual Workshop on Very Large Corpora, pages 69-85, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of the 29th Annual Conference of the Association for Computational Linguistics (ACL-91),</booktitle>
<pages>177--184</pages>
<location>Berkeley, California,</location>
<contexts>
<context position="13422" citStr="Gale and Church, 1991" startWordPosition="2096" endWordPosition="2099">ith the list of translations in Japanese. Of these English words some 14,000 were proper nouns which were directly relevant to the vocabulary typically found in international news stories. Additionally we perform lexical normalisation before calculating the matching score and remove function words with a stop list. 4.2 Model 2: Byte-length ratios For Asian language pairs we cannot rely entirely on dictionary term matching. Moreover, algorithms which rely on matching cognates cannot be applied easily to English and some Asian language. We were motivated by statistical alignment models such as (Gale and Church, 1991) to investigate whether byte-length probabilities could improve or replace the lexical matching based method. The underlying assumption is that characters in an English sentence are responsible for generating some fraction of each character in the corresponding Japanese sentence. We derived a probability density function by making the assumption that English and Japanese sentence length ratios are normally distributed. The parameters required for the model are the mean, p and variance, a, which we calculated from a training set of 450 hand-aligned sentences. These are then entered into Equatio</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>W. Gale and K. Church. 1991. A program for aligning sentences in bilingual corpora. In Proceedings of the 29th Annual Conference of the Association for Computational Linguistics (ACL-91), Berkeley, California, pages 177-184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
</authors>
<title>A program for aligning sentences in a bilingual corpora.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="2357" citStr="Gale and Church, 1993" startWordPosition="345" endWordPosition="348">he analysis of bilingual texts such as extraction of bilingual vocabulary, extraction of translation templates, word sense disambiguation, word and phrase alignment, and extraction of parameters for statistical translation models. Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching. Various methods have been developed for sentence alignment which we can categorise as either lexical such as (Chen, 1993), based on a large-scale bilingual lexicon; statistical such as (Brown et al., 1991) (Church, 1993)(Gale and Church, 1993)(Kay and Roshcheisen, 1993), based on distributional regularities of words or byte-length ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (Utsuro et al., 1994) (Wu, 1994), based on some combination of the other two. Neither of the pure approaches is entirely satisfactory for the following reasons: • Text volume limits the usefulness of statistical approaches. We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties. • Bilingual dictionary coverage limitations mean that we </context>
</contexts>
<marker>Gale, Church, 1993</marker>
<rawString>W. Gale and K. Church. 1993. A program for aligning sentences in a bilingual corpora. Computational Linguistics, 19(1):75-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
<author>M Roshcheisen</author>
</authors>
<date>1993</date>
<booktitle>Text-translation alignment. Computational Linguistics,</booktitle>
<pages>19--121</pages>
<contexts>
<context position="2384" citStr="Kay and Roshcheisen, 1993" startWordPosition="348" endWordPosition="351">l texts such as extraction of bilingual vocabulary, extraction of translation templates, word sense disambiguation, word and phrase alignment, and extraction of parameters for statistical translation models. Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching. Various methods have been developed for sentence alignment which we can categorise as either lexical such as (Chen, 1993), based on a large-scale bilingual lexicon; statistical such as (Brown et al., 1991) (Church, 1993)(Gale and Church, 1993)(Kay and Roshcheisen, 1993), based on distributional regularities of words or byte-length ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (Utsuro et al., 1994) (Wu, 1994), based on some combination of the other two. Neither of the pure approaches is entirely satisfactory for the following reasons: • Text volume limits the usefulness of statistical approaches. We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties. • Bilingual dictionary coverage limitations mean that we will often encounter proble</context>
</contexts>
<marker>Kay, Roshcheisen, 1993</marker>
<rawString>M. Kay and M. Roshcheisen. 1993. Text-translation alignment. Computational Linguistics, 19:121-142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Utsuro</author>
<author>H Ikeda</author>
<author>M Yamane</author>
<author>Y Matsumoto</author>
<author>N Nagao</author>
</authors>
<title>Bilingual text matching using bilingual dictionary and statistics.</title>
<date>1994</date>
<booktitle>In COLING-94, 15th International Conference, Kyoto, Japan,</booktitle>
<volume>2</volume>
<pages>5--9</pages>
<contexts>
<context position="2552" citStr="Utsuro et al., 1994" startWordPosition="376" endWordPosition="379">r statistical translation models. Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching. Various methods have been developed for sentence alignment which we can categorise as either lexical such as (Chen, 1993), based on a large-scale bilingual lexicon; statistical such as (Brown et al., 1991) (Church, 1993)(Gale and Church, 1993)(Kay and Roshcheisen, 1993), based on distributional regularities of words or byte-length ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (Utsuro et al., 1994) (Wu, 1994), based on some combination of the other two. Neither of the pure approaches is entirely satisfactory for the following reasons: • Text volume limits the usefulness of statistical approaches. We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties. • Bilingual dictionary coverage limitations mean that we will often encounter problems establishing a correspondence in non-general domains. • Dictionary-based approaches are founded on an assumption of lexical correspondence between language pairs. We</context>
</contexts>
<marker>Utsuro, Ikeda, Yamane, Matsumoto, Nagao, 1994</marker>
<rawString>T. Utsuro, H. Ikeda, M. Yamane, Y. Matsumoto, and N. Nagao. 1994. Bilingual text matching using bilingual dictionary and statistics. In COLING-94, 15th International Conference, Kyoto, Japan, volume 2, August 5-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Aligning a parallel English-Chinese corpus statistically with lexical criteria.</title>
<date>1994</date>
<booktitle>In 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>80--87</pages>
<location>New Mexico, USA,</location>
<contexts>
<context position="2563" citStr="Wu, 1994" startWordPosition="380" endWordPosition="381">ion models. Many software products which aid human translators now contain sentence alignment tools as an aid to speeding up editing and terminology searching. Various methods have been developed for sentence alignment which we can categorise as either lexical such as (Chen, 1993), based on a large-scale bilingual lexicon; statistical such as (Brown et al., 1991) (Church, 1993)(Gale and Church, 1993)(Kay and Roshcheisen, 1993), based on distributional regularities of words or byte-length ratios and possibly inducing a bilingual lexicon as a by-product, or hybrid such as (Utsuro et al., 1994) (Wu, 1994), based on some combination of the other two. Neither of the pure approaches is entirely satisfactory for the following reasons: • Text volume limits the usefulness of statistical approaches. We would often like to be able to align small amounts of text, or texts from various domains which do not share the same statistical properties. • Bilingual dictionary coverage limitations mean that we will often encounter problems establishing a correspondence in non-general domains. • Dictionary-based approaches are founded on an assumption of lexical correspondence between language pairs. We cannot alw</context>
<context position="4204" citStr="Wu, 1994" startWordPosition="650" endWordPosition="651">of coverage and accuracy when we have a variety of text domains, text sizes and language pairs. In this paper we seek to fill a gap in our understanding and to show how the various components of the hybrid method influence the quality of sentence alignment for Japanese and English newspaper articles. 2 Bilingual Sentence Alignment The task of sentence alignment is to match corresponding sentences in a text from one language to sentences in a translation of that text in another language. Of particular interest to us is the application to Asian language pairs. Previous studies such as (Fung and Wu, 1994) have commented 268 that methods developed for Indo-European language pairs using alphabetic characters have not addressed important issues which occur with European-Asian language pairs. For example, the language pairs are unlikely to be cognates, and they may place sentence boundaries at different points in the text. It has also been suggested by (Wu, 1994) that sentence length ratio correlations may arise partly out of historic cognate-based relationships between Indo-European languages. Methods which perform well for IndoEuropean language pairs have therefore been found to be less effectiv</context>
<context position="16200" citStr="Wu, 1994" startWordPosition="2567" endWordPosition="2568"> all, of the observed scores. Although the curve in Figure 4 shows that our training set deviated from the normal distribution at • • -2 -a -4 0 0.2 0.4 0.6 0.0 1 1.2 1.4 1.5 1.5 2 nat. Figure 4: Sentence length ratio normal check curve 271 ; Figure 5: Sentence offsets in training set the extremes we nevertheless proceeded to continue with our simulations using this model considering that the deviations occured at the extreme ends of the distribution where relatively few samples were found. The weakness of this assumption however does add extra evidence to doubts which have been raised, e.g. (Wu, 1994), about whether the bytelength model by itself can perform well. 4.3 Model 3: Offset ratios We calculated the offsets in the sentence indexes for English and Japanese sentences in an alignment relation in the hand-aligned training set. An offset difference was calculated as the Japanese sentence index minus the English sentence index within a bilingual news article pair. The values are shown as a histogram in Figure 5. As with the byte-length ratio model, we started from an assumption that sentence correspondence offsets were normally distributed. We then calculated the mean and variance for o</context>
</contexts>
<marker>Wu, 1994</marker>
<rawString>D. Wu. 1994. Aligning a parallel English-Chinese corpus statistically with lexical criteria. In 32nd Annual Meeting of the Association for Computational Linguistics, New Mexico, USA, pages 80-87, June 27-30.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>