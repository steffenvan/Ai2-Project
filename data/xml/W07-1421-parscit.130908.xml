<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.997295">
Hypothesis Transformation and Semantic Variability Rules Used in
Recognizing Textual Entailment
</title>
<author confidence="0.986452">
Adrian Iftene
</author>
<affiliation confidence="0.987359">
„Al. I. Cuza“ University, Faculty of
Computer Science, Iasi, Romania
</affiliation>
<email confidence="0.840853">
adiftene@info.uaic.ro
</email>
<sectionHeader confidence="0.951913" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999916545454545">
Based on the core approach of the tree edit
distance algorithm, the system central mod-
ule is designed to target the scope of TE –
semantic variability. The main idea is to
transform the hypothesis making use of ex-
tensive semantic knowledge from sources
like DIRT, WordNet, Wikipedia, acronyms
database. Additionally, we built a system to
acquire the extra background knowledge
needed and applied complex grammar rules
for rephrasing in English.
</bodyText>
<sectionHeader confidence="0.999389" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999636866666667">
Many NLP applications need to recognize when
the meaning of one text can be expressed by, or
inferred from, another text. Information Retrieval
(IR), Question Answering (QA), Information Ex-
traction (IE), Text Summarization (SUM) are ex-
amples of applications that need to assess such a
semantic relationship between text segments. Tex-
tual Entailment Recognition (RTE) (Dagan et al.,
2006) has recently been proposed as an application
independent task to capture such inferences.
This year our textual entailment system partici-
pated for the first time in the RTE1 competition.
Next chapters present its main parts, the detailed
results obtained and some possible future im-
provements.
</bodyText>
<sectionHeader confidence="0.988345" genericHeader="method">
2 System description
</sectionHeader>
<bodyText confidence="0.9946555">
The process requires an initial pre-processing, fol-
lowed by the execution of a core module which
uses the output of the first phase and obtains in the
end the answers for all pairs. Figure 1 shows how
</bodyText>
<footnote confidence="0.836199">
1 http://www.pascal-network.org/Challenges/RTE3/
</footnote>
<author confidence="0.822162">
Alexandra Balahur-Dobrescu
</author>
<affiliation confidence="0.9542615">
„Al. I. Cuza“ University, Faculty of
Computer Science, Iasi, Romania
</affiliation>
<email confidence="0.968551">
abalahur@info.uaic.ro
</email>
<bodyText confidence="0.9975582">
the pre-processing is realized with the MINIPAR
(Lin, 1998) and LingPipe2 modules which provide
the input for the core module. This one uses four
databases: DIRT, Acronyms, Background knowl-
edge and WordNet.
</bodyText>
<figure confidence="0.990384277777778">
Dependency
trees for
(T, H) pairs
Named
entities for
(T, H) pairs
DIRT
Core
Module3
Core
Module2
Core
Module1
P2P
Computers
Wordnet
Final
result
</figure>
<figureCaption confidence="0.999993">
Figure 1: System architecture
</figureCaption>
<bodyText confidence="0.999832375">
The system architecture is based on a peer-to-
peer networks design, in which neighboring com-
puters collaborate in order to obtain the global fit-
ness for every text-hypothesis pair. Eventually,
based on the computed score, we decide for which
pairs we have entailment. This type of architecture
was used in order to increase the computation
speed.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="method">
3 Initial pre-processing
</sectionHeader>
<bodyText confidence="0.99947475">
The first step splits the initial file into pairs of files
for text and hypothesis. All these files are then sent
to the LingPipe module in order to find the Named
entities.
</bodyText>
<footnote confidence="0.782951">
2 http://www.alias-i.com/lingpipe/
</footnote>
<figure confidence="0.9945834">
Wikipedia
Initial
data
Background
knowledge
Acronyms
LingPipe
module
Minipar
module
</figure>
<page confidence="0.970125">
125
</page>
<bodyText confidence="0.863336333333333">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 125–130,
Prague, June 2007. c�2007 Association for Computational Linguistics
In parallel, we transform with MINIPAR both
the text and the hypothesis into dependency trees.
Figure 2 shows the output associated with the sen-
tence: “Le Beau Serge was directed by Chabrol.”.
</bodyText>
<figure confidence="0.581310666666667">
direct (V)
be by
Le—Beau—Serge (N) be (be) Chabrol
obj
Le—Beau—Serge (N)
Le (U) Beau (U)
</figure>
<figureCaption confidence="0.999016428571428">
Figure 2: MINIPAR output – dependency tree
For every node from the MINIPAR output, we
consider a stamp called entity with three main fea-
tures: the node lemma, the father lemma and the
edge label (which represents the relation between
words) (like in Figure 3).
Figure 3: Entity components
</figureCaption>
<bodyText confidence="0.991928833333333">
Using this stamp, we can easily distinguish be-
tween nodes of the trees, even if these have the
same lemma and the same father. In the example
from Figure 1, for the “son” nodes we have two
entities (Le—Beau—Serge, direct, s) and
(Le—Beau—Serge, direct, obj).
</bodyText>
<sectionHeader confidence="0.988605" genericHeader="method">
4 The hypothesis tree transformation
</sectionHeader>
<bodyText confidence="0.942142461538462">
Presently, the core of our approach is based on a
tree edit distance algorithm applied on the depend-
ency trees of both the text and the hypothesis
(Kouylekov, Magnini 2005). If the distance (i.e. the
cost of the editing operations) among the two trees
is below a certain threshold, empirically estimated
on the training data, then we assign an entailment
relation between the two texts.
The main goal is to map every entity in the de-
pendency tree associated with the hypothesis
(called from now on hypothesis tree) to an entity in
the dependency tree associated with the text (called
from now on text tree).
For every mapping we calculate a local fitness
value which indicates the appropriateness between
entities. Subsequently, the global fitness is calcu-
lated from these partial values.
For every node (refers to the word contained in
the node) which can be mapped directly to a node
from the text tree, we consider the local fitness
value to be 1. When we cannot map one word of
the hypothesis to one node from the text, we have
the following possibilities:
• If the word is a verb in the hypothesis tree, we
use the DIRT resource (Lin and Pantel, 2001)
in order to transform the hypothesis tree into an
equivalent one, with the same nodes except the
verb. Our aim in performing this transforma-
tion is to find a new value for the verb which
can be better mapped in the text tree.
• If the word is marked as named entity by Ling-
Pipe, we try to use an acronyms’ database3 or if
the word is a number we try to obtain informa-
tion related to it from the background knowl-
edge. In the event that even after these
operations we cannot map the word from the
hypothesis tree to one node from the text tree,
no fitness values are computed for this case
and we decide the final result: No entailment.
</bodyText>
<listItem confidence="0.575602">
• Else, we use WordNet (Fellbaum, 1998) to
look up synonyms for this word and try to map
them to nodes from the text tree.
</listItem>
<bodyText confidence="0.999939833333333">
Following this procedure, for every transforma-
tion with DIRT or WordNet, we consider for local
fitness the similarity value indicated by these re-
sources. If after all checks, one node from the hy-
pothesis tree cannot be mapped, some penalty is
inserted in the value of the node local fitness.
</bodyText>
<subsectionHeader confidence="0.990553">
4.1 The DIRT resource
</subsectionHeader>
<bodyText confidence="0.9968152">
For the verbs in the MINIPAR output, we extract
templates with DIRT- like format. For the sample
output in Figure 2, where we have a single verb
“direct”, we obtain the following list of “full” tem-
plates:N:s:V&lt;direct&gt;V:by:N and N:obj:V&lt;direct&gt;
V:by:N. To this list we add a list of “partial” tem-
plates: N:s:V&lt;direct&gt;V:, :V&lt;direct&gt;V:by:N,
:V&lt;direct&gt;V:by:N, and N:obj:V&lt;direct&gt;V:.
In the same way, we build a list with templates
for the verbs in the text tree. With these two lists
we perform a search in the DIRT database and ex-
tract the “best” trimming, considering the template
type (full or partial) and the DIRT score.
According to the search results, we have the fol-
lowing situations:
</bodyText>
<figure confidence="0.810901769230769">
3 http://www.acronym-guide.com
node lemma
edge label
father lemma
lex-mod
lex-mod
s
126
a) left – left relations similarity
This case is described by the following two tem-
plates for the hypothesis and the text:
relation] HypothesisVerb relation2
relation] TextVerb relation3
</figure>
<bodyText confidence="0.9995966">
This is the most frequent case, in which a verb is
replaced by one of its synonyms or equivalent ex-
pressions
The transformation of the hypothesis tree is done
in two steps:
</bodyText>
<listItem confidence="0.981097666666667">
1. Replace the relation2 with relation3,
2. Replace the verb from the hypothesis with
the corresponding verb from the text. (see
</listItem>
<figureCaption confidence="0.9673605">
Figure 4).
Figure 4: Left-left relation similarity
</figureCaption>
<bodyText confidence="0.8486628">
b) right – right relations similarity: the same
idea from the previous case.
c) left – right relations similarity
This case can be described by the following two
templates for the hypothesis and the text:
</bodyText>
<subsubsectionHeader confidence="0.4253285">
relation] HypothesisVerb relation2
relation3 TextVerb relation]
</subsubsectionHeader>
<bodyText confidence="0.98636">
The transformation of the hypothesis tree is:
</bodyText>
<listItem confidence="0.994059857142857">
1. Replace the relation2 with relation3,
2. Replace the verb from the hypothesis with
the corresponding verb from the text.
3. Rotate the subtrees accordingly: left sub-
tree will be right subtree and vice-versa
right subtree will become left-subtree (as it
can be observed in Figure 5).
</listItem>
<figureCaption confidence="0.990079">
Figure 5: Left-right relation similarity
</figureCaption>
<bodyText confidence="0.851048">
This case appears for pair 161 with the verb “at-
tack“:
T: “The demonstrators, convoked by the solidarity
with Latin America committee, verbally attacked
Salvadoran President Alfredo Cristiani.”
H: “President Alfredo Cristiani was attacked by
demonstrators.”
In this case, for the text we have the template
N:subj:V&lt;attack&gt;V:obj:N, and for the hypothesis
the template N:obj:V&lt;attack&gt;V:by:N. Using DIRT,
hypothesis H is transformed into:
H’: Demonstrators attacked President Alfredo
Cristiani.
Under this new form, H is easier comparable to T.
d) right – left relations similarity: the same
idea from the previous case
For every node transformed with DIRT, we con-
sider its local fitness as being the similarity value
indicated by DIRT.
</bodyText>
<subsectionHeader confidence="0.969252">
4.2 Extended WordNet
</subsectionHeader>
<bodyText confidence="0.99989725">
For non-verbs nodes from the hypothesis tree, if in
the text tree we do not have nodes with the same
lemma, we search for their synonyms in the ex-
tended WordNet4. For every synonym, we check to
see if it appears in the text tree, and select the map-
ping with the best value according to the values
from Extended WordNet. Subsequently, we change
the word from the hypothesis tree with the word
from WordNet and also its fitness with its indicated
similarity value. For example, the relation between
“relative” and “niece” is accomplished with a score
of 0.078652.
</bodyText>
<figure confidence="0.999087692307692">
HypothesisVerb
relation2
relation]
Left
Subtree
Right
Subtree
TextVerb
relation] relation3
Left
Subtree
Right
Subtree
HypothesisVerb
relation2
relation]
TextVerb relation]
relation3
Left
Subtree
Right
Subtree
Right
Subtree
Left
Subtree
</figure>
<footnote confidence="0.707967">
4 http://xwn.hlt.utdallas.edu/downloads.html
</footnote>
<page confidence="0.955386">
127
</page>
<subsectionHeader confidence="0.991118">
4.3 Acronyms
</subsectionHeader>
<bodyText confidence="0.999734142857143">
The acronyms’ database helps our program find
relations between the acronym and its meaning:
“US - United States”, and “EU - European Union”.
We change the word with the corresponding ex-
pression from this database. Since the meaning is
the same, the local fitness is considered maximum,
i.e. 1.
</bodyText>
<subsectionHeader confidence="0.998972">
4.4 Background Knowledge
</subsectionHeader>
<bodyText confidence="0.99819225">
Some information cannot be deduced from the al-
ready used databases and thus we require addi-
tional means of gathering extra information of the
form:
</bodyText>
<table confidence="0.982891">
Argentine [is] Argentina
Netherlands [is] Holland
2 [is] two
Los Angeles [in] California
Chinese [in] China
</table>
<tableCaption confidence="0.99917">
Table 1: Background knowledge
</tableCaption>
<bodyText confidence="0.9936178">
Background knowledge was built semi-
automatically, for the named entities (NEs) and for
numbers from the hypothesis without correspon-
dence in the text. For these NEs, we used a module
to extract from Wikipedia5 snippets with informa-
tion related to them. Subsequently, we use this file
with snippets and some previously set patterns of
relations between NEs, with the goal to identify a
known relation between the NE for which we have
a problem and another NE.
If such a relation is found, we save it to an out-
put file. Usually, not all relations are correct, but
those that are will help us at the next run.
Our patterns identify two kinds of relations be-
tween words:
</bodyText>
<listItem confidence="0.9919303">
• “is”, when the module extracts information of
the form: ‘Argentine Republic’ (Spanish: &apos;Re-
publica Argentina&apos;, IPA)’ or when explanations
about the word are given in brackets, or when
the extracted information contains one verb
used to define something, like “is”, “define”,
“represent”: &apos;2&apos; (&apos;two&apos;) is a number.
• “in” when information is of the form: &apos;Chinese&apos;
refers to anything pertaining to China or in the
form Los Angeles County, California, etc.
</listItem>
<bodyText confidence="0.999965">
In this case, the local fitness for the node is set to
the maximum value for the [is]-type relations, and
it receives some penalties for the [in]-type relation.
</bodyText>
<sectionHeader confidence="0.965776" genericHeader="method">
5 Determination of entailment
</sectionHeader>
<bodyText confidence="0.998195">
After transforming the hypothesis tree, we calcu-
late a global fitness score using the extended local
fitness value for every node from the hypothesis -
which is calculated as sum of the following values:
</bodyText>
<listItem confidence="0.879981875">
1. local fitness obtained after the tree trans-
formation and node mapping,
2. parent fitness after parent mapping,
3. mapping of the node edge label from the
hypothesis tree onto the text tree,
4. node position (left, right) towards its father
in the hypothesis and position of the map-
ping nodes from the text.
</listItem>
<bodyText confidence="0.999955888888889">
After calculating this extended local fitness score,
the system computes a total fitness for all the nodes
in the hypothesis tree and a negation value associ-
ated to the hypothesis tree. Tests have shown that
out of these parameters, some are more important
(the parameter at 1.) and some less (the parameter
at 3.). Below you can observe an example of how
the calculations for 3 and 4 are performed and what
the negation rules are.
</bodyText>
<subsectionHeader confidence="0.983865">
5.1 Edge label mapping
</subsectionHeader>
<bodyText confidence="0.999735714285714">
After the process of mapping between nodes, we
check how edge labels from the hypothesis tree are
mapped onto the text tree. Thus, having two adja-
cent nodes in the hypothesis, which are linked by
an edge with a certain label, we search on the path
between the nodes’ mappings in the text tree this
label. (see Figure 6)
</bodyText>
<figure confidence="0.84080175">
father
mapping
edge label
mapping
node
mapping
Hypothesis tree
Text tree
</figure>
<figureCaption confidence="0.99548">
Figure 6: Entity mapping
</figureCaption>
<footnote confidence="0.922526">
5 http://en.wikipedia.org/wiki/Main_Page
</footnote>
<page confidence="0.994432">
128
</page>
<bodyText confidence="0.9999775">
It is possible that more nodes until the label of the
edge linking the nodes in the hypothesis exist, or it
is possible that this label is not even found on this
path. According to the distance or to the case in
which the label is missing, we insert some penalties
in the extended local fitness.
</bodyText>
<subsectionHeader confidence="0.999706">
5.2 Node position
</subsectionHeader>
<bodyText confidence="0.9978525">
After mapping the nodes, one of the two following
possible situations may be encountered:
</bodyText>
<listItem confidence="0.997882875">
• The position of the node towards its father and
the position of the mapping node towards its
father’s mapping are the same (left-left or
right-right). In this case, the extended local fit-
ness is incremented.
• The positions are different (left-right or right-
left) and in this case a penalty is applied ac-
cordingly.
</listItem>
<subsectionHeader confidence="0.999361">
5.3 Negation rules
</subsectionHeader>
<bodyText confidence="0.999994153846154">
For every verb from the hypothesis we consider a
Boolean value which indicates whether the verb
has a negation or not, or, equivalently, if it is re-
lated to a verb or adverb “diminishing” its sense or
not. Consequently, we check in its tree on its de-
scending branches to see whether one or more of
the following words are to be found (pure form of
negation or modal verb in indicative or conditional
form): “not, may, might, cannot, should, could,
etc.”. For each of these words we successively ne-
gate the initial truth value of the verb, which by
default is “false”. The final value depends on the
number of such words.
Since the mapping is done for all verbs in the
text and hypothesis, regardless of their original
form in the snippet, we also focused on studying
the impact of the original form of the verb on its
overall meaning within the text. Infinitives can be
identified when preceded by the particle “to”. Ob-
serving this behavior, one complex rule for nega-
tion was built for the particle “to” when it precedes
a verb. In this case, the sense of the infinitive is
strongly influenced by the active verb, adverb or
noun before the particle “to”, as follows: if it is
being preceded by a verb like “allow, impose, gal-
vanize” or their synonyms, or adjective like “nec-
essary, compulsory, free” or their synonyms or
noun like “attempt”, “trial” and their synonyms, the
meaning of the verb in infinitive form is stressed
upon and becomes “certain”. For all other cases,
the particle “to” diminish the certainty of the action
expressed in the infinitive-form verb. Based on the
synonyms database with the English thesaurus6, we
built two separate lists – one of “certainty stressing
(preserving)” – “positive” and one of “certainty
diminishing” – “negative” words. Some examples
of these words are “probably”, “likely” – from the
list of “negative” words and “certainly”, “abso-
lutely” – from the list of “positive” words.
</bodyText>
<subsectionHeader confidence="0.981084">
5.4 Global fitness calculation
</subsectionHeader>
<bodyText confidence="0.9999894">
We calculate for every node from the hypothesis
tree the value of the extended local fitness, and af-
terwards consider the normalized value relative to
the number of nodes from the hypothesis tree. We
denote this result by TF (total fitness):
</bodyText>
<equation confidence="0.889651666666667">
ExtendedLocalFitness
TF = nodeEH
HypothesisNodesNumber
</equation>
<bodyText confidence="0.99928125">
After calculating this value, we compute a value
NV (the negation value) indicating the number of
verbs with the same value of negation, using the
following formula:
</bodyText>
<equation confidence="0.895059333333333">
Positive _
NV =
TotalNumberOfVerbs
</equation>
<bodyText confidence="0.960120375">
where the Positive_VerbsNumber is the number of
non-negated verbs from the hypothesis using the
negation rules, and TotalNumberOfVerbs is the
total number of verbs from the hypothesis.
Because the maximum value for the extended
fitness is 4, the complementary value of the TF is
4-TF and the formula for the global fitness used is:
GlobalFitness NV * TF (1 NV) * (4 TF)
</bodyText>
<equation confidence="0.559511">
= + − −
</equation>
<table confidence="0.963176363636364">
For pair 518 we have the following:
Initial entity Node Extended
Fitness local fitness
(the, company, det) 1 3.125
(French, company, nn) 1 3.125
(railway, company, nn) 1 3.125
(company, call, s) 1 2.5
(be, call, be) 1 4
(call, -, -) 0.096 3.048
(company, call, obj) 1 1.125
(SNCF, call, desc) 1 2.625
</table>
<tableCaption confidence="0.999473">
Table 2: Entities extended fitness
</tableCaption>
<figure confidence="0.848524333333333">
6 http://thesaurus.reference.com/
node
VerbsNumber
</figure>
<page confidence="0.955777">
129
</page>
<equation confidence="0.99697925">
TF = (3.125 + 3.125 + 3.125 + 2.5 + 4 + 3.048 +
1.125 + 2.625)/8 = 22.673/8 = 2.834
NV = 1/1 = 1
GlobalFitness = 1*2.834+(1–1)*(4-2.834) = 2.834
</equation>
<bodyText confidence="0.999896">
Using the development data, we establish a
threshold value of 2.06. Thus, pair 518 will have
the answer “yes”.
</bodyText>
<sectionHeader confidence="0.999979" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999974714285714">
Our system has a different behavior on different
existing tasks, with higher results on Question An-
swering (0.87) and lower results on Information
Extraction (0.57). We submitted two runs for our
system, with different parameters used in calculat-
ing the extended local fitness. However, the results
are almost the same (see Table 3).
</bodyText>
<table confidence="0.991863333333333">
IE IR QA SUM Global
Run01 0.57 0.69 0.87 0.635 0.6913
Run02 0.57 0.685 0.865 0.645 0.6913
</table>
<tableCaption confidence="0.99911">
Table 3: Test results
</tableCaption>
<bodyText confidence="0.9985978">
To be able to see each component’s relevance, the
system was run in turn with each component re-
moved. The results in the table below show that the
system part verifying the NEs is the most impor-
tant.
</bodyText>
<table confidence="0.996949142857143">
System Description Precision Relevance
Without DIRT 0.6876 0.54 %
Without WordNet 0.6800 1.63 %
Without Acronyms 0.6838 1.08 %
Without BK 0.6775 2.00 %
Without Negations 0.6763 2.17 %
Without NEs 0.5758 16.71 %
</table>
<tableCaption confidence="0.998696">
Table 4: Components relevance
</tableCaption>
<sectionHeader confidence="0.999325" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999989545454545">
The system’s core algorithm is based on the tree
edit distance approach, however, focused on trans-
forming the hypothesis. It presently uses wide-
spread syntactic analysis tools like Minipar, lexical
resources like WordNet and LingPipe for Named
Entities recognition and semantic resources like
DIRT. The system’s originality resides firstly in
creating a part-of and equivalence ontology using
an extraction module for Wikipedia data on NEs
(the background knowledge), secondly in using a
distinct database of acronyms from different do-
mains, thirdly acquiring a set of important context
influencing terms and creating a semantic equiva-
lence set of rules based on English rephrasing con-
cepts and last, but not least, on the technical side,
using a distributed architecture for time perform-
ance enhancement.
The approach unveiled some issues related to the
dependency to parsing tools, for example separat-
ing the verb and the preposition in the case of
phrasal verbs, resulting in the change of meaning.
Another issue was identifying expressions that
change context nuances, which we denoted by
“positive” or “negative” words. Although we ap-
plied rules for them, we still require analysis to
determine their accurate quantification.
For the future, our first concern is to search for a
method to establish more precise values for penal-
ties, in order to obtain lower values for pairs with
No entailment. Furthermore, we will develop a new
method to determine the multiplication coefficients
for the parameters in the extended local fitness and
the global threshold.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="acknowledgments">
8 Acknowledgements
</sectionHeader>
<bodyText confidence="0.99997125">
The authors thank the members of the NLP group
in Iasi for their help and support at different stages
of the system development. Special thanks go to
Daniel Matei which was responsible for preparing
all the input data.
The work on this project is partially financed by
Siemens VDO Iasi and by the CEEX Rotel project
number 29.
</bodyText>
<sectionHeader confidence="0.999553" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999666166666667">
Dagan, I., Glickman, O., and Magnini, B. 2006. The
PASCAL Recognising Textual Entailment Challenge.
In Quiñonero-Candela et al., editors, MLCW 2005,
LNAI Volume 3944, pages 177-190. Springer-Verlag.
Fellbaum, C. 1998. WordNet: An Electronic Lexical
Database. MIT Press, Cambridge, Mass.
Kouylekov, M. and Magnini, B. 2005. Recognizing Tex-
tual Entailment with Tree Edit Distance Algorithms.
In Proceedings of the First Challenge Workshop Rec-
ognising Textual Entailment, Pages 17-20, 25–28
April, 2005, Southampton, U.K.
Lin, D. 1998. Dependency-based Evaluation of
MINIPAR. In Workshop on the Evaluation of Parsing
Systems, Granada, Spain, May, 1998.
Lin, D., and Pantel, P. 2001. DIRT - Discovery of Infer-
ence Rules from Text. In Proceedings of ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD-01). pp. 323-328. San Francisco, CA.
</reference>
<page confidence="0.997616">
130
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.657477">
<title confidence="0.9992255">Hypothesis Transformation and Semantic Variability Rules Used Recognizing Textual Entailment</title>
<author confidence="0.9359695">I Cuza“ University</author>
<author confidence="0.9359695">Faculty of</author>
<affiliation confidence="0.758174">Computer Science, Iasi, Romania</affiliation>
<email confidence="0.960401">adiftene@info.uaic.ro</email>
<abstract confidence="0.996885916666667">Based on the core approach of the tree edit distance algorithm, the system central module is designed to target the scope of TE – semantic variability. The main idea is to transform the hypothesis making use of extensive semantic knowledge from sources like DIRT, WordNet, Wikipedia, acronyms database. Additionally, we built a system to acquire the extra background knowledge needed and applied complex grammar rules for rephrasing in English.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>I Dagan</author>
<author>O Glickman</author>
<author>B Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>MLCW 2005, LNAI Volume 3944,</booktitle>
<pages>177--190</pages>
<editor>In Quiñonero-Candela et al., editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1057" citStr="Dagan et al., 2006" startWordPosition="153" endWordPosition="156">c knowledge from sources like DIRT, WordNet, Wikipedia, acronyms database. Additionally, we built a system to acquire the extra background knowledge needed and applied complex grammar rules for rephrasing in English. 1 Introduction Many NLP applications need to recognize when the meaning of one text can be expressed by, or inferred from, another text. Information Retrieval (IR), Question Answering (QA), Information Extraction (IE), Text Summarization (SUM) are examples of applications that need to assess such a semantic relationship between text segments. Textual Entailment Recognition (RTE) (Dagan et al., 2006) has recently been proposed as an application independent task to capture such inferences. This year our textual entailment system participated for the first time in the RTE1 competition. Next chapters present its main parts, the detailed results obtained and some possible future improvements. 2 System description The process requires an initial pre-processing, followed by the execution of a core module which uses the output of the first phase and obtains in the end the answers for all pairs. Figure 1 shows how 1 http://www.pascal-network.org/Challenges/RTE3/ Alexandra Balahur-Dobrescu „Al. I.</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Dagan, I., Glickman, O., and Magnini, B. 2006. The PASCAL Recognising Textual Entailment Challenge. In Quiñonero-Candela et al., editors, MLCW 2005, LNAI Volume 3944, pages 177-190. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<contexts>
<context position="5640" citStr="Fellbaum, 1998" startWordPosition="918" endWordPosition="919">t one, with the same nodes except the verb. Our aim in performing this transformation is to find a new value for the verb which can be better mapped in the text tree. • If the word is marked as named entity by LingPipe, we try to use an acronyms’ database3 or if the word is a number we try to obtain information related to it from the background knowledge. In the event that even after these operations we cannot map the word from the hypothesis tree to one node from the text tree, no fitness values are computed for this case and we decide the final result: No entailment. • Else, we use WordNet (Fellbaum, 1998) to look up synonyms for this word and try to map them to nodes from the text tree. Following this procedure, for every transformation with DIRT or WordNet, we consider for local fitness the similarity value indicated by these resources. If after all checks, one node from the hypothesis tree cannot be mapped, some penalty is inserted in the value of the node local fitness. 4.1 The DIRT resource For the verbs in the MINIPAR output, we extract templates with DIRT- like format. For the sample output in Figure 2, where we have a single verb “direct”, we obtain the following list of “full” template</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kouylekov</author>
<author>B Magnini</author>
</authors>
<title>Recognizing Textual Entailment with Tree Edit Distance Algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of the First Challenge Workshop Recognising Textual Entailment,</booktitle>
<pages>17--20</pages>
<location>Southampton, U.K.</location>
<marker>Kouylekov, Magnini, 2005</marker>
<rawString>Kouylekov, M. and Magnini, B. 2005. Recognizing Textual Entailment with Tree Edit Distance Algorithms. In Proceedings of the First Challenge Workshop Recognising Textual Entailment, Pages 17-20, 25–28 April, 2005, Southampton, U.K.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based Evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems,</booktitle>
<location>Granada, Spain,</location>
<contexts>
<context position="1800" citStr="Lin, 1998" startWordPosition="264" endWordPosition="265">rticipated for the first time in the RTE1 competition. Next chapters present its main parts, the detailed results obtained and some possible future improvements. 2 System description The process requires an initial pre-processing, followed by the execution of a core module which uses the output of the first phase and obtains in the end the answers for all pairs. Figure 1 shows how 1 http://www.pascal-network.org/Challenges/RTE3/ Alexandra Balahur-Dobrescu „Al. I. Cuza“ University, Faculty of Computer Science, Iasi, Romania abalahur@info.uaic.ro the pre-processing is realized with the MINIPAR (Lin, 1998) and LingPipe2 modules which provide the input for the core module. This one uses four databases: DIRT, Acronyms, Background knowledge and WordNet. Dependency trees for (T, H) pairs Named entities for (T, H) pairs DIRT Core Module3 Core Module2 Core Module1 P2P Computers Wordnet Final result Figure 1: System architecture The system architecture is based on a peer-topeer networks design, in which neighboring computers collaborate in order to obtain the global fitness for every text-hypothesis pair. Eventually, based on the computed score, we decide for which pairs we have entailment. This type </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. 1998. Dependency-based Evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems, Granada, Spain, May, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>DIRT - Discovery of Inference Rules from Text.</title>
<date>2001</date>
<booktitle>In Proceedings of ACM Conference on Knowledge Discovery and Data Mining (KDD-01).</booktitle>
<pages>323--328</pages>
<location>San Francisco, CA.</location>
<contexts>
<context position="4965" citStr="Lin and Pantel, 2001" startWordPosition="786" endWordPosition="789">dependency tree associated with the text (called from now on text tree). For every mapping we calculate a local fitness value which indicates the appropriateness between entities. Subsequently, the global fitness is calculated from these partial values. For every node (refers to the word contained in the node) which can be mapped directly to a node from the text tree, we consider the local fitness value to be 1. When we cannot map one word of the hypothesis to one node from the text, we have the following possibilities: • If the word is a verb in the hypothesis tree, we use the DIRT resource (Lin and Pantel, 2001) in order to transform the hypothesis tree into an equivalent one, with the same nodes except the verb. Our aim in performing this transformation is to find a new value for the verb which can be better mapped in the text tree. • If the word is marked as named entity by LingPipe, we try to use an acronyms’ database3 or if the word is a number we try to obtain information related to it from the background knowledge. In the event that even after these operations we cannot map the word from the hypothesis tree to one node from the text tree, no fitness values are computed for this case and we deci</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Lin, D., and Pantel, P. 2001. DIRT - Discovery of Inference Rules from Text. In Proceedings of ACM Conference on Knowledge Discovery and Data Mining (KDD-01). pp. 323-328. San Francisco, CA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>