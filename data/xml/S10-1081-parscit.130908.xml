<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.035936">
<title confidence="0.998703">
Duluth-WSI: SenseClusters Applied to the
Sense Induction Task of SemEval-2
</title>
<author confidence="0.999037">
Ted Pedersen
</author>
<affiliation confidence="0.999321">
Department of Computer Science
University of Minnesota, Duluth
</affiliation>
<address confidence="0.583259">
Duluth, MN 55812
</address>
<email confidence="0.9224065">
tpederse@d.umn.edu
http://senseclusters.sourceforge.net
</email>
<sectionHeader confidence="0.989318" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909833333333">
The Duluth-WSI systems in SemEval-2
built word co–occurrence matrices from
the task test data to create a second order
co–occurrence representation of those test
instances. The senses of words were in-
duced by clustering these instances, where
the number of clusters was automatically
predicted. The Duluth-Mix system was a
variation of WSI that used the combina-
tion of training and test data to create the
co-occurrence matrix. The Duluth-R sys-
tem was a series of random baselines.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894384615385">
The Duluth systems in the sense induction task
of SemEval-2 (Manandhar et al., 2010) were
based on SenseClusters (v1.01), a freely available
open source software package which relies on the
premise that words with similar meanings will oc-
cur in similar contexts (Purandare and Pedersen,
2004). The data for the sense induction task in-
cluded 100 ambiguous words made up of 50 nouns
and 50 verbs. There were a total of 8,915 test in-
stances and 879,807 training instances provided.
Note that neither the training nor the test data was
sense tagged. The training data was made avail-
able as a resource for participants, with the under-
standing that system evaluation would be done on
the test instances only. The organizers held back a
gold standard annotation of the test data that was
only used for evaluation.
Five Duluth-WSI systems participated in this
task, six Duluth-Mix systems, and five Duluth
Random systems. The WSI and Mix systems al-
most always represented the test instances using
second order co–occurrences, where each word in
a test instance is replaced by a vector that shows
the words with which it co-occurs. The word vec-
tors that make up a test instance are averaged to-
gether to make up a new representation for that
instance. All the test instances for a word are clus-
tered, and the number of senses is automatically
predicted by either the PK2 measure or Adapted
Gap Statistic (Pedersen and Kulkarni, 2006).
In the Duluth systems the co-occurrence matri-
ces are either based on order-dependent bigrams
or unordered pairs of words, both of which can be
separated by up to some given number of interven-
ing words. Bigrams are used to preserve distinc-
tions between collocations such as cat house and
house cat, whereas co–occurrences do not con-
sider order and would treat these two as being
equivalent.
</bodyText>
<sectionHeader confidence="0.991416" genericHeader="method">
2 Duluth-WSI systems
</sectionHeader>
<bodyText confidence="0.99883925">
The Duluth-WSI systems build co-occurrence ma-
trices from the test data by identifying bigrams or
co–occurrences that occur with up to eight inter-
mediate words between them in instances of am-
biguous nouns, and up to 23 intermediate words
for the verbs. Any bigram (bi) or co–occurrence
(co) that occurs more than 5 times with up to the
allowed number of intervening words and has sta-
tistical significance of 0.95 or above according to
the left-sided Fisher’s exact test was selected (Ped-
ersen et al., 1996). Some of the WSI systems re-
duce the co–occurrence matrix to 300 dimensions
using Singular Value Decomposition (SVD).
The resulting co-occurrence matrix was used to
create second order co–occurrence vectors to rep-
resent the test instances, which were clustered us-
ing the method of repeated bisections (rb), where
similarity was measured using the cosine. Table
1 summarizes the distinctions between the various
Duluth-WSI systems.
</bodyText>
<sectionHeader confidence="0.997314" genericHeader="method">
3 Duluth-Mix systems
</sectionHeader>
<bodyText confidence="0.99147">
The Duluth-Mix systems used the combination of
the test and training data to identify features to rep-
resent the test instances. The goal of this combi-
</bodyText>
<page confidence="0.99146">
363
</page>
<note confidence="0.7340705">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 363–366,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<tableCaption confidence="0.995905">
Table 1: Duluth-WSI Distinctions
</tableCaption>
<bodyText confidence="0.997500217391304">
nation was to increase the amount of data that was
available for feature identification. Since there
was a larger amount of data, some parameter set-
tings as used in Duluth-WSI were reduced.
For example, the Duluth-Mix-PK2 and Duluth-
Mix-Gap are identical to the Duluth-WSI and
Duluth-WSI-Gap systems, except that they limit
both nouns and verbs to 8 intervening words.
Duluth-Mix-Narrow-PK2 and Duluth-Mix-
Narrow-Gap are identical to Duluth-Mix-PK2
and Duluth-Mix-Gap except that bigrams and
co–occurrences must be made up of adjacent
words, with no intermediate words allowed.
Duluth-Mix-Uni-PK2 and Duluth-Mix-Uni-
Gap are unique among the Duluth systems in
that they do not use second order co-occurrences,
but instead rely on first order co-occurrences.
These are simply individual words (unigrams) that
occur more than 5 times in the combined test and
training data. These features are used to generate
co-occurrence vectors for the test instances which
are then clustered (this is very similar to a bag of
words model).
</bodyText>
<sectionHeader confidence="0.996182" genericHeader="method">
4 Duluth-Random systems
</sectionHeader>
<bodyText confidence="0.997321875">
Duluth-R12, Duluth-R13, Duluth-R15, and
Duluth-R110 provide random baselines. R12
randomly assigns each instance to one of two
senses, R13 to one of three, R15 to one of five,
and R110 to one of ten senses. Random numbers
are generated in the given range with equal
probability, so the distribution of assigned senses
is balanced.
</bodyText>
<sectionHeader confidence="0.99901" genericHeader="evaluation">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999162">
The evaluation of unsupervised sense discrimina-
tion and induction systems is still not standard-
ized, so an important part of any exercise like
SemEval-2 is to scrutinize the evaluation measures
used in order to determine to what degree they are
providing a useful and reasonable way of evaluat-
ing system results.
</bodyText>
<subsectionHeader confidence="0.986">
5.1 Evaluation Measures
</subsectionHeader>
<bodyText confidence="0.999955259259259">
Each participating system was scored by three dif-
ferent evaluation methods: the V-measure (Rosen-
berg and Hirschberg, 2007), the supervised recall
measure (Agirre and Soroa, 2007), and the paired
F-score (Artiles et al., 2009). The results of the
evaluation are in some sense confusing - a sys-
tem that ranks near the top according to one mea-
sure may rank at the bottom or middle of another.
There was not any single system that did well ac-
cording to all of the different measures. The sit-
uation is so extreme that in some cases a system
would perform near the top in one measure, and
then below random baselines in another. These
stark differences suggest a real need for continued
development of other methods for evaluating un-
supervised sense induction.
One minimum expectation of an evaluation
measure is that it should expose and identify ran-
dom baselines by giving them low scores that
clearly distinguish them from actual participating
systems. The scores of all the evaluation mea-
sures used in this task when applied to different
random baseline systems are summarized in Table
2. These include a number of post-evaluation ran-
dom clustering systems, which are referred to as
post-R1k, where k is the number of random clus-
ters.
</bodyText>
<subsectionHeader confidence="0.520893">
5.1.1 V-measure
</subsectionHeader>
<bodyText confidence="0.999759736842105">
The V-measure appears to be quite easily mislead
by random baselines. As evidence of that, the
Duluth-R (random) systems got increasingly bet-
ter scores the more random they became, and in
fact the post-evaluation random systems reached
levels of performance better than any of the partic-
ipating systems. Table 2 shows that the V-measure
continues to improve (rather dramatically) as ran-
domness increases.
The average number of senses in the gold stan-
dard data for all 100 words was 3.79. The offi-
cial random baseline assigned one of four random
senses to each instance of a word, and achieved
a V-measure of 4.40. Duluth-R15 improved the
V-measure to 5.30 by assigning one of five ran-
dom senses, and Duluth-R110 improved it again
to 8.60 by assigning one of ten random senses.
The more random the result, the better the score.
In fact Duluth-R110 placed sixth in the sense in-
</bodyText>
<figure confidence="0.681703571428572">
name
options
Duluth-WSI
Duluth-WSI-Gap
Duluth-WSI-SVD
Duluth-WSI-Co
Duluth-WSI-Co-Gap
</figure>
<table confidence="0.7403998">
bigrams, no SVD, PK2
bigrams, no SVD, Gap
bigrams, SVD, PK2
co-occur, no SVD, PK2
co-occur, no SVD, Gap
</table>
<page confidence="0.996226">
364
</page>
<bodyText confidence="0.999832">
duction task according to the V-measure. In post-
evaluation experiments a number of additional
random baselines were explored, where instances
were assigned senses randomly from 20, 33, and
50 possible values per word. The V-measures for
these random systems were 13.9, 18.7, and 23.2
respectively, where the latter two were better than
the first place participating system (which scored
16.2). In a post-evaluation experiment, the task
organizers found that assigning one sense per in-
stance resulted in a V–measure of 31.7.
</bodyText>
<subsubsectionHeader confidence="0.786309">
5.1.2 Supervised Recall
</subsubsectionHeader>
<bodyText confidence="0.999956256410256">
The supervised recall measure takes the sense in-
duction results (on the 8,915 test instances) as sub-
mitted by a participating system and splits that into
a training and test portion for supervised learning.
The recall attained on the test split by a classifier
learned on the training split becomes the measure
of the unsupervised system. Two different splits
were used, with 80% or 60% of the test instances
for training, and the remainder for testing.
This evaluation method was also used in
SemEval-1, where (Pedersen, 2007) noted that it
seemed to compress the results of all the systems
into a narrow band that converged around the Most
Frequent Sense result. The same appears to have
happened in 2010. The supervised recall of the
Most Frequent Sense baseline (MFS) is 58 or .59
(depending on the split), and the majority of par-
ticipating systems (and even some of the random
baselines) fall in a range of scores from .56 to .62
(a band of .06). This blurs distinctions among par-
ticipating systems with each other and with ran-
dom baselines.
The number of senses actually assigned by the
classifier learned from the training split to the in-
stances in the test split is quite small, regardless of
the number of senses discovered by the participat-
ing system. There were at most 2.06 senses identi-
fied per word based on the 80-20 split, and at most
2.27 senses per word based on the 60-40 split.
For most systems, regardless of their underlying
methodology, the number of senses the classifier
actually assigns is approximately 1.5 per word.
This shows that the supervised learning algorithm
that underlies this evaluation method gravitates to-
wards a very small number of senses and there-
fore tends to converge on the MFS baseline. This
could be caused by noise in the induced senses,
a small number of examples in the training split
for a sense, or it may be that the supervised recall
</bodyText>
<tableCaption confidence="0.99828">
Table 2: Evaluation of Random Systems
</tableCaption>
<table confidence="0.999116">
name k V F 60-40 80-20
MFS 1 0.0 63.4 58.3 58.7
Duluth-R12 2 2.3 47.8 57.7 58.5
Duluth-R13 3 3.6 38.4 57.6 58.0
Random 4 4.4 31.9 56.5 57.3
Duluth-R15 5 5.3 27.6 56.5 56.8
Duluth-R110 10 8.6 16.1 53.6 54.8
post-R120 20 13.9 7.5 46.2 48.6
post-R133 33 18.7 4.0 38.3 42.5
post-R150 50 23.2 2.3 30.0 34.2
</table>
<bodyText confidence="0.992782">
measure is making different distinctions than are
found by the unsupervised sense induction method
it seeks to evaluate.
</bodyText>
<subsubsectionHeader confidence="0.87761">
5.1.3 Paired F-score
</subsubsectionHeader>
<bodyText confidence="0.999942206896552">
The paired F-score was the only evaluation mea-
sure that seemed able to identify and expose ran-
dom baselines. Duluth-R110 was by far the most
random of the officially participating systems, and
it was by far the lowest ranked system according
to the paired F-score, which assigned it a score of
16.1. All the Duluth-R systems ranked relatively
low (20th or below). When presented with the 20,
33, and 50 random sense post–evaluation systems,
the F-score assigned those scores of 7.46, 4.00,
and 2.33, which placed them far below any of the
other systems.
However, the paired F-score also showed that
the Most Frequent Sense baseline outperformed
all of the participating systems. The systems that
scored close to the MFS tended to predict very
small numbers of senses, and so were in effect act-
ing much like the MFS baseline themselves. The
F-score is not bounded by MFS and in fact it is
possible (theoretically) to reach a score of 1.00
with a perfect assignment of instances to senses.
The lesson learned in this task is that it would have
been more effective to simply assume that there
was just one sense per word, rather than using the
senses induced by participating systems. While
this may be a frustrating conclusion, in fact it is
a reasonable observation given that in many do-
mains a single sense for a given word can tend to
dominate.
</bodyText>
<subsectionHeader confidence="0.99197">
5.2 Duluth-WSI and Duluth-Mix Results
</subsectionHeader>
<bodyText confidence="0.993207">
The Duluth-WSI systems used the test data to
build co-occurrence matrices, while the Duluth-
</bodyText>
<page confidence="0.997814">
365
</page>
<bodyText confidence="0.999971418604651">
Mix systems used both the training and test
data. Within those frameworks bigrams or co–
occurrences were used to represent features, the
number of senses was automatically discovered
with the PK2 measure or the Adapted Gap Statis-
tic, and SVD was optionally used to reduce the
dimensionality of the resulting matrix. Previous
studies using SenseClusters have noted that the
Adapted Gap Statistic tends to find a relatively
small number of clusters, and that SVD typically
does not help to improve results of unsupervised
sense induction. These findings were again con-
firmed in this task.
Mixing together all of the training and test data
for building the co–occurrence matrices was no
more effective than just using the test data. How-
ever, the Duluth-Mix systems did not finish be-
fore the end of the evaluation period. The Duluth-
Mix-Narrow-Gap and PK2 systems were able to
finish 8,211 of the 8,915 test instances (92%),
the Duluth-Mix-Gap and PK2 systems completed
7,417 instances (83%), and Duluth-Mix-Uni-PK2
and Gap systems completed 2,682 of these in-
stances (30%). While these are partial results they
seem sufficient to support this conclusion.
To be usable in practical settings, an unsuper-
vised sense induction system should discover the
number of senses accurately and automatically.
Duluth-WSI and Duluth-WSI-SVD were very suc-
cessful in that regard, and predicted 4.15 senses on
average per word (with the PK2 measure) while
the actual number of senses was 3.79.
The Duluth-WSI systems are direct descen-
dents of UMND2 which participated in SemEval-
1 (Pedersen, 2007), where Duluth-WSI-Gap is
the closest relative. However, UMND2 used
Pointwise Mutual Information (PMI) rather than
Fisher’s left sided test, and it performed clustering
with k-means rather than the method of repeated
bisections. Both UMND2 and Duluth-WSI-Gap
used the Adapted Gap Statistic, and interestingly
enough both discovered approximately 1.4 senses
on average per word.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999963153846154">
The SemEval-2 sense induction task was an oppor-
tunity to compare participating systems with each
other, and also to analyze evaluation measures. At
the very least, an evaluation measure should penal-
ize random results in a fairly significant way. This
task showed that the paired F-score is able to iden-
tify and expose random baselines, and that it drives
them far down the rankings and places them well
below participating systems. This seems prefer-
able to the V-measure, which tends to rank random
systems above all others, and to supervised recall,
which provides little or no separation between ran-
dom baselines and participating systems.
</bodyText>
<sectionHeader confidence="0.999192" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999868755555556">
E. Agirre and A. Soroa. 2007. SemEval-2007 Task
02: Evaluating word sense induction and discrim-
ination systems. In Proceedings of the Fourth
International Workshop on Semantic Evaluations
(SemEval-2007), pages 7–12, Prague, Czech Repub-
lic, June.
J. Artiles, E. Amig´o, and J. Gonzalo. 2009. The role of
named entities in Web People Search. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing, pages 534–542,
Singapore, August.
S. Manandhar, I. Klapaftis, D. Dligach, and S. Prad-
han. 2010. SemEval-2010 Task 14: Word sense
induction and disambiguation. In Proceedings of
the SemEval 2010 Workshop : the 5th International
Workshop on Semantic Evaluations, Uppsala, Swe-
den, July.
T. Pedersen and A. Kulkarni. 2006. Automatic cluster
stopping with criterion functions and the gap statis-
tic. In Proceedings of the Demonstration Session of
the Human Language Technology Conference and
the Sixth Annual Meeting of the North American
Chapter of the Association for Computational Lin-
guistics, pages 276–279, New York City, June.
T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Sig-
nificant lexical relationships. In Proceedings of the
Thirteenth National Conference on Artificial Intelli-
gence, pages 455–460, Portland, OR, August.
T. Pedersen. 2007. UMND2 : SenseClusters applied
to the sense induction task of Senseval-4. In Pro-
ceedings of the Fourth International Workshop on
Semantic Evaluations (SemEval-2007), pages 394–
397, Prague, Czech Republic, June.
A. Purandare and T. Pedersen. 2004. Word sense dis-
crimination by clustering contexts in vector and sim-
ilarity spaces. In Proceedings of the Conference on
Computational Natural Language Learning, pages
41–48, Boston, MA.
A. Rosenberg and J. Hirschberg. 2007. V-measure:
A conditional entropy-based external cluster eval-
uation measure. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning, pages 410–420, Prague, Czech Re-
public, June.
</reference>
<page confidence="0.999084">
366
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.850053">
<title confidence="0.900084">Duluth-WSI: SenseClusters Applied to the Sense Induction Task of SemEval-2</title>
<author confidence="0.999721">Ted Pedersen</author>
<affiliation confidence="0.996772">Department of Computer Science University of Minnesota, Duluth</affiliation>
<address confidence="0.991869">Duluth, MN 55812</address>
<email confidence="0.999781">tpederse@d.umn.edu</email>
<web confidence="0.998579">http://senseclusters.sourceforge.net</web>
<abstract confidence="0.998388461538462">The Duluth-WSI systems in SemEval-2 built word co–occurrence matrices from the task test data to create a second order co–occurrence representation of those test instances. The senses of words were induced by clustering these instances, where the number of clusters was automatically predicted. The Duluth-Mix system was a variation of WSI that used the combination of training and test data to create the co-occurrence matrix. The Duluth-R system was a series of random baselines.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>A Soroa</author>
</authors>
<title>SemEval-2007 Task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>7--12</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5793" citStr="Agirre and Soroa, 2007" startWordPosition="915" endWordPosition="918">in the given range with equal probability, so the distribution of assigned senses is balanced. 5 Discussion The evaluation of unsupervised sense discrimination and induction systems is still not standardized, so an important part of any exercise like SemEval-2 is to scrutinize the evaluation measures used in order to determine to what degree they are providing a useful and reasonable way of evaluating system results. 5.1 Evaluation Measures Each participating system was scored by three different evaluation methods: the V-measure (Rosenberg and Hirschberg, 2007), the supervised recall measure (Agirre and Soroa, 2007), and the paired F-score (Artiles et al., 2009). The results of the evaluation are in some sense confusing - a system that ranks near the top according to one measure may rank at the bottom or middle of another. There was not any single system that did well according to all of the different measures. The situation is so extreme that in some cases a system would perform near the top in one measure, and then below random baselines in another. These stark differences suggest a real need for continued development of other methods for evaluating unsupervised sense induction. One minimum expectation</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>E. Agirre and A. Soroa. 2007. SemEval-2007 Task 02: Evaluating word sense induction and discrimination systems. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 7–12, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Artiles</author>
<author>E Amig´o</author>
<author>J Gonzalo</author>
</authors>
<title>The role of named entities in Web People Search.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>534--542</pages>
<location>Singapore,</location>
<marker>Artiles, Amig´o, Gonzalo, 2009</marker>
<rawString>J. Artiles, E. Amig´o, and J. Gonzalo. 2009. The role of named entities in Web People Search. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 534–542, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Manandhar</author>
<author>I Klapaftis</author>
<author>D Dligach</author>
<author>S Pradhan</author>
</authors>
<title>SemEval-2010 Task 14: Word sense induction and disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the SemEval 2010 Workshop : the 5th International Workshop on Semantic Evaluations,</booktitle>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="814" citStr="Manandhar et al., 2010" startWordPosition="115" endWordPosition="118">://senseclusters.sourceforge.net Abstract The Duluth-WSI systems in SemEval-2 built word co–occurrence matrices from the task test data to create a second order co–occurrence representation of those test instances. The senses of words were induced by clustering these instances, where the number of clusters was automatically predicted. The Duluth-Mix system was a variation of WSI that used the combination of training and test data to create the co-occurrence matrix. The Duluth-R system was a series of random baselines. 1 Introduction The Duluth systems in the sense induction task of SemEval-2 (Manandhar et al., 2010) were based on SenseClusters (v1.01), a freely available open source software package which relies on the premise that words with similar meanings will occur in similar contexts (Purandare and Pedersen, 2004). The data for the sense induction task included 100 ambiguous words made up of 50 nouns and 50 verbs. There were a total of 8,915 test instances and 879,807 training instances provided. Note that neither the training nor the test data was sense tagged. The training data was made available as a resource for participants, with the understanding that system evaluation would be done on the te</context>
</contexts>
<marker>Manandhar, Klapaftis, Dligach, Pradhan, 2010</marker>
<rawString>S. Manandhar, I. Klapaftis, D. Dligach, and S. Pradhan. 2010. SemEval-2010 Task 14: Word sense induction and disambiguation. In Proceedings of the SemEval 2010 Workshop : the 5th International Workshop on Semantic Evaluations, Uppsala, Sweden, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>A Kulkarni</author>
</authors>
<title>Automatic cluster stopping with criterion functions and the gap statistic.</title>
<date>2006</date>
<booktitle>In Proceedings of the Demonstration Session of the Human Language Technology Conference and the Sixth Annual Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>276--279</pages>
<location>New York City,</location>
<contexts>
<context position="2153" citStr="Pedersen and Kulkarni, 2006" startWordPosition="343" endWordPosition="346">evaluation. Five Duluth-WSI systems participated in this task, six Duluth-Mix systems, and five Duluth Random systems. The WSI and Mix systems almost always represented the test instances using second order co–occurrences, where each word in a test instance is replaced by a vector that shows the words with which it co-occurs. The word vectors that make up a test instance are averaged together to make up a new representation for that instance. All the test instances for a word are clustered, and the number of senses is automatically predicted by either the PK2 measure or Adapted Gap Statistic (Pedersen and Kulkarni, 2006). In the Duluth systems the co-occurrence matrices are either based on order-dependent bigrams or unordered pairs of words, both of which can be separated by up to some given number of intervening words. Bigrams are used to preserve distinctions between collocations such as cat house and house cat, whereas co–occurrences do not consider order and would treat these two as being equivalent. 2 Duluth-WSI systems The Duluth-WSI systems build co-occurrence matrices from the test data by identifying bigrams or co–occurrences that occur with up to eight intermediate words between them in instances of</context>
</contexts>
<marker>Pedersen, Kulkarni, 2006</marker>
<rawString>T. Pedersen and A. Kulkarni. 2006. Automatic cluster stopping with criterion functions and the gap statistic. In Proceedings of the Demonstration Session of the Human Language Technology Conference and the Sixth Annual Meeting of the North American Chapter of the Association for Computational Linguistics, pages 276–279, New York City, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>M Kayaalp</author>
<author>R Bruce</author>
</authors>
<title>Significant lexical relationships.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>455--460</pages>
<location>Portland, OR,</location>
<contexts>
<context position="3071" citStr="Pedersen et al., 1996" startWordPosition="497" endWordPosition="501">at, whereas co–occurrences do not consider order and would treat these two as being equivalent. 2 Duluth-WSI systems The Duluth-WSI systems build co-occurrence matrices from the test data by identifying bigrams or co–occurrences that occur with up to eight intermediate words between them in instances of ambiguous nouns, and up to 23 intermediate words for the verbs. Any bigram (bi) or co–occurrence (co) that occurs more than 5 times with up to the allowed number of intervening words and has statistical significance of 0.95 or above according to the left-sided Fisher’s exact test was selected (Pedersen et al., 1996). Some of the WSI systems reduce the co–occurrence matrix to 300 dimensions using Singular Value Decomposition (SVD). The resulting co-occurrence matrix was used to create second order co–occurrence vectors to represent the test instances, which were clustered using the method of repeated bisections (rb), where similarity was measured using the cosine. Table 1 summarizes the distinctions between the various Duluth-WSI systems. 3 Duluth-Mix systems The Duluth-Mix systems used the combination of the test and training data to identify features to represent the test instances. The goal of this com</context>
</contexts>
<marker>Pedersen, Kayaalp, Bruce, 1996</marker>
<rawString>T. Pedersen, M. Kayaalp, and R. Bruce. 1996. Significant lexical relationships. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 455–460, Portland, OR, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
</authors>
<title>UMND2 : SenseClusters applied to the sense induction task of Senseval-4.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>394--397</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="9008" citStr="Pedersen, 2007" startWordPosition="1449" endWordPosition="1450">signing one sense per instance resulted in a V–measure of 31.7. 5.1.2 Supervised Recall The supervised recall measure takes the sense induction results (on the 8,915 test instances) as submitted by a participating system and splits that into a training and test portion for supervised learning. The recall attained on the test split by a classifier learned on the training split becomes the measure of the unsupervised system. Two different splits were used, with 80% or 60% of the test instances for training, and the remainder for testing. This evaluation method was also used in SemEval-1, where (Pedersen, 2007) noted that it seemed to compress the results of all the systems into a narrow band that converged around the Most Frequent Sense result. The same appears to have happened in 2010. The supervised recall of the Most Frequent Sense baseline (MFS) is 58 or .59 (depending on the split), and the majority of participating systems (and even some of the random baselines) fall in a range of scores from .56 to .62 (a band of .06). This blurs distinctions among participating systems with each other and with random baselines. The number of senses actually assigned by the classifier learned from the traini</context>
<context position="13906" citStr="Pedersen, 2007" startWordPosition="2280" endWordPosition="2281">leted 7,417 instances (83%), and Duluth-Mix-Uni-PK2 and Gap systems completed 2,682 of these instances (30%). While these are partial results they seem sufficient to support this conclusion. To be usable in practical settings, an unsupervised sense induction system should discover the number of senses accurately and automatically. Duluth-WSI and Duluth-WSI-SVD were very successful in that regard, and predicted 4.15 senses on average per word (with the PK2 measure) while the actual number of senses was 3.79. The Duluth-WSI systems are direct descendents of UMND2 which participated in SemEval1 (Pedersen, 2007), where Duluth-WSI-Gap is the closest relative. However, UMND2 used Pointwise Mutual Information (PMI) rather than Fisher’s left sided test, and it performed clustering with k-means rather than the method of repeated bisections. Both UMND2 and Duluth-WSI-Gap used the Adapted Gap Statistic, and interestingly enough both discovered approximately 1.4 senses on average per word. 6 Conclusion The SemEval-2 sense induction task was an opportunity to compare participating systems with each other, and also to analyze evaluation measures. At the very least, an evaluation measure should penalize random </context>
</contexts>
<marker>Pedersen, 2007</marker>
<rawString>T. Pedersen. 2007. UMND2 : SenseClusters applied to the sense induction task of Senseval-4. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 394– 397, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Purandare</author>
<author>T Pedersen</author>
</authors>
<title>Word sense discrimination by clustering contexts in vector and similarity spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Computational Natural Language Learning,</booktitle>
<pages>41--48</pages>
<location>Boston, MA.</location>
<contexts>
<context position="1022" citStr="Purandare and Pedersen, 2004" startWordPosition="147" endWordPosition="150">t instances. The senses of words were induced by clustering these instances, where the number of clusters was automatically predicted. The Duluth-Mix system was a variation of WSI that used the combination of training and test data to create the co-occurrence matrix. The Duluth-R system was a series of random baselines. 1 Introduction The Duluth systems in the sense induction task of SemEval-2 (Manandhar et al., 2010) were based on SenseClusters (v1.01), a freely available open source software package which relies on the premise that words with similar meanings will occur in similar contexts (Purandare and Pedersen, 2004). The data for the sense induction task included 100 ambiguous words made up of 50 nouns and 50 verbs. There were a total of 8,915 test instances and 879,807 training instances provided. Note that neither the training nor the test data was sense tagged. The training data was made available as a resource for participants, with the understanding that system evaluation would be done on the test instances only. The organizers held back a gold standard annotation of the test data that was only used for evaluation. Five Duluth-WSI systems participated in this task, six Duluth-Mix systems, and five D</context>
</contexts>
<marker>Purandare, Pedersen, 2004</marker>
<rawString>A. Purandare and T. Pedersen. 2004. Word sense discrimination by clustering contexts in vector and similarity spaces. In Proceedings of the Conference on Computational Natural Language Learning, pages 41–48, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rosenberg</author>
<author>J Hirschberg</author>
</authors>
<title>V-measure: A conditional entropy-based external cluster evaluation measure.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>410--420</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="5737" citStr="Rosenberg and Hirschberg, 2007" startWordPosition="906" endWordPosition="910">ve, and R110 to one of ten senses. Random numbers are generated in the given range with equal probability, so the distribution of assigned senses is balanced. 5 Discussion The evaluation of unsupervised sense discrimination and induction systems is still not standardized, so an important part of any exercise like SemEval-2 is to scrutinize the evaluation measures used in order to determine to what degree they are providing a useful and reasonable way of evaluating system results. 5.1 Evaluation Measures Each participating system was scored by three different evaluation methods: the V-measure (Rosenberg and Hirschberg, 2007), the supervised recall measure (Agirre and Soroa, 2007), and the paired F-score (Artiles et al., 2009). The results of the evaluation are in some sense confusing - a system that ranks near the top according to one measure may rank at the bottom or middle of another. There was not any single system that did well according to all of the different measures. The situation is so extreme that in some cases a system would perform near the top in one measure, and then below random baselines in another. These stark differences suggest a real need for continued development of other methods for evaluati</context>
</contexts>
<marker>Rosenberg, Hirschberg, 2007</marker>
<rawString>A. Rosenberg and J. Hirschberg. 2007. V-measure: A conditional entropy-based external cluster evaluation measure. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 410–420, Prague, Czech Republic, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>