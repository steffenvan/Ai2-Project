<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000076">
<title confidence="0.993595">
Combining Knowledge-based
Methods and Supervised
Learning for Effective Italian
Word Sense Disambiguation
</title>
<author confidence="0.7782485">
Pierpaolo Basile
Marco de Gemmis
Pasquale Lops
Giovanni Semeraro
</author>
<affiliation confidence="0.998942">
University of Bari (Italy)
</affiliation>
<email confidence="0.995915">
email: basilepp@di.uniba.it
</email>
<sectionHeader confidence="0.98941" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999526">
This paper presents a WSD strategy which combines a knowledge-based
method that exploits sense definitions in a dictionary and relations among
senses in a semantic network, with supervised learning methods on anno-
tated corpora. The idea behind the approach is that the knowledge-based
method can cope with the possible lack of training data, while supervised
learning can improve the precision of a knowledge-based method when
training data are available. This makes the proposed method suitable for
disambiguation of languages for which the available resources are lacking
in training data or sense definitions. In order to evaluate the effectiveness
of the proposed approach, experimental sessions were carried out on the
dataset used for the WSD task in the EVALITA 2007 initiative, devoted to
the evaluation of Natural Language Processing tools for Italian. The most
effective hybrid WSD strategy is the one that integrates the knowledge-
based approach into the supervised learning method, which outperforms
both methods taken singularly.
</bodyText>
<page confidence="0.995042">
5
</page>
<note confidence="0.815001">
6 Basile, de Gemmis, Lops, and Semeraro
</note>
<sectionHeader confidence="0.595641" genericHeader="categories and subject descriptors">
1 Background and Motivations
</sectionHeader>
<bodyText confidence="0.999235636363636">
The inherent ambiguity of human language is a greatly debated problem in many
research areas, such as information retrieval and text categorization, since the presence
of polysemous words might result in a wrong relevance judgment or classification of
documents. These problems call for alternative methods that work not only at the
lexical level of the documents, but also at the meaning level.
The task of Word Sense Disambiguation (WSD) consists in assigning the most ap-
propriate meaning to a polysemous word within a given context. Applications such
as machine translation, knowledge acquisition, common sense reasoning and others,
require knowledge about word meanings, and WSD is essential for all these applica-
tions. The assignment of senses to words is accomplished by using two major sources
of information (Nancy and Véronis, 1998):
</bodyText>
<listItem confidence="0.86157825">
1. the context of the word to be disambiguated, e.g. information contained within
the text in which the word appears;
2. external knowledge sources, including lexical resources, as well as hand-devised
knowledge sources, which provide data useful to associate words with senses.
</listItem>
<bodyText confidence="0.99986932">
All disambiguation work involves matching the context of the instance of the word
to be disambiguated with either information from an external knowledge source (also
known as knowledge-driven WSD), or information about the contexts of previously
disambiguated instances of the word derived from corpora (data-driven or corpus-
based WSD).
Corpus-based WSD exploits semantically annotated corpora to train machine learn-
ing algorithms to decide which word sense to choose in which context. Words in such
annotated corpora are tagged manually using semantic classes chosen from a particu-
lar lexical semantic resource (e.g. WORDNET (Fellbaum, 1998)). Each sense-tagged
occurrence of a particular word is transformed into a feature vector, which is then used
in an automatic learning process. The applicability of such supervised algorithms is
limited to those few words for which sense tagged data are available, and their accu-
racy is strongly influenced by the amount of labeled data available.
Knowledge-based WSD has the advantage of avoiding the need of sense-annotated
data, rather it exploits lexical knowledge stored in machine-readable dictionaries or
thesauri. Systems adopting this approach have proved to be ready-to-use and scalable,
but in general they reach lower precision than corpus-based WSD systems.
Our hypothesis is that the combination of both types of strategies can improve WSD
effectiveness, because knowledge-based methods can cope with the possible lack of
training data, while supervised learning can improve the precision of knowledge-based
methods when training data are available.
This paper presents a method for solving the semantic ambiguity of all words con-
tained in a text1. We propose a hybrid WSD algorithm that combines a knowledge-
based WSD algorithm, called JIGSAW, which we designed to work by exploiting
WORDNET-like dictionaries as sense repository, with a supervised machine learning
</bodyText>
<footnote confidence="0.969962">
1all words task tries to disambiguate all the words in a text, while lexical sample task tries to disam-
biguate only specific words
</footnote>
<note confidence="0.478116">
Combining Knowledge-based Methods and Supervised Learning 7
</note>
<bodyText confidence="0.941732428571428">
algorithm (K-Nearest Neighbor classifier). WORDNET-like dictionaries are used be-
cause they combine the characteristics of both a dictionary and a structured semantic
network, supplying definitions for the different senses of words and defining groups
of synonymous words by means of synsets, which represent distinct lexical concepts.
WORDNET also organize synsets in a conceptual structure by defining a number of
semantic relationship (IS-A, PART-OF, etc.) among them.
Mainly, the paper concentrates on two investigations:
</bodyText>
<listItem confidence="0.856769875">
1. First, corpus-based WSD is applied to words for which training examples are
provided, then JIGSAW is applied to words not covered in the first step, with the
advantage of knowing the senses of the context words already disambiguated in
the first step;
2. First, JIGSAW is applied to assign the most appropriate sense to those words
that can be disambiguated with a high level of confidence (by setting a specific
parameter in the algorithm), then the remaining words are disambiguated by the
corpus-based method.
</listItem>
<bodyText confidence="0.999967125">
The paper is organized as follows: After a brief discussion about the main works
related to our research, Section 3 gives the main ideas underlying the proposed hybrid
WSD strategy. More details about the K-NN classification algorithm and JIGSAW,
on which the hybrid WSD approach is based, are provided in Section 4 and Section
5, respectively. Experimental sessions have been carried out in order to evaluate the
proposed approach in the critical situation when training data are not much reliable,
as for Italian. Results are presented in Section 6, while conclusions and future work
close the paper.
</bodyText>
<sectionHeader confidence="0.999893" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999784555555556">
For some Natural Language Processing (NLP) tasks, such as part of speech tagging or
named entity recognition, there is a consensus on what makes a successful algorithm,
regardless of the approach considered. Instead, no such consensus has been reached
yet for the task of WSD, and previous work has considered a range of knowledge
sources, such as local collocational clues, common membership in semantically or
topically related word classes, semantic density, and others. In recent SENSEVAL-3
evaluations2, the most successful approaches for all words WSD relied on information
drawn from annotated corpora. The system developed by Decadt et al. (2002) uses two
cascaded memory-based classifiers, combined with the use of a genetic algorithm for
joint parameter optimization and feature selection. A separate word expert is learned
for each ambiguous word, using a concatenated corpus of English sense tagged texts,
including SemCor, SENSEVAL datasets, and a corpus built from WORDNET exam-
ples. The performance of this system on the SENSEVAL-3 English all words dataset
was evaluated at 65.2%. Another top ranked system is the one developed by Yuret
(2004), which combines two Naïve Bayes statistical models, one based on surround-
ing collocations and another one based on a bag of words around the target word.
The statistical models are built based on SemCor and WORDNET, for an overall dis-
ambiguation accuracy of 64.1%. All previous systems use supervised methods, thus
</bodyText>
<footnote confidence="0.991065">
2http://www.senseval.org
</footnote>
<note confidence="0.399721">
8 Basile, de Gemmis, Lops, and Semeraro
</note>
<bodyText confidence="0.999825142857143">
requiring a large amount of human intervention to annotate the training data. In the
context of the current multilingual society, this strong requirement is even increased,
since the so-called “sense-tagged data bottleneck problem” is emphasized.
To address this problem, different methods have been proposed. This includes
the automatic generation of sense-tagged data using monosemous relatives (Leacock
et al., 1998), automatically bootstrapped disambiguation patterns (Mihalcea, 2002),
parallel texts as a way to point out word senses bearing different translations in a sec-
ond language (Diab, 2004), and the use of volunteer contributions over the Web (Mi-
halcea and Chklovski, 2003). More recently, Wikipedia has been used as a source of
sense annotations for building a sense annotated corpus which can be used to train
accurate sense classifiers (Mihalcea, 2007). Even though the Wikipedia-based sense
annotations were found reliable, leading to accurate sense classifiers, one of the lim-
itations of the approach is that definitions and annotations in Wikipedia are available
almost exclusively for nouns.
On the other hand, the increasing availability of large-scale rich (lexical) knowledge
resources seems to provide new challenges to knowledge-based approaches (Navigli
and Velardi, 2005; Mihalcea, 2005). Our hypothesis is that the complementarity of
knowledge-based methods and corpus-based ones is the key to improve WSD effec-
tiveness. The aim of the paper is to define a cascade hybrid method able to exploit
both linguistic information coming from WORDNET-like dictionaries and statistical
information coming from sense-annotated corpora.
</bodyText>
<sectionHeader confidence="0.984368" genericHeader="method">
3 A Hybrid Strategy for WSD
</sectionHeader>
<bodyText confidence="0.998615">
The goal of WSD algorithms consists in assigning a word wi occurring in a document
d with its appropriate meaning or sense s. The sense s is selected from a predefined set
of possibilities, usually known as sense inventory. We adopt ITALWORDNET (Roven-
tini et al., 2003) as sense repository. The algorithm is composed by two procedures:
</bodyText>
<listItem confidence="0.827326363636363">
1. JIGSAW - It is a knowledge-based WSD algorithm based on the assumption
that the adoption of different strategies depending on Part-of-Speech (PoS) is
better than using always the same strategy. A brief description of JIGSAW is
given in Section 5, more details are reported in Basile et al. (2007b), Basile et al.
(2007a) and Semeraro et al. (2007).
2. Supervised learning procedure - A K-NN classifier (Mitchell, 1997), trained
on MultiSemCor corpus3 is adopted. Details are given in Section 4. MultiSem-
Cor is an English/Italian parallel corpus, aligned at the word level and annotated
with PoS, lemma and word senses. The parallel corpus is created by exploiting
the SemCor corpus4, which is a subset of the English Brown corpus containing
about 700,000 running words. In SemCor, all the words are tagged by PoS, and
</listItem>
<bodyText confidence="0.9419376">
more than 200,000 content words are also lemmatized and sense-tagged with
reference to the WORDNET lexical database. SemCor has been used in several
supervised WSD algorithms for English with good results. MultiSemCor con-
tains less annotations than SemCor, thus the accuracy and the coverage of the
supervised learning for Italian might be affected by poor training data.
</bodyText>
<footnote confidence="0.9991205">
3http://multisemcor.itc.it/
4http://www.cs.unt.edu/~rada/downloads.html\#semcor
</footnote>
<note confidence="0.541292">
Combining Knowledge-based Methods and Supervised Learning 9
</note>
<bodyText confidence="0.999923">
The idea is to combine both procedures in a hybrid WSD approach. A first choice
might be the adoption of the supervised method as first attempt, then JIGSAW could
be applied to words not covered in the first step. Differently, JIGSAW might be applied
first, then leaving the supervised approach to disambiguate the remaining words. An
investigation is required in order to choose the most effective combination.
</bodyText>
<sectionHeader confidence="0.986146" genericHeader="method">
4 Supervised Learning Method
</sectionHeader>
<bodyText confidence="0.999327944444445">
The goal of supervised methods is to use a set of annotated data as little as possible,
and at the same time to make the algorithm general enough to be able to disambiguate
all content words in a text. We use MultiSemCor as annotated corpus, since at present
it is the only available semantic annotated resource for Italian. The algorithm starts
with a preprocessing stage, where the text is tokenized, stemmed, lemmatized and
annotated with PoS.
Also, the collocations are identified using a sliding window approach, where a
collocation is considered to be a sequence of words that forms a compound concept
defined in ITALWORDNET (e.g. artificial intelligence). In the training step, a semantic
model is learned for each PoS, starting with the annotated corpus. These models are
then used to disambiguate words in the test corpus by annotating them with their
corresponding meaning. The models can only handle words that were previously seen
in the training corpus, and therefore their coverage is not 100%. Starting with an
annotated corpus formed by all annotated files in MultiSemCor, a separate training
dataset is built for each PoS. For each open-class word in the training corpus, a feature
vector is built and added to the corresponding training set. The following features
are used to describe an occurrence of a word in the training corpus as in Hoste et al.
(2002):
</bodyText>
<listItem confidence="0.995366444444444">
• Nouns - 2 features are included in feature vector: the first noun, verb, or adjec-
tive before the target noun, within a window of at most three words to the left,
and its PoS;
• Verbs - 4 features are included in feature vector: the first word before and the
first word after the target verb, and their PoS;
• Adjectives - all the nouns occurring in two windows, each one of six words
(before and after the target adjective) are included in the feature vector;
• Adverbs - the same as for adjectives, but vectors contain adjectives rather than
nouns.
</listItem>
<bodyText confidence="0.999898666666667">
The label of each feature vector consists of the target word and the corresponding
sense, represented as word#sense. Table 1 describes the number of vectors for each
PoS.
To annotate (disambiguate) new text, similar vectors are built for all content-words
in the text to be analyzed. Consider the target word bank, used as a noun. The algo-
rithm catches all the feature vectors of bank as a noun from the training model, and
builds the feature vector vf for the target word. Then, the algorithm computes the sim-
ilarity between each training vector and vf and ranks the training vectors in decreasing
order according to the similarity value.
</bodyText>
<note confidence="0.763028">
10 Basile, de Gemmis, Lops, and Semeraro
</note>
<tableCaption confidence="0.998633">
Table 1: Number of feature vectors
</tableCaption>
<table confidence="0.8920036">
PoS #feature vectors
Noun 38,546
Verb 18,688
Adjective 6,253
Adverb 1,576
</table>
<bodyText confidence="0.998491142857143">
The similarity is computed as Euclidean distance between vectors, where POS dis-
tance is set to 1, if POS tags are different, otherwise it is set to 0. Word distances
are computed by using the Levenshtein metric, that measures the amount of difference
between two strings as the minimum number of operations needed to transform one
string into the other, where an operation is an insertion, deletion, or substitution of a
single character (Levenshtein, 1966). Finally, the target word is labeled with the most
frequent sense in the first K vectors.
</bodyText>
<sectionHeader confidence="0.99721" genericHeader="method">
5 JIGSAW - Knowledge-based Approach
</sectionHeader>
<bodyText confidence="0.987381617647059">
JIGSAW is a WSD algorithm based on the idea of combining three different strategies
to disambiguate nouns, verbs, adjectives and adverbs. The main motivation behind
our approach is that the effectiveness of a WSD algorithm is strongly influenced by
the POS tag of the target word.
JIGSAW takes as input a document d = (w1, w2, ... , wh) and returns a list of
synsets X = (s1, s2, ... , sk) in which each element si is obtained by disambiguating
the target word wi based on the information obtained from the sense repository about
a few immediately surrounding words. We define the context C of the target word
to be a window of n words to the left and another n words to the right, for a total
of 2n surrounding words. The algorithm is based on three different procedures for
nouns, verbs, adverbs and adjectives, called JIGSAWnouns, JIGSAWverbs, JIGSAWothers,
respectively.
JIGSAWnouns - Given a set of nouns W = {w1,w2,...,wn}, obtained from docu-
ment d, with each wi having an associated sense inventory Si = {si1,si2,...,sik} of
possible senses, the goal is assigning each wi with the most appropriate sense sih E Si,
according to the similarity of wi with the other words in W (the context for wi). The
idea is to define a function ϕ(wi,sij), wi E W, sij E Si, that computes a value in [0,1]
representing the confidence with which word wi can be assigned with sense sij. In
order to measure the relatedness of two words we adopted a modified version of the
Leacock and Chodorow (1998) measure, which computes the length of the path be-
tween two concepts in a hierarchy by passing through their Most Specific Subsumer
(MSS). We introduced a constant factor depth which limits the search for the MSS to
depth ancestors, in order to avoid “poorly informative” MSSs. Moreover, in the simi-
larity computation, we introduced both a Gaussian factor G(pos(wi), pos(wj)), which
takes into account the distance between the position of the words in the text to be dis-
ambiguated, and a factor R(k), which assigns sik with a numerical value, according to
the frequency score in ITALWORDNET.
JIGSAWverbs - We define the description of a synset as the string obtained by
Combining Knowledge-based Methods and Supervised Learning 11
concatenating the gloss and the sentences that ITALWORDNET uses to explain the
usage of a synset. JIGSAWverbs includes, in the context C for the target verb wi, all the
nouns in the window of 2n words surrounding wi. For each candidate synset sik of wi,
the algorithm computes nouns(i,k), that is the set of nouns in the description for sik.
Then, for each wj in C and each synset sik, the following value is computed:
</bodyText>
<equation confidence="0.841943">
(1) maxjk = maxwlEnouns(i,k) Isim(wj,wl,depth)I
</equation>
<bodyText confidence="0.9998755">
where sim(wj,wl,depth) is the same similarity measure adopted by JIGSAWnouns.
Finally, an overall similarity score among sik and the whole context C is computed:
</bodyText>
<equation confidence="0.995283">
∑w &apos; •ECG(pos(wi), pos(wj)) ·maxjk
(2) (p(i,k) = R(k) ∑hG(pos(wi), pos(wh))
</equation>
<bodyText confidence="0.999975833333333">
where both R(k) and G(pos(wi), pos(wj)), that gives a higher weight to words closer
to the target word, are defined as in JIGSAWnouns. The synset assigned to wi is the one
with the highest (p value.
JIGSAWothers - This procedure is based on the WSD algorithm proposed in Baner-
jee and Pedersen (2002). The idea is to compare the glosses of each candidate sense
for the target word to the glosses of all the words in its context.
</bodyText>
<sectionHeader confidence="0.999655" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999939043478261">
The main goal of our investigation is to study the behavior of the hybrid algorithm
when available training resources are not much reliable, e.g. when a lower number
of sense descriptions is available, as for Italian. The hypothesis we want to evaluate
is that corpus-based methods and knowledge-based ones can be combined to improve
the accuracy of each single strategy.
Experiments have been performed on a standard test collection in the context of the
All-Words-Task, in which WSD algorithms attempt to disambiguate all words in a text.
Specifically, we used the EVALITA WSD All-Words-Task dataset5, which consists of
about 5,000 words labeled with ITALWORDNET synsets.
An important concern for the evaluation of WSD systems is the agreement rate
between human annotators on word sense assignment.
While for natural language subtasks like part-of-speech tagging, there are relatively
well defined and agreed-upon criteria of what it means to have the “correct” part of
speech assigned to a word, this is not the case for word sense assignment. Two human
annotators may genuinely disagree on their sense assignment to a word in a context,
since the distinction between the different senses for a commonly used word in a
dictionary like WORDNET tend to be rather fine.
What we would like to underline here is that it is important that human agreement
on an annotated corpus is carefully measured, in order to set an upper bound to the
performance measures: it would be futile to expect computers to agree more with the
reference corpus that human annotators among them. For example, the inter-annotator
agreement rate during the preparation of the SENSEVAL-3 WSD English All-Words-
Task dataset (Agirre et al., 2007) was approximately 72.5%.
</bodyText>
<footnote confidence="0.970662">
5http://evalita.itc.it/tasks/wsd.html
</footnote>
<note confidence="0.818789">
12 Basile, de Gemmis, Lops, and Semeraro
</note>
<bodyText confidence="0.9980005">
Unfortunately, for EVALITA dataset, the inter-annotator agreement has not been
measured, one of the reasons why the evaluation for Italian WSD is very hard. In our
experiments, we reasonably selected different baselines to compare the performance
of the proposed hybrid algorithm.
</bodyText>
<subsectionHeader confidence="0.993233">
6.1 Integrating JIGSAW into a supervised learning method
</subsectionHeader>
<bodyText confidence="0.999679333333333">
The design of the experiment is as follows: firstly, corpus-based WSD is applied to
words for which training examples are provided, then JIGSAW is applied to words
not covered by the first step, with the advantage of knowing the senses of the context
words already disambiguated in the first step. The performance of the hybrid method
was measured in terms of precision (P), recall (R), F-measure (F) and the percentage
A of disambiguation attempts, computed by counting the words for which a disam-
biguation attempt is made (the words with no training examples or sense definitions
cannot be disambiguated). Table 2 shows the baselines chosen to compare the hybrid
WSD algorithm on the All-Words-Task experiments.
</bodyText>
<tableCaption confidence="0.637972">
Table 2: Baselines for Italian All-Words-Task
</tableCaption>
<table confidence="0.998888666666667">
Setting P R F A
1sKsense 58.45 48.58 53.06 83.11
Random 43.55 35.88 39.34 83.11
JIGSAW 55.14 45.83 50.05 83.11
K-NN 59.15 11.46 19.20 19.38
K-NN + 1sKsense 57.53 47.81 52.22 83.11
</table>
<bodyText confidence="0.998452833333333">
The simplest baseline consists in assigning a random sense to each word (Random),
another common baseline in Word Sense Disambiguation is first sense (1sKsense): each
word is tagged using the first sense in ITALWORDNET that is the most commonly
(frequent) used sense. The other baselines are the two methods combined in the hybrid
WSD, taken separately, namely JIGSAW and K-NN, and the basic hybrid algorithm
“K-NN + 1sKsense”, which applies the supervised method, and then adopts the first
sense heuristic for the words without examples into training data. The K-NN baseline
achieves the highest precision, but the lowest recall due to the low coverage in the
training data (19.38%) makes this method useless for all practical purposes. Notice
that JIGSAW was the only participant to EVALITA WSD All-Words-Task, therefore
it currently represents the only available system performing WSD All-Words task for
the Italian language.
</bodyText>
<tableCaption confidence="0.997896">
Table 3: Experimental results of K-NN+JIGSAW
</tableCaption>
<table confidence="0.985605571428572">
Setting P R F A
K-NN + JIGSAW 56.62 47.05 51.39 83.11
K-NN + JIGSAW (ϕ &gt; 0.90) 61.88 26.16 36.77 42.60
K-NN + JIGSAW (ϕ &gt; 0.80) 61.40 32.21 42.25 52.06
K-NN + JIGSAW (ϕ &gt; 0.70) 60.02 36.29 45.23 60.46
K-NN + JIGSAW (ϕ &gt; 0.50) 59.58 37.38 45.93 62.74
Combining Knowledge-based Methods and Supervised Learning 13
</table>
<bodyText confidence="0.987784611111111">
Table 3 reports the results obtained by the hybrid method on the EVALITA dataset.
We study the behavior of the hybrid approach with relation to that of JIGSAW, since
this specific experiment aims at evaluating the potential improvements due to the in-
clusion of JIGSAW into K-NN. Different runs of the hybrid method have been per-
formed, each run corresponding to setting a specific value for ϕ (the confidence with
which a word wi is correctly disambiguated by JIGSAW). In each different run, the
disambiguation carried out by JIGSAW is considered reliable only when ϕ values ex-
ceed a certain threshold, otherwise any sense is assigned to the target word (this the
reason why A decreases by setting higher values for ϕ).
A positive effect on precision can be noticed by varying ϕ between 0.50 and 0.90. It
tends to grow and overcomes all the baselines, but a corresponding decrease of recall
is observed, as a consequence of more severe constraints set on ϕ. Anyway, recall is
still too low to be acceptable.
Better results are achieved when no restriction is set on ϕ (K-NN+JIGSAW in Ta-
ble 3): the recall is significantly higher than that obtained in the other runs. On the
other hand, the precision reached in this run is lower than in the others, but it is still
acceptable.
To sum up, two main conclusions can be drawn from the experiments:
</bodyText>
<listItem confidence="0.7776044">
• when no constraint is set on the knowledge-based method, the hybrid algorithm
K-NN+JIGSAW in general outperforms both JIGSAW and K-NN taken singu-
larly (F values highlighted in bold in Tables 3 and 4);
• when thresholding is introduced on ϕ, no improvement is observed on the whole
compared to K-NN+JIGSAW.
</listItem>
<bodyText confidence="0.999839333333333">
A deep analysis of results revealed that lower recall was achieved for verbs and
adjectives rather than for nouns. Indeed, disambiguation of Italian verbs and adjec-
tives is very hard, but the lower recall is probability due also to the fact that JIGSAW
uses glosses for verbs and adjectives disambiguation. As a consequence, the perfor-
mance depends on the accuracy of word descriptions in the glosses, while for nouns
the algorithm relies only the semantic relations between synsets.
</bodyText>
<subsectionHeader confidence="0.999306">
6.2 Integrating supervised learning into JIGSAW
</subsectionHeader>
<bodyText confidence="0.997277923076923">
In this experiment we test whether the supervised algorithm can help JIGSAW to dis-
ambiguate more accurately. The experiment has been organized as follows: JIGSAW
is applied to assign the most appropriate sense to the words which can be disam-
biguated with a high level of confidence (by setting the ϕ threshold), then the remain-
ing words are disambiguated by the K-NN classifier. The dataset and the baselines are
the same as in Section 6.1.
Note that, differently from the experiments described in Table 3, run JIGSAW+K-
NN has not been reported since JIGSAW covered all the target words in the first step
of the cascade hybrid method, then the K-NN method is not applied at all. Therefore,
for this run, results obtained by JIGSAW+K-NN correspond to those get by JIGSAW
alone (reported in Table 2).
Table 4 reports the results of all the runs. Results are very similar to those obtained
in the runs K-NN+JIGSAW with the same settings on ϕ. Precision tends to grow,
</bodyText>
<note confidence="0.828558">
14 Basile, de Gemmis, Lops, and Semeraro
</note>
<tableCaption confidence="0.996564">
Table 4: Experimental results of JIGSAW+K-NN
</tableCaption>
<table confidence="0.9995615">
Setting P R F A
JIGSAW (ϕ &gt; 0.90) + K-NN 61.48 27.42 37.92 44.61
JIGSAW (ϕ &gt; 0.80) + K-NN 61.17 32.59 42.52 53.28
JIGSAW (ϕ &gt; 0.70) + K-NN 59.44 36.56 45.27 61.52
</table>
<bodyText confidence="0.998013555555556">
while a corresponding decrease in recall is observed. The main outcome is that the
overall accuracy of the best combination JIGSAW+K-NN (ϕ &gt; 0.70, F value high-
lighted in bold in Table 4) is outperformed by K-NN+JIGSAW. Indeed, this result
was largely expected because the small size of the training set does not allow to cover
words not disambiguated by JIGSAW.
Even if K-NN+JIGSAW is not able to achieve the baselines set on the 1stsense
heuristic (first and last row in Table 2), we can conclude that a step toward these hard
baselines has been moved. The main outcome of the study is that the best hybrid
method on which further investigations are possible is K-NN+JIGSAW.
</bodyText>
<sectionHeader confidence="0.998002" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999972352941176">
This paper presented a method for solving the semantic ambiguity of all words con-
tained in a text. We proposed a hybrid WSD algorithm that combines a knowledge-
based WSD algorithm, called JIGSAW, which we designed to work by exploiting
WORDNET-like dictionaries as sense repository, with a supervised machine learning
algorithm (K-Nearest Neighbor classifier). The idea behind the proposed approach is
that JIGSAW can cope with the possible lack of training data, while K-NN can im-
prove the precision of JIGSAW method when training data are available. This makes
the proposed method suitable for disambiguation of languages for which the available
resources are lacking in training data or sense definitions, such as Italian.
Extensive experimental sessions were performed on the EVALITA WSD All-Words-
Task dataset, the only dataset available for the evaluation of WSD systems for the
Italian language. An investigation was carried out in order to evaluate several com-
binations of JIGSAW and K-NN. The main outcome is that the most effective hybrid
WSD strategy is the one that runs JIGSAW after K-NN, which outperforms both JIG-
SAW and K-NN taken singularly. Future work includes new experiments with other
combination methods, for example the JIGSAW output could be used as feature into
supervised system or other different supervised methods could be exploited.
</bodyText>
<sectionHeader confidence="0.998348" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99238559375">
Agirre, E., B. Magnini, O. L. de Lacalle, A. Otegi, G. Rigau, and P. Vossen (2007).
SemEval-2007 Task 1: Evaluating WSD on Cross-Language Information Retrieval.
In Proceedings of SemEval-2007. Association for Computational Linguistics.
Banerjee, S. and T. Pedersen (2002). An adapted lesk algorithm for word sense disam-
biguation using wordnet. In CICLing ’02: Proceedings of the Third International
Combining Knowledge-based Methods and Supervised Learning 15
Conference on Computational Linguistics and Intelligent Text Processing, London,
UK, pp. 136–145. Springer-Verlag.
Basile, P., M. de Gemmis, A. Gentile, P. Lops, and G. Semeraro (2007a). JIGSAW
algorithm for Word Sense Disambiguation. In SemEval-2007: 4th International
Workshop on Semantic Evaluations, pp. 398–401. ACL press.
Basile, P., M. de Gemmis, A. L. Gentile, P. Lops, and G. Semeraro (2007b). The JIG-
SAW Algorithm for Word Sense Disambiguation and Semantic Indexing of Doc-
uments. In R. Basili and M. T. Pazienza (Eds.), AI*IA, Volume 4733 of Lecture
Notes in Computer Science, pp. 314–325. Springer.
Decadt, B., V. Hoste, W. Daelemans, and A. V. den Bosch (2002). Gambl, Genetic
Algorithm optimization of Memory-based WSD. In Senseval-3: 3th International
Workshop on the Evaluation of Systems for the Semantic Analysis of Text.
Diab, M. (2004). Relieving the data acquisition bottleneck in word sense disambigua-
tion. In Proceedings of ACL. Barcelona, Spain.
Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. MIT Press.
Hoste, V., W. Daelemans, I. Hendrickx, and A. van den Bosch (2002). Evaluating the
results of a memory-based word-expert approach to unrestricted word sense dis-
ambiguation. In Proceedings of the ACL-02 workshop on Word sense disambigua-
tion: recent successes andfuture directions, Volume 8, pp. 95–101. Association for
Computational Linguistics Morristown, NJ, USA.
Leacock, C. and M. Chodorow (1998). Combining local context and WordNet simi-
larity for word sense identification, pp. 305–332. MIT Press.
Leacock, C., M. Chodorow, and G. Miller (1998). Using corpus statistics and Word-
Net relations for sense identification. Computational Linguistics 24(1), 147–165.
Levenshtein, V. I. (1966). Binary codes capable of correcting deletions, insertions,
and reversals. Soviet Physics Doklady 10(8), 707–710.
Mihalcea, R. (2002). Bootstrapping large sense tagged corpora. In Proceedings of the
3rd International Conference on Language Resources and Evaluations.
Mihalcea, R. (2005). Unsupervised large-vocabulary word sense disambiguation with
graph-based algorithms for sequence data labeling. In HLT ’05: Proceedings of
the conference on Human Language Technology and Empirical Methods in Nat-
ural Language Processing, Morristown, NJ, USA, pp. 411–418. Association for
Computational Linguistics.
Mihalcea, R. (2007). Using Wikipedia for Automatic Word Sense Disambiguation. In
Proceedings of the North American Chapter of the Association for Computational
Linguistics.
Mihalcea, R. and T. Chklovski (2003). Open Mind Word Expert: Creating Large
Annotated Data Collections with Web Users’ Help. In Proceedings of the EACL
Workshop on Linguistically Annotated Corpora, Budapest.
16 Basile, de Gemmis, Lops, and Semeraro
Mitchell, T. (1997). Machine Learning. New York: McGraw-Hill.
Nancy, I. and J. Véronis (1998). Introduction to the special issue on word sense
disambiguation: The state of the art. Computational Linguistics 24(1), 1–40.
Navigli, R. and P. Velardi (2005). Structural semantic interconnections: A knowledge-
based approach to word sense disambiguation. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence 27(7), 1075–1086.
Roventini, A., A. Alonge, F. Bertagna, N. Calzolari, J. Cancila, C. Girardi, B. Magnini,
R. Marinelli, M. Speranza, and A. Zampolli (2003). ItalWordNet: building a large
semantic database for the automatic treatment of Italian. Computational Linguis-
tics in Pisa - Linguistica Computazionale a Pisa. Linguistica Computazionale, Spe-
cial Issue XVIII-XIX, Tomo II, 745–791.
Semeraro, G., M. Degemmis, P. Lops, and P. Basile (2007). Combining learning and
word sense disambiguation for intelligent user profiling. In Proceedings of the
Twentieth International Joint Conference on Artificial Intelligence IJCAI-07, pp.
2856–2861. M. Kaufmann, San Francisco, California. ISBN: 978-I-57735-298-3.
Yuret, D. (2004). Some experiments with a naive bayes WSD system. In Senseval-3:
3th Internat. Workshop on the Evaluation of Systems for the Semantic Analysis of
Text.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.002121">
<title confidence="0.782151285714286">Combining Knowledge-based Methods and Supervised Learning for Effective Italian Word Sense Disambiguation Pierpaolo Basile Marco de Gemmis Pasquale Lops</title>
<author confidence="0.999028">Giovanni Semeraro</author>
<affiliation confidence="0.993888">University of Bari (Italy)</affiliation>
<abstract confidence="0.975964863192183">This paper presents a WSD strategy which combines a knowledge-based method that exploits sense definitions in a dictionary and relations among senses in a semantic network, with supervised learning methods on annotated corpora. The idea behind the approach is that the knowledge-based method can cope with the possible lack of training data, while supervised learning can improve the precision of a knowledge-based method when training data are available. This makes the proposed method suitable for disambiguation of languages for which the available resources are lacking in training data or sense definitions. In order to evaluate the effectiveness of the proposed approach, experimental sessions were carried out on the dataset used for the WSD task in the EVALITA 2007 initiative, devoted to the evaluation of Natural Language Processing tools for Italian. The most effective hybrid WSD strategy is the one that integrates the knowledgebased approach into the supervised learning method, which outperforms both methods taken singularly. 5 6 Basile, de Gemmis, Lops, and Semeraro 1 Background and Motivations The inherent ambiguity of human language is a greatly debated problem in many research areas, such as information retrieval and text categorization, since the presence of polysemous words might result in a wrong relevance judgment or classification of documents. These problems call for alternative methods that work not only at the level of the documents, but also at the The task of Word Sense Disambiguation (WSD) consists in assigning the most appropriate meaning to a polysemous word within a given context. Applications such as machine translation, knowledge acquisition, common sense reasoning and others, require knowledge about word meanings, and WSD is essential for all these applications. The assignment of senses to words is accomplished by using two major sources of information (Nancy and Véronis, 1998): the the word to be disambiguated, e.g. information contained within the text in which the word appears; external knowledge including lexical resources, as well as hand-devised knowledge sources, which provide data useful to associate words with senses. All disambiguation work involves matching the context of the instance of the word to be disambiguated with either information from an external knowledge source (also as or information about the contexts of previously instances of the word derived from corpora corpusexploits semantically annotated corpora to train machine learning algorithms to decide which word sense to choose in which context. Words in such annotated corpora are tagged manually using semantic classes chosen from a particulexical semantic resource (e.g. 1998)). Each sense-tagged occurrence of a particular word is transformed into a feature vector, which is then used in an automatic learning process. The applicability of such supervised algorithms is limited to those few words for which sense tagged data are available, and their accuracy is strongly influenced by the amount of labeled data available. has the advantage of avoiding the need of sense-annotated data, rather it exploits lexical knowledge stored in machine-readable dictionaries or thesauri. Systems adopting this approach have proved to be ready-to-use and scalable, but in general they reach lower precision than corpus-based WSD systems. Our hypothesis is that the combination of both types of strategies can improve WSD effectiveness, because knowledge-based methods can cope with the possible lack of training data, while supervised learning can improve the precision of knowledge-based methods when training data are available. paper presents a method for solving the semantic ambiguity of words conin a We propose a hybrid WSD algorithm that combines a knowledgebased WSD algorithm, called JIGSAW, which we designed to work by exploiting dictionaries as sense repository, with a supervised machine learning words tries to disambiguate all the words in a text, while sample tries to disambiguate only specific words Combining Knowledge-based Methods and Supervised Learning 7 (K-Nearest Neighbor classifier). dictionaries are used because they combine the characteristics of both a dictionary and a structured semantic network, supplying definitions for the different senses of words and defining groups synonymous words by means of which represent distinct lexical concepts. organize synsets in a conceptual structure by defining a number of semantic relationship (IS-A, PART-OF, etc.) among them. Mainly, the paper concentrates on two investigations: 1. First, corpus-based WSD is applied to words for which training examples are provided, then JIGSAW is applied to words not covered in the first step, with the advantage of knowing the senses of the context words already disambiguated in the first step; 2. First, JIGSAW is applied to assign the most appropriate sense to those words that can be disambiguated with a high level of confidence (by setting a specific parameter in the algorithm), then the remaining words are disambiguated by the corpus-based method. The paper is organized as follows: After a brief discussion about the main works related to our research, Section 3 gives the main ideas underlying the proposed hybrid WSD strategy. More details about the K-NN classification algorithm and JIGSAW, on which the hybrid WSD approach is based, are provided in Section 4 and Section 5, respectively. Experimental sessions have been carried out in order to evaluate the proposed approach in the critical situation when training data are not much reliable, as for Italian. Results are presented in Section 6, while conclusions and future work close the paper. 2 Related Work For some Natural Language Processing (NLP) tasks, such as part of speech tagging or named entity recognition, there is a consensus on what makes a successful algorithm, regardless of the approach considered. Instead, no such consensus has been reached yet for the task of WSD, and previous work has considered a range of knowledge sources, such as local collocational clues, common membership in semantically or topically related word classes, semantic density, and others. In recent SENSEVAL-3 the most successful approaches for words relied on information drawn from annotated corpora. The system developed by Decadt et al. (2002) uses two cascaded memory-based classifiers, combined with the use of a genetic algorithm for joint parameter optimization and feature selection. A separate word expert is learned for each ambiguous word, using a concatenated corpus of English sense tagged texts, SemCor, SENSEVAL datasets, and a corpus built from examples. The performance of this system on the SENSEVAL-3 English all words dataset was evaluated at 65.2%. Another top ranked system is the one developed by Yuret (2004), which combines two Naïve Bayes statistical models, one based on surrounding collocations and another one based on a bag of words around the target word. statistical models are built based on SemCor and for an overall disambiguation accuracy of 64.1%. All previous systems use supervised methods, thus 8 Basile, de Gemmis, Lops, and Semeraro requiring a large amount of human intervention to annotate the training data. In the context of the current multilingual society, this strong requirement is even increased, since the so-called “sense-tagged data bottleneck problem” is emphasized. To address this problem, different methods have been proposed. This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al., 1998), automatically bootstrapped disambiguation patterns (Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab, 2004), and the use of volunteer contributions over the Web (Mihalcea and Chklovski, 2003). More recently, Wikipedia has been used as a source of sense annotations for building a sense annotated corpus which can be used to train accurate sense classifiers (Mihalcea, 2007). Even though the Wikipedia-based sense annotations were found reliable, leading to accurate sense classifiers, one of the limitations of the approach is that definitions and annotations in Wikipedia are available almost exclusively for nouns. On the other hand, the increasing availability of large-scale rich (lexical) knowledge resources seems to provide new challenges to knowledge-based approaches (Navigli and Velardi, 2005; Mihalcea, 2005). Our hypothesis is that the complementarity of knowledge-based methods and corpus-based ones is the key to improve WSD effectiveness. The aim of the paper is to define a cascade hybrid method able to exploit linguistic information coming from dictionaries and statistical information coming from sense-annotated corpora. 3 A Hybrid Strategy for WSD goal of WSD algorithms consists in assigning a word in a document its appropriate meaning or sense The sense selected from a predefined set possibilities, usually known as We adopt (Roventini et al., 2003) as sense repository. The algorithm is composed by two procedures: JIGSAW is a knowledge-based WSD algorithm based on the assumption that the adoption of different strategies depending on Part-of-Speech (PoS) is better than using always the same strategy. A brief description of JIGSAW is given in Section 5, more details are reported in Basile et al. (2007b), Basile et al. (2007a) and Semeraro et al. (2007). Supervised learning procedure - K-NN classifier (Mitchell, 1997), trained MultiSemCor is adopted. Details are given in Section 4. MultiSem- Cor is an English/Italian parallel corpus, aligned at the word level and annotated with PoS, lemma and word senses. The parallel corpus is created by exploiting SemCor which is a subset of the English Brown corpus containing about 700,000 running words. In SemCor, all the words are tagged by PoS, and more than 200,000 content words are also lemmatized and sense-tagged with to the database. SemCor has been used in several supervised WSD algorithms for English with good results. MultiSemCor contains less annotations than SemCor, thus the accuracy and the coverage of the supervised learning for Italian might be affected by poor training data. Combining Knowledge-based Methods and Supervised Learning 9 The idea is to combine both procedures in a hybrid WSD approach. A first choice might be the adoption of the supervised method as first attempt, then JIGSAW could be applied to words not covered in the first step. Differently, JIGSAW might be applied first, then leaving the supervised approach to disambiguate the remaining words. An investigation is required in order to choose the most effective combination. 4 Supervised Learning Method The goal of supervised methods is to use a set of annotated data as little as possible, and at the same time to make the algorithm general enough to be able to disambiguate all content words in a text. We use MultiSemCor as annotated corpus, since at present it is the only available semantic annotated resource for Italian. The algorithm starts with a preprocessing stage, where the text is tokenized, stemmed, lemmatized and annotated with PoS. Also, the collocations are identified using a sliding window approach, where a collocation is considered to be a sequence of words that forms a compound concept in artificial intelligence). In the training step, a semantic model is learned for each PoS, starting with the annotated corpus. These models are then used to disambiguate words in the test corpus by annotating them with their corresponding meaning. The models can only handle words that were previously seen in the training corpus, and therefore their coverage is not 100%. Starting with an annotated corpus formed by all annotated files in MultiSemCor, a separate training dataset is built for each PoS. For each open-class word in the training corpus, a feature vector is built and added to the corresponding training set. The following features are used to describe an occurrence of a word in the training corpus as in Hoste et al. (2002): Nouns features are included in feature vector: the first noun, verb, or adjective before the target noun, within a window of at most three words to the left, and its PoS; Verbs features are included in feature vector: the first word before and the first word after the target verb, and their PoS; Adjectives the nouns occurring in two windows, each one of six words (before and after the target adjective) are included in the feature vector; Adverbs same as for adjectives, but vectors contain adjectives rather than nouns. The label of each feature vector consists of the target word and the corresponding represented as Table 1 describes the number of vectors for each PoS. To annotate (disambiguate) new text, similar vectors are built for all content-words the text to be analyzed. Consider the target word used as a noun. The algocatches all the feature vectors of a noun from the training model, and the feature vector the target word. Then, the algorithm computes the simbetween each training vector and ranks the training vectors in decreasing order according to the similarity value. 10 Basile, de Gemmis, Lops, and Semeraro Table 1: Number of feature vectors PoS #feature vectors Noun 38,546 Verb 18,688 Adjective 6,253 Adverb 1,576 The similarity is computed as Euclidean distance between vectors, where POS distance is set to 1, if POS tags are different, otherwise it is set to 0. Word distances computed by using the that measures the amount of difference between two strings as the minimum number of operations needed to transform one string into the other, where an operation is an insertion, deletion, or substitution of a single character (Levenshtein, 1966). Finally, the target word is labeled with the most sense in the first 5 JIGSAW - Knowledge-based Approach JIGSAW is a WSD algorithm based on the idea of combining three different strategies to disambiguate nouns, verbs, adjectives and adverbs. The main motivation behind our approach is that the effectiveness of a WSD algorithm is strongly influenced by the POS tag of the target word. takes as input a document ... , and returns a list of ... , in which each element obtained by disambiguating word on the information obtained from the sense repository about few immediately surrounding words. We define the C the target word be a window of to the left and another to the right, for a total words. The algorithm is based on three different procedures for verbs, adverbs and adjectives, called respectively. Given a set of nouns obtained from docuwith each an associated sense inventory senses, the goal is assigning each the most appropriate sense E to the the other words in context for The is to define a function E that computes a value in the confidence with which word be assigned with sense In order to measure the relatedness of two words we adopted a modified version of the Leacock and Chodorow (1998) measure, which computes the length of the path betwo concepts in a hierarchy by passing through their Specific Subsumer We introduced a constant factor limits the search for the MSS to in order to avoid “poorly informative” MSSs. Moreover, in the simicomputation, we introduced both a Gaussian factor which takes into account the distance between the position of the words in the text to be disand a factor which assigns with a numerical value, according to frequency score in We define the a synset as the string obtained by Combining Knowledge-based Methods and Supervised Learning 11 the gloss and the sentences that to explain the of a synset. includes, in the context the target verb all the in the window of surrounding For each candidate synset of algorithm computes that is the set of nouns in the description for for each each synset the following value is computed: (1) the same similarity measure adopted by an overall similarity score among and the whole context computed: &apos; (2) = both that gives a higher weight to words closer the target word, are defined as in The synset assigned to the one the highest This procedure is based on the WSD algorithm proposed in Banerjee and Pedersen (2002). The idea is to compare the glosses of each candidate sense for the target word to the glosses of all the words in its context. 6 Experiments The main goal of our investigation is to study the behavior of the hybrid algorithm when available training resources are not much reliable, e.g. when a lower number of sense descriptions is available, as for Italian. The hypothesis we want to evaluate is that corpus-based methods and knowledge-based ones can be combined to improve the accuracy of each single strategy. Experiments have been performed on a standard test collection in the context of the in which WSD algorithms attempt to disambiguate all words in a text. we used the EVALITA WSD All-Words-Task which consists of 5,000 words labeled with An important concern for the evaluation of WSD systems is the agreement rate between human annotators on word sense assignment. While for natural language subtasks like part-of-speech tagging, there are relatively well defined and agreed-upon criteria of what it means to have the “correct” part of speech assigned to a word, this is not the case for word sense assignment. Two human annotators may genuinely disagree on their sense assignment to a word in a context, since the distinction between the different senses for a commonly used word in a like to be rather fine. What we would like to underline here is that it is important that human agreement on an annotated corpus is carefully measured, in order to set an upper bound to the performance measures: it would be futile to expect computers to agree more with the reference corpus that human annotators among them. For example, the inter-annotator agreement rate during the preparation of the SENSEVAL-3 WSD English All-Words- Task dataset (Agirre et al., 2007) was approximately 72.5%. 12 Basile, de Gemmis, Lops, and Semeraro Unfortunately, for EVALITA dataset, the inter-annotator agreement has not been measured, one of the reasons why the evaluation for Italian WSD is very hard. In our experiments, we reasonably selected different baselines to compare the performance of the proposed hybrid algorithm. 6.1 Integrating JIGSAW into a supervised learning method The design of the experiment is as follows: firstly, corpus-based WSD is applied to words for which training examples are provided, then JIGSAW is applied to words not covered by the first step, with the advantage of knowing the senses of the context words already disambiguated in the first step. The performance of the hybrid method measured in terms of precision recall F-measure and the percentage disambiguation attempts, computed by counting the words for which a disambiguation attempt is made (the words with no training examples or sense definitions cannot be disambiguated). Table 2 shows the baselines chosen to compare the hybrid WSD algorithm on the All-Words-Task experiments. Table 2: Baselines for Italian All-Words-Task Setting P R F A 58.45 48.58 53.06 83.11 Random 43.55 35.88 39.34 83.11 JIGSAW 55.14 45.83 K-NN 59.15 11.46 + 57.53 47.81 52.22 83.11 simplest baseline consists in assigning a random sense to each word common baseline in Word Sense Disambiguation is first sense each is tagged using the first sense in is the most commonly (frequent) used sense. The other baselines are the two methods combined in the hybrid WSD, taken separately, namely JIGSAW and K-NN, and the basic hybrid algorithm + which applies the supervised method, and then adopts the first sense heuristic for the words without examples into training data. The K-NN baseline achieves the highest precision, but the lowest recall due to the low coverage in the training data (19.38%) makes this method useless for all practical purposes. Notice that JIGSAW was the only participant to EVALITA WSD All-Words-Task, therefore it currently represents the only available system performing WSD All-Words task for the Italian language.</abstract>
<note confidence="0.88573525">Table 3: Experimental results of K-NN+JIGSAW Setting P R F A K-NN + JIGSAW 56.62 47.05 + JIGSAW &gt; 61.88 26.16 36.77 42.60 + JIGSAW &gt; 61.40 32.21 42.25 52.06 + JIGSAW &gt; 60.02 36.29 45.23 60.46 + JIGSAW &gt; 59.58 37.38 45.93 62.74 Combining Knowledge-based Methods and Supervised Learning 13</note>
<abstract confidence="0.994770644736842">Table 3 reports the results obtained by the hybrid method on the EVALITA dataset. We study the behavior of the hybrid approach with relation to that of JIGSAW, since this specific experiment aims at evaluating the potential improvements due to the inclusion of JIGSAW into K-NN. Different runs of the hybrid method have been pereach run corresponding to setting a specific value for confidence with a word correctly disambiguated by JIGSAW). In each different run, the carried out by JIGSAW is considered reliable only when exceed a certain threshold, otherwise any sense is assigned to the target word (this the why by setting higher values for positive effect on precision can be noticed by varying and It tends to grow and overcomes all the baselines, but a corresponding decrease of recall observed, as a consequence of more severe constraints set on Anyway, recall is still too low to be acceptable. results are achieved when no restriction is set on in Table 3): the recall is significantly higher than that obtained in the other runs. On the other hand, the precision reached in this run is lower than in the others, but it is still acceptable. To sum up, two main conclusions can be drawn from the experiments: • when no constraint is set on the knowledge-based method, the hybrid algorithm K-NN+JIGSAW in general outperforms both JIGSAW and K-NN taken singularly (F values highlighted in bold in Tables 3 and 4); when thresholding is introduced on no improvement is observed on the whole compared to K-NN+JIGSAW. A deep analysis of results revealed that lower recall was achieved for verbs and adjectives rather than for nouns. Indeed, disambiguation of Italian verbs and adjecis very hard, but the lower recall is probability due also to the fact that uses glosses for verbs and adjectives disambiguation. As a consequence, the performance depends on the accuracy of word descriptions in the glosses, while for nouns the algorithm relies only the semantic relations between synsets. 6.2 Integrating supervised learning into JIGSAW In this experiment we test whether the supervised algorithm can help JIGSAW to disambiguate more accurately. The experiment has been organized as follows: JIGSAW is applied to assign the most appropriate sense to the words which can be disamwith a high level of confidence (by setting the then the remaining words are disambiguated by the K-NN classifier. The dataset and the baselines are the same as in Section 6.1. Note that, differently from the experiments described in Table 3, run JIGSAW+K- NN has not been reported since JIGSAW covered all the target words in the first step of the cascade hybrid method, then the K-NN method is not applied at all. Therefore, for this run, results obtained by JIGSAW+K-NN correspond to those get by JIGSAW alone (reported in Table 2). Table 4 reports the results of all the runs. Results are very similar to those obtained the runs K-NN+JIGSAW with the same settings on Precision tends to grow, 14 Basile, de Gemmis, Lops, and Semeraro Table 4: Experimental results of JIGSAW+K-NN Setting P R F A &gt; + K-NN 61.48 27.42 37.92 44.61 &gt; + K-NN 61.17 32.59 42.52 53.28 &gt; + K-NN 59.44 36.56 while a corresponding decrease in recall is observed. The main outcome is that the accuracy of the best combination JIGSAW+K-NN &gt; highlighted in bold in Table 4) is outperformed by K-NN+JIGSAW. Indeed, this result was largely expected because the small size of the training set does not allow to cover words not disambiguated by JIGSAW. if K-NN+JIGSAW is not able to achieve the baselines set on the heuristic (first and last row in Table 2), we can conclude that a step toward these hard baselines has been moved. The main outcome of the study is that the best hybrid method on which further investigations are possible is K-NN+JIGSAW. 7 Conclusions and Future Work paper presented a method for solving the semantic ambiguity of words contained in a text. We proposed a hybrid WSD algorithm that combines a knowledgebased WSD algorithm, called JIGSAW, which we designed to work by exploiting dictionaries as sense repository, with a supervised machine learning algorithm (K-Nearest Neighbor classifier). The idea behind the proposed approach is that JIGSAW can cope with the possible lack of training data, while K-NN can improve the precision of JIGSAW method when training data are available. This makes the proposed method suitable for disambiguation of languages for which the available resources are lacking in training data or sense definitions, such as Italian. Extensive experimental sessions were performed on the EVALITA WSD All-Words- Task dataset, the only dataset available for the evaluation of WSD systems for the Italian language. An investigation was carried out in order to evaluate several combinations of JIGSAW and K-NN. The main outcome is that the most effective hybrid WSD strategy is the one that runs JIGSAW after K-NN, which outperforms both JIG- SAW and K-NN taken singularly. Future work includes new experiments with other methods, for example the could be used as feature into supervised system or other different supervised methods could be exploited.</abstract>
<note confidence="0.911934127659575">References Agirre, E., B. Magnini, O. L. de Lacalle, A. Otegi, G. Rigau, and P. Vossen (2007). SemEval-2007 Task 1: Evaluating WSD on Cross-Language Information Retrieval. of Association for Computational Linguistics. Banerjee, S. and T. Pedersen (2002). An adapted lesk algorithm for word sense disamusing wordnet. In ’02: Proceedings of the Third International Combining Knowledge-based Methods and Supervised Learning 15 on Computational Linguistics and Intelligent Text London, UK, pp. 136–145. Springer-Verlag. Basile, P., M. de Gemmis, A. Gentile, P. Lops, and G. Semeraro (2007a). JIGSAW for Word Sense Disambiguation. In 4th International on Semantic pp. 398–401. ACL press. Basile, P., M. de Gemmis, A. L. Gentile, P. Lops, and G. Semeraro (2007b). The JIG- SAW Algorithm for Word Sense Disambiguation and Semantic Indexing of Doc- In R. Basili and M. T. Pazienza (Eds.), Volume 4733 of in Computer pp. 314–325. Springer. Decadt, B., V. Hoste, W. Daelemans, and A. V. den Bosch (2002). Gambl, Genetic optimization of Memory-based WSD. In 3th International on the Evaluation of Systems for the Semantic Analysis of Diab, M. (2004). Relieving the data acquisition bottleneck in word sense disambigua- In of Barcelona, Spain. C. (1998). An Electronic Lexical MIT Press. Hoste, V., W. Daelemans, I. Hendrickx, and A. van den Bosch (2002). Evaluating the results of a memory-based word-expert approach to unrestricted word sense dis- In of the ACL-02 workshop on Word sense disambiguarecent successes andfuture Volume 8, pp. 95–101. Association for Computational Linguistics Morristown, NJ, USA. C. and M. Chodorow (1998). local context and WordNet simifor word sense pp. 305–332. MIT Press. Leacock, C., M. Chodorow, and G. Miller (1998). Using corpus statistics and Wordrelations for sense identification. Linguistics 147–165. Levenshtein, V. I. (1966). Binary codes capable of correcting deletions, insertions, reversals. Physics Doklady 707–710. R. (2002). Bootstrapping large sense tagged corpora. In of the International Conference on Language Resources and Mihalcea, R. (2005). Unsupervised large-vocabulary word sense disambiguation with algorithms for sequence data labeling. In ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Nat- Language Morristown, NJ, USA, pp. 411–418. Association for Computational Linguistics. Mihalcea, R. (2007). Using Wikipedia for Automatic Word Sense Disambiguation. In Proceedings of the North American Chapter of the Association for Computational Mihalcea, R. and T. Chklovski (2003). Open Mind Word Expert: Creating Large Data Collections with Web Users’ Help. In of the EACL on Linguistically Annotated Corpora, 16 Basile, de Gemmis, Lops, and Semeraro T. (1997). New York: McGraw-Hill.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>B Magnini</author>
<author>O L de Lacalle</author>
<author>A Otegi</author>
<author>G Rigau</author>
<author>P Vossen</author>
</authors>
<date>2007</date>
<booktitle>SemEval-2007 Task 1: Evaluating WSD on Cross-Language Information Retrieval. In Proceedings of SemEval-2007. Association for Computational Linguistics.</booktitle>
<marker>Agirre, Magnini, de Lacalle, Otegi, Rigau, Vossen, 2007</marker>
<rawString>Agirre, E., B. Magnini, O. L. de Lacalle, A. Otegi, G. Rigau, and P. Vossen (2007). SemEval-2007 Task 1: Evaluating WSD on Cross-Language Information Retrieval. In Proceedings of SemEval-2007. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>An adapted lesk algorithm for word sense disambiguation using wordnet.</title>
<date>2002</date>
<booktitle>In CICLing ’02: Proceedings of the Third International Combining Knowledge-based Methods and Supervised Learning 15 Conference on Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>136--145</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK,</location>
<contexts>
<context position="18101" citStr="Banerjee and Pedersen (2002)" startWordPosition="2857" endWordPosition="2861">nset sik, the following value is computed: (1) maxjk = maxwlEnouns(i,k) Isim(wj,wl,depth)I where sim(wj,wl,depth) is the same similarity measure adopted by JIGSAWnouns. Finally, an overall similarity score among sik and the whole context C is computed: ∑w &apos; •ECG(pos(wi), pos(wj)) ·maxjk (2) (p(i,k) = R(k) ∑hG(pos(wi), pos(wh)) where both R(k) and G(pos(wi), pos(wj)), that gives a higher weight to words closer to the target word, are defined as in JIGSAWnouns. The synset assigned to wi is the one with the highest (p value. JIGSAWothers - This procedure is based on the WSD algorithm proposed in Banerjee and Pedersen (2002). The idea is to compare the glosses of each candidate sense for the target word to the glosses of all the words in its context. 6 Experiments The main goal of our investigation is to study the behavior of the hybrid algorithm when available training resources are not much reliable, e.g. when a lower number of sense descriptions is available, as for Italian. The hypothesis we want to evaluate is that corpus-based methods and knowledge-based ones can be combined to improve the accuracy of each single strategy. Experiments have been performed on a standard test collection in the context of the A</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Banerjee, S. and T. Pedersen (2002). An adapted lesk algorithm for word sense disambiguation using wordnet. In CICLing ’02: Proceedings of the Third International Combining Knowledge-based Methods and Supervised Learning 15 Conference on Computational Linguistics and Intelligent Text Processing, London, UK, pp. 136–145. Springer-Verlag.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Basile</author>
<author>M de Gemmis</author>
<author>A Gentile</author>
<author>P Lops</author>
<author>G</author>
</authors>
<title>Semeraro (2007a). JIGSAW algorithm for Word Sense Disambiguation. In</title>
<booktitle>SemEval-2007: 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>398--401</pages>
<publisher>ACL press.</publisher>
<marker>Basile, de Gemmis, Gentile, Lops, G, </marker>
<rawString>Basile, P., M. de Gemmis, A. Gentile, P. Lops, and G. Semeraro (2007a). JIGSAW algorithm for Word Sense Disambiguation. In SemEval-2007: 4th International Workshop on Semantic Evaluations, pp. 398–401. ACL press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>P Basile</author>
<author>M de Gemmis</author>
<author>A L Gentile</author>
<author>P Lops</author>
<author>G</author>
</authors>
<title>Semeraro (2007b). The JIGSAW Algorithm for Word Sense Disambiguation and Semantic Indexing of Documents. In</title>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>4733</volume>
<pages>314--325</pages>
<publisher>Springer.</publisher>
<marker>Basile, de Gemmis, Gentile, Lops, G, </marker>
<rawString>Basile, P., M. de Gemmis, A. L. Gentile, P. Lops, and G. Semeraro (2007b). The JIGSAW Algorithm for Word Sense Disambiguation and Semantic Indexing of Documents. In R. Basili and M. T. Pazienza (Eds.), AI*IA, Volume 4733 of Lecture Notes in Computer Science, pp. 314–325. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Decadt</author>
<author>V Hoste</author>
<author>W Daelemans</author>
<author>A V den Bosch</author>
</authors>
<title>Gambl, Genetic Algorithm optimization of Memory-based WSD.</title>
<date>2002</date>
<booktitle>In Senseval-3: 3th International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</booktitle>
<marker>Decadt, Hoste, Daelemans, den Bosch, 2002</marker>
<rawString>Decadt, B., V. Hoste, W. Daelemans, and A. V. den Bosch (2002). Gambl, Genetic Algorithm optimization of Memory-based WSD. In Senseval-3: 3th International Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
</authors>
<title>Relieving the data acquisition bottleneck in word sense disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL.</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="8384" citStr="Diab, 2004" startWordPosition="1263" endWordPosition="1264">emeraro requiring a large amount of human intervention to annotate the training data. In the context of the current multilingual society, this strong requirement is even increased, since the so-called “sense-tagged data bottleneck problem” is emphasized. To address this problem, different methods have been proposed. This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al., 1998), automatically bootstrapped disambiguation patterns (Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab, 2004), and the use of volunteer contributions over the Web (Mihalcea and Chklovski, 2003). More recently, Wikipedia has been used as a source of sense annotations for building a sense annotated corpus which can be used to train accurate sense classifiers (Mihalcea, 2007). Even though the Wikipedia-based sense annotations were found reliable, leading to accurate sense classifiers, one of the limitations of the approach is that definitions and annotations in Wikipedia are available almost exclusively for nouns. On the other hand, the increasing availability of large-scale rich (lexical) knowledge res</context>
</contexts>
<marker>Diab, 2004</marker>
<rawString>Diab, M. (2004). Relieving the data acquisition bottleneck in word sense disambiguation. In Proceedings of ACL. Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="3112" citStr="Fellbaum, 1998" startWordPosition="461" endWordPosition="462">olves matching the context of the instance of the word to be disambiguated with either information from an external knowledge source (also known as knowledge-driven WSD), or information about the contexts of previously disambiguated instances of the word derived from corpora (data-driven or corpusbased WSD). Corpus-based WSD exploits semantically annotated corpora to train machine learning algorithms to decide which word sense to choose in which context. Words in such annotated corpora are tagged manually using semantic classes chosen from a particular lexical semantic resource (e.g. WORDNET (Fellbaum, 1998)). Each sense-tagged occurrence of a particular word is transformed into a feature vector, which is then used in an automatic learning process. The applicability of such supervised algorithms is limited to those few words for which sense tagged data are available, and their accuracy is strongly influenced by the amount of labeled data available. Knowledge-based WSD has the advantage of avoiding the need of sense-annotated data, rather it exploits lexical knowledge stored in machine-readable dictionaries or thesauri. Systems adopting this approach have proved to be ready-to-use and scalable, bu</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Hoste</author>
<author>W Daelemans</author>
<author>I Hendrickx</author>
<author>A van den Bosch</author>
</authors>
<title>Evaluating the results of a memory-based word-expert approach to unrestricted word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Word sense disambiguation: recent successes andfuture directions,</booktitle>
<volume>8</volume>
<pages>95--101</pages>
<institution>Association for Computational Linguistics</institution>
<location>Morristown, NJ, USA.</location>
<marker>Hoste, Daelemans, Hendrickx, van den Bosch, 2002</marker>
<rawString>Hoste, V., W. Daelemans, I. Hendrickx, and A. van den Bosch (2002). Evaluating the results of a memory-based word-expert approach to unrestricted word sense disambiguation. In Proceedings of the ACL-02 workshop on Word sense disambiguation: recent successes andfuture directions, Volume 8, pp. 95–101. Association for Computational Linguistics Morristown, NJ, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification,</title>
<date>1998</date>
<pages>305--332</pages>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16366" citStr="Leacock and Chodorow (1998)" startWordPosition="2572" endWordPosition="2575">respectively. JIGSAWnouns - Given a set of nouns W = {w1,w2,...,wn}, obtained from document d, with each wi having an associated sense inventory Si = {si1,si2,...,sik} of possible senses, the goal is assigning each wi with the most appropriate sense sih E Si, according to the similarity of wi with the other words in W (the context for wi). The idea is to define a function ϕ(wi,sij), wi E W, sij E Si, that computes a value in [0,1] representing the confidence with which word wi can be assigned with sense sij. In order to measure the relatedness of two words we adopted a modified version of the Leacock and Chodorow (1998) measure, which computes the length of the path between two concepts in a hierarchy by passing through their Most Specific Subsumer (MSS). We introduced a constant factor depth which limits the search for the MSS to depth ancestors, in order to avoid “poorly informative” MSSs. Moreover, in the similarity computation, we introduced both a Gaussian factor G(pos(wi), pos(wj)), which takes into account the distance between the position of the words in the text to be disambiguated, and a factor R(k), which assigns sik with a numerical value, according to the frequency score in ITALWORDNET. JIGSAWve</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Leacock, C. and M. Chodorow (1998). Combining local context and WordNet similarity for word sense identification, pp. 305–332. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
<author>G Miller</author>
</authors>
<title>Using corpus statistics and WordNet relations for sense identification.</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<volume>24</volume>
<issue>1</issue>
<pages>147--165</pages>
<contexts>
<context position="8200" citStr="Leacock et al., 1998" startWordPosition="1235" endWordPosition="1238">re built based on SemCor and WORDNET, for an overall disambiguation accuracy of 64.1%. All previous systems use supervised methods, thus 2http://www.senseval.org 8 Basile, de Gemmis, Lops, and Semeraro requiring a large amount of human intervention to annotate the training data. In the context of the current multilingual society, this strong requirement is even increased, since the so-called “sense-tagged data bottleneck problem” is emphasized. To address this problem, different methods have been proposed. This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al., 1998), automatically bootstrapped disambiguation patterns (Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab, 2004), and the use of volunteer contributions over the Web (Mihalcea and Chklovski, 2003). More recently, Wikipedia has been used as a source of sense annotations for building a sense annotated corpus which can be used to train accurate sense classifiers (Mihalcea, 2007). Even though the Wikipedia-based sense annotations were found reliable, leading to accurate sense classifiers, one of the limitations of the approach</context>
</contexts>
<marker>Leacock, Chodorow, Miller, 1998</marker>
<rawString>Leacock, C., M. Chodorow, and G. Miller (1998). Using corpus statistics and WordNet relations for sense identification. Computational Linguistics 24(1), 147–165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady</journal>
<volume>10</volume>
<issue>8</issue>
<pages>707--710</pages>
<contexts>
<context position="14751" citStr="Levenshtein, 1966" startWordPosition="2289" endWordPosition="2290">ity value. 10 Basile, de Gemmis, Lops, and Semeraro Table 1: Number of feature vectors PoS #feature vectors Noun 38,546 Verb 18,688 Adjective 6,253 Adverb 1,576 The similarity is computed as Euclidean distance between vectors, where POS distance is set to 1, if POS tags are different, otherwise it is set to 0. Word distances are computed by using the Levenshtein metric, that measures the amount of difference between two strings as the minimum number of operations needed to transform one string into the other, where an operation is an insertion, deletion, or substitution of a single character (Levenshtein, 1966). Finally, the target word is labeled with the most frequent sense in the first K vectors. 5 JIGSAW - Knowledge-based Approach JIGSAW is a WSD algorithm based on the idea of combining three different strategies to disambiguate nouns, verbs, adjectives and adverbs. The main motivation behind our approach is that the effectiveness of a WSD algorithm is strongly influenced by the POS tag of the target word. JIGSAW takes as input a document d = (w1, w2, ... , wh) and returns a list of synsets X = (s1, s2, ... , sk) in which each element si is obtained by disambiguating the target word wi based on </context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Levenshtein, V. I. (1966). Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady 10(8), 707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Bootstrapping large sense tagged corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluations.</booktitle>
<contexts>
<context position="8269" citStr="Mihalcea, 2002" startWordPosition="1243" endWordPosition="1244">y of 64.1%. All previous systems use supervised methods, thus 2http://www.senseval.org 8 Basile, de Gemmis, Lops, and Semeraro requiring a large amount of human intervention to annotate the training data. In the context of the current multilingual society, this strong requirement is even increased, since the so-called “sense-tagged data bottleneck problem” is emphasized. To address this problem, different methods have been proposed. This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al., 1998), automatically bootstrapped disambiguation patterns (Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab, 2004), and the use of volunteer contributions over the Web (Mihalcea and Chklovski, 2003). More recently, Wikipedia has been used as a source of sense annotations for building a sense annotated corpus which can be used to train accurate sense classifiers (Mihalcea, 2007). Even though the Wikipedia-based sense annotations were found reliable, leading to accurate sense classifiers, one of the limitations of the approach is that definitions and annotations in Wikipedia are available almos</context>
</contexts>
<marker>Mihalcea, 2002</marker>
<rawString>Mihalcea, R. (2002). Bootstrapping large sense tagged corpora. In Proceedings of the 3rd International Conference on Language Resources and Evaluations.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling.</title>
<date>2005</date>
<booktitle>In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>411--418</pages>
<institution>Association for Computational Linguistics.</institution>
<location>Morristown, NJ, USA,</location>
<contexts>
<context position="9096" citStr="Mihalcea, 2005" startWordPosition="1368" endWordPosition="1369">ly, Wikipedia has been used as a source of sense annotations for building a sense annotated corpus which can be used to train accurate sense classifiers (Mihalcea, 2007). Even though the Wikipedia-based sense annotations were found reliable, leading to accurate sense classifiers, one of the limitations of the approach is that definitions and annotations in Wikipedia are available almost exclusively for nouns. On the other hand, the increasing availability of large-scale rich (lexical) knowledge resources seems to provide new challenges to knowledge-based approaches (Navigli and Velardi, 2005; Mihalcea, 2005). Our hypothesis is that the complementarity of knowledge-based methods and corpus-based ones is the key to improve WSD effectiveness. The aim of the paper is to define a cascade hybrid method able to exploit both linguistic information coming from WORDNET-like dictionaries and statistical information coming from sense-annotated corpora. 3 A Hybrid Strategy for WSD The goal of WSD algorithms consists in assigning a word wi occurring in a document d with its appropriate meaning or sense s. The sense s is selected from a predefined set of possibilities, usually known as sense inventory. We adopt</context>
</contexts>
<marker>Mihalcea, 2005</marker>
<rawString>Mihalcea, R. (2005). Unsupervised large-vocabulary word sense disambiguation with graph-based algorithms for sequence data labeling. In HLT ’05: Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, Morristown, NJ, USA, pp. 411–418. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Using Wikipedia for Automatic Word Sense Disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the North American Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8650" citStr="Mihalcea, 2007" startWordPosition="1306" endWordPosition="1307">ess this problem, different methods have been proposed. This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al., 1998), automatically bootstrapped disambiguation patterns (Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab, 2004), and the use of volunteer contributions over the Web (Mihalcea and Chklovski, 2003). More recently, Wikipedia has been used as a source of sense annotations for building a sense annotated corpus which can be used to train accurate sense classifiers (Mihalcea, 2007). Even though the Wikipedia-based sense annotations were found reliable, leading to accurate sense classifiers, one of the limitations of the approach is that definitions and annotations in Wikipedia are available almost exclusively for nouns. On the other hand, the increasing availability of large-scale rich (lexical) knowledge resources seems to provide new challenges to knowledge-based approaches (Navigli and Velardi, 2005; Mihalcea, 2005). Our hypothesis is that the complementarity of knowledge-based methods and corpus-based ones is the key to improve WSD effectiveness. The aim of the pape</context>
</contexts>
<marker>Mihalcea, 2007</marker>
<rawString>Mihalcea, R. (2007). Using Wikipedia for Automatic Word Sense Disambiguation. In Proceedings of the North American Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
<author>T Chklovski</author>
</authors>
<title>Open Mind Word Expert: Creating Large Annotated Data Collections with Web Users’ Help.</title>
<date>2003</date>
<booktitle>In Proceedings of the EACL Workshop on Linguistically Annotated Corpora, Budapest. 16 Basile, de Gemmis, Lops, and Semeraro</booktitle>
<publisher>McGraw-Hill.</publisher>
<location>New York:</location>
<contexts>
<context position="8468" citStr="Mihalcea and Chklovski, 2003" startWordPosition="1274" endWordPosition="1278"> the training data. In the context of the current multilingual society, this strong requirement is even increased, since the so-called “sense-tagged data bottleneck problem” is emphasized. To address this problem, different methods have been proposed. This includes the automatic generation of sense-tagged data using monosemous relatives (Leacock et al., 1998), automatically bootstrapped disambiguation patterns (Mihalcea, 2002), parallel texts as a way to point out word senses bearing different translations in a second language (Diab, 2004), and the use of volunteer contributions over the Web (Mihalcea and Chklovski, 2003). More recently, Wikipedia has been used as a source of sense annotations for building a sense annotated corpus which can be used to train accurate sense classifiers (Mihalcea, 2007). Even though the Wikipedia-based sense annotations were found reliable, leading to accurate sense classifiers, one of the limitations of the approach is that definitions and annotations in Wikipedia are available almost exclusively for nouns. On the other hand, the increasing availability of large-scale rich (lexical) knowledge resources seems to provide new challenges to knowledge-based approaches (Navigli and Ve</context>
</contexts>
<marker>Mihalcea, Chklovski, 2003</marker>
<rawString>Mihalcea, R. and T. Chklovski (2003). Open Mind Word Expert: Creating Large Annotated Data Collections with Web Users’ Help. In Proceedings of the EACL Workshop on Linguistically Annotated Corpora, Budapest. 16 Basile, de Gemmis, Lops, and Semeraro Mitchell, T. (1997). Machine Learning. New York: McGraw-Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Nancy</author>
<author>J Véronis</author>
</authors>
<title>Introduction to the special issue on word sense disambiguation: The state of the art.</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<volume>24</volume>
<issue>1</issue>
<pages>1--40</pages>
<contexts>
<context position="2189" citStr="Nancy and Véronis, 1998" startWordPosition="323" endWordPosition="326">ance judgment or classification of documents. These problems call for alternative methods that work not only at the lexical level of the documents, but also at the meaning level. The task of Word Sense Disambiguation (WSD) consists in assigning the most appropriate meaning to a polysemous word within a given context. Applications such as machine translation, knowledge acquisition, common sense reasoning and others, require knowledge about word meanings, and WSD is essential for all these applications. The assignment of senses to words is accomplished by using two major sources of information (Nancy and Véronis, 1998): 1. the context of the word to be disambiguated, e.g. information contained within the text in which the word appears; 2. external knowledge sources, including lexical resources, as well as hand-devised knowledge sources, which provide data useful to associate words with senses. All disambiguation work involves matching the context of the instance of the word to be disambiguated with either information from an external knowledge source (also known as knowledge-driven WSD), or information about the contexts of previously disambiguated instances of the word derived from corpora (data-driven or </context>
</contexts>
<marker>Nancy, Véronis, 1998</marker>
<rawString>Nancy, I. and J. Véronis (1998). Introduction to the special issue on word sense disambiguation: The state of the art. Computational Linguistics 24(1), 1–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>Structural semantic interconnections: A knowledgebased approach to word sense disambiguation.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence</journal>
<volume>27</volume>
<issue>7</issue>
<pages>1075--1086</pages>
<contexts>
<context position="9079" citStr="Navigli and Velardi, 2005" startWordPosition="1364" endWordPosition="1367">klovski, 2003). More recently, Wikipedia has been used as a source of sense annotations for building a sense annotated corpus which can be used to train accurate sense classifiers (Mihalcea, 2007). Even though the Wikipedia-based sense annotations were found reliable, leading to accurate sense classifiers, one of the limitations of the approach is that definitions and annotations in Wikipedia are available almost exclusively for nouns. On the other hand, the increasing availability of large-scale rich (lexical) knowledge resources seems to provide new challenges to knowledge-based approaches (Navigli and Velardi, 2005; Mihalcea, 2005). Our hypothesis is that the complementarity of knowledge-based methods and corpus-based ones is the key to improve WSD effectiveness. The aim of the paper is to define a cascade hybrid method able to exploit both linguistic information coming from WORDNET-like dictionaries and statistical information coming from sense-annotated corpora. 3 A Hybrid Strategy for WSD The goal of WSD algorithms consists in assigning a word wi occurring in a document d with its appropriate meaning or sense s. The sense s is selected from a predefined set of possibilities, usually known as sense in</context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>Navigli, R. and P. Velardi (2005). Structural semantic interconnections: A knowledgebased approach to word sense disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence 27(7), 1075–1086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Roventini</author>
<author>A Alonge</author>
<author>F Bertagna</author>
<author>N Calzolari</author>
<author>J Cancila</author>
<author>C Girardi</author>
<author>B Magnini</author>
<author>R Marinelli</author>
<author>M Speranza</author>
<author>A Zampolli</author>
</authors>
<title>ItalWordNet: building a large semantic database for the automatic treatment of Italian. Computational Linguistics in Pisa - Linguistica Computazionale a Pisa. Linguistica Computazionale, Special Issue XVIII-XIX, Tomo II,</title>
<date>2003</date>
<pages>745--791</pages>
<contexts>
<context position="9733" citStr="Roventini et al., 2003" startWordPosition="1467" endWordPosition="1471">is is that the complementarity of knowledge-based methods and corpus-based ones is the key to improve WSD effectiveness. The aim of the paper is to define a cascade hybrid method able to exploit both linguistic information coming from WORDNET-like dictionaries and statistical information coming from sense-annotated corpora. 3 A Hybrid Strategy for WSD The goal of WSD algorithms consists in assigning a word wi occurring in a document d with its appropriate meaning or sense s. The sense s is selected from a predefined set of possibilities, usually known as sense inventory. We adopt ITALWORDNET (Roventini et al., 2003) as sense repository. The algorithm is composed by two procedures: 1. JIGSAW - It is a knowledge-based WSD algorithm based on the assumption that the adoption of different strategies depending on Part-of-Speech (PoS) is better than using always the same strategy. A brief description of JIGSAW is given in Section 5, more details are reported in Basile et al. (2007b), Basile et al. (2007a) and Semeraro et al. (2007). 2. Supervised learning procedure - A K-NN classifier (Mitchell, 1997), trained on MultiSemCor corpus3 is adopted. Details are given in Section 4. MultiSemCor is an English/Italian p</context>
</contexts>
<marker>Roventini, Alonge, Bertagna, Calzolari, Cancila, Girardi, Magnini, Marinelli, Speranza, Zampolli, 2003</marker>
<rawString>Roventini, A., A. Alonge, F. Bertagna, N. Calzolari, J. Cancila, C. Girardi, B. Magnini, R. Marinelli, M. Speranza, and A. Zampolli (2003). ItalWordNet: building a large semantic database for the automatic treatment of Italian. Computational Linguistics in Pisa - Linguistica Computazionale a Pisa. Linguistica Computazionale, Special Issue XVIII-XIX, Tomo II, 745–791.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Semeraro</author>
<author>M Degemmis</author>
<author>P Lops</author>
<author>P Basile</author>
</authors>
<title>Combining learning and word sense disambiguation for intelligent user profiling.</title>
<date>2007</date>
<booktitle>In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence IJCAI-07,</booktitle>
<pages>2856--2861</pages>
<publisher>M. Kaufmann,</publisher>
<location>San Francisco, California. ISBN:</location>
<contexts>
<context position="10150" citStr="Semeraro et al. (2007)" startWordPosition="1537" endWordPosition="1540">ring in a document d with its appropriate meaning or sense s. The sense s is selected from a predefined set of possibilities, usually known as sense inventory. We adopt ITALWORDNET (Roventini et al., 2003) as sense repository. The algorithm is composed by two procedures: 1. JIGSAW - It is a knowledge-based WSD algorithm based on the assumption that the adoption of different strategies depending on Part-of-Speech (PoS) is better than using always the same strategy. A brief description of JIGSAW is given in Section 5, more details are reported in Basile et al. (2007b), Basile et al. (2007a) and Semeraro et al. (2007). 2. Supervised learning procedure - A K-NN classifier (Mitchell, 1997), trained on MultiSemCor corpus3 is adopted. Details are given in Section 4. MultiSemCor is an English/Italian parallel corpus, aligned at the word level and annotated with PoS, lemma and word senses. The parallel corpus is created by exploiting the SemCor corpus4, which is a subset of the English Brown corpus containing about 700,000 running words. In SemCor, all the words are tagged by PoS, and more than 200,000 content words are also lemmatized and sense-tagged with reference to the WORDNET lexical database. SemCor has b</context>
</contexts>
<marker>Semeraro, Degemmis, Lops, Basile, 2007</marker>
<rawString>Semeraro, G., M. Degemmis, P. Lops, and P. Basile (2007). Combining learning and word sense disambiguation for intelligent user profiling. In Proceedings of the Twentieth International Joint Conference on Artificial Intelligence IJCAI-07, pp. 2856–2861. M. Kaufmann, San Francisco, California. ISBN: 978-I-57735-298-3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yuret</author>
</authors>
<title>Some experiments with a naive bayes WSD system.</title>
<date>2004</date>
<booktitle>In Senseval-3: 3th Internat. Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</booktitle>
<contexts>
<context position="7400" citStr="Yuret (2004)" startWordPosition="1119" endWordPosition="1120">l words WSD relied on information drawn from annotated corpora. The system developed by Decadt et al. (2002) uses two cascaded memory-based classifiers, combined with the use of a genetic algorithm for joint parameter optimization and feature selection. A separate word expert is learned for each ambiguous word, using a concatenated corpus of English sense tagged texts, including SemCor, SENSEVAL datasets, and a corpus built from WORDNET examples. The performance of this system on the SENSEVAL-3 English all words dataset was evaluated at 65.2%. Another top ranked system is the one developed by Yuret (2004), which combines two Naïve Bayes statistical models, one based on surrounding collocations and another one based on a bag of words around the target word. The statistical models are built based on SemCor and WORDNET, for an overall disambiguation accuracy of 64.1%. All previous systems use supervised methods, thus 2http://www.senseval.org 8 Basile, de Gemmis, Lops, and Semeraro requiring a large amount of human intervention to annotate the training data. In the context of the current multilingual society, this strong requirement is even increased, since the so-called “sense-tagged data bottlen</context>
</contexts>
<marker>Yuret, 2004</marker>
<rawString>Yuret, D. (2004). Some experiments with a naive bayes WSD system. In Senseval-3: 3th Internat. Workshop on the Evaluation of Systems for the Semantic Analysis of Text.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>