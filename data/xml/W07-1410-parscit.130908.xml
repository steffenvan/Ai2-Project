<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000131">
<title confidence="0.996808">
Machine Learning with Semantic-Based Distances Between Sentences for
Textual Entailment
</title>
<author confidence="0.956576">
Daniel Ferris
</author>
<affiliation confidence="0.899543">
TALP Research Center
</affiliation>
<address confidence="0.782141">
Software Department
Universitat Polit`ecnica de Catalunya
</address>
<email confidence="0.998269">
dferres@lsi.upc.edu
</email>
<author confidence="0.868035">
Horacio Rodriguez
</author>
<affiliation confidence="0.820903">
TALP Research Center
</affiliation>
<address confidence="0.7721585">
Software Department
Universitat Polit`ecnica de Catalunya
</address>
<email confidence="0.999007">
horacio@lsi.upc.edu
</email>
<sectionHeader confidence="0.995643" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999831230769231">
This paper describes our experiments on
Textual Entailment in the context of the
Third Pascal Recognising Textual Entail-
ment (RTE-3) Evaluation Challenge. Our
system uses a Machine Learning approach
with Support Vector Machines and Ad-
aBoost to deal with the RTE challenge. We
perform a lexical, syntactic, and semantic
analysis of the entailment pairs . From this
information we compute a set of semantic-
based distances between sentences. The re-
sults look promising specially for the QA en-
tailment task.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999973666666667">
This paper describes our participation in the RTE-
3 challenge. It is our first attempt to RTE and we
have taken profit of an analysis of the approaches
followed in previous challenges (see (Dagan et al.,
2005), and (Bar-Haim et al., 2006) for overviews
of RTE-1 and RTE-2). Our approach, however, is
based on a set of semantic-based distance measures
between sentences used by our group in previous
contests in Question Answering (TREC 2004, see
(Ferr´es et al., 2005), and CLEF 2004, see (Ferr´es
et al., 2004)) , and Automatic Summarization (DUC
2006, see (Fuentes et al., 2006)). Although the use
of such measures (distance between question and
sentences in passages candidates to contain the an-
swer, distance between query and sentences candi-
dates to be included in the summary, ...) is different
for RTE task, our claim is that with some modifica-
tions the approach can be useful in this new scenario.
</bodyText>
<page confidence="0.975987">
60
</page>
<bodyText confidence="0.999921">
The organization of this paper is as follows. Af-
ter this introduction we present in section 2 a de-
scription of the measures upon which our approach
is built. Section 3 describes in detail our proposal.
Results are discussed in section 4. Conclusions and
further work is finally included in section 5.
</bodyText>
<sectionHeader confidence="0.987638" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999751">
Our approach for computing distance measures be-
tween sentences is based on the degree of overlap-
ping between the semantic content of the two sen-
tences. Obtaining the semantic content implies a
depth Linguistic Processing. Upon this semantic
representation of the sentences several distance mea-
sures are computed. We next describe such issues.
</bodyText>
<subsectionHeader confidence="0.984293">
2.1 Linguistic Processing
</subsectionHeader>
<bodyText confidence="0.9993852">
Linguistic Processing (LP) consists of a pipe of
general purpose Natural Language (NL) processors
that performs tokenization, morphologic tagging,
lemmatization, Named Entities Recognition and
Classification (NERC) with 4 basic classes (PER-
SON, LOCATION, ORGANIZATION, and OTH-
ERS), syntactic parsing and semantic labelling, with
WordNet synsets, Magnini’s domain markers and
EuroWordNet Top Concept Ontology labels. The
Spear1 parser performs full parsing and robust de-
tection of verbal predicate arguments. The syntactic
constituent structure of each sentence (including the
specification of the head of each constituent) and the
relations among constituents (subject, direct and in-
direct object, modifiers) are obtained. As a result
</bodyText>
<footnote confidence="0.9366365">
1Spear. http://www.lsi.upc.edu/˜surdeanu/
spear.html
</footnote>
<note confidence="0.342537">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 60–65,
Prague, June 2007. @2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.941997416666666">
of the performance of these processors each sen-
tence is enriched with a lexical and syntactic lan-
guage dependent representations. A semantic lan-
guage independent representation of the sentence
(called environment) is obtained from these analy-
ses (see (Ferr´es et al., 2005) for details). The en-
vironment is a semantic network like representation
built using a process to extract the semantic units
(nodes) and the semantic relations (edges) that hold
between the different tokens in the sentence. These
units and relations belong to an ontology of about
100 semantic classes (as person, city, action, mag-
nitude, etc.) and 25 relations (mostly binary) be-
tween them (e.g. time of event, actor of action, lo-
cation of event, etc.). Both classes and relations are
related by taxonomic links (see (Ferr´es et al., 2005)
for details) allowing inheritance. Consider, for in-
stance, the sentence ”Romano Prodi 1 is 2 the 3
prime 4 minister 5 of 6 Italy 7”. The following envi-
ronment is built:
i en proper person(1), entity has quality(2),
entity(5), i en country(7), quality(4),
which entity(2,1), which quality(2,5), mod(5,7),
mod(5,4).
</bodyText>
<subsectionHeader confidence="0.999541">
2.2 Semantic-Based Distance Measures
</subsectionHeader>
<bodyText confidence="0.999025272727273">
We transform each environment into a labelled di-
rected graph representation with nodes assigned to
positions in the sentence, labelled with the corre-
sponding token, and edges to predicates (a dummy
node, 0, is used for representing unary predicates).
Only unary (e.g. entity(5) in Figure 1) and binary
(e.g. in Figure 2 which quality(2,5)) predicates are
used. Over this representation a rich variety of
lexico-semantic proximity measures between sen-
tences have been built. Each measure combines two
components:
</bodyText>
<listItem confidence="0.59416">
• A lexical component that considers the set of
</listItem>
<bodyText confidence="0.9908475">
common tokens occurring in both sentences.
The size of this set and the strength of the com-
patibility links between its members are used
for defining the measure. A flexible way of
measuring token-level compatibility has been
set ranging from word-form identity, lemma
identity, overlapping of WordNet synsets, ap-
proximate string matching between Named En-
tities etc. For instance, ”Romano Prodi” is lex-
ically compatible with ”R. Prodi” with a score
of 0.5 and with ”Prodi” with a score of 0.41.
”Italy” and ”Italian” are also compatible with
score 0.7. This component defines a set of (par-
tial) weighted mapping between the tokens of
the two sentences that will be used as anchors
in the next component.
</bodyText>
<listItem confidence="0.9995595">
• A semantic component computed over the sub-
graphs corresponding to the set of lexically
compatible nodes (anchors). Four different
measures have been defined:
– Strict overlapping of unary predicates.
– Strict overlapping of binary predicates.
– Loose overlapping of unary predicates.
– Loose overlapping of binary predicates.
</listItem>
<bodyText confidence="0.9510352">
The loose versions allow a relaxed match-
ing of predicates by climbing up in the ontol-
ogy of predicates (e.g. provided that A and B
are lexically compatible, i en city(A) can match
i en proper place(B), i en proper named entity(B),
location(B) or entity(B)) 2. Obviously, loose over-
lapping implies a penalty on the score that depends
on the length of the path between the two predicates
and their informative content.
which_quality
</bodyText>
<figureCaption confidence="0.99899">
Figure 1: Example of an environment of a sentence.
</figureCaption>
<footnote confidence="0.928019">
2The ontology contains relations as i en city
</footnote>
<construct confidence="0.506739333333333">
isa i en proper place, i en proper place isa
i en proper named entity, proper place isa location,
i en proper named entity isa entity, location isa entity
</construct>
<figure confidence="0.9974855">
0
i_en_proper_person i_en_country
entity
entity_has_quality
quality
1
2
4
5
7
Romano Prodi
is
prime
minister
Italy
mod
mod
which_entity
</figure>
<page confidence="0.983739">
61
</page>
<sectionHeader confidence="0.972441" genericHeader="method">
3 System Architecture
</sectionHeader>
<bodyText confidence="0.9987545">
We have adapted the set of measures described be-
fore for RTE in the following way:
</bodyText>
<listItem confidence="0.990641117647059">
1. We follow a Machine Learning (ML) approach
for building a classifier to perform the RTE
task. In previous applications the way of
weighting and combining the different mea-
sures was based on a crude optimization using
a development corpus.
2. We extract a more complex set of features for
describing the semantic content of the Text (T)
and the Hypothesis (H) as well as the set of se-
mantic measures between them. Table 1 con-
tains a brief summary of the features used.
3. We perform minor modifications on the token-
level compatibility measures for dealing with
the asymmetry of the entailment relation (basi-
cally using the hyponymy and the verbal entail-
ment relations of WordNet)
4. We add three new task-specific features (see
</listItem>
<tableCaption confidence="0.545847">
Table 1)
</tableCaption>
<bodyText confidence="0.999890461538462">
The overall architecture of the system is depicted
in Figure 2. As usual in ML, the system proceeds in
two phases, learning and classification. The left side
of the figure shows the learning process and the right
part the classification process. The set of examples
(tuples H, T) is first processed, in both phases, by LP
for obtaining a semantic representation of the tuple
(Hsem and Tsem). From this representation a Fea-
ture Extraction component extracts a set of features.
This set is used in the learning phase for getting a
classifier that is applied to the set of features of the
test, during the classification phase, in order to ob-
tain the answer.
</bodyText>
<sectionHeader confidence="0.99979" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.99579475">
Before the submission we have performed a set of
experiments in order to choose the Machine Learn-
ing algorithms and the training sets to apply in the
final submission.
</bodyText>
<figureCaption confidence="0.997077">
Figure 2: System Architecture.
</figureCaption>
<subsectionHeader confidence="0.960804">
4.1 Machine Learning Experiments
</subsectionHeader>
<bodyText confidence="0.999903909090909">
We used the WEKA3 ML platform (Witten and
Frank, 2005) to perform the experiments. We tested
9 different ML algorithms: AdaBoostM1, Bayes
Networks, Logistic Regression, MultiBoostAB,
Naive Bayes, RBF Network, LogitBoost (Simple Lo-
gistic in WEKA), Support Vector Machines (SMO in
WEKA), and Voted Perceptron. We used the previ-
ous corpora of the RTE Challenge (RTE-1 and RTE-
2) and the RTE-3 development test. A filtering pro-
cess has been applied removing pairs with more than
two sentences in the text or hypothesis, resulting a
total of 3335 Textual Entailment (TE) pairs. The re-
sults over 10-fold-Cross-Validation using a data set
composed by RTE-1, RTE-2, and RTE-3 develop-
ment set are shown in Table 2.
The results shown that AdaBoost, LogitBoost, and
SVM obtain the best results. Then we selected Ad-
aBoost and SVM to perform the classification of the
RTE-3 test set. The SVM algorithm tries to compute
the hyperplane that best separates the set of training
examples (the hyperplane with maximum margin)
(Vapnik, 1995). On the other hand, AdaBoost com-
</bodyText>
<footnote confidence="0.9348805">
3WEKA. http://www.cs.waikato.ac.nz/ml/
weka/
</footnote>
<figure confidence="0.99819265">
T
Training set
H
Linguistic Processing
Training set (sem)
Ts
Hs
Feature Extraction
T
Test set
H
Linguistic Processing
Test set (sem)
Hs
Feature Extraction
Ts
Machine Learning
Classifier
Answers
Features Features
</figure>
<page confidence="0.988025">
62
</page>
<table confidence="0.98315925">
Features #features Description
semantic content of T 12 #locations, #persons, #dates, #actions, ...
semantic content of H 12 ...
intersection of T and H 12 ...
Strict overlapping of unary predicates 5 length of intersection
score of intersection
ratio of intersection related to shortest env
ratio of intersection related to longest env
ratio of intersection related to both (union of)
Strict overlapping of binary predicates 5 ...
Loose overlapping of unary predicates 5 ...
Loose overlapping of binary predicates 5 ...
Verbal entailment (WordNet) 1 V1 e T, V2 e H, such that V1 verbal entails V2
Antonymy 1 A1 e T, A2 e H, such that A1 and A2 are antonyms and
no token compatible with A2
#occurs in H Negation 1 Difference between # negation tokens in H and T
</table>
<tableCaption confidence="0.976868">
Table 1: Features used for classification with Machine Learning algorithms.
</tableCaption>
<table confidence="0.9997534">
Algorithm #correct Accuracy
AdaBoostM1 1989 59.6402
BayesNet 1895 56.8216
Logistic 1951 58.5007
MultiBoostAB 1959 58.7406
NaiveBayes 1911 57.3013
RBFNetwork 1853 55.5622
LogitBoost 1972 59.1304
SVM 1972 59.1304
VotedPerceptron 1969 59.0405
</table>
<tableCaption confidence="0.996536">
Table 2: Results over 10-fold-Cross-Validation us-
</tableCaption>
<bodyText confidence="0.948427428571429">
ing a filtered data set composed by RTE-1, RTE-2,
and RTE-3 (a total of 3335 entailment pairs).
bines a set of weak classifiers into a strong one us-
ing lineal combination (Freund and Schapire, 1996).
The idea is combining many moderately accurate
rules into a highly accurate prediction rule. A weak
learning algorithm is used to find the weak rules.
</bodyText>
<subsectionHeader confidence="0.998316">
4.2 Training Set Experiments
</subsectionHeader>
<bodyText confidence="0.999053608695652">
We designed two experiments in order to decide the
best training set to apply in the RTE-3 challenge. We
performed an experiment using RTE-1 and RTE-2
data sets as a training set and the RTE-3 develop-
ment set filtered (541 TE pairs) as a test set. In this
experiment AdaBoost and SVM obtained accuracies
of 0.6672 and 0.6396 respectively (see results in Ta-
ble 3. We performed the same experiment joining
the Answer Validation Exercise4 (AVE) 2006 En-
glish data set (Pe˜nas et al., 2006) and the Microsoft
Research Paraphrase Corpus5 (MSRPC) (Dolan et
al., 2004) to the previous corpora (RTE-1 and RTE-
2) resulting a total of 8585 entailment pairs filtering
pairs with a text or a hypothesis with more than 1
sentence. In our approach we considered that para-
phrases were bidirectional entailments. The para-
phrases of the MSRPC have been used as textual en-
tailments in only one direction: the first sentence in
the paraphrase has been considered the hypothesis
and the second one has been considered the text.
Using the second corpus for training and the RTE-
3 development set as test set resulted in a notable
degradation of accuracy (see Table 3).
</bodyText>
<table confidence="0.99518925">
Accuracy
Algorithm Corpus A Corpus B
AdaBoost 66.72% 53.78%
SVM 63.95% 59.88%
</table>
<tableCaption confidence="0.999346">
Table 3: Results over the RTE-3 development set
</tableCaption>
<bodyText confidence="0.84924">
filtered (541 TE pairs) using as training set corpus A
(RTE-1 and RTE-2) and corpus B (RTE-1, RTE-2,
MSRPC, and AVE2006 English)
Finally, we performed a set of experiments to de-
tect the contribution of the different features used for
Machine Learning. These experiments revealed that
</bodyText>
<footnote confidence="0.986351">
4AVE.http://nlp.uned.es/QA/AVE
5MSRPC. http://research.microsoft.com/
nlp/msr_paraphrase.htm
</footnote>
<page confidence="0.999361">
63
</page>
<bodyText confidence="0.9996802">
in the QA and IR tasks with accuracies of 0.7450
and 0.6950 respectively in the first run.
the three most relevant features were: Strict overlap-
ping of unary predicates, Semantic content of Hy-
pothesis, and Loose overlapping of unary predicates.
</bodyText>
<subsectionHeader confidence="0.995726">
4.3 Official Results
</subsectionHeader>
<bodyText confidence="0.999920933333333">
Our official results at RTE-3 Challenge are shown
in Table 4. We submitted two experiments: the first
one with AdaBoost (run1) and the second one with
SVM (run2). Training data set for final experiments
were corpus: RTE-1 (development and test), RTE-
2 (development and test), and RTE-3 development.
The test set was the RTE-3 test set without filtering
the entailments (text or hypothesis) with more than
one sentence. In this case we joined multiple sen-
tences in a unique sentence that has been processed
by the LP component.
We obtained accuracies of 0.6062 and 0.6150. In
the QA task we obtained the best per-task results
with accuracies of 0.7450 and 0.7000 with AdaBoost
and SVM respectively.
</bodyText>
<sectionHeader confidence="0.430237" genericHeader="evaluation">
Accuracy
</sectionHeader>
<table confidence="0.999223">
Task run1 run2
AdaBoost SVM
IE 0.4350 0.4950
IR 0.6950 0.6800
QA 0.7450 0.7000
SUM 0.5500 0.5850
Overall 0.6062 0.6150
</table>
<tableCaption confidence="0.997957">
Table 4: RTE-3 official results.
</tableCaption>
<sectionHeader confidence="0.991076" genericHeader="conclusions">
5 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.999976928571429">
This paper describes our experiments on Textual En-
tailment in the context of the Third Pascal Recog-
nising Textual Entailment (RTE-3) Evaluation Chal-
lenge. Our approach uses Machine Learning al-
gorithms (SVM and AdaBoost) with semantic-based
distance measures between sentences. Although fur-
ther analysis of the results is in process, we observed
that our official per-task results at RTE-3 show a dif-
ferent distribution compared with the global results
of all system at RTE-2 challenge. The RTE-2 per-
task analysis showed that most of the systems scored
higher in accuracy in the multidocument summariza-
tion (SUM) task while in our system this measure is
low. Our system at RTE-3 challenge scored higher
</bodyText>
<sectionHeader confidence="0.990109" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999856893617021">
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Joaquin Qui˜nonero Candela, Ido Dagan,
Bernardo Magnini, and Florence d’Alch´e Buc, editors,
MLCW, volume 3944 of Lecture Notes in Computer
Science, pages 177–190. Springer.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Un-
supervised construction of large paraphrase corpora:
exploiting massively parallel news sources. In COL-
ING ’04: Proceedings of the 20th international con-
ference on Computational Linguistics, page 350, Mor-
ristown, NJ, USA. Association for Computational Lin-
guistics.
Daniel Ferr´es, Samir Kanaan, Alicia Ageno, Edgar
Gonz´alez, Horacio Rodriguez, Mihai Surdeanu, and
Jordi Turmo. 2004. The TALP-QA System for Span-
ish at CLEF 2004: Structural and Hierarchical Relax-
ing of Semantic Constraints. In Carol Peters, Paul
Clough, Julio Gonzalo, Gareth J. F. Jones, Michael
Kluck, and Bernardo Magnini, editors, CLEF, volume
3491 of Lecture Notes in Computer Science, pages
557–568. Springer.
Daniel Ferr´es, Samir Kanaan, Edgar Gonz´alez, Alicia
Ageno, Horacio Rodriguez, Mihai Surdeanu, and Jordi
Turmo. 2005. TALP-QA System at TREC 2004:
Structural and Hierarchical Relaxation Over Seman-
tic Constraints. In Proceedings of the Text Retrieval
Conference (TREC-2004).
Yoav Freund and Robert E. Schapire. 1996. Experiments
with a new boosting algorithm. In International Con-
ference on Machine Learning, pages 148–156.
Maria Fuentes, Horacio Rodriguez, Jordi Turmo, and
Daniel Ferr´es. 2006. Femsum at duc 2006: Semantic-
based approach integrated in a flexible eclectic mul-
titask summarizer architecture. In Proceedings of
the Document Understanding Conference 2006 (DUC
2006). HLT-NAACL 2006 Workshop., New York City,
NY, USA, June.
Anselmo Pe˜nas, ´Alvaro Rodrigo, Valentin Sama, and Fe-
lisa Verdejo. 2006. Overview of the answer val-
idation exercise 2006. In Working Notes for the
</reference>
<page confidence="0.986441">
64
</page>
<reference confidence="0.994225666666667">
CLEF 2006 Workshop. ISBN: 2-912335-23-x, Ali-
cante, Spain, September.
Vladimir N. Vapnik. 1995. The nature of statistical
learning theory. Springer-Verlag New York, Inc., New
York, NY, USA.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques, Second
Edition (Morgan Kaufmann Series in Data Manage-
ment Systems). Morgan Kaufmann, June.
</reference>
<sectionHeader confidence="0.989993" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.76571775">
This work has been supported by the Spanish Re-
search Dept. (TEXT-MESS, TIN2006-15265-C06-
05). Daniel Ferr´es is supported by a UPC-Recerca
grant from Universitat Polit`ecnica de Catalunya
(UPC). TALP Research Center is recognized as
a Quality Research Group (2001 SGR 00254) by
DURSI, the Research Department of the Catalan
Government.
</bodyText>
<page confidence="0.999495">
65
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.793738">
<title confidence="0.980804">Machine Learning with Semantic-Based Distances Between Sentences Textual Entailment</title>
<author confidence="0.987596">Daniel</author>
<affiliation confidence="0.982652">TALP Research Software Universitat Polit`ecnica de</affiliation>
<email confidence="0.997745">dferres@lsi.upc.edu</email>
<author confidence="0.972559">Horacio</author>
<affiliation confidence="0.982536666666667">TALP Research Software Universitat Polit`ecnica de</affiliation>
<email confidence="0.995368">horacio@lsi.upc.edu</email>
<abstract confidence="0.995840571428571">This paper describes our experiments on Textual Entailment in the context of the Third Pascal Recognising Textual Entailment (RTE-3) Evaluation Challenge. Our system uses a Machine Learning approach with Support Vector Machines and AdaBoost to deal with the RTE challenge. We perform a lexical, syntactic, and semantic analysis of the entailment pairs . From this information we compute a set of semanticbased distances between sentences. The results look promising specially for the QA entailment task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Roy Bar-Haim</author>
<author>Ido Dagan</author>
<author>Bill Dolan</author>
<author>Lisa Ferro</author>
<author>Danilo Giampiccolo</author>
<author>Bernardo Magnini</author>
<author>Idan Szpektor</author>
</authors>
<title>The second pascal recognising textual entailment challenge.</title>
<date>2006</date>
<booktitle>In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</booktitle>
<contexts>
<context position="1083" citStr="Bar-Haim et al., 2006" startWordPosition="155" endWordPosition="158">valuation Challenge. Our system uses a Machine Learning approach with Support Vector Machines and AdaBoost to deal with the RTE challenge. We perform a lexical, syntactic, and semantic analysis of the entailment pairs . From this information we compute a set of semanticbased distances between sentences. The results look promising specially for the QA entailment task. 1 Introduction This paper describes our participation in the RTE3 challenge. It is our first attempt to RTE and we have taken profit of an analysis of the approaches followed in previous challenges (see (Dagan et al., 2005), and (Bar-Haim et al., 2006) for overviews of RTE-1 and RTE-2). Our approach, however, is based on a set of semantic-based distance measures between sentences used by our group in previous contests in Question Answering (TREC 2004, see (Ferr´es et al., 2005), and CLEF 2004, see (Ferr´es et al., 2004)) , and Automatic Summarization (DUC 2006, see (Fuentes et al., 2006)). Although the use of such measures (distance between question and sentences in passages candidates to contain the answer, distance between query and sentences candidates to be included in the summary, ...) is different for RTE task, our claim is that with </context>
</contexts>
<marker>Bar-Haim, Dagan, Dolan, Ferro, Giampiccolo, Magnini, Szpektor, 2006</marker>
<rawString>Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. 2006. The second pascal recognising textual entailment challenge. In Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The pascal recognising textual entailment challenge.</title>
<date>2005</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>3944</volume>
<pages>177--190</pages>
<editor>In Joaquin Qui˜nonero Candela, Ido Dagan, Bernardo Magnini, and Florence d’Alch´e Buc, editors, MLCW,</editor>
<publisher>Springer.</publisher>
<contexts>
<context position="1054" citStr="Dagan et al., 2005" startWordPosition="150" endWordPosition="153">xtual Entailment (RTE-3) Evaluation Challenge. Our system uses a Machine Learning approach with Support Vector Machines and AdaBoost to deal with the RTE challenge. We perform a lexical, syntactic, and semantic analysis of the entailment pairs . From this information we compute a set of semanticbased distances between sentences. The results look promising specially for the QA entailment task. 1 Introduction This paper describes our participation in the RTE3 challenge. It is our first attempt to RTE and we have taken profit of an analysis of the approaches followed in previous challenges (see (Dagan et al., 2005), and (Bar-Haim et al., 2006) for overviews of RTE-1 and RTE-2). Our approach, however, is based on a set of semantic-based distance measures between sentences used by our group in previous contests in Question Answering (TREC 2004, see (Ferr´es et al., 2005), and CLEF 2004, see (Ferr´es et al., 2004)) , and Automatic Summarization (DUC 2006, see (Fuentes et al., 2006)). Although the use of such measures (distance between question and sentences in passages candidates to contain the answer, distance between query and sentences candidates to be included in the summary, ...) is different for RTE </context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Joaquin Qui˜nonero Candela, Ido Dagan, Bernardo Magnini, and Florence d’Alch´e Buc, editors, MLCW, volume 3944 of Lecture Notes in Computer Science, pages 177–190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>350</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="12086" citStr="Dolan et al., 2004" startWordPosition="1922" endWordPosition="1925"> used to find the weak rules. 4.2 Training Set Experiments We designed two experiments in order to decide the best training set to apply in the RTE-3 challenge. We performed an experiment using RTE-1 and RTE-2 data sets as a training set and the RTE-3 development set filtered (541 TE pairs) as a test set. In this experiment AdaBoost and SVM obtained accuracies of 0.6672 and 0.6396 respectively (see results in Table 3. We performed the same experiment joining the Answer Validation Exercise4 (AVE) 2006 English data set (Pe˜nas et al., 2006) and the Microsoft Research Paraphrase Corpus5 (MSRPC) (Dolan et al., 2004) to the previous corpora (RTE-1 and RTE2) resulting a total of 8585 entailment pairs filtering pairs with a text or a hypothesis with more than 1 sentence. In our approach we considered that paraphrases were bidirectional entailments. The paraphrases of the MSRPC have been used as textual entailments in only one direction: the first sentence in the paraphrase has been considered the hypothesis and the second one has been considered the text. Using the second corpus for training and the RTE3 development set as test set resulted in a notable degradation of accuracy (see Table 3). Accuracy Algori</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: exploiting massively parallel news sources. In COLING ’04: Proceedings of the 20th international conference on Computational Linguistics, page 350, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Daniel Ferr´es</author>
</authors>
<title>Samir Kanaan, Alicia Ageno, Edgar Gonz´alez, Horacio Rodriguez, Mihai Surdeanu, and Jordi Turmo.</title>
<date>2004</date>
<booktitle>of Lecture Notes in Computer Science,</booktitle>
<volume>3491</volume>
<pages>557--568</pages>
<editor>In Carol Peters, Paul Clough, Julio Gonzalo, Gareth J. F. Jones, Michael Kluck, and Bernardo Magnini, editors, CLEF,</editor>
<publisher>Springer.</publisher>
<marker>Ferr´es, 2004</marker>
<rawString>Daniel Ferr´es, Samir Kanaan, Alicia Ageno, Edgar Gonz´alez, Horacio Rodriguez, Mihai Surdeanu, and Jordi Turmo. 2004. The TALP-QA System for Spanish at CLEF 2004: Structural and Hierarchical Relaxing of Semantic Constraints. In Carol Peters, Paul Clough, Julio Gonzalo, Gareth J. F. Jones, Michael Kluck, and Bernardo Magnini, editors, CLEF, volume 3491 of Lecture Notes in Computer Science, pages 557–568. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Ferr´es</author>
<author>Samir Kanaan</author>
<author>Edgar Gonz´alez</author>
<author>Alicia Ageno</author>
<author>Horacio Rodriguez</author>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
</authors>
<title>TALP-QA System at TREC 2004: Structural and Hierarchical Relaxation Over Semantic Constraints.</title>
<date>2005</date>
<booktitle>In Proceedings of the Text Retrieval Conference (TREC-2004).</booktitle>
<marker>Ferr´es, Kanaan, Gonz´alez, Ageno, Rodriguez, Surdeanu, Turmo, 2005</marker>
<rawString>Daniel Ferr´es, Samir Kanaan, Edgar Gonz´alez, Alicia Ageno, Horacio Rodriguez, Mihai Surdeanu, and Jordi Turmo. 2005. TALP-QA System at TREC 2004: Structural and Hierarchical Relaxation Over Semantic Constraints. In Proceedings of the Text Retrieval Conference (TREC-2004).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoav Freund</author>
<author>Robert E Schapire</author>
</authors>
<title>Experiments with a new boosting algorithm.</title>
<date>1996</date>
<booktitle>In International Conference on Machine Learning,</booktitle>
<pages>148--156</pages>
<contexts>
<context position="11344" citStr="Freund and Schapire, 1996" startWordPosition="1797" endWordPosition="1800">n H Negation 1 Difference between # negation tokens in H and T Table 1: Features used for classification with Machine Learning algorithms. Algorithm #correct Accuracy AdaBoostM1 1989 59.6402 BayesNet 1895 56.8216 Logistic 1951 58.5007 MultiBoostAB 1959 58.7406 NaiveBayes 1911 57.3013 RBFNetwork 1853 55.5622 LogitBoost 1972 59.1304 SVM 1972 59.1304 VotedPerceptron 1969 59.0405 Table 2: Results over 10-fold-Cross-Validation using a filtered data set composed by RTE-1, RTE-2, and RTE-3 (a total of 3335 entailment pairs). bines a set of weak classifiers into a strong one using lineal combination (Freund and Schapire, 1996). The idea is combining many moderately accurate rules into a highly accurate prediction rule. A weak learning algorithm is used to find the weak rules. 4.2 Training Set Experiments We designed two experiments in order to decide the best training set to apply in the RTE-3 challenge. We performed an experiment using RTE-1 and RTE-2 data sets as a training set and the RTE-3 development set filtered (541 TE pairs) as a test set. In this experiment AdaBoost and SVM obtained accuracies of 0.6672 and 0.6396 respectively (see results in Table 3. We performed the same experiment joining the Answer Val</context>
</contexts>
<marker>Freund, Schapire, 1996</marker>
<rawString>Yoav Freund and Robert E. Schapire. 1996. Experiments with a new boosting algorithm. In International Conference on Machine Learning, pages 148–156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Fuentes</author>
<author>Horacio Rodriguez</author>
<author>Jordi Turmo</author>
<author>Daniel Ferr´es</author>
</authors>
<title>Femsum at duc 2006: Semanticbased approach integrated in a flexible eclectic multitask summarizer architecture.</title>
<date>2006</date>
<booktitle>In Proceedings of the Document Understanding Conference</booktitle>
<location>New York City, NY, USA,</location>
<marker>Fuentes, Rodriguez, Turmo, Ferr´es, 2006</marker>
<rawString>Maria Fuentes, Horacio Rodriguez, Jordi Turmo, and Daniel Ferr´es. 2006. Femsum at duc 2006: Semanticbased approach integrated in a flexible eclectic multitask summarizer architecture. In Proceedings of the Document Understanding Conference 2006 (DUC 2006). HLT-NAACL 2006 Workshop., New York City, NY, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anselmo Pe˜nas</author>
<author>´Alvaro Rodrigo</author>
<author>Valentin Sama</author>
<author>Felisa Verdejo</author>
</authors>
<title>Overview of the answer validation exercise</title>
<date>2006</date>
<booktitle>In Working Notes for the CLEF 2006 Workshop. ISBN:</booktitle>
<pages>2--912335</pages>
<location>Alicante, Spain,</location>
<marker>Pe˜nas, Rodrigo, Sama, Verdejo, 2006</marker>
<rawString>Anselmo Pe˜nas, ´Alvaro Rodrigo, Valentin Sama, and Felisa Verdejo. 2006. Overview of the answer validation exercise 2006. In Working Notes for the CLEF 2006 Workshop. ISBN: 2-912335-23-x, Alicante, Spain, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>The nature of statistical learning theory.</title>
<date>1995</date>
<publisher>Springer-Verlag</publisher>
<location>New York,</location>
<contexts>
<context position="9726" citStr="Vapnik, 1995" startWordPosition="1544" endWordPosition="1545">A filtering process has been applied removing pairs with more than two sentences in the text or hypothesis, resulting a total of 3335 Textual Entailment (TE) pairs. The results over 10-fold-Cross-Validation using a data set composed by RTE-1, RTE-2, and RTE-3 development set are shown in Table 2. The results shown that AdaBoost, LogitBoost, and SVM obtain the best results. Then we selected AdaBoost and SVM to perform the classification of the RTE-3 test set. The SVM algorithm tries to compute the hyperplane that best separates the set of training examples (the hyperplane with maximum margin) (Vapnik, 1995). On the other hand, AdaBoost com3WEKA. http://www.cs.waikato.ac.nz/ml/ weka/ T Training set H Linguistic Processing Training set (sem) Ts Hs Feature Extraction T Test set H Linguistic Processing Test set (sem) Hs Feature Extraction Ts Machine Learning Classifier Answers Features Features 62 Features #features Description semantic content of T 12 #locations, #persons, #dates, #actions, ... semantic content of H 12 ... intersection of T and H 12 ... Strict overlapping of unary predicates 5 length of intersection score of intersection ratio of intersection related to shortest env ratio of inters</context>
</contexts>
<marker>Vapnik, 1995</marker>
<rawString>Vladimir N. Vapnik. 1995. The nature of statistical learning theory. Springer-Verlag New York, Inc., New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques, Second Edition (Morgan Kaufmann Series in Data Management Systems).</booktitle>
<publisher>Morgan Kaufmann,</publisher>
<contexts>
<context position="8760" citStr="Witten and Frank, 2005" startWordPosition="1386" endWordPosition="1389">ng a semantic representation of the tuple (Hsem and Tsem). From this representation a Feature Extraction component extracts a set of features. This set is used in the learning phase for getting a classifier that is applied to the set of features of the test, during the classification phase, in order to obtain the answer. 4 Experiments Before the submission we have performed a set of experiments in order to choose the Machine Learning algorithms and the training sets to apply in the final submission. Figure 2: System Architecture. 4.1 Machine Learning Experiments We used the WEKA3 ML platform (Witten and Frank, 2005) to perform the experiments. We tested 9 different ML algorithms: AdaBoostM1, Bayes Networks, Logistic Regression, MultiBoostAB, Naive Bayes, RBF Network, LogitBoost (Simple Logistic in WEKA), Support Vector Machines (SMO in WEKA), and Voted Perceptron. We used the previous corpora of the RTE Challenge (RTE-1 and RTE2) and the RTE-3 development test. A filtering process has been applied removing pairs with more than two sentences in the text or hypothesis, resulting a total of 3335 Textual Entailment (TE) pairs. The results over 10-fold-Cross-Validation using a data set composed by RTE-1, RTE-</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Ian H. Witten and Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques, Second Edition (Morgan Kaufmann Series in Data Management Systems). Morgan Kaufmann, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>