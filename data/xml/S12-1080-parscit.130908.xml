<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001340">
<title confidence="0.9957225">
IRIT: Textual Similarity Combining Conceptual Similarity with an N-Gram
Comparison Method
</title>
<author confidence="0.971104">
Davide Buscaldi, Ronan Tournier, Nathalie Aussenac-Gilles and Josiane Mothe
</author>
<affiliation confidence="0.824039">
IRIT
</affiliation>
<address confidence="0.8398585">
118 Route de Narbonne
Toulouse (France)
</address>
<email confidence="0.992932">
{davide.buscaldi,ronan.tournier}@irit.fr,
{nathalie.aussenac,josiane.mothe}@irit.fr
</email>
<sectionHeader confidence="0.998538" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999838857142857">
This paper describes the participation of the
IRIT team to SemEval 2012 Task 6 (Seman-
tic Textual Similarity). The method used con-
sists of a n-gram based comparison method
combined with a conceptual similarity mea-
sure that uses WordNet to calculate the sim-
ilarity between a pair of concepts.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999309">
The system used for the participation of the IRIT
team (composed by members of the research groups
SIG and MELODI) to the Semantic Textual Similar-
ity (STS) task (Agirre et al., 2012) is based on two
sub-modules:
</bodyText>
<listItem confidence="0.981863166666667">
• a module that calculates the similarity between
sentences using n-gram based similarity;
• a module that calculates the similarity between
concepts in the two sentences, using a concept
similarity measure and WordNet (Miller, 1995)
as a resource.
</listItem>
<bodyText confidence="0.999713818181818">
In Figure 1, we show the structure of the sys-
tem and the connections between the main compo-
nents. The input phrases are passed on one hand
directly to the n-gram similarity module, and on the
other they are annoted with the Stanford POS Tag-
ger (Toutanova et al., 2003). All nouns and verbs are
extracted from the tagged phrases and WordNet is
searched for synsets corresponding to the extracted
nouns and nouns associated to the verbs by the de-
rived terms relationship. The synsets are the con-
cepts used by the conceptual similarity module to
</bodyText>
<figureCaption confidence="0.999712">
Figure 1: Schema of the system.
</figureCaption>
<bodyText confidence="0.999840555555556">
calculate the concept similarity. Each module cal-
culates a similarity score using its own method; the
final similarity value is calculated as the geometric
average between the two scores, multiplied by 5 in
order to comply with the task specifications.
The n-gram based similarity relies on the idea
that two sentences are semantically related if they
contain a long enough sub-sequence of non-empty
terms. Google Web 1T (Brants and Franz, 2006)
has been used to calculate term idf, which is used
as a measure of the importance of the terms. The
conceptual similarity is based on the idea that, given
an ontology, two concepts are semantically similar
if their distance from a common ancestor is small
enough. We used three different measures: the Wu-
Palmer similarity measure (Wu and Palmer, 1994)
and two “Proxigenea” measures (Dudognon et al.,
2010). In the following we will explain in detail how
</bodyText>
<figure confidence="0.999269133333333">
Google
Web 1T
Geometric Average
and Normalisation
N-gram similarity
module
Score
Phrases
POS Tagger
Concept
similarity
module
Concept
Extraction
WordNet
</figure>
<page confidence="0.947721">
552
</page>
<note confidence="0.591472">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 552–556,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.708709">
each similarity module works.
</bodyText>
<sectionHeader confidence="0.79992" genericHeader="method">
2 N-Gram based Similarity
</sectionHeader>
<bodyText confidence="0.998181266666667">
N-gram based similarity is based on the Clustered
Keywords Positional Distance (CKPD) model pro-
posed in (Buscaldi et al., 2009). This model was
originally proposed for passage retrieval in the field
of Question Answering (QA), and it has been im-
plemented in the JIRS system1. In (Buscaldi et al.,
2006), JIRS showed to be able to obtain a better an-
swer coverage in the Question Answering task than
other traditional passage retrieval models based on
Vector Space Model, such as Lucene2. The model
has been adapted for this task by calculating the idf
weights for each term using the frequency value pro-
vided by Google Web 1T.
The similarity between a text fragment (or pas-
sage) p and another text fragment q is calculated as:
</bodyText>
<equation confidence="0.612418">
�En
i=1 wi
</equation>
<bodyText confidence="0.9779535">
Where P is the set of n-grams with the highest
weight in p, where all terms are also contained in q;
Q is the set of all the possible j-grams in q and n
is the total number of terms in the longest passage.
The weights for each term and each n-gram are cal-
culated as:
</bodyText>
<listItem confidence="0.911253">
• wi calculates the weight of the term tI as:
</listItem>
<equation confidence="0.902137333333333">
log(ni)
wi = 1 − (2)
1 + log(N)
</equation>
<bodyText confidence="0.99818825">
Where ni is the frequency of term ti in the
Google Web 1T collection, and N is the fre-
quency of the most frequent term in the Google
Web 1T collection.
</bodyText>
<listItem confidence="0.839508">
• the function h(x, P) measures the weight of
each n-gram and is defined as:
</listItem>
<equation confidence="0.95037225">
� 9
(
h x Pj) = EL1 wk if x E Pj 3
( 0 otherwise )
</equation>
<footnote confidence="0.9962805">
1http://sourceforge.net/projects/jirs/
2http://lucene.apache.org/
</footnote>
<bodyText confidence="0.995372666666667">
Where wk is the weight of the k-th term (see
Equation 2) and j is the number of terms that
compose the n-gram x;
</bodyText>
<equation confidence="0.776128">
• d(x,x�ax) is a distance factor which reduces the
1
</equation>
<bodyText confidence="0.999416166666667">
weight of the n-grams that are far from the
heaviest n-gram. The function d(x, xmax) de-
termines numerically the value of the separa-
tion according to the number of words between
a n-gram and the heaviest one. That function is
defined as show in Equation 4 :
</bodyText>
<equation confidence="0.730964">
d(x, xmax) = 1 + k· ln(1 + L) (4)
</equation>
<bodyText confidence="0.992758533333333">
Where k is a factor that determines the impor-
tance of the distance in the similarity calcula-
tion and L is the number of words between a
n-gram and the heaviest one (see Equation 3).
In our experiments, k was set to 0.1, a default
value used in JIRS.
For instance, given the following two sentences:
“Mr. President, enlargement is essential for the con-
struction of a strong and united European continent”
and “Mr. President, widening is essential for the
construction of a strong and plain continent of Eu-
rope”, the longest n-grams shared by the two sen-
tences are: “Mr. President”, “is essential for the
construction of a strong and”, “continent”.
term w(term)
Mr 0.340
President 0.312
is 0.159
essential 0.353
for 0.153
the 0.104
construction 0.332
of 0.120
a 0.139
strong 0.329
and 0.121
continent 0.427
of 0.120
Europe 0.308
widening 0.464
</bodyText>
<tableCaption confidence="0.991172">
Table 1: Term weights (idf) calculated using the fre-
quency for each term in Google Web 1T unigrams set.
</tableCaption>
<equation confidence="0.9714784">
5im(p, q) =
bxEQ
� h(x, P) 1
d(x, xmax)
(1)
</equation>
<page confidence="0.984223">
553
</page>
<table confidence="0.775993545454545">
proximity between two members of the same fam-
ily. The measure has been named “ProxiGenea”
(from the french Proximit´e G´en´ealogique, genealog-
ical proximity). We took into account three versions
of the ProxiGenea measure:
d(c0)2
pg1(c1, c2) = (7)
d(c1) ∗ d(c2)
This measure is very similar to the Wu-Palmer sim-
ilarity measure, but it emphasizes the distances be-
tween concepts;
</table>
<figureCaption confidence="0.938583">
Figure 2: Visualisation of depth calculation.
</figureCaption>
<bodyText confidence="0.9999095">
The weights have been calculated with Formula
2, using the frequencies from Google Web 1T. The
weights for each of the longest n-grams are 0.652,
1.809 and 0.427 respectively; their sum is 2.888
which divided by all the term weights contained
in the sentence gives 0.764 which is the similarity
score between the two sentences as calculated by the
n-gram based method.
</bodyText>
<sectionHeader confidence="0.994318" genericHeader="method">
3 Conceptual Similarity
</sectionHeader>
<bodyText confidence="0.99972675">
Given Cp and Cq as the sets of concepts contained in
sentence p and q, respectively, with |Cp |≥ |Cq|, the
conceptual similarity between p and q is calculated
as:
</bodyText>
<equation confidence="0.954378666666667">
s(c1, c2)
(5)
|Cp|
</equation>
<bodyText confidence="0.9996272">
where s(c1, c2) is a concept similarity measure.
Concept similarity can be calculated by different
ways. Wu and Palmer introduced in (Wu and
Palmer, 1994) a concept similarity measure defined
as:
</bodyText>
<equation confidence="0.999457666666667">
2 · d(c0)
s(c1,c2) = (6)
d(c1) + d(c2)
</equation>
<bodyText confidence="0.999485">
c0 is the most specific concept that is present both
in the synset path of c1 and c2 (see Figure 2 for de-
tails). The function returning the depth of a concept
is noted with d.
</bodyText>
<subsectionHeader confidence="0.991704">
3.1 ProxiGenea
</subsectionHeader>
<bodyText confidence="0.99992125">
By making an analogy between a family tree and
the concept hierarchy in WordNet, (Dudognon et al.,
2010; Ralalason, 2010) proposed a concept similar-
ity measure based on the principle of evaluating the
</bodyText>
<equation confidence="0.998466">
d(c0)
pg2(c1,c2) = d(c1) + d(c2) − d(c0) (8)
</equation>
<bodyText confidence="0.9989514">
In this measure, the more are the elements which are
not shared between the paths of c1 and c2, the more
the score decreases. However, if the elements are
placed more deeply in the ontology, the decrease is
less important.
</bodyText>
<equation confidence="0.9983715">
1
pg3(c1, c2) = 1 + d(c1) + d(c2) − 2 · d(c0) (9)
</equation>
<bodyText confidence="0.999980333333333">
In Table 2 we show the weights that have been
calculated for each concept, using all the above sim-
ilarity measures, and the concept that provided the
maximum weight. No Word Sense Disambiguation
process is carried out; therefore, the scores are cal-
culated taking into account all the possible senses
for the word. If the same concept is present in both
sentences, it obtains always a score of 1. In the other
cases, the maximum similarity value obtained with
any other concept is retained.
From the example in Table 2 we can see that Wu-
Palmer tends to give to the concepts a higher simi-
larity value than Proxigenea3.
The final score for the above example is cal-
culated as the geometric mean between the scores
obtained in Table 2 and 0.764 obtained from the
n-gram based similarity module, multiplied by 5.
Therefore, for each similarity measure, the final
scores of the example are, respectively: 4.029,
3.869, 3.921 and 3.703. The correct similarity value,
according to the gold standard, was 4.600.
</bodyText>
<figure confidence="0.6033568">
ss(p, q) =
max
c2ECg
E
c1ECp
</figure>
<page confidence="0.995108">
554
</page>
<table confidence="0.999777785714286">
c1, c2 wp pg1 pg2 pg3
Mr 1.000 1.000 1.000 1.000
Mr
President 1.000 1.000 1.000 1.000
President
construction 1.000 1.000 1.000 1.000
construction
continent 1.000 1.000 1.000 1.000
continent
Europe 0.400 0.160 0.250 0.143
continent
widening 0.737 0.544 0.583 0.167
enlargement
score 0.850 0.784 0.805 0.718
</table>
<tableCaption confidence="0.9887206">
Table 2: Maximum conceptual similarity weights using
the different formulae for the concepts in the example.
c1: first concept, c2: concept for which the maximum
similarity value was calculated. wp: Wu-Palmer similar-
ity; pgx: Proxigenea similarity. score is the result of (5).
</tableCaption>
<sectionHeader confidence="0.996022" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.99994025">
Before the official runs we carried out an evalua-
tion to select the best similarity measures over the
training set provided by the organisers. The results
of this evaluation are shown in Table 3. The mea-
sure selected is the normalised Pearson correlation
(Agirre et al., 2012). We evaluated also the use of
the product instead of the geometric mean for the
combination of the two scores.
</bodyText>
<table confidence="0.998675583333333">
Geometric mean
MSRpar MSRvid SMT-Eur All
pg1 0.489 0.602 0.587 0.559
pg2 0.490 0.596 0.586 0.558
pg3 0.470 0.657 0.552 0.560
wp 0.494 0.572 0.592 0.552
Scalar product
MSRpar MSRvid SMT-Eur All
pg1 0.469 0.601 0.487 0.519
pg2 0.471 0.597 0.487 0.518
pg3 0.447 0.637 0.459 0.514
wp 0.476 0.577 0.492 0.515
</table>
<tableCaption confidence="0.997979333333333">
Table 3: Results on training corpus, comparison of dif-
ferent conceptual similarity measures and combination
method. Top: geometric mean, bottom: product.
</tableCaption>
<bodyText confidence="0.999291083333333">
We used these results to select the final config-
urations for our participation to the STS task: we
selected to exclude Proxigenea 2 and to use the ge-
ometric mean to combine the scores of the n-gram
based similarity module and the conceptual similar-
ity module. Wu-Palmer similarity allowed to obtain
the best results on two train sets but Proxigenea 3
was the similarity measure that obtained the best av-
erage score thanks to the good result on MSRvid.
The official results obtained by our system are
shown in Table 4, with the ranking obtained for each
test set. We could observe that the system was well
</bodyText>
<table confidence="0.999299714285714">
r best pg3 pg1 wp
MSRPar 60 0.734 0.417 0.429 0.433
MSRvid 58 0.880 0.673 0.612 0.583
SMTeur 7 0.567 0.518 0.495 0.486
OnWN 64 0.727 0.553 0.539 0.532
SMTnews 55 0.608 0.369 0.361 0.348
All 58 0.677 0.520 0.501 0.490
</table>
<tableCaption confidence="0.838007">
Table 4: Results obtained on each test set, grouped by
conceptual similarity method. r indicates the ranking
among all the participants teams.
</tableCaption>
<bodyText confidence="0.99988175">
behind the best system in most test sets, except for
SMTeur. This was expected since our system does
not use a machine learning approach and is com-
pletely unsupervised, while the best systems used
supervised learning. We observed also that the be-
haviour of the concept similarity measures was dif-
ferent from the behaviour on the training sets. In the
competition, the best results were always obtained
with Proxigenea3 instead of Wu-Palmer, except for
the MSRpar test set.
In Table 4 we extrapolated the results for the com-
posing methods and compared them with the result
obtained after their combination. We used the pg3
configuration for the conceptual similarity measure.
From these results, we can observe that MSRvid
was a test set where the conceptual similarity alone
would have resulted better than the combination of
scores, while SMT-news was the test set where the
CKPD measure obtained the best results in compar-
ison to the result obtained by the conceptual simi-
larity alone. It was quite surprising to observe such
a good result for a method that does not take into
account any information about the structure of the
sentences, actually viewing them as “bags of con-
</bodyText>
<page confidence="0.99396">
555
</page>
<table confidence="0.9997355">
Combined pg3 CKPD
MSRPar 0.417 0.412 0.417
MSRvid 0.673 0.777 0.548
SMTeuroparl 0.518 0.486 0.467
OnWN 0.553 0.544 0.505
SMTnews 0.369 0.266 0.408
</table>
<tableCaption confidence="0.89619225">
Table 5: Results obtained for each test set using only the
conceptual similarity measure (pg3) and only the struc-
tural similarity measure (CKPD), compared to the re-
sult obtained by the complete system (Combined).
</tableCaption>
<bodyText confidence="0.999630428571429">
cepts”. This is probably due to the fact that SMT-
news is a corpus composed of automatically trans-
lated sentences, where structural similarity is an im-
portant clue for determining overall semantic sim-
ilarity. On the other hand, MSRvid sentences are
very short, and CKPD is in most cases unable to cap-
ture the semantic similarity.
</bodyText>
<sectionHeader confidence="0.999779" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999962611111111">
The proposed method combined a measure of struc-
tural similarity and a measure of conceptual simi-
larity based on WordNet. With the participation to
this task, we were interested in studying the differ-
ences between different conceptual similarity mea-
sures and in determining whether they can be used
to effectively measure the semantic similarity of text
fragments. The obtained results showed that Proxi-
genea 3 allowed us to obtain the best results, indicat-
ing that under the test conditions and with WordNet
as a resource it overperforms the Wu-Palmer mea-
sure. Further studies may be required in order to
determine if these results can be generalised to other
collections and in using different ontologies. We are
also interested in comparing the method to the Lin
concept similarity measure (Lin, 1998) which takes
into account also the importance of the local root
concept.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999522555555555">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gon-
zalez. 2012. A pilot on semantic textual similarity.
In Proceedings of the 6th International Workshop on
Semantic Evaluation (SemEval 2012), in conjunction
with the First Joint Conference on Lexical and Compu-
tational Semantcis (*SEM 2012), Montreal, Quebec,
Canada.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
corpus version 1.1.
Davide Buscaldi, Jos´e Manuel G´omez, Paolo Rosso, and
Emilio Sanchis. 2006. N-gram vs. keyword-based
passage retrieval for question answering. In CLEF,
pages 377–384.
Davide Buscaldi, Paolo Rosso, Jos´e Manuel G´omez, and
Emilio Sanchis. 2009. Answering questions with an
n-gram based passage retrieval engine. Journal of In-
telligent Information Systems (JIIS), 34(2):113–134.
Damien Dudognon, Gilles Hubert, and Bachelin Jhonn
Victorino Ralalason. 2010. Proxig´en´ea : Une mesure
de similarit´e conceptuelle. In Proceedings of the Col-
loque Veille Strat´egique Scientifique et Technologique
(VSST 2010).
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the Fifteenth In-
ternational Conference on Machine Learning, ICML
’98, pages 296–304, San Francisco, CA, USA. Mor-
gan Kaufmann Publishers Inc.
George A. Miller. 1995. Wordnet: a lexical database for
english. Commun. ACM, 38(11):39–41, November.
Bachelin Ralalason. 2010. Repr´esentation multi-facette
des documents pour leur acc`es s´emantique. Ph.D. the-
sis, Universit´e Paul Sabatier, Toulouse, September. in
French.
Kristina Toutanova, Dan Klein, Christopher D. Manning,
and Yoram Singer. 2003. Feature-rich part-of-speech
tagging with a cyclic dependency network. In Pro-
ceedings of the 2003 Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume 1,
NAACL ’03, pages 173–180.
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computational Lin-
guistics, ACL ’94, pages 133–138, Stroudsburg, PA,
USA. Association for Computational Linguistics.
</reference>
<page confidence="0.998562">
556
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.447720">
<title confidence="0.932562666666667">IRIT: Textual Similarity Combining Conceptual Similarity with an N-Gram Comparison Method Buscaldi, Ronan Tournier, Nathalie Aussenac-Gilles</title>
<author confidence="0.677672">Route de_Toulouse</author>
<abstract confidence="0.995004375">This paper describes the participation of the IRIT team to SemEval 2012 Task 6 (Semantic Textual Similarity). The method used consists of a n-gram based comparison method combined with a conceptual similarity measure that uses WordNet to calculate the similarity between a pair of concepts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez</author>
</authors>
<title>A pilot on semantic textual similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantcis (*SEM 2012),</booktitle>
<location>Montreal, Quebec, Canada.</location>
<contexts>
<context position="792" citStr="Agirre et al., 2012" startWordPosition="109" endWordPosition="112">18 Route de Narbonne Toulouse (France) {davide.buscaldi,ronan.tournier}@irit.fr, {nathalie.aussenac,josiane.mothe}@irit.fr Abstract This paper describes the participation of the IRIT team to SemEval 2012 Task 6 (Semantic Textual Similarity). The method used consists of a n-gram based comparison method combined with a conceptual similarity measure that uses WordNet to calculate the similarity between a pair of concepts. 1 Introduction The system used for the participation of the IRIT team (composed by members of the research groups SIG and MELODI) to the Semantic Textual Similarity (STS) task (Agirre et al., 2012) is based on two sub-modules: • a module that calculates the similarity between sentences using n-gram based similarity; • a module that calculates the similarity between concepts in the two sentences, using a concept similarity measure and WordNet (Miller, 1995) as a resource. In Figure 1, we show the structure of the system and the connections between the main components. The input phrases are passed on one hand directly to the n-gram similarity module, and on the other they are annoted with the Stanford POS Tagger (Toutanova et al., 2003). All nouns and verbs are extracted from the tagged p</context>
<context position="9657" citStr="Agirre et al., 2012" startWordPosition="1643" endWordPosition="1646">0.583 0.167 enlargement score 0.850 0.784 0.805 0.718 Table 2: Maximum conceptual similarity weights using the different formulae for the concepts in the example. c1: first concept, c2: concept for which the maximum similarity value was calculated. wp: Wu-Palmer similarity; pgx: Proxigenea similarity. score is the result of (5). 4 Evaluation Before the official runs we carried out an evaluation to select the best similarity measures over the training set provided by the organisers. The results of this evaluation are shown in Table 3. The measure selected is the normalised Pearson correlation (Agirre et al., 2012). We evaluated also the use of the product instead of the geometric mean for the combination of the two scores. Geometric mean MSRpar MSRvid SMT-Eur All pg1 0.489 0.602 0.587 0.559 pg2 0.490 0.596 0.586 0.558 pg3 0.470 0.657 0.552 0.560 wp 0.494 0.572 0.592 0.552 Scalar product MSRpar MSRvid SMT-Eur All pg1 0.469 0.601 0.487 0.519 pg2 0.471 0.597 0.487 0.518 pg3 0.447 0.637 0.459 0.514 wp 0.476 0.577 0.492 0.515 Table 3: Results on training corpus, comparison of different conceptual similarity measures and combination method. Top: geometric mean, bottom: product. We used these results to selec</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez. 2012. A pilot on semantic textual similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantcis (*SEM 2012), Montreal, Quebec, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1t 5-gram corpus version 1.1.</title>
<date>2006</date>
<contexts>
<context position="2091" citStr="Brants and Franz, 2006" startWordPosition="325" endWordPosition="328">uns and nouns associated to the verbs by the derived terms relationship. The synsets are the concepts used by the conceptual similarity module to Figure 1: Schema of the system. calculate the concept similarity. Each module calculates a similarity score using its own method; the final similarity value is calculated as the geometric average between the two scores, multiplied by 5 in order to comply with the task specifications. The n-gram based similarity relies on the idea that two sentences are semantically related if they contain a long enough sub-sequence of non-empty terms. Google Web 1T (Brants and Franz, 2006) has been used to calculate term idf, which is used as a measure of the importance of the terms. The conceptual similarity is based on the idea that, given an ontology, two concepts are semantically similar if their distance from a common ancestor is small enough. We used three different measures: the WuPalmer similarity measure (Wu and Palmer, 1994) and two “Proxigenea” measures (Dudognon et al., 2010). In the following we will explain in detail how Google Web 1T Geometric Average and Normalisation N-gram similarity module Score Phrases POS Tagger Concept similarity module Concept Extraction </context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram corpus version 1.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Buscaldi</author>
<author>Jos´e Manuel G´omez</author>
<author>Paolo Rosso</author>
<author>Emilio Sanchis</author>
</authors>
<title>N-gram vs. keyword-based passage retrieval for question answering. In</title>
<date>2006</date>
<booktitle>CLEF,</booktitle>
<pages>377--384</pages>
<marker>Buscaldi, G´omez, Rosso, Sanchis, 2006</marker>
<rawString>Davide Buscaldi, Jos´e Manuel G´omez, Paolo Rosso, and Emilio Sanchis. 2006. N-gram vs. keyword-based passage retrieval for question answering. In CLEF, pages 377–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Buscaldi</author>
<author>Paolo Rosso</author>
<author>Jos´e Manuel G´omez</author>
<author>Emilio Sanchis</author>
</authors>
<title>Answering questions with an n-gram based passage retrieval engine.</title>
<date>2009</date>
<journal>Journal of Intelligent Information Systems (JIIS),</journal>
<volume>34</volume>
<issue>2</issue>
<marker>Buscaldi, Rosso, G´omez, Sanchis, 2009</marker>
<rawString>Davide Buscaldi, Paolo Rosso, Jos´e Manuel G´omez, and Emilio Sanchis. 2009. Answering questions with an n-gram based passage retrieval engine. Journal of Intelligent Information Systems (JIIS), 34(2):113–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damien Dudognon</author>
<author>Gilles Hubert</author>
<author>Bachelin Jhonn Victorino Ralalason</author>
</authors>
<title>Proxig´en´ea : Une mesure de similarit´e conceptuelle.</title>
<date>2010</date>
<booktitle>In Proceedings of the Colloque Veille Strat´egique Scientifique et Technologique (VSST</booktitle>
<contexts>
<context position="2497" citStr="Dudognon et al., 2010" startWordPosition="393" endWordPosition="396">he task specifications. The n-gram based similarity relies on the idea that two sentences are semantically related if they contain a long enough sub-sequence of non-empty terms. Google Web 1T (Brants and Franz, 2006) has been used to calculate term idf, which is used as a measure of the importance of the terms. The conceptual similarity is based on the idea that, given an ontology, two concepts are semantically similar if their distance from a common ancestor is small enough. We used three different measures: the WuPalmer similarity measure (Wu and Palmer, 1994) and two “Proxigenea” measures (Dudognon et al., 2010). In the following we will explain in detail how Google Web 1T Geometric Average and Normalisation N-gram similarity module Score Phrases POS Tagger Concept similarity module Concept Extraction WordNet 552 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 552–556, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics each similarity module works. 2 N-Gram based Similarity N-gram based similarity is based on the Clustered Keywords Positional Distance (CKPD) model proposed in (Buscaldi et al., 2009). This model was originally proposed for p</context>
<context position="7334" citStr="Dudognon et al., 2010" startWordPosition="1248" endWordPosition="1251">|Cp |≥ |Cq|, the conceptual similarity between p and q is calculated as: s(c1, c2) (5) |Cp| where s(c1, c2) is a concept similarity measure. Concept similarity can be calculated by different ways. Wu and Palmer introduced in (Wu and Palmer, 1994) a concept similarity measure defined as: 2 · d(c0) s(c1,c2) = (6) d(c1) + d(c2) c0 is the most specific concept that is present both in the synset path of c1 and c2 (see Figure 2 for details). The function returning the depth of a concept is noted with d. 3.1 ProxiGenea By making an analogy between a family tree and the concept hierarchy in WordNet, (Dudognon et al., 2010; Ralalason, 2010) proposed a concept similarity measure based on the principle of evaluating the d(c0) pg2(c1,c2) = d(c1) + d(c2) − d(c0) (8) In this measure, the more are the elements which are not shared between the paths of c1 and c2, the more the score decreases. However, if the elements are placed more deeply in the ontology, the decrease is less important. 1 pg3(c1, c2) = 1 + d(c1) + d(c2) − 2 · d(c0) (9) In Table 2 we show the weights that have been calculated for each concept, using all the above similarity measures, and the concept that provided the maximum weight. No Word Sense Disa</context>
</contexts>
<marker>Dudognon, Hubert, Ralalason, 2010</marker>
<rawString>Damien Dudognon, Gilles Hubert, and Bachelin Jhonn Victorino Ralalason. 2010. Proxig´en´ea : Une mesure de similarit´e conceptuelle. In Proceedings of the Colloque Veille Strat´egique Scientifique et Technologique (VSST 2010).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fifteenth International Conference on Machine Learning, ICML ’98,</booktitle>
<pages>296--304</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the Fifteenth International Conference on Machine Learning, ICML ’98, pages 296–304, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>Wordnet: a lexical database for english.</title>
<date>1995</date>
<journal>Commun. ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="1055" citStr="Miller, 1995" startWordPosition="151" endWordPosition="152">a n-gram based comparison method combined with a conceptual similarity measure that uses WordNet to calculate the similarity between a pair of concepts. 1 Introduction The system used for the participation of the IRIT team (composed by members of the research groups SIG and MELODI) to the Semantic Textual Similarity (STS) task (Agirre et al., 2012) is based on two sub-modules: • a module that calculates the similarity between sentences using n-gram based similarity; • a module that calculates the similarity between concepts in the two sentences, using a concept similarity measure and WordNet (Miller, 1995) as a resource. In Figure 1, we show the structure of the system and the connections between the main components. The input phrases are passed on one hand directly to the n-gram similarity module, and on the other they are annoted with the Stanford POS Tagger (Toutanova et al., 2003). All nouns and verbs are extracted from the tagged phrases and WordNet is searched for synsets corresponding to the extracted nouns and nouns associated to the verbs by the derived terms relationship. The synsets are the concepts used by the conceptual similarity module to Figure 1: Schema of the system. calculate</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. Wordnet: a lexical database for english. Commun. ACM, 38(11):39–41, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bachelin Ralalason</author>
</authors>
<title>Repr´esentation multi-facette des documents pour leur acc`es s´emantique.</title>
<date>2010</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit´e Paul Sabatier,</institution>
<location>Toulouse,</location>
<note>in French.</note>
<contexts>
<context position="7352" citStr="Ralalason, 2010" startWordPosition="1252" endWordPosition="1253">tual similarity between p and q is calculated as: s(c1, c2) (5) |Cp| where s(c1, c2) is a concept similarity measure. Concept similarity can be calculated by different ways. Wu and Palmer introduced in (Wu and Palmer, 1994) a concept similarity measure defined as: 2 · d(c0) s(c1,c2) = (6) d(c1) + d(c2) c0 is the most specific concept that is present both in the synset path of c1 and c2 (see Figure 2 for details). The function returning the depth of a concept is noted with d. 3.1 ProxiGenea By making an analogy between a family tree and the concept hierarchy in WordNet, (Dudognon et al., 2010; Ralalason, 2010) proposed a concept similarity measure based on the principle of evaluating the d(c0) pg2(c1,c2) = d(c1) + d(c2) − d(c0) (8) In this measure, the more are the elements which are not shared between the paths of c1 and c2, the more the score decreases. However, if the elements are placed more deeply in the ontology, the decrease is less important. 1 pg3(c1, c2) = 1 + d(c1) + d(c2) − 2 · d(c0) (9) In Table 2 we show the weights that have been calculated for each concept, using all the above similarity measures, and the concept that provided the maximum weight. No Word Sense Disambiguation process</context>
</contexts>
<marker>Ralalason, 2010</marker>
<rawString>Bachelin Ralalason. 2010. Repr´esentation multi-facette des documents pour leur acc`es s´emantique. Ph.D. thesis, Universit´e Paul Sabatier, Toulouse, September. in French.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
<author>Yoram Singer</author>
</authors>
<title>Feature-rich part-of-speech tagging with a cyclic dependency network.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="1339" citStr="Toutanova et al., 2003" startWordPosition="202" endWordPosition="205"> MELODI) to the Semantic Textual Similarity (STS) task (Agirre et al., 2012) is based on two sub-modules: • a module that calculates the similarity between sentences using n-gram based similarity; • a module that calculates the similarity between concepts in the two sentences, using a concept similarity measure and WordNet (Miller, 1995) as a resource. In Figure 1, we show the structure of the system and the connections between the main components. The input phrases are passed on one hand directly to the n-gram similarity module, and on the other they are annoted with the Stanford POS Tagger (Toutanova et al., 2003). All nouns and verbs are extracted from the tagged phrases and WordNet is searched for synsets corresponding to the extracted nouns and nouns associated to the verbs by the derived terms relationship. The synsets are the concepts used by the conceptual similarity module to Figure 1: Schema of the system. calculate the concept similarity. Each module calculates a similarity score using its own method; the final similarity value is calculated as the geometric average between the two scores, multiplied by 5 in order to comply with the task specifications. The n-gram based similarity relies on th</context>
</contexts>
<marker>Toutanova, Klein, Manning, Singer, 2003</marker>
<rawString>Kristina Toutanova, Dan Klein, Christopher D. Manning, and Yoram Singer. 2003. Feature-rich part-of-speech tagging with a cyclic dependency network. In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, NAACL ’03, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL ’94,</booktitle>
<pages>133--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2443" citStr="Wu and Palmer, 1994" startWordPosition="385" endWordPosition="388">wo scores, multiplied by 5 in order to comply with the task specifications. The n-gram based similarity relies on the idea that two sentences are semantically related if they contain a long enough sub-sequence of non-empty terms. Google Web 1T (Brants and Franz, 2006) has been used to calculate term idf, which is used as a measure of the importance of the terms. The conceptual similarity is based on the idea that, given an ontology, two concepts are semantically similar if their distance from a common ancestor is small enough. We used three different measures: the WuPalmer similarity measure (Wu and Palmer, 1994) and two “Proxigenea” measures (Dudognon et al., 2010). In the following we will explain in detail how Google Web 1T Geometric Average and Normalisation N-gram similarity module Score Phrases POS Tagger Concept similarity module Concept Extraction WordNet 552 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 552–556, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics each similarity module works. 2 N-Gram based Similarity N-gram based similarity is based on the Clustered Keywords Positional Distance (CKPD) model proposed in (Buscaldi e</context>
<context position="6959" citStr="Wu and Palmer, 1994" startWordPosition="1177" endWordPosition="1180">ch of the longest n-grams are 0.652, 1.809 and 0.427 respectively; their sum is 2.888 which divided by all the term weights contained in the sentence gives 0.764 which is the similarity score between the two sentences as calculated by the n-gram based method. 3 Conceptual Similarity Given Cp and Cq as the sets of concepts contained in sentence p and q, respectively, with |Cp |≥ |Cq|, the conceptual similarity between p and q is calculated as: s(c1, c2) (5) |Cp| where s(c1, c2) is a concept similarity measure. Concept similarity can be calculated by different ways. Wu and Palmer introduced in (Wu and Palmer, 1994) a concept similarity measure defined as: 2 · d(c0) s(c1,c2) = (6) d(c1) + d(c2) c0 is the most specific concept that is present both in the synset path of c1 and c2 (see Figure 2 for details). The function returning the depth of a concept is noted with d. 3.1 ProxiGenea By making an analogy between a family tree and the concept hierarchy in WordNet, (Dudognon et al., 2010; Ralalason, 2010) proposed a concept similarity measure based on the principle of evaluating the d(c0) pg2(c1,c2) = d(c1) + d(c2) − d(c0) (8) In this measure, the more are the elements which are not shared between the paths </context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL ’94, pages 133–138, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>