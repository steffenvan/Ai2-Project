<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001925">
<title confidence="0.996011">
Ground Truth for Grammatical Error Correction Metrics
</title>
<author confidence="0.998212">
Courtney Napoles1 and Keisuke Sakaguchi1 and Matt Post2 and Joel Tetreault3
</author>
<affiliation confidence="0.924544">
1Center for Language and Speech Processing, Johns Hopkins University
2Human Language Technology Center of Excellence, Johns Hopkins University
3Yahoo Labs
</affiliation>
<sectionHeader confidence="0.95367" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999937833333333">
How do we know which grammatical error
correction (GEC) system is best? A num-
ber of metrics have been proposed over
the years, each motivated by weaknesses
of previous metrics; however, the metrics
themselves have not been compared to an
empirical gold standard grounded in hu-
man judgments. We conducted the first
human evaluation of GEC system outputs,
and show that the rankings produced by
metrics such as MaxMatch and I-measure
do not correlate well with this ground
truth. As a step towards better metrics,
we also propose GLEU, a simple variant
of BLEU, modified to account for both the
source and the reference, and show that it
hews much more closely to human judg-
ments.
</bodyText>
<sectionHeader confidence="0.995166" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981782608696">
Automatic metrics are a critical component for all
tasks in natural language processing. For many
tasks, such as parsing and part-of-speech tagging,
there is a single correct answer, and thus a sin-
gle metric to compute it. For other tasks, such
as machine translation or summarization, there is
no effective limit to the size of the set of correct
answers. For such tasks, metrics proliferate and
compete with each other for the role of the domi-
nant metric. In such cases, an important question
to answer is by what means such metrics should
be compared. That is, what is the metric metric?
The answer is that it should be rooted in the
end-use case for the task under consideration. This
could be some other metric further downstream of
the task, or something simpler like direct human
evaluation. This latter approach is the one often
taken in machine translation; for example, the or-
ganizers of the Workshop on Statistical Machine
Translation have long argued that human evalua-
tion is the ultimate ground truth, and have there-
fore conducted an extensive human evaluation to
produce a system ranking, which is then used to
compare metrics (Bojar et al., 2014).
Unfortunately, for the subjective task of gram-
matical error correction (GEC), no such ground
truth has ever been established. Instead, the rank-
ings produced by new metrics are justified by their
correlation with explicitly-corrected errors in one
or more references, and by appeals to intuition for
the resulting rankings. However, arguably even
more so than for machine translation, the use case
for grammatical error correction is human con-
sumption, and therefore, the ground truth ranking
should be rooted in human judgments.
We establish a ground truth for GEC by con-
ducting a human evaluation and producing a hu-
man ranking of the systems entered into the
CoNLL-2014 Shared Task on GEC. We find that
existing GEC metrics correlate very poorly with
the ranking produced by this human evaluation.
As a step in the direction of better metrics, we de-
velop the Generalized Language Evaluation Un-
derstanding metric (GLEU) inspired by BLEU,
which correlates much better with the human rank-
ing than current GEC metrics.1
</bodyText>
<sectionHeader confidence="0.714268" genericHeader="method">
2 Grammatical error correction metrics
</sectionHeader>
<bodyText confidence="0.999911777777778">
GEC is often viewed as a matter of correcting iso-
lated grammatical errors, but is much more com-
plicated, nuanced, and subjective than that. As dis-
cussed in Chodorow et al. (2012), there is often
no single correction for an error (e.g., whether to
correct a subject-verb agreement error by chang-
ing the number of the subject or the verb), and er-
rors cover a range of factors including style, reg-
ister, venue, audience, and usage questions, about
</bodyText>
<footnote confidence="0.549316">
1Our code and rankings of the CoNLL-2014 Shared Task
system outputs can be downloaded from github.com/
cnap/gec-ranking/.
588
</footnote>
<note confidence="0.712896">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 588–593,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.998839765957447">
which there can be much disagreement. In addi-
tion, errors are not always errors, as can be seen
from the existence of different style manuals at
newspapers, and questions about the legitimacy of
prescriptivist grammar conventions.
Several automatic metrics have been used for
evaluating GEC systems. F-score, the harmonic
mean of precision and recall, is one of the most
commonly used metrics. It was used as an official
evaluation metric for several shared tasks (Dale et
al., 2012; Dale and Kilgarriff, 2011), where par-
ticipants were asked to detect and correct closed-
class errors (i.e., determiners and prepositions).
One of the issues with F-score is that it fails to
capture phrase-level edits. Thus Dahlmeier and
Ng (2012) proposed the MaxMatch (M2) scorer,
which calculates the F-score over an edit lattice
that captures phrase-level edits. For GEC, M2
is the standard, having been used to rank error
correction systems in the 2013 and 2014 CoNLL
shared tasks, where the error types to be corrected
were not limited to closed-class errors. (Ng et al.,
2013; Ng et al., 2014). M2 was assessed by com-
paring its output against that of the official Help-
ing Our Own (HOO) scorer (Dale and Kilgarriff,
2011), itself based on the GNU wdiff utility.2 In
other words, it was evaluated under the assump-
tion that evaluating GEC can be reduced to check-
ing whether a set of predefined errors have been
changed into a set of associated corrections.
M2 is not without its own issues. First, phrase-
level edits can be gamed because the lattice treats
a long phrase deletion as one edit.3 Second, the
F-score does not capture the difference between
“no change” and “wrong edits” made by systems.
Chodorow et al. (2012) also list other complica-
tions arising from using F-score or M2, depending
on the application of GEC.
Considering these problems, Felice and Briscoe
(2015) proposed a new metric, I-measure, which
is based on accuracy computed by edit distance
between the source, reference, and system output.
Their results are striking: there is a negative corre-
lation between the M2 and I-measure scores (Pear-
son’s r = −0.694).
A difficulty with all these metrics is that they
require detailed annotations of the location and er-
</bodyText>
<footnote confidence="0.935863666666667">
2http://www.gnu.org/s/wdiff/
3For example, when we put a single character ‘X’ as sys-
tem ouTut for each sentence, we obtain P = 0.27, R =
</footnote>
<figure confidence="0.83681825">
0.29, M = 0.28, which would be ranked 6/13 systems in
the 2014 CoNLL shared task.
0 10 20 30 40 50
M2 Score
</figure>
<figureCaption confidence="0.974674333333333">
Figure 1: Correlation among M2, I-measure, and
BLEU scores: M2 score shows negative correla-
tions to other metrics.
</figureCaption>
<bodyText confidence="0.999990321428571">
ror type of each correction in response to an ex-
plicit error annotation scheme. Due to the inherent
subjectivity and poor definition of the task, men-
tioned above, it is difficult for annotators to reli-
ably produce these annotations (Bryant and Ng,
2015). However, this requirement can be relin-
quished by treating GEC as a text-to-text rewriting
task and borrowing metrics from machine trans-
lation, as Park and Levy (2011) did with BLEU
(Papineni et al., 2002) and METEOR (Lavie and
Agarwal, 2007).
As we will show in more detail in Section 5,
taking the twelve publicly released system out-
puts from the CoNLL-2014 Shared Task,4 we ac-
tually find a negative correlation between the M2
and BLEU scores (r = −0.772) and positive
correlation between I-measure and BLEU scores
(r = 0.949) (Figure 1). With the earlier-reported
negative correlation between I-measure and M2,
we have a troubling picture: which of these met-
rics is best? Which one actually captures and re-
wards the behaviors we would like our systems
to report? Despite these many proposed metrics,
no prior work has attempted to answer these ques-
tions by comparing them to human judgments. We
propose to answer these questions by producing a
definitive human ranking, against which the rank-
ings of different metrics can be compared.
</bodyText>
<sectionHeader confidence="0.897708" genericHeader="method">
3 The human ranking
</sectionHeader>
<bodyText confidence="0.9699435">
The Workshop on Statistical Machine Translation
(WMT) faces the same question each year as part
</bodyText>
<figure confidence="0.99001765">
4www.comp.nus.edu.sg/˜nlp/conll14st.
html
BLEU I-Measure
87
86
85
84
83
82
81
2
1
0
-1
-2
-3
-4
-5
-6
589
</figure>
<figureCaption confidence="0.999937">
Figure 2: The Appraise evaluation system.
</figureCaption>
<bodyText confidence="0.999979923076923">
of its metrics shared task. Arguing that humans
are the ultimate judge of quality, they gather hu-
man judgments and use them to produce a ranking
of the systems for each task. Machine translation
metrics are then evaluated based on how closely
they match this ranking, using Pearson’s r (prior
to 2014) or Spearman’s p (2014).
We borrow their approach to conduct a human
evaluation. We used Appraise (Federmann, 2012)5
to collect pairwise judgments among 14 systems:
the output of 12 systems entered in the CoNLL-14
Shared Task, plus the source and a reference sen-
tence. Appraise presents the judge with the source
and reference sentence6 and asks her to rank four
randomly selected systems from best to worst, ties
allowed (Figure 2). The four-way ranking is trans-
formed into a set of pairwise judgments.
We collected data from three native English
speakers, resulting in 28,146 pairwise system
judgements. Each system’s quality was estimated
and the total ranking was produced on this dataset
using the TrueSkill model (Sakaguchi et al., 2014),
as done in WMT 2014. The annotators had strong
correlations in terms of the total system ranking
and estimated quality, with the reference being
ranked at the top (Table 1).
</bodyText>
<sectionHeader confidence="0.998263" genericHeader="method">
4 Generalized BLEU
</sectionHeader>
<bodyText confidence="0.979832571428571">
Current metrics for GEC rely on references with
explicitly labeled error annotations, the type and
form of which vary from task to task and can
5github.com/cfedermann/Appraise
6CoNLL-14 has two references. For each sentence, we
randomly chose one to present as the answer and one to be
among the systems to be ranked.
</bodyText>
<table confidence="0.9982555">
Judges r p
1 and 2 0.80 0.69
1 and 3 0.73 0.80
2 and 3 0.81 0.71
</table>
<tableCaption confidence="0.8302075">
Table 1: Pearson’s r and Spearman’s p correla-
tions among judges (excluding the reference).
</tableCaption>
<bodyText confidence="0.987273181818182">
be difficult to convert. Recognizing the inher-
ent ambiguity in the error-correction task, a better
metric might be independent of such an annota-
tion scheme and only require corrected references.
This is the view of GEC as a generic text-rewriting
task, and it is natural to apply standard metrics
from machine translation. However, applied off-
the-shelf, these metrics yield unintuitive results.
For example, BLEU ranks the source sentence as
second place in the CoNLL-2014 shared task.7
The problem is partially due to the subtle but
important difference between machine translation
and monolingual text-rewriting tasks. In MT, an
untranslated word or phrase is almost always an
error, but in grammatical error correction, this is
not the case. Some, but not all, regions of the
source sentence should be changed. This obser-
vation motivates a small change to BLEU that
computes n-gram precisions over the reference but
assigns more weight to n-grams that have been
correctly changed from the source. This revised
metric, Generalized Language Evaluation Under-
standing (GLEU), rewards corrections while also
correctly crediting unchanged source text.
Recall that BLEU(C, R) (Papineni et al., 2002)
is computed as the geometric mean of the modified
precision scores of the test sentences C relative to
the references R, multiplied by a brevity penalty
to control for recall. The precisions are computed
over bags of n-grams derived from the candidate
translation and the references. Each n-gram in the
candidate sentence is “clipped” to the maximum
count of that n-gram in any of the references, en-
suring that no precision is greater than 1.
Similar to I-measure, which calculates a
weighted accuracy of edits, we calculate a
weighted precision of n-grams. In our adaptation,
we modify the precision calculation to assign ex-
tra weight to n-grams present in the candidate that
overlap with the reference but not the source (the
set of n-grams R \ S). The precision is also penal-
7Of course, it could be the case that the source sentence
is actually the second best, but our human evaluation (§5)
confirms that this is not the case.
</bodyText>
<equation confidence="0.950301222222222">
590
n E
-g C CountR\S(n-gram) − A (CountS\R(n-gram)) + CountR(n-gram)
� CountS(n-gram0) + E
n-gram&apos;∈C&apos; n-gram∈R\S
(1)
CountR\S(n-gram)
=
p0n
</equation>
<bodyText confidence="0.999706333333333">
ized by a weighted count of n-grams in the can-
didate that are in the source but not the reference
(false negatives, S \R). For a correction candidate
C with a corresponding source S and reference R,
the modified n-gram precision for GLEU(C,R,S)
is shown in Equation 1. The weight A determines
by how much incorrectly changed n-grams are pe-
nalized. Equations 2–3 describe how the counts
are collected given a bag of n-grams B.
</bodyText>
<table confidence="0.9902125">
Metric r p
GLEU0 0.542 0.555
M2 0.358 0.429
GLEU0.1 0.200 0.412
I-measure -0.051 -0.005
BLEU -0.125 -0.225
</table>
<tableCaption confidence="0.997826">
Table 3: Correlation of metrics with the human
</tableCaption>
<bodyText confidence="0.932096">
ranking (excluding the reference), as calculated
with Pearson’s r and Spearman’s p.
</bodyText>
<equation confidence="0.914646571428572">
�CountB(n-gram) = d(n-gram, n-gram0) (2)
n-gram&apos;∈B
�
1 if n-gram = n-gram0
d(n-gram, n-gram0) = (3)
0 otherwise
�
1 if c &gt; r
BP = (4)
e(1−c/r) if c ≤ r
wn log pn I (5)
In our experiments, we used N = 4 and wn =
N , which are standard parameters for MT, the
1
</equation>
<bodyText confidence="0.99989925">
same brevity penalty as BLEU (Equation 4), and
report results on A = {0.1, 0} (GLEU0.1 and
GLEU0, respectively). For this task, not penal-
izing false negatives correlates best with human
judgments, but the weight can be tuned for dif-
ferent tasks and datasets. GLEU can be easily ex-
tended to additionally punish false positives (in-
correctly editing grammatical text) as well.
</bodyText>
<sectionHeader confidence="0.999842" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.99976178">
The respective system rankings of each metric are
presented in Table 2. The human ranking is con-
siderably different from those of most of the met-
rics, a fact that is also captured in correlation co-
efficients (Table 3).8 From the human evaluation,
we learn that the source falls near the middle of
the rankings, even though the BLEU, I-measure
and M2 rank it among the best or worst systems.
M2, the metric that has been used for the
CoNLL shared tasks, only correlates moderately
with human rankings, suggesting that it is not an
ideal metric for judging the results of a competi-
tion. Even though I-measure perceptively aims to
8Pearson’s measure assumes the scores are normally dis-
tributed, which may not be true here.
predict whether an output is better or worse than
the input, it actually has a slight negative correla-
tion with human rankings. GLEU0 is the only met-
ric that strongly correlates with the human ranks,
and performs closest to the range of human-to-
human correlation (0.73 &lt; r &lt; 0.81) GLEU0
correctly ranks four out of five of the top human-
ranked systems at the top of its list, while the other
metrics rank at most three of these systems in the
top five.
All metrics deviate from the human rankings,
which may in part be because automatic metrics
equally weight all error types, when some errors
may be more tolerable to human judges than oth-
ers. For example, inserting a missing token is re-
warded the same by automatic metrics, whether it
is a comma or a verb, while a human would much
more strongly prefer the insertion of the latter. An
example of system outputs with their automatic
scores and human rankings is included in Table 4.
This example illustrates some challenges faced
when using automatic metrics to evaluate GEC.
The automatic metrics weight all corrections
equally and are limited to the gold-standard refer-
ences provided. Both automatic metrics, M2 and
GLEU, prefer the AMU output in this example,
even though it corrects one error and introduces
another. The human judges rank the UMC out-
put as the best for correcting the main verb even
though it ignored the spelling error. The UMC and
NTHU sentences both receive M2 = 0 because
they make none of the gold-standard edits, even
though UMC correctly inserts be into the sentence.
M2 does not recognize this since it is in a differ-
ent location from where the annotators placed it.
</bodyText>
<table confidence="0.898206166666667">
N
GLEU (C, R, S) = BP · exp E
n=1
591
Human BLEU I-measure M2 GLEU0 GLEU0.1
CAMB UFC UFC CUUI CUUI CUUI
AMU source source CAMB AMU AMU
RAC IITB IITB AMU UFC CAMB
CUUI SJTU SJTU POST CAMB UFC
source UMC CUUI UMC source IITB
POST CUUI PKU NTHU IITB SJTU
UFC PKU AMU PKU SJTU PKU
SJTU AMU UMC RAC PKU UMC
IITB IPN IPN SJTU UMC NTHU
PKU NTHU POST UFC NTHU POST
UMC CAMB RAC IPN POST RAC
NTHU RAC CAMB IITB RAC IPN
IPN POST NTHU source IPN source
</table>
<tableCaption confidence="0.858653">
Table 2: System outputs scored by different metrics, ranked best to worst.
</tableCaption>
<table confidence="0.9998933125">
System Sentence Scores
Original We may in actual fact communicating with a hoax Facebook acccount of a cyber –
sentence friend, which we assume to be real but in reality , it is a fake account.
Reference 1 We may in actual fact be communicating with a hoax Facebook acccount of a –
cyber friend, which we assume to be real but in reality , it is a fake account.
Reference 2 We may in actual fact be communicating with a fake Facebook account of an –
online friend, which we assume to be real but, in reality , it is a fake account.
UMC We may be in actual fact communicating with a hoax Facebook acccount of a GLEU = 0.62
cyber friend, we assume to be real but in reality , it is a fake account. M2 = 0.00
Human rank= 1
AMU We may in actual fact communicating with a hoax Facebook account of a cyber GLEU = 0.64
friend , which we assume to be real but in reality , it is a fake accounts . M2 = 0.39
Human rank= 2
NTHU We may of actual fact communicating with a hoax Facebook acccount of a cyber GLEU = 0.60
friend , which we assumed to be real but in reality , it is a fake account . M2 = 0.00
Human rank= 4
</table>
<tableCaption confidence="0.963759">
Table 4: Examples of system output (changes are in bold) and the sentence-level scores assigned by
different metrics.
</tableCaption>
<bodyText confidence="0.951939666666667">
However, GLEU awards UMC partial credit for
adding the correct unigram, and further assigns all
sentences a real score.
</bodyText>
<sectionHeader confidence="0.997464" genericHeader="conclusions">
6 Summary
</sectionHeader>
<bodyText confidence="0.99999496875">
As with other metrics used in natural language
processing tasks, grammatical error correction
metrics must be evaluated against ground truth.
The inherent subjectivity in what constitutes a
grammatical correction, together with the fact that
the use case for grammatically-corrected output is
human readers, argue for grounding metric evalu-
ations in a human evaluation, which we produced
following procedures established by the Workshop
on Statistical Machine Translation. This human
ranking shows us that the metric commonly used
for GEC is not appropriate, since it does not cor-
relate strongly; newly proposed alternatives fare
little better.
Attending to how humans perceive the quality
of the sentences, we developed GLEU by making
a simple variation to an existing metric. GLEU
more closely models human judgments than previ-
ous metrics because it rewards correct edits while
penalizing ungrammatical edits, while capturing
fluency and grammatical constraints by virtue of
using n-grams. While this simple modification to
BLEU accounts for crucial differences in a mono-
lingual setting, fares well, and could take the place
of existing metrics, especially for rapid system de-
velopment as in machine translation, there is still
room for further work as there is a gap in how well
it correlates with human judgments.
Most importantly, the results and data from this
paper establish a method for objectively evaluating
future metric proposals, which is crucial to yearly
incremental improvements to the GEC task.
</bodyText>
<sectionHeader confidence="0.995491" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999774166666667">
We would like to thank Christopher Bryant, Hwee
Tou Ng, Mariano Felice, and Ted Briscoe for shar-
ing their research with us pre-publication. We also
thank the reviewers and Wei Xu for their valuable
feedback, and Benjamin Van Durme for his sup-
port.
</bodyText>
<page confidence="0.730936">
592
</page>
<sectionHeader confidence="0.983909" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999118610526316">
Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs
Tamchyna. 2014. Findings of the 2014 Workshop
on Statistical Machine Translation. In Proceedings
of the Ninth Workshop on Statistical Machine Trans-
lation, pages 12–58, Baltimore, Maryland, USA,
June. Association for Computational Linguistics.
Christopher Bryant and Hwee Tou Ng. 2015. How
far are we from fully automatic high quality gram-
matical error correction? In Proceedings of the
53rd Annual Meeting of the Association for Com-
putational Linguistics, Beijing, China, July. Associ-
ation for Computational Linguistics.
Martin Chodorow, Markus Dickinson, Ross Israel, and
Joel Tetreault. 2012. Problems in evaluating gram-
matical error detection systems. In Proceedings of
COLING 2012, pages 611–628, Mumbai, India, De-
cember. The COLING 2012 Organizing Committee.
Daniel Dahlmeier and Hwee Tou Ng. 2012. Bet-
ter evaluation for grammatical error correction. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 568–572. Association for Computational Lin-
guistics, June.
Robert Dale and Adam Kilgarriff. 2011. Helping
our own: The HOO 2011 pilot shared task. In
Proceedings of the Generation Challenges Session
at the 13th European Workshop on Natural Lan-
guage Generation, pages 242–249, Nancy, France,
September. Association for Computational Linguis-
tics.
Robert Dale, Ilya Anisimoff, and George Narroway.
2012. HOO 2012: A report on the preposition and
determiner error correction shared task. In Pro-
ceedings of the Seventh Workshop on Building Ed-
ucational Applications Using NLP, pages 54–62,
Montr´eal, Canada, June. Association for Computa-
tional Linguistics.
Christian Federmann. 2012. Appraise: An open-
source toolkit for manual evaluation of machine
translation output. The Prague Bulletin of Mathe-
matical Linguistics, 98:25–35, September.
Mariano Felice and Ted Briscoe. 2015. Towards a
standard evaluation method for grammatical error
detection and correction. In Proceedings of the 2015
Conference of the North American Chapter of the
Association for Computational Linguistics, Denver,
CO, June. Association for Computational Linguis-
tics.
Alon Lavie and Abhaya Agarwal. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 228–231, Prague, Czech
Republic, June. Association for Computational Lin-
guistics.
Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian
Hadiwinoto, and Joel Tetreault. 2013. The CoNLL-
2013 shared task on grammatical error correction.
In Proceedings of the Seventeenth Conference on
Computational Natural Language Learning: Shared
Task, pages 1–12, Sofia, Bulgaria, August. Associa-
tion for Computational Linguistics.
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian
Hadiwinoto, Raymond Hendy Susanto, and Christo-
pher Bryant. 2014. The CoNLL-2014 shared task
on grammatical error correction. In Proceedings of
the Eighteenth Conference on Computational Natu-
ral Language Learning: Shared Task, pages 1–14,
Baltimore, Maryland, June. Association for Compu-
tational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318, Philadelphia,
Pennsylvania, USA, July. Association for Computa-
tional Linguistics.
Y. Albert Park and Roger Levy. 2011. Automated
whole sentence grammar correction using a noisy
channel model. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
934–944, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Keisuke Sakaguchi, Matt Post, and Benjamin
Van Durme. 2014. Efficient elicitation of annota-
tions for human evaluation of machine translation.
In Proceedings of the Ninth Workshop on Statistical
Machine Translation, pages 1–11, Baltimore, Mary-
land, USA, June. Association for Computational
Linguistics.
</reference>
<page confidence="0.963955">
593
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.425388">
<title confidence="0.8894558">Ground Truth for Grammatical Error Correction Metrics and and and for Language and Speech Processing, Johns Hopkins Language Technology Center of Excellence, Johns Hopkins Labs</title>
<abstract confidence="0.991418210526316">How do we know which grammatical error correction (GEC) system is best? A number of metrics have been proposed over the years, each motivated by weaknesses of previous metrics; however, the metrics themselves have not been compared to an empirical gold standard grounded in human judgments. We conducted the first evaluation GEC system outputs, and show that the rankings produced by metrics such as MaxMatch and I-measure do not correlate well with this ground truth. As a step towards better metrics, we also propose GLEU, a simple variant of BLEU, modified to account for both the source and the reference, and show that it hews much more closely to human judgments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Johannes Leveling</author>
<author>Christof Monz</author>
<author>Pavel Pecina</author>
<author>Matt Post</author>
<author>Herve Saint-Amand</author>
</authors>
<title>Radu Soricut, Lucia Specia, and Aleˇs Tamchyna.</title>
<date>2014</date>
<booktitle>Findings of the 2014 Workshop on Statistical Machine Translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>12--58</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<contexts>
<context position="2149" citStr="Bojar et al., 2014" startWordPosition="350" endWordPosition="353">be compared. That is, what is the metric metric? The answer is that it should be rooted in the end-use case for the task under consideration. This could be some other metric further downstream of the task, or something simpler like direct human evaluation. This latter approach is the one often taken in machine translation; for example, the organizers of the Workshop on Statistical Machine Translation have long argued that human evaluation is the ultimate ground truth, and have therefore conducted an extensive human evaluation to produce a system ranking, which is then used to compare metrics (Bojar et al., 2014). Unfortunately, for the subjective task of grammatical error correction (GEC), no such ground truth has ever been established. Instead, the rankings produced by new metrics are justified by their correlation with explicitly-corrected errors in one or more references, and by appeals to intuition for the resulting rankings. However, arguably even more so than for machine translation, the use case for grammatical error correction is human consumption, and therefore, the ground truth ranking should be rooted in human judgments. We establish a ground truth for GEC by conducting a human evaluation </context>
</contexts>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, Saint-Amand, 2014</marker>
<rawString>Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs Tamchyna. 2014. Findings of the 2014 Workshop on Statistical Machine Translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12–58, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Bryant</author>
<author>Hwee Tou Ng</author>
</authors>
<title>How far are we from fully automatic high quality grammatical error correction?</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China,</location>
<contexts>
<context position="6887" citStr="Bryant and Ng, 2015" startWordPosition="1130" endWordPosition="1133"> and er2http://www.gnu.org/s/wdiff/ 3For example, when we put a single character ‘X’ as system ouTut for each sentence, we obtain P = 0.27, R = 0.29, M = 0.28, which would be ranked 6/13 systems in the 2014 CoNLL shared task. 0 10 20 30 40 50 M2 Score Figure 1: Correlation among M2, I-measure, and BLEU scores: M2 score shows negative correlations to other metrics. ror type of each correction in response to an explicit error annotation scheme. Due to the inherent subjectivity and poor definition of the task, mentioned above, it is difficult for annotators to reliably produce these annotations (Bryant and Ng, 2015). However, this requirement can be relinquished by treating GEC as a text-to-text rewriting task and borrowing metrics from machine translation, as Park and Levy (2011) did with BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). As we will show in more detail in Section 5, taking the twelve publicly released system outputs from the CoNLL-2014 Shared Task,4 we actually find a negative correlation between the M2 and BLEU scores (r = −0.772) and positive correlation between I-measure and BLEU scores (r = 0.949) (Figure 1). With the earlier-reported negative correlation between I-m</context>
</contexts>
<marker>Bryant, Ng, 2015</marker>
<rawString>Christopher Bryant and Hwee Tou Ng. 2015. How far are we from fully automatic high quality grammatical error correction? In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics, Beijing, China, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Markus Dickinson</author>
<author>Ross Israel</author>
<author>Joel Tetreault</author>
</authors>
<title>Problems in evaluating grammatical error detection systems.</title>
<date>2012</date>
<booktitle>In Proceedings of COLING 2012,</booktitle>
<pages>611--628</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="3385" citStr="Chodorow et al. (2012)" startWordPosition="551" endWordPosition="554">a human ranking of the systems entered into the CoNLL-2014 Shared Task on GEC. We find that existing GEC metrics correlate very poorly with the ranking produced by this human evaluation. As a step in the direction of better metrics, we develop the Generalized Language Evaluation Understanding metric (GLEU) inspired by BLEU, which correlates much better with the human ranking than current GEC metrics.1 2 Grammatical error correction metrics GEC is often viewed as a matter of correcting isolated grammatical errors, but is much more complicated, nuanced, and subjective than that. As discussed in Chodorow et al. (2012), there is often no single correction for an error (e.g., whether to correct a subject-verb agreement error by changing the number of the subject or the verb), and errors cover a range of factors including style, register, venue, audience, and usage questions, about 1Our code and rankings of the CoNLL-2014 Shared Task system outputs can be downloaded from github.com/ cnap/gec-ranking/. 588 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 588–593, Beijing, China</context>
<context position="5761" citStr="Chodorow et al. (2012)" startWordPosition="938" endWordPosition="941"> was assessed by comparing its output against that of the official Helping Our Own (HOO) scorer (Dale and Kilgarriff, 2011), itself based on the GNU wdiff utility.2 In other words, it was evaluated under the assumption that evaluating GEC can be reduced to checking whether a set of predefined errors have been changed into a set of associated corrections. M2 is not without its own issues. First, phraselevel edits can be gamed because the lattice treats a long phrase deletion as one edit.3 Second, the F-score does not capture the difference between “no change” and “wrong edits” made by systems. Chodorow et al. (2012) also list other complications arising from using F-score or M2, depending on the application of GEC. Considering these problems, Felice and Briscoe (2015) proposed a new metric, I-measure, which is based on accuracy computed by edit distance between the source, reference, and system output. Their results are striking: there is a negative correlation between the M2 and I-measure scores (Pearson’s r = −0.694). A difficulty with all these metrics is that they require detailed annotations of the location and er2http://www.gnu.org/s/wdiff/ 3For example, when we put a single character ‘X’ as system</context>
</contexts>
<marker>Chodorow, Dickinson, Israel, Tetreault, 2012</marker>
<rawString>Martin Chodorow, Markus Dickinson, Ross Israel, and Joel Tetreault. 2012. Problems in evaluating grammatical error detection systems. In Proceedings of COLING 2012, pages 611–628, Mumbai, India, December. The COLING 2012 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Dahlmeier</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Better evaluation for grammatical error correction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>568--572</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="4782" citStr="Dahlmeier and Ng (2012)" startWordPosition="768" endWordPosition="771">existence of different style manuals at newspapers, and questions about the legitimacy of prescriptivist grammar conventions. Several automatic metrics have been used for evaluating GEC systems. F-score, the harmonic mean of precision and recall, is one of the most commonly used metrics. It was used as an official evaluation metric for several shared tasks (Dale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014). M2 was assessed by comparing its output against that of the official Helping Our Own (HOO) scorer (Dale and Kilgarriff, 2011), itself based on the GNU wdiff utility.2 In other words, it was evaluated under the assumption that evaluating GEC can </context>
</contexts>
<marker>Dahlmeier, Ng, 2012</marker>
<rawString>Daniel Dahlmeier and Hwee Tou Ng. 2012. Better evaluation for grammatical error correction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 568–572. Association for Computational Linguistics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Adam Kilgarriff</author>
</authors>
<title>Helping our own: The HOO 2011 pilot shared task.</title>
<date>2011</date>
<booktitle>In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation,</booktitle>
<pages>242--249</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Nancy, France,</location>
<contexts>
<context position="4564" citStr="Dale and Kilgarriff, 2011" startWordPosition="733" endWordPosition="736">g (Short Papers), pages 588–593, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics which there can be much disagreement. In addition, errors are not always errors, as can be seen from the existence of different style manuals at newspapers, and questions about the legitimacy of prescriptivist grammar conventions. Several automatic metrics have been used for evaluating GEC systems. F-score, the harmonic mean of precision and recall, is one of the most commonly used metrics. It was used as an official evaluation metric for several shared tasks (Dale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014). M2 was assessed by comparin</context>
</contexts>
<marker>Dale, Kilgarriff, 2011</marker>
<rawString>Robert Dale and Adam Kilgarriff. 2011. Helping our own: The HOO 2011 pilot shared task. In Proceedings of the Generation Challenges Session at the 13th European Workshop on Natural Language Generation, pages 242–249, Nancy, France, September. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ilya Anisimoff</author>
<author>George Narroway</author>
</authors>
<title>HOO 2012: A report on the preposition and determiner error correction shared task.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP,</booktitle>
<pages>54--62</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="4536" citStr="Dale et al., 2012" startWordPosition="729" endWordPosition="732"> Language Processing (Short Papers), pages 588–593, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics which there can be much disagreement. In addition, errors are not always errors, as can be seen from the existence of different style manuals at newspapers, and questions about the legitimacy of prescriptivist grammar conventions. Several automatic metrics have been used for evaluating GEC systems. F-score, the harmonic mean of precision and recall, is one of the most commonly used metrics. It was used as an official evaluation metric for several shared tasks (Dale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014).</context>
</contexts>
<marker>Dale, Anisimoff, Narroway, 2012</marker>
<rawString>Robert Dale, Ilya Anisimoff, and George Narroway. 2012. HOO 2012: A report on the preposition and determiner error correction shared task. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 54–62, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christian Federmann</author>
</authors>
<title>Appraise: An opensource toolkit for manual evaluation of machine translation output.</title>
<date>2012</date>
<booktitle>The Prague Bulletin of Mathematical Linguistics,</booktitle>
<pages>98--25</pages>
<contexts>
<context position="8606" citStr="Federmann, 2012" startWordPosition="1421" endWordPosition="1422">ion (WMT) faces the same question each year as part 4www.comp.nus.edu.sg/˜nlp/conll14st. html BLEU I-Measure 87 86 85 84 83 82 81 2 1 0 -1 -2 -3 -4 -5 -6 589 Figure 2: The Appraise evaluation system. of its metrics shared task. Arguing that humans are the ultimate judge of quality, they gather human judgments and use them to produce a ranking of the systems for each task. Machine translation metrics are then evaluated based on how closely they match this ranking, using Pearson’s r (prior to 2014) or Spearman’s p (2014). We borrow their approach to conduct a human evaluation. We used Appraise (Federmann, 2012)5 to collect pairwise judgments among 14 systems: the output of 12 systems entered in the CoNLL-14 Shared Task, plus the source and a reference sentence. Appraise presents the judge with the source and reference sentence6 and asks her to rank four randomly selected systems from best to worst, ties allowed (Figure 2). The four-way ranking is transformed into a set of pairwise judgments. We collected data from three native English speakers, resulting in 28,146 pairwise system judgements. Each system’s quality was estimated and the total ranking was produced on this dataset using the TrueSkill mo</context>
</contexts>
<marker>Federmann, 2012</marker>
<rawString>Christian Federmann. 2012. Appraise: An opensource toolkit for manual evaluation of machine translation output. The Prague Bulletin of Mathematical Linguistics, 98:25–35, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariano Felice</author>
<author>Ted Briscoe</author>
</authors>
<title>Towards a standard evaluation method for grammatical error detection and correction.</title>
<date>2015</date>
<booktitle>In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, CO,</location>
<contexts>
<context position="5916" citStr="Felice and Briscoe (2015)" startWordPosition="962" endWordPosition="965">ff utility.2 In other words, it was evaluated under the assumption that evaluating GEC can be reduced to checking whether a set of predefined errors have been changed into a set of associated corrections. M2 is not without its own issues. First, phraselevel edits can be gamed because the lattice treats a long phrase deletion as one edit.3 Second, the F-score does not capture the difference between “no change” and “wrong edits” made by systems. Chodorow et al. (2012) also list other complications arising from using F-score or M2, depending on the application of GEC. Considering these problems, Felice and Briscoe (2015) proposed a new metric, I-measure, which is based on accuracy computed by edit distance between the source, reference, and system output. Their results are striking: there is a negative correlation between the M2 and I-measure scores (Pearson’s r = −0.694). A difficulty with all these metrics is that they require detailed annotations of the location and er2http://www.gnu.org/s/wdiff/ 3For example, when we put a single character ‘X’ as system ouTut for each sentence, we obtain P = 0.27, R = 0.29, M = 0.28, which would be ranked 6/13 systems in the 2014 CoNLL shared task. 0 10 20 30 40 50 M2 Sco</context>
</contexts>
<marker>Felice, Briscoe, 2015</marker>
<rawString>Mariano Felice and Ted Briscoe. 2015. Towards a standard evaluation method for grammatical error detection and correction. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics, Denver, CO, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Abhaya Agarwal</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on Statistical Machine Translation,</booktitle>
<pages>228--231</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7130" citStr="Lavie and Agarwal, 2007" startWordPosition="1170" endWordPosition="1173"> 50 M2 Score Figure 1: Correlation among M2, I-measure, and BLEU scores: M2 score shows negative correlations to other metrics. ror type of each correction in response to an explicit error annotation scheme. Due to the inherent subjectivity and poor definition of the task, mentioned above, it is difficult for annotators to reliably produce these annotations (Bryant and Ng, 2015). However, this requirement can be relinquished by treating GEC as a text-to-text rewriting task and borrowing metrics from machine translation, as Park and Levy (2011) did with BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). As we will show in more detail in Section 5, taking the twelve publicly released system outputs from the CoNLL-2014 Shared Task,4 we actually find a negative correlation between the M2 and BLEU scores (r = −0.772) and positive correlation between I-measure and BLEU scores (r = 0.949) (Figure 1). With the earlier-reported negative correlation between I-measure and M2, we have a troubling picture: which of these metrics is best? Which one actually captures and rewards the behaviors we would like our systems to report? Despite these many proposed metrics, no prior work has attempted to answer t</context>
</contexts>
<marker>Lavie, Agarwal, 2007</marker>
<rawString>Alon Lavie and Abhaya Agarwal. 2007. METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228–231, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Yuanbin Wu</author>
<author>Christian Hadiwinoto</author>
<author>Joel Tetreault</author>
</authors>
<title>The CoNLL2013 shared task on grammatical error correction.</title>
<date>2013</date>
<booktitle>In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="5117" citStr="Ng et al., 2013" startWordPosition="824" endWordPosition="827">al shared tasks (Dale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014). M2 was assessed by comparing its output against that of the official Helping Our Own (HOO) scorer (Dale and Kilgarriff, 2011), itself based on the GNU wdiff utility.2 In other words, it was evaluated under the assumption that evaluating GEC can be reduced to checking whether a set of predefined errors have been changed into a set of associated corrections. M2 is not without its own issues. First, phraselevel edits can be gamed because the lattice treats a long phrase deletion as one edit.3 Second, the F-score does not capture the difference between “no change” and “wrong ed</context>
</contexts>
<marker>Ng, Wu, Wu, Hadiwinoto, Tetreault, 2013</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian Hadiwinoto, and Joel Tetreault. 2013. The CoNLL2013 shared task on grammatical error correction. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Siew Mei Wu</author>
<author>Ted Briscoe</author>
<author>Christian Hadiwinoto</author>
<author>Raymond Hendy Susanto</author>
<author>Christopher Bryant</author>
</authors>
<title>The CoNLL-2014 shared task on grammatical error correction.</title>
<date>2014</date>
<booktitle>In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task,</booktitle>
<pages>1--14</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland,</location>
<contexts>
<context position="5135" citStr="Ng et al., 2014" startWordPosition="828" endWordPosition="831">Dale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014). M2 was assessed by comparing its output against that of the official Helping Our Own (HOO) scorer (Dale and Kilgarriff, 2011), itself based on the GNU wdiff utility.2 In other words, it was evaluated under the assumption that evaluating GEC can be reduced to checking whether a set of predefined errors have been changed into a set of associated corrections. M2 is not without its own issues. First, phraselevel edits can be gamed because the lattice treats a long phrase deletion as one edit.3 Second, the F-score does not capture the difference between “no change” and “wrong edits” made by syste</context>
</contexts>
<marker>Ng, Wu, Briscoe, Hadiwinoto, Susanto, Bryant, 2014</marker>
<rawString>Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian Hadiwinoto, Raymond Hendy Susanto, and Christopher Bryant. 2014. The CoNLL-2014 shared task on grammatical error correction. In Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task, pages 1–14, Baltimore, Maryland, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: A method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="7093" citStr="Papineni et al., 2002" startWordPosition="1164" endWordPosition="1167">14 CoNLL shared task. 0 10 20 30 40 50 M2 Score Figure 1: Correlation among M2, I-measure, and BLEU scores: M2 score shows negative correlations to other metrics. ror type of each correction in response to an explicit error annotation scheme. Due to the inherent subjectivity and poor definition of the task, mentioned above, it is difficult for annotators to reliably produce these annotations (Bryant and Ng, 2015). However, this requirement can be relinquished by treating GEC as a text-to-text rewriting task and borrowing metrics from machine translation, as Park and Levy (2011) did with BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). As we will show in more detail in Section 5, taking the twelve publicly released system outputs from the CoNLL-2014 Shared Task,4 we actually find a negative correlation between the M2 and BLEU scores (r = −0.772) and positive correlation between I-measure and BLEU scores (r = 0.949) (Figure 1). With the earlier-reported negative correlation between I-measure and M2, we have a troubling picture: which of these metrics is best? Which one actually captures and rewards the behaviors we would like our systems to report? Despite these many proposed metrics, no</context>
<context position="11098" citStr="Papineni et al., 2002" startWordPosition="1821" endWordPosition="1824">anslation and monolingual text-rewriting tasks. In MT, an untranslated word or phrase is almost always an error, but in grammatical error correction, this is not the case. Some, but not all, regions of the source sentence should be changed. This observation motivates a small change to BLEU that computes n-gram precisions over the reference but assigns more weight to n-grams that have been correctly changed from the source. This revised metric, Generalized Language Evaluation Understanding (GLEU), rewards corrections while also correctly crediting unchanged source text. Recall that BLEU(C, R) (Papineni et al., 2002) is computed as the geometric mean of the modified precision scores of the test sentences C relative to the references R, multiplied by a brevity penalty to control for recall. The precisions are computed over bags of n-grams derived from the candidate translation and the references. Each n-gram in the candidate sentence is “clipped” to the maximum count of that n-gram in any of the references, ensuring that no precision is greater than 1. Similar to I-measure, which calculates a weighted accuracy of edits, we calculate a weighted precision of n-grams. In our adaptation, we modify the precisio</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: A method for automatic evaluation of machine translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318, Philadelphia, Pennsylvania, USA, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Albert Park</author>
<author>Roger Levy</author>
</authors>
<title>Automated whole sentence grammar correction using a noisy channel model.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>934--944</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="7055" citStr="Park and Levy (2011)" startWordPosition="1157" endWordPosition="1160">uld be ranked 6/13 systems in the 2014 CoNLL shared task. 0 10 20 30 40 50 M2 Score Figure 1: Correlation among M2, I-measure, and BLEU scores: M2 score shows negative correlations to other metrics. ror type of each correction in response to an explicit error annotation scheme. Due to the inherent subjectivity and poor definition of the task, mentioned above, it is difficult for annotators to reliably produce these annotations (Bryant and Ng, 2015). However, this requirement can be relinquished by treating GEC as a text-to-text rewriting task and borrowing metrics from machine translation, as Park and Levy (2011) did with BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). As we will show in more detail in Section 5, taking the twelve publicly released system outputs from the CoNLL-2014 Shared Task,4 we actually find a negative correlation between the M2 and BLEU scores (r = −0.772) and positive correlation between I-measure and BLEU scores (r = 0.949) (Figure 1). With the earlier-reported negative correlation between I-measure and M2, we have a troubling picture: which of these metrics is best? Which one actually captures and rewards the behaviors we would like our systems to report? D</context>
</contexts>
<marker>Park, Levy, 2011</marker>
<rawString>Y. Albert Park and Roger Levy. 2011. Automated whole sentence grammar correction using a noisy channel model. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 934–944, Portland, Oregon, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keisuke Sakaguchi</author>
<author>Matt Post</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Efficient elicitation of annotations for human evaluation of machine translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Baltimore, Maryland, USA,</location>
<marker>Sakaguchi, Post, Van Durme, 2014</marker>
<rawString>Keisuke Sakaguchi, Matt Post, and Benjamin Van Durme. 2014. Efficient elicitation of annotations for human evaluation of machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 1–11, Baltimore, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>