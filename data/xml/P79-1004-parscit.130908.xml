<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.217458">
<note confidence="0.41199125">
Schank/Riesbeck vs. Norman/Rumelhart: What&apos;s the Difference?
Marc Eisenstadt
The Open University
Milton Keynes, ENGLAND
</note>
<bodyText confidence="0.999746618320611">
This paper explores the fundamental differences between
two sentence-parsers developed in the early 1970&apos;s:
Riesbeck&apos;s parser for Schank&apos;s&apos;conceptual dependency&apos;
theory (4, 5), and the &apos;LNR&apos; parser for Norman and
Rumelhart&apos;s &apos;active semantic network&apos; theory (3). The
Riesbeck parser and the LNR parser share a common goal -
that of transforming an input sentence into a canonical
form for later use by memory/inference/paraphrase
processes. or both parsers, this transformation is the
act of &apos;comprehension&apos;, although they appear to go about
it in very different ways. Are these differences real
or apparent?
Riesbeck&apos;s parser is implemented as a production system,
in which input text can either satisfy the condition
side of any production rule within a packet of
currently-active rules, or else interrupt processing by
disabling the current packet of rules and enabling
(&apos;triggering&apos;) a new packet of rules. In operation, the
main verb of each segment of text is located, and a
pointer to its lexical decomposition (canonical form) is
established in memory. The surrounding text, primarily
noun phrases, is then systematically mapped onto vacant
case frame slots within the memory representation of the
decomposed verb. Case information is signposted by a
verb-triggered packet of production rules which expects
certain classes of entity (e.g. animate recipient) to be
encountered in the text. Phrase boundaries are handled
by keyword-triggered packets of rules which initiate and
terminate the parsing of phrases.
In contrast to this, the LNR parser is implemented as an
augmented transition network, in which input text can
either satisfy a current expectation or cause back-
tracking to a point at which an alternative expectation
can be satisfied. In operation, input text is mapped
onto a surface case frame, which is an n-ary predicate
containing a pointer to the appropriate code responsible
for decomposing the predicate into canonical form. Case
information is signposted by property-list indicators
stored in the lexical entry for verbs. These indicators
act as signals or flags which are inspected by augmented
tests on PUSH NP and PUSH PP arcs in order to decide
whether such transitions are to be allowed. Phrase
boundaries are handled by the standard ATN PUSH and POP
mechanisms, with provision for backtracking if an
initially-fulfilled expectation later turns out to have
been incorrect.
In order to determine which differences are due to
notational conventions, I have implemented versions of
both parsers in Kaplan&apos;s General Syntactic Processor
((ISP) formalism (2), a simple but elegant generalization
of ATNs. In GSP terms, Riesbeck&apos;s active packets of
production rules are grammar states, and each rule is
represented as a grammar arc. Rule-packet triggering is
handled by storing in the lexicon the GSP code which
transfers control to a new grammar state when an
interrupt is called for. Each packet is in effect a
sub-grammar of the type handled normally by an ATN PUSH
and POP. The important difference is that the expensive
actions normally associated with PUSH and POP (e.g.
saving registers, building structures) only occur after
it is safe to perform them. That is, bottom-up
interrupts and very cheap &apos;lookahead&apos; ensure that waste-
ful backtracking is largely avoided.
Riesbeck&apos;s verb-triggered packet of rules (i.e. the
entire sub-grammar which is entered after the verb is
&apos; encountered) is isomorphic to the LNR-style use of
lexical flags, which are in effect &apos;raised&apos; and
&apos;lowered&apos; solely for the benefit of augmented tests on
verb-independent arcs. Where Riesbeck depicts a
&apos;satisfied expectation&apos; by deleting the relevant
production rule from the currently-active packet, LNR
achieves the same effect by using augmented tests on
PUSH NP and PUSH PP arcs to determine whether a
particular case frame slot has already been filled.
both approaches are handled with equal ease by GSP.
In actual practice, Riesbeck&apos;s case frame expectations
are typically tests for simple selectional restrictions,
whereas LNR&apos;s case frame expectations are typically
tests for the order in-which noun phrases areencounter-
ed. Prepositions, naturally, are used by both parsers
as important case frame clues: Riesbeck has a verb-
triggered action alter the interrupt code associated
with prepositions so that they &apos;behave&apos; in precisely
the right way; this is isomorphic to LNR&apos;s flags which
are stored in the lexical entry for a verb and examined
by augmented tests on verb-independent prepositional
phrase arcs in the grammar.
The behaviour of Riesbeck&apos;s verb-triggered packets
(verb-dependent sub-grammars) is actually independent of
when a pointer to the lexical decomposition of the verb
is established (i.e. whether a pointer is added as soon
as the verb is encountered or whether it is added after
the end of the sentence has been reached). Thus, any
claims about the possible advantages of &apos;early&apos; or
&apos;instantaneous&apos; decomposition are moot. Since
Riesbeck&apos;s cases are filled primarily on the basis of
fairly simple selectional restrictions, there is no
obvious reason why his parser couldn&apos;t have built some
other kind of internal representation, based on any one
of several linguistic theories of lexical decomposition.
Although Riesbeck&apos;s decomposition could occur after the
entire sentence has been parsed, LNR&apos;s decomposition
must occur at this point, because it uses a network-
matching algorithm to find already-present structures in
memory, and relies upon the arguments of the main n-ary
predicate of the sentence being as fully specified as
possible.
Computationally, the major difference between the two
parsers is that Riesbeck&apos;s parser uses interrupts to
initiate &apos;safe&apos; PUSHes and POPs to and from sub-grammars,
whereas the LNR parser performs &apos;risky&apos; PUSHes and POPs
like any purely top-down parser. Riesbeck&apos;s mechanism
is potentially very powerful, and the performance of the
LNR parser can be improved by allowing this mechanism to
be added automatically by the compiler which transforms
an LNR augmented transition network into GSP machine
code. Each parser can thus be mapped fairly cleanly
onto the other, with the only irreconcilable difference
between them being the degree to which they rely on
verb-dependent selectional restrictions to guide the
process of filling in case frames. This character-
ization of the differences between them, based on
implementing them within a common GSP framework, is
somewhat surprising, since (a) the differences have
nothing to do with &apos;conceptual dependency&apos; or &apos;active
semantic networks&apos; and (b) the computational difference
between them immediately suggests a way to automatically
incorporate bottom-up processing into the LNR parser to
improve not only its efficiency, but also its
psychological plausibility. A GSP implementation of a
&apos;hybrid&apos; version of the two parsers is outlined in (1).
</bodyText>
<page confidence="0.996849">
15
</page>
<sectionHeader confidence="0.97895" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.999918476190476">
(1) Eisenstadt, M. Alternative parsers fur conceptual
dependency: getting there is half the fun.
Proceedings of the sixth international
joint conference on artificial
intelligence, Tokyo, 1979.
(2) Kaplan, R.M. A general syntactic processor. In
R. Rustin (Ed.) Natural language
processing,. Englewood Cliffs, N.J.:
Prentice-Hall, 1973.
(3) Norman, D.A., Rumelhart, D.E., and the LNR
Research Group. Explorations in
cognition. San Francisco: W.h. Freeman
1975.
(4) Riesbeck, C.K. Computational understanding:
analysis of sentences and context.
Working paper 4, Istituto per gli Studi
Semantici e Cognitivi, Castagnola,
Switzerland, 1974.
(5) Schank, R.C. Conceptual dependency: a theory of
natural language understanding.
Cognitive Psychology, vol. 3, no. 4, 1972.
</reference>
<page confidence="0.998702">
16
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.079320">
<title confidence="0.995274">Schank/Riesbeck vs. Norman/Rumelhart: What&apos;s the Difference?</title>
<author confidence="0.999873">Marc Eisenstadt</author>
<affiliation confidence="0.999325">The Open University</affiliation>
<address confidence="0.980444">Milton Keynes, ENGLAND</address>
<abstract confidence="0.981113140740741">This paper explores the fundamental differences between two sentence-parsers developed in the early 1970&apos;s: Riesbeck&apos;s parser for Schank&apos;s&apos;conceptual dependency&apos; and the &apos;LNR&apos; parser for Norman and Rumelhart&apos;s &apos;active semantic network&apos; theory (3). The Riesbeck parser and the LNR parser share a common goal that of transforming an input sentence into a canonical form for later use by memory/inference/paraphrase processes. or both parsers, this transformation is the act of &apos;comprehension&apos;, although they appear to go about it in very different ways. Are these differences real or apparent? Riesbeck&apos;s parser is implemented as a production system, in which input text can either satisfy the condition side of any production rule within a packet of currently-active rules, or else interrupt processing by disabling the current packet of rules and enabling (&apos;triggering&apos;) a new packet of rules. In operation, the main verb of each segment of text is located, and a pointer to its lexical decomposition (canonical form) is established in memory. The surrounding text, primarily noun phrases, is then systematically mapped onto vacant case frame slots within the memory representation of the decomposed verb. Case information is signposted by a verb-triggered packet of production rules which expects classes of entity (e.g. animaterecipient) to be encountered in the text. Phrase boundaries are handled by keyword-triggered packets of rules which initiate and terminate the parsing of phrases. In contrast to this, the LNR parser is implemented as an augmented transition network, in which input text can either satisfy a current expectation or cause backtracking to a point at which an alternative expectation can be satisfied. In operation, input text is mapped onto a surface case frame, which is an n-ary predicate containing a pointer to the appropriate code responsible for decomposing the predicate into canonical form. Case information is signposted by property-list indicators stored in the lexical entry for verbs. These indicators act as signals or flags which are inspected by augmented tests on PUSH NP and PUSH PP arcs in order to decide whether such transitions are to be allowed. Phrase boundaries are handled by the standard ATN PUSH and POP mechanisms, with provision for backtracking if an initially-fulfilled expectation later turns out to have been incorrect. In order to determine which differences are due to notational conventions, I have implemented versions of both parsers in Kaplan&apos;s General Syntactic Processor ((ISP) formalism (2), a simple but elegant generalization of ATNs. In GSP terms, Riesbeck&apos;s active packets of production rules are grammar states, and each rule is represented as a grammar arc. Rule-packet triggering is handled by storing in the lexicon the GSP code which control to a new grammar state interrupt is called for. Each packet is in effect a sub-grammar of the type handled normally by an ATN PUSH and POP. The important difference is that the expensive actions normally associated with PUSH and POP (e.g. saving registers, building structures) only occur after it is safe to perform them. That is, bottom-up interrupts and very cheap &apos;lookahead&apos; ensure that wasteful backtracking is largely avoided. Riesbeck&apos;s verb-triggered packet of rules (i.e. the entire sub-grammar which is entered after the verb is &apos; encountered) is isomorphic to the LNR-style use of lexical flags, which are in effect &apos;raised&apos; and &apos;lowered&apos; solely for the benefit of augmented tests on verb-independent arcs. Where Riesbeck depicts a &apos;satisfied expectation&apos; by deleting the relevant production rule from the currently-active packet, LNR achieves the same effect by using augmented tests on PUSH NP and PUSH PP arcs to determine whether a case frame slot has alreadybeen filled. both approaches are handled with equal ease by GSP. In actual practice, Riesbeck&apos;s case frame expectations are typically tests for simple selectional restrictions, whereas LNR&apos;s case frame expectations are typically tests for the order in-which noun phrases areencountered. Prepositions, naturally, are used by both parsers as important case frame clues: Riesbeck has a verbtriggered action alter the interrupt code associated with prepositions so that they &apos;behave&apos; in precisely the right way; this is isomorphic to LNR&apos;s flags which are stored in the lexical entry for a verb and examined by augmented tests on verb-independent prepositional phrase arcs in the grammar. The behaviour of Riesbeck&apos;s verb-triggered packets sub-grammars) is actually independentof when a pointer to the lexical decomposition of the verb is established (i.e. whether a pointer is added as soon as the verb is encountered or whether it is added after end of the sentence has been reached). Thus, claims about the possible advantages of &apos;early&apos; or decomposition Since Riesbeck&apos;s cases are filled primarily on the basis of fairly simple selectional restrictions, there is no obvious reason why his parser couldn&apos;t have built some other kind of internal representation, based on any one of several linguistic theories of lexical decomposition. Although Riesbeck&apos;s decomposition could occur after the entire sentence has been parsed, LNR&apos;s decomposition must occur at this point, because it uses a networkmatching algorithm to find already-present structures in memory, and relies upon the arguments of the main n-ary predicate of the sentence being as fully specified as possible. Computationally, the major difference between the two parsers is that Riesbeck&apos;s parser uses interrupts to initiate &apos;safe&apos; PUSHes and POPs to and from sub-grammars, whereas the LNR parser performs &apos;risky&apos; PUSHes and POPs like any purely top-down parser. Riesbeck&apos;s mechanism is potentially very powerful, and the performance of the LNR parser can be improved by allowing this mechanism to be added automatically by the compiler which transforms an LNR augmented transition network into GSP machine code. Each parser can thus be mapped fairly cleanly onto the other, with the only irreconcilable difference between them being the degree to which they rely on verb-dependent selectional restrictions to guide the process of filling in case frames. This characterization of the differences between them, based on implementing them within a common GSP framework, is somewhat surprising, since (a) the differences have nothing to do with &apos;conceptual dependency&apos; or &apos;active semantic networks&apos; and (b) the computational difference between them immediately suggests a way to automatically incorporate bottom-up processing into the LNR parser to improve not only its efficiency, but also its psychological plausibility. A GSP implementation of a &apos;hybrid&apos; version of the two parsers is outlined in (1). 15 REFERENCES (1) Eisenstadt, M. Alternative parsers fur conceptual dependency: getting there is half the fun.</abstract>
<note confidence="0.89198605">Proceedings of the sixth international joint conference on artificial intelligence,Tokyo, 1979. (2) Kaplan, R.M. A general syntactic processor. In Rustin (Ed.) language processing,.Englewood Cliffs, N.J.: Prentice-Hall, 1973. (3) Norman, D.A., Rumelhart, D.E., and the LNR Group. cognition.San Francisco: W.h. Freeman 1975. (4) Riesbeck, C.K. Computational understanding: analysis of sentences and context. Working paper 4, Istituto per gli Studi Semantici e Cognitivi, Castagnola, Switzerland, 1974. (5) Schank, R.C. Conceptual dependency: a theory of natural language understanding. Psychology,vol. 4, 1972. 16</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Eisenstadt</author>
</authors>
<title>Alternative parsers fur conceptual dependency: getting there is half the fun.</title>
<date>1979</date>
<booktitle>Proceedings of the sixth international joint conference on artificial intelligence,</booktitle>
<location>Tokyo,</location>
<marker>(1)</marker>
<rawString>Eisenstadt, M. Alternative parsers fur conceptual dependency: getting there is half the fun. Proceedings of the sixth international joint conference on artificial intelligence, Tokyo, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R M Kaplan</author>
</authors>
<title>A general syntactic processor. In</title>
<date>1973</date>
<publisher>Prentice-Hall,</publisher>
<location>N.J.:</location>
<contexts>
<context position="2704" citStr="(2)" startWordPosition="409" endWordPosition="409"> by property-list indicators stored in the lexical entry for verbs. These indicators act as signals or flags which are inspected by augmented tests on PUSH NP and PUSH PP arcs in order to decide whether such transitions are to be allowed. Phrase boundaries are handled by the standard ATN PUSH and POP mechanisms, with provision for backtracking if an initially-fulfilled expectation later turns out to have been incorrect. In order to determine which differences are due to notational conventions, I have implemented versions of both parsers in Kaplan&apos;s General Syntactic Processor ((ISP) formalism (2), a simple but elegant generalization of ATNs. In GSP terms, Riesbeck&apos;s active packets of production rules are grammar states, and each rule is represented as a grammar arc. Rule-packet triggering is handled by storing in the lexicon the GSP code which transfers control to a new grammar state when an interrupt is called for. Each packet is in effect a sub-grammar of the type handled normally by an ATN PUSH and POP. The important difference is that the expensive actions normally associated with PUSH and POP (e.g. saving registers, building structures) only occur after it is safe to perform them</context>
</contexts>
<marker>(2)</marker>
<rawString>Kaplan, R.M. A general syntactic processor. In R. Rustin (Ed.) Natural language processing,. Englewood Cliffs, N.J.: Prentice-Hall, 1973.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D A Norman</author>
<author>D E Rumelhart</author>
</authors>
<title>and the LNR Research Group. Explorations in cognition.</title>
<date>1975</date>
<location>San Francisco: W.h. Freeman</location>
<marker>(3)</marker>
<rawString>Norman, D.A., Rumelhart, D.E., and the LNR Research Group. Explorations in cognition. San Francisco: W.h. Freeman 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Riesbeck</author>
</authors>
<title>Computational understanding: analysis of sentences and context. Working paper 4,</title>
<date>1974</date>
<booktitle>Istituto per gli Studi Semantici e Cognitivi,</booktitle>
<location>Castagnola, Switzerland,</location>
<marker>(4)</marker>
<rawString>Riesbeck, C.K. Computational understanding: analysis of sentences and context. Working paper 4, Istituto per gli Studi Semantici e Cognitivi, Castagnola, Switzerland, 1974.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Schank</author>
</authors>
<title>Conceptual dependency: a theory of natural language understanding.</title>
<date>1972</date>
<journal>Cognitive Psychology,</journal>
<volume>3</volume>
<marker>(5)</marker>
<rawString>Schank, R.C. Conceptual dependency: a theory of natural language understanding. Cognitive Psychology, vol. 3, no. 4, 1972.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>