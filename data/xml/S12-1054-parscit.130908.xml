<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.044707">
<title confidence="0.892959">
EMNLP@CPH: Is frequency all there is to simplicity?
</title>
<author confidence="0.963985">
Anders Johannsen, Héctor Martínez, Sigrid Klerke†, Anders SOgaard
</author>
<affiliation confidence="0.9952985">
Centre for Language Technology
University of Copenhagen
</affiliation>
<email confidence="0.9876135">
{ajohannsen|alonso|soegaard}@hum.ku.dk
sigridklerke@gmail.com†
</email>
<sectionHeader confidence="0.982448" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999856772727273">
Our system breaks down the problem of rank-
ing a list of lexical substitutions according to
how simple they are in a given context into a
series of pairwise comparisons between can-
didates. For this we learn a binary classifier.
As only very little training data is provided,
we describe a procedure for generating artifi-
cial unlabeled data from Wordnet and a corpus
and approach the classification task as a semi-
supervised machine learning problem. We use
a co-training procedure that lets each classi-
fier increase the other classifier’s training set
with selected instances from an unlabeled data
set. Our features include n-gram probabilities
of candidate and context in a web corpus, dis-
tributional differences of candidate in a cor-
pus of “easy” sentences and a corpus of normal
sentences, syntactic complexity of documents
that are similar to the given context, candidate
length, and letter-wise recognizability of can-
didate as measured by a trigram character lan-
guage model.
</bodyText>
<sectionHeader confidence="0.997335" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99748675">
This paper describes a system for the SemEval 2012
English Lexical Simplification shared task. The
task description uses a loose definition of simplic-
ity, defining “simple words” as “words that can be
understood by a wide variety of people, including for
example people with low literacy levels or some cog-
nitive disability, children, and non-native speakers of
English” (Specia et al., 2012).
</bodyText>
<table confidence="0.8982414">
Feature r
NGMsf 0.33
NGMsf+1 0.27
NGMsf−1 0.27
Lsf -0.26
Lmax -0.26
RIproto(l) -0.18
SYcn -0.17
SYw -0.17
SYcp -0.17
</table>
<tableCaption confidence="0.760461">
Table 1: Pearson’s r correlations. The table shows
the three highest correlated features per group, all of
which are significant at the p &lt; 0.01 level
</tableCaption>
<sectionHeader confidence="0.981026" genericHeader="introduction">
2 Features
</sectionHeader>
<bodyText confidence="0.999979705882353">
We model simplicity with a range of features divided
into six groups. Five of these groups make use of
the distributional hypothesis and rely on external cor-
pora. We measure a candidate’s distribution in terms
of its lexical associations (RI), participation in syn-
tactic structures (SY), or corpus presence in order to
assess its simplicity (NGM, SW, CH). A single
group, L, measures intrinsic aspects of the substi-
tution candidate, such as its length.
The substitution candidate is either an adjective,
an adverb, a noun, or a verb, and all candidates within
a list share the same part of speech. Because word
class might influence simplicity, we allow our model
to fit parameters specific to the candidate’s part of
speech by making a copy of the features for each part
of speech which is active only when the candidate is
in the given part of speech.
</bodyText>
<figure confidence="0.998433">
Feature r
RIproto(f) -0.15
CHmax -0.14
RIorig(l) -0.11
Ltokens -0.10
CHmin 0.10
SWfreq 0.08
SWLLR 0.07
CHavg -0.04
</figure>
<page confidence="0.95359">
408
</page>
<note confidence="0.530227">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 408–412,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999421948717948">
Simple Wikipedia (SW) These two features con-
tain relative frequency counts of the substitution
form in Simple English Wikipedia (SWfreq), and the
log likelihood ratio of finding the word in the simple
corpus to finding it in regular Wikipedia (SWLLR)1.
Word length (LE) This set of three features de-
scribes the length of the substitution form in char-
acters (L,f), the length of the longest token
(Lmax), and the length of the substitution form in
tokens (Ltoken,). Word length is an integral part
of common measures of text complexity, e.g in the
English Flesch–Kincaid (Kincaid et al., 1975) in the
form of syllable count, and in the Scandinavian LIX
(Bjornsson, 1983).
Character trigram model (CHA) These three
features approximate the reading difficulty of a word
in terms of the probabilities of its forming character
trigrams, with special characters to mark word be-
ginning and end. A word with an unusual combi-
nation of characters takes longer to read and is per-
ceived as less simple (Ehri, 2005).
We calculate the minimum, average, and maxi-
mum trigram probability (CHmin, CHavg, and
CHmax).2
Web corpus N-gram (NGAM) These 12 features
were obtained from a pre-built web-scale language
model3. Features of the form N,f±i, where
0 &lt; i &lt; 4, express the probability of seeing the
substitution form together with the following (or pre-
vious) unigram, bigram, or trigram. N,f is
the probability of substitution form itself, a feature
which also is the backbone of our frequency base-
line.
Random Indexing (RI) These four features are
obtained from measures taken from a word-to-word
distributional semantic model. Random Indexing
(RI) was chosen for efficiency reasons (Sahlgren,
2005). We include features describing the seman-
tic distances between the candidate and the original
</bodyText>
<footnote confidence="0.962368666666667">
1Wikipedia dump obtained March 27, 2012. Date on the
Simple Wikipedia dump is March 22, 2012.
2Trigram probabilities derived from Google T1 unigram
counts.
3The “jun09/body” trigram model from Microsoft Web N-
gram Services.
</footnote>
<bodyText confidence="0.999962025">
form (RIorig), and between the candidate and a proto-
type vector (RIproto). For the distance between can-
didate and original, we hypothesize that annotators
would prefer a synonym closer to the original form.
A prototype distributional vector of a set of words is
built by summing the individual word vectors, thus
obtaining a representation that approximates the be-
havior of that class overall (Turney and Pantel, 2010).
Longer distances indicate that the currently exam-
ined substitution is far from the shared meaning of
all the synonyms, making it a less likely candidate.
The features are included for both lemma and surface
forms of the words.
Syntactic complexity (SY) These 23 features
measure the syntactic complexity of documents
where the substitution candidate occurs. We used
measures from (Lu, 2010) in which they describe 14
automatic measures of syntactic complexity calcu-
lated from frequency counts of 9 types of syntactic
structures. This group of syntax-metric scores builds
on two ideas.
First, syntactic complexity and word difficulty go
together. A sentence with a complicated syntax is
more likely to be made up of difficult words, and
conversely, the probability that a word in a sentence
is simple goes up when we know that the syntax of
the sentence is uncomplicated. To model this we
search for instances of the substitution candidates in
the UKWAC corpus4 and measure the syntactic com-
plexity of the documents where they occur.
Second, the perceived simplicity of a word may
change depending on the context. Consider the ad-
jective “frigid”, which may be judged to be sim-
pler than “gelid” if referring to temperature, but per-
haps less simple than “ice-cold” when characterizing
someone’s personality. These differences in word
sense are taken into account by measuring the sim-
ilarity between corpus documents and substitution
contexts and use these values to provide a weighted
average of the syntactic complexity measures.
</bodyText>
<sectionHeader confidence="0.977351" genericHeader="method">
3 Unlabeled data
</sectionHeader>
<bodyText confidence="0.999516666666667">
The unlabeled data set was generated by a three-
step procedure involving synonyms extracted from
Wordnet5 and sentences from the UKWAC corpus.
</bodyText>
<footnote confidence="0.9999625">
4http://wacky.sslmit.unibo.it/
5http://wordnet.princeton.edu/
</footnote>
<page confidence="0.993197">
409
</page>
<listItem confidence="0.872363">
1) Collection: Find synsets for unambigious lem-
</listItem>
<bodyText confidence="0.962956583333333">
mas in Wordnet. The synsets must have more than
three synonyms. Search for the lemmas in the cor-
pus. Generate unlabeled instances by replacing the
lemma with each of its synonyms. 2) Sampling: In
the unlabeled corpus, reduce the number of ranking
problems per lemma to a maximum of 10. Sample
from this pool while maintaining a distribution of
part of speech similar to that of the trial and test set.
3) Filtering: Remove instances for which there are
missing values in our features.
The unlabeled part of our final data set contains
n = 1783 problems.
</bodyText>
<sectionHeader confidence="0.996632" genericHeader="method">
4 Ranking
</sectionHeader>
<bodyText confidence="0.9999278">
We are given a number of ranking problems (n =
300 in the trial set and n = 1710 for the test data).
Each of these consists of a text extract with a posi-
tion marked for substitution, and a set of candidate
substitutions.
</bodyText>
<subsectionHeader confidence="0.993716">
4.1 Linear order
</subsectionHeader>
<bodyText confidence="0.999994352941177">
Let XW be the substitution set for the i-th problem.
We can then formalize the ranking problem by as-
suming that we have access to a set of (weighted)
preference judgments, w(a --&lt; b) for all a, b E XW
such that w(a --&lt; b) is the value of ranking item a
ahead of b. The values are the confidence-weighted
pair-wise decisions from our binary classifier. Our
goal is then to establish a total order on XW that
maximizes the value of the non-violated judgments.
This is an instance of the Linear Ordering Problem
(Martí and Reinelt, 2011), which is known to be NP-
hard. However, with problems of our size (maximum
ten items in each ranking), we escape these complex-
ity issues by a very narrow margin—10! Pt� 3.6 mil-
lion means that the number of possible orderings is
small enough to make it feasible to find the optimal
one by exhaustive enumeration of all possibilities.
</bodyText>
<subsectionHeader confidence="0.974011">
4.2 Binary classication
</subsectionHeader>
<bodyText confidence="0.999971592592593">
In order to turn our ranking problem into binary clas-
sification, we generate a new data set by enumerat-
ing all point-wise comparisons within a problem and
for each apply a transformation function 4b(a, b) =
a − b. Thus each data point in the new set is the
difference between the feature values of two candi-
dates. This enables us to learn a binary classifier for
the relation “ranks ahead of”.
We use the trial set for labeled training data L and,
in a transductive manner, treat the test set as unla-
beled data Utest. Further, we supplement the pool of
unlabeled data with artificially generated instances
Uge,,, such that U = Utest U Uge,,.
Using a co-training setup (Blum and Mitchell,
1998), we divide our features in two independent sets
and train a large margin classifier6 on each split. The
classifiers then provide labels for data in the unla-
beled set, adding the k most confidently labeled in-
stances to the training data for the other classifier, an
iterative process which continues until there is no un-
labeled data left. At the end of the training we have
two classifiers. The classification result is a mixture-
of-experts: the most confident prediction of the two
classifiers. Furthermore, as an upper-bound of the
co-training procedure, we define an oracle that re-
turns the correct answer whenever it is given by at
least one classifier.
</bodyText>
<subsectionHeader confidence="0.938853">
4.3 Ties
</subsectionHeader>
<bodyText confidence="0.999931833333333">
In many cases we have items a and b that tie—in
which case both a � b and b � a are violated. We
deal with these instances by omitting them from the
training set and setting w(a � b) = 0. For the fi-
nal ranking, our system makes no attempt to produce
ties.
</bodyText>
<sectionHeader confidence="0.996482" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999657384615385">
In our experiments we vary feature-split, size of un-
labeled data, and number of iterations. The first fea-
ture split, SY–SW, pooled all syntactic complexity
features and Wikipedia-based features in one view,
with the remaining feature groups in another view.
Our second feature split, SY–C–L, combined
the syntactic complexity features with the character
trigram language model features and the basic word
length features. Both splits produced a pair of classi-
fiers with similar performance—each had an F-score
of around .73 and an oracle score of .87 on the trial
set on the binary decision problem, and both splits
performed equally on the ranking task.
</bodyText>
<footnote confidence="0.9998435">
6Liblinear with L1 penalty and L2 loss. Parameter settings
were default. http://www.csie.ntu.edu.tw/∼cjlin/liblinear/
</footnote>
<page confidence="0.983167">
410
</page>
<table confidence="0.999362333333333">
System All N V R A
MFQ 0.449 0.367 0.456 0.487 0.493
SY–SWf 0.377 0.283 0.269 0.271 0.421
SY–SWl 0.425 0.355 0.497 0.408 0.425
SY–C–Lf 0.377 0.284 0.469 0.270 0.421
SY–C–Ll 0.435 0.362 0.481 0.465 0.439
</table>
<tableCaption confidence="0.993463">
Table 2: Performance on part of speech. Unlabeled
</tableCaption>
<bodyText confidence="0.991131038461539">
set was Utest. Subscripts tell whether the scores are
from the first or last iteration
With a large unlabeled data set available, the clas-
sifiers can avoid picking and labeling data points
with a low certainty, at least initially. The assump-
tion is that this will give us a higher quality training
set. However, as can be seen in Figure 1, none of our
systems are benefitting from the additional data. In
fact, the systems learn more when the pool of unla-
beled data is restricted to the test set.
Our submitted systems, O1 and O2 scored
0.405 and 0.393 on the test set, and 0.494 and 0.500
on the trial set. Following submission we adjusted
a parameter7 and re-ran each split with both U and
Utest.
We analyzed the performance by part of speech
and compared them to the frequency baseline as
shown in Table 2. For the frequency baseline, per-
formance is better on adverbs and adjectives alone,
and somewhat worse on nouns. Both our sys-
tems benefit from co-training on all word classes.
SY–C–L, our best performing system, no-
tably has a score reduction (compared to the base-
line) of only 5% on adverbs, eliminates the score re-
duction on nouns, and effectively beats the baseline
score on verbs with a 6% increase.
</bodyText>
<sectionHeader confidence="0.999563" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999701888888889">
The frequency baseline has proven very strong, and,
as witnessed by the correlations in Table 1, frequency
is by far the most powerful signal for “simplicity”.
But is that all there is to simplicity? Perhaps it is.
For a person with normal reading ability, a sim-
ple word may be just a word with which the per-
son is well-acquainted—one that he has seen be-
fore enough times to have a good idea about what
it means and in which contexts it is typically used.
</bodyText>
<footnote confidence="0.9586835">
7In particular, we selected a larger value for the C parameter
in the liblinear classifier.
</footnote>
<figure confidence="0.985805">
0 5000 10000 15000 20000 25000
Unlabeled datapoints
</figure>
<figureCaption confidence="0.9618525">
Figure 1: Test set kappa score vs. number of data
points labeled during co-training
</figureCaption>
<bodyText confidence="0.9992965">
And so an n-gram model might be a fair approxi-
mation. However, lexical simplicity in English may
still be something very different to readers with low
literacy. For instance, the highly complex letter-to-
sound mapping rules are likely to prevent such read-
ers from arriving at the correct pronunciation of un-
seen words and thus frequent words with exceptional
spelling patterns may not seem simple at all.
A source of misclassifications discovered in our
error analysis is the fact that substituting candidates
into the given contexts in a straight-forward manner
can introduce syntactic errors. Fixing these can re-
quire significant revisions of the sentence, and yet
the substitutions resulting in an ungrammatical sen-
tence are sometimes still preferred to grammatical al-
ternatives.8 Here, scoring the substitution and the
immediate context in a language model is of little
use. Moreover, while these odd grammatical errors
may be preferable to many non-native English speak-
ers with adequate reading skills, such errors can be
more obstructing to reading impaired users and be-
ginning language learners.
</bodyText>
<sectionHeader confidence="0.992384" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<footnote confidence="0.915766142857143">
This research is partially funded by the European Commission’s
7th Framework Program under grant agreement n° 238405
(CLARA).
8For example sentence 1528: ”However, it appears they in-
tend to pull out all stops to get what they want.” Gold: {try ev-
erything} {do everything it takes} {pull} {stop at nothing} {go
to any length} {yank}.
</footnote>
<figure confidence="0.9943904">
0.44
0.48
0.46
0.42
0.40
0.38
SYN-SW(U,,,)
SYN-CHAR-LEN(Ut,,,)
SYN-CHAR-LEN(U)
Score
</figure>
<page confidence="0.994773">
411
</page>
<sectionHeader confidence="0.98737" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999752">
C. H. Bjornsson. 1983. Readability of Newspa-
pers in 11 Languages. Reading Research Quarterly,
18(4):480–497.
A Blum and T Mitchell. 1998. Combining labeled and
unlabeled data with co-training. In Proceedings of the
eleventh annual conference on Computational learning
theory, pages 92–100. ACM.
Linnea C. Ehri. 2005. Learning to read words: The-
ory, findings, and issues. Scientific Studies of Reading,
9(2):167–188.
J P Kincaid, R P Fishburne, R L Rogers, and B S Chissom.
1975. Derivation of New Readability Formulas (Auto-
mated Readability Index, Fog Count and Flesch Read-
ing Ease Formula) for Navy Enlisted Personnel.
Xiaofei Lu. 2010. Automatic analysis of syntactic com-
plexity in second language writing. International Jour-
nal of Corpus Linguistics, 15(4):474–496.
Rafael Martí and Gerhard Reinelt. 2011. The Lin-
ear Ordering Problem: Exact and Heuristic Methods
in Combinatorial Optimization (Applied Mathematical
Sciences). Springer.
Magnus Sahlgren. 2005. An introduction to random
indexing. In Methods and Applications of Seman-
tic Indexing Workshop at the 7th International Con-
ference on Terminology and Knowledge Engineering,
TKE, volume 5.
Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea. 2012.
SemEval-2012 Task 1: English Lexical Simplifica-
tion. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.
P. D Turney and P. Pantel. 2010. From frequency to
meaning: Vector space models of semantics. Journal
of Artificial Intelligence Research, 37:141–188.
</reference>
<page confidence="0.997702">
412
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.431957">
<title confidence="0.579259">EMNLP@CPH: Is frequency all there is to simplicity?</title>
<author confidence="0.458514">Héctor Martínez Johannsen</author>
<author confidence="0.458514">Sigrid Anders</author>
<affiliation confidence="0.994352">Centre for Language University of</affiliation>
<abstract confidence="0.998559347826087">Our system breaks down the problem of ranking a list of lexical substitutions according to how simple they are in a given context into a series of pairwise comparisons between candidates. For this we learn a binary classifier. As only very little training data is provided, we describe a procedure for generating artificial unlabeled data from Wordnet and a corpus and approach the classification task as a semisupervised machine learning problem. We use a co-training procedure that lets each classifier increase the other classifier’s training set with selected instances from an unlabeled data set. Our features include n-gram probabilities of candidate and context in a web corpus, distributional differences of candidate in a corpus of “easy” sentences and a corpus of normal sentences, syntactic complexity of documents that are similar to the given context, candidate length, and letter-wise recognizability of candidate as measured by a trigram character language model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C H Bjornsson</author>
</authors>
<date>1983</date>
<journal>Readability of Newspapers in 11 Languages. Reading Research Quarterly,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="3759" citStr="Bjornsson, 1983" startWordPosition="591" endWordPosition="592">equency counts of the substitution form in Simple English Wikipedia (SWfreq), and the log likelihood ratio of finding the word in the simple corpus to finding it in regular Wikipedia (SWLLR)1. Word length (LE) This set of three features describes the length of the substitution form in characters (L,f), the length of the longest token (Lmax), and the length of the substitution form in tokens (Ltoken,). Word length is an integral part of common measures of text complexity, e.g in the English Flesch–Kincaid (Kincaid et al., 1975) in the form of syllable count, and in the Scandinavian LIX (Bjornsson, 1983). Character trigram model (CHA) These three features approximate the reading difficulty of a word in terms of the probabilities of its forming character trigrams, with special characters to mark word beginning and end. A word with an unusual combination of characters takes longer to read and is perceived as less simple (Ehri, 2005). We calculate the minimum, average, and maximum trigram probability (CHmin, CHavg, and CHmax).2 Web corpus N-gram (NGAM) These 12 features were obtained from a pre-built web-scale language model3. Features of the form N,f±i, where 0 &lt; i &lt; 4, express the </context>
</contexts>
<marker>Bjornsson, 1983</marker>
<rawString>C. H. Bjornsson. 1983. Readability of Newspapers in 11 Languages. Reading Research Quarterly, 18(4):480–497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blum</author>
<author>T Mitchell</author>
</authors>
<title>Combining labeled and unlabeled data with co-training.</title>
<date>1998</date>
<booktitle>In Proceedings of the eleventh annual conference on Computational learning theory,</booktitle>
<pages>92--100</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="9715" citStr="Blum and Mitchell, 1998" startWordPosition="1575" endWordPosition="1578">enerate a new data set by enumerating all point-wise comparisons within a problem and for each apply a transformation function 4b(a, b) = a − b. Thus each data point in the new set is the difference between the feature values of two candidates. This enables us to learn a binary classifier for the relation “ranks ahead of”. We use the trial set for labeled training data L and, in a transductive manner, treat the test set as unlabeled data Utest. Further, we supplement the pool of unlabeled data with artificially generated instances Uge,,, such that U = Utest U Uge,,. Using a co-training setup (Blum and Mitchell, 1998), we divide our features in two independent sets and train a large margin classifier6 on each split. The classifiers then provide labels for data in the unlabeled set, adding the k most confidently labeled instances to the training data for the other classifier, an iterative process which continues until there is no unlabeled data left. At the end of the training we have two classifiers. The classification result is a mixtureof-experts: the most confident prediction of the two classifiers. Furthermore, as an upper-bound of the co-training procedure, we define an oracle that returns the correct</context>
</contexts>
<marker>Blum, Mitchell, 1998</marker>
<rawString>A Blum and T Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, pages 92–100. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Linnea C Ehri</author>
</authors>
<title>Learning to read words: Theory, findings, and issues.</title>
<date>2005</date>
<journal>Scientific Studies of Reading,</journal>
<volume>9</volume>
<issue>2</issue>
<contexts>
<context position="4093" citStr="Ehri, 2005" startWordPosition="647" endWordPosition="648">(Lmax), and the length of the substitution form in tokens (Ltoken,). Word length is an integral part of common measures of text complexity, e.g in the English Flesch–Kincaid (Kincaid et al., 1975) in the form of syllable count, and in the Scandinavian LIX (Bjornsson, 1983). Character trigram model (CHA) These three features approximate the reading difficulty of a word in terms of the probabilities of its forming character trigrams, with special characters to mark word beginning and end. A word with an unusual combination of characters takes longer to read and is perceived as less simple (Ehri, 2005). We calculate the minimum, average, and maximum trigram probability (CHmin, CHavg, and CHmax).2 Web corpus N-gram (NGAM) These 12 features were obtained from a pre-built web-scale language model3. Features of the form N,f±i, where 0 &lt; i &lt; 4, express the probability of seeing the substitution form together with the following (or previous) unigram, bigram, or trigram. N,f is the probability of substitution form itself, a feature which also is the backbone of our frequency baseline. Random Indexing (RI) These four features are obtained from measures taken from a word-to-word distr</context>
</contexts>
<marker>Ehri, 2005</marker>
<rawString>Linnea C. Ehri. 2005. Learning to read words: Theory, findings, and issues. Scientific Studies of Reading, 9(2):167–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J P Kincaid</author>
<author>R P Fishburne</author>
<author>R L Rogers</author>
<author>B S Chissom</author>
</authors>
<title>Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel.</title>
<date>1975</date>
<contexts>
<context position="3682" citStr="Kincaid et al., 1975" startWordPosition="576" endWordPosition="579">utational Linguistics Simple Wikipedia (SW) These two features contain relative frequency counts of the substitution form in Simple English Wikipedia (SWfreq), and the log likelihood ratio of finding the word in the simple corpus to finding it in regular Wikipedia (SWLLR)1. Word length (LE) This set of three features describes the length of the substitution form in characters (L,f), the length of the longest token (Lmax), and the length of the substitution form in tokens (Ltoken,). Word length is an integral part of common measures of text complexity, e.g in the English Flesch–Kincaid (Kincaid et al., 1975) in the form of syllable count, and in the Scandinavian LIX (Bjornsson, 1983). Character trigram model (CHA) These three features approximate the reading difficulty of a word in terms of the probabilities of its forming character trigrams, with special characters to mark word beginning and end. A word with an unusual combination of characters takes longer to read and is perceived as less simple (Ehri, 2005). We calculate the minimum, average, and maximum trigram probability (CHmin, CHavg, and CHmax).2 Web corpus N-gram (NGAM) These 12 features were obtained from a pre-built web-scale l</context>
</contexts>
<marker>Kincaid, Fishburne, Rogers, Chissom, 1975</marker>
<rawString>J P Kincaid, R P Fishburne, R L Rogers, and B S Chissom. 1975. Derivation of New Readability Formulas (Automated Readability Index, Fog Count and Flesch Reading Ease Formula) for Navy Enlisted Personnel.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofei Lu</author>
</authors>
<title>Automatic analysis of syntactic complexity in second language writing.</title>
<date>2010</date>
<journal>International Journal of Corpus Linguistics,</journal>
<volume>15</volume>
<issue>4</issue>
<contexts>
<context position="5919" citStr="Lu, 2010" startWordPosition="929" endWordPosition="930">orm. A prototype distributional vector of a set of words is built by summing the individual word vectors, thus obtaining a representation that approximates the behavior of that class overall (Turney and Pantel, 2010). Longer distances indicate that the currently examined substitution is far from the shared meaning of all the synonyms, making it a less likely candidate. The features are included for both lemma and surface forms of the words. Syntactic complexity (SY) These 23 features measure the syntactic complexity of documents where the substitution candidate occurs. We used measures from (Lu, 2010) in which they describe 14 automatic measures of syntactic complexity calculated from frequency counts of 9 types of syntactic structures. This group of syntax-metric scores builds on two ideas. First, syntactic complexity and word difficulty go together. A sentence with a complicated syntax is more likely to be made up of difficult words, and conversely, the probability that a word in a sentence is simple goes up when we know that the syntax of the sentence is uncomplicated. To model this we search for instances of the substitution candidates in the UKWAC corpus4 and measure the syntactic com</context>
</contexts>
<marker>Lu, 2010</marker>
<rawString>Xiaofei Lu. 2010. Automatic analysis of syntactic complexity in second language writing. International Journal of Corpus Linguistics, 15(4):474–496.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rafael Martí</author>
<author>Gerhard Reinelt</author>
</authors>
<title>The Linear Ordering Problem: Exact and Heuristic Methods</title>
<date>2011</date>
<booktitle>in Combinatorial Optimization (Applied Mathematical Sciences).</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="8665" citStr="Martí and Reinelt, 2011" startWordPosition="1389" endWordPosition="1392"> position marked for substitution, and a set of candidate substitutions. 4.1 Linear order Let XW be the substitution set for the i-th problem. We can then formalize the ranking problem by assuming that we have access to a set of (weighted) preference judgments, w(a --&lt; b) for all a, b E XW such that w(a --&lt; b) is the value of ranking item a ahead of b. The values are the confidence-weighted pair-wise decisions from our binary classifier. Our goal is then to establish a total order on XW that maximizes the value of the non-violated judgments. This is an instance of the Linear Ordering Problem (Martí and Reinelt, 2011), which is known to be NPhard. However, with problems of our size (maximum ten items in each ranking), we escape these complexity issues by a very narrow margin—10! Pt� 3.6 million means that the number of possible orderings is small enough to make it feasible to find the optimal one by exhaustive enumeration of all possibilities. 4.2 Binary classication In order to turn our ranking problem into binary classification, we generate a new data set by enumerating all point-wise comparisons within a problem and for each apply a transformation function 4b(a, b) = a − b. Thus each data point in the n</context>
</contexts>
<marker>Martí, Reinelt, 2011</marker>
<rawString>Rafael Martí and Gerhard Reinelt. 2011. The Linear Ordering Problem: Exact and Heuristic Methods in Combinatorial Optimization (Applied Mathematical Sciences). Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>An introduction to random indexing.</title>
<date>2005</date>
<booktitle>In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE,</booktitle>
<volume>5</volume>
<contexts>
<context position="4790" citStr="Sahlgren, 2005" startWordPosition="753" endWordPosition="754">avg, and CHmax).2 Web corpus N-gram (NGAM) These 12 features were obtained from a pre-built web-scale language model3. Features of the form N,f±i, where 0 &lt; i &lt; 4, express the probability of seeing the substitution form together with the following (or previous) unigram, bigram, or trigram. N,f is the probability of substitution form itself, a feature which also is the backbone of our frequency baseline. Random Indexing (RI) These four features are obtained from measures taken from a word-to-word distributional semantic model. Random Indexing (RI) was chosen for efficiency reasons (Sahlgren, 2005). We include features describing the semantic distances between the candidate and the original 1Wikipedia dump obtained March 27, 2012. Date on the Simple Wikipedia dump is March 22, 2012. 2Trigram probabilities derived from Google T1 unigram counts. 3The “jun09/body” trigram model from Microsoft Web Ngram Services. form (RIorig), and between the candidate and a prototype vector (RIproto). For the distance between candidate and original, we hypothesize that annotators would prefer a synonym closer to the original form. A prototype distributional vector of a set of words is built by summing the</context>
</contexts>
<marker>Sahlgren, 2005</marker>
<rawString>Magnus Sahlgren. 2005. An introduction to random indexing. In Methods and Applications of Semantic Indexing Workshop at the 7th International Conference on Terminology and Knowledge Engineering, TKE, volume 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Sujay K Jauhar</author>
<author>Rada Mihalcea</author>
</authors>
<title>SemEval-2012 Task 1: English Lexical Simplification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="1632" citStr="Specia et al., 2012" startWordPosition="244" endWordPosition="247">rpus of normal sentences, syntactic complexity of documents that are similar to the given context, candidate length, and letter-wise recognizability of candidate as measured by a trigram character language model. 1 Introduction This paper describes a system for the SemEval 2012 English Lexical Simplification shared task. The task description uses a loose definition of simplicity, defining “simple words” as “words that can be understood by a wide variety of people, including for example people with low literacy levels or some cognitive disability, children, and non-native speakers of English” (Specia et al., 2012). Feature r NGMsf 0.33 NGMsf+1 0.27 NGMsf−1 0.27 Lsf -0.26 Lmax -0.26 RIproto(l) -0.18 SYcn -0.17 SYw -0.17 SYcp -0.17 Table 1: Pearson’s r correlations. The table shows the three highest correlated features per group, all of which are significant at the p &lt; 0.01 level 2 Features We model simplicity with a range of features divided into six groups. Five of these groups make use of the distributional hypothesis and rely on external corpora. We measure a candidate’s distribution in terms of its lexical associations (RI), participation in syntactic structures (SY), or corpus presenc</context>
</contexts>
<marker>Specia, Jauhar, Mihalcea, 2012</marker>
<rawString>Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea. 2012. SemEval-2012 Task 1: English Lexical Simplification. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
<author>P Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="5526" citStr="Turney and Pantel, 2010" startWordPosition="866" endWordPosition="869">btained March 27, 2012. Date on the Simple Wikipedia dump is March 22, 2012. 2Trigram probabilities derived from Google T1 unigram counts. 3The “jun09/body” trigram model from Microsoft Web Ngram Services. form (RIorig), and between the candidate and a prototype vector (RIproto). For the distance between candidate and original, we hypothesize that annotators would prefer a synonym closer to the original form. A prototype distributional vector of a set of words is built by summing the individual word vectors, thus obtaining a representation that approximates the behavior of that class overall (Turney and Pantel, 2010). Longer distances indicate that the currently examined substitution is far from the shared meaning of all the synonyms, making it a less likely candidate. The features are included for both lemma and surface forms of the words. Syntactic complexity (SY) These 23 features measure the syntactic complexity of documents where the substitution candidate occurs. We used measures from (Lu, 2010) in which they describe 14 automatic measures of syntactic complexity calculated from frequency counts of 9 types of syntactic structures. This group of syntax-metric scores builds on two ideas. First, synta</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>P. D Turney and P. Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>