<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000222">
<title confidence="0.986826">
Validation and Regression Testing for a Cross-linguisic Grammar Resource
</title>
<author confidence="0.999464">
Emily M. Bender, Laurie Poulson, Scott Drellishak, Chris Evans
</author>
<affiliation confidence="0.9974125">
University of Washington
Department of Linguistics
</affiliation>
<address confidence="0.580055">
Seattle WA 98195-4340 USA
</address>
<email confidence="0.999239">
{ebender,lpoulson,sfd,chrisev@u.washington.edu}
</email>
<sectionHeader confidence="0.998602" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999991884615385">
We present a validation methodology for
a cross-linguistic grammar resource which
produces output in the form of small gram-
mars based on elicited typological descrip-
tions. Evaluating the resource entails sam-
pling from a very large space of language
types, the type and range of which preclude
the use of standard test suites development
techniques. We produce a database from
which gold standard test suites for these
grammars can be generated on demand, in-
cluding well-formed strings paired with all
of their valid semantic representations as
well as a sample of ill-formed strings. These
string-semantics pairs are selected from a
set of candidates by a system of regular-
expression based filters. The filters amount
to an alternative grammar building system,
whose generative capacity is limited com-
pared to the actual grammars. We perform
error analysis of the discrepancies between
the test suites and grammars for a range of
language types, and update both systems ap-
propriately. The resulting resource serves as
a point of comparison for regression testing
in future development.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999101875">
The development and maintenance of test suites is
integral to the process of writing deep linguistic
grammars (Oepen and Flickinger, 1998; Butt and
King, 2003). Such test suites typically contain hand-
constructed examples illustrating the grammatical
phenomena treated by the grammar as well as rep-
resentative examples taken from texts from the tar-
get domain. In combination with test suite manage-
ment software such as [incr tsdbo)] (Oepen, 2002),
they are used for validation and regression testing of
precision (deep linguistic) grammars as well as the
exploration of potential changes to the grammar.
In this paper, we consider what happens when the
precision grammar resource being developed isn’t a
grammar of a particular language, but rather a cross-
linguistic grammar resource. In particular, we con-
sider the LinGO Grammar Matrix (Bender et al.,
2002; Bender and Flickinger, 2005). There are sev-
eral (related) obstacles to making effective use of
test suites in this scenario: (1) The Matrix core
grammar isn’t itself a grammar, and therefore can’t
parse any strings. (2) There is no single language
modeled by the cross-linguistic resource from which
to draw test strings. (3) The space of possible gram-
mars (alternatively, language types) modeled by the
resource is enormous, well beyond the scope of what
can be thoroughly explored.
We present a methodology for the validation and
regression testing of the Grammar Matrix that ad-
dresses these obstacles, developing the ideas origi-
nally proposed in (Poulson, 2006). In its broad out-
lines, our methodology looks like this:
</bodyText>
<listItem confidence="0.997563666666667">
• Define an abstract vocabulary to be used for test
suite purposes.
• Define an initial small set of string-semantics
pairs.
• Construct a large set of variations on the string-
semantics pairs.
</listItem>
<page confidence="0.981092">
136
</page>
<note confidence="0.8434035">
Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 136–143,
Prague, Czech Republic, June, 2007. @2007 Association for Computational Linguistics
</note>
<listItem confidence="0.836504">
• Define a set of filters that can delineate the le-
</listItem>
<bodyText confidence="0.933146">
gitimate string-semantics pairs for a particular
language type
The filters in effect constitute a parallel grammar
definition system, albeit one that creates ‘grammars’
of very limited generative capacity. As such, the out-
put of the filters cannot be taken as ground truth.
Rather, it serves as a point of comparison that al-
lows us to find discrepancies between the filters and
the Grammar Matrix which in turn can lead us to
errors in the Grammar Matrix.
</bodyText>
<sectionHeader confidence="0.997096" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999983442307692">
The Grammar Matrix is an open-source starter kit
designed to jump-start the development of broad-
coverage precision grammars, capable of both pars-
ing and generation and suitable for use in a vari-
ety of NLP applications. The Grammar Matrix is
written within the HPSG framework (Pollard and
Sag, 1994), using Minimal Recursion Semantics
(Copestake et al., 2005) for the semantic represen-
tations. The particular formalism we use is TDL
(type description language) as interpreted by the
LKB (Copestake, 2002) grammar development en-
vironment. Initial work on the Matrix (Bender et
al., 2002; Flickinger and Bender, 2003) focused on
the development of a cross-linguistic core grammar.
The core grammar provides a solid foundation for
sustained development of linguistically-motivated
yet computationally tractable grammars (e.g., (Hel-
lan and Haugereid, 2003; Kordoni and Neu, 2005)).
However, the core grammar alone cannot parse
and generate sentences: it needs to be specialized
with language-specific information such as the or-
der of daughters in its rules (e.g., head-subject or
subject-head), and it needs a lexicon. Although
word order and many other phenomena vary across
languages, there are still recurring patterns. To al-
low reuse of grammar code across languages and to
increase the size of the jump-start provided by the
Matrix, in more recent work (Bender and Flickinger,
2005; Drellishak and Bender, 2005), we have been
developing ‘libraries’ implementing realizations of
various linguistic phenomena. Through a web in-
terface, grammar developers can configure an initial
starter grammar by filling out a typological question-
naire about their language, which in turn calls a CGI
script to ‘compile’ a grammar (including language-
specific rule types, lexical entry types, rule entries,
and lexical entries) by making appropriate selections
from the libraries. These little grammars describe
very small fragments of the languages they model,
but they are not toys. Their purpose is to be good
starting points for further development.
The initial set of libraries includes: basic word or-
der of major constituents in matrix clauses (SOV et
al), optionality/obligatoriness of determiners, noun-
determiner order, NP v. PP arguments of intransitive
and transitive verbs, strategies for expressing senten-
tial negation and yes-no questions, and strategies for
constituent coordination. Even with this small set of
phenomena covered (and limiting ourselves for test-
ing purposes to maximally two coordination strate-
gies per language), we have already defined a space
of hundreds of thousands of possible grammars.1
</bodyText>
<sectionHeader confidence="0.9157085" genericHeader="method">
3 The Non-modularity of Linguistic
Phenomena
</sectionHeader>
<bodyText confidence="0.999834260869565">
In this section we discuss our findings so far about
the non-modularity of linguistic phenomena, and ar-
gue that this makes the testing of a broad sample of
grammars even more pressing.
The Grammar Matrix customization system reads
in the user’s language specification and then outputs
language-specific definitions of types (rule types,
lexical entry types and ancillary structures) that in-
herit from types defined in the crosslinguistic core
of the Matrix but add constraints appropriate for the
language at hand. Usability considerations put two
important constraints on this system: (1) The ques-
tions must be ones that are sensible to linguists, who
tend to consider phenomena one at a time. (2) The
output grammar code must be both readable and
maintainable. To achieve readable grammar code
in the output TDL, among other things, we follow
the guideline that any given constraint is stated only
once. If multiple types require the same constraint,
they should all inherit from some supertype bearing
that constraint. In addition, all constraints pertaining
to a particular type are stated in one place.
In light of the these usability considerations, we
</bodyText>
<footnote confidence="0.60530225">
&apos;If all of the choices in the customization system were in-
dependent, we would have more than 2 x 1027 grammars. In
actuality, constraints on possible combinations of choices limit
this space considerably.
</footnote>
<page confidence="0.971291">
137
</page>
<equation confidence="0.504820333333333">
comp-head-phrase := basic-head-1st-comp-phrase &amp; head-final.
subj-head-phrase := basic-head-subj-phrase &amp; head-final &amp;
[ HEAD-DTR.SYNSEM.LOCAL.CAT.VAL.COMPS &lt; &gt; ].
</equation>
<figureCaption confidence="0.999109">
Figure 1: Specialized phrase structure rule types for SOV language
</figureCaption>
<bodyText confidence="0.999946965517241">
have found that it is not possible to treat the li-
braries as black-box modules with respect to each
other. The libraries are interdependent, and the por-
tions of the script that interpret one part of the input
questionnaire frequently need to make reference to
information elicited by other parts of the question-
naire. For example, the customization system imple-
ments major constituent word order by specializing
the head-complement and head-subject rule types
provided in the core grammar. In an SOV language,
these would both be cross-classified with the type
head-final, and the head-subject rule would further
be constrained to take only complement-saturated
phrases as its head daughter. The TDL encoding of
these constraints is shown in Figure 1.
Following standard practice in HPSG, we use the
head-complement phrase not only for ordinary VPs,
but also for PPs, CPs, and auxiliary-headed VPs,
etc. Consider Polish, a free word order language that
nonetheless has prepositions. To allow complements
on either side of the head, we instantiate both head-
comp and comp-head rules, inheriting from head-
initial and head-final respectively. Yet the preposi-
tions must be barred from the head-final version lest
the grammar license postpositional phrases by mis-
take. We do this by constraining the HEAD value of
the comp-head phrase. Similarly, question particles
(such as est-ce que in French or ma in Mandarin)
are treated as complementizers: heads that select for
an S complement. Since these, too, may differ in
their word order properties from verbs (and preposi-
tions), we need information about the question par-
ticles (elicited with the rest of the information about
yes-no questions) before we have complete informa-
tion about the head-complement rule. Furthermore,
it is not simply a question of adding constraints to
existing types. Consider the case of an SOV lan-
guage with prepositions and sentence-initial ques-
tion particles. This language would need a head-
initial head-comp rule that can take only preposi-
tions and complementizers as its head. To express
the disjunction, we must use the supertype to prep
and comp. This, in turn, means that we can’t decide
what constraint to put on the head value of the head-
comp rule until we’ve considered questions as well
as the basic word order facts.
We expect to study the issue of (non-)modularity
as we add additional libraries to the resource and to
investigate whether the grammar code can be refac-
tored in such a way as to make the libraries into true
modules. We suspect it might be possible to reduce
the degree of interdependence, but not to achieve
completely independent libraries, because syntactic
phenomena are inherently interdependent. Agree-
ment in NP coordination provides an example. In
English and many other languages, coordinated NPs
are always plural and the person of the coordinated
NP is the minimal person value of the coordinands.
</bodyText>
<listItem confidence="0.999435666666667">
(1) a. A cat and a dog are/*is chasing a mouse.
b. Kim and I should handle this ourselves.
c. You and Kim should handle this yourselves.
</listItem>
<bodyText confidence="0.999849545454545">
Gender systems often display a similar hierarchy of
values, as with French coordinated NPs, where the
whole NP is feminine iff all coordinands are femi-
nine and masculine otherwise. Thus it appears that
it is not possible to define all of the necessary con-
straints on the coordination rules without having ac-
cess to information about the agreement system.
Even if we were able to make our analyses of
different linguistic phenomena completely modular,
however, we would still need to test their interaction
in the analysis of particular sentences. Any sentence
that illustrates sentential negation, a matrix yes-no
question, or coordination also necessarily illustrates
at least some aspects of word order, the presence
v. absence of determiners and case-marking adpo-
sitions, and the subcategorization of the verb that
heads the sentence. Furthermore, broad-coverage
grammars need to allow negation, questions, coor-
dination etc. all to appear in the same sentence.
Given this non-modularity, we would ideally like
to be able to validate (and do regression testing on)
the full set of grammars generable by the customiza-
</bodyText>
<page confidence="0.99902">
138
</page>
<tableCaption confidence="0.998971">
Table 1: Standardized lexicon
</tableCaption>
<figure confidence="0.51517885">
Form Description Options
determiner
nouns
intransitive, transitive verb
case-marking adpositions
negative element
coordination marks
question particle
det is optional, obligatory, impossible
subj, obj are NP or PP
preposition or postposition
adverb, prefix, suffix
word, prefix, suffix
det
n1, n2
iv, tv
p-nom, p-acc
neg
co1, co2
qpart
</figure>
<bodyText confidence="0.871276">
tion system. To approximate such thoroughness, we
instead sample from the grammar space.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="method">
4 Methodology
</sectionHeader>
<bodyText confidence="0.9999704375">
This section describes in some detail our methodol-
ogy for creating test suites on the basis of language-
type descriptions. A language type is a collection
of feature-value pairs representing a possible set
of answers to the Matrix customization question-
naire. We refer to these as language types rather
than languages, because the grammars produced by
the customization system are underspecified with re-
spect to actual languages, i.e., one and the same
starter grammar might be extended into multiple
models corresponding to multiple actual human lan-
guages. Accordingly, when we talk about the pre-
dicted (well)formedness, or (un)grammaticality, of a
candidate string, we are referring to its predicted sta-
tus with respect to a language type definition, not its
grammaticality in any particular (human) language.
</bodyText>
<subsectionHeader confidence="0.993123">
4.1 Implementation: Python and MySQL
</subsectionHeader>
<bodyText confidence="0.999991444444444">
The test suite generation system includes a MySQL
database, a collection of Python scripts that interact
with the database, and some stored SQL queries. As
the set of items we are manipulating is quite large
(and will grow as new items are added to test ad-
ditional libraries), using a database is essential for
rapid retrieval of relevant items. Furthermore, the
database facilitates the separation of procedural and
declarative knowledge in the definition of the filters.
</bodyText>
<subsectionHeader confidence="0.994627">
4.2 Abstract vocabulary for abstract strings
</subsectionHeader>
<bodyText confidence="0.9999736">
A grammar needs not just syntactic constructions
and lexical types, but also an actual lexicon. Since
we are working at the level of language types, we
are free to define this lexicon in whatever way is
most convenient. Much of the idiosyncrasy in lan-
guage resides in the lexicon, both in the form of mor-
phemes and in the particular grammatical and collo-
cational constraints associated with them. Of these
three, only the grammatical constraints are manip-
ulated in any interesting way within the Grammar
Matrix customization system. Therefore, for the test
suite, we define all of the language types to draw the
forms of their lexical items from a shared, standard-
ized vocabulary. Table 1 illustrates the vocabulary
along with the options that are currently available
for varying the grammatical constraints on the lex-
ical entries. Using the same word forms for each
grammar contributes substantially to building a sin-
gle resource that can be adapted for the testing of
each language type.
</bodyText>
<subsectionHeader confidence="0.999594">
4.3 Constructing master item set
</subsectionHeader>
<bodyText confidence="0.999987666666667">
We use string to refer to a sequence of words to
be input to a grammar and result as the (expected)
semantic representation. An item is a particular
pair of string and result. Among strings, we have
seed strings provided by the Matrix developers to
seed the test suite, and constructed strings derived
from those seed strings. The constructor function
is the algorithm for deriving new strings from the
seed strings. Seed strings are arranged into seman-
tic equivalence classes, from which one representa-
tive is designated the harvester string. We parse the
harvester string with some appropriate grammar (de-
rived from the Matrix customization system) to ex-
tract the semantic representation (result) to be paired
with each member of the equivalence class.
The seed strings, when looked at as bags of words,
should cover all possible realizations of the phe-
nomenon treated by the library. For example, the
negation library allows both inflectional and adver-
bial negation, as well as negation expressed through
both inflection and an adverb together. To illustrate
</bodyText>
<page confidence="0.995439">
139
</page>
<bodyText confidence="0.9400606">
handle, for example, the addition of all possible case
suffixes to each noun in the sentence.
negation of transitive sentences (allowing for lan-
guages with and without determiners2), we require
the seed strings in (2):
</bodyText>
<table confidence="0.734891714285714">
4.4 Filters
(2) Semtag: neg1 Semtag: neg2
n1 n2 neg tv det n1 det n2 neg tv
n1 n2 neg-tv det n1 det n2 neg-tv
n1 n2 tv-neg det n1 det n2 tv-neg
n1 n2 neg neg-tv det n1 det n2 neg neg-tv
n1 n2 neg tv-neg det n1 det n2 neg tv-neg
</table>
<bodyText confidence="0.999456352941176">
Sentential negation has the same semantic reflex
across all of its realizations, but the presence v. ab-
sence of overt determiners does have a semantic ef-
fect. Accordingly, the seed strings shown in (2) can
be grouped into two semantic equivalence classes,
shown as the first and second columns in the table,
and associated with the semantic tags ‘neg1’ and
‘neg2’, respectively. The two strings in the first row
could be designated as the harvester strings, associ-
ated with a grammar for an SOV language with op-
tional determiners preceding the noun and sentential
negation expressed as a pre-head modifier of V.
We use the LKB in conjunction with [incr tsdbo)]
to parse the harvester strings from all of the equiva-
lence classes with the appropriate grammars. Then
the seed strings and the parsing results from the har-
vester strings, as well as their semantic tags, are
stored and linked in our relational database. We use
the constructor function to create new strings from
these seed strings. This produces the master item set
that provides the basis for the test suites.
Currently, we have only one constructor function
(‘permute’) which takes in a seed string and returns
all unique permutations of the morphemes in that
seed string.3 This constructor function is effective
in producing test items that cover the range of word
order variations currently permitted by the Grammar
Matrix customization system. Currently, most of the
other kinds of variation countenanced (e.g., adver-
bial v. inflectional negation or presence v. absence
of determiners) is handled through the initial seed
string construction. As the range of phenomena han-
dled by the customization system expands, we will
develop more sophisticated constructor functions to
</bodyText>
<footnote confidence="0.64739725">
2We require additional seed strings to account for languages
with and without case-marking adpositions
3‘permute’ strips off any affixes, permutes the stems, and
then attaches the affixes to the stems in all possible ways.
</footnote>
<bodyText confidence="0.999858833333333">
The master item set provides us with an inventory
from which we can find positive (grammatical) ex-
amples for any language type generated by the sys-
tem as well as interesting negative examples for any
language type. To do so, we filter the master item
set, in two steps.
</bodyText>
<subsectionHeader confidence="0.77491">
4.4.1 Universal Filters
</subsectionHeader>
<bodyText confidence="0.97696025">
The first step is the application of ‘universal’ fil-
ters, which mark any item known to be ungrammat-
ical across all language types currently produced by
the system. For example, the word order library does
not currently provide an analysis of radically non-
configurational languages with discontinuous NPs
(e.g., Warlpiri (Hale, 1981)). Accordingly, (3) will
be ungrammatical across all language types:
(3) det det n1 n2 tv
The universal filter definitions (provided by the
developers) each comprise one or more regular ex-
pressions, a filter type that specifies how the regular
expressions are to be applied, and a list of seman-
tic tags specifying which equivalence classes they
apply to. For example, the filter that would catch
example (3) above is defined as in (4):
</bodyText>
<listItem confidence="0.943802">
(4) Filter Type: reject-unless-match
</listItem>
<equation confidence="0.937529">
Regexp: (det (n1|n2).*det (n1|n2))|
(det (n1|n2).*(n1|n2) det)|
((n1|n2) det.*det (n1|n2))|
((n1|n2) det.*(n1|n2) det)
</equation>
<bodyText confidence="0.969751625">
Sem-class: [semantic classes for all transitive
sentences with two determiners.]
We apply each filter to every item in the database.
For each filter whose semantic-class value includes
the semantic class of the item at hand, we store the
result (pass or fail) of the filter on that item. We can
then query the database to produce a list of all of the
potentially well-formed items.
</bodyText>
<subsectionHeader confidence="0.67482">
4.4.2 Specific Filters
</subsectionHeader>
<bodyText confidence="0.9999812">
The next step is to run the filters that find the
grammatical examples for a particular language
type. In order to facilitate sampling of the entire
language space, we define these filters to be sensi-
tive not to complete language type definitions, but
</bodyText>
<page confidence="0.993065">
140
</page>
<bodyText confidence="0.999602461538461">
rather to particular features (or small sets of fea-
tures) of a language type. Thus in addition to the
filter type, regular expression, and semantic class
fields, the language-specific filters also encode par-
tial descriptions of the language types to which they
apply, in the form of feature-value declarations. For
example, the filter in (5) plays a role in selecting
the correct form of negated sentences for language
types with both inflectional and adverbial negation
in complementary distribution (like English n’t and
sentential not). The first regular expression checks
for neg surrounded by white space (i.e., the negative
adverb) and the second for the negative affixes.
</bodyText>
<table confidence="0.6465155">
(5) Filter Type: reject-if-both-match
Regexp1: (\s|ˆ)neg(\s|$)
Regexp2: -neg|neg-
Sem-class: [sem. classes for all neg. sent.]
Lg-feat: and(infl neg:on,adv neg:on,
multineg:comp)
</table>
<bodyText confidence="0.999795785714286">
This filter uses a conjunctive language feature spec-
ification (three feature-value pairs that must apply),
but disjunctions are also possible. These specifica-
tions are converted to disjunctive normal form be-
fore further processing.
As with the universal filters, the results of the spe-
cific filters are stored in the database. We process
each item that passed all of the universal filters with
each specific filter. Whenever a filter’s semantic-
class value matches the semantic-class of the item
at hand, we store the value assigned by the filter
(pass or fail). We also store the feature-value pairs
required by each filter, so that we can look up the
relevant filters for a language-type definition.
</bodyText>
<subsectionHeader confidence="0.976933">
4.4.3 Recursive Linguistic Phenomena
</subsectionHeader>
<bodyText confidence="0.999988466666667">
Making the filters relative to particular semantic
classes allows us to use information about the lexi-
cal items in the sentences in the definition of the fil-
ters. This makes it easier to write regular-expression
based filters that can work across many different
complete language types. Complications arise, how-
ever, in examples illustrating recursive phenomena
To handle such phenomena with our finite-state sys-
tem, we do multiple passes with the filters. All items
with coordination are first processed with the co-
ordination filters, and then rewritten to replace any
well-formed coordinations with single constituents.
These rewritten strings are then processed with the
rest of the filters, and we store the results as the re-
sults for those filters on the original strings.
</bodyText>
<subsectionHeader confidence="0.99752">
4.5 Language types
</subsectionHeader>
<bodyText confidence="0.999984235294118">
The final kind of information we store in the
database is definitions of language types. Even
though our system allows us to create test suites for
new language types on demand, we still store the
language-type definitions of language types we have
tested, for future regression testing purposes. When
a language type is read in, the list of feature-value
pairs defining it is compared to the list of feature-
groups declared by the filters. For each group of
feature-value pairs present in the language-type def-
inition, we find all of the filters that use that group.
We then query the database for all items that pass
the filters relevant to the language type. This list
of items represents all those in the master item set
predicted to be well-formed for this language type.
From the complement of this set, we also take a ran-
dom selection of items to test for overgeneration.
</bodyText>
<subsectionHeader confidence="0.999789">
4.6 Validation of grammars
</subsectionHeader>
<bodyText confidence="0.999949875">
Once we have created the test suite for a partic-
ular language type, the developer runs the Matrix
customization system to get a starter grammar for
the same language type. The test suite is loaded
into [incr tsdbo ] and processed with the grammar.
[incr tsdbo ] allows the developer to compare the
grammar’s output with the test suite at varying lev-
els of detail: Do all and only the items predicted to
be well-formed parse? Do they get the same number
of readings as predicted? Do they get the semantic
representations predicted? A discrepancy at any of
these levels points to an error in either the Grammar
Matrix or the test suite generation system. The de-
veloper can query the database to find which filters
passed or failed a particular example as well as to
discover the provenance of the example and which
phenomena it is meant to test.
This methodology provides the ability to gener-
ate test suites for any arbitrary language type on de-
mand. Although this appears to eliminate the need to
store the test suites we do, in fact, store information
about previous test suites. This allows us to track the
evolution of the Grammar Matrix in relation to those
particular language types over time.
</bodyText>
<page confidence="0.995659">
141
</page>
<subsectionHeader confidence="0.967471">
4.7 Investment and Return
</subsectionHeader>
<bodyText confidence="0.999990551724138">
The input required from the developer in order to test
any new library is as follows: (1) Seed strings illus-
trating the range of expressions handled by the new
library, organized into equivalence classes. (2) Des-
ignated harvester strings for each equivalence class
and a grammar or grammars that can parse them to
get the target semantic representation. (3) Universal
filters specific to the phenomenon and seed strings.
(4) Specific filters picking out the right items for
each language type. (5) Analysis of discrepancies
between the test suite and the generated grammars.
This is a substantial investment on the part of the de-
veloper but we believe the investment is worth it for
the return of being able to validate a library addition
and test for any loss of coverage going forward.
Arnold et al. (1994) note that writing grammars
to generate test suites is impractical if the test suite
generating grammars aren’t substantially simpler to
write than the ‘actual’ grammars being tested. Even
though this system requires some effort to maintain,
we believe the methodology remains practical for
two reasons. First, the input required from the de-
veloper enumerated above is closely related to the
knowledge discovered in the course of building the
libraries in the first place. Second, the fact that the
filters are sensitive to only particular features of lan-
guage types means that a relatively small number of
filters can create test suites for a very large number
of language types.
</bodyText>
<sectionHeader confidence="0.999984" genericHeader="method">
5 Related Work
</sectionHeader>
<bodyText confidence="0.9999883125">
Kinyon and Rambow (2003) present an approach to
generating test suites on the basis of descriptions
of languages. The language descriptions are Meta-
Grammar (MG) hierarchies. Their approach appears
to be more flexible than the one presented here in
some ways, and more constrained in others. It does
not need any input strings, but rather produces test
items from the language description. In addition,
it annotates the output in multiple ways, including
phrase structure, dependency structure, and LFG F-
structure. On the other hand, there is no apparent
provision for creating negative (ungrammatical) test
data and it is does not appear possible to compose
new MG descriptions on the fly. Furthermore, the
focus of the MG test suite work appears to be the
generation of test suites for other grammar develop-
ment projects, but the MGs themselves are crosslin-
guistic resources in need of validation and testing.
An interesting area for future work would be the
comparison between the test suites generated by the
system described here and the MG test suites.
The key to the test-suite development process pro-
posed here is to leverage the work already being
done by the Matrix developers into a largely auto-
mated process for creating test-suite items. The in-
formation required from the developers is essentially
a structured and systematic version of the knowledge
that is required for the creation of libraries in the first
place. This basic approach, is also the basis for the
approach taken in (Br¨oker, 2000); the specific forms
of knowledge leveraged, and the test-suite develop-
ment strategies used, however, are quite different.
</bodyText>
<sectionHeader confidence="0.999901" genericHeader="method">
6 Future Work
</sectionHeader>
<bodyText confidence="0.999990739130435">
The addition of the next library to the Grammar Ma-
trix will provide us with an opportunity to try to
quantify the effect of this methodology. With the
Grammar Matrix and the filters stabilized, the vali-
dation of a new library can be carefully tracked. We
can try to quantify the number of errors obtained and
the source of the errors, e.g., library or filters.
In addition to this kind of quantification and error
analysis as a means of validating this methodology,
we also intend to undertake a comparison of the test
suites created from our database to hand built cre-
ated for Matrix-derived grammars by students in the
multilingual grammar engineering course at the Uni-
versity of Washington.4 Students in this class each
develop grammars for a different language, and cre-
ate test suites of positive and negative examples as
part of their development process. We plan to use
the lexical types in the grammars to define a map-
ping from the surface lexical items used in the test
suites to our abstract vocabulary. We can then com-
pare the hand built and autogenerated test suites in
order to gauge the thoroughness of the system pre-
sented here.
</bodyText>
<sectionHeader confidence="0.999212" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.932888">
The methodology outlined in this paper addresses
the three obstacles noted in the introduction: Al-
</bodyText>
<footnote confidence="0.845322">
4http://courses.washington.edu/ling567
</footnote>
<page confidence="0.995064">
142
</page>
<bodyText confidence="0.999974478260869">
though the Grammar Matrix core itself isn’t a gram-
mar (1), we test it by deriving grammars from it.
Since we are testing the derived grammars, we are
simultaneously testing both the Matrix core gram-
mar, the libraries, and the customization script. Al-
though there is no single language being modeled
from which to draw strings (2), we can nonethe-
less find a relevant set of strings and associate
these strings with annotations of expected well-
formedness. The lexical formatives of the strings
are drawn from a standardized set of abstract forms.
The well-formedness predictions are made on the
basis of the system of filters. The system of filters
doesn’t represent ground truth, but rather a second
pathway to the judgments in addition to the direct
use of the Matrix-derived starter grammars. These
pathways are independent enough that the one can
serve as an error check on the other. The space of
possible language types remains too large for thor-
ough testing (3). However, since our system allows
for the efficient derivation of a test suite for any arbi-
trary language type, it is inexpensive to sample that
language-type space in many different ways.
</bodyText>
<sectionHeader confidence="0.999077" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.997672">
This work has been supported by NSF grant BCS-
0644097.
</bodyText>
<sectionHeader confidence="0.99948" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999876338461538">
Doug Arnold, Martin Rondell, and Frederik Fouvry.
1994. Design and implementation of test suite tools.
Technical Report LRE 62-089 D-WP5, University of
Essex, UK.
Emily M. Bender and Dan Flickinger. 2005. Rapid pro-
totyping of scalable grammars: Towards modularity in
extensions to a language-independent core. In Proc.
IJCNLP-05 (Posters/Demos).
Emily M. Bender, Dan Flickinger, and Stephan Oepen.
2002. The grammar matrix: An open-source starter-
kit for the rapid development of cross-linguistically
consistent broad-coverage precision grammars. In
Proc. the Workshop on Grammar Engineering and
Evaluation COLING 2002, pages 8–14.
Norbert Br¨oker. 2000. The use of instrumentation in
grammar engineering. In Proc. COLING 2000, pages
118–124.
Miriam Butt and Tracy Holloway King. 2003. Gram-
mar writing, testing, and evaluation. In Handbookfor
Language Engineers, pages 129–179. CSLI.
Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A.
Sag. 2005. Minimal recursion semantics: An intro-
duction. Research on Language &amp; Computation, 3(2–
3):281–332.
Ann Copestake. 2002. Implementing Typed Feature
Structure Grammars. CSLI.
Scott Drellishak and Emily M. Bender. 2005. A coordi-
nation module for a crosslinguistic grammar resource.
In Stefan M¨uller, editor, The Proc. HPSG 2005, pages
108–128. CSLI.
Dan Flickinger and Emily M. Bender. 2003. Compo-
sitional semantics in a multilingual grammar resource.
In Proc. the Workshop on Ideas and Strategiesfor Mul-
tilingual Grammar Development, ESSLLI2003, pages
33–42.
Kenneth Hale. 1981. On the position of Warlpiri in the
typology of the base. Distributed by Indiana Univer-
sity Linguistics Club, Bloomington.
Lars Hellan and Petter Haugereid. 2003. NorSource: An
exercise in Matrix grammar-building design. In Proc.
the Workshop on Ideas and Strategies for Multilingual
Grammar Development, ESSLLI 2003, pages 41–48.
Alexandra Kinyon and Owen Rambow. 2003. The meta-
grammar: A cross-framework and cross-language test-
suite generation tool. In Proc. 4th International Work-
shop on Linguistically Interpreted Corpora.
Valia Kordoni and Julia Neu. 2005. Deep analysis
of Modern Greek. In Keh-Yih Su, Jun’ichi Tsujii,
and Jong-Hyeok Lee, editors, Lecture Notes in Com-
puter Science, volume 3248, pages 674–683. Springer-
Verlag.
Stephan Oepen and Daniel P. Flickinger. 1998. Towards
systematic grammar profiling. Test suite technology
ten years after. Journal of Computer Speech and Lan-
guage, 12 (4) (Special Issue on Evaluation):411 – 436.
Stephan Oepen. 2002. Competence and Performance
Profiling for Constraint-based Grammars: A New
Methodology, Toolkit, and Applications. Ph.D. thesis,
Universit¨at des Saarlandes.
Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase
Structure Grammar. The University of Chicago Press.
Laurie Poulson. 2006. Evaluating a cross-linguistic
grammar model: Methodology and gold-standard re-
source development. Master’s thesis, University of
Washington.
</reference>
<page confidence="0.99913">
143
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.983116">
<title confidence="0.999318">Validation and Regression Testing for a Cross-linguisic Grammar Resource</title>
<author confidence="0.999642">Emily M Bender</author>
<author confidence="0.999642">Laurie Poulson</author>
<author confidence="0.999642">Scott Drellishak</author>
<author confidence="0.999642">Chris</author>
<affiliation confidence="0.997748">University of Department of</affiliation>
<address confidence="0.997991">Seattle WA 98195-4340</address>
<abstract confidence="0.999627925925926">We present a validation methodology for a cross-linguistic grammar resource which produces output in the form of small grammars based on elicited typological descriptions. Evaluating the resource entails sampling from a very large space of language types, the type and range of which preclude the use of standard test suites development techniques. We produce a database from which gold standard test suites for these grammars can be generated on demand, including well-formed strings paired with all of their valid semantic representations as well as a sample of ill-formed strings. These string-semantics pairs are selected from a set of candidates by a system of regularexpression based filters. The filters amount to an alternative grammar building system, whose generative capacity is limited compared to the actual grammars. We perform error analysis of the discrepancies between the test suites and grammars for a range of language types, and update both systems appropriately. The resulting resource serves as a point of comparison for regression testing in future development.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Doug Arnold</author>
<author>Martin Rondell</author>
<author>Frederik Fouvry</author>
</authors>
<title>Design and implementation of test suite tools.</title>
<date>1994</date>
<tech>Technical Report LRE 62-089 D-WP5,</tech>
<institution>University of Essex, UK.</institution>
<contexts>
<context position="25798" citStr="Arnold et al. (1994)" startWordPosition="4113" endWordPosition="4116">valence classes. (2) Designated harvester strings for each equivalence class and a grammar or grammars that can parse them to get the target semantic representation. (3) Universal filters specific to the phenomenon and seed strings. (4) Specific filters picking out the right items for each language type. (5) Analysis of discrepancies between the test suite and the generated grammars. This is a substantial investment on the part of the developer but we believe the investment is worth it for the return of being able to validate a library addition and test for any loss of coverage going forward. Arnold et al. (1994) note that writing grammars to generate test suites is impractical if the test suite generating grammars aren’t substantially simpler to write than the ‘actual’ grammars being tested. Even though this system requires some effort to maintain, we believe the methodology remains practical for two reasons. First, the input required from the developer enumerated above is closely related to the knowledge discovered in the course of building the libraries in the first place. Second, the fact that the filters are sensitive to only particular features of language types means that a relatively small num</context>
</contexts>
<marker>Arnold, Rondell, Fouvry, 1994</marker>
<rawString>Doug Arnold, Martin Rondell, and Frederik Fouvry. 1994. Design and implementation of test suite tools. Technical Report LRE 62-089 D-WP5, University of Essex, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
</authors>
<title>Rapid prototyping of scalable grammars: Towards modularity in extensions to a language-independent core.</title>
<date>2005</date>
<booktitle>In Proc. IJCNLP-05 (Posters/Demos).</booktitle>
<contexts>
<context position="2257" citStr="Bender and Flickinger, 2005" startWordPosition="337" endWordPosition="340">a treated by the grammar as well as representative examples taken from texts from the target domain. In combination with test suite management software such as [incr tsdbo)] (Oepen, 2002), they are used for validation and regression testing of precision (deep linguistic) grammars as well as the exploration of potential changes to the grammar. In this paper, we consider what happens when the precision grammar resource being developed isn’t a grammar of a particular language, but rather a crosslinguistic grammar resource. In particular, we consider the LinGO Grammar Matrix (Bender et al., 2002; Bender and Flickinger, 2005). There are several (related) obstacles to making effective use of test suites in this scenario: (1) The Matrix core grammar isn’t itself a grammar, and therefore can’t parse any strings. (2) There is no single language modeled by the cross-linguistic resource from which to draw test strings. (3) The space of possible grammars (alternatively, language types) modeled by the resource is enormous, well beyond the scope of what can be thoroughly explored. We present a methodology for the validation and regression testing of the Grammar Matrix that addresses these obstacles, developing the ideas or</context>
<context position="5216" citStr="Bender and Flickinger, 2005" startWordPosition="806" endWordPosition="809">nguistically-motivated yet computationally tractable grammars (e.g., (Hellan and Haugereid, 2003; Kordoni and Neu, 2005)). However, the core grammar alone cannot parse and generate sentences: it needs to be specialized with language-specific information such as the order of daughters in its rules (e.g., head-subject or subject-head), and it needs a lexicon. Although word order and many other phenomena vary across languages, there are still recurring patterns. To allow reuse of grammar code across languages and to increase the size of the jump-start provided by the Matrix, in more recent work (Bender and Flickinger, 2005; Drellishak and Bender, 2005), we have been developing ‘libraries’ implementing realizations of various linguistic phenomena. Through a web interface, grammar developers can configure an initial starter grammar by filling out a typological questionnaire about their language, which in turn calls a CGI script to ‘compile’ a grammar (including languagespecific rule types, lexical entry types, rule entries, and lexical entries) by making appropriate selections from the libraries. These little grammars describe very small fragments of the languages they model, but they are not toys. Their purpose </context>
</contexts>
<marker>Bender, Flickinger, 2005</marker>
<rawString>Emily M. Bender and Dan Flickinger. 2005. Rapid prototyping of scalable grammars: Towards modularity in extensions to a language-independent core. In Proc. IJCNLP-05 (Posters/Demos).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>The grammar matrix: An open-source starterkit for the rapid development of cross-linguistically consistent broad-coverage precision grammars.</title>
<date>2002</date>
<booktitle>In Proc. the Workshop on Grammar Engineering and Evaluation COLING</booktitle>
<pages>8--14</pages>
<contexts>
<context position="2227" citStr="Bender et al., 2002" startWordPosition="333" endWordPosition="336"> grammatical phenomena treated by the grammar as well as representative examples taken from texts from the target domain. In combination with test suite management software such as [incr tsdbo)] (Oepen, 2002), they are used for validation and regression testing of precision (deep linguistic) grammars as well as the exploration of potential changes to the grammar. In this paper, we consider what happens when the precision grammar resource being developed isn’t a grammar of a particular language, but rather a crosslinguistic grammar resource. In particular, we consider the LinGO Grammar Matrix (Bender et al., 2002; Bender and Flickinger, 2005). There are several (related) obstacles to making effective use of test suites in this scenario: (1) The Matrix core grammar isn’t itself a grammar, and therefore can’t parse any strings. (2) There is no single language modeled by the cross-linguistic resource from which to draw test strings. (3) The space of possible grammars (alternatively, language types) modeled by the resource is enormous, well beyond the scope of what can be thoroughly explored. We present a methodology for the validation and regression testing of the Grammar Matrix that addresses these obst</context>
<context position="4419" citStr="Bender et al., 2002" startWordPosition="686" endWordPosition="689">n the Grammar Matrix. 2 Background The Grammar Matrix is an open-source starter kit designed to jump-start the development of broadcoverage precision grammars, capable of both parsing and generation and suitable for use in a variety of NLP applications. The Grammar Matrix is written within the HPSG framework (Pollard and Sag, 1994), using Minimal Recursion Semantics (Copestake et al., 2005) for the semantic representations. The particular formalism we use is TDL (type description language) as interpreted by the LKB (Copestake, 2002) grammar development environment. Initial work on the Matrix (Bender et al., 2002; Flickinger and Bender, 2003) focused on the development of a cross-linguistic core grammar. The core grammar provides a solid foundation for sustained development of linguistically-motivated yet computationally tractable grammars (e.g., (Hellan and Haugereid, 2003; Kordoni and Neu, 2005)). However, the core grammar alone cannot parse and generate sentences: it needs to be specialized with language-specific information such as the order of daughters in its rules (e.g., head-subject or subject-head), and it needs a lexicon. Although word order and many other phenomena vary across languages, th</context>
</contexts>
<marker>Bender, Flickinger, Oepen, 2002</marker>
<rawString>Emily M. Bender, Dan Flickinger, and Stephan Oepen. 2002. The grammar matrix: An open-source starterkit for the rapid development of cross-linguistically consistent broad-coverage precision grammars. In Proc. the Workshop on Grammar Engineering and Evaluation COLING 2002, pages 8–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Br¨oker</author>
</authors>
<title>The use of instrumentation in grammar engineering.</title>
<date>2000</date>
<booktitle>In Proc. COLING</booktitle>
<pages>118--124</pages>
<marker>Br¨oker, 2000</marker>
<rawString>Norbert Br¨oker. 2000. The use of instrumentation in grammar engineering. In Proc. COLING 2000, pages 118–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Butt</author>
<author>Tracy Holloway King</author>
</authors>
<title>Grammar writing, testing, and evaluation.</title>
<date>2003</date>
<booktitle>In Handbookfor Language Engineers,</booktitle>
<pages>129--179</pages>
<publisher>CSLI.</publisher>
<contexts>
<context position="1530" citStr="Butt and King, 2003" startWordPosition="224" endWordPosition="227">candidates by a system of regularexpression based filters. The filters amount to an alternative grammar building system, whose generative capacity is limited compared to the actual grammars. We perform error analysis of the discrepancies between the test suites and grammars for a range of language types, and update both systems appropriately. The resulting resource serves as a point of comparison for regression testing in future development. 1 Introduction The development and maintenance of test suites is integral to the process of writing deep linguistic grammars (Oepen and Flickinger, 1998; Butt and King, 2003). Such test suites typically contain handconstructed examples illustrating the grammatical phenomena treated by the grammar as well as representative examples taken from texts from the target domain. In combination with test suite management software such as [incr tsdbo)] (Oepen, 2002), they are used for validation and regression testing of precision (deep linguistic) grammars as well as the exploration of potential changes to the grammar. In this paper, we consider what happens when the precision grammar resource being developed isn’t a grammar of a particular language, but rather a crossling</context>
</contexts>
<marker>Butt, King, 2003</marker>
<rawString>Miriam Butt and Tracy Holloway King. 2003. Grammar writing, testing, and evaluation. In Handbookfor Language Engineers, pages 129–179. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Minimal recursion semantics: An introduction.</title>
<date>2005</date>
<journal>Research on Language &amp; Computation,</journal>
<volume>3</volume>
<issue>2</issue>
<pages>3--281</pages>
<contexts>
<context position="4193" citStr="Copestake et al., 2005" startWordPosition="651" endWordPosition="654">. As such, the output of the filters cannot be taken as ground truth. Rather, it serves as a point of comparison that allows us to find discrepancies between the filters and the Grammar Matrix which in turn can lead us to errors in the Grammar Matrix. 2 Background The Grammar Matrix is an open-source starter kit designed to jump-start the development of broadcoverage precision grammars, capable of both parsing and generation and suitable for use in a variety of NLP applications. The Grammar Matrix is written within the HPSG framework (Pollard and Sag, 1994), using Minimal Recursion Semantics (Copestake et al., 2005) for the semantic representations. The particular formalism we use is TDL (type description language) as interpreted by the LKB (Copestake, 2002) grammar development environment. Initial work on the Matrix (Bender et al., 2002; Flickinger and Bender, 2003) focused on the development of a cross-linguistic core grammar. The core grammar provides a solid foundation for sustained development of linguistically-motivated yet computationally tractable grammars (e.g., (Hellan and Haugereid, 2003; Kordoni and Neu, 2005)). However, the core grammar alone cannot parse and generate sentences: it needs to </context>
</contexts>
<marker>Copestake, Flickinger, Pollard, Sag, 2005</marker>
<rawString>Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A. Sag. 2005. Minimal recursion semantics: An introduction. Research on Language &amp; Computation, 3(2– 3):281–332.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ann Copestake</author>
</authors>
<title>Implementing Typed Feature Structure Grammars.</title>
<date>2002</date>
<publisher>CSLI.</publisher>
<contexts>
<context position="4338" citStr="Copestake, 2002" startWordPosition="675" endWordPosition="676">tween the filters and the Grammar Matrix which in turn can lead us to errors in the Grammar Matrix. 2 Background The Grammar Matrix is an open-source starter kit designed to jump-start the development of broadcoverage precision grammars, capable of both parsing and generation and suitable for use in a variety of NLP applications. The Grammar Matrix is written within the HPSG framework (Pollard and Sag, 1994), using Minimal Recursion Semantics (Copestake et al., 2005) for the semantic representations. The particular formalism we use is TDL (type description language) as interpreted by the LKB (Copestake, 2002) grammar development environment. Initial work on the Matrix (Bender et al., 2002; Flickinger and Bender, 2003) focused on the development of a cross-linguistic core grammar. The core grammar provides a solid foundation for sustained development of linguistically-motivated yet computationally tractable grammars (e.g., (Hellan and Haugereid, 2003; Kordoni and Neu, 2005)). However, the core grammar alone cannot parse and generate sentences: it needs to be specialized with language-specific information such as the order of daughters in its rules (e.g., head-subject or subject-head), and it needs </context>
</contexts>
<marker>Copestake, 2002</marker>
<rawString>Ann Copestake. 2002. Implementing Typed Feature Structure Grammars. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Drellishak</author>
<author>Emily M Bender</author>
</authors>
<title>A coordination module for a crosslinguistic grammar resource.</title>
<date>2005</date>
<booktitle>The Proc. HPSG 2005,</booktitle>
<pages>108--128</pages>
<editor>In Stefan M¨uller, editor,</editor>
<publisher>CSLI.</publisher>
<contexts>
<context position="5246" citStr="Drellishak and Bender, 2005" startWordPosition="810" endWordPosition="813">mputationally tractable grammars (e.g., (Hellan and Haugereid, 2003; Kordoni and Neu, 2005)). However, the core grammar alone cannot parse and generate sentences: it needs to be specialized with language-specific information such as the order of daughters in its rules (e.g., head-subject or subject-head), and it needs a lexicon. Although word order and many other phenomena vary across languages, there are still recurring patterns. To allow reuse of grammar code across languages and to increase the size of the jump-start provided by the Matrix, in more recent work (Bender and Flickinger, 2005; Drellishak and Bender, 2005), we have been developing ‘libraries’ implementing realizations of various linguistic phenomena. Through a web interface, grammar developers can configure an initial starter grammar by filling out a typological questionnaire about their language, which in turn calls a CGI script to ‘compile’ a grammar (including languagespecific rule types, lexical entry types, rule entries, and lexical entries) by making appropriate selections from the libraries. These little grammars describe very small fragments of the languages they model, but they are not toys. Their purpose is to be good starting points </context>
</contexts>
<marker>Drellishak, Bender, 2005</marker>
<rawString>Scott Drellishak and Emily M. Bender. 2005. A coordination module for a crosslinguistic grammar resource. In Stefan M¨uller, editor, The Proc. HPSG 2005, pages 108–128. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
<author>Emily M Bender</author>
</authors>
<title>Compositional semantics in a multilingual grammar resource.</title>
<date>2003</date>
<booktitle>In Proc. the Workshop on Ideas and Strategiesfor Multilingual Grammar Development, ESSLLI2003,</booktitle>
<pages>33--42</pages>
<contexts>
<context position="4449" citStr="Flickinger and Bender, 2003" startWordPosition="690" endWordPosition="693"> 2 Background The Grammar Matrix is an open-source starter kit designed to jump-start the development of broadcoverage precision grammars, capable of both parsing and generation and suitable for use in a variety of NLP applications. The Grammar Matrix is written within the HPSG framework (Pollard and Sag, 1994), using Minimal Recursion Semantics (Copestake et al., 2005) for the semantic representations. The particular formalism we use is TDL (type description language) as interpreted by the LKB (Copestake, 2002) grammar development environment. Initial work on the Matrix (Bender et al., 2002; Flickinger and Bender, 2003) focused on the development of a cross-linguistic core grammar. The core grammar provides a solid foundation for sustained development of linguistically-motivated yet computationally tractable grammars (e.g., (Hellan and Haugereid, 2003; Kordoni and Neu, 2005)). However, the core grammar alone cannot parse and generate sentences: it needs to be specialized with language-specific information such as the order of daughters in its rules (e.g., head-subject or subject-head), and it needs a lexicon. Although word order and many other phenomena vary across languages, there are still recurring patter</context>
</contexts>
<marker>Flickinger, Bender, 2003</marker>
<rawString>Dan Flickinger and Emily M. Bender. 2003. Compositional semantics in a multilingual grammar resource. In Proc. the Workshop on Ideas and Strategiesfor Multilingual Grammar Development, ESSLLI2003, pages 33–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Hale</author>
</authors>
<title>On the position of Warlpiri in the typology of the base. Distributed by</title>
<date>1981</date>
<institution>Indiana University Linguistics Club, Bloomington.</institution>
<contexts>
<context position="19213" citStr="Hale, 1981" startWordPosition="3038" endWordPosition="3039"> set provides us with an inventory from which we can find positive (grammatical) examples for any language type generated by the system as well as interesting negative examples for any language type. To do so, we filter the master item set, in two steps. 4.4.1 Universal Filters The first step is the application of ‘universal’ filters, which mark any item known to be ungrammatical across all language types currently produced by the system. For example, the word order library does not currently provide an analysis of radically nonconfigurational languages with discontinuous NPs (e.g., Warlpiri (Hale, 1981)). Accordingly, (3) will be ungrammatical across all language types: (3) det det n1 n2 tv The universal filter definitions (provided by the developers) each comprise one or more regular expressions, a filter type that specifies how the regular expressions are to be applied, and a list of semantic tags specifying which equivalence classes they apply to. For example, the filter that would catch example (3) above is defined as in (4): (4) Filter Type: reject-unless-match Regexp: (det (n1|n2).*det (n1|n2))| (det (n1|n2).*(n1|n2) det)| ((n1|n2) det.*det (n1|n2))| ((n1|n2) det.*(n1|n2) det) Sem-clas</context>
</contexts>
<marker>Hale, 1981</marker>
<rawString>Kenneth Hale. 1981. On the position of Warlpiri in the typology of the base. Distributed by Indiana University Linguistics Club, Bloomington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lars Hellan</author>
<author>Petter Haugereid</author>
</authors>
<title>NorSource: An exercise in Matrix grammar-building design.</title>
<date>2003</date>
<booktitle>In Proc. the Workshop on Ideas and Strategies for Multilingual Grammar Development, ESSLLI</booktitle>
<pages>41--48</pages>
<contexts>
<context position="4685" citStr="Hellan and Haugereid, 2003" startWordPosition="720" endWordPosition="724">rammar Matrix is written within the HPSG framework (Pollard and Sag, 1994), using Minimal Recursion Semantics (Copestake et al., 2005) for the semantic representations. The particular formalism we use is TDL (type description language) as interpreted by the LKB (Copestake, 2002) grammar development environment. Initial work on the Matrix (Bender et al., 2002; Flickinger and Bender, 2003) focused on the development of a cross-linguistic core grammar. The core grammar provides a solid foundation for sustained development of linguistically-motivated yet computationally tractable grammars (e.g., (Hellan and Haugereid, 2003; Kordoni and Neu, 2005)). However, the core grammar alone cannot parse and generate sentences: it needs to be specialized with language-specific information such as the order of daughters in its rules (e.g., head-subject or subject-head), and it needs a lexicon. Although word order and many other phenomena vary across languages, there are still recurring patterns. To allow reuse of grammar code across languages and to increase the size of the jump-start provided by the Matrix, in more recent work (Bender and Flickinger, 2005; Drellishak and Bender, 2005), we have been developing ‘libraries’ i</context>
</contexts>
<marker>Hellan, Haugereid, 2003</marker>
<rawString>Lars Hellan and Petter Haugereid. 2003. NorSource: An exercise in Matrix grammar-building design. In Proc. the Workshop on Ideas and Strategies for Multilingual Grammar Development, ESSLLI 2003, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandra Kinyon</author>
<author>Owen Rambow</author>
</authors>
<title>The metagrammar: A cross-framework and cross-language testsuite generation tool.</title>
<date>2003</date>
<booktitle>In Proc. 4th International Workshop on Linguistically Interpreted Corpora.</booktitle>
<contexts>
<context position="26518" citStr="Kinyon and Rambow (2003)" startWordPosition="4229" endWordPosition="4232">grammars aren’t substantially simpler to write than the ‘actual’ grammars being tested. Even though this system requires some effort to maintain, we believe the methodology remains practical for two reasons. First, the input required from the developer enumerated above is closely related to the knowledge discovered in the course of building the libraries in the first place. Second, the fact that the filters are sensitive to only particular features of language types means that a relatively small number of filters can create test suites for a very large number of language types. 5 Related Work Kinyon and Rambow (2003) present an approach to generating test suites on the basis of descriptions of languages. The language descriptions are MetaGrammar (MG) hierarchies. Their approach appears to be more flexible than the one presented here in some ways, and more constrained in others. It does not need any input strings, but rather produces test items from the language description. In addition, it annotates the output in multiple ways, including phrase structure, dependency structure, and LFG Fstructure. On the other hand, there is no apparent provision for creating negative (ungrammatical) test data and it is do</context>
</contexts>
<marker>Kinyon, Rambow, 2003</marker>
<rawString>Alexandra Kinyon and Owen Rambow. 2003. The metagrammar: A cross-framework and cross-language testsuite generation tool. In Proc. 4th International Workshop on Linguistically Interpreted Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Valia Kordoni</author>
<author>Julia Neu</author>
</authors>
<title>Deep analysis of Modern Greek.</title>
<date>2005</date>
<booktitle>Lecture Notes in Computer Science,</booktitle>
<volume>3248</volume>
<pages>674--683</pages>
<editor>In Keh-Yih Su, Jun’ichi Tsujii, and Jong-Hyeok Lee, editors,</editor>
<publisher>SpringerVerlag.</publisher>
<contexts>
<context position="4709" citStr="Kordoni and Neu, 2005" startWordPosition="725" endWordPosition="728">hin the HPSG framework (Pollard and Sag, 1994), using Minimal Recursion Semantics (Copestake et al., 2005) for the semantic representations. The particular formalism we use is TDL (type description language) as interpreted by the LKB (Copestake, 2002) grammar development environment. Initial work on the Matrix (Bender et al., 2002; Flickinger and Bender, 2003) focused on the development of a cross-linguistic core grammar. The core grammar provides a solid foundation for sustained development of linguistically-motivated yet computationally tractable grammars (e.g., (Hellan and Haugereid, 2003; Kordoni and Neu, 2005)). However, the core grammar alone cannot parse and generate sentences: it needs to be specialized with language-specific information such as the order of daughters in its rules (e.g., head-subject or subject-head), and it needs a lexicon. Although word order and many other phenomena vary across languages, there are still recurring patterns. To allow reuse of grammar code across languages and to increase the size of the jump-start provided by the Matrix, in more recent work (Bender and Flickinger, 2005; Drellishak and Bender, 2005), we have been developing ‘libraries’ implementing realizations</context>
</contexts>
<marker>Kordoni, Neu, 2005</marker>
<rawString>Valia Kordoni and Julia Neu. 2005. Deep analysis of Modern Greek. In Keh-Yih Su, Jun’ichi Tsujii, and Jong-Hyeok Lee, editors, Lecture Notes in Computer Science, volume 3248, pages 674–683. SpringerVerlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Daniel P Flickinger</author>
</authors>
<title>Towards systematic grammar profiling. Test suite technology ten years after.</title>
<date>1998</date>
<journal>Journal of Computer Speech and Language,</journal>
<booktitle>(Special Issue on Evaluation):411 –</booktitle>
<volume>12</volume>
<issue>4</issue>
<pages>436</pages>
<contexts>
<context position="1508" citStr="Oepen and Flickinger, 1998" startWordPosition="220" endWordPosition="223"> are selected from a set of candidates by a system of regularexpression based filters. The filters amount to an alternative grammar building system, whose generative capacity is limited compared to the actual grammars. We perform error analysis of the discrepancies between the test suites and grammars for a range of language types, and update both systems appropriately. The resulting resource serves as a point of comparison for regression testing in future development. 1 Introduction The development and maintenance of test suites is integral to the process of writing deep linguistic grammars (Oepen and Flickinger, 1998; Butt and King, 2003). Such test suites typically contain handconstructed examples illustrating the grammatical phenomena treated by the grammar as well as representative examples taken from texts from the target domain. In combination with test suite management software such as [incr tsdbo)] (Oepen, 2002), they are used for validation and regression testing of precision (deep linguistic) grammars as well as the exploration of potential changes to the grammar. In this paper, we consider what happens when the precision grammar resource being developed isn’t a grammar of a particular language, </context>
</contexts>
<marker>Oepen, Flickinger, 1998</marker>
<rawString>Stephan Oepen and Daniel P. Flickinger. 1998. Towards systematic grammar profiling. Test suite technology ten years after. Journal of Computer Speech and Language, 12 (4) (Special Issue on Evaluation):411 – 436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
</authors>
<title>Competence and Performance Profiling for Constraint-based Grammars: A New Methodology, Toolkit, and Applications.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit¨at des Saarlandes.</institution>
<contexts>
<context position="1816" citStr="Oepen, 2002" startWordPosition="270" endWordPosition="271">uage types, and update both systems appropriately. The resulting resource serves as a point of comparison for regression testing in future development. 1 Introduction The development and maintenance of test suites is integral to the process of writing deep linguistic grammars (Oepen and Flickinger, 1998; Butt and King, 2003). Such test suites typically contain handconstructed examples illustrating the grammatical phenomena treated by the grammar as well as representative examples taken from texts from the target domain. In combination with test suite management software such as [incr tsdbo)] (Oepen, 2002), they are used for validation and regression testing of precision (deep linguistic) grammars as well as the exploration of potential changes to the grammar. In this paper, we consider what happens when the precision grammar resource being developed isn’t a grammar of a particular language, but rather a crosslinguistic grammar resource. In particular, we consider the LinGO Grammar Matrix (Bender et al., 2002; Bender and Flickinger, 2005). There are several (related) obstacles to making effective use of test suites in this scenario: (1) The Matrix core grammar isn’t itself a grammar, and theref</context>
</contexts>
<marker>Oepen, 2002</marker>
<rawString>Stephan Oepen. 2002. Competence and Performance Profiling for Constraint-based Grammars: A New Methodology, Toolkit, and Applications. Ph.D. thesis, Universit¨at des Saarlandes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
<author>Ivan A Sag</author>
</authors>
<title>Head-Driven Phrase Structure Grammar.</title>
<date>1994</date>
<publisher>The University of Chicago Press.</publisher>
<contexts>
<context position="4133" citStr="Pollard and Sag, 1994" startWordPosition="643" endWordPosition="646">that creates ‘grammars’ of very limited generative capacity. As such, the output of the filters cannot be taken as ground truth. Rather, it serves as a point of comparison that allows us to find discrepancies between the filters and the Grammar Matrix which in turn can lead us to errors in the Grammar Matrix. 2 Background The Grammar Matrix is an open-source starter kit designed to jump-start the development of broadcoverage precision grammars, capable of both parsing and generation and suitable for use in a variety of NLP applications. The Grammar Matrix is written within the HPSG framework (Pollard and Sag, 1994), using Minimal Recursion Semantics (Copestake et al., 2005) for the semantic representations. The particular formalism we use is TDL (type description language) as interpreted by the LKB (Copestake, 2002) grammar development environment. Initial work on the Matrix (Bender et al., 2002; Flickinger and Bender, 2003) focused on the development of a cross-linguistic core grammar. The core grammar provides a solid foundation for sustained development of linguistically-motivated yet computationally tractable grammars (e.g., (Hellan and Haugereid, 2003; Kordoni and Neu, 2005)). However, the core gra</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Carl Pollard and Ivan A. Sag. 1994. Head-Driven Phrase Structure Grammar. The University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurie Poulson</author>
</authors>
<title>Evaluating a cross-linguistic grammar model: Methodology and gold-standard resource development. Master’s thesis,</title>
<date>2006</date>
<institution>University of Washington.</institution>
<contexts>
<context position="2893" citStr="Poulson, 2006" startWordPosition="441" endWordPosition="442">elated) obstacles to making effective use of test suites in this scenario: (1) The Matrix core grammar isn’t itself a grammar, and therefore can’t parse any strings. (2) There is no single language modeled by the cross-linguistic resource from which to draw test strings. (3) The space of possible grammars (alternatively, language types) modeled by the resource is enormous, well beyond the scope of what can be thoroughly explored. We present a methodology for the validation and regression testing of the Grammar Matrix that addresses these obstacles, developing the ideas originally proposed in (Poulson, 2006). In its broad outlines, our methodology looks like this: • Define an abstract vocabulary to be used for test suite purposes. • Define an initial small set of string-semantics pairs. • Construct a large set of variations on the stringsemantics pairs. 136 Proceedings of the ACL 2007 Workshop on Deep Linguistic Processing, pages 136–143, Prague, Czech Republic, June, 2007. @2007 Association for Computational Linguistics • Define a set of filters that can delineate the legitimate string-semantics pairs for a particular language type The filters in effect constitute a parallel grammar definition s</context>
</contexts>
<marker>Poulson, 2006</marker>
<rawString>Laurie Poulson. 2006. Evaluating a cross-linguistic grammar model: Methodology and gold-standard resource development. Master’s thesis, University of Washington.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>