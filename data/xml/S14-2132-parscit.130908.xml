<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.013844">
<title confidence="0.9970575">
UNAL-NLP: Cross-Lingual Phrase Sense Disambiguation with
Syntactic Dependency Trees
</title>
<author confidence="0.938588">
Emilio Silva-Schlenker
</author>
<affiliation confidence="0.8600244">
Departamento de Lingüística
Universidad Nacional de Colombia
Departamento de Ingeniería de Sistemas
Universidad de los Andes,
Bogotá D.C., Colombia
</affiliation>
<email confidence="0.995885">
esilvas@unal.edu.co
</email>
<sectionHeader confidence="0.993881" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998751666666667">
In this paper we describe our participa-
tion in the SemEval 2014, Task 5, con-
sisting of the construction of a translation
assistance system that translates L1 frag-
ments, written in L2 context, to their cor-
rect L2 translation. Our approach con-
sists of a bilingual parallel corpus, a sys-
tem of syntactic features extraction and a
statistical memory-based classification al-
gorithm. Our system ranked 4th and 6th
among the 10 participating systems that
used the English-Spanish data set.
</bodyText>
<sectionHeader confidence="0.99879" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999945285714286">
An L2 writing assistant is a tool intended for lan-
guage learners who need to improve their writing
skills. This tool lets them write a text in L2, but fall
back to their native L1 whenever they are not sure
about a certain word or expression. In these cases,
the assistant automatically translates this text for
them (van Gompel et al., 2014).
Although at first glance this may be seen as
a classification problem, it might be better ful-
filled by a cross-lingual word sense disambigua-
tion (WSD) approach, which takes context into
account by means of contextual features used in
a machine learning setting. The main differences
between this and previous approaches to cross-
lingual WSD are the bilingual nature of the input
sentences (see section 2.3) and the annotation of
target phrases, rather than single words.
The remainder of this article is organized as fol-
lows. Section 2 describes the proposed method.
A description of the system we submitted, the ob-
tained results and an error analysis are discussed
</bodyText>
<footnote confidence="0.85726825">
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<note confidence="0.984176333333333">
Sergio Jimenez and Julia Baquero
Universidad Nacional de Colombia,
Bogotá D.C., Colombia
</note>
<email confidence="0.723282">
sgjimenezv@unal.edu.co
jmbauqerov@unal.edu.co
</email>
<bodyText confidence="0.999183333333333">
in section 3. In section 4 we present a brief dis-
cussion about the results. Finally, in section 5 we
make some concluding remarks.
</bodyText>
<sectionHeader confidence="0.938273" genericHeader="method">
2 Method Description
</sectionHeader>
<bodyText confidence="0.9991446">
The core of the proposed system uses techniques
from memory-based classification to find the most
appropriate translation of a target phrase in a
given context. It receives an input as in (1) and
yields an output as in (2).
</bodyText>
<listItem confidence="0.998725">
(1) No creo que ella is coming.
(2) No creo que ella venga.
</listItem>
<bodyText confidence="0.999851695652174">
It does so on the basis of a syntactic selec-
tion of context features, a large bilingual parallel
corpus and a classifier built using the Tilburg
Memory-Based Learner, TiMBL (Daelemans et
al., 2010).
The proposed system consists of several stages.
First, a large bilingual corpus is aligned at word
and phrase level. Next, an index is built by each
phrase in the L1 side of the corpus to retrieve ef-
ficiently the occurrences of a particular L1 phrase
in the aligned corpus along with their translations
and contexts in L2 (subsection 2.1). Second, the
relevant contexts for each L1 phrase in the test set
(example sentences) are retrieved from the corpus
and a set of syntactic features are extracted from
each sentence (subsection 2.2). Third, a special
two-stage process is used to extract the same fea-
tures from the sentences in the test set to deal with
the fact that these sentences were written in two
languages (subsection 2.3). Finally, each target
phrase is translated using the IBL algorithm and
the translations were incorporated in the original
test sentences (subsection 2.4).
</bodyText>
<page confidence="0.98436">
743
</page>
<note confidence="0.731288">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 743–747,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.967726">
Input sentence Parallel example sentences
No creo que las necesidades He said Boyd already linked Dijo que Boyd ya le había rela-
afectivas de las personas estén him to Brendan. cionado con Brendan.
necesariamente linked al
matrimonio.
The three things are inextrica- Las tres cosas están es-
bly linked, and I have the for- trechamente vinculadas, y
mula right here. tengo la fórmula aquí.
</bodyText>
<tableCaption confidence="0.998839">
Table 1: An input sentence and 2 example sentences from Linguee.com.
</tableCaption>
<subsectionHeader confidence="0.991965">
2.1 Parallel Corpus Selection and
Preparation
</subsectionHeader>
<bodyText confidence="0.973633025641026">
As no training corpus was given prior to develop-
ing this system, finding and processing the most
suitable corpus for this task was paramount. As
the purpose of this system is to help language stu-
dents, the corpus needs to account for simple yet
correct everyday speech.
In an initial stage of development we opted
to use the 70-million sentences OpenSubtitles.org
corpus compiled by the Opus Project (Tiedemann,
2012), which includes many informal everyday ut-
terances, at the expense of a less accurate transla-
tion quality1. Although the use of this training cor-
pus yielded over 95% of recall on the trial corpus
given by the task organizers, only 80% of the trial
sentences had enough (&gt;100) training examples in
order to produce a quality translation. To solve this
issue, an ad-hoc corpus compilation mechanism
was created by using the Linguee.com. Thus, a
set of parallel example sentences is retrieved from
Linguee.com by querying all the L1 target phrases
from the evaluation data (see an example in Table
1).
The corpus preparation procedure consisted of
several steps. The first step was to clean the cor-
pus with the Moses cleaning script (Koehn et al.,
2007). Next, the corpus was tokenized and PoS-
tagged using FreeLing (Padró and Stanilovsky,
2012) (HMM tagger was used). After that, the
corpus was word-aligned using Giza++ (Och and
Ney, 2003) over Moses (Koehn et al., 2007). The
resulting alignment was then combined with the
tagged version of the corpus. Finally, a phrase in-
dex was built using a SMT phrase extraction algo-
rithm (Ling et al., 2010) including for each phrase
pointers to all its occurrences in the corpus for fur-
ther retrieval.
1The EPPS corpus (Lambert et al., 2005) was very useful
as a training corpus in the developing stages of this system.
It was however not used in the final system training.
</bodyText>
<subsectionHeader confidence="0.999632">
2.2 Syntactic Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999989073170732">
The syntactic tags feature is a novel feature we are
introducing for the CLWSD problem (Lefever and
Hoste, 2013). They are linearizations of syntactic
dependency trees. These trees were built by Freel-
ing’s Txala Parser (Lloberes et al., 2010) and were
introduced as individual tags in a sentence analy-
sis by parsing the tree and mapping its leaves with
their corresponding order in the source sentence.
Then, each leaf’s label and parent number was ex-
tracted. For the root, the special parent tag ‘S’ was
used.
The WSD literature commonly distinguishes
between local and global context features (Mar-
tinez and Agirre, 2001). The former are extracted
from the neighboring words and the latter are ex-
tracted from words of the whole context provided
using some heuristic to select relevant. Unlike
global features, the relevance of the surrounding
words is not put into question or are weighted
by the degree of relevance according to their po-
sition in the sentence and lexicographic distance
from the target phrase (van Gompel, 2010). There
is a linguistic explanation as to why surrounding
words play a significant role in determining the
target’s translation. Often, these words have a di-
rect dependency relation with the target. Indeed,
physical closeness is an approximation of syntac-
tic relatedness. What we propose in this paper is
that the relevance of the context words for deter-
mining a correct translation is proportional to their
syntactic relatedness to the target, rather than their
physical closeness in the sentence. Unlike Mar-
tinez et al. (2002), what we propose here is to use
syntax as a feature selector, rather than as a feature
itself.
Instead of defining a local and a global set of
relevant words, we selected a single set of relevant
words according to their syntactic relation to the
target phrase. This set consisted of all the children
of the target words, and the parents of the main
target words. The main target words are the subset
</bodyText>
<page confidence="0.983759">
744
</page>
<table confidence="0.9992624">
0 1 2 3 4 5 6
Forms Las tres cosas están estrechamente vinculadas .
Lemmas el 3 cosa estar estrechamente vincular .
PoS Tags DA0FP0 Z NCFP000 VAIP3P0 RG VMP00PF Fp
Syn Tags espec:1 espec:2 subj:3 co-v:7 espec:5 att:3 ?:7
</table>
<tableCaption confidence="0.999603">
Table 2: Tagging of the sentence “Las tres cosas están estrechamente vinculadas.”
</tableCaption>
<bodyText confidence="0.999577714285714">
of words with the highest number of (nested) chil-
dren within the target phrase. Table 3 features the
rules used for selecting the relevant words.
This Feature Extraction method uses the depen-
dency labels as a means of selecting only rele-
vant examples. Take for instance the example sen-
tences in Table 1. Given that the target word is an
attribute, the subject is included as a relevant fea-
ture, as per the last rule in Table 3. Any example
sentence in which there is no subject as the sib-
ling of the target word (as is the case for the first
example sentence in Table 1) will have an empty
feature, which increases its likelihood of not being
included in the training set of this sentence.
</bodyText>
<subsectionHeader confidence="0.999944">
2.3 Test Data Pre-processing
</subsectionHeader>
<bodyText confidence="0.999978916666667">
The test data for this task is composed of bilin-
gual input sentences, making it impossible to ob-
tain a correct tagging or parsing. To overcome this
issue, a two-stage process wherein the first stage
obtains translations for the L1 portions was per-
formed. These plausible translations are obtained
by TiMBL using as features the neighboring words
of the target phrases. Once the sentences are in a
single language (L2) they are tagged and parsed
syntactically. Finally, the second stage consists in
applying the same feature selection algorithm pro-
posed in subsection 2.2.
</bodyText>
<subsectionHeader confidence="0.996907">
2.4 Translation Selection
</subsectionHeader>
<bodyText confidence="0.999429181818182">
The processing of each sentence consists of sev-
eral steps. In the first step, the L1 target phrase
is searched for in the phrase index Given an L1
phrase, a binary search algorithm iterates through
the phrase index and returns an array of point-
ers2 to the corpus. Then, a multi-threaded subrou-
tine reads the word-aligned bilingual corpus and
extracts all the referenced sentences. Thus, for
each input sentence, a set of example bilingual
word-aligned sentences is extracted from the cor-
pus. Relevant features are extracted according to
</bodyText>
<footnote confidence="0.504187">
2Given that line breaks are just regular characters, what is
actually referenced in the phrase index are byte offsets.
</footnote>
<bodyText confidence="0.999662176470588">
a syntactic analysis as explained in subsection 2.2,
and written to text files in the C4.5 format. The
features extracted from the example sentences, as
well as the L2 translations of the target phrases
in each sentence, are used as the training set for
TiMBL, while the features extracted from the in-
put sentence are used as its (singleton) test set.
The L2 translations of each target phrase in the
example sentences are used as the classes for the
training set, in order to turn a bilingual disam-
biguation problem into a machine learning clas-
sification problem. TiMBL learns how to classify
the training feature vectors into their correspond-
ing classes and then predicts the class for the test
set feature vector, i.e. its most likely translation
using an IBL algorithm (Aha et al., 1991), which
is a variation of the k-nearest neighbor classifier.
</bodyText>
<sectionHeader confidence="0.99555" genericHeader="method">
3 System Submissions
</sectionHeader>
<bodyText confidence="0.999985111111111">
We submitted three result sets for the English-
Spanish language pair. Two of them were submit-
ted for the ‘Best’ evaluation type, and the other
one was submitted for the ‘out-of-five’ evaluation
type. The difference between these two evaluation
types is that out-of-five evaluation expects up to
five different translations for every target phrase,
while ‘best’ only accepts one. The evaluation met-
rics include accuracy and recall, and also a word-
based special type of accuracy, which takes into
account partially correct translations.
Of the two runs submitted in the ‘Best’ evalu-
ation type, Run1-best (see table 4) used our pro-
posed syntactic feature extraction method, while
Run2-best used a regular 2-word window around
the target phrase. For the Run1-oof we combined
the two methods mentioned above with different
values of k.
</bodyText>
<subsectionHeader confidence="0.753644">
3.1 Results
</subsectionHeader>
<bodyText confidence="0.99983275">
The test data consisted in 500 sentences written in
Spanish, with target English phrases. The official
results obtained by our runs are shown in Table 4.
Our control run, Run2-best, yielded slightly
</bodyText>
<page confidence="0.976651">
745
</page>
<bodyText confidence="0.900925454545454">
Case Rule Example
One of the target words is a Include any sibling which is an Our cat quiere comerse la en-
subject. auxiliary or modal verb. salada.
The parent of one of the main Include its closest sibling. No quería ni eat, ni dormir.
target words is a coordinative
conjunction.
The parent of one of the main Include its grandparent. No creo que ella is coming.
target words is a relative pro-
noun.
One of the target words is an Include any sibling which is Mis tías están very tired.
attribute. subject.
</bodyText>
<tableCaption confidence="0.998709">
Table 3: Relevant word selection rules.
</tableCaption>
<bodyText confidence="0.996556">
better results than our experimental run, Run1-
best. This means that our method of syntactic fea-
tures extraction did not improve translation qual-
ity.
</bodyText>
<subsectionHeader confidence="0.993041">
3.2 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999902117647059">
By analyzing our results, we detected three groups
of recurrent errors. The first group of errors is re-
lated to verb morphology, in which a single En-
glish verbal form corresponds to many Spanish
verbal forms. In these cases, our system often out-
puts an infinitive form or a past participle instead
of a finite verb.
The second group of errors we detected com-
prises incomplete translations. In these cases, a
single word in English has a multiword Spanish
translation, but our system often outputs a single-
word translation.
The third group of errors are related to English
words with multiple possible parts of speech, as
‘flood’, which can be a noun but also a verb. Our
system tends to output nouns instead of verbs and
vice versa.
</bodyText>
<sectionHeader confidence="0.998706" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999967785714286">
There are two main reasons as to why the syntac-
tic feature extraction method did not work. The
first reason is related to the nature of the task; the
second is related to the scope of the method.
The fact that this task involved analyzing sen-
tences partly written in two languages made syn-
tactic analysis extremely difficult as dependencies
span all over the bilingual sentence. The best solu-
tion we found for this was to divide the operation
of the system in two stages, where the first one did
not involve syntactic dependencies and provided a
working translation, and the second one used this
first translation to perform a syntactic analysis and
then rerun the classification step. This, however,
favored error propagation. Although translation
quality did improve between the two stages, there
were many cases in which a bad initial translation
involved a bad syntactic analysis, which in turn re-
sulted in a bad final translation.
A more sophisticated version of his method was
initially developed for the English-Spanish lan-
guage pair and involved several language-specific
rules. However, we decided to make this method
language-independent, so we simplified it to its ac-
tual version. This simplified version uses syntactic
dependencies as feature selectors, but the features
themselves are regular lemma/PoS combinations,
which is not always the best feature choice.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999907666666667">
Syntactic dependency relations are an important
means of analyzing the internal structure of a sen-
tence and can successfully be used to improve the
feature selection process in WSD. However, syn-
tactic parsing is far away from optimal in Spanish,
a fortiori if it involves sentences written in two lan-
guages. For this kind of task, perhaps a statistical
language model of L2 would have yielded better
results.
</bodyText>
<sectionHeader confidence="0.998294" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999978125">
We would like to specially thank Professor Sil-
via Takahashi of Universidad de los Andes for her
continued advice and support. We would also like
to thank Pedro Rodríguez for his development of
the Linguee crawler, María De-Arteaga, Alejandro
Riveros and David Hoyos for their useful sugges-
tions in the conception and development of this
project, and the rest of the UNAL-NLP team for
</bodyText>
<page confidence="0.993662">
746
</page>
<table confidence="0.9991735">
Run Recall Accuracy Word Accuracy Rank (runs) Rank (systems)
Run1-best 0.993 0.721 0.794 5 2
Run2-best 0.993 0.733 0.809 4 2
Run1-oof 0.993 0.823 0.880 6 3
</table>
<tableCaption confidence="0.99832">
Table 4: Official results.
</tableCaption>
<bodyText confidence="0.57902">
their interest and encouragement. Many thanks to
Jay C. Soper for proof-reading this article.
</bodyText>
<sectionHeader confidence="0.998041" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998987419354838">
David W. Aha, Dennis Kibler, and Marc K. Albert.
1991. Instance-based learning algorithms. Machine
Learning, 6:37–66.
Walter Daelemans, Jakub Zavrel, Ko Van der Sloot,
and Antal Van den Bosch. 2010. Timbl: Tilburg
memory-based learner. reference guide. ILK Re-
search Group, Tilburg University.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, and
Richard Zens. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the 45th Annual Meeting of the ACL on Interactive
Poster and Demonstration Sessions, page 177–180.
Patrik Lambert, Adrià Gispert, Rafael Banchs, and
José B. Mariño. 2005. Guidelines for word align-
ment evaluation and manual alignment. Language
Resources and Evaluation, 39(4):267–285, Decem-
ber.
Els Lefever and Véronique Hoste. 2013. Semeval-
2013 task 10: Cross-lingual word sense disambigua-
tion. In Second joint conference on lexical and com-
putational semantics, volume 2, page 158–166.
Wang Ling, Tiago Luís, João Graça, Luısa Coheur, and
Isabel Trancoso. 2010. Towards a general and ex-
tensible phrase-extraction algorithm. In IWSLT’10:
International Workshop on Spoken Language Trans-
lation, page 313–320.
Marina Lloberes, Irene Castellón, and Lluís Padró.
2010. Spanish FreeLing dependency grammar. In
LREC, volume 10, page 693–699.
David Martinez and Eneko Agirre. 2001. Deci-
sion lists for english and basque. In The Proceed-
ings of the Second International Workshop on Eval-
uating Word Sense Disambiguation Systems, page
115–118.
David Martínez, Eneko Agirre, and Lluís Màrquez.
2002. Syntactic features for high precision word
sense disambiguation. In Proceedings of the
19th international conference on Computational
linguistics-Volume 1, page 1–7.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational linguistics, 29(1):19–51.
Lluís Padró and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence, pages 2473–2479, Istambul, Turkey, May.
Jörg Tiedemann. 2012. Parallel data, tools and in-
terfaces in OPUS. In Proceedings of the Lan-
guage Resources and Evaluation Conference, page
2214–2218, Istambul, Turkey, May.
Maarten van Gompel, Iris Hendrickx, Antal van den
Bosh, Els Lefever, and Véronique Hoste. 2014.
Semeval-2014 task 5: L2 writing assistant. In Pro-
ceedings of the 8th International Workshop on Se-
mantic Evaluation (SemEval-2014), Dublin, Ireland,
August.
Maarten van Gompel. 2010. UvT-WSD1: a cross-
lingual word sense disambiguation system. In Pro-
ceedings of the 5th international workshop on se-
mantic evaluation, page 238–241, Uppsala, Sweden.
</reference>
<page confidence="0.997552">
747
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.651328">
<title confidence="0.9994205">UNAL-NLP: Cross-Lingual Phrase Sense Disambiguation Syntactic Dependency Trees</title>
<author confidence="0.994851">Emilio</author>
<affiliation confidence="0.9292944">Departamento de Universidad Nacional de Departamento de Ingeniería de Universidad de los Bogotá D.C.,</affiliation>
<email confidence="0.974941">esilvas@unal.edu.co</email>
<abstract confidence="0.999838923076923">In this paper we describe our participation in the SemEval 2014, Task 5, conof the construction of a system translates L1 fragments, written in L2 context, to their correct L2 translation. Our approach consists of a bilingual parallel corpus, a system of syntactic features extraction and a statistical memory-based classification algorithm. Our system ranked 4th and 6th among the 10 participating systems that used the English-Spanish data set.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David W Aha</author>
<author>Dennis Kibler</author>
<author>Marc K Albert</author>
</authors>
<title>Instance-based learning algorithms.</title>
<date>1991</date>
<booktitle>Machine Learning,</booktitle>
<pages>6--37</pages>
<contexts>
<context position="11200" citStr="Aha et al., 1991" startWordPosition="1843" endWordPosition="1846">ations of the target phrases in each sentence, are used as the training set for TiMBL, while the features extracted from the input sentence are used as its (singleton) test set. The L2 translations of each target phrase in the example sentences are used as the classes for the training set, in order to turn a bilingual disambiguation problem into a machine learning classification problem. TiMBL learns how to classify the training feature vectors into their corresponding classes and then predicts the class for the test set feature vector, i.e. its most likely translation using an IBL algorithm (Aha et al., 1991), which is a variation of the k-nearest neighbor classifier. 3 System Submissions We submitted three result sets for the EnglishSpanish language pair. Two of them were submitted for the ‘Best’ evaluation type, and the other one was submitted for the ‘out-of-five’ evaluation type. The difference between these two evaluation types is that out-of-five evaluation expects up to five different translations for every target phrase, while ‘best’ only accepts one. The evaluation metrics include accuracy and recall, and also a wordbased special type of accuracy, which takes into account partially correc</context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>David W. Aha, Dennis Kibler, and Marc K. Albert. 1991. Instance-based learning algorithms. Machine Learning, 6:37–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Ko Van der Sloot</author>
<author>Antal Van den Bosch</author>
</authors>
<title>Timbl: Tilburg memory-based learner. reference guide.</title>
<date>2010</date>
<journal>ILK Research</journal>
<institution>Group, Tilburg University.</institution>
<marker>Daelemans, Zavrel, Van der Sloot, Van den Bosch, 2010</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Ko Van der Sloot, and Antal Van den Bosch. 2010. Timbl: Tilburg memory-based learner. reference guide. ILK Research Group, Tilburg University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="5473" citStr="Koehn et al., 2007" startWordPosition="876" endWordPosition="879">s training corpus yielded over 95% of recall on the trial corpus given by the task organizers, only 80% of the trial sentences had enough (&gt;100) training examples in order to produce a quality translation. To solve this issue, an ad-hoc corpus compilation mechanism was created by using the Linguee.com. Thus, a set of parallel example sentences is retrieved from Linguee.com by querying all the L1 target phrases from the evaluation data (see an example in Table 1). The corpus preparation procedure consisted of several steps. The first step was to clean the corpus with the Moses cleaning script (Koehn et al., 2007). Next, the corpus was tokenized and PoStagged using FreeLing (Padró and Stanilovsky, 2012) (HMM tagger was used). After that, the corpus was word-aligned using Giza++ (Och and Ney, 2003) over Moses (Koehn et al., 2007). The resulting alignment was then combined with the tagged version of the corpus. Finally, a phrase index was built using a SMT phrase extraction algorithm (Ling et al., 2010) including for each phrase pointers to all its occurrences in the corpus for further retrieval. 1The EPPS corpus (Lambert et al., 2005) was very useful as a training corpus in the developing stages of this</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, and Richard Zens. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, page 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrik Lambert</author>
<author>Adrià Gispert</author>
<author>Rafael Banchs</author>
<author>José B Mariño</author>
</authors>
<title>Guidelines for word alignment evaluation and manual alignment.</title>
<date>2005</date>
<journal>Language Resources and Evaluation,</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="6003" citStr="Lambert et al., 2005" startWordPosition="966" endWordPosition="969"> The first step was to clean the corpus with the Moses cleaning script (Koehn et al., 2007). Next, the corpus was tokenized and PoStagged using FreeLing (Padró and Stanilovsky, 2012) (HMM tagger was used). After that, the corpus was word-aligned using Giza++ (Och and Ney, 2003) over Moses (Koehn et al., 2007). The resulting alignment was then combined with the tagged version of the corpus. Finally, a phrase index was built using a SMT phrase extraction algorithm (Ling et al., 2010) including for each phrase pointers to all its occurrences in the corpus for further retrieval. 1The EPPS corpus (Lambert et al., 2005) was very useful as a training corpus in the developing stages of this system. It was however not used in the final system training. 2.2 Syntactic Feature Extraction The syntactic tags feature is a novel feature we are introducing for the CLWSD problem (Lefever and Hoste, 2013). They are linearizations of syntactic dependency trees. These trees were built by Freeling’s Txala Parser (Lloberes et al., 2010) and were introduced as individual tags in a sentence analysis by parsing the tree and mapping its leaves with their corresponding order in the source sentence. Then, each leaf’s label and par</context>
</contexts>
<marker>Lambert, Gispert, Banchs, Mariño, 2005</marker>
<rawString>Patrik Lambert, Adrià Gispert, Rafael Banchs, and José B. Mariño. 2005. Guidelines for word alignment evaluation and manual alignment. Language Resources and Evaluation, 39(4):267–285, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Els Lefever</author>
<author>Véronique Hoste</author>
</authors>
<title>Semeval2013 task 10: Cross-lingual word sense disambiguation.</title>
<date>2013</date>
<booktitle>In Second joint conference on lexical and computational semantics,</booktitle>
<volume>2</volume>
<pages>158--166</pages>
<contexts>
<context position="6281" citStr="Lefever and Hoste, 2013" startWordPosition="1013" endWordPosition="1016">03) over Moses (Koehn et al., 2007). The resulting alignment was then combined with the tagged version of the corpus. Finally, a phrase index was built using a SMT phrase extraction algorithm (Ling et al., 2010) including for each phrase pointers to all its occurrences in the corpus for further retrieval. 1The EPPS corpus (Lambert et al., 2005) was very useful as a training corpus in the developing stages of this system. It was however not used in the final system training. 2.2 Syntactic Feature Extraction The syntactic tags feature is a novel feature we are introducing for the CLWSD problem (Lefever and Hoste, 2013). They are linearizations of syntactic dependency trees. These trees were built by Freeling’s Txala Parser (Lloberes et al., 2010) and were introduced as individual tags in a sentence analysis by parsing the tree and mapping its leaves with their corresponding order in the source sentence. Then, each leaf’s label and parent number was extracted. For the root, the special parent tag ‘S’ was used. The WSD literature commonly distinguishes between local and global context features (Martinez and Agirre, 2001). The former are extracted from the neighboring words and the latter are extracted from wo</context>
</contexts>
<marker>Lefever, Hoste, 2013</marker>
<rawString>Els Lefever and Véronique Hoste. 2013. Semeval2013 task 10: Cross-lingual word sense disambiguation. In Second joint conference on lexical and computational semantics, volume 2, page 158–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
</authors>
<title>Tiago Luís, João Graça, Luısa Coheur, and Isabel Trancoso.</title>
<date>2010</date>
<booktitle>In IWSLT’10: International Workshop on Spoken Language Translation,</booktitle>
<pages>313--320</pages>
<marker>Ling, 2010</marker>
<rawString>Wang Ling, Tiago Luís, João Graça, Luısa Coheur, and Isabel Trancoso. 2010. Towards a general and extensible phrase-extraction algorithm. In IWSLT’10: International Workshop on Spoken Language Translation, page 313–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marina Lloberes</author>
<author>Irene Castellón</author>
<author>Lluís Padró</author>
</authors>
<title>Spanish FreeLing dependency grammar.</title>
<date>2010</date>
<booktitle>In LREC,</booktitle>
<volume>10</volume>
<pages>693--699</pages>
<contexts>
<context position="6411" citStr="Lloberes et al., 2010" startWordPosition="1033" endWordPosition="1036">ase index was built using a SMT phrase extraction algorithm (Ling et al., 2010) including for each phrase pointers to all its occurrences in the corpus for further retrieval. 1The EPPS corpus (Lambert et al., 2005) was very useful as a training corpus in the developing stages of this system. It was however not used in the final system training. 2.2 Syntactic Feature Extraction The syntactic tags feature is a novel feature we are introducing for the CLWSD problem (Lefever and Hoste, 2013). They are linearizations of syntactic dependency trees. These trees were built by Freeling’s Txala Parser (Lloberes et al., 2010) and were introduced as individual tags in a sentence analysis by parsing the tree and mapping its leaves with their corresponding order in the source sentence. Then, each leaf’s label and parent number was extracted. For the root, the special parent tag ‘S’ was used. The WSD literature commonly distinguishes between local and global context features (Martinez and Agirre, 2001). The former are extracted from the neighboring words and the latter are extracted from words of the whole context provided using some heuristic to select relevant. Unlike global features, the relevance of the surroundin</context>
</contexts>
<marker>Lloberes, Castellón, Padró, 2010</marker>
<rawString>Marina Lloberes, Irene Castellón, and Lluís Padró. 2010. Spanish FreeLing dependency grammar. In LREC, volume 10, page 693–699.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Martinez</author>
<author>Eneko Agirre</author>
</authors>
<title>Decision lists for english and basque.</title>
<date>2001</date>
<booktitle>In The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems,</booktitle>
<pages>115--118</pages>
<contexts>
<context position="6791" citStr="Martinez and Agirre, 2001" startWordPosition="1095" endWordPosition="1099">ion The syntactic tags feature is a novel feature we are introducing for the CLWSD problem (Lefever and Hoste, 2013). They are linearizations of syntactic dependency trees. These trees were built by Freeling’s Txala Parser (Lloberes et al., 2010) and were introduced as individual tags in a sentence analysis by parsing the tree and mapping its leaves with their corresponding order in the source sentence. Then, each leaf’s label and parent number was extracted. For the root, the special parent tag ‘S’ was used. The WSD literature commonly distinguishes between local and global context features (Martinez and Agirre, 2001). The former are extracted from the neighboring words and the latter are extracted from words of the whole context provided using some heuristic to select relevant. Unlike global features, the relevance of the surrounding words is not put into question or are weighted by the degree of relevance according to their position in the sentence and lexicographic distance from the target phrase (van Gompel, 2010). There is a linguistic explanation as to why surrounding words play a significant role in determining the target’s translation. Often, these words have a direct dependency relation with the t</context>
</contexts>
<marker>Martinez, Agirre, 2001</marker>
<rawString>David Martinez and Eneko Agirre. 2001. Decision lists for english and basque. In The Proceedings of the Second International Workshop on Evaluating Word Sense Disambiguation Systems, page 115–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Martínez</author>
<author>Eneko Agirre</author>
<author>Lluís Màrquez</author>
</authors>
<title>Syntactic features for high precision word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>1--7</pages>
<marker>Martínez, Agirre, Màrquez, 2002</marker>
<rawString>David Martínez, Eneko Agirre, and Lluís Màrquez. 2002. Syntactic features for high precision word sense disambiguation. In Proceedings of the 19th international conference on Computational linguistics-Volume 1, page 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational linguistics,</journal>
<pages>29--1</pages>
<contexts>
<context position="5660" citStr="Och and Ney, 2003" startWordPosition="906" endWordPosition="909">lity translation. To solve this issue, an ad-hoc corpus compilation mechanism was created by using the Linguee.com. Thus, a set of parallel example sentences is retrieved from Linguee.com by querying all the L1 target phrases from the evaluation data (see an example in Table 1). The corpus preparation procedure consisted of several steps. The first step was to clean the corpus with the Moses cleaning script (Koehn et al., 2007). Next, the corpus was tokenized and PoStagged using FreeLing (Padró and Stanilovsky, 2012) (HMM tagger was used). After that, the corpus was word-aligned using Giza++ (Och and Ney, 2003) over Moses (Koehn et al., 2007). The resulting alignment was then combined with the tagged version of the corpus. Finally, a phrase index was built using a SMT phrase extraction algorithm (Ling et al., 2010) including for each phrase pointers to all its occurrences in the corpus for further retrieval. 1The EPPS corpus (Lambert et al., 2005) was very useful as a training corpus in the developing stages of this system. It was however not used in the final system training. 2.2 Syntactic Feature Extraction The syntactic tags feature is a novel feature we are introducing for the CLWSD problem (Lef</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lluís Padró</author>
<author>Evgeny Stanilovsky</author>
</authors>
<title>Freeling 3.0: Towards wider multilinguality.</title>
<date>2012</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference,</booktitle>
<pages>2473--2479</pages>
<location>Istambul, Turkey,</location>
<contexts>
<context position="5564" citStr="Padró and Stanilovsky, 2012" startWordPosition="890" endWordPosition="893"> organizers, only 80% of the trial sentences had enough (&gt;100) training examples in order to produce a quality translation. To solve this issue, an ad-hoc corpus compilation mechanism was created by using the Linguee.com. Thus, a set of parallel example sentences is retrieved from Linguee.com by querying all the L1 target phrases from the evaluation data (see an example in Table 1). The corpus preparation procedure consisted of several steps. The first step was to clean the corpus with the Moses cleaning script (Koehn et al., 2007). Next, the corpus was tokenized and PoStagged using FreeLing (Padró and Stanilovsky, 2012) (HMM tagger was used). After that, the corpus was word-aligned using Giza++ (Och and Ney, 2003) over Moses (Koehn et al., 2007). The resulting alignment was then combined with the tagged version of the corpus. Finally, a phrase index was built using a SMT phrase extraction algorithm (Ling et al., 2010) including for each phrase pointers to all its occurrences in the corpus for further retrieval. 1The EPPS corpus (Lambert et al., 2005) was very useful as a training corpus in the developing stages of this system. It was however not used in the final system training. 2.2 Syntactic Feature Extrac</context>
</contexts>
<marker>Padró, Stanilovsky, 2012</marker>
<rawString>Lluís Padró and Evgeny Stanilovsky. 2012. Freeling 3.0: Towards wider multilinguality. In Proceedings of the Language Resources and Evaluation Conference, pages 2473–2479, Istambul, Turkey, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jörg Tiedemann</author>
</authors>
<title>Parallel data, tools and interfaces in OPUS.</title>
<date>2012</date>
<booktitle>In Proceedings of the Language Resources and Evaluation Conference,</booktitle>
<pages>2214--2218</pages>
<location>Istambul, Turkey,</location>
<contexts>
<context position="4723" citStr="Tiedemann, 2012" startWordPosition="752" endWordPosition="753">d I have the for- trechamente vinculadas, y mula right here. tengo la fórmula aquí. Table 1: An input sentence and 2 example sentences from Linguee.com. 2.1 Parallel Corpus Selection and Preparation As no training corpus was given prior to developing this system, finding and processing the most suitable corpus for this task was paramount. As the purpose of this system is to help language students, the corpus needs to account for simple yet correct everyday speech. In an initial stage of development we opted to use the 70-million sentences OpenSubtitles.org corpus compiled by the Opus Project (Tiedemann, 2012), which includes many informal everyday utterances, at the expense of a less accurate translation quality1. Although the use of this training corpus yielded over 95% of recall on the trial corpus given by the task organizers, only 80% of the trial sentences had enough (&gt;100) training examples in order to produce a quality translation. To solve this issue, an ad-hoc corpus compilation mechanism was created by using the Linguee.com. Thus, a set of parallel example sentences is retrieved from Linguee.com by querying all the L1 target phrases from the evaluation data (see an example in Table 1). T</context>
</contexts>
<marker>Tiedemann, 2012</marker>
<rawString>Jörg Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In Proceedings of the Language Resources and Evaluation Conference, page 2214–2218, Istambul, Turkey, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maarten van Gompel</author>
<author>Iris Hendrickx</author>
<author>Antal van den Bosh</author>
<author>Els Lefever</author>
<author>Véronique Hoste</author>
</authors>
<date>2014</date>
<booktitle>Semeval-2014 task 5: L2 writing assistant. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014),</booktitle>
<location>Dublin, Ireland,</location>
<marker>van Gompel, Hendrickx, van den Bosh, Lefever, Hoste, 2014</marker>
<rawString>Maarten van Gompel, Iris Hendrickx, Antal van den Bosh, Els Lefever, and Véronique Hoste. 2014. Semeval-2014 task 5: L2 writing assistant. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval-2014), Dublin, Ireland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maarten van Gompel</author>
</authors>
<title>UvT-WSD1: a crosslingual word sense disambiguation system.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th international workshop on semantic evaluation,</booktitle>
<pages>238--241</pages>
<location>Uppsala,</location>
<marker>van Gompel, 2010</marker>
<rawString>Maarten van Gompel. 2010. UvT-WSD1: a crosslingual word sense disambiguation system. In Proceedings of the 5th international workshop on semantic evaluation, page 238–241, Uppsala, Sweden.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>