<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.960987">
Balancing Conflicting Factors in Argument Interpretation
</title>
<author confidence="0.973561">
Ingrid Zukerman, Michael Niemann and Sarah George
</author>
<affiliation confidence="0.95255">
Faculty of Information Technology
Monash University
</affiliation>
<address confidence="0.737677">
Clayton, VICTORIA 3800, AUSTRALIA
</address>
<email confidence="0.99883">
{ingrid,niemann,sarahg}@csse.monash.edu.au
</email>
<sectionHeader confidence="0.997388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913736842105">
We present a probabilistic approach for the
interpretation of arguments that casts the
selection of an interpretation as a model
selection task. In selecting the best model,
our formalism balances conflicting fac-
tors: model complexity against data fit,
and structure complexity against belief
reasonableness. We first describe our ba-
sic formalism, which considers interpreta-
tions comprising inferential relations, and
then show how our formalism is extended
to suppositions that account for the beliefs
in an argument, and justifications that ac-
count for the inferences in an interpreta-
tion. Our evaluations with users show that
the interpretations produced by our system
are acceptable, and that there is strong sup-
port for the postulated suppositions and
justifications.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999948381818182">
The source-channel approach has been often used
for word-based language tasks, such as speech
recognition and machine translation (Epstein,
1996; Och and Ney, 2002). According to this ap-
proach, an addressee receives a noisy channel (lan-
guage or speech wave), and decodes this channel
to derive the source (idea). The selected source is
that with the maximum posterior probability.
In this paper, we apply the source-channel ap-
proach to the interpretation of arguments. This
approach enables us to cast argument interpreta-
tion as a trade-off between conflicting factors, viz
model complexity against data fit, and structure
complexity against belief reasonableness. This
trade-off is inspired by the Minimum Message
Length (MML) Criterion – a model selection
method that is the basis for several machine learn-
ing techniques (Wallace, 2005). According to this
trade-off, a more complex model might fit the data
better, but the plausibility (priors) of the model
must be taken into account to avoid over-fitting.1
Our argument interpretation mechanism has
been implemented in a system called BIAS
(Bayesian Interactive Argumentation System).
BIAS presents to a user a set of facts about the
world (evidence), and the user constructs an argu-
ment about a particular goal proposition in light
of this evidence. BIAS then generates an interpre-
tation of the user’s argument, i.e., it tries to un-
derstand the argument. When people try to under-
stand an interlocutor’s discourse, their interpreta-
tion is in terms of their own beliefs and inference
patterns. Likewise, our system’s interpretations
are in terms of its underlying knowledge repre-
sentation – a Bayesian network (BN). The inter-
pretations generated by BIAS include inferences
that connect the propositions in a user’s argument,
suppositions that postulate a user’s beliefs that are
necessary to make sense of the argument, and ex-
planatory extensions that justify the inferences in
the interpretation (and in the argument). BIAS
does not generate its own arguments, rather, it in-
tegrates these components to make sense of the
user’s argument.
In this paper, we first describe our basic for-
malism, which is used to calculate the probability
of interpretations that include only inferences, and
then show how progressive enhancements of this
formalism are used for more informative interpre-
tations.
In Section 2, we explain what is an argument
interpretation, and describe briefly the interpreta-
tion process. Next, we discuss our probabilistic
formalism for selecting an interpretation, which is
the focus of this paper. In Section 4, we present
</bodyText>
<note confidence="0.74722975">
&apos;Other model selection criteria such as Akaike Informa-
tion Criterion (AIC) and Bayes Information Criterion (BIC)
(Box et al., 1994) also argue for model parsimony, but they
do so by penalizing models with more free parameters.
</note>
<page confidence="0.963522">
134
</page>
<note confidence="0.6100545">
Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 134–143,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<bodyText confidence="0.903156">
the results of our evaluations, followed by a dis-
cussion of related work, and concluding remarks.
</bodyText>
<sectionHeader confidence="0.993565" genericHeader="introduction">
2 Argument interpretation
</sectionHeader>
<bodyText confidence="0.99984375">
We define an interpretation of a user’s argument as
the tuple {5C, IG, EE}, where 5C is a supposi-
tion configuration, IG is an interpretation graph,
and EE are explanatory extensions.
</bodyText>
<listItem confidence="0.9997786">
• A Supposition Configuration is a set of sup-
positions attributed to the user (in addition to
or instead of shared beliefs) to account for the
beliefs in his or her argument.
• An Interpretation Graph is a domain struc-
</listItem>
<bodyText confidence="0.958855333333333">
ture, in our case a subnet of the domain BN,
that connects the nodes mentioned in the argu-
ment. The nodes and arcs that are included in
an interpretation graph but were not mentioned
by the user fill in additional detail from the BN,
bridging inferential leaps in the argument.
</bodyText>
<listItem confidence="0.857099">
• Explanatory Extensions are domain struc-
tures (subnets of the domain BN) that are added
to an interpretation graph to justify an infer-
ence. Contrary to suppositions, these explana-
</listItem>
<bodyText confidence="0.987586130434783">
tions contain propositions believed by the user
and the system. The presentation of these ex-
planations is motivated by the results of our
early trials, where people objected to belief dis-
continuities between the antecedents and the
consequent of inferences, i.e., increases in cer-
tainty or large changes in certainty (Zukerman
and George, 2005).
To illustrate these components, consider the ex-
ample in Figure 1. The top segment contains
a short argument, and the bottom segment con-
tains its interpretation. The middle segment con-
tains an excerpt of the domain BN which in-
cludes the interpretation; the probabilities of some
nodes are indicated with linguistic terms.2 The in-
terpretation graph, which appears inside a light
gray bubble in the BN excerpt, includes the ex-
tra node GreenInGardenAtTimeOfDeath (boxed).
Note that the propagated beliefs in this interpre-
tation graph do not match those in the argument.
To address this problem, the system supposes that
the user believes that TimeOfDeath11=TRUE, in-
stead of the BN belief of Probably (boldfaced and
</bodyText>
<footnote confidence="0.97057275">
2We use the terms Very Probable, Probable, Possible and
their negations, and Even Chance. These terms, which are
similar to those used in (Elsaesser, 1987), are most consis-
tently understood by people according to our user surveys.
</footnote>
<sectionHeader confidence="0.871002" genericHeader="method">
ARGUMENT
</sectionHeader>
<bodyText confidence="0.94648575">
Mr Green probably being in the garden at 11 implies that
he possibly had the opportunity to kill Mr Body, but
he did not murder Mr Body.
possibly
</bodyText>
<sectionHeader confidence="0.720989" genericHeader="method">
EXCERPT OF DOMAIN BN
</sectionHeader>
<subsectionHeader confidence="0.266325">
INTERPRETATION
</subsectionHeader>
<bodyText confidence="0.7527834">
Mr Green being in the garden at 11, and
probably
supposing that the time of death is 11 implies that
Mr Green probably was in the garden at the time of death.
Hence, he possibly had the opportunity to kill Mr Body, but
</bodyText>
<subsectionHeader confidence="0.527675">
Mr Green probably did not have the means.
</subsectionHeader>
<bodyText confidence="0.939041">
Therefore, he did murder Mr Body.
</bodyText>
<figure confidence="0.525707">
possibly not
</figure>
<figureCaption confidence="0.9296615">
Figure 1: Sample argument, BN excerpt and inter-
pretation
</figureCaption>
<bodyText confidence="0.999921705882353">
gray-boxed). This fixes the mismatch between the
probabilities in the argument and those in the in-
terpretation, but one problem remains: in early tri-
als we found that people objected to belief discon-
tinuities, such as the “jump in belief” from pos-
sibly having opportunity to possibly not murder-
ing Mr Body (this jump appears both in the origi-
nal argument and in the interpretation, whose be-
liefs now match those in the argument as a re-
sult of the supposition). This prompts the gen-
eration of the explanatory extension GreenHad-
Means[ProbablyNot] (white boldfaced and dark-
gray boxed). The three elements added during the
interpretation process – the extra node in the inter-
pretation graph, the supposition and the explana-
tory extension – appear in boldface italics in the
interpretation at the bottom of the figure.
</bodyText>
<subsectionHeader confidence="0.996376">
2.1 Proposing Interpretations
</subsectionHeader>
<bodyText confidence="0.9997907">
The problem of finding the best interpretation is
exponential. In previous work, we proposed an
anytime algorithm to propose interpretation graphs
and supposition configurations until time runs out
(George et al., 2004). Here we apply our algorithm
to generate interpretations comprising supposition
configurations (5C), interpretation graphs (IG)
and explanatory extensions (EE) (Figure 2).
Supposition configurations are proposed first, as
instantiated beliefs affect the plausibility of inter-
</bodyText>
<figure confidence="0.9989825">
. . .
GreenHadMeans
GreenLadder
AtWindow
GreenMurderedBody
ProbablyNot ProbablyNot EvenChance
TimeOfDeath11
GreenHadOpportunity
Probably
GreenInGardenAt
TimeOfDeath
. . .
ProbablyNot
GreenInGardenAt11
NbourHeardGreen&amp;Body
ArgueLastNight
Probably
GreenHadMotive
GreenVisitBody
LastNight
</figure>
<page confidence="0.931064">
135
</page>
<sectionHeader confidence="0.676347" genericHeader="method">
Algorithm GenerateInterpretations(Arg)
</sectionHeader>
<bodyText confidence="0.684548">
while {there is time}
{
</bodyText>
<listItem confidence="0.996587083333333">
1. Propose a supposition configuration SC that
accounts for the beliefs stated in the argument.
2. Propose an interpretation graph IG that con-
nects the nodes in Arg under supposition con-
figuration SC.
3. Propose explanatory extensions EE for inter-
pretation graph IG under supposition config-
uration SC if necessary.
4. Calculate the probability of interpretation
{SC, IG, EE}.
5. Retain the top N (=6) most probable interpre-
tations.
</listItem>
<figure confidence="0.526843">
}
</figure>
<figureCaption confidence="0.9548755">
Figure 2: Anytime algorithm for generating inter-
pretations
</figureCaption>
<bodyText confidence="0.998967826086957">
pretation graphs, which in turn affect the need for
explanatory extensions. The proposal of supposi-
tion configurations, interpretation graphs and ex-
planatory extensions is driven by the probability
of these components. In each iteration, we gener-
ate candidates for a component, calculate the prob-
ability of these candidates in the context of the
selections made in the previous steps, and proba-
bilistically select one of these candidates. That is,
higher probability candidates have a better chance
of being selected than lower probability ones (our
selection procedures are described in George et al.,
2004). For example, say that in Step 1, we selected
supposition configuration SCa. Next, in Step 2,
the probability of candidate IGs is calculated in
the context of the domain BN and SCa, and one
of the IGs is probabilistically selected, say IGb.
Similarly, in Step 3, one of the candidate EEs is
selected in the context of SCa and IGb. In the next
iteration, we probabilistically select an SC (which
could be a previously chosen one), and so on. To
generate diverse interpretations, if SCa is selected
again, a different IG will be chosen.
</bodyText>
<sectionHeader confidence="0.989873" genericHeader="method">
3 Probabilistic formalism
</sectionHeader>
<bodyText confidence="0.9999204">
Following (Wallace, 2005), our approach requires
the specification of three elements: background
knowledge, model and data. Background knowl-
edge is everything known to the system prior to in-
terpreting a user’s argument, e.g., domain knowl-
edge, shared beliefs with the user, and dialogue
history; the data is the argument; and the model
is the interpretation.
We posit that the best interpretation is that with
the highest posterior probability.
</bodyText>
<equation confidence="0.794089">
IntBest = argmaxi=j,...,qPr(SCi, IGi, EEi|Arg)
</equation>
<bodyText confidence="0.999240666666667">
where q is the number of interpretations.
After applying Bayes rule, this probability is
represented as follows.3
</bodyText>
<equation confidence="0.9124315">
Pr(SCi, IGi, EEi|Arg) = (1)
α Pr(SCi, IGi, EEi)×Pr(Arg|SCi, IGi, EEi)
</equation>
<bodyText confidence="0.999974">
where α is a normalizing constant that ensures that
the probabilities of the interpretations sum to 1
</bodyText>
<equation confidence="0.978338">
� α= �nj.
j�,Pr(SCj,IGj,EEj)XPr(Arg|SCj,IGj,EEj)
</equation>
<bodyText confidence="0.9997285">
The first factor represents model complexity,
and the second factor represents data fit.
</bodyText>
<listItem confidence="0.970626">
• Model complexity measures how difficult it is
to produce the model (interpretation) from the
background knowledge. The higher/lower the
complexity of a model, the lower/higher its
probability.
• Data fit measures how well the data (argument)
</listItem>
<bodyText confidence="0.8508354">
matches the model (interpretation). The bet-
ter/worse the match between the argument and
an interpretation, the higher/lower the proba-
bility that the speaker intended this interpreta-
tion when he or she uttered the argument.
</bodyText>
<subsectionHeader confidence="0.900637">
Model Complexity
</subsectionHeader>
<bodyText confidence="0.966304095238095">
Model complexity is a function {B, M}[0,1]
that represents the prior probability of the model
M (i.e., the interpretation) in terms of the back-
ground knowledge B. The calculation of model
complexity depends on the type of the model: nu-
merical or structural.
The probability of a numerical model depends
on the similarity between the numerical values (or
distributions) in the model and those in the back-
ground knowledge. The higher/lower this similar-
ity, the higher/lower the probability of the model.
For instance, a supposition configuration SC com-
prising beliefs that differ significantly from those
in the background knowledge will lower the prob-
ability of an interpretation. One of the functions
we have used to calculate belief probabilities is the
Zipf distribution, where the parameter is the differ-
ence between beliefs, e.g., between the supposed
&apos;In principle, Pr(SCi, IGi, EEi Arg) can be calculated
directly. However, it is not clear how to incorporate the priors
of an interpretation in the direct calculation.
</bodyText>
<page confidence="0.995171">
136
</page>
<bodyText confidence="0.991254833333333">
beliefs and the corresponding beliefs in the back-
ground knowledge (Zukerman and George, 2005).
That is, the probability of a supposed belief in
proposition P according to model M (bel M(P)),
in light of the belief in P according to background
knowledge B (bel B(P)), is
</bodyText>
<equation confidence="0.625557333333333">
0
Pr(bel M(P)|bel B(P))=
|bel M(P)−bel B(P)|γ
</equation>
<bodyText confidence="0.993224466666667">
where 0 is a normalizing constant, and -y deter-
mines the penalty assigned to the discrepancy be-
tween the beliefs in P. For example,
Pr(bel M(P)=TRUE|bel B(P)=Probable) &gt;
Pr(bel M(P)= TRUE|bel B(P)=EvenChance)
as TRUE is closer to Probable than to EvenChance.
The probability of a structural model (e.g., an
interpretation graph) is obtained from the proba-
bilities of the elements in the structure (e.g., nodes
and arcs) in light of the background knowledge.
The simplest calculation assumes that the proba-
bility of including nodes and arcs in an interpreta-
tion graph is uniform. That is, the probability of
an interpretation graph comprising n nodes and a
arcs is a function of
</bodyText>
<listItem confidence="0.999698">
• the probability of n,
• the probability of selecting n particular nodes
</listItem>
<bodyText confidence="0.538142">
from N nodes in the domain BN: (N )��,
n
</bodyText>
<listItem confidence="0.929147">
• the probability of a, and
• the probability of selecting a particular arcs
from the arcs that connect the n selected nodes.
</listItem>
<bodyText confidence="0.998155">
This calculation generally prefers small models
to larger models.4
</bodyText>
<subsectionHeader confidence="0.881249">
Data fit
</subsectionHeader>
<bodyText confidence="0.999951214285715">
Data fit is a function {M, D} —* [0, 1] that rep-
resents the probability of the data D (argument)
given the model M (interpretation). This proba-
bility hinges on the similarity between the model
and the data – the closer the data is to the model,
the higher is the probability of the data.
The calculation of the similarity between nu-
merical data and a numerical model is the same
as the calculation of the similarity between a nu-
merical model and background knowledge.
The similarity between structural data and a
structural model is a function of the number and
type of operations required to convert the model
into the data, e.g., node and arc insertions and
</bodyText>
<subsectionHeader confidence="0.582156">
4In the rare cases where n &gt; N/2, smaller models do not
yield lower probabilities.
</subsectionHeader>
<bodyText confidence="0.9997112">
deletions. For the example in Figure 1, to con-
vert the interpretation graph into the argument, we
must delete one node (GreenInGardenAtTimeOf-
Death) and its incident arcs. The more operations
need to be performed, the lower the similarity be-
tween the data and the model, and the lower the
probability of the data given the model.
We now discuss our basic probabilistic formal-
ism, which accounts for interpretation graphs, fol-
lowed by two enhancements: (1) a more complex
model that accounts for suppositions; and (2) in-
creases in background knowledge that yield a pref-
erence for larger interpretation graphs under cer-
tain circumstances, and account for explanatory
extensions.
</bodyText>
<subsectionHeader confidence="0.994654">
3.1 Basic formalism: Interpretation graphs
</subsectionHeader>
<bodyText confidence="0.9997925">
In the basic formalism, the model contains only an
interpretation graph. Thus, Equation 1 is simply
</bodyText>
<equation confidence="0.999737">
Pr(IGi|Arg) = α Pr(IGi) x Pr(Arg|IGi) (2)
</equation>
<bodyText confidence="0.99996975">
The difference in the calculations of model
complexity and data fit for numerical and struc-
tural information warrants the separation of struc-
ture and belief, which yields
</bodyText>
<equation confidence="0.939021">
Pr(IGi|Arg) = α Pr(bel IGi, struc IGi)x
Pr(bel Arg, struc Arg|bel IGi, struc IGi)
</equation>
<bodyText confidence="0.954399">
After applying the chain rule of probability
</bodyText>
<equation confidence="0.982774">
Pr(IGi|Arg) =
</equation>
<bodyText confidence="0.96588345">
αPr(bel IGi|struc IGi) x Pr(struc IGi) x
Pr(bel Arg|struc Arg, bel IGi, struc IGi) x
Pr(struc Arg|bel IGi, struc IGi)
Note that Pr(bel IGi|struc IGi) does not cal-
culate the probability of (or belief in) the nodes
in IGi. Rather, it calculates how probable are
these beliefs in light of the structure of IGi
and the expectations from the background knowl-
edge. For instance, if the belief in a node is p,
it calculates the probability of p. This proba-
bility depends on the closeness between the be-
liefs in IGi and the expected ones. Since the
beliefs in IGi are obtained algorithmically by
means of Bayesian propagation from the back-
ground knowledge, they match precisely the ex-
pectations. Hence, Pr(bel IGi|struc IGi) = 1.
We also make the following simplifying as-
sumptions for situations where the interpretation
is known (given): (1) the probability of the beliefs
in the argument depends only on the beliefs in the
</bodyText>
<page confidence="0.997798">
137
</page>
<tableCaption confidence="0.998819">
Table 1: Probability – Basic formalism
</tableCaption>
<table confidence="0.9324495">
Model complexity (against background)
↓ Pr(struc IGi) ↑ structural complexity
(model size)
Data fit with model
↑ Pr(struc Arg|struc IGi) ↓ structural discrepancy
Pr(bel Arg|bel IGi) numerical discrepancy
</table>
<bodyText confidence="0.9672375">
interpretation (and not on its structure or the ar-
gument’s structure), and (2) the probability of the
argument structure depends only on the interpreta-
tion structure (and not on its beliefs). This yields
</bodyText>
<equation confidence="0.99195">
Pr(IGi|Arg) = α Pr(struc IGi)x (3)
Pr(bel Arg|bel IGi) x Pr(struc Arg|struc IGi)
</equation>
<bodyText confidence="0.9998325">
Table 1 summarizes the calculation of these
probabilities separated according to model com-
plexity and data fit. It also shows the trade-off
between structural model complexity and struc-
tural data fit. As seen at the start of Section 3,
smaller structures generally have a lower model
complexity than larger ones. However, an increase
in structural model complexity (indicated by the ↑
next to the structural complexity and the ↓ next
to the resultant probability of the model) may re-
duce the structural discrepancy between the argu-
ment structure and the structure of the interpreta-
tion graph (indicated by the ↓ next to the structural
discrepancy and the ↑ next to the probability of the
structural data-fit). For instance, the smallest pos-
sible interpretation for the argument in Figure 1
consists of a single node, but this interpretation has
a very poor data fit with the argument.
</bodyText>
<subsectionHeader confidence="0.997742">
3.2 A more informed model
</subsectionHeader>
<bodyText confidence="0.999491">
In order to postulate suppositions that account for
the beliefs in an argument, we expand the basic
model to include supposition configurations (be-
liefs attributed to the user in addition to or instead
of the beliefs shared with the system). Now the
model comprises the pair ISCi, IGi}, and Equa-
tion 2 becomes
</bodyText>
<equation confidence="0.9787185">
Pr(SCi,IGi|Arg) = (4)
α Pr(SCi, IGi) x Pr(Arg|SCi, IGi)
</equation>
<bodyText confidence="0.9512965">
Similar probabilistic manipulations to those
performed in Section 3.1 yield
</bodyText>
<equation confidence="0.429004666666667">
Pr(SCi, IGi|Arg) = (5)
α Pr(struc IGi|SCi)xPr(SCi)x
Pr(bel Arg|SCi, bel IGi)xPr(struc Arg|struc IGi)
</equation>
<tableCaption confidence="0.985714">
Table 2: Probability – More informed model
</tableCaption>
<table confidence="0.9908045">
Model complexity (against background)
Pr(struc IGi|SC;) structural complexity
↓ Pr(SC;) ↑numerical discrepancy
Data fit with model
Pr(struc Arg|struc IGi) structural discrepancy
↑ Pr(bel Arg|SC;,bel IGi) ↓numerical discrepancy
</table>
<bodyText confidence="0.9914904375">
(Recall that suppositions pertain to beliefs only,
i.e., they don’t have a structural component.)
Table 2 summarizes the calculation of these
probabilities separated according to model com-
plexity and data fit (the elements that differ from
the basic model are boldfaced). It also shows the
trade-off between belief model complexity and be-
lief data fit. Making suppositions has a higher
model complexity (lower probability) than not
making suppositions (where SCi matches the be-
liefs in the domain BN). However, as seen in the
example in Figure 1, making a supposition that re-
duces or eliminates the discrepancy between the
beliefs in the argument and those in the interpre-
tation increases the belief data-fit considerably, at
the expense of a more complex belief model.
</bodyText>
<subsectionHeader confidence="0.998747">
3.3 Additional background knowledge
</subsectionHeader>
<bodyText confidence="0.984019416666667">
An increase in our background knowledge means
that we take into account additional factors about
the world. This extra knowledge in turn may
cause us to prefer interpretations that were pre-
viously discarded. We have considered two ad-
ditions to background knowledge: dialogue his-
tory, and users’ preferences regarding inference
patterns.
Dialogue history
Dialogue history influences the salience of a
node, and hence the probability that it was in-
cluded in a user’s argument. We have modeled
salience by means of an activation function that
decays with time (Anderson, 1983), and used this
function to moderate the probability of including
a node in an interpretation (instead of using a uni-
form distribution). We have experimented with
two activation functions: (1) a function where the
level of activation of a node is based on the fre-
quency and recency of the direct activation of this
node; and (2) a function where the level of activa-
tion of a node depends on its similarity with all the
(activated) nodes, together with the frequency and
recency of their activation (Zukerman and George,
</bodyText>
<page confidence="0.997047">
138
</page>
<bodyText confidence="0.999529733333333">
2005).
To illustrate the influence of salience, com-
pare the preferred interpretation graph in
Figure 1 (in the light gray bubble) with
an alternative path through NbourHeard-
Green&amp;BodyArgueLastNight and GreenVisit-
BodyLastNight. The preferred path has 4 nodes,
while the alternative one has 5 nodes, and hence
a lower probability. However, if the nodes in the
longer path had been recently mentioned, their
salience could overcome the size disadvantage.
Thus, although the chosen interpretation graph
may have a worse data fit than the smallest graph,
it still may have the best overall probability in
light of the additional background knowledge.
</bodyText>
<subsectionHeader confidence="0.621024">
Inference patterns
</subsectionHeader>
<bodyText confidence="0.937708615384616">
In a formative evaluation of an earlier version
of our system, we found that people objected
to inferences that had increases in certainty or
large changes in certainty (Zukerman and George,
2005). An example of an increase in certainty is
A [Probably] implies B [VeryProbably].
A large change in certainty is illustrated by
A [VeryProbably] implies B [EvenChance].
We then conducted another survey to deter-
mine the types of inferences considered acceptable
by people (from the standpoint of the beliefs in
the antecedents and the consequent). The results
from our preliminary survey prompted us to dis-
tinguish between three types of inferences: Both-
Sides, SameSide and AlmostSame.
• BothSides inferences have antecedents with be-
liefs on both “sides” of the consequent (in
favour and against), e.g.,
A[VeryProbably] &amp; B[ProbablyNot] implies
C[EvenChance].
• All the antecedents in SameSide inferences
have beliefs on “one side” of the consequent,
but at least one antecedent has the same belief
level as the consequent, e.g.,
A[VeryProbably] &amp; B[Possibly] implies
C[Possibly].
</bodyText>
<listItem confidence="0.5438125">
• All the antecedents in AlmostSame inferences
have beliefs on one side of the consequent, but
the closest antecedent is one level “up” from
the consequent, e.g.,
</listItem>
<bodyText confidence="0.998062176470588">
A[VeryProbably] &amp; B[Possibly] implies
C[EvenChance].
Our survey contained six evaluation sets, which
were done by 50 people. Each set contained an ini-
tial statement (we varied the polarity of the state-
ment in the various sets), three alternative argu-
ments that explain this statement, and the option
to say that no argument is a good explanation. The
respondents were asked to rank these options in
order of preference.
All the evaluation sets contained one argument
that was objectionable according to our prelim-
inary survey (there was an increase in belief or
a large change in belief from the antecedent to
the consequent). The two other arguments, each
of which comprises a single inference, were dis-
tributed among the six evaluation sets as follows.
</bodyText>
<listItem confidence="0.997886777777778">
• Three sets had one BothSides inference and
one SameSide inference, each with two an-
tecedents.
• Two sets had one SameSide inference, and
one AlmostSame inference, each with two an-
tecedents.
• One set had one SameSide inference with two
antecedents, and one BothSides inference com-
prising three antecedents.
</listItem>
<bodyText confidence="0.999947896551724">
In order to reduce the effect of the respondents’
domain bias, we generated two versions of the sur-
vey, where for each evaluation set we swapped the
antecedent propositions in one of the inferences
with the antecedent propositions in the other.
Our survey showed that people prefer BothSides
inferences (which contain antecedents for and
against the consequent). They also prefer Same-
Side to AlmostSame for antecedents with beliefs
in the negative range (VeryProbNot, ProbNot and
PossNot); and they did not distinguish between
SameSide and AlmostSame for antecedents with
beliefs in the positive range. Further, BothSides in-
ferences with three antecedents were preferred to
SameSide inferences with two antecedents. This
indicates that persuasiveness carries more weight
than parsimony.
These general preferences are incorporated into
our background knowledge as expectations for
a range of acceptable beliefs in the consequents
of inferences in light of their antecedents. The
farther the actual beliefs in the consequents are
from the expectations, the lower the probability
of these beliefs. Hence, it is no longer true that
Pr(bel IGZ|SCZ, struc IGZ) = 1 (Section 3.1), as
we now have a belief expectation that goes beyond
Bayesian propagation. As done at the start of Sec-
tion 3, the probability of the beliefs in an inter-
pretation is a function of the discrepancy between
</bodyText>
<page confidence="0.997681">
139
</page>
<bodyText confidence="0.9963245">
these beliefs and expected beliefs. We calculate
this probability using a variant of the Zipf distri-
bution adjusted for ranges of beliefs.
Explanatory extensions are added to an inter-
pretation in order to overcome these belief dis-
crepancies, yielding an expanded model that com-
prises the tuple ISCi, IGi, EEi}. Equation 2 now
becomes
</bodyText>
<equation confidence="0.9661435">
Pr(SCi, IGi, EEi|Arg) = (6)
α Pr(SCi, IGi, EEi) x Pr(Arg|SCi, IGi, EEi)
</equation>
<bodyText confidence="0.999693444444445">
We make simplifying assumptions similar to
those made in Section 3.1, i.e., given the interpre-
tation graph and supposition configuration, the be-
liefs in the argument depend only on the beliefs in
the interpretation, and the argument structure de-
pends only on the interpretation structure. These
assumptions, together with probabilistic manipu-
lations similar to those performed in Section 3.1,
yield
</bodyText>
<equation confidence="0.980372">
Pr(SCi, IGi, EEi|Arg) = (7)
α Pr(struc IGi|SCi)xPr(SCi)x
Pr(bel IGi|SCi, struc IGi, bel EEi, struc EEi)x
Pr(struc EEi|SCi, struc IGi, bel EEi)x
Pr(bel EEi|SCi, struc IGi, struc EEi)x
Pr(bel Arg|SCi, bel IGi)xPr(struc Arg|struc IGi)
</equation>
<bodyText confidence="0.999656083333333">
The calculation of the probability of an ex-
planatory extension is the same as the calcu-
lation for structural model complexity at the
start of Section 3. However, the nodes in
an explanatory extension are selected from the
nodes directly connected to the interpretation
graph. In addition, as for the basic model (Sec-
tion 3.1), the beliefs in the nodes in explana-
tory extensions are obtained algorithmically by
means of Bayesian propagation. Hence, there
is no discrepancy with expected beliefs, i.e.,
Pr(bel EEi|SCi, struc IGi, struc EEi) = 1.
Table 3 summarizes the calculation of these
probabilities (the elements that differ from the ba-
sic model and the enhanced model are boldfaced).
It also shows the trade-off between structural and
belief model complexity. Presenting explana-
tory extensions has a higher structural complex-
ity (lower probability) than not presenting them.
However, explanatory extensions can reduce the
numerical discrepancy between the beliefs in an
interpretation and the beliefs expected from the
background knowledge, thereby increasing the be-
lief probability of the interpretation. For instance,
</bodyText>
<tableCaption confidence="0.84896">
Table 3: Probability – Additional background
knowledge
</tableCaption>
<table confidence="0.9655145">
Model complexity (against background)
Pr(struc IGi|SCi) structural complexity
Pr(SCi) numerical discrepancy
↓ Pr(struc EEi|SCi,
struc IGi, bel EEi) ↑ structural complexity
↑ Pr(bel IGi|SCi, struc IGi,
bel EEi, struc EEi) ↓ numerical discrepancy
Data fit with model
Pr(struc Arg|struc IGi) structural discrepancy
Pr(bel Arg|SCi,bel IGi) numerical discrepancy
</table>
<tableCaption confidence="0.990431">
Table 4: Summary of Trade-offs
</tableCaption>
<table confidence="0.745444666666667">
↓ Pr model structure (IG) ↑ Pr struct. data fit
↓ Pr model belief (SC) ↑ Pr belief data fit
↓ Pr model structure (EE) ↑ Pr model belief
</table>
<bodyText confidence="0.9881618">
in the example in Figure 1, the added explanatory
extension eliminates the unacceptable jump in be-
lief.
Table 4 summarizes the trade-offs discussed in
this section.
</bodyText>
<sectionHeader confidence="0.997614" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999827666666667">
We evaluated separately each component of an
interpretation – interpretation graph, supposition
configuration and explanatory extensions.
</bodyText>
<subsectionHeader confidence="0.996543">
4.1 Interpretation graph
</subsectionHeader>
<bodyText confidence="0.999984923076923">
We prepared four evaluation sets, each of which
was done by about 20 people (Zukerman and
George, 2005). In three of the sets, the partici-
pants were given a simple argument and a few can-
didate interpretations (ranked highly by our sys-
tem). The fourth set featured a complex argument,
and only one interpretation (other candidates had
much lower probabilities). The participants were
asked to give each interpretation a score between
1 (Very UNreasonable) and 5 (Very reasonable).
Table 5 shows the results obtained for the inter-
pretation selected by our formalism for each set,
which was the top scoring interpretation. The first
</bodyText>
<tableCaption confidence="0.998298">
Table 5: Evaluation results: Interpretation graph
</tableCaption>
<table confidence="0.85602925">
Set # 1 2 3 4
Avg. score 3.38 3.68 3.35 4.00
Std. dev. 1.45 1.11 1.39 1.02
Stat. sig. (p) 0.08 0.15 0.07 NA
</table>
<page confidence="0.98746">
140
</page>
<bodyText confidence="0.999954846153846">
row shows the average score given by our subjects
to this interpretation, the second row shows the
standard deviation, and the third row the statistical
significance, derived using a paired Z-test against
alternative options (no alternatives were presented
for the fourth set). Our results show that the inter-
pretations generated by our system were generally
acceptable, but that some people gave low scores.
Our subjects’ feedback indicated that these scores
were mainly due to mismatches between beliefs in
the argument and in its interpretation, and due to
belief discontinuities. This led to the addition of
suppositions and explanatory extensions.
</bodyText>
<subsectionHeader confidence="0.997632">
4.2 Supposition configuration
</subsectionHeader>
<bodyText confidence="0.999996">
We prepared four evaluation sets, each of which
was done by 34 people (George et al., 2005). Each
set consisted of a short argument, plus a list of sup-
position options as follows: (a) four suppositions
that had a reasonably high probability according
to our formalism, (b) the option to make a free-
form supposition in line with the domain BN, and
(c) the option to suppose nothing. We then asked
our subjects to indicate which of these options was
required for the argument to make sense. Specif-
ically, they had to rank their preferred options in
order of preference (but they did not have to rank
options they disliked). Overall, there was strong
support for the supposition preferred by our for-
malism. In three of the evaluation sets, it was
ranked first by most of the trial subjects (30/34,
19/34, 20/34), with no other option a clear second.
Only in the fourth set, the supposition preferred by
our formalism was equal-first with another option,
but still was ranked first 10 times (out of 34).
</bodyText>
<subsectionHeader confidence="0.991366">
4.3 Explanatory extensions
</subsectionHeader>
<bodyText confidence="0.999994">
We constructed two evaluation sets, each of which
was done by 20 people. Each set consisted of a
short argument and two alternative interpretations
(with and without explanatory extensions). There
was strong support for the explanatory extensions
proposed by our formalism, with 57.5% of our
trial subjects favouring the interpretations with ex-
planatory extensions, compared to 37.5% of the
subjects who preferred the interpretations without
such extensions, and 5% who were indifferent.
</bodyText>
<sectionHeader confidence="0.999846" genericHeader="method">
5 Related Research
</sectionHeader>
<bodyText confidence="0.999885777777778">
An important aspect of discourse understanding
involves filling in information that was omitted by
the interlocutor. In this paper, we have presented
a probabilistic formalism that balances conflicting
factors when filling in three types of information
omitted from an argument. Interpretation graphs
fill in details in the argument’s inferences, sup-
position configurations make sense of the beliefs
in the argument, and explanatory extensions over-
come belief discontinuities.
Our approach resembles the work of Hobbs et
al. (1993) in several respects. They employed
an abductive approach where a model (interpre-
tation) is inferred from evidence (sentence); they
made assumptions as necessary; and used guid-
ing criteria pertaining to the model and the data
for choosing between candidate models. There are
also significant differences between our work and
theirs. Their interpretation focused on problems of
reference and disambiguation in single sentences,
while ours focuses on a longer discourse and the
relations between the propositions therein. This
distinction also determines the nature of the task,
as they try to find a concise model that explains
as much of the data as possible (e.g., one refer-
ent that fits many clues), while we try to find a
representation for a user’s argument. Additionally,
their domain knowledge is logic-based, while ours
is Bayesian; and they used weights to apply their
hypothesis selection criteria, while our criteria are
embodied in a probabilistic framework.
Plan recognition systems also generate one or
more interpretations of a user’s utterances, em-
ploying different resources to fill in information
omitted by the user, e.g., (Allen and Perrault,
1980; Litman and Allen, 1987; Carberry and Lam-
bert, 1999; Raskutti and Zukerman, 1991). These
plan recognition systems used a plan-based ap-
proach to propose interpretations. The first three
systems applied different types of heuristics to se-
lect an interpretation, while the fourth system used
a probabilistic approach moderated by heuristics
to select the interpretation with the highest prob-
ability. We use a probabilistic domain repre-
sentation in the form of a BN (rather than plan
libraries), and apply a probabilistic mechanism
that represents explicitly the contribution of back-
ground knowledge, model complexity and data fit
to the generation of an interpretation. Our mech-
anism, which can be applied to other domain rep-
resentations, balances different types of complex-
ities and discrepancies to select the interpretation
with the highest posterior probability.
Several researchers used maximum posterior
</bodyText>
<page confidence="0.995127">
141
</page>
<bodyText confidence="0.999977261904762">
probability as the criterion for selecting an inter-
pretation (Charniak and Goldman, 1993; Gertner
et al., 1998; Horvitz and Paek, 1999). They used
BNs to represent a probability distribution over the
set of possible explanations for the observed facts,
and selected the explanation (a node in the BN or
a value of a node) with the highest probability. We
also use BNs as our domain representation, but our
“explanation” of the facts (the user’s argument) is
a Bayesian subnet (rather than a single node) sup-
plemented by suppositions. Additionally, we cal-
culate the probability of an interpretation on the
basis of the fit between the argument and the inter-
pretation, and the complexity of the interpretation
in light of the background knowledge.
Our work on positing suppositions is related to
research on presuppositions (Kaplan, 1982; Gur-
ney et al., 1997) – a type of supposition implied
by the wording of a statement. Like our sup-
positions, presuppositions are necessary to make
sense of what is being said, but they operate at
a different knowledge level than our suppositions.
This aspect of our work is also related to research
on the recognition of flawed plans (Quilici, 1989;
Pollack, 1990; Chu-Carroll and Carberry, 2000).
These researchers used a plan-based approach to
identify erroneous beliefs that account for a user’s
statements or plan, while we use a probabilistic ap-
proach. Our approach supports the consideration
of many possible options, and integrates supposi-
tions into a broader reasoning context.
Finally, the research reported in (Joshi et al.,
1984; van Beek, 1987; Zukerman and Mc-
Conachy, 2001) considers the addition of informa-
tion to planned discourse to prevent a user’s erro-
neous inferences from this discourse. Our mech-
anism adds explanatory extensions to an interpre-
tation to prevent inferences that are objectionable
due to discontinuities in belief. Since such non-
sequiturs may also be present in system-generated
arguments, the approach presented here may be in-
corporated into argument-generation systems.
</bodyText>
<sectionHeader confidence="0.99977" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999710625">
We have offered a probabilistic approach to the in-
terpretation of arguments that casts the selection
of an interpretation as a model selection task. In
so doing, our formalism balances conflicting fac-
tors: model complexity against data fit, and struc-
ture complexity against belief reasonableness. We
have demonstrated the use of our basic formalism
for the selection of an interpretation graph, and
shown how a more complex model and additional
background knowledge account respectively for
the inclusion of suppositions and explanatory ex-
tensions in an interpretation. Our user evaluations
show that the interpretation graphs produced by
our formalism are generally acceptable, and that
there is strong support for the suppositions and ex-
planatory extensions it proposes.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999937925">
J.F. Allen and C.R. Perrault. 1980. Analyzing inten-
tion in utterances. Artificial Intelligence, 15(3):143–
178.
J. R. Anderson. 1983. The Architecture of Cogni-
tion. Harvard University Press, Cambridge, Mas-
sachusetts.
G.E.P. Box, G.M. Jenkins, and G.C. Reinsel. 1994.
Time Series Analysis: Forecasting and Control.
Prentice Hall.
S. Carberry and L. Lambert. 1999. A process model
for recognizing communicative acts and modeling
negotiation subdialogues. Computational Linguis-
tics, 25(1):1–53.
E. Charniak and R. Goldman. 1993. A Bayesian
model of plan recognition. Artificial Intelligence,
64(1):53–79.
J. Chu-Carroll and S. Carberry. 2000. Conflict res-
olution in collaborative planning dialogues. In-
ternational Journal of Human Computer Studies,
6(56):969–1015.
C. Elsaesser. 1987. Explanation of probabilistic infer-
ence for decision support systems. In Proceedings
of the AAAI-87 Workshop on Uncertainty in Artifi-
cial Intelligence, pages 394–403, Seattle, Washing-
ton.
M.E. Epstein. 1996. Statistical Source Channel Mod-
els for Natural Language Understanding. Ph.D. the-
sis, Department of Computer Science, New York
University, New York, New York.
S. George, I. Zukerman, and M. Niemann. 2004. An
anytime algorithm for interpreting arguments. In
PRICAI2004 – Proceedings of the Eighth Pacific
Rim International Conference on Artificial Intelli-
gence, pages 311–321, Auckland, New Zealand.
S. George, I. Zukerman, and M. Niemann. 2005. Mod-
eling suppositions in users’ arguments. In UM05 –
Proceedings of the 10th International Conference on
User Modeling, pages 19–29, Edinburgh, Scotland.
A. Gertner, C. Conati, and K. VanLehn. 1998. Pro-
cedural help in Andes: Generating hints using a
</reference>
<page confidence="0.978589">
142
</page>
<reference confidence="0.999088120689656">
Bayesian network student model. In AAAI98 – Pro-
ceedings of the Fifteenth National Conference on Ar-
tificial Intelligence, pages 106–111, Madison, Wis-
consin.
J. Gurney, D. Perlis, and K. Purang. 1997. Interpreting
presuppositions using active logic: From contexts to
utterances. Computational Intelligence, 13(3):391–
413.
J. R. Hobbs, M. E. Stickel, D. E. Appelt, and P. Martin.
1993. Interpretation as abduction. Artificial Intelli-
gence, 63(1-2):69–142.
E. Horvitz and T. Paek. 1999. A computational archi-
tecture for conversation. In UM99 – Proceedings of
the Seventh International Conference on User Mod-
eling, pages 201–210, Banff, Canada.
A. Joshi, B. L. Webber, and R. M. Weischedel. 1984.
Living up to expectations: Computing expert re-
sponses. In AAAI84 – Proceedings of the Fourth Na-
tional Conference on Artificial Intelligence, pages
169–175, Austin, Texas.
S. J. Kaplan. 1982. Cooperative responses from a
portable natural language query system. Artificial
Intelligence, 19:165–187.
D. Litman and J.F. Allen. 1987. A plan recognition
model for subdialogues in conversation. Cognitive
Science, 11(2):163–200.
F.J. Och and H. Ney. 2002. Discriminative training and
maximum entropy models for statistical machine
translation. In ACL’02 – Proceedings of the An-
nual Meeting of the Association for Computational
Linguistics, pages 295–302, Philadelphia, Pennsyl-
vania.
M.E. Pollack. 1990. Plans as complex mental atti-
tudes. In P. Cohen, J. Morgan, and M.E. Pollack, ed-
itors, Intentions in Communication, pages 77–103.
MIT Press.
A. Quilici. 1989. Detecting and responding to
plan-oriented misconceptions. In A. Kobsa and
W. Wahlster, editors, User Models in Dialog Sys-
tems, pages 108–132. Springer-Verlag.
B. Raskutti and I. Zukerman. 1991. Generation and se-
lection of likely interpretations during plan recogni-
tion. User Modeling and User Adapted Interaction,
1(4):323–353.
P. van Beek. 1987. A model for generating better ex-
planations. In Proceedings of the Twenty-Fifth An-
nual Meeting of the Association for Computational
Linguistics, pages 215–220, Stanford, California.
C.S. Wallace. 2005. Statistical and Inductive Inference
by Minimum Message Length. Springer, Berlin,
Germany.
I. Zukerman and S. George. 2005. A probabilistic
approach for argument interpretation. User Model-
ing and User-Adapted Interaction, Special Issue on
Language-Based Interaction, 15(1-2):5–53.
I. Zukerman and R. McConachy. 2001. WISHFUL:
A discourse planning system that considers a user’s
inferences. Computational Intelligence, 1(17):1–61.
</reference>
<page confidence="0.999132">
143
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.619024">
<title confidence="0.999774">Balancing Conflicting Factors in Argument Interpretation</title>
<author confidence="0.901298">Ingrid Zukerman</author>
<author confidence="0.901298">Michael Niemann</author>
<author confidence="0.901298">Sarah</author>
<affiliation confidence="0.795">Faculty of Information Monash</affiliation>
<address confidence="0.979346">Clayton, VICTORIA 3800,</address>
<abstract confidence="0.99629235">We present a probabilistic approach for the interpretation of arguments that casts the selection of an interpretation as a model selection task. In selecting the best model, our formalism balances conflicting factors: model complexity against data fit, and structure complexity against belief reasonableness. We first describe our basic formalism, which considers interpretations comprising inferential relations, and then show how our formalism is extended to suppositions that account for the beliefs in an argument, and justifications that account for the inferences in an interpretation. Our evaluations with users show that the interpretations produced by our system are acceptable, and that there is strong support for the postulated suppositions and justifications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J F Allen</author>
<author>C R Perrault</author>
</authors>
<title>Analyzing intention in utterances.</title>
<date>1980</date>
<journal>Artificial Intelligence,</journal>
<volume>15</volume>
<issue>3</issue>
<pages>178</pages>
<contexts>
<context position="33059" citStr="Allen and Perrault, 1980" startWordPosition="5205" endWordPosition="5208">determines the nature of the task, as they try to find a concise model that explains as much of the data as possible (e.g., one referent that fits many clues), while we try to find a representation for a user’s argument. Additionally, their domain knowledge is logic-based, while ours is Bayesian; and they used weights to apply their hypothesis selection criteria, while our criteria are embodied in a probabilistic framework. Plan recognition systems also generate one or more interpretations of a user’s utterances, employing different resources to fill in information omitted by the user, e.g., (Allen and Perrault, 1980; Litman and Allen, 1987; Carberry and Lambert, 1999; Raskutti and Zukerman, 1991). These plan recognition systems used a plan-based approach to propose interpretations. The first three systems applied different types of heuristics to select an interpretation, while the fourth system used a probabilistic approach moderated by heuristics to select the interpretation with the highest probability. We use a probabilistic domain representation in the form of a BN (rather than plan libraries), and apply a probabilistic mechanism that represents explicitly the contribution of background knowledge, mo</context>
</contexts>
<marker>Allen, Perrault, 1980</marker>
<rawString>J.F. Allen and C.R. Perrault. 1980. Analyzing intention in utterances. Artificial Intelligence, 15(3):143– 178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Anderson</author>
</authors>
<title>The Architecture of Cognition.</title>
<date>1983</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<contexts>
<context position="20397" citStr="Anderson, 1983" startWordPosition="3220" endWordPosition="3221">odel. 3.3 Additional background knowledge An increase in our background knowledge means that we take into account additional factors about the world. This extra knowledge in turn may cause us to prefer interpretations that were previously discarded. We have considered two additions to background knowledge: dialogue history, and users’ preferences regarding inference patterns. Dialogue history Dialogue history influences the salience of a node, and hence the probability that it was included in a user’s argument. We have modeled salience by means of an activation function that decays with time (Anderson, 1983), and used this function to moderate the probability of including a node in an interpretation (instead of using a uniform distribution). We have experimented with two activation functions: (1) a function where the level of activation of a node is based on the frequency and recency of the direct activation of this node; and (2) a function where the level of activation of a node depends on its similarity with all the (activated) nodes, together with the frequency and recency of their activation (Zukerman and George, 138 2005). To illustrate the influence of salience, compare the preferred interp</context>
</contexts>
<marker>Anderson, 1983</marker>
<rawString>J. R. Anderson. 1983. The Architecture of Cognition. Harvard University Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G E P Box</author>
<author>G M Jenkins</author>
<author>G C Reinsel</author>
</authors>
<title>Time Series Analysis: Forecasting and Control.</title>
<date>1994</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="3757" citStr="Box et al., 1994" startWordPosition="562" endWordPosition="565">we first describe our basic formalism, which is used to calculate the probability of interpretations that include only inferences, and then show how progressive enhancements of this formalism are used for more informative interpretations. In Section 2, we explain what is an argument interpretation, and describe briefly the interpretation process. Next, we discuss our probabilistic formalism for selecting an interpretation, which is the focus of this paper. In Section 4, we present &apos;Other model selection criteria such as Akaike Information Criterion (AIC) and Bayes Information Criterion (BIC) (Box et al., 1994) also argue for model parsimony, but they do so by penalizing models with more free parameters. 134 Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 134–143, Sydney, July 2006. c�2006 Association for Computational Linguistics the results of our evaluations, followed by a discussion of related work, and concluding remarks. 2 Argument interpretation We define an interpretation of a user’s argument as the tuple {5C, IG, EE}, where 5C is a supposition configuration, IG is an interpretation graph, and EE are explanatory extensions. • A Supposition Configuration is a set of s</context>
</contexts>
<marker>Box, Jenkins, Reinsel, 1994</marker>
<rawString>G.E.P. Box, G.M. Jenkins, and G.C. Reinsel. 1994. Time Series Analysis: Forecasting and Control. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Carberry</author>
<author>L Lambert</author>
</authors>
<title>A process model for recognizing communicative acts and modeling negotiation subdialogues.</title>
<date>1999</date>
<journal>Computational Linguistics,</journal>
<volume>25</volume>
<issue>1</issue>
<contexts>
<context position="33111" citStr="Carberry and Lambert, 1999" startWordPosition="5213" endWordPosition="5217">find a concise model that explains as much of the data as possible (e.g., one referent that fits many clues), while we try to find a representation for a user’s argument. Additionally, their domain knowledge is logic-based, while ours is Bayesian; and they used weights to apply their hypothesis selection criteria, while our criteria are embodied in a probabilistic framework. Plan recognition systems also generate one or more interpretations of a user’s utterances, employing different resources to fill in information omitted by the user, e.g., (Allen and Perrault, 1980; Litman and Allen, 1987; Carberry and Lambert, 1999; Raskutti and Zukerman, 1991). These plan recognition systems used a plan-based approach to propose interpretations. The first three systems applied different types of heuristics to select an interpretation, while the fourth system used a probabilistic approach moderated by heuristics to select the interpretation with the highest probability. We use a probabilistic domain representation in the form of a BN (rather than plan libraries), and apply a probabilistic mechanism that represents explicitly the contribution of background knowledge, model complexity and data fit to the generation of an </context>
</contexts>
<marker>Carberry, Lambert, 1999</marker>
<rawString>S. Carberry and L. Lambert. 1999. A process model for recognizing communicative acts and modeling negotiation subdialogues. Computational Linguistics, 25(1):1–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>R Goldman</author>
</authors>
<title>A Bayesian model of plan recognition.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<volume>64</volume>
<issue>1</issue>
<contexts>
<context position="34059" citStr="Charniak and Goldman, 1993" startWordPosition="5354" endWordPosition="5357">he highest probability. We use a probabilistic domain representation in the form of a BN (rather than plan libraries), and apply a probabilistic mechanism that represents explicitly the contribution of background knowledge, model complexity and data fit to the generation of an interpretation. Our mechanism, which can be applied to other domain representations, balances different types of complexities and discrepancies to select the interpretation with the highest posterior probability. Several researchers used maximum posterior 141 probability as the criterion for selecting an interpretation (Charniak and Goldman, 1993; Gertner et al., 1998; Horvitz and Paek, 1999). They used BNs to represent a probability distribution over the set of possible explanations for the observed facts, and selected the explanation (a node in the BN or a value of a node) with the highest probability. We also use BNs as our domain representation, but our “explanation” of the facts (the user’s argument) is a Bayesian subnet (rather than a single node) supplemented by suppositions. Additionally, we calculate the probability of an interpretation on the basis of the fit between the argument and the interpretation, and the complexity of</context>
</contexts>
<marker>Charniak, Goldman, 1993</marker>
<rawString>E. Charniak and R. Goldman. 1993. A Bayesian model of plan recognition. Artificial Intelligence, 64(1):53–79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Chu-Carroll</author>
<author>S Carberry</author>
</authors>
<title>Conflict resolution in collaborative planning dialogues.</title>
<date>2000</date>
<journal>International Journal of Human Computer Studies,</journal>
<volume>6</volume>
<issue>56</issue>
<contexts>
<context position="35202" citStr="Chu-Carroll and Carberry, 2000" startWordPosition="5543" endWordPosition="5546"> the basis of the fit between the argument and the interpretation, and the complexity of the interpretation in light of the background knowledge. Our work on positing suppositions is related to research on presuppositions (Kaplan, 1982; Gurney et al., 1997) – a type of supposition implied by the wording of a statement. Like our suppositions, presuppositions are necessary to make sense of what is being said, but they operate at a different knowledge level than our suppositions. This aspect of our work is also related to research on the recognition of flawed plans (Quilici, 1989; Pollack, 1990; Chu-Carroll and Carberry, 2000). These researchers used a plan-based approach to identify erroneous beliefs that account for a user’s statements or plan, while we use a probabilistic approach. Our approach supports the consideration of many possible options, and integrates suppositions into a broader reasoning context. Finally, the research reported in (Joshi et al., 1984; van Beek, 1987; Zukerman and McConachy, 2001) considers the addition of information to planned discourse to prevent a user’s erroneous inferences from this discourse. Our mechanism adds explanatory extensions to an interpretation to prevent inferences tha</context>
</contexts>
<marker>Chu-Carroll, Carberry, 2000</marker>
<rawString>J. Chu-Carroll and S. Carberry. 2000. Conflict resolution in collaborative planning dialogues. International Journal of Human Computer Studies, 6(56):969–1015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Elsaesser</author>
</authors>
<title>Explanation of probabilistic inference for decision support systems.</title>
<date>1987</date>
<booktitle>In Proceedings of the AAAI-87 Workshop on Uncertainty in Artificial Intelligence,</booktitle>
<pages>394--403</pages>
<location>Seattle, Washington.</location>
<contexts>
<context position="6202" citStr="Elsaesser, 1987" startWordPosition="963" endWordPosition="964">; the probabilities of some nodes are indicated with linguistic terms.2 The interpretation graph, which appears inside a light gray bubble in the BN excerpt, includes the extra node GreenInGardenAtTimeOfDeath (boxed). Note that the propagated beliefs in this interpretation graph do not match those in the argument. To address this problem, the system supposes that the user believes that TimeOfDeath11=TRUE, instead of the BN belief of Probably (boldfaced and 2We use the terms Very Probable, Probable, Possible and their negations, and Even Chance. These terms, which are similar to those used in (Elsaesser, 1987), are most consistently understood by people according to our user surveys. ARGUMENT Mr Green probably being in the garden at 11 implies that he possibly had the opportunity to kill Mr Body, but he did not murder Mr Body. possibly EXCERPT OF DOMAIN BN INTERPRETATION Mr Green being in the garden at 11, and probably supposing that the time of death is 11 implies that Mr Green probably was in the garden at the time of death. Hence, he possibly had the opportunity to kill Mr Body, but Mr Green probably did not have the means. Therefore, he did murder Mr Body. possibly not Figure 1: Sample argument</context>
</contexts>
<marker>Elsaesser, 1987</marker>
<rawString>C. Elsaesser. 1987. Explanation of probabilistic inference for decision support systems. In Proceedings of the AAAI-87 Workshop on Uncertainty in Artificial Intelligence, pages 394–403, Seattle, Washington.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Epstein</author>
</authors>
<title>Statistical Source Channel Models for Natural Language Understanding.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, New York University,</institution>
<location>New York, New York.</location>
<contexts>
<context position="1177" citStr="Epstein, 1996" startWordPosition="160" endWordPosition="161">escribe our basic formalism, which considers interpretations comprising inferential relations, and then show how our formalism is extended to suppositions that account for the beliefs in an argument, and justifications that account for the inferences in an interpretation. Our evaluations with users show that the interpretations produced by our system are acceptable, and that there is strong support for the postulated suppositions and justifications. 1 Introduction The source-channel approach has been often used for word-based language tasks, such as speech recognition and machine translation (Epstein, 1996; Och and Ney, 2002). According to this approach, an addressee receives a noisy channel (language or speech wave), and decodes this channel to derive the source (idea). The selected source is that with the maximum posterior probability. In this paper, we apply the source-channel approach to the interpretation of arguments. This approach enables us to cast argument interpretation as a trade-off between conflicting factors, viz model complexity against data fit, and structure complexity against belief reasonableness. This trade-off is inspired by the Minimum Message Length (MML) Criterion – a mo</context>
</contexts>
<marker>Epstein, 1996</marker>
<rawString>M.E. Epstein. 1996. Statistical Source Channel Models for Natural Language Understanding. Ph.D. thesis, Department of Computer Science, New York University, New York, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S George</author>
<author>I Zukerman</author>
<author>M Niemann</author>
</authors>
<title>An anytime algorithm for interpreting arguments.</title>
<date>2004</date>
<booktitle>In PRICAI2004 – Proceedings of the Eighth Pacific Rim International Conference on Artificial Intelligence,</booktitle>
<pages>311--321</pages>
<location>Auckland, New Zealand.</location>
<contexts>
<context position="7897" citStr="George et al., 2004" startWordPosition="1246" endWordPosition="1249">the supposition). This prompts the generation of the explanatory extension GreenHadMeans[ProbablyNot] (white boldfaced and darkgray boxed). The three elements added during the interpretation process – the extra node in the interpretation graph, the supposition and the explanatory extension – appear in boldface italics in the interpretation at the bottom of the figure. 2.1 Proposing Interpretations The problem of finding the best interpretation is exponential. In previous work, we proposed an anytime algorithm to propose interpretation graphs and supposition configurations until time runs out (George et al., 2004). Here we apply our algorithm to generate interpretations comprising supposition configurations (5C), interpretation graphs (IG) and explanatory extensions (EE) (Figure 2). Supposition configurations are proposed first, as instantiated beliefs affect the plausibility of inter. . . GreenHadMeans GreenLadder AtWindow GreenMurderedBody ProbablyNot ProbablyNot EvenChance TimeOfDeath11 GreenHadOpportunity Probably GreenInGardenAt TimeOfDeath . . . ProbablyNot GreenInGardenAt11 NbourHeardGreen&amp;Body ArgueLastNight Probably GreenHadMotive GreenVisitBody LastNight 135 Algorithm GenerateInterpretations(</context>
<context position="9627" citStr="George et al., 2004" startWordPosition="1485" endWordPosition="1488">rpretations pretation graphs, which in turn affect the need for explanatory extensions. The proposal of supposition configurations, interpretation graphs and explanatory extensions is driven by the probability of these components. In each iteration, we generate candidates for a component, calculate the probability of these candidates in the context of the selections made in the previous steps, and probabilistically select one of these candidates. That is, higher probability candidates have a better chance of being selected than lower probability ones (our selection procedures are described in George et al., 2004). For example, say that in Step 1, we selected supposition configuration SCa. Next, in Step 2, the probability of candidate IGs is calculated in the context of the domain BN and SCa, and one of the IGs is probabilistically selected, say IGb. Similarly, in Step 3, one of the candidate EEs is selected in the context of SCa and IGb. In the next iteration, we probabilistically select an SC (which could be a previously chosen one), and so on. To generate diverse interpretations, if SCa is selected again, a different IG will be chosen. 3 Probabilistic formalism Following (Wallace, 2005), our approac</context>
</contexts>
<marker>George, Zukerman, Niemann, 2004</marker>
<rawString>S. George, I. Zukerman, and M. Niemann. 2004. An anytime algorithm for interpreting arguments. In PRICAI2004 – Proceedings of the Eighth Pacific Rim International Conference on Artificial Intelligence, pages 311–321, Auckland, New Zealand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S George</author>
<author>I Zukerman</author>
<author>M Niemann</author>
</authors>
<title>Modeling suppositions in users’ arguments.</title>
<date>2005</date>
<booktitle>In UM05 – Proceedings of the 10th International Conference on User Modeling,</booktitle>
<pages>pages</pages>
<location>Edinburgh, Scotland.</location>
<contexts>
<context position="29919" citStr="George et al., 2005" startWordPosition="4712" endWordPosition="4715">ignificance, derived using a paired Z-test against alternative options (no alternatives were presented for the fourth set). Our results show that the interpretations generated by our system were generally acceptable, but that some people gave low scores. Our subjects’ feedback indicated that these scores were mainly due to mismatches between beliefs in the argument and in its interpretation, and due to belief discontinuities. This led to the addition of suppositions and explanatory extensions. 4.2 Supposition configuration We prepared four evaluation sets, each of which was done by 34 people (George et al., 2005). Each set consisted of a short argument, plus a list of supposition options as follows: (a) four suppositions that had a reasonably high probability according to our formalism, (b) the option to make a freeform supposition in line with the domain BN, and (c) the option to suppose nothing. We then asked our subjects to indicate which of these options was required for the argument to make sense. Specifically, they had to rank their preferred options in order of preference (but they did not have to rank options they disliked). Overall, there was strong support for the supposition preferred by ou</context>
</contexts>
<marker>George, Zukerman, Niemann, 2005</marker>
<rawString>S. George, I. Zukerman, and M. Niemann. 2005. Modeling suppositions in users’ arguments. In UM05 – Proceedings of the 10th International Conference on User Modeling, pages 19–29, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gertner</author>
<author>C Conati</author>
<author>K VanLehn</author>
</authors>
<title>Procedural help in Andes: Generating hints using a Bayesian network student model.</title>
<date>1998</date>
<booktitle>In AAAI98 – Proceedings of the Fifteenth National Conference on Artificial Intelligence,</booktitle>
<pages>106--111</pages>
<location>Madison, Wisconsin.</location>
<contexts>
<context position="34081" citStr="Gertner et al., 1998" startWordPosition="5358" endWordPosition="5361">se a probabilistic domain representation in the form of a BN (rather than plan libraries), and apply a probabilistic mechanism that represents explicitly the contribution of background knowledge, model complexity and data fit to the generation of an interpretation. Our mechanism, which can be applied to other domain representations, balances different types of complexities and discrepancies to select the interpretation with the highest posterior probability. Several researchers used maximum posterior 141 probability as the criterion for selecting an interpretation (Charniak and Goldman, 1993; Gertner et al., 1998; Horvitz and Paek, 1999). They used BNs to represent a probability distribution over the set of possible explanations for the observed facts, and selected the explanation (a node in the BN or a value of a node) with the highest probability. We also use BNs as our domain representation, but our “explanation” of the facts (the user’s argument) is a Bayesian subnet (rather than a single node) supplemented by suppositions. Additionally, we calculate the probability of an interpretation on the basis of the fit between the argument and the interpretation, and the complexity of the interpretation in</context>
</contexts>
<marker>Gertner, Conati, VanLehn, 1998</marker>
<rawString>A. Gertner, C. Conati, and K. VanLehn. 1998. Procedural help in Andes: Generating hints using a Bayesian network student model. In AAAI98 – Proceedings of the Fifteenth National Conference on Artificial Intelligence, pages 106–111, Madison, Wisconsin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gurney</author>
<author>D Perlis</author>
<author>K Purang</author>
</authors>
<title>Interpreting presuppositions using active logic: From contexts to utterances.</title>
<date>1997</date>
<journal>Computational Intelligence,</journal>
<volume>13</volume>
<issue>3</issue>
<pages>413</pages>
<contexts>
<context position="34828" citStr="Gurney et al., 1997" startWordPosition="5480" endWordPosition="5484">observed facts, and selected the explanation (a node in the BN or a value of a node) with the highest probability. We also use BNs as our domain representation, but our “explanation” of the facts (the user’s argument) is a Bayesian subnet (rather than a single node) supplemented by suppositions. Additionally, we calculate the probability of an interpretation on the basis of the fit between the argument and the interpretation, and the complexity of the interpretation in light of the background knowledge. Our work on positing suppositions is related to research on presuppositions (Kaplan, 1982; Gurney et al., 1997) – a type of supposition implied by the wording of a statement. Like our suppositions, presuppositions are necessary to make sense of what is being said, but they operate at a different knowledge level than our suppositions. This aspect of our work is also related to research on the recognition of flawed plans (Quilici, 1989; Pollack, 1990; Chu-Carroll and Carberry, 2000). These researchers used a plan-based approach to identify erroneous beliefs that account for a user’s statements or plan, while we use a probabilistic approach. Our approach supports the consideration of many possible options</context>
</contexts>
<marker>Gurney, Perlis, Purang, 1997</marker>
<rawString>J. Gurney, D. Perlis, and K. Purang. 1997. Interpreting presuppositions using active logic: From contexts to utterances. Computational Intelligence, 13(3):391– 413.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Hobbs</author>
<author>M E Stickel</author>
<author>D E Appelt</author>
<author>P Martin</author>
</authors>
<title>Interpretation as abduction.</title>
<date>1993</date>
<journal>Artificial Intelligence,</journal>
<pages>63--1</pages>
<contexts>
<context position="31893" citStr="Hobbs et al. (1993)" startWordPosition="5025" endWordPosition="5028">nterpretations without such extensions, and 5% who were indifferent. 5 Related Research An important aspect of discourse understanding involves filling in information that was omitted by the interlocutor. In this paper, we have presented a probabilistic formalism that balances conflicting factors when filling in three types of information omitted from an argument. Interpretation graphs fill in details in the argument’s inferences, supposition configurations make sense of the beliefs in the argument, and explanatory extensions overcome belief discontinuities. Our approach resembles the work of Hobbs et al. (1993) in several respects. They employed an abductive approach where a model (interpretation) is inferred from evidence (sentence); they made assumptions as necessary; and used guiding criteria pertaining to the model and the data for choosing between candidate models. There are also significant differences between our work and theirs. Their interpretation focused on problems of reference and disambiguation in single sentences, while ours focuses on a longer discourse and the relations between the propositions therein. This distinction also determines the nature of the task, as they try to find a c</context>
</contexts>
<marker>Hobbs, Stickel, Appelt, Martin, 1993</marker>
<rawString>J. R. Hobbs, M. E. Stickel, D. E. Appelt, and P. Martin. 1993. Interpretation as abduction. Artificial Intelligence, 63(1-2):69–142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Horvitz</author>
<author>T Paek</author>
</authors>
<title>A computational architecture for conversation.</title>
<date>1999</date>
<booktitle>In UM99 – Proceedings of the Seventh International Conference on User Modeling,</booktitle>
<pages>201--210</pages>
<location>Banff, Canada.</location>
<contexts>
<context position="34106" citStr="Horvitz and Paek, 1999" startWordPosition="5362" endWordPosition="5365">ain representation in the form of a BN (rather than plan libraries), and apply a probabilistic mechanism that represents explicitly the contribution of background knowledge, model complexity and data fit to the generation of an interpretation. Our mechanism, which can be applied to other domain representations, balances different types of complexities and discrepancies to select the interpretation with the highest posterior probability. Several researchers used maximum posterior 141 probability as the criterion for selecting an interpretation (Charniak and Goldman, 1993; Gertner et al., 1998; Horvitz and Paek, 1999). They used BNs to represent a probability distribution over the set of possible explanations for the observed facts, and selected the explanation (a node in the BN or a value of a node) with the highest probability. We also use BNs as our domain representation, but our “explanation” of the facts (the user’s argument) is a Bayesian subnet (rather than a single node) supplemented by suppositions. Additionally, we calculate the probability of an interpretation on the basis of the fit between the argument and the interpretation, and the complexity of the interpretation in light of the background </context>
</contexts>
<marker>Horvitz, Paek, 1999</marker>
<rawString>E. Horvitz and T. Paek. 1999. A computational architecture for conversation. In UM99 – Proceedings of the Seventh International Conference on User Modeling, pages 201–210, Banff, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>B L Webber</author>
<author>R M Weischedel</author>
</authors>
<title>Living up to expectations: Computing expert responses.</title>
<date>1984</date>
<booktitle>In AAAI84 – Proceedings of the Fourth National Conference on Artificial Intelligence,</booktitle>
<pages>169--175</pages>
<location>Austin, Texas.</location>
<contexts>
<context position="35545" citStr="Joshi et al., 1984" startWordPosition="5595" endWordPosition="5598">ons are necessary to make sense of what is being said, but they operate at a different knowledge level than our suppositions. This aspect of our work is also related to research on the recognition of flawed plans (Quilici, 1989; Pollack, 1990; Chu-Carroll and Carberry, 2000). These researchers used a plan-based approach to identify erroneous beliefs that account for a user’s statements or plan, while we use a probabilistic approach. Our approach supports the consideration of many possible options, and integrates suppositions into a broader reasoning context. Finally, the research reported in (Joshi et al., 1984; van Beek, 1987; Zukerman and McConachy, 2001) considers the addition of information to planned discourse to prevent a user’s erroneous inferences from this discourse. Our mechanism adds explanatory extensions to an interpretation to prevent inferences that are objectionable due to discontinuities in belief. Since such nonsequiturs may also be present in system-generated arguments, the approach presented here may be incorporated into argument-generation systems. 6 Conclusion We have offered a probabilistic approach to the interpretation of arguments that casts the selection of an interpretati</context>
</contexts>
<marker>Joshi, Webber, Weischedel, 1984</marker>
<rawString>A. Joshi, B. L. Webber, and R. M. Weischedel. 1984. Living up to expectations: Computing expert responses. In AAAI84 – Proceedings of the Fourth National Conference on Artificial Intelligence, pages 169–175, Austin, Texas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Kaplan</author>
</authors>
<title>Cooperative responses from a portable natural language query system.</title>
<date>1982</date>
<journal>Artificial Intelligence,</journal>
<pages>19--165</pages>
<contexts>
<context position="34806" citStr="Kaplan, 1982" startWordPosition="5478" endWordPosition="5479">tions for the observed facts, and selected the explanation (a node in the BN or a value of a node) with the highest probability. We also use BNs as our domain representation, but our “explanation” of the facts (the user’s argument) is a Bayesian subnet (rather than a single node) supplemented by suppositions. Additionally, we calculate the probability of an interpretation on the basis of the fit between the argument and the interpretation, and the complexity of the interpretation in light of the background knowledge. Our work on positing suppositions is related to research on presuppositions (Kaplan, 1982; Gurney et al., 1997) – a type of supposition implied by the wording of a statement. Like our suppositions, presuppositions are necessary to make sense of what is being said, but they operate at a different knowledge level than our suppositions. This aspect of our work is also related to research on the recognition of flawed plans (Quilici, 1989; Pollack, 1990; Chu-Carroll and Carberry, 2000). These researchers used a plan-based approach to identify erroneous beliefs that account for a user’s statements or plan, while we use a probabilistic approach. Our approach supports the consideration of</context>
</contexts>
<marker>Kaplan, 1982</marker>
<rawString>S. J. Kaplan. 1982. Cooperative responses from a portable natural language query system. Artificial Intelligence, 19:165–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Litman</author>
<author>J F Allen</author>
</authors>
<title>A plan recognition model for subdialogues in conversation.</title>
<date>1987</date>
<journal>Cognitive Science,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="33083" citStr="Litman and Allen, 1987" startWordPosition="5209" endWordPosition="5212">he task, as they try to find a concise model that explains as much of the data as possible (e.g., one referent that fits many clues), while we try to find a representation for a user’s argument. Additionally, their domain knowledge is logic-based, while ours is Bayesian; and they used weights to apply their hypothesis selection criteria, while our criteria are embodied in a probabilistic framework. Plan recognition systems also generate one or more interpretations of a user’s utterances, employing different resources to fill in information omitted by the user, e.g., (Allen and Perrault, 1980; Litman and Allen, 1987; Carberry and Lambert, 1999; Raskutti and Zukerman, 1991). These plan recognition systems used a plan-based approach to propose interpretations. The first three systems applied different types of heuristics to select an interpretation, while the fourth system used a probabilistic approach moderated by heuristics to select the interpretation with the highest probability. We use a probabilistic domain representation in the form of a BN (rather than plan libraries), and apply a probabilistic mechanism that represents explicitly the contribution of background knowledge, model complexity and data </context>
</contexts>
<marker>Litman, Allen, 1987</marker>
<rawString>D. Litman and J.F. Allen. 1987. A plan recognition model for subdialogues in conversation. Cognitive Science, 11(2):163–200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In ACL’02 – Proceedings of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="1197" citStr="Och and Ney, 2002" startWordPosition="162" endWordPosition="165">ic formalism, which considers interpretations comprising inferential relations, and then show how our formalism is extended to suppositions that account for the beliefs in an argument, and justifications that account for the inferences in an interpretation. Our evaluations with users show that the interpretations produced by our system are acceptable, and that there is strong support for the postulated suppositions and justifications. 1 Introduction The source-channel approach has been often used for word-based language tasks, such as speech recognition and machine translation (Epstein, 1996; Och and Ney, 2002). According to this approach, an addressee receives a noisy channel (language or speech wave), and decodes this channel to derive the source (idea). The selected source is that with the maximum posterior probability. In this paper, we apply the source-channel approach to the interpretation of arguments. This approach enables us to cast argument interpretation as a trade-off between conflicting factors, viz model complexity against data fit, and structure complexity against belief reasonableness. This trade-off is inspired by the Minimum Message Length (MML) Criterion – a model selection method</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>F.J. Och and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In ACL’02 – Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 295–302, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Pollack</author>
</authors>
<title>Plans as complex mental attitudes.</title>
<date>1990</date>
<booktitle>Intentions in Communication,</booktitle>
<pages>77--103</pages>
<editor>In P. Cohen, J. Morgan, and M.E. Pollack, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="35169" citStr="Pollack, 1990" startWordPosition="5541" endWordPosition="5542">terpretation on the basis of the fit between the argument and the interpretation, and the complexity of the interpretation in light of the background knowledge. Our work on positing suppositions is related to research on presuppositions (Kaplan, 1982; Gurney et al., 1997) – a type of supposition implied by the wording of a statement. Like our suppositions, presuppositions are necessary to make sense of what is being said, but they operate at a different knowledge level than our suppositions. This aspect of our work is also related to research on the recognition of flawed plans (Quilici, 1989; Pollack, 1990; Chu-Carroll and Carberry, 2000). These researchers used a plan-based approach to identify erroneous beliefs that account for a user’s statements or plan, while we use a probabilistic approach. Our approach supports the consideration of many possible options, and integrates suppositions into a broader reasoning context. Finally, the research reported in (Joshi et al., 1984; van Beek, 1987; Zukerman and McConachy, 2001) considers the addition of information to planned discourse to prevent a user’s erroneous inferences from this discourse. Our mechanism adds explanatory extensions to an interpr</context>
</contexts>
<marker>Pollack, 1990</marker>
<rawString>M.E. Pollack. 1990. Plans as complex mental attitudes. In P. Cohen, J. Morgan, and M.E. Pollack, editors, Intentions in Communication, pages 77–103. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Quilici</author>
</authors>
<title>Detecting and responding to plan-oriented misconceptions.</title>
<date>1989</date>
<booktitle>User Models in Dialog Systems,</booktitle>
<pages>108--132</pages>
<editor>In A. Kobsa and W. Wahlster, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="35154" citStr="Quilici, 1989" startWordPosition="5539" endWordPosition="5540">bility of an interpretation on the basis of the fit between the argument and the interpretation, and the complexity of the interpretation in light of the background knowledge. Our work on positing suppositions is related to research on presuppositions (Kaplan, 1982; Gurney et al., 1997) – a type of supposition implied by the wording of a statement. Like our suppositions, presuppositions are necessary to make sense of what is being said, but they operate at a different knowledge level than our suppositions. This aspect of our work is also related to research on the recognition of flawed plans (Quilici, 1989; Pollack, 1990; Chu-Carroll and Carberry, 2000). These researchers used a plan-based approach to identify erroneous beliefs that account for a user’s statements or plan, while we use a probabilistic approach. Our approach supports the consideration of many possible options, and integrates suppositions into a broader reasoning context. Finally, the research reported in (Joshi et al., 1984; van Beek, 1987; Zukerman and McConachy, 2001) considers the addition of information to planned discourse to prevent a user’s erroneous inferences from this discourse. Our mechanism adds explanatory extension</context>
</contexts>
<marker>Quilici, 1989</marker>
<rawString>A. Quilici. 1989. Detecting and responding to plan-oriented misconceptions. In A. Kobsa and W. Wahlster, editors, User Models in Dialog Systems, pages 108–132. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Raskutti</author>
<author>I Zukerman</author>
</authors>
<title>Generation and selection of likely interpretations during plan recognition. User Modeling and User Adapted Interaction,</title>
<date>1991</date>
<contexts>
<context position="33141" citStr="Raskutti and Zukerman, 1991" startWordPosition="5218" endWordPosition="5221">plains as much of the data as possible (e.g., one referent that fits many clues), while we try to find a representation for a user’s argument. Additionally, their domain knowledge is logic-based, while ours is Bayesian; and they used weights to apply their hypothesis selection criteria, while our criteria are embodied in a probabilistic framework. Plan recognition systems also generate one or more interpretations of a user’s utterances, employing different resources to fill in information omitted by the user, e.g., (Allen and Perrault, 1980; Litman and Allen, 1987; Carberry and Lambert, 1999; Raskutti and Zukerman, 1991). These plan recognition systems used a plan-based approach to propose interpretations. The first three systems applied different types of heuristics to select an interpretation, while the fourth system used a probabilistic approach moderated by heuristics to select the interpretation with the highest probability. We use a probabilistic domain representation in the form of a BN (rather than plan libraries), and apply a probabilistic mechanism that represents explicitly the contribution of background knowledge, model complexity and data fit to the generation of an interpretation. Our mechanism,</context>
</contexts>
<marker>Raskutti, Zukerman, 1991</marker>
<rawString>B. Raskutti and I. Zukerman. 1991. Generation and selection of likely interpretations during plan recognition. User Modeling and User Adapted Interaction, 1(4):323–353.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P van Beek</author>
</authors>
<title>A model for generating better explanations.</title>
<date>1987</date>
<booktitle>In Proceedings of the Twenty-Fifth Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>215--220</pages>
<location>Stanford, California.</location>
<marker>van Beek, 1987</marker>
<rawString>P. van Beek. 1987. A model for generating better explanations. In Proceedings of the Twenty-Fifth Annual Meeting of the Association for Computational Linguistics, pages 215–220, Stanford, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Wallace</author>
</authors>
<title>Statistical and Inductive Inference by Minimum Message Length.</title>
<date>2005</date>
<publisher>Springer,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="1871" citStr="Wallace, 2005" startWordPosition="268" endWordPosition="269">channel (language or speech wave), and decodes this channel to derive the source (idea). The selected source is that with the maximum posterior probability. In this paper, we apply the source-channel approach to the interpretation of arguments. This approach enables us to cast argument interpretation as a trade-off between conflicting factors, viz model complexity against data fit, and structure complexity against belief reasonableness. This trade-off is inspired by the Minimum Message Length (MML) Criterion – a model selection method that is the basis for several machine learning techniques (Wallace, 2005). According to this trade-off, a more complex model might fit the data better, but the plausibility (priors) of the model must be taken into account to avoid over-fitting.1 Our argument interpretation mechanism has been implemented in a system called BIAS (Bayesian Interactive Argumentation System). BIAS presents to a user a set of facts about the world (evidence), and the user constructs an argument about a particular goal proposition in light of this evidence. BIAS then generates an interpretation of the user’s argument, i.e., it tries to understand the argument. When people try to understan</context>
<context position="10214" citStr="Wallace, 2005" startWordPosition="1587" endWordPosition="1588">bed in George et al., 2004). For example, say that in Step 1, we selected supposition configuration SCa. Next, in Step 2, the probability of candidate IGs is calculated in the context of the domain BN and SCa, and one of the IGs is probabilistically selected, say IGb. Similarly, in Step 3, one of the candidate EEs is selected in the context of SCa and IGb. In the next iteration, we probabilistically select an SC (which could be a previously chosen one), and so on. To generate diverse interpretations, if SCa is selected again, a different IG will be chosen. 3 Probabilistic formalism Following (Wallace, 2005), our approach requires the specification of three elements: background knowledge, model and data. Background knowledge is everything known to the system prior to interpreting a user’s argument, e.g., domain knowledge, shared beliefs with the user, and dialogue history; the data is the argument; and the model is the interpretation. We posit that the best interpretation is that with the highest posterior probability. IntBest = argmaxi=j,...,qPr(SCi, IGi, EEi|Arg) where q is the number of interpretations. After applying Bayes rule, this probability is represented as follows.3 Pr(SCi, IGi, EEi|Ar</context>
</contexts>
<marker>Wallace, 2005</marker>
<rawString>C.S. Wallace. 2005. Statistical and Inductive Inference by Minimum Message Length. Springer, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Zukerman</author>
<author>S George</author>
</authors>
<title>A probabilistic approach for argument interpretation. User Modeling and User-Adapted Interaction, Special Issue on Language-Based Interaction,</title>
<date>2005</date>
<pages>15--1</pages>
<contexts>
<context position="5334" citStr="Zukerman and George, 2005" startWordPosition="822" endWordPosition="825">tioned by the user fill in additional detail from the BN, bridging inferential leaps in the argument. • Explanatory Extensions are domain structures (subnets of the domain BN) that are added to an interpretation graph to justify an inference. Contrary to suppositions, these explanations contain propositions believed by the user and the system. The presentation of these explanations is motivated by the results of our early trials, where people objected to belief discontinuities between the antecedents and the consequent of inferences, i.e., increases in certainty or large changes in certainty (Zukerman and George, 2005). To illustrate these components, consider the example in Figure 1. The top segment contains a short argument, and the bottom segment contains its interpretation. The middle segment contains an excerpt of the domain BN which includes the interpretation; the probabilities of some nodes are indicated with linguistic terms.2 The interpretation graph, which appears inside a light gray bubble in the BN excerpt, includes the extra node GreenInGardenAtTimeOfDeath (boxed). Note that the propagated beliefs in this interpretation graph do not match those in the argument. To address this problem, the sys</context>
<context position="12711" citStr="Zukerman and George, 2005" startWordPosition="1967" endWordPosition="1970">model. For instance, a supposition configuration SC comprising beliefs that differ significantly from those in the background knowledge will lower the probability of an interpretation. One of the functions we have used to calculate belief probabilities is the Zipf distribution, where the parameter is the difference between beliefs, e.g., between the supposed &apos;In principle, Pr(SCi, IGi, EEi Arg) can be calculated directly. However, it is not clear how to incorporate the priors of an interpretation in the direct calculation. 136 beliefs and the corresponding beliefs in the background knowledge (Zukerman and George, 2005). That is, the probability of a supposed belief in proposition P according to model M (bel M(P)), in light of the belief in P according to background knowledge B (bel B(P)), is 0 Pr(bel M(P)|bel B(P))= |bel M(P)−bel B(P)|γ where 0 is a normalizing constant, and -y determines the penalty assigned to the discrepancy between the beliefs in P. For example, Pr(bel M(P)=TRUE|bel B(P)=Probable) &gt; Pr(bel M(P)= TRUE|bel B(P)=EvenChance) as TRUE is closer to Probable than to EvenChance. The probability of a structural model (e.g., an interpretation graph) is obtained from the probabilities of the elemen</context>
<context position="21782" citStr="Zukerman and George, 2005" startWordPosition="3441" endWordPosition="3444">erred path has 4 nodes, while the alternative one has 5 nodes, and hence a lower probability. However, if the nodes in the longer path had been recently mentioned, their salience could overcome the size disadvantage. Thus, although the chosen interpretation graph may have a worse data fit than the smallest graph, it still may have the best overall probability in light of the additional background knowledge. Inference patterns In a formative evaluation of an earlier version of our system, we found that people objected to inferences that had increases in certainty or large changes in certainty (Zukerman and George, 2005). An example of an increase in certainty is A [Probably] implies B [VeryProbably]. A large change in certainty is illustrated by A [VeryProbably] implies B [EvenChance]. We then conducted another survey to determine the types of inferences considered acceptable by people (from the standpoint of the beliefs in the antecedents and the consequent). The results from our preliminary survey prompted us to distinguish between three types of inferences: BothSides, SameSide and AlmostSame. • BothSides inferences have antecedents with beliefs on both “sides” of the consequent (in favour and against), e.</context>
<context position="28456" citStr="Zukerman and George, 2005" startWordPosition="4476" endWordPosition="4479">al discrepancy Table 4: Summary of Trade-offs ↓ Pr model structure (IG) ↑ Pr struct. data fit ↓ Pr model belief (SC) ↑ Pr belief data fit ↓ Pr model structure (EE) ↑ Pr model belief in the example in Figure 1, the added explanatory extension eliminates the unacceptable jump in belief. Table 4 summarizes the trade-offs discussed in this section. 4 Evaluation We evaluated separately each component of an interpretation – interpretation graph, supposition configuration and explanatory extensions. 4.1 Interpretation graph We prepared four evaluation sets, each of which was done by about 20 people (Zukerman and George, 2005). In three of the sets, the participants were given a simple argument and a few candidate interpretations (ranked highly by our system). The fourth set featured a complex argument, and only one interpretation (other candidates had much lower probabilities). The participants were asked to give each interpretation a score between 1 (Very UNreasonable) and 5 (Very reasonable). Table 5 shows the results obtained for the interpretation selected by our formalism for each set, which was the top scoring interpretation. The first Table 5: Evaluation results: Interpretation graph Set # 1 2 3 4 Avg. scor</context>
</contexts>
<marker>Zukerman, George, 2005</marker>
<rawString>I. Zukerman and S. George. 2005. A probabilistic approach for argument interpretation. User Modeling and User-Adapted Interaction, Special Issue on Language-Based Interaction, 15(1-2):5–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Zukerman</author>
<author>R McConachy</author>
</authors>
<title>WISHFUL: A discourse planning system that considers a user’s inferences.</title>
<date>2001</date>
<journal>Computational Intelligence,</journal>
<volume>1</volume>
<issue>17</issue>
<contexts>
<context position="35592" citStr="Zukerman and McConachy, 2001" startWordPosition="5602" endWordPosition="5606">hat is being said, but they operate at a different knowledge level than our suppositions. This aspect of our work is also related to research on the recognition of flawed plans (Quilici, 1989; Pollack, 1990; Chu-Carroll and Carberry, 2000). These researchers used a plan-based approach to identify erroneous beliefs that account for a user’s statements or plan, while we use a probabilistic approach. Our approach supports the consideration of many possible options, and integrates suppositions into a broader reasoning context. Finally, the research reported in (Joshi et al., 1984; van Beek, 1987; Zukerman and McConachy, 2001) considers the addition of information to planned discourse to prevent a user’s erroneous inferences from this discourse. Our mechanism adds explanatory extensions to an interpretation to prevent inferences that are objectionable due to discontinuities in belief. Since such nonsequiturs may also be present in system-generated arguments, the approach presented here may be incorporated into argument-generation systems. 6 Conclusion We have offered a probabilistic approach to the interpretation of arguments that casts the selection of an interpretation as a model selection task. In so doing, our </context>
</contexts>
<marker>Zukerman, McConachy, 2001</marker>
<rawString>I. Zukerman and R. McConachy. 2001. WISHFUL: A discourse planning system that considers a user’s inferences. Computational Intelligence, 1(17):1–61.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>