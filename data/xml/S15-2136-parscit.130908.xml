<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.467884">
SemEval-2015 Task 6: Clinical TempEval
</title>
<author confidence="0.986151">
Steven Bethard
</author>
<affiliation confidence="0.988717">
University of Alabama at Birmingham
</affiliation>
<address confidence="0.528616">
Birmingham, AL 35294, USA
</address>
<email confidence="0.980051">
bethard@cis.uab.edu
</email>
<author confidence="0.901895">
Guergana Savova
</author>
<affiliation confidence="0.844823">
Harvard Medical School
</affiliation>
<address confidence="0.586535">
Boston, MA 02115, USA
Guergana.Savova@
</address>
<email confidence="0.863869">
childrens.harvard.edu
</email>
<sectionHeader confidence="0.993781" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997512866666667">
Clinical TempEval 2015 brought the tempo-
ral information extraction tasks of past Temp-
Eval campaigns to the clinical domain. Nine
sub-tasks were included, covering problems in
time expression identification, event expression
identification and temporal relation identifica-
tion. Participant systems were trained and eval-
uated on a corpus of clinical notes and pathol-
ogy reports from the Mayo Clinic, annotated
with an extension of TimeML for the clinical
domain. Three teams submitted a total of 13
system runs, with the best systems achieving
near-human performance on identifying events
and times, but with a large performance gap
still remaining for temporal relations.
</bodyText>
<sectionHeader confidence="0.998782" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999083">
The TempEval shared tasks have, since 2007, pro-
vided a focus for research on temporal information
extraction (Verhagen et al., 2007; Verhagen et al.,
2010; UzZaman et al., 2013). Participant systems
compete to identify critical components of the time-
line of a text, including time expressions, event ex-
pressions and temporal relations. However, the Temp-
Eval campaigns to date have focused primarily on
in-document timelines derived from news articles.
Clinical TempEval brings these temporal informa-
tion extraction tasks to the clinical domain, using
clinical notes and pathology reports from the Mayo
Clinic. This follows recent interest in temporal infor-
mation extraction for the clinical domain, e.g., the
i2b2 2012 shared task (Sun et al., 2013), and broad-
ens our understanding of the language of time beyond
newswire expressions and structure.
</bodyText>
<author confidence="0.420284">
Leon Derczynski
</author>
<affiliation confidence="0.520146">
University of Sheffield
</affiliation>
<address confidence="0.474639">
Sheffield, S1 4DP, UK
</address>
<email confidence="0.690776">
leon@dcs.shef.ac.uk
</email>
<author confidence="0.995635">
James Pustejovsky, Marc Verhagen
</author>
<affiliation confidence="0.8638435">
Brandeis University
Waltham, MA 02453, USA
</affiliation>
<email confidence="0.996949">
jamesp@cs.brandeis.edu
marc@cs.brandeis.edu
</email>
<bodyText confidence="0.9970268">
Clinical TempEval focuses on discrete, well-
defined tasks which allow rapid, reliable and repeat-
able evaluation. Participating systems are expected
to take as input raw text such as:
April 23, 2014: The patient did not have
any postoperative bleeding so we will re-
sume chemotherapy with a larger bolus on
Friday even if there is slight nausea.
And output annotations over the text that capture the
following kinds of information:
</bodyText>
<listItem confidence="0.9949895">
• April 23, 2014: TIMEX3
– TYPE=DATE
• postoperative: TIMEX3
– TYPE=PREPOSTEXP
– CONTAINS
• bleeding: EVENT
– POLARITY=NEG
– BEFORE document creation time
• resume: EVENT
– TYPE=ASPECTUAL
– AFTER document creation time
• chemotherapy: EVENT
– AFTER document creation time
• bolus: EVENT
– AFTER document creation time
• Friday: TIMEX3
– TYPE=DATE
– CONTAINS
• nausea: EVENT
– DEGREE=LITTLE
– MODALITY=HYPOTHETICAL
– AFTER document creation time
</listItem>
<page confidence="0.979643">
806
</page>
<note confidence="0.7690015">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 806–814,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.99926">
That is, the systems should identify the time expres-
sions, event expressions, attributes of those expres-
sions, and temporal relations between them.
</bodyText>
<sectionHeader confidence="0.977542" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.999377">
The Clinical TempEval corpus was based on a set of
600 clinical notes and pathology reports from cancer
patients at the Mayo Clinic. These notes were man-
ually de-identified by the Mayo Clinic to replace
names, locations, etc. with generic placeholders,
but time expressions were not altered. The notes
were then manually annotated by the THYME project
(thyme.healthnlp.org) using an extension of ISO-
TimeML for the annotation of times, events and tem-
poral relations in clinical notes (Styler et al., 2014b).
This extension includes additions such as new time
expression types (e.g., PREPOSTEXP for expressions
like postoperative), new EVENT attributes (e.g., DE-
GREE=LITTLE for expressions like slight nausea),
and an increased focus on temporal relations of type
CONTAINS (a.k.a. INCLUDES).
The annotation procedure was as follows:
</bodyText>
<listItem confidence="0.99641075">
1. Annotators identified time and event expres-
sions, along with their attributes
2. Adjudicators revised and finalized the time and
event expressions and their attributes
3. Annotators identified temporal relations be-
tween pairs of events and events and times
4. Adjudicators revised and finalized the temporal
relations
</listItem>
<bodyText confidence="0.99602">
More details on the corpus annotation process are
documented in a separate article (Styler et al., 2014a).
Because the data contained incompletely de-
identified clinical data (the time expressions were
retained), participants were required to sign a data
use agreement with the Mayo Clinic to obtain the
raw text of the clinical notes and pathology reports.1
The event, time and temporal relation annotations
were distributed separately from the text, in an open
source repository2 using the Anafora standoff format
(Chen and Styler, 2013).
</bodyText>
<footnote confidence="0.996378333333333">
1The details of this process are described at http://thyme.
healthnlp.org/
2https://github.com/stylerw/thymedata
</footnote>
<table confidence="0.998615">
Train Dev
Documents 293 147
EVENTs 38890 20974
TIMEX3s 3833 2078
TLINKs with TYPE=CONTAINS 11176 6173
</table>
<tableCaption confidence="0.8096435">
Table 1: Number of documents, event expressions, time
expressions and narrative container relations in the train-
ing and development portions of the THYME data. (Dev
is the Clinical TempEval 2015 test set.)
</tableCaption>
<bodyText confidence="0.999920454545455">
The corpus was split into three portions: Train
(50%), Dev (25%) and Test (25%). For Clinical
TempEval 2015, the Train portion was used for train-
ing and the Dev portion was used for testing. The Test
portion was not distributed, and was reserved as a test
set for a future iteration of the shared task. Table 1
shows the number of documents, event expressions
(EVENT annotations), time expressions (TIMEX3 an-
notations) and narrative container relations (TLINK
annotations with TYPE=CONTAINS attributes) in the
Train and Dev portions of the corpus.
</bodyText>
<sectionHeader confidence="0.987544" genericHeader="method">
3 Tasks
</sectionHeader>
<bodyText confidence="0.6987355">
A total of nine tasks were included, grouped into
three categories:
</bodyText>
<listItem confidence="0.953813529411765">
• Identifying time expressions (TIMEX3 annota-
tions in the THYME corpus) consisting of the
following components3:
– The spans (character offsets) of the expres-
sion in the text
– Class: DATE, TIME, DURATION, QUAN-
TIFIER, PREPOSTEXP or SET
• Identifying event expressions (EVENT annota-
tions in the THYME corpus) consisting of the
following components:
– The spans (character offsets) of the expres-
sion in the text
– Contextual Modality: ACTUAL, HYPO-
THETICAL, HEDGED or GENERIC
– Degree: MOST, LITTLE or N/A
– Polarity: POS or NEG
– Type: ASPECTUAL, EVIDENTIAL or N/A
</listItem>
<footnote confidence="0.917598">
3Normalized time values (e.g. 2015-02-05) were originally
planned, but annotation was not completed in time.
</footnote>
<page confidence="0.990878">
807
</page>
<listItem confidence="0.6302175">
• Identifying temporal relations between events
and times, focusing on the following types:
</listItem>
<bodyText confidence="0.979594363636364">
– Relations between events and the doc-
ument creation time (BEFORE, OVER-
LAP, BEFORE-OVERLAP or AFTER), rep-
resented by DOCTIMEREL annotations in
the THYME corpus
– Narrative container relations (Pustejovsky
and Stubbs, 2011) between events and/or
times, represented by TLINK annotations
with TYPE=CONTAINS in the THYME cor-
pus
The evaluation was run in two phases:
</bodyText>
<listItem confidence="0.998591166666667">
1. Systems were given access only to the raw text,
and were asked to identify time expressions,
event expressions and temporal relations
2. Systems were given access to the raw text and
the manual event and time annotations, and were
asked to identify only temporal relations
</listItem>
<sectionHeader confidence="0.974928" genericHeader="method">
4 Evaluation Metrics
</sectionHeader>
<bodyText confidence="0.986542">
All of the tasks were evaluated using the standard
metrics of precision (P), recall (R) and F1:
</bodyText>
<equation confidence="0.999814">
P = |S n H|
2 · P · R
P + R
</equation>
<bodyText confidence="0.9998182">
where S is the set of items predicted by the system
and H is the set of items manually annotated by the
humans. Applying these metrics to the tasks only
requires a definition of what is considered an “item”
for each task.
</bodyText>
<listItem confidence="0.990383857142857">
• For evaluating the spans of event expressions
or time expressions, items were tuples of (be-
gin, end) character offsets. Thus, systems only
received credit for identifying events and times
with exactly the same character offsets as the
manually annotated ones.
• For evaluating the attributes of event expres-
</listItem>
<bodyText confidence="0.957191777777778">
sions or time expressions – Class, Contextual
Modality, Degree, Polarity and Type – items
were tuples of (begin, end, value) where begin
and end are character offsets and value is the
value that was given to the relevant attribute.
Thus, systems only received credit for an event
(or time) attribute if they both found an event
(or time) with the correct character offsets and
then assigned the correct value for that attribute.
</bodyText>
<listItem confidence="0.733949210526316">
• For relations between events and the document
creation time, items were tuples of (begin, end,
value), just as if it were an event attribute. Thus,
systems only received credit if they found a
correct event and assigned the correct relation
(BEFORE, OVERLAP, BEFORE-OVERLAP or
AFTER) between that event and the document
creation time. Note that in the second phase of
the evaluation, when manual event annotations
were given as input, precision, recall and F1 are
all equivalent to standard accuracy.
• For narrative container relations, items were tu-
ples of ((begin1, end1), (begin2, end2)), where
the begins and ends corresponded to the char-
acter offsets of the events or times participating
in the relation. Thus, systems only received
credit for a narrative container relation if they
found both events/times and correctly assigned
a CONTAINS relation between them.
</listItem>
<bodyText confidence="0.999764125">
For attributes, an additional metric measures how
accurately a system predicts the attribute values on
just those events or times that the system predicted.
The goal here is to allow a comparison across systems
for assigning attribute values, even when different
systems produce very different numbers of events
and times. This is calculated by dividing the F1 on
the attribute by the F1 on identifying the spans:
</bodyText>
<equation confidence="0.958436666666667">
attribute F1
A=
span F1
</equation>
<bodyText confidence="0.99991875">
For the narrative container relations, additional met-
rics were included that took into account temporal
closure, where additional relations can be determin-
istically inferred from other relations (e.g., A CON-
</bodyText>
<equation confidence="0.918848666666667">
|S|
R = |S n H|
|H|
F1 =
808
TAINS B and B CONTAINS C, so A CONTAINS C):
Pclosure = |S|
|S ∩ closure(H)|
|closure(S) ∩ H|
Rclosure = |H|
Fclosure = 2 · Pclosure · Rclosure
Pclosure + Rclosure
</equation>
<bodyText confidence="0.999942818181818">
These measures take the approach of prior work (Uz-
Zaman and Allen, 2011) and TempEval 2013 (UzZa-
man et al., 2013), following the intuition that preci-
sion should measure the fraction of system-predicted
relations that can be verified from the human an-
notations (either the original human annotations or
annotations inferred from those through closure), and
that recall should measure the fraction of human-
annotated relations that can be verified from the sys-
tem output (either the original system predictions or
predictions inferred from those through closure).
</bodyText>
<sectionHeader confidence="0.992857" genericHeader="method">
5 Baseline Systems
</sectionHeader>
<bodyText confidence="0.99980776">
Two rule-based systems were used as baselines to
compare the participating systems against.
memorize For all tasks but the narrative container
task, a memorization-based baseline was used.
To train the model, all phrases annotated as ei-
ther events or times in the training data were
collected. All exact character matches for these
phrases in the training data were then examined,
and only phrases that were annotated as events
or times greater than 50% of the time were re-
tained. For each phrase, the most frequently an-
notated type (event or time) and attribute values
for instances of that phrase were determined.
To predict with the model, the raw text of the
test data was searched for all exact character
matches of any of the memorized phrases, pre-
ferring longer phrases when multiple matches
overlapped. Wherever a phrase match was
found, an event or time with the memorized
(most frequent) attribute values was predicted.
closest For the narrative container task, a proximity-
based baseline was used. Each time expression
was predicted to be a narrative container, con-
taining only the closest event expression to it in
the text.
</bodyText>
<sectionHeader confidence="0.938939" genericHeader="method">
6 Participating Systems
</sectionHeader>
<bodyText confidence="0.998356476190476">
Three research teams submitted a total of 13 runs:
BluLab The team from Stockholm University and
University of Utah participated in all tasks, us-
ing supervised classifiers with features gen-
erated by the Apache clinical Text Analysis
and Knowledge Extraction System (cTAKES)4.
Their runs differed in whether and how many
rules were used to constrain the search for nar-
rative container relations.
KPSCMI The team from Kaiser Permanente South-
ern California participated in the time expres-
sion tasks. Their runs compared an extended
version of the rule-based HeidelTime5 system
(run 1) with systems based on supervised classi-
fiers (run 2-3).
UFPRSheffield The team from Universidade Fed-
eral do Paran´a and University of Sheffield par-
ticipated in the time expression tasks. Their
runs compared in-house rule-based systems (the
Hynx runs) to systems based on supervised clas-
sifiers (the SVM runs).
</bodyText>
<sectionHeader confidence="0.983978" genericHeader="method">
7 Human Agreement
</sectionHeader>
<bodyText confidence="0.999905071428571">
We also give two types of human agreement on the
task, measured with the same evaluation metrics as
the systems:
ann-ann Inter-annotator agreement between the two
independent human annotators who annotated
each document. This is the most commonly re-
ported type of agreement, and often considered
to be an upper bound on system performance.
adj-ann Inter-annotator agreement between the ad-
judicator and the two independent annotators.
This is usually a better bound on system perfor-
mance in adjudicated corpora, since the models
are trained on the adjudicated data, not on the
individual annotator data.
</bodyText>
<footnote confidence="0.9997885">
4https://ctakes.apache.org
5https://code.google.com/p/heideltime/
</footnote>
<page confidence="0.977543">
809
</page>
<table confidence="0.99981175">
Team P span F1 P span + class A
R R F1
Baseline: memorize 0.743 0.372 0.496 0.723 0.362 0.483 0.974
BluLab: run 1-3 0.797 0.664 0.725 0.778 0.652 0.709 0.819
KPSCMI: run 1 0.272 0.782 0.404 0.223 0.642 0.331 0.948
KPSCMI: run 2 0.705 0.683 0.694 0.668 0.648 0.658 0.948
KPSCMI: run 3 0.693 0.706 0.699 0.657 0.669 0.663 0.973
UFPRSheffield-SVM: run 1 0.732 0.661 0.695 0.712 0.643 0.676 0.977
UFPRSheffield-SVM: run 2 0.741 0.655 0.695 0.723 0.640 0.679 0.950
UFPRSheffield-Hynx: run 1 0.479 0.747 0.584 0.455 0.709 0.555 0.952
UFPRSheffield-Hynx: run 2 0.494 0.770 0.602 0.470 0.733 0.573 0.951
UFPRSheffield-Hynx: run 3 0.311 0.794 0.447 0.296 0.756 0.425 0.951
UFPRSheffield-Hynx: run 4 0.311 0.795 0.447 0.296 0.756 0.425 0.952
UFPRSheffield-Hynx: run 5 0.411 0.795 0.542 0.391 0.756 0.516 0.978
Agreement: ann-ann - - 0.690 - - 0.644 0.933
Agreement: adj-ann - - 0.774 - - 0.747 0.965
</table>
<tableCaption confidence="0.86174225">
Table 2: System performance and annotator agreement on TIMEX3 tasks: identifying the time expression’s span
(character offsets) and class (DATE, TIME, DURATION, QUANTIFIER, PREPOSTEXP or SET). The best system score
from each column is in bold. The three BluLab runs are combined because they all have identical performance (since
they only differ in their approach to narrative container relations).
</tableCaption>
<bodyText confidence="0.99950775">
Precision and recall are not reported in these scenar-
ios since they depend on the arbitrary choice of one
annotator as the “human” (H) and the other as the
“system” (S).
Note that since temporal relations between events
and the document creation time were annotated at
the same time as the events themselves, agreement
for this task is only reported in phase 1 of the evalu-
ation. Similarly, since narrative container relations
were only annotated after events and times had been
adjudicated, agreement for this task is only reported
in phase 2 of the evaluation.
</bodyText>
<sectionHeader confidence="0.959519" genericHeader="evaluation">
8 Evaluation Results
</sectionHeader>
<subsectionHeader confidence="0.987362">
8.1 Time Expressions
</subsectionHeader>
<bodyText confidence="0.999985225806452">
Table 2 shows results on the time expression tasks.
The BluLab system achieved the best F1 at identify-
ing time expressions, 0.725. The other machine learn-
ing systems (KPSCMI run 2-3 and UFPRSheffield-
SVM run 1-2) achieved F1 in the 0.690-0.700
range. The rule-based systems (KPSCMI run 1 and
UFPRSheffield-Hynx run 1-5) all achieved higher
recall than the machine learning systems, but at sub-
stantial costs to precision. All systems outperformed
the memorization baseline in terms of recall, and all
machine-learning systems outperformed it in terms
of F1, but only the BluLab system outperformed the
baseline in terms of precision.
The BluLab system also achieved the best F1 for
predicting the classes of time expressions, though
this is primarily due to achieving a higher F1
at identifying time expressions in the first place.
UFPRSheffield-Hynx run 5 achieved the best accu-
racy on predicting classes for the time expressions
it found, 0.978, though on this metric it only outper-
formed the memorization baseline by 0.004.
Across the time expression tasks, systems did not
quite achieve performance at the level of human
agreement. For the spans of time expressions, the
top system achieved 0.725 F1, compared to 0.774
adjudicator-annotator F1, though almost half of the
systems exceeded the lower annotator-annotator F1
of 0.690. For the classes of time expressions, the
story was similar for F1, though several models ex-
ceeded the adjudicator-annotator accuracy of 0.965
on just the time expressions predicted by the system.
</bodyText>
<subsectionHeader confidence="0.992646">
8.2 Event Expressions
</subsectionHeader>
<bodyText confidence="0.999640666666667">
Table 3 shows results on the event expression tasks.
The BluLab system outperformed the memorization
baseline on almost every metric on every task. The
</bodyText>
<page confidence="0.99319">
810
</page>
<table confidence="0.99763704">
span span + modality span + degree
Team P R F1 P R F1 A P R F1 A
Baseline: memorize 0.876 0.810 0.842 0.810 0.749 0.778 0.924 0.871 0.806 0.838 0.995
BluLab: run 1-3 0.887 0.864 0.875 0.834 0.813 0.824 0.942 0.882 0.859 0.870 0.994
Agreement: ann-ann - - 0.819 - - 0.779 0.951 - - 0.815 0.995
Agreement: adj-ann - - 0.880 - - 0.855 0.972 - - 0.877 0.997
- 0.798 0.974
- - 0.869 0.988
-
Team
P R F1 A
span + type
span + polarity
P R F1 A
0.834 0.812 0.823 0.941
Baseline: memorize
BluLab: run 1-3
0.846 0.783 0.813 0.966
0.800 0.740 0.769 0.913
0.868 0.846 0.857 0.979
-
- 0.773 0.944
- - 0.853 0.969
Agreement: ann-ann
Agreement: adj-ann
</table>
<tableCaption confidence="0.95503">
Table 3: System performance and annotator agreement on EVENT tasks: identifying the event expression’s span
(character offsets), contextual modality (ACTUAL, HYPOTHETICAL, HEDGED or GENERIC), degree (MOST, LITTLE or
N/A), polarity (POS or NEG) and type (ASPECTUAL, EVIDENTIAL or N/A). The best system score from each column
is in bold.
</tableCaption>
<bodyText confidence="0.9999915625">
one exception was the semantic type of the event,
where the memorization baseline had a better preci-
sion and also a better accuracy on the classes of the
events that it identified.
The BluLab system got close to the level of
adjudicator-annotator agreement on identifying the
spans of event expressions (0.875 vs. 0.880 F1),
identifying the degree of events (0.870 vs. 0.877
F1), and identifying the polarity of events (0.857
vs. 0.869 F1), and it generally met or exceeded the
lower annotator-annotator agreement on these tasks.
There is a larger gap (3+ points of F1) between the
system performance and adjudicator-annotator agree-
ment for event modality and event type, though only
a small gap (&lt;1 point of F1) for the lower annotator-
annotator agreement on these tasks.
</bodyText>
<subsectionHeader confidence="0.998449">
8.3 Temporal Relations
</subsectionHeader>
<bodyText confidence="0.985009625">
Table 4 shows performance on the temporal relation
tasks. In detecting the relations between events and
the document creation time, the BluLab system sub-
stantially outperformed the memorization baseline,
achieving F1 of 0.702 on system-predicted events
(phase 1) and F1 of 0.791 on manually annotated
events (phase 2). In identifying narrative container re-
lations, the best BluLab system (run 2) outperformed
the proximity-based baseline when using system-
predicted events (Fclosure of 0.123 vs. 0.106) but
not when using manually annotated events (Fclosure
0.702
though this sys-
tem did exceed the lower annotator-annotator agree-
For narrative container relations
and TIMEX3s,
the gap was much greater, with adjudicator-annotator
agreement at 0.672 Fclosure, and the top system (the
Even the lower
annotator-annotator agreement of 0.475 Fclosure was
The results of Clinical
suggest that
a small number of temporal information extraction
tasks are solved by current state-of-the-art systems,
but for the majority of tasks, there is still room for im-
provement. Identifying events, their degrees and their
polarities were the easiest tasks for the participants,
with the best systems achieving within about 0.01
of human agreement on the tasks. Systems for iden-
tifying event modality and event type were not far
behind, achieving within about 0.03 of human
TempEval 2015
agree-
811 of 0.181 vs. 0.260). Across both phase 1 and phase
2 for narrative container relations, the top
system always had the best recall, while the baseline
system always had the best precision.
Annotator agreement was higher than system per-
formance on all temporal relation tasks. For rela-
tions between events and the document creation time,
adjudicator-annotator agreement was 0.761
com-
pared to the best
ment of 0.628
using manually annotated
baseline system) at 0.260
much higher than the system performan
BluLab
</bodyText>
<figure confidence="0.931375733333333">
F1,
system’s
F1,
F1.
EVENTs
Fclosure.
ce.
9 Discussion
Narrative containers
Without closure
P R F1
To document time
P R F1
With closure
P R F1
</figure>
<figureCaption confidence="0.309992">
Phase 1: systems are given only the raw text
</figureCaption>
<table confidence="0.999238">
Baseline: memorize 0.600 0.555 0.577 - - - - - -
Baseline: closest - - - 0.368 0.061 0.104 0.400 0.061 0.106
BluLab: run 1 0.712 0.693 0.702 0.085 0.080 0.082 0.100 0.099 0.100
BluLab: run 2 0.712 0.693 0.702 0.080 0.142 0.102 0.094 0.179 0.123
BluLab: run 3 0.712 0.693 0.702 0.084 0.086 0.085 0.090 0.103 0.096
Agreement: ann-ann - - 0.628 - - - - - -
Agreement: adj-ann - - 0.761 - - - - - -
Phase 2: systems are given manually annotated EVENTs and TIMEX3s
Baseline: memorize - - 0.608 - - - - - -
Baseline: closest - - - 0.514 0.170 0.255 0.554 0.170 0.260
BluLab: run 1 - - 0.791 0.100 0.104 0.102 0.117 0.128 0.123
BluLab: run 2 - - 0.791 0.109 0.210 0.143 0.140 0.254 0.181
BluLab: run 3 - - 0.791 0.119 0.137 0.128 0.150 0.155 0.153
Agreement: ann-ann - - - - - 0.449 - - 0.475
Agreement: adj-ann - - - - - 0.655 - - 0.672
</table>
<tableCaption confidence="0.997928">
Table 4: System performance and annotator agreement on temporal relation tasks: identifying relations between events
</tableCaption>
<bodyText confidence="0.98242466">
and the document creation time (DOCTIMEREL), and identifying narrative container relations (CONTAINS). The best
system score from each column is in bold.
ment. Time expressions and relations to the docu-
ment creation time were at the next level of difficulty,
with a gap of about 0.05 from human agreement.
Identifying narrative container relations was by
far the most difficult task, with the best systems
down by more than 0.40 from human agreement. In
absolute terms, performance on narrative container
relations was also quite low, with system Fclosure
scores in the 0.10-0.12 range on system-generated
events and times, and in the 0.12-0.26 range on
manually-annotated events and times. For compari-
son, in TempEval 2013, which used newswire data,
Fclosure scores were in the 0.24-0.36 range on system-
generated events and times and in the 0.35-0.56 range
on manually-annotated events and times (UzZaman
et al., 2013). One major difference between the cor-
pora is that the narrative container relations in the
clinical domain often span many sentences, while
almost all of the relations in TempEval 2013 were ei-
ther within the same sentence or across adjacent sen-
tences. Most past research systems have also focused
on identifying within-sentence and adjacent-sentence
relations. This focus on local relations might explain
the poor performance on the more distant relations
in the THYME corpus. But further investigation is
needed to better understand the challenge here.
In almost all tasks, the submitted systems substan-
tially outperformed the baselines. The one exception
to this was the narrative containers task. The base-
line there – which simply predicted that each time
expression contained the nearest event expression to
it in the text – achieved 4 times the precision of the
best submitted system and consequently achieved the
best Fl by a large margin. This suggests that future
systems may want to incorporate better measures of
proximity that can capture some of what the baseline
is finding.
While machine learning methods were overall the
most successful, for time expression identification,
the submitted rule-based systems achieved the best
recall. This is counter to the usual assumption that
rule-based systems will be more precise, and that
machine learning systems will sacrifice precision to
increase recall. The difference is likely that the rule-
based systems were aiming for good coverage, trying
to find all potential time expressions, but had too
few constraints to discard such phrases in inappro-
priate contexts. The baseline system is suggestive
</bodyText>
<page confidence="0.991018">
812
</page>
<bodyText confidence="0.999987450980392">
of this possibility: it has a constraint to only memo-
rize phrases that corresponded with time expressions
more than 50% of the time, and it has high preci-
sion (0.743) and low recall (0.372) as is typically
expected of a rule-based system, but if the constraint
is removed, it has low precision (0.126) and high
recall (0.521) like the participant rule-based systems.
Clinical TempEval was the first TempEval exercise
to use narrative containers, a significant shift from
prior exercises. Annotator agreement in the dataset
is moderate, but needs to be further improved. Sim-
ilar agreement scores were found when annotating
temporal relations in prior corpora (for TempEval or
using TimeML), although these typically involved
the application of more complex temporal relation
ontologies. The narrative container approach is com-
paratively simple. The low annotator-adjudicator
scores (i.e. below 0.90, a score generally recognized
to indicate a production-quality resource) suggests
that annotation is difficult independent of the num-
ber of potential temporal relation types. Difficulty
may lie in the comprehension and reification of the
potentially complex temporal structures described in
natural language text. Nevertheless, systems did well
on the DCT task, achieving high scores – similar to
the pattern seen in Task D of TempEval-2, which had
a comparable scoring metric.
Though the results of Clinical TempEval 2015
are encouraging, they were limited somewhat by the
small number of participants in the task. There are
two likely reasons for this. First, there were many
different sub-tasks for Clinical TempEval, meaning
that to compete in all sub-tasks, a large number of
sub-systems had to be developed in a limited amount
of time (six months or less). This relatively high
barrier for entry meant that of the 15 research groups
that managed to sign a data use agreement and obtain
the data before the competition, only 3 submitted sys-
tems to compete. Second, the data use agreement pro-
cess was time consuming, and more than 10 research
groups who began the data use agreement process
were unable to complete it before the evaluation.
In future iterations of Clinical TempEval, we ex-
pect these issues to be reduced. The next Clinical
TempEval will use the current Train and Dev data as
the training set, and as these data are already avail-
able, this leaves research teams with a year or more
to develop systems. Furthermore, arrangements with
the Mayo Clinic have been made to further expedite
the data use agreement process, which should signifi-
cantly reduce the wait time for new participants.
</bodyText>
<sectionHeader confidence="0.993974" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9972925">
This work was partially supported by funding from
R01LM010090 (THYME) from the National Library
of Medicine and from the European Union’s Seventh
Framework Programme (grant No. 611233, PHEME).
</bodyText>
<sectionHeader confidence="0.998716" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999738710526316">
Wei-Te Chen and Will Styler. 2013. Anafora: A web-
based general purpose annotation tool. In Proceedings
of the 2013 NAACL HLT Demonstration Session, pages
14–19, Atlanta, Georgia, June.
James Pustejovsky and Amber Stubbs. 2011. Increasing
informativeness in temporal annotation. In Proceedings
of the 5th Linguistic Annotation Workshop, pages 152–
160, Portland, Oregon, USA, June.
William F. Styler, IV, Steven Bethard, Sean Finan, Martha
Palmer, Sameer Pradhan, Piet de Groen, Brad Erick-
son, Timothy Miller, Chen Lin, Guergana Savova, and
James Pustejovsky. 2014a. Temporal annotation in the
clinical domain. Transactions of the Association for
Computational Linguistics, 2:143–154.
William F. Styler, IV, Guergana Savova, Martha Palmer,
James Pustejovsky, Tim O’Gorman, and Piet C.
de Groen. 2014b. THYME annotation guidelines, 2.
Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013.
Evaluating temporal relations in clinical text: 2012 i2b2
challenge. Journal of the American Medical Informat-
ics Association, 20(5):806–813.
Naushad UzZaman and James Allen. 2011. Temporal
evaluation. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 351–356, Portland,
Oregon, USA, June.
Naushad UzZaman, Hector Llorens, Leon Derczynski,
James Allen, Marc Verhagen, and James Pustejovsky.
2013. SemEval-2013 Task 1: TempEval-3: Evaluating
Time Expressions, Events, and Temporal Relations. In
Second Joint Conference on Lexical and Computational
Semantics (*SEM), Volume 2: Proceedings of the Sev-
enth International Workshop on Semantic Evaluation
(SemEval 2013), pages 1–9, Atlanta, Georgia, USA,
June.
Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark
Hepple, Graham Katz, and James Pustejovsky. 2007.
SemEval-2007 Task 15: TempEval Temporal Relation
</reference>
<page confidence="0.989608">
813
</page>
<reference confidence="0.996482125">
Identification. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 75–80, Prague, Czech Republic, June.
Marc Verhagen, Roser Sauri, Tommaso Caselli, and James
Pustejovsky. 2010. SemEval-2010 Task 13: TempEval-
2. In Proceedings of the 5th International Workshop on
Semantic Evaluation, pages 57–62, Uppsala, Sweden,
July.
</reference>
<page confidence="0.998516">
814
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.478706">
<title confidence="0.990997">SemEval-2015 Task 6: Clinical TempEval</title>
<author confidence="0.996993">Steven</author>
<affiliation confidence="0.999079">University of Alabama at</affiliation>
<address confidence="0.923886">Birmingham, AL 35294,</address>
<email confidence="0.999775">bethard@cis.uab.edu</email>
<author confidence="0.532463">Guergana</author>
<affiliation confidence="0.999142">Harvard Medical</affiliation>
<address confidence="0.997768">Boston, MA 02115,</address>
<email confidence="0.999821">childrens.harvard.edu</email>
<abstract confidence="0.999192">Clinical TempEval 2015 brought the temporal information extraction tasks of past Temp- Eval campaigns to the clinical domain. Nine sub-tasks were included, covering problems in time expression identification, event expression identification and temporal relation identification. Participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain. Three teams submitted a total of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Wei-Te Chen</author>
<author>Will Styler</author>
</authors>
<title>Anafora: A webbased general purpose annotation tool.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 NAACL HLT Demonstration Session,</booktitle>
<pages>14--19</pages>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="4888" citStr="Chen and Styler, 2013" startWordPosition="722" endWordPosition="725">s and events and times 4. Adjudicators revised and finalized the temporal relations More details on the corpus annotation process are documented in a separate article (Styler et al., 2014a). Because the data contained incompletely deidentified clinical data (the time expressions were retained), participants were required to sign a data use agreement with the Mayo Clinic to obtain the raw text of the clinical notes and pathology reports.1 The event, time and temporal relation annotations were distributed separately from the text, in an open source repository2 using the Anafora standoff format (Chen and Styler, 2013). 1The details of this process are described at http://thyme. healthnlp.org/ 2https://github.com/stylerw/thymedata Train Dev Documents 293 147 EVENTs 38890 20974 TIMEX3s 3833 2078 TLINKs with TYPE=CONTAINS 11176 6173 Table 1: Number of documents, event expressions, time expressions and narrative container relations in the training and development portions of the THYME data. (Dev is the Clinical TempEval 2015 test set.) The corpus was split into three portions: Train (50%), Dev (25%) and Test (25%). For Clinical TempEval 2015, the Train portion was used for training and the Dev portion was used</context>
</contexts>
<marker>Chen, Styler, 2013</marker>
<rawString>Wei-Te Chen and Will Styler. 2013. Anafora: A webbased general purpose annotation tool. In Proceedings of the 2013 NAACL HLT Demonstration Session, pages 14–19, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Amber Stubbs</author>
</authors>
<title>Increasing informativeness in temporal annotation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th Linguistic Annotation Workshop,</booktitle>
<pages>152--160</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="6925" citStr="Pustejovsky and Stubbs, 2011" startWordPosition="1037" endWordPosition="1040">character offsets) of the expression in the text – Contextual Modality: ACTUAL, HYPOTHETICAL, HEDGED or GENERIC – Degree: MOST, LITTLE or N/A – Polarity: POS or NEG – Type: ASPECTUAL, EVIDENTIAL or N/A 3Normalized time values (e.g. 2015-02-05) were originally planned, but annotation was not completed in time. 807 • Identifying temporal relations between events and times, focusing on the following types: – Relations between events and the document creation time (BEFORE, OVERLAP, BEFORE-OVERLAP or AFTER), represented by DOCTIMEREL annotations in the THYME corpus – Narrative container relations (Pustejovsky and Stubbs, 2011) between events and/or times, represented by TLINK annotations with TYPE=CONTAINS in the THYME corpus The evaluation was run in two phases: 1. Systems were given access only to the raw text, and were asked to identify time expressions, event expressions and temporal relations 2. Systems were given access to the raw text and the manual event and time annotations, and were asked to identify only temporal relations 4 Evaluation Metrics All of the tasks were evaluated using the standard metrics of precision (P), recall (R) and F1: P = |S n H| 2 · P · R P + R where S is the set of items predicted b</context>
</contexts>
<marker>Pustejovsky, Stubbs, 2011</marker>
<rawString>James Pustejovsky and Amber Stubbs. 2011. Increasing informativeness in temporal annotation. In Proceedings of the 5th Linguistic Annotation Workshop, pages 152– 160, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>William F Styler</author>
<author>Steven Bethard</author>
<author>Sean Finan</author>
<author>Martha Palmer</author>
<author>Sameer Pradhan</author>
<author>Piet de Groen</author>
<author>Brad Erickson</author>
<author>Timothy Miller</author>
<author>Chen Lin</author>
</authors>
<booktitle>Guergana Savova, and James Pustejovsky. 2014a. Temporal annotation in the clinical domain. Transactions of the Association for Computational Linguistics,</booktitle>
<pages>2--143</pages>
<marker>Styler, Bethard, Finan, Palmer, Pradhan, de Groen, Erickson, Miller, Lin, </marker>
<rawString>William F. Styler, IV, Steven Bethard, Sean Finan, Martha Palmer, Sameer Pradhan, Piet de Groen, Brad Erickson, Timothy Miller, Chen Lin, Guergana Savova, and James Pustejovsky. 2014a. Temporal annotation in the clinical domain. Transactions of the Association for Computational Linguistics, 2:143–154.</rawString>
</citation>
<citation valid="false">
<authors>
<author>William F Styler</author>
<author>Guergana Savova</author>
<author>Martha Palmer</author>
<author>James Pustejovsky</author>
<author>Tim O’Gorman</author>
<author>Piet C de Groen</author>
</authors>
<booktitle>2014b. THYME annotation guidelines,</booktitle>
<pages>2</pages>
<marker>Styler, Savova, Palmer, Pustejovsky, O’Gorman, de Groen, </marker>
<rawString>William F. Styler, IV, Guergana Savova, Martha Palmer, James Pustejovsky, Tim O’Gorman, and Piet C. de Groen. 2014b. THYME annotation guidelines, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiyi Sun</author>
<author>Anna Rumshisky</author>
<author>Ozlem Uzuner</author>
</authors>
<title>Evaluating temporal relations in clinical text: 2012 i2b2 challenge.</title>
<date>2013</date>
<journal>Journal of the American Medical Informatics Association,</journal>
<volume>20</volume>
<issue>5</issue>
<contexts>
<context position="1680" citStr="Sun et al., 2013" startWordPosition="242" endWordPosition="245"> Verhagen et al., 2010; UzZaman et al., 2013). Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations. However, the TempEval campaigns to date have focused primarily on in-document timelines derived from news articles. Clinical TempEval brings these temporal information extraction tasks to the clinical domain, using clinical notes and pathology reports from the Mayo Clinic. This follows recent interest in temporal information extraction for the clinical domain, e.g., the i2b2 2012 shared task (Sun et al., 2013), and broadens our understanding of the language of time beyond newswire expressions and structure. Leon Derczynski University of Sheffield Sheffield, S1 4DP, UK leon@dcs.shef.ac.uk James Pustejovsky, Marc Verhagen Brandeis University Waltham, MA 02453, USA jamesp@cs.brandeis.edu marc@cs.brandeis.edu Clinical TempEval focuses on discrete, welldefined tasks which allow rapid, reliable and repeatable evaluation. Participating systems are expected to take as input raw text such as: April 23, 2014: The patient did not have any postoperative bleeding so we will resume chemotherapy with a larger bol</context>
</contexts>
<marker>Sun, Rumshisky, Uzuner, 2013</marker>
<rawString>Weiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013. Evaluating temporal relations in clinical text: 2012 i2b2 challenge. Journal of the American Medical Informatics Association, 20(5):806–813.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naushad UzZaman</author>
<author>James Allen</author>
</authors>
<title>Temporal evaluation.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>351--356</pages>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="10227" citStr="UzZaman and Allen, 2011" startWordPosition="1595" endWordPosition="1599">ifferent numbers of events and times. This is calculated by dividing the F1 on the attribute by the F1 on identifying the spans: attribute F1 A= span F1 For the narrative container relations, additional metrics were included that took into account temporal closure, where additional relations can be deterministically inferred from other relations (e.g., A CON|S| R = |S n H| |H| F1 = 808 TAINS B and B CONTAINS C, so A CONTAINS C): Pclosure = |S| |S ∩ closure(H)| |closure(S) ∩ H| Rclosure = |H| Fclosure = 2 · Pclosure · Rclosure Pclosure + Rclosure These measures take the approach of prior work (UzZaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of system-predicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of humanannotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure). 5 Baseline Systems Two rule-based systems were used as baselines to compare the participating systems against. </context>
</contexts>
<marker>UzZaman, Allen, 2011</marker>
<rawString>Naushad UzZaman and James Allen. 2011. Temporal evaluation. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 351–356, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Naushad UzZaman</author>
<author>Hector Llorens</author>
<author>Leon Derczynski</author>
<author>James Allen</author>
<author>Marc Verhagen</author>
<author>James Pustejovsky</author>
</authors>
<title>SemEval-2013 Task 1: TempEval-3: Evaluating Time Expressions, Events, and Temporal Relations.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>1--9</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="1108" citStr="UzZaman et al., 2013" startWordPosition="156" endWordPosition="159"> temporal relation identification. Participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain. Three teams submitted a total of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations. 1 Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations. However, the TempEval campaigns to date have focused primarily on in-document timelines derived from news articles. Clinical TempEval brings these temporal information extraction tasks to the clinical domain, using clinical notes and pathology reports from the Mayo Clinic. This follows recent interest in temporal information extraction for the clinical domain, e.g., the i2b2 2012 shared task (Sun et al., 2013), and broadens our understan</context>
<context position="10268" citStr="UzZaman et al., 2013" startWordPosition="1603" endWordPosition="1607">s calculated by dividing the F1 on the attribute by the F1 on identifying the spans: attribute F1 A= span F1 For the narrative container relations, additional metrics were included that took into account temporal closure, where additional relations can be deterministically inferred from other relations (e.g., A CON|S| R = |S n H| |H| F1 = 808 TAINS B and B CONTAINS C, so A CONTAINS C): Pclosure = |S| |S ∩ closure(H)| |closure(S) ∩ H| Rclosure = |H| Fclosure = 2 · Pclosure · Rclosure Pclosure + Rclosure These measures take the approach of prior work (UzZaman and Allen, 2011) and TempEval 2013 (UzZaman et al., 2013), following the intuition that precision should measure the fraction of system-predicted relations that can be verified from the human annotations (either the original human annotations or annotations inferred from those through closure), and that recall should measure the fraction of humanannotated relations that can be verified from the system output (either the original system predictions or predictions inferred from those through closure). 5 Baseline Systems Two rule-based systems were used as baselines to compare the participating systems against. memorize For all tasks but the narrative </context>
<context position="22788" citStr="UzZaman et al., 2013" startWordPosition="3642" endWordPosition="3645">eement. Identifying narrative container relations was by far the most difficult task, with the best systems down by more than 0.40 from human agreement. In absolute terms, performance on narrative container relations was also quite low, with system Fclosure scores in the 0.10-0.12 range on system-generated events and times, and in the 0.12-0.26 range on manually-annotated events and times. For comparison, in TempEval 2013, which used newswire data, Fclosure scores were in the 0.24-0.36 range on systemgenerated events and times and in the 0.35-0.56 range on manually-annotated events and times (UzZaman et al., 2013). One major difference between the corpora is that the narrative container relations in the clinical domain often span many sentences, while almost all of the relations in TempEval 2013 were either within the same sentence or across adjacent sentences. Most past research systems have also focused on identifying within-sentence and adjacent-sentence relations. This focus on local relations might explain the poor performance on the more distant relations in the THYME corpus. But further investigation is needed to better understand the challenge here. In almost all tasks, the submitted systems su</context>
</contexts>
<marker>UzZaman, Llorens, Derczynski, Allen, Verhagen, Pustejovsky, 2013</marker>
<rawString>Naushad UzZaman, Hector Llorens, Leon Derczynski, James Allen, Marc Verhagen, and James Pustejovsky. 2013. SemEval-2013 Task 1: TempEval-3: Evaluating Time Expressions, Events, and Temporal Relations. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 1–9, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Robert Gaizauskas</author>
<author>Frank Schilder</author>
<author>Mark Hepple</author>
<author>Graham Katz</author>
<author>James Pustejovsky</author>
</authors>
<title>SemEval-2007 Task 15: TempEval Temporal Relation Identification.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007),</booktitle>
<pages>75--80</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1062" citStr="Verhagen et al., 2007" startWordPosition="148" endWordPosition="151">ification, event expression identification and temporal relation identification. Participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain. Three teams submitted a total of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations. 1 Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations. However, the TempEval campaigns to date have focused primarily on in-document timelines derived from news articles. Clinical TempEval brings these temporal information extraction tasks to the clinical domain, using clinical notes and pathology reports from the Mayo Clinic. This follows recent interest in temporal information extraction for the clinical domain, e.g., the i2b2 2012 shared task </context>
</contexts>
<marker>Verhagen, Gaizauskas, Schilder, Hepple, Katz, Pustejovsky, 2007</marker>
<rawString>Marc Verhagen, Robert Gaizauskas, Frank Schilder, Mark Hepple, Graham Katz, and James Pustejovsky. 2007. SemEval-2007 Task 15: TempEval Temporal Relation Identification. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007), pages 75–80, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Roser Sauri</author>
<author>Tommaso Caselli</author>
<author>James Pustejovsky</author>
</authors>
<date>2010</date>
<booktitle>SemEval-2010 Task 13: TempEval2. In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>57--62</pages>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="1085" citStr="Verhagen et al., 2010" startWordPosition="152" endWordPosition="155">sion identification and temporal relation identification. Participant systems were trained and evaluated on a corpus of clinical notes and pathology reports from the Mayo Clinic, annotated with an extension of TimeML for the clinical domain. Three teams submitted a total of 13 system runs, with the best systems achieving near-human performance on identifying events and times, but with a large performance gap still remaining for temporal relations. 1 Introduction The TempEval shared tasks have, since 2007, provided a focus for research on temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013). Participant systems compete to identify critical components of the timeline of a text, including time expressions, event expressions and temporal relations. However, the TempEval campaigns to date have focused primarily on in-document timelines derived from news articles. Clinical TempEval brings these temporal information extraction tasks to the clinical domain, using clinical notes and pathology reports from the Mayo Clinic. This follows recent interest in temporal information extraction for the clinical domain, e.g., the i2b2 2012 shared task (Sun et al., 2013), and</context>
</contexts>
<marker>Verhagen, Sauri, Caselli, Pustejovsky, 2010</marker>
<rawString>Marc Verhagen, Roser Sauri, Tommaso Caselli, and James Pustejovsky. 2010. SemEval-2010 Task 13: TempEval2. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 57–62, Uppsala, Sweden, July.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>