<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000330">
<title confidence="0.997485">
A Novel Content Enriching Model for Microblog Using News Corpus
</title>
<author confidence="0.993981">
Yunlun Yang&apos;, Zhihong Deng2∗, Hongliang Yu3
</author>
<affiliation confidence="0.922038666666667">
Key Laboratory of Machine Perception (Ministry of Education),
School of Electronics Engineering and Computer Science,
Peking University, Beijing 100871, China
</affiliation>
<email confidence="0.953644666666667">
&apos;incomparable-lun@pku.edu.cn
2zhdeng@cis.pku.edu.cn
3yuhongliang324@gmail.com
</email>
<sectionHeader confidence="0.993605" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999095">
In this paper, we propose a novel model for
enriching the content of microblogs by ex-
ploiting external knowledge, thus improv-
ing the data sparseness problem in short
text classification. We assume that mi-
croblogs share the same topics with ex-
ternal knowledge. We first build an opti-
mization model to infer the topics of mi-
croblogs by employing the topic-word dis-
tribution of the external knowledge. Then
the content of microblogs is further en-
riched by relevant words from external
knowledge. Experiments on microblog
classification show that our approach is
effective and outperforms traditional text
classification methods.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.989279283018868">
During the past decade, the short text represen-
tation has been intensively studied. Previous re-
searches (Phan et al., 2008; Guo and Diab, 2012)
show that while traditional methods are not so
powerful due to the data sparseness problem, some
semantic analysis based approaches are proposed
and proved effective, and various topic models are
among the most frequently used techniques in this
area. Meanwhile, external knowledge has been
found helpful (Hu et al., 2009) in tackling the da-
ta scarcity problem by enriching short texts with
informative context. Well-organized knowledge
bases such as Wikipedia and WordNet are com-
mon tools used in relevant methods.
Nowadays, most of the work on short text fo-
cuses on microblog. As a new form of short tex-
t, microblog has some unique features like infor-
mal spelling and emerging words, and many mi-
croblogs are strongly related to up-to-date topics
as well. Every day, a great quantity of microblogs
∗Corresponding author
more than we can read is pushed to us, and find-
ing what we are interested in becomes rather dif-
ficult, so the ability of choosing what kind of mi-
croblogs to read is urgently demanded by common
user. Such ability can be implemented by effective
short text classification.
Treating microblogs as standard texts and di-
rectly classifying them cannot achieve the goal of
effective classification because of sparseness prob-
lem. On the other hand, news on the Internet is
of information abundance and many microblogs
are news-related. They share up-to-date topics
and sometimes quote each other. Thus, external
knowledge, such as news, provides rich supple-
mentary information for analysing and mining mi-
croblogs.
Motivated by the idea of using topic model and
external knowledge mentioned above, we present
an LDA-based enriching method using the news
corpus, and apply it to the task of microblog clas-
sification. The basic assumption in our model is
that news articles and microblogs tend to share the
same topics. We first infer the topic distribution
of each microblog based on the topic-word distri-
bution of news corpus obtained by the LDA esti-
mation. With the above two distributions, we then
add a number of words from news as additional
information to microblogs by evaluating the relat-
edness of between each word and microblog, since
words not appearing in the microblog may still be
highly relevant.
To sum up, our contributions are:
</bodyText>
<listItem confidence="0.97600875">
(1) We formulate the topic inference problem for
short texts as a convex optimization problem.
(2) We enrich the content of microblogs by infer-
ring the association between microblogs and
external words in a probabilistic perspective.
(3) We evaluate our method on the real dataset-
s and experiment results outperform the base-
line methods.
</listItem>
<page confidence="0.969287">
218
</page>
<bodyText confidence="0.987202333333333">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 218–223,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
2 Related Work (c) Select relevant words from external knowl-
edge to enrich the content of microblogs.
Based on the idea of exploiting external knowl-
edge, many methods are proposed to improve the
representation of short texts for classification and
clustering. Among them, some directly utilize
the structure information of organized knowledge
base or search engine. Banerjee et al. (2007) use
the title and the description of news article as two
separate query strings to select related concept-
s as additional feature. Hu et al. (2009) present
a framework to improve the performance of short
text clustering by mining informative context with
the integration of Wikipedia and WordNet.
However, to better leverage external resource,
some other methods introduce topic models. Phan
et al. (2008) present a framework including an
approach for short text topic inference and adds
abstract words as extra feature. Guo and Diab
(2012) modify classic topic models and propos-
es a matrix-factorization based model for sentence
similarity calculation tasks.
Those methods without topic model usually re-
ly greatly on the performance of search system or
the completeness of knowledge base, and lack in-
depth analysis for external resources. Compared
with our method, the topic model based method-
s mentioned above remain in finding latent space
representation of short text and ignore that rele-
vant words from external knowledge are informa-
tive as well.
</bodyText>
<sectionHeader confidence="0.999322" genericHeader="method">
3 Our Model
</sectionHeader>
<bodyText confidence="0.999166818181818">
We formulate the problem as follows. Let
EK = {de1, ... , de M.} denote external knowl-
edge consisting of Me documents. V e =
{we1, ... , we N.} represents its vocabulary. Let
MB = {dm1 , ... , dmM,,t} denote microblog set and
its vocabulary is Vm = {wm1 , ... , wmN,,t}. Our
task is to enrich each microblog with additional
information so as to improve microblog’s repre-
sentation.
The model we proposed mainly consists of three
steps:
</bodyText>
<listItem confidence="0.9087348">
(a) Topic inference for external knowledge by
running LDA estimation.
(b) Topic inference for microblogs by employing
the word distributions of topics obtained from
step (a).
</listItem>
<subsectionHeader confidence="0.992587">
3.1 Topic Inference for External Knowledge
</subsectionHeader>
<bodyText confidence="0.999829909090909">
We do topic analysis for EK using LDA esti-
mation (Blei et al., 2003) in this section and we
choose LDA as the topic analysis model because
of its broadly proved effectivity and ease of under-
standing.
In LDA, each document has a distribution over
all topics P(zk|dj), and each topic has a distri-
bution over all words P(wi|zk), where zk, dj and
wi represent the topic, document and word respec-
tively. The optimization problem is formulated as
maximizing the log likelihood on the corpus:
</bodyText>
<equation confidence="0.9664055">
�max � Xij log P(zk|dj)P(wi|zk) (1)
i j k
</equation>
<bodyText confidence="0.999798588235294">
In this formulation, Xij represents the term fre-
quency of word wi in document dj. P(zk|dj)
and P(wi|zk) are parameters to be inferred, cor-
responding to the topic distribution of each doc-
ument and the word distribution of each topic re-
spectively. Estimating parameters for LDA by di-
rectly and exactly maximizing the likelihood of
the corpus in (1) is intractable, so we use Gibbs
Sampling for estimation.
After performing LDA model (K topics) esti-
mation on EK, we obtain the topic distribution-
s of document de j (j = 1, ... , Me), denoted as
P(zek|dej) (k = 1, ... , K), and the word distri-
bution of topic zek (k = 1, ... , K), denoted as
P(wei |zek) (i = 1, ... , Ne). Step (b) greatly re-
lies on the word distributions of topics we have
obtained here.
</bodyText>
<subsectionHeader confidence="0.999093">
3.2 Topic Inference for Microblog
</subsectionHeader>
<bodyText confidence="0.999768692307692">
In this section, we infer the topic distribution of
each microblog. Because of the assumption that
microblogs share the same topics with external
corpus, the “topic distribution” here refers to a dis-
tribution over all topics on EK.
Differing from step (a), the method used for
topic inference for microblogs is not directly run-
ning LDA estimation on microblog collection but
following the topics from external knowledge to
ensure topic consistence. We employ the word
distributions of topics obtained from step (a), i.e.
P(wei |zek), and formulate the optimization prob-
lem in a similar form to Formula (1) as follows:
</bodyText>
<page confidence="0.956612">
219
</page>
<equation confidence="0.972833666666667">
X X X
max Xij log P(ze k|dm j )P(we i |ze k),
P (ze k|dm j )
</equation>
<bodyText confidence="0.983675294117647">
i j k
where Xij represents the term frequency of word
wei in microblog dmj , and P(zek|dmj ) denote the dis-
tribution of microblog dm j over all topics on EK.
Obviously most Xij are zero and we ignore those
words that do not appear in V e.
Compared with the original LDA optimization
problem (1), the topic inference problem for mi-
croblog (2) follows the idea of document gener-
ation process, but replaces topics to be estimated
with known topics from other corpus. As a result,
parameters to be inferred are only the topic distri-
bution of every microblog.
It is noteworthy that since the word distribution
of every topic P(wei |zek) is known, Formula (2) can
be further solved by separating it into Mm sub-
problems:
</bodyText>
<equation confidence="0.95818475">
X X
max Xij log P(zek|dmj )P(wei |zek)
P(ze k|dm j ) i k
for j = 1,...,Mm
</equation>
<bodyText confidence="0.9958462">
These Mm subproblems correspond to the Mm
microblogs and can be easily proved convexity.
After solving them, we obtain the topic distribu-
tions of microblog dmj (j = 1, ... , Mm), denoted
as P(zek|dmj ) (k = 1, ... , K).
</bodyText>
<subsectionHeader confidence="0.998457">
3.3 Select Relevant Words for Microblog
</subsectionHeader>
<bodyText confidence="0.9992285">
To enrich the content of every microblog, we s-
elect relevant words from external knowledge in
this section.
Based on the results of step (a)&amp;(b), we calcu-
late the word distributions of microblogs as fol-
lows:
</bodyText>
<equation confidence="0.9900785">
XP(wei |dmj ) = P(zek|dmj )P(wei |zek), (4)
k
</equation>
<bodyText confidence="0.997376434782608">
where P(wei |dmj ) represents the probability that
word we iwill appear in microblog dmj . In other
words, though some words may not actually ap-
pears in a microblog, there is still a probability that
it is highly relevant to the microblog. Intuitively,
this probability indicates the strength of associa-
tion between a word and a microblog. The word
distribution of every microblog is based on topic
analysis and its accuracy relies heavily on the ac-
curacy of topic inference in step (b). In fact, the
more words a microblog includes, the more accu-
rate its topic inference will be, and this can be re-
garded as an explanation of the low efficiency of
data sparseness problem.
For microblog dmj , we sort all words by
P(wei |dmj ) in descending order. Having known
the top L relevant words according to the result of
sorting, we redefine the “term frequency” of every
word after adding these L words to microblog dmj
as additional content. Supposing these L words
are wej1, wej2, ... , wejL, the revised term frequency
of word w E {wej1, ... , wjeL} is defined as fol-
lows:
</bodyText>
<equation confidence="0.999554">
P(w|dmj ) * L, (5)
RTF (w, dmj ) = L e m
Pp=1 P(wjp |dj )
</equation>
<bodyText confidence="0.9973934">
where RTF(·) is the revised term frequency.
As the Equation (5) shows, the revised term fre-
quency of every word is proportional to probabili-
ty P(wi|dmj ) rather than a constant.
So far, we can add these L words and their re-
vised term frequency as additional information to
microblog dmj . The revised term frequency plays
the same role as TF in common text representation
vector, so we calculate the TFIDF of the added
words as:
</bodyText>
<equation confidence="0.927491">
TFIDF(w, dmj ) = RTF(w, dmj )·IDF(w) (6)
</equation>
<bodyText confidence="0.9997095">
Note that IDF(w) is changed as arrival of new
words for each microblog. The TFIDF vector of
a microblog with additional words is called en-
hanced vector.
</bodyText>
<sectionHeader confidence="0.999808" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.996998">
4.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999963">
To evaluate our method, we build our own dataset-
s. We crawl 95028 Chinese news reports from
Sina News website, segment them, and remove
stop words and rare words. After preprocessing,
these news documents are used as external knowl-
edge. As for microblog, we crawl a number of
microblogs from Sina Weibo, and ask unbiased
assessors to manually classify them into 9 cate-
gories following the column setting of Sina News.
</bodyText>
<footnote confidence="0.9980995">
Sina News: http://news.sina.com.cn/
Sina Weibo: http://www.weibo.com/
</footnote>
<page confidence="0.997502">
220
</page>
<bodyText confidence="0.9999741">
After the manual classification, we remove short
microblogs (less than 10 words), usernames, links
and some special characters, then we segmen-
t them and remove rare words as well. Finally, we
get 1671 classified microblogs as our microblog
dataset. The size of each category is shown in Ta-
ble 1.
from 75.52% to 84.53% when considering addi-
tional information, which means our method in-
deed improves the representation of microblogs.
</bodyText>
<subsectionHeader confidence="0.6219745">
4.3 Parameter Tuning
4.3.1 Effect of Added Words
</subsectionHeader>
<table confidence="0.955945909090909">
0.85
Category #Microblog
Finance 229
Stock 80
Entertainment 162
Military Affairs 179
Technologies 204
Digital Products 194
Sports 195
Society 214
Daily Life 214
</table>
<tableCaption confidence="0.999863">
Table 1: Microblog number of every category
</tableCaption>
<bodyText confidence="0.999878">
There are some important details of our imple-
mentation. In step (a) of Section 3.1 we estimate
LDA model using GibbsLDA++, a C/C++ imple-
mentation of LDA using Gibbs Sampling. In step
(b) of Section3.2, OPTI toolbox on Matlab is used
to help solve the convex problems. In the clas-
sification tasks shown below, we use LibSVM as
classifier and perform ten-fold cross validation to
evaluate the classification accuracy.
</bodyText>
<subsectionHeader confidence="0.955009">
4.2 Classification Results
</subsectionHeader>
<table confidence="0.9985415">
Representation Average Accuracy
TFIDF vector 0.7552
Boolean vector 0.7203
Enhanced vector 0.8453
</table>
<tableCaption confidence="0.9158665">
Table 2: Classification accuracy with different rep-
resentations
</tableCaption>
<bodyText confidence="0.999519571428571">
In this section, we report the average preci-
sion of each method as shown in Table 2. The
enhanced vector is the representation generated
by our method. Two baselines are TFIDF vec-
tor (Jones, 1972) and boolean vector (word oc-
currence) of the original microblog. In the table,
our method increases the classification accuracy
</bodyText>
<footnote confidence="0.9835915">
GibbsLDA++: http://gibbslda.sourceforge.net
OPTI Toolbox: http://www.i2c2.aut.ac.nz/Wiki/OPTI/
SVM.NET: http://www.matthewajohnson.org/software
/svm.html
</footnote>
<table confidence="0.561739">
50 100 150 200 300 400 500
Number of Added Words (L)
</table>
<figureCaption confidence="0.9911205">
Figure 1: Classification accuracy changes accord-
ing to topics and added words
</figureCaption>
<bodyText confidence="0.9998803">
The experiment corresponding to Figure 1 is to
discover how the classification accuracy changes
when we fix the number of topics (K = 100)
and change the number of added words (L) in our
method. Result shows that more added words do
not mean higher accuracy. By studying some cas-
es, we find out that if we add too many words,
the proportion of “noisy words” will increase. We
reach the best result when number of added words
is 300.
</bodyText>
<subsectionHeader confidence="0.53502">
4.3.2 Effect of Topic Number
</subsectionHeader>
<figure confidence="0.940854333333333">
0.85
50 100 200 300
Number of Topics (K)
</figure>
<figureCaption confidence="0.9903485">
Figure 2: Classification accuracy changing ac-
cording to the number of topics
</figureCaption>
<bodyText confidence="0.999500666666667">
The experiment corresponding to Figure 2 is to
discover how the classification accuracy changes
when we fix the number of added words (L =
</bodyText>
<figure confidence="0.99682875">
0.845
0.84
Average Accuracy
0.835
0.83
0.825
0.82
0.815
0.81
0.805
Average Accuracy 0.845
0.84
0.835
0.83
0.825
0.82
</figure>
<page confidence="0.991195">
221
</page>
<table confidence="0.980388142857143">
Microblog (Translated) Top Relevant Words (Translated)
Kim Jong Un held an emergency meeting this morn- South Korea, America, North Korea, work,
ing, and commanded the missile units to prepare for safety, claim, military, exercise, united, report
attacking U.S. military bases at any time.
Shenzhou Nine will carry three astronauts, including day, satellite, launch, research, technology,
the first Chinese female astronaut, and launch in a system, mission, aerospace, success, Chang’e
proper time during the middle of June. Two
</table>
<tableCaption confidence="0.999873">
Table 3: Case study (Translated from Chinese)
</tableCaption>
<bodyText confidence="0.764024875">
300) and change the number of topics (K) in
our method. As we can see, the accuracy does
not grow monotonously as the number of topic-
s increases. Blindly enlarging the topic number
will not improve the accuracy. The best result is
reached when topic number is 100, and similar ex-
periments adding different number of words show
the same condition of reaching the best result.
</bodyText>
<subsectionHeader confidence="0.4176175">
4.3.3 Effect of Revised Term Frequency
Number of Added Words (L)
</subsectionHeader>
<figureCaption confidence="0.9129675">
Figure 3: Classification accuracy changing ac-
cording to the redefinition of term frequency
</figureCaption>
<bodyText confidence="0.999500529411765">
The experiment corrsponding to Figure 3 is to
discover whether our redefining “term frequency”
as revised term frequency in step (c) of Section
3.3 will affect the classification accuracy and how.
The results should be analysed in two aspects. On
one hand, without redefinition, the accuracy re-
mains in a stable high level and tends to decrease
as we add more words. One reason for the de-
creasing is that “noisy words” have a increasing
negative impact on the accuracy as the propor-
tion of “noisy words” grows with the number of
added words. On the other hand, the best result
is reached when we use the revise term frequen-
cy. This suggests that our redefinition for term fre-
quency shows better improvement for microblog
representation under certain conditions, but is not
optimal under all situations.
</bodyText>
<subsectionHeader confidence="0.999442">
4.4 Case Study
</subsectionHeader>
<bodyText confidence="0.999979866666667">
In Table 3, we select several cases consisting of
microblogs and their top relevant words .
In the first case, we successfully find the country
name according to its leader’s name and limited
information in the sentence. Other related coun-
tries and events are also selected by our model as
they often appear together in news. In the other
case, relevant words are among the most frequent-
ly used words in news and have close semantic re-
lations with the microblogs in certain aspects.
As we can see, based on topic analysis, our
model shows strong ability of mining relevan-
t words. Other cases show that the model can be
further improved by removing the noisy and mean-
ingless ones among added words.
</bodyText>
<sectionHeader confidence="0.990526" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999983">
We propose an effective content enriching method
for microblog, to enhance classification accuracy.
News corpus is exploited as external knowledge.
As for techniques, our method uses LDA as its
topic analysis model and formulates topic infer-
ence for new data as convex optimization prob-
lems. Compared with traditional representation,
enriched microblog shows great improvement in
classification tasks.
As we do not control the quality of added words,
our future work starts from building a filter to se-
lect better additional information. And to make the
most of external knowledge, better ways to build
topic space should be considered.
</bodyText>
<sectionHeader confidence="0.998237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.906601">
This work is supported by National Natural Sci-
ence Foundation of China (Grant No. 61170091).
</bodyText>
<figure confidence="0.993487846153846">
Using RTF Using TF
50 100 150 200 300 400 500
Average Accuracy
0.845
0.835
0.825
0.815
0.805
0.85
0.84
0.83
0.82
0.81
</figure>
<page confidence="0.990559">
222
</page>
<sectionHeader confidence="0.995096" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99984637254902">
Banerjee, S., Ramanathan, K., and Gupta, A. 2007,
July. Clustering short texts using wikipedia. In Pro-
ceedings of the 30th annual international ACM SI-
GIR conference on Research and development in in-
formation retrieval (pp. 787-788). ACM.
Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent
Dirichlet Allocation. In Journal of machine Learn-
ing research, 3, 993-1022.
Bollegala, D., Matsuo, Y., and Ishizuka, M. 2007.
Measuring semantic similarity between words using
web search engines. www, 7, 757-766.
Boyd, S. P., and Vandenberghe, L. 2004. Convex opti-
mization. Cambridge university press.
Gabrilovich, E., and Markovitch, S. 2007, January.
Computing Semantic Relatedness Using Wikipedia-
based Explicit Semantic Analysis. In IJCAI (Vol. 7,
pp. 1606-1611).
Guo, W., and Diab, M. 2012, July. Modeling sentences
in the latent space. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics: Long Papers-Volume 1 (pp. 864-872).
Guo, W., and Diab, M. 2012, July. Learning the latent
semantics of a concept from its definition. In Pro-
ceedings of the 50th Annual Meeting of the Associ-
ation for Computational Linguistics: Short Papers-
Volume 2 (pp. 140-144).
Hu, X., Sun, N., Zhang, C., and Chua, T. S. 2009,
November. Exploiting internal and external seman-
tics for the clustering of short texts using world
knowledge. In Proceedings of the 18th ACM con-
ference on Information and knowledge management
(pp. 919-928). ACM.
Jones, K. S. 1972. A statistical interpretation of term
specificity and its application in retrieval. In Journal
of documentation, 28(1), 11-21
Phan, X. H., Nguyen, L. M., and Horiguchi, S. 2008,
April. Learning to Classify Short and Sparse Text &amp;
Web with Hidden Topics from Large-scale Data Col-
lections. In Proceedings of the 17th international
conference on World Wide Web (pp. 91-100). ACM.
Sahami, M., and Heilman, T. D. 2006, May. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th inter-
national conference on World Wide Web (pp. 377-
386). ACM.
Zubiaga, A., and Ji, H. 2013, May. Harnessing we-
b page directories for large-scale classification of
tweets. In Proceedings of the 22nd international
conference on World Wide Web companion (pp. 225-
226). International World Wide Web Conferences S-
teering Committee.
</reference>
<page confidence="0.999164">
223
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.429694">
<title confidence="0.999112">A Novel Content Enriching Model for Microblog Using News Corpus</title>
<author confidence="0.918471">Zhihong Hongliang</author>
<affiliation confidence="0.782909333333333">Key Laboratory of Machine Perception (Ministry of School of Electronics Engineering and Computer Peking University, Beijing 100871,</affiliation>
<abstract confidence="0.998972764705882">In this paper, we propose a novel model for enriching the content of microblogs by exploiting external knowledge, thus improving the data sparseness problem in short text classification. We assume that microblogs share the same topics with external knowledge. We first build an optimization model to infer the topics of microblogs by employing the topic-word distribution of the external knowledge. Then the content of microblogs is further enriched by relevant words from external knowledge. Experiments on microblog classification show that our approach is effective and outperforms traditional text classification methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>K Ramanathan</author>
<author>A Gupta</author>
</authors>
<title>Clustering short texts using wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval</booktitle>
<pages>787--788</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4337" citStr="Banerjee et al. (2007)" startWordPosition="672" endWordPosition="675"> baseline methods. 218 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 218–223, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2 Related Work (c) Select relevant words from external knowledge to enrich the content of microblogs. Based on the idea of exploiting external knowledge, many methods are proposed to improve the representation of short texts for classification and clustering. Among them, some directly utilize the structure information of organized knowledge base or search engine. Banerjee et al. (2007) use the title and the description of news article as two separate query strings to select related concepts as additional feature. Hu et al. (2009) present a framework to improve the performance of short text clustering by mining informative context with the integration of Wikipedia and WordNet. However, to better leverage external resource, some other methods introduce topic models. Phan et al. (2008) present a framework including an approach for short text topic inference and adds abstract words as extra feature. Guo and Diab (2012) modify classic topic models and proposes a matrix-factoriza</context>
</contexts>
<marker>Banerjee, Ramanathan, Gupta, 2007</marker>
<rawString>Banerjee, S., Ramanathan, K., and Gupta, A. 2007, July. Clustering short texts using wikipedia. In Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval (pp. 787-788). ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Blei</author>
<author>A Y Ng</author>
<author>M I Jordan</author>
</authors>
<title>Latent Dirichlet Allocation.</title>
<date>2003</date>
<journal>In Journal of machine Learning research,</journal>
<volume>3</volume>
<pages>993--1022</pages>
<contexts>
<context position="6122" citStr="Blei et al., 2003" startWordPosition="968" endWordPosition="971">f Me documents. V e = {we1, ... , we N.} represents its vocabulary. Let MB = {dm1 , ... , dmM,,t} denote microblog set and its vocabulary is Vm = {wm1 , ... , wmN,,t}. Our task is to enrich each microblog with additional information so as to improve microblog’s representation. The model we proposed mainly consists of three steps: (a) Topic inference for external knowledge by running LDA estimation. (b) Topic inference for microblogs by employing the word distributions of topics obtained from step (a). 3.1 Topic Inference for External Knowledge We do topic analysis for EK using LDA estimation (Blei et al., 2003) in this section and we choose LDA as the topic analysis model because of its broadly proved effectivity and ease of understanding. In LDA, each document has a distribution over all topics P(zk|dj), and each topic has a distribution over all words P(wi|zk), where zk, dj and wi represent the topic, document and word respectively. The optimization problem is formulated as maximizing the log likelihood on the corpus: �max � Xij log P(zk|dj)P(wi|zk) (1) i j k In this formulation, Xij represents the term frequency of word wi in document dj. P(zk|dj) and P(wi|zk) are parameters to be inferred, corre</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent Dirichlet Allocation. In Journal of machine Learning research, 3, 993-1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bollegala</author>
<author>Y Matsuo</author>
<author>M Ishizuka</author>
</authors>
<title>Measuring semantic similarity between words using web search engines.</title>
<date>2007</date>
<journal>www,</journal>
<volume>7</volume>
<pages>757--766</pages>
<marker>Bollegala, Matsuo, Ishizuka, 2007</marker>
<rawString>Bollegala, D., Matsuo, Y., and Ishizuka, M. 2007. Measuring semantic similarity between words using web search engines. www, 7, 757-766.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Boyd</author>
<author>L Vandenberghe</author>
</authors>
<title>Convex optimization. Cambridge university press.</title>
<date>2004</date>
<marker>Boyd, Vandenberghe, 2004</marker>
<rawString>Boyd, S. P., and Vandenberghe, L. 2004. Convex optimization. Cambridge university press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gabrilovich</author>
<author>S Markovitch</author>
</authors>
<title>Computing Semantic Relatedness Using Wikipediabased Explicit Semantic Analysis.</title>
<date>2007</date>
<journal>In IJCAI</journal>
<volume>7</volume>
<pages>1606--1611</pages>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Gabrilovich, E., and Markovitch, S. 2007, January. Computing Semantic Relatedness Using Wikipediabased Explicit Semantic Analysis. In IJCAI (Vol. 7, pp. 1606-1611).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Guo</author>
<author>M Diab</author>
</authors>
<title>Modeling sentences in the latent space.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume</booktitle>
<volume>1</volume>
<pages>864--872</pages>
<contexts>
<context position="1138" citStr="Guo and Diab, 2012" startWordPosition="160" endWordPosition="163"> text classification. We assume that microblogs share the same topics with external knowledge. We first build an optimization model to infer the topics of microblogs by employing the topic-word distribution of the external knowledge. Then the content of microblogs is further enriched by relevant words from external knowledge. Experiments on microblog classification show that our approach is effective and outperforms traditional text classification methods. 1 Introduction During the past decade, the short text representation has been intensively studied. Previous researches (Phan et al., 2008; Guo and Diab, 2012) show that while traditional methods are not so powerful due to the data sparseness problem, some semantic analysis based approaches are proposed and proved effective, and various topic models are among the most frequently used techniques in this area. Meanwhile, external knowledge has been found helpful (Hu et al., 2009) in tackling the data scarcity problem by enriching short texts with informative context. Well-organized knowledge bases such as Wikipedia and WordNet are common tools used in relevant methods. Nowadays, most of the work on short text focuses on microblog. As a new form of sho</context>
<context position="4877" citStr="Guo and Diab (2012)" startWordPosition="758" endWordPosition="761">nformation of organized knowledge base or search engine. Banerjee et al. (2007) use the title and the description of news article as two separate query strings to select related concepts as additional feature. Hu et al. (2009) present a framework to improve the performance of short text clustering by mining informative context with the integration of Wikipedia and WordNet. However, to better leverage external resource, some other methods introduce topic models. Phan et al. (2008) present a framework including an approach for short text topic inference and adds abstract words as extra feature. Guo and Diab (2012) modify classic topic models and proposes a matrix-factorization based model for sentence similarity calculation tasks. Those methods without topic model usually rely greatly on the performance of search system or the completeness of knowledge base, and lack indepth analysis for external resources. Compared with our method, the topic model based methods mentioned above remain in finding latent space representation of short text and ignore that relevant words from external knowledge are informative as well. 3 Our Model We formulate the problem as follows. Let EK = {de1, ... , de M.} denote exte</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Guo, W., and Diab, M. 2012, July. Modeling sentences in the latent space. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1 (pp. 864-872).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Guo</author>
<author>M Diab</author>
</authors>
<title>Learning the latent semantics of a concept from its definition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short PapersVolume</booktitle>
<volume>2</volume>
<pages>140--144</pages>
<contexts>
<context position="1138" citStr="Guo and Diab, 2012" startWordPosition="160" endWordPosition="163"> text classification. We assume that microblogs share the same topics with external knowledge. We first build an optimization model to infer the topics of microblogs by employing the topic-word distribution of the external knowledge. Then the content of microblogs is further enriched by relevant words from external knowledge. Experiments on microblog classification show that our approach is effective and outperforms traditional text classification methods. 1 Introduction During the past decade, the short text representation has been intensively studied. Previous researches (Phan et al., 2008; Guo and Diab, 2012) show that while traditional methods are not so powerful due to the data sparseness problem, some semantic analysis based approaches are proposed and proved effective, and various topic models are among the most frequently used techniques in this area. Meanwhile, external knowledge has been found helpful (Hu et al., 2009) in tackling the data scarcity problem by enriching short texts with informative context. Well-organized knowledge bases such as Wikipedia and WordNet are common tools used in relevant methods. Nowadays, most of the work on short text focuses on microblog. As a new form of sho</context>
<context position="4877" citStr="Guo and Diab (2012)" startWordPosition="758" endWordPosition="761">nformation of organized knowledge base or search engine. Banerjee et al. (2007) use the title and the description of news article as two separate query strings to select related concepts as additional feature. Hu et al. (2009) present a framework to improve the performance of short text clustering by mining informative context with the integration of Wikipedia and WordNet. However, to better leverage external resource, some other methods introduce topic models. Phan et al. (2008) present a framework including an approach for short text topic inference and adds abstract words as extra feature. Guo and Diab (2012) modify classic topic models and proposes a matrix-factorization based model for sentence similarity calculation tasks. Those methods without topic model usually rely greatly on the performance of search system or the completeness of knowledge base, and lack indepth analysis for external resources. Compared with our method, the topic model based methods mentioned above remain in finding latent space representation of short text and ignore that relevant words from external knowledge are informative as well. 3 Our Model We formulate the problem as follows. Let EK = {de1, ... , de M.} denote exte</context>
</contexts>
<marker>Guo, Diab, 2012</marker>
<rawString>Guo, W., and Diab, M. 2012, July. Learning the latent semantics of a concept from its definition. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short PapersVolume 2 (pp. 140-144).</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Hu</author>
<author>N Sun</author>
<author>C Zhang</author>
<author>T S Chua</author>
</authors>
<title>Exploiting internal and external semantics for the clustering of short texts using world knowledge.</title>
<date>2009</date>
<booktitle>In Proceedings of the 18th ACM conference on Information and knowledge management</booktitle>
<pages>919--928</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1461" citStr="Hu et al., 2009" startWordPosition="210" endWordPosition="213">. Experiments on microblog classification show that our approach is effective and outperforms traditional text classification methods. 1 Introduction During the past decade, the short text representation has been intensively studied. Previous researches (Phan et al., 2008; Guo and Diab, 2012) show that while traditional methods are not so powerful due to the data sparseness problem, some semantic analysis based approaches are proposed and proved effective, and various topic models are among the most frequently used techniques in this area. Meanwhile, external knowledge has been found helpful (Hu et al., 2009) in tackling the data scarcity problem by enriching short texts with informative context. Well-organized knowledge bases such as Wikipedia and WordNet are common tools used in relevant methods. Nowadays, most of the work on short text focuses on microblog. As a new form of short text, microblog has some unique features like informal spelling and emerging words, and many microblogs are strongly related to up-to-date topics as well. Every day, a great quantity of microblogs ∗Corresponding author more than we can read is pushed to us, and finding what we are interested in becomes rather difficult</context>
<context position="4484" citStr="Hu et al. (2009)" startWordPosition="698" endWordPosition="701">Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics 2 Related Work (c) Select relevant words from external knowledge to enrich the content of microblogs. Based on the idea of exploiting external knowledge, many methods are proposed to improve the representation of short texts for classification and clustering. Among them, some directly utilize the structure information of organized knowledge base or search engine. Banerjee et al. (2007) use the title and the description of news article as two separate query strings to select related concepts as additional feature. Hu et al. (2009) present a framework to improve the performance of short text clustering by mining informative context with the integration of Wikipedia and WordNet. However, to better leverage external resource, some other methods introduce topic models. Phan et al. (2008) present a framework including an approach for short text topic inference and adds abstract words as extra feature. Guo and Diab (2012) modify classic topic models and proposes a matrix-factorization based model for sentence similarity calculation tasks. Those methods without topic model usually rely greatly on the performance of search sys</context>
</contexts>
<marker>Hu, Sun, Zhang, Chua, 2009</marker>
<rawString>Hu, X., Sun, N., Zhang, C., and Chua, T. S. 2009, November. Exploiting internal and external semantics for the clustering of short texts using world knowledge. In Proceedings of the 18th ACM conference on Information and knowledge management (pp. 919-928). ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Jones</author>
</authors>
<title>A statistical interpretation of term specificity and its application in retrieval.</title>
<date>1972</date>
<journal>In Journal of documentation,</journal>
<volume>28</volume>
<issue>1</issue>
<pages>11--21</pages>
<contexts>
<context position="13118" citStr="Jones, 1972" startWordPosition="2196" endWordPosition="2197">f Section3.2, OPTI toolbox on Matlab is used to help solve the convex problems. In the classification tasks shown below, we use LibSVM as classifier and perform ten-fold cross validation to evaluate the classification accuracy. 4.2 Classification Results Representation Average Accuracy TFIDF vector 0.7552 Boolean vector 0.7203 Enhanced vector 0.8453 Table 2: Classification accuracy with different representations In this section, we report the average precision of each method as shown in Table 2. The enhanced vector is the representation generated by our method. Two baselines are TFIDF vector (Jones, 1972) and boolean vector (word occurrence) of the original microblog. In the table, our method increases the classification accuracy GibbsLDA++: http://gibbslda.sourceforge.net OPTI Toolbox: http://www.i2c2.aut.ac.nz/Wiki/OPTI/ SVM.NET: http://www.matthewajohnson.org/software /svm.html 50 100 150 200 300 400 500 Number of Added Words (L) Figure 1: Classification accuracy changes according to topics and added words The experiment corresponding to Figure 1 is to discover how the classification accuracy changes when we fix the number of topics (K = 100) and change the number of added words (L) in our </context>
</contexts>
<marker>Jones, 1972</marker>
<rawString>Jones, K. S. 1972. A statistical interpretation of term specificity and its application in retrieval. In Journal of documentation, 28(1), 11-21</rawString>
</citation>
<citation valid="true">
<authors>
<author>X H Phan</author>
<author>L M Nguyen</author>
<author>S Horiguchi</author>
</authors>
<title>Learning to Classify Short and Sparse Text &amp; Web with Hidden Topics from Large-scale Data Collections.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th international conference on World Wide Web</booktitle>
<pages>91--100</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1117" citStr="Phan et al., 2008" startWordPosition="156" endWordPosition="159">ss problem in short text classification. We assume that microblogs share the same topics with external knowledge. We first build an optimization model to infer the topics of microblogs by employing the topic-word distribution of the external knowledge. Then the content of microblogs is further enriched by relevant words from external knowledge. Experiments on microblog classification show that our approach is effective and outperforms traditional text classification methods. 1 Introduction During the past decade, the short text representation has been intensively studied. Previous researches (Phan et al., 2008; Guo and Diab, 2012) show that while traditional methods are not so powerful due to the data sparseness problem, some semantic analysis based approaches are proposed and proved effective, and various topic models are among the most frequently used techniques in this area. Meanwhile, external knowledge has been found helpful (Hu et al., 2009) in tackling the data scarcity problem by enriching short texts with informative context. Well-organized knowledge bases such as Wikipedia and WordNet are common tools used in relevant methods. Nowadays, most of the work on short text focuses on microblog.</context>
<context position="4742" citStr="Phan et al. (2008)" startWordPosition="736" endWordPosition="739">osed to improve the representation of short texts for classification and clustering. Among them, some directly utilize the structure information of organized knowledge base or search engine. Banerjee et al. (2007) use the title and the description of news article as two separate query strings to select related concepts as additional feature. Hu et al. (2009) present a framework to improve the performance of short text clustering by mining informative context with the integration of Wikipedia and WordNet. However, to better leverage external resource, some other methods introduce topic models. Phan et al. (2008) present a framework including an approach for short text topic inference and adds abstract words as extra feature. Guo and Diab (2012) modify classic topic models and proposes a matrix-factorization based model for sentence similarity calculation tasks. Those methods without topic model usually rely greatly on the performance of search system or the completeness of knowledge base, and lack indepth analysis for external resources. Compared with our method, the topic model based methods mentioned above remain in finding latent space representation of short text and ignore that relevant words fr</context>
</contexts>
<marker>Phan, Nguyen, Horiguchi, 2008</marker>
<rawString>Phan, X. H., Nguyen, L. M., and Horiguchi, S. 2008, April. Learning to Classify Short and Sparse Text &amp; Web with Hidden Topics from Large-scale Data Collections. In Proceedings of the 17th international conference on World Wide Web (pp. 91-100). ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sahami</author>
<author>T D Heilman</author>
</authors>
<title>A webbased kernel function for measuring the similarity of short text snippets.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web</booktitle>
<pages>377--386</pages>
<publisher>ACM.</publisher>
<marker>Sahami, Heilman, 2006</marker>
<rawString>Sahami, M., and Heilman, T. D. 2006, May. A webbased kernel function for measuring the similarity of short text snippets. In Proceedings of the 15th international conference on World Wide Web (pp. 377-386). ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Zubiaga</author>
<author>H Ji</author>
</authors>
<title>Harnessing web page directories for large-scale classification of tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the 22nd international conference on World Wide Web companion</booktitle>
<pages>225--226</pages>
<institution>International World Wide Web Conferences Steering Committee.</institution>
<marker>Zubiaga, Ji, 2013</marker>
<rawString>Zubiaga, A., and Ji, H. 2013, May. Harnessing web page directories for large-scale classification of tweets. In Proceedings of the 22nd international conference on World Wide Web companion (pp. 225-226). International World Wide Web Conferences Steering Committee.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>