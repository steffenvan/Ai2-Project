<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001264">
<title confidence="0.9974385">
FBK: Machine Translation Evaluation and Word Similarity metrics
for Semantic Textual Similarity
</title>
<author confidence="0.761533">
Jos´e Guilherme C. de Souza
</author>
<affiliation confidence="0.769178666666667">
Fondazione Bruno Kessler
University of Trento
Povo, Trento, Italy
</affiliation>
<email confidence="0.996173">
desouza@fbk.eu
</email>
<author confidence="0.889977">
Matteo Negri
</author>
<affiliation confidence="0.683940333333333">
Fondazione Bruno Kessler
Povo, Trento
Italy
</affiliation>
<email confidence="0.994533">
negri@fbk.eu
</email>
<author confidence="0.874282">
Yashar Mehdad
</author>
<affiliation confidence="0.682535666666667">
Fondazione Bruno Kessler
Povo, Trento
Italy
</affiliation>
<email confidence="0.996318">
mehdad@fbk.eu
</email>
<sectionHeader confidence="0.99736" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999936166666667">
This paper describes the participation of FBK
in the Semantic Textual Similarity (STS) task
organized within Semeval 2012. Our ap-
proach explores lexical, syntactic and se-
mantic machine translation evaluation metrics
combined with distributional and knowledge-
based word similarity metrics. Our best
model achieves 60.77% correlation with hu-
man judgements (Mean score) and ranked 20
out of 88 submitted runs in the Mean rank-
ing, where the average correlation across all
the sub-portions of the test set is considered.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999834780487805">
The Semantic Textual Similarity (STS) task pro-
posed at SemEval 2012 consists of examining the
degree of semantic equivalence between two sen-
tences and assigning a score to quantify such sim-
ilarity ranging from 0 (the two texts are about dif-
ferent topics) to 5 (the two texts are semantically
equivalent). The complete description of the task,
the datasets and the evaluation methodology adopted
can be found in (Agirre et al., 2012).
Typical approaches to measure semantic textual
similarity exploit information at the lexical level.
The proposed solutions range from calculating the
overlap of common words between the two text seg-
ments (Salton et al., 1997) to the application of
knowledge-based and corpus-based word similarity
metrics to cope with the low recall achieved by on
simple lexical matching (Mihalcea et al., 2006).
Our participation in the STS task is inspired by
previous work on paraphrase recognition, in which
machine translation (MT) evaluation metrics are
used to identify whether a pair of sentences are
semantically equivalent or not (Finch and Hwang,
2005; Wan et al., 2006). Our approach to semantic
textual similarity makes use of not only lexical in-
formation but also syntactic and semantic informa-
tion. To this aim, our metrics are based on different
natural language processing tools that provide syn-
tactic and semantic annotation. These include shal-
low parsing, constituency parsing, dependency pars-
ing, semantic roles labeling, discourse representa-
tion analyzer, and named entities recognition. In ad-
dition, we employed distributional and knowledge-
based word similarity metrics in an attempt to im-
prove the results given by the MT metrics. The com-
puted scores are used as features to train a regression
model in a supervised learning framework.
Our best run model achieves 60.77% correlation
with human judgements when evaluating the seman-
tic similarity of texts from the entire test set and
was ranked in the 20th position (out of 88 submit-
ted runs) in the Mean ranking.
</bodyText>
<sectionHeader confidence="0.994312" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999958928571428">
The system has been designed following a ma-
chine learning based approach in which a regres-
sion model is induced using different shallow and
deep linguistic features extracted from the datasets.
The STS training corpora are first preprocessed us-
ing different tools that annotate the texts at different
levels. Using the preprocessed data, the features are
extracted for each pair and used to train a model that
will be applied to unseen test pairs. The training
set is composed by three datasets (MSRpar, MSRvid
and SMTeuroparl) which combined contain a total
of 2234 instances. The test data is composed by a
different sample of the same three datasets plus in-
stances derived from two additional corpora (OnWN
</bodyText>
<page confidence="0.981975">
624
</page>
<note confidence="0.5304845">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 624–630,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.997396166666667">
and SMTnews). The datasets construction and anno-
tation are described in (Agirre et al., 2012).
Our system exploits two sets of features which re-
spectively build on MT evaluation metrics (2.1) and
word similarity metrics (2.2). The whole feature set
is summarized in figure 1.
</bodyText>
<subsectionHeader confidence="0.978289">
2.1 Machine Translation Evaluation Metrics
</subsectionHeader>
<bodyText confidence="0.999977">
MT evaluation metrics are designed to assess
whether the output of a MT system is semantically
equivalent to a set of reference translations. The
MT evaluation metrics described in this section, im-
plemented in the Asiya Open Toolkit for Automatic
Machine Translation (Meta-) Evaluation1 (Gim´enez
and M`arquez, 2010) are used to extract features at
different linguistic levels: lexical, syntactic and se-
mantic. For the syntactic and semantic levels, Asiya
calculates similarity measures based on the linguis-
tic elements provided by each kind of annotation.
Linguistic elements are defined as “the linguistic
units, structures, or relationships” (Gim´enez, 2008)
(e.g. dependency relations, discourse relations,
named entities, part-of-speech tags, among others).
(Gim´enez, 2008) defines two simple measures us-
ing the linguistic elements of a given linguistic level:
overlapping and matching. Overlapping is a
measure of the proportion of items inside the lin-
guistic elements of a certain type shared by both
texts. Matching is defined in the same way with
the difference that the order between the items inside
a linguistic element is taken into consideration. That
is, the items of a linguistic element are concatenated
in a single unit from left to right.
</bodyText>
<subsectionHeader confidence="0.946833">
2.1.1 Lexical Level
</subsectionHeader>
<bodyText confidence="0.999830666666667">
At the lexical level we explored different n-gram
and edit distance based metrics. The difference
among them is in the way each algorithm calcu-
lates the lexical similarity, which yields to differ-
ent results. We used the following n-gram-based
metrics: BLEU (Papineni et al., 2002), NIST (Dod-
dington, 2002), ROUGE (Lin and Och, 2004), GTM
(Melamed et al., 2003), METEOR (Banerjee and
Lavie, 2005). Besides those, we also used metrics
based on edit distance. Such metrics calculate the
number of edit operations (e.g. insertions, deletions,
and substitutions) necessary to transform one text
</bodyText>
<footnote confidence="0.94082">
1http://nlp.lsi.upc.edu/asiya/
</footnote>
<bodyText confidence="0.999875428571429">
into the other (the lower the number of edit oper-
ations, the higher the similarity score). The edit-
distance-based metrics used were: WER (Nieß en et
al., 2000), PER (Tillmann et al., 1997), TER (Snover
et al., 2006) and TER-Plus (Snover et al., 2009). The
lexical metrics form a group of metrics that we here-
after call lex.
</bodyText>
<subsectionHeader confidence="0.883958">
2.1.2 Syntactic Level
</subsectionHeader>
<bodyText confidence="0.99993358974359">
The syntactic level was explored by running con-
stituency parsing (cp), dependency parsing (dp),
and shallow parsing (sp). Constituency trees were
produced by the Max-Ent reranking parser (Char-
niak, 2005). The constituency parse trees were
exploited by using three different classes of met-
rics that were designed to calculate the similarities
between the trees of two texts: overlapping in
function of a given part-of-speech; matching in
function of a given constituency type; and syntactic
tree matching (STM) metric proposed by (Liu and
Gildea, 2005).
Dependency trees were obtained using MINI-
PAR (Lin, 2003). Two types of metrics were used
to calculate the similarity between two texts using
dependency trees. In the first, different similarity
measures were calculated taking into consideration
three different perspectives: overlap of words that
hang in the same level or in a deeper level of the
dependency tree; overlap between words that hang
directly from terminal nodes given a specified part-
of-speech; and overlap between words that are ruled
by non-terminal nodes given a specified grammat-
ical relation (subject, object, relative clause, among
others). The second type is an implementation of the
head-word chain matching introduced in (Liu and
Gildea, 2005).
The shallow syntax approach proposed by
(Gim´enez, 2008) uses three different tools to ex-
plore the parts-of-speech, word lemmas and base
phrases chunks, respectively: SVMTool (Gim´enez
and M`arquez, 2004), Freeling (Carreras et al., 2004)
and Phreco (Carreras et al., 2005). In this type of
metrics the idea is to measure the similarity between
the two texts using parts-of-speech and chunk types.
The following metrics were used: overlapping
according to the part-of-speech; overlapping ac-
cording to the chunk type; the accumulated NIST
metric (Doddington, 2002) scores over different
</bodyText>
<page confidence="0.997916">
625
</page>
<figureCaption confidence="0.999887">
Figure 1: A summary of the class of features explored.
</figureCaption>
<bodyText confidence="0.9810985">
sequences (lemmas, parts-of-speech, base phrase
chunks and chunk IOB labels).
</bodyText>
<subsectionHeader confidence="0.846671">
2.1.3 Semantic Level
</subsectionHeader>
<bodyText confidence="0.999988419354839">
At the semantic level we aplored three different
types of information, namely: discourse represen-
tations, named entities and semantic roles. Here-
after they are respectively referred to as dr, ne, and
sr features. The discourse relations are automat-
ically annotated using the C&amp;C Tools (Clark and
Curran, 2004). The following metrics using seman-
tic tree representations were proposed by (Gim´enez,
2008). A metric similar to the STM in which se-
mantic trees are used instead of constituency trees;
the overlapping between discourse representa-
tion structures according to their type; and the mor-
phosyntactic overlapping of discourse represen-
tation structures that share the same type.
Named entities metrics are calculated by com-
paring the entities that appear in each text. The
named entities were annotated using the BIOS pack-
age (Surdeanu et al., 2005). Two types of metrics
were used: the overlapping between the named
entities in each sentence according to their type and
the matching between the named entities in func-
tion of their type.
Semantic roles were automatically annotated us-
ing the SwiRL package (Surdeanu and Turmo,
2005). The arguments and adjuncts annotated in
each sentence are compared according to three dif-
ferent metrics: overlapping between the seman-
tic roles according to their type; the matching be-
tween the semantic roles according to their type; and
the overlapping of the roles without taking into
consideration their lexical realization.
</bodyText>
<subsectionHeader confidence="0.999513">
2.2 Word Similarity Metrics
</subsectionHeader>
<bodyText confidence="0.9998588">
Besides the MT evaluation metrics, we experi-
mented with lexical semantics by calculating word
similarity metrics. For that, we followed a distri-
butional and a knowledge-based word similarity ap-
proach.
</bodyText>
<subsectionHeader confidence="0.645363">
2.2.1 Distributional Word Similarity
</subsectionHeader>
<bodyText confidence="0.9999715">
As some previous work on semantic textual tex-
tual similarity (Mihalcea et al., 2006) and textual
entailment (Kouylekov et al., 2010; Mehdad et al.,
2010) have shown, distributional word similarity
measures can improve the performance of both tasks
by allowing matches between terms that are lexically
different. We measure the word similarity comput-
ing a set of Latent Semantic Analysis (LSA) metrics
over Wikipedia. The 200,000 most visited articles
of Wikipedia were extracted and cleaned to build the
</bodyText>
<page confidence="0.995977">
626
</page>
<bodyText confidence="0.999306583333333">
term-by-document matrix using the jLSI tool2.
Using this model we designed three different sim-
ilarity metrics that compute the similarity between
all elements in one text with all elements in the other
text. For two metrics we calculate the similarities
between different parts-of-speech: (i) similarity over
nouns and adjectives, and (ii) similarity over verbs.
The third metric computes the similarity between
all words in the two sentences. The similarity is
computed by averaging the pairwise similarity using
the LSA model between the elements of each text.
These metrics are hereafter called lsa.
</bodyText>
<subsectionHeader confidence="0.48949">
2.2.2 Knowledge-based Word Similarity
</subsectionHeader>
<bodyText confidence="0.999309">
In order to incorporate world knowledge informa-
tion about entities (persons, organizations, locations,
among others) into our model we experimented with
knowledge-based (thesaurus-based) word similarity
metrics. Usually such approaches have a very lim-
ited coverage of concepts due to the reduced size of
the available thesauri. In order to increase the cov-
erage we extracted concepts from the YAGO2 se-
mantic knowledge base (Hoffart et al., 2011) derived
from Wikipedia, Wordnet (Miller, 1995) and Geon-
ames3. YAGO2 contains knowledge about 10 mil-
lion entities and more than 120 million facts about
these entities.
In order to link the entities in the text to the enti-
ties in YAGO2 we have used “The Wiki Machine”
(TWM) tool4. The tool solves the linking problem
by disambiguating each entity mention in the text
(excluding pronouns) using Wikipedia to provide the
sense inventory and the training data (Giuliano et
al., 2009). After preprocessing the datasets with
TWM the entities are annotated with their respective
Wikipedia entries represented by their URLs. Using
the entity’s URL it is possible to retrieve the Word-
net synsets related to the entity’s entry in YAGO2
and explore different knowledge-based metrics to
compute word similarity between entities.
In our experiments we selected three differ-
ent algorithms to calculate word similarity using
YAGO2: Wu-Palmer (Zhibiao and Palmer, 1994),
the Leacock-Chodorow (Leacock et al., 1998) and
</bodyText>
<footnote confidence="0.9849855">
2http://hlt.fbk.eu/en/technology/jlsi
3http://www.geonames.org/
4http://thewikimachine.fbk.eu/html/
index.html
</footnote>
<bodyText confidence="0.999818571428571">
the path distance (score based on the shortest path
that connects the senses in the Wordnet hyper-
nym/hyponym taxonomy). Two classes of metrics
were designed: (i) the average of the similarity be-
tween all the entities in each sentence and (ii) the
similarity of the pair of elements which have the
shortest path in the Wordnet taxonomy among all
possible pairs. There are six different metrics using
the three algorithms in total. An extra metric was
designed using only TWM. The metric is calculated
by taking the number of common entities in the two
sentences divided by the total number of entities an-
notated in the two sentences. The metrics described
in this section are part of the yago group.
</bodyText>
<sectionHeader confidence="0.999065" genericHeader="method">
3 Experiments and Discussion
</sectionHeader>
<bodyText confidence="0.9999935">
In this section we present our experiments settings,
the configuration of the runs submitted and discuss
the results obtained. All our experiments were made
using half of the training set for training and half
for testing (development). Ten different random-
izations were run over the training data in order
to obtain ten different pairs of train/development
sets and reduce overfitting. We tried several differ-
ent regression algorithms and the best performance
was achieved with the implementation of Support
Vector Machines (SVM) of the SVMLight package
(Joachims, 1998). We used the radial basis function
kernel with default parameters without any special
tuning for the different datasets.
</bodyText>
<subsectionHeader confidence="0.99968">
3.1 Submitted Runs and Results
</subsectionHeader>
<bodyText confidence="0.997991615384615">
Based on the results achieved with different feature
sets over training data we have selected the best
combinations for our submission. The feature sets
for each run are:
Run 1: lex, lsa, yago, and a selection of
features in the cp, dp, sp, dr, ne and sr
groups, forming a total of 286 features.
Run 2: lex, lsa, and yago, in a total of 50
features.
Run 3: lex and lsa, forming a total of 43
features.
The results obtained by our three submitted runs
are summarized in table 1. The table reports the
</bodyText>
<page confidence="0.996441">
627
</page>
<table confidence="0.999603416666667">
Runs submitted Run 3 Base PE
Run 1 Run 2
Development 0.885 0.863 0.859 - -
MSp 0.249 0.512 0.516 0.433 0.577
MSv 0.611 0.780 0.777 0.299 0.818
SMTe 0.149 0.379 0.441 0.454 0.450
Wn 0.421 0.622 0.629 0.586 0.629
Test 0.243 0.547 0.608 0.390 0.608
SMTn
All 0.563 0.643 0.651 0.310 0.789
Allnrm 0.712 0.808 0.810 0.673 0.633
Mean 0.362 0.588 0.607 0.435 0.829
</table>
<tableCaption confidence="0.4996995">
Table 1: Results of each run for each dataset (MSRpar,
MSRvid, SMTeuroparl, OnWn, SMTnews) calculated
with the Pearson correlation between the system’s out-
puts and the gold standard annotation. Official scores ob-
tained using the three evaluation scores All, Allnrm and
Mean. Development row presents the average results for
each run in the whole training dataset. Base is the of-
ficial baseline system. Post Evaluation is the experiment
ran after the evaluation period with models trained for the
specific datasets.
</tableCaption>
<bodyText confidence="0.999981027027027">
Pearson correlation between the system output and
the gold standard annotation provided by the task or-
ganizers. The table also presents the official scores
used to rank the systems and described in (Agirre et
al., 2012). Our best model, Run 3, was ranked 20th
according to the Mean score, 25th according to the
RankNrm score and 32th according to the All score
among 88 submitted runs.
The “Development” row reports the results of our
three best models in the development phase. The
results obtained for the three training datasets are
higher than the results obtained for the testing. One
hypothesis that might explain this behavior is over-
fitting during the training phase due to the way we
divided the training set and carried out the experi-
ments. A different experiment setting to carry out
the development should be tried to evaluate this hy-
pothesis.
To our surprise, in the test datasets the results of
Run 1 and Run 3 swapped positions: in the train-
ing setting Run 1 was the best model and Run 3 the
third best. The performance of Run 3 was relatively
stable across the five datasets ranging from about
the 30th to the 48th position the exception being
the SMTnews dataset. In this dataset Run 3 was the
best performing run of the evaluation exercise (and
Run 2 the second). One possible explanation for this
behavior is the fact that Run 3 is based on lexical
features that do not take into consideration the syn-
tactic structure of the two texts and therefore is not
penalized by the noise introduced by the texts gen-
erated by MT systems. This hypothesis, however,
does not explain why Run 3 score for the SMTeu-
roparl dataset was below the baseline score. Error
analysis of the effects of different group of features
in the test datasets is required to better understand
such behaviors.
</bodyText>
<subsectionHeader confidence="0.999495">
3.2 Post-evaluation Experiments
</subsectionHeader>
<bodyText confidence="0.999956304347826">
After the evaluation period, as a first step towards
the required error analysis and a better comprehen-
sion of the potential of our approach, we performed
an experiment to assess the impact of having mod-
els trained for specific datasets. In this experiment,
each training dataset (MSRpar, MSRvid and SMTeu-
roparl) was used to train a model. Each dataset’s
model was tested on its respective test dataset. The
model for the surprise datasets (OnWn and SMT-
news) were trained using the whole training dataset.
We used the Run 3 feature set (the best run in the
official evaluation). The results of the experiment
are reported in the column “Exp” of table 1. The
impact of having specific models for each dataset
is high. The Mean score goes from .607 to .829
and improvements are also observed in the All score
(0.789). These scores would rank our system at the
7th position in the Mean rank. However, it is impor-
tant to notice that in a real-world setting, knowledge
about the source of data is not always available. We
consider that having a general model that does not
rely on this kind of information represents a more re-
alistic way to confront with real-world applications.
</bodyText>
<sectionHeader confidence="0.989683" genericHeader="method">
4 Final Remarks
</sectionHeader>
<bodyText confidence="0.977294">
In this paper we described FBK’s participation in
the STS Semeval 2012 task. Our approach is based
on a combination of MT evaluation metrics, distri-
butional, and knowledge-based word similarity met-
rics. Our best run achieved the 20th position among
88 runs in the Mean overall ranking. An error analy-
sis of the problematic test pairs is required to under-
stand the potential of our feature sets and improve
the overall performance of our approach. Along this
direction, a first experiment with our best features
and a different strategy already led to significant im-
provements in the Mean and All scores (from .651 to
.789 and from .607 to .829, respectively).
</bodyText>
<sectionHeader confidence="0.984263" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9996388">
This work has been partially supported by the EC-
funded project CoSyne (FP7-ICT-4-24853).
The authors would like to thank Claudio Giuliano
for kindly helping us to preprocess the datasets with
the Wiki Machine.
</bodyText>
<sectionHeader confidence="0.992682" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999218705263158">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonza-
lez. 2012. SemEval-2012 Task 6: A Pilot on Semantic
Textual Similarity. In 6th International Workshop on
Semantic Evaluation (SemEval 2012), in conjunction
with the First Joint Conference on Lexical and Com-
putational Semantics (*SEM 2012).
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with improved
correlation with human judgments. In ACL 2005
Workshop on Intrinsic and Extrinsic Evaluation Mea-
sures for MT and/or Summarization.
Xavier Carreras, Isaac Chao, Llu´ıs Padro, and Muntsa
Padr´o. 2004. Freeling: An open-source suite of lan-
guage analyzers. In 4th International Conference on
Language Resources and Evaluation (LREC), pages
239–242.
Xavier Carreras, Llu´ıs M`arquez, and Jorge Catro. 2005.
Filtering-Ranking Perceptron Learning. Machine
Learning, 60:41–75.
Eugene Charniak. 2005. Coarse-to-fine n-best parsing
and MaxEnt discriminative reranking. In Proceedings
of the 43rd Annual Meeting on, volume 1, pages 173–
180.
Stephen Clark and James R. Curran. 2004. Parsing the
WSJ using CCG and log-linear models. In ACL ’04
Proceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics.
George Doddington. 2002. Automatic evaluation of ma-
chine translation quality using n-gram co-occurrence
statistics. In Proceedings of the Second Interna-
tional Conference on Human Language Technology
Research, pages 138–145. Morgan Kaufmann Publish-
ers Inc.
Andrew Finch and YS Hwang. 2005. Using ma-
chine translation evaluation techniques to determine
sentence-level semantic equivalence. In Third Inter-
national Workshop on Paraphrasing, pages 17–24.
Jes´us Gim´enez and Llu´ıs M`arquez. 2004. SVMTool: A
general POS tagger generator based on Support Vector
Machines. In 4th International Conference on Lan-
guage Resources and Evaluation (LREC), pages 43–
46.
Jes´us Gim´enez and Llu´ıs M`arquez. 2010. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-) Evaluation. The Prague Bulletin of Mathe-
matical Linguistics, (94):77–86.
J. Gim´enez. 2008. Empirical Machine Translation and
its Evaluation. Ph.D. thesis.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and Carlo
Strapparava. 2009. Kernel methods for minimally su-
pervised wsd. Computational Linguistics, 35(4):513–
528.
Johannes Hoffart, Fabian M. FM Suchanek, Klaus
Berberich, Edwin Lewis Kelham, Gerard de Melo, and
Gerhard Weikum. 2011. YAGO2: Exploring and
Querying World Knowledge in Time, Space, Context,
and Many Languages. In 20th International World
Wide Web Conference (WWW 2011), pages 229–232.
Thorsten Joachims. 1998. Making Large-Scale SVM
Learning Practical. In Bernhard Scholkopf, Christo-
pher J. C. Burges, and Alexander J. Smola, editors,
Advances in Kernel Methods - Support Vector Learn-
ing, pages 41–56. MIT Press, Cambridge, USA.
Milen Kouylekov, Yashar Mehdad, and Matteo Negri.
2010. Mining Wikipedia for Large-Scale Reposito-
ries of Context-Sensitive Entailment Rules. In Seventh
international conference on Language Resources and
Evaluation (LREC 2010), pages 3550–3553, La Val-
letta, Malta.
Claudia Leacock, George A. Miller, and Martin
Chodorow. 1998. Using corpus statistics and Word-
Net relations for sense identification. Computational
Linguistics, 24(1):147–166.
C.Y. Lin and F.J. Och. 2004. Automatic evaluation
of machine translation quality using longest common
subsequence and skip-bigram statistics. In Proceed-
ings of the 42nd Annual Meeting on Association for
Computational Linguistics, page 605. Association for
Computational Linguistics.
Dekang Lin. 2003. Dependency-Based Evaluation of
Minipar. Text, Speech and Language Technology,
20:317–329.
Ding Liu and Daniel Gildea. 2005. Syntactic features
for evaluation of machine translation. In ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization, num-
ber June, pages 25–32.
Yashar Mehdad, Alessandro Moschitti, and Fabio Mas-
simo Zanzotto. 2010. Syntactic/semantic structures
for textual entailment recognition. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the ACL, number June,
pages 1020–1028.
I. Dan Melamed, Ryan Green, and Joseph P. Turian.
2003. Precision and Recall of Machine Translation. In
</reference>
<page confidence="0.999056">
629
</page>
<bodyText confidence="0.9565115">
of the 32nd annual meeting on Association for Com-
putational Linguistics, pages 133–138.
</bodyText>
<reference confidence="0.998263462962963">
Proceedings of the Joint Conference on Human Lan-
guage Technology and the North American Chapter of
the Association for Computational Linguistics (HLT-
NAACL).
Rada Mihalcea, Courtney Corley, and Carlo Strappar-
ava. 2006. Corpus-based and knowledge-based mea-
sures of text semantic similarity. In Proceedings of the
American Association for Artificial Intelligence, pages
775–780.
George A. Miller. 1995. WordNet: A Lexical Database
for English. Communications of the ACM, 38(11):39–
41.
Sonja Nieß en, Franz Josef Och, Gregor Leusch, and Her-
mann Ney. 2000. An evaluation tool for machine
translation: Fast evaluation for MT research. In Lan-
guage Resources and Evaluation, pages 0–6.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: a Method for Automatic Eval-
uation of Machine Translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), number July, pages 311–318.
Gerard Salton, Amit Singhal, and Mandar Mitra. 1997.
Automatic text structuring and summarization. Infor-
mation Processing &amp;, 33(2):193–207.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study of
Translation Edit Rate with Targeted Human Annota-
tion. In Association for Machine Translation in the
Americas.
Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2009. TER-Plus: paraphrase, se-
mantic, and alignment enhancements to Translation
Edit Rate. Machine Translation, 23(2-3):117–127,
December.
Mihai Surdeanu and Jordi Turmo. 2005. Semantic
role labeling using complete syntactic analysis. In
9th Conference on Computational Natural Language
Learning (CoNLL), number June, pages 221–224.
Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005.
Named Entity Recognition from Spontaneous Open-
domain Speech. In 9th International Conference on
Speech Communication and Technology (Interspeech),
pages 3433–3436.
C Tillmann, S Vogel, H Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP Based Search for Statistical
Translation. In Fifth European Conference on Speech
Communication and Technology, pages 2667–2670.
Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris.
2006. Using Dependency-Based Features to Take the
”Para-farce” out of Paraphrase. In 2006 Australasian
Language Technology Workshop (ALTW2006), num-
ber 2005, pages 131–138.
Wu Zhibiao and Martha Palmer. 1994. Verb Seman-
tics and Lexical Selection. In ACL ’94 Proceedings
</reference>
<page confidence="0.997628">
630
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.069887">
<title confidence="0.995907">FBK: Machine Translation Evaluation and Word Similarity for Semantic Textual Similarity</title>
<author confidence="0.9941845">Jos´e Guilherme C de_Fondazione Bruno</author>
<affiliation confidence="0.999838">University of</affiliation>
<address confidence="0.870422">Povo, Trento,</address>
<email confidence="0.998325">desouza@fbk.eu</email>
<author confidence="0.8470115">Matteo Fondazione Bruno</author>
<affiliation confidence="0.610493">Povo,</affiliation>
<email confidence="0.952868">negri@fbk.eu</email>
<author confidence="0.5028715">Yashar Fondazione Bruno</author>
<affiliation confidence="0.639421">Povo,</affiliation>
<email confidence="0.986078">mehdad@fbk.eu</email>
<abstract confidence="0.996923230769231">This paper describes the participation of FBK in the Semantic Textual Similarity (STS) task organized within Semeval 2012. Our approach explores lexical, syntactic and semantic machine translation evaluation metrics combined with distributional and knowledgebased word similarity metrics. Our best model achieves 60.77% correlation with hujudgements and ranked 20 of 88 submitted runs in the ranking, where the average correlation across all the sub-portions of the test set is considered.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez</author>
</authors>
<date>2012</date>
<booktitle>SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<contexts>
<context position="1319" citStr="Agirre et al., 2012" startWordPosition="195" endWordPosition="198">judgements (Mean score) and ranked 20 out of 88 submitted runs in the Mean ranking, where the average correlation across all the sub-portions of the test set is considered. 1 Introduction The Semantic Textual Similarity (STS) task proposed at SemEval 2012 consists of examining the degree of semantic equivalence between two sentences and assigning a score to quantify such similarity ranging from 0 (the two texts are about different topics) to 5 (the two texts are semantically equivalent). The complete description of the task, the datasets and the evaluation methodology adopted can be found in (Agirre et al., 2012). Typical approaches to measure semantic textual similarity exploit information at the lexical level. The proposed solutions range from calculating the overlap of common words between the two text segments (Salton et al., 1997) to the application of knowledge-based and corpus-based word similarity metrics to cope with the low recall achieved by on simple lexical matching (Mihalcea et al., 2006). Our participation in the STS task is inspired by previous work on paraphrase recognition, in which machine translation (MT) evaluation metrics are used to identify whether a pair of sentences are seman</context>
<context position="3888" citStr="Agirre et al., 2012" startWordPosition="600" endWordPosition="603">xtracted for each pair and used to train a model that will be applied to unseen test pairs. The training set is composed by three datasets (MSRpar, MSRvid and SMTeuroparl) which combined contain a total of 2234 instances. The test data is composed by a different sample of the same three datasets plus instances derived from two additional corpora (OnWN 624 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 624–630, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics and SMTnews). The datasets construction and annotation are described in (Agirre et al., 2012). Our system exploits two sets of features which respectively build on MT evaluation metrics (2.1) and word similarity metrics (2.2). The whole feature set is summarized in figure 1. 2.1 Machine Translation Evaluation Metrics MT evaluation metrics are designed to assess whether the output of a MT system is semantically equivalent to a set of reference translations. The MT evaluation metrics described in this section, implemented in the Asiya Open Toolkit for Automatic Machine Translation (Meta-) Evaluation1 (Gim´enez and M`arquez, 2010) are used to extract features at different linguistic leve</context>
<context position="15861" citStr="Agirre et al., 2012" startWordPosition="2468" endWordPosition="2471">rson correlation between the system’s outputs and the gold standard annotation. Official scores obtained using the three evaluation scores All, Allnrm and Mean. Development row presents the average results for each run in the whole training dataset. Base is the official baseline system. Post Evaluation is the experiment ran after the evaluation period with models trained for the specific datasets. Pearson correlation between the system output and the gold standard annotation provided by the task organizers. The table also presents the official scores used to rank the systems and described in (Agirre et al., 2012). Our best model, Run 3, was ranked 20th according to the Mean score, 25th according to the RankNrm score and 32th according to the All score among 88 submitted runs. The “Development” row reports the results of our three best models in the development phase. The results obtained for the three training datasets are higher than the results obtained for the testing. One hypothesis that might explain this behavior is overfitting during the training phase due to the way we divided the training set and carried out the experiments. A different experiment setting to carry out the development should b</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez. 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In 6th International Workshop on Semantic Evaluation (SemEval 2012), in conjunction with the First Joint Conference on Lexical and Computational Semantics (*SEM 2012).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Alon Lavie</author>
</authors>
<title>METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.</title>
<date>2005</date>
<booktitle>In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</booktitle>
<contexts>
<context position="5788" citStr="Banerjee and Lavie, 2005" startWordPosition="893" endWordPosition="896">ay with the difference that the order between the items inside a linguistic element is taken into consideration. That is, the items of a linguistic element are concatenated in a single unit from left to right. 2.1.1 Lexical Level At the lexical level we explored different n-gram and edit distance based metrics. The difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1http://nlp.lsi.upc.edu/asiya/ into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was expl</context>
</contexts>
<marker>Banerjee, Lavie, 2005</marker>
<rawString>Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. In ACL 2005 Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Isaac Chao</author>
<author>Llu´ıs Padro</author>
<author>Muntsa Padr´o</author>
</authors>
<title>Freeling: An open-source suite of language analyzers.</title>
<date>2004</date>
<booktitle>In 4th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>239--242</pages>
<marker>Carreras, Chao, Padro, Padr´o, 2004</marker>
<rawString>Xavier Carreras, Isaac Chao, Llu´ıs Padro, and Muntsa Padr´o. 2004. Freeling: An open-source suite of language analyzers. In 4th International Conference on Language Resources and Evaluation (LREC), pages 239–242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Llu´ıs M`arquez</author>
<author>Jorge Catro</author>
</authors>
<title>Filtering-Ranking Perceptron Learning.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--41</pages>
<marker>Carreras, M`arquez, Catro, 2005</marker>
<rawString>Xavier Carreras, Llu´ıs M`arquez, and Jorge Catro. 2005. Filtering-Ranking Perceptron Learning. Machine Learning, 60:41–75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on,</booktitle>
<volume>1</volume>
<pages>173--180</pages>
<contexts>
<context position="6563" citStr="Charniak, 2005" startWordPosition="1014" endWordPosition="1016">s) necessary to transform one text 1http://nlp.lsi.upc.edu/asiya/ into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part-of-speech; matching in function of a given constituency type; and syntactic tree matching (STM) metric proposed by (Liu and Gildea, 2005). Dependency trees were obtained using MINIPAR (Lin, 2003). Two types of metrics were used to calculate the similarity between two texts using dependency trees. In the first, different similarity measures were calculated taking into consideration three</context>
</contexts>
<marker>Charniak, 2005</marker>
<rawString>Eugene Charniak. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In Proceedings of the 43rd Annual Meeting on, volume 1, pages 173– 180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Parsing the WSJ using CCG and log-linear models.</title>
<date>2004</date>
<booktitle>In ACL ’04 Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="8684" citStr="Clark and Curran, 2004" startWordPosition="1336" endWordPosition="1339"> overlapping according to the part-of-speech; overlapping according to the chunk type; the accumulated NIST metric (Doddington, 2002) scores over different 625 Figure 1: A summary of the class of features explored. sequences (lemmas, parts-of-speech, base phrase chunks and chunk IOB labels). 2.1.3 Semantic Level At the semantic level we aplored three different types of information, namely: discourse representations, named entities and semantic roles. Hereafter they are respectively referred to as dr, ne, and sr features. The discourse relations are automatically annotated using the C&amp;C Tools (Clark and Curran, 2004). The following metrics using semantic tree representations were proposed by (Gim´enez, 2008). A metric similar to the STM in which semantic trees are used instead of constituency trees; the overlapping between discourse representation structures according to their type; and the morphosyntactic overlapping of discourse representation structures that share the same type. Named entities metrics are calculated by comparing the entities that appear in each text. The named entities were annotated using the BIOS package (Surdeanu et al., 2005). Two types of metrics were used: the overlapping between</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCG and log-linear models. In ACL ’04 Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic evaluation of machine translation quality using n-gram co-occurrence statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the Second International Conference on Human Language Technology Research,</booktitle>
<pages>138--145</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="5698" citStr="Doddington, 2002" startWordPosition="879" endWordPosition="881">elements of a certain type shared by both texts. Matching is defined in the same way with the difference that the order between the items inside a linguistic element is taken into consideration. That is, the items of a linguistic element are concatenated in a single unit from left to right. 2.1.1 Lexical Level At the lexical level we explored different n-gram and edit distance based metrics. The difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1http://nlp.lsi.upc.edu/asiya/ into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group</context>
<context position="8194" citStr="Doddington, 2002" startWordPosition="1263" endWordPosition="1264"> chain matching introduced in (Liu and Gildea, 2005). The shallow syntax approach proposed by (Gim´enez, 2008) uses three different tools to explore the parts-of-speech, word lemmas and base phrases chunks, respectively: SVMTool (Gim´enez and M`arquez, 2004), Freeling (Carreras et al., 2004) and Phreco (Carreras et al., 2005). In this type of metrics the idea is to measure the similarity between the two texts using parts-of-speech and chunk types. The following metrics were used: overlapping according to the part-of-speech; overlapping according to the chunk type; the accumulated NIST metric (Doddington, 2002) scores over different 625 Figure 1: A summary of the class of features explored. sequences (lemmas, parts-of-speech, base phrase chunks and chunk IOB labels). 2.1.3 Semantic Level At the semantic level we aplored three different types of information, namely: discourse representations, named entities and semantic roles. Hereafter they are respectively referred to as dr, ne, and sr features. The discourse relations are automatically annotated using the C&amp;C Tools (Clark and Curran, 2004). The following metrics using semantic tree representations were proposed by (Gim´enez, 2008). A metric simila</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the Second International Conference on Human Language Technology Research, pages 138–145. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Finch</author>
<author>YS Hwang</author>
</authors>
<title>Using machine translation evaluation techniques to determine sentence-level semantic equivalence.</title>
<date>2005</date>
<booktitle>In Third International Workshop on Paraphrasing,</booktitle>
<pages>17--24</pages>
<contexts>
<context position="1967" citStr="Finch and Hwang, 2005" startWordPosition="294" endWordPosition="297">sure semantic textual similarity exploit information at the lexical level. The proposed solutions range from calculating the overlap of common words between the two text segments (Salton et al., 1997) to the application of knowledge-based and corpus-based word similarity metrics to cope with the low recall achieved by on simple lexical matching (Mihalcea et al., 2006). Our participation in the STS task is inspired by previous work on paraphrase recognition, in which machine translation (MT) evaluation metrics are used to identify whether a pair of sentences are semantically equivalent or not (Finch and Hwang, 2005; Wan et al., 2006). Our approach to semantic textual similarity makes use of not only lexical information but also syntactic and semantic information. To this aim, our metrics are based on different natural language processing tools that provide syntactic and semantic annotation. These include shallow parsing, constituency parsing, dependency parsing, semantic roles labeling, discourse representation analyzer, and named entities recognition. In addition, we employed distributional and knowledgebased word similarity metrics in an attempt to improve the results given by the MT metrics. The comp</context>
</contexts>
<marker>Finch, Hwang, 2005</marker>
<rawString>Andrew Finch and YS Hwang. 2005. Using machine translation evaluation techniques to determine sentence-level semantic equivalence. In Third International Workshop on Paraphrasing, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>SVMTool: A general POS tagger generator based on Support Vector Machines.</title>
<date>2004</date>
<booktitle>In 4th International Conference on Language Resources and Evaluation (LREC),</booktitle>
<pages>43--46</pages>
<marker>Gim´enez, M`arquez, 2004</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2004. SVMTool: A general POS tagger generator based on Support Vector Machines. In 4th International Conference on Language Resources and Evaluation (LREC), pages 43– 46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Asiya: An Open Toolkit for Automatic Machine Translation (Meta-) Evaluation. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2010</date>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2010. Asiya: An Open Toolkit for Automatic Machine Translation (Meta-) Evaluation. The Prague Bulletin of Mathematical Linguistics, (94):77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Gim´enez</author>
</authors>
<title>Empirical Machine Translation and its Evaluation.</title>
<date>2008</date>
<tech>Ph.D. thesis.</tech>
<marker>Gim´enez, 2008</marker>
<rawString>J. Gim´enez. 2008. Empirical Machine Translation and its Evaluation. Ph.D. thesis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudio Giuliano</author>
<author>Alfio Massimiliano Gliozzo</author>
<author>Carlo Strapparava</author>
</authors>
<title>Kernel methods for minimally supervised wsd.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>4</issue>
<pages>528</pages>
<contexts>
<context position="12179" citStr="Giuliano et al., 2009" startWordPosition="1877" endWordPosition="1880">f the available thesauri. In order to increase the coverage we extracted concepts from the YAGO2 semantic knowledge base (Hoffart et al., 2011) derived from Wikipedia, Wordnet (Miller, 1995) and Geonames3. YAGO2 contains knowledge about 10 million entities and more than 120 million facts about these entities. In order to link the entities in the text to the entities in YAGO2 we have used “The Wiki Machine” (TWM) tool4. The tool solves the linking problem by disambiguating each entity mention in the text (excluding pronouns) using Wikipedia to provide the sense inventory and the training data (Giuliano et al., 2009). After preprocessing the datasets with TWM the entities are annotated with their respective Wikipedia entries represented by their URLs. Using the entity’s URL it is possible to retrieve the Wordnet synsets related to the entity’s entry in YAGO2 and explore different knowledge-based metrics to compute word similarity between entities. In our experiments we selected three different algorithms to calculate word similarity using YAGO2: Wu-Palmer (Zhibiao and Palmer, 1994), the Leacock-Chodorow (Leacock et al., 1998) and 2http://hlt.fbk.eu/en/technology/jlsi 3http://www.geonames.org/ 4http://thew</context>
</contexts>
<marker>Giuliano, Gliozzo, Strapparava, 2009</marker>
<rawString>Claudio Giuliano, Alfio Massimiliano Gliozzo, and Carlo Strapparava. 2009. Kernel methods for minimally supervised wsd. Computational Linguistics, 35(4):513– 528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Hoffart</author>
<author>Fabian M FM Suchanek</author>
<author>Klaus Berberich</author>
<author>Edwin Lewis Kelham</author>
<author>Gerard de Melo</author>
<author>Gerhard Weikum</author>
</authors>
<title>YAGO2: Exploring and Querying World Knowledge in Time, Space, Context, and Many Languages.</title>
<date>2011</date>
<booktitle>In 20th International World Wide Web Conference (WWW</booktitle>
<pages>229--232</pages>
<marker>Hoffart, Suchanek, Berberich, Kelham, de Melo, Weikum, 2011</marker>
<rawString>Johannes Hoffart, Fabian M. FM Suchanek, Klaus Berberich, Edwin Lewis Kelham, Gerard de Melo, and Gerhard Weikum. 2011. YAGO2: Exploring and Querying World Knowledge in Time, Space, Context, and Many Languages. In 20th International World Wide Web Conference (WWW 2011), pages 229–232.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thorsten Joachims</author>
</authors>
<title>Making Large-Scale SVM Learning Practical.</title>
<date>1998</date>
<booktitle>Advances in Kernel Methods - Support Vector Learning,</booktitle>
<pages>41--56</pages>
<editor>In Bernhard Scholkopf, Christopher J. C. Burges, and Alexander J. Smola, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, USA.</location>
<contexts>
<context position="14113" citStr="Joachims, 1998" startWordPosition="2171" endWordPosition="2172">he yago group. 3 Experiments and Discussion In this section we present our experiments settings, the configuration of the runs submitted and discuss the results obtained. All our experiments were made using half of the training set for training and half for testing (development). Ten different randomizations were run over the training data in order to obtain ten different pairs of train/development sets and reduce overfitting. We tried several different regression algorithms and the best performance was achieved with the implementation of Support Vector Machines (SVM) of the SVMLight package (Joachims, 1998). We used the radial basis function kernel with default parameters without any special tuning for the different datasets. 3.1 Submitted Runs and Results Based on the results achieved with different feature sets over training data we have selected the best combinations for our submission. The feature sets for each run are: Run 1: lex, lsa, yago, and a selection of features in the cp, dp, sp, dr, ne and sr groups, forming a total of 286 features. Run 2: lex, lsa, and yago, in a total of 50 features. Run 3: lex and lsa, forming a total of 43 features. The results obtained by our three submitted r</context>
</contexts>
<marker>Joachims, 1998</marker>
<rawString>Thorsten Joachims. 1998. Making Large-Scale SVM Learning Practical. In Bernhard Scholkopf, Christopher J. C. Burges, and Alexander J. Smola, editors, Advances in Kernel Methods - Support Vector Learning, pages 41–56. MIT Press, Cambridge, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milen Kouylekov</author>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
</authors>
<title>Mining Wikipedia for Large-Scale Repositories of Context-Sensitive Entailment Rules.</title>
<date>2010</date>
<booktitle>In Seventh international conference on Language Resources and Evaluation (LREC 2010),</booktitle>
<pages>3550--3553</pages>
<location>La Valletta,</location>
<contexts>
<context position="10237" citStr="Kouylekov et al., 2010" startWordPosition="1576" endWordPosition="1579">: overlapping between the semantic roles according to their type; the matching between the semantic roles according to their type; and the overlapping of the roles without taking into consideration their lexical realization. 2.2 Word Similarity Metrics Besides the MT evaluation metrics, we experimented with lexical semantics by calculating word similarity metrics. For that, we followed a distributional and a knowledge-based word similarity approach. 2.2.1 Distributional Word Similarity As some previous work on semantic textual textual similarity (Mihalcea et al., 2006) and textual entailment (Kouylekov et al., 2010; Mehdad et al., 2010) have shown, distributional word similarity measures can improve the performance of both tasks by allowing matches between terms that are lexically different. We measure the word similarity computing a set of Latent Semantic Analysis (LSA) metrics over Wikipedia. The 200,000 most visited articles of Wikipedia were extracted and cleaned to build the 626 term-by-document matrix using the jLSI tool2. Using this model we designed three different similarity metrics that compute the similarity between all elements in one text with all elements in the other text. For two metrics</context>
</contexts>
<marker>Kouylekov, Mehdad, Negri, 2010</marker>
<rawString>Milen Kouylekov, Yashar Mehdad, and Matteo Negri. 2010. Mining Wikipedia for Large-Scale Repositories of Context-Sensitive Entailment Rules. In Seventh international conference on Language Resources and Evaluation (LREC 2010), pages 3550–3553, La Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>George A Miller</author>
<author>Martin Chodorow</author>
</authors>
<title>Using corpus statistics and WordNet relations for sense identification.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="12698" citStr="Leacock et al., 1998" startWordPosition="1953" endWordPosition="1956">pronouns) using Wikipedia to provide the sense inventory and the training data (Giuliano et al., 2009). After preprocessing the datasets with TWM the entities are annotated with their respective Wikipedia entries represented by their URLs. Using the entity’s URL it is possible to retrieve the Wordnet synsets related to the entity’s entry in YAGO2 and explore different knowledge-based metrics to compute word similarity between entities. In our experiments we selected three different algorithms to calculate word similarity using YAGO2: Wu-Palmer (Zhibiao and Palmer, 1994), the Leacock-Chodorow (Leacock et al., 1998) and 2http://hlt.fbk.eu/en/technology/jlsi 3http://www.geonames.org/ 4http://thewikimachine.fbk.eu/html/ index.html the path distance (score based on the shortest path that connects the senses in the Wordnet hypernym/hyponym taxonomy). Two classes of metrics were designed: (i) the average of the similarity between all the entities in each sentence and (ii) the similarity of the pair of elements which have the shortest path in the Wordnet taxonomy among all possible pairs. There are six different metrics using the three algorithms in total. An extra metric was designed using only TWM. The metri</context>
</contexts>
<marker>Leacock, Miller, Chodorow, 1998</marker>
<rawString>Claudia Leacock, George A. Miller, and Martin Chodorow. 1998. Using corpus statistics and WordNet relations for sense identification. Computational Linguistics, 24(1):147–166.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Y Lin</author>
<author>F J Och</author>
</authors>
<title>Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<pages>605</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5725" citStr="Lin and Och, 2004" startWordPosition="883" endWordPosition="886"> shared by both texts. Matching is defined in the same way with the difference that the order between the items inside a linguistic element is taken into consideration. That is, the items of a linguistic element are concatenated in a single unit from left to right. 2.1.1 Lexical Level At the lexical level we explored different n-gram and edit distance based metrics. The difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1http://nlp.lsi.upc.edu/asiya/ into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereaft</context>
</contexts>
<marker>Lin, Och, 2004</marker>
<rawString>C.Y. Lin and F.J. Och. 2004. Automatic evaluation of machine translation quality using longest common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, page 605. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Dependency-Based Evaluation of Minipar.</title>
<date>2003</date>
<journal>Text, Speech and Language Technology,</journal>
<pages>20--317</pages>
<contexts>
<context position="6969" citStr="Lin, 2003" startWordPosition="1078" endWordPosition="1079"> The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part-of-speech; matching in function of a given constituency type; and syntactic tree matching (STM) metric proposed by (Liu and Gildea, 2005). Dependency trees were obtained using MINIPAR (Lin, 2003). Two types of metrics were used to calculate the similarity between two texts using dependency trees. In the first, different similarity measures were calculated taking into consideration three different perspectives: overlap of words that hang in the same level or in a deeper level of the dependency tree; overlap between words that hang directly from terminal nodes given a specified partof-speech; and overlap between words that are ruled by non-terminal nodes given a specified grammatical relation (subject, object, relative clause, among others). The second type is an implementation of the h</context>
</contexts>
<marker>Lin, 2003</marker>
<rawString>Dekang Lin. 2003. Dependency-Based Evaluation of Minipar. Text, Speech and Language Technology, 20:317–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>25--32</pages>
<location>number</location>
<contexts>
<context position="6911" citStr="Liu and Gildea, 2005" startWordPosition="1067" endWordPosition="1070"> a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part-of-speech; matching in function of a given constituency type; and syntactic tree matching (STM) metric proposed by (Liu and Gildea, 2005). Dependency trees were obtained using MINIPAR (Lin, 2003). Two types of metrics were used to calculate the similarity between two texts using dependency trees. In the first, different similarity measures were calculated taking into consideration three different perspectives: overlap of words that hang in the same level or in a deeper level of the dependency tree; overlap between words that hang directly from terminal nodes given a specified partof-speech; and overlap between words that are ruled by non-terminal nodes given a specified grammatical relation (subject, object, relative clause, am</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, number June, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Alessandro Moschitti</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Syntactic/semantic structures for textual entailment recognition.</title>
<date>2010</date>
<booktitle>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, number June,</booktitle>
<pages>1020--1028</pages>
<contexts>
<context position="10259" citStr="Mehdad et al., 2010" startWordPosition="1580" endWordPosition="1583">e semantic roles according to their type; the matching between the semantic roles according to their type; and the overlapping of the roles without taking into consideration their lexical realization. 2.2 Word Similarity Metrics Besides the MT evaluation metrics, we experimented with lexical semantics by calculating word similarity metrics. For that, we followed a distributional and a knowledge-based word similarity approach. 2.2.1 Distributional Word Similarity As some previous work on semantic textual textual similarity (Mihalcea et al., 2006) and textual entailment (Kouylekov et al., 2010; Mehdad et al., 2010) have shown, distributional word similarity measures can improve the performance of both tasks by allowing matches between terms that are lexically different. We measure the word similarity computing a set of Latent Semantic Analysis (LSA) metrics over Wikipedia. The 200,000 most visited articles of Wikipedia were extracted and cleaned to build the 626 term-by-document matrix using the jLSI tool2. Using this model we designed three different similarity metrics that compute the similarity between all elements in one text with all elements in the other text. For two metrics we calculate the simi</context>
</contexts>
<marker>Mehdad, Moschitti, Zanzotto, 2010</marker>
<rawString>Yashar Mehdad, Alessandro Moschitti, and Fabio Massimo Zanzotto. 2010. Syntactic/semantic structures for textual entailment recognition. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, number June, pages 1020–1028.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
<author>Ryan Green</author>
<author>Joseph P Turian</author>
</authors>
<title>Precision and Recall of Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Joint Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLTNAACL).</booktitle>
<contexts>
<context position="5753" citStr="Melamed et al., 2003" startWordPosition="888" endWordPosition="891">tching is defined in the same way with the difference that the order between the items inside a linguistic element is taken into consideration. That is, the items of a linguistic element are concatenated in a single unit from left to right. 2.1.1 Lexical Level At the lexical level we explored different n-gram and edit distance based metrics. The difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1http://nlp.lsi.upc.edu/asiya/ into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic</context>
</contexts>
<marker>Melamed, Green, Turian, 2003</marker>
<rawString>I. Dan Melamed, Ryan Green, and Joseph P. Turian. 2003. Precision and Recall of Machine Translation. In Proceedings of the Joint Conference on Human Language Technology and the North American Chapter of the Association for Computational Linguistics (HLTNAACL).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the American Association for Artificial Intelligence,</booktitle>
<pages>775--780</pages>
<contexts>
<context position="1716" citStr="Mihalcea et al., 2006" startWordPosition="255" endWordPosition="258">om 0 (the two texts are about different topics) to 5 (the two texts are semantically equivalent). The complete description of the task, the datasets and the evaluation methodology adopted can be found in (Agirre et al., 2012). Typical approaches to measure semantic textual similarity exploit information at the lexical level. The proposed solutions range from calculating the overlap of common words between the two text segments (Salton et al., 1997) to the application of knowledge-based and corpus-based word similarity metrics to cope with the low recall achieved by on simple lexical matching (Mihalcea et al., 2006). Our participation in the STS task is inspired by previous work on paraphrase recognition, in which machine translation (MT) evaluation metrics are used to identify whether a pair of sentences are semantically equivalent or not (Finch and Hwang, 2005; Wan et al., 2006). Our approach to semantic textual similarity makes use of not only lexical information but also syntactic and semantic information. To this aim, our metrics are based on different natural language processing tools that provide syntactic and semantic annotation. These include shallow parsing, constituency parsing, dependency par</context>
<context position="10190" citStr="Mihalcea et al., 2006" startWordPosition="1569" endWordPosition="1572">e compared according to three different metrics: overlapping between the semantic roles according to their type; the matching between the semantic roles according to their type; and the overlapping of the roles without taking into consideration their lexical realization. 2.2 Word Similarity Metrics Besides the MT evaluation metrics, we experimented with lexical semantics by calculating word similarity metrics. For that, we followed a distributional and a knowledge-based word similarity approach. 2.2.1 Distributional Word Similarity As some previous work on semantic textual textual similarity (Mihalcea et al., 2006) and textual entailment (Kouylekov et al., 2010; Mehdad et al., 2010) have shown, distributional word similarity measures can improve the performance of both tasks by allowing matches between terms that are lexically different. We measure the word similarity computing a set of Latent Semantic Analysis (LSA) metrics over Wikipedia. The 200,000 most visited articles of Wikipedia were extracted and cleaned to build the 626 term-by-document matrix using the jLSI tool2. Using this model we designed three different similarity metrics that compute the similarity between all elements in one text with </context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the American Association for Artificial Intelligence, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: A Lexical Database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<pages>41</pages>
<contexts>
<context position="11747" citStr="Miller, 1995" startWordPosition="1805" endWordPosition="1806">A model between the elements of each text. These metrics are hereafter called lsa. 2.2.2 Knowledge-based Word Similarity In order to incorporate world knowledge information about entities (persons, organizations, locations, among others) into our model we experimented with knowledge-based (thesaurus-based) word similarity metrics. Usually such approaches have a very limited coverage of concepts due to the reduced size of the available thesauri. In order to increase the coverage we extracted concepts from the YAGO2 semantic knowledge base (Hoffart et al., 2011) derived from Wikipedia, Wordnet (Miller, 1995) and Geonames3. YAGO2 contains knowledge about 10 million entities and more than 120 million facts about these entities. In order to link the entities in the text to the entities in YAGO2 we have used “The Wiki Machine” (TWM) tool4. The tool solves the linking problem by disambiguating each entity mention in the text (excluding pronouns) using Wikipedia to provide the sense inventory and the training data (Giuliano et al., 2009). After preprocessing the datasets with TWM the entities are annotated with their respective Wikipedia entries represented by their URLs. Using the entity’s URL it is p</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A. Miller. 1995. WordNet: A Lexical Database for English. Communications of the ACM, 38(11):39– 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sonja Nieß en</author>
<author>Franz Josef Och</author>
<author>Gregor Leusch</author>
<author>Hermann Ney</author>
</authors>
<title>An evaluation tool for machine translation: Fast evaluation for MT research.</title>
<date>2000</date>
<booktitle>In Language Resources and Evaluation,</booktitle>
<pages>0--6</pages>
<contexts>
<context position="6173" citStr="en et al., 2000" startWordPosition="950" endWordPosition="953">ilarity, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1http://nlp.lsi.upc.edu/asiya/ into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part</context>
</contexts>
<marker>en, Och, Leusch, Ney, 2000</marker>
<rawString>Sonja Nieß en, Franz Josef Och, Gregor Leusch, and Hermann Ney. 2000. An evaluation tool for machine translation: Fast evaluation for MT research. In Language Resources and Evaluation, pages 0–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>Weijing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), number July,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="5673" citStr="Papineni et al., 2002" startWordPosition="874" endWordPosition="877">f items inside the linguistic elements of a certain type shared by both texts. Matching is defined in the same way with the difference that the order between the items inside a linguistic element is taken into consideration. That is, the items of a linguistic element are concatenated in a single unit from left to right. 2.1.1 Lexical Level At the lexical level we explored different n-gram and edit distance based metrics. The difference among them is in the way each algorithm calculates the lexical similarity, which yields to different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1http://nlp.lsi.upc.edu/asiya/ into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lex</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), number July, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerard Salton</author>
<author>Amit Singhal</author>
<author>Mandar Mitra</author>
</authors>
<title>Automatic text structuring and summarization.</title>
<date>1997</date>
<booktitle>Information Processing &amp;amp;,</booktitle>
<pages>33--2</pages>
<contexts>
<context position="1546" citStr="Salton et al., 1997" startWordPosition="229" endWordPosition="232">ask proposed at SemEval 2012 consists of examining the degree of semantic equivalence between two sentences and assigning a score to quantify such similarity ranging from 0 (the two texts are about different topics) to 5 (the two texts are semantically equivalent). The complete description of the task, the datasets and the evaluation methodology adopted can be found in (Agirre et al., 2012). Typical approaches to measure semantic textual similarity exploit information at the lexical level. The proposed solutions range from calculating the overlap of common words between the two text segments (Salton et al., 1997) to the application of knowledge-based and corpus-based word similarity metrics to cope with the low recall achieved by on simple lexical matching (Mihalcea et al., 2006). Our participation in the STS task is inspired by previous work on paraphrase recognition, in which machine translation (MT) evaluation metrics are used to identify whether a pair of sentences are semantically equivalent or not (Finch and Hwang, 2005; Wan et al., 2006). Our approach to semantic textual similarity makes use of not only lexical information but also syntactic and semantic information. To this aim, our metrics ar</context>
</contexts>
<marker>Salton, Singhal, Mitra, 1997</marker>
<rawString>Gerard Salton, Amit Singhal, and Mandar Mitra. 1997. Automatic text structuring and summarization. Information Processing &amp;amp;, 33(2):193–207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation. In Association for Machine Translation in the Americas.</title>
<date>2006</date>
<contexts>
<context position="6229" citStr="Snover et al., 2006" startWordPosition="960" endWordPosition="963">the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1http://nlp.lsi.upc.edu/asiya/ into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part-of-speech; matching in function of a given constituency</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Association for Machine Translation in the Americas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew G Snover</author>
<author>Nitin Madnani</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
</authors>
<title>TER-Plus: paraphrase, semantic, and alignment enhancements to Translation Edit Rate. Machine Translation,</title>
<date>2009</date>
<pages>23--2</pages>
<contexts>
<context position="6264" citStr="Snover et al., 2009" startWordPosition="966" endWordPosition="969"> BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1http://nlp.lsi.upc.edu/asiya/ into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part-of-speech; matching in function of a given constituency type; and syntactic tree matching </context>
</contexts>
<marker>Snover, Madnani, Dorr, Schwartz, 2009</marker>
<rawString>Matthew G. Snover, Nitin Madnani, Bonnie Dorr, and Richard Schwartz. 2009. TER-Plus: paraphrase, semantic, and alignment enhancements to Translation Edit Rate. Machine Translation, 23(2-3):117–127, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
</authors>
<title>Semantic role labeling using complete syntactic analysis.</title>
<date>2005</date>
<booktitle>In 9th Conference on Computational Natural Language Learning (CoNLL), number June,</booktitle>
<pages>221--224</pages>
<contexts>
<context position="9510" citStr="Surdeanu and Turmo, 2005" startWordPosition="1467" endWordPosition="1470">g between discourse representation structures according to their type; and the morphosyntactic overlapping of discourse representation structures that share the same type. Named entities metrics are calculated by comparing the entities that appear in each text. The named entities were annotated using the BIOS package (Surdeanu et al., 2005). Two types of metrics were used: the overlapping between the named entities in each sentence according to their type and the matching between the named entities in function of their type. Semantic roles were automatically annotated using the SwiRL package (Surdeanu and Turmo, 2005). The arguments and adjuncts annotated in each sentence are compared according to three different metrics: overlapping between the semantic roles according to their type; the matching between the semantic roles according to their type; and the overlapping of the roles without taking into consideration their lexical realization. 2.2 Word Similarity Metrics Besides the MT evaluation metrics, we experimented with lexical semantics by calculating word similarity metrics. For that, we followed a distributional and a knowledge-based word similarity approach. 2.2.1 Distributional Word Similarity As s</context>
</contexts>
<marker>Surdeanu, Turmo, 2005</marker>
<rawString>Mihai Surdeanu and Jordi Turmo. 2005. Semantic role labeling using complete syntactic analysis. In 9th Conference on Computational Natural Language Learning (CoNLL), number June, pages 221–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Jordi Turmo</author>
<author>Eli Comelles</author>
</authors>
<title>Named Entity Recognition from Spontaneous Opendomain Speech.</title>
<date>2005</date>
<booktitle>In 9th International Conference on Speech Communication and Technology (Interspeech),</booktitle>
<pages>3433--3436</pages>
<contexts>
<context position="9227" citStr="Surdeanu et al., 2005" startWordPosition="1421" endWordPosition="1424">ations are automatically annotated using the C&amp;C Tools (Clark and Curran, 2004). The following metrics using semantic tree representations were proposed by (Gim´enez, 2008). A metric similar to the STM in which semantic trees are used instead of constituency trees; the overlapping between discourse representation structures according to their type; and the morphosyntactic overlapping of discourse representation structures that share the same type. Named entities metrics are calculated by comparing the entities that appear in each text. The named entities were annotated using the BIOS package (Surdeanu et al., 2005). Two types of metrics were used: the overlapping between the named entities in each sentence according to their type and the matching between the named entities in function of their type. Semantic roles were automatically annotated using the SwiRL package (Surdeanu and Turmo, 2005). The arguments and adjuncts annotated in each sentence are compared according to three different metrics: overlapping between the semantic roles according to their type; the matching between the semantic roles according to their type; and the overlapping of the roles without taking into consideration their lexical </context>
</contexts>
<marker>Surdeanu, Turmo, Comelles, 2005</marker>
<rawString>Mihai Surdeanu, Jordi Turmo, and Eli Comelles. 2005. Named Entity Recognition from Spontaneous Opendomain Speech. In 9th International Conference on Speech Communication and Technology (Interspeech), pages 3433–3436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Tillmann</author>
<author>S Vogel</author>
<author>H Ney</author>
<author>A Zubiaga</author>
<author>H Sawaf</author>
</authors>
<title>Accelerated DP Based Search for Statistical Translation. In</title>
<date>1997</date>
<booktitle>Fifth European Conference on Speech Communication and Technology,</booktitle>
<pages>2667--2670</pages>
<contexts>
<context position="6202" citStr="Tillmann et al., 1997" startWordPosition="955" endWordPosition="958">o different results. We used the following n-gram-based metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), ROUGE (Lin and Och, 2004), GTM (Melamed et al., 2003), METEOR (Banerjee and Lavie, 2005). Besides those, we also used metrics based on edit distance. Such metrics calculate the number of edit operations (e.g. insertions, deletions, and substitutions) necessary to transform one text 1http://nlp.lsi.upc.edu/asiya/ into the other (the lower the number of edit operations, the higher the similarity score). The editdistance-based metrics used were: WER (Nieß en et al., 2000), PER (Tillmann et al., 1997), TER (Snover et al., 2006) and TER-Plus (Snover et al., 2009). The lexical metrics form a group of metrics that we hereafter call lex. 2.1.2 Syntactic Level The syntactic level was explored by running constituency parsing (cp), dependency parsing (dp), and shallow parsing (sp). Constituency trees were produced by the Max-Ent reranking parser (Charniak, 2005). The constituency parse trees were exploited by using three different classes of metrics that were designed to calculate the similarities between the trees of two texts: overlapping in function of a given part-of-speech; matching in funct</context>
</contexts>
<marker>Tillmann, Vogel, Ney, Zubiaga, Sawaf, 1997</marker>
<rawString>C Tillmann, S Vogel, H Ney, A. Zubiaga, and H. Sawaf. 1997. Accelerated DP Based Search for Statistical Translation. In Fifth European Conference on Speech Communication and Technology, pages 2667–2670.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Wan</author>
<author>Mark Dras</author>
<author>Robert Dale</author>
<author>C´ecile Paris</author>
</authors>
<title>Using Dependency-Based Features to Take the ”Para-farce” out of Paraphrase. In</title>
<date>2006</date>
<booktitle>Australasian Language Technology Workshop (ALTW2006), number</booktitle>
<pages>131--138</pages>
<contexts>
<context position="1986" citStr="Wan et al., 2006" startWordPosition="298" endWordPosition="301">imilarity exploit information at the lexical level. The proposed solutions range from calculating the overlap of common words between the two text segments (Salton et al., 1997) to the application of knowledge-based and corpus-based word similarity metrics to cope with the low recall achieved by on simple lexical matching (Mihalcea et al., 2006). Our participation in the STS task is inspired by previous work on paraphrase recognition, in which machine translation (MT) evaluation metrics are used to identify whether a pair of sentences are semantically equivalent or not (Finch and Hwang, 2005; Wan et al., 2006). Our approach to semantic textual similarity makes use of not only lexical information but also syntactic and semantic information. To this aim, our metrics are based on different natural language processing tools that provide syntactic and semantic annotation. These include shallow parsing, constituency parsing, dependency parsing, semantic roles labeling, discourse representation analyzer, and named entities recognition. In addition, we employed distributional and knowledgebased word similarity metrics in an attempt to improve the results given by the MT metrics. The computed scores are use</context>
</contexts>
<marker>Wan, Dras, Dale, Paris, 2006</marker>
<rawString>Stephen Wan, Mark Dras, Robert Dale, and C´ecile Paris. 2006. Using Dependency-Based Features to Take the ”Para-farce” out of Paraphrase. In 2006 Australasian Language Technology Workshop (ALTW2006), number 2005, pages 131–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wu Zhibiao</author>
<author>Martha Palmer</author>
</authors>
<title>Verb Semantics and Lexical Selection.</title>
<date>1994</date>
<booktitle>In ACL ’94 Proceedings</booktitle>
<contexts>
<context position="12653" citStr="Zhibiao and Palmer, 1994" startWordPosition="1947" endWordPosition="1950">ating each entity mention in the text (excluding pronouns) using Wikipedia to provide the sense inventory and the training data (Giuliano et al., 2009). After preprocessing the datasets with TWM the entities are annotated with their respective Wikipedia entries represented by their URLs. Using the entity’s URL it is possible to retrieve the Wordnet synsets related to the entity’s entry in YAGO2 and explore different knowledge-based metrics to compute word similarity between entities. In our experiments we selected three different algorithms to calculate word similarity using YAGO2: Wu-Palmer (Zhibiao and Palmer, 1994), the Leacock-Chodorow (Leacock et al., 1998) and 2http://hlt.fbk.eu/en/technology/jlsi 3http://www.geonames.org/ 4http://thewikimachine.fbk.eu/html/ index.html the path distance (score based on the shortest path that connects the senses in the Wordnet hypernym/hyponym taxonomy). Two classes of metrics were designed: (i) the average of the similarity between all the entities in each sentence and (ii) the similarity of the pair of elements which have the shortest path in the Wordnet taxonomy among all possible pairs. There are six different metrics using the three algorithms in total. An extra </context>
</contexts>
<marker>Zhibiao, Palmer, 1994</marker>
<rawString>Wu Zhibiao and Martha Palmer. 1994. Verb Semantics and Lexical Selection. In ACL ’94 Proceedings</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>