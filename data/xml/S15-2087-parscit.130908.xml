<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003024">
<title confidence="0.859672">
IIIT-H at SemEval 2015: Twitter Sentiment Analysis
The good, the bad and the neutral!
</title>
<author confidence="0.91833">
Ayushi Dalmia, Manish Gupta∗, Vasudeva Varma
</author>
<affiliation confidence="0.87403">
Search and Information Extraction Lab
International Institute of Information Technology, Hyderabad
</affiliation>
<email confidence="0.997227">
{ayushi.dalmia@research.iiit.ac.in, manish.gupta@iiit.ac.in, vv@iiit.ac.in }
</email>
<sectionHeader confidence="0.993851" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999393619047619">
This paper describes the system that was sub-
mitted to SemEval2015 Task 10: Sentiment
Analysis in Twitter. We participated in Sub-
task B: Message Polarity Classification. The
task is a message level classification of tweets
into positive, negative and neutral sentiments.
Our model is primarily a supervised one which
consists of well designed features fed into an
SVM classifier. In previous runs of this task,
it was found that lexicons played an important
role in determining the sentiment of a tweet.
We use existing lexicons to extract lexicon
specific features. The lexicon based features
are further augmented by tweet specific fea-
tures. We also improve our system by using
acronym and emoticon dictionaries. The pro-
posed system achieves an F1 score of 59.83
and 67.04 on the Test Data and Progress Data
respectively. This placed us at the 181h posi-
tion for the Test Dataset and the 161h position
for the Progress Test Dataset.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.985294795918368">
Micro-blogging has become a very popular com-
munication tool among Internet users. Millions of
users share opinions on different aspects of life, ev-
eryday on popular websites such as Twitter, Tum-
blr and Facebook. Spurred by this growth, compa-
nies and media organizations are increasingly seek-
ing ways to mine these social media for informa-
tion about what people think about their companies
and products. Political parties may be interested to
know if people support their program or not. Social
organizations may need to know people’s opinion
on current debates. All this information can be ob-
tained from micro-blogging services, as their users
post their opinions on many aspects of their life reg-
ularly.
Twitter contains an enormous number of text posts
∗The author is also a researcher at Microsoft (gman-
ish@microsoft.com)
and the rate of posts is increasing every day. Its au-
dience varies from regular users to celebrities, com-
pany representatives, politicians, and even coun-
try presidents. Therefore, it is possible to collect
text posts of users from different social and inter-
est groups. However, analyzing Twitter data comes
with its own bag of difficulties. Tweets are small in
length, thus ambiguous. The informal style of writ-
ing, a distinct usage of orthography, acronymization
and a different set of elements like hashtags, user
mentions demand a different approach to solve this
problem.
In this work we present the description of the super-
vised machine learning system developed while par-
ticipating in the shared task of message based sen-
timent analysis in SemEval 2015 (Rosenthal et al.,
2015). The system takes as input a tweet message,
pre-processes it, extracts features and finally classi-
fies it as either positive, negative or neutral. Tweets
in the positive and negative classes are subjective in
nature. However, the neutral class consists of both
subjective tweets which do not have any polarity as
well as objective tweets.
Our paper is organized as follows. We discuss re-
lated work in Section 2. In Section 3, we discuss the
existing resources which we use in our system. In
Section 4 we present the proposed system and give
a detailed description for the same. We present ex-
perimental results and the ranking of our system for
different datasets in Section 5. The paper is summa-
rized in Section 6.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999711428571429">
Sentiment analysis has been an active area of re-
search since a long time. A number of sur-
veys (Pang and Lee, 2008; Liu and Zhang, 2012) and
books (Liu, 2010) give a thorough analysis of the
existing techniques in sentiment analysis. Attempts
have been made to analyze sentiments at different
levels starting from document (Pang and Lee, 2004),
</bodyText>
<page confidence="0.934461">
520
</page>
<bodyText confidence="0.922783375">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 520–526,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
sentences (Hu and Liu, 2004) to phrases (Wilson et
al., 2009; Agarwal et al., 2009). However, micro-
blogging data is different from regular text as it is ex-
tremely noisy in nature. A lot of interesting work has
been done in order to identify sentiments from Twit-
ter micro-blogging data also. (Go et al., 2009) used
emoticons as noisy labels and distant supervision to
classify tweets into positive or negative class. (Agar-
wal et al., 2011) introduced POS-specific prior po-
larity features along with using a tree kernel for
tweet classification. Besides these two major pa-
pers, a lot of work from the previous runs of the Se-
mEval is available (Rosenthal et al., 2014; Nakov et
al., 2013).
</bodyText>
<sectionHeader confidence="0.999894" genericHeader="method">
3 Resources
</sectionHeader>
<subsectionHeader confidence="0.999735">
3.1 Annotated Data
</subsectionHeader>
<bodyText confidence="0.999991142857143">
Tweet IDs labeled as positive, negative or neutral
were given by the task organizers. In order to build
the system we first downloaded these tweets. The
task organizers provided us with a certain number of
tweet IDs. However, it was not possible to retrieve
the content of all the tweet IDs due to changes in the
privacy settings. Some of the tweets were probably
deleted or may not be public at the time of down-
load. Thus we were not able to download the tweet
content of all the tweets IDs provided by the organiz-
ers. For the training and the dev-test datasets, while
the organizers provided us with 9684 and 1654 tweet
IDs respectively, we were able to retrieve only 7966
and 1368 tweets, respectively.
</bodyText>
<subsectionHeader confidence="0.999848">
3.2 Sentiment Lexicons
</subsectionHeader>
<bodyText confidence="0.9974826">
It has been found that lexicons play an important
role in determining the polarity of a message. Sev-
eral lexicons have been proposed in the past which
are used popularly in the field of sentiment analy-
sis. We use the following lexicons to generate our
lexicon based features: (1) Bing Liu’s Opinion Lexi-
con1, (2) MPQA Subjectivity Lexicon (Wilson et al.,
2005), (3) NRC Hashtag Sentiment Lexicon (Mo-
hammad et al., 2013), and (4) Sentiment140 Lexi-
con (Mohammad et al., 2013).
</bodyText>
<subsectionHeader confidence="0.989521">
3.3 Dictionary
</subsectionHeader>
<bodyText confidence="0.990276">
Besides the above sentiment lexicons, we used two
other dictionaries described as follows.
</bodyText>
<footnote confidence="0.98916">
1http://www.cs.uic.edu/˜liub/FBS/
opinion-lexicon-English.rar
</footnote>
<listItem confidence="0.957151444444445">
• Emoticon Dictionary: We use the emoticons
list 2 and manually annotate the related senti-
ment. We categorize the emoticons into four
classes as follows: (1) Extremely- Positive, (2)
Positive, (3) Extremely- Negative, and (4) Neg-
ative.
• Acronym Dictionary: We crawl the
noslang.com website 3 in order to obtain
the acronym expansion of the most commonly
</listItem>
<bodyText confidence="0.8810485">
used acronyms on the web. The acronym
dictionary helps in expanding the tweet text
and thereby improves the overall sentiment
score. The acronym dictionary has 5297
entries. For example, asap has the translation
As soon as possible.
Other than this we also use Tweet NLP (Owoputi
et al., 2013), a Twitter specific tweet tokenizer and
tagger which provides a fast and robust Java-based
tokenizer and part-of-speech tagger for Twitter.
</bodyText>
<sectionHeader confidence="0.980627" genericHeader="method">
4 System Overview
</sectionHeader>
<bodyText confidence="0.999723636363636">
Figure 1 gives a brief overview of our system. In the
offline stage, the system takes the tweet IDs and the
N-Gram model as inputs (shown in red) to learn a
classifier. The classifier is then used online to pro-
cess a test tweet and output (shown in green) its sen-
timent. The basic building blocks of the system in-
clude Pre-processing, Feature Extraction and Clas-
sification. We first build a baseline model based on
unigram, bigrams and trigrams and later add more
features to it. In this section we discuss each mod-
ule in detail.
</bodyText>
<subsectionHeader confidence="0.999679">
4.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.999446">
Since the tweets are very noisy, they need a lot of
pre-processing. Table 1 lists the various steps of pre-
processing applied on the tweets. They are discussed
as follows.
</bodyText>
<listItem confidence="0.981438">
• Tokenization
</listItem>
<bodyText confidence="0.923815">
After downloading the tweets using the tweet
IDs provided in the dataset, we first tokenize
them. This is done using the Tweet-NLP
tool (Gimpel et al., 2011) developed by ARK
Social Media Search. This tool tokenizes the
</bodyText>
<footnote confidence="0.999178333333333">
2http://en.wikipedia.org/wiki/List_of_
emoticons
3http://www.noslang.com/dictionary/
</footnote>
<page confidence="0.99026">
521
</page>
<figure confidence="0.997340769230769">
Tweet IDs
Sentiment of
Test Tweet
Classifier
Feature
Extractor
Test Tweet
N-Gram
Model
Tweet
Downloader
Tokenization
Preprocessing
</figure>
<bodyText confidence="0.994518">
place the emoticons by their sentiment polarity
by looking up in the Emoticon Dictionary gen-
erated using the dictionary mentioned in Sec-
tion 3.
</bodyText>
<listItem confidence="0.983565">
• Remove Urls
</listItem>
<bodyText confidence="0.9999106">
The urls which are present in the tweet are
shortened due to the limitation on the length
of the tweet text. These shortened urls do
not carry much information regarding the sen-
timent of the tweet. Thus these are removed.
</bodyText>
<figureCaption confidence="0.993735">
Figure 1: System Architecture (Red: Inputs, Green: Out-
puts).
</figureCaption>
<tableCaption confidence="0.993298">
Table 1: List of Pre-processing Steps.
</tableCaption>
<table confidence="0.954362583333333">
Tokenisation
Remove Non-English Tweets
Replace Emoticons
Remove Urls
Remove Target Mentions
Remove Punctuations from Hashtags
Handle Sequences of Repeated Characters
Remove Numbers
Remove Nouns and Prepositions
Remove Stop Words
Handle Negative Mentions
Expand Acronyms
</table>
<bodyText confidence="0.999446625">
tweet and returns the POS tags of the tweet
along with the confidence score. It is important
to note that this is a Twitter specific tagger and
tags the Twitter specific entries like emoticons,
hashtags and mentions along with the regular
parts of speech. After obtaining the tokenized
and tagged tweets, we move to the next step of
preprocessing.
</bodyText>
<listItem confidence="0.971672">
• Remove Non-English Tweets
</listItem>
<bodyText confidence="0.99874025">
Twitter allows more than 60 languages. How-
ever, this work currently focuses on English to-
kens only. We remove the tweets with non-
English tokens.
</bodyText>
<listItem confidence="0.963006">
• Replace Emoticons
</listItem>
<bodyText confidence="0.9033315">
Emoticons play an important role in determin-
ing the sentiment of the tweet. Hence we re-
</bodyText>
<listItem confidence="0.985756">
• Remove Target Mentions
</listItem>
<bodyText confidence="0.9829814">
The target mentions in a tweet done using ‘@’
are usually the twitter handle of people or orga-
nizations. This information is also not needed
to determine the sentiment of the tweet. Hence
they are removed.
</bodyText>
<listItem confidence="0.990287">
• Remove Punctuations from Hashtags
</listItem>
<bodyText confidence="0.9786922">
Hashtags represent a concise summary of the
tweet, and hence are very critical. In order to
capture the relevant information from hashtags,
all special characters and punctuations are re-
moved before using them as a feature.
</bodyText>
<listItem confidence="0.989427">
• Handle Sequences of Repeated Characters
</listItem>
<bodyText confidence="0.976091285714286">
Twitter provides a platform for users to ex-
press their opinion in an informal way. Tweets
are written in a noisy form, without any fo-
cus on correct structure and spelling. Spell
correction is an important part in sentiment
analysis of user-generated content. People use
words like ‘coooool’ and ‘hunnnnngry’ in or-
der to emphasize the emotion. In order to cap-
ture such expressions, we replace the sequence
of more than three similar characters by three
characters. For example, ‘wooooow’ is re-
placed by ‘wooow’. We replace by three char-
acters so as to distinguish words like ‘wow’ and
‘wooooow’.
</bodyText>
<listItem confidence="0.940962166666667">
• Remove Numbers
Numbers are of no use when measuring sen-
timent. Thus, numbers which are obtained as
tokenized units from the tokenizer are removed
in order to refine the tweet content.
• Remove Nouns and Prepositions
</listItem>
<bodyText confidence="0.99704">
Given a tweet token, we identify the word as
a noun word by looking at its part-of-speech
</bodyText>
<page confidence="0.986716">
522
</page>
<bodyText confidence="0.999131166666667">
tag assigned by the tokenizer. If the majority
sense (most commonly used sense) of that word
is noun, we discard the word. Noun words do
not carry sentiment and thus are of no use in our
experiment. Similarly we remove prepositions
too.
</bodyText>
<listItem confidence="0.984333">
• Remove Stop Words
</listItem>
<bodyText confidence="0.999294">
Stop words play a negative role in the task
of sentiment classification. Stop words occur
in both positive and negative training set, thus
adding more ambiguity in the model formation.
Also, stop words do not carry any sentiment in-
formation and thus are of no use.
</bodyText>
<listItem confidence="0.985551">
• Handle Negative Mentions
</listItem>
<bodyText confidence="0.999510166666667">
Negation plays a very important role in de-
termining the sentiment of the tweet. Tweets
consist of various notions of negation. Words
which are either ‘no’, ‘not’ or ending with
‘n’t’ are replaced by a common word indicat-
ing negation.
</bodyText>
<listItem confidence="0.956058">
• Expand Acronyms
</listItem>
<bodyText confidence="0.99420775">
As described in Section 3 we use an acronym
expansion list. In the pre-processing step we
expand the acronyms if they are present in the
tweet.
</bodyText>
<subsectionHeader confidence="0.715106">
4.2 Baseline Model
</subsectionHeader>
<bodyText confidence="0.999995666666667">
We first generate a baseline model as discussed
in (Bakliwal et al., 2012). We perform the pre-
processing steps listed in Section 4.1 and learn the
positive, negative and neutral frequencies of uni-
grams, bigrams and trigrams in our training data.
Every token is given three probability scores: Posi-
tive Probability (Pp), Negative Probability (Np) and
Neutral Probability (NEp). Given a token, let Pf
denote the frequency in positive training set, Nf de-
note the frequency in negative training set and NEf
denote the frequency in neutral training set. The
probability scores are then computed as follows.
</bodyText>
<equation confidence="0.942274555555556">
Pf
Pp = (1)
Pf + Nf + NEf
Nf
Np = (2)
Pf + Nf + NEf
NEf
NEp = (3)
Pf + Nf + NEf
</equation>
<bodyText confidence="0.999985892857143">
Next we create a feature vector of tokens which
can distinguish the sentiment of the tweet with high
confidence. For example, presence of tokens like
am happy!, love love, bullsh*t ! helps in determin-
ing that the tweet carries positive, negative or neu-
tral sentiment with high confidence. We call such
words, Emotion Determiner. A token is consid-
ered to be an Emotion Determiner if the probabil-
ity of the emotion for any one sentiment is greater
than or equal to the probability of the other two sen-
timents by a certain threshold. It is found that we
need different thresholds for unigrams, bigrams and
trigrams. The threshold parameters are tuned and
the optimal threshold values are found to be 0.7, 0.8
and 0.9 for the unigram, bigram and trigram tokens,
respectively. Note that before calculating the proba-
bility values, we filter out those tokens which are in-
frequent (appear in less than 10 tweets). This serves
as a baseline model. Thus, our baseline model is
learned using a training dataset which contains for
every given tweet, a binary vector of length equal
to the set of Emotion Determiners with 1 indicat-
ing its presence and 0 indicating its absence in the
tweet. After building this model we will append
the features discussed in Section 4.3. After append-
ing the features to the baseline model, we get en-
hanced richer vectors containing Emotion Determin-
ers along with the new feature values.
</bodyText>
<subsectionHeader confidence="0.998127">
4.3 Feature Extraction
</subsectionHeader>
<bodyText confidence="0.999986529411765">
We propose a set of features listed in Table 2 for our
experiments. There are a total of 34 features. We
calculate these features for the whole tweet in case
of message based sentiment analysis. We can divide
the features into two classes: a) Tweet Based Fea-
tures, and b) Lexicon Based Features. Table 2 sum-
marizes the features used in our experiment. Here
features f1 − f22 are tweet based features while fea-
tures f23 − f34 are lexicon based features.
A number of our features are based on prior polar-
ity score of the tweet. For obtaining the prior polar-
ity of words, we use AFINN dictionary 4 and extend
it using SENTIWORDNET (Esuli and Sebastiani,
2006). We first look up the tokens in the tweet in
the AFINN lexicon. This dictionary of about 2490
English language words assigns every word a pleas-
antness score between -5 (Negative) and +5 (Posi-
</bodyText>
<footnote confidence="0.988158">
4http://www2.imm.dtu.dk/pubdb/views/
publication_details.php?id=6010
</footnote>
<page confidence="0.998524">
523
</page>
<tableCaption confidence="0.997978">
Table 2: Description of the Features used in the Model.
</tableCaption>
<table confidence="0.998371481481482">
Feature Description Feature ID
Prior Polarity Score of the Tweet f0
Brown Clusters f1
Percentage of Capitalised Words f2
# of Positive Capitalised Words f3
# of Negative Capitalised Words f4
Presence of Capitalised Words f5
# of Positive Hashtags f6
# of Negative Hashtags f7
# of Positive Emoticons f8
# of Extremely Positive Emoticons f9
# of Negative Emoticons f10
# of Extremely Negative Emoticons f11
# of Negation f12
# Positive POS Tags f13
# Negative POS Tags f14
Total POS Tags Score f15
# of special characters like ? ! and * f16.f17, f18
# of POS (Noun, Verb, Adverb, Adjective) f19, f20, f21, f22
# of words with nonzero score using Bing Liu’s Opinion Lexicon f23
# of words with nonzero score using MPQA Subjectivity Lexicon f24
# of words with nonzero score using NRC Hashtag Sentiment Lexicon f25
# of words with nonzero score using Sentiment140 Lexicon f26
Maximum positive score for a token in the message using Bing Liu’s Opinion Lexicon f27
Maximum positive score for a token in the message using MPQA Subjectivity Lexicon f28
Maximum positive score for a token in the message using NRC Hashtag Sentiment Lexicon f29
Maximum positive score for a token in the message using Sentiment140 Lexicon f30
</table>
<figureCaption confidence="0.86992025">
Total score of the message using Bing Liu’s Opinion Lexicon f31
Total score of the message using MPQA Subjectivity Lexicon f32
Total score of the message using NRC Hashtag Sentiment Lexicon f33
Total score of the message using Sentiment140 Lexicon f34
</figureCaption>
<bodyText confidence="0.999925692307692">
tive). We normalize the scores by diving each score
by the scale (which is equal to 5) to obtain a score
between -1 and +1. If a word is not directly found
in the dictionary we retrieve all its synonyms from
SENTIWORDNET. We then look for each of the
synonyms in AFINN. If any synonym is found in
AFINN, we assign the original word the same pleas-
antness score as its synonym. If none of the syn-
onyms is present in AFINN, we perform a second
level look up in the SENTIWORDNET dictionary to
find synonyms of synonyms. If the word is present
in SENTIWORDNET, we assign the score retrieved
from SENTIWORDNET (between -1 and +1).
</bodyText>
<page confidence="0.998922">
524
</page>
<tableCaption confidence="0.999692">
Table 3: Accuracy on 3-way classification task extending the baseline with additional features. All fi refer to Table 2.
</tableCaption>
<table confidence="0.999935285714286">
Model F Measure
Positive Class Negative Class Neutral Class Macro-Average
Baseline Model 36.93 30.66 15.38 33.79
+f0 37.14 36.48 59.02 36.81
+ f0 - f1 63.73 47.19 66.24 55.46
+ f0 - f5 63.66 47.50 66.08 55.58
+ f0 - f7 63.58 47.55 66.08 55.56
+ f0 - f11 63.18 46.98 66.06 55.08
+ f0 - f12 63.14 48.52 65.75 55.83
+ f0 - f15 63.84 48.40 66.11 56.12
+ f0 + f18 64.42 48.57 66.30 56.50
+ f0 - f22 64.00 48.09 66.48 56.04
+ f0 - f22 + 67.50 52.26 66.57 59.83
Lexicon Based Features (f23 - f34)
</table>
<footnote confidence="0.91335">
5http://scikit-learn.org/stable/modules/svm.
</footnote>
<subsectionHeader confidence="0.993793">
4.4 Classification
</subsectionHeader>
<bodyText confidence="0.9999913">
After pre-processing and feature extraction we feed
the features into a classifier. We tried various classi-
fiers using the Scikit library 5. After extensive exper-
imentation it was found that SVM gave the best per-
formance. The parameters of the model were com-
puted using grid search. It was found that the model
performed best with radial basis function kernel and
0.75 as the penalty parameter C of the error term.
All the experimental are performed using these pa-
rameters for the model.
</bodyText>
<sectionHeader confidence="0.999967" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999928">
In this section we present the experimental results
for the classification task. We first present the score
and rank obtained by the system on various test
dataset followed by a discussion on the feature anal-
ysis for our system.
</bodyText>
<subsectionHeader confidence="0.990309">
5.1 Overall Performance
</subsectionHeader>
<bodyText confidence="0.999969">
The evaluation metric used in the competition is the
macro-averaged F measure calculated over the posi-
tive and negative classes. Table 4 presents the over-
all performance of our system for different datasets.
</bodyText>
<subsectionHeader confidence="0.996565">
5.2 Feature Analysis
</subsectionHeader>
<bodyText confidence="0.999933888888889">
Table 3 represents the results of the ablation experi-
ment on the Twitter Test Data 2015. Using this abla-
tion experiment, one can understand which features
play an important role in identifying the sentiment
of the tweet. It can be observed that the brown clus-
ters plays an important role in determining the class
of the tweet and improves the F-measure by around
20. Also, lexicon based features play a significant
role by improving the F-measure by 3.
</bodyText>
<sectionHeader confidence="0.999358" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999914142857143">
We presented results for sentiment analysis on Twit-
ter by building a supervised system which combines
lexicon based features with tweet specific features.
We reported the overall accuracy for 3-way classifi-
cation tasks: positive, negative and neutral. For our
feature based approach, we perform feature analysis
which reveals that the most important features are
</bodyText>
<tableCaption confidence="0.997912">
Table 4: Overall Performance of the System.
</tableCaption>
<table confidence="0.999573625">
Dataset Our Score Best Score Rank
Twitter 2015 59.83 64.84 18
Twitter Sarcasm 2015 52.67 65.77 23
Twitter 2014 67.04 74.42 16
Twitter 2013 65.68 72.80 20
Twitter Sarcasm 2014 57.50 59.11 2
Live Journal 2014 69.91 75.34 21
SMS 2013 62.25 68.49 19
</table>
<page confidence="0.997445">
525
</page>
<bodyText confidence="0.9999684">
those that combine the prior polarity of words and
the lexicon based features. In the future, we will
explore even richer linguistic analysis, for example,
parsing, semantic analysis and topic modeling to im-
prove our feature extraction component.
</bodyText>
<sectionHeader confidence="0.945736" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99994775">
We thank Mayank Gupta and Arpit Jaiswal, Interna-
tional Institute of Information Technology, Hyder-
abad, India for assisting with the experiments as well
as for interesting discussions on the subject. We
would like to thank the SemEval 2015 shared task
organizers for their support throughout this work.
We would also like to thank the anonymous review-
ers for their valuable comments.
</bodyText>
<sectionHeader confidence="0.998946" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873556818182">
Apoorv Agarwal, Fadi Biadsy, and Kathleen R. Mcke-
own. 2009. Contextual Phrase-level Polarity Analysis
Using Lexical Affect Scoring and Syntactic N-grams.
In Proceedings of the 12th Conference of the European
Chapter of the ACL (EACL), pages 24–32.
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment Analysis
of Twitter Data. In Proceedings of the Workshop on
Languages in Social Media (LSM), pages 30–38.
Akshat Bakliwal, Piyush Arora, Senthil Madhappan,
Nikhil Kapre, Mukesh Singh, and Vasudeva Varma.
2012. Mining Sentiments from Tweets. In Pro-
ceedings of the 3rd Workshop in Computational
Approaches to Subjectivity and Sentiment Analysis
(WASSA), pages 11–18.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A Publicly Available Lexical Resource
for Opinion Mining. In Proceedings of the 5th Confer-
ence on Language Resources and Evaluation (LREC,
pages 417–422.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A Smith. 2011. Part-of-speech Tagging for
Twitter: Annotation, Features, and Experiments. In
Proc. of the 49th Annual Meeting of the ACL: Human
Language Technologies (HLT), pages 42–47.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter Sentiment Classification using Distant Supervision.
CS224N Project Report, Stanford, pages 1–12.
Minqing Hu and Bing Liu. 2004. Mining and Summa-
rizing Customer Reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD), pages 168–
177.
Bing Liu and Lei Zhang. 2012. A Survey of Opinion
Mining and Sentiment Analysis. In Charu C. Aggar-
wal and ChengXiang Zhai, editors, Mining Text Data,
pages 415–463.
Bing Liu. 2010. Sentiment Analysis and Subjectivity.
In Handbook of Natural Language Processing, Second
Edition. Taylor and Francis Group, Boca.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. NRC-Canada: Building the State-of-the-
Art in Sentiment Analysis of Tweets. In Proceedings
of the 7th International Workshop on Semantic Evalu-
ation Exercises, Atlanta, Georgia, USA.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. SemEval-2013 Task 2: Sentiment Analysis in
Twitter. In The 2,d Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the 7th International Workshop on Seman-
tic Evaluation, pages 312–320.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and A. Noah Smith. 2013.
Improved Part-of-Speech Tagging for Online Conver-
sational Text with Word Clusters. In Proceedings of
the 2013 Conference of the North American Chapter of
the ACL: Human Language Technologies, pages 380–
390.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. In Proceedings of
the 42,d Meeting of the ACL, pages 271–278.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1–135.
Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin
Stoyanov. 2014. SemEval-2014 Task 9: Sentiment
Analysis in Twitter. In Proceedings of the 8th Interna-
tional Workshop on Semantic Evaluation, pages 73–
80.
Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif M Mohammad, Alan Ritter, and Veselin Stoy-
anov. 2015. SemEval-2015 Task 10: Sentiment Anal-
ysis in Twitter. In Proceedings of the 9th International
Workshop on Semantic Evaluation.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-level
Sentiment Analysis. In Proceedings of the Confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing (HLT),
pages 347–354.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing Contextual Polarity: An Explo-
ration of Features for Phrase-level Sentiment Analysis.
Computational Linguistics, 35(3):399–433.
</reference>
<page confidence="0.998498">
526
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.139318">
<note confidence="0.572316">IIIT-H at SemEval 2015: Twitter Sentiment The good, the bad and the neutral!</note>
<title confidence="0.466397">Dalmia, Manish Vasudeva Search and Information Extraction</title>
<affiliation confidence="0.99777">International Institute of Information Technology,</affiliation>
<email confidence="0.84206">manish.gupta@iiit.ac.in,vv@iiit.ac.in</email>
<abstract confidence="0.987147">This paper describes the system that was submitted to SemEval2015 Task 10: Sentiment Analysis in Twitter. We participated in Subtask B: Message Polarity Classification. The task is a message level classification of tweets into positive, negative and neutral sentiments. Our model is primarily a supervised one which consists of well designed features fed into an SVM classifier. In previous runs of this task, it was found that lexicons played an important role in determining the sentiment of a tweet. We use existing lexicons to extract lexicon specific features. The lexicon based features are further augmented by tweet specific features. We also improve our system by using acronym and emoticon dictionaries. The prosystem achieves an F1 score of the Test Data and Progress Data This placed us at the posifor the Test Dataset and the position for the Progress Test Dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Fadi Biadsy</author>
<author>Kathleen R Mckeown</author>
</authors>
<title>Contextual Phrase-level Polarity Analysis Using Lexical Affect Scoring and Syntactic N-grams.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL),</booktitle>
<pages>24--32</pages>
<contexts>
<context position="4217" citStr="Agarwal et al., 2009" startWordPosition="671" endWordPosition="674">d Work Sentiment analysis has been an active area of research since a long time. A number of surveys (Pang and Lee, 2008; Liu and Zhang, 2012) and books (Liu, 2010) give a thorough analysis of the existing techniques in sentiment analysis. Attempts have been made to analyze sentiments at different levels starting from document (Pang and Lee, 2004), 520 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 520–526, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sentences (Hu and Liu, 2004) to phrases (Wilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in order to identify sentiments from Twitter micro-blogging data also. (Go et al., 2009) used emoticons as noisy labels and distant supervision to classify tweets into positive or negative class. (Agarwal et al., 2011) introduced POS-specific prior polarity features along with using a tree kernel for tweet classification. Besides these two major papers, a lot of work from the previous runs of the SemEval is available (Rosenthal et al., 2014; Nakov et al., 201</context>
</contexts>
<marker>Agarwal, Biadsy, Mckeown, 2009</marker>
<rawString>Apoorv Agarwal, Fadi Biadsy, and Kathleen R. Mckeown. 2009. Contextual Phrase-level Polarity Analysis Using Lexical Affect Scoring and Syntactic N-grams. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL), pages 24–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Apoorv Agarwal</author>
<author>Boyi Xie</author>
<author>Ilia Vovsha</author>
<author>Owen Rambow</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Sentiment Analysis of Twitter Data.</title>
<date>2011</date>
<booktitle>In Proceedings of the Workshop on Languages in Social Media (LSM),</booktitle>
<pages>30--38</pages>
<contexts>
<context position="4572" citStr="Agarwal et al., 2011" startWordPosition="731" endWordPosition="735">Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 520–526, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sentences (Hu and Liu, 2004) to phrases (Wilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in order to identify sentiments from Twitter micro-blogging data also. (Go et al., 2009) used emoticons as noisy labels and distant supervision to classify tweets into positive or negative class. (Agarwal et al., 2011) introduced POS-specific prior polarity features along with using a tree kernel for tweet classification. Besides these two major papers, a lot of work from the previous runs of the SemEval is available (Rosenthal et al., 2014; Nakov et al., 2013). 3 Resources 3.1 Annotated Data Tweet IDs labeled as positive, negative or neutral were given by the task organizers. In order to build the system we first downloaded these tweets. The task organizers provided us with a certain number of tweet IDs. However, it was not possible to retrieve the content of all the tweet IDs due to changes in the privacy</context>
</contexts>
<marker>Agarwal, Xie, Vovsha, Rambow, Passonneau, 2011</marker>
<rawString>Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow, and Rebecca Passonneau. 2011. Sentiment Analysis of Twitter Data. In Proceedings of the Workshop on Languages in Social Media (LSM), pages 30–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshat Bakliwal</author>
<author>Piyush Arora</author>
<author>Senthil Madhappan</author>
<author>Nikhil Kapre</author>
<author>Mukesh Singh</author>
<author>Vasudeva Varma</author>
</authors>
<title>Mining Sentiments from Tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis (WASSA),</booktitle>
<pages>11--18</pages>
<contexts>
<context position="12062" citStr="Bakliwal et al., 2012" startWordPosition="1970" endWordPosition="1973">y in the model formation. Also, stop words do not carry any sentiment information and thus are of no use. • Handle Negative Mentions Negation plays a very important role in determining the sentiment of the tweet. Tweets consist of various notions of negation. Words which are either ‘no’, ‘not’ or ending with ‘n’t’ are replaced by a common word indicating negation. • Expand Acronyms As described in Section 3 we use an acronym expansion list. In the pre-processing step we expand the acronyms if they are present in the tweet. 4.2 Baseline Model We first generate a baseline model as discussed in (Bakliwal et al., 2012). We perform the preprocessing steps listed in Section 4.1 and learn the positive, negative and neutral frequencies of unigrams, bigrams and trigrams in our training data. Every token is given three probability scores: Positive Probability (Pp), Negative Probability (Np) and Neutral Probability (NEp). Given a token, let Pf denote the frequency in positive training set, Nf denote the frequency in negative training set and NEf denote the frequency in neutral training set. The probability scores are then computed as follows. Pf Pp = (1) Pf + Nf + NEf Nf Np = (2) Pf + Nf + NEf NEf NEp = (3) Pf + N</context>
</contexts>
<marker>Bakliwal, Arora, Madhappan, Kapre, Singh, Varma, 2012</marker>
<rawString>Akshat Bakliwal, Piyush Arora, Senthil Madhappan, Nikhil Kapre, Mukesh Singh, and Vasudeva Varma. 2012. Mining Sentiments from Tweets. In Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis (WASSA), pages 11–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SENTIWORDNET: A Publicly Available Lexical Resource for Opinion Mining.</title>
<date>2006</date>
<booktitle>In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC,</booktitle>
<pages>417--422</pages>
<contexts>
<context position="14756" citStr="Esuli and Sebastiani, 2006" startWordPosition="2444" endWordPosition="2447">listed in Table 2 for our experiments. There are a total of 34 features. We calculate these features for the whole tweet in case of message based sentiment analysis. We can divide the features into two classes: a) Tweet Based Features, and b) Lexicon Based Features. Table 2 summarizes the features used in our experiment. Here features f1 − f22 are tweet based features while features f23 − f34 are lexicon based features. A number of our features are based on prior polarity score of the tweet. For obtaining the prior polarity of words, we use AFINN dictionary 4 and extend it using SENTIWORDNET (Esuli and Sebastiani, 2006). We first look up the tokens in the tweet in the AFINN lexicon. This dictionary of about 2490 English language words assigns every word a pleasantness score between -5 (Negative) and +5 (Posi4http://www2.imm.dtu.dk/pubdb/views/ publication_details.php?id=6010 523 Table 2: Description of the Features used in the Model. Feature Description Feature ID Prior Polarity Score of the Tweet f0 Brown Clusters f1 Percentage of Capitalised Words f2 # of Positive Capitalised Words f3 # of Negative Capitalised Words f4 Presence of Capitalised Words f5 # of Positive Hashtags f6 # of Negative Hashtags f7 # o</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Andrea Esuli and Fabrizio Sebastiani. 2006. SENTIWORDNET: A Publicly Available Lexical Resource for Opinion Mining. In Proceedings of the 5th Conference on Language Resources and Evaluation (LREC, pages 417–422.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech Tagging for Twitter: Annotation, Features, and Experiments.</title>
<date>2011</date>
<booktitle>In Proc. of the 49th Annual Meeting of the ACL: Human Language Technologies (HLT),</booktitle>
<pages>42--47</pages>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A Smith. 2011. Part-of-speech Tagging for Twitter: Annotation, Features, and Experiments. In Proc. of the 49th Annual Meeting of the ACL: Human Language Technologies (HLT), pages 42–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alec Go</author>
<author>Richa Bhayani</author>
<author>Lei Huang</author>
</authors>
<title>Twitter Sentiment Classification using Distant Supervision. CS224N Project Report,</title>
<date>2009</date>
<pages>1--12</pages>
<location>Stanford,</location>
<contexts>
<context position="4442" citStr="Go et al., 2009" startWordPosition="711" endWordPosition="714">analysis. Attempts have been made to analyze sentiments at different levels starting from document (Pang and Lee, 2004), 520 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 520–526, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sentences (Hu and Liu, 2004) to phrases (Wilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in order to identify sentiments from Twitter micro-blogging data also. (Go et al., 2009) used emoticons as noisy labels and distant supervision to classify tweets into positive or negative class. (Agarwal et al., 2011) introduced POS-specific prior polarity features along with using a tree kernel for tweet classification. Besides these two major papers, a lot of work from the previous runs of the SemEval is available (Rosenthal et al., 2014; Nakov et al., 2013). 3 Resources 3.1 Annotated Data Tweet IDs labeled as positive, negative or neutral were given by the task organizers. In order to build the system we first downloaded these tweets. The task organizers provided us with a ce</context>
</contexts>
<marker>Go, Bhayani, Huang, 2009</marker>
<rawString>Alec Go, Richa Bhayani, and Lei Huang. 2009. Twitter Sentiment Classification using Distant Supervision. CS224N Project Report, Stanford, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and Summarizing Customer Reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),</booktitle>
<pages>168--177</pages>
<contexts>
<context position="4162" citStr="Hu and Liu, 2004" startWordPosition="661" endWordPosition="664">n 5. The paper is summarized in Section 6. 2 Related Work Sentiment analysis has been an active area of research since a long time. A number of surveys (Pang and Lee, 2008; Liu and Zhang, 2012) and books (Liu, 2010) give a thorough analysis of the existing techniques in sentiment analysis. Attempts have been made to analyze sentiments at different levels starting from document (Pang and Lee, 2004), 520 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 520–526, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sentences (Hu and Liu, 2004) to phrases (Wilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in order to identify sentiments from Twitter micro-blogging data also. (Go et al., 2009) used emoticons as noisy labels and distant supervision to classify tweets into positive or negative class. (Agarwal et al., 2011) introduced POS-specific prior polarity features along with using a tree kernel for tweet classification. Besides these two major papers, a lot of work from the previous runs of the SemEval </context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and Summarizing Customer Reviews. In Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), pages 168– 177.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Lei Zhang</author>
</authors>
<title>A Survey of Opinion Mining and Sentiment Analysis.</title>
<date>2012</date>
<booktitle>In Charu C. Aggarwal and ChengXiang Zhai, editors, Mining Text Data,</booktitle>
<pages>415--463</pages>
<contexts>
<context position="3738" citStr="Liu and Zhang, 2012" startWordPosition="600" endWordPosition="603">onsists of both subjective tweets which do not have any polarity as well as objective tweets. Our paper is organized as follows. We discuss related work in Section 2. In Section 3, we discuss the existing resources which we use in our system. In Section 4 we present the proposed system and give a detailed description for the same. We present experimental results and the ranking of our system for different datasets in Section 5. The paper is summarized in Section 6. 2 Related Work Sentiment analysis has been an active area of research since a long time. A number of surveys (Pang and Lee, 2008; Liu and Zhang, 2012) and books (Liu, 2010) give a thorough analysis of the existing techniques in sentiment analysis. Attempts have been made to analyze sentiments at different levels starting from document (Pang and Lee, 2004), 520 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 520–526, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sentences (Hu and Liu, 2004) to phrases (Wilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting wor</context>
</contexts>
<marker>Liu, Zhang, 2012</marker>
<rawString>Bing Liu and Lei Zhang. 2012. A Survey of Opinion Mining and Sentiment Analysis. In Charu C. Aggarwal and ChengXiang Zhai, editors, Mining Text Data, pages 415–463.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
</authors>
<title>Sentiment Analysis and Subjectivity.</title>
<date>2010</date>
<booktitle>In Handbook of Natural Language Processing, Second Edition. Taylor and Francis Group,</booktitle>
<location>Boca.</location>
<contexts>
<context position="3760" citStr="Liu, 2010" startWordPosition="606" endWordPosition="607">s which do not have any polarity as well as objective tweets. Our paper is organized as follows. We discuss related work in Section 2. In Section 3, we discuss the existing resources which we use in our system. In Section 4 we present the proposed system and give a detailed description for the same. We present experimental results and the ranking of our system for different datasets in Section 5. The paper is summarized in Section 6. 2 Related Work Sentiment analysis has been an active area of research since a long time. A number of surveys (Pang and Lee, 2008; Liu and Zhang, 2012) and books (Liu, 2010) give a thorough analysis of the existing techniques in sentiment analysis. Attempts have been made to analyze sentiments at different levels starting from document (Pang and Lee, 2004), 520 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 520–526, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sentences (Hu and Liu, 2004) to phrases (Wilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in ord</context>
</contexts>
<marker>Liu, 2010</marker>
<rawString>Bing Liu. 2010. Sentiment Analysis and Subjectivity. In Handbook of Natural Language Processing, Second Edition. Taylor and Francis Group, Boca.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>NRC-Canada: Building the State-of-theArt in Sentiment Analysis of Tweets.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation Exercises,</booktitle>
<location>Atlanta, Georgia, USA.</location>
<contexts>
<context position="6000" citStr="Mohammad et al., 2013" startWordPosition="980" endWordPosition="984">e training and the dev-test datasets, while the organizers provided us with 9684 and 1654 tweet IDs respectively, we were able to retrieve only 7966 and 1368 tweets, respectively. 3.2 Sentiment Lexicons It has been found that lexicons play an important role in determining the polarity of a message. Several lexicons have been proposed in the past which are used popularly in the field of sentiment analysis. We use the following lexicons to generate our lexicon based features: (1) Bing Liu’s Opinion Lexicon1, (2) MPQA Subjectivity Lexicon (Wilson et al., 2005), (3) NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013), and (4) Sentiment140 Lexicon (Mohammad et al., 2013). 3.3 Dictionary Besides the above sentiment lexicons, we used two other dictionaries described as follows. 1http://www.cs.uic.edu/˜liub/FBS/ opinion-lexicon-English.rar • Emoticon Dictionary: We use the emoticons list 2 and manually annotate the related sentiment. We categorize the emoticons into four classes as follows: (1) Extremely- Positive, (2) Positive, (3) Extremely- Negative, and (4) Negative. • Acronym Dictionary: We crawl the noslang.com website 3 in order to obtain the acronym expansion of the most commonly used acronyms on the </context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M. Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the State-of-theArt in Sentiment Analysis of Tweets. In Proceedings of the 7th International Workshop on Semantic Evaluation Exercises, Atlanta, Georgia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>SemEval-2013 Task 2: Sentiment Analysis in Twitter.</title>
<date>2013</date>
<booktitle>In The 2,d Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the 7th International Workshop on Semantic Evaluation,</booktitle>
<pages>312--320</pages>
<contexts>
<context position="4819" citStr="Nakov et al., 2013" startWordPosition="775" endWordPosition="778">wal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in order to identify sentiments from Twitter micro-blogging data also. (Go et al., 2009) used emoticons as noisy labels and distant supervision to classify tweets into positive or negative class. (Agarwal et al., 2011) introduced POS-specific prior polarity features along with using a tree kernel for tweet classification. Besides these two major papers, a lot of work from the previous runs of the SemEval is available (Rosenthal et al., 2014; Nakov et al., 2013). 3 Resources 3.1 Annotated Data Tweet IDs labeled as positive, negative or neutral were given by the task organizers. In order to build the system we first downloaded these tweets. The task organizers provided us with a certain number of tweet IDs. However, it was not possible to retrieve the content of all the tweet IDs due to changes in the privacy settings. Some of the tweets were probably deleted or may not be public at the time of download. Thus we were not able to download the tweet content of all the tweets IDs provided by the organizers. For the training and the dev-test datasets, whi</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. SemEval-2013 Task 2: Sentiment Analysis in Twitter. In The 2,d Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the 7th International Workshop on Semantic Evaluation, pages 312–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>A Noah Smith</author>
</authors>
<title>Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the ACL: Human Language Technologies,</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and A. Noah Smith. 2013. Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters. In Proceedings of the 2013 Conference of the North American Chapter of the ACL: Human Language Technologies, pages 380– 390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42,d Meeting of the ACL,</booktitle>
<pages>271--278</pages>
<contexts>
<context position="3945" citStr="Pang and Lee, 2004" startWordPosition="632" endWordPosition="635">ources which we use in our system. In Section 4 we present the proposed system and give a detailed description for the same. We present experimental results and the ranking of our system for different datasets in Section 5. The paper is summarized in Section 6. 2 Related Work Sentiment analysis has been an active area of research since a long time. A number of surveys (Pang and Lee, 2008; Liu and Zhang, 2012) and books (Liu, 2010) give a thorough analysis of the existing techniques in sentiment analysis. Attempts have been made to analyze sentiments at different levels starting from document (Pang and Lee, 2004), 520 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 520–526, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sentences (Hu and Liu, 2004) to phrases (Wilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in order to identify sentiments from Twitter micro-blogging data also. (Go et al., 2009) used emoticons as noisy labels and distant supervision to classify tweets into positive or negative cl</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts. In Proceedings of the 42,d Meeting of the ACL, pages 271–278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval,</title>
<date>2008</date>
<pages>2--1</pages>
<contexts>
<context position="3716" citStr="Pang and Lee, 2008" startWordPosition="596" endWordPosition="599"> the neutral class consists of both subjective tweets which do not have any polarity as well as objective tweets. Our paper is organized as follows. We discuss related work in Section 2. In Section 3, we discuss the existing resources which we use in our system. In Section 4 we present the proposed system and give a detailed description for the same. We present experimental results and the ranking of our system for different datasets in Section 5. The paper is summarized in Section 6. 2 Related Work Sentiment analysis has been an active area of research since a long time. A number of surveys (Pang and Lee, 2008; Liu and Zhang, 2012) and books (Liu, 2010) give a thorough analysis of the existing techniques in sentiment analysis. Attempts have been made to analyze sentiments at different levels starting from document (Pang and Lee, 2004), 520 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 520–526, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sentences (Hu and Liu, 2004) to phrases (Wilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A </context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion Mining and Sentiment Analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Alan Ritter</author>
<author>Preslav Nakov</author>
<author>Veselin Stoyanov</author>
</authors>
<title>SemEval-2014 Task 9: Sentiment Analysis in Twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="4798" citStr="Rosenthal et al., 2014" startWordPosition="771" endWordPosition="774">ilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in order to identify sentiments from Twitter micro-blogging data also. (Go et al., 2009) used emoticons as noisy labels and distant supervision to classify tweets into positive or negative class. (Agarwal et al., 2011) introduced POS-specific prior polarity features along with using a tree kernel for tweet classification. Besides these two major papers, a lot of work from the previous runs of the SemEval is available (Rosenthal et al., 2014; Nakov et al., 2013). 3 Resources 3.1 Annotated Data Tweet IDs labeled as positive, negative or neutral were given by the task organizers. In order to build the system we first downloaded these tweets. The task organizers provided us with a certain number of tweet IDs. However, it was not possible to retrieve the content of all the tweet IDs due to changes in the privacy settings. Some of the tweets were probably deleted or may not be public at the time of download. Thus we were not able to download the tweet content of all the tweets IDs provided by the organizers. For the training and the d</context>
</contexts>
<marker>Rosenthal, Ritter, Nakov, Stoyanov, 2014</marker>
<rawString>Sara Rosenthal, Alan Ritter, Preslav Nakov, and Veselin Stoyanov. 2014. SemEval-2014 Task 9: Sentiment Analysis in Twitter. In Proceedings of the 8th International Workshop on Semantic Evaluation, pages 73– 80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>SemEval-2015 Task 10: Sentiment Analysis in Twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="2872" citStr="Rosenthal et al., 2015" startWordPosition="447" endWordPosition="450">sidents. Therefore, it is possible to collect text posts of users from different social and interest groups. However, analyzing Twitter data comes with its own bag of difficulties. Tweets are small in length, thus ambiguous. The informal style of writing, a distinct usage of orthography, acronymization and a different set of elements like hashtags, user mentions demand a different approach to solve this problem. In this work we present the description of the supervised machine learning system developed while participating in the shared task of message based sentiment analysis in SemEval 2015 (Rosenthal et al., 2015). The system takes as input a tweet message, pre-processes it, extracts features and finally classifies it as either positive, negative or neutral. Tweets in the positive and negative classes are subjective in nature. However, the neutral class consists of both subjective tweets which do not have any polarity as well as objective tweets. Our paper is organized as follows. We discuss related work in Section 2. In Section 3, we discuss the existing resources which we use in our system. In Section 4 we present the proposed system and give a detailed description for the same. We present experiment</context>
</contexts>
<marker>Rosenthal, Nakov, Kiritchenko, Mohammad, Ritter, Stoyanov, 2015</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M Mohammad, Alan Ritter, and Veselin Stoyanov. 2015. SemEval-2015 Task 10: Sentiment Analysis in Twitter. In Proceedings of the 9th International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity in Phrase-level Sentiment Analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT),</booktitle>
<pages>347--354</pages>
<contexts>
<context position="5941" citStr="Wilson et al., 2005" startWordPosition="971" endWordPosition="974"> of all the tweets IDs provided by the organizers. For the training and the dev-test datasets, while the organizers provided us with 9684 and 1654 tweet IDs respectively, we were able to retrieve only 7966 and 1368 tweets, respectively. 3.2 Sentiment Lexicons It has been found that lexicons play an important role in determining the polarity of a message. Several lexicons have been proposed in the past which are used popularly in the field of sentiment analysis. We use the following lexicons to generate our lexicon based features: (1) Bing Liu’s Opinion Lexicon1, (2) MPQA Subjectivity Lexicon (Wilson et al., 2005), (3) NRC Hashtag Sentiment Lexicon (Mohammad et al., 2013), and (4) Sentiment140 Lexicon (Mohammad et al., 2013). 3.3 Dictionary Besides the above sentiment lexicons, we used two other dictionaries described as follows. 1http://www.cs.uic.edu/˜liub/FBS/ opinion-lexicon-English.rar • Emoticon Dictionary: We use the emoticons list 2 and manually annotate the related sentiment. We categorize the emoticons into four classes as follows: (1) Extremely- Positive, (2) Positive, (3) Extremely- Negative, and (4) Negative. • Acronym Dictionary: We crawl the noslang.com website 3 in order to obtain the a</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing Contextual Polarity in Phrase-level Sentiment Analysis. In Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing (HLT), pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing Contextual Polarity: An Exploration of Features for Phrase-level Sentiment Analysis.</title>
<date>2009</date>
<journal>Computational Linguistics,</journal>
<volume>35</volume>
<issue>3</issue>
<contexts>
<context position="4194" citStr="Wilson et al., 2009" startWordPosition="667" endWordPosition="670">n Section 6. 2 Related Work Sentiment analysis has been an active area of research since a long time. A number of surveys (Pang and Lee, 2008; Liu and Zhang, 2012) and books (Liu, 2010) give a thorough analysis of the existing techniques in sentiment analysis. Attempts have been made to analyze sentiments at different levels starting from document (Pang and Lee, 2004), 520 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 520–526, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics sentences (Hu and Liu, 2004) to phrases (Wilson et al., 2009; Agarwal et al., 2009). However, microblogging data is different from regular text as it is extremely noisy in nature. A lot of interesting work has been done in order to identify sentiments from Twitter micro-blogging data also. (Go et al., 2009) used emoticons as noisy labels and distant supervision to classify tweets into positive or negative class. (Agarwal et al., 2011) introduced POS-specific prior polarity features along with using a tree kernel for tweet classification. Besides these two major papers, a lot of work from the previous runs of the SemEval is available (Rosenthal et al., </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2009</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2009. Recognizing Contextual Polarity: An Exploration of Features for Phrase-level Sentiment Analysis. Computational Linguistics, 35(3):399–433.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>