<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.030932">
<title confidence="0.978389">
Tools for Collecting Speech Corpora via Mechanical-Turk
</title>
<author confidence="0.998887">
Ian Lane&apos;,2, Alex Waibel&apos;,2
</author>
<affiliation confidence="0.878809666666667">
&apos;Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
</affiliation>
<email confidence="0.990015">
{ianlane,ahw}@cs.cmu.edu
</email>
<sectionHeader confidence="0.993644" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999496">
To rapidly port speech applications to
new languages one of the most difficult
tasks is the initial collection of sufficient
speech corpora. State-of-the-art automatic
speech recognition systems are typical
trained on hundreds of hours of speech
data. While pre-existing corpora do exist
for major languages, a sufficient amount
of quality speech data is not available for
most world languages. While previous
works have focused on the collection of
translations and the transcription of audio
via Mechanical-Turk mechanisms, in this
paper we introduce two tools which ena-
ble the collection of speech data remotely.
We then compare the quality of audio col-
lected from paid part-time staff and unsu-
pervised volunteers, and determine that
basic user training is critical to obtain us-
able data.
</bodyText>
<sectionHeader confidence="0.987914" genericHeader="method">
&apos; Introduction
</sectionHeader>
<bodyText confidence="0.999938333333333">
In order to port a spoken language application to a
new language, first an automatic speech recogni-
tion (ASR) system must be developed. For many
languages pre-existing corpora do not exist and
thus speech data must be collected before devel-
opment can begin. The collection of speech corpo-
ra is an expensive undertaking and obtaining this
data rapidly, for example in response to a disaster,
cannot be done using the typical methodology in
which corpora are collected in controlled environ-
ments.
To build an ASR system for a new language, two
sets of data are required; first, a text corpus con-
sisting of written transcriptions of utterances users
are likely to speak to the system, this is used to
</bodyText>
<page confidence="0.991096">
184
</page>
<author confidence="0.830272">
Matthias Eck2, Kay Rottmann2
</author>
<footnote confidence="0.444889">
2Mobile Technologies LLC
Pittsburgh, PA, USA
matthias.eck@jibbigo.com
kay.rottmann@jibbigo.com
</footnote>
<bodyText confidence="0.999952725">
train the language model (LM) applied during
ASR; and second, a corpora of recordings of
speech, which are used to train an acoustic model
(AM). Text corpora for a new language can be
created by manually translating a pre-existing cor-
pus (or a sub-set of that corpus) into the new lan-
guage and crowd-sourcing methodologies can be
used to rapidly perform this task. Rapidly creating
corpora of speech data, however, is not trivial.
Generally speech corpora are collected in con-
trolled environments where speakers are super-
vised by experts to ensure the equipment is setup
correctly and recordings are performed adequately.
However, for most languages performing this task
on-site, where developers are located, is impractic-
al as there may not be a local community of speak-
ers of the required language. An alternative is to
perform the data collection remotely, allowing
speakers to record speech on their own PCs or mo-
bile devices in their home country or wherever
they are located. While previous works have fo-
cused on the generation of translations (Razavian,
2009) and transcribing of audio (Marge, 2010) via
Mechanical-Turk, in this paper we focus on the
collection of speech corpora using a Mechanical-
Turk type framework.
Previous works (Voxforge), (Gruenstein, 2009),
(Schultz, 2007) have developed solutions for col-
lecting speech data remotely via web-based inter-
faces. A web-based system for the collection of
open-source speech corpora has been developed by
the group at www.voxforge.org. Speech recordings
are collected for ten major European languages and
speakers can either record audio directly on the
website or they can call in on a dedicated phone
line. In (Gruenstein, 2009) spontaneous speech
(US English) was collected via a web-based mem-
ory game. In this system speech prompts were not
provided, but rather a voice-based memory game
was used to gather and partially annotate
</bodyText>
<note confidence="0.919759">
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 184–187,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999326">
Figure 1: Screenshots from Speech Collection iPhone A
</figureCaption>
<bodyText confidence="0.999970047619048">
spontaneous speech. In comparison to the above
works which focus on the collection of data for
major languages, the SPICE project (Schultz,
2007) provides a set of web-based tools to enable
developers to create voice-based applications for
less-common languages. In addition to tools for
defining the phonetic units of a language and creat-
ing pronunciation dictionaries, this system also
includes tools to create prompts and collect speech
data from volunteers over the web.
In this paper, we describe two tools we have de-
veloped to collect speech corpora remotely. The
first, a Mobile smart-phone based system which
allows speakers to record prompted speech directly
on their phones and second, a web-based system
which allows recordings to be collected remotely
on PCs. We compare the quality of audio collected
from paid part-time staff and unsupervised volun-
teers and determine that basic user training and
automatic feedback mechanisms are required to
obtain usable data.
</bodyText>
<subsectionHeader confidence="0.416025">
2 Collection of Speech on Mobile Devices
</subsectionHeader>
<bodyText confidence="0.999834657142857">
Today&apos;s smart-phones are able to record quality
audio onboard and generally have the ability to
connect to the internet via a fast wifi-connection.
This makes them an ideal platform for collecting
speech data in the field. Speech data can be col-
lected by a user at any time in any location, and the
data can be uploaded at a later time when a wire-
less connection is available. At Mobile Technolo-
gies we have developed an iPhone application to
perform this task.
The collection procedure consists of three steps.
First, on start-up a small amount of personal in-
formation, namely, gender and age, are requested
from the user. They then select the language for
which they intend to provide speech data. The mo-
bile-device ID, personal information and language
selected is used as an identifier for individual
speakers. Next, collection of speech data is per-
formed. Collection is performed offline, enabling
data to be collected in the field where there may
not be a persistent internet connection. A prompt is
randomly selected from an onboard database of
sentences and is presented to the user, who reads
the sentence aloud holding down a push-to-talk
button while speaking. During the speech collec-
tion stage, the system automatically proceeds to the
following prompt when the current recording is
complete. The user however has the ability to go
back to previous recordings, listen to it and re-
speak the sentence if any issues are found. Finally,
the speech data is uploaded using a wireless collec-
tion. Data is uploaded one utterance at a time to an
FTP server. Uploading each utterance individually
allows the user to halt the upload and continue it at
a later time if required.
</bodyText>
<page confidence="0.996608">
185
</page>
<figureCaption confidence="0.978673">
Figure 2: Java applet for Web-based recording
</figureCaption>
<sectionHeader confidence="0.904185" genericHeader="method">
3 Collection via Web-based Recording
</sectionHeader>
<bodyText confidence="0.9999765">
One of the most popular websites for crowd-
sourcing is Amazon Mechanical Turk (AMT).
&amp;quot;Requesters&amp;quot; post Human Intelligence Tasks
(HITs) to this website and &amp;quot;Workers&amp;quot; browse the
HITs, perform tasks and get paid a predefined
amount after submitting their work. It has been
reported that over 100,000 workers from 100 coun-
tries are using AMT (Pontin, 2007).
AMT allows two general types of HITs. A Ques-
tion Form HIT is based on a provided XML tem-
plate and only allows certain elements in the HIT.
However, it is possible to integrate an external
JAVA applet within a Question Form HIT which
allows for some flexibility. Questions can also be
hosted on an external website which increases flex-
ibility for the HIT developer while remaining
tightly integrated in the AMT environment.
For collection of audio data Amazon does not offer
any integrated tools. We thus designed and imple-
mented a Java applet for web based speech collec-
tion. The Java applet can easily be incorporated in
the AMT Question-Form mechanism and could
also be used as part of an External-Question HIT.
Currently the Java applet provides the same basic
functionality as outlined for the iPhone application.
The applet sequentially shows a number of
prompts to record. The user can skip a sentence,
playback a recording to check the quality and also
redo the recording for the current sentence (see
screenshot in Figure 2).
After the user is finished, the recorded sentences
are uploaded to a web-server using an HTTP Post
request. An important difference is the necessity to
be online during the speech recordings.
</bodyText>
<sectionHeader confidence="0.973548" genericHeader="method">
4 Evaluation of Recorded Audio
</sectionHeader>
<bodyText confidence="0.988041">
One issue when collecting speech data remotely is
the quality of the resulting audio. When collection
</bodyText>
<table confidence="0.999162125">
Paid Employees
Language English
Number of Speakers 10
Utterances Evaluated 445
Volunteers
Language Haitian Creole
Number of Speakers 3
Utterances Evaluated 167
</table>
<tableCaption confidence="0.994343">
Table 1: Details of Evaluated Corpora
</tableCaption>
<table confidence="0.9996348">
1 Recorded utterance is empty
2 Utterance is not segmented correctly
3 Recording is clipped
4 Recording contains audible echo
5 Recording contains audible noise
</table>
<tableCaption confidence="0.9482205">
Table 2: Annotations used to label poor quality
recordings
</tableCaption>
<bodyText confidence="0.998339555555556">
is performed in a controlled environment, the de-
veloper can ensure that the recording equipment is
setup correctly, background noise is kept to a min-
imum and the speaker is adequately trained to use
the recording equipment. However, the same is not
guaranteed when collecting speech remotely via
mechanical-turk frameworks.
When recording prompted speech there are three
types of issues that result in unsuitable data:
</bodyText>
<listItem confidence="0.976286375">
• Garbage Audio: recordings that are emp-
ty, clipped, have insufficient power, or are
incorrectly segmented.
• Low quality recordings: low Signal-to-
Noise recordings due to poor equipment or
large background noise
• Speaker errors: Misspeaking of prompts,
both accidental and malicious
</listItem>
<bodyText confidence="0.9797263">
To verify the quality of audio recorded in unsuper-
vised environments we compared two sets of
speech data. First, in an earlier data collection task
we collected 445 prompted utterances from 10 US-
English speakers. This data collection was per-
formed in a quiet office environment with technic-
al supervision. Speakers were paid a fee for their
time. As a comparison a similar collection of Hai-
tian Creole was performed. In this case data was
collected on a volunteer basis and supervision was
</bodyText>
<tableCaption confidence="0.8673225">
limited. Details of the collected data are shown in
Table 1.
</tableCaption>
<page confidence="0.996418">
186
</page>
<figureCaption confidence="0.986566">
Figure 3: Percentage of recorded utterances de-
termined to be inadequate for acoustic model
training. Annotations limited to five issues
listed in Table 1.
</figureCaption>
<bodyText confidence="0.999981885714286">
To determine the frequency of the quality issues
listed above, we manually verified the two sets of
collected speech. The recording of each utterance
was listened to and if the audio file was determined
to be of low quality it was annotated with one of
the tags listed in Table 2. The percentage of utter-
ances labeled with each annotation is shown for the
English and volunteer Haitian Creole cases in Fig-
ure 3.
Around 10% of the English recordings were found
to have issues. Clipping occurred in approximately
5% and a distinct echo was present in the record-
ings for one speaker. For the Haitian Creole case
the yield of useable audio was significantly lower
than that obtained for English. For all three speak-
ers clipping was more prevalent and the level of
background noise was higher. We discovered that
due to lack of training, one of the volunteers had
significant issues with the push-to-talk interface in
our system. This led to many empty or incorrectly
segmented recordings. In both cases, prompts were
generally spoken accurately and technical prob-
lems caused poor quality recordings.
We believe the large difference in the yield of high
quality recordings, 90% for English compared to
65% for Haitian Creole case, is directly due to the
lack of training speakers received and the volun-
teer nature of the Haitian Creole task. By incorpo-
rating a basic tutorial when users first start our
tools and an explicit feedback mechanism which
automatically detects quality issues and prompts
users to correct them we expect the yield of high
quality recordings to increase significantly. In the
near future we plan to use the tools to collect data
from large communities of remote users.
</bodyText>
<sectionHeader confidence="0.996977" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999966">
In this work, we have described two applications
that allow speech corpora to be collected remotely,
either directly on Mobile smart-phones or on a PC
via a web-based interface. We also investigated the
quality of recordings made by unsupervised volun-
teers and found that although prompts were gener-
ally read accurately, lack of training led to a
significantly lower yield of high quality record-
ings.
In the near future we plan to use the tools to collect
data from large communities of remote users. We
will also investigate the user of tutorials and feed-
back to improve the yield of high quality data.
</bodyText>
<sectionHeader confidence="0.996039" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999775">
We would like to thank the Haitian volunteers who
gave their time to help with this data collection.
</bodyText>
<sectionHeader confidence="0.99926" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9991243">
N. S. Razavian, S Vogel, &amp;quot;The Web as a Platform
to Build Machine Translation Resources&amp;quot;,
IWIC2009
M. Marge, S. Banerjee and A. Rudnicky, &amp;quot;Using
the Amazon Mechanical Turk for Transcription
of Spoken Language&amp;quot;, IEEE-ICASSP, 2010
Voxforge, www.voxforge.org
A. Gruenstein, I. McGraw, and A. Sutherland, &amp;quot;A
self-transcribing speech corpus: collecting con-
tinuous speech with an online educational
game,&amp;quot; Submitted to the Speech and Language
Technology in Education (SLaTE) Workshop,
2009.
T. Schultz, et. al, &amp;quot;SPICE: Web-based Tools for
Rapid Language Adaptation in Speech
Processing Systems&amp;quot;, In the Proceedings of
INTERSPEECH, Antwerp, Belgium, 2007.
J. Pontin, &amp;quot;Artificial Intelligence, With Help From
the Humans&amp;quot;, The New York Times, 25 March
2007
</reference>
<page confidence="0.997919">
187
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.993186">Tools for Collecting Speech Corpora via Mechanical-Turk</title>
<author confidence="0.6871185">Alex Technologies</author>
<affiliation confidence="0.997442">Carnegie Mellon</affiliation>
<address confidence="0.958132">Pittsburgh, PA 15213,</address>
<email confidence="0.998987">ianlane@cs.cmu.edu</email>
<email confidence="0.998987">ahw@cs.cmu.edu</email>
<abstract confidence="0.987040945945946">To rapidly port speech applications to new languages one of the most difficult tasks is the initial collection of sufficient speech corpora. State-of-the-art automatic speech recognition systems are typical trained on hundreds of hours of speech data. While pre-existing corpora do exist for major languages, a sufficient amount of quality speech data is not available for most world languages. While previous works have focused on the collection of translations and the transcription of audio via Mechanical-Turk mechanisms, in this paper we introduce two tools which enable the collection of speech data remotely. We then compare the quality of audio collected from paid part-time staff and unsupervised volunteers, and determine that basic user training is critical to obtain usable data. In order to port a spoken language application to a new language, first an automatic speech recognition (ASR) system must be developed. For many languages pre-existing corpora do not exist and thus speech data must be collected before development can begin. The collection of speech corpora is an expensive undertaking and obtaining this data rapidly, for example in response to a disaster, cannot be done using the typical methodology in which corpora are collected in controlled environments. To build an ASR system for a new language, two sets of data are required; first, a text corpus consisting of written transcriptions of utterances users are likely to speak to the system, this is used to 184</abstract>
<author confidence="0.594838">Kay</author>
<affiliation confidence="0.652375">Technologies</affiliation>
<address confidence="0.844347">Pittsburgh, PA,</address>
<email confidence="0.953224">kay.rottmann@jibbigo.com</email>
<abstract confidence="0.993926431654676">train the language model (LM) applied during ASR; and second, a corpora of recordings of speech, which are used to train an acoustic model (AM). Text corpora for a new language can be created by manually translating a pre-existing corpus (or a sub-set of that corpus) into the new language and crowd-sourcing methodologies can be used to rapidly perform this task. Rapidly creating corpora of speech data, however, is not trivial. Generally speech corpora are collected in controlled environments where speakers are supervised by experts to ensure the equipment is setup correctly and recordings are performed adequately. However, for most languages performing this task on-site, where developers are located, is impractical as there may not be a local community of speakers of the required language. An alternative is to perform the data collection remotely, allowing speakers to record speech on their own PCs or mobile devices in their home country or wherever they are located. While previous works have focused on the generation of translations (Razavian, 2009) and transcribing of audio (Marge, 2010) via Mechanical-Turk, in this paper we focus on the collection of speech corpora using a Mechanical- Turk type framework. Previous works (Voxforge), (Gruenstein, 2009), (Schultz, 2007) have developed solutions for collecting speech data remotely via web-based interfaces. A web-based system for the collection of open-source speech corpora has been developed by the group at www.voxforge.org. Speech recordings are collected for ten major European languages and speakers can either record audio directly on the website or they can call in on a dedicated phone line. In (Gruenstein, 2009) spontaneous speech (US English) was collected via a web-based memory game. In this system speech prompts were not provided, but rather a voice-based memory game was used to gather and partially annotate of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical pages 184–187, Angeles, California, June 2010. Association for Computational Linguistics 1: from Speech Collection iPhone A spontaneous speech. In comparison to the above works which focus on the collection of data for major languages, the SPICE project (Schultz, 2007) provides a set of web-based tools to enable developers to create voice-based applications for less-common languages. In addition to tools for defining the phonetic units of a language and creating pronunciation dictionaries, this system also includes tools to create prompts and collect speech data from volunteers over the web. In this paper, we describe two tools we have developed to collect speech corpora remotely. The first, a Mobile smart-phone based system which allows speakers to record prompted speech directly on their phones and second, a web-based system which allows recordings to be collected remotely on PCs. We compare the quality of audio collected from paid part-time staff and unsupervised volunteers and determine that basic user training and automatic feedback mechanisms are required to obtain usable data. 2 Collection of Speech on Mobile Devices Today&apos;s smart-phones are able to record quality audio onboard and generally have the ability to connect to the internet via a fast wifi-connection. This makes them an ideal platform for collecting speech data in the field. Speech data can be collected by a user at any time in any location, and the can be uploaded at a later time when a wireless connection is available. At Mobile Technologies we have developed an iPhone application to perform this task. The collection procedure consists of three steps. First, on start-up a small amount of personal information, namely, gender and age, are requested from the user. They then select the language for which they intend to provide speech data. The mobile-device ID, personal information and language selected is used as an identifier for individual speakers. Next, collection of speech data is performed. Collection is performed offline, enabling data to be collected in the field where there may not be a persistent internet connection. A prompt is randomly selected from an onboard database of sentences and is presented to the user, who reads the sentence aloud holding down a push-to-talk button while speaking. During the speech collection stage, the system automatically proceeds to the following prompt when the current recording is complete. The user however has the ability to go back to previous recordings, listen to it and respeak the sentence if any issues are found. Finally, the speech data is uploaded using a wireless collection. Data is uploaded one utterance at a time to an FTP server. Uploading each utterance individually allows the user to halt the upload and continue it at a later time if required. 185 2: applet for Web-based recording 3 Collection via Web-based Recording One of the most popular websites for crowdsourcing is Amazon Mechanical Turk (AMT). &amp;quot;Requesters&amp;quot; post Human Intelligence Tasks (HITs) to this website and &amp;quot;Workers&amp;quot; browse the HITs, perform tasks and get paid a predefined amount after submitting their work. It has been reported that over 100,000 workers from 100 countries are using AMT (Pontin, 2007). AMT allows two general types of HITs. A Question Form HIT is based on a provided XML template and only allows certain elements in the HIT. However, it is possible to integrate an external JAVA applet within a Question Form HIT which allows for some flexibility. Questions can also be hosted on an external website which increases flexibility for the HIT developer while remaining tightly integrated in the AMT environment. For collection of audio data Amazon does not offer any integrated tools. We thus designed and implemented a Java applet for web based speech collection. The Java applet can easily be incorporated in the AMT Question-Form mechanism and could also be used as part of an External-Question HIT. Currently the Java applet provides the same basic functionality as outlined for the iPhone application. The applet sequentially shows a number of prompts to record. The user can skip a sentence, playback a recording to check the quality and also redo the recording for the current sentence (see screenshot in Figure 2). After the user is finished, the recorded sentences are uploaded to a web-server using an HTTP Post request. An important difference is the necessity to be online during the speech recordings. 4 Evaluation of Recorded Audio One issue when collecting speech data remotely is</abstract>
<title confidence="0.553717">the quality of the resulting audio. When collection Paid Employees Language English</title>
<note confidence="0.7794051">Number of Speakers 10 Utterances Evaluated 445 Volunteers Language Haitian Creole Number of Speakers 3 Utterances Evaluated 167 1: of Evaluated Corpora 1 Recorded utterance is empty 2 Utterance is not segmented correctly 3 Recording is clipped</note>
<abstract confidence="0.998090211111111">4 Recording contains audible echo 5 Recording contains audible noise 2: used to label poor recordings is performed in a controlled environment, the developer can ensure that the recording equipment is setup correctly, background noise is kept to a minimum and the speaker is adequately trained to use the recording equipment. However, the same is not guaranteed when collecting speech remotely via mechanical-turk frameworks. When recording prompted speech there are three types of issues that result in unsuitable data: Garbage Audio: that are empty, clipped, have insufficient power, or are incorrectly segmented. Low quality recordings: Signal-to- Noise recordings due to poor equipment or large background noise Speaker errors: of prompts, both accidental and malicious To verify the quality of audio recorded in unsupervised environments we compared two sets of speech data. First, in an earlier data collection task we collected 445 prompted utterances from 10 US- English speakers. This data collection was performed in a quiet office environment with technical supervision. Speakers were paid a fee for their time. As a comparison a similar collection of Haitian Creole was performed. In this case data was collected on a volunteer basis and supervision was limited. Details of the collected data are shown in Table 1. 186 3: of recorded utterances determined to be inadequate for acoustic model training. Annotations limited to five issues listed in Table 1. To determine the frequency of the quality issues listed above, we manually verified the two sets of collected speech. The recording of each utterance was listened to and if the audio file was determined to be of low quality it was annotated with one of the tags listed in Table 2. The percentage of utterances labeled with each annotation is shown for the English and volunteer Haitian Creole cases in Figure 3. Around 10% of the English recordings were found to have issues. Clipping occurred in approximately 5% and a distinct echo was present in the recordings for one speaker. For the Haitian Creole case the yield of useable audio was significantly lower than that obtained for English. For all three speakers clipping was more prevalent and the level of background noise was higher. We discovered that due to lack of training, one of the volunteers had significant issues with the push-to-talk interface in our system. This led to many empty or incorrectly segmented recordings. In both cases, prompts were generally spoken accurately and technical problems caused poor quality recordings. We believe the large difference in the yield of high quality recordings, 90% for English compared to 65% for Haitian Creole case, is directly due to the lack of training speakers received and the volunteer nature of the Haitian Creole task. By incorporating a basic tutorial when users first start our tools and an explicit feedback mechanism which automatically detects quality issues and prompts users to correct them we expect the yield of high quality recordings to increase significantly. In the near future we plan to use the tools to collect data from large communities of remote users. 5 Conclusions and Future Work In this work, we have described two applications that allow speech corpora to be collected remotely, either directly on Mobile smart-phones or on a PC via a web-based interface. We also investigated the quality of recordings made by unsupervised volunteers and found that although prompts were generally read accurately, lack of training led to a significantly lower yield of high quality recordings. In the near future we plan to use the tools to collect data from large communities of remote users. We will also investigate the user of tutorials and feedback to improve the yield of high quality data. Acknowledgements We would like to thank the Haitian volunteers who gave their time to help with this data collection.</abstract>
<title confidence="0.892355">References</title>
<author confidence="0.902222">N S Razavian</author>
<author confidence="0.902222">S Vogel</author>
<author confidence="0.902222">The Web as a Platform</author>
<affiliation confidence="0.609751">to Build Machine Translation Resources&amp;quot;,</affiliation>
<address confidence="0.640074">IWIC2009</address>
<abstract confidence="0.3005918">M. Marge, S. Banerjee and A. Rudnicky, &amp;quot;Using the Amazon Mechanical Turk for Transcription of Spoken Language&amp;quot;, IEEE-ICASSP, 2010 Voxforge,www.voxforge.org A. Gruenstein, I. McGraw, and A. Sutherland, &amp;quot;A self-transcribing speech corpus: collecting continuous speech with an online educational game,&amp;quot; Submitted to the Speech and Language Technology in Education (SLaTE) Workshop, 2009.</abstract>
<title confidence="0.660898333333333">T. Schultz, et. al, &amp;quot;SPICE: Web-based Tools for Rapid Language Adaptation in Speech Processing Systems&amp;quot;, In the Proceedings of</title>
<note confidence="0.742348">INTERSPEECH, Antwerp, Belgium, 2007. J. Pontin, &amp;quot;Artificial Intelligence, With Help From the Humans&amp;quot;, The New York Times, 25 March 2007 187</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N S Razavian</author>
<author>S Vogel</author>
</authors>
<title>The Web as a Platform to Build Machine Translation Resources&amp;quot;,</title>
<date>2009</date>
<note>Voxforge, www.voxforge.org</note>
<marker>Razavian, Vogel, 2009</marker>
<rawString>N. S. Razavian, S Vogel, &amp;quot;The Web as a Platform to Build Machine Translation Resources&amp;quot;, IWIC2009 M. Marge, S. Banerjee and A. Rudnicky, &amp;quot;Using the Amazon Mechanical Turk for Transcription of Spoken Language&amp;quot;, IEEE-ICASSP, 2010 Voxforge, www.voxforge.org</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Gruenstein</author>
<author>I McGraw</author>
<author>A Sutherland</author>
</authors>
<title>A self-transcribing speech corpus: collecting continuous speech with an online educational game,&amp;quot; Submitted to the Speech and Language Technology in Education (SLaTE) Workshop,</title>
<date>2009</date>
<marker>Gruenstein, McGraw, Sutherland, 2009</marker>
<rawString>A. Gruenstein, I. McGraw, and A. Sutherland, &amp;quot;A self-transcribing speech corpus: collecting continuous speech with an online educational game,&amp;quot; Submitted to the Speech and Language Technology in Education (SLaTE) Workshop, 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Schultz</author>
</authors>
<title>SPICE: Web-based Tools for Rapid Language Adaptation in Speech Processing Systems&amp;quot;,</title>
<date>2007</date>
<booktitle>In the Proceedings of INTERSPEECH,</booktitle>
<location>Antwerp, Belgium,</location>
<contexts>
<context position="3125" citStr="Schultz, 2007" startWordPosition="485" endWordPosition="486">site, where developers are located, is impractical as there may not be a local community of speakers of the required language. An alternative is to perform the data collection remotely, allowing speakers to record speech on their own PCs or mobile devices in their home country or wherever they are located. While previous works have focused on the generation of translations (Razavian, 2009) and transcribing of audio (Marge, 2010) via Mechanical-Turk, in this paper we focus on the collection of speech corpora using a MechanicalTurk type framework. Previous works (Voxforge), (Gruenstein, 2009), (Schultz, 2007) have developed solutions for collecting speech data remotely via web-based interfaces. A web-based system for the collection of open-source speech corpora has been developed by the group at www.voxforge.org. Speech recordings are collected for ten major European languages and speakers can either record audio directly on the website or they can call in on a dedicated phone line. In (Gruenstein, 2009) spontaneous speech (US English) was collected via a web-based memory game. In this system speech prompts were not provided, but rather a voice-based memory game was used to gather and partially an</context>
</contexts>
<marker>Schultz, 2007</marker>
<rawString>T. Schultz, et. al, &amp;quot;SPICE: Web-based Tools for Rapid Language Adaptation in Speech Processing Systems&amp;quot;, In the Proceedings of INTERSPEECH, Antwerp, Belgium, 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pontin</author>
</authors>
<title>Artificial Intelligence, With Help From the Humans&amp;quot;, The</title>
<date>2007</date>
<location>New York Times, 25</location>
<contexts>
<context position="7133" citStr="Pontin, 2007" startWordPosition="1131" endWordPosition="1132">ta is uploaded one utterance at a time to an FTP server. Uploading each utterance individually allows the user to halt the upload and continue it at a later time if required. 185 Figure 2: Java applet for Web-based recording 3 Collection via Web-based Recording One of the most popular websites for crowdsourcing is Amazon Mechanical Turk (AMT). &amp;quot;Requesters&amp;quot; post Human Intelligence Tasks (HITs) to this website and &amp;quot;Workers&amp;quot; browse the HITs, perform tasks and get paid a predefined amount after submitting their work. It has been reported that over 100,000 workers from 100 countries are using AMT (Pontin, 2007). AMT allows two general types of HITs. A Question Form HIT is based on a provided XML template and only allows certain elements in the HIT. However, it is possible to integrate an external JAVA applet within a Question Form HIT which allows for some flexibility. Questions can also be hosted on an external website which increases flexibility for the HIT developer while remaining tightly integrated in the AMT environment. For collection of audio data Amazon does not offer any integrated tools. We thus designed and implemented a Java applet for web based speech collection. The Java applet can ea</context>
</contexts>
<marker>Pontin, 2007</marker>
<rawString>J. Pontin, &amp;quot;Artificial Intelligence, With Help From the Humans&amp;quot;, The New York Times, 25 March 2007</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>