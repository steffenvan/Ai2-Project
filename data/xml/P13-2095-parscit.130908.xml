<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.009896">
<title confidence="0.998858">
Extracting Definitions and Hypernym Relations relying on Syntactic
Dependencies and Support Vector Machines
</title>
<author confidence="0.998733">
Guido Boella
</author>
<affiliation confidence="0.9997905">
University of Turin
Department of Computer Science
</affiliation>
<email confidence="0.988694">
boella@di.unito.it
</email>
<author confidence="0.992959">
Luigi Di Caro
</author>
<affiliation confidence="0.9997075">
University of Turin
Department of Computer Science
</affiliation>
<email confidence="0.994271">
dicaro@di.unito.it
</email>
<sectionHeader confidence="0.993862" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999266944444444">
In this paper we present a technique to
reveal definitions and hypernym relations
from text. Instead of using pattern match-
ing methods that rely on lexico-syntactic
patterns, we propose a technique which
only uses syntactic dependencies between
terms extracted with a syntactic parser.
The assumption is that syntactic informa-
tion are more robust than patterns when
coping with length and complexity of the
sentences. Afterwards, we transform such
syntactic contexts in abstract representa-
tions, that are then fed into a Support
Vector Machine classifier. The results on
an annotated dataset of definitional sen-
tences demonstrate the validity of our ap-
proach overtaking current state-of-the-art
techniques.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999225941176471">
Nowadays, there is a huge amount of textual
data coming from different sources of informa-
tion. Wikipedia1, for example, is a free encyclo-
pedia that currently contains 4,208,409 English ar-
ticles2. Even Social Networks play a role in the
construction of data that can be useful for Infor-
mation Extraction tasks like Sentiment Analysis,
Question Answering, and so forth.
From another point of view, there is the need
of having more structured data in the forms of
ontologies, in order to allow semantics-based re-
trieval and reasoning. Ontology Learning is
a task that permits to automatically (or semi-
automatically) extract structured knowledge from
plain text. Manual construction of ontologies usu-
ally requires strong efforts from domain experts,
and it thus needs an automatization in such sense.
</bodyText>
<footnote confidence="0.999368">
1http://www.wikipedia.org/
2April 12, 2013.
</footnote>
<bodyText confidence="0.999813515151515">
In this paper, we focus on the extraction of hy-
pernym relations. The first step of such task relies
on the identification of what (Navigli and Velardi,
2010) called definitional sentences, i.e., sentences
that contain at least one hypernym relation. This
subtask is important by itself for many tasks like
Question Answering (Cui et al., 2007), construc-
tion of glossaries (Klavans and Muresan, 2001),
extraction of taxonomic and non-taxonomic rela-
tions (Navigli, 2009; Snow et al., 2004), enrich-
ment of concepts (Gangemi et al., 2003; Cataldi et
al., 2009), and so forth.
Hypernym relation extraction involves two as-
pects: linguistic knowlege, and model learning.
Patterns collapse both of them, preventing to face
them separately with the most suitable techniques.
First, patterns have limited expressivity; then, lin-
guistic knowledge inside patterns is learned from
small corpora, so it is likely to have low coverage.
Classification strictly depends on the learned pat-
terns, so performance decreases, and the available
classification techniques are restricted to those
compatible with the pattern approach. Instead, we
use a syntactic parser for the first aspect (with all
its native and domain-independent knowledge on
language expressivity), and a state-of-the-art ap-
proach to learn models with the use of Support
Vector Machine classifiers.
Our assumption is that syntax is less dependent
than learned patterns from the length and the com-
plexity of textual expressions. In some way, pat-
terns grasp syntactic relationships, but they actu-
ally do not use them as input knowledge.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.875314333333333">
In this section we present the current state of the
art concerning the automatic extraction of defini-
tions and hypernym relations from plain text. We
will use the term definitional sentence referring to
the more general meaning given by (Navigli and
Velardi, 2010): A sentence that provides a for-
</bodyText>
<page confidence="0.964049">
532
</page>
<bodyText confidence="0.982577841463415">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 532–537,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
mal explanation for the term of interest, and more
specifically as a sentence containing at least one
hypernym relation.
So far, most of the proposed techniques rely on
lexico-syntactic patterns, either manually or semi-
automatically produced (Hovy et al., 2003; Zhang
and Jiang, 2009; Westerhout, 2009). Such pat-
terns are sequences of words like “is a” or “refers
to”, rather than more complex sequences includ-
ing part-of-speech tags.
In the work of (Westerhout, 2009), after a man-
ual identification of types of definitions and related
patterns contained in a corpus, he successively ap-
plied Machine Learning techniques on syntactic
and location features to improve the results.
A fully-automatic approach has been proposed
by (Borg et al., 2009), where the authors applied
genetic algorithms to the extraction of English def-
initions containing the keyword “is”. In detail,
they assign weights to a set of features for the clas-
sification of definitional sentences, reaching a pre-
cision of 62% and a recall of 52%.
Then, (Cui et al., 2007) proposed an approach
based on soft patterns, i.e., probabilistic lexico-
semantic patterns that are able to generalize over
rigid patterns enabling partial matching by cal-
culating a generative degree-of-match probability
between a test instance and the set of training in-
stances.
Similarly to our approach, (Fahmi and Bouma,
2006) used three different Machine Learning algo-
rithms to distinguish actual definitions from other
sentences also relying on syntactic features, reach-
ing high accuracy levels.
The work of (Klavans and Muresan, 2001) re-
lies on a rule-based system that makes use of “cue
phrases” and structural indicators that frequently
introduce definitions, reaching 87% of precision
and 75% of recall on a small and domain-specific
corpus.
As for the task of definition extraction, most
of the existing approaches use symbolic methods
that are based on lexico-syntactic patterns, which
are manually crafted or deduced automatically.
The seminal work of (Hearst, 1992) represents the
main approach based on fixed patterns like “NPx
is a/an NPy” and “NPx such as NPy”, that usu-
ally imply &lt; x IS-A y &gt;.
The main drawback of such technique is that it
does not face the high variability of how a relation
can be expressed in natural language. Still, it gen-
erally extracts single-word terms rather than well-
formed and compound concepts. (Berland and
Charniak, 1999) proposed similar lexico-syntactic
patterns to extract part-whole relationships.
(Del Gaudio and Branco, 2007) proposed a rule-
based approach to the extraction of hypernyms
that, however, leads to very low accuracy values
in terms of Precision.
(Ponzetto and Strube, 2007) proposed a
technique to extract hypernym relations from
Wikipedia by means of methods based on the
connectivity of the network and classical lexico-
syntactic patterns. (Yamada et al., 2009) extended
their work by combining extracted Wikipedia en-
tries with new terms contained in additional web
documents, using a distributional similarity-based
approach.
Finally, pure statistical approaches present
techniques for the extraction of hierarchies of
terms based on words frequency as well as co-
occurrence values, relying on clustering proce-
dures (Candan et al., 2008; Fortuna et al., 2006;
Yang and Callan, 2008). The central hypothesis is
that similar words tend to occur together in similar
contexts (Harris, 1954). Despite this, they are de-
fined by (Biemann, 2005) as prototype-based on-
tologies rather than formal terminological ontolo-
gies, and they usually suffer from the problem of
data sparsity in case of small corpora.
</bodyText>
<sectionHeader confidence="0.995567" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999990428571429">
In this section we present our approach to identify
hypernym relations within plain text. Our method-
ology consists in relaxing the problem into two
easier subtasks. Given a relation rel(x, y) con-
tained in a sentence, the task becomes to find 1)
a possible x, and 2) a possible y. In case of more
than one possible x or y, a further step is needed
to associate the correct x to the right y.
By seeing the problem as two different classi-
fication problems, there is no need to create ab-
stract patterns between the target terms. In ad-
dition to this, the general problem of identifying
definitional sentences can be seen as to find at least
one x and one y in a sentence.
</bodyText>
<subsectionHeader confidence="0.999693">
3.1 Local Syntactic Information
</subsectionHeader>
<bodyText confidence="0.99991675">
Dependency parsing is a procedure that extracts
syntactic dependencies among the terms contained
in a sentence. The idea is that, given a hyper-
nym relation, hyponyms and hypernyms may be
</bodyText>
<page confidence="0.991875">
533
</page>
<bodyText confidence="0.999908896551724">
characterized by specific sets of syntactic contexts.
According to this assumption, the task can be seen
as a classification problem where each term in a
sentence has to be classified as hyponym, hyper-
nym, or neither of the two.
For each noun, we construct a textual represen-
tation containing its syntactic dependencies (i.e.,
its syntactic context). In particular, for each syn-
tactic dependency dep(a, b) (or dep(b, a)) of a tar-
get noun a, we create an abstract token3 dep-
target-ˆb (or dep-ˆb-target), where bˆ becomes the
generic string “noun” in case it is another noun;
otherwise it is equal to b. This way, the nouns are
transformed into abstract strings; on the contrary,
no abstraction is done for verbs.
For instance, let us consider the sentence “The
Albedo of an object is the extent to which it dif-
fusely reflects light from the sun”. After the Part-
Of-Speech annotation, the parser will extract a se-
ries of syntactic dependencies like “det(Albedo,
The)”, “nsubj(extent, Albedo)”, “prepof(Albedo,
object)”, where det identifies a determiner, nsubj
represents a noun phrase which is the syntac-
tic subject of a clause, and so forth4. Then,
such dependencies will be transformed in abstract
terms like “det-target-the”, “nsubj-noun-target”,
and “prepof-target-noun”. These triples represent
the feature space on which the Support Vector Ma-
chine classifiers will construct the models.
</bodyText>
<subsectionHeader confidence="0.999856">
3.2 Learning phase
</subsectionHeader>
<bodyText confidence="0.9999701875">
Our model assumes a transformation of the local
syntactic information into labelled numeric vec-
tors. More in detail, given a sentence S annotated
with the terms linked by the hypernym relation,
the system produces as many input instances as
the number of nouns contained in S. For each
noun n in S, the method produces two instances
Snx and Syn, associated to the label positive or neg-
ative depending on their presence in the target re-
lation (i.e., as x or y respectively). If a noun is
not involved in a hypernym relation, both the two
instances will have the label negative. At the end
of this process, two training sets are built, i.e., one
for each relation argument, namely the x-set and
the y-set. All the instances of both the datasets are
then transformed into numeric vectors according
</bodyText>
<footnote confidence="0.9724216">
3We make use of the term “abstract” to indicate that some
words are replaced with more general entity identifiers.
4A complete overview of the Stan-
ford dependencies is available at
http://nlp.stanford.edu/software/dependencies manual.pdf.
</footnote>
<bodyText confidence="0.999774944444445">
to the Vector Space Model (Salton et al., 1975),
and are finally fed into a Support Vector Machine
classifier5 (Cortes and Vapnik, 1995). We refer to
the two resulting models as the x-model and the
y-model. These models are binary classifiers that,
given the local syntactic information of a noun, es-
timate if it can be respectively an x or a y in a hy-
pernym relation.
Once the x-model and the y-model are built, we
can both classify definitional sentences and extract
hypernym relations. In the next section we deepen
our proposed strategy in that sense.
The whole set of instances of all the sentences
are fed into two Support Vector Machine classi-
fiers, one for each target label (i.e., x and y).
At this point, it is possible to classify each term
as possible x or y by querying the respective clas-
sifiers with its local syntactic information.
</bodyText>
<sectionHeader confidence="0.7913" genericHeader="method">
4 Setting of the Tasks
</sectionHeader>
<bodyText confidence="0.999523">
In this section we present how our proposed tech-
nique is able to classify definitional sentences un-
raveling hypernym relations.
</bodyText>
<subsectionHeader confidence="0.998188">
4.1 Classification of definitional sentences
</subsectionHeader>
<bodyText confidence="0.998495142857143">
As already mentioned in previous sections, we la-
bel as definitional all the sentences that contain at
least one noun n classified as x, and one noun m
classified as y (where n =� m). In this phase, it
is not further treated the case of having more than
one x or y in one single sentence. Thus, given an
input sentence:
</bodyText>
<listItem confidence="0.971579666666667">
1. we extract all the nouns (POS-tagging),
2. we extract all the syntactic dependencies of
the nouns (dependency parsing),
3. we fed each noun (i.e., its instance) to the x-
model and to the y model,
4. we check if there exist at least one noun clas-
sified as x and one noun classified as y: in
this case, we classify the sentences as defini-
tional.
</listItem>
<subsectionHeader confidence="0.998268">
4.2 Extraction of hypernym relations
</subsectionHeader>
<bodyText confidence="0.99995775">
Our method for extracting hypernym relations
makes use of both the x-model and the y-model
as for the the task of classifying definitional sen-
tences. If exactly one x and one y are identified
</bodyText>
<footnote confidence="0.949071">
5We used the Sequential Minimal Optimization imple-
mentation of the Weka framework (Hall et al., 2009).
</footnote>
<page confidence="0.997814">
534
</page>
<bodyText confidence="0.8901865">
in the same sentence, they are directly connected
and the relation is extracted. The only constraint
is that x and y must be connected within the same
parse tree.
Now, considering our target relation hyp(x, y),
in case the sentence contains more than one noun
that is classified as x (or y), there are two possible
scenarios:
</bodyText>
<listItem confidence="0.990776">
1. there are actually more than one x (or y), or
2. the classifiers returned some false positive.
</listItem>
<bodyText confidence="0.998862473684211">
Up to now, we decided to keep all the possi-
ble combinations, without further filtering opera-
tions6. Finally, in case of multiple classifications
of both x and y, i.e., if there are multiple x and
multiple y at the same time, the problem becomes
to select which x is linked to which y7. To do this,
we simply calculate the distance between these
terms in the parse tree (the closer the terms, the
better the connection between the two). Neverthe-
less, in the used corpus, only around 1.4% of the
sentences are classified with multiple x and y.
Finally, since our method is able to extract
single nouns that can be involved in a hyper-
nym relation, we included modifiers preceded by
preposition “of”, while the other modifiers are re-
moved. For example, considering the sentence
“An Archipelago is a chain of islands”, the whole
chunk “chain of islands” is extracted from the sin-
gle triggered noun chain.
</bodyText>
<sectionHeader confidence="0.997894" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999974066666667">
In this section we present the evaluation of our
approach, that we carried out on an annotated
dataset of definitional sentences (Navigli et al.,
2010). The corpus contains 4,619 sentences ex-
tracted from Wikipedia, and only 1,908 are anno-
tated as definitional. On a first instance, we test the
classifiers on the extraction of hyponyms (x) and
hypernyms (y) from the definitional sentences, in-
dependently. Then, we evaluate the classification
of definitional sentences. Finally, we evaluate the
ability of our technique when extracting whole hy-
pernym relations. With the used dataset, the con-
structed training sets for the two classifiers (x-set
and y-set) resulted to have approximately 1,500
features.
</bodyText>
<footnote confidence="0.9985715">
6We only used the constraint that x has to be different
from y.
7Notice that this is different from the case in which a sin-
gle noun is labeled as both x and y.
</footnote>
<table confidence="0.999645">
Alg. P R F Acc
WCL-3 98.8% 60.7% 75.2 % 83.4 %
Star P. 86.7% 66.1% 75.0 % 81.8 %
Bigrams 66.7% 82.7% 73.8 % 75.8 %
Our sys. 88.0% 76.0% 81.6% 89.6%
</table>
<tableCaption confidence="0.871834833333333">
Table 1: Evaluation results for the classification of
definitional sentences, in terms of Precision (P),
Recall (R), F-Measure (F), and Accuracy (Acc),
using 10-folds cross validation. For the WCL-3
approach and the Star Patterns see (Navigli and
Velardi, 2010), and (Cui et al., 2007) for Bigrams.
</tableCaption>
<table confidence="0.999084666666667">
Algorithm P R F
WCL-3 78.58% 60.74% * 68.56%
Our system 83.05% 68.64% 75.16%
</table>
<tableCaption confidence="0.991448">
Table 2: Evaluation results for the hypernym re-
</tableCaption>
<bodyText confidence="0.930582285714286">
lation extraction, in terms of Precision (P), Re-
call (R), and F-Measure (F). For the WCL-3 ap-
proach, see (Navigli and Velardi, 2010). These re-
sults are obtained using 10-folds cross validation
(* Recall has been inherited from the definition
classification task, since no indication has been re-
ported in their contribution).
</bodyText>
<subsectionHeader confidence="0.734622">
5.1 Results
</subsectionHeader>
<bodyText confidence="0.999430809523809">
In this section we present the evaluation of our
technique on both the tasks of classifying def-
initional sentences and extracting hypernym re-
lations. Notice that our approach is susceptible
from the errors given by the POS-tagger8 and the
syntactic parser9 . In spite of this, our approach
demonstrates how syntax can be more robust for
identifying semantic relations. Our approach does
not make use of the full parse tree, and we are not
dependent on a complete and correct result of the
parser.
The goal of our evaluation is twofold: first, we
evaluate the ability of classifying definitional sen-
tences; finally, we measure the accuracy of the hy-
pernym relation extraction.
A definitional sentences is extracted only if at
least one x and one y are found in the same sen-
tence. Table 1 shows the accuracy of the ap-
proach for this task. As can be seen, our pro-
posed approach has a high Precision, with a high
Recall. Although Precision is lower than the pat-
</bodyText>
<footnote confidence="0.9999005">
8http://nlp.stanford.edu/software/tagger.shtml
9http://www-nlp.stanford.edu/software/lex-parser.shtml
</footnote>
<page confidence="0.997418">
535
</page>
<bodyText confidence="0.999825888888889">
tern matching approach proposed by (Navigli and
Velardi, 2010), our Recall is higher, leading to an
higher overall F-Measure.
Table 2 shows the results of the extraction of
the whole hypernym relations. Note that our ap-
proach has high levels of accuracy. In particular,
even in this task, our system outperforms the pat-
tern matching algorithm proposed by (Navigli and
Velardi, 2010) in terms of Precision and Recall.
</bodyText>
<sectionHeader confidence="0.982881" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999993432432433">
We presented an approach to reveal definitions and
extract underlying hypernym relations from plain
text, making use of local syntactic information fed
into a Support Vector Machine classifier. The aim
of this work was to revisit these tasks as classical
supervised learning problems that usually carry to
high accuracy levels with high performance when
faced with standard Machine Learning techniques.
Our first results on this method highlight the va-
lidity of the approach by significantly improving
current state-of-the-art techniques in the classifi-
cation of definitional sentences as well as in the
extraction of hypernym relations from text. In fu-
ture works, we aim at using larger syntactic con-
texts. In fact, currently, the detection does not
surpass the sentence level, while taxonomical in-
formation can be even contained in different sen-
tences or paragraphs. We also aim at evaluating
our approach on the construction of entire tax-
onomies starting from domain-specific text cor-
pora, as in (Navigli et al., 2011; Velardi et al.,
2012). Finally, the desired result of the task of ex-
tracting hypernym relations from text (as for any
semantic relationships in general) depends on the
domain and the specific later application. Thus,
we think that a precise evaluation and comparison
of any systems strictly depends on these factors.
For instance, given a sentence like “In mathemat-
ics, computing, linguistics and related disciplines,
an algorithm is a sequence of instructions” one
could want to extract only “instructions” as hyper-
nym (as done in the annotation), rather than the en-
tire chunk “sequence of instructions” (as extracted
by our technique). Both results can be valid, and
a further discrimination can only be done if a spe-
cific application or use of this knowlege is taken
into consideration.
</bodyText>
<sectionHeader confidence="0.990244" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999767905660377">
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Annual Meeting Association
for Computational Linguistics, volume 37, pages
57–64. Association for Computational Linguistics.
C. Biemann. 2005. Ontology learning from text: A
survey of methods. In LDVforum, volume 20, pages
75–93.
C. Borg, M. Rosner, and G. Pace. 2009. Evolutionary
algorithms for definition extraction. In Proceedings
of the 1st Workshop on Definition Extraction, pages
26–32. Association for Computational Linguistics.
K.S. Candan, L. Di Caro, and M.L. Sapino. 2008. Cre-
ating tag hierarchies for effective navigation in so-
cial media. In Proceedings of the 2008 ACM work-
shop on Search in social media, pages 75–82. ACM.
Mario Cataldi, Claudio Schifanella, K Selc¸uk Can-
dan, Maria Luisa Sapino, and Luigi Di Caro. 2009.
Cosena: a context-based search and navigation sys-
tem. In Proceedings of the International Confer-
ence on Management of Emergent Digital EcoSys-
tems, page 33. ACM.
C. Cortes and V. Vapnik. 1995. Support-vector net-
works. Machine learning, 20(3):273–297.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007.
Soft pattern matching models for definitional ques-
tion answering. ACM Trans. Inf. Syst., 25(2), April.
R. Del Gaudio and A. Branco. 2007. Automatic ex-
traction of definitions in portuguese: A rule-based
approach. Progress in Artificial Intelligence, pages
659–670.
I. Fahmi and G. Bouma. 2006. Learning to iden-
tify definitions using syntactic features. In Pro-
ceedings of the EACL 2006 workshop on Learning
Structured Information in Natural Language Appli-
cations, pages 64–71.
B. Fortuna, D. Mladeniˇc, and M. Grobelnik. 2006.
Semi-automatic construction of topic ontologies.
Semantics, Web and Mining, pages 121–131.
Aldo Gangemi, Roberto Navigli, and Paola Velardi.
2003. The ontowordnet project: Extension and
axiomatization of conceptual relations in wordnet.
In Robert Meersman, Zahir Tari, and DouglasC.
Schmidt, editors, On The Move to Meaningful In-
ternet Systems 2003: CoopIS, DOA, and ODBASE,
volume 2888 of Lecture Notes in Computer Science,
pages 820–838. Springer Berlin Heidelberg.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10–
18.
Zellig Harris. 1954. Distributional structure. Word,
10(23):146–162.
</reference>
<page confidence="0.9934">
536
</page>
<reference confidence="0.994787225">
M.A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the 14th conference on Computational linguistics-
Volume 2, pages 539–545. Association for Compu-
tational Linguistics.
E. Hovy, A. Philpot, J. Klavans, U. Germann, P. Davis,
and S. Popper. 2003. Extending metadata defi-
nitions by automatically extracting and organizing
glossary definitions. In Proceedings of the 2003 an-
nual national conference on Digital government re-
search, pages 1–6. Digital Government Society of
North America.
J.L. Klavans and S. Muresan. 2001. Evaluation of
the definder system for fully automatic glossary con-
struction. In Proceedings of the AMIA Symposium,
page 324. American Medical Informatics Associa-
tion.
Roberto Navigli and Paola Velardi. 2010. Learning
word-class lattices for definition and hypernym ex-
traction. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 1318–1327, Uppsala, Sweden, July. As-
sociation for Computational Linguistics.
Roberto Navigli, Paola Velardi, and Juana Mara Ruiz-
Martnez. 2010. An annotated dataset for extracting
definitions and hypernyms from the web. In Pro-
ceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC’10),
Valletta, Malta. European Language Resources As-
sociation (ELRA).
R. Navigli, P. Velardi, and S. Faralli. 2011. A graph-
based algorithm for inducing lexical taxonomies
from scratch. In Proceedings of the Twenty-
Second international joint conference on Artificial
Intelligence-Volume Volume Three, pages 1872–
1877. AAAI Press.
R. Navigli. 2009. Using cycles and quasi-cycles to dis-
ambiguate dictionary glosses. In Proceedings of the
12th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 594–
602. Association for Computational Linguistics.
S.P. Ponzetto and M. Strube. 2007. Deriving a large
scale taxonomy from wikipedia. In Proceedings
of the national conference on artificial intelligence,
volume 22, page 1440. Menlo Park, CA; Cambridge,
MA; London; AAAI Press; MIT Press; 1999.
G. Salton, A. Wong, and C. S. Yang. 1975. A vec-
tor space model for automatic indexing. Commun.
ACM, 18(11):613–620, November.
R. Snow, D. Jurafsky, and A.Y. Ng. 2004. Learn-
ing syntactic patterns for automatic hypernym dis-
covery. Advances in Neural Information Processing
Systems 17.
Paola Velardi, Stefano Faralli, and Roberto Navigli.
2012. Ontolearn reloaded: A graph-based algorithm
for taxonomy induction. Computational Linguistics,
pages 1–72.
Eline Westerhout. 2009. Definition extraction using
linguistic and structural features. In Proceedings
of the 1st Workshop on Definition Extraction, WDE
’09, pages 61–67, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
I. Yamada, K. Torisawa, J. Kazama, K. Kuroda, M. Mu-
rata, S. De Saeger, F. Bond, and A. Sumida. 2009.
Hypernym discovery based on distributional simi-
larity and hierarchical structures. In Proceedings
of the 2009 Conference on Empirical Methods in
Natural Language Processing: Volume 2-Volume 2,
pages 929–937. Association for Computational Lin-
guistics.
H. Yang and J. Callan. 2008. Ontology generation for
large email collections. In Proceedings of the 2008
international conference on Digital government re-
search, pages 254–261. Digital Government Society
of North America.
Chunxia Zhang and Peng Jiang. 2009. Automatic ex-
traction of definitions. In Computer Science and
Information Technology, 2009. ICCSIT 2009. 2nd
IEEE International Conference on, pages 364 –368,
aug.
</reference>
<page confidence="0.997366">
537
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.916748">
<title confidence="0.99984">Extracting Definitions and Hypernym Relations relying on Dependencies and Support Vector Machines</title>
<author confidence="0.999331">Guido</author>
<affiliation confidence="0.9993165">University of Department of Computer</affiliation>
<email confidence="0.988633">boella@di.unito.it</email>
<author confidence="0.997897">Luigi Di</author>
<affiliation confidence="0.9991705">University of Department of Computer</affiliation>
<email confidence="0.988095">dicaro@di.unito.it</email>
<abstract confidence="0.996983315789474">In this paper we present a technique to reveal definitions and hypernym relations from text. Instead of using pattern matching methods that rely on lexico-syntactic patterns, we propose a technique which only uses syntactic dependencies between terms extracted with a syntactic parser. The assumption is that syntactic information are more robust than patterns when coping with length and complexity of the sentences. Afterwards, we transform such syntactic contexts in abstract representations, that are then fed into a Support Vector Machine classifier. The results on an annotated dataset of definitional sentences demonstrate the validity of our approach overtaking current state-of-the-art techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Berland</author>
<author>E Charniak</author>
</authors>
<title>Finding parts in very large corpora.</title>
<date>1999</date>
<booktitle>In Annual Meeting Association for Computational Linguistics,</booktitle>
<volume>37</volume>
<pages>57--64</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6340" citStr="Berland and Charniak, 1999" startWordPosition="973" endWordPosition="976">corpus. As for the task of definition extraction, most of the existing approaches use symbolic methods that are based on lexico-syntactic patterns, which are manually crafted or deduced automatically. The seminal work of (Hearst, 1992) represents the main approach based on fixed patterns like “NPx is a/an NPy” and “NPx such as NPy”, that usually imply &lt; x IS-A y &gt;. The main drawback of such technique is that it does not face the high variability of how a relation can be expressed in natural language. Still, it generally extracts single-word terms rather than wellformed and compound concepts. (Berland and Charniak, 1999) proposed similar lexico-syntactic patterns to extract part-whole relationships. (Del Gaudio and Branco, 2007) proposed a rulebased approach to the extraction of hypernyms that, however, leads to very low accuracy values in terms of Precision. (Ponzetto and Strube, 2007) proposed a technique to extract hypernym relations from Wikipedia by means of methods based on the connectivity of the network and classical lexicosyntactic patterns. (Yamada et al., 2009) extended their work by combining extracted Wikipedia entries with new terms contained in additional web documents, using a distributional s</context>
</contexts>
<marker>Berland, Charniak, 1999</marker>
<rawString>M. Berland and E. Charniak. 1999. Finding parts in very large corpora. In Annual Meeting Association for Computational Linguistics, volume 37, pages 57–64. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Biemann</author>
</authors>
<title>Ontology learning from text: A survey of methods.</title>
<date>2005</date>
<booktitle>In LDVforum,</booktitle>
<volume>20</volume>
<pages>75--93</pages>
<contexts>
<context position="7376" citStr="Biemann, 2005" startWordPosition="1130" endWordPosition="1131">tactic patterns. (Yamada et al., 2009) extended their work by combining extracted Wikipedia entries with new terms contained in additional web documents, using a distributional similarity-based approach. Finally, pure statistical approaches present techniques for the extraction of hierarchies of terms based on words frequency as well as cooccurrence values, relying on clustering procedures (Candan et al., 2008; Fortuna et al., 2006; Yang and Callan, 2008). The central hypothesis is that similar words tend to occur together in similar contexts (Harris, 1954). Despite this, they are defined by (Biemann, 2005) as prototype-based ontologies rather than formal terminological ontologies, and they usually suffer from the problem of data sparsity in case of small corpora. 3 Approach In this section we present our approach to identify hypernym relations within plain text. Our methodology consists in relaxing the problem into two easier subtasks. Given a relation rel(x, y) contained in a sentence, the task becomes to find 1) a possible x, and 2) a possible y. In case of more than one possible x or y, a further step is needed to associate the correct x to the right y. By seeing the problem as two different</context>
</contexts>
<marker>Biemann, 2005</marker>
<rawString>C. Biemann. 2005. Ontology learning from text: A survey of methods. In LDVforum, volume 20, pages 75–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Borg</author>
<author>M Rosner</author>
<author>G Pace</author>
</authors>
<title>Evolutionary algorithms for definition extraction.</title>
<date>2009</date>
<booktitle>In Proceedings of the 1st Workshop on Definition Extraction,</booktitle>
<pages>26--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="4678" citStr="Borg et al., 2009" startWordPosition="706" endWordPosition="709"> most of the proposed techniques rely on lexico-syntactic patterns, either manually or semiautomatically produced (Hovy et al., 2003; Zhang and Jiang, 2009; Westerhout, 2009). Such patterns are sequences of words like “is a” or “refers to”, rather than more complex sequences including part-of-speech tags. In the work of (Westerhout, 2009), after a manual identification of types of definitions and related patterns contained in a corpus, he successively applied Machine Learning techniques on syntactic and location features to improve the results. A fully-automatic approach has been proposed by (Borg et al., 2009), where the authors applied genetic algorithms to the extraction of English definitions containing the keyword “is”. In detail, they assign weights to a set of features for the classification of definitional sentences, reaching a precision of 62% and a recall of 52%. Then, (Cui et al., 2007) proposed an approach based on soft patterns, i.e., probabilistic lexicosemantic patterns that are able to generalize over rigid patterns enabling partial matching by calculating a generative degree-of-match probability between a test instance and the set of training instances. Similarly to our approach, (F</context>
</contexts>
<marker>Borg, Rosner, Pace, 2009</marker>
<rawString>C. Borg, M. Rosner, and G. Pace. 2009. Evolutionary algorithms for definition extraction. In Proceedings of the 1st Workshop on Definition Extraction, pages 26–32. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Candan</author>
<author>L Di Caro</author>
<author>M L Sapino</author>
</authors>
<title>Creating tag hierarchies for effective navigation in social media.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 ACM workshop on Search in social media,</booktitle>
<pages>75--82</pages>
<publisher>ACM.</publisher>
<marker>Candan, Di Caro, Sapino, 2008</marker>
<rawString>K.S. Candan, L. Di Caro, and M.L. Sapino. 2008. Creating tag hierarchies for effective navigation in social media. In Proceedings of the 2008 ACM workshop on Search in social media, pages 75–82. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mario Cataldi</author>
<author>Claudio Schifanella</author>
<author>K Selc¸uk Candan</author>
<author>Maria Luisa Sapino</author>
<author>Luigi Di Caro</author>
</authors>
<title>Cosena: a context-based search and navigation system.</title>
<date>2009</date>
<booktitle>In Proceedings of the International Conference on Management of Emergent Digital EcoSystems,</booktitle>
<pages>33</pages>
<publisher>ACM.</publisher>
<marker>Cataldi, Schifanella, Candan, Sapino, Di Caro, 2009</marker>
<rawString>Mario Cataldi, Claudio Schifanella, K Selc¸uk Candan, Maria Luisa Sapino, and Luigi Di Caro. 2009. Cosena: a context-based search and navigation system. In Proceedings of the International Conference on Management of Emergent Digital EcoSystems, page 33. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cortes</author>
<author>V Vapnik</author>
</authors>
<title>Support-vector networks.</title>
<date>1995</date>
<booktitle>Machine learning,</booktitle>
<pages>20--3</pages>
<contexts>
<context position="11018" citStr="Cortes and Vapnik, 1995" startWordPosition="1733" endWordPosition="1736"> will have the label negative. At the end of this process, two training sets are built, i.e., one for each relation argument, namely the x-set and the y-set. All the instances of both the datasets are then transformed into numeric vectors according 3We make use of the term “abstract” to indicate that some words are replaced with more general entity identifiers. 4A complete overview of the Stanford dependencies is available at http://nlp.stanford.edu/software/dependencies manual.pdf. to the Vector Space Model (Salton et al., 1975), and are finally fed into a Support Vector Machine classifier5 (Cortes and Vapnik, 1995). We refer to the two resulting models as the x-model and the y-model. These models are binary classifiers that, given the local syntactic information of a noun, estimate if it can be respectively an x or a y in a hypernym relation. Once the x-model and the y-model are built, we can both classify definitional sentences and extract hypernym relations. In the next section we deepen our proposed strategy in that sense. The whole set of instances of all the sentences are fed into two Support Vector Machine classifiers, one for each target label (i.e., x and y). At this point, it is possible to cla</context>
</contexts>
<marker>Cortes, Vapnik, 1995</marker>
<rawString>C. Cortes and V. Vapnik. 1995. Support-vector networks. Machine learning, 20(3):273–297.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hang Cui</author>
<author>Min-Yen Kan</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Soft pattern matching models for definitional question answering.</title>
<date>2007</date>
<journal>ACM Trans. Inf. Syst.,</journal>
<volume>25</volume>
<issue>2</issue>
<contexts>
<context position="2190" citStr="Cui et al., 2007" startWordPosition="324" endWordPosition="327">rmits to automatically (or semiautomatically) extract structured knowledge from plain text. Manual construction of ontologies usually requires strong efforts from domain experts, and it thus needs an automatization in such sense. 1http://www.wikipedia.org/ 2April 12, 2013. In this paper, we focus on the extraction of hypernym relations. The first step of such task relies on the identification of what (Navigli and Velardi, 2010) called definitional sentences, i.e., sentences that contain at least one hypernym relation. This subtask is important by itself for many tasks like Question Answering (Cui et al., 2007), construction of glossaries (Klavans and Muresan, 2001), extraction of taxonomic and non-taxonomic relations (Navigli, 2009; Snow et al., 2004), enrichment of concepts (Gangemi et al., 2003; Cataldi et al., 2009), and so forth. Hypernym relation extraction involves two aspects: linguistic knowlege, and model learning. Patterns collapse both of them, preventing to face them separately with the most suitable techniques. First, patterns have limited expressivity; then, linguistic knowledge inside patterns is learned from small corpora, so it is likely to have low coverage. Classification strictl</context>
<context position="4970" citStr="Cui et al., 2007" startWordPosition="756" endWordPosition="759">-speech tags. In the work of (Westerhout, 2009), after a manual identification of types of definitions and related patterns contained in a corpus, he successively applied Machine Learning techniques on syntactic and location features to improve the results. A fully-automatic approach has been proposed by (Borg et al., 2009), where the authors applied genetic algorithms to the extraction of English definitions containing the keyword “is”. In detail, they assign weights to a set of features for the classification of definitional sentences, reaching a precision of 62% and a recall of 52%. Then, (Cui et al., 2007) proposed an approach based on soft patterns, i.e., probabilistic lexicosemantic patterns that are able to generalize over rigid patterns enabling partial matching by calculating a generative degree-of-match probability between a test instance and the set of training instances. Similarly to our approach, (Fahmi and Bouma, 2006) used three different Machine Learning algorithms to distinguish actual definitions from other sentences also relying on syntactic features, reaching high accuracy levels. The work of (Klavans and Muresan, 2001) relies on a rule-based system that makes use of “cue phrase</context>
<context position="15561" citStr="Cui et al., 2007" startWordPosition="2529" endWordPosition="2532">imately 1,500 features. 6We only used the constraint that x has to be different from y. 7Notice that this is different from the case in which a single noun is labeled as both x and y. Alg. P R F Acc WCL-3 98.8% 60.7% 75.2 % 83.4 % Star P. 86.7% 66.1% 75.0 % 81.8 % Bigrams 66.7% 82.7% 73.8 % 75.8 % Our sys. 88.0% 76.0% 81.6% 89.6% Table 1: Evaluation results for the classification of definitional sentences, in terms of Precision (P), Recall (R), F-Measure (F), and Accuracy (Acc), using 10-folds cross validation. For the WCL-3 approach and the Star Patterns see (Navigli and Velardi, 2010), and (Cui et al., 2007) for Bigrams. Algorithm P R F WCL-3 78.58% 60.74% * 68.56% Our system 83.05% 68.64% 75.16% Table 2: Evaluation results for the hypernym relation extraction, in terms of Precision (P), Recall (R), and F-Measure (F). For the WCL-3 approach, see (Navigli and Velardi, 2010). These results are obtained using 10-folds cross validation (* Recall has been inherited from the definition classification task, since no indication has been reported in their contribution). 5.1 Results In this section we present the evaluation of our technique on both the tasks of classifying definitional sentences and extrac</context>
</contexts>
<marker>Cui, Kan, Chua, 2007</marker>
<rawString>Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007. Soft pattern matching models for definitional question answering. ACM Trans. Inf. Syst., 25(2), April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Del Gaudio</author>
<author>A Branco</author>
</authors>
<title>Automatic extraction of definitions in portuguese: A rule-based approach.</title>
<date>2007</date>
<booktitle>Progress in Artificial Intelligence,</booktitle>
<pages>659--670</pages>
<contexts>
<context position="6450" citStr="Gaudio and Branco, 2007" startWordPosition="986" endWordPosition="989">ased on lexico-syntactic patterns, which are manually crafted or deduced automatically. The seminal work of (Hearst, 1992) represents the main approach based on fixed patterns like “NPx is a/an NPy” and “NPx such as NPy”, that usually imply &lt; x IS-A y &gt;. The main drawback of such technique is that it does not face the high variability of how a relation can be expressed in natural language. Still, it generally extracts single-word terms rather than wellformed and compound concepts. (Berland and Charniak, 1999) proposed similar lexico-syntactic patterns to extract part-whole relationships. (Del Gaudio and Branco, 2007) proposed a rulebased approach to the extraction of hypernyms that, however, leads to very low accuracy values in terms of Precision. (Ponzetto and Strube, 2007) proposed a technique to extract hypernym relations from Wikipedia by means of methods based on the connectivity of the network and classical lexicosyntactic patterns. (Yamada et al., 2009) extended their work by combining extracted Wikipedia entries with new terms contained in additional web documents, using a distributional similarity-based approach. Finally, pure statistical approaches present techniques for the extraction of hierar</context>
</contexts>
<marker>Gaudio, Branco, 2007</marker>
<rawString>R. Del Gaudio and A. Branco. 2007. Automatic extraction of definitions in portuguese: A rule-based approach. Progress in Artificial Intelligence, pages 659–670.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Fahmi</author>
<author>G Bouma</author>
</authors>
<title>Learning to identify definitions using syntactic features.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 workshop on Learning Structured Information in Natural Language Applications,</booktitle>
<pages>64--71</pages>
<contexts>
<context position="5299" citStr="Fahmi and Bouma, 2006" startWordPosition="805" endWordPosition="808">), where the authors applied genetic algorithms to the extraction of English definitions containing the keyword “is”. In detail, they assign weights to a set of features for the classification of definitional sentences, reaching a precision of 62% and a recall of 52%. Then, (Cui et al., 2007) proposed an approach based on soft patterns, i.e., probabilistic lexicosemantic patterns that are able to generalize over rigid patterns enabling partial matching by calculating a generative degree-of-match probability between a test instance and the set of training instances. Similarly to our approach, (Fahmi and Bouma, 2006) used three different Machine Learning algorithms to distinguish actual definitions from other sentences also relying on syntactic features, reaching high accuracy levels. The work of (Klavans and Muresan, 2001) relies on a rule-based system that makes use of “cue phrases” and structural indicators that frequently introduce definitions, reaching 87% of precision and 75% of recall on a small and domain-specific corpus. As for the task of definition extraction, most of the existing approaches use symbolic methods that are based on lexico-syntactic patterns, which are manually crafted or deduced </context>
</contexts>
<marker>Fahmi, Bouma, 2006</marker>
<rawString>I. Fahmi and G. Bouma. 2006. Learning to identify definitions using syntactic features. In Proceedings of the EACL 2006 workshop on Learning Structured Information in Natural Language Applications, pages 64–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Fortuna</author>
<author>D Mladeniˇc</author>
<author>M Grobelnik</author>
</authors>
<title>Semi-automatic construction of topic ontologies. Semantics, Web and Mining,</title>
<date>2006</date>
<pages>121--131</pages>
<marker>Fortuna, Mladeniˇc, Grobelnik, 2006</marker>
<rawString>B. Fortuna, D. Mladeniˇc, and M. Grobelnik. 2006. Semi-automatic construction of topic ontologies. Semantics, Web and Mining, pages 121–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aldo Gangemi</author>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>The ontowordnet project: Extension and axiomatization of conceptual relations in wordnet.</title>
<date>2003</date>
<booktitle>On The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE,</booktitle>
<volume>2888</volume>
<pages>820--838</pages>
<editor>In Robert Meersman, Zahir Tari, and DouglasC. Schmidt, editors,</editor>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="2380" citStr="Gangemi et al., 2003" startWordPosition="353" endWordPosition="356">hus needs an automatization in such sense. 1http://www.wikipedia.org/ 2April 12, 2013. In this paper, we focus on the extraction of hypernym relations. The first step of such task relies on the identification of what (Navigli and Velardi, 2010) called definitional sentences, i.e., sentences that contain at least one hypernym relation. This subtask is important by itself for many tasks like Question Answering (Cui et al., 2007), construction of glossaries (Klavans and Muresan, 2001), extraction of taxonomic and non-taxonomic relations (Navigli, 2009; Snow et al., 2004), enrichment of concepts (Gangemi et al., 2003; Cataldi et al., 2009), and so forth. Hypernym relation extraction involves two aspects: linguistic knowlege, and model learning. Patterns collapse both of them, preventing to face them separately with the most suitable techniques. First, patterns have limited expressivity; then, linguistic knowledge inside patterns is learned from small corpora, so it is likely to have low coverage. Classification strictly depends on the learned patterns, so performance decreases, and the available classification techniques are restricted to those compatible with the pattern approach. Instead, we use a synta</context>
</contexts>
<marker>Gangemi, Navigli, Velardi, 2003</marker>
<rawString>Aldo Gangemi, Roberto Navigli, and Paola Velardi. 2003. The ontowordnet project: Extension and axiomatization of conceptual relations in wordnet. In Robert Meersman, Zahir Tari, and DouglasC. Schmidt, editors, On The Move to Meaningful Internet Systems 2003: CoopIS, DOA, and ODBASE, volume 2888 of Lecture Notes in Computer Science, pages 820–838. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The weka data mining software: an update.</title>
<date>2009</date>
<journal>ACM SIGKDD Explorations Newsletter,</journal>
<volume>11</volume>
<issue>1</issue>
<pages>18</pages>
<contexts>
<context position="12921" citStr="Hall et al., 2009" startWordPosition="2073" endWordPosition="2076">ll the syntactic dependencies of the nouns (dependency parsing), 3. we fed each noun (i.e., its instance) to the xmodel and to the y model, 4. we check if there exist at least one noun classified as x and one noun classified as y: in this case, we classify the sentences as definitional. 4.2 Extraction of hypernym relations Our method for extracting hypernym relations makes use of both the x-model and the y-model as for the the task of classifying definitional sentences. If exactly one x and one y are identified 5We used the Sequential Minimal Optimization implementation of the Weka framework (Hall et al., 2009). 534 in the same sentence, they are directly connected and the relation is extracted. The only constraint is that x and y must be connected within the same parse tree. Now, considering our target relation hyp(x, y), in case the sentence contains more than one noun that is classified as x (or y), there are two possible scenarios: 1. there are actually more than one x (or y), or 2. the classifiers returned some false positive. Up to now, we decided to keep all the possible combinations, without further filtering operations6. Finally, in case of multiple classifications of both x and y, i.e., if</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H Witten. 2009. The weka data mining software: an update. ACM SIGKDD Explorations Newsletter, 11(1):10– 18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<journal>Distributional structure. Word,</journal>
<volume>10</volume>
<issue>23</issue>
<contexts>
<context position="7325" citStr="Harris, 1954" startWordPosition="1121" endWordPosition="1122">onnectivity of the network and classical lexicosyntactic patterns. (Yamada et al., 2009) extended their work by combining extracted Wikipedia entries with new terms contained in additional web documents, using a distributional similarity-based approach. Finally, pure statistical approaches present techniques for the extraction of hierarchies of terms based on words frequency as well as cooccurrence values, relying on clustering procedures (Candan et al., 2008; Fortuna et al., 2006; Yang and Callan, 2008). The central hypothesis is that similar words tend to occur together in similar contexts (Harris, 1954). Despite this, they are defined by (Biemann, 2005) as prototype-based ontologies rather than formal terminological ontologies, and they usually suffer from the problem of data sparsity in case of small corpora. 3 Approach In this section we present our approach to identify hypernym relations within plain text. Our methodology consists in relaxing the problem into two easier subtasks. Given a relation rel(x, y) contained in a sentence, the task becomes to find 1) a possible x, and 2) a possible y. In case of more than one possible x or y, a further step is needed to associate the correct x to </context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(23):146–162.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proceedings of the 14th conference on Computational linguisticsVolume 2,</booktitle>
<pages>539--545</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5948" citStr="Hearst, 1992" startWordPosition="905" endWordPosition="906">ng algorithms to distinguish actual definitions from other sentences also relying on syntactic features, reaching high accuracy levels. The work of (Klavans and Muresan, 2001) relies on a rule-based system that makes use of “cue phrases” and structural indicators that frequently introduce definitions, reaching 87% of precision and 75% of recall on a small and domain-specific corpus. As for the task of definition extraction, most of the existing approaches use symbolic methods that are based on lexico-syntactic patterns, which are manually crafted or deduced automatically. The seminal work of (Hearst, 1992) represents the main approach based on fixed patterns like “NPx is a/an NPy” and “NPx such as NPy”, that usually imply &lt; x IS-A y &gt;. The main drawback of such technique is that it does not face the high variability of how a relation can be expressed in natural language. Still, it generally extracts single-word terms rather than wellformed and compound concepts. (Berland and Charniak, 1999) proposed similar lexico-syntactic patterns to extract part-whole relationships. (Del Gaudio and Branco, 2007) proposed a rulebased approach to the extraction of hypernyms that, however, leads to very low acc</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M.A. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proceedings of the 14th conference on Computational linguisticsVolume 2, pages 539–545. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hovy</author>
<author>A Philpot</author>
<author>J Klavans</author>
<author>U Germann</author>
<author>P Davis</author>
<author>S Popper</author>
</authors>
<title>Extending metadata definitions by automatically extracting and organizing glossary definitions.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 annual national conference on Digital government research,</booktitle>
<pages>1--6</pages>
<publisher>Digital Government Society of North America.</publisher>
<contexts>
<context position="4192" citStr="Hovy et al., 2003" startWordPosition="629" endWordPosition="632">from plain text. We will use the term definitional sentence referring to the more general meaning given by (Navigli and Velardi, 2010): A sentence that provides a for532 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 532–537, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics mal explanation for the term of interest, and more specifically as a sentence containing at least one hypernym relation. So far, most of the proposed techniques rely on lexico-syntactic patterns, either manually or semiautomatically produced (Hovy et al., 2003; Zhang and Jiang, 2009; Westerhout, 2009). Such patterns are sequences of words like “is a” or “refers to”, rather than more complex sequences including part-of-speech tags. In the work of (Westerhout, 2009), after a manual identification of types of definitions and related patterns contained in a corpus, he successively applied Machine Learning techniques on syntactic and location features to improve the results. A fully-automatic approach has been proposed by (Borg et al., 2009), where the authors applied genetic algorithms to the extraction of English definitions containing the keyword “is</context>
</contexts>
<marker>Hovy, Philpot, Klavans, Germann, Davis, Popper, 2003</marker>
<rawString>E. Hovy, A. Philpot, J. Klavans, U. Germann, P. Davis, and S. Popper. 2003. Extending metadata definitions by automatically extracting and organizing glossary definitions. In Proceedings of the 2003 annual national conference on Digital government research, pages 1–6. Digital Government Society of North America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Klavans</author>
<author>S Muresan</author>
</authors>
<title>Evaluation of the definder system for fully automatic glossary construction.</title>
<date>2001</date>
<booktitle>In Proceedings of the AMIA Symposium,</booktitle>
<pages>324</pages>
<publisher>American Medical Informatics Association.</publisher>
<contexts>
<context position="2246" citStr="Klavans and Muresan, 2001" startWordPosition="332" endWordPosition="335">xtract structured knowledge from plain text. Manual construction of ontologies usually requires strong efforts from domain experts, and it thus needs an automatization in such sense. 1http://www.wikipedia.org/ 2April 12, 2013. In this paper, we focus on the extraction of hypernym relations. The first step of such task relies on the identification of what (Navigli and Velardi, 2010) called definitional sentences, i.e., sentences that contain at least one hypernym relation. This subtask is important by itself for many tasks like Question Answering (Cui et al., 2007), construction of glossaries (Klavans and Muresan, 2001), extraction of taxonomic and non-taxonomic relations (Navigli, 2009; Snow et al., 2004), enrichment of concepts (Gangemi et al., 2003; Cataldi et al., 2009), and so forth. Hypernym relation extraction involves two aspects: linguistic knowlege, and model learning. Patterns collapse both of them, preventing to face them separately with the most suitable techniques. First, patterns have limited expressivity; then, linguistic knowledge inside patterns is learned from small corpora, so it is likely to have low coverage. Classification strictly depends on the learned patterns, so performance decrea</context>
<context position="5510" citStr="Klavans and Muresan, 2001" startWordPosition="836" endWordPosition="839">l sentences, reaching a precision of 62% and a recall of 52%. Then, (Cui et al., 2007) proposed an approach based on soft patterns, i.e., probabilistic lexicosemantic patterns that are able to generalize over rigid patterns enabling partial matching by calculating a generative degree-of-match probability between a test instance and the set of training instances. Similarly to our approach, (Fahmi and Bouma, 2006) used three different Machine Learning algorithms to distinguish actual definitions from other sentences also relying on syntactic features, reaching high accuracy levels. The work of (Klavans and Muresan, 2001) relies on a rule-based system that makes use of “cue phrases” and structural indicators that frequently introduce definitions, reaching 87% of precision and 75% of recall on a small and domain-specific corpus. As for the task of definition extraction, most of the existing approaches use symbolic methods that are based on lexico-syntactic patterns, which are manually crafted or deduced automatically. The seminal work of (Hearst, 1992) represents the main approach based on fixed patterns like “NPx is a/an NPy” and “NPx such as NPy”, that usually imply &lt; x IS-A y &gt;. The main drawback of such tec</context>
</contexts>
<marker>Klavans, Muresan, 2001</marker>
<rawString>J.L. Klavans and S. Muresan. 2001. Evaluation of the definder system for fully automatic glossary construction. In Proceedings of the AMIA Symposium, page 324. American Medical Informatics Association.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
</authors>
<title>Learning word-class lattices for definition and hypernym extraction.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1318--1327</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Uppsala, Sweden,</location>
<contexts>
<context position="2004" citStr="Navigli and Velardi, 2010" startWordPosition="296" endWordPosition="299"> another point of view, there is the need of having more structured data in the forms of ontologies, in order to allow semantics-based retrieval and reasoning. Ontology Learning is a task that permits to automatically (or semiautomatically) extract structured knowledge from plain text. Manual construction of ontologies usually requires strong efforts from domain experts, and it thus needs an automatization in such sense. 1http://www.wikipedia.org/ 2April 12, 2013. In this paper, we focus on the extraction of hypernym relations. The first step of such task relies on the identification of what (Navigli and Velardi, 2010) called definitional sentences, i.e., sentences that contain at least one hypernym relation. This subtask is important by itself for many tasks like Question Answering (Cui et al., 2007), construction of glossaries (Klavans and Muresan, 2001), extraction of taxonomic and non-taxonomic relations (Navigli, 2009; Snow et al., 2004), enrichment of concepts (Gangemi et al., 2003; Cataldi et al., 2009), and so forth. Hypernym relation extraction involves two aspects: linguistic knowlege, and model learning. Patterns collapse both of them, preventing to face them separately with the most suitable tec</context>
<context position="3709" citStr="Navigli and Velardi, 2010" startWordPosition="558" endWordPosition="561">essivity), and a state-of-the-art approach to learn models with the use of Support Vector Machine classifiers. Our assumption is that syntax is less dependent than learned patterns from the length and the complexity of textual expressions. In some way, patterns grasp syntactic relationships, but they actually do not use them as input knowledge. 2 Related Work In this section we present the current state of the art concerning the automatic extraction of definitions and hypernym relations from plain text. We will use the term definitional sentence referring to the more general meaning given by (Navigli and Velardi, 2010): A sentence that provides a for532 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 532–537, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics mal explanation for the term of interest, and more specifically as a sentence containing at least one hypernym relation. So far, most of the proposed techniques rely on lexico-syntactic patterns, either manually or semiautomatically produced (Hovy et al., 2003; Zhang and Jiang, 2009; Westerhout, 2009). Such patterns are sequences of words like “is a” or “refers to”, rather t</context>
<context position="15537" citStr="Navigli and Velardi, 2010" startWordPosition="2524" endWordPosition="2527">nd y-set) resulted to have approximately 1,500 features. 6We only used the constraint that x has to be different from y. 7Notice that this is different from the case in which a single noun is labeled as both x and y. Alg. P R F Acc WCL-3 98.8% 60.7% 75.2 % 83.4 % Star P. 86.7% 66.1% 75.0 % 81.8 % Bigrams 66.7% 82.7% 73.8 % 75.8 % Our sys. 88.0% 76.0% 81.6% 89.6% Table 1: Evaluation results for the classification of definitional sentences, in terms of Precision (P), Recall (R), F-Measure (F), and Accuracy (Acc), using 10-folds cross validation. For the WCL-3 approach and the Star Patterns see (Navigli and Velardi, 2010), and (Cui et al., 2007) for Bigrams. Algorithm P R F WCL-3 78.58% 60.74% * 68.56% Our system 83.05% 68.64% 75.16% Table 2: Evaluation results for the hypernym relation extraction, in terms of Precision (P), Recall (R), and F-Measure (F). For the WCL-3 approach, see (Navigli and Velardi, 2010). These results are obtained using 10-folds cross validation (* Recall has been inherited from the definition classification task, since no indication has been reported in their contribution). 5.1 Results In this section we present the evaluation of our technique on both the tasks of classifying definitio</context>
<context position="17161" citStr="Navigli and Velardi, 2010" startWordPosition="2787" endWordPosition="2790">e goal of our evaluation is twofold: first, we evaluate the ability of classifying definitional sentences; finally, we measure the accuracy of the hypernym relation extraction. A definitional sentences is extracted only if at least one x and one y are found in the same sentence. Table 1 shows the accuracy of the approach for this task. As can be seen, our proposed approach has a high Precision, with a high Recall. Although Precision is lower than the pat8http://nlp.stanford.edu/software/tagger.shtml 9http://www-nlp.stanford.edu/software/lex-parser.shtml 535 tern matching approach proposed by (Navigli and Velardi, 2010), our Recall is higher, leading to an higher overall F-Measure. Table 2 shows the results of the extraction of the whole hypernym relations. Note that our approach has high levels of accuracy. In particular, even in this task, our system outperforms the pattern matching algorithm proposed by (Navigli and Velardi, 2010) in terms of Precision and Recall. 6 Conclusion and Future Work We presented an approach to reveal definitions and extract underlying hypernym relations from plain text, making use of local syntactic information fed into a Support Vector Machine classifier. The aim of this work w</context>
</contexts>
<marker>Navigli, Velardi, 2010</marker>
<rawString>Roberto Navigli and Paola Velardi. 2010. Learning word-class lattices for definition and hypernym extraction. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1318–1327, Uppsala, Sweden, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Paola Velardi</author>
<author>Juana Mara RuizMartnez</author>
</authors>
<title>An annotated dataset for extracting definitions and hypernyms from the web.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta,</location>
<contexts>
<context position="14415" citStr="Navigli et al., 2010" startWordPosition="2334" endWordPosition="2337">eless, in the used corpus, only around 1.4% of the sentences are classified with multiple x and y. Finally, since our method is able to extract single nouns that can be involved in a hypernym relation, we included modifiers preceded by preposition “of”, while the other modifiers are removed. For example, considering the sentence “An Archipelago is a chain of islands”, the whole chunk “chain of islands” is extracted from the single triggered noun chain. 5 Evaluation In this section we present the evaluation of our approach, that we carried out on an annotated dataset of definitional sentences (Navigli et al., 2010). The corpus contains 4,619 sentences extracted from Wikipedia, and only 1,908 are annotated as definitional. On a first instance, we test the classifiers on the extraction of hyponyms (x) and hypernyms (y) from the definitional sentences, independently. Then, we evaluate the classification of definitional sentences. Finally, we evaluate the ability of our technique when extracting whole hypernym relations. With the used dataset, the constructed training sets for the two classifiers (x-set and y-set) resulted to have approximately 1,500 features. 6We only used the constraint that x has to be d</context>
</contexts>
<marker>Navigli, Velardi, RuizMartnez, 2010</marker>
<rawString>Roberto Navigli, Paola Velardi, and Juana Mara RuizMartnez. 2010. An annotated dataset for extracting definitions and hypernyms from the web. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
<author>S Faralli</author>
</authors>
<title>A graphbased algorithm for inducing lexical taxonomies from scratch.</title>
<date>2011</date>
<booktitle>In Proceedings of the TwentySecond international joint conference on Artificial Intelligence-Volume Volume Three,</booktitle>
<pages>1872--1877</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="18565" citStr="Navigli et al., 2011" startWordPosition="3011" endWordPosition="3014">s. Our first results on this method highlight the validity of the approach by significantly improving current state-of-the-art techniques in the classification of definitional sentences as well as in the extraction of hypernym relations from text. In future works, we aim at using larger syntactic contexts. In fact, currently, the detection does not surpass the sentence level, while taxonomical information can be even contained in different sentences or paragraphs. We also aim at evaluating our approach on the construction of entire taxonomies starting from domain-specific text corpora, as in (Navigli et al., 2011; Velardi et al., 2012). Finally, the desired result of the task of extracting hypernym relations from text (as for any semantic relationships in general) depends on the domain and the specific later application. Thus, we think that a precise evaluation and comparison of any systems strictly depends on these factors. For instance, given a sentence like “In mathematics, computing, linguistics and related disciplines, an algorithm is a sequence of instructions” one could want to extract only “instructions” as hypernym (as done in the annotation), rather than the entire chunk “sequence of instruc</context>
</contexts>
<marker>Navigli, Velardi, Faralli, 2011</marker>
<rawString>R. Navigli, P. Velardi, and S. Faralli. 2011. A graphbased algorithm for inducing lexical taxonomies from scratch. In Proceedings of the TwentySecond international joint conference on Artificial Intelligence-Volume Volume Three, pages 1872– 1877. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
</authors>
<title>Using cycles and quasi-cycles to disambiguate dictionary glosses.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>594--602</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2314" citStr="Navigli, 2009" startWordPosition="343" endWordPosition="344">ually requires strong efforts from domain experts, and it thus needs an automatization in such sense. 1http://www.wikipedia.org/ 2April 12, 2013. In this paper, we focus on the extraction of hypernym relations. The first step of such task relies on the identification of what (Navigli and Velardi, 2010) called definitional sentences, i.e., sentences that contain at least one hypernym relation. This subtask is important by itself for many tasks like Question Answering (Cui et al., 2007), construction of glossaries (Klavans and Muresan, 2001), extraction of taxonomic and non-taxonomic relations (Navigli, 2009; Snow et al., 2004), enrichment of concepts (Gangemi et al., 2003; Cataldi et al., 2009), and so forth. Hypernym relation extraction involves two aspects: linguistic knowlege, and model learning. Patterns collapse both of them, preventing to face them separately with the most suitable techniques. First, patterns have limited expressivity; then, linguistic knowledge inside patterns is learned from small corpora, so it is likely to have low coverage. Classification strictly depends on the learned patterns, so performance decreases, and the available classification techniques are restricted to t</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>R. Navigli. 2009. Using cycles and quasi-cycles to disambiguate dictionary glosses. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 594– 602. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S P Ponzetto</author>
<author>M Strube</author>
</authors>
<title>Deriving a large scale taxonomy from wikipedia.</title>
<date>2007</date>
<booktitle>In Proceedings of the national conference on artificial intelligence,</booktitle>
<volume>22</volume>
<pages>1440</pages>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<contexts>
<context position="6611" citStr="Ponzetto and Strube, 2007" startWordPosition="1012" endWordPosition="1015">n fixed patterns like “NPx is a/an NPy” and “NPx such as NPy”, that usually imply &lt; x IS-A y &gt;. The main drawback of such technique is that it does not face the high variability of how a relation can be expressed in natural language. Still, it generally extracts single-word terms rather than wellformed and compound concepts. (Berland and Charniak, 1999) proposed similar lexico-syntactic patterns to extract part-whole relationships. (Del Gaudio and Branco, 2007) proposed a rulebased approach to the extraction of hypernyms that, however, leads to very low accuracy values in terms of Precision. (Ponzetto and Strube, 2007) proposed a technique to extract hypernym relations from Wikipedia by means of methods based on the connectivity of the network and classical lexicosyntactic patterns. (Yamada et al., 2009) extended their work by combining extracted Wikipedia entries with new terms contained in additional web documents, using a distributional similarity-based approach. Finally, pure statistical approaches present techniques for the extraction of hierarchies of terms based on words frequency as well as cooccurrence values, relying on clustering procedures (Candan et al., 2008; Fortuna et al., 2006; Yang and Cal</context>
</contexts>
<marker>Ponzetto, Strube, 2007</marker>
<rawString>S.P. Ponzetto and M. Strube. 2007. Deriving a large scale taxonomy from wikipedia. In Proceedings of the national conference on artificial intelligence, volume 22, page 1440. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>A Wong</author>
<author>C S Yang</author>
</authors>
<title>A vector space model for automatic indexing.</title>
<date>1975</date>
<journal>Commun. ACM,</journal>
<volume>18</volume>
<issue>11</issue>
<contexts>
<context position="10929" citStr="Salton et al., 1975" startWordPosition="1719" endWordPosition="1722">spectively). If a noun is not involved in a hypernym relation, both the two instances will have the label negative. At the end of this process, two training sets are built, i.e., one for each relation argument, namely the x-set and the y-set. All the instances of both the datasets are then transformed into numeric vectors according 3We make use of the term “abstract” to indicate that some words are replaced with more general entity identifiers. 4A complete overview of the Stanford dependencies is available at http://nlp.stanford.edu/software/dependencies manual.pdf. to the Vector Space Model (Salton et al., 1975), and are finally fed into a Support Vector Machine classifier5 (Cortes and Vapnik, 1995). We refer to the two resulting models as the x-model and the y-model. These models are binary classifiers that, given the local syntactic information of a noun, estimate if it can be respectively an x or a y in a hypernym relation. Once the x-model and the y-model are built, we can both classify definitional sentences and extract hypernym relations. In the next section we deepen our proposed strategy in that sense. The whole set of instances of all the sentences are fed into two Support Vector Machine cla</context>
</contexts>
<marker>Salton, Wong, Yang, 1975</marker>
<rawString>G. Salton, A. Wong, and C. S. Yang. 1975. A vector space model for automatic indexing. Commun. ACM, 18(11):613–620, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Snow</author>
<author>D Jurafsky</author>
<author>A Y Ng</author>
</authors>
<title>Learning syntactic patterns for automatic hypernym discovery.</title>
<date>2004</date>
<booktitle>Advances in Neural Information Processing Systems 17.</booktitle>
<contexts>
<context position="2334" citStr="Snow et al., 2004" startWordPosition="345" endWordPosition="348">strong efforts from domain experts, and it thus needs an automatization in such sense. 1http://www.wikipedia.org/ 2April 12, 2013. In this paper, we focus on the extraction of hypernym relations. The first step of such task relies on the identification of what (Navigli and Velardi, 2010) called definitional sentences, i.e., sentences that contain at least one hypernym relation. This subtask is important by itself for many tasks like Question Answering (Cui et al., 2007), construction of glossaries (Klavans and Muresan, 2001), extraction of taxonomic and non-taxonomic relations (Navigli, 2009; Snow et al., 2004), enrichment of concepts (Gangemi et al., 2003; Cataldi et al., 2009), and so forth. Hypernym relation extraction involves two aspects: linguistic knowlege, and model learning. Patterns collapse both of them, preventing to face them separately with the most suitable techniques. First, patterns have limited expressivity; then, linguistic knowledge inside patterns is learned from small corpora, so it is likely to have low coverage. Classification strictly depends on the learned patterns, so performance decreases, and the available classification techniques are restricted to those compatible with</context>
</contexts>
<marker>Snow, Jurafsky, Ng, 2004</marker>
<rawString>R. Snow, D. Jurafsky, and A.Y. Ng. 2004. Learning syntactic patterns for automatic hypernym discovery. Advances in Neural Information Processing Systems 17.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paola Velardi</author>
<author>Stefano Faralli</author>
<author>Roberto Navigli</author>
</authors>
<title>Ontolearn reloaded: A graph-based algorithm for taxonomy induction. Computational Linguistics,</title>
<date>2012</date>
<pages>1--72</pages>
<contexts>
<context position="18588" citStr="Velardi et al., 2012" startWordPosition="3015" endWordPosition="3018">n this method highlight the validity of the approach by significantly improving current state-of-the-art techniques in the classification of definitional sentences as well as in the extraction of hypernym relations from text. In future works, we aim at using larger syntactic contexts. In fact, currently, the detection does not surpass the sentence level, while taxonomical information can be even contained in different sentences or paragraphs. We also aim at evaluating our approach on the construction of entire taxonomies starting from domain-specific text corpora, as in (Navigli et al., 2011; Velardi et al., 2012). Finally, the desired result of the task of extracting hypernym relations from text (as for any semantic relationships in general) depends on the domain and the specific later application. Thus, we think that a precise evaluation and comparison of any systems strictly depends on these factors. For instance, given a sentence like “In mathematics, computing, linguistics and related disciplines, an algorithm is a sequence of instructions” one could want to extract only “instructions” as hypernym (as done in the annotation), rather than the entire chunk “sequence of instructions” (as extracted by</context>
</contexts>
<marker>Velardi, Faralli, Navigli, 2012</marker>
<rawString>Paola Velardi, Stefano Faralli, and Roberto Navigli. 2012. Ontolearn reloaded: A graph-based algorithm for taxonomy induction. Computational Linguistics, pages 1–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eline Westerhout</author>
</authors>
<title>Definition extraction using linguistic and structural features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 1st Workshop on Definition Extraction, WDE ’09,</booktitle>
<pages>61--67</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4234" citStr="Westerhout, 2009" startWordPosition="637" endWordPosition="638">nitional sentence referring to the more general meaning given by (Navigli and Velardi, 2010): A sentence that provides a for532 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 532–537, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics mal explanation for the term of interest, and more specifically as a sentence containing at least one hypernym relation. So far, most of the proposed techniques rely on lexico-syntactic patterns, either manually or semiautomatically produced (Hovy et al., 2003; Zhang and Jiang, 2009; Westerhout, 2009). Such patterns are sequences of words like “is a” or “refers to”, rather than more complex sequences including part-of-speech tags. In the work of (Westerhout, 2009), after a manual identification of types of definitions and related patterns contained in a corpus, he successively applied Machine Learning techniques on syntactic and location features to improve the results. A fully-automatic approach has been proposed by (Borg et al., 2009), where the authors applied genetic algorithms to the extraction of English definitions containing the keyword “is”. In detail, they assign weights to a set</context>
</contexts>
<marker>Westerhout, 2009</marker>
<rawString>Eline Westerhout. 2009. Definition extraction using linguistic and structural features. In Proceedings of the 1st Workshop on Definition Extraction, WDE ’09, pages 61–67, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Yamada</author>
<author>K Torisawa</author>
<author>J Kazama</author>
<author>K Kuroda</author>
<author>M Murata</author>
<author>S De Saeger</author>
<author>F Bond</author>
<author>A Sumida</author>
</authors>
<title>Hypernym discovery based on distributional similarity and hierarchical structures.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>929--937</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Yamada, Torisawa, Kazama, Kuroda, Murata, De Saeger, Bond, Sumida, 2009</marker>
<rawString>I. Yamada, K. Torisawa, J. Kazama, K. Kuroda, M. Murata, S. De Saeger, F. Bond, and A. Sumida. 2009. Hypernym discovery based on distributional similarity and hierarchical structures. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 929–937. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yang</author>
<author>J Callan</author>
</authors>
<title>Ontology generation for large email collections.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 international conference on Digital government research,</booktitle>
<pages>254--261</pages>
<publisher>North America.</publisher>
<contexts>
<context position="7221" citStr="Yang and Callan, 2008" startWordPosition="1103" endWordPosition="1106">trube, 2007) proposed a technique to extract hypernym relations from Wikipedia by means of methods based on the connectivity of the network and classical lexicosyntactic patterns. (Yamada et al., 2009) extended their work by combining extracted Wikipedia entries with new terms contained in additional web documents, using a distributional similarity-based approach. Finally, pure statistical approaches present techniques for the extraction of hierarchies of terms based on words frequency as well as cooccurrence values, relying on clustering procedures (Candan et al., 2008; Fortuna et al., 2006; Yang and Callan, 2008). The central hypothesis is that similar words tend to occur together in similar contexts (Harris, 1954). Despite this, they are defined by (Biemann, 2005) as prototype-based ontologies rather than formal terminological ontologies, and they usually suffer from the problem of data sparsity in case of small corpora. 3 Approach In this section we present our approach to identify hypernym relations within plain text. Our methodology consists in relaxing the problem into two easier subtasks. Given a relation rel(x, y) contained in a sentence, the task becomes to find 1) a possible x, and 2) a possi</context>
</contexts>
<marker>Yang, Callan, 2008</marker>
<rawString>H. Yang and J. Callan. 2008. Ontology generation for large email collections. In Proceedings of the 2008 international conference on Digital government research, pages 254–261. Digital Government Society of North America.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunxia Zhang</author>
<author>Peng Jiang</author>
</authors>
<title>Automatic extraction of definitions.</title>
<date>2009</date>
<booktitle>In Computer Science and Information Technology,</booktitle>
<pages>364--368</pages>
<contexts>
<context position="4215" citStr="Zhang and Jiang, 2009" startWordPosition="633" endWordPosition="636"> will use the term definitional sentence referring to the more general meaning given by (Navigli and Velardi, 2010): A sentence that provides a for532 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 532–537, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics mal explanation for the term of interest, and more specifically as a sentence containing at least one hypernym relation. So far, most of the proposed techniques rely on lexico-syntactic patterns, either manually or semiautomatically produced (Hovy et al., 2003; Zhang and Jiang, 2009; Westerhout, 2009). Such patterns are sequences of words like “is a” or “refers to”, rather than more complex sequences including part-of-speech tags. In the work of (Westerhout, 2009), after a manual identification of types of definitions and related patterns contained in a corpus, he successively applied Machine Learning techniques on syntactic and location features to improve the results. A fully-automatic approach has been proposed by (Borg et al., 2009), where the authors applied genetic algorithms to the extraction of English definitions containing the keyword “is”. In detail, they assi</context>
</contexts>
<marker>Zhang, Jiang, 2009</marker>
<rawString>Chunxia Zhang and Peng Jiang. 2009. Automatic extraction of definitions. In Computer Science and Information Technology, 2009. ICCSIT 2009. 2nd IEEE International Conference on, pages 364 –368, aug.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>