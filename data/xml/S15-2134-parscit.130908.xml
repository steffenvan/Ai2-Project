<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.986309">
SemEval-2015 Task 5: QA TEMPEVAL - Evaluating Temporal Information
Understanding with Question Answering
</title>
<author confidence="0.9618205">
Hector Llorens♣, Nathanael Chambers♦, Naushad UzZaman♣,
Nasrin Mostafazadeh✶, James Allen✶, James Pustejovsky♠♣ Nuance Communications, USA
</author>
<affiliation confidence="0.982769666666667">
♦ United States Naval Academy, USA
✶ University of Rochester, USA
♠ Brandeis University, USA
</affiliation>
<email confidence="0.996182">
hector.llorens@nuance.com, nchamber@usna.edu
</email>
<sectionHeader confidence="0.995617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999510357142857">
QA TempEval shifts the goal of previous
TempEvals away from an intrinsic evaluation
methodology toward a more extrinsic goal of
question answering. This evaluation requires
systems to capture temporal information rele-
vant to perform an end-user task, as opposed
to corpus-based evaluation where all temporal
information is equally important. Evaluation
results show that the best automated TimeML
annotations reach over 30% recall on ques-
tions with ‘yes’ answer and about 50% on eas-
ier questions with ‘no’ answers. Features that
helped achieve better results are event corefer-
ence and a time expression reasoner.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998786830188679">
QA TempEval is a follow up of the TempEval series
in SemEval: TempEval-1 (Verhagen et al., 2007),
TempEval-2 (Verhagen et al., 2010), and TempEval-
3 (UzZaman et al., 2013). TempEval focuses on
evaluating systems that extract temporal expressions
(timexes), events, and temporal relations as defined
in the TimeML standard (Pustejovsky et al., 2003)
(timeml.org). QA TempEval is unique in its focus
on evaluating temporal information that directly ad-
dress a QA task. TimeML was originally developed
to support research in complex temporal QA within
the field of artificial intelligence (AI). However, de-
spite its original goal, the complexity of temporal
QA has caused most research on automatic TimeML
systems to focus on a more straightforward temporal
information extraction (IE) task. QA TempEval still
requires systems to extract temporal relations just
like previous TempEvals, however, the QA evalua-
tion is solely based on how well the relations answer
questions about the documents. It is no longer about
annotation accuracy, but rather the accuracy for tar-
geted questions.
Not only does QA represent a more natural way
to evaluate temporal information understanding (Uz-
Zaman et al., 2012), but also annotating docu-
ments with question sets requires much less exper-
tise and effort for humans than corpus-based evalua-
tion which requires full manual annotation of tempo-
ral information. In QA TempEval a document does
not require the markup of all the temporal entities
and relations, but rather a markup of a few key re-
lations central to the text. Although the evaluation
schema changes in QA TempEval, the task for par-
ticipating systems remains the same: extracting tem-
poral information from plain text documents.
Here we re-use TempEval-3 task ABC, where sys-
tems are required to perform end-to-end TimeML
annotation from plain text, including the complete
set of temporal relations (Allen, 1983). However,
unlike TempEval-3, there are no subtasks focusing
on specific elements (such as an event extraction
evaluation). Also, instead of IE performance mea-
surement for evaluation, a QA performance (on a set
of human-created temporal questions on documents)
is used to rank systems. The participating systems
are supposed to annotate temporal entities relations
across the document, and the relations are used to
build a larger knowledge base of temporal links to
obtain answers to the temporal questions.
In QA TempEval, annotators are not required to
tag and order all events, but instead ask questions
about temporal relations that are relevant or inter-
esting to the document, hence this evaluation bet-
</bodyText>
<page confidence="0.947037">
792
</page>
<note confidence="0.953329">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 792–800,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999896545454546">
ter captures the understanding of the most important
temporal information in a document. Annotators are
not limited to relations between entities appearing
in the same or consecutive sentences, i.e., they can
ask any question that comes naturally to a reader’s
mind, e.g., “did the election happen (e3) before the
president gave (e27) the speech”. Finally, QA Tem-
pEval is unique in expanding beyond the news genre
and including Wikipedia articles and blog posts. In
the upcoming sections we will discuss details of the
conducted task and evaluation methodology.
</bodyText>
<sectionHeader confidence="0.988979" genericHeader="introduction">
2 Task Description
</sectionHeader>
<bodyText confidence="0.998696">
The task for participant systems is equivalent to
TempEval-3 task ABC, see Figure 1. Systems must
annotate temporal expressions, events, and temporal
relations between them1. The input to participants
is a set of unannotated text documents in TempEval-
</bodyText>
<listItem confidence="0.970710217391305">
3 format. Participating systems are required to an-
notate the plain documents following the TimeML
scheme, divided into two types of elements:
• Temporal entities: These include events
(EVENT tag, “came”, “attack”) and temporal
expressions (timexes, TIMEX3 tag, e.g., “yes-
terday”, “8 p.m.”) as well as their attributes like
event class, timex type, and normalized values.
• Temporal relations: A temporal relation
(tlink, TLINK tag) describes a pair of entities
and the temporal relation between them. The
TimeML relations map to the 13 Allen interval
relations. The included relations are: SIMUL-
TANEOUS (and IDENTITY), BEFORE, AFTER,
IBEFORE, IAFTER, IS INCLUDED, INCLUDES
(and DURING), BEGINS, BEGUN BY, ENDS,
and ENDED BY. Since the TimeML DURING
does not have a clear mapping, we map it to
SIMULTANEOUS for simplicity. The following
illustrates how the expression “6:00 pm” BE-
GINS the state of being “in the gym”.
(1) John was in the gym between 6:00 p.m
and 7:00 p.m.
</listItem>
<bodyText confidence="0.7624525">
Each system’s annotations represent its temporal
knowledge of the documents. These annotations are
</bodyText>
<footnote confidence="0.873179">
1http://alt.qcri.org/semeval2015/task5
</footnote>
<figureCaption confidence="0.9087082">
Figure 1: Task - Equivalent to TempEval-3 task ABC
then used as input to a temporal QA system (Uz-
Zaman et al., 2012) that will answer questions on
behalf of the systems, and the accuracy of their an-
swers is compared across systems.
</figureCaption>
<sectionHeader confidence="0.992081" genericHeader="method">
3 QA Evaluation Methodology
</sectionHeader>
<bodyText confidence="0.976718">
The main difference between QA TempEval and
earlier TempEval editions is that the systems are
not scored regarding how similar their annotation
to a human annotated key is, but how useful is
their TimeML annotation to answer human anno-
tated temporal questions. There are different kinds
of temporal questions that could be answered given
a TimeML annotation of a document. However, this
first QA TempEval focuses on yes/no questions in
the following format:
IS &lt;entityA&gt; &lt;RELATION&gt; &lt;entityB&gt; ?
(e.g., is event-A before event-B ?)
This makes it easier for human annotators to
create accurate question sets with their answers.
Other types of questions such as list-based make
it more difficult and arguable in edge cases (e.g.,
list events between event-A and event-B). Questions
about events not included in the document are not
possible, but theoretically one could ask about any
time reference. Due to the difficulty of mapping ex-
ternal time references to a specific time expression
in the document, these types of questions are not in-
cluded in the evaluation.
The questions can involve any of the thirteen rela-
tions described above. Two relations not in the set of
thirteen, OVERLAPS and OVERLAPPED BY, cannot
be explicitly annotated in TimeML, but they could
happen implicitly (i.e., be inferred from other rela-
tions) if needed by an application.
The evaluation process is illustrated in Figure 2.
After the testing period, the participants send their
TimeML annotations of the test documents. Orga-
nizers evaluate the TimeML annotations of all the
participating systems with a set of questions. The
</bodyText>
<page confidence="0.999143">
793
</page>
<figureCaption confidence="0.998655">
Figure 2: QA based on participant annotations
</figureCaption>
<bodyText confidence="0.993630666666667">
systems are scored comparing the expected answers
provided by human annotators against the predicted
answers obtained from the system’s TimeML anno-
tations.
Given a system’s TimeML annotated documents,
the process consists of three main steps:
</bodyText>
<listItem confidence="0.604708666666667">
• ID Normalization: Entities are referenced by
TimeML tag ids (e.g., eid23). The yes/no ques-
tions must contain two entities with IDs (e.g.,
</listItem>
<bodyText confidence="0.93536875">
“is event[eid23] after event[eid99] ?”). The en-
tities of the question are annotated in the cor-
responding key document. However, systems
may provide different ids to the same entities.
Therefore, we align the system annotation IDs
with the question IDs that are annotated in the
key docs using the TempEval-3 normalization
tool2.
</bodyText>
<listItem confidence="0.646341">
• Timegraph Generation: The normalized
TimeML docs are used to build a graph of
time points representing the temporal relations
of the events and timexes identified by each
</listItem>
<bodyText confidence="0.982968333333333">
system. Here we use Timegraph (Gerevini
et al., 1993) for computing temporal closure
as proposed by Miller and Schubert (1990).
The Timegraph is first initialized by adding the
TimeML explicit relations. Then the Time-
graph’s reasoning mechanism infers implicit
relations through rules such as transitivity.
For example, if eventA BEFORE eventB and
eventB BEFORE eventC, then the implicit re-
lation eventA BEFORE eventC can be inferred.
Timegraph expands a system’s TimeML anno-
tations and can answer both explicit and im-
</bodyText>
<footnote confidence="0.769718">
2https://github.com/hllorens/timeml-normalizer
</footnote>
<listItem confidence="0.76968125">
plicit Allen temporal relation questions, includ-
ing OVERLAPS.
• Question Processing: Answering questions re-
quires temporal information understanding and
reasoning. Note that asking ‘IS &lt;entity1&gt;
&lt;relation&gt; &lt;entity2&gt;?’ is not only asking if
there is that explicit tlink between them, but
also, if it is not, if that relation can be in-
ferred from other tlinks implicitly. Unlike cor-
pus based evaluation, the system gets credit if
its annotations provide the correct answer re-
gardless of whether it annotates other irrele-
</listItem>
<bodyText confidence="0.98833325">
vant information or not. In order to answer
the questions about TimeML entities (based on
time intervals) using Timegraph, we convert the
queries to point-based queries. For answering
yes/no questions, we check the necessary point
relations in Timegraph to verify an interval re-
lation. For example, to answer the question
“is event1 AFTER event2”, our system verifies
whether start(event1) &gt; end(event2); if it is ver-
ified then the answer is true (YES), if it con-
flicts with the Timegraph then it is false (NO),
otherwise it is UNKNOWN.
</bodyText>
<sectionHeader confidence="0.989602" genericHeader="method">
4 QA Scoring
</sectionHeader>
<bodyText confidence="0.9998306">
For each question we compare the obtained answer
from the Timegraph (created with system annota-
tions) and the expected answer (human annotated).
The scoring is based on the following Algorithm 1.
With this we calculate the following measures:
</bodyText>
<listItem confidence="0.994008">
• Precision (P) = num correct
num answered
• Recall (R) = num correct
num questions
</listItem>
<page confidence="0.931726">
794
</page>
<equation confidence="0.5688294">
num questions=0
num answered=0
num correct=0
foreach question q ∈ questionset do
num questions += 1
</equation>
<construct confidence="0.779408166666667">
if predicted ans[q] != unknown
or key ans[q] == unknown then
num answered += 1
if predicted ans[q] == key ans[q] then
num correct += 1
Algorithm 1: QA Scoring
</construct>
<listItem confidence="0.9248435">
• F-measure (F1) —_ 2∗P∗R
P+R
</listItem>
<bodyText confidence="0.9649395">
We use Recall (QA accuracy) as the main metric and
F1 is used in case of draw.
</bodyText>
<sectionHeader confidence="0.998014" genericHeader="method">
5 Datasets
</sectionHeader>
<bodyText confidence="0.998092384615385">
In QA TempEval, the creation of datasets does not
require the manual annotation of all TimeML ele-
ments in source docs. The annotation task in QA
TempEval only requires reading the doc, making
temporal questions, providing the correct answers,
and identifying entities included in the questions by
bounding them in the text and designating an ID.
The format of the question sets is as follows:
&lt;question-num&gt;|&lt;source-doc&gt;|
&lt;question-with-ids&gt;|&lt;NL-question&gt;|
&lt;answer&gt;|[opt-extra-info]
Following is an example question and its corre-
sponding annotated document:
</bodyText>
<equation confidence="0.493585">
3|APW.tml|IS ei21 AFTER ei19|
</equation>
<bodyText confidence="0.594614">
Was he cited after becoming general?|yes
</bodyText>
<construct confidence="0.8218343">
APW.tml (KEY)
Farkas &lt;event eid=&amp;quot;e19&amp;quot;&gt;became&lt;/event&gt;
a general. He was
&lt;event eid=&amp;quot;e21&amp;quot;&gt;cited&lt;/event&gt;...
APW.tml (system annotation, full-TimeML)
Farkas &lt;event eid=&amp;quot;e15&amp;quot;...&gt;became&lt;/event&gt;
a general. He was
&lt;event eid=&amp;quot;e24&amp;quot;...&gt;cited&lt;/event&gt;...
&lt;tlink eventID=e15 relatedToEventID=e24
relType=before /&gt;
</construct>
<subsectionHeader confidence="0.998521">
5.1 Training Data
</subsectionHeader>
<bodyText confidence="0.9996854">
TimeML training data consists of TempEval-3 an-
notated data: TimeBank, AQUAINT (TBAQ,
TempEval-3 training), and TE-3 Platinum
(TempEval-3 testing). Furthermore, a question-set
in the format explained earlier is provided to the
participants for training purposes. It consists of
79 Yes/No questions and answers about TimeBank
documents (UzZaman et al., 2012). Participants
can easily extend the question-set by designing new
questions over TimeML corpora.
</bodyText>
<subsectionHeader confidence="0.999662">
5.2 Test Data
</subsectionHeader>
<bodyText confidence="0.999477">
The test dataset comprises three domains:
</bodyText>
<listItem confidence="0.948989">
• News articles (Wikinews, WSJ, NYT): This
covers the traditional TempEval domain used
in all the previous editions.
• Wikipedia3 articles (history, biographical):
This covers documents about people or history,
which are rich in temporal entities.
• Informal blog posts (narrative): We hand se-
lected blog entries from the Blog Authorship
Corpus (Schler et al., 2006). They are in nar-
rative nature, such as the ones describing per-
sonal events as opposed to entries with opinions
and political commentary.
</listItem>
<bodyText confidence="0.997430909090909">
For each of these domains, human experts se-
lect the documents, create the set of questions to-
gether with the correct answer, and annotate the cor-
responding entities of the questions in the key doc-
uments. The resulting question-set is then peer-
reviewed by the human experts. Table 1 depicts
statistics of the test dataset. In this table, the col-
umn dist- shows the number of questions about
entities that are in the same or consecutive sen-
tences while dist+ refers to questions about non-
consecutive (more distant) entities.
</bodyText>
<table confidence="0.9978058">
docs words quest yes no dist- dist+
news 10 6920 99 93 6 40 59
wiki 10 14842 130 117 13 58 72
blogs 8 2053 65 65 0 30 35
total 28 23815 294 275 19 128 166
</table>
<tableCaption confidence="0.99967">
Table 1: Test Data
</tableCaption>
<bodyText confidence="0.99976175">
Annotators were asked to create positive (yes)
questions unless a negative (no) question came nat-
urally. This is due to the fact that we can auto-
matically generate negative questions from positive
questions, but not the other way around. Note that
the number of questions about distant entities is con-
siderable. TimeML training data and thus systems
tend to only annotate temporal relations about less
</bodyText>
<footnote confidence="0.990857">
3http://en.wikipedia.org
</footnote>
<page confidence="0.996084">
795
</page>
<bodyText confidence="0.999752666666667">
distant entities. Therefore, to answer distant ques-
tions the necessary implicit relations must be obtain-
able from the annotated explicit relations.
</bodyText>
<subsectionHeader confidence="0.998634">
5.3 Development Time Cost
</subsectionHeader>
<bodyText confidence="0.999993055555556">
One of the claims of QA evaluation of tempo-
ral text understanding (UzZaman et al., 2012) is
that the time cost of creating question sets in QA
schema is lower than the one for fully annotating
a document with TimeML elements and attributes.
Both tasks involve reading the document. How-
ever, question-set creation only requires designing
yes/no questions paired with answers and annotating
the corresponding entities in the document, while
full TimeML annotation needs identifying all enti-
ties, their attributes, and large set of relations among
them. There is not any rigorous information avail-
able about the time it takes to perform these different
annotation tasks. Comparison is difficult since many
factors play a role in timing (e.g., human annotators
skills, dedicated software help). In order to provide
an approximate comparison, following we present
information regarding some real experiences:
</bodyText>
<listItem confidence="0.997797789473684">
• Question Set annotation (about 10 questions
per document, without dedicated software
help): QA TempEval consists of 28 docs
(23,815 words), i.e., about 850 words per doc-
ument. Human annotators reported that the an-
notation task from raw text took them 30min-
2h per document, i.e., 15min-1h for 360 words.
• TimeML all-elements and attributes annotation
(with dedicated software help): Annotators of
the Spanish TimeBank spent a year to com-
plete the annotation working 3h/day, approx-
imately 3h per document or 360 words. We
don’t have available to us similar data for the
English TimeBank’s creation.
• Other experiences regarding full TimeML an-
notation such as correcting a pre-annotated
document by a system took about 2-3h per
document. TLINK annotation reportedly took
about 1.5h per document.
</listItem>
<bodyText confidence="0.986963">
We do not aim to provide an exact quantification
or comparison; however, based on the information
we have available, creating a QA test set takes con-
siderably less time than full TimeML annotation.
TimeML annotated documents can also be used for
training and evaluating temporal extraction systems,
whereas TempQA annotated documents can be used
only for evaluation. Given that we have enough an-
notated data, TempQA helps to easily create more
data to evaluate temporal systems in new domains.
</bodyText>
<sectionHeader confidence="0.960636" genericHeader="method">
6 Participating Systems
</sectionHeader>
<bodyText confidence="0.86184775">
Nine approaches addressing automatic TimeML
annotation for English were presented in the
QA TempEval evaluation, divided into two groups:
Regular participants, optimized for task:
</bodyText>
<listItem confidence="0.985858071428571">
• HITSZ-ICRC4. rule-based timex module,
SVM (liblinear) for event and relation detection
and classification
• hlt-fbk-ev1-trel1. SVM, separated event de-
tection and classification, without event co-
reference
• hlt-fbk-ev1-trel2. SVM, separated event de-
tection and classification, with event coref
• hlt-fbk-ev2-trel1. SVM, all predicates are
events and classification decides, without event
co-reference
• hlt-fbk-ev2-trel2. SVM, all predicates are
events and classification decides, with event co-
reference
Off-the-Shelf Systems, not optimized on task:
• CAEVO5 (Chambers et al., 2014). Cascading
classifiers that add temporal links with transi-
tive expansion. A wide range of rule-based and
supervised classifiers are included
• ClearTK6 (Bethard, 2013) A pipeline of
machine-learning classification models, each
of which have simple morphosyntactic annota-
tion pipeline as feature set
• TIPSemB (Llorens et al., 2010) CRF-SVM
model with morphosyntactic features
• TIPSem (Llorens et al., 2010) TIPSemB + lex-
ical (WordNet) and combinational (PropBank
roles) semantic features
</listItem>
<footnote confidence="0.994215666666667">
4Annotations Submitted 1-day after the deadline
5Off-the-shelf system: the author was co-organizer
6Off-the-shelf system: trained and tested by organizers
</footnote>
<page confidence="0.997895">
796
</page>
<sectionHeader confidence="0.973262" genericHeader="method">
7 Time Expression Reasoner (TREFL)
</sectionHeader>
<bodyText confidence="0.996229959183674">
As an extra evaluation, task organizers added a
new run for each system augmented with a post-
processing step. The goal is to analyze how a gen-
eral time expression reasoner could improve results.
The TREFL component is straightforward: resolve
all time expressions, and add temporal relations be-
tween the time expressions when the relation is un-
ambiguous based on their resolved times.
We define a “timex reference” as a temporal ex-
pression consisting of a date or time (e.g., “Jan 12,
1999”, “tomorrow”) that is normalized to a Grego-
rian calendar interval (e.g., 1999-01-12, 2015-06-
06). These are perfectly suited for ordering in time.
In addition, finding timex references and obtain-
ing their normalized values are tasks in which au-
tomatic systems perform with over 90% accuracy.
Thus, given a system normalized-values, we can au-
tomatically produce timex-timex reference relations
or links (TREFL) that represent a temporal rela-
tion backbone (base Timegraph) with high accuracy.
This backbone can then assist the much more diffi-
cult event-event and event-timex links that are later
predicted by system classifiers. Any relations pre-
dicted by a classifier can be discarded if they are in-
consistent with this TREFL backbone.
For example, if a system TimeML annotation con-
tains three timexes t1 (1999), t2 (1998-01-15), and
t3 (1999-08), a minimal set of relations can be de-
terministically extracted as t2 BEFORE t1 and t3
IS INCLUDED t1. The corresponding Timegraph is:
t2 &lt; t1 start &lt; t3 start &lt; t3 end &lt; t1 end
To automatically obtain such minimal set of re-
lations from the system timex-values, the TREFL
component orders them by date and granularity us-
ing SIMULTANEOUS, BEFORE, BEGINS, IS INCLUDED,
or ENDS relations. More complicated cases have not
been included in this evaluation for simplicity.
The only drawback or risk of this strategy is that
some of the system timex-values could be incorrect,
but previous work suggests these errors are less nu-
merous than those occurring in later event-event re-
lation extraction. Our hypothesis is that (i) many
systems do not include a strategy like this, and (ii)
even taking into account the drawback of this strat-
egy most systems would benefit from using it, reach-
ing a higher performance. The evaluation compares
original systems with their TREFL-augmented vari-
ant that discarded system relations in conflict with
its TREFL Timegraph.
</bodyText>
<sectionHeader confidence="0.989086" genericHeader="evaluation">
8 Evaluation
</sectionHeader>
<bodyText confidence="0.99999125">
The objective of this evaluation is to measure and
compare QA performance of TimeML annotations
of participating and off-the-shelf systems. Partici-
pants were given the documents of the previously
defined test set (TE3-input format). They were
asked to annotate them with their systems within a 5-
day period. Organizers evaluated the submitted an-
notations using the test question-sets. Result tables
include Precision (P), Recall (R), F-measure (F1),
percentage of the answered questions (awd%) and
number of correct answers (corr). As mentioned ear-
lier, Recall is the main measure for ranking systems.
The percentage of the questions which are answered
by the system provides a coverage metric, measur-
ing a system’s ability to provide more complete set
of annotation on entities and relations.
</bodyText>
<subsectionHeader confidence="0.907842">
8.1 Results without TREFL
</subsectionHeader>
<bodyText confidence="0.896844">
Table 2 shows the combined results over all three
genres in the test set, comprising 294 test questions.
</bodyText>
<table confidence="0.996307090909091">
Measures Questions
System P R F1 awd% corr
HITSZ-ICRC .54 .06 .12 .12 19
hlt-fbk-ev1-trel1 .57 .17 .26 .30 50
hlt-fbk-ev1-trel2 .47 .23 .31 .50 69
hlt-fbk-ev2-trel1 .55 .17 .26 .32 51
hlt-fbk-ev2-trel2 .49 .30 .37 .62 89
ClearTK .59 .06 .11 .10 17
CAEVO .56 .17 .26 .31 51
TIPSemB .47 .13 .20 .28 38
TIPSem .60 .15 .24 .26 45
</table>
<tableCaption confidence="0.999051">
Table 2: QA Results over all domains.
</tableCaption>
<bodyText confidence="0.999007818181818">
The participant system hlt-fbk-ev2-trel2 system
(.30 R) outperformed all the others by a significant
margin. CAEVO performed best among the off-the-
shelf systems, but behind the winning participant re-
call by 13% absolute. The awd% of the hlt-fbk-ev2-
trel2 system doubles the one by the best off-the-shelf
system, CAEVO. Interstingly, CAEVO and the two
hlt-fbk trel1 systems performed approximately the
same. The trel2 versions included event coreference.
Table 3 shows three result tables from the three
genres: news, wiki, and blogs. The best overall sys-
</bodyText>
<page confidence="0.991366">
797
</page>
<table confidence="0.999927527777778">
News Genre Results
Measures Questions
System P R F1 awd% corr
HITSZ-ICRC .47 .08 .14 .17 8
hlt-fbk-ev1-trel1 .59 .17 .27 .29 17
hlt-fbk-ev1-trel2 .43 .23 .30 .55 23
hlt-fbk-ev2-trel1 .56 .20 .30 .36 20
hlt-fbk-ev2-trel2 .43 .29 .35 .69 29
ClearTK .60 .06 .11 .10 6
CAEVO .59 .17 .27 .29 17
TIPSemB .50 .16 .24 .32 16
TIPSem .52 .11 .18 .21 11
Wiki Genre Results
Measures Questions
System P R F1 awd% corr
HITSZ-ICRC .83 .08 .14 .09 10
hlt-fbk-ev1-trel1 .55 .16 .25 .29 21
hlt-fbk-ev1-trel2 .52 .26 .35 .50 34
hlt-fbk-ev2-trel1 .58 .17 .26 .29 22
hlt-fbk-ev2-trel2 .62 .36 .46 .58 47
ClearTK .60 .05 .09 .08 6
CAEVO .59 .17 .26 .28 22
TIPSemB .52 .13 .21 .25 17
TIPSem .74 .19 .30 .26 25
Blogs Genre Results
Measures Questions
System P R F1 awd% corr
HITSZ-ICRC .17 .02 .03 .09 1
hlt-fbk-ev1-trel1 .57 .18 .28 .32 12
hlt-fbk-ev1-trel2 .43 .18 .26 .43 12
hlt-fbk-ev2-trel1 .47 .14 .21 .29 9
hlt-fbk-ev2-trel2 .34 .20 .25 .58 13
ClearTK .56 .08 .14 .14 5
CAEVO .48 .18 .27 .38 12
TIPSemB .31 .08 .12 .25 5
TIPSem .45 .14 .21 .31 9
</table>
<tableCaption confidence="0.979971">
Table 3: QA Results broken down by genre, based on 99
News, 130 Wiki, and 65 Blog questions.
</tableCaption>
<bodyText confidence="0.9939333125">
tem, hlt-fbk-ev2-trel2, maintained its top position.
The main difference in genre results appears to be
the smaller blog corpus where the leading hlt-fbk-
ev2-trel2 participant and CAEVO performed simi-
larly, .20 and .18 R respectively. The hlt-fbk system
exhibited similar behavior as the other genres show-
ing a high coverage, as demonstrated by awd% met-
ric. However, it simply guessed incorrectly much
more often (precision dropped to the 30’s).
We make note that the ClearTK off-the-shelf sys-
tem’s lower performance is because it was used
without modification from its TempEval-3 submis-
sion. ClearTK was TempEval-3 best system, partly
due to its optimization to the task where it maxi-
mized precision and not recall. It likely would per-
form better if optimized to this new QA task.
</bodyText>
<subsectionHeader confidence="0.949993">
8.2 Results with TREFL
</subsectionHeader>
<bodyText confidence="0.810094">
Table 4 shows the results for systems augmented
with TREFL (explained in Section 7).
</bodyText>
<table confidence="0.995446818181818">
Measures Questions
P R F1 awd% corr
.58 .09 .15 .15 25
.62 .28 .38 .45 81
.55 .31 .40 .57 92
.61 .29 .39 .48 86
.51 .34 .40 .67 99
ClearTK (TREFL not applied because of its TLINK format)
CAEVO .60 .21 .32 .36 63
TIPSemB .64 .24 .35 .37 70
TIPSem .68 .27 .38 .40 79
</table>
<tableCaption confidence="0.999707">
Table 4: QA Results augmented with TREFL
</tableCaption>
<bodyText confidence="0.997335785714286">
Recall went up on all systems (by 49% relative
on average), but the degree of improvement var-
ied. Recall of the top system (hlt-fbk-ev2-trel2) im-
proved 4% absolute (13% relative). The largest gain
was with TIPSem which improved from .15 to .27,
becoming the top off-the-shelf system. TREFL is
mainly focused on improving recall which explains
the differences. The best system had higher recall
already, so TREFL had less contribution. TIPSem
had lower recall, so it sees the greatest gain. TREFL
did not penalize TIPSem precision as much as it did
for other systems. That made TIPSem obtain the top
F1 in wiki and blogs domains.
By genre, on average TREFL improved systems’
relative recall by 60% (news), 48% (wiki), and 47%
(news).
In news and wiki, hlt-fbk-ev2-trel2+terfl was the
system answering correctly more questions about
distant entities (22 news, 20 wiki), while for blogs
it was TIPSem (9).
We also found that hlt-fbk-ev2-trel2+terfl answers
more questions that no other system is capable of
answering (4 news, 11 wiki, 5 blogs), demonstrat-
ing that it has some features that others system lack.
One of the distinguishing features of this system, re-
quired to answer some of the testset questions, is
event co-reference (clustering) which could be re-
sponsible for this good result.
</bodyText>
<figure confidence="0.697190166666667">
System
HITSZ-ICRC
hlt-fbk-ev1-trel1
hlt-fbk-ev1-trel2
hlt-fbk-ev2-trel1
hlt-fbk-ev2-trel2
</figure>
<bodyText confidence="0.9998876">
Analyzing the questions answered correctly after
the TREFL augmentation, in both the news and wiki
domains, we found that around 35% of the ques-
tions were not answered by any system because they
didn’t find a temporal entity in the question (either
an event or time expression, or both). This is mostly
because no system found one of the entities in the
question. In the blog genre, 50% of errors were
due to missing entities, and blogs/news were 75%.
These missing entity errors exist in both the original
system submissions and this TREFL augmentation.
The remaining unanswered questions were simply
due to sparsity in relation annotation. The relation
needed to answer the question is neither annotated
nor do transitive inferences exist.
</bodyText>
<subsectionHeader confidence="0.839872">
8.3 Results with TREFL (no-questions)
</subsectionHeader>
<bodyText confidence="0.999976518518518">
As mentioned earlier the evaluation is mainly fo-
cused on positive questions (with yes answer) since
annotating them provides more information and neg-
ative questions can be automatically generated from
them. Moreover, in general, answering positive
questions is more challenging, e.g., asking IS e1 BE-
FORE e2 requires a system to guess the single correct
relation if the correct answer is yes; However, if the
correct answer is no, there are 12 possible correct
relations (all but BEFORE).
In order to have more insight into this issue, we
automatically obtained negative questions by ask-
ing about the opposite7 relation with “no” as the ex-
pected answer. For example, IS e1 BEFORE e2 (yes)
becomes IS e1 AFTER e2 (no). The aim of this evalu-
ation is to analyze system performance in determin-
ing if a relation is not correct. In this easier test,
participating and off-the-shelf systems obtain better
results going over .50 R in the news domain. The
best obtained recalls are .52 in news, .39 in wiki,
and .42 in blogs, as compared to .38, .36 and .22
obtained for yes-questions in the main test.
It is interesting to see that in this negative alterna-
tive, systems were better in blogs than in wiki, un-
like in the positive test. Likewise the positive vari-
ant, the addition of trefl has improved results, but the
improvements is smaller in this case.
</bodyText>
<note confidence="0.439685">
7SIMULTANEOUS has no opposite and IAFTER was used.
</note>
<sectionHeader confidence="0.881698" genericHeader="conclusions">
9 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9997608125">
QA evaluation task attempts to measure how far we
are on temporal information understanding applied
to temporal QA (an extrinsic task) instead of only
TimeML annotation accuracy. One of the benefits
of QA evaluation is that test set creation time and
human expertise required is considerably less than
in TimeML annotation. QA TempEval also included
Wikipedia and blog domains, in addition to the reg-
ular news domain, for the first time. Evaluation re-
sults suggest that we are still far from systems that
more deeply understand temporal aspects of natural
language and can answer temporal questions. The
best overall recall was 30% (34% with TREFL).
This top result is higher than best off-the-shelf sys-
tem 17% (27% with TREFL).
The main findings include:
</bodyText>
<listItem confidence="0.984346714285714">
• The only system using event co-reference ob-
tained the best results, so adding event coref
may help other systems.
• Adding TREFL improved the QA recall of all
systems, ranging from 3% to 12% absolute
(13% to 80% relative).
• Training data is news, but the best system
</listItem>
<bodyText confidence="0.934382375">
performed well on Wikipedia. Some off-
the-shelf systems even performed better on
Wikipedia/blogs than on the news domain.
• Human annotators annotated as many ques-
tions about close entities as distant entities. In
the same line, automated systems were capable
of answering correctly approximately the same
amount of questions of each type.
As future work we aim to extend the analysis
of the results presented in this paper. On the one
hand, by explaining TREFL technique and its ef-
fects in more detail. On the other hand, by finding
out what features made some systems unique being
the only ones capable of answering certain questions
correctly. The question-sets8, tools and results9 have
been released for future research.
</bodyText>
<sectionHeader confidence="0.998865" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9785825">
We want to thank participant Paramita Mirza for her collabo-
ration on reviewing and correcting the test data, and also Marc
Verhagen and Roser Sauri for their help on approximating the
time-cost of human TimeML annotation.
</bodyText>
<footnote confidence="0.9995245">
8http://bitbucket.org/hector_llorens/qa-tempeval-test-data
9http://alt.qcri.org/semeval2015/task5/
</footnote>
<page confidence="0.998033">
799
</page>
<sectionHeader confidence="0.995681" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923890909091">
James F. Allen. 1983. Maintaining knowledge
about temporal intervals. Communications of ACM,
26(11):832–843.
Steven Bethard. 2013. Cleartk-timeml: A minimalist ap-
proach to tempeval 2013. In Second Joint Conference
on Lexical and Computational Semantics (*SEM), Vol-
ume 2: Proceedings of the Seventh International Work-
shop on Semantic Evaluation (SemEval 2013), pages
10–14.
Nathanael Chambers, Taylor Cassidy, Bill McDowell,
and Steven Bethard. 2014. Dense event ordering with
a multi-pass architecture. Transactions of the Associ-
ation for Computational Linguistics, 2(10):273–284.
Alfonso Gerevini, Lenhart Schubert, and Stephanie Scha-
effer. 1993. Temporal reasoning in Timegraph I–II.
SIGARTBulletin, 4(3):21–25.
Hector Llorens, Estela Saquete, and Borja Navarro.
2010. TIPSem (English and Spanish): Evaluating
CRFs and Semantic Roles in TempEval-2. In Proceed-
ings of SemEval-5, pages 284–291.
Stephanie Miller and Lenhart Schubert. 1990. Time
revisited. In Computational Intelligence, volume 26,
pages 108–118.
James Pustejovsky, Jos´e M. Casta˜no, Robert Ingria,
Roser Saur´ı, Robert Gaizauskas, Andrea Setzer, and
Graham Katz. 2003. TimeML: Robust Specification
of Event and Timexes in Text. In IWCS-5.
Jonathan Schler, Moshe Koppel, Shlomo Argamon, and
James W. Pennebaker. 2006. Effects of age and gen-
der on blogging. In AAAI Spring Symposium: Compu-
tational Approaches to Analyzing Weblogs, pages 199–
205.
Naushad UzZaman, Hector Llorens, and James Allen.
2012. Evaluating temporal information understanding
with temporal question answering. In Proceedings of
IEEE International Conference on Semantic Comput-
ing.
Naushad UzZaman, Hector Llorens, Leon Derczynski,
James Allen, Marc Verhagen, and James Pustejovsky.
2013. Semeval-2013 task 1: Tempeval-3: Evaluating
time expressions, events, and temporal relations. In
2nd Joint Conference on Lexical and Computational
Semantics (*SEM), Vol 2: Proceedings of the Seventh
International Workshop on Semantic Evaluation (Se-
mEval 2013), pages 1–9, Atlanta, Georgia, USA, June.
Marc Verhagen, Robert Gaizauskas, Mark Hepple, Frank
Schilder, Graham Katz, and James Pustejovsky. 2007.
Semeval-2007 task 15: Tempeval temporal relation
identification. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, pages 75–
80, Prague.
Marc Verhagen, Roser Saur´ı, Tommaso Caselli, and
James Pustejovsky. 2010. Semeval-2010 task 13:
Tempeval-2. In Proceedings of SemEval-5, pages 57–
62.
</reference>
<page confidence="0.996943">
800
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.302391">
<title confidence="0.9615755">Task 5: QA Evaluating Temporal Understanding with Question Answering</title>
<author confidence="0.927116">Nathanael Naushad</author>
<affiliation confidence="0.7612355">James Communications, USA ♦ United States Naval Academy, of Rochester, University, USA</affiliation>
<email confidence="0.999766">hector.llorens@nuance.com,nchamber@usna.edu</email>
<abstract confidence="0.997478733333333">QA TempEval shifts the goal of previous TempEvals away from an intrinsic evaluation methodology toward a more extrinsic goal of question answering. This evaluation requires systems to capture temporal information relevant to perform an end-user task, as opposed to corpus-based evaluation where all temporal information is equally important. Evaluation results show that the best automated TimeML annotations reach over 30% recall on questions with ‘yes’ answer and about 50% on easier questions with ‘no’ answers. Features that helped achieve better results are event coreference and a time expression reasoner.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James F Allen</author>
</authors>
<title>Maintaining knowledge about temporal intervals.</title>
<date>1983</date>
<journal>Communications of ACM,</journal>
<volume>26</volume>
<issue>11</issue>
<contexts>
<context position="2919" citStr="Allen, 1983" startWordPosition="438" endWordPosition="439">r humans than corpus-based evaluation which requires full manual annotation of temporal information. In QA TempEval a document does not require the markup of all the temporal entities and relations, but rather a markup of a few key relations central to the text. Although the evaluation schema changes in QA TempEval, the task for participating systems remains the same: extracting temporal information from plain text documents. Here we re-use TempEval-3 task ABC, where systems are required to perform end-to-end TimeML annotation from plain text, including the complete set of temporal relations (Allen, 1983). However, unlike TempEval-3, there are no subtasks focusing on specific elements (such as an event extraction evaluation). Also, instead of IE performance measurement for evaluation, a QA performance (on a set of human-created temporal questions on documents) is used to rank systems. The participating systems are supposed to annotate temporal entities relations across the document, and the relations are used to build a larger knowledge base of temporal links to obtain answers to the temporal questions. In QA TempEval, annotators are not required to tag and order all events, but instead ask qu</context>
</contexts>
<marker>Allen, 1983</marker>
<rawString>James F. Allen. 1983. Maintaining knowledge about temporal intervals. Communications of ACM, 26(11):832–843.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bethard</author>
</authors>
<title>Cleartk-timeml: A minimalist approach to tempeval 2013.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>10--14</pages>
<contexts>
<context position="17310" citStr="Bethard, 2013" startWordPosition="2668" endWordPosition="2669">arated event detection and classification, without event coreference • hlt-fbk-ev1-trel2. SVM, separated event detection and classification, with event coref • hlt-fbk-ev2-trel1. SVM, all predicates are events and classification decides, without event co-reference • hlt-fbk-ev2-trel2. SVM, all predicates are events and classification decides, with event coreference Off-the-Shelf Systems, not optimized on task: • CAEVO5 (Chambers et al., 2014). Cascading classifiers that add temporal links with transitive expansion. A wide range of rule-based and supervised classifiers are included • ClearTK6 (Bethard, 2013) A pipeline of machine-learning classification models, each of which have simple morphosyntactic annotation pipeline as feature set • TIPSemB (Llorens et al., 2010) CRF-SVM model with morphosyntactic features • TIPSem (Llorens et al., 2010) TIPSemB + lexical (WordNet) and combinational (PropBank roles) semantic features 4Annotations Submitted 1-day after the deadline 5Off-the-shelf system: the author was co-organizer 6Off-the-shelf system: trained and tested by organizers 796 7 Time Expression Reasoner (TREFL) As an extra evaluation, task organizers added a new run for each system augmented wi</context>
</contexts>
<marker>Bethard, 2013</marker>
<rawString>Steven Bethard. 2013. Cleartk-timeml: A minimalist approach to tempeval 2013. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 10–14.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Taylor Cassidy</author>
<author>Bill McDowell</author>
<author>Steven Bethard</author>
</authors>
<title>Dense event ordering with a multi-pass architecture.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<volume>2</volume>
<issue>10</issue>
<contexts>
<context position="17142" citStr="Chambers et al., 2014" startWordPosition="2642" endWordPosition="2645">ular participants, optimized for task: • HITSZ-ICRC4. rule-based timex module, SVM (liblinear) for event and relation detection and classification • hlt-fbk-ev1-trel1. SVM, separated event detection and classification, without event coreference • hlt-fbk-ev1-trel2. SVM, separated event detection and classification, with event coref • hlt-fbk-ev2-trel1. SVM, all predicates are events and classification decides, without event co-reference • hlt-fbk-ev2-trel2. SVM, all predicates are events and classification decides, with event coreference Off-the-Shelf Systems, not optimized on task: • CAEVO5 (Chambers et al., 2014). Cascading classifiers that add temporal links with transitive expansion. A wide range of rule-based and supervised classifiers are included • ClearTK6 (Bethard, 2013) A pipeline of machine-learning classification models, each of which have simple morphosyntactic annotation pipeline as feature set • TIPSemB (Llorens et al., 2010) CRF-SVM model with morphosyntactic features • TIPSem (Llorens et al., 2010) TIPSemB + lexical (WordNet) and combinational (PropBank roles) semantic features 4Annotations Submitted 1-day after the deadline 5Off-the-shelf system: the author was co-organizer 6Off-the-sh</context>
</contexts>
<marker>Chambers, Cassidy, McDowell, Bethard, 2014</marker>
<rawString>Nathanael Chambers, Taylor Cassidy, Bill McDowell, and Steven Bethard. 2014. Dense event ordering with a multi-pass architecture. Transactions of the Association for Computational Linguistics, 2(10):273–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfonso Gerevini</author>
<author>Lenhart Schubert</author>
<author>Stephanie Schaeffer</author>
</authors>
<date>1993</date>
<booktitle>Temporal reasoning in Timegraph I–II. SIGARTBulletin,</booktitle>
<pages>4--3</pages>
<contexts>
<context position="8606" citStr="Gerevini et al., 1993" startWordPosition="1328" endWordPosition="1331">3). The yes/no questions must contain two entities with IDs (e.g., “is event[eid23] after event[eid99] ?”). The entities of the question are annotated in the corresponding key document. However, systems may provide different ids to the same entities. Therefore, we align the system annotation IDs with the question IDs that are annotated in the key docs using the TempEval-3 normalization tool2. • Timegraph Generation: The normalized TimeML docs are used to build a graph of time points representing the temporal relations of the events and timexes identified by each system. Here we use Timegraph (Gerevini et al., 1993) for computing temporal closure as proposed by Miller and Schubert (1990). The Timegraph is first initialized by adding the TimeML explicit relations. Then the Timegraph’s reasoning mechanism infers implicit relations through rules such as transitivity. For example, if eventA BEFORE eventB and eventB BEFORE eventC, then the implicit relation eventA BEFORE eventC can be inferred. Timegraph expands a system’s TimeML annotations and can answer both explicit and im2https://github.com/hllorens/timeml-normalizer plicit Allen temporal relation questions, including OVERLAPS. • Question Processing: Ans</context>
</contexts>
<marker>Gerevini, Schubert, Schaeffer, 1993</marker>
<rawString>Alfonso Gerevini, Lenhart Schubert, and Stephanie Schaeffer. 1993. Temporal reasoning in Timegraph I–II. SIGARTBulletin, 4(3):21–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hector Llorens</author>
<author>Estela Saquete</author>
<author>Borja Navarro</author>
</authors>
<title>TIPSem (English and Spanish): Evaluating CRFs and Semantic Roles in TempEval-2.</title>
<date>2010</date>
<booktitle>In Proceedings of SemEval-5,</booktitle>
<pages>284--291</pages>
<contexts>
<context position="17474" citStr="Llorens et al., 2010" startWordPosition="2690" endWordPosition="2693"> • hlt-fbk-ev2-trel1. SVM, all predicates are events and classification decides, without event co-reference • hlt-fbk-ev2-trel2. SVM, all predicates are events and classification decides, with event coreference Off-the-Shelf Systems, not optimized on task: • CAEVO5 (Chambers et al., 2014). Cascading classifiers that add temporal links with transitive expansion. A wide range of rule-based and supervised classifiers are included • ClearTK6 (Bethard, 2013) A pipeline of machine-learning classification models, each of which have simple morphosyntactic annotation pipeline as feature set • TIPSemB (Llorens et al., 2010) CRF-SVM model with morphosyntactic features • TIPSem (Llorens et al., 2010) TIPSemB + lexical (WordNet) and combinational (PropBank roles) semantic features 4Annotations Submitted 1-day after the deadline 5Off-the-shelf system: the author was co-organizer 6Off-the-shelf system: trained and tested by organizers 796 7 Time Expression Reasoner (TREFL) As an extra evaluation, task organizers added a new run for each system augmented with a postprocessing step. The goal is to analyze how a general time expression reasoner could improve results. The TREFL component is straightforward: resolve all t</context>
</contexts>
<marker>Llorens, Saquete, Navarro, 2010</marker>
<rawString>Hector Llorens, Estela Saquete, and Borja Navarro. 2010. TIPSem (English and Spanish): Evaluating CRFs and Semantic Roles in TempEval-2. In Proceedings of SemEval-5, pages 284–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephanie Miller</author>
<author>Lenhart Schubert</author>
</authors>
<title>Time revisited.</title>
<date>1990</date>
<journal>In Computational Intelligence,</journal>
<volume>26</volume>
<pages>108--118</pages>
<contexts>
<context position="8679" citStr="Miller and Schubert (1990)" startWordPosition="1339" endWordPosition="1342">s event[eid23] after event[eid99] ?”). The entities of the question are annotated in the corresponding key document. However, systems may provide different ids to the same entities. Therefore, we align the system annotation IDs with the question IDs that are annotated in the key docs using the TempEval-3 normalization tool2. • Timegraph Generation: The normalized TimeML docs are used to build a graph of time points representing the temporal relations of the events and timexes identified by each system. Here we use Timegraph (Gerevini et al., 1993) for computing temporal closure as proposed by Miller and Schubert (1990). The Timegraph is first initialized by adding the TimeML explicit relations. Then the Timegraph’s reasoning mechanism infers implicit relations through rules such as transitivity. For example, if eventA BEFORE eventB and eventB BEFORE eventC, then the implicit relation eventA BEFORE eventC can be inferred. Timegraph expands a system’s TimeML annotations and can answer both explicit and im2https://github.com/hllorens/timeml-normalizer plicit Allen temporal relation questions, including OVERLAPS. • Question Processing: Answering questions requires temporal information understanding and reasonin</context>
</contexts>
<marker>Miller, Schubert, 1990</marker>
<rawString>Stephanie Miller and Lenhart Schubert. 1990. Time revisited. In Computational Intelligence, volume 26, pages 108–118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Pustejovsky</author>
<author>Jos´e M Casta˜no</author>
<author>Robert Ingria</author>
<author>Roser Saur´ı</author>
<author>Robert Gaizauskas</author>
<author>Andrea Setzer</author>
<author>Graham Katz</author>
</authors>
<title>TimeML: Robust Specification of Event and Timexes in Text.</title>
<date>2003</date>
<booktitle>In IWCS-5.</booktitle>
<marker>Pustejovsky, Casta˜no, Ingria, Saur´ı, Gaizauskas, Setzer, Katz, 2003</marker>
<rawString>James Pustejovsky, Jos´e M. Casta˜no, Robert Ingria, Roser Saur´ı, Robert Gaizauskas, Andrea Setzer, and Graham Katz. 2003. TimeML: Robust Specification of Event and Timexes in Text. In IWCS-5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Schler</author>
<author>Moshe Koppel</author>
<author>Shlomo Argamon</author>
<author>James W Pennebaker</author>
</authors>
<title>Effects of age and gender on blogging.</title>
<date>2006</date>
<booktitle>In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs,</booktitle>
<pages>199--205</pages>
<contexts>
<context position="12720" citStr="Schler et al., 2006" startWordPosition="1946" endWordPosition="1949">oses. It consists of 79 Yes/No questions and answers about TimeBank documents (UzZaman et al., 2012). Participants can easily extend the question-set by designing new questions over TimeML corpora. 5.2 Test Data The test dataset comprises three domains: • News articles (Wikinews, WSJ, NYT): This covers the traditional TempEval domain used in all the previous editions. • Wikipedia3 articles (history, biographical): This covers documents about people or history, which are rich in temporal entities. • Informal blog posts (narrative): We hand selected blog entries from the Blog Authorship Corpus (Schler et al., 2006). They are in narrative nature, such as the ones describing personal events as opposed to entries with opinions and political commentary. For each of these domains, human experts select the documents, create the set of questions together with the correct answer, and annotate the corresponding entities of the questions in the key documents. The resulting question-set is then peerreviewed by the human experts. Table 1 depicts statistics of the test dataset. In this table, the column dist- shows the number of questions about entities that are in the same or consecutive sentences while dist+ refer</context>
</contexts>
<marker>Schler, Koppel, Argamon, Pennebaker, 2006</marker>
<rawString>Jonathan Schler, Moshe Koppel, Shlomo Argamon, and James W. Pennebaker. 2006. Effects of age and gender on blogging. In AAAI Spring Symposium: Computational Approaches to Analyzing Weblogs, pages 199– 205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naushad UzZaman</author>
<author>Hector Llorens</author>
<author>James Allen</author>
</authors>
<title>Evaluating temporal information understanding with temporal question answering.</title>
<date>2012</date>
<booktitle>In Proceedings of IEEE International Conference on Semantic Computing.</booktitle>
<contexts>
<context position="2214" citStr="UzZaman et al., 2012" startWordPosition="321" endWordPosition="325">elligence (AI). However, despite its original goal, the complexity of temporal QA has caused most research on automatic TimeML systems to focus on a more straightforward temporal information extraction (IE) task. QA TempEval still requires systems to extract temporal relations just like previous TempEvals, however, the QA evaluation is solely based on how well the relations answer questions about the documents. It is no longer about annotation accuracy, but rather the accuracy for targeted questions. Not only does QA represent a more natural way to evaluate temporal information understanding (UzZaman et al., 2012), but also annotating documents with question sets requires much less expertise and effort for humans than corpus-based evaluation which requires full manual annotation of temporal information. In QA TempEval a document does not require the markup of all the temporal entities and relations, but rather a markup of a few key relations central to the text. Although the evaluation schema changes in QA TempEval, the task for participating systems remains the same: extracting temporal information from plain text documents. Here we re-use TempEval-3 task ABC, where systems are required to perform end</context>
<context position="5872" citStr="UzZaman et al., 2012" startWordPosition="891" endWordPosition="895">nd IDENTITY), BEFORE, AFTER, IBEFORE, IAFTER, IS INCLUDED, INCLUDES (and DURING), BEGINS, BEGUN BY, ENDS, and ENDED BY. Since the TimeML DURING does not have a clear mapping, we map it to SIMULTANEOUS for simplicity. The following illustrates how the expression “6:00 pm” BEGINS the state of being “in the gym”. (1) John was in the gym between 6:00 p.m and 7:00 p.m. Each system’s annotations represent its temporal knowledge of the documents. These annotations are 1http://alt.qcri.org/semeval2015/task5 Figure 1: Task - Equivalent to TempEval-3 task ABC then used as input to a temporal QA system (UzZaman et al., 2012) that will answer questions on behalf of the systems, and the accuracy of their answers is compared across systems. 3 QA Evaluation Methodology The main difference between QA TempEval and earlier TempEval editions is that the systems are not scored regarding how similar their annotation to a human annotated key is, but how useful is their TimeML annotation to answer human annotated temporal questions. There are different kinds of temporal questions that could be answered given a TimeML annotation of a document. However, this first QA TempEval focuses on yes/no questions in the following format</context>
<context position="12200" citStr="UzZaman et al., 2012" startWordPosition="1868" endWordPosition="1871"> general. He was &lt;event eid=&amp;quot;e21&amp;quot;&gt;cited&lt;/event&gt;... APW.tml (system annotation, full-TimeML) Farkas &lt;event eid=&amp;quot;e15&amp;quot;...&gt;became&lt;/event&gt; a general. He was &lt;event eid=&amp;quot;e24&amp;quot;...&gt;cited&lt;/event&gt;... &lt;tlink eventID=e15 relatedToEventID=e24 relType=before /&gt; 5.1 Training Data TimeML training data consists of TempEval-3 annotated data: TimeBank, AQUAINT (TBAQ, TempEval-3 training), and TE-3 Platinum (TempEval-3 testing). Furthermore, a question-set in the format explained earlier is provided to the participants for training purposes. It consists of 79 Yes/No questions and answers about TimeBank documents (UzZaman et al., 2012). Participants can easily extend the question-set by designing new questions over TimeML corpora. 5.2 Test Data The test dataset comprises three domains: • News articles (Wikinews, WSJ, NYT): This covers the traditional TempEval domain used in all the previous editions. • Wikipedia3 articles (history, biographical): This covers documents about people or history, which are rich in temporal entities. • Informal blog posts (narrative): We hand selected blog entries from the Blog Authorship Corpus (Schler et al., 2006). They are in narrative nature, such as the ones describing personal events as o</context>
<context position="14248" citStr="UzZaman et al., 2012" startWordPosition="2209" endWordPosition="2212">ve (no) question came naturally. This is due to the fact that we can automatically generate negative questions from positive questions, but not the other way around. Note that the number of questions about distant entities is considerable. TimeML training data and thus systems tend to only annotate temporal relations about less 3http://en.wikipedia.org 795 distant entities. Therefore, to answer distant questions the necessary implicit relations must be obtainable from the annotated explicit relations. 5.3 Development Time Cost One of the claims of QA evaluation of temporal text understanding (UzZaman et al., 2012) is that the time cost of creating question sets in QA schema is lower than the one for fully annotating a document with TimeML elements and attributes. Both tasks involve reading the document. However, question-set creation only requires designing yes/no questions paired with answers and annotating the corresponding entities in the document, while full TimeML annotation needs identifying all entities, their attributes, and large set of relations among them. There is not any rigorous information available about the time it takes to perform these different annotation tasks. Comparison is diffic</context>
</contexts>
<marker>UzZaman, Llorens, Allen, 2012</marker>
<rawString>Naushad UzZaman, Hector Llorens, and James Allen. 2012. Evaluating temporal information understanding with temporal question answering. In Proceedings of IEEE International Conference on Semantic Computing.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Naushad UzZaman</author>
<author>Hector Llorens</author>
<author>Leon Derczynski</author>
<author>James Allen</author>
<author>Marc Verhagen</author>
<author>James Pustejovsky</author>
</authors>
<title>Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations.</title>
<date>2013</date>
<booktitle>In 2nd Joint Conference on Lexical and Computational Semantics (*SEM), Vol 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>1--9</pages>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="1189" citStr="UzZaman et al., 2013" startWordPosition="166" endWordPosition="169">ems to capture temporal information relevant to perform an end-user task, as opposed to corpus-based evaluation where all temporal information is equally important. Evaluation results show that the best automated TimeML annotations reach over 30% recall on questions with ‘yes’ answer and about 50% on easier questions with ‘no’ answers. Features that helped achieve better results are event coreference and a time expression reasoner. 1 Introduction QA TempEval is a follow up of the TempEval series in SemEval: TempEval-1 (Verhagen et al., 2007), TempEval-2 (Verhagen et al., 2010), and TempEval3 (UzZaman et al., 2013). TempEval focuses on evaluating systems that extract temporal expressions (timexes), events, and temporal relations as defined in the TimeML standard (Pustejovsky et al., 2003) (timeml.org). QA TempEval is unique in its focus on evaluating temporal information that directly address a QA task. TimeML was originally developed to support research in complex temporal QA within the field of artificial intelligence (AI). However, despite its original goal, the complexity of temporal QA has caused most research on automatic TimeML systems to focus on a more straightforward temporal information extra</context>
</contexts>
<marker>UzZaman, Llorens, Derczynski, Allen, Verhagen, Pustejovsky, 2013</marker>
<rawString>Naushad UzZaman, Hector Llorens, Leon Derczynski, James Allen, Marc Verhagen, and James Pustejovsky. 2013. Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations. In 2nd Joint Conference on Lexical and Computational Semantics (*SEM), Vol 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 1–9, Atlanta, Georgia, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Robert Gaizauskas</author>
<author>Mark Hepple</author>
<author>Frank Schilder</author>
<author>Graham Katz</author>
<author>James Pustejovsky</author>
</authors>
<title>Semeval-2007 task 15: Tempeval temporal relation identification.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations,</booktitle>
<pages>75--80</pages>
<location>Prague.</location>
<contexts>
<context position="1115" citStr="Verhagen et al., 2007" startWordPosition="154" endWordPosition="157"> a more extrinsic goal of question answering. This evaluation requires systems to capture temporal information relevant to perform an end-user task, as opposed to corpus-based evaluation where all temporal information is equally important. Evaluation results show that the best automated TimeML annotations reach over 30% recall on questions with ‘yes’ answer and about 50% on easier questions with ‘no’ answers. Features that helped achieve better results are event coreference and a time expression reasoner. 1 Introduction QA TempEval is a follow up of the TempEval series in SemEval: TempEval-1 (Verhagen et al., 2007), TempEval-2 (Verhagen et al., 2010), and TempEval3 (UzZaman et al., 2013). TempEval focuses on evaluating systems that extract temporal expressions (timexes), events, and temporal relations as defined in the TimeML standard (Pustejovsky et al., 2003) (timeml.org). QA TempEval is unique in its focus on evaluating temporal information that directly address a QA task. TimeML was originally developed to support research in complex temporal QA within the field of artificial intelligence (AI). However, despite its original goal, the complexity of temporal QA has caused most research on automatic Ti</context>
</contexts>
<marker>Verhagen, Gaizauskas, Hepple, Schilder, Katz, Pustejovsky, 2007</marker>
<rawString>Marc Verhagen, Robert Gaizauskas, Mark Hepple, Frank Schilder, Graham Katz, and James Pustejovsky. 2007. Semeval-2007 task 15: Tempeval temporal relation identification. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 75– 80, Prague.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Verhagen</author>
<author>Roser Saur´ı</author>
<author>Tommaso Caselli</author>
<author>James Pustejovsky</author>
</authors>
<date>2010</date>
<booktitle>Semeval-2010 task 13: Tempeval-2. In Proceedings of SemEval-5,</booktitle>
<pages>57--62</pages>
<marker>Verhagen, Saur´ı, Caselli, Pustejovsky, 2010</marker>
<rawString>Marc Verhagen, Roser Saur´ı, Tommaso Caselli, and James Pustejovsky. 2010. Semeval-2010 task 13: Tempeval-2. In Proceedings of SemEval-5, pages 57– 62.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>