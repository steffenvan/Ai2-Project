<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.610889">
Formal aspects and parsing issues of dependency theory
</title>
<note confidence="0.8592965">
Vincenzo Lombardo and Leonardo Lesmo
Dipartimento di Informatica and Centro di Scienza Cognitiva
Universita&apos; di Torino
c.so Svizzera 185 - 10149 Torino - Italy
</note>
<email confidence="0.925835">
{vincenzo, lesmo}@di.unito.it
</email>
<sectionHeader confidence="0.99193" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998004444444445">
The paper investigates the problem of providing a
formal device for the dependency approach to
syntax, and to link it with a parsing model. After
reviewing the basic tenets of the paradigm and the
few existing mathematical results, we describe a
dependency formalism which is able to deal with
long-distance dependencies. Finally, we present an
Earley-style parser for the formalism and discuss the
(polynomial) complexity results.
</bodyText>
<sectionHeader confidence="0.998602" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999534916666667">
Many authors have developed dependency
theories that cover cross-linguistically the most
significant phenomena of natural language syntax:
the approaches range from generative formalisms
(Sgall et al. 1986), to lexically-based descriptions
(Mel&apos;cuk 1988), to hierarchical organizations of
linguistic knowledge (Hudson 1990) (Fraser,
Hudson 1992), to constrained categorial grammars
(Milward 1994). Also, a number of parsers have
been developed for some dependency frameworks
(Covington 1990) (Kwon, Yoon 1991) (Sleator,
Temperley 1993) (Hahn et al. 1994) (Lombardo,
Lesmo 1996), including a stochastic treatment
(Eisner 1996) and an object-oriented parallel
parsing method (Neuhaus, Hahn 1996).
However, dependency theories have never been
explicitly linked to formal models. Parsers and
applications usually refer to grammars built around
a core of dependency concepts, but there is a great
variety in the description of syntactic constraints,
from rules that are very similar to CFG productions
(Gaifrnan 1965) to individual binary relations on
words or syntactic categories (Covington 1990)
(Sleator, Temperley 1993).
</bodyText>
<figure confidence="0.938841">
know
SUBV
likes
SUBJ/ \OBJ
John beans
</figure>
<figureCaption confidence="0.9375532">
Figure 1. A dependency tree for the sentence &amp;quot;I know
John likes beans&amp;quot;. The leftward or rightward orientation
of the edges represents the order constraints: the
dependents that precede (respectively, follow) the head
stand on its left (resp. right).
</figureCaption>
<bodyText confidence="0.999932918367347">
The basic idea of dependency is that the syntactic
structure of a sentence is described in terms of
binary relations (dependency relations) on pairs of
words, a head (parent), and a dependent (daughter),
respectively; these relations usually form a tree, the
dependency tree (fig. 1).
The linguistic merits of dependency syntax have
been widely debated (e.g. (Hudson 1990)).
Dependency syntax is attractive because of the
immediate mapping of dependency trees on the
predicate-arguments structure and because of the
treatment of free-word order constructs (Sgall et al.
1986) (Mel&apos;cuk 1988). Desirable properties of
lexicalized formalisms (Schabes 1990), like finite
ambiguity and decidability of string acceptance,
intuitively hold for dependency syntax.
On the contrary, the formal studies on
dependency theories are rare in the literature.
Gaifman (1965) showed that projective
dependency grammars, expressed by dependency
rules on syntactic categories, are weakly equivalent
to context-free grammars. And, in fact, it is
possible to devise 0(n3) parsers for this formalism
(Lombardo, Lesmo 1996), or other projective
variations (Milward 1994) (Eisner 1996). On the
controlled relaxation of projective constraints, Nasr
(1995) has introduced the condition of pseudo-
projectivity, which provides some controlled looser
constraints on arc crossing in a dependency tree,
and has developed a polynomial parser based on a
graph-structured stack. Neuhaus and Broker (1997)
have recently showed that the general recognition
problem for non-projective dependency grammars
(what they call discontinuous DG) is NP-complete.
They have devised a discontinuous DG with
exclusively lexical categories (no traces, as most
dependency theories do), and dealing with free
word order constructs through a looser subtree
ordering. This formalism, considered as the most
straightforward extension to a projective
formalism, permits the reduction of the vertex
cover problem to the dependency recognition
problem, thus yielding the NP-completeness result.
However, even if banned from the dependency
literature, the use of non lexical categories is only a
notational variant of some graph structures already
present in some formalisms (see, e.g., Word
Grammar (Hudson 1990)). This paper introduces a
lexicalized dependency formalism, which deals
</bodyText>
<page confidence="0.994524">
787
</page>
<bodyText confidence="0.999957769230769">
with long distance dependencies, and a polynomial
parsing algorithm. The formalism is projective, and
copes with long-distance dependency phenomena
through the introduction of non lexical categories.
The non lexical categories allow us to keep
inalterate the condition of projectivity, encoded in
the notion of derivation. The core of the grammar
relies on predicate-argument structures associated
with lexical items, where the head is a word and
dependents are categories linked by edges labelled
with dependency relations. Free word order
constructs are dealt with by constraining
displacements via a set data structure in the
derivation relation. The introduction of non lexical
categories also permits the resolution of the
inconsistencies pointed out by Neuhaus and Broker
in Word Grammar (1997).
The parser is an Earley type parser with a
polynomial complexity, that encodes the
dependency trees associated with a sentence.
The paper is organized as follows. The next
section presents a formal dependency system that
describes the linguistic knowledge. Section 3
presents an Earley-type parser: we illustrate the
algorithm, trace an example, and discuss the
complexity results. Section 4 concludes the paper.
</bodyText>
<subsectionHeader confidence="0.589973">
2. A dependency formalism
</subsectionHeader>
<bodyText confidence="0.984902257142857">
The basic idea of dependency is that the
syntactic structure of a sentence is described in
terms of binary relations (dependency relations) on
pairs of words, a head (or parent), and a dependent
(daughter), respectively; these relations form a tree,
the dependency tree. In this section we introduce a
formal dependency system. The formalism is
expressed via dependency rules which describe one
level of a dependency tree. Then, we introduce a
notion of derivation that allows us to define the
language generated by a dependency grammar of
this form.
The grammar and the lexicon coincide, since the
rules are lexicalized: the head of the rule is a word
of a certain category, i.e. the lexical anchor. From
the linguistic point of view we can recognize two
types of dependency rules: primitive dependency
rules, which represent subcategorization frames,
and non-primitive dependency rules, which result
from the application of lexical metarules to
primitive and non-primitive dependency rules.
Lexical metaniles (not dealt with in this paper)
obey general principles of linguistic theories.
A dependency grammar is a six-tuple &lt;W, C, S,
D, I, H&gt;, where
W is a finite set of symbols (words of a natural
language);
C is a set of syntactic categories (among which the
special category E);
S is a non-empty set of root categories (C S);
D is the set of dependency relations, e.g. SUBJ,
OBJ, XCOMP, P-OBJ, PRED (among which the
special relation VISITOR&apos;);
I is a finite set of symbols (among which the
special symbol 0), called u-indices;
</bodyText>
<equation confidence="0.5702215">
H is a set of dependency rules of the form
x:X (&lt;r #
</equation>
<listItem confidence="0.988297333333333">
• .. &lt;1* mY mu m&apos;c m&gt;)
1) xEW , is the head of the rule;
2) Xe C, is its syntactic category;
3) an element &lt;rj Yj uj tj&gt; is a d-quadruple
(which describes a dependent); the sequence
of d-quads, including the symbol #
(representing the linear position of the head,
# is a special symbol), is called the d-quad
sequence. We have that
</listItem>
<equation confidence="0.9912795">
3a)r1eD,j E (1, i-1, i+1, m);
3b)YjEC, j E (1, ..., i-1, m);
3c) ujE I, j E
3d) i is a (possibly empty) set of triples &lt;u,
r, Y&gt;, called u-triples, where UE I, r D,
YE C.
</equation>
<bodyText confidence="0.983909592592592">
Finally, it holds that:
I) For each UE I that appears in a u-triple &lt;u, r,
Y&gt;€ U, there exists exactly one d-quad
&lt;riYiuiti&gt; in the same rule such that u=ui, i j.
II) For each u=ui of a d-quad &lt;riyjuiti&gt;, there
exists exactly one u-triple &lt;u, r, Y&gt;E t, i*j, in
the same rule.
Intuitively, a dependency rule constrains one node
(head) and its dependents in a dependency tree: the
d-quad sequence states the order of elements, both
the head (# position) and the dependents (d-quads).
The grammar is lexicalized, because each
dependency rule has a lexical anchor in its head
(x:X). A d-quad &lt;rjYjujtj&gt; identifies a dependent
of category Yi, connected with the head via a
dependency refation rj. Each element of the d-quad
sequence is possibly associated with a u-index (uj)
and a set of u-triples (rj). Both uj and Tican be null
elements, i.e. 0 and 0, respectively. A u-triple
component of the d-quad) &lt;u, R, Y&gt; bounds the
area of the dependency tree where the trace can be
located. Given the constraints I and II, there is a
one-to-one correspondence between the u-indices
and the u-triples of the d-quads. Given that a
dependency rule constrains one head and its direct
dependents in the dependency tree, we have that
the dependent indexed by uk is coindexed with a
</bodyText>
<footnote confidence="0.994457666666667">
1 The relation VISITOR (Hudson 1990) accounts for
displaced elements and, differently from the other
relations, is not semantically interpreted.
</footnote>
<page confidence="0.99457">
788
</page>
<bodyText confidence="0.9997958">
trace node in the subtree rooted by the dependent
containing the u-triple &lt;uk, R, Y&gt;.
Now we introduce a notion of derivation for this
formalism. As one dependency rule can be used
more than once in a derivation process, it is
necessary to replace the u-indices with unique
symbols (progressive integers) before the actual
use. The replacement must be consistent in the u
and the t components. When all the indices in the
rule are replaced, we say that the dependency rule
(as well as the u-triple) is instantiated.
A triple consisting of a word w (E W) or the trace
symbol e (ft W) and two integers j.t and v is a word
object of the grammar.
Given a grammar G, the set of word objects of G is
</bodyText>
<equation confidence="0.827847">
Wx(G)={ x E Wu {e))•
</equation>
<bodyText confidence="0.9060966">
A pair consisting of a category X (E C) and a string
of instantiated u-triples y is a category object of the
grammar (X(y)).
A 4-tuple consisting of a dependency relation r
(ED), a category object X(y1), an integer k, a set of
instantiated u-triples y2 is a derivation object of the
grammar. Given a grammar G, the set of derivation
objects of G is
Cx(G) = (&lt;r,Y(71),u,y2&gt; /
re D, Ye C, u is an integer,
Yl ,y2 are strings of instantiated u-triples).
Let c43vE Wx (G)* and WE (Wx (G) Cx(G) )*. The
derivation relation holds as follows:
a &lt;R,X(yp),u,yx&gt;
21:YY21(PP21)),uu12,&apos;TT12&gt;.
</bodyText>
<equation confidence="0.671940111111111">
,ti-i &gt;
uxo
&lt;ri-f-DY i+1(Pi+1),u
&lt;rm,Ym(pm),um,tm&gt;
lif
where x:X (&lt;riYiuvti&gt; #
&lt;rm Y mUratm &gt;) is a
dependency rule, and pi pm=yp u yx.
&lt;r,X( &lt;j,r,X&gt;),u,0&gt; j = a uei ii
</equation>
<bodyText confidence="0.971789090909091">
We define as the reflexive, transitive closure
of
Given a grammar G, L(G) is the language of
sequences of word objects:
L&apos;(G)= { a E Wx (G)* /
&lt;TOP, Q(0), 0,0&gt; a and QE S(G)}
where TOP is a dummy dependency relation. The
language generated by the grammar G, L(G), is
defined through the function t:
L(G)= { WE Wx (G)* w=t(a) and aE L&apos;(G)),
where t is defined recursively as
</bodyText>
<equation confidence="0.980429">
t(-) = -;
t(jlwv a) = w t(a);
t(Liev a) = t(a).
</equation>
<bodyText confidence="0.982038">
where - is the empty sequence.
As an example, consider the grammar
</bodyText>
<equation confidence="0.9719705">
Gi= &lt;
W(G1) = (I, John, beans, know, likes)
C(G1) = (V, V+EX, N)
S(G1) = {V, V+EX}
D(G1) = {SUBJ, OBJ, VISITOR, TOP)
I(G1)= {0, ul)
</equation>
<bodyText confidence="0.822758">
T(G1)&gt;,
where T(G1) includes the following dependency
</bodyText>
<listItem confidence="0.901436142857143">
rules:
1. I: N (#);
2. John: N (#);
3. beans: N (#);
4. likes: V (&lt;SUBJ, N, 0, 0&gt;# &lt;OBJ, N, 0,0)&gt;);
5. know: V+EX (&lt;VIS1TOR, N, ul, 0&gt;
&lt;SUBJ, N. 0, 0&gt;
</listItem>
<bodyText confidence="0.941670333333333">
&lt;SCOMP, V, 0, (&lt;ul,OBJ,N&gt;)&gt;).
A derivation for the sentence &amp;quot;Beans I know John
likes&amp;quot; is the following:
</bodyText>
<equation confidence="0.884390444444444">
&lt;TOP, V+EX(0), 0, 0&gt;
&lt;VISITOR, N(0), 1, 0&gt; &lt;SUBJ, N(0), 0, 0&gt; know
&lt;SCOMP, V(0), 0, (&lt;1,OBJ,N&gt;)&gt;
ibeans &lt;SUBJ, N(0), 0, 0&gt; know
&lt;SCOMP, V (0), 0, (&lt;1,OBJ,N&gt;)&gt;
ibeans I know &lt;SCOMP, V(0&apos;), 0, (&lt;1,OBJ,N&gt;)&gt;
lbeallS I know &lt;SUBJ, N(0), 0, 0&gt; likes
&lt;OBJ, N(&lt;1,OBJ,N&gt;), 0,0&gt;
lbeallS I know John likes
</equation>
<bodyText confidence="0.8821825">
&lt;OBJ, N(&lt;1,OBJ,N›), 0, 0&gt;
ibeans I know John likes £1
The dependency tree corresponding to this
derivation is in fig. 2.
</bodyText>
<sectionHeader confidence="0.932731" genericHeader="method">
3. Parsing issues
</sectionHeader>
<bodyText confidence="0.9733792">
In this section we describe an Earley-style parser
for the formalism in section 2. The parser is an off-
line algorithm: the first step scans the input
sentence to select the appropriate dependency rules
know
</bodyText>
<figure confidence="0.5678264">
,SU SCOMP
BJ
ibeans I likes
sUBJ/ \c■BJ
John el
</figure>
<figureCaption confidence="0.953502">
Figure 2. Dependency tree of the sentence &amp;quot;Beans I
know John likes&amp;quot;, given the grammar GI .
</figureCaption>
<page confidence="0.990213">
789
</page>
<bodyText confidence="0.998261634920635">
from the grammar. The selection is carried out by
matching the head of the rules with the words of
the sentence. The second step follows Earley&apos;s
phases on the dependency rules, together with the
treatment of u-indices and u-triples. This off-line
technique is not uncommon in lexicalized
grammars, since each Earley&apos;s prediction would
waste much computation time (a grammar factor)
in the body of the algorithm, because dependency
rules do not abstract over categories (cf. (Schabes
1990)).
In order to recognize a sentence of n words, n+1
sets Si of items are built. An item represents a
subtree of the total dependency tree associated with
the sentence. An item is a 5-tuple &lt;Dotted-rule,
Position, It-index, v-index, T-stack&gt;. Dotted-rule is
a dependency rule with a dot between two d-quads
of the d-quad sequence. Position is the input
position where the parsing of the subtree
represented by this item began (the leftmost
position on the spanned string). ii-index and v-
index are two integers that correspond to the
indices of a word object in a derivation. T-stack is
a stack of sets of u-triples to be satisfied yet: the
sets of u-triples (including empty sets, when
applicable) provided by the various items are
stacked in order to verify the consumption of one
u-triple in the appropriate subtree (cf. the notion of
derivation above). Each time the parser predicts a
dependent, the set of u-triples associated with it is
pushed onto the stack. In order for an item to enter
the completer phase, the top of T-stack must be the
empty set, that means that all the u-triples
associated with that item have been satisfied. The
sets of u-triples in T-stack are always inserted at
the top, after having checked that each u-triple is
not already present in T-stack (the check neglects
the u-index). In case a u-triple is present, the
deeper u-triple is deleted, and the T-stack will only
contain the u-triple in the top set (see the derivation
relation above). When satisfying a u-triple, the T-
stack is treated as a set data structure, since the
formalism does not pose any constraint on the
order of consumption of the u-triples.
Following Earley&apos;s style, the general idea is to
move the dot rightward, while predicting the
dependents of the head of the rule. The dot can
advance across a d-quadruple &lt;rjYjujti&gt; or across
the special symbol #. The d-quad immediately
following the dot can be indexed ui. This is
acknowledged by predicting an item (representing
the subtree rooted by that dependent), and inserting
a new progressive integer in the fourth component
of the item (v -index). ti is pushed onto T-stack: the
substructure rooted by a node of category Yi must
contain the trace nodes of the type licensed by the
u-triples. The prediction of a trace occurs as in the
case 2) of the derivation process. When an item P
contains a dotted rule with the dot at its end and a
T-stack with the empty set 0 as the top symbol, the
parser looks for the items that can advance the dot,
given the completion of the dotted dependency rule
in P. Here is the algorithm.
</bodyText>
<equation confidence="0.972298153846154">
Sentence: wow wn_j
Grammar G=&lt;W,C,S,D,I,H&gt;
initialization
Loi each x:Q( 8)e H(G), where QE S(G)
replace each u-index in 8 with a progressive integer;
INSERT &lt;x:Q(• 8), 0, 0, 0, [J&gt; into SO
body
•:.a. each set S (0&lt;ign)
f.g/ each P=&lt;y: Y(1 • 8), j, v, T-stack&gt; in Si
---&gt; completer (including pseudocompleter).
if 8 is the empty sequence and TOP(T-stack0
kr each &lt;x: X( A. • &lt;R, Y, ux, Tx&gt;
j&amp;quot;, v&apos;, T-stack&apos;&gt; in Si
T-stack&amp;quot; &lt;- POP(T-stack);
INSERT &lt;x: X(X, &lt;R, Y, ux, tx&gt; • c),
j&apos;, v&apos;, T-stack&amp;quot;&gt; into Si;
---&gt; predictor:
If 6= &lt;R 8, Z E,, u8, t3&gt; !hal
fig each rule z: Z8(0)
replace each u-index in 0 with a prog. integer;
T-stack&apos; &lt;- PUS H-UNION(c 8, T-stack);
INSERT &lt;z: Z8(• 0), i, 0, us, T-stack&apos;&gt; into Si;
&gt; pseudopredictor:
&lt;u, R 8, Z 8&gt; e UNION (seti I seti in T-stack);
DELETE &lt;u, Ra, Z8&gt; from T-stack;
T-stack&apos; &lt;- PUS H(0 , T-stack);
INSERT &lt;E: Z8(. #), i, U. u T-stack&apos;&gt; into S;
---&gt; scanner:
if 8 = #1 then
ffY=wi
INSERT &lt;y: Y(y # • II),
j. II, V. T-stack&gt; into Si+1
&gt; pseudoscanner:
elseif y=E
INSERT &lt;E: Y(# j, g, v, T-stack&gt; into Si;
endfor
endfor.
termination
&lt;x: Q(at .), 0, L, v, []&gt; E Sn, where Qe S(G)
</equation>
<bodyText confidence="0.802202571428571">
then accept else reject endif.
At the beginning (initialization), the parser
initializes the set So, by inserting all the dotted
rules (x:Q(5)€ H(G)) that have a head of a root
category (QE S(G)). The dot precedes the whole d-
quad sequence (5). Each u-index of the rule is
replaced by a progressive integer, in both the u and
</bodyText>
<page confidence="0.989099">
790
</page>
<bodyText confidence="0.9998654">
the components of the d-quads. Both 11 and v-
indices are null (0), and T-stack is empty ([]).
The body consists of an external loop on the
sets Si (0 i 5 n) and an inner loop on the single
items of the set Si. Let
</bodyText>
<equation confidence="0.951205">
P = &lt;y: Y(ri • ),j,5 11, v, T-stack&gt;
</equation>
<bodyText confidence="0.999955243243243">
be a generic item. Following Earley&apos;s schema, the
parser invokes three phases on the item P:
completer, predictor and scanner. Because of the
derivation of traces (e) from the u-triples in T -
stack, we need to add some code (the so-called
pseudo-phases) that deals with completion,
prediction and scanning of these entities.
Completer: When 5 is an empty sequence (all the
d-quad sequence has been traversed) and the top
of T-stack is the empty set 0 (all the triples
concerning this item have been satisfied), the
dotted rule has been completely analyzed. The
completer looks for the items in Sj which were
waiting for completion (return items; j is the
return Position of the item P). The return items
must contain a dotted rule where the dot
immediately precedes a d-quad &lt;R,Y,ox,tx&gt;,
where Y is the head category of the dotted rule in
the item P. Their generic form is &lt;x: X(2. •
&lt;R,Y,ux,tx&gt; C), j&apos;, v&apos;, T-stack&apos;&gt;. These items
from Sj are inserted into Si after having
advanced the dot to the right of &lt;R,Y,u,,T,&gt;.
Before inserting the items, we need updating the
T-stack component, because some u-triples could
have been satisfied (and, then, deleted from the
T-stack). The new T-stack&amp;quot; is the T-stack of the
completed item after popping the top element 0.
Predictor: If the dotted rule of P has a d-quad
&lt;R8,4,u5,ts&gt; immediately after the dot, then the
parser is expecting a subute headed by a word of
category Zs. This expectation is encoded by
inserting a new item (predicted item) in the set,
for each rule associated with Zs (of the form
z:Z8(0)). Again, each u-index of the new item (d -
quad sequence 0) is replaced by a progressive
integer. The v -index component of the predicted
item is set to us. Finally, the parser prepares the
new T-stack&apos;, by pushing the new u-triples
introduced by 28, that are to be satisfied by the
items predicted after the current item. This
operation is accomplished by the primitive
PUSH-UNION, which also accounts for the non
repetition of u-triples in T-stack. As stated in the
derivation relation through the UNION
operation, there cannot be two u-triples with the
same relation and syntactic category in T-stack.
In case of a repetition of a u-triple, PUSH deletes
the old u-triple and inserts the new one (with the
same u-index) in the topmost set. Finally,
INSERT joins the new item to the set Si.
The pseudopredictor accounts for the
safisfaction of the the u-triples when the
appropriate conditions hold. The current d-quad
in P, &lt;128,13,us,ts&gt;, can be the dependent which
satisfies the u-triple &lt;u,R8,Z6&gt; in T-stack (the
UNION operation gathers all the u-triples
scattered through the T-stack): in addition to
updating T-stack (PUSH( 0 ,T-stack)) and
inserting the u-index us in the v component as
usual, the parser also inserts the u-index u in the
t.t component to coindex the appropriate distant
element. Then it inserts an item (trace item) with
a fictitious dotted dependency rule for the trace.
Scanner: When the dot precedes the symbol #, the
parser can scan the current input word wi (if y,
the head of the item P, is equal to it), or
pseudoscan a trace item, respectively. The result
is the insertion of a new item in the subsequent
set (S.,.1) or the same set (Si), respectively.
At the end of the external loop (termination), the
sentence is accepted if an item of a root category Q
with a dotted rule completely recognized, spanning
the whole sentence (Position=0), an empty T-stack
must be in the set S.
</bodyText>
<subsectionHeader confidence="0.998699">
3.1. An example
</subsectionHeader>
<bodyText confidence="0.967117083333333">
In this section, we trace the parsing of the
sentence &amp;quot;Beans I know John likes&amp;quot;. In this
example we neglect the problem of subject-verb
agreement: it can be coded by inserting the AGR
features in the category label (in a similar way to
the +EX feature in the grammar GO; the comments
on the right help to follow the events; the separator
symbol I helps to keep trace of the sets in the stack;
finally, we have left in plain text the d-quad
sequence of the dotted rules; the other components
of the items appear in boldface.
So
</bodyText>
<table confidence="0.728883307692308">
&lt;know: V +EX (• &lt;VISITOR, N, 1,0&gt; (initialization)
&lt;SUBJ, N, 0, 0&gt;
&lt;SCOMP, V, 0, &lt;1, OBJ, N»),
0, 0, 0,[1&gt;
&lt;likes: V (• &lt;SUBJ, N, 0, 0&gt; (initialization)
&lt;OBJ, V, 0, 0&gt;),
0, 0, 0,0&gt;
&lt;beans: N (• # ), 0, 0, 1,[0]&gt; (predictor &amp;quot;know&amp;quot;)
&lt;I: N (• #), 0, 0, 1, [0]&gt; (predictor &amp;quot;know&amp;quot;)
&lt;John: N ( • # ), 0, 0, 1, [0 ]). (predictor &amp;quot;know&amp;quot;)
&lt;beans: N (• #), 0, 0, 0, [0]&gt; (predictor &amp;quot;likes&amp;quot;)
&lt;I: N (• # ), 0, 0, 0, [0]&gt; (predictor &amp;quot;likes&amp;quot;)
&lt;John: N ( • # ), 0, 0, 0, [0]&gt; (predictor &amp;quot;likes&amp;quot;)
</table>
<note confidence="0.330029">
S1 [beans]
&lt;beans: N (# •), 0, 0, 1, [0]&gt; (scanner)
</note>
<page confidence="0.933282">
791
</page>
<table confidence="0.475459432432432">
&lt;beans: N (# •), 0, 0, 0, [0]&gt; (scanner)
&lt;know: V +EX (&lt;VIS1TOR, N, 1,0&gt; (completer &amp;quot;beans&amp;quot;)
• &lt;SUBJ, N, 0, 0&gt;
&lt;SCOMP, V, 0, &lt;1, OBJ, N»),
0, 0, 0, 0&gt;
&lt;likes: V (&lt;SUBJ, N, 0, 0&gt; (completer &amp;quot;beans&amp;quot;)
• #
&lt;OBJ, V, 0, 0&gt;),
0, 0, 0, 0&gt;
&lt;beans: N (• #), 1, 0, 040&gt; (predictor &amp;quot;know&amp;quot;)
N (0 #), 1, 0, 0, [ (predictor &amp;quot;know&amp;quot; )
&lt;John: N(• #), 1, 0, 0, 10 ]&gt; (predictor &amp;quot;know&amp;quot; )
S2 [I]
&lt;I: N (# • ), 1, 0, 0, [0f0]&gt; (scanner)
&lt;know: V +EX (&lt;VISITOR, N, 1, 0&gt; (completer &amp;quot;I&amp;quot; )
&lt;SUBJ, N, 0, 0&gt;
• #
&lt;SCOMP, V. 0, &lt;1, OBJ, N»),
0, 0, 0, 0&gt;
S3 [know]
&lt;know: V +EX (&lt;VISITOR, N, 1, 0&gt; (scanner)
&lt;SUBJ, N, 0, 0&gt;
• &lt;SCOMP, V, 0, &lt;1, OBJ, N»),
0, 0, 0, 0&gt;
&lt;likes: V (• &lt;SUBJ, N, 0, 0&gt; (predictor &amp;quot;know&amp;quot;)
&lt;OBJ, V. 0, 0&gt;),
3,0, 0, [ {&lt;1, OBJ, N&gt;}l&gt;
&lt;beans: N (• # ), 3, 0, 2,[{&lt;1, OW, N&gt;) I0]&gt; (p. &amp;quot;know&amp;quot;)
N (0 #), 3, 0, 2, [ {&lt;1, OBJ, N&gt;}10]&gt; (p. &amp;quot;know&amp;quot;)
&lt;John: N(• #), 3, 0, 2, [{&lt;1, OBJ, N&gt;} j0]&gt; (p. &amp;quot;know&amp;quot;)
&lt;beans: N (• # ), 3, 0, 0, [{&lt;1, OBJ, N&gt;110]&gt; (p. &amp;quot;likes&amp;quot;)
&lt;I: N (• # ), 3, 0, 0, [(&lt;1, OBJ, N&gt;)10]&gt; (p. &amp;quot;likes&amp;quot;)
&lt;John: N(• #), 3, 0, 0, [{&lt;1, OBJ, N&gt;)10]&gt; (p. &amp;quot;likes&amp;quot;)
S4 [John]
&lt;John: N(# 0), 3, 0, 2, [{d, OBJ, N&gt;)10]&gt; (scanner)
&lt;John: N (# •), 3, 0,0, [{&lt;l, OBJ, N&gt;}10]&gt; (scanner)
&lt;know: V +EX (&lt;VISITOR, N, 2, 0&gt; (completer &amp;quot;John&amp;quot;)
</table>
<reference confidence="0.722739695652174">
• &lt;SUBJ, N, 0, 0&gt;
&lt;SCOMP, V.0, &lt;2, OBJ, N»),
3, 0, 0, [{&lt;1, OBJ, N&gt;)]&gt;
&lt;likes: V (&lt;SUBJ, N, 0, 0&gt; (completer &amp;quot;John&amp;quot;)
• #
&lt;OBJ, V, 0, 0&gt;),
3,0, 0, [ (&lt;1, OBJ, N&gt;)]&gt;
Ss [likes]
&lt;likes: V (&lt;SUBJ, N, 0, 0&gt; (scanner)
• &lt;OBI, N, 0,0&gt;),
0, 0, 0, [ {&lt;1, OBJ, N&gt;ll&gt;
&lt;beans: N (• #), 5, 0,0, [{&lt;1, OBJ, N&gt;)10]&gt; (p. &amp;quot;likes&amp;quot;)
d: N (• #), 5, 0, 0, [{&lt;1, OBJ, N&gt;)10]&gt; (p. &amp;quot;likes&amp;quot; )
&lt;John: N ( • # ), 5, 0, 0, [{&lt;1, OBJ, N&gt;)10]&gt; (p. &amp;quot;likes&amp;quot;)
&lt;E: N (• # ), 5, 0, 0, [01&gt; (pseudopmdictor )
:N (# •), 5, 0, 0, [0]&gt; (pseudopredictor )
&lt;likes: V (&lt;SUBJ, N, 0, 0&gt; (completer)
&lt;OBJ, N, 0, 0&gt; • ),
3, 0, 0, [0]&gt;
&lt;know: V +EX (&lt;VISITOR, N, 1, 0&gt; (completer)
&lt;SUBJ, N, 0, 0&gt;
&lt;SCOMP, V. 0, &lt;1, OBJ, N» • ),
0, 0,0, 0&gt;
</reference>
<subsectionHeader confidence="0.69159">
3.2. Complexity results
</subsectionHeader>
<bodyText confidence="0.999476846153846">
The parser has a polynomial complexity. The
space complexity of the parser. i.e. the number of
items, is 0(n3+ IDIICN.
) Each item is a 5-tuple
&lt;Dotted-rule, Position, 1.t-index, v-index, T-stack&gt;:
Dotted rules are in a number which is a constant of
the grammar, but in off-line parsing this number is
bounded by 0(n). Position is bounded by 0(n). 11.-
index and v -index are two integers that keep trace
of u-triple satisfaction, and do not add an own
contribution to the complexity count. T-stack has a
number of elements which depends on the
maximum length of the chain of predictions. Since
the number of rules is 0(n), the size of the stack is
0(n). The elements of T-stack contain all the u-
triples introduced up to an item and which are to
be satisfied (deleted) yet. A u-triple is of the form
&lt;u,R,Y&gt;: u is an integer that is ininfluent, RE D,
Ye C. Because of the PUSH-UNION operation on
T-stack, the number of possible u-triples scattered
throughout the elements of T-stack is IDIICI. The
number of different stacks is given by the
dispositions of IDIICI u-triples on 0(n) elements;
so, 0(n Then, the number of items in a set of
items is bounded by 0(n2+ IDIICI) and there are n
sets of items (0(n3+ IDI
The time complexity of the parser is 0(n7+3 1D1
ICI), Each of the three phases executes an INSERT
of an item in a set. The cost of the INSERT
operation depends on the implementation of the set
data structure; we assume it to be linear (0(01-
Dud)) to make easy calculations. The phase
completer executes at most 0(n2+ ID10)) actions
per each pair of items (two for-loops). The pairs of
items are 0(n62 IDI ICI ■•
) But to execute the action
of the completer, one of the sets must have the
index equal to one of the positions, so 0(n5 21D1
1C1). Thus, the completer costs 0(n73 IDI ICI). The
</bodyText>
<page confidence="0.989785">
792
</page>
<bodyText confidence="0.9995278">
phase predictor, executes 0(n) actions for each
item to introduce the predictions (&amp;quot;for each rule&amp;quot;
loop); then, the loop of the pseudopredictor is
0(ID110) (UNION+DELETE), a grammar factor.
Finally it inserts the new item in the set (0(n2+
The total number of items is 0(0+ID IC)
and, so, the cost of the predictor 0(n6 + 21D1 ICI),
The phase scanner executes the INSERT operation
per item, and the items are at most 0(n3+ IDI ICI).
Thus, the scanner costs 0(n5+2 IDI ICI). The total
complexity of the algorithm is 0(n71-3 &apos;Ma).
We are conscious that the (grammar dependent)
exponent can be very high, but the treatment of the
set data structure for the u-triples requires
expensive operations (cf. a stack). Actually this
formalism is able to deal a high degree of free
word order (for a comparable result, see (Becker,
Rambow 1995)). Also, the complexity factor due
to the cardinalities of the sets D and C is greatly
reduced if we consider that linguistic constraints
restrict the displacement of several categories and
relations. A better estimation of complexity can
only be done when we consider empirically the
impact of the linguistic constraints in writing a
wide coverage grammar.
</bodyText>
<sectionHeader confidence="0.993595" genericHeader="conclusions">
4. Conclusions
</sectionHeader>
<bodyText confidence="0.999950444444445">
The paper has described a dependency
formalism and an Earley-type parser with a
polynomial complexity.
The introduction of non lexical categories in a
dependency formalism allows the treatment of
long distance dependencies and of free word order,
and to aovid the NP-completeness. The grammar
factor at the exponent can be reduced if we
furtherly restrict the long distance dependencies
through the introduction of a more restrictive data
structure than the set, as it happens in some
constrained phrase structure formalisms (Vijay-
Schanker, Weir 1994).
A compilation step in the parser can produce
parse tables that account for left-corner
information (this optimization of the Earley
algorithm has already been proven fruitful in
(Lombardo, Lesmo 1996)).
</bodyText>
<sectionHeader confidence="0.996625" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999881830769231">
Becker T., Rambow 0., Parsing non-immediate
dominance relations, Proc. IWPT 95, Prague,
1995, 26-33.
Covington M. A., Parsing Discontinuous
Constituents in Dependency Grammar,
Computational Linguistics 16, 1990, 234-236.
Earley J., An Efficient Context-free Parsing
Algorithm. CACM 13, 1970,94-102.
Eisner J., Three New Probabilistic Models for
Dependency Parsing: An Exploration, Proc.
COLING 96, Copenhagen, 1996, 340-345.
Fraser N.M., Hudson R. A., Inheritance in Word
Grammar, Computational Linguistics 18,
1992, 133-158.
Gaifman H., Dependency Systems and Phrase
Structure Systems, Information and Control 8,
1965, 304-337.
Hahn U., Schacht S., Broker N., Concurrent,
Object-Oriented Natural Language Parsing:
The ParseTalk Model, CLIF Report 9/94,
Albert-Ludwigs-Univ., Freiburg, Germany (also
in Journal of Human-Computer Studies).
Hudson R., English Word Grammar, Basil
Blackwell, Oxford, 1990.
Kwon H., Yo on A., Unification-Based
Dependency Parsing of Governor-Final
Languages, Proc. IWPT 91, Cancun, 1991, 182-
192.
Lombardo V., Lesmo L., An Earley-type
recognizer for dependency grammar, Proc.
COLING 96, Copenhagen, 1996, 723-728.
Mel&apos;cuk I., Dependency Syntax: Theory and
Practice, SUNY Press, Albany, 1988.
Milward D., Dynamic Dependency Grammar,
Linguistics and Phylosophy, December 1994.
Nasr A., A formalism and a parser for lexicalized
dependency grammar, Proc. IWPT 95, Prague,
1995, 186-195.
Neuhaus P., Broker N., The Complexity of
Recognition of Linguistically Adequate
Dependency Grammars, Proc. ACL/EACL97,
Madrid, 1997, 337-343.
Neuhaus P., Hahn U., Restricted Parallelism in
Object-Oriented Parsing, Proc. COLING 96,
Copenhagen, 1996, 502-507.
Rambow 0., Joshi A., A Formal Look at
Dependency Grammars and Phrase-Structure
Grammars, with Special Consideration of
Word-Order Phenomena, Int. Workshop on The
Meaning-Text Theory, Darrnstadt, 1992.
Schabes Y., Mathematical and Computational
Aspects Of Lexicalized Grammars, Ph.D.
Dissertation MS-CIS-90-48, Dept. of Computer
and Information Science, University of
Pennsylvania, Philadelphia (PA), August 1990.
Sgall P., Haijcova E., Panevova J., The Meaning of
Sentence in its Semantic and Pragmatic
Aspects, Dordrecht Reidel Publ. Co., Dordrecht,
1986.
Sleator D. D., Temperley D., Parsing English with
a Link Grammar, Proc. of IWPT93, 1993, 277 -
291.
Vijay-Schanker K., Weir D. J., Parsing some
constrained grammar formalisms,
Computational Linguistics 19/4, 1994
</reference>
<page confidence="0.99898">
793
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.834430">
<title confidence="0.990409">Formal aspects and parsing issues of dependency theory</title>
<author confidence="0.998905">Vincenzo Lombardo</author>
<author confidence="0.998905">Leonardo Lesmo</author>
<affiliation confidence="0.999185">Dipartimento di Informatica and Centro di Scienza Cognitiva Universita&apos; di Torino</affiliation>
<address confidence="0.854491">c.so Svizzera 185 - 10149 Torino - Italy</address>
<email confidence="0.995027">vincenzo@di.unito.it</email>
<email confidence="0.995027">lesmo@di.unito.it</email>
<abstract confidence="0.9991499">The paper investigates the problem of providing a formal device for the dependency approach to syntax, and to link it with a parsing model. After reviewing the basic tenets of the paradigm and the few existing mathematical results, we describe a dependency formalism which is able to deal with long-distance dependencies. Finally, we present an Earley-style parser for the formalism and discuss the (polynomial) complexity results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>N</author>
</authors>
<title>(completer &amp;quot;John&amp;quot;)</title>
<journal>OBJ, N&gt;)]&gt; V (&lt;SUBJ, N,</journal>
<volume>0</volume>
<pages>(scanner)</pages>
<marker>N, </marker>
<rawString>• &lt;SUBJ, N, 0, 0&gt; &lt;SCOMP, V.0, &lt;2, OBJ, N»), 3, 0, 0, [{&lt;1, OBJ, N&gt;)]&gt; &lt;likes: V (&lt;SUBJ, N, 0, 0&gt; (completer &amp;quot;John&amp;quot;) • # &lt;OBJ, V, 0, 0&gt;), 3,0, 0, [ (&lt;1, OBJ, N&gt;)]&gt; Ss [likes] &lt;likes: V (&lt;SUBJ, N, 0, 0&gt; (scanner)</rawString>
</citation>
<citation valid="false">
<authors>
<author>N</author>
</authors>
<title>OBJ, N&gt;)10]&gt; (p. &amp;quot;likes&amp;quot;) d:</title>
<journal>OBJ, N&gt;ll&gt; N (• #),</journal>
<volume>0</volume>
<pages>0</pages>
<marker>N, </marker>
<rawString>• &lt;OBI, N, 0,0&gt;), 0, 0, 0, [ {&lt;1, OBJ, N&gt;ll&gt; &lt;beans: N (• #), 5, 0,0, [{&lt;1, OBJ, N&gt;)10]&gt; (p. &amp;quot;likes&amp;quot;) d: N (• #), 5, 0, 0, [{&lt;1, OBJ, N&gt;)10]&gt; (p. &amp;quot;likes&amp;quot; ) &lt;John: N ( • # ), 5, 0, 0, [{&lt;1, OBJ, N&gt;)10]&gt; (p. &amp;quot;likes&amp;quot;) &lt;E: N (• # ), 5, 0, 0, [01&gt; (pseudopmdictor ) :N (# •), 5, 0, 0, [0]&gt; (pseudopredictor ) &lt;likes: V (&lt;SUBJ, N, 0, 0&gt; (completer) &lt;OBJ, N, 0, 0&gt; • ), 3, 0, 0, [0]&gt;</rawString>
</citation>
<citation valid="false">
<authors>
<author>V EX</author>
</authors>
<volume>1</volume>
<pages>0</pages>
<marker>EX, </marker>
<rawString>&lt;know: V +EX (&lt;VISITOR, N, 1, 0&gt; (completer) &lt;SUBJ, N, 0, 0&gt;</rawString>
</citation>
<citation valid="false">
<authors>
<author>V</author>
</authors>
<title>1, OBJ,</title>
<journal>N» • ),</journal>
<volume>0</volume>
<pages>0</pages>
<marker>V, </marker>
<rawString>&lt;SCOMP, V. 0, &lt;1, OBJ, N» • ), 0, 0,0, 0&gt;</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Becker</author>
</authors>
<title>Rambow 0., Parsing non-immediate dominance relations,</title>
<date>1995</date>
<booktitle>Proc. IWPT 95,</booktitle>
<pages>26--33</pages>
<location>Prague,</location>
<marker>Becker, 1995</marker>
<rawString>Becker T., Rambow 0., Parsing non-immediate dominance relations, Proc. IWPT 95, Prague, 1995, 26-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Covington</author>
</authors>
<title>Parsing Discontinuous Constituents in Dependency Grammar,</title>
<date>1990</date>
<journal>Computational Linguistics</journal>
<volume>16</volume>
<pages>234--236</pages>
<contexts>
<context position="1193" citStr="Covington 1990" startWordPosition="164" endWordPosition="165">nally, we present an Earley-style parser for the formalism and discuss the (polynomial) complexity results. 1. Introduction Many authors have developed dependency theories that cover cross-linguistically the most significant phenomena of natural language syntax: the approaches range from generative formalisms (Sgall et al. 1986), to lexically-based descriptions (Mel&apos;cuk 1988), to hierarchical organizations of linguistic knowledge (Hudson 1990) (Fraser, Hudson 1992), to constrained categorial grammars (Milward 1994). Also, a number of parsers have been developed for some dependency frameworks (Covington 1990) (Kwon, Yoon 1991) (Sleator, Temperley 1993) (Hahn et al. 1994) (Lombardo, Lesmo 1996), including a stochastic treatment (Eisner 1996) and an object-oriented parallel parsing method (Neuhaus, Hahn 1996). However, dependency theories have never been explicitly linked to formal models. Parsers and applications usually refer to grammars built around a core of dependency concepts, but there is a great variety in the description of syntactic constraints, from rules that are very similar to CFG productions (Gaifrnan 1965) to individual binary relations on words or syntactic categories (Covington 199</context>
</contexts>
<marker>Covington, 1990</marker>
<rawString>Covington M. A., Parsing Discontinuous Constituents in Dependency Grammar, Computational Linguistics 16, 1990, 234-236.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Earley</author>
</authors>
<title>An Efficient Context-free Parsing Algorithm.</title>
<journal>CACM</journal>
<volume>13</volume>
<pages>1970--94</pages>
<marker>Earley, </marker>
<rawString>Earley J., An Efficient Context-free Parsing Algorithm. CACM 13, 1970,94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three New Probabilistic Models for Dependency Parsing: An Exploration,</title>
<date>1996</date>
<booktitle>Proc. COLING 96,</booktitle>
<pages>340--345</pages>
<location>Copenhagen,</location>
<contexts>
<context position="1327" citStr="Eisner 1996" startWordPosition="183" endWordPosition="184">have developed dependency theories that cover cross-linguistically the most significant phenomena of natural language syntax: the approaches range from generative formalisms (Sgall et al. 1986), to lexically-based descriptions (Mel&apos;cuk 1988), to hierarchical organizations of linguistic knowledge (Hudson 1990) (Fraser, Hudson 1992), to constrained categorial grammars (Milward 1994). Also, a number of parsers have been developed for some dependency frameworks (Covington 1990) (Kwon, Yoon 1991) (Sleator, Temperley 1993) (Hahn et al. 1994) (Lombardo, Lesmo 1996), including a stochastic treatment (Eisner 1996) and an object-oriented parallel parsing method (Neuhaus, Hahn 1996). However, dependency theories have never been explicitly linked to formal models. Parsers and applications usually refer to grammars built around a core of dependency concepts, but there is a great variety in the description of syntactic constraints, from rules that are very similar to CFG productions (Gaifrnan 1965) to individual binary relations on words or syntactic categories (Covington 1990) (Sleator, Temperley 1993). know SUBV likes SUBJ/ \OBJ John beans Figure 1. A dependency tree for the sentence &amp;quot;I know John likes be</context>
<context position="3271" citStr="Eisner 1996" startWordPosition="467" endWordPosition="468">structs (Sgall et al. 1986) (Mel&apos;cuk 1988). Desirable properties of lexicalized formalisms (Schabes 1990), like finite ambiguity and decidability of string acceptance, intuitively hold for dependency syntax. On the contrary, the formal studies on dependency theories are rare in the literature. Gaifman (1965) showed that projective dependency grammars, expressed by dependency rules on syntactic categories, are weakly equivalent to context-free grammars. And, in fact, it is possible to devise 0(n3) parsers for this formalism (Lombardo, Lesmo 1996), or other projective variations (Milward 1994) (Eisner 1996). On the controlled relaxation of projective constraints, Nasr (1995) has introduced the condition of pseudoprojectivity, which provides some controlled looser constraints on arc crossing in a dependency tree, and has developed a polynomial parser based on a graph-structured stack. Neuhaus and Broker (1997) have recently showed that the general recognition problem for non-projective dependency grammars (what they call discontinuous DG) is NP-complete. They have devised a discontinuous DG with exclusively lexical categories (no traces, as most dependency theories do), and dealing with free word</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Eisner J., Three New Probabilistic Models for Dependency Parsing: An Exploration, Proc. COLING 96, Copenhagen, 1996, 340-345.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N M Fraser</author>
<author>R A Hudson</author>
</authors>
<title>Inheritance in Word Grammar,</title>
<date>1992</date>
<journal>Computational Linguistics</journal>
<volume>18</volume>
<pages>133--158</pages>
<marker>Fraser, Hudson, 1992</marker>
<rawString>Fraser N.M., Hudson R. A., Inheritance in Word Grammar, Computational Linguistics 18, 1992, 133-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Gaifman</author>
</authors>
<title>Dependency Systems and Phrase Structure Systems,</title>
<date>1965</date>
<journal>Information and Control</journal>
<volume>8</volume>
<pages>304--337</pages>
<contexts>
<context position="2968" citStr="Gaifman (1965)" startWordPosition="425" endWordPosition="426">orm a tree, the dependency tree (fig. 1). The linguistic merits of dependency syntax have been widely debated (e.g. (Hudson 1990)). Dependency syntax is attractive because of the immediate mapping of dependency trees on the predicate-arguments structure and because of the treatment of free-word order constructs (Sgall et al. 1986) (Mel&apos;cuk 1988). Desirable properties of lexicalized formalisms (Schabes 1990), like finite ambiguity and decidability of string acceptance, intuitively hold for dependency syntax. On the contrary, the formal studies on dependency theories are rare in the literature. Gaifman (1965) showed that projective dependency grammars, expressed by dependency rules on syntactic categories, are weakly equivalent to context-free grammars. And, in fact, it is possible to devise 0(n3) parsers for this formalism (Lombardo, Lesmo 1996), or other projective variations (Milward 1994) (Eisner 1996). On the controlled relaxation of projective constraints, Nasr (1995) has introduced the condition of pseudoprojectivity, which provides some controlled looser constraints on arc crossing in a dependency tree, and has developed a polynomial parser based on a graph-structured stack. Neuhaus and Br</context>
</contexts>
<marker>Gaifman, 1965</marker>
<rawString>Gaifman H., Dependency Systems and Phrase Structure Systems, Information and Control 8, 1965, 304-337.</rawString>
</citation>
<citation valid="false">
<authors>
<author>U Hahn</author>
<author>S Schacht</author>
<author>N Broker</author>
<author>Concurrent</author>
</authors>
<title>Object-Oriented Natural Language Parsing: The ParseTalk Model,</title>
<tech>CLIF Report 9/94,</tech>
<location>Albert-Ludwigs-Univ., Freiburg, Germany</location>
<note>(also in Journal of Human-Computer Studies).</note>
<marker>Hahn, Schacht, Broker, Concurrent, </marker>
<rawString>Hahn U., Schacht S., Broker N., Concurrent, Object-Oriented Natural Language Parsing: The ParseTalk Model, CLIF Report 9/94, Albert-Ludwigs-Univ., Freiburg, Germany (also in Journal of Human-Computer Studies).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hudson</author>
</authors>
<title>English Word Grammar,</title>
<date>1990</date>
<location>Basil Blackwell, Oxford,</location>
<contexts>
<context position="1025" citStr="Hudson 1990" startWordPosition="141" endWordPosition="142"> basic tenets of the paradigm and the few existing mathematical results, we describe a dependency formalism which is able to deal with long-distance dependencies. Finally, we present an Earley-style parser for the formalism and discuss the (polynomial) complexity results. 1. Introduction Many authors have developed dependency theories that cover cross-linguistically the most significant phenomena of natural language syntax: the approaches range from generative formalisms (Sgall et al. 1986), to lexically-based descriptions (Mel&apos;cuk 1988), to hierarchical organizations of linguistic knowledge (Hudson 1990) (Fraser, Hudson 1992), to constrained categorial grammars (Milward 1994). Also, a number of parsers have been developed for some dependency frameworks (Covington 1990) (Kwon, Yoon 1991) (Sleator, Temperley 1993) (Hahn et al. 1994) (Lombardo, Lesmo 1996), including a stochastic treatment (Eisner 1996) and an object-oriented parallel parsing method (Neuhaus, Hahn 1996). However, dependency theories have never been explicitly linked to formal models. Parsers and applications usually refer to grammars built around a core of dependency concepts, but there is a great variety in the description of s</context>
<context position="2483" citStr="Hudson 1990" startWordPosition="358" endWordPosition="359">A dependency tree for the sentence &amp;quot;I know John likes beans&amp;quot;. The leftward or rightward orientation of the edges represents the order constraints: the dependents that precede (respectively, follow) the head stand on its left (resp. right). The basic idea of dependency is that the syntactic structure of a sentence is described in terms of binary relations (dependency relations) on pairs of words, a head (parent), and a dependent (daughter), respectively; these relations usually form a tree, the dependency tree (fig. 1). The linguistic merits of dependency syntax have been widely debated (e.g. (Hudson 1990)). Dependency syntax is attractive because of the immediate mapping of dependency trees on the predicate-arguments structure and because of the treatment of free-word order constructs (Sgall et al. 1986) (Mel&apos;cuk 1988). Desirable properties of lexicalized formalisms (Schabes 1990), like finite ambiguity and decidability of string acceptance, intuitively hold for dependency syntax. On the contrary, the formal studies on dependency theories are rare in the literature. Gaifman (1965) showed that projective dependency grammars, expressed by dependency rules on syntactic categories, are weakly equi</context>
<context position="4364" citStr="Hudson 1990" startWordPosition="623" endWordPosition="624">ntinuous DG with exclusively lexical categories (no traces, as most dependency theories do), and dealing with free word order constructs through a looser subtree ordering. This formalism, considered as the most straightforward extension to a projective formalism, permits the reduction of the vertex cover problem to the dependency recognition problem, thus yielding the NP-completeness result. However, even if banned from the dependency literature, the use of non lexical categories is only a notational variant of some graph structures already present in some formalisms (see, e.g., Word Grammar (Hudson 1990)). This paper introduces a lexicalized dependency formalism, which deals 787 with long distance dependencies, and a polynomial parsing algorithm. The formalism is projective, and copes with long-distance dependency phenomena through the introduction of non lexical categories. The non lexical categories allow us to keep inalterate the condition of projectivity, encoded in the notion of derivation. The core of the grammar relies on predicate-argument structures associated with lexical items, where the head is a word and dependents are categories linked by edges labelled with dependency relations</context>
<context position="9073" citStr="Hudson 1990" startWordPosition="1412" endWordPosition="1413">. Each element of the d-quad sequence is possibly associated with a u-index (uj) and a set of u-triples (rj). Both uj and Tican be null elements, i.e. 0 and 0, respectively. A u-triple component of the d-quad) &lt;u, R, Y&gt; bounds the area of the dependency tree where the trace can be located. Given the constraints I and II, there is a one-to-one correspondence between the u-indices and the u-triples of the d-quads. Given that a dependency rule constrains one head and its direct dependents in the dependency tree, we have that the dependent indexed by uk is coindexed with a 1 The relation VISITOR (Hudson 1990) accounts for displaced elements and, differently from the other relations, is not semantically interpreted. 788 trace node in the subtree rooted by the dependent containing the u-triple &lt;uk, R, Y&gt;. Now we introduce a notion of derivation for this formalism. As one dependency rule can be used more than once in a derivation process, it is necessary to replace the u-indices with unique symbols (progressive integers) before the actual use. The replacement must be consistent in the u and the t components. When all the indices in the rule are replaced, we say that the dependency rule (as well as th</context>
</contexts>
<marker>Hudson, 1990</marker>
<rawString>Hudson R., English Word Grammar, Basil Blackwell, Oxford, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kwon</author>
</authors>
<title>Yo on A., Unification-Based Dependency Parsing of Governor-Final Languages,</title>
<date>1991</date>
<booktitle>Proc. IWPT 91,</booktitle>
<pages>182--192</pages>
<location>Cancun,</location>
<marker>Kwon, 1991</marker>
<rawString>Kwon H., Yo on A., Unification-Based Dependency Parsing of Governor-Final Languages, Proc. IWPT 91, Cancun, 1991, 182-192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Lombardo</author>
<author>L Lesmo</author>
</authors>
<title>An Earley-type recognizer for dependency grammar,</title>
<date>1996</date>
<booktitle>Proc. COLING 96,</booktitle>
<pages>723--728</pages>
<location>Copenhagen,</location>
<marker>Lombardo, Lesmo, 1996</marker>
<rawString>Lombardo V., Lesmo L., An Earley-type recognizer for dependency grammar, Proc. COLING 96, Copenhagen, 1996, 723-728.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mel&apos;cuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice,</title>
<date>1988</date>
<publisher>SUNY Press,</publisher>
<location>Albany,</location>
<contexts>
<context position="956" citStr="Mel&apos;cuk 1988" startWordPosition="133" endWordPosition="134">ch to syntax, and to link it with a parsing model. After reviewing the basic tenets of the paradigm and the few existing mathematical results, we describe a dependency formalism which is able to deal with long-distance dependencies. Finally, we present an Earley-style parser for the formalism and discuss the (polynomial) complexity results. 1. Introduction Many authors have developed dependency theories that cover cross-linguistically the most significant phenomena of natural language syntax: the approaches range from generative formalisms (Sgall et al. 1986), to lexically-based descriptions (Mel&apos;cuk 1988), to hierarchical organizations of linguistic knowledge (Hudson 1990) (Fraser, Hudson 1992), to constrained categorial grammars (Milward 1994). Also, a number of parsers have been developed for some dependency frameworks (Covington 1990) (Kwon, Yoon 1991) (Sleator, Temperley 1993) (Hahn et al. 1994) (Lombardo, Lesmo 1996), including a stochastic treatment (Eisner 1996) and an object-oriented parallel parsing method (Neuhaus, Hahn 1996). However, dependency theories have never been explicitly linked to formal models. Parsers and applications usually refer to grammars built around a core of depe</context>
<context position="2701" citStr="Mel&apos;cuk 1988" startWordPosition="389" endWordPosition="390">ts left (resp. right). The basic idea of dependency is that the syntactic structure of a sentence is described in terms of binary relations (dependency relations) on pairs of words, a head (parent), and a dependent (daughter), respectively; these relations usually form a tree, the dependency tree (fig. 1). The linguistic merits of dependency syntax have been widely debated (e.g. (Hudson 1990)). Dependency syntax is attractive because of the immediate mapping of dependency trees on the predicate-arguments structure and because of the treatment of free-word order constructs (Sgall et al. 1986) (Mel&apos;cuk 1988). Desirable properties of lexicalized formalisms (Schabes 1990), like finite ambiguity and decidability of string acceptance, intuitively hold for dependency syntax. On the contrary, the formal studies on dependency theories are rare in the literature. Gaifman (1965) showed that projective dependency grammars, expressed by dependency rules on syntactic categories, are weakly equivalent to context-free grammars. And, in fact, it is possible to devise 0(n3) parsers for this formalism (Lombardo, Lesmo 1996), or other projective variations (Milward 1994) (Eisner 1996). On the controlled relaxation</context>
</contexts>
<marker>Mel&apos;cuk, 1988</marker>
<rawString>Mel&apos;cuk I., Dependency Syntax: Theory and Practice, SUNY Press, Albany, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Milward</author>
</authors>
<title>Dynamic Dependency Grammar, Linguistics and Phylosophy,</title>
<date>1994</date>
<contexts>
<context position="1098" citStr="Milward 1994" startWordPosition="150" endWordPosition="151"> we describe a dependency formalism which is able to deal with long-distance dependencies. Finally, we present an Earley-style parser for the formalism and discuss the (polynomial) complexity results. 1. Introduction Many authors have developed dependency theories that cover cross-linguistically the most significant phenomena of natural language syntax: the approaches range from generative formalisms (Sgall et al. 1986), to lexically-based descriptions (Mel&apos;cuk 1988), to hierarchical organizations of linguistic knowledge (Hudson 1990) (Fraser, Hudson 1992), to constrained categorial grammars (Milward 1994). Also, a number of parsers have been developed for some dependency frameworks (Covington 1990) (Kwon, Yoon 1991) (Sleator, Temperley 1993) (Hahn et al. 1994) (Lombardo, Lesmo 1996), including a stochastic treatment (Eisner 1996) and an object-oriented parallel parsing method (Neuhaus, Hahn 1996). However, dependency theories have never been explicitly linked to formal models. Parsers and applications usually refer to grammars built around a core of dependency concepts, but there is a great variety in the description of syntactic constraints, from rules that are very similar to CFG productions</context>
<context position="3257" citStr="Milward 1994" startWordPosition="465" endWordPosition="466">-word order constructs (Sgall et al. 1986) (Mel&apos;cuk 1988). Desirable properties of lexicalized formalisms (Schabes 1990), like finite ambiguity and decidability of string acceptance, intuitively hold for dependency syntax. On the contrary, the formal studies on dependency theories are rare in the literature. Gaifman (1965) showed that projective dependency grammars, expressed by dependency rules on syntactic categories, are weakly equivalent to context-free grammars. And, in fact, it is possible to devise 0(n3) parsers for this formalism (Lombardo, Lesmo 1996), or other projective variations (Milward 1994) (Eisner 1996). On the controlled relaxation of projective constraints, Nasr (1995) has introduced the condition of pseudoprojectivity, which provides some controlled looser constraints on arc crossing in a dependency tree, and has developed a polynomial parser based on a graph-structured stack. Neuhaus and Broker (1997) have recently showed that the general recognition problem for non-projective dependency grammars (what they call discontinuous DG) is NP-complete. They have devised a discontinuous DG with exclusively lexical categories (no traces, as most dependency theories do), and dealing </context>
</contexts>
<marker>Milward, 1994</marker>
<rawString>Milward D., Dynamic Dependency Grammar, Linguistics and Phylosophy, December 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Nasr</author>
</authors>
<title>A formalism and a parser for lexicalized dependency grammar,</title>
<date>1995</date>
<booktitle>Proc. IWPT 95,</booktitle>
<pages>186--195</pages>
<location>Prague,</location>
<contexts>
<context position="3340" citStr="Nasr (1995)" startWordPosition="476" endWordPosition="477">xicalized formalisms (Schabes 1990), like finite ambiguity and decidability of string acceptance, intuitively hold for dependency syntax. On the contrary, the formal studies on dependency theories are rare in the literature. Gaifman (1965) showed that projective dependency grammars, expressed by dependency rules on syntactic categories, are weakly equivalent to context-free grammars. And, in fact, it is possible to devise 0(n3) parsers for this formalism (Lombardo, Lesmo 1996), or other projective variations (Milward 1994) (Eisner 1996). On the controlled relaxation of projective constraints, Nasr (1995) has introduced the condition of pseudoprojectivity, which provides some controlled looser constraints on arc crossing in a dependency tree, and has developed a polynomial parser based on a graph-structured stack. Neuhaus and Broker (1997) have recently showed that the general recognition problem for non-projective dependency grammars (what they call discontinuous DG) is NP-complete. They have devised a discontinuous DG with exclusively lexical categories (no traces, as most dependency theories do), and dealing with free word order constructs through a looser subtree ordering. This formalism, </context>
</contexts>
<marker>Nasr, 1995</marker>
<rawString>Nasr A., A formalism and a parser for lexicalized dependency grammar, Proc. IWPT 95, Prague, 1995, 186-195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Neuhaus</author>
<author>N Broker</author>
</authors>
<title>The Complexity of Recognition of Linguistically Adequate Dependency Grammars,</title>
<date>1997</date>
<booktitle>Proc. ACL/EACL97,</booktitle>
<pages>337--343</pages>
<location>Madrid,</location>
<contexts>
<context position="3579" citStr="Neuhaus and Broker (1997)" startWordPosition="509" endWordPosition="512">Gaifman (1965) showed that projective dependency grammars, expressed by dependency rules on syntactic categories, are weakly equivalent to context-free grammars. And, in fact, it is possible to devise 0(n3) parsers for this formalism (Lombardo, Lesmo 1996), or other projective variations (Milward 1994) (Eisner 1996). On the controlled relaxation of projective constraints, Nasr (1995) has introduced the condition of pseudoprojectivity, which provides some controlled looser constraints on arc crossing in a dependency tree, and has developed a polynomial parser based on a graph-structured stack. Neuhaus and Broker (1997) have recently showed that the general recognition problem for non-projective dependency grammars (what they call discontinuous DG) is NP-complete. They have devised a discontinuous DG with exclusively lexical categories (no traces, as most dependency theories do), and dealing with free word order constructs through a looser subtree ordering. This formalism, considered as the most straightforward extension to a projective formalism, permits the reduction of the vertex cover problem to the dependency recognition problem, thus yielding the NP-completeness result. However, even if banned from the</context>
</contexts>
<marker>Neuhaus, Broker, 1997</marker>
<rawString>Neuhaus P., Broker N., The Complexity of Recognition of Linguistically Adequate Dependency Grammars, Proc. ACL/EACL97, Madrid, 1997, 337-343.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Neuhaus</author>
<author>U Hahn</author>
</authors>
<title>Restricted Parallelism in Object-Oriented Parsing,</title>
<date>1996</date>
<booktitle>Proc. COLING 96,</booktitle>
<pages>502--507</pages>
<location>Copenhagen,</location>
<marker>Neuhaus, Hahn, 1996</marker>
<rawString>Neuhaus P., Hahn U., Restricted Parallelism in Object-Oriented Parsing, Proc. COLING 96, Copenhagen, 1996, 502-507.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
</authors>
<title>A Formal Look at Dependency Grammars and Phrase-Structure Grammars,</title>
<date>1992</date>
<booktitle>with Special Consideration of Word-Order Phenomena, Int. Workshop on The Meaning-Text Theory, Darrnstadt,</booktitle>
<marker>Joshi, 1992</marker>
<rawString>Rambow 0., Joshi A., A Formal Look at Dependency Grammars and Phrase-Structure Grammars, with Special Consideration of Word-Order Phenomena, Int. Workshop on The Meaning-Text Theory, Darrnstadt, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Schabes</author>
</authors>
<title>Mathematical and Computational Aspects Of Lexicalized Grammars,</title>
<date>1990</date>
<tech>Ph.D. Dissertation MS-CIS-90-48,</tech>
<institution>Dept. of Computer and Information Science, University of Pennsylvania,</institution>
<location>Philadelphia (PA),</location>
<contexts>
<context position="2764" citStr="Schabes 1990" startWordPosition="396" endWordPosition="397"> syntactic structure of a sentence is described in terms of binary relations (dependency relations) on pairs of words, a head (parent), and a dependent (daughter), respectively; these relations usually form a tree, the dependency tree (fig. 1). The linguistic merits of dependency syntax have been widely debated (e.g. (Hudson 1990)). Dependency syntax is attractive because of the immediate mapping of dependency trees on the predicate-arguments structure and because of the treatment of free-word order constructs (Sgall et al. 1986) (Mel&apos;cuk 1988). Desirable properties of lexicalized formalisms (Schabes 1990), like finite ambiguity and decidability of string acceptance, intuitively hold for dependency syntax. On the contrary, the formal studies on dependency theories are rare in the literature. Gaifman (1965) showed that projective dependency grammars, expressed by dependency rules on syntactic categories, are weakly equivalent to context-free grammars. And, in fact, it is possible to devise 0(n3) parsers for this formalism (Lombardo, Lesmo 1996), or other projective variations (Milward 1994) (Eisner 1996). On the controlled relaxation of projective constraints, Nasr (1995) has introduced the cond</context>
<context position="12855" citStr="Schabes 1990" startWordPosition="2091" endWordPosition="2092">I likes sUBJ/ \c■BJ John el Figure 2. Dependency tree of the sentence &amp;quot;Beans I know John likes&amp;quot;, given the grammar GI . 789 from the grammar. The selection is carried out by matching the head of the rules with the words of the sentence. The second step follows Earley&apos;s phases on the dependency rules, together with the treatment of u-indices and u-triples. This off-line technique is not uncommon in lexicalized grammars, since each Earley&apos;s prediction would waste much computation time (a grammar factor) in the body of the algorithm, because dependency rules do not abstract over categories (cf. (Schabes 1990)). In order to recognize a sentence of n words, n+1 sets Si of items are built. An item represents a subtree of the total dependency tree associated with the sentence. An item is a 5-tuple &lt;Dotted-rule, Position, It-index, v-index, T-stack&gt;. Dotted-rule is a dependency rule with a dot between two d-quads of the d-quad sequence. Position is the input position where the parsing of the subtree represented by this item began (the leftmost position on the spanned string). ii-index and vindex are two integers that correspond to the indices of a word object in a derivation. T-stack is a stack of sets</context>
</contexts>
<marker>Schabes, 1990</marker>
<rawString>Schabes Y., Mathematical and Computational Aspects Of Lexicalized Grammars, Ph.D. Dissertation MS-CIS-90-48, Dept. of Computer and Information Science, University of Pennsylvania, Philadelphia (PA), August 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Sgall</author>
<author>E Haijcova</author>
<author>J Panevova</author>
</authors>
<title>The Meaning of Sentence in its Semantic and Pragmatic Aspects,</title>
<date>1986</date>
<location>Dordrecht Reidel Publ. Co., Dordrecht,</location>
<contexts>
<context position="908" citStr="Sgall et al. 1986" startWordPosition="126" endWordPosition="129">f providing a formal device for the dependency approach to syntax, and to link it with a parsing model. After reviewing the basic tenets of the paradigm and the few existing mathematical results, we describe a dependency formalism which is able to deal with long-distance dependencies. Finally, we present an Earley-style parser for the formalism and discuss the (polynomial) complexity results. 1. Introduction Many authors have developed dependency theories that cover cross-linguistically the most significant phenomena of natural language syntax: the approaches range from generative formalisms (Sgall et al. 1986), to lexically-based descriptions (Mel&apos;cuk 1988), to hierarchical organizations of linguistic knowledge (Hudson 1990) (Fraser, Hudson 1992), to constrained categorial grammars (Milward 1994). Also, a number of parsers have been developed for some dependency frameworks (Covington 1990) (Kwon, Yoon 1991) (Sleator, Temperley 1993) (Hahn et al. 1994) (Lombardo, Lesmo 1996), including a stochastic treatment (Eisner 1996) and an object-oriented parallel parsing method (Neuhaus, Hahn 1996). However, dependency theories have never been explicitly linked to formal models. Parsers and applications usual</context>
<context position="2686" citStr="Sgall et al. 1986" startWordPosition="385" endWordPosition="388"> the head stand on its left (resp. right). The basic idea of dependency is that the syntactic structure of a sentence is described in terms of binary relations (dependency relations) on pairs of words, a head (parent), and a dependent (daughter), respectively; these relations usually form a tree, the dependency tree (fig. 1). The linguistic merits of dependency syntax have been widely debated (e.g. (Hudson 1990)). Dependency syntax is attractive because of the immediate mapping of dependency trees on the predicate-arguments structure and because of the treatment of free-word order constructs (Sgall et al. 1986) (Mel&apos;cuk 1988). Desirable properties of lexicalized formalisms (Schabes 1990), like finite ambiguity and decidability of string acceptance, intuitively hold for dependency syntax. On the contrary, the formal studies on dependency theories are rare in the literature. Gaifman (1965) showed that projective dependency grammars, expressed by dependency rules on syntactic categories, are weakly equivalent to context-free grammars. And, in fact, it is possible to devise 0(n3) parsers for this formalism (Lombardo, Lesmo 1996), or other projective variations (Milward 1994) (Eisner 1996). On the contro</context>
</contexts>
<marker>Sgall, Haijcova, Panevova, 1986</marker>
<rawString>Sgall P., Haijcova E., Panevova J., The Meaning of Sentence in its Semantic and Pragmatic Aspects, Dordrecht Reidel Publ. Co., Dordrecht, 1986.</rawString>
</citation>
<citation valid="false">
<authors>
<author>D D Sleator</author>
<author>D Temperley</author>
</authors>
<note>Parsing English with</note>
<marker>Sleator, Temperley, </marker>
<rawString>Sleator D. D., Temperley D., Parsing English with</rawString>
</citation>
<citation valid="true">
<title>a Link Grammar,</title>
<date>1993</date>
<booktitle>Proc. of IWPT93,</booktitle>
<pages>277--291</pages>
<marker>1993</marker>
<rawString>a Link Grammar, Proc. of IWPT93, 1993, 277 -291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Schanker</author>
<author>D J Weir</author>
</authors>
<title>Parsing some constrained grammar formalisms,</title>
<date>1994</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<marker>Vijay-Schanker, Weir, 1994</marker>
<rawString>Vijay-Schanker K., Weir D. J., Parsing some constrained grammar formalisms, Computational Linguistics 19/4, 1994</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>