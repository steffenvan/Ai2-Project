<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004731">
<title confidence="0.982093">
Exploring Two Biomedical Text Genres for Disease Recognition
</title>
<author confidence="0.998873">
Aur6lie N6v6ol, Won Kim, W. John Wilbur, Zhiyong Lu*
</author>
<affiliation confidence="0.918302333333333">
National Center for Biotechnology Information
U.S. National Library of Medicine
Bethesda, MD 20894, USA
</affiliation>
<email confidence="0.984464">
{neveola,wonkim,wilbur,luzh}@ncbi.nlm.nih.gov
</email>
<sectionHeader confidence="0.995365" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999502846153846">
In the framework of contextual information
retrieval in the biomedical domain, this paper
reports on the automatic detection of disease
concepts in two genres of biomedical text:
sentences from the literature and PubMed user
queries. A statistical model and a Natural
Language Processing algorithm for disease
recognition were applied on both corpora.
While both methods show good performance
(F=77% vs. F=76%) on the sentence corpus,
results on the query corpus indicate that the
statistical model is more robust (F=74% vs.
F=70%).
</bodyText>
<sectionHeader confidence="0.998983" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99966478">
Contextual Information Retrieval (IR) is making
use of additional information or assumptions about
the users’ needs beyond the obvious intent of the
query. IR systems need to go beyond the task of
providing generally relevant information by assist-
ing users in finding information that is relevant to
them and their specific needs at the time of the
search. A practical example of a Google contextual
IR feature is when the search engine returns a map
showing restaurant locations to a user entering a
query such as “Paris restaurants.”
The contextual aspects of a user’s search were
defined for example by Saracevic (1997) who dis-
cussed integrating the cognitive, affective, and sit-
uational levels of human computer interaction in
IR systems. Other research efforts studied users’
search behavior based on their level of domain
knowledge (Zhang et al., 2005) or aimed at mod-
eling users’ interests and search habits (Rose and
Levinson, 2004; Teevan et al., 2005).
Information about the search context may be
sought explicitly from the user through profiling or
relevance feedback (Shen et al., 2005). Recent
work also exploited query log analysis and basic
computer environment information (Wen et al.
2004), which involve no explicit interaction with
the user. In adaptive information retrieval, context
information is inferred based on query analysis and
collection characteristics (Bai and Nie 2008).
In the biomedical domain, a need for contextual
information retrieval was identified in particular
for clinical queries submitted to PubMed (Pratt and
Wasserman, 2000). Building on the idea that a spe-
cific type of document is required for searches with
a “clinical” context, the PubMed Clinical Queries
portal was developed (Haynes and Wilczynski,
2004). A perhaps more prominent contextual fea-
ture of PubMed is the “citation sensor”, which
identifies queries classified by Rose and Levinson
as reflecting a “Navigational” or “Obtain resource”
goal. For example, the citation sensor will identify
and retrieve a specific citation if the user enters the
article title as the query. The analysis of Entrez
logs shows that MEDLINE is the most popular
database among the 30 or so databases maintained
by the National Center for Biotechnology Informa-
tion (NCBI) as it receives most of Entrez traffic.
This suggests that there is a need to complement
the information retrieved from MEDLINE by giv-
ing contextual access to other NCBI resources re-
</bodyText>
<page confidence="0.979173">
144
</page>
<note confidence="0.9060725">
Proceedings of the Workshop on BioNLP, pages 144–152,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999936545454546">
levant to users’ queries, such as Entrez Gene, Clin-
ical Q&amp;A or BookShelf. In addition, the NLM es-
timated that about 1/3 of PubMed users are not
biomedical professionals. In this light, providing
an access point to consumer information such as
the Genetics Home Reference might also be useful.
To achieve this, the sensor project was recently
launched with the goal of recognizing a variety of
biomedical concepts (e.g. gene, protein and drug
names) in PubMed queries. These high-level con-
cepts will help characterize users’ search context in
order to provide them with information related to
their need beyond PubMed. For instance, if a user
query contains the drug name “Lipitor”, it will be
recognized by the drug sensor and additional in-
formation on this drug from Clinical Q&amp;A will be
shown in the side bar in addition to default
PubMed results. Since disease names are common
in PubMed queries, the goal of this work is to in-
vestigate and benchmark computational techniques
for automatic disease name recognition as an aid to
implementing PubMed search contexts.
</bodyText>
<sectionHeader confidence="0.999748" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99944125">
Despite a significant body of literature in biomedi-
cal named entity recognition, most work has been
focused on gene, protein, drug and chemical names
through challenges such as BioCreAtIvE1 or the
TREC Genomics/Chemical tracks (Park and Kim,
2006). Other work addressed the identification of
“medical problems” in clinical text (Aronson et al.
2007; Meystre and Haug, 2005). This task was the
topic of a Medical NLP challenge2, which released
a corpus of anonymized radiography reports anno-
tated with ICD9 codes. Although there is some
interest in the biomedical community in the identi-
fication of disease names and more specifically the
identification of relationships between diseases and
genes or proteins (Rindflesh and Fizman, 2003),
there are very few resources available to train or
evaluate automatic disease recognition systems. To
the best of our knowledge, the only publicly avail-
able corpus for disease identification in the litera-
ture was developed by Jimeno et al. (2008). The
authors annotated 551 MEDLINE sentences with
UMLS concepts and used this dataset to bench-
mark three different automatic methods for disease
name recognition. A MEDLINE corpus annotated
</bodyText>
<footnote confidence="0.998115">
1 http://biocreative.sourceforge.net/
2 http://www.computationalmedicine.org/challenge/index.php
</footnote>
<bodyText confidence="0.99983512">
with “malignancy” mentions and part-of-speech
tags is also available (Jin et al. 2006). This corpus
is targeted to a very restricted type of diseases. The
annotations are also domain specific, so that “can-
cer of the lung” is not considered a malignancy
mention but a mention of malignancy and a men-
tion of malignancy location.
As in previous studies, we aim to investigate the
complexity of automatic disease recognition using
state-of-the-art computational techniques. This
work is novel in at least three aspects: first, in ad-
dition to using the MEDLINE sentence corpus
(Jimeno et al 2008), we developed a new corpus
comprising disease annotations on 500 randomly
selected PubMed queries. This allowed us to inves-
tigate the influence of local context3 through the
comparison of system performance between two
different genres of biomedical text. Second, by
using a knowledge based tool previously ben-
chmarked on the same MEDLINE corpus (Jimeno
et al. 2008), we show that significant performance
differences can be observed when parameters are
adjusted. Finally, a state-of-the-art statistical ap-
proach was adapted for disease name recognition
and evaluated on both corpora.
</bodyText>
<sectionHeader confidence="0.9606025" genericHeader="method">
3 Two Biomedical Corpora with disease
annotations
</sectionHeader>
<bodyText confidence="0.998925263157895">
The first issue in the development of such a corpus
is to define the very concept of disease. Among the
numerous terminological resources available, such
as Medical Subject Headings (MeSH®, 4,354 dis-
ease concepts) or the International Classification of
Diseases (ICD9, ~18,000 disease concepts), the
UMLS Metathesaurus® is the most comprehensive:
the 2008AB release includes 252,284 concepts in
the disorder Semantic Group defined by McCray
et al. (2001). The UMLS Metathesaurus is part of
the Semantic Network, which also includes a set of
broad subject categories, or Semantic Types, that
provide a consistent categorization of all concepts
represented in the Metathesaurus. The Semantic
Groups aim at providing an even broader categori-
zation for UMLS concepts. For example, the dis-
order Semantic Group comprises 12 Semantic
Types including Disease or Syndrome, Cell or Mo-
lecular Dysfunction and Congenital Abnormalities.
</bodyText>
<footnote confidence="0.980695">
3 Here, by context, we mean the information surrounding a
disease mention available in the corpora. This is different from
the “search context” previously discussed.
</footnote>
<page confidence="0.996409">
145
</page>
<listItem confidence="0.860817230769231">
Furthermore, like the gene mention (Morgan et
al. 2008) and gene normalization (Smith et al.
2008) tasks in BioCreative II, the task of disease
name recognition can also be performed at two
different levels:
1. disease mention: the detection of a snippet
of text that refers to a disease concept (e.g.
“alzheimer” in the sample query shown in
Table 2)
2. disease concept: the recognition of a con-
trolled vocabulary disease concept (e.g.
“C0002395-alzheimer’s disease” in our Ta-
ble 2 example) in text.
</listItem>
<bodyText confidence="0.9854255">
In this work, we evaluate and report system per-
formance at the concept level.
</bodyText>
<subsectionHeader confidence="0.999899">
3.1 Biomedical literature corpus
</subsectionHeader>
<bodyText confidence="0.9767425">
Sentence Kniest dysplasia is a moderately
severe chondrodysplasia pheno-
type that results from mutations
in the gene for type ii collagen
col2a1.
Annotations C0265279-Kniest dysplasia
C0343284-Chondrodysplasia,
unspecified
</bodyText>
<tableCaption confidence="0.997621">
Table 1: Excerpt of literature corpus (PMID: 7874117)
</tableCaption>
<bodyText confidence="0.999991222222222">
The corpus made available by Jimeno et al. con-
sists of 551 MEDLINE sentences annotated with
UMLS concepts or concept clusters: concepts that
were found to be linked to the same term. For ex-
ample, the concepts “Pancreatic carcinoma”
(C0235974) and “Malignant neoplasm of pan-
creas” (C0346647) share the same synonym “Pan-
creas Cancer”, thus they were clustered. The
sentences were selected from a set of articles cu-
rated for Online Mendelian Inheritance in Man
(OMIM) and contain an average of 27(+/- 11) to-
kens, where tokens are defined as sequences of
characters separated by white space. A set of
UMLS concepts (or clusters) is associated with
each sentence in the corpus. However, no boun-
dary information linking a phrase in a sentence to
an annotation was available. Table 1 shows a sam-
ple sentence and its annotations.
</bodyText>
<subsectionHeader confidence="0.999953">
3.2 Biomedical query corpus
</subsectionHeader>
<bodyText confidence="0.999914136363636">
A total of 500 PubMed queries were randomly se-
lected and divided into two batches of 300 and 200
queries, respectively. Queries were on average
3.45(+/- 2.64) tokens long in the 300 query batch
and 3.58(+/- 4.63) for the 200 query batch, which
is consistent with the average length of PubMed
queries (3 tokens) reported by Herskovic et al.
(2007).
The queries in the first set were annotated using
Knowtator (Ogren, 2006) by three annotators with
different backgrounds (one biologist, one informa-
tion scientist, one computational linguist). Two
annotators annotated the queries using UMLS con-
cepts from the disorder group, while the other an-
notator simply annotated diseases without
reference to UMLS concepts. Table 2 shows a
sample query and its annotations. A consensus set
was obtained after a meeting between the annota-
tors where diverging annotations were discussed
and annotators agreed on a final, unique, version of
all annotations. The consensus set contains 89 dis-
ease concepts (76 unique).
</bodyText>
<table confidence="0.919026">
Query alzheimer csf amyloid
Annotations Ann. 1: “alzheimer”; 0-8;
Ann. 2, 3: “alzheimer”; 0-8;
C0002395-alzheimer’s disease
</table>
<tableCaption confidence="0.705030333333333">
Table 2: Excerpt of annotated 300-query corpus. Boun-
dary information is given as the character interval of the
annotated string in the query (here, 0-8).
</tableCaption>
<bodyText confidence="0.9998948">
The queries in the second set were annotated
with UMLS concepts from the disorder group by
one of the annotators who also worked on the pre-
vious set. In this set, 53 disease concepts were an-
notated (51 unique).
</bodyText>
<sectionHeader confidence="0.953942" genericHeader="method">
4 Automatic disease recognition
</sectionHeader>
<bodyText confidence="0.999928555555555">
With the perspective of a contextual IR applica-
tion where the disease concepts found in queries
will be used to refer users to disease-specific in-
formation in databases other than MEDLINE, we
are concerned with high precision performance.
For this reason, we decided to experiment with
methods that showed the highest precision when
compared to others. In addition, given the size of
the corpora available and the type of the annota-
</bodyText>
<page confidence="0.994906">
146
</page>
<bodyText confidence="0.88313175">
tions, machine learning methods such as CRFs or
SVM did not seem applicable.
Table 3 shows a description of the training and
test sets for each corpus.
</bodyText>
<table confidence="0.9990592">
Data Lit. Corpus Query Corpus
Training 276 sentences 300 queries (89
(487 disease con- disease concepts,
cepts, 185 unique) 76 unique)
Testing 275 sentences 200 queries (53
(437 disease con- disease concepts,
cepts, 185 unique) 51 unique)
All 551 sentences 500 queries (142
(924 disease con- disease concepts,
cepts, 280 unique) 120 unique)
</table>
<tableCaption confidence="0.999847">
Table 3: Description of the training and test sets
</tableCaption>
<subsectionHeader confidence="0.99528">
4.1 Natural Language Processing
</subsectionHeader>
<bodyText confidence="0.998019">
Disease recognition was performed using the Natu-
ral Language Processing algorithm implemented in
MetaMap (Aronson, 2001)4. The tool was re-
stricted to retrieve concepts from the disorder
group, using the UMLS 2008AB release and
“longest match” feature.
In practice, MetaMap parses the input text into
noun phrases, generates variants of these phrases
using knowledge sources such as the SPECIALIST
lexicon, and maps the phrases to UMLS concepts.
</bodyText>
<subsectionHeader confidence="0.956323">
4.2 Priority Model
</subsectionHeader>
<bodyText confidence="0.999972125">
The priority model was first introduced in (Tanabe
and Wilbur, 2006) and is adapted here to detect
disease mentions in free text. Because our evalua-
tion is performed at the concept level, the mentions
extracted by the model are then mapped to UMLS
using MetaMap.
The priority model approach is based on two sets
of phrases: one names of diseases, D, and one
names of non-diseases, N. One trains the model to
assign two numbers, p and q, to each token t that
appears in a phrase in either D or N. Roughly, p is
the probability that a phrase from D or N that has
the token t in it is actually from D and q is the rela-
tive weight that should be assigned to t for this
purpose and represents a quality estimate. Given a
phrase
</bodyText>
<footnote confidence="0.508838">
4 Additional information is also available at
http://metamap.nlm.nih.gov/
</footnote>
<bodyText confidence="0.996715564102564">
and for each the corresponding numbers and
q; we estimate the probability that by
The training procedure for the model actually
chooses the values of all the and quantities to
optimize the values over all of D and N.
For this work we have extended the approach to
include a quantity
which represents a weighted average of all the
quality numbers . We apply this formula to ob-
tain as long as If we
replace all numbers by in (2) and (3) to
obtainqual .
For this application we obtained the sets D and
N from the SEMCAT data (Tanabe, Thom et al.
2006) supplemented with the latest UMLS data.
We removed any term from D and N that contained
less than five characters in order to decrease the
occurrence of ambiguous terms. Also the 1,000
most frequent terms from D were examined ma-
nually and the ambiguous ones were removed. The
end result is a set of 332,984 phrases in D and
4,253,758 phrases in N. We trained the priority
model on D and N and applied the resulting train-
ing to compute for each phrase in D and N a vector
of values . In this way D and N are
converted to and . We then constructed a
Mahalanobis classifier (Duda, Hart and Stork,
2001) for two dimensional vectors as the differ-
ence in the Mahalanobis distance of any such vec-
tor to Gaussian approximations to and . We
refer to the number produced by this classifier as
the Mahalanobis score. By randomly dividing both
D and N into three equal size pieces and training
on two from each and testing on the third, in a
three-fold cross validation we found the Mahala-
nobis classifier to perform at 98.4% average preci-
sion and 93.9% precision-recall breakeven point.
In a final step we applied a simple regression me-
thod to estimate the probability that a given Maha-
</bodyText>
<page confidence="0.992548">
147
</page>
<bodyText confidence="0.982421784313726">
lanobis score was produced by a phrase belonging
to D and not N. Given a phrase phr we will denote
this final probability produced as PMA(phr).
The second important ingredient of our statistic-
al process is how we produce phrases from a piece
of text. Given a string of text TX we apply tokeni-
zation to TX to produce an ordered set of tokens
t1,t2,...,tn . Among the tokens produced will be
punctuation marks and stop words and we denote
the set of all such tokens by Z . We call a token
segment maximal if it contains no ele-
ment of and if either or and
likewise if or . Given text TX we
will denote the set of all maximal token segments
produced in this way by Now given a
maximal token segment mts= tj,...,tk we define
two different methods of finding phrases in mts.
The first assumes we are given an arbitrary set of
phrases PH. We recursively define a set of phrases
I mts,PH beginning with this set empty and
with the parameter . Each iteration consists
of asking for the largest for which
. If there is such a we add
tu,...,ty to and set .
Otherwise we set . We repeat this process
as long as . The second approach assumes
we are given an arbitrary set of two token phrases
P2. Again we recursively define a set of phrases
J mts, P2 beginning with this set empty and
with the parameter . Each iteration consists
of asking for the largest for which given any
, . If there is such av
we add to and set
. Otherwise we set . We repeat
this process as long as .
In order to apply our phrase extraction proce-
dures we need good sets of phrases. In addition to
D and N already defined above, we use another set
of phrases defined as follows. Let R denote the set
of all token strings with two or more tokens which
do not contain tokens from Z and for which there
are at least three MEDLINE records (title and ab-
stract text only) in which the token string is re-
peated at least twice.
We then define R R D N . We make
use of in addition to D and N. For the setP2
we take the set of all two token phrases in
MEDLINE documents for which the two tokens
co-occur as this phrase much more than expected,
i.e., with a (based on the two-by-two
contingency table).
</bodyText>
<figureCaption confidence="0.999009">
Figure 1: Phrase finding algorithm
</figureCaption>
<bodyText confidence="0.9999505">
With these preliminaries, our phrase finding al-
gorithm in pseudo-code is shown in Figure 1.
The output of this algorithm may then be filtered
by setting a threshold on the PMA values to accept.
</bodyText>
<sectionHeader confidence="0.999984" genericHeader="method">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.999551">
5.1 Assessing the difficulty of the task
</subsectionHeader>
<bodyText confidence="0.999889785714286">
To assess the difficulty of disease recognition, we
computed the inter-annotator agreement (IAA) on
the 300-query corpus. Agreement was computed at
the disease mention level for all three annotators
and at the disease concept level for the two annota-
tors who produced UMLS annotations.
Inter-annotator agreement measures for NLP
applications have been recently discussed by
Artstein and Poesio (2008) who advocate for the
use of chance corrected measures. However, in our
case, agreement was partly computed on a very
large set of categories (UMLS concepts) so we
decided to use Knowtator’s built-in feature, which
computes IAA as the percentage of agreement and
</bodyText>
<figure confidence="0.998272181818182">
#Initialization: Given a text TX, set and
#Processing: While( ){
I. select
II. If( mts, D )
else if( mts,R )
else if( mts,N )
else
if( mts, P2 )
else
#Return: All pairs
}
</figure>
<page confidence="0.988271">
148
</page>
<bodyText confidence="0.999732916666667">
allows partial string matches. For example, in the
query “dog model transient ischemic attacks”, an-
notator 1 selected “ischemic attacks” as a disorder
while annotator 2 and 3 selected “transient ischem-
ic attacks” as UMLS concept C0007787: Attacks,
Transient Ischemic. In this case, at the subclass
level (“disorder”) we have a match for this annota-
tion. But at the exact span or exact category level,
there is no match. Table 4 shows details of IAA at
the disease mention level when partial matches are
taken into account. For exact span matches, the
IAA is lower, at 64.87% on average.
</bodyText>
<table confidence="0.99262275">
Disorder IAA Ann. 1 Ann. 2 Ann. 3
Ann. 1 100% 71.77% 75.86%
Ann. 2 100% 71.68%
Ann. 3 100%
</table>
<tableCaption confidence="0.9917625">
Table 4: Agreement on disease mention annotations
(partial match allowed) – average is 73.10%
</tableCaption>
<bodyText confidence="0.999907625">
At the concept level, the agreement (when par-
tial matches were allowed) varied significantly
depending on the semantic types. It ranged be-
tween 33% for Findings and 83% for Mental or
Behavioral Dysfunction. However, agreement on
the most frequent category, Disease or Syndrome,
was 72%, which is close to the annotators’ overall
agreement at the mention level. One major cause
of disagreement was ambiguity caused by concepts
that were clustered by Jimeno et al. For example,
in query “osteoporosis and “fracture pattern”, an-
notator 2 marked “osteoporosis” with both
“C0029456-osteoporosis”(a Disease or Syndrome
concept) and “C1962963-osteoporosis adverse
event”(a Finding concept) while annotator 3 only
used “C0029456-osteoporosis”.
</bodyText>
<subsectionHeader confidence="0.98326">
5.2 Results on Literature corpus
</subsectionHeader>
<bodyText confidence="0.9998336">
As shown in Table 3, the corpus was randomly
split into a training set (276 sentences) and a test
set (275 sentences). The training set was used to
determine the optimal probability threshold for the
Priority Model and parameter selection for Meta-
Map, respectively.
Priority Model parameter adjustments: the first
result observed from applying the Priority Model
was that D yielded about 90% of the output of the
algorithm. Also results coming from andP2
were not well mapped to UMLS concepts by Me-
taMap. As a result, in this work we ignored disease
candidates retrieved based on R and . The best
F-measure was obtained for a threshold of 0.3,
which was consequently used on the test set.
Since the Priority Model algorithm does not per-
form any mapping to a controlled vocabulary
source, the mapping was performed by applying
MetaMap to the snippets of text returned with a
probability value above the threshold.
</bodyText>
<table confidence="0.998536">
Threshold P R F
0 64 73 67
.1 67 73 70
.2 67 73 70
.3 68 73 71
.4 68 73 70
.5 68 72 69
.6 68 72 69
.7 68 72 69
.8 68 68 68
.9 65 60 62
</table>
<tableCaption confidence="0.900932333333333">
Table 5: Precision (P), Recall (R) and F-measure of the
Priority Model on the training set for different values of
the probability threshold.
</tableCaption>
<bodyText confidence="0.997945217391304">
The results presented in Table 5 were obtained
before any MetaMap adjustments were made.
MetaMap parameter adjustments: an error anal-
ysis was performed to adjust MetaMap settings.
Errors fell into the following categories:
A more specific disease should have been
recognized (e.g. “deficiency” vs. “C2 defi-
ciency”)
The definition of a cluster was lacking
(e.g. “G6PD deficiency” comprised
C0237987- Glucose-6-phosphate dehydro-
genase deficiency anemia and C0017758-
Glucosphosphate Dehydrogenase Defi-
ciency but not C0017920- Deficiency of
glucose-6-phosphatase)
MetaMap mapping was erroneous (e.g.
“hereditary breast” was mapped to
C0729233-Dissecting aneurysm of the
thoracic aorta instead of C0346153-
Hereditary Breast Cancer)
The results of inter-annotator agreement and fur-
ther study of MetaMap mappings indicated that
concepts with the semantic type Findings seemed
</bodyText>
<page confidence="0.997932">
149
</page>
<bodyText confidence="0.998652666666667">
to be frequently retrieved erroneously. For this rea-
son, we also experimented not taking Findings into
account as an additional adjustment for MetaMap.
Table 6 shows the results of applying the MetaMap
adjustments yielded from the error analysis on the
training corpus.
</bodyText>
<table confidence="0.997635">
Threshold Findings P R F
.3 Yes 80 78 79
.3 No 85 78 81
</table>
<tableCaption confidence="0.996495666666667">
Table 6: performance of the Priority Model on the train-
ing set for threshold .3 depending on whether mappings
to Findings are used in the “adjustments”
</tableCaption>
<bodyText confidence="0.99825825">
MetaMap disorder detection was also performed
directly on the training corpus. An error analysis
similar to what was presented above was carried
out to determine the best parameters. Table 7 be-
low shows the results obtained when all concepts
from the 12 Semantic Types (STs) in the disorder
group are taken into account with no adjustments
(“raw”). Then, results including the adjustments
from the error analysis are shown when all 12 STs
are taken into account, when Findings are excluded
(11STs) and when only the most frequent 6STs in
the training set are taken into account.
</bodyText>
<table confidence="0.9997602">
Processing P R F
Raw (12 STs) 50 77 61
Adjusted (12 STs) 52 75 61
Adjusted (11 STs) 57 73 64
Adjusted (6 STs) 77 72 74
</table>
<tableCaption confidence="0.999767">
Table 7: Performance of MetaMap on the training set
</tableCaption>
<bodyText confidence="0.985488666666667">
Finally, Table 8 shows the performance of both
methods on the test set, using the optimal settings
determined on the training set:
</bodyText>
<table confidence="0.99965">
Method P R F
Priority Model 80 74 77
MetaMap 75 78 76
</table>
<tableCaption confidence="0.920035">
Table 8: Precision (P), Recall (R) and F-measure of the
Priority Model and MetaMap on the test set
</tableCaption>
<subsectionHeader confidence="0.975485">
5.3 Results on Query Corpus
</subsectionHeader>
<bodyText confidence="0.997671758620689">
The 300-query corpus was used as a training set
and the 200-query corpus was used as a test set.
For consistency with work on the literature corpus,
we assessed the disease recognition on a gold stan-
dard set including “clusters” of UMLS concepts
were appropriate. As previously with the Literature
corpus, we used the training set to determine the
best settings for each method. The performance of
the Priority Model at different values of the proba-
bility threshold, based on the use of D and N as the
sets of sample phrases is similar to that obtained
with the literature corpus; 0.3 stands out as one of
the three values for which the best F-measure is
obtained (tied with .5 and .8).
Because of the brevity of queries vs. sentences,
the MetaMap error analysis was very succinct and
resulted in:
Removal of C0011860-Diabetes mellitus
type 2 as mapping for “diabetes”
Removal of all occurrences of C0600688-
Toxicity and C0424653-Weight symptom
(finding)
Adjustment on the number of STs taken in-
to account
The difference in performance obtained on the
training set for the different MetaMap adjustments
considered is shown in Table 9 when MetaMap
was applied to Priority Model output and in Table
10 when it was applied directly on the queries.
</bodyText>
<table confidence="0.995363666666667">
Threshold Findings P R F
.3 Yes 60 72 65
.3 No 73 70 71
</table>
<tableCaption confidence="0.995992333333333">
Table 9: performance of the Priority Model on the train-
ing set for threshold .3 depending on whether mappings
to Findings are used in the “adjustments”
</tableCaption>
<table confidence="0.9998794">
Processing P R F
Raw (12 STs) 41 82 55
Adjusted (12 STs) 44 82 57
Adjusted (11 STs) 58 81 68
Adjusted (6 STs) 64 75 69
</table>
<tableCaption confidence="0.999454">
Table 10: performance of MetaMap on the training set
</tableCaption>
<bodyText confidence="0.988967">
Finally, Table 11 shows the performance of both
methods on the test set, using the optimal settings
determined on the training set:
</bodyText>
<table confidence="0.999430666666667">
Method P R F
Priority Model 76 72 74
MetaMap 66 74 70
</table>
<tableCaption confidence="0.903464">
Table 11: Precision (P), Recall (R) and F-measure of
the Priority Model and MetaMap on the test set
</tableCaption>
<page confidence="0.997935">
150
</page>
<sectionHeader confidence="0.999568" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999966042253521">
Comparing the Two Methods. The performance
of both methods on the query corpus is comparable
to inter-annotator agreement (F=70-74 vs. IAA=72
on Disease and Syndromes). On both corpora, the
Priority Model achieves higher precision and F-
measure, while MetaMap achieves better recall.
Comparing the results obtained with MetaMap
with those reported by Jimeno et al., precision is
lower, but recall is much higher. This is likely to
be due to the different MetaMap settings, and the
use of different UMLS versions - Jimeno et al. did
not provide any of this information, but based on
the publication date of their paper, it is likely that
they used one of the 2006 UMLS releases. Meystre
and Haug (2006) also found that significant per-
formance differences could be obtained with Me-
taMap by adjusting the content of the knowledge
sources used.
On both text genres, 0.3 was found to be the op-
timal probability threshold for the Priority Model.
Based on the performance at different values of the
threshold, it seems that the model is quite efficient
at ruling out highly unlikely diseases. However, for
values above .3 the performance does not vary
greatly.
Comparing Text Genres. For both methods,
disease recognition seems more efficient on sen-
tences. This is to be expected: sentences provide
more context (e.g. more tokens surrounding the
disease mention are available) and allow for more
efficient disambiguation, for example on acro-
nyms. Although acronyms are frequent both in
queries and sentences, more undefined acronyms
are found in queries. However, the difference in
performance between the two methods seems
higher on the query corpus. This indicates that the
Priority Model could be more robust to sparse con-
text.
It should be noted that there were diseases in all
sentences in the literature corpus vs. about 1/3 to
1/2 of the queries. In addition, the query corpus
included many author names, which could create
confusion with disease names (in particular for the
Priority Model). This difficulty was not found in
the sentence corpus. However, sentences some-
times contain negated mention of diseases, which
never occurred in the query corpus where little to
no syntax is used.
We also noticed that while Findings seemed to
be generally problematic concepts in both corpora,
other concepts such as Injury and Poisoning were
much more prevalent in the query corpus. For this
reason, for the general task of disease recognition,
a drastic restriction to as little as 6 STs is probably
not advisable.
Limitations of the study. One limitation of our
study is the relatively small number of disease
concepts in the query corpus. Although the query
and sentence corpus contain about 500 que-
ries/sentences each, there are significantly less dis-
ease concepts found in queries compared to
sentences. As a result, there is also less repetition
in the disease concept found. This is partly due to
the brevity of queries compared to sentences but
mainly to the fact that while all the sentences in the
literature corpus had at least one disease concept,
this was not the case for the query corpus. We are
currently addressing this issue with the ongoing
development of a large scale query corpus anno-
tated for diseases and other relevant biomedical
entities.
</bodyText>
<sectionHeader confidence="0.999209" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999685">
We found that of the two steps of disease recogni-
tion, disease mention gets the higher inter-
annotator agreement (vs. concept mapping). We
have applied a statistical and an NLP method for
the automatic recognition of disease concepts in
two genres of biomedical text. While both methods
show good performance (F=77% vs. F=76%) on
the sentence corpus, results indicate that the statis-
tical model is more robust on the query corpus
where very little disease context information is
available (F=74% vs. F=70%). As a result, the
priority model will be used for disease detection in
PubMed queries in order to characterize users’
search contexts for contextual IR.
</bodyText>
<sectionHeader confidence="0.998296" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999502714285714">
This research was supported by the Intramural Re-
search Program of the NIH, National Library of
Medicine. The authors would like to thank S.
Shooshan and T. Tao for their contribution to the
annotation of the query corpus; colleagues in the
NCBI engineering branch for their valuable feed-
back at every step of the project.
</bodyText>
<page confidence="0.997851">
151
</page>
<sectionHeader confidence="0.989643" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999776754901961">
Alan R. Aronson, Olivier Bodenreider, Dina Demner-
Fushman, Kin Wah Fung, Vivan E. Lee, James G.
Mork et al. 2007. From Indexing the Biomedical Li-
terature to Coding Clinical Text: Experience with
MTI and Machine Learning Approaches. ACL
Workshop BioNLP.
Alan Aronson. 2001. Effective mapping of biomedical
text to the UMLS Metathesaurus: the MetaMap pro-
gram. Proceedings of AMIA Symp:17-21.
Ron Artstein and Massimo Poesio. 2008. Inter-Coder
Agreement for Computational Linguistics. Compu-
tational Linguistics 34(4): 555-596
Jing Bai, and Jian-Yun Nie. 2008. Adapting information
retrieval to query contexts. Information Processing &amp;
Management. 44(6):1902-22
Robert O. Duda, Peter. E. Hart and David G. Stork.
2001. Pattern Classification. New York: John Wiley
&amp; Sons, Inc.
R. Brian Haynes and Nancy L. Wilczynski. 2004. Op-
timal search strategies for retrieving scientifically
strong studies of diagnosis from Medline: analytical
survey. BMJ. 328(7447):1040.
Jorge R. Herskovic, Len Y. Tanaka, William Hersh and
Elmer V. Bernstam. 2007. A day in the life of
PubMed: analysis of a typical day&apos;s query log. Jour-
nal of the American Medical Informatics Association.
14(2):212-20.
Antonio Jimeno, Ernesto Jimenez-Ruiz, Vivian Lee,
Sylvain Gaudan, Rafael Berlanga and Dietrich
Rebholz-Schuhmann. 2008. Assessment of disease
named entity recognition on a corpus of annotated
sentences. BMC Bioinformatics. 11;9 Suppl 3:S3.
Yang Jin, Ryan T McDonald, Kevin Lerman, Mark A
Mandel, Steven Carroll, Mark Y Liberman et al.
2006. Automated recognition of malignancy men-
tions in biomedical literature. BMC Bioinformatics.
7:492.
Alexa T. McCray, Anita Burgun and Olivier Bodenreid-
er. 2001. Aggregating UMLS semantic types for
reducing conceptual complexity. Proceedings of
Medinfo 10(Pt 1):216-20.
Stéphane Meystre and Peter J. Haug. 2006. Natural lan-
guage processing to extract medical problems from
electronic clinical documents: performance evalua-
tion. J Biomed Inform. 39(6):589-99.
Alexander A. Morgan, Zhiyong Lu, Xinglong Wang,
Aaron M. Cohen, Juliane Fluck, Patrick Ruch et al.
2008. Overview of BioCreative II gene normaliza-
tion. Genome Biol. 9 Suppl 2:S3.
Phillip V. Ogren. 2006. Knowtator: A plug-in for creat-
ing training and evaluation data sets for Biomedical
Natural Language systems. 9th Intl. Protégé Confe-
rence
Jong C. Park and Jung-Jae Kim. 2006. Named Entity
Recognition. In S. Ananiadou and J. McNaught
(Eds), Text Mining for Biology and Biomedicine (pp.
121-42). Boston|London:Artech House Inc.
Wanda Pratt and Henry Wasserman. 2000. QueryCat:
automatic categorization of MEDLINE queries. Pro-
ceedings of AMIA Symp:655-9.
Tom C. Rindflesh and Marcelo Fiszman. 2003. The
interaction of domain knowledge and linguistic struc-
ture in natural language processing: interpreting
hypernymic propositions in biomedical text. J Bio-
med Inform. 36(6):462-77
Daniel E. Rose and Danny Levinson. 2004. Understand-
ing user goals in web search. In Proceedings of the
13th international Conference on World Wide
Web:13-9
Tefko Saracevic. 1997. The Stratified Model of Infor-
mation Retrieval Interaction: Extension and Applica-
tion. Proceedings of the 60th meeting of the.
American Society for Information Science:313-27
Xuehua Shen, Bin Tan and ChengXiang Zhai. 2005
Context-sensitive information retrieval using impli-
cit feedback, In Proceedings of the 28th annual in-
ternational conference ACM SIGIR conference on
Research and development in information retrieval:
43-50.
Larry Smith, Laurraine K. Tanabe, Rie J. Ando, Cheng-
Ju Kuo, I-Fang Chung , Chun-Nan Hsu et al. 2008.
Overview of BioCreative II gene mention recogni-
tion. Genome Biol. 9 Suppl 2:S2.
Laurraine K. Tanabe, Lynn. H. Thom, Wayne Matten,
Donald C. Comeau and W. John Wilbur. 2006.
SemCat: semantically categorized entities for ge-
nomics. Proceedings of AMIA Symp: 754-8.
Laurraine K. Tanabe and W. John Wilbur. 2006. A
Priority Model for Named Entities. Proceedings of
HLT-NAACL BioNLP Workshop:33-40
Jaime Teevan, Susan T. Dumais and Eric Horvitz. 2005.
Personalizing search via automated analysis of in-
terests and activities. In Proceeding of ACM-
SIGIR’05:449–56.
Ji-Rong Wen, Ni Lao, Wei-Ying Ma. 2004. Probabilis-
tic model for contextual retrieval. Proceedings of
ACM-SIGIR’04:57–63
Xiangmin Zhang, Hermina G.B. Anghelescu and Xiao-
jun Yuan. 2005. Domain knowledge, search beha-
vior, and search effectiveness of engineering and
science students: An exploratory study, Information
Research 10(2): 217.
</reference>
<page confidence="0.998128">
152
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.152765">
<title confidence="0.984485">Exploring Two Biomedical Text Genres for Disease Recognition</title>
<author confidence="0.523596">Aurlie Nvol</author>
<author confidence="0.523596">Won Kim</author>
<author confidence="0.523596">W John Wilbur</author>
<author confidence="0.523596">Zhiyong Lu</author>
<affiliation confidence="0.661441">National Center for Biotechnology U.S. National Library of</affiliation>
<address confidence="0.984111">Bethesda, MD 20894,</address>
<email confidence="0.996786">neveola@ncbi.nlm.nih.gov</email>
<email confidence="0.996786">wonkim@ncbi.nlm.nih.gov</email>
<email confidence="0.996786">wilbur@ncbi.nlm.nih.gov</email>
<email confidence="0.996786">luzh@ncbi.nlm.nih.gov</email>
<abstract confidence="0.9461605">In the framework of contextual information retrieval in the biomedical domain, this paper reports on the automatic detection of disease concepts in two genres of biomedical text: sentences from the literature and PubMed user queries. A statistical model and a Natural Language Processing algorithm for disease recognition were applied on both corpora. While both methods show good performance on the sentence corpus, results on the query corpus indicate that the model is more robust (F=74% F=70%).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alan R Aronson</author>
<author>Olivier Bodenreider</author>
<author>Dina DemnerFushman</author>
<author>Kin Wah Fung</author>
<author>Vivan E Lee</author>
<author>James G Mork</author>
</authors>
<date>2007</date>
<booktitle>From Indexing the Biomedical Literature to Coding Clinical Text: Experience with MTI and Machine Learning Approaches. ACL Workshop BioNLP.</booktitle>
<contexts>
<context position="4830" citStr="Aronson et al. 2007" startWordPosition="747" endWordPosition="750">n addition to default PubMed results. Since disease names are common in PubMed queries, the goal of this work is to investigate and benchmark computational techniques for automatic disease name recognition as an aid to implementing PubMed search contexts. 2 Related Work Despite a significant body of literature in biomedical named entity recognition, most work has been focused on gene, protein, drug and chemical names through challenges such as BioCreAtIvE1 or the TREC Genomics/Chemical tracks (Park and Kim, 2006). Other work addressed the identification of “medical problems” in clinical text (Aronson et al. 2007; Meystre and Haug, 2005). This task was the topic of a Medical NLP challenge2, which released a corpus of anonymized radiography reports annotated with ICD9 codes. Although there is some interest in the biomedical community in the identification of disease names and more specifically the identification of relationships between diseases and genes or proteins (Rindflesh and Fizman, 2003), there are very few resources available to train or evaluate automatic disease recognition systems. To the best of our knowledge, the only publicly available corpus for disease identification in the literature </context>
</contexts>
<marker>Aronson, Bodenreider, DemnerFushman, Fung, Lee, Mork, 2007</marker>
<rawString>Alan R. Aronson, Olivier Bodenreider, Dina DemnerFushman, Kin Wah Fung, Vivan E. Lee, James G. Mork et al. 2007. From Indexing the Biomedical Literature to Coding Clinical Text: Experience with MTI and Machine Learning Approaches. ACL Workshop BioNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Aronson</author>
</authors>
<title>Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program.</title>
<date>2001</date>
<booktitle>Proceedings of AMIA Symp:17-21.</booktitle>
<contexts>
<context position="12468" citStr="Aronson, 2001" startWordPosition="1938" endWordPosition="1939"> 3 shows a description of the training and test sets for each corpus. Data Lit. Corpus Query Corpus Training 276 sentences 300 queries (89 (487 disease con- disease concepts, cepts, 185 unique) 76 unique) Testing 275 sentences 200 queries (53 (437 disease con- disease concepts, cepts, 185 unique) 51 unique) All 551 sentences 500 queries (142 (924 disease con- disease concepts, cepts, 280 unique) 120 unique) Table 3: Description of the training and test sets 4.1 Natural Language Processing Disease recognition was performed using the Natural Language Processing algorithm implemented in MetaMap (Aronson, 2001)4. The tool was restricted to retrieve concepts from the disorder group, using the UMLS 2008AB release and “longest match” feature. In practice, MetaMap parses the input text into noun phrases, generates variants of these phrases using knowledge sources such as the SPECIALIST lexicon, and maps the phrases to UMLS concepts. 4.2 Priority Model The priority model was first introduced in (Tanabe and Wilbur, 2006) and is adapted here to detect disease mentions in free text. Because our evaluation is performed at the concept level, the mentions extracted by the model are then mapped to UMLS using Me</context>
</contexts>
<marker>Aronson, 2001</marker>
<rawString>Alan Aronson. 2001. Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program. Proceedings of AMIA Symp:17-21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-Coder Agreement for Computational Linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics</journal>
<volume>34</volume>
<issue>4</issue>
<pages>555--596</pages>
<contexts>
<context position="18170" citStr="Artstein and Poesio (2008)" startWordPosition="2986" endWordPosition="2989">aries, our phrase finding algorithm in pseudo-code is shown in Figure 1. The output of this algorithm may then be filtered by setting a threshold on the PMA values to accept. 5 Results 5.1 Assessing the difficulty of the task To assess the difficulty of disease recognition, we computed the inter-annotator agreement (IAA) on the 300-query corpus. Agreement was computed at the disease mention level for all three annotators and at the disease concept level for the two annotators who produced UMLS annotations. Inter-annotator agreement measures for NLP applications have been recently discussed by Artstein and Poesio (2008) who advocate for the use of chance corrected measures. However, in our case, agreement was partly computed on a very large set of categories (UMLS concepts) so we decided to use Knowtator’s built-in feature, which computes IAA as the percentage of agreement and #Initialization: Given a text TX, set and #Processing: While( ){ I. select II. If( mts, D ) else if( mts,R ) else if( mts,N ) else if( mts, P2 ) else #Return: All pairs } 148 allows partial string matches. For example, in the query “dog model transient ischemic attacks”, annotator 1 selected “ischemic attacks” as a disorder while annot</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-Coder Agreement for Computational Linguistics. Computational Linguistics 34(4): 555-596</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jing Bai</author>
<author>Jian-Yun Nie</author>
</authors>
<title>Adapting information retrieval to query contexts.</title>
<date>2008</date>
<booktitle>Information Processing &amp; Management.</booktitle>
<pages>44--6</pages>
<contexts>
<context position="2222" citStr="Bai and Nie 2008" startWordPosition="332" endWordPosition="335">rch behavior based on their level of domain knowledge (Zhang et al., 2005) or aimed at modeling users’ interests and search habits (Rose and Levinson, 2004; Teevan et al., 2005). Information about the search context may be sought explicitly from the user through profiling or relevance feedback (Shen et al., 2005). Recent work also exploited query log analysis and basic computer environment information (Wen et al. 2004), which involve no explicit interaction with the user. In adaptive information retrieval, context information is inferred based on query analysis and collection characteristics (Bai and Nie 2008). In the biomedical domain, a need for contextual information retrieval was identified in particular for clinical queries submitted to PubMed (Pratt and Wasserman, 2000). Building on the idea that a specific type of document is required for searches with a “clinical” context, the PubMed Clinical Queries portal was developed (Haynes and Wilczynski, 2004). A perhaps more prominent contextual feature of PubMed is the “citation sensor”, which identifies queries classified by Rose and Levinson as reflecting a “Navigational” or “Obtain resource” goal. For example, the citation sensor will identify a</context>
</contexts>
<marker>Bai, Nie, 2008</marker>
<rawString>Jing Bai, and Jian-Yun Nie. 2008. Adapting information retrieval to query contexts. Information Processing &amp; Management. 44(6):1902-22</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hart</author>
<author>David G Stork</author>
</authors>
<title>Pattern Classification.</title>
<date>2001</date>
<publisher>John Wiley &amp; Sons, Inc.</publisher>
<location>New York:</location>
<contexts>
<context position="14757" citStr="Hart and Stork, 2001" startWordPosition="2355" endWordPosition="2358">be, Thom et al. 2006) supplemented with the latest UMLS data. We removed any term from D and N that contained less than five characters in order to decrease the occurrence of ambiguous terms. Also the 1,000 most frequent terms from D were examined manually and the ambiguous ones were removed. The end result is a set of 332,984 phrases in D and 4,253,758 phrases in N. We trained the priority model on D and N and applied the resulting training to compute for each phrase in D and N a vector of values . In this way D and N are converted to and . We then constructed a Mahalanobis classifier (Duda, Hart and Stork, 2001) for two dimensional vectors as the difference in the Mahalanobis distance of any such vector to Gaussian approximations to and . We refer to the number produced by this classifier as the Mahalanobis score. By randomly dividing both D and N into three equal size pieces and training on two from each and testing on the third, in a three-fold cross validation we found the Mahalanobis classifier to perform at 98.4% average precision and 93.9% precision-recall breakeven point. In a final step we applied a simple regression method to estimate the probability that a given Maha147 lanobis score was pr</context>
</contexts>
<marker>Hart, Stork, 2001</marker>
<rawString>Robert O. Duda, Peter. E. Hart and David G. Stork. 2001. Pattern Classification. New York: John Wiley &amp; Sons, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Brian Haynes</author>
<author>Nancy L Wilczynski</author>
</authors>
<title>Optimal search strategies for retrieving scientifically strong studies of diagnosis from Medline: analytical survey.</title>
<date>2004</date>
<journal>BMJ.</journal>
<volume>328</volume>
<issue>7447</issue>
<contexts>
<context position="2577" citStr="Haynes and Wilczynski, 2004" startWordPosition="386" endWordPosition="389">uery log analysis and basic computer environment information (Wen et al. 2004), which involve no explicit interaction with the user. In adaptive information retrieval, context information is inferred based on query analysis and collection characteristics (Bai and Nie 2008). In the biomedical domain, a need for contextual information retrieval was identified in particular for clinical queries submitted to PubMed (Pratt and Wasserman, 2000). Building on the idea that a specific type of document is required for searches with a “clinical” context, the PubMed Clinical Queries portal was developed (Haynes and Wilczynski, 2004). A perhaps more prominent contextual feature of PubMed is the “citation sensor”, which identifies queries classified by Rose and Levinson as reflecting a “Navigational” or “Obtain resource” goal. For example, the citation sensor will identify and retrieve a specific citation if the user enters the article title as the query. The analysis of Entrez logs shows that MEDLINE is the most popular database among the 30 or so databases maintained by the National Center for Biotechnology Information (NCBI) as it receives most of Entrez traffic. This suggests that there is a need to complement the info</context>
</contexts>
<marker>Haynes, Wilczynski, 2004</marker>
<rawString>R. Brian Haynes and Nancy L. Wilczynski. 2004. Optimal search strategies for retrieving scientifically strong studies of diagnosis from Medline: analytical survey. BMJ. 328(7447):1040.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jorge R Herskovic</author>
<author>Len Y Tanaka</author>
<author>William Hersh</author>
<author>Elmer V Bernstam</author>
</authors>
<title>A day in the life of PubMed: analysis of a typical day&apos;s query log.</title>
<date>2007</date>
<journal>Journal of the American Medical Informatics Association.</journal>
<pages>14--2</pages>
<contexts>
<context position="10159" citStr="Herskovic et al. (2007)" startWordPosition="1571" endWordPosition="1574">y white space. A set of UMLS concepts (or clusters) is associated with each sentence in the corpus. However, no boundary information linking a phrase in a sentence to an annotation was available. Table 1 shows a sample sentence and its annotations. 3.2 Biomedical query corpus A total of 500 PubMed queries were randomly selected and divided into two batches of 300 and 200 queries, respectively. Queries were on average 3.45(+/- 2.64) tokens long in the 300 query batch and 3.58(+/- 4.63) for the 200 query batch, which is consistent with the average length of PubMed queries (3 tokens) reported by Herskovic et al. (2007). The queries in the first set were annotated using Knowtator (Ogren, 2006) by three annotators with different backgrounds (one biologist, one information scientist, one computational linguist). Two annotators annotated the queries using UMLS concepts from the disorder group, while the other annotator simply annotated diseases without reference to UMLS concepts. Table 2 shows a sample query and its annotations. A consensus set was obtained after a meeting between the annotators where diverging annotations were discussed and annotators agreed on a final, unique, version of all annotations. The </context>
</contexts>
<marker>Herskovic, Tanaka, Hersh, Bernstam, 2007</marker>
<rawString>Jorge R. Herskovic, Len Y. Tanaka, William Hersh and Elmer V. Bernstam. 2007. A day in the life of PubMed: analysis of a typical day&apos;s query log. Journal of the American Medical Informatics Association. 14(2):212-20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Jimeno</author>
<author>Ernesto Jimenez-Ruiz</author>
<author>Vivian Lee</author>
<author>Sylvain Gaudan</author>
<author>Rafael Berlanga</author>
<author>Dietrich Rebholz-Schuhmann</author>
</authors>
<title>Assessment of disease named entity recognition on a corpus of annotated sentences.</title>
<date>2008</date>
<journal>BMC Bioinformatics.</journal>
<volume>11</volume>
<note>Suppl 3:S3.</note>
<contexts>
<context position="5467" citStr="Jimeno et al. (2008)" startWordPosition="847" endWordPosition="850">g, 2005). This task was the topic of a Medical NLP challenge2, which released a corpus of anonymized radiography reports annotated with ICD9 codes. Although there is some interest in the biomedical community in the identification of disease names and more specifically the identification of relationships between diseases and genes or proteins (Rindflesh and Fizman, 2003), there are very few resources available to train or evaluate automatic disease recognition systems. To the best of our knowledge, the only publicly available corpus for disease identification in the literature was developed by Jimeno et al. (2008). The authors annotated 551 MEDLINE sentences with UMLS concepts and used this dataset to benchmark three different automatic methods for disease name recognition. A MEDLINE corpus annotated 1 http://biocreative.sourceforge.net/ 2 http://www.computationalmedicine.org/challenge/index.php with “malignancy” mentions and part-of-speech tags is also available (Jin et al. 2006). This corpus is targeted to a very restricted type of diseases. The annotations are also domain specific, so that “cancer of the lung” is not considered a malignancy mention but a mention of malignancy and a mention of malign</context>
<context position="6713" citStr="Jimeno et al. 2008" startWordPosition="1032" endWordPosition="1035">us studies, we aim to investigate the complexity of automatic disease recognition using state-of-the-art computational techniques. This work is novel in at least three aspects: first, in addition to using the MEDLINE sentence corpus (Jimeno et al 2008), we developed a new corpus comprising disease annotations on 500 randomly selected PubMed queries. This allowed us to investigate the influence of local context3 through the comparison of system performance between two different genres of biomedical text. Second, by using a knowledge based tool previously benchmarked on the same MEDLINE corpus (Jimeno et al. 2008), we show that significant performance differences can be observed when parameters are adjusted. Finally, a state-of-the-art statistical approach was adapted for disease name recognition and evaluated on both corpora. 3 Two Biomedical Corpora with disease annotations The first issue in the development of such a corpus is to define the very concept of disease. Among the numerous terminological resources available, such as Medical Subject Headings (MeSH®, 4,354 disease concepts) or the International Classification of Diseases (ICD9, ~18,000 disease concepts), the UMLS Metathesaurus® is the most </context>
</contexts>
<marker>Jimeno, Jimenez-Ruiz, Lee, Gaudan, Berlanga, Rebholz-Schuhmann, 2008</marker>
<rawString>Antonio Jimeno, Ernesto Jimenez-Ruiz, Vivian Lee, Sylvain Gaudan, Rafael Berlanga and Dietrich Rebholz-Schuhmann. 2008. Assessment of disease named entity recognition on a corpus of annotated sentences. BMC Bioinformatics. 11;9 Suppl 3:S3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Jin</author>
<author>Ryan T McDonald</author>
<author>Kevin Lerman</author>
<author>Mark A Mandel</author>
<author>Steven Carroll</author>
<author>Mark Y Liberman</author>
</authors>
<title>Automated recognition of malignancy mentions in biomedical literature.</title>
<date>2006</date>
<journal>BMC Bioinformatics.</journal>
<pages>7--492</pages>
<contexts>
<context position="5841" citStr="Jin et al. 2006" startWordPosition="892" endWordPosition="895">e are very few resources available to train or evaluate automatic disease recognition systems. To the best of our knowledge, the only publicly available corpus for disease identification in the literature was developed by Jimeno et al. (2008). The authors annotated 551 MEDLINE sentences with UMLS concepts and used this dataset to benchmark three different automatic methods for disease name recognition. A MEDLINE corpus annotated 1 http://biocreative.sourceforge.net/ 2 http://www.computationalmedicine.org/challenge/index.php with “malignancy” mentions and part-of-speech tags is also available (Jin et al. 2006). This corpus is targeted to a very restricted type of diseases. The annotations are also domain specific, so that “cancer of the lung” is not considered a malignancy mention but a mention of malignancy and a mention of malignancy location. As in previous studies, we aim to investigate the complexity of automatic disease recognition using state-of-the-art computational techniques. This work is novel in at least three aspects: first, in addition to using the MEDLINE sentence corpus (Jimeno et al 2008), we developed a new corpus comprising disease annotations on 500 randomly selected PubMed quer</context>
</contexts>
<marker>Jin, McDonald, Lerman, Mandel, Carroll, Liberman, 2006</marker>
<rawString>Yang Jin, Ryan T McDonald, Kevin Lerman, Mark A Mandel, Steven Carroll, Mark Y Liberman et al. 2006. Automated recognition of malignancy mentions in biomedical literature. BMC Bioinformatics. 7:492.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexa T McCray</author>
<author>Anita Burgun</author>
<author>Olivier Bodenreider</author>
</authors>
<title>Aggregating UMLS semantic types for reducing conceptual complexity.</title>
<date>2001</date>
<booktitle>Proceedings of Medinfo 10(Pt</booktitle>
<pages>1--216</pages>
<contexts>
<context position="7435" citStr="McCray et al. (2001)" startWordPosition="1137" endWordPosition="1140">ly, a state-of-the-art statistical approach was adapted for disease name recognition and evaluated on both corpora. 3 Two Biomedical Corpora with disease annotations The first issue in the development of such a corpus is to define the very concept of disease. Among the numerous terminological resources available, such as Medical Subject Headings (MeSH®, 4,354 disease concepts) or the International Classification of Diseases (ICD9, ~18,000 disease concepts), the UMLS Metathesaurus® is the most comprehensive: the 2008AB release includes 252,284 concepts in the disorder Semantic Group defined by McCray et al. (2001). The UMLS Metathesaurus is part of the Semantic Network, which also includes a set of broad subject categories, or Semantic Types, that provide a consistent categorization of all concepts represented in the Metathesaurus. The Semantic Groups aim at providing an even broader categorization for UMLS concepts. For example, the disorder Semantic Group comprises 12 Semantic Types including Disease or Syndrome, Cell or Molecular Dysfunction and Congenital Abnormalities. 3 Here, by context, we mean the information surrounding a disease mention available in the corpora. This is different from the “se</context>
</contexts>
<marker>McCray, Burgun, Bodenreider, 2001</marker>
<rawString>Alexa T. McCray, Anita Burgun and Olivier Bodenreider. 2001. Aggregating UMLS semantic types for reducing conceptual complexity. Proceedings of Medinfo 10(Pt 1):216-20.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stéphane Meystre</author>
<author>Peter J Haug</author>
</authors>
<title>Natural language processing to extract medical problems from electronic clinical documents: performance evaluation.</title>
<date>2006</date>
<journal>J Biomed Inform.</journal>
<pages>39--6</pages>
<contexts>
<context position="26383" citStr="Meystre and Haug (2006)" startWordPosition="4386" endWordPosition="4389">s is comparable to inter-annotator agreement (F=70-74 vs. IAA=72 on Disease and Syndromes). On both corpora, the Priority Model achieves higher precision and Fmeasure, while MetaMap achieves better recall. Comparing the results obtained with MetaMap with those reported by Jimeno et al., precision is lower, but recall is much higher. This is likely to be due to the different MetaMap settings, and the use of different UMLS versions - Jimeno et al. did not provide any of this information, but based on the publication date of their paper, it is likely that they used one of the 2006 UMLS releases. Meystre and Haug (2006) also found that significant performance differences could be obtained with MetaMap by adjusting the content of the knowledge sources used. On both text genres, 0.3 was found to be the optimal probability threshold for the Priority Model. Based on the performance at different values of the threshold, it seems that the model is quite efficient at ruling out highly unlikely diseases. However, for values above .3 the performance does not vary greatly. Comparing Text Genres. For both methods, disease recognition seems more efficient on sentences. This is to be expected: sentences provide more cont</context>
</contexts>
<marker>Meystre, Haug, 2006</marker>
<rawString>Stéphane Meystre and Peter J. Haug. 2006. Natural language processing to extract medical problems from electronic clinical documents: performance evaluation. J Biomed Inform. 39(6):589-99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander A Morgan</author>
<author>Zhiyong Lu</author>
<author>Xinglong Wang</author>
<author>Aaron M Cohen</author>
<author>Juliane Fluck</author>
<author>Patrick Ruch</author>
</authors>
<date>2008</date>
<booktitle>Overview of BioCreative II gene normalization. Genome Biol. 9 Suppl 2:S3.</booktitle>
<contexts>
<context position="8130" citStr="Morgan et al. 2008" startWordPosition="1242" endWordPosition="1245">s a set of broad subject categories, or Semantic Types, that provide a consistent categorization of all concepts represented in the Metathesaurus. The Semantic Groups aim at providing an even broader categorization for UMLS concepts. For example, the disorder Semantic Group comprises 12 Semantic Types including Disease or Syndrome, Cell or Molecular Dysfunction and Congenital Abnormalities. 3 Here, by context, we mean the information surrounding a disease mention available in the corpora. This is different from the “search context” previously discussed. 145 Furthermore, like the gene mention (Morgan et al. 2008) and gene normalization (Smith et al. 2008) tasks in BioCreative II, the task of disease name recognition can also be performed at two different levels: 1. disease mention: the detection of a snippet of text that refers to a disease concept (e.g. “alzheimer” in the sample query shown in Table 2) 2. disease concept: the recognition of a controlled vocabulary disease concept (e.g. “C0002395-alzheimer’s disease” in our Table 2 example) in text. In this work, we evaluate and report system performance at the concept level. 3.1 Biomedical literature corpus Sentence Kniest dysplasia is a moderately s</context>
</contexts>
<marker>Morgan, Lu, Wang, Cohen, Fluck, Ruch, 2008</marker>
<rawString>Alexander A. Morgan, Zhiyong Lu, Xinglong Wang, Aaron M. Cohen, Juliane Fluck, Patrick Ruch et al. 2008. Overview of BioCreative II gene normalization. Genome Biol. 9 Suppl 2:S3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phillip V Ogren</author>
</authors>
<title>Knowtator: A plug-in for creating training and evaluation data sets for Biomedical Natural Language systems.</title>
<date>2006</date>
<booktitle>9th Intl. Protégé Conference</booktitle>
<contexts>
<context position="10234" citStr="Ogren, 2006" startWordPosition="1585" endWordPosition="1586">n the corpus. However, no boundary information linking a phrase in a sentence to an annotation was available. Table 1 shows a sample sentence and its annotations. 3.2 Biomedical query corpus A total of 500 PubMed queries were randomly selected and divided into two batches of 300 and 200 queries, respectively. Queries were on average 3.45(+/- 2.64) tokens long in the 300 query batch and 3.58(+/- 4.63) for the 200 query batch, which is consistent with the average length of PubMed queries (3 tokens) reported by Herskovic et al. (2007). The queries in the first set were annotated using Knowtator (Ogren, 2006) by three annotators with different backgrounds (one biologist, one information scientist, one computational linguist). Two annotators annotated the queries using UMLS concepts from the disorder group, while the other annotator simply annotated diseases without reference to UMLS concepts. Table 2 shows a sample query and its annotations. A consensus set was obtained after a meeting between the annotators where diverging annotations were discussed and annotators agreed on a final, unique, version of all annotations. The consensus set contains 89 disease concepts (76 unique). Query alzheimer csf</context>
</contexts>
<marker>Ogren, 2006</marker>
<rawString>Phillip V. Ogren. 2006. Knowtator: A plug-in for creating training and evaluation data sets for Biomedical Natural Language systems. 9th Intl. Protégé Conference</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong C Park</author>
<author>Jung-Jae Kim</author>
</authors>
<title>Named Entity Recognition. In</title>
<date>2006</date>
<pages>121--42</pages>
<publisher>Boston|London:Artech House Inc.</publisher>
<contexts>
<context position="4729" citStr="Park and Kim, 2006" startWordPosition="732" endWordPosition="735">drug sensor and additional information on this drug from Clinical Q&amp;A will be shown in the side bar in addition to default PubMed results. Since disease names are common in PubMed queries, the goal of this work is to investigate and benchmark computational techniques for automatic disease name recognition as an aid to implementing PubMed search contexts. 2 Related Work Despite a significant body of literature in biomedical named entity recognition, most work has been focused on gene, protein, drug and chemical names through challenges such as BioCreAtIvE1 or the TREC Genomics/Chemical tracks (Park and Kim, 2006). Other work addressed the identification of “medical problems” in clinical text (Aronson et al. 2007; Meystre and Haug, 2005). This task was the topic of a Medical NLP challenge2, which released a corpus of anonymized radiography reports annotated with ICD9 codes. Although there is some interest in the biomedical community in the identification of disease names and more specifically the identification of relationships between diseases and genes or proteins (Rindflesh and Fizman, 2003), there are very few resources available to train or evaluate automatic disease recognition systems. To the be</context>
</contexts>
<marker>Park, Kim, 2006</marker>
<rawString>Jong C. Park and Jung-Jae Kim. 2006. Named Entity Recognition. In S. Ananiadou and J. McNaught (Eds), Text Mining for Biology and Biomedicine (pp. 121-42). Boston|London:Artech House Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanda Pratt</author>
<author>Henry Wasserman</author>
</authors>
<title>QueryCat: automatic categorization of MEDLINE queries.</title>
<date>2000</date>
<booktitle>Proceedings of AMIA Symp:655-9.</booktitle>
<contexts>
<context position="2391" citStr="Pratt and Wasserman, 2000" startWordPosition="356" endWordPosition="359">van et al., 2005). Information about the search context may be sought explicitly from the user through profiling or relevance feedback (Shen et al., 2005). Recent work also exploited query log analysis and basic computer environment information (Wen et al. 2004), which involve no explicit interaction with the user. In adaptive information retrieval, context information is inferred based on query analysis and collection characteristics (Bai and Nie 2008). In the biomedical domain, a need for contextual information retrieval was identified in particular for clinical queries submitted to PubMed (Pratt and Wasserman, 2000). Building on the idea that a specific type of document is required for searches with a “clinical” context, the PubMed Clinical Queries portal was developed (Haynes and Wilczynski, 2004). A perhaps more prominent contextual feature of PubMed is the “citation sensor”, which identifies queries classified by Rose and Levinson as reflecting a “Navigational” or “Obtain resource” goal. For example, the citation sensor will identify and retrieve a specific citation if the user enters the article title as the query. The analysis of Entrez logs shows that MEDLINE is the most popular database among the </context>
</contexts>
<marker>Pratt, Wasserman, 2000</marker>
<rawString>Wanda Pratt and Henry Wasserman. 2000. QueryCat: automatic categorization of MEDLINE queries. Proceedings of AMIA Symp:655-9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom C Rindflesh</author>
<author>Marcelo Fiszman</author>
</authors>
<title>The interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text.</title>
<date>2003</date>
<journal>J Biomed Inform.</journal>
<pages>36--6</pages>
<marker>Rindflesh, Fiszman, 2003</marker>
<rawString>Tom C. Rindflesh and Marcelo Fiszman. 2003. The interaction of domain knowledge and linguistic structure in natural language processing: interpreting hypernymic propositions in biomedical text. J Biomed Inform. 36(6):462-77</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel E Rose</author>
<author>Danny Levinson</author>
</authors>
<title>Understanding user goals in web search.</title>
<date>2004</date>
<booktitle>In Proceedings of the 13th international Conference on World Wide Web:13-9</booktitle>
<contexts>
<context position="1760" citStr="Rose and Levinson, 2004" startWordPosition="264" endWordPosition="267">pecific needs at the time of the search. A practical example of a Google contextual IR feature is when the search engine returns a map showing restaurant locations to a user entering a query such as “Paris restaurants.” The contextual aspects of a user’s search were defined for example by Saracevic (1997) who discussed integrating the cognitive, affective, and situational levels of human computer interaction in IR systems. Other research efforts studied users’ search behavior based on their level of domain knowledge (Zhang et al., 2005) or aimed at modeling users’ interests and search habits (Rose and Levinson, 2004; Teevan et al., 2005). Information about the search context may be sought explicitly from the user through profiling or relevance feedback (Shen et al., 2005). Recent work also exploited query log analysis and basic computer environment information (Wen et al. 2004), which involve no explicit interaction with the user. In adaptive information retrieval, context information is inferred based on query analysis and collection characteristics (Bai and Nie 2008). In the biomedical domain, a need for contextual information retrieval was identified in particular for clinical queries submitted to Pub</context>
</contexts>
<marker>Rose, Levinson, 2004</marker>
<rawString>Daniel E. Rose and Danny Levinson. 2004. Understanding user goals in web search. In Proceedings of the 13th international Conference on World Wide Web:13-9</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tefko Saracevic</author>
</authors>
<title>The Stratified Model of Information Retrieval Interaction: Extension and Application.</title>
<date>1997</date>
<booktitle>Proceedings of the 60th meeting of the. American Society for Information Science:313-27</booktitle>
<contexts>
<context position="1443" citStr="Saracevic (1997)" startWordPosition="216" endWordPosition="217">tual Information Retrieval (IR) is making use of additional information or assumptions about the users’ needs beyond the obvious intent of the query. IR systems need to go beyond the task of providing generally relevant information by assisting users in finding information that is relevant to them and their specific needs at the time of the search. A practical example of a Google contextual IR feature is when the search engine returns a map showing restaurant locations to a user entering a query such as “Paris restaurants.” The contextual aspects of a user’s search were defined for example by Saracevic (1997) who discussed integrating the cognitive, affective, and situational levels of human computer interaction in IR systems. Other research efforts studied users’ search behavior based on their level of domain knowledge (Zhang et al., 2005) or aimed at modeling users’ interests and search habits (Rose and Levinson, 2004; Teevan et al., 2005). Information about the search context may be sought explicitly from the user through profiling or relevance feedback (Shen et al., 2005). Recent work also exploited query log analysis and basic computer environment information (Wen et al. 2004), which involve </context>
</contexts>
<marker>Saracevic, 1997</marker>
<rawString>Tefko Saracevic. 1997. The Stratified Model of Information Retrieval Interaction: Extension and Application. Proceedings of the 60th meeting of the. American Society for Information Science:313-27</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuehua Shen</author>
<author>Bin Tan</author>
<author>ChengXiang Zhai</author>
</authors>
<title>Context-sensitive information retrieval using implicit feedback,</title>
<date>2005</date>
<booktitle>In Proceedings of the 28th annual international conference ACM SIGIR conference on Research and development in information retrieval:</booktitle>
<pages>43--50</pages>
<contexts>
<context position="1919" citStr="Shen et al., 2005" startWordPosition="289" endWordPosition="292">to a user entering a query such as “Paris restaurants.” The contextual aspects of a user’s search were defined for example by Saracevic (1997) who discussed integrating the cognitive, affective, and situational levels of human computer interaction in IR systems. Other research efforts studied users’ search behavior based on their level of domain knowledge (Zhang et al., 2005) or aimed at modeling users’ interests and search habits (Rose and Levinson, 2004; Teevan et al., 2005). Information about the search context may be sought explicitly from the user through profiling or relevance feedback (Shen et al., 2005). Recent work also exploited query log analysis and basic computer environment information (Wen et al. 2004), which involve no explicit interaction with the user. In adaptive information retrieval, context information is inferred based on query analysis and collection characteristics (Bai and Nie 2008). In the biomedical domain, a need for contextual information retrieval was identified in particular for clinical queries submitted to PubMed (Pratt and Wasserman, 2000). Building on the idea that a specific type of document is required for searches with a “clinical” context, the PubMed Clinical </context>
</contexts>
<marker>Shen, Tan, Zhai, 2005</marker>
<rawString>Xuehua Shen, Bin Tan and ChengXiang Zhai. 2005 Context-sensitive information retrieval using implicit feedback, In Proceedings of the 28th annual international conference ACM SIGIR conference on Research and development in information retrieval: 43-50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Larry Smith</author>
<author>Laurraine K Tanabe</author>
<author>Rie J Ando</author>
</authors>
<title>ChengJu Kuo, I-Fang Chung , Chun-Nan Hsu et al.</title>
<date>2008</date>
<contexts>
<context position="8173" citStr="Smith et al. 2008" startWordPosition="1249" endWordPosition="1252">ntic Types, that provide a consistent categorization of all concepts represented in the Metathesaurus. The Semantic Groups aim at providing an even broader categorization for UMLS concepts. For example, the disorder Semantic Group comprises 12 Semantic Types including Disease or Syndrome, Cell or Molecular Dysfunction and Congenital Abnormalities. 3 Here, by context, we mean the information surrounding a disease mention available in the corpora. This is different from the “search context” previously discussed. 145 Furthermore, like the gene mention (Morgan et al. 2008) and gene normalization (Smith et al. 2008) tasks in BioCreative II, the task of disease name recognition can also be performed at two different levels: 1. disease mention: the detection of a snippet of text that refers to a disease concept (e.g. “alzheimer” in the sample query shown in Table 2) 2. disease concept: the recognition of a controlled vocabulary disease concept (e.g. “C0002395-alzheimer’s disease” in our Table 2 example) in text. In this work, we evaluate and report system performance at the concept level. 3.1 Biomedical literature corpus Sentence Kniest dysplasia is a moderately severe chondrodysplasia phenotype that resul</context>
</contexts>
<marker>Smith, Tanabe, Ando, 2008</marker>
<rawString>Larry Smith, Laurraine K. Tanabe, Rie J. Ando, ChengJu Kuo, I-Fang Chung , Chun-Nan Hsu et al. 2008. Overview of BioCreative II gene mention recognition. Genome Biol. 9 Suppl 2:S2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Thom</author>
<author>Wayne Matten</author>
<author>Donald C Comeau</author>
<author>W John Wilbur</author>
</authors>
<title>SemCat: semantically categorized entities for genomics.</title>
<date>2006</date>
<booktitle>Proceedings of AMIA Symp:</booktitle>
<pages>754--8</pages>
<contexts>
<context position="14157" citStr="Thom et al. 2006" startWordPosition="2241" endWordPosition="2244">itional information is also available at http://metamap.nlm.nih.gov/ and for each the corresponding numbers and q; we estimate the probability that by The training procedure for the model actually chooses the values of all the and quantities to optimize the values over all of D and N. For this work we have extended the approach to include a quantity which represents a weighted average of all the quality numbers . We apply this formula to obtain as long as If we replace all numbers by in (2) and (3) to obtainqual . For this application we obtained the sets D and N from the SEMCAT data (Tanabe, Thom et al. 2006) supplemented with the latest UMLS data. We removed any term from D and N that contained less than five characters in order to decrease the occurrence of ambiguous terms. Also the 1,000 most frequent terms from D were examined manually and the ambiguous ones were removed. The end result is a set of 332,984 phrases in D and 4,253,758 phrases in N. We trained the priority model on D and N and applied the resulting training to compute for each phrase in D and N a vector of values . In this way D and N are converted to and . We then constructed a Mahalanobis classifier (Duda, Hart and Stork, 2001)</context>
</contexts>
<marker>Thom, Matten, Comeau, Wilbur, 2006</marker>
<rawString>Laurraine K. Tanabe, Lynn. H. Thom, Wayne Matten, Donald C. Comeau and W. John Wilbur. 2006. SemCat: semantically categorized entities for genomics. Proceedings of AMIA Symp: 754-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurraine K Tanabe</author>
<author>W John Wilbur</author>
</authors>
<title>A Priority Model for Named Entities.</title>
<date>2006</date>
<journal>Proceedings of HLT-NAACL BioNLP</journal>
<pages>33--40</pages>
<contexts>
<context position="12880" citStr="Tanabe and Wilbur, 2006" startWordPosition="2001" endWordPosition="2004"> unique) Table 3: Description of the training and test sets 4.1 Natural Language Processing Disease recognition was performed using the Natural Language Processing algorithm implemented in MetaMap (Aronson, 2001)4. The tool was restricted to retrieve concepts from the disorder group, using the UMLS 2008AB release and “longest match” feature. In practice, MetaMap parses the input text into noun phrases, generates variants of these phrases using knowledge sources such as the SPECIALIST lexicon, and maps the phrases to UMLS concepts. 4.2 Priority Model The priority model was first introduced in (Tanabe and Wilbur, 2006) and is adapted here to detect disease mentions in free text. Because our evaluation is performed at the concept level, the mentions extracted by the model are then mapped to UMLS using MetaMap. The priority model approach is based on two sets of phrases: one names of diseases, D, and one names of non-diseases, N. One trains the model to assign two numbers, p and q, to each token t that appears in a phrase in either D or N. Roughly, p is the probability that a phrase from D or N that has the token t in it is actually from D and q is the relative weight that should be assigned to t for this pur</context>
</contexts>
<marker>Tanabe, Wilbur, 2006</marker>
<rawString>Laurraine K. Tanabe and W. John Wilbur. 2006. A Priority Model for Named Entities. Proceedings of HLT-NAACL BioNLP Workshop:33-40</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Teevan</author>
<author>Susan T Dumais</author>
<author>Eric Horvitz</author>
</authors>
<title>Personalizing search via automated analysis of interests and activities.</title>
<date>2005</date>
<booktitle>In Proceeding of ACMSIGIR’05:449–56.</booktitle>
<contexts>
<context position="1782" citStr="Teevan et al., 2005" startWordPosition="268" endWordPosition="271"> of the search. A practical example of a Google contextual IR feature is when the search engine returns a map showing restaurant locations to a user entering a query such as “Paris restaurants.” The contextual aspects of a user’s search were defined for example by Saracevic (1997) who discussed integrating the cognitive, affective, and situational levels of human computer interaction in IR systems. Other research efforts studied users’ search behavior based on their level of domain knowledge (Zhang et al., 2005) or aimed at modeling users’ interests and search habits (Rose and Levinson, 2004; Teevan et al., 2005). Information about the search context may be sought explicitly from the user through profiling or relevance feedback (Shen et al., 2005). Recent work also exploited query log analysis and basic computer environment information (Wen et al. 2004), which involve no explicit interaction with the user. In adaptive information retrieval, context information is inferred based on query analysis and collection characteristics (Bai and Nie 2008). In the biomedical domain, a need for contextual information retrieval was identified in particular for clinical queries submitted to PubMed (Pratt and Wasserm</context>
</contexts>
<marker>Teevan, Dumais, Horvitz, 2005</marker>
<rawString>Jaime Teevan, Susan T. Dumais and Eric Horvitz. 2005. Personalizing search via automated analysis of interests and activities. In Proceeding of ACMSIGIR’05:449–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ji-Rong Wen</author>
<author>Ni Lao</author>
<author>Wei-Ying Ma</author>
</authors>
<title>Probabilistic model for contextual retrieval.</title>
<date>2004</date>
<booktitle>Proceedings of ACM-SIGIR’04:57–63</booktitle>
<contexts>
<context position="2027" citStr="Wen et al. 2004" startWordPosition="305" endWordPosition="308"> for example by Saracevic (1997) who discussed integrating the cognitive, affective, and situational levels of human computer interaction in IR systems. Other research efforts studied users’ search behavior based on their level of domain knowledge (Zhang et al., 2005) or aimed at modeling users’ interests and search habits (Rose and Levinson, 2004; Teevan et al., 2005). Information about the search context may be sought explicitly from the user through profiling or relevance feedback (Shen et al., 2005). Recent work also exploited query log analysis and basic computer environment information (Wen et al. 2004), which involve no explicit interaction with the user. In adaptive information retrieval, context information is inferred based on query analysis and collection characteristics (Bai and Nie 2008). In the biomedical domain, a need for contextual information retrieval was identified in particular for clinical queries submitted to PubMed (Pratt and Wasserman, 2000). Building on the idea that a specific type of document is required for searches with a “clinical” context, the PubMed Clinical Queries portal was developed (Haynes and Wilczynski, 2004). A perhaps more prominent contextual feature of P</context>
</contexts>
<marker>Wen, Lao, Ma, 2004</marker>
<rawString>Ji-Rong Wen, Ni Lao, Wei-Ying Ma. 2004. Probabilistic model for contextual retrieval. Proceedings of ACM-SIGIR’04:57–63</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiangmin Zhang</author>
<author>Hermina G B Anghelescu</author>
<author>Xiaojun Yuan</author>
</authors>
<title>Domain knowledge, search behavior, and search effectiveness of engineering and science students: An exploratory study,</title>
<date>2005</date>
<journal>Information Research</journal>
<volume>10</volume>
<issue>2</issue>
<pages>217</pages>
<contexts>
<context position="1679" citStr="Zhang et al., 2005" startWordPosition="250" endWordPosition="253">y assisting users in finding information that is relevant to them and their specific needs at the time of the search. A practical example of a Google contextual IR feature is when the search engine returns a map showing restaurant locations to a user entering a query such as “Paris restaurants.” The contextual aspects of a user’s search were defined for example by Saracevic (1997) who discussed integrating the cognitive, affective, and situational levels of human computer interaction in IR systems. Other research efforts studied users’ search behavior based on their level of domain knowledge (Zhang et al., 2005) or aimed at modeling users’ interests and search habits (Rose and Levinson, 2004; Teevan et al., 2005). Information about the search context may be sought explicitly from the user through profiling or relevance feedback (Shen et al., 2005). Recent work also exploited query log analysis and basic computer environment information (Wen et al. 2004), which involve no explicit interaction with the user. In adaptive information retrieval, context information is inferred based on query analysis and collection characteristics (Bai and Nie 2008). In the biomedical domain, a need for contextual informa</context>
</contexts>
<marker>Zhang, Anghelescu, Yuan, 2005</marker>
<rawString>Xiangmin Zhang, Hermina G.B. Anghelescu and Xiaojun Yuan. 2005. Domain knowledge, search behavior, and search effectiveness of engineering and science students: An exploratory study, Information Research 10(2): 217.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>