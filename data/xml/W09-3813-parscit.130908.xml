<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.091013">
<bodyText confidence="0.979772795918368">
2 Discourse analysis with the RST Dis-
course Treebank
The discourse parsing approach presented here is
based on the formalization of Rhetorical Struc-
ture Theory (RST) (Mann and Thompson, 1988)
used in the RST Discourse Treebank (Carlson et
al., 2003). In this scheme, the discourse structure
of a document is represented as a tree, where the
leaves are contiguous spans of text, called ele-
mentary discourse units, or EDUs. Each node in
the tree corresponds to a contiguous span of text
formed by concatenation of the spans corre-
sponding to the node’s children, and represents a
rhetorical relation (attribution, enablement,
elaboration, consequence, etc.) between these
text segments. In addition, each node is marked
as a nucleus or as a satellite, depending on
whether its text span represents an essential unit
of information, or a supporting or background
unit of information, respectively. While the no-
tions of nucleus and satellite are in some ways
analogous to head and dependent in syntactic
dependencies, RST allows for multi-nuclear rela-
tions, where two nodes marked as nucleus can be
linked into one node.
Our parsing framework includes three compo-
nents: (1) syntactic dependency parsing, where
standard techniques for sentence-level parsing
are applied; (2) discourse segmentation, which
uses syntactic and lexical information to segment
text into EDUs; and (3) discourse parsing, which
produces a discourse structure tree from a string
of EDUs, also benefiting from syntactic informa-
tion. In contrast to the approach of Soricut and
Marcu (2003), which also includes syntactic
parsing, discourse segmentation and discourse
parsing, our approach assumes that the unit of
processing for discourse parsing is an entire
document, and that discourse relations may exist
within sentences as well as across sentences,
while Soricut and Marcu’s processes one sen-
tence at a time, independently, finding only dis-
course relations within individual sentences.
Parsing entire documents at a time is made pos-
sible in our approach through the use of linear-
time transition-based parsing. An additional mi-
nor difference is that in our approach syntactic
information is represented using dependencies,
while Soricut and Marcu used constituent trees.
</bodyText>
<subsectionHeader confidence="0.9131755">
2.1 Syntactic parsing and discourse seg-
mentation
</subsectionHeader>
<bodyText confidence="0.999994488372093">
Assuming the document has been segmented into
sentences, a task for which there are approaches
with very high accuracy (Gillick, 2009), we start
by finding the dependency structure for each sen-
tence. This includes part-of-speech (POS) tag-
ging using a CRF tagger trained on the Wall
Street Journal portion of the Penn Treebank, and
transition-based dependency parsing using the
shift-reduce arc-standard algorithm (Vivre, 2004)
trained with the averaged perceptron (Collins,
2002). The dependency parser is also trained
with the WSJ Penn Treebank, converted to de-
pendencies using the head percolation rules of
Yamada and Matsumoto (2003).
Discourse segmentation is performed as a bi-
nary classification task on each word, where the
decision is whether or not to insert an EDU
boundary between the word and the next word.
In a sentence of length n, containing the words
w1, w2 É wn, we perform one classification per
word, in order. For word wi, the binary choice is
whether to insert an EDU boundary between wi
and wi+1. The EDUs are then the words between
EDU boundaries (assuming boundaries exist in
the beginning and end of each sentence).
The features used for classification are: the
current word, its POS tag, its dependency label,
and the direction to its head (whether the head
appears before or after the word); the previous
two words, their POS tags and dependency la-
bels; the next two words, their POS tags and de-
pendency labels; the direction from the previous
word to its head; the leftmost dependent to the
right of the current word, and its POS tag; the
rightmost dependent to the left of the current
word, and its POS tag; whether the head of the
current word is between the previous EDU
boundary and the current word; whether the head
of the next word is between the previous EDU
boundary and the current word. In addition, we
used templates that combine these features (in
pairs or triples). Classification was done with
the averaged perceptron.
</bodyText>
<subsectionHeader confidence="0.991179">
2.2 Transition-based discourse parsing
</subsectionHeader>
<bodyText confidence="0.99997525">
RST trees can be represented in a similar way as
constituent trees in the Penn Treebank, with a
few differences: the trees represent entire docu-
ments, instead of single sentences; the leaves of
the trees are EDUs consisting of one or more
contiguous words; and the node labels contain
nucleus/satellite status, and possibly the name of
a discourse relation. Once the document has
been segmented into a sequence of EDUs, we
use a transition-based constituent parsing ap-
proach (Sagae and Lavie, 2005) to build an RST
tree for the document.
</bodyText>
<page confidence="0.992301">
82
</page>
<bodyText confidence="0.999990347222223">
Sagae and Lavie’s constituent parsing algo-
rithm uses a stack that holds subtrees, and con-
sumes the input string (in our case, a sequence of
EDUs) from left to right, using four types of ac-
tions: (1) shift, which removes the next token
from the input string, and pushes a subtree con-
taining exactly that token onto the stack; (2) re-
duce-unary-LABEL, which pops the stack, and
push onto it a new subtree where a node with
label LABEL dominates the subtree that was
popped (3) reduce-left-LABEL, and (4) reduce-
right-LABEL, which each pops two items from
the stack, and pushes onto it a new subtree with
root LABEL, which has as right child the subtree
previously on top of the stack, and as left child
the subtree previously immediately below the top
of the stack. The difference between reduce-left
and reduce-right is whether the head of the new
subtree comes from the left or right child. The
algorithm assumes trees are lexicalized, and in
our use of the algorithm for discourse parsing,
heads are entire EDUs, and not single words.
Our process for lexicalization of discourse
trees, which is required for the parsing algorithm
to function properly, is a simple percolation of
“head EDUs,” performed in the same way as
lexical heads can be assigned in Penn Treebank-
style trees using a head percolation table
(Collins, 1999). To determine head EDUs, we
use the nucleus/satellite status of nodes, as fol-
lows: for each node, the leftmost child with nu-
cleus status is the head; if no child is a nucleus,
the leftmost satellite is the head. Most nodes
have exactly two children, one nucleus and one
satellite. The parsing algorithm deals only with
binary trees. We use the same binarization trans-
form as Sagae and Lavie, converting the trees in
the training set to binary trees prior to training
the parser, and converting the binary trees pro-
duced by the parser at run-time into n-ary trees.
As with the dependency parser and discourse
segmenter, learning is performed using the aver-
aged perceptron. We use similar features as Sa-
gae and Lavie, with one main difference: since
there is usually no single head-word associated
with each node, but a EDU that contains a se-
quence of words, we use the dependency struc-
ture of the EDU to determine what lexical fea-
tures and POS tags should be used as features
associated with each RST tree node. In place of
the head-word and POS tag of the top four items
on the stack, and the next four items in the input,
we use subsets of the words and POS tags in the
EDUs for each of those items. The subset of
words (and POS tags) that represent an EDU
contain the first two and last words in the EDU,
and each word in the EDU whose head is outside
of the EDU. In the vast majority of EDUs, this
subset of words with heads outside the EDU (the
EDU head set) contains a single word. In addi-
tion, we extract these features for the top three
(not four) items on the stack, and the next three
(not four) words in the input. For the top two
items on the stack, in addition to subsets of
words and POS tags described above, we also
take the words and POS tags for the leftmost and
rightmost children of each word in the EDU head
set. Finally, we use feature templates that com-
bine these and other individual features from Sa-
gae and Lavie, who used a polynomial kernel
and had no need for such templates (at the cost of
increased time for both training and running).
</bodyText>
<sectionHeader confidence="0.999793" genericHeader="abstract">
3 Results
</sectionHeader>
<bodyText confidence="0.999863956521739">
To test our discourse parsing approach, we used
the standard training and testing sections of the
RST Discourse Treebank and the compacted 18-
label set described by Carlson et al. (2003). We
used approximately 5% of the standard training
set as a development set.
Our part-of-speech tagger and syntactic parser
were not trained using the standard splits of the
Penn Treebank for those tasks, since there are
documents in the RST Discourse Treebank test
section that are included in the usual training sets
for POS taggers and parsers. The POS tagger
and syntactic parser were then trained on sec-
tions 2 to 21 of the WSJ Penn Treebank, exclud-
ing the specific documents used in the test sec-
tion of the RST Discourse Treebank.
Table 1 shows the precision, recall and f-score
of our discourse segmentation approach on the
test set, compared to that of Soricut and Marcu
(2003) and Marcu (1999). In all cases, results
were obtained with automatically produced syn-
tactic structures. We also include the total time
required for syntactic parsing (required in our
</bodyText>
<table confidence="0.99948375">
Prec. Recall F-score Time
Marcu99 83.3 77.1 80.1 -
S&amp;M03 83.5 82.7 83.1 361s
this work 87.4 86.0 86.7 40s
</table>
<tableCaption confidence="0.997172">
Table 1: Precision, recall, f-score and time
</tableCaption>
<bodyText confidence="0.3590664">
for discourse segmenters, tested on the RST
Discourse Treebank. Time includes syntactic
parsing, Charniak (2000) for S&amp;M03, and
our implemetation of Nivre arc-standard for
our segmenter.
</bodyText>
<page confidence="0.994023">
83
</page>
<table confidence="0.8853374">
F-score Time
Marcu99 37.2 -
S&amp;M03 49.0 481s
this work 52.9 69s
human 77.0 -
</table>
<tableCaption confidence="0.731664">
Table 2: F-score for bracketing of RST dis-
</tableCaption>
<bodyText confidence="0.997913434782609">
course trees on the test set of the RST Dis-
course Treebank, and total time (syntactic
parsing, segmentation and discourse parsing)
required to parse the test set (S&amp;M03 and our
approach were run on the same hardware).
segmentation approach and Soricut and Marcu’s)
and segmentation. For comparison with previous
results, we include only segmentation within sen-
tences (if all discourse boundaries are counted,
including sentence boundaries, our f-score is
92.9).
Using our discourse segmentation and transi-
tion-based discourse parsing approach, we obtain
42.9 precision and 46.2 recall (44.5 f-score) for
all discourse structures in the test set. Table 2
shows f-score of labeled bracketing for discourse
relations within sentences only, for comparison
with previously published results. We note that
human performance on this task has f-score 77.0.
While our f-score is still far below that of hu-
man performance, we have achieved a large gain
in speed of processing compared to a state-of-
the-art approach.
</bodyText>
<sectionHeader confidence="0.999386" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999981272727273">
We have presented an approach to discourse
analysis based on transition-based algorithms for
dependency and constituent trees. Dependency
parsing is used to determine the syntactic struc-
ture of text, which is then used in discourse seg-
mentation and parsing. A simple discriminative
approach to segmentation results in an overall
improvement in discourse parsing f-score, and
the use of a linear-time algorithm results in an a
large improvement in speed over a state-of-the-
art approach.
</bodyText>
<sectionHeader confidence="0.998384" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999958571428571">
The work described here has been sponsored by
the U.S. Army Research, Development, and En-
gineering Command (RDECOM). Statements
and opinions expressed do not necessarily reflect
the position or the policy of the United States
Government, and no official endorsement should
be inferred.
</bodyText>
<sectionHeader confidence="0.996526" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999761725490196">
Buchholz, S. and Marsi, E. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc.
of CoNLL 2006 Shared Task.
Carlson, L., Marcu, D., and Okurowski, M. E. 2003.
Building a discourse-tagged corpus in the frame-
work of Rhetorical Structure Theory. In J. van
Kuppevelt and R. W. Smith, editors, Current and
New Directions in Discourse and Dialogue. Klu-
wer Academic Publishers.
Charniak, E. 2000. A maximum-entropy-inspired
parser. In Proc. of NAACL.
Collins, M. 1999. Head-driven statistical models for
natural language processing. PhD dissertation,
University of Pennsylvania.
Collins, M. 2002. Discriminative Training Methods
for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proc. of
EMNLP. Philadelphia, PA.
Gillick, D. 2009. Sentence Boundary Detection and
the Problem with the U.S. In Proc. of the NAACL
HLT: Short Papers. Boulder, Colorado.
Henderson, J., Merlo, P., Musillo, G., Titov, I. 2008.
A Latent Variable Model of Synchronous Parsing
for Syntactic and Semantic Dependencies. In Proc.
of CoNLL 2008 Shared Task, Manchester, UK.
Mann, W. C. and Thompson, S. A. 1988. Rhetorical
Structure Theory: toward a functional theory of
text organization. Text, 8(3):243-281.
Marcu, D. 1999. A decision-based approach to rhe-
torical parsing. In Proc. of the Annual Meeting of
the Association for Computational Linguistics.
McDonald, R., Pereira, F., Ribarov, K., and Hajic, J.
2005. Non-projective dependency parsing using
spanning tree algorithms. In Proc. of HLT/EMNLP.
Nivre, J. 2004. Incrementality in Deterministic De-
pendency Parsing. In Incremental Parsing: Bring-
ing Engineering and Cognition Together (work-
shop at ACL-2004). Barcelona, Spain.
Nivre, J. and Scholz, M. 2004. Deterministic Depend-
ency Parsing of English Text. In Proc. of COLING.
Sagae, K. and Lavie, A. 2005. A classifier-based
parser with linear run-time complexity. In Proc. of
IWPT.
Sagae, K. and Tsujii, J. 2008. Shift-reduce depend-
ency DAG parsing. In Proc. of COLING.
Soricut, R. and Marcu, D. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proc. of NAACL. Edmonton, Canada.
Yamada, H. and Matsumoto, Y. 2003. Statistical de-
pendency analysis with support vector machines. In
Proc. of IWPT.
</reference>
<page confidence="0.999245">
84
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.002176">
<title confidence="0.7515205">2 Discourse analysis with the RST Discourse Treebank</title>
<abstract confidence="0.992675206106871">The discourse parsing approach presented here is based on the formalization of Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) used in the RST Discourse Treebank (Carlson et al., 2003). In this scheme, the discourse structure of a document is represented as a tree, where the are contiguous spans of text, called elediscourse or EDUs. Each node in the tree corresponds to a contiguous span of text formed by concatenation of the spans corresponding to the node’s children, and represents a relation etc.) between these text segments. In addition, each node is marked a as a depending on whether its text span represents an essential unit of information, or a supporting or background unit of information, respectively. While the noof in some ways to syntactic dependencies, RST allows for multi-nuclear relawhere two nodes marked as be linked into one node. Our parsing framework includes three components: (1) syntactic dependency parsing, where standard techniques for sentence-level parsing are applied; (2) discourse segmentation, which uses syntactic and lexical information to segment text into EDUs; and (3) discourse parsing, which produces a discourse structure tree from a string of EDUs, also benefiting from syntactic information. In contrast to the approach of Soricut and Marcu (2003), which also includes syntactic parsing, discourse segmentation and discourse parsing, our approach assumes that the unit of processing for discourse parsing is an entire document, and that discourse relations may exist within sentences as well as across sentences, while Soricut and Marcu’s processes one sentence at a time, independently, finding only discourse relations within individual sentences. Parsing entire documents at a time is made possible in our approach through the use of lineartime transition-based parsing. An additional minor difference is that in our approach syntactic information is represented using dependencies, while Soricut and Marcu used constituent trees. 2.1 Syntactic parsing and discourse segmentation Assuming the document has been segmented into sentences, a task for which there are approaches with very high accuracy (Gillick, 2009), we start by finding the dependency structure for each sentence. This includes part-of-speech (POS) tagging using a CRF tagger trained on the Wall Street Journal portion of the Penn Treebank, and transition-based dependency parsing using the shift-reduce arc-standard algorithm (Vivre, 2004) trained with the averaged perceptron (Collins, 2002). The dependency parser is also trained with the WSJ Penn Treebank, converted to dependencies using the head percolation rules of Yamada and Matsumoto (2003). Discourse segmentation is performed as a binary classification task on each word, where the decision is whether or not to insert an EDU boundary between the word and the next word. a sentence of length containing the words we perform one classification per in order. For word the binary choice is to insert an EDU boundary between The EDUs are then the words between EDU boundaries (assuming boundaries exist in the beginning and end of each sentence). The features used for classification are: the current word, its POS tag, its dependency label, and the direction to its head (whether the head appears before or after the word); the previous two words, their POS tags and dependency labels; the next two words, their POS tags and dependency labels; the direction from the previous word to its head; the leftmost dependent to the right of the current word, and its POS tag; the rightmost dependent to the left of the current word, and its POS tag; whether the head of the current word is between the previous EDU boundary and the current word; whether the head of the next word is between the previous EDU boundary and the current word. In addition, we used templates that combine these features (in pairs or triples). Classification was done with the averaged perceptron. 2.2 Transition-based discourse parsing RST trees can be represented in a similar way as constituent trees in the Penn Treebank, with a few differences: the trees represent entire documents, instead of single sentences; the leaves of the trees are EDUs consisting of one or more contiguous words; and the node labels contain nucleus/satellite status, and possibly the name of a discourse relation. Once the document has been segmented into a sequence of EDUs, we use a transition-based constituent parsing approach (Sagae and Lavie, 2005) to build an RST tree for the document. 82 Sagae and Lavie’s constituent parsing algorithm uses a stack that holds subtrees, and consumes the input string (in our case, a sequence of EDUs) from left to right, using four types of ac- (1) which removes the next token from the input string, and pushes a subtree conexactly that token onto the stack; (2) rewhich pops the stack, and push onto it a new subtree where a node with the subtree that was (3) and (4) reducewhich each pops two items from the stack, and pushes onto it a new subtree with which has as right child the subtree previously on top of the stack, and as left child the subtree previously immediately below the top the stack. The difference between whether the head of the new subtree comes from the left or right child. The algorithm assumes trees are lexicalized, and in our use of the algorithm for discourse parsing, heads are entire EDUs, and not single words. Our process for lexicalization of discourse trees, which is required for the parsing algorithm to function properly, is a simple percolation of “head EDUs,” performed in the same way as lexical heads can be assigned in Penn Treebankstyle trees using a head percolation table (Collins, 1999). To determine head EDUs, we use the nucleus/satellite status of nodes, as follows: for each node, the leftmost child with nucleus status is the head; if no child is a nucleus, the leftmost satellite is the head. Most nodes have exactly two children, one nucleus and one satellite. The parsing algorithm deals only with binary trees. We use the same binarization transform as Sagae and Lavie, converting the trees in the training set to binary trees prior to training the parser, and converting the binary trees proby the parser at run-time into trees. As with the dependency parser and discourse segmenter, learning is performed using the averaged perceptron. We use similar features as Sagae and Lavie, with one main difference: since there is usually no single head-word associated with each node, but a EDU that contains a sequence of words, we use the dependency structure of the EDU to determine what lexical features and POS tags should be used as features associated with each RST tree node. In place of the head-word and POS tag of the top four items on the stack, and the next four items in the input, we use subsets of the words and POS tags in the EDUs for each of those items. The subset of words (and POS tags) that represent an EDU contain the first two and last words in the EDU, and each word in the EDU whose head is outside of the EDU. In the vast majority of EDUs, this subset of words with heads outside the EDU (the head contains a single word. In addition, we extract these features for the top three (not four) items on the stack, and the next three (not four) words in the input. For the top two items on the stack, in addition to subsets of words and POS tags described above, we also take the words and POS tags for the leftmost and rightmost children of each word in the EDU head set. Finally, we use feature templates that combine these and other individual features from Sagae and Lavie, who used a polynomial kernel and had no need for such templates (at the cost of increased time for both training and running). 3 Results To test our discourse parsing approach, we used the standard training and testing sections of the RST Discourse Treebank and the compacted 18label set described by Carlson et al. (2003). We used approximately 5% of the standard training set as a development set. Our part-of-speech tagger and syntactic parser using the standard splits of the Penn Treebank for those tasks, since there are documents in the RST Discourse Treebank test section that are included in the usual training sets for POS taggers and parsers. The POS tagger and syntactic parser were then trained on sections 2 to 21 of the WSJ Penn Treebank, excluding the specific documents used in the test section of the RST Discourse Treebank. Table 1 shows the precision, recall and f-score of our discourse segmentation approach on the test set, compared to that of Soricut and Marcu (2003) and Marcu (1999). In all cases, results were obtained with automatically produced syntactic structures. We also include the total time required for syntactic parsing (required in our Prec. Recall F-score Time Marcu99 83.3 77.1 80.1 - S&amp;M03 83.5 82.7 83.1 361s this work 87.4 86.0 86.7 40s Table 1: Precision, recall, f-score and time for discourse segmenters, tested on the RST Discourse Treebank. Time includes syntactic parsing, Charniak (2000) for S&amp;M03, and our implemetation of Nivre arc-standard for our segmenter. 83 F-score Time Marcu99 37.2 - S&amp;M03 49.0 481s work 69s human 77.0 - Table 2: F-score for bracketing of RST discourse trees on the test set of the RST Discourse Treebank, and total time (syntactic parsing, segmentation and discourse parsing) required to parse the test set (S&amp;M03 and our approach were run on the same hardware). segmentation approach and Soricut and Marcu’s) and segmentation. For comparison with previous results, we include only segmentation within sentences (if all discourse boundaries are counted, including sentence boundaries, our f-score is 92.9). Using our discourse segmentation and transition-based discourse parsing approach, we obtain 42.9 precision and 46.2 recall (44.5 f-score) for all discourse structures in the test set. Table 2 shows f-score of labeled bracketing for discourse sentences for comparison with previously published results. We note that human performance on this task has f-score 77.0. While our f-score is still far below that of human performance, we have achieved a large gain in speed of processing compared to a state-ofthe-art approach. 4 Conclusion We have presented an approach to discourse analysis based on transition-based algorithms for dependency and constituent trees. Dependency parsing is used to determine the syntactic structure of text, which is then used in discourse segmentation and parsing. A simple discriminative approach to segmentation results in an overall improvement in discourse parsing f-score, and the use of a linear-time algorithm results in an a large improvement in speed over a state-of-theart approach. Acknowledgments The work described here has been sponsored by the U.S. Army Research, Development, and Engineering Command (RDECOM). Statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred.</abstract>
<note confidence="0.8782956">References Buchholz, S. and Marsi, E. 2006. CoNLL-X shared on multilingual dependency parsing. In CoNLL 2006 Shared Carlson, L., Marcu, D., and Okurowski, M. E. 2003.</note>
<title confidence="0.859791">Building a discourse-tagged corpus in the frame-</title>
<author confidence="0.8131895">In J van_and R W Smith</author>
<author confidence="0.8131895">editors</author>
<note confidence="0.67327625">Directions in Discourse and Dialogue. Kluwer Academic Publishers. Charniak, E. 2000. A maximum-entropy-inspired In of NAACL.</note>
<author confidence="0.888029">statistical models for</author>
<affiliation confidence="0.9557375">language PhD dissertation, University of Pennsylvania.</affiliation>
<address confidence="0.643884">Collins, M. 2002. Discriminative Training Methods</address>
<title confidence="0.653925">for Hidden Markov Models: Theory and Experiwith Perceptron Algorithms. In of</title>
<note confidence="0.883367771428571">PA. Gillick, D. 2009. Sentence Boundary Detection and Problem with the U.S. In of the NAACL Short Papers. Colorado. Henderson, J., Merlo, P., Musillo, G., Titov, I. 2008. A Latent Variable Model of Synchronous Parsing Syntactic and Semantic In Proc. CoNLL 2008 Shared Manchester, UK. Mann, W. C. and Thompson, S. A. 1988. Rhetorical Structure Theory: toward a functional theory of organization. 8(3):243-281. Marcu, D. 1999. A decision-based approach to rheparsing. In of the Annual Meeting of the Association for Computational Linguistics. McDonald, R., Pereira, F., Ribarov, K., and Hajic, J. 2005. Non-projective dependency parsing using tree algorithms. In of Nivre, J. 2004. Incrementality in Deterministic De- Parsing. In Parsing: Bring- Engineering and Cognition Together (workshop at ACL-2004). Barcelona, Spain. Nivre, J. and Scholz, M. 2004. Deterministic Depend- Parsing of English Text. In of Sagae, K. and Lavie, A. 2005. A classifier-based with linear run-time complexity. In of IWPT. Sagae, K. and Tsujii, J. 2008. Shift-reduce depend- DAG parsing. In of COLING. Soricut, R. and Marcu, D. 2003. Sentence level discourse parsing using syntactic and lexical informa- In of Edmonton, Canada. Yamada, H. and Matsumoto, Y. 2003. Statistical dependency analysis with support vector machines. In of 84</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>E Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. of CoNLL</booktitle>
<note>Shared Task.</note>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Buchholz, S. and Marsi, E. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proc. of CoNLL 2006 Shared Task.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Carlson</author>
<author>D Marcu</author>
<author>M E Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory.</title>
<date>2003</date>
<booktitle>Current and New Directions in Discourse and Dialogue.</booktitle>
<editor>In J. van Kuppevelt and R. W. Smith, editors,</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="8421" citStr="Carlson et al. (2003)" startWordPosition="1418" endWordPosition="1421">items on the stack, in addition to subsets of words and POS tags described above, we also take the words and POS tags for the leftmost and rightmost children of each word in the EDU head set. Finally, we use feature templates that combine these and other individual features from Sagae and Lavie, who used a polynomial kernel and had no need for such templates (at the cost of increased time for both training and running). 3 Results To test our discourse parsing approach, we used the standard training and testing sections of the RST Discourse Treebank and the compacted 18- label set described by Carlson et al. (2003). We used approximately 5% of the standard training set as a development set. Our part-of-speech tagger and syntactic parser were not trained using the standard splits of the Penn Treebank for those tasks, since there are documents in the RST Discourse Treebank test section that are included in the usual training sets for POS taggers and parsers. The POS tagger and syntactic parser were then trained on sections 2 to 21 of the WSJ Penn Treebank, excluding the specific documents used in the test section of the RST Discourse Treebank. Table 1 shows the precision, recall and f-score of our discour</context>
</contexts>
<marker>Carlson, Marcu, Okurowski, 2003</marker>
<rawString>Carlson, L., Marcu, D., and Okurowski, M. E. 2003. Building a discourse-tagged corpus in the framework of Rhetorical Structure Theory. In J. van Kuppevelt and R. W. Smith, editors, Current and New Directions in Discourse and Dialogue. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="9554" citStr="Charniak (2000)" startWordPosition="1608" endWordPosition="1609">Discourse Treebank. Table 1 shows the precision, recall and f-score of our discourse segmentation approach on the test set, compared to that of Soricut and Marcu (2003) and Marcu (1999). In all cases, results were obtained with automatically produced syntactic structures. We also include the total time required for syntactic parsing (required in our Prec. Recall F-score Time Marcu99 83.3 77.1 80.1 - S&amp;M03 83.5 82.7 83.1 361s this work 87.4 86.0 86.7 40s Table 1: Precision, recall, f-score and time for discourse segmenters, tested on the RST Discourse Treebank. Time includes syntactic parsing, Charniak (2000) for S&amp;M03, and our implemetation of Nivre arc-standard for our segmenter. 83 F-score Time Marcu99 37.2 - S&amp;M03 49.0 481s this work 52.9 69s human 77.0 - Table 2: F-score for bracketing of RST discourse trees on the test set of the RST Discourse Treebank, and total time (syntactic parsing, segmentation and discourse parsing) required to parse the test set (S&amp;M03 and our approach were run on the same hardware). segmentation approach and Soricut and Marcu’s) and segmentation. For comparison with previous results, we include only segmentation within sentences (if all discourse boundaries are coun</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Charniak, E. 2000. A maximum-entropy-inspired parser. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Head-driven statistical models for natural language processing.</title>
<date>1999</date>
<institution>University of Pennsylvania.</institution>
<note>PhD dissertation,</note>
<contexts>
<context position="6158" citStr="Collins, 1999" startWordPosition="1002" endWordPosition="1003"> previously immediately below the top of the stack. The difference between reduce-left and reduce-right is whether the head of the new subtree comes from the left or right child. The algorithm assumes trees are lexicalized, and in our use of the algorithm for discourse parsing, heads are entire EDUs, and not single words. Our process for lexicalization of discourse trees, which is required for the parsing algorithm to function properly, is a simple percolation of “head EDUs,” performed in the same way as lexical heads can be assigned in Penn Treebankstyle trees using a head percolation table (Collins, 1999). To determine head EDUs, we use the nucleus/satellite status of nodes, as follows: for each node, the leftmost child with nucleus status is the head; if no child is a nucleus, the leftmost satellite is the head. Most nodes have exactly two children, one nucleus and one satellite. The parsing algorithm deals only with binary trees. We use the same binarization transform as Sagae and Lavie, converting the trees in the training set to binary trees prior to training the parser, and converting the binary trees produced by the parser at run-time into n-ary trees. As with the dependency parser and d</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Collins, M. 1999. Head-driven statistical models for natural language processing. PhD dissertation, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms.</title>
<date>2002</date>
<booktitle>In Proc. of EMNLP.</booktitle>
<location>Philadelphia, PA.</location>
<contexts>
<context position="2771" citStr="Collins, 2002" startWordPosition="421" endWordPosition="422">on is represented using dependencies, while Soricut and Marcu used constituent trees. 2.1 Syntactic parsing and discourse segmentation Assuming the document has been segmented into sentences, a task for which there are approaches with very high accuracy (Gillick, 2009), we start by finding the dependency structure for each sentence. This includes part-of-speech (POS) tagging using a CRF tagger trained on the Wall Street Journal portion of the Penn Treebank, and transition-based dependency parsing using the shift-reduce arc-standard algorithm (Vivre, 2004) trained with the averaged perceptron (Collins, 2002). The dependency parser is also trained with the WSJ Penn Treebank, converted to dependencies using the head percolation rules of Yamada and Matsumoto (2003). Discourse segmentation is performed as a binary classification task on each word, where the decision is whether or not to insert an EDU boundary between the word and the next word. In a sentence of length n, containing the words w1, w2 É wn, we perform one classification per word, in order. For word wi, the binary choice is whether to insert an EDU boundary between wi and wi+1. The EDUs are then the words between EDU boundaries (assuming</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>Collins, M. 2002. Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proc. of EMNLP. Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gillick</author>
</authors>
<title>Sentence Boundary Detection and the Problem with the U.S.</title>
<date>2009</date>
<booktitle>In Proc. of the NAACL HLT: Short Papers.</booktitle>
<location>Boulder, Colorado.</location>
<contexts>
<context position="2426" citStr="Gillick, 2009" startWordPosition="371" endWordPosition="372">entences, while Soricut and Marcu’s processes one sentence at a time, independently, finding only discourse relations within individual sentences. Parsing entire documents at a time is made possible in our approach through the use of lineartime transition-based parsing. An additional minor difference is that in our approach syntactic information is represented using dependencies, while Soricut and Marcu used constituent trees. 2.1 Syntactic parsing and discourse segmentation Assuming the document has been segmented into sentences, a task for which there are approaches with very high accuracy (Gillick, 2009), we start by finding the dependency structure for each sentence. This includes part-of-speech (POS) tagging using a CRF tagger trained on the Wall Street Journal portion of the Penn Treebank, and transition-based dependency parsing using the shift-reduce arc-standard algorithm (Vivre, 2004) trained with the averaged perceptron (Collins, 2002). The dependency parser is also trained with the WSJ Penn Treebank, converted to dependencies using the head percolation rules of Yamada and Matsumoto (2003). Discourse segmentation is performed as a binary classification task on each word, where the deci</context>
</contexts>
<marker>Gillick, 2009</marker>
<rawString>Gillick, D. 2009. Sentence Boundary Detection and the Problem with the U.S. In Proc. of the NAACL HLT: Short Papers. Boulder, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Henderson</author>
<author>P Merlo</author>
<author>G Musillo</author>
<author>I Titov</author>
</authors>
<title>A Latent Variable Model of Synchronous Parsing for Syntactic and Semantic Dependencies.</title>
<date>2008</date>
<booktitle>In Proc. of CoNLL</booktitle>
<location>Manchester, UK.</location>
<marker>Henderson, Merlo, Musillo, Titov, 2008</marker>
<rawString>Henderson, J., Merlo, P., Musillo, G., Titov, I. 2008. A Latent Variable Model of Synchronous Parsing for Syntactic and Semantic Dependencies. In Proc. of CoNLL 2008 Shared Task, Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W C Mann</author>
<author>S A Thompson</author>
</authors>
<title>Rhetorical Structure Theory: toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text,</tech>
<pages>8--3</pages>
<marker>Mann, Thompson, 1988</marker>
<rawString>Mann, W. C. and Thompson, S. A. 1988. Rhetorical Structure Theory: toward a functional theory of text organization. Text, 8(3):243-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>A decision-based approach to rhetorical parsing.</title>
<date>1999</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="9124" citStr="Marcu (1999)" startWordPosition="1541" endWordPosition="1542">peech tagger and syntactic parser were not trained using the standard splits of the Penn Treebank for those tasks, since there are documents in the RST Discourse Treebank test section that are included in the usual training sets for POS taggers and parsers. The POS tagger and syntactic parser were then trained on sections 2 to 21 of the WSJ Penn Treebank, excluding the specific documents used in the test section of the RST Discourse Treebank. Table 1 shows the precision, recall and f-score of our discourse segmentation approach on the test set, compared to that of Soricut and Marcu (2003) and Marcu (1999). In all cases, results were obtained with automatically produced syntactic structures. We also include the total time required for syntactic parsing (required in our Prec. Recall F-score Time Marcu99 83.3 77.1 80.1 - S&amp;M03 83.5 82.7 83.1 361s this work 87.4 86.0 86.7 40s Table 1: Precision, recall, f-score and time for discourse segmenters, tested on the RST Discourse Treebank. Time includes syntactic parsing, Charniak (2000) for S&amp;M03, and our implemetation of Nivre arc-standard for our segmenter. 83 F-score Time Marcu99 37.2 - S&amp;M03 49.0 481s this work 52.9 69s human 77.0 - Table 2: F-score</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Marcu, D. 1999. A decision-based approach to rhetorical parsing. In Proc. of the Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proc. of HLT/EMNLP.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>McDonald, R., Pereira, F., Ribarov, K., and Hajic, J. 2005. Non-projective dependency parsing using spanning tree algorithms. In Proc. of HLT/EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Incrementality in Deterministic Dependency Parsing. In Incremental Parsing: Bringing Engineering and Cognition Together (workshop at ACL-2004).</title>
<date>2004</date>
<location>Barcelona,</location>
<marker>Nivre, 2004</marker>
<rawString>Nivre, J. 2004. Incrementality in Deterministic Dependency Parsing. In Incremental Parsing: Bringing Engineering and Cognition Together (workshop at ACL-2004). Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>M Scholz</author>
</authors>
<title>Deterministic Dependency Parsing of English Text.</title>
<date>2004</date>
<booktitle>In Proc. of COLING.</booktitle>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Nivre, J. and Scholz, M. 2004. Deterministic Dependency Parsing of English Text. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>A Lavie</author>
</authors>
<title>A classifier-based parser with linear run-time complexity.</title>
<date>2005</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="4791" citStr="Sagae and Lavie, 2005" startWordPosition="763" endWordPosition="766">eatures (in pairs or triples). Classification was done with the averaged perceptron. 2.2 Transition-based discourse parsing RST trees can be represented in a similar way as constituent trees in the Penn Treebank, with a few differences: the trees represent entire documents, instead of single sentences; the leaves of the trees are EDUs consisting of one or more contiguous words; and the node labels contain nucleus/satellite status, and possibly the name of a discourse relation. Once the document has been segmented into a sequence of EDUs, we use a transition-based constituent parsing approach (Sagae and Lavie, 2005) to build an RST tree for the document. 82 Sagae and Lavie’s constituent parsing algorithm uses a stack that holds subtrees, and consumes the input string (in our case, a sequence of EDUs) from left to right, using four types of actions: (1) shift, which removes the next token from the input string, and pushes a subtree containing exactly that token onto the stack; (2) reduce-unary-LABEL, which pops the stack, and push onto it a new subtree where a node with label LABEL dominates the subtree that was popped (3) reduce-left-LABEL, and (4) reduceright-LABEL, which each pops two items from the st</context>
</contexts>
<marker>Sagae, Lavie, 2005</marker>
<rawString>Sagae, K. and Lavie, A. 2005. A classifier-based parser with linear run-time complexity. In Proc. of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>J Tsujii</author>
</authors>
<title>Shift-reduce dependency DAG parsing.</title>
<date>2008</date>
<booktitle>In Proc. of COLING.</booktitle>
<marker>Sagae, Tsujii, 2008</marker>
<rawString>Sagae, K. and Tsujii, J. 2008. Shift-reduce dependency DAG parsing. In Proc. of COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>In Proc. of NAACL.</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="1556" citStr="Soricut and Marcu (2003)" startWordPosition="239" endWordPosition="242">lite are in some ways analogous to head and dependent in syntactic dependencies, RST allows for multi-nuclear relations, where two nodes marked as nucleus can be linked into one node. Our parsing framework includes three components: (1) syntactic dependency parsing, where standard techniques for sentence-level parsing are applied; (2) discourse segmentation, which uses syntactic and lexical information to segment text into EDUs; and (3) discourse parsing, which produces a discourse structure tree from a string of EDUs, also benefiting from syntactic information. In contrast to the approach of Soricut and Marcu (2003), which also includes syntactic parsing, discourse segmentation and discourse parsing, our approach assumes that the unit of processing for discourse parsing is an entire document, and that discourse relations may exist within sentences as well as across sentences, while Soricut and Marcu’s processes one sentence at a time, independently, finding only discourse relations within individual sentences. Parsing entire documents at a time is made possible in our approach through the use of lineartime transition-based parsing. An additional minor difference is that in our approach syntactic informat</context>
<context position="9107" citStr="Soricut and Marcu (2003)" startWordPosition="1536" endWordPosition="1539">evelopment set. Our part-of-speech tagger and syntactic parser were not trained using the standard splits of the Penn Treebank for those tasks, since there are documents in the RST Discourse Treebank test section that are included in the usual training sets for POS taggers and parsers. The POS tagger and syntactic parser were then trained on sections 2 to 21 of the WSJ Penn Treebank, excluding the specific documents used in the test section of the RST Discourse Treebank. Table 1 shows the precision, recall and f-score of our discourse segmentation approach on the test set, compared to that of Soricut and Marcu (2003) and Marcu (1999). In all cases, results were obtained with automatically produced syntactic structures. We also include the total time required for syntactic parsing (required in our Prec. Recall F-score Time Marcu99 83.3 77.1 80.1 - S&amp;M03 83.5 82.7 83.1 361s this work 87.4 86.0 86.7 40s Table 1: Precision, recall, f-score and time for discourse segmenters, tested on the RST Discourse Treebank. Time includes syntactic parsing, Charniak (2000) for S&amp;M03, and our implemetation of Nivre arc-standard for our segmenter. 83 F-score Time Marcu99 37.2 - S&amp;M03 49.0 481s this work 52.9 69s human 77.0 -</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>Soricut, R. and Marcu, D. 2003. Sentence level discourse parsing using syntactic and lexical information. In Proc. of NAACL. Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Yamada</author>
<author>Y Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proc. of IWPT.</booktitle>
<contexts>
<context position="2928" citStr="Yamada and Matsumoto (2003)" startWordPosition="444" endWordPosition="447">the document has been segmented into sentences, a task for which there are approaches with very high accuracy (Gillick, 2009), we start by finding the dependency structure for each sentence. This includes part-of-speech (POS) tagging using a CRF tagger trained on the Wall Street Journal portion of the Penn Treebank, and transition-based dependency parsing using the shift-reduce arc-standard algorithm (Vivre, 2004) trained with the averaged perceptron (Collins, 2002). The dependency parser is also trained with the WSJ Penn Treebank, converted to dependencies using the head percolation rules of Yamada and Matsumoto (2003). Discourse segmentation is performed as a binary classification task on each word, where the decision is whether or not to insert an EDU boundary between the word and the next word. In a sentence of length n, containing the words w1, w2 É wn, we perform one classification per word, in order. For word wi, the binary choice is whether to insert an EDU boundary between wi and wi+1. The EDUs are then the words between EDU boundaries (assuming boundaries exist in the beginning and end of each sentence). The features used for classification are: the current word, its POS tag, its dependency label, </context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Yamada, H. and Matsumoto, Y. 2003. Statistical dependency analysis with support vector machines. In Proc. of IWPT.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>