<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99801625">
ummrization: Scaling Up M
Hierarchical Summarization:
Scaling Up Multi-Document Summarization
Summarization
</title>
<author confidence="0.989038">
Janara Christensen Stephen Soderland
</author>
<affiliation confidence="0.910415333333333">
Computer Science &amp; Engineering
University of Washington
Seattle, USA
</affiliation>
<email confidence="0.9934715">
janara@cs.washington.edu
soderlan@cs.washington.edu
</email>
<author confidence="0.948444">
Gagan Bansal Mausam
</author>
<affiliation confidence="0.911115333333333">
Computer Science &amp; Engineering
Indian Institute of Technology
Delhi, India
</affiliation>
<email confidence="0.815029333333333">
gaganbansal1993@gmail.com
mausam@cse.iitd.ac.in
s
</email>
<sectionHeader confidence="0.953752" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9984488">
Multi-document summarization (MDS)
or topics that coer large amouns of in-
ormation simple short summaries are in
systems have been designed for short, un-
ufficient complex topics require more
structured summaries of 10-15 documents,
nformation and moe stucture to under
and are inadequate for larger document
tand Weprpose a new pproah to scal
collections. We propose a new approach
to scaling up summarization called hierar-
gummitilldhihicl
chical summarization, and present the first
umition d pes th fit il
implemented system, SUMMA.
nd SM
SUMMA produces a hierarchy of relatively
short summaries, in whichthe top level
UM pdc a rarchy o reivy
provides a general overview and users can
t su, w t p v p
navigate the hierarchy to drill down for
ides a geneal overviw and users can
more details on topics of interest. SUMMA
avigate the hierarchy to drill own for
more details on topics of interet. Com
optimizes for coherence as well as cover-
ageof salient information. In an Amazon
ared to flat multidocumnt summaries,
Mechanical Turk evaluation, users pref-
sers prefer SUMMA ten times as often
nd larn just as much, and compared to
ered SUMMA ten times as often as flat
imelines, users prefr SUMMA three tmes
MDS and three times as often as timelines.
</bodyText>
<sectionHeader confidence="0.998781" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.90570575">
The explosion in the number of documents
on the Web necessitates automated approaches
Intoduction
that organize and summarize large document col-
The explosion in the number of documents ove
he Web necessitates auomatd approaches that
lections on a complex topic. Existing methods
for multi-document summarization (MDS) are de-
rganize nd summarize large document collec
ion on a comple topic Eisting thod fo
signed to produce short summaries of 10-15 doc-
uments.1 MDS systems do not scale to data sets
ltidmt ummitio (MDS) hdl
ten times larger and proportionately longer sum-
015duentdcathfltuy
maries: they either cannot run on large input or
r iffii fo lag umizatin.
produce a disorganized summary that is difficult
ii d
to understand.
o lrgsle
ersWe present a novel MDS paradigm, hierarchi-
ga e informn creny nd
nable personalized interaction with the summary
cal summarization, which operates on large doc-
o that users can explore the various aspects of in
ument collections, creating summaries that orga-
ormation in different levels of deail based on in
nize the information coherently. It mimics how
ividual interet
someone with a general interest in a complex topic
would learnabout it from a expert – first, the ex-
To this end, we present a novel MDS paadigm,
pet would provide an overview, and then more
ierarchical summarzaion. Hierarchical summa-
t 1In the DUC evaluations, summaries have a budget of 665
g p g
ih
byts and cover 10 documents.
ollections. It mimics ho
</bodyText>
<figureCaption confidence="0.93013975">
A hierarchical summary of the 1998 embassy
Figure 1: n xample of a hierarchical summary for the 199
bombings. Each rectangle rpresets a summary and each
embassy bombings, with one branch of the hierrchy high
</figureCaption>
<bodyText confidence="0.8290335">
xi,j isda sentencecwithin aesummary. The root summary pro-
videssantoverview of thetevents of August 1998. When the
third sntece is slected, a more detailed summary of the
provdes an overview of the events of August 1998 When th
missile striks is displayed. Selcting the second sentence of
last sentence is selected a more detailed summary of the mi
that summary produces a more detailed summary of the US’
sile strikes is produced and when the middle sentence of th
options.
summar
specific information about various aspects. Hi-
it fom a pt fit, he pert wl giv
erarchical summarization has the following novel
an eiw, d h ifi ifori
characteristics:
but vi ps I the fog nve
</bodyText>
<listItem confidence="0.854838333333333">
• The summary is hierarchically organized
ci
along one or more organizational principles
such as time, location, entities, or events.
• Each non-leaf summary is associated with a
set of child summaries where each gives de-
tails of an element (e.g. sentence) in the par-
ent summary.
• A user can navigate within the hierarchical
summary by clicking on an element of a par-
ent summary to view the associated child
summary.
</listItem>
<bodyText confidence="0.994492916666667">
For example, given the topic, “1998 embassy
bombings,” the first summary (Figure 1) might
On Aug 7 1998, car bombs
exploded outside US em-
bassies in Kenya and Tanza-
nia. Several days later, the
US began investigations into
bombings. The US retali-
ated with missile strikes on
suspected terrorist camps
in Afghanistan and Sudan
on Aug 20.
</bodyText>
<figure confidence="0.974592347826087">
x1,1
x1,2
01,s
x2,1
x2,2
x3,1
x3,2
x3,3
x4,1
04,2
x4,3
x5,1
x5,2
x5,3
x6,1
x6,2
x7,1
x7,2
x8,1
x8,2
x8,3
x9,1
x9,2
</figure>
<figureCaption confidence="0.896315285714286">
Some questioned the timing
of Clinton’s decision to launch
strikes. On Aug 22 with bin
aden having survived the
strikes the US outlined other
efforts to damage his net-
work. Russia, Sudan, Pakistan,
and Afghanistan condemned the
strikes.
Clinton proposed meth-
ods to inflict financial
damage on bin Laden.
Another possibility is
for the United States to
negotiate with the Tal-
iban to surrender bin
Laden. But diplomats
who have dealt with
the Taliban doubt that
anything could come of
such negotiations.
</figureCaption>
<page confidence="0.965184">
902
</page>
<bodyText confidence="0.942053166666667">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 902–912,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
mention that the US retaliated by striking
Afghanistan and Sudan. The user can click on this
information to learn more about these attacks. In
this way, the system can present large amounts of
information without overwhelming the user, and
the user can tailor the output to their interests.
In this paper, we describe SUMMA, the first
hierarchical summarization system for multi-
document summarization.2 It operates on a corpus
of related news articles. SUMMA hierarchically
clusters the sentences by time, and then summa-
rizes the clusters using an objective function that
optimizes salience and coherence.
We conducted an Amazon Mechanical Turk
(AMT) evaluation where AMT workers compared
the output of SUMMA to that of timelines and flat
summaries. SUMMA output was judged superior
more than three times as often as timelines, and
users learned more in twice as many cases. Users
overwhelmingly preferred hierarchical summaries
to flat summaries (92%) and learned just as much.
Our main contributions are as follows:
</bodyText>
<listItem confidence="0.9720675">
• We introduce and formalize the novel task of
hierarchical summarization.
• We present SUMMA, the first hierarchical
summarization system, which operates on
news corpora and summarizes over an or-
der of magnitude more documents than tra-
ditional MDS systems, producing summaries
an order of magnitude larger.
• We present a user study which demonstrates
the value of hierarchical summarization over
timelines and flat multi-document summaries
in learning about a complex topic.
</listItem>
<bodyText confidence="0.999945">
In the next section, we formalize hierarchical
summarization. We then describe our methodol-
ogy to implement the SUMMA hierarchical sum-
marization system: hierarchical clustering in Sec-
tion 3 and creating summaries based on that clus-
tering in Section 4. We discuss our experiments in
Section 5, related work in Section 6, and conclu-
sions in Section 7.
</bodyText>
<sectionHeader confidence="0.98314" genericHeader="method">
2 Hierarchical Summarization
</sectionHeader>
<bodyText confidence="0.999992857142857">
We propose a new task for large-scale summariza-
tion called hierarchical summarization. Input to a
hierarchical summarization system is a set of re-
lated documents D and a budget b for each sum-
mary within the hierarchy (in bytes, words, or sen-
tences). The output is the hierarchical summary
H, which we define formally as follows.
</bodyText>
<footnote confidence="0.840755">
2http://knowitall.cs.washington.edu/summa/
</footnote>
<bodyText confidence="0.999610472222222">
Definition A hierarchical summary H of a docu-
ment collection D is a set of summaries X orga-
nized into a hierarchy. The top of the hierarchy
is a summary X1 representing all of D, and each
summary Xi consists of summary units xi,j (e.g.
the jth sentence of summary i) that point to a child
summary, except at the leaf nodes of the hierarchy.
A child summary adds more detail to the infor-
mation in its parent summary unit. The child sum-
mary may include sub-events or background and
reactions to the event or topic in the parent.
We define several metrics in Section 4 for
a well-constructed hierarchical summary. Each
summary should maximize coverage of salient in-
formation; it should minimize redundancy; and
it should have intra-cluster coherence as well as
parent-to-child coherence.
Hierarchical summarization has two important
strengths in the context of large-scale summariza-
tion. First, the information presented at the start
is small and grows only as the user directs it, so
as not to overwhelm the user. Second, each user
directs his or her own experience, so a user inter-
ested in one aspect need only explore that section
of the data without having to view or understand
the entire summary. The parent-to-child links pro-
vide a means for a user to navigate, drilling down
for more details on topics of interest.
There are several possible organizing principles
for the hierarchy – by date, by entities, by loca-
tions, or by events. Some organizing principles
will fit the data in a document collection better
than others. A system may select different orga-
nization for different portions of the hierarchy, for
example, organizing first by location or prominent
entity and then by date for the next level.
</bodyText>
<sectionHeader confidence="0.981556" genericHeader="method">
3 Hierarchical Clustering
</sectionHeader>
<bodyText confidence="0.999869857142857">
Having defined the task, we now describe
the methodology behind our implementation,
SUMMA. In future work we intend to design a
system that dynamically selects the best organiz-
ing principle for each level of the hierarchy. In
this first implementation, we have opted for tem-
poral organization, since this is generally the most
appropriate for news events.
The problem of hierarchical summarization as
described in Section 2 has all of the requirements
of MDS, and additional complexities of inducing a
hierarchical structure, processing an order of mag-
nitude bigger input, generating a much larger out-
put, and enforcing coherence between parent and
</bodyText>
<page confidence="0.997857">
903
</page>
<figure confidence="0.741289">
Hierarchical Clustering H Hierarchical Summary X
</figure>
<figureCaption confidence="0.91391225">
Figure 2: Examples of input and output o hierarchical sum p
Figu 2: Exampes of a hierarchical clutering and a hier
marization. The inputhsentencesutarents E S, the number
inputnsentencesnisuN,nandethe summary sentences are x ∈
</figureCaption>
<bodyText confidence="0.9989924">
ple). A hierarchical clustering is a tree in which
a cluster gp is thetparenteof cluster gc,nthen-eac
sentence in gc is alsouinergp. Thiseorganizes t
information into manageable, semantically-relate
sections and induces a hierarchical structure over
</bodyText>
<equation confidence="0.41919775">
s in g� in g� Th ogae
ifi i bl illl
the input.
section
</equation>
<bodyText confidence="0.998986387096774">
Tthe hierarchical clustering serves as input to th
second steper—rcsummarizing given the hierarch
Theshierarchicalssummary follows theehierarchi-
cal structure of the clustering. Each node in the
hierarchy has an associated flat summary, which
summarizeszthehsentences iin that cluster. More-
over,othetnumbereoffssntencessiin a flat summary is
exactlycequalutottheenumberoof child clusters of the
node, since the user will click a sentencegto get to
the child summary. See Figure 2 for an illustration
of this correspondence.
Becauseuwe areiinterestedarin temporal hierar-
chicalesummarization, we hierarchicallyycluster all
the sentences in the input documents by time.
Unfortunately, neither agglomerative nor divisive
clustering is suitable, since both assumeaa binary
splitfat eachpnodea(Berkhin,n2006). Theunumber of
clusters at each split should be what is most natural
for the input data. We design a recursive clustering
algorithm that automatically chooses the appropri-
ate number of clusters at each split.
Before clustering, we timestamp all sentences.
We use SUTime (Chang and Manning, 2012) to
normalize temporal references, and we parse the
sentences with the Stanford parser (Klein and
Manning, 2003) and use a set of simple heuristics
where C is a clustering, BC
to determine if the timestampspinpthe sentence re-
mrfcacpli
fer to the root verb. If no timestamp is given, we
use the article date.
</bodyText>
<subsectionHeader confidence="0.996885">
3.1 Temporal Clustering
</subsectionHeader>
<bodyText confidence="0.990847782608695">
After acquiringathe timestamps, we must hierar-
chically cluster thessentenceseinto sets that make
sense to summarize together. Since we wish to
partition along the temporal dimension, our prob-
lempreduces toridentifying the best dates at which
to split a cluster intopsubclusters. We-identify these
datessby looking for bursts of activity.
News tends to be bursty – many arti
cles on a
topic appear at once and then taper out (Kleinberg,
2002). For example, Figure 3 shows the number of
articles per day related to the 1998 embassy bomb-
ings published in the New York Times (identified
using a key word search). There were two main
events – on the 7th, the embassies were bombed
and on the 20th, the US retaliated through mis-
sile strikes. The figure shows a correspondence
between these events and news spikes.
Ideal splits for this example would occur just
before each spike in coverage. However, when
there is little differentiation in news coverage, we
prefer clusters evenly spaced acrossntime. We thus
chooseiclusters C = {c1, ... , ckI as follows:
</bodyText>
<equation confidence="0.993306">
αE(C) (1)
</equation>
<bodyText confidence="0.79664575">
is the burstiness
of the set of clusters, E(C) is the evenness of the
clusters, and α is the tradoff parameter.
set of cluters E(C) is the evenness of the
</bodyText>
<equation confidence="0.987943666666667">
B(C)p= 1:burst(c)
∈C
c)
</equation>
<bodyText confidence="0.821553125">
burst(c) is the difference in the number of sen-
s()edifencehubese
tences published the day before the first date in c
blihed th d bf the fit dt i
and the average number of sentences published on
e average number of sentences published on
the first an secnd date of c:
st and second date of c:
</bodyText>
<equation confidence="0.9998645">
) pub(di) + pub(di+l)
burst(c) = 2 — pub(di−1) (3)
</equation>
<bodyText confidence="0.99996975">
where d is a date indexed over time, such that dj
is a day before dj+1, and di is the first date in c.
pub(di) is the number of sentences published on
di. The evenness of the split is measured by:
</bodyText>
<equation confidence="0.653141">
size(c) (4)
</equation>
<bodyText confidence="0.970709">
child summaries.
where size(c) is the number of dates in cluster c.
We perform hierarchical clustering top-down, at
each point solving for Equation 1. α was set using
a grid-search over a development set.
</bodyText>
<figure confidence="0.953290294117647">
x5,1
x5,2
x5,3
x2,1
x2,2
x5,1
xs,2
x1,1
x1,2
x1,3
x3,1
x3,2
x3,3
x7,1
x7,2
x7,3
x4,1
x4,2
x8,1
x8,2
sj+1
. . .
sl
sl+1
. . .
sN
si+1
. . .
sj
s1
. . .
sk
sk+1
. . .
si
sj+1
. . .
sN
s1
. . .
sN
s1
. . .
si
X.
ofe
to
if
h
he
d
</figure>
<bodyText confidence="0.970215666666667">
We simplify the problem by decomposing it in
two steps: hierarchical clustering and summariz-
ing over the clustering (see Figure 2 for an exam-
</bodyText>
<figure confidence="0.764149875">
Ymaximize B(C) +
( )
(2)
E(C) = min
c∈C
904
2
1 12 1 1 1 2 22 2
</figure>
<figureCaption confidence="0.997149">
Figure 3: News coverage by date for the embassy bombings
in Tanzania and Kenya. There are spikes in the number of
articles published at the two major events.
</figureCaption>
<subsectionHeader confidence="0.99974">
3.2 Choosing the number of clusters
</subsectionHeader>
<bodyText confidence="0.9992284">
We cannot know a priori the number of clusters
for a given topic. However, when the number of
clusters is too large for the given summary budget,
the sentences will have to be too short, and when
the number of clusters is too small, we will not use
enough of the budget. We set the maximum num-
ber of clusters kmax and minimum number of clus-
ters kmin to be a function of the budget b and the
average sentence length in the cluster savg, such
that kmax - savg ≤ b and kmin - savg ≥ b/2.
Given a maximum and minimum number of
clusters, we must determine the appropriate num-
ber of clusters. At each level, we cluster the sen-
tences by the method described above and choose
the number of clusters k according to the gap
statistic (Tibshirani et al., 2000). Specifically, for
each level, the algorithm will cluster repeatedly
with k varying from the minimum to the maxi-
mum. The algorithm will return the k that max-
imizes the gap statistic:
</bodyText>
<equation confidence="0.999101">
Gapn(k) = E∗n{log(Wk)} − log(Wk) (5)
</equation>
<bodyText confidence="0.99994425">
where Wk is the score for the clusters computed
with Equation 1, and E∗n is the expectation under
a sample of size n from a reference distribution.
Ideally, the maximum depth of the clustering
would be a function of the number of sentences
in each cluster, but in our implementation, we set
the maximum depth to three, which works well for
the size of the datasets we use (300 articles).
</bodyText>
<sectionHeader confidence="0.969776" genericHeader="method">
4 Summarizing within the Hierarchy
</sectionHeader>
<bodyText confidence="0.999643">
After the sentences are clustered, we have a struc-
ture for the hierarchical summary that dictates the
number of summaries and the number of sentences
in each summary. We also have the set of sen-
tences from which each summary is drawn.
Intuitively, each cluster summary in the hierar-
chical summary should convey the most salient
information in that cluster. Furthermore, the hier-
archical summary should not include redundant
sentences. A hierarchical summary that is only
salient and nonredundant may still not be suitable
if the sentences within a cluster summary are dis-
connected or if the parent sentence for a summary
does not relate to the child summary. Thus, a hi-
erarchical summary must also have intra-cluster
coherence and parent-to-child coherence.
</bodyText>
<subsectionHeader confidence="0.978168">
4.1 Salience
</subsectionHeader>
<bodyText confidence="0.9999825">
Salience is the value of each sentence to the topic
from which the documents are drawn. We measure
salience of a summary (Sal(X)) as the sum of the
saliences of individual sentences (Ei Sal(xi)).
Following previous research in MDS, we com-
puted individual saliences using a linear regres-
sion classifier trained on ROUGE scores over the
DUC’03 dataset (Lin, 2004; Christensen et al.,
2013). This method finds those sentences more
salient that mention nouns or verbs that occur fre-
quently in the cluster.
In preliminary experiments, we noticed that
many sentences that were reaction sentences were
given a higher salience than action sentences. For
example, the reaction sentence, “President Clinton
vowed to track down the perpetrators behind the
bombs that exploded outside the embassies in Tan-
zania and Kenya on Friday,” would have a higher
score than the action sentence, “Bombs exploded
outside the embassies in Tanzania and Kenya on
Friday.” This problem occurs because the first sen-
tence has a higher ROUGE score (it covers more
important words than the second sentence). To ad-
just for this problem, we use only words identified
in the main clause (heuristically identified via the
parse tree) to compute our salience scores.
</bodyText>
<subsectionHeader confidence="0.926913">
4.2 Redundancy
</subsectionHeader>
<bodyText confidence="0.999210375">
We identify redundant sentences using a linear
regression classifier trained on a manually la-
beled subset of the DUC’03 sentences. The fea-
tures include shared noun counts, sentence length,
TF*IDF cosine similarity, timestamp difference,
and features drawn from information extraction
such as number of shared tuples in Open IE
(Mausam et al., 2012).
</bodyText>
<page confidence="0.991614">
905
</page>
<subsectionHeader confidence="0.994455">
4.3 Summary Coherence
</subsectionHeader>
<bodyText confidence="0.999608043478261">
We require two types of coherence: coherence be-
tween the parent and child summaries and coher-
ence within each summary Xi.
We rely on the approximate discourse graph
(ADG) that was proposed in (Christensen et al.,
2013) as the basis for measuring coherence. Each
node in the ADG is a sentence from the dataset.
An edge from sentence si to sj with positive
weight indicates that sj may follow si in a coher-
ent summary, e.g. continued mention of an event
or entity, or coreference link between si and sj.
A negative edge indicates an unfulfilled discourse
cue or co-reference mention.
Parent-to-Child Coherence: Users navigate the
hierarchical summary from parent sentence to
child summary, so if the parent sentence bears no
relation to the child summary, the user will be un-
derstandably confused. The parent sentence must
have positive evidence of coherence with the sen-
tences in its child summary.
We estimate parent to child coherence as the co-
herence between a parent sentence and each sen-
tence in its child summary as:
</bodyText>
<subsectionHeader confidence="0.994567">
4.4 Objective Function
</subsectionHeader>
<bodyText confidence="0.999909176470589">
Having estimated salience, redundancy, and two
forms of coherence, we can now put this informa-
tion together into a single objective function that
measures the quality of a candidate hierarchical
summary.
Intuitively, the objective function should bal-
ance salience and coherence. Furthermore, the
summary should not contain redundant informa-
tion and each cluster summary should honor the
given budget, i.e., maximum summary length b.
We treat redundancy and budget as hard con-
straints and coherence and salience as soft con-
straints. Lastly, we require that sentences are
drawn from the cluster that they represent and that
the number of sentences in the summary corre-
sponding to each non-leaf cluster c is equivalent
to the number of child clusters of c. We optimize:
</bodyText>
<equation confidence="0.996090555555555">
maximize: F(x) °= Sal(X) + βPCoh(X) + γCCoh(X)
`dc E C : �
s.t. i=1..|Xc |len(xc,i) &lt; b
`dxi, xj E X : redundant(xi, xj) = 0
`dc E C, `dxc E Xc : xc E c
`dc E C : JXcJ = #children(c)
� � wG+(xpc, xc,i)) (6) The tradeoff parameters Q and γ were set based
PCoh(X) = i=1..|Xc |on a development set.
c∈C
</equation>
<bodyText confidence="0.963508272727273">
where xpc is the parent sentence for cluster c and
wG+(xpc, xc,i) is the sum of the positive edge
weights from xpc to xc,i in the ADG G.
Intra-cluster Coherence: In traditional MDS, the
documents are usually quite focused, allowing for
highly focused summaries. In hierarchical sum-
marization, however, a cluster summary may span
hundreds of documents and a wide range of infor-
mation. For this reason, we may consider a sum-
mary acceptable even if it has limited positive evi-
dence of coherence in the ADG, as long as there
is no negative evidence in the form of negative
edges. For example, the following is a reasonable
summary for events spanning two weeks:
s1 Bombs exploded at two US embassies.
s2 US missiles struck in Afghanistan and Sudan.
Our measure of intra-cluster coherence mini-
mizes the number of missing references. These
are coreference mentions or discourse cues where
none of the sentences read before (either in an an-
cestor summary or in the current summary) con-
tain an antecedent:
</bodyText>
<equation confidence="0.9962045">
CCoh(X) = − � � #missingRef(xc,i) (7)
cEC i=1..|Xc|
</equation>
<subsectionHeader confidence="0.953191">
4.5 Algorithm
</subsectionHeader>
<bodyText confidence="0.999929956521739">
Optimizing this objective function is NP-hard, so
we approximate a solution by using beam search
over the space of partial hierarchical summaries.
Notice the contribution from a sentence depends
on individual salience, coherence (CCoh) based
on sentences visible on the user path down the hi-
erarchy to this sentence, and coherence (PCoh)
based on its parent sentence and its child sum-
mary. Since most of the sentence contributions de-
pend on the path from the root to the sentence, we
build our partial summary by incrementally adding
a sentence top-down in the hierarchy and from first
sentence to last within a cluster summary.
To account for PCoh, we estimate the contribu-
tion of the sentence by jointly identifying its best
child summary. However, we do not fix the child
summary at this time – we simply use it to estimate
PCoh when using that sentence. Since computing
the best child summary is also intractable we ap-
proximate a solution by a local search algorithm
over the child cluster.
Overall, our algorithm is a two level nested
search algorithm – beam search in the outer loop to
</bodyText>
<page confidence="0.99679">
906
</page>
<bodyText confidence="0.993470888888889">
word search. We selected topics which were be-
tween five and fifteen years old so that evaluators
would have relatively less pre-existing knowledge
about the topic.
search through the space of partial summaries and
local search (hill climbing with random restarts) in
the inner loop to pick the best sentence to add to
the existing partial summary. We use a beam of
size ten in our implementation.
</bodyText>
<note confidence="0.626272">
5.1 User Preference
</note>
<sectionHeader confidence="0.993956" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.9993652">
Our experiments are designed to evaluate how ef-
fective hierarchical summarization is in summa-
rizing a large, complex topic and how well this
helps users learn about the topic. Our evaluation
addresses the following questions:
</bodyText>
<listItem confidence="0.9956504">
• Do users prefer hierarchical summaries for
topic exploration? (Section 5.1)
• Are hierarchical summaries more effective
than other methods for learning about com-
plex events? (Section 5.2)
• How informative are the hierarchical sum-
maries compared to the other methods? (Sec-
tion 5.3)
• How coherent is the hierarchical structure in
the summaries? (Section 5.4)
</listItem>
<bodyText confidence="0.999954375">
We compared SUMMA against two baseline sys-
tems which represent the main NLP methods for
large-scale summarization: an algorithm for cre-
ating timelines over sentences (Chieu and Lee,
2004),3 and a state-of-the-art flat MDS system
(Lin and Bilmes, 2011).4 Each system was given
the same budget (over 10 times the traditional
MDS budget, which is 665 bytes).
We evaluated the questions on ten news topics,
representing a range of tasks: (1) Pope John Paul
II’s death and the 2005 Papal Conclave, (2) Bush v.
Gore, (3) the Tulip Revolution, (4) Daniel Pearl’s
kidnapping, (5) the Lockerbie bombing handover
of suspects, (6) the Kargil War, (7) NATO’s bomb-
ing of Yugoslavia in 1999, (8) Pinochet’s arrest in
London, (9) the 2005 London bombings, and (10)
the crash and investigation of SwissAir Flight 111.
We chose topics containing a set of related events
that unfolded over several months and were promi-
nent enough to be reported in at least 300 articles.
We drew our articles from the Gigaword corpus,
which contains articles from the New York Times
and other major newspapers. For each topic, we
used the 300 documents that best matched a key
</bodyText>
<footnote confidence="0.8426295">
3Unfortunately, we were unable to obtain more recent
timeline systems from authors of the systems.
4(Christensen et al., 2013) is a state-of-the-art coherent
MDS system, but does not scale to 300 documents.
</footnote>
<bodyText confidence="0.989155282051283">
In our first experiment, we simply wished to eval-
uate which system users most prefer. We hired
Amazon Mechanical Turk (AMT) workers and as-
signed two topics to each worker. We paired up
workers such that one worker would see output
from SUMMA for the first topic and a competing
system for the second and the other worker would
see the reverse. For quality control, we asked
workers to complete a qualification task first, in
which they were required to write a short summary
of a news article. We also manually removed spam
from our results. Previous work has used AMT
workers for summary evaluations and has shown
high correlations with expert ratings (Christensen
et al., 2013). Five workers were hired to view each
topic-system pair.
We asked the workers to choose which format
they preferred and to explain why. The results are
as follows:
SUMMA 76% TIMELINE 24%
SUMMA 92% FLAT-MDS 8%
Users preferred the hierarchical summaries
three times more often than timelines and over
ten times more often than flat summaries. When
we examined the reasons given by the users, we
found that the people who preferred the hierar-
chical summaries liked that they gave a big pic-
ture overview and were then allowed to drill down
deeper. Some also explained that it was eas-
ier to remember information when presented with
the overview first. Typical responses included,
“Could gather and absorb the information at my
own pace,” and, “Easier to follow and understand.”
When users preferred the timelines, they usually
remarked that it was more familiar, i.e. “I liked
the familiarity of the format. I am used to these
timelines and they feel comfortable.” Users com-
plained that the flat summaries were disjointed,
confusing, and very frustrating to read.
</bodyText>
<subsectionHeader confidence="0.996297">
5.2 Knowledge Acquisition
</subsectionHeader>
<bodyText confidence="0.9999326">
Evaluating how much a user learned is inherently
difficult, more so when the goal is to allow the user
the freedom to explore information based on indi-
vidual interest. For this reason, instead of asking a
set of predefined questions, we assess the knowl-
</bodyText>
<page confidence="0.994346">
907
</page>
<bodyText confidence="0.999965565217391">
edge gain by following the methodology of (Sha-
haf et al., 2012) – asking users to write a paragraph
summarizing the information learned.
Using the same setup as in the previous exper-
iment, for each topic, five AMT workers spent
three minutes reading through a timeline or sum-
mary and were then asked to write a description
of what they had learned. Workers were not al-
lowed to see the timeline or summary while writ-
ing. We collected five descriptions for each topic-
system combination. We then asked other AMT
workers to read and compare the descriptions writ-
ten by the first set of workers. Each evaluator was
presented with a corresponding Wikipedia article
and descriptions from a pair of users (timeline vs.
SUMMA or flat MDS vs. SUMMA). The descrip-
tions were randomly ordered to remove bias. The
workers were asked which user appeared to have
learned more and why. For each pair of descrip-
tions, four workers evaluated the pair. Standard
checks such as approval rating, location filtering,
etc. were used for removing spam. The results of
this experiment are as follows:
</bodyText>
<table confidence="0.510025333333333">
Prefer Indiff. Prefer
SUMMA 58% 17% TIMELINE 25%
SUMMA 40% 22% FLAT-MDS 38%
</table>
<bodyText confidence="0.99986876">
Descriptions written by workers using SUMMA
were preferred over twice as often as those from
timelines. We looked more closely at those cases
where the participants either preferred the time-
lines or were indifferent and found that this pref-
erence was most common when the topic was not
dominated by a few major events, but was instead
a series of similarly important events. For exam-
ple, in the kidnapping and beheading of Daniel
Pearl there were two or three obviously major
events, whereas in the Kargil War there were many
smaller important events. In latter cases, the hier-
archical summaries provided little advantage over
the timelines because it was more difficult to ar-
range the sentences hierarchically.
Since SUMMA was judged to be so much supe-
rior to flat MDS systems in Section 5.1, it is sur-
prising that users descriptions from flat MDS were
preferred nearly as often as those from SUMMA.
While the flat summaries were disjointed, they
were good at including salient information, with
the most salient tending to be near the start of the
summary. Thus, descriptions from both SUMMA
and flat MDS generally covered the most salient
information.
</bodyText>
<subsectionHeader confidence="0.979079">
5.3 Informativeness
</subsectionHeader>
<bodyText confidence="0.999584777777778">
In this experiment, we assess the salience of the
information captured by the different systems, and
the ability of SUMMA to organize the information
so that more important information is placed at
higher levels.
ROUGE Evaluation: We first automatically
assessed informativeness by calculating the
ROUGE-1 scores of the output of each of the sys-
tems. For the gold standard comparison summary,
we use the Wikipedia articles for the topics.5
Note that there is no good translation of ROUGE
for hierarchical summarization. Thus, we simply
use the traditional ROUGE metric, which will
not capture any of the hierarchical format. This
score will essentially serve as a rough measure of
coverage of the entire summary to the Wikipedia
article. The scores for each of the systems are as
follows:
</bodyText>
<table confidence="0.99241325">
P R F1
SUMMA 0.25 0.67 0.31
TIMELINE 0.28 0.65 0.33
FLAT-MDS 0.30 0.64 0.34
</table>
<bodyText confidence="0.999937956521739">
None of the differences are significant. From
this evaluation, one can gather that the systems
have similar coverage of the Wikipedia articles.
Manual Evaluation: While ROUGE serves as a
rough measure of coverage, we were interested in
gathering more fine-grained information on the in-
formativeness of each system. We performed an
additional manual evaluation that assesses the re-
call of important events for each system.
We first identified which events were most im-
portant in a news story. Because reading 300 arti-
cles per topic is impractical, we asked AMT work-
ers to read a Wikipedia article on the same topic
and then identify the three most important events
and the five most important secondary events. We
aggregated responses from ten workers per topic
and chose the three most common primary and five
most common secondary events.
One of the authors then manually identified the
presence of these events in the hierarchical sum-
maries, the timelines and the flat MDS summaries.
Below we show event recall (the percentage of the
events that were mentioned).
</bodyText>
<footnote confidence="0.958383333333333">
5We excluded one topic (the handover of the Lockerbie
bombing suspects) because the corresponding Wikipedia ar-
ticle had insufficient information.
</footnote>
<page confidence="0.985078">
908
</page>
<table confidence="0.745155666666667">
Events SUMMA TIMELINE FLAT-MDS
Prim. 96% 74% 93%
Sec. 76% 53% 64%
</table>
<bodyText confidence="0.980871217391304">
The difference in recall between SUMMA and
TIMELINE was significant in both cases, and the
difference between SUMMA and FLAT-MDS was
not. In general, the flat summaries were quite re-
dundant, which contributed to the slightly lower
event recall. The timelines, on the other hand,
were both incoherent and at the same time re-
ported less important facts.
We also evaluated at what level in the hierar-
chy the events were identified for the hierarchical
summaries. The event recall shows the percentage
of events mentioned at that level or above in the
hierarchical summary:
Events Level 1 Level 2 Level 3
Prim. 63% 81% 96%
Sec. 27% 51% 76%
81% of the primary events are present in the first
or second level, and 76% of the secondary events
are mentioned by the third level. While recog-
nizing primary events is relatively simple because
they are repeated frequently, identification of im-
portant secondary events often requires external
knowledge.
</bodyText>
<subsectionHeader confidence="0.98166">
5.4 Parent-to-Child Coherence
</subsectionHeader>
<bodyText confidence="0.998579095238095">
We next tested the hierarchical coherence. One of
the authors graded how much each non-leaf sen-
tence in a summary was coherent with its child
summary on a scale of one to five, with one be-
ing incoherent and five being perfectly coherent.
We used the coherence scale from DUC’04.6
Level 1 Level 2
Coherence 3.8 3.4
We found that for the top level of the summary,
the parent sentence generally represented the most
important event in the cluster and the child sum-
mary usually expressed details or reactions of the
event. The lower coherence scores were often the
result of too few lexical connections or lack of a
theme or story. While the facts of the sentences
made sense together, the summaries sometimes
did not read as if they were written by a human,
but as a series of disparate sentences.
For the second level, the problems were more
basic. The parent sentence occasionally expressed
a less important fact that the child summary did
</bodyText>
<footnote confidence="0.777801">
6http://duc.nist.gov/duc2004/quality.questions.txt
</footnote>
<bodyText confidence="0.999754277777778">
not then expand on or, more commonly, the child
summary was not focused enough. This result
stems from two problems in our algorithm. First,
summarizing sentences are rare, making good
choices for parent sentences difficult to find. The
second problem relates to the difficulty in identify-
ing whether two sentences are on the same topic.
For example, suppose the parent sentence is, “A
Swissair plane Wednesday night crashed off Nova
Scotia, Canada.” A very good child sentence is,
“The airline confirmed that all passengers died.”
However, based on their surface features, the sen-
tence, “A plane made an unscheduled landing after
a Swissair plane crashed off the coast of Canada,”
appears to be a better choice.
Even though there is scope for improvement, we
find these coherence scores encouraging for a first
algorithm for the task.
</bodyText>
<sectionHeader confidence="0.999989" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999902818181818">
Traditional approaches to large-scale summariza-
tion have included flat summaries and timelines.
There are two primary shortcomings to these ap-
proaches: first, they require the user to sort
through large amounts of potentially overwhelm-
ing information, and second, the output is static
– users with different interests will see the same
information. Below we describe related work on
traditional MDS, structured summaries, timelines,
discovering threads of documents and the uses of
hierarchies in generating summaries.
</bodyText>
<subsectionHeader confidence="0.947385">
6.1 Traditional MDS
</subsectionHeader>
<bodyText confidence="0.99997647368421">
Traditionally, MDS systems have focused on three
to six sentence summaries covering 10-15 docu-
ments. Most extractive summarization research
aims to maximize coverage while reducing redun-
dancy (e.g. (Carbonell and Goldstein, 1998; Sag-
gion and Gaizauskas, 2004; Radev et al., 2004)).
Lin and Bilmes (2011) proposed a state-of-the-art
system that uses submodularity in sentence selec-
tion to accomplish these goals. Christensen et al.
(2013) presented an algorithm for coherent MDS,
but it does not scale to larger output.
Structured Summaries: Some research has ex-
plored generating structured summaries. These
approaches attempt to identify major aspects of
a topic, but do not compile content to describe
those aspects. Rather, they rely on pre-existing, la-
beled paragraphs (for example, a paragraph titled,
“Symptoms of Meningitis”). Aspects are identi-
fied either by a training corpus of articles in the
</bodyText>
<page confidence="0.995343">
909
</page>
<bodyText confidence="0.999769833333333">
same domain (Sauper and Barzilay, 2009), by an
entity-aspect LDA model (Li et al., 2010), or by
Wikipedia templates of related topics (Yao et al.,
2011). These methods assume a common struc-
ture for all topics in a category, and do not allow
for more than two levels in the structure.
Timeline Generation: Recent papers in timeline
generation have emphasized the relationship with
summarization. Yan et al. (2011b) balanced co-
herence and diversity to create timelines, Yan et
al. (2011a) used inter-date and intra-date sentence
dependencies, and Chieu and Lee (2004) used sen-
tence similarity. Others have emphasized identify-
ing important dates, primarily by bursts of news
(Swan and Allen, 2000; Akcora et al., 2010; Hu
et al., 2011; Kessler et al., 2012). While time-
lines can be useful for understanding events, they
do not generalize to other domains. Additionally,
long timelines can be overwhelming, short time-
lines have low information content, and there is
no method for personalized exploration.
Document Threads: A related track of research
investigates discovering threads of documents.
While we aim to summarize collections of infor-
mation, this track seeks to identify relationships
between documents. This research operates on the
document level, while ours operates on the sen-
tence level. Shahaf and Guestrin (2010) formal-
ized the characteristics of a good chain of articles
and proposed an algorithm to connect two speci-
fied articles. Gillenwater et al. (2012) proposed
a probabilistic technique for extracting a diverse
set of threads from a given collection. Shahaf et
al. (2012) extended work on coherent threads to
finding coherent maps of documents, where a map
is set of intersecting threads representing how the
threads interact and relate.
Summarization and Hierarchies: A few papers
have examined the relationship between summa-
rization and hierarchies. Some focused on cre-
ating a hierarchical summary of a single docu-
ment (Buyukkokten et al., 2001; Otterbacher et
al., 2006), relying on the structure inherent in sin-
gle documents. Others investigated creating hier-
archies of words or phrases to organize documents
(Lawrie et al., 2001; Lawrie, 2003; Takahashi et
al., 2007; Haghighi and Vanderwende, 2009).
Other research identifies the hierarchical struc-
ture of the documents and generates a summary
that prioritizes more general information accord-
ing to the structure (Ouyang et al., 2009; Celikyil-
maz and Hakkani-Tur, 2010), or gains coverage by
drawing sentences from different parts of the hier-
archy (Yang and Wang, 2003; Wang et al., 2006).
</bodyText>
<sectionHeader confidence="0.999069" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999975">
We have introduced a new paradigm for large-
scale summarization called hierarchical summa-
rization, which allows a user to navigate a hier-
archy of relatively short summaries. We present
SUMMA, an implemented hierarchical news sum-
marization system,7 and demonstrate its effective-
ness in a user study that compares SUMMA with
a timeline system and a flat MDS system. When
compared to timelines, users learned more with
SUMMA in twice as many cases, and SUMMA was
preferred more than three times as often. When
compared to flat summaries, users overwhelming
preferred SUMMA and learned just as much.
This first implementation performs temporal
clustering – in future work, we will investigate dy-
namically selecting an organizing principle that is
best suited to the data at each level of the hierar-
chy: by entity, by location, by event, or by date.
We also intend to scale the system to even larger
document collections, and explore joint clustering
and summarization. Lastly, we plan to research
hierarchical summarization in other domains.
</bodyText>
<sectionHeader confidence="0.998387" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999797391304348">
We thank Amitabha Bagchi, Niranjan Balasubra-
manian, Danish Contractor, Oren Etzioni, Tony
Fader, Carlos Guestrin, Prachi Jain, Lucy Van-
derwende, Luke Zettlemoyer, and the anonymous
reviewers for their helpful suggestions and feed-
back. We thank Hui Lin and Jeff Bilmes for
providing us with their code. This research was
supported in part by ARO contract W911NF-
13-1-0246, DARPA Air Force Research Labora-
tory (AFRL) contract FA8750-13-2-0019, UW-
IITD subcontract RP02815, and the Yahoo! Fac-
ulty Research and Engagement Award. This pa-
per is also supported in part by the Intelligence
Advanced Research Projects Activity (IARPA)
via AFRL contract number FA8650-10-C-7058.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
The views and conclusions contained herein are
those of the authors and should not be interpreted
as necessarily representing the official policies
or endorsements, either expressed or implied, of
IARPA, AFRL, or the U.S. Government.
</bodyText>
<footnote confidence="0.976975">
7http://knowitall.cs.washington.edu/summa/
</footnote>
<page confidence="0.988318">
910
</page>
<bodyText confidence="0.7289212">
Jon Kleinberg. 2002. Bursty and hierarchical struc-
ture in streams. In Proceedings of the Eighth ACM
SIGKDD International Conference on Knowledge
Discovery and Data Mining, KDD ’02, pages 91–
101.
</bodyText>
<sectionHeader confidence="0.839631" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99083206185567">
C. G. Akcora, M. A. Bayir, M. Demirbas, and H. Fer-
hatosmanoglu. 2010. Identifying breakpoints in
public opinion. In 1st KDD Workshop on Social Me-
dia Analytics.
Berkhin Berkhin. 2006. A survey of clustering data
mining techniques. Grouping Multidimensional
Data, pages 25–71.
Orkut Buyukkokten, Hector Garcia-Molina, and An-
dreas Paepcke. 2001. Seeing the whole in parts:
Text summarization for web browsing on handheld
devices. In Proceedings of WWW 2001, pages 652–
662.
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings
of SIGIR 1998, pages 335–336.
Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hy-
brid hierarchical model for multi-document summa-
rization. In Proceedings of ACL 2010, pages 815–
824.
Angel Chang and Christopher Manning. 2012. SU-
Time: A library for recognizing and normalizing
time expressions. In Proceedings of LREC 2012.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of SIGIR 2004, pages 425–432.
Janara Christensen, Mausam, Stephen Soderland, and
Oren Etzioni. 2013. Towards coherent multi-
document summarization. In Proceedings of
NAACL 2013.
Jennifer Gillenwater, Alex Kulesza, and Ben Taskar.
2012. Discovering diverse and salient threads in
document collections. In Proceedings of EMNLP-
CoNLL 2012, pages 710–720.
Aria Haghighi and Lucy Vanderwende. 2009. Explor-
ing content models for multi-document summariza-
tion. Proceedings of NAACL 2009, pages 362–370.
Po Hu, Minlie Huang, Peng Xu, Weichang Li, Adam K.
Usadi, and Xiaoyan Zhu. 2011. Generating
breakpoint-based timeline overview for news topic
retrospection. In Proceedings of ICDM 2011.
Remy Kessler, Xavier Tannier, Caroline Hag`ege,
V´eronique Moriceau, and Andr´e Bittar. 2012. Find-
ing salient dates for building thematic timelines. In
Proceedings of ACL 2012, pages 730–739.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st Meeting of the Association for Computational
Linguistics, pages 423–430.
Dawn Lawrie, W. Bruce Croft, and Arnold Rosenberg.
2001. Finding topic words for hierarchical summa-
rization. In Proceedings of SIGIR ’01, pages 349–
357.
Dawn J. Lawrie. 2003. Language models for hierar-
chical summarization. Ph.D. thesis, University of
Massachusetts Amherst.
Peng Li, Jing Jiang, and Yinglin Wang. 2010. Gener-
ating templates of entity summaries with an entity-
aspect model and pattern mining. In Proceedings of
ACL 2010, pages 640–649.
Hui Lin and Jeff Bilmes. 2011. A class of submodular
functions for document summarization. In Proceed-
ings of ACL 2011, pages 510–520.
Chin-Yew Lin. 2004. ROUGE: A package for au-
tomatic evaluation of summaries. In Text Summa-
rization Branches Out: Proceedings of the ACL-04
Workshop, pages 74–81.
Mausam, Michael Schmitz, Robert Bart, Stephen
Soderland, and Oren Etzioni. 2012. Open language
learning for information extraction. In Proceedings
of EMNLP 2012, pages 523–534.
Jahna Otterbacher, Dragomir Radev, and Omer Ka-
reem. 2006. News to go: Hierarchical text sum-
marization for mobile devices. In Proceedings of
SIGIR 2006, pages 589–596.
You Ouyang, Wenji Li, and Qin Lu. 2009. An
integrated multi-document summarization approach
based on word hierarchical representation. In Pro-
ceedings of the ACLShort 2009, pages 113–116.
Dragomir R. Radev, Hongyan Jing, Malgorzata Stys,
and Daniel Tam. 2004. Centroid-based summariza-
tion of multiple documents. Information Processing
and Management, 40(6):919–938.
Horacio Saggion and Robert Gaizauskas. 2004. Multi-
document summarization by cluster/profile rele-
vance and redundancy removal. In Proceedings of
DUC 2004.
Christina Sauper and Regina Barzilay. 2009. Automat-
ically generating Wikipedia articles: A structure-
aware approach. In Proceedings ofACL 2009, pages
208–216.
Dafna Shahaf and Carlos Guestrin. 2010. Connecting
the dots between news articles. In Proceedings of
KDD 2010, pages 623–632.
Dafna Shahaf, Carlos Guestrin, and Eric Horvitz.
2012. Trains of thought: Generating information
maps. In Proceedings of WWW 2012.
</reference>
<page confidence="0.976837">
911
</page>
<reference confidence="0.99981228125">
Russell Swan and James Allen. 2000. Automatic gen-
eration of overview timelines. In Proceedings of SI-
GIR 2000, pages 49–56.
Kou Takahashi, Takao Miura, and Isamu Shioya. 2007.
Hierarchical summarizing and evaluating for web
pages. In Proceedings of the 1st workshop on
emerging research opportunities for Web Data Man-
agement (EROW 2007).
Robert Tibshirani, Guenther Walther, and Trevor
Hastie. 2000. Estimating the number of clusters in
a dataset via the gap statistic. Journal of the Royal
Statistical Society, Series B, 32(2):411–423.
Fu Lee Wang, Christopher C. Yang, and Xiaodong Shi.
2006. Multi-document summarization for terrorism
information extraction. In Proceedings of ISI’06.
Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,
Xiaoming Li, and Yan Zhang. 2011a. Timeline gen-
eration through evolutionary trans-temporal summa-
rization. In Proceedings of EMNLP 2011, pages
433–443.
Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,
Xiaoming Li, and Yan Zhang. 2011b. Evolutionary
timeline summarization: A balanced optimization
framework via iterative substitution. In Proceeding
of SIGIR 2011, pages 745–754.
Christopher C. Yang and Fu Lee Wang. 2003. Fractal
summarization: summarization based on fractal the-
ory. In Proceedings of SIGIR 2003, pages 391–392.
Conglei Yao, Xu Jia, Sicong Shou, Shicong Feng, Feng
Zhou, and Hongyan Liu. 2011. Autopedia: Auto-
matic domain-independent wikipedia article genera-
tion. In Proceedings of WWW 2011, pages 161–162.
</reference>
<page confidence="0.997346">
912
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.280545">
<title confidence="0.999188">ummrization: Scaling Up Hierarchical Summarization: Scaling Up Multi-Document Summarization</title>
<author confidence="0.999879">Janara Christensen Stephen</author>
<affiliation confidence="0.9998275">Computer Science &amp; University of</affiliation>
<address confidence="0.884102">Seattle, USA</address>
<email confidence="0.999533">soderlan@cs.washington.edu</email>
<author confidence="0.992776">Gagan Bansal Mausam</author>
<affiliation confidence="0.9996925">Computer Science &amp; Engineering Indian Institute of Technology</affiliation>
<address confidence="0.879955">Delhi, India</address>
<abstract confidence="0.9682355">mausam@cse.iitd.ac.in s Abstract Multi-document summarization (MDS) or topics that coer large amouns of inormation simple short summaries are in systems have been designed for short, unufficient complex topics require more structured summaries of 10-15 documents, nformation and moe stucture to under and are inadequate for larger document tand Weprpose a new pproah to scal collections. We propose a new approach scaling up summarization called hierarand present the first pes th fit system, a hierarchy of relatively short summaries, in whichthe top level a rarchy o reivy provides a general overview and users can t su, w t p v navigate the hierarchy to drill down for ides a geneal overviw and users can details on topics of interest. avigate the hierarchy to drill own for more details on topics of interet. Com optimizes for coherence as well as coverageof salient information. In an Amazon ared to flat multidocumnt summaries, Mechanical Turk evaluation, users prefprefer times as often nd larn just as much, and compared to times as often as flat users prefr tmes and three as often as</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C G Akcora</author>
<author>M A Bayir</author>
<author>M Demirbas</author>
<author>H Ferhatosmanoglu</author>
</authors>
<title>Identifying breakpoints in public opinion.</title>
<date>2010</date>
<booktitle>In 1st KDD Workshop on Social Media Analytics.</booktitle>
<contexts>
<context position="36761" citStr="Akcora et al., 2010" startWordPosition="6066" endWordPosition="6069"> Wikipedia templates of related topics (Yao et al., 2011). These methods assume a common structure for all topics in a category, and do not allow for more than two levels in the structure. Timeline Generation: Recent papers in timeline generation have emphasized the relationship with summarization. Yan et al. (2011b) balanced coherence and diversity to create timelines, Yan et al. (2011a) used inter-date and intra-date sentence dependencies, and Chieu and Lee (2004) used sentence similarity. Others have emphasized identifying important dates, primarily by bursts of news (Swan and Allen, 2000; Akcora et al., 2010; Hu et al., 2011; Kessler et al., 2012). While timelines can be useful for understanding events, they do not generalize to other domains. Additionally, long timelines can be overwhelming, short timelines have low information content, and there is no method for personalized exploration. Document Threads: A related track of research investigates discovering threads of documents. While we aim to summarize collections of information, this track seeks to identify relationships between documents. This research operates on the document level, while ours operates on the sentence level. Shahaf and Gue</context>
</contexts>
<marker>Akcora, Bayir, Demirbas, Ferhatosmanoglu, 2010</marker>
<rawString>C. G. Akcora, M. A. Bayir, M. Demirbas, and H. Ferhatosmanoglu. 2010. Identifying breakpoints in public opinion. In 1st KDD Workshop on Social Media Analytics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Berkhin Berkhin</author>
</authors>
<title>A survey of clustering data mining techniques. Grouping Multidimensional Data,</title>
<date>2006</date>
<pages>25--71</pages>
<marker>Berkhin, 2006</marker>
<rawString>Berkhin Berkhin. 2006. A survey of clustering data mining techniques. Grouping Multidimensional Data, pages 25–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Orkut Buyukkokten</author>
<author>Hector Garcia-Molina</author>
<author>Andreas Paepcke</author>
</authors>
<title>Seeing the whole in parts: Text summarization for web browsing on handheld devices.</title>
<date>2001</date>
<booktitle>In Proceedings of WWW</booktitle>
<pages>652--662</pages>
<contexts>
<context position="38015" citStr="Buyukkokten et al., 2001" startWordPosition="6260" endWordPosition="6263">aracteristics of a good chain of articles and proposed an algorithm to connect two specified articles. Gillenwater et al. (2012) proposed a probabilistic technique for extracting a diverse set of threads from a given collection. Shahaf et al. (2012) extended work on coherent threads to finding coherent maps of documents, where a map is set of intersecting threads representing how the threads interact and relate. Summarization and Hierarchies: A few papers have examined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated creating hierarchies of words or phrases to organize documents (Lawrie et al., 2001; Lawrie, 2003; Takahashi et al., 2007; Haghighi and Vanderwende, 2009). Other research identifies the hierarchical structure of the documents and generates a summary that prioritizes more general information according to the structure (Ouyang et al., 2009; Celikyilmaz and Hakkani-Tur, 2010), or gains coverage by drawing sentences from different parts of the hierarchy (Yang and Wang, 2003; Wang et al., 2006). </context>
</contexts>
<marker>Buyukkokten, Garcia-Molina, Paepcke, 2001</marker>
<rawString>Orkut Buyukkokten, Hector Garcia-Molina, and Andreas Paepcke. 2001. Seeing the whole in parts: Text summarization for web browsing on handheld devices. In Proceedings of WWW 2001, pages 652– 662.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of MMR, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>335--336</pages>
<contexts>
<context position="35368" citStr="Carbonell and Goldstein, 1998" startWordPosition="5846" endWordPosition="5849">e approaches: first, they require the user to sort through large amounts of potentially overwhelming information, and second, the output is static – users with different interests will see the same information. Below we describe related work on traditional MDS, structured summaries, timelines, discovering threads of documents and the uses of hierarchies in generating summaries. 6.1 Traditional MDS Traditionally, MDS systems have focused on three to six sentence summaries covering 10-15 documents. Most extractive summarization research aims to maximize coverage while reducing redundancy (e.g. (Carbonell and Goldstein, 1998; Saggion and Gaizauskas, 2004; Radev et al., 2004)). Lin and Bilmes (2011) proposed a state-of-the-art system that uses submodularity in sentence selection to accomplish these goals. Christensen et al. (2013) presented an algorithm for coherent MDS, but it does not scale to larger output. Structured Summaries: Some research has explored generating structured summaries. These approaches attempt to identify major aspects of a topic, but do not compile content to describe those aspects. Rather, they rely on pre-existing, labeled paragraphs (for example, a paragraph titled, “Symptoms of Meningiti</context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of MMR, diversity-based reranking for reordering documents and producing summaries. In Proceedings of SIGIR 1998, pages 335–336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-Tur</author>
</authors>
<title>A hybrid hierarchical model for multi-document summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL 2010,</booktitle>
<pages>815--824</pages>
<contexts>
<context position="38494" citStr="Celikyilmaz and Hakkani-Tur, 2010" startWordPosition="6331" endWordPosition="6335">ined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated creating hierarchies of words or phrases to organize documents (Lawrie et al., 2001; Lawrie, 2003; Takahashi et al., 2007; Haghighi and Vanderwende, 2009). Other research identifies the hierarchical structure of the documents and generates a summary that prioritizes more general information according to the structure (Ouyang et al., 2009; Celikyilmaz and Hakkani-Tur, 2010), or gains coverage by drawing sentences from different parts of the hierarchy (Yang and Wang, 2003; Wang et al., 2006). 7 Conclusions We have introduced a new paradigm for largescale summarization called hierarchical summarization, which allows a user to navigate a hierarchy of relatively short summaries. We present SUMMA, an implemented hierarchical news summarization system,7 and demonstrate its effectiveness in a user study that compares SUMMA with a timeline system and a flat MDS system. When compared to timelines, users learned more with SUMMA in twice as many cases, and SUMMA was prefer</context>
</contexts>
<marker>Celikyilmaz, Hakkani-Tur, 2010</marker>
<rawString>Asli Celikyilmaz and Dilek Hakkani-Tur. 2010. A hybrid hierarchical model for multi-document summarization. In Proceedings of ACL 2010, pages 815– 824.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angel Chang</author>
<author>Christopher Manning</author>
</authors>
<title>SUTime: A library for recognizing and normalizing time expressions.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="11915" citStr="Chang and Manning, 2012" startWordPosition="1881" endWordPosition="1884">ure 2 for an illustration of this correspondence. Becauseuwe areiinterestedarin temporal hierarchicalesummarization, we hierarchicallyycluster all the sentences in the input documents by time. Unfortunately, neither agglomerative nor divisive clustering is suitable, since both assumeaa binary splitfat eachpnodea(Berkhin,n2006). Theunumber of clusters at each split should be what is most natural for the input data. We design a recursive clustering algorithm that automatically chooses the appropriate number of clusters at each split. Before clustering, we timestamp all sentences. We use SUTime (Chang and Manning, 2012) to normalize temporal references, and we parse the sentences with the Stanford parser (Klein and Manning, 2003) and use a set of simple heuristics where C is a clustering, BC to determine if the timestampspinpthe sentence remrfcacpli fer to the root verb. If no timestamp is given, we use the article date. 3.1 Temporal Clustering After acquiringathe timestamps, we must hierarchically cluster thessentenceseinto sets that make sense to summarize together. Since we wish to partition along the temporal dimension, our problempreduces toridentifying the best dates at which to split a cluster intopsu</context>
</contexts>
<marker>Chang, Manning, 2012</marker>
<rawString>Angel Chang and Christopher Manning. 2012. SUTime: A library for recognizing and normalizing time expressions. In Proceedings of LREC 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Yoong Keok Lee</author>
</authors>
<title>Query based event extraction along a timeline.</title>
<date>2004</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>425--432</pages>
<contexts>
<context position="24131" citStr="Chieu and Lee, 2004" startWordPosition="3995" endWordPosition="3998">earn about the topic. Our evaluation addresses the following questions: • Do users prefer hierarchical summaries for topic exploration? (Section 5.1) • Are hierarchical summaries more effective than other methods for learning about complex events? (Section 5.2) • How informative are the hierarchical summaries compared to the other methods? (Section 5.3) • How coherent is the hierarchical structure in the summaries? (Section 5.4) We compared SUMMA against two baseline systems which represent the main NLP methods for large-scale summarization: an algorithm for creating timelines over sentences (Chieu and Lee, 2004),3 and a state-of-the-art flat MDS system (Lin and Bilmes, 2011).4 Each system was given the same budget (over 10 times the traditional MDS budget, which is 665 bytes). We evaluated the questions on ten news topics, representing a range of tasks: (1) Pope John Paul II’s death and the 2005 Papal Conclave, (2) Bush v. Gore, (3) the Tulip Revolution, (4) Daniel Pearl’s kidnapping, (5) the Lockerbie bombing handover of suspects, (6) the Kargil War, (7) NATO’s bombing of Yugoslavia in 1999, (8) Pinochet’s arrest in London, (9) the 2005 London bombings, and (10) the crash and investigation of SwissA</context>
<context position="36612" citStr="Chieu and Lee (2004)" startWordPosition="6042" endWordPosition="6045">ied either by a training corpus of articles in the 909 same domain (Sauper and Barzilay, 2009), by an entity-aspect LDA model (Li et al., 2010), or by Wikipedia templates of related topics (Yao et al., 2011). These methods assume a common structure for all topics in a category, and do not allow for more than two levels in the structure. Timeline Generation: Recent papers in timeline generation have emphasized the relationship with summarization. Yan et al. (2011b) balanced coherence and diversity to create timelines, Yan et al. (2011a) used inter-date and intra-date sentence dependencies, and Chieu and Lee (2004) used sentence similarity. Others have emphasized identifying important dates, primarily by bursts of news (Swan and Allen, 2000; Akcora et al., 2010; Hu et al., 2011; Kessler et al., 2012). While timelines can be useful for understanding events, they do not generalize to other domains. Additionally, long timelines can be overwhelming, short timelines have low information content, and there is no method for personalized exploration. Document Threads: A related track of research investigates discovering threads of documents. While we aim to summarize collections of information, this track seeks</context>
</contexts>
<marker>Chieu, Lee, 2004</marker>
<rawString>Hai Leong Chieu and Yoong Keok Lee. 2004. Query based event extraction along a timeline. In Proceedings of SIGIR 2004, pages 425–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janara Christensen</author>
<author>Stephen Soderland Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Towards coherent multidocument summarization.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL</booktitle>
<contexts>
<context position="17420" citStr="Christensen et al., 2013" startWordPosition="2877" endWordPosition="2880">ences within a cluster summary are disconnected or if the parent sentence for a summary does not relate to the child summary. Thus, a hierarchical summary must also have intra-cluster coherence and parent-to-child coherence. 4.1 Salience Salience is the value of each sentence to the topic from which the documents are drawn. We measure salience of a summary (Sal(X)) as the sum of the saliences of individual sentences (Ei Sal(xi)). Following previous research in MDS, we computed individual saliences using a linear regression classifier trained on ROUGE scores over the DUC’03 dataset (Lin, 2004; Christensen et al., 2013). This method finds those sentences more salient that mention nouns or verbs that occur frequently in the cluster. In preliminary experiments, we noticed that many sentences that were reaction sentences were given a higher salience than action sentences. For example, the reaction sentence, “President Clinton vowed to track down the perpetrators behind the bombs that exploded outside the embassies in Tanzania and Kenya on Friday,” would have a higher score than the action sentence, “Bombs exploded outside the embassies in Tanzania and Kenya on Friday.” This problem occurs because the first sent</context>
<context position="18872" citStr="Christensen et al., 2013" startWordPosition="3108" endWordPosition="3111">alience scores. 4.2 Redundancy We identify redundant sentences using a linear regression classifier trained on a manually labeled subset of the DUC’03 sentences. The features include shared noun counts, sentence length, TF*IDF cosine similarity, timestamp difference, and features drawn from information extraction such as number of shared tuples in Open IE (Mausam et al., 2012). 905 4.3 Summary Coherence We require two types of coherence: coherence between the parent and child summaries and coherence within each summary Xi. We rely on the approximate discourse graph (ADG) that was proposed in (Christensen et al., 2013) as the basis for measuring coherence. Each node in the ADG is a sentence from the dataset. An edge from sentence si to sj with positive weight indicates that sj may follow si in a coherent summary, e.g. continued mention of an event or entity, or coreference link between si and sj. A negative edge indicates an unfulfilled discourse cue or co-reference mention. Parent-to-Child Coherence: Users navigate the hierarchical summary from parent sentence to child summary, so if the parent sentence bears no relation to the child summary, the user will be understandably confused. The parent sentence mu</context>
<context position="25213" citStr="Christensen et al., 2013" startWordPosition="4177" endWordPosition="4180">’s bombing of Yugoslavia in 1999, (8) Pinochet’s arrest in London, (9) the 2005 London bombings, and (10) the crash and investigation of SwissAir Flight 111. We chose topics containing a set of related events that unfolded over several months and were prominent enough to be reported in at least 300 articles. We drew our articles from the Gigaword corpus, which contains articles from the New York Times and other major newspapers. For each topic, we used the 300 documents that best matched a key 3Unfortunately, we were unable to obtain more recent timeline systems from authors of the systems. 4(Christensen et al., 2013) is a state-of-the-art coherent MDS system, but does not scale to 300 documents. In our first experiment, we simply wished to evaluate which system users most prefer. We hired Amazon Mechanical Turk (AMT) workers and assigned two topics to each worker. We paired up workers such that one worker would see output from SUMMA for the first topic and a competing system for the second and the other worker would see the reverse. For quality control, we asked workers to complete a qualification task first, in which they were required to write a short summary of a news article. We also manually removed </context>
<context position="35577" citStr="Christensen et al. (2013)" startWordPosition="5878" endWordPosition="5881">low we describe related work on traditional MDS, structured summaries, timelines, discovering threads of documents and the uses of hierarchies in generating summaries. 6.1 Traditional MDS Traditionally, MDS systems have focused on three to six sentence summaries covering 10-15 documents. Most extractive summarization research aims to maximize coverage while reducing redundancy (e.g. (Carbonell and Goldstein, 1998; Saggion and Gaizauskas, 2004; Radev et al., 2004)). Lin and Bilmes (2011) proposed a state-of-the-art system that uses submodularity in sentence selection to accomplish these goals. Christensen et al. (2013) presented an algorithm for coherent MDS, but it does not scale to larger output. Structured Summaries: Some research has explored generating structured summaries. These approaches attempt to identify major aspects of a topic, but do not compile content to describe those aspects. Rather, they rely on pre-existing, labeled paragraphs (for example, a paragraph titled, “Symptoms of Meningitis”). Aspects are identified either by a training corpus of articles in the 909 same domain (Sauper and Barzilay, 2009), by an entity-aspect LDA model (Li et al., 2010), or by Wikipedia templates of related top</context>
</contexts>
<marker>Christensen, Mausam, Etzioni, 2013</marker>
<rawString>Janara Christensen, Mausam, Stephen Soderland, and Oren Etzioni. 2013. Towards coherent multidocument summarization. In Proceedings of NAACL 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Gillenwater</author>
<author>Alex Kulesza</author>
<author>Ben Taskar</author>
</authors>
<title>Discovering diverse and salient threads in document collections.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLPCoNLL 2012,</booktitle>
<pages>710--720</pages>
<contexts>
<context position="37519" citStr="Gillenwater et al. (2012)" startWordPosition="6183" endWordPosition="6186">ains. Additionally, long timelines can be overwhelming, short timelines have low information content, and there is no method for personalized exploration. Document Threads: A related track of research investigates discovering threads of documents. While we aim to summarize collections of information, this track seeks to identify relationships between documents. This research operates on the document level, while ours operates on the sentence level. Shahaf and Guestrin (2010) formalized the characteristics of a good chain of articles and proposed an algorithm to connect two specified articles. Gillenwater et al. (2012) proposed a probabilistic technique for extracting a diverse set of threads from a given collection. Shahaf et al. (2012) extended work on coherent threads to finding coherent maps of documents, where a map is set of intersecting threads representing how the threads interact and relate. Summarization and Hierarchies: A few papers have examined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated </context>
</contexts>
<marker>Gillenwater, Kulesza, Taskar, 2012</marker>
<rawString>Jennifer Gillenwater, Alex Kulesza, and Ben Taskar. 2012. Discovering diverse and salient threads in document collections. In Proceedings of EMNLPCoNLL 2012, pages 710–720.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Lucy Vanderwende</author>
</authors>
<title>Exploring content models for multi-document summarization.</title>
<date>2009</date>
<booktitle>Proceedings of NAACL</booktitle>
<pages>362--370</pages>
<contexts>
<context position="38273" citStr="Haghighi and Vanderwende, 2009" startWordPosition="6299" endWordPosition="6302">xtended work on coherent threads to finding coherent maps of documents, where a map is set of intersecting threads representing how the threads interact and relate. Summarization and Hierarchies: A few papers have examined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated creating hierarchies of words or phrases to organize documents (Lawrie et al., 2001; Lawrie, 2003; Takahashi et al., 2007; Haghighi and Vanderwende, 2009). Other research identifies the hierarchical structure of the documents and generates a summary that prioritizes more general information according to the structure (Ouyang et al., 2009; Celikyilmaz and Hakkani-Tur, 2010), or gains coverage by drawing sentences from different parts of the hierarchy (Yang and Wang, 2003; Wang et al., 2006). 7 Conclusions We have introduced a new paradigm for largescale summarization called hierarchical summarization, which allows a user to navigate a hierarchy of relatively short summaries. We present SUMMA, an implemented hierarchical news summarization system</context>
</contexts>
<marker>Haghighi, Vanderwende, 2009</marker>
<rawString>Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. Proceedings of NAACL 2009, pages 362–370.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Po Hu</author>
<author>Minlie Huang</author>
<author>Peng Xu</author>
<author>Weichang Li</author>
<author>Adam K Usadi</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Generating breakpoint-based timeline overview for news topic retrospection.</title>
<date>2011</date>
<booktitle>In Proceedings of ICDM</booktitle>
<contexts>
<context position="36778" citStr="Hu et al., 2011" startWordPosition="6070" endWordPosition="6073">of related topics (Yao et al., 2011). These methods assume a common structure for all topics in a category, and do not allow for more than two levels in the structure. Timeline Generation: Recent papers in timeline generation have emphasized the relationship with summarization. Yan et al. (2011b) balanced coherence and diversity to create timelines, Yan et al. (2011a) used inter-date and intra-date sentence dependencies, and Chieu and Lee (2004) used sentence similarity. Others have emphasized identifying important dates, primarily by bursts of news (Swan and Allen, 2000; Akcora et al., 2010; Hu et al., 2011; Kessler et al., 2012). While timelines can be useful for understanding events, they do not generalize to other domains. Additionally, long timelines can be overwhelming, short timelines have low information content, and there is no method for personalized exploration. Document Threads: A related track of research investigates discovering threads of documents. While we aim to summarize collections of information, this track seeks to identify relationships between documents. This research operates on the document level, while ours operates on the sentence level. Shahaf and Guestrin (2010) form</context>
</contexts>
<marker>Hu, Huang, Xu, Li, Usadi, Zhu, 2011</marker>
<rawString>Po Hu, Minlie Huang, Peng Xu, Weichang Li, Adam K. Usadi, and Xiaoyan Zhu. 2011. Generating breakpoint-based timeline overview for news topic retrospection. In Proceedings of ICDM 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remy Kessler</author>
<author>Xavier Tannier</author>
<author>Caroline Hag`ege</author>
<author>V´eronique Moriceau</author>
<author>Andr´e Bittar</author>
</authors>
<title>Finding salient dates for building thematic timelines.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL 2012,</booktitle>
<pages>730--739</pages>
<marker>Kessler, Tannier, Hag`ege, Moriceau, Bittar, 2012</marker>
<rawString>Remy Kessler, Xavier Tannier, Caroline Hag`ege, V´eronique Moriceau, and Andr´e Bittar. 2012. Finding salient dates for building thematic timelines. In Proceedings of ACL 2012, pages 730–739.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="12027" citStr="Klein and Manning, 2003" startWordPosition="1898" endWordPosition="1901">ion, we hierarchicallyycluster all the sentences in the input documents by time. Unfortunately, neither agglomerative nor divisive clustering is suitable, since both assumeaa binary splitfat eachpnodea(Berkhin,n2006). Theunumber of clusters at each split should be what is most natural for the input data. We design a recursive clustering algorithm that automatically chooses the appropriate number of clusters at each split. Before clustering, we timestamp all sentences. We use SUTime (Chang and Manning, 2012) to normalize temporal references, and we parse the sentences with the Stanford parser (Klein and Manning, 2003) and use a set of simple heuristics where C is a clustering, BC to determine if the timestampspinpthe sentence remrfcacpli fer to the root verb. If no timestamp is given, we use the article date. 3.1 Temporal Clustering After acquiringathe timestamps, we must hierarchically cluster thessentenceseinto sets that make sense to summarize together. Since we wish to partition along the temporal dimension, our problempreduces toridentifying the best dates at which to split a cluster intopsubclusters. We-identify these datessby looking for bursts of activity. News tends to be bursty – many arti cles o</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dawn Lawrie</author>
<author>W Bruce Croft</author>
<author>Arnold Rosenberg</author>
</authors>
<title>Finding topic words for hierarchical summarization.</title>
<date>2001</date>
<booktitle>In Proceedings of SIGIR ’01,</booktitle>
<pages>349--357</pages>
<contexts>
<context position="38202" citStr="Lawrie et al., 2001" startWordPosition="6289" endWordPosition="6292"> of threads from a given collection. Shahaf et al. (2012) extended work on coherent threads to finding coherent maps of documents, where a map is set of intersecting threads representing how the threads interact and relate. Summarization and Hierarchies: A few papers have examined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated creating hierarchies of words or phrases to organize documents (Lawrie et al., 2001; Lawrie, 2003; Takahashi et al., 2007; Haghighi and Vanderwende, 2009). Other research identifies the hierarchical structure of the documents and generates a summary that prioritizes more general information according to the structure (Ouyang et al., 2009; Celikyilmaz and Hakkani-Tur, 2010), or gains coverage by drawing sentences from different parts of the hierarchy (Yang and Wang, 2003; Wang et al., 2006). 7 Conclusions We have introduced a new paradigm for largescale summarization called hierarchical summarization, which allows a user to navigate a hierarchy of relatively short summaries. </context>
</contexts>
<marker>Lawrie, Croft, Rosenberg, 2001</marker>
<rawString>Dawn Lawrie, W. Bruce Croft, and Arnold Rosenberg. 2001. Finding topic words for hierarchical summarization. In Proceedings of SIGIR ’01, pages 349– 357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dawn J Lawrie</author>
</authors>
<title>Language models for hierarchical summarization.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts Amherst.</institution>
<contexts>
<context position="38216" citStr="Lawrie, 2003" startWordPosition="6293" endWordPosition="6294">ven collection. Shahaf et al. (2012) extended work on coherent threads to finding coherent maps of documents, where a map is set of intersecting threads representing how the threads interact and relate. Summarization and Hierarchies: A few papers have examined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated creating hierarchies of words or phrases to organize documents (Lawrie et al., 2001; Lawrie, 2003; Takahashi et al., 2007; Haghighi and Vanderwende, 2009). Other research identifies the hierarchical structure of the documents and generates a summary that prioritizes more general information according to the structure (Ouyang et al., 2009; Celikyilmaz and Hakkani-Tur, 2010), or gains coverage by drawing sentences from different parts of the hierarchy (Yang and Wang, 2003; Wang et al., 2006). 7 Conclusions We have introduced a new paradigm for largescale summarization called hierarchical summarization, which allows a user to navigate a hierarchy of relatively short summaries. We present SUM</context>
</contexts>
<marker>Lawrie, 2003</marker>
<rawString>Dawn J. Lawrie. 2003. Language models for hierarchical summarization. Ph.D. thesis, University of Massachusetts Amherst.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Jing Jiang</author>
<author>Yinglin Wang</author>
</authors>
<title>Generating templates of entity summaries with an entityaspect model and pattern mining.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL</booktitle>
<pages>640--649</pages>
<contexts>
<context position="36135" citStr="Li et al., 2010" startWordPosition="5966" endWordPosition="5969">ction to accomplish these goals. Christensen et al. (2013) presented an algorithm for coherent MDS, but it does not scale to larger output. Structured Summaries: Some research has explored generating structured summaries. These approaches attempt to identify major aspects of a topic, but do not compile content to describe those aspects. Rather, they rely on pre-existing, labeled paragraphs (for example, a paragraph titled, “Symptoms of Meningitis”). Aspects are identified either by a training corpus of articles in the 909 same domain (Sauper and Barzilay, 2009), by an entity-aspect LDA model (Li et al., 2010), or by Wikipedia templates of related topics (Yao et al., 2011). These methods assume a common structure for all topics in a category, and do not allow for more than two levels in the structure. Timeline Generation: Recent papers in timeline generation have emphasized the relationship with summarization. Yan et al. (2011b) balanced coherence and diversity to create timelines, Yan et al. (2011a) used inter-date and intra-date sentence dependencies, and Chieu and Lee (2004) used sentence similarity. Others have emphasized identifying important dates, primarily by bursts of news (Swan and Allen,</context>
</contexts>
<marker>Li, Jiang, Wang, 2010</marker>
<rawString>Peng Li, Jing Jiang, and Yinglin Wang. 2010. Generating templates of entity summaries with an entityaspect model and pattern mining. In Proceedings of ACL 2010, pages 640–649.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hui Lin</author>
<author>Jeff Bilmes</author>
</authors>
<title>A class of submodular functions for document summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL 2011,</booktitle>
<pages>510--520</pages>
<contexts>
<context position="24195" citStr="Lin and Bilmes, 2011" startWordPosition="4005" endWordPosition="4008">estions: • Do users prefer hierarchical summaries for topic exploration? (Section 5.1) • Are hierarchical summaries more effective than other methods for learning about complex events? (Section 5.2) • How informative are the hierarchical summaries compared to the other methods? (Section 5.3) • How coherent is the hierarchical structure in the summaries? (Section 5.4) We compared SUMMA against two baseline systems which represent the main NLP methods for large-scale summarization: an algorithm for creating timelines over sentences (Chieu and Lee, 2004),3 and a state-of-the-art flat MDS system (Lin and Bilmes, 2011).4 Each system was given the same budget (over 10 times the traditional MDS budget, which is 665 bytes). We evaluated the questions on ten news topics, representing a range of tasks: (1) Pope John Paul II’s death and the 2005 Papal Conclave, (2) Bush v. Gore, (3) the Tulip Revolution, (4) Daniel Pearl’s kidnapping, (5) the Lockerbie bombing handover of suspects, (6) the Kargil War, (7) NATO’s bombing of Yugoslavia in 1999, (8) Pinochet’s arrest in London, (9) the 2005 London bombings, and (10) the crash and investigation of SwissAir Flight 111. We chose topics containing a set of related event</context>
<context position="35443" citStr="Lin and Bilmes (2011)" startWordPosition="5859" endWordPosition="5862">ally overwhelming information, and second, the output is static – users with different interests will see the same information. Below we describe related work on traditional MDS, structured summaries, timelines, discovering threads of documents and the uses of hierarchies in generating summaries. 6.1 Traditional MDS Traditionally, MDS systems have focused on three to six sentence summaries covering 10-15 documents. Most extractive summarization research aims to maximize coverage while reducing redundancy (e.g. (Carbonell and Goldstein, 1998; Saggion and Gaizauskas, 2004; Radev et al., 2004)). Lin and Bilmes (2011) proposed a state-of-the-art system that uses submodularity in sentence selection to accomplish these goals. Christensen et al. (2013) presented an algorithm for coherent MDS, but it does not scale to larger output. Structured Summaries: Some research has explored generating structured summaries. These approaches attempt to identify major aspects of a topic, but do not compile content to describe those aspects. Rather, they rely on pre-existing, labeled paragraphs (for example, a paragraph titled, “Symptoms of Meningitis”). Aspects are identified either by a training corpus of articles in the </context>
</contexts>
<marker>Lin, Bilmes, 2011</marker>
<rawString>Hui Lin and Jeff Bilmes. 2011. A class of submodular functions for document summarization. In Proceedings of ACL 2011, pages 510–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>ROUGE: A package for automatic evaluation of summaries.</title>
<date>2004</date>
<booktitle>In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<contexts>
<context position="17393" citStr="Lin, 2004" startWordPosition="2875" endWordPosition="2876">if the sentences within a cluster summary are disconnected or if the parent sentence for a summary does not relate to the child summary. Thus, a hierarchical summary must also have intra-cluster coherence and parent-to-child coherence. 4.1 Salience Salience is the value of each sentence to the topic from which the documents are drawn. We measure salience of a summary (Sal(X)) as the sum of the saliences of individual sentences (Ei Sal(xi)). Following previous research in MDS, we computed individual saliences using a linear regression classifier trained on ROUGE scores over the DUC’03 dataset (Lin, 2004; Christensen et al., 2013). This method finds those sentences more salient that mention nouns or verbs that occur frequently in the cluster. In preliminary experiments, we noticed that many sentences that were reaction sentences were given a higher salience than action sentences. For example, the reaction sentence, “President Clinton vowed to track down the perpetrators behind the bombs that exploded outside the embassies in Tanzania and Kenya on Friday,” would have a higher score than the action sentence, “Bombs exploded outside the embassies in Tanzania and Kenya on Friday.” This problem oc</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Schmitz Mausam</author>
<author>Robert Bart</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Open language learning for information extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP 2012,</booktitle>
<pages>523--534</pages>
<contexts>
<context position="18626" citStr="Mausam et al., 2012" startWordPosition="3067" endWordPosition="3070">the first sentence has a higher ROUGE score (it covers more important words than the second sentence). To adjust for this problem, we use only words identified in the main clause (heuristically identified via the parse tree) to compute our salience scores. 4.2 Redundancy We identify redundant sentences using a linear regression classifier trained on a manually labeled subset of the DUC’03 sentences. The features include shared noun counts, sentence length, TF*IDF cosine similarity, timestamp difference, and features drawn from information extraction such as number of shared tuples in Open IE (Mausam et al., 2012). 905 4.3 Summary Coherence We require two types of coherence: coherence between the parent and child summaries and coherence within each summary Xi. We rely on the approximate discourse graph (ADG) that was proposed in (Christensen et al., 2013) as the basis for measuring coherence. Each node in the ADG is a sentence from the dataset. An edge from sentence si to sj with positive weight indicates that sj may follow si in a coherent summary, e.g. continued mention of an event or entity, or coreference link between si and sj. A negative edge indicates an unfulfilled discourse cue or co-reference</context>
</contexts>
<marker>Mausam, Bart, Soderland, Etzioni, 2012</marker>
<rawString>Mausam, Michael Schmitz, Robert Bart, Stephen Soderland, and Oren Etzioni. 2012. Open language learning for information extraction. In Proceedings of EMNLP 2012, pages 523–534.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jahna Otterbacher</author>
<author>Dragomir Radev</author>
<author>Omer Kareem</author>
</authors>
<title>News to go: Hierarchical text summarization for mobile devices.</title>
<date>2006</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>589--596</pages>
<contexts>
<context position="38042" citStr="Otterbacher et al., 2006" startWordPosition="6264" endWordPosition="6267">ain of articles and proposed an algorithm to connect two specified articles. Gillenwater et al. (2012) proposed a probabilistic technique for extracting a diverse set of threads from a given collection. Shahaf et al. (2012) extended work on coherent threads to finding coherent maps of documents, where a map is set of intersecting threads representing how the threads interact and relate. Summarization and Hierarchies: A few papers have examined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated creating hierarchies of words or phrases to organize documents (Lawrie et al., 2001; Lawrie, 2003; Takahashi et al., 2007; Haghighi and Vanderwende, 2009). Other research identifies the hierarchical structure of the documents and generates a summary that prioritizes more general information according to the structure (Ouyang et al., 2009; Celikyilmaz and Hakkani-Tur, 2010), or gains coverage by drawing sentences from different parts of the hierarchy (Yang and Wang, 2003; Wang et al., 2006). 7 Conclusions We have intro</context>
</contexts>
<marker>Otterbacher, Radev, Kareem, 2006</marker>
<rawString>Jahna Otterbacher, Dragomir Radev, and Omer Kareem. 2006. News to go: Hierarchical text summarization for mobile devices. In Proceedings of SIGIR 2006, pages 589–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>You Ouyang</author>
<author>Wenji Li</author>
<author>Qin Lu</author>
</authors>
<title>An integrated multi-document summarization approach based on word hierarchical representation.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACLShort</booktitle>
<pages>113--116</pages>
<contexts>
<context position="38458" citStr="Ouyang et al., 2009" startWordPosition="6327" endWordPosition="6330"> few papers have examined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated creating hierarchies of words or phrases to organize documents (Lawrie et al., 2001; Lawrie, 2003; Takahashi et al., 2007; Haghighi and Vanderwende, 2009). Other research identifies the hierarchical structure of the documents and generates a summary that prioritizes more general information according to the structure (Ouyang et al., 2009; Celikyilmaz and Hakkani-Tur, 2010), or gains coverage by drawing sentences from different parts of the hierarchy (Yang and Wang, 2003; Wang et al., 2006). 7 Conclusions We have introduced a new paradigm for largescale summarization called hierarchical summarization, which allows a user to navigate a hierarchy of relatively short summaries. We present SUMMA, an implemented hierarchical news summarization system,7 and demonstrate its effectiveness in a user study that compares SUMMA with a timeline system and a flat MDS system. When compared to timelines, users learned more with SUMMA in twice</context>
</contexts>
<marker>Ouyang, Li, Lu, 2009</marker>
<rawString>You Ouyang, Wenji Li, and Qin Lu. 2009. An integrated multi-document summarization approach based on word hierarchical representation. In Proceedings of the ACLShort 2009, pages 113–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Hongyan Jing</author>
<author>Malgorzata Stys</author>
<author>Daniel Tam</author>
</authors>
<title>Centroid-based summarization of multiple documents.</title>
<date>2004</date>
<booktitle>Information Processing and Management,</booktitle>
<volume>40</volume>
<issue>6</issue>
<contexts>
<context position="35419" citStr="Radev et al., 2004" startWordPosition="5855" endWordPosition="5858">rge amounts of potentially overwhelming information, and second, the output is static – users with different interests will see the same information. Below we describe related work on traditional MDS, structured summaries, timelines, discovering threads of documents and the uses of hierarchies in generating summaries. 6.1 Traditional MDS Traditionally, MDS systems have focused on three to six sentence summaries covering 10-15 documents. Most extractive summarization research aims to maximize coverage while reducing redundancy (e.g. (Carbonell and Goldstein, 1998; Saggion and Gaizauskas, 2004; Radev et al., 2004)). Lin and Bilmes (2011) proposed a state-of-the-art system that uses submodularity in sentence selection to accomplish these goals. Christensen et al. (2013) presented an algorithm for coherent MDS, but it does not scale to larger output. Structured Summaries: Some research has explored generating structured summaries. These approaches attempt to identify major aspects of a topic, but do not compile content to describe those aspects. Rather, they rely on pre-existing, labeled paragraphs (for example, a paragraph titled, “Symptoms of Meningitis”). Aspects are identified either by a training co</context>
</contexts>
<marker>Radev, Jing, Stys, Tam, 2004</marker>
<rawString>Dragomir R. Radev, Hongyan Jing, Malgorzata Stys, and Daniel Tam. 2004. Centroid-based summarization of multiple documents. Information Processing and Management, 40(6):919–938.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Saggion</author>
<author>Robert Gaizauskas</author>
</authors>
<title>Multidocument summarization by cluster/profile relevance and redundancy removal.</title>
<date>2004</date>
<booktitle>In Proceedings of DUC</booktitle>
<contexts>
<context position="35398" citStr="Saggion and Gaizauskas, 2004" startWordPosition="5850" endWordPosition="5854">re the user to sort through large amounts of potentially overwhelming information, and second, the output is static – users with different interests will see the same information. Below we describe related work on traditional MDS, structured summaries, timelines, discovering threads of documents and the uses of hierarchies in generating summaries. 6.1 Traditional MDS Traditionally, MDS systems have focused on three to six sentence summaries covering 10-15 documents. Most extractive summarization research aims to maximize coverage while reducing redundancy (e.g. (Carbonell and Goldstein, 1998; Saggion and Gaizauskas, 2004; Radev et al., 2004)). Lin and Bilmes (2011) proposed a state-of-the-art system that uses submodularity in sentence selection to accomplish these goals. Christensen et al. (2013) presented an algorithm for coherent MDS, but it does not scale to larger output. Structured Summaries: Some research has explored generating structured summaries. These approaches attempt to identify major aspects of a topic, but do not compile content to describe those aspects. Rather, they rely on pre-existing, labeled paragraphs (for example, a paragraph titled, “Symptoms of Meningitis”). Aspects are identified ei</context>
</contexts>
<marker>Saggion, Gaizauskas, 2004</marker>
<rawString>Horacio Saggion and Robert Gaizauskas. 2004. Multidocument summarization by cluster/profile relevance and redundancy removal. In Proceedings of DUC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christina Sauper</author>
<author>Regina Barzilay</author>
</authors>
<title>Automatically generating Wikipedia articles: A structureaware approach.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL 2009,</booktitle>
<pages>208--216</pages>
<contexts>
<context position="36086" citStr="Sauper and Barzilay, 2009" startWordPosition="5957" endWordPosition="5960">-of-the-art system that uses submodularity in sentence selection to accomplish these goals. Christensen et al. (2013) presented an algorithm for coherent MDS, but it does not scale to larger output. Structured Summaries: Some research has explored generating structured summaries. These approaches attempt to identify major aspects of a topic, but do not compile content to describe those aspects. Rather, they rely on pre-existing, labeled paragraphs (for example, a paragraph titled, “Symptoms of Meningitis”). Aspects are identified either by a training corpus of articles in the 909 same domain (Sauper and Barzilay, 2009), by an entity-aspect LDA model (Li et al., 2010), or by Wikipedia templates of related topics (Yao et al., 2011). These methods assume a common structure for all topics in a category, and do not allow for more than two levels in the structure. Timeline Generation: Recent papers in timeline generation have emphasized the relationship with summarization. Yan et al. (2011b) balanced coherence and diversity to create timelines, Yan et al. (2011a) used inter-date and intra-date sentence dependencies, and Chieu and Lee (2004) used sentence similarity. Others have emphasized identifying important da</context>
</contexts>
<marker>Sauper, Barzilay, 2009</marker>
<rawString>Christina Sauper and Regina Barzilay. 2009. Automatically generating Wikipedia articles: A structureaware approach. In Proceedings ofACL 2009, pages 208–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dafna Shahaf</author>
<author>Carlos Guestrin</author>
</authors>
<title>Connecting the dots between news articles.</title>
<date>2010</date>
<booktitle>In Proceedings of KDD</booktitle>
<pages>623--632</pages>
<contexts>
<context position="37373" citStr="Shahaf and Guestrin (2010)" startWordPosition="6159" endWordPosition="6162">a et al., 2010; Hu et al., 2011; Kessler et al., 2012). While timelines can be useful for understanding events, they do not generalize to other domains. Additionally, long timelines can be overwhelming, short timelines have low information content, and there is no method for personalized exploration. Document Threads: A related track of research investigates discovering threads of documents. While we aim to summarize collections of information, this track seeks to identify relationships between documents. This research operates on the document level, while ours operates on the sentence level. Shahaf and Guestrin (2010) formalized the characteristics of a good chain of articles and proposed an algorithm to connect two specified articles. Gillenwater et al. (2012) proposed a probabilistic technique for extracting a diverse set of threads from a given collection. Shahaf et al. (2012) extended work on coherent threads to finding coherent maps of documents, where a map is set of intersecting threads representing how the threads interact and relate. Summarization and Hierarchies: A few papers have examined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a</context>
</contexts>
<marker>Shahaf, Guestrin, 2010</marker>
<rawString>Dafna Shahaf and Carlos Guestrin. 2010. Connecting the dots between news articles. In Proceedings of KDD 2010, pages 623–632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dafna Shahaf</author>
<author>Carlos Guestrin</author>
<author>Eric Horvitz</author>
</authors>
<title>Trains of thought: Generating information maps.</title>
<date>2012</date>
<booktitle>In Proceedings of WWW</booktitle>
<contexts>
<context position="27379" citStr="Shahaf et al., 2012" startWordPosition="4541" endWordPosition="4545">referred the timelines, they usually remarked that it was more familiar, i.e. “I liked the familiarity of the format. I am used to these timelines and they feel comfortable.” Users complained that the flat summaries were disjointed, confusing, and very frustrating to read. 5.2 Knowledge Acquisition Evaluating how much a user learned is inherently difficult, more so when the goal is to allow the user the freedom to explore information based on individual interest. For this reason, instead of asking a set of predefined questions, we assess the knowl907 edge gain by following the methodology of (Shahaf et al., 2012) – asking users to write a paragraph summarizing the information learned. Using the same setup as in the previous experiment, for each topic, five AMT workers spent three minutes reading through a timeline or summary and were then asked to write a description of what they had learned. Workers were not allowed to see the timeline or summary while writing. We collected five descriptions for each topicsystem combination. We then asked other AMT workers to read and compare the descriptions written by the first set of workers. Each evaluator was presented with a corresponding Wikipedia article and </context>
<context position="37640" citStr="Shahaf et al. (2012)" startWordPosition="6202" endWordPosition="6205">or personalized exploration. Document Threads: A related track of research investigates discovering threads of documents. While we aim to summarize collections of information, this track seeks to identify relationships between documents. This research operates on the document level, while ours operates on the sentence level. Shahaf and Guestrin (2010) formalized the characteristics of a good chain of articles and proposed an algorithm to connect two specified articles. Gillenwater et al. (2012) proposed a probabilistic technique for extracting a diverse set of threads from a given collection. Shahaf et al. (2012) extended work on coherent threads to finding coherent maps of documents, where a map is set of intersecting threads representing how the threads interact and relate. Summarization and Hierarchies: A few papers have examined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated creating hierarchies of words or phrases to organize documents (Lawrie et al., 2001; Lawrie, 2003; Takahashi et al., 2007</context>
</contexts>
<marker>Shahaf, Guestrin, Horvitz, 2012</marker>
<rawString>Dafna Shahaf, Carlos Guestrin, and Eric Horvitz. 2012. Trains of thought: Generating information maps. In Proceedings of WWW 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Russell Swan</author>
<author>James Allen</author>
</authors>
<title>Automatic generation of overview timelines.</title>
<date>2000</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>49--56</pages>
<contexts>
<context position="36740" citStr="Swan and Allen, 2000" startWordPosition="6062" endWordPosition="6065">i et al., 2010), or by Wikipedia templates of related topics (Yao et al., 2011). These methods assume a common structure for all topics in a category, and do not allow for more than two levels in the structure. Timeline Generation: Recent papers in timeline generation have emphasized the relationship with summarization. Yan et al. (2011b) balanced coherence and diversity to create timelines, Yan et al. (2011a) used inter-date and intra-date sentence dependencies, and Chieu and Lee (2004) used sentence similarity. Others have emphasized identifying important dates, primarily by bursts of news (Swan and Allen, 2000; Akcora et al., 2010; Hu et al., 2011; Kessler et al., 2012). While timelines can be useful for understanding events, they do not generalize to other domains. Additionally, long timelines can be overwhelming, short timelines have low information content, and there is no method for personalized exploration. Document Threads: A related track of research investigates discovering threads of documents. While we aim to summarize collections of information, this track seeks to identify relationships between documents. This research operates on the document level, while ours operates on the sentence </context>
</contexts>
<marker>Swan, Allen, 2000</marker>
<rawString>Russell Swan and James Allen. 2000. Automatic generation of overview timelines. In Proceedings of SIGIR 2000, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kou Takahashi</author>
<author>Takao Miura</author>
<author>Isamu Shioya</author>
</authors>
<title>Hierarchical summarizing and evaluating for web pages.</title>
<date>2007</date>
<booktitle>In Proceedings of the 1st</booktitle>
<contexts>
<context position="38240" citStr="Takahashi et al., 2007" startWordPosition="6295" endWordPosition="6298">. Shahaf et al. (2012) extended work on coherent threads to finding coherent maps of documents, where a map is set of intersecting threads representing how the threads interact and relate. Summarization and Hierarchies: A few papers have examined the relationship between summarization and hierarchies. Some focused on creating a hierarchical summary of a single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated creating hierarchies of words or phrases to organize documents (Lawrie et al., 2001; Lawrie, 2003; Takahashi et al., 2007; Haghighi and Vanderwende, 2009). Other research identifies the hierarchical structure of the documents and generates a summary that prioritizes more general information according to the structure (Ouyang et al., 2009; Celikyilmaz and Hakkani-Tur, 2010), or gains coverage by drawing sentences from different parts of the hierarchy (Yang and Wang, 2003; Wang et al., 2006). 7 Conclusions We have introduced a new paradigm for largescale summarization called hierarchical summarization, which allows a user to navigate a hierarchy of relatively short summaries. We present SUMMA, an implemented hiera</context>
</contexts>
<marker>Takahashi, Miura, Shioya, 2007</marker>
<rawString>Kou Takahashi, Takao Miura, and Isamu Shioya. 2007. Hierarchical summarizing and evaluating for web pages. In Proceedings of the 1st workshop on emerging research opportunities for Web Data Management (EROW 2007).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Tibshirani</author>
<author>Guenther Walther</author>
<author>Trevor Hastie</author>
</authors>
<title>Estimating the number of clusters in a dataset via the gap statistic.</title>
<date>2000</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>32</volume>
<issue>2</issue>
<contexts>
<context position="15618" citStr="Tibshirani et al., 2000" startWordPosition="2577" endWordPosition="2580">iven summary budget, the sentences will have to be too short, and when the number of clusters is too small, we will not use enough of the budget. We set the maximum number of clusters kmax and minimum number of clusters kmin to be a function of the budget b and the average sentence length in the cluster savg, such that kmax - savg ≤ b and kmin - savg ≥ b/2. Given a maximum and minimum number of clusters, we must determine the appropriate number of clusters. At each level, we cluster the sentences by the method described above and choose the number of clusters k according to the gap statistic (Tibshirani et al., 2000). Specifically, for each level, the algorithm will cluster repeatedly with k varying from the minimum to the maximum. The algorithm will return the k that maximizes the gap statistic: Gapn(k) = E∗n{log(Wk)} − log(Wk) (5) where Wk is the score for the clusters computed with Equation 1, and E∗n is the expectation under a sample of size n from a reference distribution. Ideally, the maximum depth of the clustering would be a function of the number of sentences in each cluster, but in our implementation, we set the maximum depth to three, which works well for the size of the datasets we use (300 ar</context>
</contexts>
<marker>Tibshirani, Walther, Hastie, 2000</marker>
<rawString>Robert Tibshirani, Guenther Walther, and Trevor Hastie. 2000. Estimating the number of clusters in a dataset via the gap statistic. Journal of the Royal Statistical Society, Series B, 32(2):411–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fu Lee Wang</author>
<author>Christopher C Yang</author>
<author>Xiaodong Shi</author>
</authors>
<title>Multi-document summarization for terrorism information extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of ISI’06.</booktitle>
<contexts>
<context position="38613" citStr="Wang et al., 2006" startWordPosition="6353" endWordPosition="6356">kkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated creating hierarchies of words or phrases to organize documents (Lawrie et al., 2001; Lawrie, 2003; Takahashi et al., 2007; Haghighi and Vanderwende, 2009). Other research identifies the hierarchical structure of the documents and generates a summary that prioritizes more general information according to the structure (Ouyang et al., 2009; Celikyilmaz and Hakkani-Tur, 2010), or gains coverage by drawing sentences from different parts of the hierarchy (Yang and Wang, 2003; Wang et al., 2006). 7 Conclusions We have introduced a new paradigm for largescale summarization called hierarchical summarization, which allows a user to navigate a hierarchy of relatively short summaries. We present SUMMA, an implemented hierarchical news summarization system,7 and demonstrate its effectiveness in a user study that compares SUMMA with a timeline system and a flat MDS system. When compared to timelines, users learned more with SUMMA in twice as many cases, and SUMMA was preferred more than three times as often. When compared to flat summaries, users overwhelming preferred SUMMA and learned jus</context>
</contexts>
<marker>Wang, Yang, Shi, 2006</marker>
<rawString>Fu Lee Wang, Christopher C. Yang, and Xiaodong Shi. 2006. Multi-document summarization for terrorism information extraction. In Proceedings of ISI’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Liang Kong</author>
<author>Congrui Huang</author>
<author>Xiaojun Wan</author>
<author>Xiaoming Li</author>
<author>Yan Zhang</author>
</authors>
<title>Timeline generation through evolutionary trans-temporal summarization.</title>
<date>2011</date>
<booktitle>In Proceedings of EMNLP 2011,</booktitle>
<pages>433--443</pages>
<contexts>
<context position="36458" citStr="Yan et al. (2011" startWordPosition="6019" endWordPosition="6022"> those aspects. Rather, they rely on pre-existing, labeled paragraphs (for example, a paragraph titled, “Symptoms of Meningitis”). Aspects are identified either by a training corpus of articles in the 909 same domain (Sauper and Barzilay, 2009), by an entity-aspect LDA model (Li et al., 2010), or by Wikipedia templates of related topics (Yao et al., 2011). These methods assume a common structure for all topics in a category, and do not allow for more than two levels in the structure. Timeline Generation: Recent papers in timeline generation have emphasized the relationship with summarization. Yan et al. (2011b) balanced coherence and diversity to create timelines, Yan et al. (2011a) used inter-date and intra-date sentence dependencies, and Chieu and Lee (2004) used sentence similarity. Others have emphasized identifying important dates, primarily by bursts of news (Swan and Allen, 2000; Akcora et al., 2010; Hu et al., 2011; Kessler et al., 2012). While timelines can be useful for understanding events, they do not generalize to other domains. Additionally, long timelines can be overwhelming, short timelines have low information content, and there is no method for personalized exploration. Document </context>
</contexts>
<marker>Yan, Kong, Huang, Wan, Li, Zhang, 2011</marker>
<rawString>Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan, Xiaoming Li, and Yan Zhang. 2011a. Timeline generation through evolutionary trans-temporal summarization. In Proceedings of EMNLP 2011, pages 433–443.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Yan</author>
<author>Xiaojun Wan</author>
<author>Jahna Otterbacher</author>
<author>Liang Kong</author>
<author>Xiaoming Li</author>
<author>Yan Zhang</author>
</authors>
<title>Evolutionary timeline summarization: A balanced optimization framework via iterative substitution.</title>
<date>2011</date>
<booktitle>In Proceeding of SIGIR 2011,</booktitle>
<pages>745--754</pages>
<contexts>
<context position="36458" citStr="Yan et al. (2011" startWordPosition="6019" endWordPosition="6022"> those aspects. Rather, they rely on pre-existing, labeled paragraphs (for example, a paragraph titled, “Symptoms of Meningitis”). Aspects are identified either by a training corpus of articles in the 909 same domain (Sauper and Barzilay, 2009), by an entity-aspect LDA model (Li et al., 2010), or by Wikipedia templates of related topics (Yao et al., 2011). These methods assume a common structure for all topics in a category, and do not allow for more than two levels in the structure. Timeline Generation: Recent papers in timeline generation have emphasized the relationship with summarization. Yan et al. (2011b) balanced coherence and diversity to create timelines, Yan et al. (2011a) used inter-date and intra-date sentence dependencies, and Chieu and Lee (2004) used sentence similarity. Others have emphasized identifying important dates, primarily by bursts of news (Swan and Allen, 2000; Akcora et al., 2010; Hu et al., 2011; Kessler et al., 2012). While timelines can be useful for understanding events, they do not generalize to other domains. Additionally, long timelines can be overwhelming, short timelines have low information content, and there is no method for personalized exploration. Document </context>
</contexts>
<marker>Yan, Wan, Otterbacher, Kong, Li, Zhang, 2011</marker>
<rawString>Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong, Xiaoming Li, and Yan Zhang. 2011b. Evolutionary timeline summarization: A balanced optimization framework via iterative substitution. In Proceeding of SIGIR 2011, pages 745–754.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher C Yang</author>
<author>Fu Lee Wang</author>
</authors>
<title>Fractal summarization: summarization based on fractal theory.</title>
<date>2003</date>
<booktitle>In Proceedings of SIGIR</booktitle>
<pages>391--392</pages>
<contexts>
<context position="38593" citStr="Yang and Wang, 2003" startWordPosition="6349" endWordPosition="6352">single document (Buyukkokten et al., 2001; Otterbacher et al., 2006), relying on the structure inherent in single documents. Others investigated creating hierarchies of words or phrases to organize documents (Lawrie et al., 2001; Lawrie, 2003; Takahashi et al., 2007; Haghighi and Vanderwende, 2009). Other research identifies the hierarchical structure of the documents and generates a summary that prioritizes more general information according to the structure (Ouyang et al., 2009; Celikyilmaz and Hakkani-Tur, 2010), or gains coverage by drawing sentences from different parts of the hierarchy (Yang and Wang, 2003; Wang et al., 2006). 7 Conclusions We have introduced a new paradigm for largescale summarization called hierarchical summarization, which allows a user to navigate a hierarchy of relatively short summaries. We present SUMMA, an implemented hierarchical news summarization system,7 and demonstrate its effectiveness in a user study that compares SUMMA with a timeline system and a flat MDS system. When compared to timelines, users learned more with SUMMA in twice as many cases, and SUMMA was preferred more than three times as often. When compared to flat summaries, users overwhelming preferred S</context>
</contexts>
<marker>Yang, Wang, 2003</marker>
<rawString>Christopher C. Yang and Fu Lee Wang. 2003. Fractal summarization: summarization based on fractal theory. In Proceedings of SIGIR 2003, pages 391–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Conglei Yao</author>
<author>Xu Jia</author>
</authors>
<title>Sicong Shou, Shicong Feng, Feng Zhou, and Hongyan Liu.</title>
<date>2011</date>
<booktitle>In Proceedings of WWW 2011,</booktitle>
<pages>161--162</pages>
<marker>Yao, Jia, 2011</marker>
<rawString>Conglei Yao, Xu Jia, Sicong Shou, Shicong Feng, Feng Zhou, and Hongyan Liu. 2011. Autopedia: Automatic domain-independent wikipedia article generation. In Proceedings of WWW 2011, pages 161–162.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>