<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.952949">
Statistical QA - Classifier vs. Re-ranker: What’s the difference?
</title>
<author confidence="0.989981">
Deepak Ravichandran, Eduard Hovy, and Franz Josef Och
</author>
<affiliation confidence="0.996498">
Information Sciences Institute
University of Southern California
</affiliation>
<address confidence="0.9174455">
4676 Admiralty Way
Marina del Rey, CA 90292
</address>
<email confidence="0.999704">
{ravichan,hovy,och}@isi.edu
</email>
<sectionHeader confidence="0.996674" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999965461538462">
In this paper, we show that we can ob-
tain a good baseline performance for
Question Answering (QA) by using
only 4 simple features. Using these fea-
tures, we contrast two approaches used
for a Maximum Entropy based QA sys-
tem. We view the QA problem as a
classification problem and as a re-
ranking problem. Our results indicate
that the QA system viewed as a re-
ranker clearly outperforms the QA sys-
tem used as a classifier. Both systems
are trained using the same data.
</bodyText>
<sectionHeader confidence="0.998755" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9974074">
Open-Domain factoid Question Answering (QA)
is defined as the task of answering fact-based
questions phrased in Natural Language. Exam-
ples of some question and answers that fall in
the fact-based category are:
</bodyText>
<listItem confidence="0.996402666666667">
1. What is the capital of Japan? - Tokyo
2. What is acetaminophen? - Non-aspirin pain killer
3. Where is the Eiffel Tower? - Paris
</listItem>
<bodyText confidence="0.9999645">
The architecture of most of QA systems consists
of two basic modules: the information retrieval
(IR) module and the answer pinpointing module.
These two modules are used in a typical pipeline
architecture.
For a given question, the IR module finds a
set of relevant segments. Each segment typically
consists of at most R sentences1. The answer
pinpointing module processes each of these seg-
ments and finds the appropriate answer phrase.
</bodyText>
<footnote confidence="0.354426">
1 In our experiments we use R=1
</footnote>
<bodyText confidence="0.999701466666667">
phrase. Evaluation of a QA system is judged on
the basis on the final output answer and the cor-
responding evidence provided by the segment.
This paper focuses on the answer pinpointing
module. Typical QA systems perform re-ranking
of candidate answers as an important step in
pinpointing. The goal is to rank the most likely
answer first by using either symbolic or statisti-
cal methods. Some QA systems make use of
statistical answer pinpointing (Xu et. al, 2002;
Ittycheriah, 2001; Ittycheriah and Salim, 2002)
by treating it as a classification problem. In this
paper, we cast the pinpointing problem in a sta-
tistical framework and compare two approaches,
classification and re-ranking.
</bodyText>
<sectionHeader confidence="0.941309" genericHeader="method">
2 Statistical Answer Pinpointing
</sectionHeader>
<subsectionHeader confidence="0.885805">
2.1 Answer Modeling
</subsectionHeader>
<bodyText confidence="0.995272518518518">
The answer-pinpointing module gets as input a
question q and a set of possible answer candi-
dates { a1 a2...aA } . It outputs one of the answer
a E {a1 a2...aA } from the candidate answer set.
We consider two ways of modeling this prob-
lem.
One approach is the traditional classification
view (Ittycheriah, 2001) where we present each
Question-Answer pair to the classifier which
classifies it as either correct answer (true) or in-
correct answer (false), based on some evidence
(features).
In this case, we model P(c |a, {a1a2...aA} ,q) .
Here, c = {true, false} signifies the correctness
of the answer a with respect to the question q.
The probability P(c |a, {a1a2...aA} ,q) for each QA
pair is modeled independently of other such
pairs. Thus, for the same question, many QA
pairs are presented to the classifier as independ-
ent events (histories). If the training corpus con-
tains Q questions with A answers for each ques-
tion, the total number of events (histories) would
be equal to QA with two classes (futures) (cor-
rect or incorrect answer) for each event. Once
the probabilities P(c|a, {a1a2...aA} ,q) have been
computed, the system has to return the best an-
swer. The following decision rule is used:
</bodyText>
<equation confidence="0.6495365">

a arg max [ P ( true  |a , { a 1 a 2 ... a }, q )]
= A
a
</equation>
<bodyText confidence="0.996288181818182">
Another way of viewing the problem is as a
re-ranking task. This is possible because the QA
task requires the identification of only one cor-
rect answer, instead of identifying all the correct
answer in the collection. In this case, we model
P(a  |{a1a2 ... aA} ,q) . If the training corpus contains
Q questions with A answers for each question,
the total number of events (histories) would be
equal to Q, with A classes (futures). This view
requires the following decision-rule to identify
the answer that seems most promising:
</bodyText>
<equation confidence="0.77761975">

where,
m,c ; m =1,.... , M; c = {true, false } are the model
parameters.
</equation>
<bodyText confidence="0.855937">
The decision rule for choosing the best an-
swer is:
</bodyText>
<figure confidence="0.6190738">

a = arg max[P(true  |a, {a1 a2...aA }, q)]
a
fm (a, {a1 a2 ... aA} ,q) ]
a
</figure>
<bodyText confidence="0.997939">
The above decision rule requires comparison of
different probabilities of the
form P(true  |a, {a1 a2...aA } , q) . However, these
probabilities are modeled as independent events
(histories) in the classifier and hence the training
criterion does not make them directly compara-
ble.
For the re-ranker, we model the probability
as:
</bodyText>
<equation confidence="0.754322148148148">
M
fm(a, { a1 a2...aA}, q)]
M
= arg max
M
[
true
=
m 1
m ,
P(a |{a1a2 ... aA},q)
=
m
exp[
1
m
a arg max [ P ( a  |{ a 1 a 2 ... a }, q )]
= A
a
 
exp[
a ’
m
fm(a’, {a1a2 ...aA},q)]
 m
1
In summary,
</equation>
<table confidence="0.520584">
Classifier Re-ranker
#Events (Histo- QA Q
ries)
#Classes (Futures) 2 A
per event
</table>
<bodyText confidence="0.91226825">
where,
Q = total number of questions.
A = total number of answer chunks considered
for each question.
</bodyText>
<subsectionHeader confidence="0.999804">
2.2 Maximum Entropy formulation
</subsectionHeader>
<bodyText confidence="0.999921375">
We use Maximum Entropy to model the given
problem both as a classifier and a re-ranker. We
define M feature functions,
fm(a, {a1a2 ... aA} ,q),m=1 , .... , M , that may be useful
in characterizing the task. Della Pietra et. al
(1995) contains good description of Maximum
Entropy models.
We model the classifier as follows:
</bodyText>
<equation confidence="0.507016">
M
</equation>
<bodyText confidence="0.9989728">
where,
m; m =1,...., M are the model parameters.
Note that for the classifier the model parameters
are m,c , whereas for the re-ranker they are m.
This is because for the classifier, each feature
function has different weights associated with
each class (future). Hence, the classifier has
twice the model parameters as compared to the
re-ranker.
The decision rule for the re-ranker is given by:
</bodyText>
<equation confidence="0.970442833333333">

P(a |{a1 a2 ... aA }, q)]
a
. M
fm (a, {a1 a2 ...aA }, q)]
a
</equation>
<bodyText confidence="0.9985185">
The re-ranker makes the probabilities
P(a  |{a1 a2...aA } , q) , considered for the decision
rule, directly comparable against each other, by
incorporating them into the training criterion
itself. Table 1 summarizes the differences of the
two models.
</bodyText>
<equation confidence="0.964745">
a = arg max [
= arg max
[
m
=
m 1
exp  [  m , cf m a a a a } , )]
( , { ... q
1 2 A
P(c  |a, {a1 a2..aA },q) = m=1
M
 
exp[ m c
,
c ’
’ f a a a a
m( , { ...
1 2 A
} , )]
q
</equation>
<page confidence="0.875132">
m=1
</page>
<tableCaption confidence="0.998687">
Table 1 : Model comparison between a Classifier and Re-ranker
</tableCaption>
<table confidence="0.997928064516129">
Classifier Re-Ranker
Mode M M
ling exp[ m c m ( , { ... } , )] exp[  f a a a a q
Equa-   f a a a a q m m ( , { ... } , )]
tion , 1 2 A  1 2 A
m = 1 =
m 1
P a a a a ) =
(  |{ }
... , q
1 2 A M
(  |, { ... } , ) = exp[ m fm a a a a q
P c a a a a q  ( ’, { ... } , )]
1 2 A M a ’ 1 2 A
exp[  m c f m a a a a q  
( , { ... } , )] m=1
  , ’ 1 2 A
c ’
m =1
Deci-  
sion P true a a a a q P a a a a q
Rule a = arg max { (  |, { ... }, )} a = arg max[ (  |{ ... }, )]
1 2 A 1 2 A
a a
M .
f a a a a q M
= arg max [ ( , { ... } , ) ] = m fm a a a a q
m m true arg max[ ( , { ... }, )]
, 1 2 A 1 2 A
a =  a =
m 1 m 1
</table>
<subsectionHeader confidence="0.991339">
2.3 Feature Functions
</subsectionHeader>
<bodyText confidence="0.99709975">
Using above formulation to model the probabil-
ity distribution we need to come up with features
fj. We use only four basic feature functions for
our system.
</bodyText>
<listItem confidence="0.99686121875">
1. Frequency: It has been observed that the
correct answer has a higher frequency
(Magnini et al.; 2002) in the collection of
answer chunks (C). Hence we count the
number of time a potential answer occurs in
the IR output and use its logarithm as a fea-
ture. This is a positive continuous valued
feature.
2. Expected Answer Class: Most of the current
QA systems employ some type of Answer
Class Identification module. Thus questions
like “When did Bill Clinton go to college?”,
would be identified as a question asking
about a time (or a time period), “Where is
the sea of tranquility?” would be identified
as a question asking for a location. If the an-
swer class matches the expected answer
class (derived from the question by the an-
swer identification module) this feature fires
(i.e., it has a value of 1). Details of this mod-
ule are explained in Hovy et al. (2002). This
is a binary-valued feature.
3. Question Word Absent: Usually a correct
answer sentence contains a few of the ques-
tion words. This feature fires if the candidate
answer does not contain any of the question
words. This is also a binary valued feature.
4. Word Match: It is the sum of ITF2 values for
the words in the question that matches iden-
tically with the words in the answer sen-
tence. This is a positive continuous valued
feature.
</listItem>
<subsectionHeader confidence="0.988137">
2.4 Training
</subsectionHeader>
<bodyText confidence="0.999776333333333">
We train our Maximum Entropy model using
Generalized Iterative scaling (Darroch and
Ratcliff, 1972) approach by using YASMET3 .
</bodyText>
<sectionHeader confidence="0.996579" genericHeader="method">
3 Evaluation Metric
</sectionHeader>
<bodyText confidence="0.999834888888889">
The performance of the QA system is highly
dependent on the performance of the two indi-
vidual modules IR and answer-pinpointing. The
system would have excellent performance if
both have good accuracy. Hence, we need a
good evaluation metric to evaluate each of these
components individually. One standard metric
for IR is recall and precision. We can modify
this metric for QA as follows:
</bodyText>
<footnote confidence="0.844096333333333">
2 ITF = Inverse Term Frequency. We take a large inde-
pendent corpus &amp; estimate ITF(W) =1/(count(W)), where
W = Word.
3 YASMET. (Yet Another Small Maximum Entropy
Toolkit) http://www-i6.informatik.rwth-aachen.de/
Colleagues/och/software/YASMET.html
</footnote>
<table confidence="0.9557036">
Question:
1395 Who is Tom Cruise married to ?
IR Output:
1 Tom Cruise is married to actress Nicole Kidman and they have two adopted children .
2 Tom Cruise is married to Nicole Kidman .
.
.
Output of Chunker: (The number to the left of each chunk records the IR sentence from
which that particular chunk came)
1 Tom Cruise
</table>
<figure confidence="0.85995452">
1 Tom
1 Cruise
1 is married
1 married
1 actress Nicole Kidman and they
1 actress Nicole Kidman
1 actress
1 Nicole Kidman
1 Nicole
1 Kidman
1 they
1 two adopted children
1 two
1 adopted
1 children
2 Tom Cruise
2 Tom
2 Cruise
2 is married
2 married
2 Nicole Kidman
2 Nicole
2 Kidman
.
.
</figure>
<figureCaption confidence="0.995581">
Figure 1 : Candidate answer extraction for a question.
</figureCaption>
<table confidence="0.9785125">
Recall = # relevant answer segment returned
# Total relevant answer segments
Precision = # relevant answer segments returned
# Total segments returned
</table>
<bodyText confidence="0.99994564516129">
It is almost impossible to measure recall be-
cause the IR collection is typically large and in-
volves several hundreds of thousands of
documents. Hence, we evaluate our IR by only
the precision measure at top N segments. This
method is actually a rather sloppy approximation
to the original recall and precision measure.
Questions with fewer correct answers in the col-
lection would have a lower precision score as
compared to questions with many answers.
Similarly, it is unclear how one would evaluate
answer questions with No Answer (NIL) in the
collection using this metric. All these questions
would have zero precision from the IR collec-
tion.
The answer-pinpointing module is evaluated
by checking if the answer returned by the system
as the top ranked (#1) answer is correct/incorrect
with respect to the IR collection and the true
answer. Hence, if the IR system fails to return
even a single sentence that contains the correct
answer for the given question, we do not penal-
ize the answer-pinpointing module. It is again
unclear how to evaluate questions with No an-
swer (NIL). (Here, for our experiments we at-
tribute the error to the IR module.)
Finally, the combined system is evaluated by
using the standard technique, wherein the An-
swer (ranked #1) returned by the system is
judged to be either correct or incorrect and then
the average is taken.
</bodyText>
<sectionHeader confidence="0.999772" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.942186">
4.1 Framework
Information Retrieval
</subsectionHeader>
<bodyText confidence="0.999459909090909">
For our experiments, we use the Web search
engine AltaVista. For every question, we re-
move stop-words and present all other question
words as query to the Web search engine. The
top relevant documents are downloaded. We
apply a sentence segmentor, and identify those
sentences that have high ITF overlapping words
with the given question. The sentences are then
re-ranked accordingly and only the top K sen-
tences (segments) are presented as output of the
IR system.
</bodyText>
<subsectionHeader confidence="0.936144">
Candidate Answer Extraction
</subsectionHeader>
<bodyText confidence="0.999870666666667">
For a given question, the IR returns top K
segments. For our experiments a segment con-
sists of one sentence. We parse each of the sen-
tences and obtain a set of chunks, where each
chunk is a node of the parse tree. Each chunk is
viewed as a potential answer. For our experi-
ments we restrict the number of potential an-
swers to be at most 5000. We illustrate this
process in Figure 1.
</bodyText>
<subsectionHeader confidence="0.308062">
Training/Test Data
</subsectionHeader>
<tableCaption confidence="0.984552">
Table 2 : Training size and sources.
</tableCaption>
<table confidence="0.9975254">
Training + Test
Validation
Question collec- TREC 9 + TREC11
tion TREC 10
Total questions 1192 500
</table>
<bodyText confidence="0.999966266666667">
We use the TREC 9 and TREC 10 data sets
for training and the TREC 11 data set for testing.
We initially apply the IR step as described above
and obtain a set of at most 5000 answers. For
each such answer we use the pattern file sup-
plied by NIST to tag answer chunks as either
correct (1) or incorrect (0). This is a very noisy
way of tagging data. In some cases, even though
the answer chunk may be tagged as correct it
may not be supported by the accompanying sen-
tence, while in other cases, a correct chunk may
be graded as incorrect, since the pattern file list
did not represent a exhaustive list of answers.
We set aside 20% of the training data for valida-
tion.
</bodyText>
<subsectionHeader confidence="0.997626">
4.2 Classifier vs. Re-Ranker
</subsectionHeader>
<bodyText confidence="0.999965028571429">
We evaluate the performance of the QA system
viewed as a classifier (with a post-processing
step) and as a re-ranker. In order to do a fair
evaluation of the system we test the performance
of the QA system under varying conditions of
the output of the IR system. The results are
shown in Table 3.
The results should be read in the following
way: We use the same IR system. However, dur-
ing each run of the experiment we consider only
the top K sentences returned by the IR system
K={1,10,50,100,150,200}. The column “cor-
rect” represents the number of questions the en-
tire QA (IR + re-ranker) system answered
correctly. “IR Loss” represents the average
number of questions for which the IR failed
completely (i.e., the IR did not return even a sin-
gle sentence that contains the correct answer).
The IR precision is the precision of the IR sys-
tem for the number of sentences considered. An-
swer-pinpointing performance is based on the
metric described above. Finally, the overall
score is the score of the entire QA system. (i.e.,
precision at rank#1).
The “Overall Precision&amp;quot; column indicates
that the re-ranker clearly outperforms the classi-
fier. However, it is also very interesting to com-
pare the performance of the re-ranker “Overall
Precision” with the “Answer-Pinpointing preci-
sion”. For example, in the last row, for the re-
ranker the “Answer-Pinpointing Precision” is
0.5182 whereas the “Overall Precision” is only
0.34. The difference is due to the performance of
the poor performance of the IR system (“IR
Loss” = 0.344).
</bodyText>
<tableCaption confidence="0.9882">
Table 3 : Results for Classifier and Re-ranker under varying conditions of IR.
</tableCaption>
<table confidence="0.997822928571429">
IR Sen- Total IR Precision IR Loss Answer-Pinpointing Number Correct Overall Precision
tences ques- Precision
tions
Classifier Re-ranker Classifier Re-ranker Classifier Re-ranker
1 500 0.266 0.742 0.0027 0.3565 29 46 0.058 0.092
10 500 0.2018 0.48 0.0016 0.4269 7 111 0.014 0.222
50 500 0.1155 0.386 0.0015 0.4885 6 150 0.012 0.3
100 500 0.0878 0.362 0.0015 0.5015 5 160 0.01 0.32
150 500 0.0763 0.35 0.0015 0.5138 5 167 0.01 0.334
200 500 0.0703 0.344 0.0015 0.5182 3 170 0.01 0.34
IR Sentences = Total IR sentences considered for every question
IR Precision = Precision @ (IR Sentences)
IR Loss = (Number of Questions for which the IR did not produce a single answer)/(Total Questions)
Overall Precision = (Number Correct)/(Total Questions)
</table>
<subsectionHeader confidence="0.950018">
4.3 Oracle IR system
</subsectionHeader>
<bodyText confidence="0.999327461538462">
In order to determine the performance of the
answer pinpointing module alone, we perform
the so-called oracle IR experiment. Here, we
present to the answer pinpointing module only
those sentences from IR that contain an answer4.
The task of the answer pinpointing module is to
pick out of the correct answer from the given
collection. We report results in Table 4. In these
results too the re-ranker has better performance
as compared to the classifier. However, as we
see from the results, there is a lot of room for
improvement for the re-ranker system, even with
a perfect IR system.
</bodyText>
<sectionHeader confidence="0.999785" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9633475">
Our experiments clearly indicate that the QA
system viewed as a re-ranker outperforms the
QA system viewed as a classifier. The difference
stem from the following reasons:
1. The classification training criteria work on a
more difficult objective function of trying to
find whether each candidate answer answers
the given question, as opposed to trying to
find the best answer for the given question.
Hence, the same feature set that works for
the re-ranker need not work for the classi-
fier. The feature set used in this problem is
not good enough to help the classifier dis-
tinguish between correct and incorrect an-
4 This was performed by extracting all the sentences that
were judged to have the correct answer by human evalua-
tors during the TREC 2002 evaluations.
swers for the given question (even though it
is good for the re-ranker to come up with the
best answer).
2. The comparison of probabilities across dif-
ferent events (histories) for the classifier,
during the decision rule process, is problem-
atic. This is because the probabilities, which
we obtain after the classification approach,
are only a poor estimate of the true probabil-
ity. The re-ranker, however, directly allows
these probabilities to be comparable by in-
corporating them into the model itself.
3. The QA system viewed as a classifier suf-
fers from the problem of a highly unbal-
anced data set. We have less than 1%
positive examples and more than 99% nega-
tive examples (we had almost 4 million
training data events) in the problem. Ittyche-
riah (2001), and Ittycheriah and Roukos
(2002), use a more controlled environment
for training their system. They have 23%
positive examples and 77% negative exam-
ples. They prune out most of the incorrect
answer initially, using a pre-processing step
by using either a rule-based system (Ittyche-
riah, 2001) or a statistical system (Ittyche-
riah et al., 2002); and hence obtain a much
more manageable distribution in the training
phase of the Maximum Entropy model.
</bodyText>
<tableCaption confidence="0.996526">
Table 4 : Performance with a perfect IR system
</tableCaption>
<table confidence="0.99921525">
Total ques- IR precision Answer-Pinpointing
tions Precision
Classifier Re-ranker
429 1.0 0.156 0.578
</table>
<sectionHeader confidence="0.998105" genericHeader="method">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.99998515625">
The re-ranker system is very robust in handling
large amounts of data and still produces reason-
able results. There is no need for a major pre-
processing step (for eliminating undesirable in-
correct answers from the training) or the post-
processing step (for selecting the most promis-
ing answer.)
We also consider it significant that a QA sys-
tem with just 4 features (viz. Frequency,
Expected Answer Type, Question word absent,
and ITF word match) is a good baseline system
and performs better than the median perform-
ance of all the QA systems in the TREC 2002
evaluations5.
Ittycheriah (2001), and Ittycheriah and Rou-
kos (2002) have shown good results by using a
range of features for Maximum Entropy QA sys-
tems. Also, the results indicate that there is
scope for research in IR for QA systems. The
QA system has an upper ceiling on performance
due to the quality of the IR system. The QA
community has yet to address these problems in
a principled way, and the IR details of most of
the system are hidden behind the complicated
system architecture.
The re-ranking model basically changes the
objective function for training and the system is
directly optimized on the evaluation function
criteria (though still using Maximum Likelihood
training). Also this approach seems to be very
robust to noisy training data and is highly scal-
able.
</bodyText>
<sectionHeader confidence="0.993745" genericHeader="conclusions">
Acknowledgements.
</sectionHeader>
<bodyText confidence="0.941379214285714">
This work was supported by the Advance Re-
search and Development Activity (ARDA)’s
Advanced Question Answering for Intelligence
(AQUAINT) Program under contract number
5 However, since the IR system used here was from the
Web, our results are not directly comparable with the
TREC systems.
MDA908-02-C-007. The authors wish to ex-
press particular gratitude to Dr. Abraham It-
tycheriah, both for his supervision and education
of the first author during his summer visit to
IBM TJ Watson Research Center in 2002 and
for his thoughtful comments on this paper,
which was inspired by his work.
</bodyText>
<sectionHeader confidence="0.99944" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999925225806452">
Darroch, J. N., and D. Ratcliff. 1972. Generalized
iterative scaling for log-linear models. Annals of
Mathematical Statistics, 43:1470–1480.
Hermjakob, U. 1997. Learning Parse and Translation
Decisions from Examples with Rich Context.
Ph.D. Dissertation, University of Texas at Austin,
Austin, TX.
Hovy, E.H., U. Hermjakob, D. Ravichandran. 2002.
A Question/Answer Typology with Surface Text
Patterns. Proceedings of the DARPA Human Lan-
guage Technology Conferenc,. San Diego, CA,
247–250.
Ittycheriah, A. 2001. Trainable Question Answering
System. Ph.D. Dissertation, Rutgers, The State
University of New Jersey, New Brunswick, NJ.
Ittycheriah., A., and S. Roukos. 2002. IBM’S Ques-
tion Answering System-TREC-11. Proceedings of
TREC 2002, NIST, MD, 394–401.
Magnini, B, M. Negri, R. Prevete, and H. Tanev.
2002. Is it the Right Answer? Exploiting Web Re-
dundancy for Answer Validation. Proceedings of
the 40th Meeting of the Association of Computa-
tional Linguistics, Philadelphia, PA, 425–432.
Della Pietra, S., V. Della Pietra, and J. Lafferty.
1995. Inducing Features of Random Fields, Tech-
nical Report Department of Computer Science,
Carnegie-Mellon University, CMU–CS-95–144.
Xu, J., A. J. Licuanan, S. May, R. Miller, and R.
Weischedel. 2002. TREC2002QA at BBN: An-
swer Selection and Confidence Estimation. Pro-
ceedings of TREC 2002. NIST MD. 290–295
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.974770">
<title confidence="0.997158">Statistical QA - Classifier vs. Re-ranker: What’s the difference?</title>
<author confidence="0.998845">Deepak Ravichandran</author>
<author confidence="0.998845">Eduard Hovy</author>
<author confidence="0.998845">Franz Josef</author>
<affiliation confidence="0.9996195">Information Sciences University of Southern</affiliation>
<address confidence="0.992061">4676 Admiralty Marina del Rey, CA 90292</address>
<email confidence="0.999232">ravichan@isi.edu</email>
<email confidence="0.999232">hovy@isi.edu</email>
<email confidence="0.999232">och@isi.edu</email>
<abstract confidence="0.999671642857143">In this paper, we show that we can obtain a good baseline performance for Question Answering (QA) by using only 4 simple features. Using these features, we contrast two approaches used for a Maximum Entropy based QA system. We view the QA problem as a classification problem and as a reranking problem. Our results indicate that the QA system viewed as a reranker clearly outperforms the QA system used as a classifier. Both systems are trained using the same data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>43--1470</pages>
<contexts>
<context position="8536" citStr="Darroch and Ratcliff, 1972" startWordPosition="1677" endWordPosition="1680"> a value of 1). Details of this module are explained in Hovy et al. (2002). This is a binary-valued feature. 3. Question Word Absent: Usually a correct answer sentence contains a few of the question words. This feature fires if the candidate answer does not contain any of the question words. This is also a binary valued feature. 4. Word Match: It is the sum of ITF2 values for the words in the question that matches identically with the words in the answer sentence. This is a positive continuous valued feature. 2.4 Training We train our Maximum Entropy model using Generalized Iterative scaling (Darroch and Ratcliff, 1972) approach by using YASMET3 . 3 Evaluation Metric The performance of the QA system is highly dependent on the performance of the two individual modules IR and answer-pinpointing. The system would have excellent performance if both have good accuracy. Hence, we need a good evaluation metric to evaluate each of these components individually. One standard metric for IR is recall and precision. We can modify this metric for QA as follows: 2 ITF = Inverse Term Frequency. We take a large independent corpus &amp; estimate ITF(W) =1/(count(W)), where W = Word. 3 YASMET. (Yet Another Small Maximum Entropy T</context>
</contexts>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>Darroch, J. N., and D. Ratcliff. 1972. Generalized iterative scaling for log-linear models. Annals of Mathematical Statistics, 43:1470–1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Hermjakob</author>
</authors>
<title>Learning Parse and Translation Decisions from Examples with Rich Context.</title>
<date>1997</date>
<institution>Ph.D. Dissertation, University of Texas at Austin,</institution>
<location>Austin, TX.</location>
<marker>Hermjakob, 1997</marker>
<rawString>Hermjakob, U. 1997. Learning Parse and Translation Decisions from Examples with Rich Context. Ph.D. Dissertation, University of Texas at Austin, Austin, TX.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E H Hovy</author>
<author>U Hermjakob</author>
<author>D Ravichandran</author>
</authors>
<title>A Question/Answer Typology with Surface Text Patterns.</title>
<date>2002</date>
<booktitle>Proceedings of the DARPA Human Language Technology Conferenc,.</booktitle>
<pages>247--250</pages>
<location>San Diego, CA,</location>
<contexts>
<context position="7983" citStr="Hovy et al. (2002)" startWordPosition="1582" endWordPosition="1585">feature. This is a positive continuous valued feature. 2. Expected Answer Class: Most of the current QA systems employ some type of Answer Class Identification module. Thus questions like “When did Bill Clinton go to college?”, would be identified as a question asking about a time (or a time period), “Where is the sea of tranquility?” would be identified as a question asking for a location. If the answer class matches the expected answer class (derived from the question by the answer identification module) this feature fires (i.e., it has a value of 1). Details of this module are explained in Hovy et al. (2002). This is a binary-valued feature. 3. Question Word Absent: Usually a correct answer sentence contains a few of the question words. This feature fires if the candidate answer does not contain any of the question words. This is also a binary valued feature. 4. Word Match: It is the sum of ITF2 values for the words in the question that matches identically with the words in the answer sentence. This is a positive continuous valued feature. 2.4 Training We train our Maximum Entropy model using Generalized Iterative scaling (Darroch and Ratcliff, 1972) approach by using YASMET3 . 3 Evaluation Metri</context>
</contexts>
<marker>Hovy, Hermjakob, Ravichandran, 2002</marker>
<rawString>Hovy, E.H., U. Hermjakob, D. Ravichandran. 2002. A Question/Answer Typology with Surface Text Patterns. Proceedings of the DARPA Human Language Technology Conferenc,. San Diego, CA, 247–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
</authors>
<title>Trainable Question Answering System.</title>
<date>2001</date>
<tech>Ph.D. Dissertation,</tech>
<institution>Rutgers, The State University of New Jersey,</institution>
<location>New Brunswick, NJ.</location>
<contexts>
<context position="2038" citStr="Ittycheriah, 2001" startWordPosition="334" endWordPosition="335">The answer pinpointing module processes each of these segments and finds the appropriate answer phrase. 1 In our experiments we use R=1 phrase. Evaluation of a QA system is judged on the basis on the final output answer and the corresponding evidence provided by the segment. This paper focuses on the answer pinpointing module. Typical QA systems perform re-ranking of candidate answers as an important step in pinpointing. The goal is to rank the most likely answer first by using either symbolic or statistical methods. Some QA systems make use of statistical answer pinpointing (Xu et. al, 2002; Ittycheriah, 2001; Ittycheriah and Salim, 2002) by treating it as a classification problem. In this paper, we cast the pinpointing problem in a statistical framework and compare two approaches, classification and re-ranking. 2 Statistical Answer Pinpointing 2.1 Answer Modeling The answer-pinpointing module gets as input a question q and a set of possible answer candidates { a1 a2...aA } . It outputs one of the answer a E {a1 a2...aA } from the candidate answer set. We consider two ways of modeling this problem. One approach is the traditional classification view (Ittycheriah, 2001) where we present each Questi</context>
<context position="17661" citStr="Ittycheriah (2001)" startWordPosition="3243" endWordPosition="3245">across different events (histories) for the classifier, during the decision rule process, is problematic. This is because the probabilities, which we obtain after the classification approach, are only a poor estimate of the true probability. The re-ranker, however, directly allows these probabilities to be comparable by incorporating them into the model itself. 3. The QA system viewed as a classifier suffers from the problem of a highly unbalanced data set. We have less than 1% positive examples and more than 99% negative examples (we had almost 4 million training data events) in the problem. Ittycheriah (2001), and Ittycheriah and Roukos (2002), use a more controlled environment for training their system. They have 23% positive examples and 77% negative examples. They prune out most of the incorrect answer initially, using a pre-processing step by using either a rule-based system (Ittycheriah, 2001) or a statistical system (Ittycheriah et al., 2002); and hence obtain a much more manageable distribution in the training phase of the Maximum Entropy model. Table 4 : Performance with a perfect IR system Total ques- IR precision Answer-Pinpointing tions Precision Classifier Re-ranker 429 1.0 0.156 0.578</context>
</contexts>
<marker>Ittycheriah, 2001</marker>
<rawString>Ittycheriah, A. 2001. Trainable Question Answering System. Ph.D. Dissertation, Rutgers, The State University of New Jersey, New Brunswick, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ittycheriah</author>
<author>S Roukos</author>
</authors>
<date>2002</date>
<booktitle>IBM’S Question Answering System-TREC-11. Proceedings of TREC 2002, NIST, MD,</booktitle>
<pages>394--401</pages>
<contexts>
<context position="17696" citStr="Ittycheriah and Roukos (2002)" startWordPosition="3247" endWordPosition="3250">(histories) for the classifier, during the decision rule process, is problematic. This is because the probabilities, which we obtain after the classification approach, are only a poor estimate of the true probability. The re-ranker, however, directly allows these probabilities to be comparable by incorporating them into the model itself. 3. The QA system viewed as a classifier suffers from the problem of a highly unbalanced data set. We have less than 1% positive examples and more than 99% negative examples (we had almost 4 million training data events) in the problem. Ittycheriah (2001), and Ittycheriah and Roukos (2002), use a more controlled environment for training their system. They have 23% positive examples and 77% negative examples. They prune out most of the incorrect answer initially, using a pre-processing step by using either a rule-based system (Ittycheriah, 2001) or a statistical system (Ittycheriah et al., 2002); and hence obtain a much more manageable distribution in the training phase of the Maximum Entropy model. Table 4 : Performance with a perfect IR system Total ques- IR precision Answer-Pinpointing tions Precision Classifier Re-ranker 429 1.0 0.156 0.578 6 Conclusion The re-ranker system </context>
</contexts>
<marker>Ittycheriah, Roukos, 2002</marker>
<rawString>Ittycheriah., A., and S. Roukos. 2002. IBM’S Question Answering System-TREC-11. Proceedings of TREC 2002, NIST, MD, 394–401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Magnini</author>
<author>M Negri</author>
<author>R Prevete</author>
<author>H Tanev</author>
</authors>
<title>Is it the Right Answer? Exploiting Web Redundancy for Answer Validation.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Meeting of the Association of Computational Linguistics,</booktitle>
<pages>425--432</pages>
<location>Philadelphia, PA,</location>
<marker>Magnini, Negri, Prevete, Tanev, 2002</marker>
<rawString>Magnini, B, M. Negri, R. Prevete, and H. Tanev. 2002. Is it the Right Answer? Exploiting Web Redundancy for Answer Validation. Proceedings of the 40th Meeting of the Association of Computational Linguistics, Philadelphia, PA, 425–432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Della Pietra</author>
<author>V Della Pietra S</author>
<author>J Lafferty</author>
</authors>
<title>Inducing Features of Random Fields,</title>
<date>1995</date>
<tech>Technical Report</tech>
<institution>Department of Computer Science, Carnegie-Mellon University,</institution>
<marker>Pietra, S, Lafferty, 1995</marker>
<rawString>Della Pietra, S., V. Della Pietra, and J. Lafferty. 1995. Inducing Features of Random Fields, Technical Report Department of Computer Science, Carnegie-Mellon University, CMU–CS-95–144.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Xu</author>
<author>A J Licuanan</author>
<author>S May</author>
<author>R Miller</author>
<author>R Weischedel</author>
</authors>
<title>TREC2002QA at BBN: Answer Selection and Confidence Estimation.</title>
<date>2002</date>
<booktitle>Proceedings of TREC 2002. NIST MD.</booktitle>
<pages>290--295</pages>
<marker>Xu, Licuanan, May, Miller, Weischedel, 2002</marker>
<rawString>Xu, J., A. J. Licuanan, S. May, R. Miller, and R. Weischedel. 2002. TREC2002QA at BBN: Answer Selection and Confidence Estimation. Proceedings of TREC 2002. NIST MD. 290–295</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>